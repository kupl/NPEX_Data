{
    "hive_00a116a": {
        "bug_id": "hive_00a116a",
        "commit": "https://github.com/apache/hive/commit/00a116a5a4a9e0c5b9c96887dbf1edb5f02e4c09",
        "file": [
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hive/blob/00a116a5a4a9e0c5b9c96887dbf1edb5f02e4c09/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java?ref=00a116a5a4a9e0c5b9c96887dbf1edb5f02e4c09",
                "deletions": 14,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "patch": "@@ -599,6 +599,24 @@ public Path getDefaultDestDir(Configuration conf) throws LoginException, IOExcep\n \n     // we need the directory on hdfs to which we shall put all these files\n     // Use HIVE_JAR_DIRECTORY only if it's set explicitly; otherwise use default directory\n+    String hdfsDirPathStr = getHiveJarDirectory(conf);\n+\n+    String allFiles = auxJars + \",\" + addedJars + \",\" + addedFiles + \",\" + addedArchives;\n+    String[] allFilesArr = allFiles.split(\",\");\n+    for (String file : allFilesArr) {\n+      if (!StringUtils.isNotBlank(file)) {\n+        continue;\n+      }\n+      String hdfsFilePathStr = hdfsDirPathStr + \"/\" + getResourceBaseName(file);\n+      LocalResource localResource = localizeResource(new Path(file),\n+          new Path(hdfsFilePathStr), conf);\n+      tmpResources.add(localResource);\n+    }\n+\n+    return tmpResources;\n+  }\n+\n+  public String getHiveJarDirectory(Configuration conf) throws IOException, LoginException {\n     FileStatus fstatus = null;\n     String hdfsDirPathStr = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_JAR_DIRECTORY, null);\n     if (hdfsDirPathStr != null) {\n@@ -618,20 +636,7 @@ public Path getDefaultDestDir(Configuration conf) throws LoginException, IOExcep\n       Path destDir = getDefaultDestDir(conf);\n       hdfsDirPathStr = destDir.toString();\n     }\n-\n-    String allFiles = auxJars + \",\" + addedJars + \",\" + addedFiles + \",\" + addedArchives;\n-    String[] allFilesArr = allFiles.split(\",\");\n-    for (String file : allFilesArr) {\n-      if (!StringUtils.isNotBlank(file)) {\n-        continue;\n-      }\n-      String hdfsFilePathStr = hdfsDirPathStr + \"/\" + getResourceBaseName(file);\n-      LocalResource localResource = localizeResource(new Path(file),\n-          new Path(hdfsFilePathStr), conf);\n-      tmpResources.add(localResource);\n-    }\n-\n-    return tmpResources;\n+    return hdfsDirPathStr;\n   }\n \n   // the api that finds the jar being used by this class on disk",
                "raw_url": "https://github.com/apache/hive/raw/00a116a5a4a9e0c5b9c96887dbf1edb5f02e4c09/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "sha": "e20d8bf386c654584999733313536e39e202414f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/00a116a5a4a9e0c5b9c96887dbf1edb5f02e4c09/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java?ref=00a116a5a4a9e0c5b9c96887dbf1edb5f02e4c09",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java",
                "patch": "@@ -247,7 +247,7 @@ private Path createTezDir(String sessionId)\n    */\n   private LocalResource createHiveExecLocalResource()\n     throws IOException, LoginException, URISyntaxException {\n-    String hiveJarDir = conf.getVar(HiveConf.ConfVars.HIVE_JAR_DIRECTORY);\n+    String hiveJarDir = utils.getHiveJarDirectory(conf);\n     String currentVersionPathStr = utils.getExecJarPathLocal();\n     String currentJarName = utils.getResourceBaseName(currentVersionPathStr);\n     FileSystem fs = null;",
                "raw_url": "https://github.com/apache/hive/raw/00a116a5a4a9e0c5b9c96887dbf1edb5f02e4c09/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java",
                "sha": "3262ae3ef527059e29e9f2a4af17b1a921ce97e5",
                "status": "modified"
            }
        ],
        "message": "HIVE-6690 : NPE in tez session state (Sergey Shelukhin, reviewed by Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1579293 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/4376224ce5f08fff4ddcf32ccee01b41c1c890ad",
        "patched_files": [
            "TezSessionState.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestTezSessionState.java"
        ]
    },
    "hive_01526cf": {
        "bug_id": "hive_01526cf",
        "commit": "https://github.com/apache/hive/commit/01526cfa663c24e903ba1c493deb308efdd5439e",
        "file": [
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hive/blob/01526cfa663c24e903ba1c493deb308efdd5439e/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDate.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDate.java?ref=01526cfa663c24e903ba1c493deb308efdd5439e",
                "deletions": 2,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDate.java",
                "patch": "@@ -33,6 +33,7 @@\n import org.apache.hadoop.hive.serde2.io.TimestampWritable;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter;\n import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;\n@@ -63,14 +64,20 @@\n   public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {\n     if (arguments.length != 1) {\n       throw new UDFArgumentLengthException(\n-          \"to_date() requires 1 argument, got \" + arguments.length);\n+        \"to_date() requires 1 argument, got \" + arguments.length);\n+    }\n+    if (arguments[0].getCategory() != Category.PRIMITIVE) {\n+      throw new UDFArgumentException(\"to_date() only accepts STRING/TIMESTAMP/DATEWRITABLE types, got \"\n+          + arguments[0].getTypeName());\n     }\n     argumentOI = (PrimitiveObjectInspector) arguments[0];\n     inputType = argumentOI.getPrimitiveCategory();\n     ObjectInspector outputOI = PrimitiveObjectInspectorFactory.writableStringObjectInspector;\n     switch (inputType) {\n+    case CHAR:\n+    case VARCHAR:\n     case STRING:\n-      // textConverter = new TextConverter(argumentOI);\n+      inputType = PrimitiveCategory.STRING;\n       textConverter = ObjectInspectorConverters.getConverter(\n         argumentOI, PrimitiveObjectInspectorFactory.writableStringObjectInspector);\n       break;\n@@ -91,6 +98,10 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen\n \n   @Override\n   public Object evaluate(DeferredObject[] arguments) throws HiveException {\n+    if (arguments[0].get() == null) {\n+      return null;\n+    }\n+\n     switch (inputType) {\n     case STRING:\n       Date date;",
                "raw_url": "https://github.com/apache/hive/raw/01526cfa663c24e903ba1c493deb308efdd5439e/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDate.java",
                "sha": "9ee948019189b8d38c55f7a82a460e7f9a60e101",
                "status": "modified"
            },
            {
                "additions": 55,
                "blob_url": "https://github.com/apache/hive/blob/01526cfa663c24e903ba1c493deb308efdd5439e/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToUnixTimeStamp.java",
                "changes": 78,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToUnixTimeStamp.java?ref=01526cfa663c24e903ba1c493deb308efdd5439e",
                "deletions": 23,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToUnixTimeStamp.java",
                "patch": "@@ -31,11 +31,18 @@\n import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorUDFUnixTimeStampString;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter;\n+import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.primitive.DateObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;\n import org.apache.hadoop.hive.serde2.objectinspector.primitive.StringObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.primitive.TimestampObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping;\n import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n \n /**\n  * deterministic version of UDFUnixTimeStamp. enforces argument\n@@ -46,10 +53,10 @@\n @VectorizedExpressions({VectorUDFUnixTimeStampLong.class, VectorUDFUnixTimeStampString.class})\n public class GenericUDFToUnixTimeStamp extends GenericUDF {\n \n-  private transient StringObjectInspector intputTextOI;\n   private transient DateObjectInspector inputDateOI;\n   private transient TimestampObjectInspector inputTimestampOI;\n-  private transient StringObjectInspector patternOI;\n+  private transient Converter inputTextConverter;\n+  private transient Converter patternConverter;\n \n   private transient String lasPattern = \"yyyy-MM-dd HH:mm:ss\";\n   private transient final SimpleDateFormat formatter = new SimpleDateFormat(lasPattern);\n@@ -62,26 +69,44 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen\n \n   protected void initializeInput(ObjectInspector[] arguments) throws UDFArgumentException {\n     if (arguments.length < 1) {\n-      throw new UDFArgumentLengthException(\"The function TO_UNIX_TIMESTAMP \" +\n+      throw new UDFArgumentLengthException(\"The function \" + getName().toUpperCase() +\n           \"requires at least one argument\");\n     }\n+    for (ObjectInspector argument : arguments) {\n+      if (arguments[0].getCategory() != Category.PRIMITIVE) {\n+        throw new UDFArgumentException(getName().toUpperCase() +\n+            \" only takes string/date/timestamp types, got \" + argument.getTypeName());\n+      }\n+    }\n \n-    if (arguments[0] instanceof StringObjectInspector) {\n-      intputTextOI = (StringObjectInspector) arguments[0];\n-      if (arguments.length > 1) {\n-        if (!(arguments[1] instanceof StringObjectInspector)) {\n-          throw new UDFArgumentException(\n-            \"The time pattern for \" + getName().toUpperCase() + \" should be string type\");\n+    PrimitiveObjectInspector arg1OI = (PrimitiveObjectInspector) arguments[0];\n+    switch (arg1OI.getPrimitiveCategory()) {\n+      case CHAR:\n+      case VARCHAR:\n+      case STRING:\n+        inputTextConverter = ObjectInspectorConverters.getConverter(arg1OI,\n+            PrimitiveObjectInspectorFactory.javaStringObjectInspector);\n+        if (arguments.length > 1) {\n+          PrimitiveObjectInspector arg2OI = (PrimitiveObjectInspector) arguments[1];\n+          if (PrimitiveObjectInspectorUtils.getPrimitiveGrouping(arg2OI.getPrimitiveCategory())\n+              != PrimitiveGrouping.STRING_GROUP) {\n+            throw new UDFArgumentException(\n+              \"The time pattern for \" + getName().toUpperCase() + \" should be string type\");\n+          }\n+          patternConverter = ObjectInspectorConverters.getConverter(arg2OI,\n+              PrimitiveObjectInspectorFactory.javaStringObjectInspector);\n         }\n-        patternOI = (StringObjectInspector) arguments[1];\n-      }\n-    } else if (arguments[0] instanceof DateObjectInspector) {\n-      inputDateOI = (DateObjectInspector) arguments[0];\n-    } else if (arguments[0] instanceof TimestampObjectInspector) {\n-      inputTimestampOI = (TimestampObjectInspector) arguments[0];\n-    } else {\n-      throw new UDFArgumentException(\n-          \"The function \" + getName().toUpperCase() + \" takes only string or timestamp types\");\n+        break;\n+\n+      case DATE:\n+        inputDateOI = (DateObjectInspector) arguments[0];\n+        break;\n+      case TIMESTAMP:\n+        inputTimestampOI = (TimestampObjectInspector) arguments[0];\n+        break;\n+      default:\n+        throw new UDFArgumentException(\n+            \"The function \" + getName().toUpperCase() + \" takes only string/date/timestamp types\");\n     }\n   }\n \n@@ -93,13 +118,20 @@ protected String getName() {\n \n   @Override\n   public Object evaluate(DeferredObject[] arguments) throws HiveException {\n-    if (intputTextOI != null) {\n-      String textVal = intputTextOI.getPrimitiveJavaObject(arguments[0].get());\n+    if (arguments[0].get() == null) {\n+      return null;\n+    }\n+\n+    if (inputTextConverter != null) {\n+      String textVal = (String) inputTextConverter.convert(arguments[0].get());\n       if (textVal == null) {\n         return null;\n       }\n-      if (patternOI != null) {\n-        String patternVal = patternOI.getPrimitiveJavaObject(arguments[1].get());\n+      if (patternConverter != null) {\n+        if (arguments[1].get() == null) {\n+          return null;\n+        }\n+        String patternVal = (String) patternConverter.convert(arguments[1].get());\n         if (patternVal == null) {\n           return null;\n         }\n@@ -118,7 +150,7 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {\n       retValue.set(inputDateOI.getPrimitiveWritableObject(arguments[0].get())\n                    .getTimeInSeconds());\n       return retValue;\n-\t}\n+    }\n     Timestamp timestamp = inputTimestampOI.getPrimitiveJavaObject(arguments[0].get());\n     retValue.set(timestamp.getTime() / 1000);\n     return retValue;",
                "raw_url": "https://github.com/apache/hive/raw/01526cfa663c24e903ba1c493deb308efdd5439e/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFToUnixTimeStamp.java",
                "sha": "65a2297fa12a877ed35b8c8b291fcbf384deb9ea",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hive/blob/01526cfa663c24e903ba1c493deb308efdd5439e/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDate.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDate.java?ref=01526cfa663c24e903ba1c493deb308efdd5439e",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDate.java",
                "patch": "@@ -45,6 +45,11 @@ public void testStringToDate() throws HiveException {\n     Text output = (Text) udf.evaluate(args);\n \n     assertEquals(\"to_date() test for STRING failed \", \"2009-07-30\", output.toString());\n+\n+    // Try with null args\n+    DeferredObject[] nullArgs = { new DeferredJavaObject(null) };\n+    output = (Text) udf.evaluate(nullArgs);\n+    assertNull(\"to_date() with null STRING\", output);\n   }\n \n   public void testTimestampToDate() throws HiveException {\n@@ -59,6 +64,11 @@ public void testTimestampToDate() throws HiveException {\n     Text output = (Text) udf.evaluate(args);\n \n     assertEquals(\"to_date() test for TIMESTAMP failed \", \"2009-07-30\", output.toString());\n+\n+    // Try with null args\n+    DeferredObject[] nullArgs = { new DeferredJavaObject(null) };\n+    output = (Text) udf.evaluate(nullArgs);\n+    assertNull(\"to_date() with null TIMESTAMP\", output);\n   }\n \n   public void testDateWritablepToDate() throws HiveException {\n@@ -72,6 +82,11 @@ public void testDateWritablepToDate() throws HiveException {\n     Text output = (Text) udf.evaluate(args);\n \n     assertEquals(\"to_date() test for DATEWRITABLE failed \", \"2009-07-30\", output.toString());\n+\n+    // Try with null args\n+    DeferredObject[] nullArgs = { new DeferredJavaObject(null) };\n+    output = (Text) udf.evaluate(nullArgs);\n+    assertNull(\"to_date() with null DATE\", output);\n   }\n \n }",
                "raw_url": "https://github.com/apache/hive/raw/01526cfa663c24e903ba1c493deb308efdd5439e/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDate.java",
                "sha": "0d40ff7cb3c47ed4c2a8d2e88d9d8c32eb87d09d",
                "status": "modified"
            },
            {
                "additions": 126,
                "blob_url": "https://github.com/apache/hive/blob/01526cfa663c24e903ba1c493deb308efdd5439e/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFToUnixTimestamp.java",
                "changes": 126,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFToUnixTimestamp.java?ref=01526cfa663c24e903ba1c493deb308efdd5439e",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFToUnixTimestamp.java",
                "patch": "@@ -0,0 +1,126 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.udf.generic;\n+\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredJavaObject;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredObject;\n+import org.apache.hadoop.hive.serde2.io.DateWritable;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritable;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.Text;\n+\n+import junit.framework.TestCase;\n+\n+public class TestGenericUDFToUnixTimestamp extends TestCase {\n+\n+  public static void runAndVerify(GenericUDFToUnixTimeStamp udf,\n+      Object arg, Object expected) throws HiveException {\n+    DeferredObject[] args = { new DeferredJavaObject(arg) };\n+    Object result = udf.evaluate(args);\n+    if (expected == null) {\n+      assertNull(result);\n+    } else {\n+      assertEquals(expected.toString(), result.toString());\n+    }\n+  }\n+\n+  public static void runAndVerify(GenericUDFToUnixTimeStamp udf,\n+      Object arg1, Object arg2, Object expected) throws HiveException {\n+    DeferredObject[] args = { new DeferredJavaObject(arg1), new DeferredJavaObject(arg2) };\n+    Object result = udf.evaluate(args);\n+    if (expected == null) {\n+      assertNull(result);\n+    } else {\n+      assertEquals(expected.toString(), result.toString());\n+    }\n+  }\n+\n+  public void testTimestamp() throws HiveException {\n+    GenericUDFToUnixTimeStamp udf = new GenericUDFToUnixTimeStamp();\n+    ObjectInspector valueOI = PrimitiveObjectInspectorFactory.writableTimestampObjectInspector;\n+    ObjectInspector[] arguments = {valueOI};\n+    udf.initialize(arguments);\n+\n+    Timestamp ts = Timestamp.valueOf(\"1970-01-01 00:00:00\");\n+    runAndVerify(udf,\n+        new TimestampWritable(ts),\n+        new LongWritable(ts.getTime() / 1000));\n+\n+    ts = Timestamp.valueOf(\"2001-02-03 01:02:03\");\n+    runAndVerify(udf,\n+        new TimestampWritable(ts),\n+        new LongWritable(ts.getTime() / 1000));\n+\n+    // test null values\n+    runAndVerify(udf, null, null);\n+  }\n+\n+  public void testDate() throws HiveException {\n+    GenericUDFToUnixTimeStamp udf = new GenericUDFToUnixTimeStamp();\n+    ObjectInspector valueOI = PrimitiveObjectInspectorFactory.writableDateObjectInspector;\n+    ObjectInspector[] arguments = {valueOI};\n+    udf.initialize(arguments);\n+\n+    Date date = Date.valueOf(\"1970-01-01\");\n+    runAndVerify(udf,\n+        new DateWritable(date),\n+        new LongWritable(date.getTime() / 1000));\n+\n+    // test null values\n+    runAndVerify(udf, null, null);\n+  }\n+\n+  public void testString() throws HiveException {\n+    GenericUDFToUnixTimeStamp udf1 = new GenericUDFToUnixTimeStamp();\n+    ObjectInspector valueOI = PrimitiveObjectInspectorFactory.writableStringObjectInspector;\n+    ObjectInspector[] arguments = {valueOI};\n+    udf1.initialize(arguments);\n+\n+    String val = \"2001-01-01 01:02:03\";\n+    runAndVerify(udf1,\n+        new Text(val),\n+        new LongWritable(Timestamp.valueOf(val).getTime() / 1000));\n+\n+    // test null values\n+    runAndVerify(udf1, null, null);\n+\n+    // Try 2-arg version\n+    GenericUDFToUnixTimeStamp udf2 = new GenericUDFToUnixTimeStamp();\n+    ObjectInspector[] args2 = {valueOI, valueOI};\n+    udf2.initialize(args2);\n+\n+    val = \"2001-01-01\";\n+    String format = \"yyyy-MM-dd\";\n+    runAndVerify(udf2,\n+        new Text(val),\n+        new Text(format),\n+        new LongWritable(Date.valueOf(val).getTime() / 1000));\n+\n+    // test null values\n+    runAndVerify(udf2, null, null, null);\n+    runAndVerify(udf2, null, new Text(format), null);\n+    runAndVerify(udf2, new Text(val), null, null);\n+  }\n+}",
                "raw_url": "https://github.com/apache/hive/raw/01526cfa663c24e903ba1c493deb308efdd5439e/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFToUnixTimestamp.java",
                "sha": "52d30d3007a0ef00ee67c2cecd7df24fb3fd5a32",
                "status": "added"
            }
        ],
        "message": "HIVE-6645 : to_date()/to_unix_timestamp() fail with NPE if input is null (Jason Dere via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1579500 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/bbdded414f1b841e8d40b4d735b565f1a2ffe066",
        "patched_files": [
            "GenericUDFToUnixTimeStamp.java",
            "GenericUDFDate.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestGenericUDFDate.java",
            "TestGenericUDFToUnixTimestamp.java"
        ]
    },
    "hive_01fe664": {
        "bug_id": "hive_01fe664",
        "commit": "https://github.com/apache/hive/commit/01fe6647430ef5dfcc120f93bdcab40677166c7c",
        "file": [
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStoreTxns.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStoreTxns.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c",
                "deletions": 0,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStoreTxns.java",
                "patch": "@@ -75,6 +75,21 @@ public void testTxns() throws Exception {\n     Assert.assertFalse(validTxns.isTxnCommitted(4));\n   }\n \n+  @Test\n+  public void testOpenTxnNotExcluded() throws Exception {\n+    List<Long> tids = client.openTxns(\"me\", 3).getTxn_ids();\n+    Assert.assertEquals(1L, (long) tids.get(0));\n+    Assert.assertEquals(2L, (long) tids.get(1));\n+    Assert.assertEquals(3L, (long) tids.get(2));\n+    client.rollbackTxn(1);\n+    client.commitTxn(2);\n+    ValidTxnList validTxns = client.getValidTxns(3);\n+    Assert.assertFalse(validTxns.isTxnCommitted(1));\n+    Assert.assertTrue(validTxns.isTxnCommitted(2));\n+    Assert.assertTrue(validTxns.isTxnCommitted(3));\n+    Assert.assertFalse(validTxns.isTxnCommitted(4));\n+  }\n+\n   @Test\n   public void testTxnRange() throws Exception {\n     ValidTxnList validTxns = client.getValidTxns();",
                "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/TestHiveMetaStoreTxns.java",
                "sha": "7f0a6b3b7daecb9cafb5034877d43ee0aeec882b",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c",
                "deletions": 1,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java",
                "patch": "@@ -1689,7 +1689,12 @@ public void cancelDelegationToken(String tokenStrForm) throws MetaException, TEx\n \n   @Override\n   public ValidTxnList getValidTxns() throws TException {\n-    return TxnHandler.createValidTxnList(client.get_open_txns());\n+    return TxnHandler.createValidTxnList(client.get_open_txns(), 0);\n+  }\n+\n+  @Override\n+  public ValidTxnList getValidTxns(long currentTxn) throws TException {\n+    return TxnHandler.createValidTxnList(client.get_open_txns(), currentTxn);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java",
                "sha": "94c22450e417b3ffee293aef5a848ddcfc339c49",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/metastore/src/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c",
                "deletions": 0,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java",
                "patch": "@@ -1085,6 +1085,15 @@ Function getFunction(String dbName, String funcName)\n    */\n   ValidTxnList getValidTxns() throws TException;\n \n+  /**\n+   * Get a structure that details valid transactions.\n+   * @param currentTxn The current transaction of the caller.  This will be removed from the\n+   *                   exceptions list so that the caller sees records from his own transaction.\n+   * @return list of valid transactions\n+   * @throws TException\n+   */\n+  ValidTxnList getValidTxns(long currentTxn) throws TException;\n+\n   /**\n    * Initiate a transaction.\n    * @param user User who is opening this transaction.  This is the Hive user,",
                "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/metastore/src/java/org/apache/hadoop/hive/metastore/IMetaStoreClient.java",
                "sha": "066ab68443fdd1bd04e14c8a3b1b2bbac9130d6c",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c",
                "deletions": 2,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java",
                "patch": "@@ -233,12 +233,22 @@ public GetOpenTxnsResponse getOpenTxns() throws MetaException {\n     }\n   }\n \n-  public static ValidTxnList createValidTxnList(GetOpenTxnsResponse txns) {\n+  /**\n+   * Transform a {@link org.apache.hadoop.hive.metastore.api.GetOpenTxnsResponse} to a\n+   * {@link org.apache.hadoop.hive.common.ValidTxnList}.\n+   * @param txns txn list from the metastore\n+   * @param currentTxn Current transaction that the user has open.  If this is greater than 0 it\n+   *                   will be removed from the exceptions list so that the user sees his own\n+   *                   transaction as valid.\n+   * @return a valid txn list.\n+   */\n+  public static ValidTxnList createValidTxnList(GetOpenTxnsResponse txns, long currentTxn) {\n     long highWater = txns.getTxn_high_water_mark();\n     Set<Long> open = txns.getOpen_txns();\n-    long[] exceptions = new long[open.size()];\n+    long[] exceptions = new long[open.size() - (currentTxn > 0 ? 1 : 0)];\n     int i = 0;\n     for(long txn: open) {\n+      if (currentTxn > 0 && currentTxn == txn) continue;\n       exceptions[i++] = txn;\n     }\n     return new ValidTxnListImpl(exceptions, highWater);",
                "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java",
                "sha": "6f44169af90e747838bd7e2ccb5652f59ac26475",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/Driver.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/Driver.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c",
                "deletions": 11,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java",
                "patch": "@@ -390,6 +390,9 @@ public int compile(String command, boolean resetTaskIds) {\n       tree = ParseUtils.findRootNonNullToken(tree);\n       perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.PARSE);\n \n+      // Initialize the transaction manager.  This must be done before analyze is called\n+      SessionState.get().initTxnMgr(conf);\n+\n       perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.ANALYZE);\n       BaseSemanticAnalyzer sem = SemanticAnalyzerFactory.get(conf, tree);\n       List<HiveSemanticAnalyzerHook> saHooks =\n@@ -889,9 +892,12 @@ private int recordValidTxns() {\n \n   /**\n    * Acquire read and write locks needed by the statement. The list of objects to be locked are\n-   * obtained from he inputs and outputs populated by the compiler. The lock acuisition scheme is\n+   * obtained from the inputs and outputs populated by the compiler. The lock acuisition scheme is\n    * pretty simple. If all the locks cannot be obtained, error out. Deadlock is avoided by making\n    * sure that the locks are lexicographically sorted.\n+   *\n+   * This method also records the list of valid transactions.  This must be done after any\n+   * transactions have been opened and locks acquired.\n    **/\n   private int acquireLocksAndOpenTxn() {\n     PerfLogger perfLogger = PerfLogger.getPerfLogger();\n@@ -931,7 +937,7 @@ private int acquireLocksAndOpenTxn() {\n \n       txnMgr.acquireLocks(plan, ctx, userFromUGI);\n \n-      return 0;\n+      return recordValidTxns();\n     } catch (LockException e) {\n       errorMessage = \"FAILED: Error in acquiring locks: \" + e.getMessage();\n       SQLState = ErrorMsg.findSQLState(e.getMessage());\n@@ -1108,11 +1114,6 @@ private CommandProcessorResponse runInternal(String command, boolean alreadyComp\n     SessionState ss = SessionState.get();\n     try {\n       ckLock = checkConcurrency();\n-      try {\n-        ss.initTxnMgr(conf);\n-      } catch (LockException e) {\n-        throw new SemanticException(e.getMessage(), e);\n-      }\n     } catch (SemanticException e) {\n       errorMessage = \"FAILED: Error in semantic analysis: \" + e.getMessage();\n       SQLState = ErrorMsg.findSQLState(e.getMessage());\n@@ -1121,11 +1122,8 @@ private CommandProcessorResponse runInternal(String command, boolean alreadyComp\n           + org.apache.hadoop.util.StringUtils.stringifyException(e));\n       return createProcessorResponse(10);\n     }\n-    int ret = recordValidTxns();\n-    if (ret != 0) {\n-      return createProcessorResponse(ret);\n-    }\n \n+    int ret;\n     if (!alreadyCompiled) {\n       ret = compileInternal(command);\n       if (ret != 0) {",
                "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/Driver.java",
                "sha": "4826abcc4fce6102642480c576bd823ec59f9743",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java",
                "patch": "@@ -286,7 +286,7 @@ public void heartbeat() throws LockException {\n   public ValidTxnList getValidTxns() throws LockException {\n     init();\n     try {\n-      return client.getValidTxns();\n+      return client.getValidTxns(txnId);\n     } catch (TException e) {\n       throw new LockException(ErrorMsg.METASTORE_COMMUNICATION_FAILED.getMsg(),\n           e);",
                "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DbTxnManager.java",
                "sha": "46b441a13d6d981dd6aea5dfc1c0a2abad884cc1",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java",
                "patch": "@@ -76,7 +76,7 @@ public void run() {\n         // don't doom the entire thread.\n         try {\n           ShowCompactResponse currentCompactions = txnHandler.showCompact(new ShowCompactRequest());\n-          ValidTxnList txns = TxnHandler.createValidTxnList(txnHandler.getOpenTxns());\n+          ValidTxnList txns = TxnHandler.createValidTxnList(txnHandler.getOpenTxns(), 0);\n           Set<CompactionInfo> potentials = txnHandler.findPotentialCompactions(abortedThreshold);\n           LOG.debug(\"Found \" + potentials.size() + \" potential compactions, \" +\n               \"checking to see if we should compact any of them\");",
                "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java",
                "sha": "c1d0fe11c709c90996ce4d18bd41199d3257d780",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java?ref=01fe6647430ef5dfcc120f93bdcab40677166c7c",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java",
                "patch": "@@ -120,7 +120,7 @@ public void run() {\n \n         final boolean isMajor = ci.isMajorCompaction();\n         final ValidTxnList txns =\n-            TxnHandler.createValidTxnList(txnHandler.getOpenTxns());\n+            TxnHandler.createValidTxnList(txnHandler.getOpenTxns(), 0);\n         final StringBuffer jobName = new StringBuffer(name);\n         jobName.append(\"-compactor-\");\n         jobName.append(ci.getFullPartitionName());",
                "raw_url": "https://github.com/apache/hive/raw/01fe6647430ef5dfcc120f93bdcab40677166c7c/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java",
                "sha": "249fece426f05b7c2757d8fd52c6959a80941091",
                "status": "modified"
            }
        ],
        "message": "HIVE-8203 ACID operations result in NPE when run through HS2 (Alan Gates, reviewed by Eugene Koifman)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1627914 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/b30756c2dac8b3d9adfdb131e3185498d4dd322b",
        "patched_files": [
            "TxnHandler.java",
            "DbTxnManager.java",
            "Worker.java",
            "IMetaStoreClient.java",
            "HiveMetaStoreClient.java",
            "Initiator.java",
            "Driver.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestInitiator.java",
            "TestHiveMetaStoreTxns.java",
            "TestWorker.java",
            "TestTxnHandler.java",
            "TestDbTxnManager.java"
        ]
    },
    "hive_0410bf1": {
        "bug_id": "hive_0410bf1",
        "commit": "https://github.com/apache/hive/commit/0410bf17a361514f88774cf0545ec07271a26ab8",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/0410bf17a361514f88774cf0545ec07271a26ab8/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java?ref=0410bf17a361514f88774cf0545ec07271a26ab8",
                "deletions": 9,
                "filename": "itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java",
                "patch": "@@ -28,6 +28,7 @@\n import org.apache.hadoop.hive.ql.io.orc.OrcSerde;\n import org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat;\n import org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat;\n+import org.apache.hadoop.hive.ql.io.parquet.VectorizedColumnReaderTestBase;\n import org.apache.hadoop.hive.ql.io.parquet.read.DataWritableReadSupport;\n import org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector;\n import org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe;\n@@ -332,15 +333,7 @@ public RecordReader getVectorizedRecordReader(Path inputPath) throws Exception {\n       // types.\n       conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);\n       conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, \"0,1,2,3,6\");\n-      conf.set(ReadSupport.PARQUET_READ_SCHEMA, \"test schema\");\n-      HiveConf.setBoolVar(conf, HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED, true);\n-      HiveConf.setVar(conf, HiveConf.ConfVars.PLAN, \"//tmp\");\n-      Job vectorJob = new Job(conf, \"read vector\");\n-      ParquetInputFormat.setInputPaths(vectorJob, inputPath);\n-      ParquetInputFormat parquetInputFormat = new ParquetInputFormat(GroupReadSupport.class);\n-      InputSplit split = (InputSplit) parquetInputFormat.getSplits(vectorJob).get(0);\n-      initialVectorizedRowBatchCtx(conf);\n-      return new VectorizedParquetRecordReader(split, new JobConf(conf));\n+      return VectorizedColumnReaderTestBase.createTestParquetReader(\"test schema\", conf);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/0410bf17a361514f88774cf0545ec07271a26ab8/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java",
                "sha": "e4d11fdb126e7687db70770179f8dcaf8d2f5b7b",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/VectorizedParquetRecordReader.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/VectorizedParquetRecordReader.java?ref=0410bf17a361514f88774cf0545ec07271a26ab8",
                "deletions": 18,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/VectorizedParquetRecordReader.java",
                "patch": "@@ -118,20 +118,6 @@\n    */\n   protected long totalRowCount = 0;\n \n-  @VisibleForTesting\n-  public VectorizedParquetRecordReader(\n-    InputSplit inputSplit,\n-    JobConf conf) {\n-    try {\n-      serDeStats = new SerDeStats();\n-      projectionPusher = new ProjectionPusher();\n-      initialize(inputSplit, conf);\n-    } catch (Throwable e) {\n-      LOG.error(\"Failed to create the vectorized reader due to exception \" + e);\n-      throw new RuntimeException(e);\n-    }\n-  }\n-\n   public VectorizedParquetRecordReader(\n       org.apache.hadoop.mapred.InputSplit oldInputSplit, JobConf conf) {\n     this(oldInputSplit, conf, null, null, null);\n@@ -146,6 +132,10 @@ public VectorizedParquetRecordReader(\n       this.cacheConf = cacheConf;\n       serDeStats = new SerDeStats();\n       projectionPusher = new ProjectionPusher();\n+      colsToInclude = ColumnProjectionUtils.getReadColumnIDs(conf);\n+      //initialize the rowbatchContext\n+      jobConf = conf;\n+      rbCtx = Utilities.getVectorizedRowBatchCtx(jobConf);\n       ParquetInputSplit inputSplit = getSplit(oldInputSplit, conf);\n       if (inputSplit != null) {\n         initialize(inputSplit, conf);\n@@ -171,10 +161,6 @@ private void initPartitionValues(FileSplit fileSplit, JobConf conf) throws IOExc\n   public void initialize(\n     InputSplit oldSplit,\n     JobConf configuration) throws IOException, InterruptedException {\n-    colsToInclude = ColumnProjectionUtils.getReadColumnIDs(configuration);\n-    //initialize the rowbatchContext\n-    jobConf = configuration;\n-    rbCtx = Utilities.getVectorizedRowBatchCtx(jobConf);\n     // the oldSplit may be null during the split phase\n     if (oldSplit == null) {\n       return;",
                "raw_url": "https://github.com/apache/hive/raw/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/vector/VectorizedParquetRecordReader.java",
                "sha": "1d9dba7842835b8024410ae8d15e9b6d2f159675",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hive/blob/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestVectorizedColumnReader.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestVectorizedColumnReader.java?ref=0410bf17a361514f88774cf0545ec07271a26ab8",
                "deletions": 4,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestVectorizedColumnReader.java",
                "patch": "@@ -22,15 +22,21 @@\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.io.IOConstants;\n import org.apache.hadoop.hive.ql.io.parquet.vector.VectorizedParquetRecordReader;\n+import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n+import org.apache.hadoop.mapred.FileSplit;\n import org.apache.hadoop.mapred.JobConf;\n-import org.apache.hadoop.mapreduce.InputSplit;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.parquet.hadoop.ParquetInputFormat;\n+import org.apache.parquet.hadoop.ParquetInputSplit;\n import org.junit.AfterClass;\n+import org.junit.Assert;\n import org.junit.BeforeClass;\n import org.junit.Test;\n \n import java.io.IOException;\n \n import static junit.framework.TestCase.assertFalse;\n+import static org.apache.parquet.hadoop.api.ReadSupport.PARQUET_READ_SCHEMA;\n \n public class TestVectorizedColumnReader extends VectorizedColumnReaderTestBase {\n   static boolean isDictionaryEncoding = false;\n@@ -97,16 +103,34 @@ public void decimalRead() throws Exception {\n     decimalRead(isDictionaryEncoding);\n   }\n \n+  private class TestVectorizedParquetRecordReader extends VectorizedParquetRecordReader {\n+    public TestVectorizedParquetRecordReader(\n+        org.apache.hadoop.mapred.InputSplit oldInputSplit, JobConf conf) {\n+      super(oldInputSplit, conf);\n+    }\n+    @Override\n+    protected ParquetInputSplit getSplit(\n+        org.apache.hadoop.mapred.InputSplit oldInputSplit, JobConf conf) {\n+      return null;\n+    }\n+  }\n+\n   @Test\n   public void testNullSplitForParquetReader() throws Exception {\n     Configuration conf = new Configuration();\n     conf.set(IOConstants.COLUMNS,\"int32_field\");\n     conf.set(IOConstants.COLUMNS_TYPES,\"int\");\n+    conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);\n+    conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, \"0\");\n+    conf.set(PARQUET_READ_SCHEMA, \"message test { required int32 int32_field;}\");\n     HiveConf.setBoolVar(conf, HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED, true);\n     HiveConf.setVar(conf, HiveConf.ConfVars.PLAN, \"//tmp\");\n+    Job vectorJob = new Job(conf, \"read vector\");\n+    ParquetInputFormat.setInputPaths(vectorJob, file);\n     initialVectorizedRowBatchCtx(conf);\n-    VectorizedParquetRecordReader reader =\n-        new VectorizedParquetRecordReader((InputSplit)null, new JobConf(conf));\n-    assertFalse(reader.next(reader.createKey(), reader.createValue()));\n+    FileSplit fsplit = getFileSplit(vectorJob);\n+    JobConf jobConf = new JobConf(conf);\n+    TestVectorizedParquetRecordReader testReader = new TestVectorizedParquetRecordReader(fsplit, jobConf);\n+    Assert.assertNull(\"Test should return null split from getSplit() method\", testReader.getSplit(fsplit, jobConf));\n   }\n }",
                "raw_url": "https://github.com/apache/hive/raw/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestVectorizedColumnReader.java",
                "sha": "81d8cffa85fabc4ccdb52a9ff6ece28b33a48ca5",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hive/blob/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/VectorizedColumnReaderTestBase.java",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/VectorizedColumnReaderTestBase.java?ref=0410bf17a361514f88774cf0545ec07271a26ab8",
                "deletions": 17,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/io/parquet/VectorizedColumnReaderTestBase.java",
                "patch": "@@ -41,6 +41,7 @@\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.mapred.FileSplit;\n import org.apache.hadoop.mapreduce.InputSplit;\n import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapreduce.Job;\n@@ -213,18 +214,22 @@ protected static boolean isNull(int index) {\n     return (index % NULL_FREQUENCY == 0);\n   }\n \n-  protected VectorizedParquetRecordReader createParquetReader(String schemaString, Configuration conf)\n+  public static VectorizedParquetRecordReader createTestParquetReader(String schemaString, Configuration conf)\n     throws IOException, InterruptedException, HiveException {\n     conf.set(PARQUET_READ_SCHEMA, schemaString);\n     HiveConf.setBoolVar(conf, HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED, true);\n     HiveConf.setVar(conf, HiveConf.ConfVars.PLAN, \"//tmp\");\n-\n     Job vectorJob = new Job(conf, \"read vector\");\n     ParquetInputFormat.setInputPaths(vectorJob, file);\n+    initialVectorizedRowBatchCtx(conf);\n+    return new VectorizedParquetRecordReader(getFileSplit(vectorJob),new JobConf(conf));\n+  }\n+\n+  protected static FileSplit getFileSplit(Job vectorJob) throws IOException, InterruptedException {\n     ParquetInputFormat parquetInputFormat = new ParquetInputFormat(GroupReadSupport.class);\n     InputSplit split = (InputSplit) parquetInputFormat.getSplits(vectorJob).get(0);\n-    initialVectorizedRowBatchCtx(conf);\n-    return new VectorizedParquetRecordReader(split, new JobConf(conf));\n+    FileSplit fsplit = new FileSplit(file,0L,split.getLength(),split.getLocations());\n+    return fsplit;\n   }\n \n   protected static void writeData(ParquetWriter<Group> writer, boolean isDictionaryEncoding) throws IOException {\n@@ -295,7 +300,7 @@ protected static void writeData(ParquetWriter<Group> writer, boolean isDictionar\n     writer.close();\n   }\n \n-  protected void initialVectorizedRowBatchCtx(Configuration conf) throws HiveException {\n+  protected static void initialVectorizedRowBatchCtx(Configuration conf) throws HiveException {\n     MapWork mapWork = new MapWork();\n     VectorizedRowBatchCtx rbCtx = new VectorizedRowBatchCtx();\n     rbCtx.init(createStructObjectInspector(conf), new String[0]);\n@@ -304,7 +309,7 @@ protected void initialVectorizedRowBatchCtx(Configuration conf) throws HiveExcep\n     Utilities.setMapWork(conf, mapWork);\n   }\n \n-  private StructObjectInspector createStructObjectInspector(Configuration conf) {\n+  private static StructObjectInspector createStructObjectInspector(Configuration conf) {\n     // Create row related objects\n     String columnNames = conf.get(IOConstants.COLUMNS);\n     List<String> columnNamesList = DataWritableReadSupport.getColumnNames(columnNames);\n@@ -321,7 +326,7 @@ protected void intRead(boolean isDictionaryEncoding) throws InterruptedException\n     conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);\n     conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, \"0\");\n     VectorizedParquetRecordReader reader =\n-      createParquetReader(\"message test { required int32 int32_field;}\", conf);\n+      createTestParquetReader(\"message test { required int32 int32_field;}\", conf);\n     VectorizedRowBatch previous = reader.createValue();\n     try {\n       int c = 0;\n@@ -350,7 +355,7 @@ protected void longRead(boolean isDictionaryEncoding) throws Exception {\n     conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);\n     conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, \"0\");\n     VectorizedParquetRecordReader reader =\n-      createParquetReader(\"message test { required int64 int64_field;}\", conf);\n+      createTestParquetReader(\"message test { required int64 int64_field;}\", conf);\n     VectorizedRowBatch previous = reader.createValue();\n     try {\n       int c = 0;\n@@ -379,7 +384,7 @@ protected void doubleRead(boolean isDictionaryEncoding) throws Exception {\n     conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);\n     conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, \"0\");\n     VectorizedParquetRecordReader reader =\n-      createParquetReader(\"message test { required double double_field;}\", conf);\n+      createTestParquetReader(\"message test { required double double_field;}\", conf);\n     VectorizedRowBatch previous = reader.createValue();\n     try {\n       int c = 0;\n@@ -409,7 +414,7 @@ protected void floatRead(boolean isDictionaryEncoding) throws Exception {\n     conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);\n     conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, \"0\");\n     VectorizedParquetRecordReader reader =\n-      createParquetReader(\"message test { required float float_field;}\", conf);\n+      createTestParquetReader(\"message test { required float float_field;}\", conf);\n     VectorizedRowBatch previous = reader.createValue();\n     try {\n       int c = 0;\n@@ -439,7 +444,7 @@ protected void booleanRead() throws Exception {\n     conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);\n     conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, \"0\");\n     VectorizedParquetRecordReader reader =\n-      createParquetReader(\"message test { required boolean boolean_field;}\", conf);\n+      createTestParquetReader(\"message test { required boolean boolean_field;}\", conf);\n     VectorizedRowBatch previous = reader.createValue();\n     try {\n       int c = 0;\n@@ -468,7 +473,7 @@ protected void binaryRead(boolean isDictionaryEncoding) throws Exception {\n     conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);\n     conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, \"0\");\n     VectorizedParquetRecordReader reader =\n-      createParquetReader(\"message test { required binary binary_field_some_null;}\", conf);\n+      createTestParquetReader(\"message test { required binary binary_field_some_null;}\", conf);\n     VectorizedRowBatch previous = reader.createValue();\n     int c = 0;\n     try {\n@@ -511,7 +516,7 @@ protected void structRead(boolean isDictionaryEncoding) throws Exception {\n       + \"  optional double b;\\n\"\n       + \"}\\n\"\n       + \"}\\n\";\n-    VectorizedParquetRecordReader reader = createParquetReader(schema, conf);\n+    VectorizedParquetRecordReader reader = createTestParquetReader(schema, conf);\n     VectorizedRowBatch previous = reader.createValue();\n     int c = 0;\n     try {\n@@ -551,7 +556,7 @@ protected void nestedStructRead0(boolean isDictionaryEncoding) throws Exception\n       + \"  }\"\n       + \"optional double e;\\n\"\n       + \"}\\n\";\n-    VectorizedParquetRecordReader reader = createParquetReader(schema, conf);\n+    VectorizedParquetRecordReader reader = createTestParquetReader(schema, conf);\n     VectorizedRowBatch previous = reader.createValue();\n     int c = 0;\n     try {\n@@ -592,7 +597,7 @@ protected void nestedStructRead1(boolean isDictionaryEncoding) throws Exception\n       + \"    optional int32 c;\\n\"\n       + \"  }\"\n       + \"}\\n\";\n-    VectorizedParquetRecordReader reader = createParquetReader(schema, conf);\n+    VectorizedParquetRecordReader reader = createTestParquetReader(schema, conf);\n     VectorizedRowBatch previous = reader.createValue();\n     int c = 0;\n     try {\n@@ -628,7 +633,7 @@ protected void structReadSomeNull(boolean isDictionaryEncoding) throws Exception\n       + \"  optional int32 f;\\n\"\n       + \"  optional double g;\\n\"\n       + \"}\\n\";\n-    VectorizedParquetRecordReader reader = createParquetReader(schema, conf);\n+    VectorizedParquetRecordReader reader = createTestParquetReader(schema, conf);\n     VectorizedRowBatch previous = reader.createValue();\n     int c = 0;\n     try {\n@@ -669,7 +674,7 @@ protected void decimalRead(boolean isDictionaryEncoding) throws Exception {\n     conf.setBoolean(ColumnProjectionUtils.READ_ALL_COLUMNS, false);\n     conf.set(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR, \"0\");\n     VectorizedParquetRecordReader reader =\n-      createParquetReader(\"message hive_schema { required value (DECIMAL(5,2));}\", conf);\n+      createTestParquetReader(\"message hive_schema { required value (DECIMAL(5,2));}\", conf);\n     VectorizedRowBatch previous = reader.createValue();\n     try {\n       int c = 0;",
                "raw_url": "https://github.com/apache/hive/raw/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/VectorizedColumnReaderTestBase.java",
                "sha": "1a5d0952fa0ed1b85a1cba843d9537ef5b5d0c3c",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hive/blob/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/queries/clientpositive/vectorization_parquet_projection.q",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/vectorization_parquet_projection.q?ref=0410bf17a361514f88774cf0545ec07271a26ab8",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/vectorization_parquet_projection.q",
                "patch": "@@ -77,3 +77,20 @@ group by m1[\"color\"];\n \n select m1[\"color\"], count(*) from parquet_project_test\n group by m1[\"color\"];\n+\n+\n+create table if not exists parquet_nullsplit(key string, val string) partitioned by (len string)\n+stored as parquet;\n+\n+insert into table parquet_nullsplit partition(len='1')\n+values ('one', 'red');\n+\n+explain vectorization select count(*) from parquet_nullsplit where len = '1';\n+select count(*) from parquet_nullsplit where len = '1';\n+\n+explain vectorization select count(*) from parquet_nullsplit where len = '99';\n+select count(*) from parquet_nullsplit where len = '99';\n+\n+drop table parquet_nullsplit;\n+drop table parquet_project_test;\n+drop table parquet_types_staging;",
                "raw_url": "https://github.com/apache/hive/raw/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/queries/clientpositive/vectorization_parquet_projection.q",
                "sha": "2bcaa987015a98084e6178991e542d13c5d65455",
                "status": "modified"
            },
            {
                "additions": 211,
                "blob_url": "https://github.com/apache/hive/blob/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/results/clientpositive/spark/vectorization_parquet_projection.q.out",
                "changes": 211,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/vectorization_parquet_projection.q.out?ref=0410bf17a361514f88774cf0545ec07271a26ab8",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/spark/vectorization_parquet_projection.q.out",
                "patch": "@@ -456,3 +456,214 @@ POSTHOOK: Input: default@parquet_project_test\n blue\t7\n green\t7\n red\t8\n+PREHOOK: query: create table if not exists parquet_nullsplit(key string, val string) partitioned by (len string)\n+stored as parquet\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@parquet_nullsplit\n+POSTHOOK: query: create table if not exists parquet_nullsplit(key string, val string) partitioned by (len string)\n+stored as parquet\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@parquet_nullsplit\n+PREHOOK: query: insert into table parquet_nullsplit partition(len='1')\n+values ('one', 'red')\n+PREHOOK: type: QUERY\n+PREHOOK: Output: default@parquet_nullsplit@len=1\n+POSTHOOK: query: insert into table parquet_nullsplit partition(len='1')\n+values ('one', 'red')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Output: default@parquet_nullsplit@len=1\n+POSTHOOK: Lineage: parquet_nullsplit PARTITION(len=1).key SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+POSTHOOK: Lineage: parquet_nullsplit PARTITION(len=1).val SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col2, type:string, comment:), ]\n+PREHOOK: query: explain vectorization select count(*) from parquet_nullsplit where len = '1'\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain vectorization select count(*) from parquet_nullsplit where len = '1'\n+POSTHOOK: type: QUERY\n+PLAN VECTORIZATION:\n+  enabled: true\n+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Spark\n+      Edges:\n+        Reducer 2 <- Map 1 (GROUP, 1)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: parquet_nullsplit\n+                  Statistics: Num rows: 1 Data size: 2 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    Statistics: Num rows: 1 Data size: 2 Basic stats: COMPLETE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count()\n+                      mode: hash\n+                      outputColumnNames: _col0\n+                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        sort order: \n+                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                        value expressions: _col0 (type: bigint)\n+            Execution mode: vectorized\n+            Map Vectorization:\n+                enabled: true\n+                enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true\n+                inputFormatFeatureSupport: []\n+                featureSupportInUse: []\n+                inputFileFormats: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\n+                allNative: false\n+                usesVectorUDFAdaptor: false\n+                vectorized: true\n+        Reducer 2 \n+            Execution mode: vectorized\n+            Reduce Vectorization:\n+                enabled: true\n+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true\n+                allNative: false\n+                usesVectorUDFAdaptor: false\n+                vectorized: true\n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: select count(*) from parquet_nullsplit where len = '1'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@parquet_nullsplit\n+PREHOOK: Input: default@parquet_nullsplit@len=1\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from parquet_nullsplit where len = '1'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@parquet_nullsplit\n+POSTHOOK: Input: default@parquet_nullsplit@len=1\n+#### A masked pattern was here ####\n+1\n+PREHOOK: query: explain vectorization select count(*) from parquet_nullsplit where len = '99'\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain vectorization select count(*) from parquet_nullsplit where len = '99'\n+POSTHOOK: type: QUERY\n+PLAN VECTORIZATION:\n+  enabled: true\n+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Spark\n+      Edges:\n+        Reducer 2 <- Map 1 (GROUP, 1)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: parquet_nullsplit\n+                  Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+                  Filter Operator\n+                    predicate: (len = '99') (type: boolean)\n+                    Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+                    Select Operator\n+                      Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+                      Group By Operator\n+                        aggregations: count()\n+                        mode: hash\n+                        outputColumnNames: _col0\n+                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                        Reduce Output Operator\n+                          sort order: \n+                          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                          value expressions: _col0 (type: bigint)\n+            Execution mode: vectorized\n+            Map Vectorization:\n+                enabled: true\n+                inputFormatFeatureSupport: []\n+                featureSupportInUse: []\n+                allNative: false\n+                usesVectorUDFAdaptor: false\n+                vectorized: true\n+        Reducer 2 \n+            Execution mode: vectorized\n+            Reduce Vectorization:\n+                enabled: true\n+                enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true, hive.execution.engine spark IN [tez, spark] IS true\n+                allNative: false\n+                usesVectorUDFAdaptor: false\n+                vectorized: true\n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: select count(*) from parquet_nullsplit where len = '99'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@parquet_nullsplit\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from parquet_nullsplit where len = '99'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@parquet_nullsplit\n+#### A masked pattern was here ####\n+0\n+PREHOOK: query: drop table parquet_nullsplit\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@parquet_nullsplit\n+PREHOOK: Output: default@parquet_nullsplit\n+POSTHOOK: query: drop table parquet_nullsplit\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@parquet_nullsplit\n+POSTHOOK: Output: default@parquet_nullsplit\n+PREHOOK: query: drop table parquet_project_test\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@parquet_project_test\n+PREHOOK: Output: default@parquet_project_test\n+POSTHOOK: query: drop table parquet_project_test\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@parquet_project_test\n+POSTHOOK: Output: default@parquet_project_test\n+PREHOOK: query: drop table parquet_types_staging\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@parquet_types_staging\n+PREHOOK: Output: default@parquet_types_staging\n+POSTHOOK: query: drop table parquet_types_staging\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@parquet_types_staging\n+POSTHOOK: Output: default@parquet_types_staging",
                "raw_url": "https://github.com/apache/hive/raw/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/results/clientpositive/spark/vectorization_parquet_projection.q.out",
                "sha": "53d059f16125cf6ad48e9bd398eae20f7984f7bf",
                "status": "modified"
            },
            {
                "additions": 193,
                "blob_url": "https://github.com/apache/hive/blob/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/results/clientpositive/vectorization_parquet_projection.q.out",
                "changes": 193,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/vectorization_parquet_projection.q.out?ref=0410bf17a361514f88774cf0545ec07271a26ab8",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/vectorization_parquet_projection.q.out",
                "patch": "@@ -426,3 +426,196 @@ POSTHOOK: Input: default@parquet_project_test\n blue\t7\n green\t7\n red\t8\n+PREHOOK: query: create table if not exists parquet_nullsplit(key string, val string) partitioned by (len string)\n+stored as parquet\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@parquet_nullsplit\n+POSTHOOK: query: create table if not exists parquet_nullsplit(key string, val string) partitioned by (len string)\n+stored as parquet\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@parquet_nullsplit\n+PREHOOK: query: insert into table parquet_nullsplit partition(len='1')\n+values ('one', 'red')\n+PREHOOK: type: QUERY\n+PREHOOK: Output: default@parquet_nullsplit@len=1\n+POSTHOOK: query: insert into table parquet_nullsplit partition(len='1')\n+values ('one', 'red')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Output: default@parquet_nullsplit@len=1\n+POSTHOOK: Lineage: parquet_nullsplit PARTITION(len=1).key SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+POSTHOOK: Lineage: parquet_nullsplit PARTITION(len=1).val SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col2, type:string, comment:), ]\n+PREHOOK: query: explain vectorization select count(*) from parquet_nullsplit where len = '1'\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain vectorization select count(*) from parquet_nullsplit where len = '1'\n+POSTHOOK: type: QUERY\n+PLAN VECTORIZATION:\n+  enabled: true\n+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: parquet_nullsplit\n+            Statistics: Num rows: 1 Data size: 2 Basic stats: COMPLETE Column stats: NONE\n+            Select Operator\n+              Statistics: Num rows: 1 Data size: 2 Basic stats: COMPLETE Column stats: NONE\n+              Group By Operator\n+                aggregations: count()\n+                mode: hash\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  sort order: \n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  value expressions: _col0 (type: bigint)\n+      Execution mode: vectorized\n+      Map Vectorization:\n+          enabled: true\n+          enabledConditionsMet: hive.vectorized.use.vectorized.input.format IS true\n+          inputFormatFeatureSupport: []\n+          featureSupportInUse: []\n+          inputFileFormats: org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat\n+          allNative: false\n+          usesVectorUDFAdaptor: false\n+          vectorized: true\n+      Reduce Vectorization:\n+          enabled: false\n+          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true\n+          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations: count(VALUE._col0)\n+          mode: mergepartial\n+          outputColumnNames: _col0\n+          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+          File Output Operator\n+            compressed: false\n+            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+            table:\n+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: select count(*) from parquet_nullsplit where len = '1'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@parquet_nullsplit\n+PREHOOK: Input: default@parquet_nullsplit@len=1\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from parquet_nullsplit where len = '1'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@parquet_nullsplit\n+POSTHOOK: Input: default@parquet_nullsplit@len=1\n+#### A masked pattern was here ####\n+1\n+PREHOOK: query: explain vectorization select count(*) from parquet_nullsplit where len = '99'\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain vectorization select count(*) from parquet_nullsplit where len = '99'\n+POSTHOOK: type: QUERY\n+PLAN VECTORIZATION:\n+  enabled: true\n+  enabledConditionsMet: [hive.vectorized.execution.enabled IS true]\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: parquet_nullsplit\n+            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+            Filter Operator\n+              predicate: (len = '99') (type: boolean)\n+              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+              Select Operator\n+                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE\n+                Group By Operator\n+                  aggregations: count()\n+                  mode: hash\n+                  outputColumnNames: _col0\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    sort order: \n+                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: _col0 (type: bigint)\n+      Execution mode: vectorized\n+      Map Vectorization:\n+          enabled: true\n+          inputFormatFeatureSupport: []\n+          featureSupportInUse: []\n+          allNative: false\n+          usesVectorUDFAdaptor: false\n+          vectorized: true\n+      Reduce Vectorization:\n+          enabled: false\n+          enableConditionsMet: hive.vectorized.execution.reduce.enabled IS true\n+          enableConditionsNotMet: hive.execution.engine mr IN [tez, spark] IS false\n+      Reduce Operator Tree:\n+        Group By Operator\n+          aggregations: count(VALUE._col0)\n+          mode: mergepartial\n+          outputColumnNames: _col0\n+          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+          File Output Operator\n+            compressed: false\n+            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+            table:\n+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: select count(*) from parquet_nullsplit where len = '99'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@parquet_nullsplit\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from parquet_nullsplit where len = '99'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@parquet_nullsplit\n+#### A masked pattern was here ####\n+0\n+PREHOOK: query: drop table parquet_nullsplit\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@parquet_nullsplit\n+PREHOOK: Output: default@parquet_nullsplit\n+POSTHOOK: query: drop table parquet_nullsplit\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@parquet_nullsplit\n+POSTHOOK: Output: default@parquet_nullsplit\n+PREHOOK: query: drop table parquet_project_test\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@parquet_project_test\n+PREHOOK: Output: default@parquet_project_test\n+POSTHOOK: query: drop table parquet_project_test\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@parquet_project_test\n+POSTHOOK: Output: default@parquet_project_test\n+PREHOOK: query: drop table parquet_types_staging\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@parquet_types_staging\n+PREHOOK: Output: default@parquet_types_staging\n+POSTHOOK: query: drop table parquet_types_staging\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@parquet_types_staging\n+POSTHOOK: Output: default@parquet_types_staging",
                "raw_url": "https://github.com/apache/hive/raw/0410bf17a361514f88774cf0545ec07271a26ab8/ql/src/test/results/clientpositive/vectorization_parquet_projection.q.out",
                "sha": "e167523cbe06b4a505fde7e3eeeef6378996d949",
                "status": "modified"
            }
        ],
        "message": "HIVE-17961 : NPE during initialization of VectorizedParquetRecordReader when input split is null (Vihang Karajgaonkar, reviewed by Ferdinand Xu)",
        "parent": "https://github.com/apache/hive/commit/bff9da2cc03da848189c7266ee57069dde3fe668",
        "patched_files": [
            "VectorizedParquetRecordReader.java",
            "ColumnarStorageBench.java",
            "vectorization_parquet_projection.java",
            "VectorizedColumnReaderTestBase.java",
            "VectorizedColumnReader.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestVectorizedColumnReader.java"
        ]
    },
    "hive_042b2ef": {
        "bug_id": "hive_042b2ef",
        "commit": "https://github.com/apache/hive/commit/042b2ef7df6af8b93adeb936d94c4079153467ff",
        "file": [
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hive/blob/042b2ef7df6af8b93adeb936d94c4079153467ff/llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java?ref=042b2ef7df6af8b93adeb936d94c4079153467ff",
                "deletions": 12,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java",
                "patch": "@@ -1352,21 +1352,23 @@ private int getNextFreeListItem(int offset) {\n \n     public void deallocate(LlapAllocatorBuffer buffer, boolean isAfterMove) {\n       assert data != null;\n-      int pos = buffer.byteBuffer.position();\n-      // Note: this is called by someone who has ensured the buffer is not going to be moved.\n-      int headerIx = pos >>> minAllocLog2;\n-      int freeListIx = freeListFromAllocSize(buffer.allocSize);\n-      if (assertsEnabled && !isAfterMove) {\n-        LlapAllocatorBuffer buf = buffers[headerIx];\n-        if (buf != buffer) {\n-          failWithLog(arenaIx + \":\" + headerIx + \" => \"\n+      if (buffer != null && buffer.byteBuffer != null) {\n+        int pos = buffer.byteBuffer.position();\n+        // Note: this is called by someone who has ensured the buffer is not going to be moved.\n+        int headerIx = pos >>> minAllocLog2;\n+        int freeListIx = freeListFromAllocSize(buffer.allocSize);\n+        if (assertsEnabled && !isAfterMove) {\n+          LlapAllocatorBuffer buf = buffers[headerIx];\n+          if (buf != buffer) {\n+            failWithLog(arenaIx + \":\" + headerIx + \" => \"\n               + toDebugString(buffer) + \", \" + toDebugString(buf));\n+          }\n+          assertBufferLooksValid(freeListFromHeader(headers[headerIx]), buf, arenaIx, headerIx);\n+          checkHeader(headerIx, freeListIx, true);\n         }\n-        assertBufferLooksValid(freeListFromHeader(headers[headerIx]), buf, arenaIx, headerIx);\n-        checkHeader(headerIx, freeListIx, true);\n+        buffers[headerIx] = null;\n+        addToFreeListWithMerge(headerIx, freeListIx, buffer, CasLog.Src.DEALLOC);\n       }\n-      buffers[headerIx] = null;\n-      addToFreeListWithMerge(headerIx, freeListIx, buffer, CasLog.Src.DEALLOC);\n     }\n \n     private void addToFreeListWithMerge(int headerIx, int freeListIx,",
                "raw_url": "https://github.com/apache/hive/raw/042b2ef7df6af8b93adeb936d94c4079153467ff/llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java",
                "sha": "013f3538b4b75f62f9e4c40ddbdd9d1b0b8ed8b1",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/042b2ef7df6af8b93adeb936d94c4079153467ff/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java?ref=042b2ef7df6af8b93adeb936d94c4079153467ff",
                "deletions": 0,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java",
                "patch": "@@ -20,6 +20,7 @@\n import org.apache.orc.impl.RecordReaderUtils;\n \n import java.nio.ByteBuffer;\n+import java.util.ArrayList;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n@@ -40,6 +41,7 @@\n \n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Function;\n+import com.google.common.base.Joiner;\n \n public class LowLevelCacheImpl implements LowLevelCache, BufferUsageManager, LlapIoDebugDump {\n   private static final int DEFAULT_CLEANUP_INTERVAL = 600;\n@@ -457,6 +459,10 @@ public void debugDumpShort(StringBuilder sb) {\n       try {\n         int fileLocked = 0, fileUnlocked = 0, fileEvicted = 0, fileMoving = 0;\n         if (e.getValue().getCache().isEmpty()) continue;\n+        List<LlapDataBuffer> lockedBufs = null;\n+        if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {\n+          lockedBufs = new ArrayList<>();\n+        }\n         for (Map.Entry<Long, LlapDataBuffer> e2 : e.getValue().getCache().entrySet()) {\n           int newRc = e2.getValue().tryIncRef();\n           if (newRc < 0) {\n@@ -470,6 +476,9 @@ public void debugDumpShort(StringBuilder sb) {\n           try {\n             if (newRc > 1) { // We hold one refcount.\n               ++fileLocked;\n+              if (lockedBufs != null) {\n+                lockedBufs.add(e2.getValue());\n+              }\n             } else {\n               ++fileUnlocked;\n             }\n@@ -483,6 +492,9 @@ public void debugDumpShort(StringBuilder sb) {\n         allMoving += fileMoving;\n         sb.append(\"\\n  file \" + e.getKey() + \": \" + fileLocked + \" locked, \" + fileUnlocked\n             + \" unlocked, \" + fileEvicted + \" evicted, \" + fileMoving + \" being moved\");\n+        if (fileLocked > 0 && LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {\n+          LlapIoImpl.LOCKING_LOGGER.trace(\"locked-buffers: {}\", lockedBufs);\n+        }\n       } finally {\n         e.getValue().decRef();\n       }",
                "raw_url": "https://github.com/apache/hive/raw/042b2ef7df6af8b93adeb936d94c4079153467ff/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java",
                "sha": "e012d7dbf9f5dab0bcc88c8cf680dc6ffd8baf71",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/042b2ef7df6af8b93adeb936d94c4079153467ff/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java?ref=042b2ef7df6af8b93adeb936d94c4079153467ff",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java",
                "patch": "@@ -1964,7 +1964,9 @@ public void readIndexStreams(OrcIndex index, StripeInformation stripe,\n     } finally {\n       // Release the unreleased buffers. See class comment about refcounts.\n       try {\n-        releaseInitialRefcounts(toRead.next);\n+        if (toRead != null) {\n+          releaseInitialRefcounts(toRead.next);\n+        }\n         releaseBuffers(toRelease.keySet(), true);\n       } catch (Throwable t) {\n         if (!hasError) throw new IOException(t);",
                "raw_url": "https://github.com/apache/hive/raw/042b2ef7df6af8b93adeb936d94c4079153467ff/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java",
                "sha": "759594aea33b468abcf58a4930a8081176c98a51",
                "status": "modified"
            }
        ],
        "message": "HIVE-20249 : LLAP IO: NPE during refCount decrement (Prasanth Jayachandran, reviewed by Sergey Shelukhin)",
        "parent": "https://github.com/apache/hive/commit/d4b7b93e27a8a28dde8f4584d883faba86f0203c",
        "patched_files": [
            "BuddyAllocator.java",
            "EncodedReaderImpl.java",
            "LowLevelCacheImpl.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestBuddyAllocator.java",
            "TestLowLevelCacheImpl.java",
            "TestEncodedReaderImpl.java"
        ]
    },
    "hive_044f1fa": {
        "bug_id": "hive_044f1fa",
        "commit": "https://github.com/apache/hive/commit/044f1fa3ce868d4880adbe8c92a24e15e8b88dba",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/044f1fa3ce868d4880adbe8c92a24e15e8b88dba/beeline/src/test/org/apache/hive/beeline/TestBeeLineHistory.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/beeline/src/test/org/apache/hive/beeline/TestBeeLineHistory.java?ref=044f1fa3ce868d4880adbe8c92a24e15e8b88dba",
                "deletions": 1,
                "filename": "beeline/src/test/org/apache/hive/beeline/TestBeeLineHistory.java",
                "patch": "@@ -33,7 +33,7 @@\n  */\n public class TestBeeLineHistory {\n \n-  private static final String fileName = \"history\";\n+  private static final String fileName = System.getProperty(\"test.tmp.dir\") + \"/history\";\n \n   @BeforeClass\n   public static void beforeTests() throws Exception {",
                "raw_url": "https://github.com/apache/hive/raw/044f1fa3ce868d4880adbe8c92a24e15e8b88dba/beeline/src/test/org/apache/hive/beeline/TestBeeLineHistory.java",
                "sha": "5f99a0edb401e1d220b81b2953ca839fd93d0a7b",
                "status": "modified"
            }
        ],
        "message": "HIVE-14974: TestBeeLineHistory throws NPE in ShutdownHook (Prasanth Jayachandran reviewed by Siddharth Seth)",
        "parent": "https://github.com/apache/hive/commit/f562dfb5207e8246e5f12696e0d7f373c3e3bf4c",
        "patched_files": [],
        "repo": "hive",
        "unit_tests": [
            "TestBeeLineHistory.java"
        ]
    },
    "hive_0870ab9": {
        "bug_id": "hive_0870ab9",
        "commit": "https://github.com/apache/hive/commit/0870ab9ca6b622b850d83c0a583e1c3a123c33e7",
        "file": [
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hive/blob/0870ab9ca6b622b850d83c0a583e1c3a123c33e7/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=0870ab9ca6b622b850d83c0a583e1c3a123c33e7",
                "deletions": 0,
                "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "patch": "@@ -3389,12 +3389,25 @@ public Partition exchange_partition(Map<String, String> partitionSpecs,\n     public List<Partition> exchange_partitions(Map<String, String> partitionSpecs,\n         String sourceDbName, String sourceTableName, String destDbName,\n         String destTableName) throws TException {\n+      if (partitionSpecs == null || sourceDbName == null || sourceTableName == null\n+          || destDbName == null || destTableName == null) {\n+        throw new MetaException(\"The DB and table name for the source and destination tables,\"\n+            + \" and the partition specs must not be null.\");\n+      }\n       boolean success = false;\n       boolean pathCreated = false;\n       RawStore ms = getMS();\n       ms.openTransaction();\n       Table destinationTable = ms.getTable(destDbName, destTableName);\n+      if (destinationTable == null) {\n+        throw new MetaException(\n+            \"The destination table \" + destDbName + \".\" + destTableName + \" not found\");\n+      }\n       Table sourceTable = ms.getTable(sourceDbName, sourceTableName);\n+      if (sourceTable == null) {\n+        throw new MetaException(\n+            \"The source table \" + sourceDbName + \".\" + sourceTableName + \" not found\");\n+      }\n       List<String> partVals = MetaStoreUtils.getPvals(sourceTable.getPartitionKeys(),\n           partitionSpecs);\n       List<String> partValsPresent = new ArrayList<> ();",
                "raw_url": "https://github.com/apache/hive/raw/0870ab9ca6b622b850d83c0a583e1c3a123c33e7/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "sha": "6838dd7d14f78ebcb0e53c9f9d11f666f7ab204b",
                "status": "modified"
            },
            {
                "additions": 79,
                "blob_url": "https://github.com/apache/hive/blob/0870ab9ca6b622b850d83c0a583e1c3a123c33e7/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestExchangePartitions.java",
                "changes": 288,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestExchangePartitions.java?ref=0870ab9ca6b622b850d83c0a583e1c3a123c33e7",
                "deletions": 209,
                "filename": "standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestExchangePartitions.java",
                "patch": "@@ -38,7 +38,6 @@\n import org.apache.hadoop.hive.metastore.client.builder.TableBuilder;\n import org.apache.hadoop.hive.metastore.minihms.AbstractMetaStoreService;\n import org.apache.thrift.TException;\n-import org.apache.thrift.transport.TTransportException;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -290,160 +289,100 @@ public void testExchangePartitionsNonExistingPartLocation() throws Exception {\n         sourceTable.getTableName(), destTable.getDbName(), destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsNonExistingSourceTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, DB_NAME, \"nonexistingtable\",\n-          destTable.getDbName(), destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, DB_NAME, \"nonexistingtable\", destTable.getDbName(),\n+        destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsNonExistingSourceDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, \"nonexistingdb\", sourceTable.getTableName(),\n-          destTable.getDbName(), destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, \"nonexistingdb\", sourceTable.getTableName(),\n+        destTable.getDbName(), destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsNonExistingDestTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), DB_NAME, \"nonexistingtable\");\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        DB_NAME, \"nonexistingtable\");\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsNonExistingDestDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), \"nonexistingdb\", destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        \"nonexistingdb\", destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsEmptySourceTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, DB_NAME, \"\", destTable.getDbName(),\n-          destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, DB_NAME, \"\", destTable.getDbName(),\n+        destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsEmptySourceDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, \"\", sourceTable.getTableName(),\n-          destTable.getDbName(), destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, \"\", sourceTable.getTableName(),\n+        destTable.getDbName(), destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsEmptyDestTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), DB_NAME, \"\");\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        DB_NAME, \"\");\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsEmptyDestDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), \"\", destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        \"\", destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsNullSourceTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, DB_NAME, null, destTable.getDbName(),\n-          destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, DB_NAME, null, destTable.getDbName(),\n+        destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsNullSourceDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, null, sourceTable.getTableName(),\n-          destTable.getDbName(), destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, null, sourceTable.getTableName(),\n+        destTable.getDbName(), destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsNullDestTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), DB_NAME, null);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        DB_NAME, null);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsNullDestDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partitions(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), null, destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partitions(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        null, destTable.getTableName());\n   }\n \n   @Test(expected = MetaException.class)\n@@ -454,15 +393,10 @@ public void testExchangePartitionsEmptyPartSpec() throws Exception {\n         sourceTable.getTableName(), destTable.getDbName(), destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionsNullPartSpec() throws Exception {\n-    try {\n-      client.exchange_partitions(null, sourceTable.getDbName(), sourceTable.getTableName(), null,\n-          destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: NPE should not be thrown\n-    }\n+    client.exchange_partitions(null, sourceTable.getDbName(), sourceTable.getTableName(), null,\n+        destTable.getTableName());\n   }\n \n   @Test(expected = MetaException.class)\n@@ -881,160 +815,100 @@ public void testExchangePartitionNonExistingPartLocation() throws Exception {\n         sourceTable.getTableName(), destTable.getDbName(), destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionNonExistingSourceTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, DB_NAME, \"nonexistingtable\",\n-          destTable.getDbName(), destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, DB_NAME, \"nonexistingtable\", destTable.getDbName(),\n+        destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionNonExistingSourceDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, \"nonexistingdb\", sourceTable.getTableName(),\n-          destTable.getDbName(), destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, \"nonexistingdb\", sourceTable.getTableName(),\n+        destTable.getDbName(), destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionNonExistingDestTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), DB_NAME, \"nonexistingtable\");\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        DB_NAME, \"nonexistingtable\");\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionNonExistingDestDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), \"nonexistingdb\", destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        \"nonexistingdb\", destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionEmptySourceTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, DB_NAME, \"\", destTable.getDbName(),\n-          destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, DB_NAME, \"\", destTable.getDbName(),\n+        destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionEmptySourceDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, \"\", sourceTable.getTableName(),\n-          destTable.getDbName(), destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, \"\", sourceTable.getTableName(), destTable.getDbName(),\n+        destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionEmptyDestTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), DB_NAME, \"\");\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        DB_NAME, \"\");\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionEmptyDestDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), \"\", destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        \"\", destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionNullSourceTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, DB_NAME, null, destTable.getDbName(),\n-          destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, DB_NAME, null, destTable.getDbName(),\n+        destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionNullSourceDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, null, sourceTable.getTableName(),\n-          destTable.getDbName(), destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, null, sourceTable.getTableName(),\n+        destTable.getDbName(), destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionNullDestTable() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), DB_NAME, null);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        DB_NAME, null);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionNullDestDB() throws Exception {\n \n     Map<String, String> partitionSpecs = getPartitionSpec(partitions[1]);\n-    try {\n-      client.exchange_partition(partitionSpecs, sourceTable.getDbName(),\n-          sourceTable.getTableName(), null, destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: Non existing table or db should be handled correctly and NPE should not occur.\n-    }\n+    client.exchange_partition(partitionSpecs, sourceTable.getDbName(), sourceTable.getTableName(),\n+        null, destTable.getTableName());\n   }\n \n   @Test(expected = MetaException.class)\n@@ -1045,15 +919,11 @@ public void testExchangePartitionEmptyPartSpec() throws Exception {\n         sourceTable.getTableName(), destTable.getDbName(), destTable.getTableName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testExchangePartitionNullPartSpec() throws Exception {\n-    try {\n-      client.exchange_partition(null, sourceTable.getDbName(), sourceTable.getTableName(), null,\n-          destTable.getTableName());\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: NPE should not be thrown\n-    }\n+\n+    client.exchange_partition(null, sourceTable.getDbName(), sourceTable.getTableName(), null,\n+        destTable.getTableName());\n   }\n \n   @Test(expected = MetaException.class)",
                "raw_url": "https://github.com/apache/hive/raw/0870ab9ca6b622b850d83c0a583e1c3a123c33e7/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestExchangePartitions.java",
                "sha": "c9b9e9b28650d84c198b482458561d1b2694f65a",
                "status": "modified"
            }
        ],
        "message": "HIVE-18892: Fix NPEs in HiveMetastore.exchange_partitions method (Marta Kuczora, reviewed by Sahil Takiar and Peter Vary)",
        "parent": "https://github.com/apache/hive/commit/68459cf0bfe67dfe72da9095a1dac6b84ede93b0",
        "patched_files": [
            "HiveMetaStore.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveMetaStore.java",
            "TestExchangePartitions.java"
        ]
    },
    "hive_09daf83": {
        "bug_id": "hive_09daf83",
        "commit": "https://github.com/apache/hive/commit/09daf83acca5cc0e1526b3a232f9ae027e9526f9",
        "file": [
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hive/blob/09daf83acca5cc0e1526b3a232f9ae027e9526f9/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveMetaDataResultSet.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveMetaDataResultSet.java?ref=09daf83acca5cc0e1526b3a232f9ae027e9526f9",
                "deletions": 3,
                "filename": "jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveMetaDataResultSet.java",
                "patch": "@@ -29,9 +29,21 @@\n   public HiveMetaDataResultSet(final List<String> columnNames\n           , final List<String> columnTypes\n           , final List<M> data) throws SQLException {\n-    this.data = new ArrayList<M>(data);\n-    this.columnNames = new ArrayList<String>(columnNames);\n-    this.columnTypes = new ArrayList<String>(columnTypes);\n+    if (data!=null) {\n+      this.data = new ArrayList<M>(data);\n+    } else {\n+      this.data =  new ArrayList<M>();\n+    }\n+    if (columnNames!=null) {\n+      this.columnNames = new ArrayList<String>(columnNames);\n+    } else {\n+      this.columnNames =  new ArrayList<String>();\n+    }\n+    if (columnTypes!=null) {\n+      this.columnTypes = new ArrayList<String>(columnTypes);\n+    } else {\n+      this.columnTypes =  new ArrayList<String>();\n+    }\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hive/raw/09daf83acca5cc0e1526b3a232f9ae027e9526f9/jdbc/src/java/org/apache/hadoop/hive/jdbc/HiveMetaDataResultSet.java",
                "sha": "9f7ae42046f61df418bc12b7de5461b91ed4c457",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hive/blob/09daf83acca5cc0e1526b3a232f9ae027e9526f9/jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java?ref=09daf83acca5cc0e1526b3a232f9ae027e9526f9",
                "deletions": 1,
                "filename": "jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java",
                "patch": "@@ -481,7 +481,17 @@ public void testMetaDataGetCatalogs() throws SQLException {\n       cnt++;\n     }\n     rs.close();\n-    assertEquals(\"Incorrect schema count\", 1, cnt);\n+    assertEquals(\"Incorrect catalog count\", 1, cnt);\n+  }\n+\n+  public void testMetaDataGetSchemas() throws SQLException {\n+    ResultSet rs = (ResultSet)con.getMetaData().getSchemas();\n+    int cnt = 0;\n+    while (rs.next()) {\n+      cnt++;\n+    }\n+    rs.close();\n+    assertEquals(\"Incorrect schema count\", 0, cnt);\n   }\n \n   public void testMetaDataGetTableTypes() throws SQLException {",
                "raw_url": "https://github.com/apache/hive/raw/09daf83acca5cc0e1526b3a232f9ae027e9526f9/jdbc/src/test/org/apache/hadoop/hive/jdbc/TestJdbcDriver.java",
                "sha": "b6a7c93267ae2733a606cd60b00b280c79723129",
                "status": "modified"
            }
        ],
        "message": "HIVE-2069. NullPointerException on getSchemas (Bennie Schut via Ning Zhang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1084653 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/f8dc84294b4671fff58b75e381c308aa15be2fea",
        "patched_files": [
            "HiveMetaDataResultSet.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestJdbcDriver.java"
        ]
    },
    "hive_0a81e1e": {
        "bug_id": "hive_0a81e1e",
        "commit": "https://github.com/apache/hive/commit/0a81e1ec309673007bc3e1a37cf88c92edfc285c",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/0a81e1ec309673007bc3e1a37cf88c92edfc285c/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=0a81e1ec309673007bc3e1a37cf88c92edfc285c",
                "deletions": 0,
                "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "patch": "@@ -1290,6 +1290,9 @@ public Database get_database(final String name) throws NoSuchObjectException, Me\n     @Override\n     public Database get_database_core(String catName, final String name) throws NoSuchObjectException, MetaException {\n       Database db = null;\n+      if (name == null) {\n+        throw new MetaException(\"Database name cannot be null.\");\n+      }\n       try {\n         db = getMS().getDatabase(catName, name);\n       } catch (MetaException | NoSuchObjectException e) {\n@@ -1364,6 +1367,9 @@ private void drop_database_core(RawStore ms, String catName,\n       List<Path> tablePaths = new ArrayList<>();\n       List<Path> partitionPaths = new ArrayList<>();\n       Map<String, String> transactionalListenerResponses = Collections.emptyMap();\n+      if (name == null) {\n+        throw new MetaException(\"Database name cannot be null.\");\n+      }\n       try {\n         ms.openTransaction();\n         db = ms.getDatabase(catName, name);",
                "raw_url": "https://github.com/apache/hive/raw/0a81e1ec309673007bc3e1a37cf88c92edfc285c/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "sha": "450da4f0f0491b881200ee1123b8064b023ea3e2",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/0a81e1ec309673007bc3e1a37cf88c92edfc285c/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestDatabases.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestDatabases.java?ref=0a81e1ec309673007bc3e1a37cf88c92edfc285c",
                "deletions": 31,
                "filename": "standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestDatabases.java",
                "patch": "@@ -40,7 +40,6 @@\n import org.apache.hadoop.hive.metastore.minihms.AbstractMetaStoreService;\n import org.apache.hadoop.hive.metastore.utils.SecurityUtils;\n import org.apache.thrift.TException;\n-import org.apache.thrift.transport.TTransportException;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -227,51 +226,27 @@ public void testGetDatabaseNoSuchDatabase() throws Exception {\n     client.getDatabase(\"no_such_database\");\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testGetDatabaseNullName() throws Exception {\n     // Missing database name in the query\n-    try {\n-      client.getDatabase(null);\n-      // TODO: Should have a check on the server side.\n-      Assert.fail(\"Expected a NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.getDatabase(null);\n   }\n \n   @Test(expected = NoSuchObjectException.class)\n   public void testDropDatabaseNoSuchDatabase() throws Exception {\n     client.dropDatabase(\"no_such_database\");\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testDropDatabaseNullName() throws Exception {\n     // Missing database in the query\n-    try {\n-      client.dropDatabase(null);\n-      // TODO: Should be checked on server side\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.dropDatabase(null);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testDropDatabaseDefaultDatabase() throws Exception {\n     // Check if it is possible to drop default database\n-    try {\n-      client.dropDatabase(DEFAULT_DATABASE);\n-      // TODO: Should be checked on server side\n-      Assert.fail(\"Expected an MetaException or TTransportException to be thrown\");\n-    } catch (MetaException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.dropDatabase(DEFAULT_DATABASE);\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hive/raw/0a81e1ec309673007bc3e1a37cf88c92edfc285c/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestDatabases.java",
                "sha": "d558de66d04c529286d91bce2dad8603a4ebfe0b",
                "status": "modified"
            }
        ],
        "message": "HIVE-19075: Fix NPE when trying to drop or get DB with null name (Marta Kuczora, via Peter Vary)",
        "parent": "https://github.com/apache/hive/commit/eb40ea57eac4c3ff46f638cf4ab83bec71b5eda5",
        "patched_files": [
            "HiveMetaStore.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveMetaStore.java",
            "TestDatabases.java"
        ]
    },
    "hive_0af2fb0": {
        "bug_id": "hive_0af2fb0",
        "commit": "https://github.com/apache/hive/commit/0af2fb0cac270cd6223a4abaf6e410f05450bfef",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hive/blob/0af2fb0cac270cd6223a4abaf6e410f05450bfef/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java?ref=0af2fb0cac270cd6223a4abaf6e410f05450bfef",
                "deletions": 8,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java",
                "patch": "@@ -1992,17 +1992,18 @@ public static void registerTableFunction(String name, Class<? extends TableFunct\n    * @return true if function is a UDAF, has WindowFunctionDescription annotation and the annotations\n    *         confirms a ranking function, false otherwise\n    */\n-  public static boolean isRankingFunction(String name){\n+  public static boolean isRankingFunction(String name) {\n     FunctionInfo info = getFunctionInfo(name);\n+    if (info == null) {\n+      return false;\n+    }\n     GenericUDAFResolver res = info.getGenericUDAFResolver();\n-    if (res != null){\n-      WindowFunctionDescription desc =\n-          AnnotationUtils.getAnnotation(res.getClass(), WindowFunctionDescription.class);\n-      if (desc != null){\n-        return desc.rankingFunction();\n-      }\n+    if (res == null) {\n+      return false;\n     }\n-    return false;\n+    WindowFunctionDescription desc =\n+        AnnotationUtils.getAnnotation(res.getClass(), WindowFunctionDescription.class);\n+    return (desc != null) && desc.rankingFunction();\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hive/raw/0af2fb0cac270cd6223a4abaf6e410f05450bfef/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionRegistry.java",
                "sha": "451598aa964786e118307643cf92c6f31fcab9aa",
                "status": "modified"
            }
        ],
        "message": "HIVE-8551 : NPE in FunctionRegistry (affects CBO in negative tests) (Sergey Shelukhin, reviewed by Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1633692 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/4dd080d286a6db1df33ad643ac618407c1af1a45",
        "patched_files": [
            "FunctionRegistry.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestFunctionRegistry.java"
        ]
    },
    "hive_0b81099": {
        "bug_id": "hive_0b81099",
        "commit": "https://github.com/apache/hive/commit/0b810991a12bb7c8d52f64b781772a1deabcbe53",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/0b810991a12bb7c8d52f64b781772a1deabcbe53/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java?ref=0b810991a12bb7c8d52f64b781772a1deabcbe53",
                "deletions": 5,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "patch": "@@ -215,6 +215,7 @@\n   public static final String MAPRED_MAPPER_CLASS = \"mapred.mapper.class\";\n   public static final String MAPRED_REDUCER_CLASS = \"mapred.reducer.class\";\n   public static final String HIVE_ADDED_JARS = \"hive.added.jars\";\n+  public static final String VECTOR_MODE = \"VECTOR_MODE\";\n   public static String MAPNAME = \"Map \";\n   public static String REDUCENAME = \"Reducer \";\n \n@@ -3238,12 +3239,18 @@ private static void resetUmaskInConf(Configuration conf, boolean unsetUmask, Str\n    * but vectorization disallowed eg. for FetchOperator execution.\n    */\n   public static boolean isVectorMode(Configuration conf) {\n-    if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED) &&\n-        Utilities.getPlanPath(conf) != null && Utilities\n-        .getMapWork(conf).getVectorMode()) {\n-      return true;\n+    if (conf.get(VECTOR_MODE) != null) {\n+      // this code path is necessary, because with HS2 and client\n+      // side split generation we end up not finding the map work.\n+      // This is because of thread local madness (tez split\n+      // generation is multi-threaded - HS2 plan cache uses thread\n+      // locals).\n+      return conf.getBoolean(VECTOR_MODE, false);\n+    } else {\n+      return HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_VECTORIZATION_ENABLED)\n+        && Utilities.getPlanPath(conf) != null\n+        && Utilities.getMapWork(conf).getVectorMode();\n     }\n-    return false;\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hive/raw/0b810991a12bb7c8d52f64b781772a1deabcbe53/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "sha": "fce11c86fcdf1e9c72575ce5025208e1018a3591",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hive/blob/0b810991a12bb7c8d52f64b781772a1deabcbe53/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java?ref=0b810991a12bb7c8d52f64b781772a1deabcbe53",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "patch": "@@ -614,6 +614,15 @@ private Vertex createVertex(JobConf conf, MapWork mapWork,\n       }\n     } else {\n       // Setup client side split generation.\n+\n+      // we need to set this, because with HS2 and client side split\n+      // generation we end up not finding the map work. This is\n+      // because of thread local madness (tez split generation is\n+      // multi-threaded - HS2 plan cache uses thread locals). Setting\n+      // VECTOR_MODE causes the split gen code to use the conf instead\n+      // of the map work.\n+      conf.setBoolean(Utilities.VECTOR_MODE, mapWork.getVectorMode());\n+\n       dataSource = MRInputHelpers.configureMRInputWithLegacySplitGeneration(conf, new Path(tezDir,\n           \"split_\" + mapWork.getName().replaceAll(\" \", \"_\")), true);\n       numTasks = dataSource.getNumberOfShards();",
                "raw_url": "https://github.com/apache/hive/raw/0b810991a12bb7c8d52f64b781772a1deabcbe53/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "sha": "e8864aee6428d3cc6b165acbc188cac8d22ffd61",
                "status": "modified"
            }
        ],
        "message": "HIVE-12740: NPE with HS2 when using null input format (Vikram Dixit K via Gunther Hagleitner)",
        "parent": "https://github.com/apache/hive/commit/6e513b06c12e508af655a6bea4aef55b86619cc6",
        "patched_files": [
            "Utilities.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestUtilities.java"
        ]
    },
    "hive_0cbf45c": {
        "bug_id": "hive_0cbf45c",
        "commit": "https://github.com/apache/hive/commit/0cbf45cfc046f39bed4533ab83542002e79b4f5b",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/0cbf45cfc046f39bed4533ab83542002e79b4f5b/llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapDaemonProtocolClientProxy.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapDaemonProtocolClientProxy.java?ref=0cbf45cfc046f39bed4533ab83542002e79b4f5b",
                "deletions": 7,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapDaemonProtocolClientProxy.java",
                "patch": "@@ -16,8 +16,6 @@\n \n import javax.net.SocketFactory;\n \n-import java.io.ByteArrayInputStream;\n-import java.io.DataInputStream;\n import java.io.IOException;\n import java.security.PrivilegedAction;\n import java.util.HashSet;\n@@ -49,7 +47,6 @@\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n import org.apache.hadoop.hive.llap.LlapNodeId;\n-import org.apache.hadoop.hive.llap.configuration.LlapConfiguration;\n import org.apache.hadoop.hive.llap.daemon.LlapDaemonProtocolBlockingPB;\n import org.apache.hadoop.hive.llap.daemon.impl.LlapDaemonProtocolClientImpl;\n import org.apache.hadoop.hive.llap.daemon.rpc.LlapDaemonProtocolProtos.QueryCompleteRequestProto;\n@@ -71,9 +68,9 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-public class TaskCommunicator extends AbstractService {\n+public class LlapDaemonProtocolClientProxy extends AbstractService {\n \n-  private static final Logger LOG = LoggerFactory.getLogger(TaskCommunicator.class);\n+  private static final Logger LOG = LoggerFactory.getLogger(LlapDaemonProtocolClientProxy.class);\n \n   private final ConcurrentMap<String, LlapDaemonProtocolBlockingPB> hostProxies;\n \n@@ -85,9 +82,9 @@\n   private volatile ListenableFuture<Void> requestManagerFuture;\n   private final Token<LlapTokenIdentifier> llapToken;\n \n-  public TaskCommunicator(\n+  public LlapDaemonProtocolClientProxy(\n       int numThreads, Configuration conf, Token<LlapTokenIdentifier> llapToken) {\n-    super(TaskCommunicator.class.getSimpleName());\n+    super(LlapDaemonProtocolClientProxy.class.getSimpleName());\n     this.hostProxies = new ConcurrentHashMap<>();\n     this.socketFactory = NetUtils.getDefaultSocketFactory(conf);\n     this.llapToken = llapToken;",
                "previous_filename": "llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/TaskCommunicator.java",
                "raw_url": "https://github.com/apache/hive/raw/0cbf45cfc046f39bed4533ab83542002e79b4f5b/llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapDaemonProtocolClientProxy.java",
                "sha": "2884e40fe787a1b0884b6832f6a05fa6945baf4a",
                "status": "renamed"
            },
            {
                "additions": 143,
                "blob_url": "https://github.com/apache/hive/blob/0cbf45cfc046f39bed4533ab83542002e79b4f5b/llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java",
                "changes": 187,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java?ref=0cbf45cfc046f39bed4533ab83542002e79b4f5b",
                "deletions": 44,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java",
                "patch": "@@ -22,8 +22,10 @@\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n import java.util.concurrent.atomic.AtomicLong;\n \n+import com.google.common.annotations.VisibleForTesting;\n import com.google.common.base.Preconditions;\n import com.google.common.collect.BiMap;\n import com.google.common.collect.HashBiMap;\n@@ -79,6 +81,9 @@\n \n   private static final Logger LOG = LoggerFactory.getLogger(LlapTaskCommunicator.class);\n \n+  private static final boolean isInfoEnabled = LOG.isInfoEnabled();\n+  private static final boolean isDebugEnabed = LOG.isDebugEnabled();\n+\n   private final SubmitWorkRequestProto BASE_SUBMIT_WORK_REQUEST;\n   private final ConcurrentMap<String, ByteBuffer> credentialMap;\n \n@@ -88,11 +93,17 @@\n   private final SourceStateTracker sourceStateTracker;\n   private final Set<LlapNodeId> nodesForQuery = new HashSet<>();\n \n-  private TaskCommunicator communicator;\n+  private LlapDaemonProtocolClientProxy communicator;\n   private long deleteDelayOnDagComplete;\n   private final LlapTaskUmbilicalProtocol umbilical;\n   private final Token<LlapTokenIdentifier> token;\n \n+  // These two structures track the list of known nodes, and the list of nodes which are sending in keep-alive heartbeats.\n+  // Primarily for debugging purposes a.t.m, since there's some unexplained TASK_TIMEOUTS which are currently being observed.\n+  private final ConcurrentMap<LlapNodeId, Long> knownNodeMap = new ConcurrentHashMap<>();\n+  private final ConcurrentMap<LlapNodeId, PingingNodeInfo> pingedNodeMap = new ConcurrentHashMap<>();\n+\n+\n   private volatile String currentDagName;\n \n   public LlapTaskCommunicator(\n@@ -131,7 +142,7 @@ public void initialize() throws Exception {\n     super.initialize();\n     Configuration conf = getConf();\n     int numThreads = HiveConf.getIntVar(conf, ConfVars.LLAP_DAEMON_COMMUNICATOR_NUM_THREADS);\n-    this.communicator = new TaskCommunicator(numThreads, conf, token);\n+    this.communicator = new LlapDaemonProtocolClientProxy(numThreads, conf, token);\n     this.deleteDelayOnDagComplete = HiveConf.getTimeVar(\n         conf, ConfVars.LLAP_FILE_CLEANUP_DELAY_SECONDS, TimeUnit.SECONDS);\n     LOG.info(\"Running LlapTaskCommunicator with \"\n@@ -235,6 +246,7 @@ public void registerRunningTaskAttempt(final ContainerId containerId, final Task\n     }\n \n     LlapNodeId nodeId = LlapNodeId.getInstance(host, port);\n+    registerKnownNode(nodeId);\n     entityTracker.registerTaskAttempt(containerId, taskSpec.getTaskAttemptID(), host, port);\n     nodesForQuery.add(nodeId);\n \n@@ -254,7 +266,7 @@ public void registerRunningTaskAttempt(final ContainerId containerId, final Task\n     getContext()\n         .taskStartedRemotely(taskSpec.getTaskAttemptID(), containerId);\n     communicator.sendSubmitWork(requestProto, host, port,\n-        new TaskCommunicator.ExecuteRequestCallback<SubmitWorkResponseProto>() {\n+        new LlapDaemonProtocolClientProxy.ExecuteRequestCallback<SubmitWorkResponseProto>() {\n           @Override\n           public void setResponse(SubmitWorkResponseProto response) {\n             if (response.hasSubmissionState()) {\n@@ -333,14 +345,14 @@ private void sendTaskTerminated(final TezTaskAttemptID taskAttemptId,\n     LOG.info(\n         \"DBG: Attempting to send terminateRequest for fragment {} due to internal preemption invoked by {}\",\n         taskAttemptId.toString(), invokedByContainerEnd ? \"containerEnd\" : \"taskEnd\");\n-    LlapNodeId nodeId = entityTracker.getNodeIfForTaskAttempt(taskAttemptId);\n+    LlapNodeId nodeId = entityTracker.getNodeIdForTaskAttempt(taskAttemptId);\n     // NodeId can be null if the task gets unregistered due to failure / being killed by the daemon itself\n     if (nodeId != null) {\n       TerminateFragmentRequestProto request =\n           TerminateFragmentRequestProto.newBuilder().setDagName(currentDagName)\n               .setFragmentIdentifierString(taskAttemptId.toString()).build();\n       communicator.sendTerminateFragment(request, nodeId.getHostname(), nodeId.getPort(),\n-          new TaskCommunicator.ExecuteRequestCallback<TerminateFragmentResponseProto>() {\n+          new LlapDaemonProtocolClientProxy.ExecuteRequestCallback<TerminateFragmentResponseProto>() {\n             @Override\n             public void setResponse(TerminateFragmentResponseProto response) {\n             }\n@@ -365,7 +377,7 @@ public void dagComplete(final String dagName) {\n     for (final LlapNodeId llapNodeId : nodesForQuery) {\n       LOG.info(\"Sending dagComplete message for {}, to {}\", dagName, llapNodeId);\n       communicator.sendQueryComplete(request, llapNodeId.getHostname(), llapNodeId.getPort(),\n-          new TaskCommunicator.ExecuteRequestCallback<LlapDaemonProtocolProtos.QueryCompleteResponseProto>() {\n+          new LlapDaemonProtocolClientProxy.ExecuteRequestCallback<LlapDaemonProtocolProtos.QueryCompleteResponseProto>() {\n             @Override\n             public void setResponse(LlapDaemonProtocolProtos.QueryCompleteResponseProto response) {\n             }\n@@ -391,7 +403,7 @@ public void onVertexStateUpdated(VertexStateUpdate vertexStateUpdate) {\n   public void sendStateUpdate(final String host, final int port,\n                               final SourceStateUpdatedRequestProto request) {\n     communicator.sendSourceStateUpdate(request, host, port,\n-        new TaskCommunicator.ExecuteRequestCallback<SourceStateUpdatedResponseProto>() {\n+        new LlapDaemonProtocolClientProxy.ExecuteRequestCallback<SourceStateUpdatedResponseProto>() {\n           @Override\n           public void setResponse(SourceStateUpdatedResponseProto response) {\n           }\n@@ -409,6 +421,79 @@ public void indicateError(Throwable t) {\n   }\n \n \n+  private static class PingingNodeInfo {\n+    final AtomicLong logTimestamp;\n+    final AtomicInteger pingCount;\n+\n+    PingingNodeInfo(long currentTs) {\n+      logTimestamp = new AtomicLong(currentTs);\n+      pingCount = new AtomicInteger(1);\n+    }\n+  }\n+\n+  public void registerKnownNode(LlapNodeId nodeId) {\n+    Long old = knownNodeMap.putIfAbsent(nodeId,\n+        TimeUnit.MILLISECONDS.convert(System.nanoTime(), TimeUnit.NANOSECONDS));\n+    if (old == null) {\n+      if (isInfoEnabled) {\n+        LOG.info(\"Added new known node: {}\", nodeId);\n+      }\n+    }\n+  }\n+\n+  public void registerPingingNode(LlapNodeId nodeId) {\n+    long currentTs = TimeUnit.MILLISECONDS.convert(System.nanoTime(), TimeUnit.NANOSECONDS);\n+    PingingNodeInfo ni = new PingingNodeInfo(currentTs);\n+    PingingNodeInfo old = pingedNodeMap.put(nodeId, ni);\n+    if (old == null) {\n+      if (isInfoEnabled) {\n+        LOG.info(\"Added new pinging node: [{}]\", nodeId);\n+      }\n+    } else {\n+      old.pingCount.incrementAndGet();\n+    }\n+    // The node should always be known by this point. Log occasionally if it is not known.\n+    if (!knownNodeMap.containsKey(nodeId)) {\n+      if (old == null) {\n+        // First time this is seen. Log it.\n+        LOG.warn(\"Received ping from unknownNode: [{}], count={}\", nodeId, ni.pingCount.get());\n+      } else {\n+        // Pinged before. Log only occasionally.\n+        if (currentTs > old.logTimestamp.get() + 5000l) { // 5 seconds elapsed. Log again.\n+          LOG.warn(\"Received ping from unknownNode: [{}], count={}\", nodeId, old.pingCount.get());\n+          old.logTimestamp.set(currentTs);\n+        }\n+      }\n+\n+    }\n+  }\n+\n+\n+  private final AtomicLong nodeNotFoundLogTime = new AtomicLong(0);\n+\n+  void nodePinged(String hostname, int port) {\n+    LlapNodeId nodeId = LlapNodeId.getInstance(hostname, port);\n+    registerPingingNode(nodeId);\n+    BiMap<ContainerId, TezTaskAttemptID> biMap =\n+        entityTracker.getContainerAttemptMapForNode(nodeId);\n+    if (biMap != null) {\n+      synchronized (biMap) {\n+        for (Map.Entry<ContainerId, TezTaskAttemptID> entry : biMap.entrySet()) {\n+          getContext().taskAlive(entry.getValue());\n+          getContext().containerAlive(entry.getKey());\n+        }\n+      }\n+    } else {\n+      long currentTs = TimeUnit.MILLISECONDS.convert(System.nanoTime(), TimeUnit.NANOSECONDS);\n+      if (currentTs > nodeNotFoundLogTime.get() + 5000l) {\n+        LOG.warn(\"Received ping from node without any registered tasks or containers: \" + hostname +\n+            \":\" + port +\n+            \". Could be caused by pre-emption by the AM,\" +\n+            \" or a mismatched hostname. Enable debug logging for mismatched host names\");\n+        nodeNotFoundLogTime.set(currentTs);\n+      }\n+    }\n+  }\n \n   private void resetCurrentDag(String newDagName) {\n     // Working on the assumption that a single DAG runs at a time per AM.\n@@ -454,6 +539,8 @@ private ByteBuffer serializeCredentials(Credentials credentials) throws IOExcept\n     return ByteBuffer.wrap(containerTokens_dob.getData(), 0, containerTokens_dob.getLength());\n   }\n \n+\n+\n   protected class LlapTaskUmbilicalProtocolImpl implements LlapTaskUmbilicalProtocol {\n \n     private final TezTaskUmbilicalProtocol tezUmbilical;\n@@ -475,7 +562,7 @@ public TezHeartbeatResponse heartbeat(TezHeartbeatRequest request) throws IOExce\n \n     @Override\n     public void nodeHeartbeat(Text hostname, int port) throws IOException {\n-      entityTracker.nodePinged(hostname.toString(), port);\n+      nodePinged(hostname.toString(), port);\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Received heartbeat from [\" + hostname + \":\" + port +\"]\");\n       }\n@@ -502,17 +589,28 @@ public ProtocolSignature getProtocolSignature(String protocol, long clientVersio\n     }\n   }\n \n-  private final class EntityTracker {\n-    private final ConcurrentMap<TezTaskAttemptID, LlapNodeId> attemptToNodeMap = new ConcurrentHashMap<>();\n-    private final ConcurrentMap<ContainerId, LlapNodeId> containerToNodeMap = new ConcurrentHashMap<>();\n-    private final ConcurrentMap<LlapNodeId, BiMap<ContainerId, TezTaskAttemptID>> nodeMap = new ConcurrentHashMap<>();\n+  /**\n+   * Track the association between known containers and taskAttempts, along with the nodes they are assigned to.\n+   */\n+  @VisibleForTesting\n+  static final class EntityTracker {\n+    @VisibleForTesting\n+    final ConcurrentMap<TezTaskAttemptID, LlapNodeId> attemptToNodeMap = new ConcurrentHashMap<>();\n+    @VisibleForTesting\n+    final ConcurrentMap<ContainerId, LlapNodeId> containerToNodeMap = new ConcurrentHashMap<>();\n+    @VisibleForTesting\n+    final ConcurrentMap<LlapNodeId, BiMap<ContainerId, TezTaskAttemptID>> nodeMap = new ConcurrentHashMap<>();\n \n     void registerTaskAttempt(ContainerId containerId, TezTaskAttemptID taskAttemptId, String host, int port) {\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Registering \" + containerId + \", \" + taskAttemptId + \" for node: \" + host + \":\" + port);\n       }\n       LlapNodeId llapNodeId = LlapNodeId.getInstance(host, port);\n       attemptToNodeMap.putIfAbsent(taskAttemptId, llapNodeId);\n+\n+      registerContainer(containerId, host, port);\n+\n+      // nodeMap registration.\n       BiMap<ContainerId, TezTaskAttemptID> tmpMap = HashBiMap.create();\n       BiMap<ContainerId, TezTaskAttemptID> old = nodeMap.putIfAbsent(llapNodeId, tmpMap);\n       BiMap<ContainerId, TezTaskAttemptID> usedInstance;\n@@ -538,10 +636,9 @@ void unregisterTaskAttempt(TezTaskAttemptID attemptId) {\n         synchronized(bMap) {\n           matched = bMap.inverse().remove(attemptId);\n         }\n-      }\n-      // Removing here. Registration into the map has to make sure to put\n-      if (bMap.isEmpty()) {\n-        nodeMap.remove(llapNodeId);\n+        if (bMap.isEmpty()) {\n+          nodeMap.remove(llapNodeId);\n+        }\n       }\n \n       // Remove the container mapping\n@@ -552,23 +649,29 @@ void unregisterTaskAttempt(TezTaskAttemptID attemptId) {\n     }\n \n     void registerContainer(ContainerId containerId, String hostname, int port) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Registering \" + containerId + \" for node: \" + hostname + \":\" + port);\n+      }\n       containerToNodeMap.putIfAbsent(containerId, LlapNodeId.getInstance(hostname, port));\n+      // nodeMap registration is not required, since there's no taskId association.\n     }\n \n     LlapNodeId getNodeIdForContainer(ContainerId containerId) {\n       return containerToNodeMap.get(containerId);\n     }\n \n-    LlapNodeId getNodeIfForTaskAttempt(TezTaskAttemptID taskAttemptId) {\n+    LlapNodeId getNodeIdForTaskAttempt(TezTaskAttemptID taskAttemptId) {\n       return attemptToNodeMap.get(taskAttemptId);\n     }\n \n     ContainerId getContainerIdForAttempt(TezTaskAttemptID taskAttemptId) {\n-      LlapNodeId llapNodeId = getNodeIfForTaskAttempt(taskAttemptId);\n+      LlapNodeId llapNodeId = getNodeIdForTaskAttempt(taskAttemptId);\n       if (llapNodeId != null) {\n         BiMap<TezTaskAttemptID, ContainerId> bMap = nodeMap.get(llapNodeId).inverse();\n         if (bMap != null) {\n-          return bMap.get(taskAttemptId);\n+          synchronized (bMap) {\n+            return bMap.get(taskAttemptId);\n+          }\n         } else {\n           return null;\n         }\n@@ -582,7 +685,9 @@ TezTaskAttemptID getTaskAttemptIdForContainer(ContainerId containerId) {\n       if (llapNodeId != null) {\n         BiMap<ContainerId, TezTaskAttemptID> bMap = nodeMap.get(llapNodeId);\n         if (bMap != null) {\n-          return bMap.get(containerId);\n+          synchronized (bMap) {\n+            return bMap.get(containerId);\n+          }\n         } else {\n           return null;\n         }\n@@ -604,10 +709,9 @@ void unregisterContainer(ContainerId containerId) {\n         synchronized(bMap) {\n           matched = bMap.remove(containerId);\n         }\n-      }\n-      // Removing here. Registration into the map has to make sure to put\n-      if (bMap.isEmpty()) {\n-        nodeMap.remove(llapNodeId);\n+        if (bMap.isEmpty()) {\n+          nodeMap.remove(llapNodeId);\n+        }\n       }\n \n       // Remove the container mapping\n@@ -616,25 +720,20 @@ void unregisterContainer(ContainerId containerId) {\n       }\n     }\n \n-    private final AtomicLong nodeNotFoundLogTime = new AtomicLong(0);\n-    void nodePinged(String hostname, int port) {\n-      LlapNodeId nodeId = LlapNodeId.getInstance(hostname, port);\n-      BiMap<ContainerId, TezTaskAttemptID> biMap = nodeMap.get(nodeId);\n-      if (biMap != null) {\n-        synchronized(biMap) {\n-          for (Map.Entry<ContainerId, TezTaskAttemptID> entry : biMap.entrySet()) {\n-            getContext().taskAlive(entry.getValue());\n-            getContext().containerAlive(entry.getKey());\n-          }\n-        }\n-      } else {\n-        if (System.currentTimeMillis() > nodeNotFoundLogTime.get() + 5000l) {\n-          LOG.warn(\"Received ping from unknown node: \" + hostname + \":\" + port +\n-              \". Could be caused by pre-emption by the AM,\" +\n-              \" or a mismatched hostname. Enable debug logging for mismatched host names\");\n-          nodeNotFoundLogTime.set(System.currentTimeMillis());\n-        }\n-      }\n+    /**\n+     * Return a {@link BiMap} containing container->taskAttemptId mapping for the host specified.\n+     * </p>\n+     * <p/>\n+     * This method return the internal structure used by the EntityTracker. Users must synchronize\n+     * on the structure to ensure correct usage.\n+     *\n+     * @param llapNodeId\n+     * @return\n+     */\n+    BiMap<ContainerId, TezTaskAttemptID> getContainerAttemptMapForNode(LlapNodeId llapNodeId) {\n+      BiMap<ContainerId, TezTaskAttemptID> biMap = nodeMap.get(llapNodeId);\n+      return biMap;\n     }\n+\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hive/raw/0cbf45cfc046f39bed4533ab83542002e79b4f5b/llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java",
                "sha": "5c370eef18e3037ebdc8abee4497cca04e461d1c",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hive/blob/0cbf45cfc046f39bed4533ab83542002e79b4f5b/llap-server/src/test/org/apache/hadoop/hive/llap/tezplugins/TestLlapDaemonProtocolClientProxy.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/test/org/apache/hadoop/hive/llap/tezplugins/TestLlapDaemonProtocolClientProxy.java?ref=0cbf45cfc046f39bed4533ab83542002e79b4f5b",
                "deletions": 9,
                "filename": "llap-server/src/test/org/apache/hadoop/hive/llap/tezplugins/TestLlapDaemonProtocolClientProxy.java",
                "patch": "@@ -28,7 +28,7 @@\n import org.apache.hadoop.hive.llap.LlapNodeId;\n import org.junit.Test;\n \n-public class TestTaskCommunicator {\n+public class TestLlapDaemonProtocolClientProxy {\n \n   @Test (timeout = 5000)\n   public void testMultipleNodes() {\n@@ -38,8 +38,8 @@ public void testMultipleNodes() {\n     LlapNodeId nodeId2 = LlapNodeId.getInstance(\"host2\", 1025);\n \n     Message mockMessage = mock(Message.class);\n-    TaskCommunicator.ExecuteRequestCallback mockExecuteRequestCallback = mock(\n-        TaskCommunicator.ExecuteRequestCallback.class);\n+    LlapDaemonProtocolClientProxy.ExecuteRequestCallback mockExecuteRequestCallback = mock(\n+        LlapDaemonProtocolClientProxy.ExecuteRequestCallback.class);\n \n     // Request two messages\n     requestManager.queueRequest(\n@@ -66,8 +66,8 @@ public void testSingleInvocationPerNode() {\n     LlapNodeId nodeId1 = LlapNodeId.getInstance(\"host1\", 1025);\n \n     Message mockMessage = mock(Message.class);\n-    TaskCommunicator.ExecuteRequestCallback mockExecuteRequestCallback = mock(\n-        TaskCommunicator.ExecuteRequestCallback.class);\n+    LlapDaemonProtocolClientProxy.ExecuteRequestCallback mockExecuteRequestCallback = mock(\n+        LlapDaemonProtocolClientProxy.ExecuteRequestCallback.class);\n \n     // First request for host.\n     requestManager.queueRequest(\n@@ -101,7 +101,7 @@ public void testSingleInvocationPerNode() {\n   }\n \n \n-  static class RequestManagerForTest extends TaskCommunicator.RequestManager {\n+  static class RequestManagerForTest extends LlapDaemonProtocolClientProxy.RequestManager {\n \n     int numSubmissionsCounters = 0;\n     private Map<LlapNodeId, MutableInt> numInvocationsPerNode = new HashMap<>();\n@@ -110,7 +110,7 @@ public RequestManagerForTest(int numThreads) {\n       super(numThreads);\n     }\n \n-    protected void submitToExecutor(TaskCommunicator.CallableRequest request, LlapNodeId nodeId) {\n+    protected void submitToExecutor(LlapDaemonProtocolClientProxy.CallableRequest request, LlapNodeId nodeId) {\n       numSubmissionsCounters++;\n       MutableInt nodeCount = numInvocationsPerNode.get(nodeId);\n       if (nodeCount == null) {\n@@ -127,10 +127,10 @@ void reset() {\n \n   }\n \n-  static class CallableRequestForTest extends TaskCommunicator.CallableRequest<Message, Message> {\n+  static class CallableRequestForTest extends LlapDaemonProtocolClientProxy.CallableRequest<Message, Message> {\n \n     protected CallableRequestForTest(LlapNodeId nodeId, Message message,\n-                                     TaskCommunicator.ExecuteRequestCallback<Message> callback) {\n+                                     LlapDaemonProtocolClientProxy.ExecuteRequestCallback<Message> callback) {\n       super(nodeId, message, callback);\n     }\n ",
                "previous_filename": "llap-server/src/test/org/apache/hadoop/hive/llap/tezplugins/TestTaskCommunicator.java",
                "raw_url": "https://github.com/apache/hive/raw/0cbf45cfc046f39bed4533ab83542002e79b4f5b/llap-server/src/test/org/apache/hadoop/hive/llap/tezplugins/TestLlapDaemonProtocolClientProxy.java",
                "sha": "a6af8c2405c9ead687ac5ddc5531f55a7c6c258d",
                "status": "renamed"
            },
            {
                "additions": 100,
                "blob_url": "https://github.com/apache/hive/blob/0cbf45cfc046f39bed4533ab83542002e79b4f5b/llap-server/src/test/org/apache/hadoop/hive/llap/tezplugins/TestLlapTaskCommunicator.java",
                "changes": 100,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/test/org/apache/hadoop/hive/llap/tezplugins/TestLlapTaskCommunicator.java?ref=0cbf45cfc046f39bed4533ab83542002e79b4f5b",
                "deletions": 0,
                "filename": "llap-server/src/test/org/apache/hadoop/hive/llap/tezplugins/TestLlapTaskCommunicator.java",
                "patch": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ *  you may not use this file except in compliance with the License.\n+ *  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.llap.tezplugins;\n+\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.mock;\n+\n+import org.apache.hadoop.hive.llap.LlapNodeId;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.tez.dag.records.TezTaskAttemptID;\n+import org.junit.Test;\n+\n+public class TestLlapTaskCommunicator {\n+\n+  @Test (timeout = 5000)\n+  public void testEntityTracker1() {\n+    LlapTaskCommunicator.EntityTracker entityTracker = new LlapTaskCommunicator.EntityTracker();\n+\n+    String host1 = \"host1\";\n+    String host2 = \"host2\";\n+    String host3 = \"host3\";\n+    int port = 1451;\n+\n+\n+    // Simple container registration and un-registration without any task attempt being involved.\n+    ContainerId containerId101 = constructContainerId(101);\n+    entityTracker.registerContainer(containerId101, host1, port);\n+    assertEquals(LlapNodeId.getInstance(host1, port), entityTracker.getNodeIdForContainer(containerId101));\n+\n+    entityTracker.unregisterContainer(containerId101);\n+    assertNull(entityTracker.getContainerAttemptMapForNode(LlapNodeId.getInstance(host1, port)));\n+    assertNull(entityTracker.getNodeIdForContainer(containerId101));\n+    assertEquals(0, entityTracker.nodeMap.size());\n+    assertEquals(0, entityTracker.attemptToNodeMap.size());\n+    assertEquals(0, entityTracker.containerToNodeMap.size());\n+\n+\n+    // Simple task registration and un-registration.\n+    ContainerId containerId1 = constructContainerId(1);\n+    TezTaskAttemptID taskAttemptId1 = constructTaskAttemptId(1);\n+    entityTracker.registerTaskAttempt(containerId1, taskAttemptId1, host1, port);\n+    assertEquals(LlapNodeId.getInstance(host1, port), entityTracker.getNodeIdForContainer(containerId1));\n+    assertEquals(LlapNodeId.getInstance(host1, port), entityTracker.getNodeIdForTaskAttempt(taskAttemptId1));\n+\n+    entityTracker.unregisterTaskAttempt(taskAttemptId1);\n+    assertNull(entityTracker.getContainerAttemptMapForNode(LlapNodeId.getInstance(host1, port)));\n+    assertNull(entityTracker.getNodeIdForContainer(containerId1));\n+    assertNull(entityTracker.getNodeIdForTaskAttempt(taskAttemptId1));\n+    assertEquals(0, entityTracker.nodeMap.size());\n+    assertEquals(0, entityTracker.attemptToNodeMap.size());\n+    assertEquals(0, entityTracker.containerToNodeMap.size());\n+\n+    // Register taskAttempt, unregister container. TaskAttempt should also be unregistered\n+    ContainerId containerId201 = constructContainerId(201);\n+    TezTaskAttemptID taskAttemptId201 = constructTaskAttemptId(201);\n+    entityTracker.registerTaskAttempt(containerId201, taskAttemptId201, host1, port);\n+    assertEquals(LlapNodeId.getInstance(host1, port), entityTracker.getNodeIdForContainer(containerId201));\n+    assertEquals(LlapNodeId.getInstance(host1, port), entityTracker.getNodeIdForTaskAttempt(taskAttemptId201));\n+\n+    entityTracker.unregisterContainer(containerId201);\n+    assertNull(entityTracker.getContainerAttemptMapForNode(LlapNodeId.getInstance(host1, port)));\n+    assertNull(entityTracker.getNodeIdForContainer(containerId201));\n+    assertNull(entityTracker.getNodeIdForTaskAttempt(taskAttemptId201));\n+    assertEquals(0, entityTracker.nodeMap.size());\n+    assertEquals(0, entityTracker.attemptToNodeMap.size());\n+    assertEquals(0, entityTracker.containerToNodeMap.size());\n+\n+    entityTracker.unregisterTaskAttempt(taskAttemptId201); // No errors\n+  }\n+\n+\n+  private ContainerId constructContainerId(int id) {\n+    ContainerId containerId = mock(ContainerId.class);\n+    doReturn(id).when(containerId).getId();\n+    doReturn((long)id).when(containerId).getContainerId();\n+    return containerId;\n+  }\n+\n+  private TezTaskAttemptID constructTaskAttemptId(int id) {\n+    TezTaskAttemptID taskAttemptId = mock(TezTaskAttemptID.class);\n+    doReturn(id).when(taskAttemptId).getId();\n+    return taskAttemptId;\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/hive/raw/0cbf45cfc046f39bed4533ab83542002e79b4f5b/llap-server/src/test/org/apache/hadoop/hive/llap/tezplugins/TestLlapTaskCommunicator.java",
                "sha": "8f3d10474b95fa9f609272ba59356979f9e5642c",
                "status": "added"
            }
        ],
        "message": "HIVE-12577: NPE in LlapTaskCommunicator when unregistering containers (Siddarth Seth, reviewed by Sergey Shelukhin)",
        "parent": "https://github.com/apache/hive/commit/b340ecb5e163277e86def59b454b8c041ece39d5",
        "patched_files": [
            "LlapTaskCommunicator.java",
            "LlapDaemonProtocolClientProxy.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestLlapTaskCommunicator.java",
            "TestLlapDaemonProtocolClientProxy.java"
        ]
    },
    "hive_0d93438": {
        "bug_id": "hive_0d93438",
        "commit": "https://github.com/apache/hive/commit/0d93438a3543cb64cbe2ebcdc21e5b40c1dd86e6",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/0d93438a3543cb64cbe2ebcdc21e5b40c1dd86e6/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=0d93438a3543cb64cbe2ebcdc21e5b40c1dd86e6",
                "deletions": 4,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "patch": "@@ -3910,7 +3910,7 @@ static boolean isRegex(String pattern, HiveConf conf) {\n    * @throws SemanticException\n    */\n   private void handleInsertStatementSpec(List<ExprNodeDesc> col_list, String dest,\n-                                         RowResolver out_rwsch, RowResolver inputRR, QB qb,\n+                                         RowResolver outputRR, RowResolver inputRR, QB qb,\n                                          ASTNode selExprList) throws SemanticException {\n     //(z,x)\n     List<String> targetTableSchema = qb.getParseInfo().getDestSchemaForClause(dest);//specified in the query\n@@ -3932,7 +3932,7 @@ private void handleInsertStatementSpec(List<ExprNodeDesc> col_list, String dest,\n     Map<String, ColumnInfo> targetCol2ColumnInfo = new HashMap<String, ColumnInfo>();\n     int colListPos = 0;\n     for(String targetCol : targetTableSchema) {\n-      targetCol2ColumnInfo.put(targetCol, out_rwsch.getColumnInfos().get(colListPos));\n+      targetCol2ColumnInfo.put(targetCol, outputRR.getColumnInfos().get(colListPos));\n       targetCol2Projection.put(targetCol, col_list.get(colListPos++));\n     }\n     Table target = qb.getMetaData().getDestTableForAlias(dest);\n@@ -3976,16 +3976,17 @@ private void handleInsertStatementSpec(List<ExprNodeDesc> col_list, String dest,\n         t.setText(\"TOK_NULL\");\n         ExprNodeDesc exp = genExprNodeDesc(new ASTNode(t), inputRR, tcCtx);\n         new_col_list.add(exp);\n-        final String tableAlias = \"\";//is this OK? this column doesn't come from any table\n+        final String tableAlias = null;//this column doesn't come from any table\n         ColumnInfo colInfo = new ColumnInfo(getColumnInternalName(colListPos),\n           exp.getWritableObjectInspector(), tableAlias, false);\n         newSchema.add(colInfo);\n+        outputRR.addMappingOnly(colInfo.getTabAlias(), colInfo.getInternalName(), colInfo);\n       }\n       colListPos++;\n     }\n     col_list.clear();\n     col_list.addAll(new_col_list);\n-    out_rwsch.setRowSchema(new RowSchema(newSchema));\n+    outputRR.setRowSchema(new RowSchema(newSchema));\n   }\n   String recommendName(ExprNodeDesc exp, String colAlias) {\n     if (!colAlias.startsWith(autogenColAliasPrfxLbl)) {",
                "raw_url": "https://github.com/apache/hive/raw/0d93438a3543cb64cbe2ebcdc21e5b40c1dd86e6/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "sha": "7f355e5d015f13d86a79efd782a562c5f485b1db",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hive/blob/0d93438a3543cb64cbe2ebcdc21e5b40c1dd86e6/ql/src/test/queries/clientpositive/insert_into_with_schema2.q",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/insert_into_with_schema2.q?ref=0d93438a3543cb64cbe2ebcdc21e5b40c1dd86e6",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/insert_into_with_schema2.q",
                "patch": "@@ -0,0 +1,23 @@\n+-- SORT_QUERY_RESULTS;\n+\n+set hive.enforce.bucketing=true;\n+\n+create table studenttab10k (age2 int);\n+insert into studenttab10k values(1);\n+\n+create table student_acid (age int, grade int)\n+ clustered by (age) into 1 buckets;\n+\n+insert into student_acid(age) select * from studenttab10k;\n+\n+select * from student_acid;\n+\n+insert into student_acid(grade, age) select 3 g, * from studenttab10k;\n+\n+select * from student_acid;\n+\n+insert into student_acid(grade, age) values(20, 2);\n+\n+insert into student_acid(age) values(22);\n+\n+select * from student_acid;",
                "raw_url": "https://github.com/apache/hive/raw/0d93438a3543cb64cbe2ebcdc21e5b40c1dd86e6/ql/src/test/queries/clientpositive/insert_into_with_schema2.q",
                "sha": "b7c6b58aad9d0b99a2e619a09a02c2189fcee277",
                "status": "added"
            },
            {
                "additions": 98,
                "blob_url": "https://github.com/apache/hive/blob/0d93438a3543cb64cbe2ebcdc21e5b40c1dd86e6/ql/src/test/results/clientpositive/insert_into_with_schema2.q.out",
                "changes": 98,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/insert_into_with_schema2.q.out?ref=0d93438a3543cb64cbe2ebcdc21e5b40c1dd86e6",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/insert_into_with_schema2.q.out",
                "patch": "@@ -0,0 +1,98 @@\n+PREHOOK: query: create table studenttab10k (age2 int)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@studenttab10k\n+POSTHOOK: query: create table studenttab10k (age2 int)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@studenttab10k\n+PREHOOK: query: insert into studenttab10k values(1)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@values__tmp__table__1\n+PREHOOK: Output: default@studenttab10k\n+POSTHOOK: query: insert into studenttab10k values(1)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@values__tmp__table__1\n+POSTHOOK: Output: default@studenttab10k\n+POSTHOOK: Lineage: studenttab10k.age2 EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+PREHOOK: query: create table student_acid (age int, grade int)\n+ clustered by (age) into 1 buckets\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@student_acid\n+POSTHOOK: query: create table student_acid (age int, grade int)\n+ clustered by (age) into 1 buckets\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@student_acid\n+PREHOOK: query: insert into student_acid(age) select * from studenttab10k\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@studenttab10k\n+PREHOOK: Output: default@student_acid\n+POSTHOOK: query: insert into student_acid(age) select * from studenttab10k\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@studenttab10k\n+POSTHOOK: Output: default@student_acid\n+POSTHOOK: Lineage: student_acid.age SIMPLE [(studenttab10k)studenttab10k.FieldSchema(name:age2, type:int, comment:null), ]\n+POSTHOOK: Lineage: student_acid.grade EXPRESSION []\n+PREHOOK: query: select * from student_acid\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@student_acid\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from student_acid\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@student_acid\n+#### A masked pattern was here ####\n+1\tNULL\n+PREHOOK: query: insert into student_acid(grade, age) select 3 g, * from studenttab10k\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@studenttab10k\n+PREHOOK: Output: default@student_acid\n+POSTHOOK: query: insert into student_acid(grade, age) select 3 g, * from studenttab10k\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@studenttab10k\n+POSTHOOK: Output: default@student_acid\n+POSTHOOK: Lineage: student_acid.age SIMPLE [(studenttab10k)studenttab10k.FieldSchema(name:age2, type:int, comment:null), ]\n+POSTHOOK: Lineage: student_acid.grade SIMPLE []\n+PREHOOK: query: select * from student_acid\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@student_acid\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from student_acid\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@student_acid\n+#### A masked pattern was here ####\n+1\t3\n+1\tNULL\n+PREHOOK: query: insert into student_acid(grade, age) values(20, 2)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@values__tmp__table__2\n+PREHOOK: Output: default@student_acid\n+POSTHOOK: query: insert into student_acid(grade, age) values(20, 2)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@values__tmp__table__2\n+POSTHOOK: Output: default@student_acid\n+POSTHOOK: Lineage: student_acid.age EXPRESSION [(values__tmp__table__2)values__tmp__table__2.FieldSchema(name:tmp_values_col2, type:string, comment:), ]\n+POSTHOOK: Lineage: student_acid.grade EXPRESSION [(values__tmp__table__2)values__tmp__table__2.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+PREHOOK: query: insert into student_acid(age) values(22)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@values__tmp__table__3\n+PREHOOK: Output: default@student_acid\n+POSTHOOK: query: insert into student_acid(age) values(22)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@values__tmp__table__3\n+POSTHOOK: Output: default@student_acid\n+POSTHOOK: Lineage: student_acid.age EXPRESSION [(values__tmp__table__3)values__tmp__table__3.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+POSTHOOK: Lineage: student_acid.grade EXPRESSION []\n+PREHOOK: query: select * from student_acid\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@student_acid\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from student_acid\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@student_acid\n+#### A masked pattern was here ####\n+1\t3\n+1\tNULL\n+2\t20\n+22\tNULL",
                "raw_url": "https://github.com/apache/hive/raw/0d93438a3543cb64cbe2ebcdc21e5b40c1dd86e6/ql/src/test/results/clientpositive/insert_into_with_schema2.q.out",
                "sha": "a55a82f8f9c3c43a728b7d2d106b1bbb24a3337c",
                "status": "added"
            }
        ],
        "message": "HIVE-10776 - Schema on insert for bucketed tables throwing NullPointerException(Eugene Koifman, reviewed by Alan Gates)",
        "parent": "https://github.com/apache/hive/commit/95929308b43bd741220eeca60896eadb92496510",
        "patched_files": [
            "SemanticAnalyzer.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestSemanticAnalyzer.java"
        ]
    },
    "hive_0f3998a": {
        "bug_id": "hive_0f3998a",
        "commit": "https://github.com/apache/hive/commit/0f3998af3c5e9d8168b32d5085165892c2942101",
        "file": [
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hive/blob/0f3998af3c5e9d8168b32d5085165892c2942101/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java?ref=0f3998af3c5e9d8168b32d5085165892c2942101",
                "deletions": 6,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java",
                "patch": "@@ -1161,6 +1161,12 @@ public boolean dropTable(String dbName, String tableName) throws MetaException,\n           pm.deletePersistentAll(partGrants);\n         }\n \n+        // TODO# temporary; will be removed with ACID. Otherwise, need to do direct delete w/o get.\n+        List<MTableWrite> mtw = getTableWrites(dbName, tableName, -1, -1);\n+        if (mtw != null && mtw.size() > 0) {\n+          pm.deletePersistentAll(mtw);\n+        }\n+\n         List<MPartitionColumnPrivilege> partColGrants = listTableAllPartitionColumnGrants(dbName,\n             tableName);\n         if (partColGrants != null && partColGrants.size() > 0) {\n@@ -8866,17 +8872,25 @@ public MTableWrite getTableWrite(\n   public List<MTableWrite> getTableWrites(\n       String dbName, String tblName, long from, long to) throws MetaException {\n     boolean success = false;\n+    dbName = HiveStringUtils.normalizeIdentifier(dbName);\n+    tblName = HiveStringUtils.normalizeIdentifier(tblName);\n     Query query = null;\n     openTransaction();\n     try {\n-      query = pm.newQuery(MTableWrite.class,\n-          \"table.tableName == t1 && table.database.name == t2 && writeId > t3 && writeId < t4\");\n-      query.declareParameters(\n-          \"java.lang.String t1, java.lang.String t2, java.lang.Long t3, java.lang.Long t4\");\n+      String queryStr = \"table.tableName == t1 && table.database.name == t2 && writeId > t3\",\n+          argStr = \"java.lang.String t1, java.lang.String t2, java.lang.Long t3\";\n+      if (to >= 0) {\n+        queryStr += \" && writeId < t4\";\n+        argStr += \", java.lang.Long t4\";\n+      }\n+      query = pm.newQuery(MTableWrite.class, queryStr);\n+      query.declareParameters(argStr);\n       query.setOrdering(\"writeId asc\");\n       @SuppressWarnings(\"unchecked\")\n-      List<MTableWrite> writes =\n-        (List<MTableWrite>) query.executeWithArray(tblName, dbName, from, to);\n+      List<MTableWrite> writes = (List<MTableWrite>)(to >= 0\n+         ? query.executeWithArray(tblName, dbName, from, to)\n+         : query.executeWithArray(tblName, dbName, from));\n+      pm.retrieveAll(writes);\n       success = true;\n       return (writes == null || writes.isEmpty()) ? null : new ArrayList<>(writes);\n     } finally {",
                "raw_url": "https://github.com/apache/hive/raw/0f3998af3c5e9d8168b32d5085165892c2942101/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java",
                "sha": "8ad70590037dcac358c83b28f8e151d26ca9b9ce",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/0f3998af3c5e9d8168b32d5085165892c2942101/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java?ref=0f3998af3c5e9d8168b32d5085165892c2942101",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java",
                "patch": "@@ -374,6 +374,7 @@ public boolean doNext(WritableComparable key, Writable value) throws IOException\n       Utilities.copyTableJobPropertiesToConf(currDesc.getTableDesc(), job);\n       InputFormat inputFormat = getInputFormatFromCache(formatter, job);\n       String inputs = processCurrPathForMmWriteIds(inputFormat);\n+      Utilities.LOG14535.info(\"Setting fetch inputs to \" + inputs);\n       if (inputs == null) return null;\n       job.set(\"mapred.input.dir\", inputs);\n ",
                "raw_url": "https://github.com/apache/hive/raw/0f3998af3c5e9d8168b32d5085165892c2942101/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java",
                "sha": "dadae4d950b4a24b32043540098c1ce15feaf294",
                "status": "modified"
            }
        ],
        "message": "HIVE-14990 : run all tests for MM tables and fix the issues that are found - issue with FetchOperator, drop, NPE (Sergey Shelukhin)",
        "parent": "https://github.com/apache/hive/commit/c587404d4d2f0995af822aa6871d4aadfbf6aeda",
        "patched_files": [
            "ObjectStore.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestObjectStore.java"
        ]
    },
    "hive_10cfdd9": {
        "bug_id": "hive_10cfdd9",
        "commit": "https://github.com/apache/hive/commit/10cfdd96b69084bf0da7b864113d073006147e77",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/10cfdd96b69084bf0da7b864113d073006147e77/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/TestHBaseMetastoreSql.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/TestHBaseMetastoreSql.java?ref=10cfdd96b69084bf0da7b864113d073006147e77",
                "deletions": 2,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/TestHBaseMetastoreSql.java",
                "patch": "@@ -79,7 +79,7 @@ public void database() throws Exception {\n     Assert.assertEquals(0, rsp.getResponseCode());\n   }\n \n-  @Ignore\n+  @Test\n   public void table() throws Exception {\n     driver.run(\"create table tbl (c int)\");\n     CommandProcessorResponse rsp = driver.run(\"insert into table tbl values (3)\");\n@@ -92,7 +92,7 @@ public void table() throws Exception {\n     Assert.assertEquals(0, rsp.getResponseCode());\n   }\n \n-  @Ignore\n+  @Test\n   public void partitionedTable() throws Exception {\n     driver.run(\"create table parttbl (c int) partitioned by (ds string)\");\n     CommandProcessorResponse rsp =\n@@ -113,8 +113,11 @@ public void partitionedTable() throws Exception {\n     Assert.assertEquals(0, rsp.getResponseCode());\n     rsp = driver.run(\"alter table parttbl touch partition (ds = 'whenever')\");\n     Assert.assertEquals(0, rsp.getResponseCode());\n+    // TODO - Can't do this until getPartitionsByExpr implemented\n+    /*\n     rsp = driver.run(\"alter table parttbl drop partition (ds = 'whenever')\");\n     Assert.assertEquals(0, rsp.getResponseCode());\n+    */\n     rsp = driver.run(\"select * from parttbl\");\n     Assert.assertEquals(0, rsp.getResponseCode());\n     rsp = driver.run(\"select * from parttbl where ds = 'today'\");",
                "raw_url": "https://github.com/apache/hive/raw/10cfdd96b69084bf0da7b864113d073006147e77/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/TestHBaseMetastoreSql.java",
                "sha": "676c3896da53129e6d813ae0dcfe61f278563c35",
                "status": "modified"
            },
            {
                "additions": 122,
                "blob_url": "https://github.com/apache/hive/blob/10cfdd96b69084bf0da7b864113d073006147e77/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/SharedStorageDescriptor.java",
                "changes": 770,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/SharedStorageDescriptor.java?ref=10cfdd96b69084bf0da7b864113d073006147e77",
                "deletions": 648,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/hbase/SharedStorageDescriptor.java",
                "patch": "@@ -27,751 +27,225 @@\n import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n \n import java.util.ArrayList;\n-import java.util.Collection;\n import java.util.Iterator;\n import java.util.List;\n-import java.util.Map;\n \n /**\n  * A {@link org.apache.hadoop.hive.metastore.api.StorageDescriptor} with most of it's content\n- * shared.  Location and parameters are left alone, everything else is redirected to a shared\n- * reference in the cache.\n+ * shallow copied from the underlying storage descriptor.  Location and parameters are left alone.\n+ * To avoid issues when users change the contents, all lists and nested structures (cols, serde,\n+ * buckets, sortCols, and skewed) are deep copied when they are accessed for reading or writing.\n+ * (It has to be done on read as well because there's no way to guarantee the user won't change the\n+ * nested structure or list, which would result in changing every storage descriptor sharing that\n+ * structure.)  Users wishing better performance can call setReadyOnly(), which will prevent the\n+ * copies.\n  */\n public class SharedStorageDescriptor extends StorageDescriptor {\n   static final private Log LOG = LogFactory.getLog(SharedStorageDescriptor.class.getName());\n-  private StorageDescriptor shared;\n-  private boolean copied = false;\n-  private CopyOnWriteColList colList = null;\n-  private CopyOnWriteOrderList orderList = null;\n-  private CopyOnWriteBucketList bucketList = null;\n+  private boolean colsCopied = false;\n+  private boolean serdeCopied = false;\n+  private boolean bucketsCopied = false;\n+  private boolean sortCopied = false;\n+  private boolean skewedCopied = false;\n \n   SharedStorageDescriptor() {\n   }\n \n-  public SharedStorageDescriptor(SharedStorageDescriptor that) {\n-    this.setLocation(that.getLocation());\n-    this.setParameters(that.getParameters());\n-    this.shared = that.shared;\n+  void setShared(StorageDescriptor shared) {\n+    if (shared.getCols() != null) super.setCols(shared.getCols());\n+    // Skip location\n+    if (shared.getInputFormat() != null) super.setInputFormat(shared.getInputFormat());\n+    if (shared.getOutputFormat() != null) super.setOutputFormat(shared.getOutputFormat());\n+    super.setCompressed(shared.isCompressed());\n+    super.setNumBuckets(shared.getNumBuckets());\n+    if (shared.getSerdeInfo() != null) super.setSerdeInfo(shared.getSerdeInfo());\n+    if (shared.getBucketCols() != null) super.setBucketCols(shared.getBucketCols());\n+    if (shared.getSortCols() != null) super.setSortCols(shared.getSortCols());\n+    // skip parameters\n+    if (shared.getSkewedInfo() != null) super.setSkewedInfo(shared.getSkewedInfo());\n+    super.setStoredAsSubDirectories(shared.isStoredAsSubDirectories());\n   }\n \n-  @Override\n-  public StorageDescriptor deepCopy() {\n-    return new SharedStorageDescriptor(this);\n+  /**\n+   * Promise that you'll only use this shared storage descriptor in a read only mode.\n+   * This prevents the copies of the nested structures and lists when reading them.  However, the\n+   * caller must not change the structures or lists returned to it, as this will change all\n+   * storage descriptor sharing that list.\n+   */\n+  public void setReadOnly() {\n+    colsCopied = serdeCopied = bucketsCopied = sortCopied = skewedCopied = true;\n   }\n \n   @Override\n-  public boolean isSetCols() {\n-    return shared.isSetCols();\n+  public void addToCols(FieldSchema fs) {\n+    copyCols();\n+    super.addToCols(fs);\n   }\n \n   @Override\n   public List<FieldSchema> getCols() {\n-    return copied ? shared.getCols() : (\n-        shared.getCols() == null ? null : copyCols(shared.getCols()));\n-  }\n-\n-  @Override\n-  public int getColsSize() {\n-    return shared.getColsSize();\n-  }\n-\n-  @Override\n-  public Iterator<FieldSchema> getColsIterator() {\n-    return shared.getColsIterator();\n+    copyCols();\n+    return super.getCols();\n   }\n \n   @Override\n   public void setCols(List<FieldSchema> cols) {\n-    copyOnWrite();\n-    shared.setCols(cols);\n-  }\n-\n-  @Override\n-  public void addToCols(FieldSchema fs) {\n-    copyOnWrite();\n-    shared.addToCols(fs);\n+    colsCopied = true;\n+    super.setCols(cols);\n   }\n \n   @Override\n   public void unsetCols() {\n-    copyOnWrite();\n-    shared.unsetCols();\n+    colsCopied = true;\n+    super.unsetCols();\n   }\n \n   @Override\n-  public boolean isSetInputFormat() {\n-    return shared.isSetInputFormat();\n-  }\n-\n-  @Override\n-  public String getInputFormat() {\n-    return shared.getInputFormat();\n-  }\n-\n-  @Override\n-  public void setInputFormat(String inputFormat) {\n-    copyOnWrite();\n-    shared.setInputFormat(inputFormat);\n-  }\n-\n-  @Override\n-  public void unsetInputFormat() {\n-    copyOnWrite();\n-    shared.unsetInputFormat();\n-  }\n-\n-  @Override\n-  public boolean isSetOutputFormat() {\n-    return shared.isSetOutputFormat();\n-  }\n-\n-  @Override\n-  public String getOutputFormat() {\n-    return shared.getOutputFormat();\n-  }\n-\n-  @Override\n-  public void setOutputFormat(String outputFormat) {\n-    copyOnWrite();\n-    shared.setOutputFormat(outputFormat);\n-  }\n-\n-  @Override\n-  public void unsetOutputFormat() {\n-    copyOnWrite();\n-    shared.unsetOutputFormat();\n-  }\n-\n-  @Override\n-  public boolean isSetCompressed() {\n-    return shared.isSetCompressed();\n-  }\n-\n-  @Override\n-  public boolean isCompressed() {\n-    return shared.isCompressed();\n-  }\n-\n-  @Override\n-  public void setCompressed(boolean isCompressed) {\n-    copyOnWrite();\n-    shared.setCompressed(isCompressed);\n-  }\n-\n-  @Override\n-  public void unsetCompressed() {\n-    copyOnWrite();\n-    shared.unsetCompressed();\n-  }\n-\n-  @Override\n-  public boolean isSetNumBuckets() {\n-    return shared.isSetNumBuckets();\n-  }\n-\n-  @Override\n-  public int getNumBuckets() {\n-    return shared.getNumBuckets();\n-  }\n-\n-  @Override\n-  public void setNumBuckets(int numBuckets) {\n-    copyOnWrite();\n-    shared.setNumBuckets(numBuckets);\n-  }\n-\n-  @Override\n-  public void unsetNumBuckets() {\n-    copyOnWrite();\n-    shared.unsetNumBuckets();\n+  public Iterator<FieldSchema> getColsIterator() {\n+    copyCols();\n+    return super.getColsIterator();\n   }\n \n-  @Override\n-  public boolean isSetSerdeInfo() {\n-    return shared.isSetSerdeInfo();\n+  private void copyCols() {\n+    if (!colsCopied) {\n+      colsCopied = true;\n+      if (super.getCols() != null) {\n+        List<FieldSchema> cols = new ArrayList<FieldSchema>(super.getColsSize());\n+        for (FieldSchema fs : super.getCols()) cols.add(new FieldSchema(fs));\n+        super.setCols(cols);\n+      }\n+    }\n   }\n \n   @Override\n   public SerDeInfo getSerdeInfo() {\n-    return copied ? shared.getSerdeInfo() : (\n-        shared.getSerdeInfo() == null ? null : new SerDeInfoWrapper(shared.getSerdeInfo()));\n+    copySerde();\n+    return super.getSerdeInfo();\n   }\n \n   @Override\n   public void setSerdeInfo(SerDeInfo serdeInfo) {\n-    copyOnWrite();\n-    shared.setSerdeInfo(serdeInfo);\n+    serdeCopied = true;\n+    super.setSerdeInfo(serdeInfo);\n   }\n \n   @Override\n   public void unsetSerdeInfo() {\n-    copyOnWrite();\n-    shared.unsetSerdeInfo();\n-  }\n-\n-  @Override\n-  public boolean isSetBucketCols() {\n-    return shared.isSetBucketCols();\n+    serdeCopied = true;\n+    super.unsetSerdeInfo();\n   }\n \n-  @Override\n-  public List<String> getBucketCols() {\n-    return copied ? shared.getBucketCols() : (\n-        shared.getBucketCols() == null ? null : copyBucketCols(shared.getBucketCols()));\n-  }\n-\n-  @Override\n-  public int getBucketColsSize() {\n-    return shared.getBucketColsSize();\n+  private void copySerde() {\n+    if (!serdeCopied) {\n+      serdeCopied = true;\n+      if (super.getSerdeInfo() != null) super.setSerdeInfo(new SerDeInfo(super.getSerdeInfo()));\n+    }\n   }\n \n   @Override\n-  public Iterator<String> getBucketColsIterator() {\n-    return shared.getBucketColsIterator();\n+  public void addToBucketCols(String bucket) {\n+    copyBucketCols();\n+    super.addToBucketCols(bucket);\n   }\n \n   @Override\n-  public void setBucketCols(List<String> bucketCols) {\n-    copyOnWrite();\n-    shared.setBucketCols(bucketCols);\n+  public List<String> getBucketCols() {\n+    copyBucketCols();\n+    return super.getBucketCols();\n   }\n \n   @Override\n-  public void addToBucketCols(String bucketCol) {\n-    copyOnWrite();\n-    shared.addToBucketCols(bucketCol);\n+  public void setBucketCols(List<String> buckets) {\n+    bucketsCopied = true;\n+    super.setBucketCols(buckets);\n   }\n \n   @Override\n   public void unsetBucketCols() {\n-    copyOnWrite();\n-    shared.unsetBucketCols();\n+    bucketsCopied = true;\n+    super.unsetBucketCols();\n   }\n \n   @Override\n-  public boolean isSetSortCols() {\n-    return shared.isSetSortCols();\n+  public Iterator<String> getBucketColsIterator() {\n+    copyBucketCols();\n+    return super.getBucketColsIterator();\n   }\n \n-  @Override\n-  public List<Order> getSortCols() {\n-    return copied ? shared.getSortCols() : (\n-        shared.getSortCols() == null ? null : copySort(shared.getSortCols()));\n+  private void copyBucketCols() {\n+    if (!bucketsCopied) {\n+      bucketsCopied = true;\n+      if (super.getBucketCols() != null) {\n+        List<String> buckets = new ArrayList<String>(super.getBucketColsSize());\n+        for (String bucket : super.getBucketCols()) buckets.add(bucket);\n+        super.setBucketCols(buckets);\n+      }\n+    }\n   }\n \n   @Override\n-  public int getSortColsSize() {\n-    return shared.getSortColsSize();\n+  public void addToSortCols(Order sort) {\n+    copySort();\n+    super.addToSortCols(sort);\n   }\n \n   @Override\n-  public Iterator<Order> getSortColsIterator() {\n-    return shared.getSortColsIterator();\n+  public List<Order> getSortCols() {\n+    copySort();\n+    return super.getSortCols();\n   }\n \n   @Override\n-  public void setSortCols(List<Order> sortCols) {\n-    copyOnWrite();\n-    shared.setSortCols(sortCols);\n+  public void setSortCols(List<Order> sorts) {\n+    sortCopied = true;\n+    super.setSortCols(sorts);\n   }\n \n   @Override\n-  public void addToSortCols(Order sortCol) {\n-    copyOnWrite();\n-    shared.addToSortCols(sortCol);\n+  public void unsetSortCols() {\n+    sortCopied = true;\n+    super.unsetSortCols();\n   }\n \n   @Override\n-  public void unsetSortCols() {\n-    copyOnWrite();\n-    shared.unsetSortCols();\n+  public Iterator<Order> getSortColsIterator() {\n+    copySort();\n+    return super.getSortColsIterator();\n   }\n \n-  @Override\n-  public boolean isSetSkewedInfo() {\n-    return shared.isSetSkewedInfo();\n+  private void copySort() {\n+    if (!sortCopied) {\n+      sortCopied = true;\n+      if (super.getSortCols() != null) {\n+        List<Order> sortCols = new ArrayList<Order>(super.getSortColsSize());\n+        for (Order sortCol : super.getSortCols()) sortCols.add(new Order(sortCol));\n+        super.setSortCols(sortCols);\n+      }\n+    }\n   }\n \n   @Override\n   public SkewedInfo getSkewedInfo() {\n-    return copied ? shared.getSkewedInfo() : (\n-        shared.getSkewedInfo() == null ? null : new SkewWrapper(shared.getSkewedInfo()));\n+    copySkewed();\n+    return super.getSkewedInfo();\n   }\n \n   @Override\n   public void setSkewedInfo(SkewedInfo skewedInfo) {\n-    copyOnWrite();\n-    shared.setSkewedInfo(skewedInfo);\n+    skewedCopied = true;\n+    super.setSkewedInfo(skewedInfo);\n   }\n \n   @Override\n   public void unsetSkewedInfo() {\n-    copyOnWrite();\n-    shared.unsetSkewedInfo();\n-  }\n-\n-  @Override\n-  public boolean isSetStoredAsSubDirectories() {\n-    return shared.isSetStoredAsSubDirectories();\n-  }\n-\n-  @Override\n-  public boolean isStoredAsSubDirectories() {\n-    return shared.isStoredAsSubDirectories();\n-  }\n-\n-  @Override\n-  public void setStoredAsSubDirectories(boolean sasd) {\n-    copyOnWrite();\n-    shared.setStoredAsSubDirectories(sasd);\n-  }\n-\n-  @Override\n-  public void unsetStoredAsSubDirectories() {\n-    copyOnWrite();\n-    shared.unsetStoredAsSubDirectories();\n+    skewedCopied = true;\n+    super.unsetSkewedInfo();\n   }\n \n-  void setShared(StorageDescriptor sd) {\n-    shared = sd;\n-  }\n-\n-  StorageDescriptor getShared() {\n-    return shared;\n-  }\n-\n-  private void copyOnWrite() {\n-    if (!copied) {\n-      shared = new StorageDescriptor(shared);\n-      copied = true;\n-    }\n-  }\n-\n-  private class SerDeInfoWrapper extends SerDeInfo {\n-\n-    SerDeInfoWrapper(SerDeInfo serde) {\n-      super(serde);\n-    }\n-\n-    @Override\n-    public void setName(String name) {\n-      copyOnWrite();\n-      shared.getSerdeInfo().setName(name);\n-    }\n-\n-    @Override\n-    public void unsetName() {\n-      copyOnWrite();\n-      shared.getSerdeInfo().unsetName();\n-    }\n-\n-    @Override\n-    public void setSerializationLib(String lib) {\n-      copyOnWrite();\n-      shared.getSerdeInfo().setSerializationLib(lib);\n-    }\n-\n-    @Override\n-    public void unsetSerializationLib() {\n-      copyOnWrite();\n-      shared.getSerdeInfo().unsetSerializationLib();\n-    }\n-\n-    @Override\n-    public void setParameters(Map<String, String> parameters) {\n-      copyOnWrite();\n-      shared.getSerdeInfo().setParameters(parameters);\n-    }\n-\n-    @Override\n-    public void unsetParameters() {\n-      copyOnWrite();\n-      shared.getSerdeInfo().unsetParameters();\n-    }\n-\n-    @Override\n-    public void putToParameters(String key, String value) {\n-      copyOnWrite();\n-      shared.getSerdeInfo().putToParameters(key, value);\n+  private void copySkewed() {\n+    if (!skewedCopied) {\n+      skewedCopied = true;\n+      if (super.getSkewedInfo() != null) super.setSkewedInfo(new SkewedInfo(super.getSkewedInfo()));\n     }\n   }\n-\n-  private class SkewWrapper extends SkewedInfo {\n-    SkewWrapper(SkewedInfo skew) {\n-      super(skew);\n-    }\n-\n-    @Override\n-    public void setSkewedColNames(List<String> skewedColNames) {\n-      copyOnWrite();\n-      shared.getSkewedInfo().setSkewedColNames(skewedColNames);\n-    }\n-\n-    @Override\n-    public void unsetSkewedColNames() {\n-      copyOnWrite();\n-      shared.getSkewedInfo().unsetSkewedColNames();\n-    }\n-\n-    @Override\n-    public void addToSkewedColNames(String skewCol) {\n-      copyOnWrite();\n-      shared.getSkewedInfo().addToSkewedColNames(skewCol);\n-    }\n-\n-    @Override\n-    public void setSkewedColValues(List<List<String>> skewedColValues) {\n-      copyOnWrite();\n-      shared.getSkewedInfo().setSkewedColValues(skewedColValues);\n-    }\n-\n-    @Override\n-    public void unsetSkewedColValues() {\n-      copyOnWrite();\n-      shared.getSkewedInfo().unsetSkewedColValues();\n-    }\n-\n-    @Override\n-    public void addToSkewedColValues(List<String> skewedColValue) {\n-      copyOnWrite();\n-      shared.getSkewedInfo().addToSkewedColValues(skewedColValue);\n-    }\n-\n-    @Override\n-    public void setSkewedColValueLocationMaps(Map<List<String>, String> maps) {\n-      copyOnWrite();\n-      shared.getSkewedInfo().setSkewedColValueLocationMaps(maps);\n-    }\n-\n-    @Override\n-    public void unsetSkewedColValueLocationMaps() {\n-      copyOnWrite();\n-      shared.getSkewedInfo().unsetSkewedColValueLocationMaps();\n-    }\n-\n-    @Override\n-    public void putToSkewedColValueLocationMaps(List<String> key, String value) {\n-      copyOnWrite();\n-      shared.getSkewedInfo().putToSkewedColValueLocationMaps(key, value);\n-    }\n-  }\n-\n-  private CopyOnWriteOrderList copySort(List<Order> sort) {\n-    if (orderList == null) {\n-      orderList = new CopyOnWriteOrderList(sort.size());\n-      for (int i = 0; i < sort.size(); i++) {\n-        orderList.secretAdd(new OrderWrapper(i, sort.get(i)));\n-      }\n-    }\n-    return orderList;\n-  }\n-\n-  private class CopyOnWriteOrderList extends ArrayList<Order> {\n-\n-    CopyOnWriteOrderList(int size) {\n-      super(size);\n-    }\n-\n-    private void secretAdd(OrderWrapper order) {\n-      super.add(order);\n-    }\n-\n-    @Override\n-    public boolean add(Order t) {\n-      copyOnWrite();\n-      return shared.getSortCols().add(t);\n-    }\n-\n-    @Override\n-    public boolean remove(Object o) {\n-      copyOnWrite();\n-      return shared.getSortCols().remove(o);\n-    }\n-\n-    @Override\n-    public boolean addAll(Collection<? extends Order> c) {\n-      copyOnWrite();\n-      return shared.getSortCols().addAll(c);\n-    }\n-\n-    @Override\n-    public boolean addAll(int index, Collection<? extends Order> c) {\n-      copyOnWrite();\n-      return shared.getSortCols().addAll(c);\n-    }\n-\n-    @Override\n-    public boolean removeAll(Collection<?> c) {\n-      copyOnWrite();\n-      return shared.getSortCols().removeAll(c);\n-    }\n-\n-    @Override\n-    public boolean retainAll(Collection<?> c) {\n-      copyOnWrite();\n-      return shared.getSortCols().retainAll(c);\n-    }\n-\n-    @Override\n-    public void clear() {\n-      copyOnWrite();\n-      shared.getSortCols().clear();\n-    }\n-\n-    @Override\n-    public Order set(int index, Order element) {\n-      copyOnWrite();\n-      return shared.getSortCols().set(index, element);\n-    }\n-\n-    @Override\n-    public void add(int index, Order element) {\n-      copyOnWrite();\n-      shared.getSortCols().add(index, element);\n-    }\n-\n-    @Override\n-    public Order remove(int index) {\n-      copyOnWrite();\n-      return shared.getSortCols().remove(index);\n-    }\n-  }\n-\n-  private class OrderWrapper extends Order {\n-    final private int pos;\n-\n-    OrderWrapper(int pos, Order order) {\n-      super(order);\n-      this.pos = pos;\n-    }\n-\n-    @Override\n-    public void setCol(String col) {\n-      copyOnWrite();\n-      shared.getSortCols().get(pos).setCol(col);\n-    }\n-\n-    @Override\n-    public void unsetCol() {\n-      copyOnWrite();\n-      shared.getSortCols().get(pos).unsetCol();\n-    }\n-\n-    @Override\n-    public void setOrder(int order) {\n-      copyOnWrite();\n-      shared.getSortCols().get(pos).setOrder(order);\n-    }\n-\n-    @Override\n-    public void unsetOrder() {\n-      copyOnWrite();\n-      shared.getSortCols().get(pos).unsetOrder();\n-    }\n-  }\n-\n-  private CopyOnWriteColList copyCols(List<FieldSchema> cols) {\n-    if (colList == null) {\n-      colList = new CopyOnWriteColList(cols.size());\n-      for (int i = 0; i < cols.size(); i++) {\n-        colList.secretAdd(new FieldSchemaWrapper(i, cols.get(i)));\n-      }\n-    }\n-    return colList;\n-  }\n-\n-  private class CopyOnWriteColList extends ArrayList<FieldSchema> {\n-\n-    CopyOnWriteColList(int size) {\n-      super(size);\n-    }\n-\n-    private void secretAdd(FieldSchemaWrapper col) {\n-      super.add(col);\n-    }\n-\n-    @Override\n-    public boolean add(FieldSchema t) {\n-      copyOnWrite();\n-      return shared.getCols().add(t);\n-    }\n-\n-    @Override\n-    public boolean remove(Object o) {\n-      copyOnWrite();\n-      return shared.getCols().remove(o);\n-    }\n-\n-    @Override\n-    public boolean addAll(Collection<? extends FieldSchema> c) {\n-      copyOnWrite();\n-      return shared.getCols().addAll(c);\n-    }\n-\n-    @Override\n-    public boolean addAll(int index, Collection<? extends FieldSchema> c) {\n-      copyOnWrite();\n-      return shared.getCols().addAll(c);\n-    }\n-\n-    @Override\n-    public boolean removeAll(Collection<?> c) {\n-      copyOnWrite();\n-      return shared.getCols().removeAll(c);\n-    }\n-\n-    @Override\n-    public boolean retainAll(Collection<?> c) {\n-      copyOnWrite();\n-      return shared.getCols().retainAll(c);\n-    }\n-\n-    @Override\n-    public void clear() {\n-      copyOnWrite();\n-      shared.getCols().clear();\n-    }\n-\n-    @Override\n-    public FieldSchema set(int index, FieldSchema element) {\n-      copyOnWrite();\n-      return shared.getCols().set(index, element);\n-    }\n-\n-    @Override\n-    public void add(int index, FieldSchema element) {\n-      copyOnWrite();\n-      shared.getCols().add(index, element);\n-    }\n-\n-    @Override\n-    public FieldSchema remove(int index) {\n-      copyOnWrite();\n-      return shared.getCols().remove(index);\n-    }\n-  }\n-\n-  private class FieldSchemaWrapper extends FieldSchema {\n-    final private int pos;\n-\n-    FieldSchemaWrapper(int pos, FieldSchema col) {\n-      super(col);\n-      this.pos = pos;\n-    }\n-\n-    @Override\n-    public void setName(String name) {\n-      copyOnWrite();\n-      shared.getCols().get(pos).setName(name);\n-    }\n-\n-    @Override\n-    public void unsetName() {\n-      copyOnWrite();\n-      shared.getCols().get(pos).unsetName();\n-    }\n-\n-    @Override\n-    public void setType(String type) {\n-      copyOnWrite();\n-      shared.getCols().get(pos).setType(type);\n-    }\n-\n-    @Override\n-    public void unsetType() {\n-      copyOnWrite();\n-      shared.getCols().get(pos).unsetType();\n-    }\n-\n-    @Override\n-    public void setComment(String comment) {\n-      copyOnWrite();\n-      shared.getCols().get(pos).setComment(comment);\n-    }\n-\n-    @Override\n-    public void unsetComment() {\n-      copyOnWrite();\n-      shared.getCols().get(pos).unsetComment();\n-    }\n-  }\n-\n-  private CopyOnWriteBucketList copyBucketCols(List<String> cols) {\n-    if (bucketList == null) {\n-      bucketList = new CopyOnWriteBucketList(cols);\n-    }\n-    return bucketList;\n-  }\n-\n-  private class CopyOnWriteBucketList extends ArrayList<String> {\n-\n-    CopyOnWriteBucketList(Collection<String> c) {\n-      super(c);\n-    }\n-\n-    private void secretAdd(String col) {\n-      super.add(col);\n-    }\n-\n-    @Override\n-    public boolean add(String t) {\n-      copyOnWrite();\n-      return shared.getBucketCols().add(t);\n-    }\n-\n-    @Override\n-    public boolean remove(Object o) {\n-      copyOnWrite();\n-      return shared.getBucketCols().remove(o);\n-    }\n-\n-    @Override\n-    public boolean addAll(Collection<? extends String> c) {\n-      copyOnWrite();\n-      return shared.getBucketCols().addAll(c);\n-    }\n-\n-    @Override\n-    public boolean addAll(int index, Collection<? extends String> c) {\n-      copyOnWrite();\n-      return shared.getBucketCols().addAll(c);\n-    }\n-\n-    @Override\n-    public boolean removeAll(Collection<?> c) {\n-      copyOnWrite();\n-      return shared.getBucketCols().removeAll(c);\n-    }\n-\n-    @Override\n-    public boolean retainAll(Collection<?> c) {\n-      copyOnWrite();\n-      return shared.getBucketCols().retainAll(c);\n-    }\n-\n-    @Override\n-    public void clear() {\n-      copyOnWrite();\n-      shared.getBucketCols().clear();\n-    }\n-\n-    @Override\n-    public String set(int index, String element) {\n-      copyOnWrite();\n-      return shared.getBucketCols().set(index, element);\n-    }\n-\n-    @Override\n-    public void add(int index, String element) {\n-      copyOnWrite();\n-      shared.getBucketCols().add(index, element);\n-    }\n-\n-    @Override\n-    public String remove(int index) {\n-      copyOnWrite();\n-      return shared.getBucketCols().remove(index);\n-    }\n-  }\n-\n }",
                "raw_url": "https://github.com/apache/hive/raw/10cfdd96b69084bf0da7b864113d073006147e77/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/SharedStorageDescriptor.java",
                "sha": "d772dca50db704313c70c53d42237bfe785430bd",
                "status": "modified"
            },
            {
                "additions": 74,
                "blob_url": "https://github.com/apache/hive/blob/10cfdd96b69084bf0da7b864113d073006147e77/metastore/src/test/org/apache/hadoop/hive/metastore/hbase/TestSharedStorageDescriptor.java",
                "changes": 122,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/test/org/apache/hadoop/hive/metastore/hbase/TestSharedStorageDescriptor.java?ref=10cfdd96b69084bf0da7b864113d073006147e77",
                "deletions": 48,
                "filename": "metastore/src/test/org/apache/hadoop/hive/metastore/hbase/TestSharedStorageDescriptor.java",
                "patch": "@@ -20,13 +20,16 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n import org.apache.hadoop.hive.metastore.api.Order;\n import org.apache.hadoop.hive.metastore.api.SerDeInfo;\n import org.apache.hadoop.hive.metastore.api.SkewedInfo;\n import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n import org.junit.Assert;\n import org.junit.Test;\n \n+import java.util.ArrayList;\n+import java.util.Iterator;\n import java.util.List;\n \n \n@@ -37,30 +40,6 @@\n   private static final Log LOG = LogFactory.getLog(TestHBaseStore.class.getName());\n \n \n-  @Test\n-  public void location() {\n-    StorageDescriptor sd = new StorageDescriptor();\n-    SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n-    ssd.setLocation(\"here\");\n-    ssd.setShared(sd);\n-    ssd.setLocation(\"there\");\n-    Assert.assertTrue(sd == ssd.getShared());\n-  }\n-\n-  @Test\n-  public void changeOnInputFormat() {\n-    StorageDescriptor sd = new StorageDescriptor();\n-    sd.setInputFormat(\"input\");\n-    SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n-    ssd.setShared(sd);\n-    Assert.assertEquals(\"input\", ssd.getInputFormat());\n-    ssd.setInputFormat(\"different\");\n-    Assert.assertFalse(sd == ssd.getShared());\n-    Assert.assertEquals(\"input\", sd.getInputFormat());\n-    Assert.assertEquals(\"different\", ssd.getInputFormat());\n-    Assert.assertEquals(\"input\", sd.getInputFormat());\n-  }\n-\n   @Test\n   public void changeOnSerde() {\n     StorageDescriptor sd = new StorageDescriptor();\n@@ -69,32 +48,33 @@ public void changeOnSerde() {\n     sd.setSerdeInfo(serde);\n     SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n     ssd.setShared(sd);\n-    Assert.assertEquals(\"serde\", ssd.getSerdeInfo().getName());\n     ssd.getSerdeInfo().setName(\"different\");\n-    Assert.assertFalse(sd == ssd.getShared());\n+    Assert.assertFalse(sd.getSerdeInfo() == ssd.getSerdeInfo());\n     Assert.assertEquals(\"serde\", serde.getName());\n     Assert.assertEquals(\"different\", ssd.getSerdeInfo().getName());\n     Assert.assertEquals(\"serde\", sd.getSerdeInfo().getName());\n   }\n \n   @Test\n-  public void multipleChangesDontCauseMultipleCopies() {\n+  public void changeOnSkewed() {\n+    StorageDescriptor sd = new StorageDescriptor();\n+    SkewedInfo skew = new SkewedInfo();\n+    sd.setSkewedInfo(skew);\n+    SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n+    ssd.setShared(sd);\n+    ssd.setSkewedInfo(new SkewedInfo());\n+    Assert.assertFalse(sd.getSkewedInfo() == ssd.getSkewedInfo());\n+  }\n+\n+  @Test\n+  public void changeOnUnset() {\n     StorageDescriptor sd = new StorageDescriptor();\n-    sd.setInputFormat(\"input\");\n-    sd.setOutputFormat(\"output\");\n+    SkewedInfo skew = new SkewedInfo();\n+    sd.setSkewedInfo(skew);\n     SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n     ssd.setShared(sd);\n-    Assert.assertEquals(\"input\", ssd.getInputFormat());\n-    ssd.setInputFormat(\"different\");\n-    Assert.assertFalse(sd == ssd.getShared());\n-    Assert.assertEquals(\"input\", sd.getInputFormat());\n-    Assert.assertEquals(\"different\", ssd.getInputFormat());\n-    StorageDescriptor keep = ssd.getShared();\n-    ssd.setOutputFormat(\"different_output\");\n-    Assert.assertEquals(\"different\", ssd.getInputFormat());\n-    Assert.assertEquals(\"different_output\", ssd.getOutputFormat());\n-    Assert.assertEquals(\"output\", sd.getOutputFormat());\n-    Assert.assertTrue(keep == ssd.getShared());\n+    ssd.unsetSkewedInfo();\n+    Assert.assertFalse(sd.getSkewedInfo() == ssd.getSkewedInfo());\n   }\n \n   @Test\n@@ -103,25 +83,71 @@ public void changeOrder() {\n     sd.addToSortCols(new Order(\"fred\", 1));\n     SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n     ssd.setShared(sd);\n-    Assert.assertEquals(1, ssd.getSortCols().get(0).getOrder());\n     ssd.getSortCols().get(0).setOrder(2);\n-    Assert.assertFalse(sd == ssd.getShared());\n+    Assert.assertFalse(sd.getSortCols() == ssd.getSortCols());\n     Assert.assertEquals(2, ssd.getSortCols().get(0).getOrder());\n     Assert.assertEquals(1, sd.getSortCols().get(0).getOrder());\n   }\n \n   @Test\n-  public void changeOrderList() {\n+  public void unsetOrder() {\n     StorageDescriptor sd = new StorageDescriptor();\n     sd.addToSortCols(new Order(\"fred\", 1));\n     SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n     ssd.setShared(sd);\n-    Assert.assertEquals(1, ssd.getSortCols().get(0).getOrder());\n-    List<Order> list = ssd.getSortCols();\n-    list.add(new Order(\"bob\", 2));\n-    Assert.assertFalse(sd == ssd.getShared());\n-    Assert.assertEquals(2, ssd.getSortColsSize());\n+    ssd.unsetSortCols();\n+    Assert.assertFalse(sd.getSortCols() == ssd.getSortCols());\n+    Assert.assertEquals(0, ssd.getSortColsSize());\n     Assert.assertEquals(1, sd.getSortColsSize());\n   }\n \n+  @Test\n+  public void changeBucketList() {\n+    StorageDescriptor sd = new StorageDescriptor();\n+    sd.addToBucketCols(new String(\"fred\"));\n+    SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n+    ssd.setShared(sd);\n+    List<String> list = ssd.getBucketCols();\n+    list.add(new String(\"bob\"));\n+    Assert.assertFalse(sd.getBucketCols() == ssd.getBucketCols());\n+    Assert.assertEquals(2, ssd.getBucketColsSize());\n+    Assert.assertEquals(1, sd.getBucketColsSize());\n+  }\n+\n+  @Test\n+  public void addToColList() {\n+    StorageDescriptor sd = new StorageDescriptor();\n+    sd.addToCols(new FieldSchema(\"fred\", \"string\", \"\"));\n+    SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n+    ssd.setShared(sd);\n+    ssd.addToCols(new FieldSchema(\"joe\", \"int\", \"\"));\n+    Assert.assertFalse(sd.getCols() == ssd.getCols());\n+    Assert.assertEquals(2, ssd.getColsSize());\n+    Assert.assertEquals(1, sd.getColsSize());\n+  }\n+\n+  @Test\n+  public void colIterator() {\n+    StorageDescriptor sd = new StorageDescriptor();\n+    sd.addToCols(new FieldSchema(\"fred\", \"string\", \"\"));\n+    SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n+    ssd.setShared(sd);\n+    Iterator<FieldSchema> iter = ssd.getColsIterator();\n+    Assert.assertTrue(iter.hasNext());\n+    Assert.assertEquals(\"fred\", iter.next().getName());\n+    Assert.assertFalse(sd.getCols() == ssd.getCols());\n+  }\n+\n+  @Test\n+  public void setReadOnly() {\n+    StorageDescriptor sd = new StorageDescriptor();\n+    sd.addToCols(new FieldSchema(\"fred\", \"string\", \"\"));\n+    SharedStorageDescriptor ssd = new SharedStorageDescriptor();\n+    ssd.setShared(sd);\n+    ssd.setReadOnly();\n+    List<FieldSchema> cols = ssd.getCols();\n+    Assert.assertEquals(1, cols.size());\n+    Assert.assertTrue(sd.getCols() == ssd.getCols());\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hive/raw/10cfdd96b69084bf0da7b864113d073006147e77/metastore/src/test/org/apache/hadoop/hive/metastore/hbase/TestSharedStorageDescriptor.java",
                "sha": "fdfb6d12cc6018cdf492bb2838d051419612c5ef",
                "status": "modified"
            }
        ],
        "message": "HIVE-10010 Alter table results in NPE [hbase-metastore branch] (Alan Gates)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/hbase-metastore@1668071 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/35ac8b0257614523064c1b3647a91e123b78e999",
        "patched_files": [
            "SharedStorageDescriptor.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestSharedStorageDescriptor.java",
            "TestHBaseMetastoreSql.java"
        ]
    },
    "hive_13938db": {
        "bug_id": "hive_13938db",
        "commit": "https://github.com/apache/hive/commit/13938db46ff89fc4a425854b4795df604ced8ba9",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/13938db46ff89fc4a425854b4795df604ced8ba9/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java?ref=13938db46ff89fc4a425854b4795df604ced8ba9",
                "deletions": 1,
                "filename": "itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java",
                "patch": "@@ -3044,8 +3044,13 @@ private void testInsertOverwrite(HiveStatement stmt) throws SQLException {\n   public void testGetQueryId() throws Exception {\n     HiveStatement stmt = (HiveStatement) con.createStatement();\n     HiveStatement stmt1 = (HiveStatement) con.createStatement();\n-    stmt.executeAsync(\"create database query_id_test with dbproperties ('repl.source.for' = '1, 2, 3')\");\n+\n+    // Returns null if no query is running.\n     String queryId = stmt.getQueryId();\n+    assertTrue(queryId == null);\n+\n+    stmt.executeAsync(\"create database query_id_test with dbproperties ('repl.source.for' = '1, 2, 3')\");\n+    queryId = stmt.getQueryId();\n     assertFalse(queryId.isEmpty());\n     stmt.getUpdateCount();\n ",
                "raw_url": "https://github.com/apache/hive/raw/13938db46ff89fc4a425854b4795df604ced8ba9/itests/hive-unit/src/test/java/org/apache/hive/jdbc/TestJdbcDriver2.java",
                "sha": "14187cc83bfd6f90496477de828dce773de605ac",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hive/blob/13938db46ff89fc4a425854b4795df604ced8ba9/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java?ref=13938db46ff89fc4a425854b4795df604ced8ba9",
                "deletions": 3,
                "filename": "jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java",
                "patch": "@@ -18,8 +18,8 @@\n \n package org.apache.hive.jdbc;\n \n-import com.google.common.annotations.VisibleForTesting;\n import org.apache.commons.codec.binary.Base64;\n+import org.apache.hadoop.hive.common.classification.InterfaceAudience.LimitedPrivate;\n import org.apache.hive.jdbc.logs.InPlaceUpdateStream;\n import org.apache.hive.service.cli.RowSet;\n import org.apache.hive.service.cli.RowSetFactory;\n@@ -37,7 +37,6 @@\n import org.apache.hive.service.rpc.thrift.TGetOperationStatusResp;\n import org.apache.hive.service.rpc.thrift.TGetQueryIdReq;\n import org.apache.hive.service.rpc.thrift.TOperationHandle;\n-import org.apache.hive.service.rpc.thrift.TOperationState;\n import org.apache.hive.service.rpc.thrift.TSessionHandle;\n import org.apache.thrift.TException;\n import org.slf4j.Logger;\n@@ -1007,12 +1006,26 @@ public void setInPlaceUpdateStream(InPlaceUpdateStream stream) {\n     this.inPlaceUpdateStream = stream;\n   }\n \n-  @VisibleForTesting\n+  /**\n+   * Returns the Query ID if it is running.\n+   * This method is a public API for usage outside of Hive, although it is not part of the\n+   * interface java.sql.Statement.\n+   * @return Valid query ID if it is running else returns NULL.\n+   * @throws SQLException If any internal failures.\n+   */\n+  @LimitedPrivate(value={\"Hive and closely related projects.\"})\n   public String getQueryId() throws SQLException {\n+    if (stmtHandle == null) {\n+      // If query is not running or already closed.\n+      return null;\n+    }\n     try {\n       return client.GetQueryId(new TGetQueryIdReq(stmtHandle)).getQueryId();\n     } catch (TException e) {\n       throw new SQLException(e);\n+    } catch (Exception e) {\n+      // If concurrently the query is closed before we fetch queryID.\n+      return null;\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/hive/raw/13938db46ff89fc4a425854b4795df604ced8ba9/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java",
                "sha": "d9b625466ca8880c739977f52eabc92ce44aa98d",
                "status": "modified"
            }
        ],
        "message": "HIVE-21421: HiveStatement.getQueryId throws NPE when query is not running (Sankar Hariappan, reviewed by Mahesh Kumar Behera)\n\nSigned-off-by: Sankar Hariappan <sankarh@apache.org>",
        "parent": "https://github.com/apache/hive/commit/9f2f101feec4283f020b72b918e0961215e76789",
        "patched_files": [
            "HiveStatement.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestJdbcDriver2.java",
            "TestHiveStatement.java"
        ]
    },
    "hive_1912d19": {
        "bug_id": "hive_1912d19",
        "commit": "https://github.com/apache/hive/commit/1912d19f2f4f8e3d32a088623563d022275a3ab5",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/1912d19f2f4f8e3d32a088623563d022275a3ab5/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/HCatRecordSerDe.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/HCatRecordSerDe.java?ref=1912d19f2f4f8e3d32a088623563d022275a3ab5",
                "deletions": 2,
                "filename": "hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/HCatRecordSerDe.java",
                "patch": "@@ -29,6 +29,7 @@\n import org.apache.hadoop.hive.serde.serdeConstants;\n import org.apache.hadoop.hive.serde2.SerDe;\n import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeSpec;\n import org.apache.hadoop.hive.serde2.SerDeStats;\n import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;\n@@ -51,6 +52,10 @@\n /**\n  * SerDe class for serializing to and from HCatRecord\n  */\n+\n+@SerDeSpec(schemaProps = {serdeConstants.LIST_COLUMNS,\n+                          serdeConstants.LIST_COLUMN_TYPES})\n+\n public class HCatRecordSerDe implements SerDe {\n \n   private static final Logger LOG = LoggerFactory.getLogger(HCatRecordSerDe.class);\n@@ -124,7 +129,7 @@ public Object deserialize(Writable data) throws SerDeException {\n       throw new SerDeException(getClass().getName() + \": expects HCatRecord!\");\n     }\n \n-    return (HCatRecord) data;\n+    return data;\n   }\n \n   /**\n@@ -302,7 +307,7 @@ private static Object serializePrimitiveField(Object field,\n    */\n   @Override\n   public ObjectInspector getObjectInspector() throws SerDeException {\n-    return (ObjectInspector) cachedObjectInspector;\n+    return cachedObjectInspector;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hive/raw/1912d19f2f4f8e3d32a088623563d022275a3ab5/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/HCatRecordSerDe.java",
                "sha": "81c79438feae7794f3d197477048e1136b78e389",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/1912d19f2f4f8e3d32a088623563d022275a3ab5/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java?ref=1912d19f2f4f8e3d32a088623563d022275a3ab5",
                "deletions": 1,
                "filename": "hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java",
                "patch": "@@ -40,6 +40,7 @@\n import org.apache.hadoop.hive.serde.serdeConstants;\n import org.apache.hadoop.hive.serde2.SerDe;\n import org.apache.hadoop.hive.serde2.SerDeException;\n+import org.apache.hadoop.hive.serde2.SerDeSpec;\n import org.apache.hadoop.hive.serde2.SerDeStats;\n import org.apache.hadoop.hive.serde2.SerDeUtils;\n import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;\n@@ -84,6 +85,10 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+@SerDeSpec(schemaProps = {serdeConstants.LIST_COLUMNS,\n+                          serdeConstants.LIST_COLUMN_TYPES,\n+                          serdeConstants.TIMESTAMP_FORMATS})\n+\n public class JsonSerDe implements SerDe {\n \n   private static final Logger LOG = LoggerFactory.getLogger(JsonSerDe.class);\n@@ -497,7 +502,7 @@ private static void buildJSONString(StringBuilder sb, Object o, ObjectInspector\n           break;\n         }\n         case STRING: {\n-          String s = \n+          String s =\n                   SerDeUtils.escapeString(((StringObjectInspector) poi).getPrimitiveJavaObject(o));\n           appendWithQuotes(sb, s);\n           break;",
                "raw_url": "https://github.com/apache/hive/raw/1912d19f2f4f8e3d32a088623563d022275a3ab5/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/JsonSerDe.java",
                "sha": "9b325b66607ec0b08faba023aeacd5ff758b3612",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/1912d19f2f4f8e3d32a088623563d022275a3ab5/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java?ref=1912d19f2f4f8e3d32a088623563d022275a3ab5",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java",
                "patch": "@@ -604,7 +604,13 @@ private boolean needConversion(PartitionDesc partitionDesc) {\n   // if table and all partitions have the same schema and serde, no need to convert\n   private boolean needConversion(TableDesc tableDesc, List<PartitionDesc> partDescs) {\n     Class<?> tableSerDe = tableDesc.getDeserializerClass();\n-    String[] schemaProps = AnnotationUtils.getAnnotation(tableSerDe, SerDeSpec.class).schemaProps();\n+    SerDeSpec spec = AnnotationUtils.getAnnotation(tableSerDe, SerDeSpec.class);\n+    if (null == spec) {\n+      // Serde may not have this optional annotation defined in which case be conservative\n+      // and say conversion is needed.\n+      return true;\n+    }\n+    String[] schemaProps = spec.schemaProps();\n     Properties tableProps = tableDesc.getProperties();\n     for (PartitionDesc partitionDesc : partDescs) {\n       if (!tableSerDe.getName().equals(partitionDesc.getDeserializerClassName())) {",
                "raw_url": "https://github.com/apache/hive/raw/1912d19f2f4f8e3d32a088623563d022275a3ab5/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchOperator.java",
                "sha": "258d28ec79b034c80481b177c9465c45025db0b5",
                "status": "modified"
            }
        ],
        "message": "HIVE-10437 : NullPointerException on queries where map/reduce is not involved on tables with partitions (Ashutosh Chauhan via Gunther Hagleitner)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>",
        "parent": "https://github.com/apache/hive/commit/0cad50a193ba777f9271808f057caae674738817",
        "patched_files": [
            "HCatRecordSerDe.java",
            "JsonSerDe.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHCatRecordSerDe.java",
            "TestJsonSerDe.java"
        ]
    },
    "hive_1a3e4be": {
        "bug_id": "hive_1a3e4be",
        "commit": "https://github.com/apache/hive/commit/1a3e4be3dbd485f2630c7249254727ce58374d1c",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/1a3e4be3dbd485f2630c7249254727ce58374d1c/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java?ref=1a3e4be3dbd485f2630c7249254727ce58374d1c",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java",
                "patch": "@@ -1864,7 +1864,7 @@ public Void call() throws Exception {\n         for (Partition p : partitionsMap.values()) {\n           partNames.add(p.getName());\n         }\n-        metaStoreClient.addDynamicPartitions(txnId, tbl.getDbName(), tbl.getTableName(),\n+        getMSC().addDynamicPartitions(txnId, tbl.getDbName(), tbl.getTableName(),\n           partNames, AcidUtils.toDataOperationType(operation));\n       }\n       LOG.info(\"Loaded \" + partitionsMap.size() + \" partitions\");",
                "raw_url": "https://github.com/apache/hive/raw/1a3e4be3dbd485f2630c7249254727ce58374d1c/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java",
                "sha": "de6adb5047bf6457d031050b6e72d54a6534ccfa",
                "status": "modified"
            }
        ],
        "message": "HIVE-14814: metastoreClient is used directly in Hive cause NPE (Prasanth Jayachandran reviewed by Eugene Koifman)",
        "parent": "https://github.com/apache/hive/commit/c9224d58cce6e0b0520598894e962c48ce9d97e3",
        "patched_files": [
            "Hive.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHive.java"
        ]
    },
    "hive_1ae7c0e": {
        "bug_id": "hive_1ae7c0e",
        "commit": "https://github.com/apache/hive/commit/1ae7c0e9045057a48c9debc829e4a0afa9aefd1a",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/1ae7c0e9045057a48c9debc829e4a0afa9aefd1a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/InStream.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/InStream.java?ref=1ae7c0e9045057a48c9debc829e4a0afa9aefd1a",
                "deletions": 2,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/InStream.java",
                "patch": "@@ -697,9 +697,10 @@ public static DiskRangeList uncompressStream(long fileId, long baseOffset, DiskR\n           toDecompress = new ArrayList<ProcCacheChunk>();\n           toRelease = (zcr == null) ? null : new ArrayList<ByteBuffer>();\n         }\n-        lastCached = addOneCompressionBuffer(bc, zcr, bufferSize,\n+        ProcCacheChunk newCached = addOneCompressionBuffer(bc, zcr, bufferSize,\n             cache, streamBuffer.cacheBuffers, toDecompress, toRelease);\n-        next = (lastCached != null) ? lastCached.next : null;\n+        lastCached = (newCached == null) ? lastCached : newCached;\n+        next = (newCached != null) ? newCached.next : null;\n         currentCOffset = (next != null) ? next.getOffset() : -1;\n       }\n ",
                "raw_url": "https://github.com/apache/hive/raw/1ae7c0e9045057a48c9debc829e4a0afa9aefd1a/ql/src/java/org/apache/hadoop/hive/ql/io/orc/InStream.java",
                "sha": "db65259a718219530558ac9b0ff362ca7edf8012",
                "status": "modified"
            }
        ],
        "message": "HIVE-10333 : LLAP: NPE due to failure to find position (Sergey Shelukhin)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/llap@1673608 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/d533b3935dc973a6e14d444ae1492f043a5353c0",
        "patched_files": [
            "InStream.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestInStream.java"
        ]
    },
    "hive_1c33fea": {
        "bug_id": "hive_1c33fea",
        "commit": "https://github.com/apache/hive/commit/1c33fea890bc01a85eb336caf5d73a85652f91a3",
        "file": [
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 0,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java",
                "patch": "@@ -91,6 +91,7 @@\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertNull;\n import static org.apache.hadoop.hive.metastore.ReplChangeManager.SOURCE_OF_REPLICATION;\n+import org.junit.Assert;\n \n public class TestReplicationScenarios {\n \n@@ -3185,6 +3186,47 @@ public void testLoadCmPathMissing() throws IOException {\n     fs.create(path, false);\n   }\n \n+  @Test\n+  public void testDumpWithTableDirMissing() throws IOException {\n+    String dbName = createDB(testName.getMethodName(), driver);\n+    run(\"CREATE TABLE \" + dbName + \".normal(a int)\", driver);\n+    run(\"INSERT INTO \" + dbName + \".normal values (1)\", driver);\n+\n+    Path path = new Path(System.getProperty(\"test.warehouse.dir\", \"\"));\n+    path = new Path(path, dbName.toLowerCase() + \".db\");\n+    path = new Path(path, \"normal\");\n+    FileSystem fs = path.getFileSystem(hconf);\n+    fs.delete(path);\n+\n+    advanceDumpDir();\n+    CommandProcessorResponse ret = driver.run(\"REPL DUMP \" + dbName);\n+    Assert.assertEquals(ret.getResponseCode(), ErrorMsg.FILE_NOT_FOUND.getErrorCode());\n+\n+    run(\"DROP TABLE \" + dbName + \".normal\", driver);\n+    run(\"drop database \" + dbName, true, driver);\n+  }\n+\n+  @Test\n+  public void testDumpWithPartitionDirMissing() throws IOException {\n+    String dbName = createDB(testName.getMethodName(), driver);\n+    run(\"CREATE TABLE \" + dbName + \".normal(a int) PARTITIONED BY (part int)\", driver);\n+    run(\"INSERT INTO \" + dbName + \".normal partition (part= 124) values (1)\", driver);\n+\n+    Path path = new Path(System.getProperty(\"test.warehouse.dir\",\"\"));\n+    path = new Path(path, dbName.toLowerCase()+\".db\");\n+    path = new Path(path, \"normal\");\n+    path = new Path(path, \"part=124\");\n+    FileSystem fs = path.getFileSystem(hconf);\n+    fs.delete(path);\n+\n+    advanceDumpDir();\n+    CommandProcessorResponse ret = driver.run(\"REPL DUMP \" + dbName);\n+    Assert.assertEquals(ret.getResponseCode(), ErrorMsg.FILE_NOT_FOUND.getErrorCode());\n+\n+    run(\"DROP TABLE \" + dbName + \".normal\", driver);\n+    run(\"drop database \" + dbName, true, driver);\n+  }\n+\n   @Test\n   public void testDumpNonReplDatabase() throws IOException {\n     String dbName = createDBNonRepl(testName.getMethodName(), driver);",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java",
                "sha": "46c623d34bb8de2f0ded145b77e52dc34f3e7d49",
                "status": "modified"
            },
            {
                "additions": 51,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 1,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java",
                "patch": "@@ -32,6 +32,10 @@\n import org.apache.hadoop.hive.metastore.InjectableBehaviourObjectStore.CallerArguments;\n import org.apache.hadoop.hive.metastore.InjectableBehaviourObjectStore.BehaviourInjection;\n import static org.apache.hadoop.hive.metastore.ReplChangeManager.SOURCE_OF_REPLICATION;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.ql.ErrorMsg;\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;\n import org.junit.rules.TestName;\n import org.junit.rules.TestRule;\n import org.slf4j.Logger;\n@@ -63,10 +67,11 @@\n   protected static final Logger LOG = LoggerFactory.getLogger(TestReplicationScenarios.class);\n   private static WarehouseInstance primary, replica, replicaNonAcid;\n   private String primaryDbName, replicatedDbName;\n+  private static HiveConf conf;\n \n   @BeforeClass\n   public static void classLevelSetup() throws Exception {\n-    Configuration conf = new Configuration();\n+    conf = new HiveConf(TestReplicationScenariosAcidTables.class);\n     conf.set(\"dfs.client.use.datanode.hostname\", \"true\");\n     conf.set(\"hadoop.proxyuser.\" + Utils.getUGI().getShortUserName() + \".hosts\", \"*\");\n     MiniDFSCluster miniDFSCluster =\n@@ -432,4 +437,49 @@ public Boolean apply(@Nullable CallerArguments args) {\n             .run(\"select name from t2 order by name\")\n             .verifyResults(Arrays.asList(\"bob\", \"carl\"));\n   }\n+\n+  @Test\n+  public void testDumpAcidTableWithPartitionDirMissing() throws Throwable {\n+    String dbName = testName.getMethodName();\n+    primary.run(\"CREATE DATABASE \" + dbName + \" WITH DBPROPERTIES ( '\" +\n+            SOURCE_OF_REPLICATION + \"' = '1,2,3')\")\n+    .run(\"CREATE TABLE \" + dbName + \".normal (a int) PARTITIONED BY (part int)\" +\n+            \" STORED AS ORC TBLPROPERTIES ('transactional'='true')\")\n+    .run(\"INSERT INTO \" + dbName + \".normal partition (part= 124) values (1)\");\n+\n+    Path path = new Path(primary.warehouseRoot, dbName.toLowerCase()+\".db\");\n+    path = new Path(path, \"normal\");\n+    path = new Path(path, \"part=124\");\n+    FileSystem fs = path.getFileSystem(conf);\n+    fs.delete(path);\n+\n+    CommandProcessorResponse ret = primary.runCommand(\"REPL DUMP \" + dbName +\n+            \" with ('hive.repl.dump.include.acid.tables' = 'true')\");\n+    Assert.assertEquals(ret.getResponseCode(), ErrorMsg.FILE_NOT_FOUND.getErrorCode());\n+\n+    primary.run(\"DROP TABLE \" + dbName + \".normal\");\n+    primary.run(\"drop database \" + dbName);\n+  }\n+\n+  @Test\n+  public void testDumpAcidTableWithTableDirMissing() throws Throwable {\n+    String dbName = testName.getMethodName();\n+    primary.run(\"CREATE DATABASE \" + dbName + \" WITH DBPROPERTIES ( '\" +\n+            SOURCE_OF_REPLICATION + \"' = '1,2,3')\")\n+            .run(\"CREATE TABLE \" + dbName + \".normal (a int) \" +\n+                    \" STORED AS ORC TBLPROPERTIES ('transactional'='true')\")\n+            .run(\"INSERT INTO \" + dbName + \".normal values (1)\");\n+\n+    Path path = new Path(primary.warehouseRoot, dbName.toLowerCase()+\".db\");\n+    path = new Path(path, \"normal\");\n+    FileSystem fs = path.getFileSystem(conf);\n+    fs.delete(path);\n+\n+    CommandProcessorResponse ret = primary.runCommand(\"REPL DUMP \" + dbName +\n+            \" with ('hive.repl.dump.include.acid.tables' = 'true')\");\n+    Assert.assertEquals(ret.getResponseCode(), ErrorMsg.FILE_NOT_FOUND.getErrorCode());\n+\n+    primary.run(\"DROP TABLE \" + dbName + \".normal\");\n+    primary.run(\"drop database \" + dbName);\n+  }\n }",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcidTables.java",
                "sha": "86c040532f0c07a6e79dd2418496a11545beb9f9",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 36,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java",
                "patch": "@@ -68,6 +68,9 @@\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n import static org.apache.hadoop.hive.metastore.ReplChangeManager.SOURCE_OF_REPLICATION;\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;\n+import org.apache.hadoop.hive.ql.ErrorMsg;\n+import org.junit.Assert;\n \n public class TestReplicationScenariosAcrossInstances {\n   @Rule\n@@ -878,41 +881,6 @@ private void verifyIfSrcOfReplPropMissing(Map<String, String> props) {\n     assertFalse(props.containsKey(SOURCE_OF_REPLICATION));\n   }\n \n-  @Test\n-  public void testIfCkptSetForObjectsByBootstrapReplLoad() throws Throwable {\n-    WarehouseInstance.Tuple tuple = primary\n-            .run(\"use \" + primaryDbName)\n-            .run(\"create table t1 (id int)\")\n-            .run(\"insert into table t1 values (10)\")\n-            .run(\"create table t2 (place string) partitioned by (country string)\")\n-            .run(\"insert into table t2 partition(country='india') values ('bangalore')\")\n-            .run(\"insert into table t2 partition(country='uk') values ('london')\")\n-            .run(\"insert into table t2 partition(country='us') values ('sfo')\")\n-            .dump(primaryDbName, null);\n-\n-    replica.load(replicatedDbName, tuple.dumpLocation)\n-            .run(\"use \" + replicatedDbName)\n-            .run(\"repl status \" + replicatedDbName)\n-            .verifyResult(tuple.lastReplicationId)\n-            .run(\"show tables\")\n-            .verifyResults(new String[] { \"t1\", \"t2\" })\n-            .run(\"select country from t2\")\n-            .verifyResults(Arrays.asList(\"india\", \"uk\", \"us\"));\n-\n-    Database db = replica.getDatabase(replicatedDbName);\n-    verifyIfCkptSet(db.getParameters(), tuple.dumpLocation);\n-    Table t1 = replica.getTable(replicatedDbName, \"t1\");\n-    verifyIfCkptSet(t1.getParameters(), tuple.dumpLocation);\n-    Table t2 = replica.getTable(replicatedDbName, \"t2\");\n-    verifyIfCkptSet(t2.getParameters(), tuple.dumpLocation);\n-    Partition india = replica.getPartition(replicatedDbName, \"t2\", Collections.singletonList(\"india\"));\n-    verifyIfCkptSet(india.getParameters(), tuple.dumpLocation);\n-    Partition us = replica.getPartition(replicatedDbName, \"t2\", Collections.singletonList(\"us\"));\n-    verifyIfCkptSet(us.getParameters(), tuple.dumpLocation);\n-    Partition uk = replica.getPartition(replicatedDbName, \"t2\", Collections.singletonList(\"uk\"));\n-    verifyIfCkptSet(uk.getParameters(), tuple.dumpLocation);\n-  }\n-\n   @Test\n   public void testIncrementalDumpMultiIteration() throws Throwable {\n     WarehouseInstance.Tuple bootstrapTuple = primary.dump(primaryDbName, null);\n@@ -1182,7 +1150,9 @@ public Boolean apply(@Nullable CallerArguments args) {\n     assertEquals(0, replica.getForeignKeyList(replicatedDbName, \"t2\").size());\n \n     // Retry with different dump should fail.\n-    replica.loadFailure(replicatedDbName, tuple2.dumpLocation);\n+    CommandProcessorResponse ret = replica.runCommand(\"REPL LOAD \" + replicatedDbName +\n+            \" FROM '\" + tuple2.dumpLocation + \"'\");\n+    Assert.assertEquals(ret.getResponseCode(), ErrorMsg.REPL_BOOTSTRAP_LOAD_PATH_NOT_VALID.getErrorCode());\n \n     // Verify if create table is not called on table t1 but called for t2 and t3.\n     // Also, allow constraint creation only on t1 and t3. Foreign key creation on t2 fails.",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java",
                "sha": "08f013031fe9c1e34da14649bb92a65fae562ee0",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 1,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java",
                "patch": "@@ -77,6 +77,7 @@\n   HiveConf hiveConf;\n   MiniDFSCluster miniDFSCluster;\n   private HiveMetaStoreClient client;\n+  public final Path warehouseRoot;\n \n   private static int uniqueIdentifier = 0;\n \n@@ -90,7 +91,7 @@\n     assert miniDFSCluster.isDataNodeUp();\n     DistributedFileSystem fs = miniDFSCluster.getFileSystem();\n \n-    Path warehouseRoot = mkDir(fs, \"/warehouse\" + uniqueIdentifier);\n+    warehouseRoot = mkDir(fs, \"/warehouse\" + uniqueIdentifier);\n     if (StringUtils.isNotEmpty(keyNameForEncryptedZone)) {\n       fs.createEncryptionZone(warehouseRoot, keyNameForEncryptedZone);\n     }\n@@ -199,6 +200,10 @@ public WarehouseInstance run(String command) throws Throwable {\n     return this;\n   }\n \n+  public CommandProcessorResponse runCommand(String command) throws Throwable {\n+    return driver.run(command);\n+  }\n+\n   WarehouseInstance runFailure(String command) throws Throwable {\n     CommandProcessorResponse ret = driver.run(command);\n     if (ret.getException() == null) {",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java",
                "sha": "f666df11415dc238e2cd9f7c190fd16ae7132929",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java",
                "patch": "@@ -502,7 +502,8 @@\n   //if the error message is changed for REPL_EVENTS_MISSING_IN_METASTORE, then need modification in getNextNotification\n   //method in HiveMetaStoreClient\n   REPL_EVENTS_MISSING_IN_METASTORE(20016, \"Notification events are missing in the meta store.\"),\n-  REPL_BOOTSTRAP_LOAD_PATH_NOT_VALID(20017, \"Target database is bootstrapped from some other path.\"),\n+  REPL_BOOTSTRAP_LOAD_PATH_NOT_VALID(20017, \"Load path {0} not valid as target database is bootstrapped \" +\n+          \"from some other path : {1}.\"),\n   REPL_FILE_MISSING_FROM_SRC_AND_CM_PATH(20018, \"File is missing from both source and cm path.\"),\n   REPL_LOAD_PATH_NOT_FOUND(20019, \"Load path does not exist.\"),\n   REPL_DATABASE_IS_NOT_SOURCE_OF_REPLICATION(20020,",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java",
                "sha": "b2c9daa436039a0bd2ea57a3cb25c7d09b9c52b6",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java",
                "patch": "@@ -121,6 +121,10 @@ protected int execute(DriverContext driverContext) {\n         lastReplId = incrementalDump(dumpRoot, dmd, cmRoot);\n       }\n       prepareReturnValues(Arrays.asList(dumpRoot.toUri().toString(), String.valueOf(lastReplId)), dumpSchema);\n+    } catch (RuntimeException e) {\n+      LOG.error(\"failed\", e);\n+      setException(e);\n+      return ErrorMsg.getErrorMsg(e.getMessage()).getErrorCode();\n     } catch (Exception e) {\n       LOG.error(\"failed\", e);\n       setException(e);",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplDumpTask.java",
                "sha": "e48657c35d87a52f4269eae15f7146c333c0e125",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/LoadDatabase.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/LoadDatabase.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/LoadDatabase.java",
                "patch": "@@ -79,7 +79,7 @@ public TaskTracker tasks() throws SemanticException {\n       }\n       return tracker;\n     } catch (Exception e) {\n-      throw new SemanticException(e);\n+      throw new SemanticException(e.getMessage(), e);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/bootstrap/load/LoadDatabase.java",
                "sha": "0fd305a0f9abb5f19a8bf226fdcfcb056b1286d2",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/ReplUtils.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/ReplUtils.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 3,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/ReplUtils.java",
                "patch": "@@ -19,6 +19,7 @@\n \n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.api.InvalidOperationException;\n+import org.apache.hadoop.hive.ql.ErrorMsg;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.TaskFactory;\n import org.apache.hadoop.hive.ql.exec.repl.ReplStateLogWork;\n@@ -115,9 +116,8 @@ public static boolean replCkptStatus(String dbName, Map<String, String> props, S\n       if (props.get(REPL_CHECKPOINT_KEY).equals(dumpRoot)) {\n         return true;\n       }\n-      throw new InvalidOperationException(\"REPL LOAD with Dump: \" + dumpRoot\n-              + \" is not allowed as the target DB: \" + dbName\n-              + \" is already bootstrap loaded by another Dump \" + props.get(REPL_CHECKPOINT_KEY));\n+      throw new InvalidOperationException(ErrorMsg.REPL_BOOTSTRAP_LOAD_PATH_NOT_VALID.format(dumpRoot,\n+              props.get(REPL_CHECKPOINT_KEY)));\n     }\n     return false;\n   }",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/util/ReplUtils.java",
                "sha": "b1f731ff96da06e7ed677d7685b40bc3cae9511e",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/PartitionExport.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/PartitionExport.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 6,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/PartitionExport.java",
                "patch": "@@ -20,6 +20,7 @@\n import com.google.common.util.concurrent.ThreadFactoryBuilder;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hadoop.hive.ql.metadata.Partition;\n import org.apache.hadoop.hive.ql.metadata.PartitionIterable;\n import org.apache.hadoop.hive.ql.parse.ReplicationSpec;\n@@ -29,13 +30,15 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.util.LinkedList;\n import java.util.List;\n import java.util.concurrent.ArrayBlockingQueue;\n import java.util.concurrent.BlockingQueue;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n import java.util.concurrent.ThreadFactory;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.Future;\n \n import static org.apache.hadoop.hive.ql.parse.repl.dump.TableExport.Paths;\n \n@@ -70,10 +73,11 @@\n     this.callersSession = SessionState.get();\n   }\n \n-  void write(final ReplicationSpec forReplicationSpec) throws InterruptedException {\n+  void write(final ReplicationSpec forReplicationSpec) throws InterruptedException, HiveException {\n+    List<Future<?>> futures = new LinkedList<>();\n     ExecutorService producer = Executors.newFixedThreadPool(1,\n         new ThreadFactoryBuilder().setNameFormat(\"partition-submitter-thread-%d\").build());\n-    producer.submit(() -> {\n+    futures.add(producer.submit(() -> {\n       SessionState.setCurrentSessionState(callersSession);\n       for (Partition partition : partitionIterable) {\n         try {\n@@ -83,7 +87,7 @@ void write(final ReplicationSpec forReplicationSpec) throws InterruptedException\n               \"Error while queuing up the partitions for export of data files\", e);\n         }\n       }\n-    });\n+    }));\n     producer.shutdown();\n \n     ThreadFactory namingThreadFactory =\n@@ -102,7 +106,7 @@ void write(final ReplicationSpec forReplicationSpec) throws InterruptedException\n         continue;\n       }\n       LOG.debug(\"scheduling partition dump {}\", partition.getName());\n-      consumer.submit(() -> {\n+      futures.add(consumer.submit(() -> {\n         String partitionName = partition.getName();\n         String threadName = Thread.currentThread().getName();\n         LOG.debug(\"Thread: {}, start partition dump {}\", threadName, partitionName);\n@@ -115,11 +119,19 @@ void write(final ReplicationSpec forReplicationSpec) throws InterruptedException\n                   .export(forReplicationSpec);\n           LOG.debug(\"Thread: {}, finish partition dump {}\", threadName, partitionName);\n         } catch (Exception e) {\n-          throw new RuntimeException(\"Error while export of data files\", e);\n+          throw new RuntimeException(e.getMessage(), e);\n         }\n-      });\n+      }));\n     }\n     consumer.shutdown();\n+    for (Future<?> future : futures) {\n+      try {\n+        future.get();\n+      } catch (Exception e) {\n+        LOG.error(\"failed\", e.getCause());\n+        throw new HiveException(e.getCause().getMessage(), e.getCause());\n+      }\n+    }\n     // may be drive this via configuration as well.\n     consumer.awaitTermination(Long.MAX_VALUE, TimeUnit.SECONDS);\n   }",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/PartitionExport.java",
                "sha": "9e2479938278a01abe39e44fc5512e6f1e56525b",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java",
                "patch": "@@ -22,6 +22,7 @@\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.api.Database;\n import org.apache.hadoop.hive.metastore.api.NotificationEvent;\n+import org.apache.hadoop.hive.ql.ErrorMsg;\n import org.apache.hadoop.hive.ql.exec.Utilities;\n import org.apache.hadoop.hive.ql.io.AcidUtils;\n import org.apache.hadoop.hive.ql.metadata.Hive;\n@@ -37,6 +38,7 @@\n import org.slf4j.LoggerFactory;\n \n import java.io.DataOutputStream;\n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.util.Collection;\n import java.util.Collections;\n@@ -204,7 +206,11 @@ public static boolean shouldReplicate(NotificationEvent tableForEvent,\n   static List<Path> getDataPathList(Path fromPath, ReplicationSpec replicationSpec, HiveConf conf)\n           throws IOException {\n     if (replicationSpec.isTransactionalTableDump()) {\n-      return AcidUtils.getValidDataPaths(fromPath, conf, replicationSpec.getValidWriteIdList());\n+      try {\n+        return AcidUtils.getValidDataPaths(fromPath, conf, replicationSpec.getValidWriteIdList());\n+      } catch (FileNotFoundException e) {\n+        throw new IOException(ErrorMsg.FILE_NOT_FOUND.format(e.getMessage()), e);\n+      }\n     } else {\n       return Collections.singletonList(fromPath);\n     }",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/Utils.java",
                "sha": "976104c210d0ef6bf633817a8d53d04a173ad001",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FileOperations.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FileOperations.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FileOperations.java",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.hive.ql.parse.repl.dump.io;\n \n import java.io.BufferedWriter;\n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.io.OutputStreamWriter;\n import java.util.ArrayList;\n@@ -46,6 +47,8 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import static org.apache.hadoop.hive.ql.ErrorMsg.FILE_NOT_FOUND;\n+\n //TODO: this object is created once to call one method and then immediately destroyed.\n //So it's basically just a roundabout way to pass arguments to a static method. Simplify?\n public class FileOperations {\n@@ -156,6 +159,10 @@ private void exportFilesAsList() throws SemanticException, IOException, LoginExc\n         }\n         done = true;\n       } catch (IOException e) {\n+        if (e instanceof FileNotFoundException) {\n+          logger.error(\"exporting data files in dir : \" + dataPathList + \" to \" + exportRootDataDir + \" failed\");\n+          throw new FileNotFoundException(FILE_NOT_FOUND.format(e.getMessage()));\n+        }\n         repeat++;\n         logger.info(\"writeFilesList failed\", e);\n         if (repeat >= FileUtils.MAX_IO_ERROR_RETRY) {",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FileOperations.java",
                "sha": "e8eaae6961e7071e18e559291024ca369bfe5eb5",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/1c33fea890bc01a85eb336caf5d73a85652f91a3/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java?ref=1c33fea890bc01a85eb336caf5d73a85652f91a3",
                "deletions": 5,
                "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java",
                "patch": "@@ -125,13 +125,10 @@ public void alterTable(RawStore msdb, Warehouse wh, String catName, String dbnam\n \n     Table oldt = null;\n \n-    List<TransactionalMetaStoreEventListener> transactionalListeners = null;\n-    List<MetaStoreEventListener> listeners = null;\n+    List<TransactionalMetaStoreEventListener> transactionalListeners = handler.getTransactionalListeners();\n+    List<MetaStoreEventListener> listeners = handler.getListeners();\n     Map<String, String> txnAlterTableEventResponses = Collections.emptyMap();\n \n-    transactionalListeners = handler.getTransactionalListeners();\n-    listeners = handler.getListeners();\n-\n     try {\n       boolean rename = false;\n       List<Partition> parts;",
                "raw_url": "https://github.com/apache/hive/raw/1c33fea890bc01a85eb336caf5d73a85652f91a3/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java",
                "sha": "93ac74c68b5e14687852a76fdb298557bf79a08a",
                "status": "modified"
            }
        ],
        "message": "HIVE-19970: Replication dump has a NPE when table is empty (Mahesh Kumar Behera, reviewed by Peter Vary, Sankar Hariappan)",
        "parent": "https://github.com/apache/hive/commit/80c3bb58e24f13f82ca698486645c6c72364d75d",
        "patched_files": [
            "Utils.java",
            "FileOperations.java",
            "WarehouseInstance.java",
            "ErrorMsg.java",
            "PartitionExport.java",
            "ReplUtils.java",
            "LoadDatabase.java",
            "ReplDumpTask.java",
            "HiveAlterHandler.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveAlterHandler.java",
            "TestReplicationScenarios.java",
            "TestReplicationScenariosAcidTables.java",
            "TestReplicationScenariosAcrossInstances.java",
            "TestErrorMsg.java"
        ]
    },
    "hive_1d159ff": {
        "bug_id": "hive_1d159ff",
        "commit": "https://github.com/apache/hive/commit/1d159ffd3c5016b78ca2814b837c02ab3f4be1de",
        "file": [
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hive/blob/1d159ffd3c5016b78ca2814b837c02ab3f4be1de/druid-handler/src/java/org/apache/hadoop/hive/druid/io/HiveDruidSplit.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/druid-handler/src/java/org/apache/hadoop/hive/druid/io/HiveDruidSplit.java?ref=1d159ffd3c5016b78ca2814b837c02ab3f4be1de",
                "deletions": 2,
                "filename": "druid-handler/src/java/org/apache/hadoop/hive/druid/io/HiveDruidSplit.java",
                "patch": "@@ -49,22 +49,37 @@ public HiveDruidSplit(String druidQuery, Path dummyPath, String hosts[]) {\n   public void write(DataOutput out) throws IOException {\n     super.write(out);\n     out.writeUTF(druidQuery);\n+    out.writeInt(hosts.length);\n+    for (String host : hosts) {\n+      out.writeUTF(host);\n+    }\n   }\n \n   @Override\n   public void readFields(DataInput in) throws IOException {\n     super.readFields(in);\n     druidQuery = in.readUTF();\n+    int length = in.readInt();\n+    String[] listHosts = new String[length];\n+    for (int i = 0; i < length; i++) {\n+      listHosts[i] = in.readUTF();\n+    }\n+    hosts = listHosts;\n   }\n \n   public String getDruidQuery() {\n     return druidQuery;\n   }\n \n+  @Override\n+  public String[] getLocations() throws IOException {\n+    return hosts;\n+  }\n+\n   @Override\n   public String toString() {\n-    return \"HiveDruidSplit{\" + druidQuery + \", \" \n-            + (hosts == null ? \"empty hosts\" : Arrays.toString(hosts))  + \"}\";\n+    return \"HiveDruidSplit{\" + druidQuery + \", \"\n+            + (hosts == null ? \"empty hosts\" : Arrays.toString(hosts)) + \"}\";\n   }\n \n }",
                "raw_url": "https://github.com/apache/hive/raw/1d159ffd3c5016b78ca2814b837c02ab3f4be1de/druid-handler/src/java/org/apache/hadoop/hive/druid/io/HiveDruidSplit.java",
                "sha": "5159b426df985a15817ec13071be783ab18cc51a",
                "status": "modified"
            },
            {
                "additions": 46,
                "blob_url": "https://github.com/apache/hive/blob/1d159ffd3c5016b78ca2814b837c02ab3f4be1de/druid-handler/src/test/org/apache/hadoop/hive/druid/io/TestHiveDruidSplit.java",
                "changes": 46,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/druid-handler/src/test/org/apache/hadoop/hive/druid/io/TestHiveDruidSplit.java?ref=1d159ffd3c5016b78ca2814b837c02ab3f4be1de",
                "deletions": 0,
                "filename": "druid-handler/src/test/org/apache/hadoop/hive/druid/io/TestHiveDruidSplit.java",
                "patch": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.druid.io;\n+\n+import org.apache.hadoop.fs.Path;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataInputStream;\n+import java.io.DataOutput;\n+import java.io.DataOutputStream;\n+import java.io.IOException;\n+\n+public class TestHiveDruidSplit {\n+  @Test\n+  public void testSerDeser() throws IOException {\n+    HiveDruidSplit hiveDruidSplit = new HiveDruidSplit(\"query string\", new Path(\"test-path\"), new String []{\"host:8080\", \"host2:8090\"});\n+    ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();\n+    DataOutput dataOutput = new DataOutputStream(byteArrayOutputStream);\n+    hiveDruidSplit.write(dataOutput);\n+    ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(byteArrayOutputStream.toByteArray());\n+    HiveDruidSplit actualHiveDruidSplit = new HiveDruidSplit();\n+    actualHiveDruidSplit.readFields(new DataInputStream(byteArrayInputStream));\n+    Assert.assertEquals(actualHiveDruidSplit.getDruidQuery(), \"query string\");\n+    Assert.assertArrayEquals(actualHiveDruidSplit.getLocations(),  new String []{\"host:8080\", \"host2:8090\"});\n+  }\n+\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hive/raw/1d159ffd3c5016b78ca2814b837c02ab3f4be1de/druid-handler/src/test/org/apache/hadoop/hive/druid/io/TestHiveDruidSplit.java",
                "sha": "234c783d25354cfa48890f659cc0083eff1c0c66",
                "status": "added"
            }
        ],
        "message": "HIVE-16122: NPE Hive Druid split introduced by HIVE-15928 (Slim Bouguerra, reviewed by Jesus Camacho Rodriguez)",
        "parent": "https://github.com/apache/hive/commit/7f4a3e17ec2fa886276a7f278e5846e0e7ebc8a6",
        "patched_files": [
            "HiveDruidSplit.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveDruidSplit.java"
        ]
    },
    "hive_1f25c46": {
        "bug_id": "hive_1f25c46",
        "commit": "https://github.com/apache/hive/commit/1f25c46a2bf50483e09c756803d78e078dc37b92",
        "file": [
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hive/blob/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=1f25c46a2bf50483e09c756803d78e078dc37b92",
                "deletions": 1,
                "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "patch": "@@ -3250,6 +3250,12 @@ public boolean equals(Object obj) {\n                 part.getTableName(), part.toString());\n             throw new MetaException(errorMsg);\n           }\n+          if (part.getValues() == null || part.getValues().isEmpty()) {\n+            throw new MetaException(\"The partition values cannot be null or empty.\");\n+          }\n+          if (part.getValues().contains(null)) {\n+            throw new MetaException(\"Partition value cannot be null.\");\n+          }\n \n           boolean shouldAdd = startAddPartition(ms, part, ifNotExists);\n           if (!shouldAdd) {\n@@ -3410,7 +3416,10 @@ public AddPartitionsResult add_partitions_req(AddPartitionsRequest request)\n     public int add_partitions(final List<Partition> parts) throws MetaException,\n         InvalidObjectException, AlreadyExistsException {\n       startFunction(\"add_partition\");\n-      if (parts.size() == 0) {\n+      if (parts == null) {\n+        throw new MetaException(\"Partition list cannot be null.\");\n+      }\n+      if (parts.isEmpty()) {\n         return 0;\n       }\n \n@@ -3471,6 +3480,9 @@ private int add_partitions_pspec_core(RawStore ms, String catName, String dbName\n                                           boolean ifNotExists)\n         throws TException {\n       boolean success = false;\n+      if (dbName == null || tblName == null) {\n+        throw new MetaException(\"The database and table name cannot be null.\");\n+      }\n       // Ensures that the list doesn't have dups, and keeps track of directories we have created.\n       final Map<PartValEqWrapperLite, Boolean> addedPartitions = new ConcurrentHashMap<>();\n       PartitionSpecProxy partitionSpecProxy = PartitionSpecProxy.Factory.get(partSpecs);\n@@ -3496,12 +3508,18 @@ private int add_partitions_pspec_core(RawStore ms, String catName, String dbName\n           // will be created if the list contains an invalid partition.\n           final Partition part = partitionIterator.getCurrent();\n \n+          if (part.getDbName() == null || part.getTableName() == null) {\n+            throw new MetaException(\"The database and table name must be set in the partition.\");\n+          }\n           if (!part.getTableName().equalsIgnoreCase(tblName) || !part.getDbName().equalsIgnoreCase(dbName)) {\n             String errorMsg = String.format(\n                 \"Partition does not belong to target table %s.%s. It belongs to the table %s.%s : %s\",\n                 dbName, tblName, part.getDbName(), part.getTableName(), part.toString());\n             throw new MetaException(errorMsg);\n           }\n+          if (part.getValues() == null || part.getValues().isEmpty()) {\n+            throw new MetaException(\"The partition values cannot be null or empty.\");\n+          }\n \n           boolean shouldAdd = startAddPartition(ms, part, ifNotExists);\n           if (!shouldAdd) {\n@@ -3733,6 +3751,9 @@ private Partition add_partition_core(final RawStore ms,\n \n         firePreEvent(new PreAddPartitionEvent(tbl, part, this));\n \n+        if (part.getValues() == null || part.getValues().isEmpty()) {\n+          throw new MetaException(\"The partition values cannot be null or empty.\");\n+        }\n         boolean shouldAdd = startAddPartition(ms, part, false);\n         assert shouldAdd; // start would throw if it already existed here\n         boolean madeDir = createLocationForAddedPartition(tbl, part);\n@@ -3789,6 +3810,9 @@ public Partition add_partition_with_environment_context(\n         final Partition part, EnvironmentContext envContext)\n         throws InvalidObjectException, AlreadyExistsException,\n         MetaException {\n+      if (part == null) {\n+        throw new MetaException(\"Partition cannot be null.\");\n+      }\n       startTableFunction(\"add_partition\",\n           part.getCatName(), part.getDbName(), part.getTableName());\n       Partition ret = null;",
                "raw_url": "https://github.com/apache/hive/raw/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "sha": "9c88cf9e063f22a5ba60f16c34ce45457b0c9375",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hive/blob/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java?ref=1f25c46a2bf50483e09c756803d78e078dc37b92",
                "deletions": 1,
                "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java",
                "patch": "@@ -649,7 +649,7 @@ public Partition add_partition(Partition new_part) throws TException {\n \n   public Partition add_partition(Partition new_part, EnvironmentContext envContext)\n       throws TException {\n-    if (!new_part.isSetCatName()) new_part.setCatName(getDefaultCatalog(conf));\n+    if (new_part != null && !new_part.isSetCatName()) new_part.setCatName(getDefaultCatalog(conf));\n     Partition p = client.add_partition_with_environment_context(new_part, envContext);\n     return deepCopy(p);\n   }\n@@ -664,6 +664,9 @@ public Partition add_partition(Partition new_part, EnvironmentContext envContext\n    */\n   @Override\n   public int add_partitions(List<Partition> new_parts) throws TException {\n+    if (new_parts == null || new_parts.contains(null)) {\n+      throw new MetaException(\"Partitions cannot be null.\");\n+    }\n     if (new_parts != null && !new_parts.isEmpty() && !new_parts.get(0).isSetCatName()) {\n       final String defaultCat = getDefaultCatalog(conf);\n       new_parts.forEach(p -> p.setCatName(defaultCat));\n@@ -674,6 +677,9 @@ public int add_partitions(List<Partition> new_parts) throws TException {\n   @Override\n   public List<Partition> add_partitions(\n       List<Partition> parts, boolean ifNotExists, boolean needResults) throws TException {\n+    if (parts == null || parts.contains(null)) {\n+      throw new MetaException(\"Partitions cannot be null.\");\n+    }\n     if (parts.isEmpty()) {\n       return needResults ? new ArrayList<>() : null;\n     }\n@@ -688,6 +694,9 @@ public int add_partitions(List<Partition> new_parts) throws TException {\n \n   @Override\n   public int add_partitions_pspec(PartitionSpecProxy partitionSpec) throws TException {\n+    if (partitionSpec == null) {\n+      throw new MetaException(\"PartitionSpec cannot be null.\");\n+    }\n     if (partitionSpec.getCatName() == null) partitionSpec.setCatName(getDefaultCatalog(conf));\n     return client.add_partitions_pspec(partitionSpec.toPartitionSpec());\n   }",
                "raw_url": "https://github.com/apache/hive/raw/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java",
                "sha": "4f686a028c9e98ac26564d05e658b19806fca83e",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/CompositePartitionSpecProxy.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/CompositePartitionSpecProxy.java?ref=1f25c46a2bf50483e09c756803d78e078dc37b92",
                "deletions": 3,
                "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/CompositePartitionSpecProxy.java",
                "patch": "@@ -40,7 +40,7 @@\n   private List<PartitionSpecProxy> partitionSpecProxies;\n   private int size = 0;\n \n-  protected CompositePartitionSpecProxy(List<PartitionSpec> partitionSpecs) {\n+  protected CompositePartitionSpecProxy(List<PartitionSpec> partitionSpecs) throws MetaException {\n     this.partitionSpecs = partitionSpecs;\n     if (partitionSpecs.isEmpty()) {\n       catName = null;\n@@ -63,13 +63,13 @@ protected CompositePartitionSpecProxy(List<PartitionSpec> partitionSpecs) {\n   }\n \n   @Deprecated\n-  protected CompositePartitionSpecProxy(String dbName, String tableName, List<PartitionSpec> partitionSpecs) {\n+  protected CompositePartitionSpecProxy(String dbName, String tableName, List<PartitionSpec> partitionSpecs) throws MetaException {\n     this(DEFAULT_CATALOG_NAME, dbName, tableName, partitionSpecs);\n \n   }\n \n   protected CompositePartitionSpecProxy(String catName, String dbName, String tableName,\n-                                        List<PartitionSpec> partitionSpecs) {\n+                                        List<PartitionSpec> partitionSpecs) throws MetaException {\n     this.catName = catName;\n     this.dbName = dbName;\n     this.tableName = tableName;",
                "raw_url": "https://github.com/apache/hive/raw/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/CompositePartitionSpecProxy.java",
                "sha": "91d790aa6473c4d3941b11c81938b38dea93c135",
                "status": "modified"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/hive/blob/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionListComposingSpecProxy.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionListComposingSpecProxy.java?ref=1f25c46a2bf50483e09c756803d78e078dc37b92",
                "deletions": 1,
                "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionListComposingSpecProxy.java",
                "patch": "@@ -20,6 +20,7 @@\n \n import org.apache.hadoop.hive.metastore.api.MetaException;\n import org.apache.hadoop.hive.metastore.api.Partition;\n+import org.apache.hadoop.hive.metastore.api.PartitionListComposingSpec;\n import org.apache.hadoop.hive.metastore.api.PartitionSpec;\n \n import java.util.Arrays;\n@@ -33,9 +34,24 @@\n \n   private PartitionSpec partitionSpec;\n \n-  protected PartitionListComposingSpecProxy(PartitionSpec partitionSpec) {\n+  protected PartitionListComposingSpecProxy(PartitionSpec partitionSpec) throws MetaException {\n     assert partitionSpec.isSetPartitionList()\n         : \"Partition-list should have been set.\";\n+    PartitionListComposingSpec partitionList = partitionSpec.getPartitionList();\n+    if (partitionList == null || partitionList.getPartitions() == null) {\n+      throw new MetaException(\"The partition list cannot be null.\");\n+    }\n+    for (Partition partition : partitionList.getPartitions()) {\n+      if (partition == null) {\n+        throw new MetaException(\"Partition cannot be null.\");\n+      }\n+      if (partition.getValues() == null || partition.getValues().isEmpty()) {\n+        throw new MetaException(\"The partition value list cannot be null or empty.\");\n+      }\n+      if (partition.getValues().contains(null)) {\n+        throw new MetaException(\"Partition value cannot be null.\");\n+      }\n+    }\n     this.partitionSpec = partitionSpec;\n   }\n \n@@ -102,6 +118,10 @@ public void setRootLocation(String newRootPath) throws MetaException {\n       throw new MetaException(\"No common root-path. Can't replace root-path!\");\n     }\n \n+    if (newRootPath == null) {\n+      throw new MetaException(\"Root path cannot be null.\");\n+    }\n+\n     for (Partition partition : partitionSpec.getPartitionList().getPartitions()) {\n       String location = partition.getSd().getLocation();\n       if (location.startsWith(oldRootPath)) {",
                "raw_url": "https://github.com/apache/hive/raw/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionListComposingSpecProxy.java",
                "sha": "585b8fd8b6b29cf4bc563b25f67002042c8cd71f",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionSpecProxy.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionSpecProxy.java?ref=1f25c46a2bf50483e09c756803d78e078dc37b92",
                "deletions": 2,
                "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionSpecProxy.java",
                "patch": "@@ -100,8 +100,9 @@\n      * Factory method. Construct PartitionSpecProxy from raw PartitionSpec.\n      * @param partSpec Raw PartitionSpec from the Thrift API.\n      * @return PartitionSpecProxy instance.\n+     * @throws MetaException\n      */\n-    public static PartitionSpecProxy get(PartitionSpec partSpec) {\n+    public static PartitionSpecProxy get(PartitionSpec partSpec) throws MetaException {\n \n       if (partSpec == null) {\n         return null;\n@@ -123,8 +124,9 @@ public static PartitionSpecProxy get(PartitionSpec partSpec) {\n      * Factory method to construct CompositePartitionSpecProxy.\n      * @param partitionSpecs List of raw PartitionSpecs.\n      * @return A CompositePartitionSpecProxy instance.\n+     * @throws MetaException\n      */\n-    public static PartitionSpecProxy get(List<PartitionSpec> partitionSpecs) {\n+    public static PartitionSpecProxy get(List<PartitionSpec> partitionSpecs) throws MetaException {\n       return new CompositePartitionSpecProxy(partitionSpecs);\n     }\n ",
                "raw_url": "https://github.com/apache/hive/raw/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionSpecProxy.java",
                "sha": "18664463633f5de404e4d6b6b0ed3a726082dcf2",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionSpecWithSharedSDProxy.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionSpecWithSharedSDProxy.java?ref=1f25c46a2bf50483e09c756803d78e078dc37b92",
                "deletions": 1,
                "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionSpecWithSharedSDProxy.java",
                "patch": "@@ -38,8 +38,11 @@\n \n   private PartitionSpec partitionSpec;\n \n-  public PartitionSpecWithSharedSDProxy(PartitionSpec partitionSpec) {\n+  public PartitionSpecWithSharedSDProxy(PartitionSpec partitionSpec) throws MetaException {\n     assert partitionSpec.isSetSharedSDPartitionSpec();\n+    if (partitionSpec.getSharedSDPartitionSpec().getSd() == null) {\n+      throw new MetaException(\"The shared storage descriptor must be set.\");\n+    }\n     this.partitionSpec = partitionSpec;\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/partition/spec/PartitionSpecWithSharedSDProxy.java",
                "sha": "5b462066f4f2f196d4b9154b1b996d6a5053794a",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hive/blob/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestAddPartitions.java",
                "changes": 71,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestAddPartitions.java?ref=1f25c46a2bf50483e09c756803d78e078dc37b92",
                "deletions": 46,
                "filename": "standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestAddPartitions.java",
                "patch": "@@ -46,7 +46,6 @@\n import org.apache.hadoop.hive.metastore.client.builder.TableBuilder;\n import org.apache.hadoop.hive.metastore.minihms.AbstractMetaStoreService;\n import org.apache.thrift.TException;\n-import org.apache.thrift.transport.TTransportException;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -512,7 +511,7 @@ public void testAddPartitionTooManyValues() throws Exception {\n   @Test(expected = MetaException.class)\n   public void testAddPartitionNoPartColOnTable() throws Exception {\n \n-    Table origTable = new TableBuilder()\n+    new TableBuilder()\n         .setDbName(DB_NAME)\n         .setTableName(TABLE_NAME)\n         .addCol(\"test_id\", \"int\", \"test col id\")\n@@ -575,27 +574,19 @@ public void testAddPartitionMorePartColInTable() throws Exception {\n     client.add_partition(partition);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionNullPartition() throws Exception {\n-    try {\n-      client.add_partition(null);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: NPE should not be thrown.\n-    }\n+\n+    client.add_partition(null);\n   }\n \n-  @Test\n-  public void testAddPartitionNullValue() throws Exception {\n+  @Test(expected = MetaException.class)\n+  public void testAddPartitionNullValues() throws Exception {\n \n     createTable();\n     Partition partition = buildPartition(DB_NAME, TABLE_NAME, null);\n-    try {\n-      client.add_partition(partition);\n-    } catch (NullPointerException e) {\n-      // TODO: This works different in remote and embedded mode.\n-      // In embedded mode, no exception happens.\n-    }\n+    partition.setValues(null);\n+    client.add_partition(partition);\n   }\n \n   @Test\n@@ -698,14 +689,10 @@ public void testAddPartitionsWithDefaultAttributes() throws Exception {\n     verifyPartitionAttributesDefaultValues(part, table.getSd().getLocation());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionsNullList() throws Exception {\n-    try {\n-      client.add_partitions(null);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: NPE should not be thrown\n-    }\n+\n+    client.add_partitions(null);\n   }\n \n   @Test\n@@ -1159,31 +1146,23 @@ public void testAddPartitionsMorePartColInTable() throws Exception {\n     client.add_partitions(partitions);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionsNullPartition() throws Exception {\n-    try {\n-      List<Partition> partitions = new ArrayList<>();\n-      partitions.add(null);\n-      client.add_partitions(partitions);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (TTransportException | NullPointerException e) {\n-      // TODO: NPE should not be thrown\n-    }\n+\n+    List<Partition> partitions = new ArrayList<>();\n+    partitions.add(null);\n+    client.add_partitions(partitions);\n   }\n \n-  @Test\n-  public void testAddPartitionsNullValue() throws Exception {\n+  @Test(expected = MetaException.class)\n+  public void testAddPartitionsNullValues() throws Exception {\n \n     createTable();\n     Partition partition = buildPartition(DB_NAME, TABLE_NAME, null);\n+    partition.setValues(null);\n     List<Partition> partitions = new ArrayList<>();\n     partitions.add(partition);\n-    try {\n-      client.add_partitions(partitions);\n-    } catch (NullPointerException e) {\n-      // TODO: This works different in remote and embedded mode.\n-      // In embedded mode, no exception happens.\n-    }\n+    client.add_partitions(partitions);\n   }\n \n   @Test\n@@ -1313,9 +1292,9 @@ public void testAddPartsMultipleValues() throws Exception {\n     verifyPartition(table, \"year=2016/month=march\", Lists.newArrayList(\"2016\", \"march\"), 3);\n   }\n \n-  @Test(expected = NullPointerException.class)\n+  @Test(expected = MetaException.class)\n   public void testAddPartsNullList() throws Exception {\n-    // TODO: NPE should not be thrown\n+\n     client.add_partitions(null, false, false);\n   }\n \n@@ -1429,9 +1408,9 @@ public void testAddPartsAlreadyExistsIfExistsTrue() throws Exception {\n     Assert.assertTrue(partitionNames.contains(\"year=2017\"));\n   }\n \n-  @Test(expected = NullPointerException.class)\n+  @Test(expected = MetaException.class)\n   public void testAddPartsNullPartition() throws Exception {\n-    // TODO: NPE should not be thrown\n+\n     List<Partition> partitions = new ArrayList<>();\n     partitions.add(null);\n     client.add_partitions(partitions, false, false);\n@@ -1452,7 +1431,7 @@ private Table createTable(String dbName, String tableName, String location) thro\n \n   private Table createTable(String dbName, String tableName, List<FieldSchema> partCols,\n       String location) throws Exception {\n-    Table table = new TableBuilder()\n+    new TableBuilder()\n         .setDbName(dbName)\n         .setTableName(tableName)\n         .addCol(\"test_id\", \"int\", \"test col id\")",
                "raw_url": "https://github.com/apache/hive/raw/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestAddPartitions.java",
                "sha": "88064d920f30ec1dfa60db51a12baa6848b6c788",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hive/blob/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestAddPartitionsFromPartSpec.java",
                "changes": 89,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestAddPartitionsFromPartSpec.java?ref=1f25c46a2bf50483e09c756803d78e078dc37b92",
                "deletions": 65,
                "filename": "standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestAddPartitionsFromPartSpec.java",
                "patch": "@@ -27,7 +27,6 @@\n import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n import org.apache.hadoop.hive.metastore.annotation.MetastoreCheckinTest;\n import org.apache.hadoop.hive.metastore.api.AlreadyExistsException;\n-import org.apache.hadoop.hive.metastore.api.Database;\n import org.apache.hadoop.hive.metastore.api.FieldSchema;\n import org.apache.hadoop.hive.metastore.api.InvalidObjectException;\n import org.apache.hadoop.hive.metastore.api.MetaException;\n@@ -45,7 +44,6 @@\n import org.apache.hadoop.hive.metastore.minihms.AbstractMetaStoreService;\n import org.apache.hadoop.hive.metastore.partition.spec.PartitionSpecProxy;\n import org.apache.thrift.TException;\n-import org.apache.thrift.transport.TTransportException;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -170,10 +168,9 @@ public void testAddPartitionSpecsMultipleValues() throws Exception {\n \n   // TODO add tests for partitions in other catalogs\n \n-  @Test(expected = NullPointerException.class)\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionSpecNullSpec() throws Exception {\n \n-    // TODO: NPE should not be thrown.\n     client.add_partitions_pspec(null);\n   }\n \n@@ -186,51 +183,36 @@ public void testAddPartitionSpecEmptyPartList() throws Exception {\n     client.add_partitions_pspec(partitionSpec);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionSpecNullPartList() throws Exception {\n \n     createTable();\n     List<Partition> partitions = null;\n     PartitionSpecProxy partitionSpec = buildPartitionSpec(DB_NAME, TABLE_NAME, null, partitions);\n-    try {\n-      client.add_partitions_pspec(partitionSpec);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (NullPointerException | TTransportException e) {\n-      // TODO: NPE should not be thrown.\n-    }\n+    client.add_partitions_pspec(partitionSpec);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionSpecNoDB() throws Exception {\n \n     createTable();\n     Partition partition = buildPartition(DB_NAME, TABLE_NAME, DEFAULT_YEAR_VALUE);\n     PartitionSpecProxy partitionSpecProxy =\n         buildPartitionSpec(null, TABLE_NAME, null, Lists.newArrayList(partition));\n-    try {\n-      client.add_partitions_pspec(partitionSpecProxy);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (NullPointerException | TTransportException e) {\n-      // TODO: NPE should not be thrown.\n-    }\n+    client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionSpecNoTable() throws Exception {\n \n     createTable();\n     Partition partition = buildPartition(DB_NAME, TABLE_NAME, DEFAULT_YEAR_VALUE);\n     PartitionSpecProxy partitionSpecProxy =\n         buildPartitionSpec(DB_NAME, null, null, Lists.newArrayList(partition));\n-    try {\n-      client.add_partitions_pspec(partitionSpecProxy);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (NullPointerException | TTransportException e) {\n-      // TODO: NPE should not be thrown.\n-    }\n+    client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionSpecNoDBAndTableInPartition() throws Exception {\n \n     createTable();\n@@ -239,12 +221,7 @@ public void testAddPartitionSpecNoDBAndTableInPartition() throws Exception {\n     partition.setTableName(null);\n     PartitionSpecProxy partitionSpecProxy =\n         buildPartitionSpec(DB_NAME, TABLE_NAME, null, Lists.newArrayList(partition));\n-    try {\n-      client.add_partitions_pspec(partitionSpecProxy);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (NullPointerException | TTransportException e) {\n-      // TODO: NPE should not be thrown.\n-    }\n+    client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n   @Test\n@@ -346,7 +323,7 @@ public void testAddPartitionSpecDiffDBName() throws Exception {\n     }\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionSpecNullPart() throws Exception {\n \n     createTable();\n@@ -357,11 +334,7 @@ public void testAddPartitionSpecNullPart() throws Exception {\n     partitions.add(partition2);\n     PartitionSpecProxy partitionSpecProxy =\n         buildPartitionSpec(DB_NAME, TABLE_NAME, null, partitions);\n-    try {\n-      client.add_partitions_pspec(partitionSpecProxy);\n-    } catch (NullPointerException e) {\n-      // TODO: NPE should not be thrown.\n-    }\n+    client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n   @Test\n@@ -457,7 +430,7 @@ public void testAddPartitionSpecChangeRootPathFromNull() throws Exception {\n     client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n-  @Test(expected = NullPointerException.class)\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionSpecChangeRootPathToNull() throws Exception {\n \n     Table table = createTable();\n@@ -467,7 +440,6 @@ public void testAddPartitionSpecChangeRootPathToNull() throws Exception {\n         buildPartitionSpec(DB_NAME, TABLE_NAME, rootPath, Lists.newArrayList(partition));\n     partitionSpecProxy.setRootLocation(null);\n     client.add_partitions_pspec(partitionSpecProxy);\n-    // TODO: NPE should not be thrown.\n   }\n \n   @Test(expected = MetaException.class)\n@@ -596,20 +568,15 @@ public void testAddPartitionSpecNullSd() throws Exception {\n     client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAddPartitionSpecWithSharedSDNullSd() throws Exception {\n \n     createTable();\n     PartitionWithoutSD partition = buildPartitionWithoutSD(Lists.newArrayList(\"2002\"), 0);\n     StorageDescriptor sd = null;\n     PartitionSpecProxy partitionSpecProxy =\n         buildPartitionSpecWithSharedSD(Lists.newArrayList(partition), sd);\n-    try {\n-      client.add_partitions_pspec(partitionSpecProxy);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (NullPointerException | TTransportException e) {\n-      // TODO: NPE should not be thrown.\n-    }\n+    client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n   @Test(expected = MetaException.class)\n@@ -734,7 +701,7 @@ public void testAddPartitionsForViewNullPartSd() throws Exception {\n     Assert.assertNull(part.getSd());\n   }\n \n-  @Test\n+  @Test(expected=MetaException.class)\n   public void testAddPartitionSpecWithSharedSDNoValue() throws Exception {\n \n     Table table = createTable();\n@@ -743,12 +710,7 @@ public void testAddPartitionSpecWithSharedSDNoValue() throws Exception {\n     String location = table.getSd().getLocation() + \"/nullValueTest/\";\n     PartitionSpecProxy partitionSpecProxy =\n         buildPartitionSpecWithSharedSD(Lists.newArrayList(partition), buildSD(location));\n-    try {\n-      client.add_partitions_pspec(partitionSpecProxy);\n-      Assert.fail(\"Exception should have been thrown.\");\n-    } catch (NullPointerException | TTransportException e) {\n-      // TODO: NPE should not be thrown.\n-    }\n+    client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n   @Test(expected=MetaException.class)\n@@ -767,18 +729,15 @@ public void testAddPartitionSpecNoValue() throws Exception {\n     client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n-  @Test\n-  public void testAddPartitionSpecNullValue() throws Exception {\n+  @Test(expected = MetaException.class)\n+  public void testAddPartitionSpecNullValues() throws Exception {\n \n     createTable();\n     Partition partition = buildPartition(DB_NAME, TABLE_NAME, null);\n+    partition.setValues(null);\n     PartitionSpecProxy partitionSpecProxy =\n         buildPartitionSpec(DB_NAME, TABLE_NAME, null, Lists.newArrayList(partition));\n-    try {\n-      client.add_partitions_pspec(partitionSpecProxy);\n-    } catch (NullPointerException e) {\n-      // TODO: NPE should not be thrown\n-    }\n+    client.add_partitions_pspec(partitionSpecProxy);\n   }\n \n   @Test\n@@ -938,7 +897,7 @@ public void testAddPartitionSpecMoreThanThreadCountsOneFails() throws Exception\n \n   // Helper methods\n   private void createDB(String dbName) throws TException {\n-    Database db = new DatabaseBuilder().setName(dbName).create(client, metaStore.getConf());\n+    new DatabaseBuilder().setName(dbName).create(client, metaStore.getConf());\n   }\n \n   private Table createTable() throws Exception {\n@@ -948,7 +907,7 @@ private Table createTable() throws Exception {\n \n   private Table createTable(String dbName, String tableName, List<FieldSchema> partCols,\n       String location) throws Exception {\n-    Table table = new TableBuilder()\n+    new TableBuilder()\n         .setDbName(dbName)\n         .setTableName(tableName)\n         .addCol(\"test_id\", \"int\", \"test col id\")\n@@ -1073,7 +1032,7 @@ private PartitionWithoutSD buildPartitionWithoutSD(List<String> values, int inde\n   }\n \n   private PartitionSpecProxy buildPartitionSpec(String dbName, String tableName, String rootPath,\n-      List<Partition> partitions) {\n+      List<Partition> partitions) throws MetaException {\n \n     PartitionSpec partitionSpec = new PartitionSpec();\n     partitionSpec.setDbName(dbName);\n@@ -1104,7 +1063,7 @@ private StorageDescriptor buildSD(String location) {\n   }\n \n   private PartitionSpecProxy buildPartitionSpecWithSharedSD(List<PartitionWithoutSD> partitions,\n-      StorageDescriptor sd) {\n+      StorageDescriptor sd) throws MetaException {\n \n     PartitionSpec partitionSpec = new PartitionSpec();\n     partitionSpec.setDbName(DB_NAME);",
                "raw_url": "https://github.com/apache/hive/raw/1f25c46a2bf50483e09c756803d78e078dc37b92/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestAddPartitionsFromPartSpec.java",
                "sha": "debcd0eee9d6adde14a7484304ee7d74f1e914fd",
                "status": "modified"
            }
        ],
        "message": "HIVE-19158: Fix NPE in the HiveMetastore add partition tests (Marta Kuczora, reviewed by Peter Vary and Sahil Takiar)",
        "parent": "https://github.com/apache/hive/commit/fb22f576da7383f9cb1d24b66f4090d07b6bde07",
        "patched_files": [
            "PartitionSpecWithSharedSDProxy.java",
            "PartitionListComposingSpecProxy.java",
            "HiveMetaStore.java",
            "HiveMetaStoreClient.java",
            "CompositePartitionSpecProxy.java",
            "PartitionSpecProxy.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveMetaStore.java",
            "TestAddPartitions.java",
            "TestAddPartitionsFromPartSpec.java"
        ]
    },
    "hive_2018314": {
        "bug_id": "hive_2018314",
        "commit": "https://github.com/apache/hive/commit/20183144e9db1bf22c663634ff82ad6d63c4637d",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/20183144e9db1bf22c663634ff82ad6d63c4637d/beeline/src/java/org/apache/hive/beeline/Commands.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/beeline/src/java/org/apache/hive/beeline/Commands.java?ref=20183144e9db1bf22c663634ff82ad6d63c4637d",
                "deletions": 4,
                "filename": "beeline/src/java/org/apache/hive/beeline/Commands.java",
                "patch": "@@ -96,8 +96,12 @@ public boolean metadata(String line) {\n \n \n   public boolean metadata(String cmd, String[] args) {\n+    if (!(beeLine.assertConnection())) {\n+      return false;\n+    }\n+\n     try {\n-      Method[] m = beeLine.getDatabaseConnection().getDatabaseMetaData().getClass().getMethods();\n+      Method[] m = beeLine.getDatabaseMetaData().getClass().getMethods();\n       Set<String> methodNames = new TreeSet<String>();\n       Set<String> methodNamesUpper = new TreeSet<String>();\n       for (int i = 0; i < m.length; i++) {\n@@ -114,7 +118,7 @@ public boolean metadata(String cmd, String[] args) {\n         return false;\n       }\n \n-      Object res = beeLine.getReflector().invoke(beeLine.getDatabaseConnection().getDatabaseMetaData(),\n+      Object res = beeLine.getReflector().invoke(beeLine.getDatabaseMetaData(),\n           DatabaseMetaData.class, cmd, Arrays.asList(args));\n \n       if (res instanceof ResultSet) {\n@@ -224,7 +228,7 @@ public boolean nativesql(String sql) throws Exception {\n     if (sql.startsWith(\"native\")) {\n       sql = sql.substring(\"native\".length() + 1);\n     }\n-    String nat = beeLine.getDatabaseConnection().getConnection().nativeSQL(sql);\n+    String nat = beeLine.getConnection().nativeSQL(sql);\n     beeLine.output(nat);\n     return true;\n   }\n@@ -568,7 +572,7 @@ public boolean dbinfo(String line) {\n     for (int i = 0; i < m.length; i++) {\n       try {\n         beeLine.output(beeLine.getColorBuffer().pad(m[i], padlen).append(\n-            \"\" + beeLine.getReflector().invoke(beeLine.getDatabaseConnection().getDatabaseMetaData(),\n+            \"\" + beeLine.getReflector().invoke(beeLine.getDatabaseMetaData(),\n                 m[i], new Object[0])));\n       } catch (Exception e) {\n         beeLine.handleException(e);",
                "raw_url": "https://github.com/apache/hive/raw/20183144e9db1bf22c663634ff82ad6d63c4637d/beeline/src/java/org/apache/hive/beeline/Commands.java",
                "sha": "3799cc109880815a14c275eb81a6c878f0ba3edc",
                "status": "modified"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/hive/blob/20183144e9db1bf22c663634ff82ad6d63c4637d/beeline/src/test/org/apache/hive/beeline/src/test/TestBeeLineWithArgs.java",
                "changes": 45,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/beeline/src/test/org/apache/hive/beeline/src/test/TestBeeLineWithArgs.java?ref=20183144e9db1bf22c663634ff82ad6d63c4637d",
                "deletions": 11,
                "filename": "beeline/src/test/org/apache/hive/beeline/src/test/TestBeeLineWithArgs.java",
                "patch": "@@ -18,23 +18,19 @@\n \n package org.apache.hive.beeline.src.test;\n \n-import java.io.File;\n-import java.io.IOException;\n-import java.io.PrintStream;\n import java.io.ByteArrayOutputStream;\n+import java.io.File;\n import java.io.FileOutputStream;\n-\n-import junit.framework.TestCase;\n-import org.junit.AfterClass;\n-import org.junit.BeforeClass;\n-import org.junit.Test;\n-import org.junit.Assert;\n+import java.io.PrintStream;\n+import java.io.UnsupportedEncodingException;\n \n import org.apache.hadoop.hive.conf.HiveConf;\n-import org.apache.hadoop.hive.ql.parse.SemanticException;\n import org.apache.hive.beeline.BeeLine;\n import org.apache.hive.service.server.HiveServer2;\n-import org.apache.hive.service.cli.HiveSQLException;\n+import org.junit.AfterClass;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n \n /**\n  * TestBeeLineWithArgs - executes tests of the command-line arguments to BeeLine\n@@ -216,4 +212,31 @@ public void testNegativeScriptFile() throws Throwable {\n       throw e;\n     }\n   }\n+\n+  /**\n+   * HIVE-4566\n+   * @throws UnsupportedEncodingException\n+   */\n+  @Test\n+  public void testNPE() throws UnsupportedEncodingException {\n+    BeeLine beeLine = new BeeLine();\n+\n+    ByteArrayOutputStream os = new ByteArrayOutputStream();\n+    PrintStream beelineOutputStream = new PrintStream(os);\n+    beeLine.setOutputStream(beelineOutputStream);\n+    beeLine.setErrorStream(beelineOutputStream);\n+\n+    beeLine.runCommands( new String[] {\"!typeinfo\"} );\n+    String output = os.toString(\"UTF8\");\n+    Assert.assertFalse( output.contains(\"java.lang.NullPointerException\") );\n+    Assert.assertTrue( output.contains(\"No current connection\") );\n+\n+    beeLine.runCommands( new String[] {\"!nativesql\"} );\n+    output = os.toString(\"UTF8\");\n+    Assert.assertFalse( output.contains(\"java.lang.NullPointerException\") );\n+    Assert.assertTrue( output.contains(\"No current connection\") );\n+\n+    System.out.println(\">>> PASSED \" + \"testNPE\" );\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hive/raw/20183144e9db1bf22c663634ff82ad6d63c4637d/beeline/src/test/org/apache/hive/beeline/src/test/TestBeeLineWithArgs.java",
                "sha": "030f6b0e8d3b48a73b3680b6649dd24d9cc2ec23",
                "status": "modified"
            }
        ],
        "message": "HIVE-4566 : NullPointerException if typeinfo and nativesql commands are executed at beeline before a DB connection is established (Xuefu Zhang via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1489672 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/4c2d2953d3eefb8caf921a01f726d1251cc77053",
        "patched_files": [
            "Commands.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestBeeLineWithArgs.java"
        ]
    },
    "hive_208a217": {
        "bug_id": "hive_208a217",
        "commit": "https://github.com/apache/hive/commit/208a217dce6484043bea60cd9274fd6979b801e4",
        "file": [
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hive/blob/208a217dce6484043bea60cd9274fd6979b801e4/ql/src/test/org/apache/hadoop/hive/ql/exec/TestStatsPublisherEnhanced.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/exec/TestStatsPublisherEnhanced.java?ref=208a217dce6484043bea60cd9274fd6979b801e4",
                "deletions": 10,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/exec/TestStatsPublisherEnhanced.java",
                "patch": "@@ -43,24 +43,25 @@\n \n   public TestStatsPublisherEnhanced(String name) {\n     super(name);\n-  }\n-\n-  @Override\n-  protected void setUp() {\n     conf = new JobConf(TestStatsPublisherEnhanced.class);\n-\n     conf.set(\"hive.stats.dbclass\", \"jdbc:derby\");\n-\n     statsImplementationClass = HiveConf.getVar(conf, HiveConf.ConfVars.HIVESTATSDBCLASS);\n     StatsFactory.setImplementation(statsImplementationClass, conf);\n+  }\n \n+  @Override\n+  protected void setUp() {\n     stats = new HashMap<String, String>();\n-    StatsAggregator sa = StatsFactory.getStatsAggregator();\n-    sa.connect(conf);\n-    sa.cleanUp(\"file_0\");\n-    sa.closeConnection();\n   }\n \n+  @Override\n+  protected void tearDown() {\n+    StatsAggregator sa = StatsFactory.getStatsAggregator();\n+    assertNotNull(sa);\n+    assertTrue(sa.connect(conf));\n+    assertTrue(sa.cleanUp(\"file_0\"));\n+    assertTrue(sa.closeConnection());\n+  }\n \n   private void fillStatMap(String numRows, String rawDataSize) {\n     stats.clear();",
                "raw_url": "https://github.com/apache/hive/raw/208a217dce6484043bea60cd9274fd6979b801e4/ql/src/test/org/apache/hadoop/hive/ql/exec/TestStatsPublisherEnhanced.java",
                "sha": "99176683b06cbde08026289702872232307fd11c",
                "status": "modified"
            }
        ],
        "message": "HIVE-2920 : TestStatsPublisherEnhanced throws NPE on JDBC connection failure (Carl Steinbach via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1308731 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/d4f2881b31b9457e393b19ea255e9e323b0ab565",
        "patched_files": [],
        "repo": "hive",
        "unit_tests": [
            "TestStatsPublisherEnhanced.java"
        ]
    },
    "hive_221dbe0": {
        "bug_id": "hive_221dbe0",
        "commit": "https://github.com/apache/hive/commit/221dbe085950e198d0766d60d6b00a70b30e5935",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/221dbe085950e198d0766d60d6b00a70b30e5935/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java?ref=221dbe085950e198d0766d60d6b00a70b30e5935",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java",
                "patch": "@@ -224,7 +224,9 @@ public void closeWriters(boolean abort) throws HiveException {\n     private void commit(FileSystem fs, List<Path> commitPaths) throws HiveException {\n       for (int idx = 0; idx < outPaths.length; ++idx) {\n         try {\n-          commitOneOutPath(idx, fs, commitPaths);\n+          if (outPaths[idx] != null) {\n+            commitOneOutPath(idx, fs, commitPaths);\n+          }\n         } catch (IOException e) {\n           throw new HiveException(\"Unable to commit output from: \" +\n               outPaths[idx] + \" to: \" + finalPaths[idx], e);",
                "raw_url": "https://github.com/apache/hive/raw/221dbe085950e198d0766d60d6b00a70b30e5935/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java",
                "sha": "949a9e84a73bd474523d713f1a895aa72843cbc4",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hive/blob/221dbe085950e198d0766d60d6b00a70b30e5935/ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java?ref=221dbe085950e198d0766d60d6b00a70b30e5935",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java",
                "patch": "@@ -184,6 +184,47 @@ public void testNoBuckets() throws Exception {\n     assertExpectedFileSet(expectedFiles, getWarehouseDir() + \"/nobuckets\");\n   }\n \n+  @Test\n+  public void testNoBucketsDP() throws Exception {\n+    hiveConf.setVar(HiveConf.ConfVars.DYNAMICPARTITIONINGMODE, \"nonstrict\");\n+    int[][] sourceVals1 = {{0,0,0},{3,3,3}};\n+    int[][] sourceVals2 = {{1,1,1},{2,2,2}};\n+    int[][] sourceVals3 = {{3,3,3},{4,4,4}};\n+    int[][] sourceVals4 = {{5,5,5},{6,6,6}};\n+    runStatementOnDriver(\"drop table if exists tmp\");\n+    runStatementOnDriver(\"create table tmp (c1 integer, c2 integer) partitioned by (c3 integer) stored as orc \" +\n+      \"tblproperties('transactional'='false')\");\n+    runStatementOnDriver(\"insert into tmp \" + makeValuesClause(sourceVals1));\n+    runStatementOnDriver(\"insert into tmp \" + makeValuesClause(sourceVals2));\n+    runStatementOnDriver(\"insert into tmp \" + makeValuesClause(sourceVals3));\n+    runStatementOnDriver(\"insert into tmp \" + makeValuesClause(sourceVals4));\n+    runStatementOnDriver(\"drop table if exists nobuckets\");\n+    runStatementOnDriver(\"create table nobuckets (c1 integer, c2 integer) partitioned by (c3 integer) stored \" +\n+      \"as orc tblproperties('transactional'='true', 'transactional_properties'='default')\");\n+    String stmt = \"insert into nobuckets partition(c3) select * from tmp\";\n+    runStatementOnDriver(stmt);\n+    List<String> rs = runStatementOnDriver(\n+      \"select ROW__ID, c1, c2, c3, INPUT__FILE__NAME from nobuckets order by ROW__ID\");\n+    Assert.assertEquals(\"\", 8, rs.size());\n+    LOG.warn(\"after insert\");\n+    for(String s : rs) {\n+      LOG.warn(s);\n+    }\n+\n+    rs = runStatementOnDriver(\n+      \"select * from nobuckets where c2 in (0,3)\");\n+    Assert.assertEquals(3, rs.size());\n+    runStatementOnDriver(\"update nobuckets set c2 = 17 where c2 in(0,3)\");\n+    rs = runStatementOnDriver(\"select ROW__ID, c1, c2, c3, INPUT__FILE__NAME from nobuckets order by INPUT__FILE__NAME, ROW__ID\");\n+    LOG.warn(\"after update\");\n+    for(String s : rs) {\n+      LOG.warn(s);\n+    }\n+    rs = runStatementOnDriver(\n+      \"select * from nobuckets where c2=17\");\n+    Assert.assertEquals(3, rs.size());\n+  }\n+\n   /**\n    * See CTAS tests in TestAcidOnTez\n    */",
                "raw_url": "https://github.com/apache/hive/raw/221dbe085950e198d0766d60d6b00a70b30e5935/ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java",
                "sha": "bbe9d5a58773a0aeca7d090450805102a2df4c95",
                "status": "modified"
            }
        ],
        "message": "HIVE-20038: Update queries on non-bucketed + partitioned tables throws NPE (Prasanth Jayachandran reviewed by Gopal V)",
        "parent": "https://github.com/apache/hive/commit/b122aea4ec8775c158fff975ffa472be7bfc0711",
        "patched_files": [
            "FileSinkOperator.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestTxnNoBuckets.java",
            "TestFileSinkOperator.java"
        ]
    },
    "hive_22371f5": {
        "bug_id": "hive_22371f5",
        "commit": "https://github.com/apache/hive/commit/22371f51f365ab609862dc493a86aed17212dac5",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/22371f51f365ab609862dc493a86aed17212dac5/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java?ref=22371f51f365ab609862dc493a86aed17212dac5",
                "deletions": 5,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java",
                "patch": "@@ -176,6 +176,8 @@ static void internalBeforeClassSetup(Map<String, String> additionalProperties, b\n     hconf.set(HiveConf.ConfVars.METASTORE_RAW_STORE_IMPL.varname,\n         \"org.apache.hadoop.hive.metastore.InjectableBehaviourObjectStore\");\n     hconf.setBoolVar(HiveConf.ConfVars.HIVEOPTIMIZEMETADATAQUERIES, true);\n+    hconf.setBoolVar(HiveConf.ConfVars.HIVESTATSAUTOGATHER, true);\n+    hconf.setBoolVar(HiveConf.ConfVars.HIVE_STATS_RELIABLE, true);\n     System.setProperty(HiveConf.ConfVars.PREEXECHOOKS.varname, \" \");\n     System.setProperty(HiveConf.ConfVars.POSTEXECHOOKS.varname, \" \");\n \n@@ -850,20 +852,20 @@ public void testIncrementalAdds() throws IOException {\n     // Now, we load data into the tables, and see if an incremental\n     // repl drop/load can duplicate it.\n \n-    run(\"LOAD DATA LOCAL INPATH '\" + unptn_locn + \"' OVERWRITE INTO TABLE \" + dbName + \".unptned\", driver);\n+    run(\"LOAD DATA LOCAL INPATH '\" + unptn_locn + \"' OVERWRITE INTO TABLE \" + dbName + \".unptned\", true, driver);\n     verifySetup(\"SELECT * from \" + dbName + \".unptned\", unptn_data, driver);\n     run(\"CREATE TABLE \" + dbName + \".unptned_late AS SELECT * from \" + dbName + \".unptned\", driver);\n     verifySetup(\"SELECT * from \" + dbName + \".unptned_late\", unptn_data, driver);\n \n-    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_1 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned PARTITION(b=1)\", driver);\n+    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_1 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned PARTITION(b=1)\", true, driver);\n     verifySetup(\"SELECT a from \" + dbName + \".ptned WHERE b=1\", ptn_data_1, driver);\n-    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_2 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned PARTITION(b=2)\", driver);\n+    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_2 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned PARTITION(b=2)\", true, driver);\n     verifySetup(\"SELECT a from \" + dbName + \".ptned WHERE b=2\", ptn_data_2, driver);\n \n     run(\"CREATE TABLE \" + dbName + \".ptned_late(a string) PARTITIONED BY (b int) STORED AS TEXTFILE\", driver);\n-    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_1 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned_late PARTITION(b=1)\", driver);\n+    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_1 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned_late PARTITION(b=1)\", true, driver);\n     verifySetup(\"SELECT a from \" + dbName + \".ptned_late WHERE b=1\",ptn_data_1, driver);\n-    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_2 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned_late PARTITION(b=2)\", driver);\n+    run(\"LOAD DATA LOCAL INPATH '\" + ptn_locn_2 + \"' OVERWRITE INTO TABLE \" + dbName + \".ptned_late PARTITION(b=2)\", true, driver);\n     verifySetup(\"SELECT a from \" + dbName + \".ptned_late WHERE b=2\", ptn_data_2, driver);\n \n     // Perform REPL-DUMP/LOAD",
                "raw_url": "https://github.com/apache/hive/raw/22371f51f365ab609862dc493a86aed17212dac5/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenarios.java",
                "sha": "6f47056ca72bb532730a25381a1973be81361ff1",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/22371f51f365ab609862dc493a86aed17212dac5/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java?ref=22371f51f365ab609862dc493a86aed17212dac5",
                "deletions": 2,
                "filename": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java",
                "patch": "@@ -823,7 +823,7 @@ private void blockPartitionLocationChangesOnReplSource(Database db, Table tbl,\n \n     // Do not allow changing location of a managed table as alter event doesn't capture the\n     // new files list. So, it may cause data inconsistency.\n-    if (ec.isSetProperties()) {\n+    if ((ec != null) && ec.isSetProperties()) {\n       String alterType = ec.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n       if (alterType != null && alterType.equalsIgnoreCase(ALTERLOCATION) &&\n               tbl.getTableType().equalsIgnoreCase(TableType.MANAGED_TABLE.name())) {\n@@ -846,7 +846,7 @@ private void validateTableChangesOnReplSource(Database db, Table oldTbl, Table n\n     // Do not allow changing location of a managed table as alter event doesn't capture the\n     // new files list. So, it may cause data inconsistency. We do this whether or not strict\n     // managed is true on the source cluster.\n-    if (ec.isSetProperties()) {\n+    if ((ec != null) && ec.isSetProperties()) {\n         String alterType = ec.getProperties().get(ALTER_TABLE_OPERATION_TYPE);\n         if (alterType != null && alterType.equalsIgnoreCase(ALTERLOCATION) &&\n             oldTbl.getTableType().equalsIgnoreCase(TableType.MANAGED_TABLE.name())) {",
                "raw_url": "https://github.com/apache/hive/raw/22371f51f365ab609862dc493a86aed17212dac5/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java",
                "sha": "ee0e52879ae4129257e81de4f14cebf3640bb781",
                "status": "modified"
            }
        ],
        "message": "HIVE-21811: Load data into partitioned table throws NPE if DB is enabled for replication (Sankar Hariappan, reviewed by Thejas M Nair)\n\nSigned-off-by: Sankar Hariappan <sankarh@apache.org>",
        "parent": "https://github.com/apache/hive/commit/81117db0e59aa673adc454d0da40d0a146555ae2",
        "patched_files": [
            "HiveAlterHandler.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestReplicationScenarios.java",
            "TestHiveAlterHandler.java"
        ]
    },
    "hive_24d63f7": {
        "bug_id": "hive_24d63f7",
        "commit": "https://github.com/apache/hive/commit/24d63f73899d1dc63b632364b84314e99c844417",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/24d63f73899d1dc63b632364b84314e99c844417/itests/src/test/resources/testconfiguration.properties",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=24d63f73899d1dc63b632364b84314e99c844417",
                "deletions": 1,
                "filename": "itests/src/test/resources/testconfiguration.properties",
                "patch": "@@ -146,7 +146,8 @@ minitez.query.files=bucket_map_join_tez1.q,\\\n   tez_join_tests.q,\\\n   tez_joins_explain.q,\\\n   tez_schema_evolution.q,\\\n-  tez_union.q\n+  tez_union.q,\\\n+  tez_union_decimal.q\n \n beeline.positive.exclude=add_part_exist.q,\\\n   alter1.q,\\",
                "raw_url": "https://github.com/apache/hive/raw/24d63f73899d1dc63b632364b84314e99c844417/itests/src/test/resources/testconfiguration.properties",
                "sha": "861e438960a27880e9232d5d7ca7e7f9db04c65d",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/hive/blob/24d63f73899d1dc63b632364b84314e99c844417/ql/src/test/queries/clientpositive/tez_union_decimal.q",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/tez_union_decimal.q?ref=24d63f73899d1dc63b632364b84314e99c844417",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/tez_union_decimal.q",
                "patch": "@@ -0,0 +1,37 @@\n+select sum(a) from (\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as decimal) a from src tablesample (1 rows)\n+) t;\n+\n+select sum(a) from (\n+  select cast(1 as tinyint) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as tinyint) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t;\n+\n+select sum(a) from (\n+  select cast(1 as smallint) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as smallint) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t;\n+\n+select sum(a) from (\n+  select cast(1 as int) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as int) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t;\n+\n+select sum(a) from (\n+  select cast(1 as bigint) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as bigint) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t;",
                "raw_url": "https://github.com/apache/hive/raw/24d63f73899d1dc63b632364b84314e99c844417/ql/src/test/queries/clientpositive/tez_union_decimal.q",
                "sha": "0f56e6a12425651f22a718891c4e9da4181b2256",
                "status": "added"
            },
            {
                "additions": 101,
                "blob_url": "https://github.com/apache/hive/blob/24d63f73899d1dc63b632364b84314e99c844417/ql/src/test/results/clientpositive/tez/tez_union_decimal.q.out",
                "changes": 101,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_union_decimal.q.out?ref=24d63f73899d1dc63b632364b84314e99c844417",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/tez/tez_union_decimal.q.out",
                "patch": "@@ -0,0 +1,101 @@\n+PREHOOK: query: select sum(a) from (\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as decimal) a from src tablesample (1 rows)\n+) t\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(a) from (\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as decimal) a from src tablesample (1 rows)\n+) t\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+#### A masked pattern was here ####\n+1\n+PREHOOK: query: select sum(a) from (\n+  select cast(1 as tinyint) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as tinyint) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(a) from (\n+  select cast(1 as tinyint) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as tinyint) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+#### A masked pattern was here ####\n+2\n+PREHOOK: query: select sum(a) from (\n+  select cast(1 as smallint) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as smallint) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(a) from (\n+  select cast(1 as smallint) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as smallint) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+#### A masked pattern was here ####\n+2\n+PREHOOK: query: select sum(a) from (\n+  select cast(1 as int) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as int) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(a) from (\n+  select cast(1 as int) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as int) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+#### A masked pattern was here ####\n+2\n+PREHOOK: query: select sum(a) from (\n+  select cast(1 as bigint) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as bigint) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(a) from (\n+  select cast(1 as bigint) a from src tablesample (1 rows)\n+  union all\n+  select cast(null as bigint) a from src tablesample (1 rows)\n+  union all\n+  select cast(1.1 as decimal) a from src tablesample (1 rows)\n+) t\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+#### A masked pattern was here ####\n+2",
                "raw_url": "https://github.com/apache/hive/raw/24d63f73899d1dc63b632364b84314e99c844417/ql/src/test/results/clientpositive/tez/tez_union_decimal.q.out",
                "sha": "29332bed6540cc4d785312166f70aaec14a74b17",
                "status": "added"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantByteObjectInspector.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantByteObjectInspector.java?ref=24d63f73899d1dc63b632364b84314e99c844417",
                "deletions": 0,
                "filename": "serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantByteObjectInspector.java",
                "patch": "@@ -48,6 +48,9 @@ public ByteWritable getWritableConstantValue() {\n \n   @Override\n   public int precision() {\n+    if (value == null) {\n+      return super.precision();\n+    }\n     return BigDecimal.valueOf(value.get()).precision();\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantByteObjectInspector.java",
                "sha": "3214e11a7875c8688e74bcc573b700d4c8110a38",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantHiveDecimalObjectInspector.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantHiveDecimalObjectInspector.java?ref=24d63f73899d1dc63b632364b84314e99c844417",
                "deletions": 0,
                "filename": "serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantHiveDecimalObjectInspector.java",
                "patch": "@@ -58,11 +58,17 @@ public HiveDecimalWritable getWritableConstantValue() {\n \n   @Override\n   public int precision() {\n+    if (value == null) {\n+      return super.precision();\n+    }\n     return value.getHiveDecimal().precision();\n   }\n \n   @Override\n   public int scale() {\n+    if (value == null) {\n+      return super.scale();\n+    }\n     return value.getHiveDecimal().scale();\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantHiveDecimalObjectInspector.java",
                "sha": "b87d1f817aaaba2ec61326e3371ab0ceae8789b5",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantIntObjectInspector.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantIntObjectInspector.java?ref=24d63f73899d1dc63b632364b84314e99c844417",
                "deletions": 0,
                "filename": "serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantIntObjectInspector.java",
                "patch": "@@ -48,6 +48,9 @@ public IntWritable getWritableConstantValue() {\n \n   @Override\n   public int precision() {\n+    if (value == null) {\n+      return super.precision();\n+    }\n     return BigDecimal.valueOf(value.get()).precision();\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantIntObjectInspector.java",
                "sha": "0a24c2c26229d04320105bce86ee6a51a3ad30b4",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantLongObjectInspector.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantLongObjectInspector.java?ref=24d63f73899d1dc63b632364b84314e99c844417",
                "deletions": 0,
                "filename": "serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantLongObjectInspector.java",
                "patch": "@@ -48,6 +48,9 @@ public LongWritable getWritableConstantValue() {\n \n   @Override\n   public int precision() {\n+    if (value == null) {\n+      return super.precision();\n+    }\n     return BigDecimal.valueOf(value.get()).precision();\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantLongObjectInspector.java",
                "sha": "1973d48d708a24dd3e89e3c10b4254ff84ea4321",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantShortObjectInspector.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantShortObjectInspector.java?ref=24d63f73899d1dc63b632364b84314e99c844417",
                "deletions": 0,
                "filename": "serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantShortObjectInspector.java",
                "patch": "@@ -48,6 +48,9 @@ public ShortWritable getWritableConstantValue() {\n \n   @Override\n   public int precision() {\n+    if (value == null) {\n+      return super.precision();\n+    }\n     return BigDecimal.valueOf(value.get()).precision();\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/24d63f73899d1dc63b632364b84314e99c844417/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/primitive/WritableConstantShortObjectInspector.java",
                "sha": "2f7479abf6b85f7ae1d86ee0e88930a47e116c3a",
                "status": "modified"
            }
        ],
        "message": "HIVE-7738 : tez select sum(decimal) from union all of decimal and null throws NPE (Alexander Pivovarov, reviewed by Gopal V)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1619032 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/d5d12a008d659625665c24f43e982fb54289d54e",
        "patched_files": [
            "WritableConstantByteObjectInspector.java",
            "tez_union_decimal.java",
            "WritableConstantHiveDecimalObjectInspector.java",
            "WritableConstantShortObjectInspector.java",
            "WritableConstantLongObjectInspector.java",
            "WritableConstantIntObjectInspector.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "testconfiguration.java"
        ]
    },
    "hive_2706f22": {
        "bug_id": "hive_2706f22",
        "commit": "https://github.com/apache/hive/commit/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/itests/src/test/resources/testconfiguration.properties",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a",
                "deletions": 1,
                "filename": "itests/src/test/resources/testconfiguration.properties",
                "patch": "@@ -305,7 +305,8 @@ minitez.query.files=bucket_map_join_tez1.q,\\\n   tez_smb_main.q,\\\n   tez_smb_1.q,\\\n   vectorized_dynamic_partition_pruning.q,\\\n-  tez_multi_union.q\n+  tez_multi_union.q,\\\n+  tez_join.q\n \n encrypted.query.files=encryption_join_unencrypted_tbl.q,\\\n   encryption_insert_partition_static.q,\\",
                "raw_url": "https://github.com/apache/hive/raw/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/itests/src/test/resources/testconfiguration.properties",
                "sha": "91dcc03381d317decaad329031bb31b0b61e8bc4",
                "status": "modified"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/hive/blob/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java",
                "changes": 75,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java?ref=2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a",
                "deletions": 37,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java",
                "patch": "@@ -111,7 +111,7 @@\n         }\n \n         if (parentOp instanceof ReduceSinkOperator) {\n-          ReduceSinkOperator rs = (ReduceSinkOperator)parentOp;\n+          ReduceSinkOperator rs = (ReduceSinkOperator) parentOp;\n           estimatedBuckets = (estimatedBuckets < rs.getConf().getNumReducers()) ?\n               rs.getConf().getNumReducers() : estimatedBuckets;\n         }\n@@ -133,10 +133,10 @@\n       if (retval == null) {\n         return retval;\n       } else {\n-          // only case is full outer join with SMB enabled which is not possible. Convert to regular\n-          // join.\n-          convertJoinSMBJoin(joinOp, context, 0, 0, false, false);\n-          return null;\n+        // only case is full outer join with SMB enabled which is not possible. Convert to regular\n+        // join.\n+        convertJoinSMBJoin(joinOp, context, 0, 0, false, false);\n+        return null;\n       }\n     }\n \n@@ -160,8 +160,10 @@\n     }\n \n     MapJoinOperator mapJoinOp = convertJoinMapJoin(joinOp, context, mapJoinConversionPos);\n-    // map join operator by default has no bucket cols\n-    mapJoinOp.setOpTraits(new OpTraits(null, -1, null));\n+    // map join operator by default has no bucket cols and num of reduce sinks\n+    // reduced by 1\n+    mapJoinOp\n+        .setOpTraits(new OpTraits(null, -1, null, joinOp.getOpTraits().getNumReduceSinks()));\n     mapJoinOp.setStatistics(joinOp.getStatistics());\n     // propagate this change till the next RS\n     for (Operator<? extends OperatorDesc> childOp : mapJoinOp.getChildOperators()) {\n@@ -176,7 +178,8 @@ private Object checkAndConvertSMBJoin(OptimizeTezProcContext context, JoinOperat\n       TezBucketJoinProcCtx tezBucketJoinProcCtx) throws SemanticException {\n     // we cannot convert to bucket map join, we cannot convert to\n     // map join either based on the size. Check if we can convert to SMB join.\n-    if (context.conf.getBoolVar(HiveConf.ConfVars.HIVE_AUTO_SORTMERGE_JOIN) == false) {\n+    if ((context.conf.getBoolVar(HiveConf.ConfVars.HIVE_AUTO_SORTMERGE_JOIN) == false)\n+        || (joinOp.getOpTraits().getNumReduceSinks() >= 2)) {\n       convertJoinSMBJoin(joinOp, context, 0, 0, false, false);\n       return null;\n     }\n@@ -221,7 +224,7 @@ private Object checkAndConvertSMBJoin(OptimizeTezProcContext context, JoinOperat\n       convertJoinSMBJoin(joinOp, context, pos, 0, false, false);\n     }\n     return null;\n-}\n+  }\n \n   // replaces the join operator with a new CommonJoinOperator, removes the\n   // parent reduce sinks\n@@ -240,9 +243,9 @@ private void convertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcContext cont\n           new MapJoinDesc(\n                   MapJoinProcessor.getKeys(joinOp.getConf().isLeftInputJoin(),\n                   joinOp.getConf().getBaseSrc(), joinOp).getSecond(),\n-              null, joinDesc.getExprs(), null, null,\n-              joinDesc.getOutputColumnNames(), mapJoinConversionPos, joinDesc.getConds(),\n-              joinDesc.getFilters(), joinDesc.getNoOuterJoin(), null);\n+                  null, joinDesc.getExprs(), null, null,\n+                  joinDesc.getOutputColumnNames(), mapJoinConversionPos, joinDesc.getConds(),\n+                  joinDesc.getFilters(), joinDesc.getNoOuterJoin(), null);\n       mapJoinDesc.setNullSafes(joinDesc.getNullSafes());\n       mapJoinDesc.setFilterMap(joinDesc.getFilterMap());\n       mapJoinDesc.resetOrder();\n@@ -251,9 +254,9 @@ private void convertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcContext cont\n     CommonMergeJoinOperator mergeJoinOp =\n         (CommonMergeJoinOperator) OperatorFactory.get(new CommonMergeJoinDesc(numBuckets,\n             isSubQuery, mapJoinConversionPos, mapJoinDesc), joinOp.getSchema());\n-    OpTraits opTraits =\n-        new OpTraits(joinOp.getOpTraits().getBucketColNames(), numBuckets, joinOp.getOpTraits()\n-            .getSortCols());\n+    int numReduceSinks = joinOp.getOpTraits().getNumReduceSinks();\n+    OpTraits opTraits = new OpTraits(joinOp.getOpTraits().getBucketColNames(), numBuckets, joinOp\n+        .getOpTraits().getSortCols(), numReduceSinks);\n     mergeJoinOp.setOpTraits(opTraits);\n     mergeJoinOp.setStatistics(joinOp.getStatistics());\n \n@@ -289,8 +292,7 @@ private void convertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcContext cont\n \n     if (adjustParentsChildren) {\n       mergeJoinOp.getConf().setGenJoinKeys(true);\n-      List<Operator<? extends OperatorDesc>> newParentOpList =\n-          new ArrayList<Operator<? extends OperatorDesc>>();\n+      List<Operator<? extends OperatorDesc>> newParentOpList = new ArrayList<Operator<? extends OperatorDesc>>();\n       for (Operator<? extends OperatorDesc> parentOp : mergeJoinOp.getParentOperators()) {\n         for (Operator<? extends OperatorDesc> grandParentOp : parentOp.getParentOperators()) {\n           grandParentOp.getChildOperators().remove(parentOp);\n@@ -328,7 +330,8 @@ private void setAllChildrenTraitsToNull(Operator<? extends OperatorDesc> current\n     if (currentOp instanceof ReduceSinkOperator) {\n       return;\n     }\n-    currentOp.setOpTraits(new OpTraits(null, -1, null));\n+    currentOp.setOpTraits(new OpTraits(null, -1, null,\n+        currentOp.getOpTraits().getNumReduceSinks()));\n     for (Operator<? extends OperatorDesc> childOp : currentOp.getChildOperators()) {\n       if ((childOp instanceof ReduceSinkOperator) || (childOp instanceof GroupByOperator)) {\n         break;\n@@ -351,7 +354,7 @@ private boolean convertJoinBucketMapJoin(JoinOperator joinOp, OptimizeTezProcCon\n \n     // we can set the traits for this join operator\n     OpTraits opTraits = new OpTraits(joinOp.getOpTraits().getBucketColNames(),\n-        tezBucketJoinProcCtx.getNumBuckets(), null);\n+        tezBucketJoinProcCtx.getNumBuckets(), null, joinOp.getOpTraits().getNumReduceSinks());\n     mapJoinOp.setOpTraits(opTraits);\n     mapJoinOp.setStatistics(joinOp.getStatistics());\n     setNumberOfBucketsOnChildren(mapJoinOp);\n@@ -377,8 +380,7 @@ private boolean checkConvertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcCont\n \n     ReduceSinkOperator bigTableRS =\n         (ReduceSinkOperator) joinOp.getParentOperators().get(bigTablePosition);\n-    int numBuckets = bigTableRS.getParentOperators().get(0).getOpTraits()\n-            .getNumBuckets();\n+    int numBuckets = bigTableRS.getParentOperators().get(0).getOpTraits().getNumBuckets();\n \n     // the sort and bucket cols have to match on both sides for this\n     // transformation of the join operation\n@@ -425,13 +427,12 @@ private void setNumberOfBucketsOnChildren(Operator<? extends OperatorDesc> curre\n   }\n \n   /*\n-   * If the parent reduce sink of the big table side has the same emit key cols\n-   * as its parent, we can create a bucket map join eliminating the reduce sink.\n+   * If the parent reduce sink of the big table side has the same emit key cols as its parent, we\n+   * can create a bucket map join eliminating the reduce sink.\n    */\n   private boolean checkConvertJoinBucketMapJoin(JoinOperator joinOp,\n       OptimizeTezProcContext context, int bigTablePosition,\n-      TezBucketJoinProcCtx tezBucketJoinProcCtx)\n-  throws SemanticException {\n+      TezBucketJoinProcCtx tezBucketJoinProcCtx) throws SemanticException {\n     // bail on mux-operator because mux operator masks the emit keys of the\n     // constituent reduce sinks\n     if (!(joinOp.getParentOperators().get(0) instanceof ReduceSinkOperator)) {\n@@ -453,8 +454,8 @@ private boolean checkConvertJoinBucketMapJoin(JoinOperator joinOp,\n     }\n \n     /*\n-     * this is the case when the big table is a sub-query and is probably\n-     * already bucketed by the join column in say a group by operation\n+     * this is the case when the big table is a sub-query and is probably already bucketed by the\n+     * join column in say a group by operation\n      */\n     boolean isSubQuery = false;\n     if (numBuckets < 0) {\n@@ -492,7 +493,8 @@ private boolean checkColEquality(List<List<String>> grandParentColNames,\n           // all columns need to be at least a subset of the parentOfParent's bucket cols\n           ExprNodeDesc exprNodeDesc = colExprMap.get(colName);\n           if (exprNodeDesc instanceof ExprNodeColumnDesc) {\n-            if (((ExprNodeColumnDesc)exprNodeDesc).getColumn().equals(listBucketCols.get(colCount))) {\n+            if (((ExprNodeColumnDesc) exprNodeDesc).getColumn()\n+                .equals(listBucketCols.get(colCount))) {\n               colCount++;\n             } else {\n               break;\n@@ -562,14 +564,13 @@ public int getMapJoinConversionPos(JoinOperator joinOp, OptimizeTezProcContext c\n \n       Statistics currInputStat = parentOp.getStatistics();\n       if (currInputStat == null) {\n-        LOG.warn(\"Couldn't get statistics from: \"+parentOp);\n+        LOG.warn(\"Couldn't get statistics from: \" + parentOp);\n         return -1;\n       }\n \n       long inputSize = currInputStat.getDataSize();\n-      if ((bigInputStat == null) ||\n-          ((bigInputStat != null) &&\n-          (inputSize > bigInputStat.getDataSize()))) {\n+      if ((bigInputStat == null)\n+          || ((bigInputStat != null) && (inputSize > bigInputStat.getDataSize()))) {\n \n         if (bigTableFound) {\n           // cannot convert to map join; we've already chosen a big table\n@@ -639,11 +640,11 @@ public MapJoinOperator convertJoinMapJoin(JoinOperator joinOp, OptimizeTezProcCo\n       }\n     }\n \n-    //can safely convert the join to a map join.\n+    // can safely convert the join to a map join.\n     MapJoinOperator mapJoinOp =\n         MapJoinProcessor.convertJoinOpMapJoinOp(context.conf, joinOp,\n-                joinOp.getConf().isLeftInputJoin(), joinOp.getConf().getBaseSrc(),\n-                joinOp.getConf().getMapAliases(), bigTablePosition, true);\n+            joinOp.getConf().isLeftInputJoin(), joinOp.getConf().getBaseSrc(),\n+            joinOp.getConf().getMapAliases(), bigTablePosition, true);\n \n     Operator<? extends OperatorDesc> parentBigTableOp =\n         mapJoinOp.getParentOperators().get(bigTablePosition);\n@@ -667,7 +668,7 @@ public MapJoinOperator convertJoinMapJoin(JoinOperator joinOp, OptimizeTezProcCo\n             parentBigTableOp.getParentOperators().get(0));\n       }\n       parentBigTableOp.getParentOperators().get(0).removeChild(parentBigTableOp);\n-      for (Operator<? extends OperatorDesc> op : mapJoinOp.getParentOperators()) {\n+      for (Operator<? extends OperatorDesc>op : mapJoinOp.getParentOperators()) {\n         if (!(op.getChildOperators().contains(mapJoinOp))) {\n           op.getChildOperators().add(mapJoinOp);\n         }\n@@ -681,7 +682,7 @@ public MapJoinOperator convertJoinMapJoin(JoinOperator joinOp, OptimizeTezProcCo\n   private boolean hasDynamicPartitionBroadcast(Operator<?> parent) {\n     boolean hasDynamicPartitionPruning = false;\n \n-    for (Operator<?> op: parent.getChildOperators()) {\n+    for (Operator<?> op : parent.getChildOperators()) {\n       while (op != null) {\n         if (op instanceof AppMasterEventOperator && op.getConf() instanceof DynamicPruningEventDesc) {\n           // found dynamic partition pruning operator",
                "raw_url": "https://github.com/apache/hive/raw/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java",
                "sha": "ce4312029ebc0cc01d3cfcca60cc9cc10a8e0f51",
                "status": "modified"
            },
            {
                "additions": 62,
                "blob_url": "https://github.com/apache/hive/blob/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java",
                "changes": 96,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java?ref=2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a",
                "deletions": 34,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java",
                "patch": "@@ -82,7 +82,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n   }\n \n   /*\n-   * Reduce sink operator is the de-facto operator \n+   * Reduce sink operator is the de-facto operator\n    * for determining keyCols (emit keys of a map phase)\n    */\n   public static class ReduceSinkRule implements NodeProcessor {\n@@ -106,34 +106,37 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n       List<List<String>> listBucketCols = new ArrayList<List<String>>();\n       listBucketCols.add(bucketCols);\n       int numBuckets = -1;\n+      int numReduceSinks = 1;\n       OpTraits parentOpTraits = rs.getParentOperators().get(0).getConf().getOpTraits();\n       if (parentOpTraits != null) {\n         numBuckets = parentOpTraits.getNumBuckets();\n+        numReduceSinks += parentOpTraits.getNumReduceSinks();\n       }\n-      OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listBucketCols);\n+      OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listBucketCols, numReduceSinks);\n       rs.setOpTraits(opTraits);\n       return null;\n     }\n   }\n \n   /*\n-   * Table scan has the table object and pruned partitions that has information such as\n-   * bucketing, sorting, etc. that is used later for optimization.\n+   * Table scan has the table object and pruned partitions that has information\n+   * such as bucketing, sorting, etc. that is used later for optimization.\n    */\n   public static class TableScanRule implements NodeProcessor {\n \n-    public boolean checkBucketedTable(Table tbl, \n-        ParseContext pGraphContext,\n+    public boolean checkBucketedTable(Table tbl, ParseContext pGraphContext,\n         PrunedPartitionList prunedParts) throws SemanticException {\n \n       if (tbl.isPartitioned()) {\n         List<Partition> partitions = prunedParts.getNotDeniedPartns();\n         // construct a mapping of (Partition->bucket file names) and (Partition -> bucket number)\n         if (!partitions.isEmpty()) {\n           for (Partition p : partitions) {\n-            List<String> fileNames =\n-                AbstractBucketJoinProc.getBucketFilePathsOfPartition(p.getDataLocation(), pGraphContext);\n-            // The number of files for the table should be same as number of buckets.\n+            List<String> fileNames = \n+                AbstractBucketJoinProc.getBucketFilePathsOfPartition(p.getDataLocation(), \n+                    pGraphContext);\n+            // The number of files for the table should be same as number of\n+            // buckets.\n             int bucketCount = p.getBucketCount();\n \n             if (fileNames.size() != 0 && fileNames.size() != bucketCount) {\n@@ -143,8 +146,9 @@ public boolean checkBucketedTable(Table tbl,\n         }\n       } else {\n \n-        List<String> fileNames =\n-            AbstractBucketJoinProc.getBucketFilePathsOfPartition(tbl.getDataLocation(), pGraphContext);\n+        List<String> fileNames = \n+            AbstractBucketJoinProc.getBucketFilePathsOfPartition(tbl.getDataLocation(), \n+                pGraphContext);\n         Integer num = new Integer(tbl.getNumBuckets());\n \n         // The number of files for the table should be same as number of buckets.\n@@ -183,7 +187,8 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         }\n         sortedColsList.add(sortCols);\n       }\n-      OpTraits opTraits = new OpTraits(bucketColsList, numBuckets, sortedColsList);\n+      // num reduce sinks hardcoded to 0 because TS has no parents\n+      OpTraits opTraits = new OpTraits(bucketColsList, numBuckets, sortedColsList, 0);\n       ts.setOpTraits(opTraits);\n       return null;\n     }\n@@ -208,17 +213,22 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n       }\n \n       List<List<String>> listBucketCols = new ArrayList<List<String>>();\n+      int numReduceSinks = 0;\n+      OpTraits parentOpTraits = gbyOp.getParentOperators().get(0).getOpTraits();\n+      if (parentOpTraits != null) {\n+        numReduceSinks = parentOpTraits.getNumReduceSinks();\n+      }\n       listBucketCols.add(gbyKeys);\n-      OpTraits opTraits = new OpTraits(listBucketCols, -1, listBucketCols);\n+      OpTraits opTraits = new OpTraits(listBucketCols, -1, listBucketCols, numReduceSinks);\n       gbyOp.setOpTraits(opTraits);\n       return null;\n     }\n   }\n \n   public static class SelectRule implements NodeProcessor {\n \n-    public List<List<String>> getConvertedColNames(List<List<String>> parentColNames,\n-        SelectOperator selOp) {\n+    public List<List<String>> getConvertedColNames(\n+        List<List<String>> parentColNames, SelectOperator selOp) {\n       List<List<String>> listBucketCols = new ArrayList<List<String>>();\n       if (selOp.getColumnExprMap() != null) {\n         if (parentColNames != null) {\n@@ -244,8 +254,8 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n     @Override\n     public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         Object... nodeOutputs) throws SemanticException {\n-      SelectOperator selOp = (SelectOperator)nd;\n-      List<List<String>> parentBucketColNames =\n+      SelectOperator selOp = (SelectOperator) nd;\n+      List<List<String>> parentBucketColNames = \n           selOp.getParentOperators().get(0).getOpTraits().getBucketColNames();\n \n       List<List<String>> listBucketCols = null;\n@@ -254,18 +264,21 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         if (parentBucketColNames != null) {\n           listBucketCols = getConvertedColNames(parentBucketColNames, selOp);\n         }\n-        List<List<String>> parentSortColNames = selOp.getParentOperators().get(0).getOpTraits()\n-            .getSortCols();\n+        List<List<String>> parentSortColNames = \n+            selOp.getParentOperators().get(0).getOpTraits().getSortCols();\n         if (parentSortColNames != null) {\n           listSortCols = getConvertedColNames(parentSortColNames, selOp);\n         }\n       }\n \n       int numBuckets = -1;\n-      if (selOp.getParentOperators().get(0).getOpTraits() != null) {\n-        numBuckets = selOp.getParentOperators().get(0).getOpTraits().getNumBuckets();\n+      int numReduceSinks = 0;\n+      OpTraits parentOpTraits = selOp.getParentOperators().get(0).getOpTraits();\n+      if (parentOpTraits != null) {\n+        numBuckets = parentOpTraits.getNumBuckets();\n+        numReduceSinks = parentOpTraits.getNumReduceSinks();\n       }\n-      OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listSortCols);\n+      OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listSortCols, numReduceSinks);\n       selOp.setOpTraits(opTraits);\n       return null;\n     }\n@@ -276,26 +289,31 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n     @Override\n     public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         Object... nodeOutputs) throws SemanticException {\n-      JoinOperator joinOp = (JoinOperator)nd;\n+      JoinOperator joinOp = (JoinOperator) nd;\n       List<List<String>> bucketColsList = new ArrayList<List<String>>();\n       List<List<String>> sortColsList = new ArrayList<List<String>>();\n       byte pos = 0;\n+      int numReduceSinks = 0; // will be set to the larger of the parents\n       for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {\n         if (!(parentOp instanceof ReduceSinkOperator)) {\n           // can be mux operator\n           break;\n         }\n-        ReduceSinkOperator rsOp = (ReduceSinkOperator)parentOp;\n+        ReduceSinkOperator rsOp = (ReduceSinkOperator) parentOp;\n         if (rsOp.getOpTraits() == null) {\n           ReduceSinkRule rsRule = new ReduceSinkRule();\n           rsRule.process(rsOp, stack, procCtx, nodeOutputs);\n         }\n-        bucketColsList.add(getOutputColNames(joinOp, rsOp.getOpTraits().getBucketColNames(), pos));\n-        sortColsList.add(getOutputColNames(joinOp, rsOp.getOpTraits().getSortCols(), pos));\n+        OpTraits parentOpTraits = rsOp.getOpTraits();\n+        bucketColsList.add(getOutputColNames(joinOp, parentOpTraits.getBucketColNames(), pos));\n+        sortColsList.add(getOutputColNames(joinOp, parentOpTraits.getSortCols(), pos));\n+        if (parentOpTraits.getNumReduceSinks() > numReduceSinks) {\n+          numReduceSinks = parentOpTraits.getNumReduceSinks();\n+        }\n         pos++;\n       }\n \n-      joinOp.setOpTraits(new OpTraits(bucketColsList, -1, bucketColsList));\n+      joinOp.setOpTraits(new OpTraits(bucketColsList, -1, bucketColsList, numReduceSinks));\n       return null;\n     }\n \n@@ -311,7 +329,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         for (String colName : colNames) {\n           for (ExprNodeDesc exprNode : joinOp.getConf().getExprs().get(pos)) {\n             if (exprNode instanceof ExprNodeColumnDesc) {\n-              if(((ExprNodeColumnDesc)(exprNode)).getColumn().equals(colName)) {\n+              if (((ExprNodeColumnDesc) (exprNode)).getColumn().equals(colName)) {\n                 for (Entry<String, ExprNodeDesc> entry : joinOp.getColumnExprMap().entrySet()) {\n                   if (entry.getValue().isSame(exprNode)) {\n                     bucketColNames.add(entry.getKey());\n@@ -338,20 +356,30 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n   }\n \n   /*\n-   *  When we have operators that have multiple parents, it is not\n-   *  clear which parent's traits we need to propagate forward.\n+   * When we have operators that have multiple parents, it is not clear which\n+   * parent's traits we need to propagate forward.\n    */\n   public static class MultiParentRule implements NodeProcessor {\n \n     @Override\n     public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         Object... nodeOutputs) throws SemanticException {\n-      OpTraits opTraits = new OpTraits(null, -1, null);\n       @SuppressWarnings(\"unchecked\")\n-      Operator<? extends OperatorDesc> operator = (Operator<? extends OperatorDesc>)nd;\n+      Operator<? extends OperatorDesc> operator = (Operator<? extends OperatorDesc>) nd;\n+\n+      int numReduceSinks = 0;\n+      for (Operator<?> parentOp : operator.getParentOperators()) {\n+        if (parentOp.getOpTraits() == null) {\n+          continue;\n+        }\n+        if (parentOp.getOpTraits().getNumReduceSinks() > numReduceSinks) {\n+          numReduceSinks = parentOp.getOpTraits().getNumReduceSinks();\n+        }\n+      }\n+      OpTraits opTraits = new OpTraits(null, -1, null, numReduceSinks);\n       operator.setOpTraits(opTraits);\n       return null;\n-    } \n+    }\n   }\n \n   public static NodeProcessor getTableScanRule() {\n@@ -361,7 +389,7 @@ public static NodeProcessor getTableScanRule() {\n   public static NodeProcessor getReduceSinkRule() {\n     return new ReduceSinkRule();\n   }\n-  \n+\n   public static NodeProcessor getSelectRule() {\n     return new SelectRule();\n   }",
                "raw_url": "https://github.com/apache/hive/raw/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java",
                "sha": "62428dbdd61d28b63d59388f00b3c0c0d328d593",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java?ref=2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java",
                "patch": "@@ -103,7 +103,7 @@\n     }\n \n     // we can set the traits for this join operator\n-    OpTraits opTraits = new OpTraits(bucketColNames, numBuckets, null);\n+    OpTraits opTraits = new OpTraits(bucketColNames, numBuckets, null, joinOp.getOpTraits().getNumReduceSinks());\n     mapJoinOp.setOpTraits(opTraits);\n     mapJoinOp.setStatistics(joinOp.getStatistics());\n     setNumberOfBucketsOnChildren(mapJoinOp);",
                "raw_url": "https://github.com/apache/hive/raw/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java",
                "sha": "a455175efccf401b2286e32e731bc970de768f90",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java?ref=2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java",
                "patch": "@@ -25,11 +25,14 @@\n   List<List<String>> bucketColNames;\n   List<List<String>> sortColNames;\n   int numBuckets;\n+  int numReduceSinks;\n \n-  public OpTraits(List<List<String>> bucketColNames, int numBuckets, List<List<String>> sortColNames) {\n+  public OpTraits(List<List<String>> bucketColNames, int numBuckets,\n+      List<List<String>> sortColNames, int numReduceSinks) {\n     this.bucketColNames = bucketColNames;\n     this.numBuckets = numBuckets;\n     this.sortColNames = sortColNames;\n+    this.numReduceSinks = numReduceSinks;\n   }\n \n   public List<List<String>> getBucketColNames() {\n@@ -55,4 +58,12 @@ public void setSortColNames(List<List<String>> sortColNames) {\n   public List<List<String>> getSortCols() {\n     return sortColNames;\n   }\n+\n+  public void setNumReduceSinks(int numReduceSinks) {\n+    this.numReduceSinks = numReduceSinks;\n+  }\n+\n+  public int getNumReduceSinks() {\n+    return this.numReduceSinks;\n+  }\n }",
                "raw_url": "https://github.com/apache/hive/raw/2706f227b8ed7ac1e9baea4ffb0b3cc04b03431a/ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java",
                "sha": "a687a3d0b9ae439066a09550becc75be2bb2327e",
                "status": "modified"
            }
        ],
        "message": "HIVE-9886: Hive on tez: NPE when converting join to SMB in sub-query (Vikram Dixit K, reviewed by Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1665372 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/b63aeeebbc8f0f5570570b3790045bea75ed4848",
        "patched_files": [
            "OpTraits.java",
            "OpTraitsRulesProcFactory.java",
            "ConvertJoinMapJoin.java",
            "SparkMapJoinOptimizer.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "testconfiguration.java"
        ]
    },
    "hive_27f77f0": {
        "bug_id": "hive_27f77f0",
        "commit": "https://github.com/apache/hive/commit/27f77f054baf2e7cd1ea55282fbbb80c055f5aa9",
        "file": [
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hive/blob/27f77f054baf2e7cd1ea55282fbbb80c055f5aa9/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=27f77f054baf2e7cd1ea55282fbbb80c055f5aa9",
                "deletions": 11,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "patch": "@@ -11831,6 +11831,7 @@ public RelNode apply(RelOptCluster cluster, RelOptSchema relOptSchema, SchemaPlu\n       optiqPreCboPlan = applyPreCBOTransforms(optiqGenPlan, HiveDefaultRelMetadataProvider.INSTANCE);\n       List<RelMetadataProvider> list = Lists.newArrayList();\n       list.add(HiveDefaultRelMetadataProvider.INSTANCE);\n+      RelTraitSet desiredTraits = cluster.traitSetOf(HiveRel.CONVENTION, RelCollationImpl.EMPTY);\n \n       if (!HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_CBO_GREEDY_JOIN_ORDER)) {\n         planner.registerMetadataProviders(list);\n@@ -11846,18 +11847,15 @@ public RelNode apply(RelOptCluster cluster, RelOptSchema relOptSchema, SchemaPlu\n           planner.addRule(HivePullUpProjectsAboveJoinRule.LEFT_PROJECT);\n           planner.addRule(HivePullUpProjectsAboveJoinRule.RIGHT_PROJECT);\n           planner.addRule(HiveMergeProjectRule.INSTANCE);\n+        }\n \n-          RelTraitSet desiredTraits = cluster\n-              .traitSetOf(HiveRel.CONVENTION, RelCollationImpl.EMPTY);\n-\n-          RelNode rootRel = optiqPreCboPlan;\n-          if (!optiqPreCboPlan.getTraitSet().equals(desiredTraits)) {\n-            rootRel = planner.changeTraits(optiqPreCboPlan, desiredTraits);\n-          }\n-          planner.setRoot(rootRel);\n-\n-          optiqOptimizedPlan = planner.findBestExp();\n+        RelNode rootRel = optiqPreCboPlan;\n+        if (!optiqPreCboPlan.getTraitSet().equals(desiredTraits)) {\n+          rootRel = planner.changeTraits(optiqPreCboPlan, desiredTraits);\n         }\n+        planner.setRoot(rootRel);\n+\n+        optiqOptimizedPlan = planner.findBestExp();\n       } else {\n         final HepProgram hepPgm = new HepProgramBuilder().addMatchOrder(HepMatchOrder.BOTTOM_UP)\n             .addRuleInstance(new ConvertMultiJoinRule(HiveJoinRel.class))\n@@ -11869,7 +11867,12 @@ public RelNode apply(RelOptCluster cluster, RelOptSchema relOptSchema, SchemaPlu\n         RelMetadataProvider chainedProvider = ChainedRelMetadataProvider.of(list);\n         cluster.setMetadataProvider(new CachingRelMetadataProvider(chainedProvider, hepPlanner));\n \n-        hepPlanner.setRoot(optiqPreCboPlan);\n+        RelNode rootRel = optiqPreCboPlan;\n+        if (!optiqPreCboPlan.getTraitSet().equals(desiredTraits)) {\n+          rootRel = hepPlanner.changeTraits(optiqPreCboPlan, desiredTraits);\n+        }\n+        hepPlanner.setRoot(rootRel);\n+\n         optiqOptimizedPlan = hepPlanner.findBestExp();\n       }\n ",
                "raw_url": "https://github.com/apache/hive/raw/27f77f054baf2e7cd1ea55282fbbb80c055f5aa9/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "sha": "3da2491f5198f3eed9d119545fec81dc72c7f7dc",
                "status": "modified"
            }
        ],
        "message": "HIVE-7515 Fix NPE in CBO (Laljo John Pullokkaran via Harish Butani)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/cbo@1613482 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/60be7236a80f57cc23a1a9d0dc8b00062ca5397e",
        "patched_files": [
            "SemanticAnalyzer.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestSemanticAnalyzer.java"
        ]
    },
    "hive_2d87c12": {
        "bug_id": "hive_2d87c12",
        "commit": "https://github.com/apache/hive/commit/2d87c12d914044540a4f5ed7fe500e3c245fbead",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/2d87c12d914044540a4f5ed7fe500e3c245fbead/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=2d87c12d914044540a4f5ed7fe500e3c245fbead",
                "deletions": 2,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "patch": "@@ -2024,8 +2024,14 @@ public boolean equals(Object obj) {\n         if (!p1.isSetValues() || !p2.isSetValues()) return p1.isSetValues() == p2.isSetValues();\n         if (p1.getValues().size() != p2.getValues().size()) return false;\n         for (int i = 0; i < p1.getValues().size(); ++i) {\n-          String v1 = p1.getValues().get(i), v2 = p2.getValues().get(i);\n-          if ((v1 == null && v2 != null) || !v1.equals(v2)) return false;\n+          String v1 = p1.getValues().get(i);\n+          String v2 = p2.getValues().get(i);\n+          if (v1 == null && v2 == null) {\n+            continue;\n+          }\n+          if (v1 == null || !v1.equals(v2)) {\n+            return false;\n+          }\n         }\n         return true;\n       }",
                "raw_url": "https://github.com/apache/hive/raw/2d87c12d914044540a4f5ed7fe500e3c245fbead/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "sha": "e7960487680dd26059ee512dd28b66c90848aa57",
                "status": "modified"
            }
        ],
        "message": "HIVE-10590 fix potential NPE in HiveMetaStore.equals (Alexander Pivovarov, reviewed by Ashutosh Chauhan)",
        "parent": "https://github.com/apache/hive/commit/bc0138c436add2335d2045b6c7bf86bc6a15cc27",
        "patched_files": [
            "HiveMetaStore.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveMetaStore.java"
        ]
    },
    "hive_2f686d4": {
        "bug_id": "hive_2f686d4",
        "commit": "https://github.com/apache/hive/commit/2f686d4c0c20540079660de202c619e42ed5cd4f",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/2f686d4c0c20540079660de202c619e42ed5cd4f/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFMapValues.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFMapValues.java?ref=2f686d4c0c20540079660de202c619e42ed5cd4f",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFMapValues.java",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.hive.ql.udf.generic;\n \n import java.util.ArrayList;\n+import java.util.Map;\n \n import org.apache.hadoop.hive.ql.exec.Description;\n import org.apache.hadoop.hive.ql.exec.UDFArgumentException;\n@@ -61,7 +62,10 @@ public ObjectInspector initialize(ObjectInspector[] arguments)\n   public Object evaluate(DeferredObject[] arguments) throws HiveException {\n     retArray.clear();\n     Object mapObj = arguments[0].get();\n-    retArray.addAll(mapOI.getMap(mapObj).values());\n+    Map<?, ?> map = mapOI.getMap(mapObj);\n+    if (map != null) {\n+      retArray.addAll(map.values());\n+    }\n     return retArray;\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/2f686d4c0c20540079660de202c619e42ed5cd4f/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFMapValues.java",
                "sha": "3bd5864499ea7d9e4d6e8e7f2284ec96f445ad06",
                "status": "modified"
            },
            {
                "additions": 56,
                "blob_url": "https://github.com/apache/hive/blob/2f686d4c0c20540079660de202c619e42ed5cd4f/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFMapValues.java",
                "changes": 56,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFMapValues.java?ref=2f686d4c0c20540079660de202c619e42ed5cd4f",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFMapValues.java",
                "patch": "@@ -0,0 +1,56 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.udf.generic;\n+\n+import java.io.IOException;\n+import java.util.Map;\n+\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredJavaObject;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredObject;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.objectinspector.StandardListObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+public class TestGenericUDFMapValues {\n+\n+  @Test\n+  public void testNullMap() throws HiveException, IOException {\n+    ObjectInspector[] inputOIs = {\n+        ObjectInspectorFactory.getStandardMapObjectInspector(\n+            PrimitiveObjectInspectorFactory.writableStringObjectInspector,\n+            PrimitiveObjectInspectorFactory.writableStringObjectInspector),\n+    };\n+\n+    Map<String, String> input = null;\n+    DeferredObject[] args = {\n+        new DeferredJavaObject(input)\n+    };\n+\n+  GenericUDFMapValues udf = new GenericUDFMapValues();\n+    StandardListObjectInspector oi = (StandardListObjectInspector) udf.initialize(inputOIs);\n+    Object res = udf.evaluate(args);\n+    Assert.assertTrue(oi.getList(res).isEmpty());\n+    udf.close();\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/hive/raw/2f686d4c0c20540079660de202c619e42ed5cd4f/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFMapValues.java",
                "sha": "44676ed6f877666b89f2f3e7109a5d924597e901",
                "status": "added"
            }
        ],
        "message": "HIVE-14617: NPE in UDF MapValues() if input is null (reviewed by Chao)",
        "parent": "https://github.com/apache/hive/commit/9343fee5d10ab5ab64692d9723a6c35e77adefc3",
        "patched_files": [
            "GenericUDFMapValues.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestGenericUDFMapValues.java"
        ]
    },
    "hive_2f7abcc": {
        "bug_id": "hive_2f7abcc",
        "commit": "https://github.com/apache/hive/commit/2f7abcc6039ca4fddda2c90d5d0184c70c663614",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/2f7abcc6039ca4fddda2c90d5d0184c70c663614/jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcInputFormat.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcInputFormat.java?ref=2f7abcc6039ca4fddda2c90d5d0184c70c663614",
                "deletions": 1,
                "filename": "jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcInputFormat.java",
                "patch": "@@ -132,7 +132,7 @@\n         intervals.get(intervals.size()-1).setRight(null);\n         splits = new InputSplit[intervals.size()];\n         for (int i = 0; i < intervals.size(); i++) {\n-          splits[i] = new JdbcInputSplit(partitionColumn, intervals.get(i).getLeft(), intervals.get(i).getRight());\n+          splits[i] = new JdbcInputSplit(partitionColumn, intervals.get(i).getLeft(), intervals.get(i).getRight(), tablePaths[0]);\n         }\n       } else {\n         int numRecords = dbAccessor.getTotalNumberOfRecords(job);",
                "raw_url": "https://github.com/apache/hive/raw/2f7abcc6039ca4fddda2c90d5d0184c70c663614/jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcInputFormat.java",
                "sha": "14c5a777965b7158092331e06610e08b9f649b94",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hive/blob/2f7abcc6039ca4fddda2c90d5d0184c70c663614/jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcInputSplit.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcInputSplit.java?ref=2f7abcc6039ca4fddda2c90d5d0184c70c663614",
                "deletions": 9,
                "filename": "jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcInputSplit.java",
                "patch": "@@ -32,7 +32,6 @@\n   private String lowerBound = null;\n   private String upperBound = null;\n \n-\n   public JdbcInputSplit() {\n     super(null, 0, 0, EMPTY_ARRAY);\n     this.limit = -1;\n@@ -51,14 +50,8 @@ public JdbcInputSplit(int limit, int offset, Path dummyPath) {\n     this.offset = offset;\n   }\n \n-  public JdbcInputSplit(int limit, int offset) {\n-    super(null, 0, 0, EMPTY_ARRAY);\n-    this.limit = limit;\n-    this.offset = offset;\n-  }\n-\n-  public JdbcInputSplit(String partitionColumn, String lowerBound, String upperBound) {\n-    super(null, 0, 0, EMPTY_ARRAY);\n+  public JdbcInputSplit(String partitionColumn, String lowerBound, String upperBound, Path dummyPath) {\n+    super(dummyPath, 0, 0, EMPTY_ARRAY);\n     this.partitionColumn = partitionColumn;\n     this.lowerBound = lowerBound;\n     this.upperBound = upperBound;\n@@ -72,7 +65,17 @@ public void write(DataOutput out) throws IOException {\n     if (partitionColumn != null) {\n       out.writeBoolean(true);\n       out.writeUTF(partitionColumn);\n+    } else {\n+      out.writeBoolean(false);\n+    }\n+    if (lowerBound != null) {\n+      out.writeBoolean(true);\n       out.writeUTF(lowerBound);\n+    } else {\n+      out.writeBoolean(false);\n+    }\n+    if (upperBound != null) {\n+      out.writeBoolean(true);\n       out.writeUTF(upperBound);\n     } else {\n       out.writeBoolean(false);\n@@ -88,7 +91,13 @@ public void readFields(DataInput in) throws IOException {\n     boolean partitionColumnExists = in.readBoolean();\n     if (partitionColumnExists) {\n       partitionColumn = in.readUTF();\n+    }\n+    boolean lowerBoundExists = in.readBoolean();\n+    if (lowerBoundExists) {\n       lowerBound = in.readUTF();\n+    }\n+    boolean upperBoundExists = in.readBoolean();\n+    if (upperBoundExists) {\n       upperBound = in.readUTF();\n     }\n   }",
                "raw_url": "https://github.com/apache/hive/raw/2f7abcc6039ca4fddda2c90d5d0184c70c663614/jdbc-handler/src/main/java/org/apache/hive/storage/jdbc/JdbcInputSplit.java",
                "sha": "e591413aec6c5686c97693d8227d34bed2e68ca0",
                "status": "modified"
            }
        ],
        "message": "HIVE-20829: JdbcStorageHandler range split throws NPE (Daniel Dai, reviewed by Thejas Nair)\n\nSigned-off-by: Thejas M Nair <thejas@hortonworks.com>",
        "parent": "https://github.com/apache/hive/commit/1656e1bd892bb47e50ee8352813b0dab6f230bb4",
        "patched_files": [
            "JdbcInputFormat.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestJdbcInputFormat.java"
        ]
    },
    "hive_2fc11e4": {
        "bug_id": "hive_2fc11e4",
        "commit": "https://github.com/apache/hive/commit/2fc11e41e110f43f674426b3786387ba80eaa0e0",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/2fc11e41e110f43f674426b3786387ba80eaa0e0/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/schema/HCatSchema.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/schema/HCatSchema.java?ref=2fc11e41e110f43f674426b3786387ba80eaa0e0",
                "deletions": 7,
                "filename": "hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/schema/HCatSchema.java",
                "patch": "@@ -58,9 +58,9 @@ public HCatSchema(final List<HCatFieldSchema> fieldSchemas) {\n       if (field == null)\n         throw new IllegalArgumentException(\"Field cannot be null\");\n \n-      String fieldName = field.getName();\n+      String fieldName = normalizeName(field.getName());\n       if (fieldPositionMap.containsKey(fieldName))\n-        throw new IllegalArgumentException(\"Field named \" + fieldName +\n+        throw new IllegalArgumentException(\"Field named \" + field.getName() +\n           \" already exists\");\n       fieldPositionMap.put(fieldName, idx);\n       fieldNames.add(fieldName);\n@@ -72,7 +72,7 @@ public void append(final HCatFieldSchema hfs) throws HCatException {\n     if (hfs == null)\n       throw new HCatException(\"Attempt to append null HCatFieldSchema in HCatSchema.\");\n \n-    String fieldName = hfs.getName();\n+    String fieldName = normalizeName(hfs.getName());\n     if (fieldPositionMap.containsKey(fieldName))\n       throw new HCatException(\"Attempt to append HCatFieldSchema with already \" +\n         \"existing name: \" + fieldName + \".\");\n@@ -98,7 +98,7 @@ public void append(final HCatFieldSchema hfs) throws HCatException {\n    * present, returns null.\n    */\n   public Integer getPosition(String fieldName) {\n-    return fieldPositionMap.get(fieldName);\n+    return fieldPositionMap.get(normalizeName(fieldName));\n   }\n \n   public HCatFieldSchema get(String fieldName) throws HCatException {\n@@ -134,9 +134,14 @@ public void remove(final HCatFieldSchema hcatFieldSchema) throws HCatException {\n     }     \n     fieldSchemas.remove(hcatFieldSchema);\n     // Re-align the positionMap by -1 for the columns appearing after hcatFieldSchema.\n-    reAlignPositionMap(fieldPositionMap.get(hcatFieldSchema.getName())+1, -1);\n-    fieldPositionMap.remove(hcatFieldSchema.getName());\n-    fieldNames.remove(hcatFieldSchema.getName());\n+    String fieldName = normalizeName(hcatFieldSchema.getName());\n+    reAlignPositionMap(fieldPositionMap.get(fieldName)+1, -1);\n+    fieldPositionMap.remove(fieldName);\n+    fieldNames.remove(fieldName);\n+  }\n+\n+  private String normalizeName(String name) {\n+    return name == null ? null : name.toLowerCase();\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hive/raw/2fc11e41e110f43f674426b3786387ba80eaa0e0/hcatalog/core/src/main/java/org/apache/hive/hcatalog/data/schema/HCatSchema.java",
                "sha": "c0209dbf90f29808e21897db79c7e155b90445df",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hive/blob/2fc11e41e110f43f674426b3786387ba80eaa0e0/hcatalog/core/src/test/java/org/apache/hive/hcatalog/data/TestJsonSerDe.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/core/src/test/java/org/apache/hive/hcatalog/data/TestJsonSerDe.java?ref=2fc11e41e110f43f674426b3786387ba80eaa0e0",
                "deletions": 0,
                "filename": "hcatalog/core/src/test/java/org/apache/hive/hcatalog/data/TestJsonSerDe.java",
                "patch": "@@ -22,6 +22,7 @@\n import java.sql.Date;\n import java.sql.Timestamp;\n import java.util.ArrayList;\n+import java.util.Arrays;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n@@ -287,4 +288,23 @@ public void testLooseJsonReadability() throws Exception {\n \n   }\n \n+  public void testUpperCaseKey() throws Exception {\n+    Configuration conf = new Configuration();\n+    Properties props = new Properties();\n+\n+    props.put(serdeConstants.LIST_COLUMNS, \"empid,name\");\n+    props.put(serdeConstants.LIST_COLUMN_TYPES, \"int,string\");\n+    JsonSerDe rjsd = new JsonSerDe();\n+    SerDeUtils.initializeSerDe(rjsd, conf, props, null);\n+\n+    Text text1 = new Text(\"{ \\\"empId\\\" : 123, \\\"name\\\" : \\\"John\\\" } \");\n+    Text text2 = new Text(\"{ \\\"empId\\\" : 456, \\\"name\\\" : \\\"Jane\\\" } \");\n+\n+    HCatRecord expected1 = new DefaultHCatRecord(Arrays.<Object>asList(123, \"John\"));\n+    HCatRecord expected2 = new DefaultHCatRecord(Arrays.<Object>asList(456, \"Jane\"));\n+\n+    assertTrue(HCatDataCheckUtil.recordsEqual((HCatRecord)rjsd.deserialize(text1), expected1));\n+    assertTrue(HCatDataCheckUtil.recordsEqual((HCatRecord)rjsd.deserialize(text2), expected2));\n+\n+  }\n }",
                "raw_url": "https://github.com/apache/hive/raw/2fc11e41e110f43f674426b3786387ba80eaa0e0/hcatalog/core/src/test/java/org/apache/hive/hcatalog/data/TestJsonSerDe.java",
                "sha": "b4a810a394a4f67d6cf134fad4ff27cda3e97f12",
                "status": "modified"
            }
        ],
        "message": "HIVE-7075 - JsonSerde raises NullPointerException when object key is not lower case (Yibing Shi, Navis Ryu via Sushanth Sowmyan)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1601499 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/ec1097ccf8b333ea183c3d7f87e20db2fb7d7bf4",
        "patched_files": [
            "HCatSchema.java",
            "JsonSerDe.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHCatSchema.java",
            "TestJsonSerDe.java"
        ]
    },
    "hive_309cb0d": {
        "bug_id": "hive_309cb0d",
        "commit": "https://github.com/apache/hive/commit/309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java?ref=309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java",
                "patch": "@@ -471,6 +471,8 @@\n   INVALID_JOIN_CONDITION(10407, \"Error parsing condition in join\"),\n   INVALID_TARGET_COLUMN_IN_SET_CLAUSE(10408, \"Target column \\\"{0}\\\" of set clause is not found in table \\\"{1}\\\".\", true),\n   HIVE_GROUPING_FUNCTION_EXPR_NOT_IN_GROUPBY(10409, \"Expression in GROUPING function not present in GROUP BY\"),\n+  ALTER_TABLE_NON_PARTITIONED_TABLE_CASCADE_NOT_SUPPORTED(10410,\n+      \"Alter table with non-partitioned table does not support cascade\"),\n   //========================== 20000 range starts here ========================//\n   SCRIPT_INIT_ERROR(20000, \"Unable to initialize custom script.\"),\n   SCRIPT_IO_ERROR(20001, \"An error occurred while reading or writing to your custom script. \"",
                "raw_url": "https://github.com/apache/hive/raw/309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java",
                "sha": "226ba1895fc1b7d613083887c9bd79ea883c32ec",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java?ref=309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java",
                "patch": "@@ -1494,6 +1494,12 @@ private void addInputsOutputsAlterTable(String tableName, Map<String, String> pa\n     }\n \n     Table tab = getTable(tableName, true);\n+    // cascade only occurs with partitioned table\n+    if (isCascade && !tab.isPartitioned()) {\n+      throw new SemanticException(\n+          ErrorMsg.ALTER_TABLE_NON_PARTITIONED_TABLE_CASCADE_NOT_SUPPORTED);\n+    }\n+\n     // Determine the lock type to acquire\n     WriteEntity.WriteType writeType = WriteEntity.determineAlterTableWriteType(op);\n ",
                "raw_url": "https://github.com/apache/hive/raw/309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java",
                "sha": "55f07afabace4f836deab2f4505fad9112552e78",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6/ql/src/test/queries/clientnegative/alter_table_non_partitioned_table_cascade.q",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/alter_table_non_partitioned_table_cascade.q?ref=309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientnegative/alter_table_non_partitioned_table_cascade.q",
                "patch": "@@ -0,0 +1,4 @@\n+drop table if exists alter_table_non_partition_cascade;\n+create table alter_table_non_partitioned_cascade(c1 string, c2 string);\n+describe alter_table_non_partitioned_cascade;\n+alter table alter_table_non_partitioned_cascade add columns (c3 string) cascade;\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hive/raw/309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6/ql/src/test/queries/clientnegative/alter_table_non_partitioned_table_cascade.q",
                "sha": "47ce383c8c656f3d648034e931d7f0a30608a06d",
                "status": "added"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/hive/blob/309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6/ql/src/test/results/clientnegative/alter_table_non_partitioned_table_cascade.q.out",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/alter_table_non_partitioned_table_cascade.q.out?ref=309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6",
                "deletions": 0,
                "filename": "ql/src/test/results/clientnegative/alter_table_non_partitioned_table_cascade.q.out",
                "patch": "@@ -0,0 +1,21 @@\n+PREHOOK: query: drop table if exists alter_table_non_partition_cascade\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: drop table if exists alter_table_non_partition_cascade\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: create table alter_table_non_partitioned_cascade(c1 string, c2 string)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@alter_table_non_partitioned_cascade\n+POSTHOOK: query: create table alter_table_non_partitioned_cascade(c1 string, c2 string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@alter_table_non_partitioned_cascade\n+PREHOOK: query: describe alter_table_non_partitioned_cascade\n+PREHOOK: type: DESCTABLE\n+PREHOOK: Input: default@alter_table_non_partitioned_cascade\n+POSTHOOK: query: describe alter_table_non_partitioned_cascade\n+POSTHOOK: type: DESCTABLE\n+POSTHOOK: Input: default@alter_table_non_partitioned_cascade\n+c1                  \tstring              \t                    \n+c2                  \tstring              \t                    \n+FAILED: SemanticException [Error 10410]: Alter table with non-partitioned table does not support cascade",
                "raw_url": "https://github.com/apache/hive/raw/309cb0d432653be4ef8b1028ccc4ed5d5f5ae7e6/ql/src/test/results/clientnegative/alter_table_non_partitioned_table_cascade.q.out",
                "sha": "afa648dfde19b83762abc557b5b7cb10a98fa901",
                "status": "added"
            }
        ],
        "message": "HIVE-16877 : NPE when issue query like alter table ... cascade onto non-partitioned table (Wang Haihua via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>",
        "parent": "https://github.com/apache/hive/commit/db21c3ff6c09ca920b9406efe95694d110219483",
        "patched_files": [
            "ErrorMsg.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestErrorMsg.java"
        ]
    },
    "hive_31591bf": {
        "bug_id": "hive_31591bf",
        "commit": "https://github.com/apache/hive/commit/31591bf1833fb3858c599addbe53aa57ac381ba4",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/31591bf1833fb3858c599addbe53aa57ac381ba4/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/InternalUtil.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/InternalUtil.java?ref=31591bf1833fb3858c599addbe53aa57ac381ba4",
                "deletions": 0,
                "filename": "hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/InternalUtil.java",
                "patch": "@@ -164,6 +164,8 @@ private static Properties getSerdeProperties(HCatTableInfo info, HCatSchema s)\n       MetaStoreUtils.getColumnNamesFromFieldSchema(fields));\n     props.setProperty(org.apache.hadoop.hive.serde.serdeConstants.LIST_COLUMN_TYPES,\n       MetaStoreUtils.getColumnTypesFromFieldSchema(fields));\n+    props.setProperty(\"columns.comments\",\n+      MetaStoreUtils.getColumnCommentsFromFieldSchema(fields));\n \n     // setting these props to match LazySimpleSerde\n     props.setProperty(org.apache.hadoop.hive.serde.serdeConstants.SERIALIZATION_NULL_FORMAT, \"\\\\N\");",
                "raw_url": "https://github.com/apache/hive/raw/31591bf1833fb3858c599addbe53aa57ac381ba4/hcatalog/core/src/main/java/org/apache/hive/hcatalog/mapreduce/InternalUtil.java",
                "sha": "31001814097966b2b40ccebd4300322939e38ea5",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hive/blob/31591bf1833fb3858c599addbe53aa57ac381ba4/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java?ref=31591bf1833fb3858c599addbe53aa57ac381ba4",
                "deletions": 0,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java",
                "patch": "@@ -1115,6 +1115,17 @@ public static String getColumnTypesFromFieldSchema(\n     return sb.toString();\n   }\n \n+  public static String getColumnCommentsFromFieldSchema(List<FieldSchema> fieldSchemas) {\n+    StringBuilder sb = new StringBuilder();\n+    for (int i = 0; i < fieldSchemas.size(); i++) {\n+      if (i > 0) {\n+        sb.append(SerDeUtils.COLUMN_COMMENTS_DELIMITER);\n+      }\n+      sb.append(fieldSchemas.get(i).getComment());\n+    }\n+    return sb.toString();\n+  }\n+\n   public static void makeDir(Path path, HiveConf hiveConf) throws MetaException {\n     FileSystem fs;\n     try {",
                "raw_url": "https://github.com/apache/hive/raw/31591bf1833fb3858c599addbe53aa57ac381ba4/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java",
                "sha": "38dc4062f8c5f485618f67a38c9c58b742d57438",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/31591bf1833fb3858c599addbe53aa57ac381ba4/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java?ref=31591bf1833fb3858c599addbe53aa57ac381ba4",
                "deletions": 0,
                "filename": "serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java",
                "patch": "@@ -62,6 +62,7 @@\n   public static final char QUOTE = '\"';\n   public static final char COLON = ':';\n   public static final char COMMA = ',';\n+  public static final char COLUMN_COMMENTS_DELIMITER = '\\0';\n   public static final String LBRACKET = \"[\";\n   public static final String RBRACKET = \"]\";\n   public static final String LBRACE = \"{\";",
                "raw_url": "https://github.com/apache/hive/raw/31591bf1833fb3858c599addbe53aa57ac381ba4/serde/src/java/org/apache/hadoop/hive/serde2/SerDeUtils.java",
                "sha": "8dada5a324545bf80e26c5766545ff735f7fce40",
                "status": "modified"
            }
        ],
        "message": "HIVE-10428 : NPE in RegexSerDe using HCat (Jason Dere via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>",
        "parent": "https://github.com/apache/hive/commit/aaa112dbfd0b52285925dad4160f34487b8dfd82",
        "patched_files": [
            "MetaStoreUtils.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestMetaStoreUtils.java"
        ]
    },
    "hive_348a592": {
        "bug_id": "hive_348a592",
        "commit": "https://github.com/apache/hive/commit/348a592154be07045bd58ff23134dc78e95de16a",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/348a592154be07045bd58ff23134dc78e95de16a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java?ref=348a592154be07045bd58ff23134dc78e95de16a",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java",
                "patch": "@@ -480,7 +480,7 @@ protected TezSessionPoolSession createSession(String sessionId) {\n    */\n   private static boolean canWorkWithSameSession(TezSessionState session, HiveConf conf)\n        throws HiveException {\n-    if (session == null || conf == null) {\n+    if (session == null || conf == null || !session.isOpen()) {\n       return false;\n     }\n ",
                "raw_url": "https://github.com/apache/hive/raw/348a592154be07045bd58ff23134dc78e95de16a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java",
                "sha": "8f459473b0ab2b788abdd7806fe8ac1eaa82944a",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/348a592154be07045bd58ff23134dc78e95de16a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java?ref=348a592154be07045bd58ff23134dc78e95de16a",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "patch": "@@ -137,6 +137,9 @@ public int execute(DriverContext driverContext) {\n       // Need to remove this static hack. But this is the way currently to get a session.\n       SessionState ss = SessionState.get();\n       session = ss.getTezSession();\n+      if (session != null && !session.isOpen()) {\n+        LOG.warn(\"The session: \" + session + \" has not been opened\");\n+      }\n       session = TezSessionPoolManager.getInstance().getSession(\n           session, conf, false, getWork().getLlapMode());\n       ss.setTezSession(session);",
                "raw_url": "https://github.com/apache/hive/raw/348a592154be07045bd58ff23134dc78e95de16a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "sha": "6c8bf29ce2ad6097824a931205e0dc4d9a337910",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/348a592154be07045bd58ff23134dc78e95de16a/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/SampleTezSessionState.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/SampleTezSessionState.java?ref=348a592154be07045bd58ff23134dc78e95de16a",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/exec/tez/SampleTezSessionState.java",
                "patch": "@@ -65,6 +65,7 @@ public void open(HiveConf conf) throws IOException, LoginException, URISyntaxExc\n     UserGroupInformation ugi = Utils.getUGI();\n     user = ugi.getShortUserName();\n     this.doAsEnabled = conf.getBoolVar(HiveConf.ConfVars.HIVE_SERVER2_ENABLE_DOAS);\n+    setOpen(true);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hive/raw/348a592154be07045bd58ff23134dc78e95de16a/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/SampleTezSessionState.java",
                "sha": "2d1c687a4e9cc86622a682f9a2099b5a7d061de4",
                "status": "modified"
            }
        ],
        "message": "HIVE-16114 : NullPointerException in TezSessionPoolManager when getting the session (Zhihua Deng, reviewed by Sergey Shelukhin)",
        "parent": "https://github.com/apache/hive/commit/bfe930c5c3ee03180a0d78aaa12f83af13973b44",
        "patched_files": [
            "TezTask.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestTezTask.java"
        ]
    },
    "hive_358242f": {
        "bug_id": "hive_358242f",
        "commit": "https://github.com/apache/hive/commit/358242f905bf0e94282e03c919850f63f4a7a7be",
        "file": [
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/hive/blob/358242f905bf0e94282e03c919850f63f4a7a7be/ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java?ref=358242f905bf0e94282e03c919850f63f4a7a7be",
                "deletions": 9,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java",
                "patch": "@@ -382,7 +382,7 @@ protected ASTNode firstDot(ASTNode dot) {\n     /*\n      * row resolver of the SubQuery.\n      * Set by the SemanticAnalyzer after the Plan for the SubQuery is genned.\n-     * This is neede in case the SubQuery select list contains a TOK_ALLCOLREF\n+     * This is needed in case the SubQuery select list contains a TOK_ALLCOLREF\n      */\n     RowResolver sqRR;\n \n@@ -513,7 +513,10 @@ void validateAndRewriteAST(RowResolver outerQueryRR,\n       String outerQueryAlias,\n       Set<String> outerQryAliases) throws SemanticException {\n \n-    ASTNode selectClause = (ASTNode) subQueryAST.getChild(1).getChild(1);\n+    ASTNode fromClause = getChildFromSubqueryAST(\"From\", HiveParser.TOK_FROM);\n+    ASTNode insertClause = getChildFromSubqueryAST(\"Insert\", HiveParser.TOK_INSERT);\n+\n+    ASTNode selectClause = (ASTNode) insertClause.getChild(1);\n \n     int selectExprStart = 0;\n     if ( selectClause.getChild(0).getType() == HiveParser.TOK_HINTLIST ) {\n@@ -537,15 +540,15 @@ void validateAndRewriteAST(RowResolver outerQueryRR,\n      * Restriction 17.s :: SubQuery cannot use the same table alias as one used in\n      * the Outer Query.\n      */\n-    List<String> sqAliases = SubQueryUtils.getTableAliasesInSubQuery(this);\n+    List<String> sqAliases = SubQueryUtils.getTableAliasesInSubQuery(fromClause);\n     String sharedAlias = null;\n     for(String s : sqAliases ) {\n       if ( outerQryAliases.contains(s) ) {\n         sharedAlias = s;\n       }\n     }\n     if ( sharedAlias != null) {\n-      ASTNode whereClause = SubQueryUtils.subQueryWhere(subQueryAST);\n+      ASTNode whereClause = SubQueryUtils.subQueryWhere(insertClause);\n \n       if ( whereClause != null ) {\n         ASTNode u = SubQueryUtils.hasUnQualifiedColumnReferences(whereClause);\n@@ -581,7 +584,7 @@ void validateAndRewriteAST(RowResolver outerQueryRR,\n       containsAggregationExprs = containsAggregationExprs | ( r == 1 );\n     }\n \n-    rewrite(outerQueryRR, forHavingClause, outerQueryAlias);\n+    rewrite(outerQueryRR, forHavingClause, outerQueryAlias, insertClause, selectClause);\n \n     SubQueryUtils.setOriginDeep(subQueryAST, originalSQASTOrigin);\n \n@@ -631,6 +634,16 @@ void validateAndRewriteAST(RowResolver outerQueryRR,\n \n   }\n \n+  private ASTNode getChildFromSubqueryAST(String errorMsg, int type) throws SemanticException {\n+    ASTNode childAST = (ASTNode) subQueryAST.getFirstChildWithType(type);\n+    if (childAST == null && errorMsg != null) {\n+      subQueryAST.setOrigin(originalSQASTOrigin);\n+      throw new SemanticException(ErrorMsg.INVALID_SUBQUERY_EXPRESSION.getMsg(\n+          subQueryAST, errorMsg + \" clause is missing in SubQuery.\"));\n+    }\n+    return childAST;\n+  }\n+\n   private void setJoinType() {\n     if ( operator.getType() == SubQueryType.NOT_IN ||\n         operator.getType() == SubQueryType.NOT_EXISTS ) {\n@@ -744,7 +757,7 @@ String getNextCorrExprAlias() {\n    *         R2.x = min(R1.y)\n    *      Where R1 is an outer table reference, and R2 is a SubQuery table reference.\n    *   b. When hoisting the correlation predicate to a join predicate, we need to\n-   *      rewrite it to be in the form the Join code allows: so the predicte needs\n+   *      rewrite it to be in the form the Join code allows: so the predict needs\n    *      to contain a qualified column references.\n    *      We handle this by generating a new name for the aggregation expression,\n    *      like R1._gby_sq_col_1 and adding this mapping to the Outer Query's\n@@ -753,9 +766,8 @@ String getNextCorrExprAlias() {\n    */\n   private void rewrite(RowResolver parentQueryRR,\n       boolean forHavingClause,\n-      String outerQueryAlias) throws SemanticException {\n-    ASTNode selectClause = (ASTNode) subQueryAST.getChild(1).getChild(1);\n-    ASTNode whereClause = SubQueryUtils.subQueryWhere(subQueryAST);\n+      String outerQueryAlias, ASTNode insertClause, ASTNode selectClause) throws SemanticException {\n+    ASTNode whereClause = SubQueryUtils.subQueryWhere(insertClause);\n \n     if ( whereClause == null ) {\n       return;",
                "raw_url": "https://github.com/apache/hive/raw/358242f905bf0e94282e03c919850f63f4a7a7be/ql/src/java/org/apache/hadoop/hive/ql/parse/QBSubQuery.java",
                "sha": "1b6b33b1545aa11233f08b6418e3329b0cbd0d64",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/358242f905bf0e94282e03c919850f63f4a7a7be/ql/src/java/org/apache/hadoop/hive/ql/parse/SubQueryUtils.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SubQueryUtils.java?ref=358242f905bf0e94282e03c919850f63f4a7a7be",
                "deletions": 8,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SubQueryUtils.java",
                "patch": "@@ -43,7 +43,7 @@ static void extractConjuncts(ASTNode node, List<ASTNode> conjuncts) {\n   }\n \n   /*\n-   * Remove the SubQuery from the Where CLause Tree.\n+   * Remove the SubQuery from the Where Clause Tree.\n    * return the remaining WhereClause.\n    */\n   static ASTNode rewriteParentQueryWhere(ASTNode whereCond, ASTNode subQuery)\n@@ -271,10 +271,9 @@ static int checkAggOrWindowing(ASTNode expressionTree) throws SemanticException\n     return r;\n   }\n \n-  static List<String> getTableAliasesInSubQuery(QBSubQuery sq) {\n+  static List<String> getTableAliasesInSubQuery(ASTNode fromClause) {\n     List<String> aliases = new ArrayList<String>();\n-    ASTNode joinAST = (ASTNode) sq.getSubQueryAST().getChild(0);\n-    getTableAliasesInSubQuery((ASTNode) joinAST.getChild(0), aliases);\n+    getTableAliasesInSubQuery((ASTNode) fromClause.getChild(0), aliases);\n     return aliases;\n   }\n \n@@ -318,10 +317,10 @@ else if ( type == HiveParser.TOK_TABLE_OR_COL ) {\n     return null;\n   }\n   \n-  static ASTNode subQueryWhere(ASTNode subQueryAST) {\n-    if ( subQueryAST.getChild(1).getChildCount() > 2 &&\n-        subQueryAST.getChild(1).getChild(2).getType() == HiveParser.TOK_WHERE ) {\n-      return (ASTNode) subQueryAST.getChild(1).getChild(2);\n+  static ASTNode subQueryWhere(ASTNode insertClause) {\n+    if (insertClause.getChildCount() > 2 &&\n+        insertClause.getChild(2).getType() == HiveParser.TOK_WHERE ) {\n+      return (ASTNode) insertClause.getChild(2);\n     }\n     return null;\n   }",
                "raw_url": "https://github.com/apache/hive/raw/358242f905bf0e94282e03c919850f63f4a7a7be/ql/src/java/org/apache/hadoop/hive/ql/parse/SubQueryUtils.java",
                "sha": "57868b757b8dbbb012cb63a0c3fd7246e15adfe6",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/358242f905bf0e94282e03c919850f63f4a7a7be/ql/src/test/queries/clientnegative/subquery_missing_from.q",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/subquery_missing_from.q?ref=358242f905bf0e94282e03c919850f63f4a7a7be",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientnegative/subquery_missing_from.q",
                "patch": "@@ -0,0 +1 @@\n+select * from src where src.key in (select key);\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hive/raw/358242f905bf0e94282e03c919850f63f4a7a7be/ql/src/test/queries/clientnegative/subquery_missing_from.q",
                "sha": "3b49ac6a0a8f00dbc26a4071042d6e05574f0767",
                "status": "added"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/358242f905bf0e94282e03c919850f63f4a7a7be/ql/src/test/results/clientnegative/subquery_missing_from.q.out",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/subquery_missing_from.q.out?ref=358242f905bf0e94282e03c919850f63f4a7a7be",
                "deletions": 0,
                "filename": "ql/src/test/results/clientnegative/subquery_missing_from.q.out",
                "patch": "@@ -0,0 +1,3 @@\n+FAILED: SemanticException Line 0:-1 Invalid SubQuery expression 'key' in definition of SubQuery sq_1 [\n+src.key in (select key)\n+] used as sq_1 at Line 1:32: From clause is missing in SubQuery.",
                "raw_url": "https://github.com/apache/hive/raw/358242f905bf0e94282e03c919850f63f4a7a7be/ql/src/test/results/clientnegative/subquery_missing_from.q.out",
                "sha": "eaf7735adbe8c24db3acab363c9db800b9908956",
                "status": "added"
            }
        ],
        "message": "HIVE-9113 : Explain on query failed with NPE (Navis reviewed by Szehon Ho)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1646390 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/08b8e0d960042818f37fd0dc68fa872a98009e17",
        "patched_files": [
            "QBSubQuery.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestQBSubQuery.java"
        ]
    },
    "hive_35ea45a": {
        "bug_id": "hive_35ea45a",
        "commit": "https://github.com/apache/hive/commit/35ea45a1e1a7b700df2984a234f444c4a65ccb0d",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/itests/src/test/resources/testconfiguration.properties",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=35ea45a1e1a7b700df2984a234f444c4a65ccb0d",
                "deletions": 0,
                "filename": "itests/src/test/resources/testconfiguration.properties",
                "patch": "@@ -1506,6 +1506,7 @@ miniSparkOnYarn.only.query.files=spark_combine_equivalent_work.q,\\\n   spark_dynamic_partition_pruning_2.q,\\\n   spark_dynamic_partition_pruning_3.q,\\\n   spark_dynamic_partition_pruning_4.q,\\\n+  spark_dynamic_partition_pruning_5.q,\\\n   spark_dynamic_partition_pruning_mapjoin_only.q,\\\n   spark_constprog_dpp.q,\\\n   spark_dynamic_partition_pruning_recursive_mapjoin.q,\\",
                "raw_url": "https://github.com/apache/hive/raw/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/itests/src/test/resources/testconfiguration.properties",
                "sha": "d51e5cd17631d6372c3ceea452a94e354b8113f5",
                "status": "modified"
            },
            {
                "additions": 113,
                "blob_url": "https://github.com/apache/hive/blob/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkUtilities.java",
                "changes": 115,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkUtilities.java?ref=35ea45a1e1a7b700df2984a234f444c4a65ccb0d",
                "deletions": 2,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkUtilities.java",
                "patch": "@@ -17,26 +17,34 @@\n  */\n package org.apache.hadoop.hive.ql.exec.spark;\n \n-import java.io.File;\n import java.io.IOException;\n import java.net.URI;\n-import java.net.URISyntaxException;\n+import java.util.ArrayDeque;\n import java.util.ArrayList;\n import java.util.Collection;\n+import java.util.Deque;\n+import java.util.HashSet;\n+import java.util.Set;\n \n import com.google.common.base.Preconditions;\n import org.apache.commons.io.FilenameUtils;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.GroupByOperator;\n import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorUtils;\n+import org.apache.hadoop.hive.ql.exec.SelectOperator;\n+import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.TaskFactory;\n import org.apache.hadoop.hive.ql.exec.spark.session.SparkSession;\n import org.apache.hadoop.hive.ql.exec.spark.session.SparkSessionManager;\n import org.apache.hadoop.hive.ql.io.HiveKey;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.parse.spark.OptimizeSparkProcContext;\n+import org.apache.hadoop.hive.ql.parse.spark.SparkPartitionPruningSinkOperator;\n import org.apache.hadoop.hive.ql.plan.BaseWork;\n import org.apache.hadoop.hive.ql.plan.SparkWork;\n import org.apache.hadoop.hive.ql.session.SessionState;\n@@ -210,6 +218,33 @@ public static void collectOp(Collection<Operator<?>> result, Operator<?> root, C\n     }\n   }\n \n+  /**\n+   * Collect operators of type T starting from root. Matching operators will be put into result.\n+   * Set seen can be used to skip search in certain branches.\n+   */\n+  public static <T extends Operator<?>> void collectOp(Operator<?> root, Class<T> cls,\n+      Collection<T> result, Set<Operator<?>> seen) {\n+    if (seen.contains(root)) {\n+      return;\n+    }\n+    Deque<Operator<?>> deque = new ArrayDeque<>();\n+    deque.add(root);\n+    while (!deque.isEmpty()) {\n+      Operator<?> op = deque.remove();\n+      seen.add(op);\n+      if (cls.isInstance(op)) {\n+        result.add((T) op);\n+      }\n+      if (op.getChildOperators() != null) {\n+        for (Operator<?> child : op.getChildOperators()) {\n+          if (!seen.contains(child)) {\n+            deque.add(child);\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n   /**\n    * remove currTask from the children of its parentTask\n    * remove currTask from the parent of its childrenTask\n@@ -227,4 +262,80 @@ public static void removeEmptySparkTask(SparkTask currTask) {\n     //remove currTask from childTasks\n     currTask.removeFromChildrenTasks();\n   }\n+\n+  /**\n+   * For DPP sinks w/ common join, we'll split the tree and what's above the branching\n+   * operator is computed multiple times. Therefore it may not be good for performance to support\n+   * nested DPP sinks, i.e. one DPP sink depends on other DPP sinks.\n+   * The following is an example:\n+   *\n+   *             TS          TS\n+   *             |           |\n+   *            ...         FIL\n+   *            |           |  \\\n+   *            RS         RS  SEL\n+   *              \\        /    |\n+   *     TS          JOIN      GBY\n+   *     |         /     \\      |\n+   *    RS        RS    SEL   DPP2\n+   *     \\       /       |\n+   *       JOIN         GBY\n+   *                     |\n+   *                    DPP1\n+   *\n+   * where DPP1 depends on DPP2.\n+   *\n+   * To avoid such case, we'll visit all the branching operators. If a branching operator has any\n+   * further away DPP branches w/ common join in its sub-tree, such branches will be removed.\n+   * In the above example, the branch of DPP1 will be removed.\n+   */\n+  public static void removeNestedDPP(OptimizeSparkProcContext procContext) {\n+    Set<SparkPartitionPruningSinkOperator> allDPPs = new HashSet<>();\n+    Set<Operator<?>> seen = new HashSet<>();\n+    // collect all DPP sinks\n+    for (TableScanOperator root : procContext.getParseContext().getTopOps().values()) {\n+      SparkUtilities.collectOp(root, SparkPartitionPruningSinkOperator.class, allDPPs, seen);\n+    }\n+    // collect all branching operators\n+    Set<Operator<?>> branchingOps = new HashSet<>();\n+    for (SparkPartitionPruningSinkOperator dpp : allDPPs) {\n+      branchingOps.add(dpp.getBranchingOp());\n+    }\n+    // remember the branching ops we have visited\n+    Set<Operator<?>> visited = new HashSet<>();\n+    for (Operator<?> branchingOp : branchingOps) {\n+      if (!visited.contains(branchingOp)) {\n+        visited.add(branchingOp);\n+        seen.clear();\n+        Set<SparkPartitionPruningSinkOperator> nestedDPPs = new HashSet<>();\n+        for (Operator<?> branch : branchingOp.getChildOperators()) {\n+          if (!isDirectDPPBranch(branch)) {\n+            SparkUtilities.collectOp(branch, SparkPartitionPruningSinkOperator.class, nestedDPPs,\n+                seen);\n+          }\n+        }\n+        for (SparkPartitionPruningSinkOperator nestedDPP : nestedDPPs) {\n+          visited.add(nestedDPP.getBranchingOp());\n+          // if a DPP is with MJ, the tree won't be split and so we don't have to remove it\n+          if (!nestedDPP.isWithMapjoin()) {\n+            OperatorUtils.removeBranch(nestedDPP);\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  // whether of pattern \"SEL - GBY - DPP\"\n+  private static boolean isDirectDPPBranch(Operator<?> op) {\n+    if (op instanceof SelectOperator && op.getChildOperators() != null\n+        && op.getChildOperators().size() == 1) {\n+      op = op.getChildOperators().get(0);\n+      if (op instanceof GroupByOperator && op.getChildOperators() != null\n+          && op.getChildOperators().size() == 1) {\n+        op = op.getChildOperators().get(0);\n+        return op instanceof SparkPartitionPruningSinkOperator;\n+      }\n+    }\n+    return false;\n+  }\n }",
                "raw_url": "https://github.com/apache/hive/raw/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkUtilities.java",
                "sha": "130338624edf86fba4ad57323c520d9f467e809f",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hive/blob/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java?ref=35ea45a1e1a7b700df2984a234f444c4a65ccb0d",
                "deletions": 7,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java",
                "patch": "@@ -126,11 +126,16 @@ protected void optimizeOperatorPlan(ParseContext pCtx, Set<ReadEntity> inputs,\n     // Run Join releated optimizations\n     runJoinOptimizations(procCtx);\n \n-    // Remove DPP based on expected size of the output data\n-    runRemoveDynamicPruning(procCtx);\n+    if(conf.isSparkDPPAny()){\n+      // Remove DPP based on expected size of the output data\n+      runRemoveDynamicPruning(procCtx);\n \n-    // Remove cyclic dependencies for DPP\n-    runCycleAnalysisForPartitionPruning(procCtx);\n+      // Remove cyclic dependencies for DPP\n+      runCycleAnalysisForPartitionPruning(procCtx);\n+\n+      // Remove nested DPPs\n+      SparkUtilities.removeNestedDPP(procCtx);\n+    }\n \n     // Re-run constant propagation so we fold any new constants introduced by the operator optimizers\n     // Specifically necessary for DPP because we might have created lots of \"and true and true\" conditions\n@@ -161,9 +166,6 @@ private void runRemoveDynamicPruning(OptimizeSparkProcContext procCtx) throws Se\n   }\n \n   private void runCycleAnalysisForPartitionPruning(OptimizeSparkProcContext procCtx) {\n-    if (!conf.isSparkDPPAny()) {\n-      return;\n-    }\n \n     boolean cycleFree = false;\n     while (!cycleFree) {",
                "raw_url": "https://github.com/apache/hive/raw/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkCompiler.java",
                "sha": "aba15187f9fd662253373043c151e3d8313f5552",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hive/blob/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/ql/src/test/queries/clientpositive/spark_dynamic_partition_pruning_5.q",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/spark_dynamic_partition_pruning_5.q?ref=35ea45a1e1a7b700df2984a234f444c4a65ccb0d",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/spark_dynamic_partition_pruning_5.q",
                "patch": "@@ -0,0 +1,24 @@\n+set hive.spark.dynamic.partition.pruning=true;\n+\n+-- This qfile tests whether we can handle nested DPP sinks\n+\n+create table part1(key string, value string) partitioned by (p string);\n+insert into table part1 partition (p='1') select * from src;\n+\n+create table part2(key string, value string) partitioned by (p string);\n+insert into table part2 partition (p='1') select * from src;\n+\n+create table regular1 as select * from src limit 2;\n+\n+-- nested DPP is removed, upper most DPP is w/ common join\n+explain select * from src join part1 on src.key=part1.p join part2 on src.value=part2.p;\n+\n+-- nested DPP is removed, upper most DPP is w/ map join\n+set hive.auto.convert.join=true;\n+-- ensure regular1 is treated as small table, and partitioned tables are not\n+set hive.auto.convert.join.noconditionaltask.size=20;\n+explain select * from regular1 join part1 on regular1.key=part1.p join part2 on regular1.value=part2.p;\n+\n+drop table part1;\n+drop table part2;\n+drop table regular1;\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hive/raw/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/ql/src/test/queries/clientpositive/spark_dynamic_partition_pruning_5.q",
                "sha": "488378776e4d5a2ed1f0a30238f72c12c83cc3b1",
                "status": "added"
            },
            {
                "additions": 335,
                "blob_url": "https://github.com/apache/hive/blob/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/ql/src/test/results/clientpositive/spark/spark_dynamic_partition_pruning_5.q.out",
                "changes": 335,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/spark_dynamic_partition_pruning_5.q.out?ref=35ea45a1e1a7b700df2984a234f444c4a65ccb0d",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/spark/spark_dynamic_partition_pruning_5.q.out",
                "patch": "@@ -0,0 +1,335 @@\n+PREHOOK: query: create table part1(key string, value string) partitioned by (p string)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@part1\n+POSTHOOK: query: create table part1(key string, value string) partitioned by (p string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@part1\n+PREHOOK: query: insert into table part1 partition (p='1') select * from src\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@part1@p=1\n+POSTHOOK: query: insert into table part1 partition (p='1') select * from src\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@part1@p=1\n+POSTHOOK: Lineage: part1 PARTITION(p=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: part1 PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: create table part2(key string, value string) partitioned by (p string)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@part2\n+POSTHOOK: query: create table part2(key string, value string) partitioned by (p string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@part2\n+PREHOOK: query: insert into table part2 partition (p='1') select * from src\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@part2@p=1\n+POSTHOOK: query: insert into table part2 partition (p='1') select * from src\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@part2@p=1\n+POSTHOOK: Lineage: part2 PARTITION(p=1).key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: part2 PARTITION(p=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: create table regular1 as select * from src limit 2\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@src\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@regular1\n+POSTHOOK: query: create table regular1 as select * from src limit 2\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@regular1\n+POSTHOOK: Lineage: regular1.key SIMPLE [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: regular1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: explain select * from src join part1 on src.key=part1.p join part2 on src.value=part2.p\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain select * from src join part1 on src.key=part1.p join part2 on src.value=part2.p\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-2 is a root stage\n+  Stage-1 depends on stages: Stage-2\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-2\n+    Spark\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 6 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Filter Operator\n+                    predicate: (key is not null and value is not null) (type: boolean)\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    Select Operator\n+                      expressions: key (type: string), value (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                      Select Operator\n+                        expressions: _col0 (type: string)\n+                        outputColumnNames: _col0\n+                        Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                        Group By Operator\n+                          keys: _col0 (type: string)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                          Spark Partition Pruning Sink Operator\n+                            Target column: [1:p (string)]\n+                            partition key expr: [p]\n+                            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                            target works: [Map 1]\n+\n+  Stage: Stage-1\n+    Spark\n+      Edges:\n+        Reducer 2 <- Map 1 (PARTITION-LEVEL SORT, 4), Map 4 (PARTITION-LEVEL SORT, 4)\n+        Reducer 3 <- Map 5 (PARTITION-LEVEL SORT, 4), Reducer 2 (PARTITION-LEVEL SORT, 4)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: part1\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string), p (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: _col2 (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: _col2 (type: string)\n+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                      value expressions: _col0 (type: string), _col1 (type: string)\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Filter Operator\n+                    predicate: (key is not null and value is not null) (type: boolean)\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    Select Operator\n+                      expressions: key (type: string), value (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: string)\n+                        sort order: +\n+                        Map-reduce partition columns: _col0 (type: string)\n+                        Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                        value expressions: _col1 (type: string)\n+        Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: part2\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string), p (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: _col2 (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: _col2 (type: string)\n+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                      value expressions: _col0 (type: string), _col1 (type: string)\n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                keys:\n+                  0 _col2 (type: string)\n+                  1 _col0 (type: string)\n+                outputColumnNames: _col0, _col1, _col2, _col3, _col4\n+                Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col4 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col4 (type: string)\n+                  Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE\n+                  value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)\n+        Reducer 3 \n+            Reduce Operator Tree:\n+              Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                keys:\n+                  0 _col4 (type: string)\n+                  1 _col2 (type: string)\n+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7\n+                Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE\n+                Select Operator\n+                  expressions: _col3 (type: string), _col4 (type: string), _col0 (type: string), _col1 (type: string), _col2 (type: string), _col5 (type: string), _col6 (type: string), _col7 (type: string)\n+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7\n+                  Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE\n+                  File Output Operator\n+                    compressed: false\n+                    Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE\n+                    table:\n+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: explain select * from regular1 join part1 on regular1.key=part1.p join part2 on regular1.value=part2.p\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain select * from regular1 join part1 on regular1.key=part1.p join part2 on regular1.value=part2.p\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-2 is a root stage\n+  Stage-1 depends on stages: Stage-2\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-2\n+    Spark\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: regular1\n+                  Statistics: Num rows: 2 Data size: 20 Basic stats: COMPLETE Column stats: NONE\n+                  Filter Operator\n+                    predicate: (key is not null and value is not null) (type: boolean)\n+                    Statistics: Num rows: 2 Data size: 20 Basic stats: COMPLETE Column stats: NONE\n+                    Select Operator\n+                      expressions: key (type: string), value (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 2 Data size: 20 Basic stats: COMPLETE Column stats: NONE\n+                      Spark HashTable Sink Operator\n+                        keys:\n+                          0 _col2 (type: string)\n+                          1 _col0 (type: string)\n+                      Select Operator\n+                        expressions: _col0 (type: string)\n+                        outputColumnNames: _col0\n+                        Statistics: Num rows: 2 Data size: 20 Basic stats: COMPLETE Column stats: NONE\n+                        Group By Operator\n+                          keys: _col0 (type: string)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Statistics: Num rows: 2 Data size: 20 Basic stats: COMPLETE Column stats: NONE\n+                          Spark Partition Pruning Sink Operator\n+                            Target column: [1:p (string)]\n+                            partition key expr: [p]\n+                            Statistics: Num rows: 2 Data size: 20 Basic stats: COMPLETE Column stats: NONE\n+                            target works: [Map 1]\n+            Local Work:\n+              Map Reduce Local Work\n+\n+  Stage: Stage-1\n+    Spark\n+      Edges:\n+        Reducer 2 <- Map 1 (PARTITION-LEVEL SORT, 4), Map 4 (PARTITION-LEVEL SORT, 4)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: part1\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string), p (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    Map Join Operator\n+                      condition map:\n+                           Inner Join 0 to 1\n+                      keys:\n+                        0 _col2 (type: string)\n+                        1 _col0 (type: string)\n+                      outputColumnNames: _col0, _col1, _col2, _col3, _col4\n+                      input vertices:\n+                        1 Map 3\n+                      Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: _col4 (type: string)\n+                        sort order: +\n+                        Map-reduce partition columns: _col4 (type: string)\n+                        Statistics: Num rows: 550 Data size: 5843 Basic stats: COMPLETE Column stats: NONE\n+                        value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)\n+            Local Work:\n+              Map Reduce Local Work\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: part2\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string), p (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: _col2 (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: _col2 (type: string)\n+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                      value expressions: _col0 (type: string), _col1 (type: string)\n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                keys:\n+                  0 _col4 (type: string)\n+                  1 _col2 (type: string)\n+                outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7\n+                Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE\n+                Select Operator\n+                  expressions: _col3 (type: string), _col4 (type: string), _col0 (type: string), _col1 (type: string), _col2 (type: string), _col5 (type: string), _col6 (type: string), _col7 (type: string)\n+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7\n+                  Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE\n+                  File Output Operator\n+                    compressed: false\n+                    Statistics: Num rows: 605 Data size: 6427 Basic stats: COMPLETE Column stats: NONE\n+                    table:\n+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: drop table part1\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@part1\n+PREHOOK: Output: default@part1\n+POSTHOOK: query: drop table part1\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@part1\n+POSTHOOK: Output: default@part1\n+PREHOOK: query: drop table part2\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@part2\n+PREHOOK: Output: default@part2\n+POSTHOOK: query: drop table part2\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@part2\n+POSTHOOK: Output: default@part2\n+PREHOOK: query: drop table regular1\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@regular1\n+PREHOOK: Output: default@regular1\n+POSTHOOK: query: drop table regular1\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@regular1\n+POSTHOOK: Output: default@regular1",
                "raw_url": "https://github.com/apache/hive/raw/35ea45a1e1a7b700df2984a234f444c4a65ccb0d/ql/src/test/results/clientpositive/spark/spark_dynamic_partition_pruning_5.q.out",
                "sha": "189a43bd159a1376a0f7c8b76153e6c34609b154",
                "status": "added"
            }
        ],
        "message": "HIVE-18148: NPE in SparkDynamicPartitionPruningResolver (Rui reviewed by Liyun Zhang and Sahil Takiar)",
        "parent": "https://github.com/apache/hive/commit/a7c41ba2afb36398134a824bc461a50499114feb",
        "patched_files": [
            "SparkCompiler.java",
            "spark_dynamic_partition_pruning_5.java",
            "SparkUtilities.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "testconfiguration.java"
        ]
    },
    "hive_3633db2": {
        "bug_id": "hive_3633db2",
        "commit": "https://github.com/apache/hive/commit/3633db25fadd39fdcad15d95af1cf69cc6e2429e",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/itests/src/test/resources/testconfiguration.properties",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e",
                "deletions": 0,
                "filename": "itests/src/test/resources/testconfiguration.properties",
                "patch": "@@ -3,6 +3,7 @@ minimr.query.files=auto_sortmerge_join_16.q,\\\n   bucket4.q,\\\n   bucket5.q,\\\n   bucket6.q,\\\n+  bucket_many.q,\\\n   bucket_num_reducers.q,\\\n   bucket_num_reducers2.q,\\\n   bucketizedhiveinputformat.q,\\",
                "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/itests/src/test/resources/testconfiguration.properties",
                "sha": "eeb46cc16dc9c2a0e7f05b503efa9ee87cd1f067",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java",
                "patch": "@@ -125,7 +125,7 @@\n   protected transient Object[] cachedValues;\n   protected transient List<List<Integer>> distinctColIndices;\n   protected transient Random random;\n-  protected transient int bucketNumber;\n+  protected transient int bucketNumber = -1;\n \n   /**\n    * This two dimensional array holds key data and a corresponding Union object\n@@ -552,6 +552,7 @@ private BytesWritable makeValueWritable(Object row) throws Exception {\n     // in case of bucketed table, insert the bucket number as the last column in value\n     if (bucketEval != null) {\n       length -= 1;\n+      assert bucketNumber >= 0;\n       cachedValues[length] = new Text(String.valueOf(bucketNumber));\n     }\n ",
                "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/java/org/apache/hadoop/hive/ql/exec/ReduceSinkOperator.java",
                "sha": "859a28fc965ed75c965c41af2e1442f3eda99523",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/queries/clientpositive/bucket_many.q",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/bucket_many.q?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/bucket_many.q",
                "patch": "@@ -0,0 +1,16 @@\n+set hive.enforce.bucketing = true;\n+set mapred.reduce.tasks = 16;\n+\n+create table bucket_many(key int, value string) clustered by (key) into 256 buckets;\n+\n+explain extended\n+insert overwrite table bucket_many\n+select * from src;\n+\n+insert overwrite table bucket_many\n+select * from src;\n+\n+explain\n+select * from bucket_many tablesample (bucket 1 out of 256) s;\n+\n+select * from bucket_many tablesample (bucket 1 out of 256) s;",
                "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/queries/clientpositive/bucket_many.q",
                "sha": "1f0b795f870f699963890d3ffddc0f47ce5bb214",
                "status": "added"
            },
            {
                "additions": 230,
                "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/bucket_many.q.out",
                "changes": 230,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/bucket_many.q.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/bucket_many.q.out",
                "patch": "@@ -0,0 +1,230 @@\n+PREHOOK: query: create table bucket_many(key int, value string) clustered by (key) into 256 buckets\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@bucket_many\n+POSTHOOK: query: create table bucket_many(key int, value string) clustered by (key) into 256 buckets\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@bucket_many\n+PREHOOK: query: explain extended\n+insert overwrite table bucket_many\n+select * from src\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain extended\n+insert overwrite table bucket_many\n+select * from src\n+POSTHOOK: type: QUERY\n+ABSTRACT SYNTAX TREE:\n+  \n+TOK_QUERY\n+   TOK_FROM\n+      TOK_TABREF\n+         TOK_TABNAME\n+            src\n+   TOK_INSERT\n+      TOK_DESTINATION\n+         TOK_TAB\n+            TOK_TABNAME\n+               bucket_many\n+      TOK_SELECT\n+         TOK_SELEXPR\n+            TOK_ALLCOLREF\n+\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+  Stage-2 depends on stages: Stage-0\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: src\n+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+            GatherStats: false\n+            Select Operator\n+              expressions: key (type: string), value (type: string)\n+              outputColumnNames: _col0, _col1\n+              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+              Reduce Output Operator\n+                sort order: \n+                Map-reduce partition columns: UDFToInteger(_col0) (type: int)\n+                Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                tag: -1\n+                value expressions: _col0 (type: string), _col1 (type: string)\n+                auto parallelism: false\n+      Path -> Alias:\n+#### A masked pattern was here ####\n+      Path -> Partition:\n+#### A masked pattern was here ####\n+          Partition\n+            base file name: src\n+            input format: org.apache.hadoop.mapred.TextInputFormat\n+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+            properties:\n+              COLUMN_STATS_ACCURATE true\n+              bucket_count -1\n+              columns key,value\n+              columns.comments 'default','default'\n+              columns.types string:string\n+#### A masked pattern was here ####\n+              name default.src\n+              numFiles 1\n+              numRows 500\n+              rawDataSize 5312\n+              serialization.ddl struct src { string key, string value}\n+              serialization.format 1\n+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              totalSize 5812\n+#### A masked pattern was here ####\n+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+          \n+              input format: org.apache.hadoop.mapred.TextInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+              properties:\n+                COLUMN_STATS_ACCURATE true\n+                bucket_count -1\n+                columns key,value\n+                columns.comments 'default','default'\n+                columns.types string:string\n+#### A masked pattern was here ####\n+                name default.src\n+                numFiles 1\n+                numRows 500\n+                rawDataSize 5312\n+                serialization.ddl struct src { string key, string value}\n+                serialization.format 1\n+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                totalSize 5812\n+#### A masked pattern was here ####\n+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              name: default.src\n+            name: default.src\n+      Truncated Path -> Alias:\n+        /src [src]\n+      Needs Tagging: false\n+      Reduce Operator Tree:\n+        Select Operator\n+          expressions: UDFToInteger(VALUE._col0) (type: int), VALUE._col1 (type: string)\n+          outputColumnNames: _col0, _col1\n+          Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+          File Output Operator\n+            compressed: false\n+            GlobalTableId: 1\n+#### A masked pattern was here ####\n+            NumFilesPerFileSink: 16\n+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+#### A masked pattern was here ####\n+            table:\n+                input format: org.apache.hadoop.mapred.TextInputFormat\n+                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                properties:\n+                  bucket_count 256\n+                  bucket_field_name key\n+                  columns key,value\n+                  columns.comments \n+                  columns.types int:string\n+#### A masked pattern was here ####\n+                  name default.bucket_many\n+                  serialization.ddl struct bucket_many { i32 key, string value}\n+                  serialization.format 1\n+                  serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+#### A masked pattern was here ####\n+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                name: default.bucket_many\n+            TotalFiles: 256\n+            GatherStats: true\n+            MultiFileSpray: true\n+\n+  Stage: Stage-0\n+    Move Operator\n+      tables:\n+          replace: true\n+#### A masked pattern was here ####\n+          table:\n+              input format: org.apache.hadoop.mapred.TextInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+              properties:\n+                bucket_count 256\n+                bucket_field_name key\n+                columns key,value\n+                columns.comments \n+                columns.types int:string\n+#### A masked pattern was here ####\n+                name default.bucket_many\n+                serialization.ddl struct bucket_many { i32 key, string value}\n+                serialization.format 1\n+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+#### A masked pattern was here ####\n+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              name: default.bucket_many\n+\n+  Stage: Stage-2\n+    Stats-Aggr Operator\n+#### A masked pattern was here ####\n+\n+PREHOOK: query: insert overwrite table bucket_many\n+select * from src\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@bucket_many\n+POSTHOOK: query: insert overwrite table bucket_many\n+select * from src\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@bucket_many\n+POSTHOOK: Lineage: bucket_many.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: bucket_many.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: explain\n+select * from bucket_many tablesample (bucket 1 out of 256) s\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select * from bucket_many tablesample (bucket 1 out of 256) s\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: s\n+            Statistics: Num rows: 55 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+            Filter Operator\n+              predicate: (((hash(key) & 2147483647) % 256) = 0) (type: boolean)\n+              Statistics: Num rows: 27 Data size: 2853 Basic stats: COMPLETE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: int), value (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 27 Data size: 2853 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 27 Data size: 2853 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: select * from bucket_many tablesample (bucket 1 out of 256) s\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@bucket_many\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from bucket_many tablesample (bucket 1 out of 256) s\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@bucket_many\n+#### A masked pattern was here ####\n+256\tval_256\n+0\tval_0\n+0\tval_0\n+0\tval_0\n+256\tval_256",
                "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/bucket_many.q.out",
                "sha": "9f09163ea1cdcd5f1097fe47b02b6ba5eb8e4012",
                "status": "added"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/cbo_gby.q.out",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/cbo_gby.q.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e",
                "deletions": 2,
                "filename": "ql/src/test/results/clientpositive/spark/cbo_gby.q.out",
                "patch": "@@ -11,10 +11,10 @@ POSTHOOK: Input: default@cbo_t1\n POSTHOOK: Input: default@cbo_t1@dt=2014\n #### A masked pattern was here ####\n 1\t4\t12\n+ 1 \t4\t2\n NULL\tNULL\tNULL\n  1\t4\t2\n 1 \t4\t2\n- 1 \t4\t2\n PREHOOK: query: select x, y, count(*) from (select key, (c_int+c_float+1+2) as x, sum(c_int) as y from cbo_t1 group by c_float, cbo_t1.c_int, key) R group by y, x\n PREHOOK: type: QUERY\n PREHOOK: Input: default@cbo_t1\n@@ -25,9 +25,9 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: default@cbo_t1\n POSTHOOK: Input: default@cbo_t1@dt=2014\n #### A masked pattern was here ####\n+5.0\t2\t3\n NULL\tNULL\t1\n 5.0\t12\t1\n-5.0\t2\t3\n PREHOOK: query: select cbo_t3.c_int, c, count(*) from (select key as a, c_int+1 as b, sum(c_int) as c from cbo_t1 where (cbo_t1.c_int + 1 >= 0) and (cbo_t1.c_int > 0 or cbo_t1.c_float >= 0) group by c_float, cbo_t1.c_int, key order by a) cbo_t1 join (select key as p, c_int+1 as q, sum(c_int) as r from cbo_t2 where (cbo_t2.c_int + 1 >= 0) and (cbo_t2.c_int > 0 or cbo_t2.c_float >= 0)  group by c_float, cbo_t2.c_int, key order by q/10 desc, r asc) cbo_t2 on cbo_t1.a=p join cbo_t3 on cbo_t1.a=key where (b + cbo_t2.q >= 0) and (b > 0 or c_int >= 0) group by cbo_t3.c_int, c order by cbo_t3.c_int+c desc, c\n PREHOOK: type: QUERY\n PREHOOK: Input: default@cbo_t1",
                "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/cbo_gby.q.out",
                "sha": "9ca8a881cbbc09c31a2218bc8fbb7fa56bbe61db",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/cbo_udf_udaf.q.out",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/cbo_udf_udaf.q.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e",
                "deletions": 1,
                "filename": "ql/src/test/results/clientpositive/spark/cbo_udf_udaf.q.out",
                "patch": "@@ -79,9 +79,9 @@ POSTHOOK: Input: default@cbo_t1\n POSTHOOK: Input: default@cbo_t1@dt=2014\n #### A masked pattern was here ####\n NULL\t0\tNULL\n+1 \t2\t1.0\n  1 \t2\t1.0\n  1\t2\t1.0\n-1 \t2\t1.0\n 1\t12\t1.0\n PREHOOK: query: select count(distinct c_int) as a, avg(c_float) from cbo_t1 group by c_float order by a\n PREHOOK: type: QUERY",
                "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/cbo_udf_udaf.q.out",
                "sha": "ded043f626b0926075688a03b659b651756a27c7",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/groupby_complex_types_multi_single_reducer.q.out",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/groupby_complex_types_multi_single_reducer.q.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e",
                "deletions": 19,
                "filename": "ql/src/test/results/clientpositive/spark/groupby_complex_types_multi_single_reducer.q.out",
                "patch": "@@ -204,16 +204,16 @@ POSTHOOK: query: SELECT DEST1.* FROM DEST1\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@dest1\n #### A masked pattern was here ####\n-[\"166\"]\t1\n-[\"169\"]\t4\n+[\"118\"]\t2\n+[\"180\"]\t1\n+[\"201\"]\t1\n+[\"202\"]\t1\n [\"238\"]\t2\n-[\"258\"]\t1\n-[\"306\"]\t1\n-[\"384\"]\t3\n-[\"392\"]\t1\n-[\"435\"]\t1\n-[\"455\"]\t1\n-[\"468\"]\t4\n+[\"273\"]\t3\n+[\"282\"]\t2\n+[\"419\"]\t1\n+[\"432\"]\t1\n+[\"467\"]\t1\n PREHOOK: query: SELECT DEST2.* FROM DEST2\n PREHOOK: type: QUERY\n PREHOOK: Input: default@dest2\n@@ -222,13 +222,13 @@ POSTHOOK: query: SELECT DEST2.* FROM DEST2\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@dest2\n #### A masked pattern was here ####\n-{\"120\":\"val_120\"}\t2\n-{\"129\":\"val_129\"}\t2\n-{\"160\":\"val_160\"}\t1\n-{\"26\":\"val_26\"}\t2\n-{\"27\":\"val_27\"}\t1\n-{\"288\":\"val_288\"}\t2\n-{\"298\":\"val_298\"}\t3\n-{\"30\":\"val_30\"}\t1\n-{\"311\":\"val_311\"}\t3\n-{\"74\":\"val_74\"}\t1\n+{\"0\":\"val_0\"}\t3\n+{\"138\":\"val_138\"}\t4\n+{\"170\":\"val_170\"}\t1\n+{\"19\":\"val_19\"}\t1\n+{\"222\":\"val_222\"}\t1\n+{\"223\":\"val_223\"}\t2\n+{\"226\":\"val_226\"}\t1\n+{\"489\":\"val_489\"}\t4\n+{\"8\":\"val_8\"}\t1\n+{\"80\":\"val_80\"}\t1",
                "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/groupby_complex_types_multi_single_reducer.q.out",
                "sha": "9fe3b7227c04480249534b0595d2e4e1c2a5e0d4",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/lateral_view_explode2.q.out",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/lateral_view_explode2.q.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e",
                "deletions": 2,
                "filename": "ql/src/test/results/clientpositive/spark/lateral_view_explode2.q.out",
                "patch": "@@ -93,9 +93,9 @@ POSTHOOK: query: SELECT col1, col2 FROM src LATERAL VIEW explode2(array(1,2,3))\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n #### A masked pattern was here ####\n-2\t2\n-1\t1\n 3\t3\n+1\t1\n+2\t2\n PREHOOK: query: DROP TEMPORARY FUNCTION explode2\n PREHOOK: type: DROPFUNCTION\n PREHOOK: Output: explode2",
                "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/lateral_view_explode2.q.out",
                "sha": "41d60f584d71ee2fd1fffbb94d090230c6a0f9f3",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/union_remove_25.q.out",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/union_remove_25.q.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e",
                "deletions": 1,
                "filename": "ql/src/test/results/clientpositive/spark/union_remove_25.q.out",
                "patch": "@@ -424,7 +424,7 @@ Partition Parameters:\n \tnumFiles            \t2                   \n \tnumRows             \t-1                  \n \trawDataSize         \t-1                  \n-\ttotalSize           \t6814                \n+\ttotalSize           \t6826                \n #### A masked pattern was here ####\n \t \t \n # Storage Information\t \t ",
                "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/union_remove_25.q.out",
                "sha": "5853cc03cd4db548b727eee098362a769d63d4df",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/union_top_level.q.out",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/union_top_level.q.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e",
                "deletions": 8,
                "filename": "ql/src/test/results/clientpositive/spark/union_top_level.q.out",
                "patch": "@@ -348,18 +348,18 @@ POSTHOOK: Input: default@src\n 0\tval_0\n 0\tval_0\n 0\tval_0\n-10\tval_10\n-10\tval_10\n+0\tval_0\n+0\tval_0\n+100\tval_100\n+100\tval_100\n+100\tval_100\n+100\tval_100\n+100\tval_100\n+100\tval_100\n 100\tval_100\n 100\tval_100\n-103\tval_103\n-103\tval_103\n-104\tval_104\n-104\tval_104\n 104\tval_104\n 104\tval_104\n-111\tval_111\n-111\tval_111\n PREHOOK: query: -- ctas\n explain\n create table union_top as",
                "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/union_top_level.q.out",
                "sha": "a64fc9572e40f28d8b564564461626aade96dd2b",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/vector_cast_constant.q.java1.7.out",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/vector_cast_constant.q.java1.7.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e",
                "deletions": 8,
                "filename": "ql/src/test/results/clientpositive/spark/vector_cast_constant.q.java1.7.out",
                "patch": "@@ -191,13 +191,13 @@ POSTHOOK: query: SELECT\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@over1korc\n #### A masked pattern was here ####\n+65636\t50.0\t50.0\t50\n+65550\t50.0\t50.0\t50\n+65592\t50.0\t50.0\t50\n+65744\t50.0\t50.0\t50\n+65668\t50.0\t50.0\t50\n+65722\t50.0\t50.0\t50\n 65598\t50.0\t50.0\t50\n-65694\t50.0\t50.0\t50\n-65678\t50.0\t50.0\t50\n-65684\t50.0\t50.0\t50\n+65568\t50.0\t50.0\t50\n 65596\t50.0\t50.0\t50\n-65692\t50.0\t50.0\t50\n-65630\t50.0\t50.0\t50\n-65674\t50.0\t50.0\t50\n-65628\t50.0\t50.0\t50\n-65776\t50.0\t50.0\t50\n+65738\t50.0\t50.0\t50",
                "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/vector_cast_constant.q.java1.7.out",
                "sha": "aaac8aac04ec925019b04e482a34e73e869512e7",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/vector_cast_constant.q.java1.8.out",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/vector_cast_constant.q.java1.8.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e",
                "deletions": 8,
                "filename": "ql/src/test/results/clientpositive/spark/vector_cast_constant.q.java1.8.out",
                "patch": "@@ -191,13 +191,13 @@ POSTHOOK: query: SELECT\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@over1korc\n #### A masked pattern was here ####\n-65788\t50.0\t50.0\t50\n+65636\t50.0\t50.0\t50\n+65550\t50.0\t50.0\t50\n+65592\t50.0\t50.0\t50\n+65744\t50.0\t50.0\t50\n+65722\t50.0\t50.0\t50\n+65668\t50.0\t50.0\t50\n 65598\t50.0\t50.0\t50\n-65694\t50.0\t50.0\t50\n-65678\t50.0\t50.0\t50\n-65684\t50.0\t50.0\t50\n 65596\t50.0\t50.0\t50\n-65692\t50.0\t50.0\t50\n-65630\t50.0\t50.0\t50\n-65674\t50.0\t50.0\t50\n-65628\t50.0\t50.0\t50\n+65568\t50.0\t50.0\t50\n+65738\t50.0\t50.0\t50",
                "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/vector_cast_constant.q.java1.8.out",
                "sha": "44ecd09a326e5ca2558c2cf679d62c38ebba9814",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/vectorized_timestamp_funcs.q.out",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/vectorized_timestamp_funcs.q.out?ref=3633db25fadd39fdcad15d95af1cf69cc6e2429e",
                "deletions": 2,
                "filename": "ql/src/test/results/clientpositive/spark/vectorized_timestamp_funcs.q.out",
                "patch": "@@ -768,7 +768,7 @@ FROM alltypesorc_string\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@alltypesorc_string\n #### A masked pattern was here ####\n-1123143.857\n+1123143.8569999998\n PREHOOK: query: EXPLAIN SELECT\n   avg(ctimestamp1),\n   variance(ctimestamp1),\n@@ -868,4 +868,4 @@ FROM alltypesorc_string\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@alltypesorc_string\n #### A masked pattern was here ####\n-2.8798560435897438E13\t8.970772952794215E19\t8.970772952794215E19\t9.206845925236167E19\t9.471416447815086E9\t9.471416447815086E9\t9.471416447815086E9\t9.595231068211004E9\n+2.8798560435897438E13\t8.970772952794214E19\t8.970772952794214E19\t9.206845925236167E19\t9.471416447815086E9\t9.471416447815086E9\t9.471416447815086E9\t9.595231068211004E9",
                "raw_url": "https://github.com/apache/hive/raw/3633db25fadd39fdcad15d95af1cf69cc6e2429e/ql/src/test/results/clientpositive/spark/vectorized_timestamp_funcs.q.out",
                "sha": "304458215b4dcbc4d49321ba5f14ca5a87f2ec26",
                "status": "modified"
            }
        ],
        "message": "HIVE-10538: Fix NPE in FileSinkOperator from hashcode mismatch (Peter Slawski reviewed by Prasanth Jayachandran)",
        "parent": "https://github.com/apache/hive/commit/0af1d6e7d36fbd8071a35692f51d72eb86b0e9da",
        "patched_files": [
            "groupby_complex_types_multi_single_reducer.java",
            "cbo_gby.java",
            "lateral_view_explode2.java",
            "union_top_level.java",
            "vectorized_timestamp_funcs.java",
            "bucket_many.java",
            "vector_cast_constant.java",
            "ReduceSinkOperator.java",
            "cbo_udf_udaf.java",
            "union_remove_25.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "testconfiguration.java"
        ]
    },
    "hive_372782f": {
        "bug_id": "hive_372782f",
        "commit": "https://github.com/apache/hive/commit/372782f82c45977bcac588f5d7bc02eecf96726d",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/372782f82c45977bcac588f5d7bc02eecf96726d/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java?ref=372782f82c45977bcac588f5d7bc02eecf96726d",
                "deletions": 2,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "patch": "@@ -273,9 +273,8 @@ private static BaseWork getBaseWork(Configuration conf, String name) {\n       assert path != null;\n       gWork = gWorkMap.get(path);\n       if (gWork == null) {\n-        String jtConf = ShimLoader.getHadoopShims().getJobLauncherRpcAddress(conf);\n         Path localPath;\n-        if (jtConf.equals(\"local\")) {\n+        if (ShimLoader.getHadoopShims().isLocalMode(conf)) {\n           localPath = path;\n         } else {\n           localPath = new Path(name);",
                "raw_url": "https://github.com/apache/hive/raw/372782f82c45977bcac588f5d7bc02eecf96726d/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "sha": "fd5a4f09b9e6f9a8b73e0f10ce033f33d37bf04f",
                "status": "modified"
            }
        ],
        "message": "HIVE-4003 - NullPointerException in exec.Utilities (Mark Grover via Brock Noland)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1521188 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/e765ec241f52f71d372de65276d2e133f970f423",
        "patched_files": [
            "Utilities.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestUtilities.java"
        ]
    },
    "hive_3b8fdc1": {
        "bug_id": "hive_3b8fdc1",
        "commit": "https://github.com/apache/hive/commit/3b8fdc1dc7228fbd9107ee003d9dbe9e2dfb8692",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/3b8fdc1dc7228fbd9107ee003d9dbe9e2dfb8692/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java?ref=3b8fdc1dc7228fbd9107ee003d9dbe9e2dfb8692",
                "deletions": 1,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java",
                "patch": "@@ -550,7 +550,9 @@ private OrcFileMetadata getOrReadFileMetadata() throws IOException {\n           }\n           // Create new key object to reuse for gets; we've used the old one to put in cache.\n           stripeKey = new OrcBatchKey(fileId, 0, 0);\n-        } else {\n+        }\n+        // We might have got an old value from cache; recheck it has indexes.\n+        if (!value.hasAllIndexes(globalInc)) {\n           if (DebugUtils.isTraceOrcEnabled()) {\n             LlapIoImpl.LOG.info(\"Updating indexes in stripe \" + stripeKey.stripeIx\n                 + \" metadata for includes: \" + DebugUtils.toString(globalInc));",
                "raw_url": "https://github.com/apache/hive/raw/3b8fdc1dc7228fbd9107ee003d9dbe9e2dfb8692/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java",
                "sha": "5cf0780291687438b4ad396443854051660fd9d6",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/3b8fdc1dc7228fbd9107ee003d9dbe9e2dfb8692/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java?ref=3b8fdc1dc7228fbd9107ee003d9dbe9e2dfb8692",
                "deletions": 5,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java",
                "patch": "@@ -46,6 +46,7 @@\n import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n import org.apache.hadoop.hive.ql.io.filters.BloomFilterIO;\n+import org.apache.hadoop.hive.ql.io.orc.OrcProto.RowIndexEntry;\n import org.apache.hadoop.hive.ql.io.orc.RecordReaderUtils.ByteBufferAllocatorPool;\n import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;\n import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;\n@@ -705,18 +706,24 @@ public SargApplier(SearchArgument sarg, String[] columnNames, long rowIndexStrid\n       boolean hasSelected = false, hasSkipped = false;\n       for (int rowGroup = 0; rowGroup < result.length; ++rowGroup) {\n         for (int pred = 0; pred < leafValues.length; ++pred) {\n-          if (filterColumns[pred] != -1) {\n-            OrcProto.ColumnStatistics stats =\n-                indexes[filterColumns[pred]].getEntry(rowGroup).getStatistics();\n+          int columnIx = filterColumns[pred];\n+          if (columnIx != -1) {\n+            if (indexes[columnIx] == null) {\n+              throw new AssertionError(\"Index is not populated for \" + columnIx);\n+            }\n+            RowIndexEntry entry = indexes[columnIx].getEntry(rowGroup);\n+            if (entry == null) {\n+              throw new AssertionError(\"RG is not populated for \" + columnIx + \" rg \" + rowGroup);\n+            }\n+            OrcProto.ColumnStatistics stats = entry.getStatistics();\n             OrcProto.BloomFilter bf = null;\n             if (bloomFilterIndices != null && bloomFilterIndices[filterColumns[pred]] != null) {\n               bf = bloomFilterIndices[filterColumns[pred]].getBloomFilter(rowGroup);\n             }\n             leafValues[pred] = evaluatePredicateProto(stats, sargLeaves.get(pred), bf);\n             if (LOG.isTraceEnabled()) {\n               LOG.trace(\"Stats = \" + stats);\n-              LOG.trace(\"Setting \" + sargLeaves.get(pred) + \" to \" +\n-                  leafValues[pred]);\n+              LOG.trace(\"Setting \" + sargLeaves.get(pred) + \" to \" + leafValues[pred]);\n             }\n           } else {\n             // the column is a virtual column",
                "raw_url": "https://github.com/apache/hive/raw/3b8fdc1dc7228fbd9107ee003d9dbe9e2dfb8692/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java",
                "sha": "3b9856234a0a03b83b393685fca2ac7c7e0a50d8",
                "status": "modified"
            }
        ],
        "message": "HIVE-11222 : LLAP: occasional NPE in parallel queries in ORC reader (Sergey Shelukhin, reviewed by Prasanth Jayachandran)",
        "parent": "https://github.com/apache/hive/commit/fcc45db48ba0ad22a520ce5ec75e615d40a8c277",
        "patched_files": [
            "RecordReaderImpl.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestRecordReaderImpl.java"
        ]
    },
    "hive_3ed7dc2": {
        "bug_id": "hive_3ed7dc2",
        "commit": "https://github.com/apache/hive/commit/3ed7dc2b82f84ade5f2be0cb85a95b49dc30086c",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/3ed7dc2b82f84ade5f2be0cb85a95b49dc30086c/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java?ref=3ed7dc2b82f84ade5f2be0cb85a95b49dc30086c",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java",
                "patch": "@@ -247,6 +247,8 @@\n   TRUNCATE_FOR_NON_MANAGED_TABLE(10146, \"Cannot truncate non-managed table {0}.\", true),\n   TRUNCATE_FOR_NON_NATIVE_TABLE(10147, \"Cannot truncate non-native table {0}.\", true),\n   PARTSPEC_FOR_NON_PARTITIONED_TABLE(10148, \"Partition spec for non partitioned table {0}.\", true),\n+  INVALID_TABLE_IN_ON_CLAUSE_OF_MERGE(10149, \"No columns from target table ''{0}'' found in ON \" +\n+    \"clause ''{1}'' of MERGE statement.\", true),\n \n   LOAD_INTO_STORED_AS_DIR(10195, \"A stored-as-directories table cannot be used as target for LOAD\"),\n   ALTER_TBL_STOREDASDIR_NOT_SKEWED(10196, \"This operation is only valid on skewed table.\"),",
                "raw_url": "https://github.com/apache/hive/raw/3ed7dc2b82f84ade5f2be0cb85a95b49dc30086c/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java",
                "sha": "6013218cdfce53787a41a06f0a280cebecb5f56a",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hive/blob/3ed7dc2b82f84ade5f2be0cb85a95b49dc30086c/ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java?ref=3ed7dc2b82f84ade5f2be0cb85a95b49dc30086c",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java",
                "patch": "@@ -1205,6 +1205,15 @@ private void addColumn2Table(String tableName, String columnName) {\n     private String getPredicate() {\n       //normilize table name for mapping\n       List<String> targetCols = table2column.get(targetTableNameInSourceQuery.toLowerCase());\n+      if(targetCols == null) {\n+        /*e.g. ON source.t=1\n+        * this is not strictly speaking invlaid but it does ensure that all columns from target\n+        * table are all NULL for every row.  This would make any WHEN MATCHED clause invalid since\n+        * we don't have a ROW__ID.  The WHEN NOT MATCHED could be meaningful but it's just data from\n+        * source satisfying source.t=1...  not worth the effort to support this*/\n+        throw new IllegalArgumentException(ErrorMsg.INVALID_TABLE_IN_ON_CLAUSE_OF_MERGE\n+          .format(targetTableNameInSourceQuery, onClauseAsString));\n+      }\n       StringBuilder sb = new StringBuilder();\n       for(String col : targetCols) {\n         if(sb.length() > 0) {",
                "raw_url": "https://github.com/apache/hive/raw/3ed7dc2b82f84ade5f2be0cb85a95b49dc30086c/ql/src/java/org/apache/hadoop/hive/ql/parse/UpdateDeleteSemanticAnalyzer.java",
                "sha": "e798328bce0df4d374977dc14de6e66a688bf41a",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hive/blob/3ed7dc2b82f84ade5f2be0cb85a95b49dc30086c/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java?ref=3ed7dc2b82f84ade5f2be0cb85a95b49dc30086c",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java",
                "patch": "@@ -842,4 +842,13 @@ public void testSetClauseFakeColumn() throws Exception {\n     Assert.assertEquals(ErrorMsg.INVALID_TARGET_COLUMN_IN_SET_CLAUSE,\n       ((HiveException)cpr.getException()).getCanonicalErrorMsg());\n   }\n+  @Test\n+  public void testBadOnClause() throws Exception {\n+    CommandProcessorResponse cpr = runStatementOnDriverNegative(\"merge into \" + Table.ACIDTBL +\n+      \" trgt using (select * from \" + Table.NONACIDORCTBL +\n+      \"src) sub on sub.a = target.a when not matched then insert values (sub.a,sub.b)\");\n+    Assert.assertTrue(\"Error didn't match: \" + cpr, cpr.getErrorMessage().contains(\n+      \"No columns from target table 'trgt' found in ON clause '`sub`.`a` = `target`.`a`' of MERGE statement.\"));\n+\n+  }\n }",
                "raw_url": "https://github.com/apache/hive/raw/3ed7dc2b82f84ade5f2be0cb85a95b49dc30086c/ql/src/test/org/apache/hadoop/hive/ql/TestTxnCommands.java",
                "sha": "c110089614a5972fbc6d22f3302883a743e3b007",
                "status": "modified"
            }
        ],
        "message": "HIVE-15755 NullPointerException on invalid table name in ON clause of Merge statement (Eugene Koifman, reviewed by Wei Zheng)",
        "parent": "https://github.com/apache/hive/commit/3c230a62d5200a2e7f138d51746e09a8d028b3ca",
        "patched_files": [
            "UpdateDeleteSemanticAnalyzer.java",
            "ErrorMsg.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestTxnCommands.java",
            "TestUpdateDeleteSemanticAnalyzer.java",
            "TestErrorMsg.java"
        ]
    },
    "hive_406e935": {
        "bug_id": "hive_406e935",
        "commit": "https://github.com/apache/hive/commit/406e935f27f60bb01c53d54bdb2c91429c95207e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java?ref=406e935f27f60bb01c53d54bdb2c91429c95207e",
                "deletions": 3,
                "filename": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
                "patch": "@@ -2733,10 +2733,10 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n         \"Whether the LLAP IO layer is enabled for non-vectorized queries that read inputs\\n\" +\n         \"that can be vectorized\"),\n     LLAP_IO_MEMORY_MODE(\"hive.llap.io.memory.mode\", \"cache\",\n-        new StringSet(\"cache\", \"allocator\", \"none\"),\n+        new StringSet(\"cache\", \"none\"),\n         \"LLAP IO memory usage; 'cache' (the default) uses data and metadata cache with a\\n\" +\n-        \"custom off-heap allocator, 'allocator' uses the custom allocator without the caches,\\n\" +\n-        \"'none' doesn't use either (this mode may result in significant performance degradation)\"),\n+        \"custom off-heap allocator, 'none' doesn't use either (this mode may result in\\n\" +\n+        \"significant performance degradation)\"),\n     LLAP_ALLOCATOR_MIN_ALLOC(\"hive.llap.io.allocator.alloc.min\", \"16Kb\", new SizeValidator(),\n         \"Minimum allocation possible from LLAP buddy allocator. Allocations below that are\\n\" +\n         \"padded to minimum allocation. For ORC, should generally be the same as the expected\\n\" +",
                "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
                "sha": "cb0d96f50a2d38822cacb09fd98833c925d2af77",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/EvictionDispatcher.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/EvictionDispatcher.java?ref=406e935f27f60bb01c53d54bdb2c91429c95207e",
                "deletions": 2,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/EvictionDispatcher.java",
                "patch": "@@ -26,10 +26,10 @@\n  * Eviction dispatcher - uses double dispatch to route eviction notifications to correct caches.\n  */\n public final class EvictionDispatcher implements EvictionListener {\n-  private final LowLevelCacheImpl dataCache;\n+  private final LowLevelCache dataCache;\n   private final OrcMetadataCache metadataCache;\n \n-  public EvictionDispatcher(LowLevelCacheImpl dataCache, OrcMetadataCache metadataCache) {\n+  public EvictionDispatcher(LowLevelCache dataCache, OrcMetadataCache metadataCache) {\n     this.dataCache = dataCache;\n     this.metadataCache = metadataCache;\n   }",
                "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/EvictionDispatcher.java",
                "sha": "b6fd3e35fcbc4146a03acbf48f4cca8afee21e07",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCache.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCache.java?ref=406e935f27f60bb01c53d54bdb2c91429c95207e",
                "deletions": 0,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCache.java",
                "patch": "@@ -59,4 +59,7 @@ DiskRangeList getFileData(Object fileKey, DiskRangeList range, long baseOffset,\n    */\n   long[] putFileData(Object fileKey, DiskRange[] ranges, MemoryBuffer[] chunks,\n       long baseOffset, Priority priority, LowLevelCacheCounters qfCounters);\n+\n+  /** Notifies the cache that a particular buffer should be removed due to eviction. */\n+  void notifyEvicted(MemoryBuffer buffer);\n }",
                "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCache.java",
                "sha": "19c589a9fd7835c7693344355a3ce8034bdde89c",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java?ref=406e935f27f60bb01c53d54bdb2c91429c95207e",
                "deletions": 2,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java",
                "patch": "@@ -67,7 +67,7 @@ public LowLevelCacheImpl(LlapDaemonCacheMetrics metrics, LowLevelCachePolicy cac\n     this.doAssumeGranularBlocks = doAssumeGranularBlocks;\n   }\n \n-  public void init() {\n+  public void startThreads() {\n     if (cleanupInterval < 0) return;\n     cleanupThread = new CleanupThread(cleanupInterval);\n     cleanupThread.start();\n@@ -368,7 +368,8 @@ public static LlapDataBuffer allocateFake() {\n     return fake;\n   }\n \n-  public final void notifyEvicted(LlapDataBuffer buffer) {\n+  @Override\n+  public final void notifyEvicted(MemoryBuffer buffer) {\n     allocator.deallocateEvicted(buffer);\n     newEvictions.incrementAndGet();\n   }",
                "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelCacheImpl.java",
                "sha": "ea458cab4a3f6b4c8289eff97ee4bcfd6994aa84",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/SimpleAllocator.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/SimpleAllocator.java?ref=406e935f27f60bb01c53d54bdb2c91429c95207e",
                "deletions": 0,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/SimpleAllocator.java",
                "patch": "@@ -68,6 +68,7 @@ public void deallocate(MemoryBuffer buffer) {\n     LlapDataBuffer buf = (LlapDataBuffer)buffer;\n     ByteBuffer bb = buf.byteBuffer;\n     buf.byteBuffer = null;\n+    if (!bb.isDirect()) return;\n     Field field = cleanerField;\n     if (field == null) return;\n     try {",
                "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/SimpleAllocator.java",
                "sha": "d8f59d1fa98e163a39e90b3d623b4c4bebe87169",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/SimpleBufferManager.java",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/SimpleBufferManager.java?ref=406e935f27f60bb01c53d54bdb2c91429c95207e",
                "deletions": 2,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/SimpleBufferManager.java",
                "patch": "@@ -20,12 +20,15 @@\n import java.util.List;\n \n import org.apache.hadoop.hive.common.io.Allocator;\n+import org.apache.hadoop.hive.common.io.DataCache.BooleanRef;\n+import org.apache.hadoop.hive.common.io.DataCache.DiskRangeListFactory;\n+import org.apache.hadoop.hive.common.io.DiskRange;\n+import org.apache.hadoop.hive.common.io.DiskRangeList;\n import org.apache.hadoop.hive.common.io.encoded.MemoryBuffer;\n-import org.apache.hadoop.hive.llap.DebugUtils;\n import org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl;\n import org.apache.hadoop.hive.llap.metrics.LlapDaemonCacheMetrics;\n \n-public class SimpleBufferManager implements BufferUsageManager {\n+public class SimpleBufferManager implements BufferUsageManager, LowLevelCache {\n   private final Allocator allocator;\n   private final LlapDaemonCacheMetrics metrics;\n \n@@ -73,4 +76,30 @@ public boolean incRefBuffer(MemoryBuffer buffer) {\n   public Allocator getAllocator() {\n     return allocator;\n   }\n+\n+  @Override\n+  public DiskRangeList getFileData(Object fileKey, DiskRangeList range, long baseOffset,\n+      DiskRangeListFactory factory, LowLevelCacheCounters qfCounters, BooleanRef gotAllData) {\n+    return range; // Nothing changes - no cache.\n+  }\n+\n+  @Override\n+  public long[] putFileData(Object fileKey, DiskRange[] ranges,\n+      MemoryBuffer[] chunks, long baseOffset, Priority priority,\n+      LowLevelCacheCounters qfCounters) {\n+    for (int i = 0; i < chunks.length; ++i) {\n+      LlapDataBuffer buffer = (LlapDataBuffer)chunks[i];\n+      if (LlapIoImpl.LOCKING_LOGGER.isTraceEnabled()) {\n+        LlapIoImpl.LOCKING_LOGGER.trace(\"Locking {} at put time (no cache)\", buffer);\n+      }\n+      boolean canLock = lockBuffer(buffer);\n+      assert canLock;\n+    }\n+    return null;\n+  }\n+\n+  @Override\n+  public void notifyEvicted(MemoryBuffer buffer) {\n+    throw new UnsupportedOperationException(\"Buffer manager doesn't have cache\");\n+  }\n }",
                "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/cache/SimpleBufferManager.java",
                "sha": "d1eee045499fadcc238a7ca8e28939222e9b1500",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java?ref=406e935f27f60bb01c53d54bdb2c91429c95207e",
                "deletions": 19,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java",
                "patch": "@@ -41,6 +41,7 @@\n import org.apache.hadoop.hive.llap.cache.BufferUsageManager;\n import org.apache.hadoop.hive.llap.cache.EvictionAwareAllocator;\n import org.apache.hadoop.hive.llap.cache.EvictionDispatcher;\n+import org.apache.hadoop.hive.llap.cache.LowLevelCache;\n import org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl;\n import org.apache.hadoop.hive.llap.cache.LowLevelCacheMemoryManager;\n import org.apache.hadoop.hive.llap.cache.LowLevelCachePolicy;\n@@ -71,7 +72,7 @@\n   public static final Logger CACHE_LOGGER = LoggerFactory.getLogger(\"LlapIoCache\");\n   public static final Logger LOCKING_LOGGER = LoggerFactory.getLogger(\"LlapIoLocking\");\n \n-  private static final String MODE_CACHE = \"cache\", MODE_ALLOCATOR = \"allocator\";\n+  private static final String MODE_CACHE = \"cache\";\n \n   private final ColumnVectorProducer cvp;\n   private final ExecutorService executor;\n@@ -82,9 +83,8 @@\n \n   private LlapIoImpl(Configuration conf) throws IOException {\n     String ioMode = HiveConf.getVar(conf, HiveConf.ConfVars.LLAP_IO_MEMORY_MODE);\n-    boolean useLowLevelCache = LlapIoImpl.MODE_CACHE.equalsIgnoreCase(ioMode),\n-        useAllocOnly = !useLowLevelCache && LlapIoImpl.MODE_ALLOCATOR.equalsIgnoreCase(ioMode);\n-    LOG.info(\"Initializing LLAP IO in {} mode\", ioMode);\n+    boolean useLowLevelCache = LlapIoImpl.MODE_CACHE.equalsIgnoreCase(ioMode);\n+    LOG.info(\"Initializing LLAP IO in {} mode\", useLowLevelCache ? LlapIoImpl.MODE_CACHE : \"none\");\n     String displayName = \"LlapDaemonCacheMetrics-\" + MetricsUtils.getHostName();\n     String sessionId = conf.get(\"llap.daemon.metrics.sessionid\");\n     this.cacheMetrics = LlapDaemonCacheMetrics.create(displayName, sessionId);\n@@ -109,7 +109,7 @@ private LlapIoImpl(Configuration conf) throws IOException {\n         sessionId);\n \n     OrcMetadataCache metadataCache = null;\n-    LowLevelCacheImpl orcCache = null;\n+    LowLevelCache cache = null;\n     BufferUsageManager bufferManager = null;\n     if (useLowLevelCache) {\n       // Memory manager uses cache policy to trigger evictions, so create the policy first.\n@@ -122,23 +122,21 @@ private LlapIoImpl(Configuration conf) throws IOException {\n       // Cache uses allocator to allocate and deallocate, create allocator and then caches.\n       EvictionAwareAllocator allocator = new BuddyAllocator(conf, memManager, cacheMetrics);\n       this.allocator = allocator;\n-      orcCache = new LowLevelCacheImpl(cacheMetrics, cachePolicy, allocator, true);\n+      LowLevelCacheImpl cacheImpl = new LowLevelCacheImpl(\n+          cacheMetrics, cachePolicy, allocator, true);\n+      cache = cacheImpl;\n       boolean useGapCache = HiveConf.getBoolVar(conf, ConfVars.LLAP_CACHE_ENABLE_ORC_GAP_CACHE);\n       metadataCache = new OrcMetadataCache(memManager, cachePolicy, useGapCache);\n       // And finally cache policy uses cache to notify it of eviction. The cycle is complete!\n-      cachePolicy.setEvictionListener(new EvictionDispatcher(orcCache, metadataCache));\n-      cachePolicy.setParentDebugDumper(orcCache);\n-      orcCache.init(); // Start the cache threads.\n-      bufferManager = orcCache; // Cache also serves as buffer manager.\n+      cachePolicy.setEvictionListener(new EvictionDispatcher(cache, metadataCache));\n+      cachePolicy.setParentDebugDumper(cacheImpl);\n+      cacheImpl.startThreads(); // Start the cache threads.\n+      bufferManager = cacheImpl; // Cache also serves as buffer manager.\n     } else {\n-      if (useAllocOnly) {\n-        LowLevelCacheMemoryManager memManager = new LowLevelCacheMemoryManager(\n-            conf, null, cacheMetrics);\n-        allocator = new BuddyAllocator(conf, memManager, cacheMetrics);\n-      } else {\n-        allocator = new SimpleAllocator(conf);\n-      }\n-      bufferManager = new SimpleBufferManager(allocator, cacheMetrics);\n+      this.allocator = new SimpleAllocator(conf);\n+      SimpleBufferManager sbm = new SimpleBufferManager(allocator, cacheMetrics);\n+      bufferManager = sbm;\n+      cache = sbm;\n     }\n     // IO thread pool. Listening is used for unhandled errors for now (TODO: remove?)\n     int numThreads = HiveConf.getIntVar(conf, HiveConf.ConfVars.LLAP_IO_THREADPOOL_SIZE);\n@@ -148,7 +146,7 @@ private LlapIoImpl(Configuration conf) throws IOException {\n         new ThreadFactoryBuilder().setNameFormat(\"IO-Elevator-Thread-%d\").setDaemon(true).build());\n     // TODO: this should depends on input format and be in a map, or something.\n     this.cvp = new OrcColumnVectorProducer(\n-        metadataCache, orcCache, bufferManager, conf, cacheMetrics, ioMetrics);\n+        metadataCache, cache, bufferManager, conf, cacheMetrics, ioMetrics);\n     LOG.info(\"LLAP IO initialized\");\n \n     registerMXBeans();",
                "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapIoImpl.java",
                "sha": "8048624a2b7c0562525d2ce147f8ca1a9de1c8ff",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java?ref=406e935f27f60bb01c53d54bdb2c91429c95207e",
                "deletions": 2,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java",
                "patch": "@@ -25,7 +25,6 @@\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.llap.cache.BufferUsageManager;\n import org.apache.hadoop.hive.llap.cache.LowLevelCache;\n-import org.apache.hadoop.hive.llap.cache.LowLevelCacheImpl;\n import org.apache.hadoop.hive.llap.counters.QueryFragmentCounters;\n import org.apache.hadoop.hive.llap.io.api.impl.ColumnVectorBatch;\n import org.apache.hadoop.hive.llap.io.api.impl.LlapIoImpl;\n@@ -48,7 +47,7 @@\n   private LlapDaemonIOMetrics ioMetrics;\n \n   public OrcColumnVectorProducer(OrcMetadataCache metadataCache,\n-      LowLevelCacheImpl lowLevelCache, BufferUsageManager bufferManager,\n+      LowLevelCache lowLevelCache, BufferUsageManager bufferManager,\n       Configuration conf, LlapDaemonCacheMetrics cacheMetrics, LlapDaemonIOMetrics ioMetrics) {\n     LlapIoImpl.LOG.info(\"Initializing ORC column vector producer\");\n ",
                "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/io/decode/OrcColumnVectorProducer.java",
                "sha": "12275acd5a39dcb3e5536c011d12c1c8afd38d55",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java?ref=406e935f27f60bb01c53d54bdb2c91429c95207e",
                "deletions": 6,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java",
                "patch": "@@ -835,12 +835,11 @@ public DataWrapperForOrc() throws IOException {\n     @Override\n     public DiskRangeList getFileData(Object fileKey, DiskRangeList range,\n         long baseOffset, DiskRangeListFactory factory, BooleanRef gotAllData) {\n-      DiskRangeList result = (lowLevelCache == null) ? range\n-          : lowLevelCache.getFileData(fileKey, range, baseOffset, factory, counters, gotAllData);\n+      DiskRangeList result = lowLevelCache.getFileData(\n+          fileKey, range, baseOffset, factory, counters, gotAllData);\n       if (LlapIoImpl.ORC_LOGGER.isTraceEnabled()) {\n-        LlapIoImpl.ORC_LOGGER.trace(\"Disk ranges after data cache (file \" + fileKey\n-            + \", base offset \" + baseOffset + \"): \"\n-            + RecordReaderUtils.stringifyDiskRanges(range.next));\n+        LlapIoImpl.ORC_LOGGER.trace(\"Disk ranges after data cache (file \" + fileKey +\n+            \", base offset \" + baseOffset + \"): \" + RecordReaderUtils.stringifyDiskRanges(range));\n       }\n       if (gotAllData.value) return result;\n       return (metadataCache == null) ? range\n@@ -851,7 +850,7 @@ public DiskRangeList getFileData(Object fileKey, DiskRangeList range,\n     public long[] putFileData(Object fileKey, DiskRange[] ranges,\n         MemoryBuffer[] data, long baseOffset) {\n       if (data != null) {\n-        return (lowLevelCache == null) ? null : lowLevelCache.putFileData(\n+        return lowLevelCache.putFileData(\n             fileKey, ranges, data, baseOffset, Priority.NORMAL, counters);\n       } else if (metadataCache != null) {\n         metadataCache.putIncompleteCbs(fileKey, ranges, baseOffset);",
                "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java",
                "sha": "eb8ee6cd4c10e82f8adcfed407c58854828aceb9",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/main/resources/llap-daemon-log4j2.properties",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/main/resources/llap-daemon-log4j2.properties?ref=406e935f27f60bb01c53d54bdb2c91429c95207e",
                "deletions": 2,
                "filename": "llap-server/src/main/resources/llap-daemon-log4j2.properties",
                "patch": "@@ -100,7 +100,10 @@ appender.query-routing.routes.route-mdc.file-mdc.app.layout.type = PatternLayout\n appender.query-routing.routes.route-mdc.file-mdc.app.layout.pattern = %d{ISO8601} %5p [%t (%X{fragmentId})] %c{2}: %m%n\n \n # list of all loggers\n-loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, HistoryLogger, LlapIoImpl, LlapIoOrc, LlapIoCache, LlapIoLocking\n+loggers = EncodedReader, NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX, HistoryLogger, LlapIoImpl, LlapIoOrc, LlapIoCache, LlapIoLocking\n+\n+logger.EncodedReader.name = org.apache.hadoop.hive.ql.io.orc.encoded.EncodedReaderImpl\n+logger.EncodedReader.level = INFO\n \n logger.LlapIoImpl.name = LlapIoImpl\n logger.LlapIoImpl.level = INFO\n@@ -109,7 +112,7 @@ logger.LlapIoOrc.name = LlapIoOrc\n logger.LlapIoOrc.level = WARN\n \n logger.LlapIoCache.name = LlapIoCache\n-logger.LlapIOCache.level = WARN\n+logger.LlapIoCache.level = WARN\n \n logger.LlapIoLocking.name = LlapIoLocking\n logger.LlapIoLocking.level = WARN",
                "raw_url": "https://github.com/apache/hive/raw/406e935f27f60bb01c53d54bdb2c91429c95207e/llap-server/src/main/resources/llap-daemon-log4j2.properties",
                "sha": "0c953d1fa58f79d60bb059cf1110269285570358",
                "status": "modified"
            }
        ],
        "message": "HIVE-14621 : LLAP: memory.mode = none has NPE (Sergey Shelukhin, reviewed by Prasanth Jayachandran)",
        "parent": "https://github.com/apache/hive/commit/0705323db28edf13777d29d3a0add48f19936db0",
        "patched_files": [
            "HiveConf.java",
            "LowLevelCacheImpl.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestLowLevelCacheImpl.java",
            "TestHiveConf.java"
        ]
    },
    "hive_4156c5d": {
        "bug_id": "hive_4156c5d",
        "commit": "https://github.com/apache/hive/commit/4156c5da5099e3fa9b220229fe99ef0d609cd7ac",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/itests/src/test/resources/testconfiguration.properties",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=4156c5da5099e3fa9b220229fe99ef0d609cd7ac",
                "deletions": 0,
                "filename": "itests/src/test/resources/testconfiguration.properties",
                "patch": "@@ -70,6 +70,7 @@ disabled.query.files=ql_rewrite_gbtoidx.q,\\\n   smb_mapjoin_8.q\n \n minitez.query.files.shared=acid_globallimit.q,\\\n+  deleteAnalyze.q,\\\n   empty_join.q,\\\n   alter_merge_2_orc.q,\\\n   alter_merge_orc.q,\\",
                "raw_url": "https://github.com/apache/hive/raw/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/itests/src/test/resources/testconfiguration.properties",
                "sha": "c891d40cbfba32e4c5035cd514dadf9d4f519223",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hive/blob/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseUtils.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseUtils.java?ref=4156c5da5099e3fa9b220229fe99ef0d609cd7ac",
                "deletions": 11,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseUtils.java",
                "patch": "@@ -1205,17 +1205,22 @@ static StorageDescriptorParts deserializeTable(String dbName, String tableName,\n       if (decimalData.isSetBitVectors()) {\n         builder.setBitVectors(decimalData.getBitVectors());\n       }\n-      builder.setDecimalStats(\n-          HbaseMetastoreProto.ColumnStats.DecimalStats\n-              .newBuilder()\n-              .setLowValue(\n-                  HbaseMetastoreProto.ColumnStats.DecimalStats.Decimal.newBuilder()\n-                      .setUnscaled(ByteString.copyFrom(decimalData.getLowValue().getUnscaled()))\n-                      .setScale(decimalData.getLowValue().getScale()).build())\n-              .setHighValue(\n-                  HbaseMetastoreProto.ColumnStats.DecimalStats.Decimal.newBuilder()\n-                      .setUnscaled(ByteString.copyFrom(decimalData.getHighValue().getUnscaled()))\n-                      .setScale(decimalData.getHighValue().getScale()).build())).build();\n+      if (decimalData.getLowValue() != null && decimalData.getHighValue() != null) {\n+        builder.setDecimalStats(\n+            HbaseMetastoreProto.ColumnStats.DecimalStats\n+                .newBuilder()\n+                .setLowValue(\n+                    HbaseMetastoreProto.ColumnStats.DecimalStats.Decimal.newBuilder()\n+                        .setUnscaled(ByteString.copyFrom(decimalData.getLowValue().getUnscaled()))\n+                        .setScale(decimalData.getLowValue().getScale()).build())\n+                .setHighValue(\n+                    HbaseMetastoreProto.ColumnStats.DecimalStats.Decimal.newBuilder()\n+                        .setUnscaled(ByteString.copyFrom(decimalData.getHighValue().getUnscaled()))\n+                        .setScale(decimalData.getHighValue().getScale()).build())).build();\n+      } else {\n+        builder.setDecimalStats(HbaseMetastoreProto.ColumnStats.DecimalStats.newBuilder().clear()\n+            .build());\n+      }\n       break;\n \n     default:",
                "raw_url": "https://github.com/apache/hive/raw/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseUtils.java",
                "sha": "d1cff0691559ad6f4f340eb6a0aff9e80b4d77af",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java?ref=4156c5da5099e3fa9b220229fe99ef0d609cd7ac",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java",
                "patch": "@@ -412,7 +412,7 @@ private void completeInitialization(Collection<Future<?>> fs) throws HiveExcepti\n   }\n \n   /**\n-   * This metod can be used to retrieve the results from async operations\n+   * This method can be used to retrieve the results from async operations\n    * started at init time - before the operator pipeline is started.\n    *\n    * @param os",
                "raw_url": "https://github.com/apache/hive/raw/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/java/org/apache/hadoop/hive/ql/exec/Operator.java",
                "sha": "636f079471a7f92eef99ee94a156f0b1946cd25c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java?ref=4156c5da5099e3fa9b220229fe99ef0d609cd7ac",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java",
                "patch": "@@ -1792,6 +1792,7 @@ private long computeNewRowCount(List<Long> rowCountParents, long denom) {\n         }\n       }\n \n+      denom = denom == 0 ? 1 : denom;\n       factor = (double) max / (double) denom;\n \n       for (int i = 0; i < rowCountParents.size(); i++) {",
                "raw_url": "https://github.com/apache/hive/raw/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/java/org/apache/hadoop/hive/ql/optimizer/stats/annotation/StatsRulesProcFactory.java",
                "sha": "3944e10d38836d6fd5dd65b12468147d6574a106",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hive/blob/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/test/queries/clientpositive/deleteAnalyze.q",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/deleteAnalyze.q?ref=4156c5da5099e3fa9b220229fe99ef0d609cd7ac",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/deleteAnalyze.q",
                "patch": "@@ -0,0 +1,31 @@\n+set hive.stats.autogather=true;\n+\n+dfs ${system:test.dfs.mkdir} ${system:test.tmp.dir}/testdeci2;\n+\n+create table testdeci2(\n+id int,\n+amount decimal(10,3),\n+sales_tax decimal(10,3),\n+item string)\n+stored as orc location '${system:test.tmp.dir}/testdeci2';\n+\n+insert into table testdeci2 values(1,12.123,12345.123,'desk1'),(2,123.123,1234.123,'desk2');\n+\n+describe formatted testdeci2;\n+\n+dfs -rmr ${system:test.tmp.dir}/testdeci2/000000_0;\n+\n+describe formatted testdeci2 amount;\n+\n+analyze table testdeci2 compute statistics for columns;\n+\n+set hive.stats.fetch.column.stats=true;\n+\n+analyze table testdeci2 compute statistics for columns;\n+\n+explain\n+select s.id,\n+coalesce(d.amount,0) as sales,\n+coalesce(d.sales_tax,0) as tax\n+from testdeci2 s join testdeci2 d\n+on s.item=d.item and d.id=2;",
                "raw_url": "https://github.com/apache/hive/raw/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/test/queries/clientpositive/deleteAnalyze.q",
                "sha": "7e5371cc6154c235dfada3ba419f0b838d24434c",
                "status": "added"
            },
            {
                "additions": 173,
                "blob_url": "https://github.com/apache/hive/blob/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/test/results/clientpositive/deleteAnalyze.q.out",
                "changes": 173,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/deleteAnalyze.q.out?ref=4156c5da5099e3fa9b220229fe99ef0d609cd7ac",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/deleteAnalyze.q.out",
                "patch": "@@ -0,0 +1,173 @@\n+PREHOOK: query: create table testdeci2(\n+id int,\n+amount decimal(10,3),\n+sales_tax decimal(10,3),\n+item string)\n+#### A masked pattern was here ####\n+PREHOOK: type: CREATETABLE\n+#### A masked pattern was here ####\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@testdeci2\n+POSTHOOK: query: create table testdeci2(\n+id int,\n+amount decimal(10,3),\n+sales_tax decimal(10,3),\n+item string)\n+#### A masked pattern was here ####\n+POSTHOOK: type: CREATETABLE\n+#### A masked pattern was here ####\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@testdeci2\n+PREHOOK: query: insert into table testdeci2 values(1,12.123,12345.123,'desk1'),(2,123.123,1234.123,'desk2')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@values__tmp__table__1\n+PREHOOK: Output: default@testdeci2\n+POSTHOOK: query: insert into table testdeci2 values(1,12.123,12345.123,'desk1'),(2,123.123,1234.123,'desk2')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@values__tmp__table__1\n+POSTHOOK: Output: default@testdeci2\n+POSTHOOK: Lineage: testdeci2.amount EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col2, type:string, comment:), ]\n+POSTHOOK: Lineage: testdeci2.id EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+POSTHOOK: Lineage: testdeci2.item SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col4, type:string, comment:), ]\n+POSTHOOK: Lineage: testdeci2.sales_tax EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col3, type:string, comment:), ]\n+PREHOOK: query: describe formatted testdeci2\n+PREHOOK: type: DESCTABLE\n+PREHOOK: Input: default@testdeci2\n+POSTHOOK: query: describe formatted testdeci2\n+POSTHOOK: type: DESCTABLE\n+POSTHOOK: Input: default@testdeci2\n+# col_name            \tdata_type           \tcomment             \n+\t \t \n+id                  \tint                 \t                    \n+amount              \tdecimal(10,3)       \t                    \n+sales_tax           \tdecimal(10,3)       \t                    \n+item                \tstring              \t                    \n+\t \t \n+# Detailed Table Information\t \t \n+Database:           \tdefault             \t \n+#### A masked pattern was here ####\n+Retention:          \t0                   \t \n+#### A masked pattern was here ####\n+Table Type:         \tMANAGED_TABLE       \t \n+Table Parameters:\t \t \n+\tnumFiles            \t1                   \n+\ttotalSize           \t578                 \n+#### A masked pattern was here ####\n+\t \t \n+# Storage Information\t \t \n+SerDe Library:      \torg.apache.hadoop.hive.ql.io.orc.OrcSerde\t \n+InputFormat:        \torg.apache.hadoop.hive.ql.io.orc.OrcInputFormat\t \n+OutputFormat:       \torg.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\t \n+Compressed:         \tNo                  \t \n+Num Buckets:        \t-1                  \t \n+Bucket Columns:     \t[]                  \t \n+Sort Columns:       \t[]                  \t \n+Storage Desc Params:\t \t \n+\tserialization.format\t1                   \n+#### A masked pattern was here ####\n+PREHOOK: query: describe formatted testdeci2 amount\n+PREHOOK: type: DESCTABLE\n+PREHOOK: Input: default@testdeci2\n+POSTHOOK: query: describe formatted testdeci2 amount\n+POSTHOOK: type: DESCTABLE\n+POSTHOOK: Input: default@testdeci2\n+# col_name            \tdata_type           \tmin                 \tmax                 \tnum_nulls           \tdistinct_count      \tavg_col_len         \tmax_col_len         \tnum_trues           \tnum_falses          \tcomment             \n+\t \t \t \t \t \t \t \t \t \t \n+amount              \tdecimal(10,3)       \t                    \t                    \t                    \t                    \t                    \t                    \t                    \t                    \tfrom deserializer   \n+PREHOOK: query: analyze table testdeci2 compute statistics for columns\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@testdeci2\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table testdeci2 compute statistics for columns\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@testdeci2\n+#### A masked pattern was here ####\n+PREHOOK: query: analyze table testdeci2 compute statistics for columns\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@testdeci2\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table testdeci2 compute statistics for columns\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@testdeci2\n+#### A masked pattern was here ####\n+PREHOOK: query: explain\n+select s.id,\n+coalesce(d.amount,0) as sales,\n+coalesce(d.sales_tax,0) as tax\n+from testdeci2 s join testdeci2 d\n+on s.item=d.item and d.id=2\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select s.id,\n+coalesce(d.amount,0) as sales,\n+coalesce(d.sales_tax,0) as tax\n+from testdeci2 s join testdeci2 d\n+on s.item=d.item and d.id=2\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: s\n+            Statistics: Num rows: 5 Data size: 440 Basic stats: COMPLETE Column stats: COMPLETE\n+            Filter Operator\n+              predicate: item is not null (type: boolean)\n+              Statistics: Num rows: 5 Data size: 440 Basic stats: COMPLETE Column stats: COMPLETE\n+              Select Operator\n+                expressions: id (type: int), item (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 5 Data size: 440 Basic stats: COMPLETE Column stats: COMPLETE\n+                Reduce Output Operator\n+                  key expressions: _col1 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col1 (type: string)\n+                  Statistics: Num rows: 5 Data size: 440 Basic stats: COMPLETE Column stats: COMPLETE\n+                  value expressions: _col0 (type: int)\n+          TableScan\n+            alias: s\n+            Statistics: Num rows: 1 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE\n+            Filter Operator\n+              predicate: ((id = 2) and item is not null) (type: boolean)\n+              Statistics: Num rows: 1 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE\n+              Select Operator\n+                expressions: amount (type: decimal(10,3)), sales_tax (type: decimal(10,3)), item (type: string)\n+                outputColumnNames: _col1, _col2, _col3\n+                Statistics: Num rows: 1 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE\n+                Reduce Output Operator\n+                  key expressions: _col3 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col3 (type: string)\n+                  Statistics: Num rows: 1 Data size: 312 Basic stats: COMPLETE Column stats: COMPLETE\n+                  value expressions: _col1 (type: decimal(10,3)), _col2 (type: decimal(10,3))\n+      Reduce Operator Tree:\n+        Join Operator\n+          condition map:\n+               Inner Join 0 to 1\n+          keys:\n+            0 _col1 (type: string)\n+            1 _col3 (type: string)\n+          outputColumnNames: _col0, _col3, _col4\n+          Statistics: Num rows: 5 Data size: 1140 Basic stats: COMPLETE Column stats: COMPLETE\n+          Select Operator\n+            expressions: _col0 (type: int), COALESCE(_col3,0) (type: decimal(13,3)), COALESCE(_col4,0) (type: decimal(13,3))\n+            outputColumnNames: _col0, _col1, _col2\n+            Statistics: Num rows: 5 Data size: 1140 Basic stats: COMPLETE Column stats: COMPLETE\n+            File Output Operator\n+              compressed: false\n+              Statistics: Num rows: 5 Data size: 1140 Basic stats: COMPLETE Column stats: COMPLETE\n+              table:\n+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+",
                "raw_url": "https://github.com/apache/hive/raw/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/test/results/clientpositive/deleteAnalyze.q.out",
                "sha": "7b9391b36f0371b5c2639973091c2c27efef6192",
                "status": "added"
            },
            {
                "additions": 140,
                "blob_url": "https://github.com/apache/hive/blob/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/test/results/clientpositive/tez/deleteAnalyze.q.out",
                "changes": 140,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/deleteAnalyze.q.out?ref=4156c5da5099e3fa9b220229fe99ef0d609cd7ac",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/tez/deleteAnalyze.q.out",
                "patch": "@@ -0,0 +1,140 @@\n+PREHOOK: query: create table testdeci2(\n+id int,\n+amount decimal(10,3),\n+sales_tax decimal(10,3),\n+item string)\n+#### A masked pattern was here ####\n+PREHOOK: type: CREATETABLE\n+#### A masked pattern was here ####\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@testdeci2\n+POSTHOOK: query: create table testdeci2(\n+id int,\n+amount decimal(10,3),\n+sales_tax decimal(10,3),\n+item string)\n+#### A masked pattern was here ####\n+POSTHOOK: type: CREATETABLE\n+#### A masked pattern was here ####\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@testdeci2\n+PREHOOK: query: insert into table testdeci2 values(1,12.123,12345.123,'desk1'),(2,123.123,1234.123,'desk2')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@values__tmp__table__1\n+PREHOOK: Output: default@testdeci2\n+POSTHOOK: query: insert into table testdeci2 values(1,12.123,12345.123,'desk1'),(2,123.123,1234.123,'desk2')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@values__tmp__table__1\n+POSTHOOK: Output: default@testdeci2\n+POSTHOOK: Lineage: testdeci2.amount EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col2, type:string, comment:), ]\n+POSTHOOK: Lineage: testdeci2.id EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+POSTHOOK: Lineage: testdeci2.item SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col4, type:string, comment:), ]\n+POSTHOOK: Lineage: testdeci2.sales_tax EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col3, type:string, comment:), ]\n+PREHOOK: query: describe formatted testdeci2\n+PREHOOK: type: DESCTABLE\n+PREHOOK: Input: default@testdeci2\n+POSTHOOK: query: describe formatted testdeci2\n+POSTHOOK: type: DESCTABLE\n+POSTHOOK: Input: default@testdeci2\n+# col_name            \tdata_type           \tcomment             \n+\t \t \n+id                  \tint                 \t                    \n+amount              \tdecimal(10,3)       \t                    \n+sales_tax           \tdecimal(10,3)       \t                    \n+item                \tstring              \t                    \n+\t \t \n+# Detailed Table Information\t \t \n+Database:           \tdefault             \t \n+#### A masked pattern was here ####\n+Retention:          \t0                   \t \n+#### A masked pattern was here ####\n+Table Type:         \tMANAGED_TABLE       \t \n+Table Parameters:\t \t \n+\tnumFiles            \t1                   \n+\ttotalSize           \t578                 \n+#### A masked pattern was here ####\n+\t \t \n+# Storage Information\t \t \n+SerDe Library:      \torg.apache.hadoop.hive.ql.io.orc.OrcSerde\t \n+InputFormat:        \torg.apache.hadoop.hive.ql.io.orc.OrcInputFormat\t \n+OutputFormat:       \torg.apache.hadoop.hive.ql.io.orc.OrcOutputFormat\t \n+Compressed:         \tNo                  \t \n+Num Buckets:        \t-1                  \t \n+Bucket Columns:     \t[]                  \t \n+Sort Columns:       \t[]                  \t \n+Storage Desc Params:\t \t \n+\tserialization.format\t1                   \n+#### A masked pattern was here ####\n+PREHOOK: query: describe formatted testdeci2 amount\n+PREHOOK: type: DESCTABLE\n+PREHOOK: Input: default@testdeci2\n+POSTHOOK: query: describe formatted testdeci2 amount\n+POSTHOOK: type: DESCTABLE\n+POSTHOOK: Input: default@testdeci2\n+# col_name            \tdata_type           \tmin                 \tmax                 \tnum_nulls           \tdistinct_count      \tavg_col_len         \tmax_col_len         \tnum_trues           \tnum_falses          \tcomment             \n+\t \t \t \t \t \t \t \t \t \t \n+amount              \tdecimal(10,3)       \t                    \t                    \t                    \t                    \t                    \t                    \t                    \t                    \tfrom deserializer   \n+PREHOOK: query: analyze table testdeci2 compute statistics for columns\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@testdeci2\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table testdeci2 compute statistics for columns\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@testdeci2\n+#### A masked pattern was here ####\n+PREHOOK: query: analyze table testdeci2 compute statistics for columns\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@testdeci2\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table testdeci2 compute statistics for columns\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@testdeci2\n+#### A masked pattern was here ####\n+PREHOOK: query: explain\n+select s.id,\n+coalesce(d.amount,0) as sales,\n+coalesce(d.sales_tax,0) as tax\n+from testdeci2 s join testdeci2 d\n+on s.item=d.item and d.id=2\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select s.id,\n+coalesce(d.amount,0) as sales,\n+coalesce(d.sales_tax,0) as tax\n+from testdeci2 s join testdeci2 d\n+on s.item=d.item and d.id=2\n+POSTHOOK: type: QUERY\n+Plan optimized by CBO.\n+\n+Vertex dependency in root stage\n+Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 3 (SIMPLE_EDGE)\n+\n+Stage-0\n+  Fetch Operator\n+    limit:-1\n+    Stage-1\n+      Reducer 2\n+      File Output Operator [FS_10]\n+        Select Operator [SEL_9] (rows=5 width=228)\n+          Output:[\"_col0\",\"_col1\",\"_col2\"]\n+          Merge Join Operator [MERGEJOIN_15] (rows=5 width=228)\n+            Conds:RS_6._col1=RS_7._col3(Inner),Output:[\"_col0\",\"_col3\",\"_col4\"]\n+          <-Map 1 [SIMPLE_EDGE]\n+            SHUFFLE [RS_6]\n+              PartitionCols:_col1\n+              Select Operator [SEL_2] (rows=5 width=88)\n+                Output:[\"_col0\",\"_col1\"]\n+                Filter Operator [FIL_13] (rows=5 width=88)\n+                  predicate:item is not null\n+                  TableScan [TS_0] (rows=5 width=88)\n+                    default@testdeci2,s,Tbl:COMPLETE,Col:COMPLETE,Output:[\"id\",\"item\"]\n+          <-Map 3 [SIMPLE_EDGE]\n+            SHUFFLE [RS_7]\n+              PartitionCols:_col3\n+              Select Operator [SEL_5] (rows=1 width=312)\n+                Output:[\"_col1\",\"_col2\",\"_col3\"]\n+                Filter Operator [FIL_14] (rows=1 width=312)\n+                  predicate:((id = 2) and item is not null)\n+                  TableScan [TS_3] (rows=1 width=312)\n+                    default@testdeci2,s,Tbl:COMPLETE,Col:COMPLETE,Output:[\"id\",\"amount\",\"sales_tax\",\"item\"]\n+",
                "raw_url": "https://github.com/apache/hive/raw/4156c5da5099e3fa9b220229fe99ef0d609cd7ac/ql/src/test/results/clientpositive/tez/deleteAnalyze.q.out",
                "sha": "47f2a201b078d95988f82730800ad86f6f945cef",
                "status": "added"
            }
        ],
        "message": "HIVE-13621: compute stats in certain cases fails with NPE (Vikram Dixit K, Pengcheng Xiong, reviewed by Gunther Hagleitner)",
        "parent": "https://github.com/apache/hive/commit/64c96e1e92a490a069f9cc924b2ec476187f98ea",
        "patched_files": [
            "Operator.java",
            "deleteAnalyze.java",
            "HBaseUtils.java",
            "StatsRulesProcFactory.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "testconfiguration.java"
        ]
    },
    "hive_4271bbf": {
        "bug_id": "hive_4271bbf",
        "commit": "https://github.com/apache/hive/commit/4271bbfb02ae81901b1b44e15e0ca6bd1407de9d",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/4271bbfb02ae81901b1b44e15e0ca6bd1407de9d/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosExternalTables.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosExternalTables.java?ref=4271bbfb02ae81901b1b44e15e0ca6bd1407de9d",
                "deletions": 1,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosExternalTables.java",
                "patch": "@@ -216,7 +216,13 @@ public void externalTableReplicationWithCustomPaths() throws Throwable {\n     DistributedFileSystem fs = primary.miniDFSCluster.getFileSystem();\n     fs.mkdirs(externalTableLocation, new FsPermission(\"777\"));\n \n-    List<String> loadWithClause = externalTableBasePathWithClause();\n+    // Create base directory but use HDFS path without schema or authority details.\n+    // Hive should pick up the local cluster's HDFS schema/authority.\n+    externalTableBasePathWithClause();\n+    List<String> loadWithClause = Collections.singletonList(\n+            \"'\" + HiveConf.ConfVars.REPL_EXTERNAL_TABLE_BASE_DIR.varname + \"'='\"\n+                    + REPLICA_EXTERNAL_BASE + \"'\"\n+    );\n \n     WarehouseInstance.Tuple bootstrapTuple = primary.run(\"use \" + primaryDbName)\n         .run(\"create external table a (i int, j int) \"",
                "raw_url": "https://github.com/apache/hive/raw/4271bbfb02ae81901b1b44e15e0ca6bd1407de9d/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosExternalTables.java",
                "sha": "40ce4b4518ff557331c75126e924726cda0dbd8f",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hive/blob/4271bbfb02ae81901b1b44e15e0ca6bd1407de9d/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplExternalTables.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplExternalTables.java?ref=4271bbfb02ae81901b1b44e15e0ca6bd1407de9d",
                "deletions": 7,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplExternalTables.java",
                "patch": "@@ -23,6 +23,7 @@\n import org.apache.hadoop.hive.common.FileUtils;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.TableType;\n+import org.apache.hadoop.hive.ql.ErrorMsg;\n import org.apache.hadoop.hive.ql.metadata.Hive;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hadoop.hive.ql.metadata.Partition;\n@@ -38,7 +39,6 @@\n import java.io.InputStreamReader;\n import java.io.OutputStream;\n import java.io.StringWriter;\n-import java.net.URI;\n import java.nio.charset.StandardCharsets;\n import java.util.Base64;\n import java.util.HashSet;\n@@ -62,13 +62,21 @@\n \n   private ReplExternalTables(){}\n \n-  public static String externalTableLocation(HiveConf hiveConf, String location) {\n-    String currentPath = new Path(location).toUri().getPath();\n+  public static String externalTableLocation(HiveConf hiveConf, String location) throws SemanticException {\n     String baseDir = hiveConf.get(HiveConf.ConfVars.REPL_EXTERNAL_TABLE_BASE_DIR.varname);\n-    URI basePath = new Path(baseDir).toUri();\n-    String dataPath = currentPath.replaceFirst(Path.SEPARATOR, basePath.getPath() + Path.SEPARATOR);\n-    Path dataLocation = new Path(basePath.getScheme(), basePath.getAuthority(), dataPath);\n-    LOG.debug(\"incoming location: {} , new location: {}\", location, dataLocation.toString());\n+    Path basePath = new Path(baseDir);\n+    Path currentPath = new Path(location);\n+    String targetPathWithoutSchemeAndAuth = basePath.toUri().getPath() + currentPath.toUri().getPath();\n+    Path dataLocation;\n+    try {\n+      dataLocation = PathBuilder.fullyQualifiedHDFSUri(\n+              new Path(targetPathWithoutSchemeAndAuth),\n+              basePath.getFileSystem(hiveConf)\n+      );\n+    } catch (IOException e) {\n+      throw new SemanticException(ErrorMsg.INVALID_PATH.getMsg(), e);\n+    }\n+    LOG.info(\"Incoming external table location: {} , new location: {}\", location, dataLocation.toString());\n     return dataLocation.toString();\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/4271bbfb02ae81901b1b44e15e0ca6bd1407de9d/ql/src/java/org/apache/hadoop/hive/ql/exec/repl/ReplExternalTables.java",
                "sha": "59b7c1c2ccb38d743bd4582014e790f8f8fb810e",
                "status": "modified"
            }
        ],
        "message": "HIVE-21186: External tables replication throws NPE if hive.repl.replica.external.table.base.dir is not fully qualified HDFS path (Sankar Hariappan, reviewed by Mahesh Kumar Behera)\n\nSigned-off-by: Sankar Hariappan <sankarh@apache.org>",
        "parent": "https://github.com/apache/hive/commit/dfc4b8edbd1ad8c394634c67fbd1f06ba03e4d7f",
        "patched_files": [
            "ReplExternalTables.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestReplicationScenariosExternalTables.java"
        ]
    },
    "hive_43e2f96": {
        "bug_id": "hive_43e2f96",
        "commit": "https://github.com/apache/hive/commit/43e2f9632130d569c93ebeeb11297813ca34b80c",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/43e2f9632130d569c93ebeeb11297813ca34b80c/llap-server/src/test/org/apache/hadoop/hive/llap/security/TestLlapSignerImpl.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/test/org/apache/hadoop/hive/llap/security/TestLlapSignerImpl.java?ref=43e2f9632130d569c93ebeeb11297813ca34b80c",
                "deletions": 2,
                "filename": "llap-server/src/test/org/apache/hadoop/hive/llap/security/TestLlapSignerImpl.java",
                "patch": "@@ -106,9 +106,9 @@ public void testSigning() throws Exception {\n   private FakeSecretManager rollKey(FakeSecretManager fsm, int idToPreserve) throws IOException {\n     // Adding keys is PITA - there's no way to plug into timed rolling; just create a new fsm.\n     DelegationKey dk = fsm.getDelegationKey(idToPreserve), curDk = fsm.getCurrentKey();\n-    if (curDk.getKeyId() != idToPreserve) {\n+    if (curDk == null || curDk.getKeyId() != idToPreserve) {\n       LOG.warn(\"The current key is not the one we expect; key rolled in background? Signed with \"\n-          + idToPreserve + \" but got \" + curDk.getKeyId());\n+          + idToPreserve + \" but got \" + (curDk == null ? \"null\" : curDk.getKeyId()));\n     }\n     // Regardless of the above, we should have the key we've signed with.\n     assertNotNull(dk);",
                "raw_url": "https://github.com/apache/hive/raw/43e2f9632130d569c93ebeeb11297813ca34b80c/llap-server/src/test/org/apache/hadoop/hive/llap/security/TestLlapSignerImpl.java",
                "sha": "6be6836bdbcf5dc160bfa2926cf4c4c5f31522dd",
                "status": "modified"
            }
        ],
        "message": "HIVE-19628 : possible NPE in LLAP testSigning (Sergey Shelukhin, reviewed by Jason Dere)",
        "parent": "https://github.com/apache/hive/commit/f4352e5339694d290b1a146feb2577d4f96d14eb",
        "patched_files": [
            "LlapSignerImpl.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestLlapSignerImpl.java"
        ]
    },
    "hive_460d97d": {
        "bug_id": "hive_460d97d",
        "commit": "https://github.com/apache/hive/commit/460d97daa215607674ea7516fec1692c443d7bc8",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/CreateTableHook.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/CreateTableHook.java?ref=460d97daa215607674ea7516fec1692c443d7bc8",
                "deletions": 3,
                "filename": "hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/CreateTableHook.java",
                "patch": "@@ -26,7 +26,6 @@\n \n import org.apache.commons.lang.StringUtils;\n import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.api.FieldSchema;\n import org.apache.hadoop.hive.ql.exec.DDLTask;\n import org.apache.hadoop.hive.ql.exec.Task;\n@@ -195,8 +194,7 @@ public void postAnalyze(HiveSemanticAnalyzerHookContext context,\n \n         //authorize against the table operation so that location permissions can be checked if any\n \n-        if (HiveConf.getBoolVar(context.getConf(),\n-          HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED)) {\n+        if (HCatAuthUtil.isAuthorizationEnabled(context.getConf())) {\n           authorize(table, Privilege.CREATE);\n         }\n       } catch (HiveException ex) {",
                "raw_url": "https://github.com/apache/hive/raw/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/CreateTableHook.java",
                "sha": "8b6922325e3a7096e8e048dc1664a38e16efc039",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/hive/blob/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatAuthUtil.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatAuthUtil.java?ref=460d97daa215607674ea7516fec1692c443d7bc8",
                "deletions": 0,
                "filename": "hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatAuthUtil.java",
                "patch": "@@ -0,0 +1,36 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.hive.hcatalog.cli.SemanticAnalysis;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+\n+final class HCatAuthUtil {\n+  public static boolean isAuthorizationEnabled(Configuration conf) {\n+    // the session state getAuthorizer can return null even if authorization is\n+    // enabled if the V2 api of authorizer in use.\n+    // The additional authorization checks happening in hcatalog are designed to\n+    // work with  storage based authorization (on client side). It should not try doing\n+    // additional checks if a V2 authorizer is in use. The reccomended configuration is to\n+    // use storage based authorization in metastore server\n+    return HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED)\n+        && SessionState.get().getAuthorizer() != null;\n+  }\n+}",
                "raw_url": "https://github.com/apache/hive/raw/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatAuthUtil.java",
                "sha": "6dce9c4b1d1218ce1f8b4ebffee0eaacb31d25cd",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzerBase.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzerBase.java?ref=460d97daa215607674ea7516fec1692c443d7bc8",
                "deletions": 3,
                "filename": "hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzerBase.java",
                "patch": "@@ -22,7 +22,6 @@\n import java.io.Serializable;\n import java.util.List;\n \n-import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.api.Database;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.metadata.AuthorizationException;\n@@ -89,8 +88,7 @@ public void postAnalyze(HiveSemanticAnalyzerHookContext context,\n   protected void authorizeDDL(HiveSemanticAnalyzerHookContext context,\n                 List<Task<? extends Serializable>> rootTasks) throws SemanticException {\n \n-    if (!HiveConf.getBoolVar(context.getConf(),\n-      HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED)) {\n+    if (!HCatAuthUtil.isAuthorizationEnabled(context.getConf())) {\n       return;\n     }\n ",
                "raw_url": "https://github.com/apache/hive/raw/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzerBase.java",
                "sha": "5b3ef946613a0b26b3dab340b44f8eb9338ccaf7",
                "status": "modified"
            },
            {
                "additions": 84,
                "blob_url": "https://github.com/apache/hive/blob/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/TestHCatAuthUtil.java",
                "changes": 84,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/TestHCatAuthUtil.java?ref=460d97daa215607674ea7516fec1692c443d7bc8",
                "deletions": 0,
                "filename": "hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/TestHCatAuthUtil.java",
                "patch": "@@ -0,0 +1,84 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.hive.hcatalog.cli.SemanticAnalysis;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider;\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizer;\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthorizerFactory;\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzPluginException;\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAuthzSessionContext;\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveMetastoreClientFactory;\n+import org.apache.hadoop.hive.ql.session.SessionState;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+/**\n+ * Test HCatAuthUtil\n+ */\n+public class TestHCatAuthUtil {\n+\n+  public static class DummyV2AuthorizerFactory implements HiveAuthorizerFactory {\n+\n+    @Override\n+    public HiveAuthorizer createHiveAuthorizer(HiveMetastoreClientFactory metastoreClientFactory,\n+        HiveConf conf, HiveAuthenticationProvider hiveAuthenticator, HiveAuthzSessionContext ctx)\n+        throws HiveAuthzPluginException {\n+      return Mockito.mock(HiveAuthorizer.class);\n+    }\n+  }\n+\n+  /**\n+   * Test with auth enabled and v1 auth\n+   */\n+  @Test\n+  public void authEnabledV1Auth() throws Exception {\n+    HiveConf hcatConf = new HiveConf(this.getClass());\n+    hcatConf.setBoolVar(ConfVars.HIVE_AUTHORIZATION_ENABLED, true);\n+    SessionState.start(hcatConf);\n+    assertTrue(\"hcat auth should be enabled\", HCatAuthUtil.isAuthorizationEnabled(hcatConf));\n+  }\n+\n+  /**\n+   * Test with auth enabled and v2 auth\n+   */\n+  @Test\n+  public void authEnabledV2Auth() throws Exception {\n+    HiveConf hcatConf = new HiveConf(this.getClass());\n+    hcatConf.setBoolVar(ConfVars.HIVE_AUTHORIZATION_ENABLED, true);\n+    hcatConf.setVar(ConfVars.HIVE_AUTHORIZATION_MANAGER, DummyV2AuthorizerFactory.class.getName());\n+    SessionState.start(hcatConf);\n+    assertFalse(\"hcat auth should be disabled\", HCatAuthUtil.isAuthorizationEnabled(hcatConf));\n+  }\n+\n+  /**\n+   * Test with auth disabled\n+   */\n+  @Test\n+  public void authDisabled() throws Exception {\n+    HiveConf hcatConf = new HiveConf(this.getClass());\n+    hcatConf.setBoolVar(ConfVars.HIVE_AUTHORIZATION_ENABLED, false);\n+    SessionState.start(hcatConf);\n+    assertFalse(\"hcat auth should be disabled\", HCatAuthUtil.isAuthorizationEnabled(hcatConf));\n+  }\n+}",
                "raw_url": "https://github.com/apache/hive/raw/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/core/src/test/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/TestHCatAuthUtil.java",
                "sha": "830dcb8119c255127f64a128a1d7b66f041bfb3e",
                "status": "added"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hive/blob/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/pom.xml",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/pom.xml?ref=460d97daa215607674ea7516fec1692c443d7bc8",
                "deletions": 0,
                "filename": "hcatalog/pom.xml",
                "patch": "@@ -46,6 +46,15 @@\n     <module>streaming</module>\n   </modules>\n \n+  <dependencies>\n+    <dependency>\n+      <groupId>org.mockito</groupId>\n+      <artifactId>mockito-all</artifactId>\n+      <version>${mockito-all.version}</version>\n+      <scope>test</scope>\n+    </dependency>\n+  </dependencies>\n+\n   <profiles>\n     <profile>\n       <id>hadoop-1</id>",
                "raw_url": "https://github.com/apache/hive/raw/460d97daa215607674ea7516fec1692c443d7bc8/hcatalog/pom.xml",
                "sha": "cff3837dd20df14e0950c30a1db1da8b61df6339",
                "status": "modified"
            }
        ],
        "message": "HIVE-8408 : hcat cli throws NPE when authorizer using new api is enabled (Thejas Nair, reviewed by Sushanth Sowmyan\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1630996 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/a112a576871e8ee778d0e1c719aef7a467875cfc",
        "patched_files": [
            "CreateTableHook.java",
            "HCatSemanticAnalyzerBase.java",
            "pom.java",
            "HCatAuthUtil.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHCatAuthUtil.java"
        ]
    },
    "hive_463546f": {
        "bug_id": "hive_463546f",
        "commit": "https://github.com/apache/hive/commit/463546f4b03e29aaf9f55c11e33f13e53be1840d",
        "file": [
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d",
                "deletions": 11,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "patch": "@@ -604,7 +604,7 @@ public PreWarmContext createPreWarmContext(TezSessionConfiguration sessionConfig\n     combinedResources.putAll(sessionConfig.getSessionResources());\n \n     try {\n-      for(LocalResource lr : localizeTempFiles(conf)) {\n+      for(LocalResource lr : localizeTempFilesFromConf(getHiveJarDirectory(conf), conf)) {\n         combinedResources.put(getBaseName(lr), lr);\n       }\n     } catch(LoginException le) {\n@@ -665,7 +665,8 @@ public Path getDefaultDestDir(Configuration conf) throws LoginException, IOExcep\n    * @throws IOException when hdfs operation fails\n    * @throws LoginException when getDefaultDestDir fails with the same exception\n    */\n-  public List<LocalResource> localizeTempFiles(Configuration conf) throws IOException, LoginException {\n+  public List<LocalResource> localizeTempFilesFromConf(\n+      String hdfsDirPathStr, Configuration conf) throws IOException, LoginException {\n     List<LocalResource> tmpResources = new ArrayList<LocalResource>();\n \n     String addedFiles = Utilities.getResourceFiles(conf, SessionState.ResourceType.FILE);\n@@ -683,15 +684,32 @@ public Path getDefaultDestDir(Configuration conf) throws LoginException, IOExcep\n \n     String auxJars = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEAUXJARS);\n \n-    // need to localize the additional jars and files\n+    String allFiles = auxJars + \",\" + addedJars + \",\" + addedFiles + \",\" + addedArchives;\n+    addTempFiles(conf, tmpResources, hdfsDirPathStr, allFiles.split(\",\"));\n+    return tmpResources;\n+  }\n \n-    // we need the directory on hdfs to which we shall put all these files\n-    // Use HIVE_JAR_DIRECTORY only if it's set explicitly; otherwise use default directory\n-    String hdfsDirPathStr = getHiveJarDirectory(conf);\n+  /**\n+   * Localizes files, archives and jars from a provided array of names.\n+   * @param hdfsDirPathStr Destination directoty in HDFS.\n+   * @param conf Configuration.\n+   * @param inputOutputJars The file names to localize.\n+   * @return List<LocalResource> local resources to add to execution\n+   * @throws IOException when hdfs operation fails.\n+   * @throws LoginException when getDefaultDestDir fails with the same exception\n+   */\n+  public List<LocalResource> localizeTempFiles(String hdfsDirPathStr, Configuration conf,\n+      String[] inputOutputJars) throws IOException, LoginException {\n+    if (inputOutputJars == null) return null;\n+    List<LocalResource> tmpResources = new ArrayList<LocalResource>();\n+    addTempFiles(conf, tmpResources, hdfsDirPathStr, inputOutputJars);\n+    return tmpResources;\n+  }\n \n-    String allFiles = auxJars + \",\" + addedJars + \",\" + addedFiles + \",\" + addedArchives;\n-    String[] allFilesArr = allFiles.split(\",\");\n-    for (String file : allFilesArr) {\n+  private void addTempFiles(Configuration conf,\n+      List<LocalResource> tmpResources, String hdfsDirPathStr,\n+      String[] files) throws IOException {\n+    for (String file : files) {\n       if (!StringUtils.isNotBlank(file)) {\n         continue;\n       }\n@@ -700,8 +718,6 @@ public Path getDefaultDestDir(Configuration conf) throws LoginException, IOExcep\n           new Path(hdfsFilePathStr), conf);\n       tmpResources.add(localResource);\n     }\n-\n-    return tmpResources;\n   }\n \n   public String getHiveJarDirectory(Configuration conf) throws IOException, LoginException {",
                "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "sha": "14d188ff54e4d7e98704139cbf9c262f056967f1",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d",
                "deletions": 12,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java",
                "patch": "@@ -64,7 +64,7 @@ public void startPool() throws Exception {\n       HiveConf newConf = new HiveConf(initConf);\n       TezSessionState sessionState = defaultQueuePool.take();\n       newConf.set(\"tez.queue.name\", sessionState.getQueueName());\n-      sessionState.open(TezSessionState.makeSessionId(), newConf);\n+      sessionState.open(newConf);\n       defaultQueuePool.put(sessionState);\n     }\n   }\n@@ -91,7 +91,7 @@ public void setupPool(HiveConf conf) throws InterruptedException {\n         if (queue.length() == 0) {\n           continue;\n         }\n-        TezSessionState sessionState = createSession();\n+        TezSessionState sessionState = createSession(TezSessionState.makeSessionId());\n         sessionState.setQueueName(queue);\n         sessionState.setDefault();\n         LOG.info(\"Created new tez session for queue: \" + queue +\n@@ -102,7 +102,7 @@ public void setupPool(HiveConf conf) throws InterruptedException {\n     }\n   }\n \n-  private TezSessionState getSession(HiveConf conf)\n+  private TezSessionState getSession(HiveConf conf, boolean doOpen)\n       throws Exception {\n \n     String queueName = conf.get(\"tez.queue.name\");\n@@ -120,7 +120,7 @@ private TezSessionState getSession(HiveConf conf)\n       LOG.info(\"QueueName: \" + queueName + \" nonDefaultUser: \" + nonDefaultUser +\n           \" defaultQueuePool: \" + defaultQueuePool +\n           \" blockingQueueLength: \" + blockingQueueLength);\n-      return getNewSessionState(conf, queueName);\n+      return getNewSessionState(conf, queueName, doOpen);\n     }\n \n     LOG.info(\"Choosing a session from the defaultQueuePool\");\n@@ -130,16 +130,21 @@ private TezSessionState getSession(HiveConf conf)\n   /**\n    * @param conf HiveConf that is used to initialize the session\n    * @param queueName could be null. Set in the tez session.\n+   * @param doOpen\n    * @return\n    * @throws Exception\n    */\n   private TezSessionState getNewSessionState(HiveConf conf,\n-      String queueName) throws Exception {\n-    TezSessionState retTezSessionState = createSession();\n+      String queueName, boolean doOpen) throws Exception {\n+    TezSessionState retTezSessionState = createSession(TezSessionState.makeSessionId());\n     retTezSessionState.setQueueName(queueName);\n-    retTezSessionState.open(TezSessionState.makeSessionId(), conf);\n+    String what = \"Created\";\n+    if (doOpen) {\n+      retTezSessionState.open(conf);\n+      what = \"Started\";\n+    }\n \n-    LOG.info(\"Started a new session for queue: \" + queueName +\n+    LOG.info(what + \" a new session for queue: \" + queueName +\n         \" session id: \" + retTezSessionState.getSessionId());\n     return retTezSessionState;\n   }\n@@ -179,11 +184,12 @@ public void stop() throws Exception {\n     }\n   }\n \n-  protected TezSessionState createSession() {\n-    return new TezSessionState();\n+  protected TezSessionState createSession(String sessionId) {\n+    return new TezSessionState(sessionId);\n   }\n \n-  public TezSessionState getSession(TezSessionState session, HiveConf conf) throws Exception {\n+  public TezSessionState getSession(\n+      TezSessionState session, HiveConf conf, boolean doOpen) throws Exception {\n     if (canWorkWithSameSession(session, conf)) {\n       return session;\n     }\n@@ -192,7 +198,7 @@ public TezSessionState getSession(TezSessionState session, HiveConf conf) throws\n       session.close(false);\n     }\n \n-    return getSession(conf);\n+    return getSession(conf, doOpen);\n   }\n \n   /*",
                "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionPoolManager.java",
                "sha": "6cbe8c55d55cc15bb6458f31a1e813cf27fae83e",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d",
                "deletions": 5,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java",
                "patch": "@@ -23,6 +23,7 @@\n import java.net.URISyntaxException;\n import java.util.Collections;\n import java.util.HashMap;\n+import java.util.HashSet;\n import java.util.LinkedList;\n import java.util.List;\n import java.util.Map;\n@@ -68,6 +69,8 @@\n   private String queueName;\n   private boolean defaultQueue = false;\n \n+  private HashSet<String> additionalAmFiles = null;\n+\n   private static List<TezSessionState> openSessions\n     = Collections.synchronizedList(new LinkedList<TezSessionState>());\n \n@@ -83,8 +86,9 @@ public TezSessionState(DagUtils utils) {\n    * Constructor. We do not automatically connect, because we only want to\n    * load tez classes when the user has tez installed.\n    */\n-  public TezSessionState() {\n+  public TezSessionState(String sessionId) {\n     this(DagUtils.getInstance());\n+    this.sessionId = sessionId;\n   }\n \n   /**\n@@ -106,6 +110,11 @@ public static String makeSessionId() {\n     return UUID.randomUUID().toString();\n   }\n \n+  public void open(HiveConf conf)\n+      throws IOException, LoginException, URISyntaxException, TezException {\n+    open(conf, null);\n+  }\n+\n   /**\n    * Creates a tez session. A session is tied to either a cli/hs2 session. You can\n    * submit multiple DAGs against a session (as long as they are executed serially).\n@@ -114,10 +123,8 @@ public static String makeSessionId() {\n    * @throws LoginException\n    * @throws TezException\n    */\n-  public void open(String sessionId, HiveConf conf)\n-    throws IOException, LoginException, URISyntaxException, TezException {\n-\n-    this.sessionId = sessionId;\n+  public void open(HiveConf conf, List<LocalResource> additionalLr)\n+    throws IOException, LoginException, IllegalArgumentException, URISyntaxException, TezException {\n     this.conf = conf;\n \n     // create the tez tmp dir\n@@ -135,6 +142,14 @@ public void open(String sessionId, HiveConf conf)\n     // configuration for the application master\n     Map<String, LocalResource> commonLocalResources = new HashMap<String, LocalResource>();\n     commonLocalResources.put(utils.getBaseName(appJarLr), appJarLr);\n+    if (additionalLr != null) {\n+      additionalAmFiles = new HashSet<String>();\n+      for (LocalResource lr : additionalLr) {\n+        String baseName = utils.getBaseName(lr);\n+        additionalAmFiles.add(baseName);\n+        commonLocalResources.put(baseName, lr);\n+      }\n+    }\n \n     // Create environment for AM.\n     Map<String, String> amEnv = new HashMap<String, String>();\n@@ -174,6 +189,15 @@ public void open(String sessionId, HiveConf conf)\n     openSessions.add(this);\n   }\n \n+  public boolean hasResources(List<LocalResource> lrs) {\n+    if (lrs == null || lrs.isEmpty()) return true;\n+    if (additionalAmFiles == null || additionalAmFiles.isEmpty()) return false;\n+    for (LocalResource lr : lrs) {\n+      if (!additionalAmFiles.contains(utils.getBaseName(lr))) return false;\n+    }\n+    return true;\n+  }\n+\n   /**\n    * Close a tez session. Will cleanup any tez/am related resources. After closing a session\n    * no further DAGs can be executed against it.\n@@ -202,6 +226,7 @@ public void close(boolean keepTmpDir) throws TezException, IOException {\n     tezScratchDir = null;\n     conf = null;\n     appJarLr = null;\n+    additionalAmFiles = null;\n   }\n \n   public String getSessionId() {",
                "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezSessionState.java",
                "sha": "74940e68846184586a4fc97cc70868ffd5bd1379",
                "status": "modified"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "changes": 61,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d",
                "deletions": 23,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "patch": "@@ -23,6 +23,7 @@\n import java.util.Collections;\n import java.util.EnumSet;\n import java.util.HashMap;\n+import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n import java.util.Map;\n@@ -103,45 +104,62 @@ public int execute(DriverContext driverContext) {\n     TezSessionState session = null;\n \n     try {\n-      // Get or create Context object. If we create it we have to clean\n-      // it later as well.\n+      // Get or create Context object. If we create it we have to clean it later as well.\n       ctx = driverContext.getCtx();\n       if (ctx == null) {\n         ctx = new Context(conf);\n         cleanContext = true;\n       }\n \n-      // Need to remove this static hack. But this is the way currently to\n-      // get a session.\n+      // Need to remove this static hack. But this is the way currently to get a session.\n       SessionState ss = SessionState.get();\n       session = ss.getTezSession();\n-      session = TezSessionPoolManager.getInstance().getSession(session, conf);\n+      session = TezSessionPoolManager.getInstance().getSession(session, conf, false);\n       ss.setTezSession(session);\n \n-      // if it's not running start it.\n-      if (!session.isOpen()) {\n-        // can happen if the user sets the tez flag after the session was\n-        // established\n-        LOG.info(\"Tez session hasn't been created yet. Opening session\");\n-        session.open(session.getSessionId(), conf);\n-      }\n+      // jobConf will hold all the configuration for hadoop, tez, and hive\n+      JobConf jobConf = utils.createConfiguration(conf);\n+\n+      // Get all user jars from work (e.g. input format stuff).\n+      String[] inputOutputJars = work.configureJobConfAndExtractJars(jobConf);\n \n       // we will localize all the files (jars, plans, hashtables) to the\n-      // scratch dir. let's create this first.\n+      // scratch dir. let's create this and tmp first.\n       Path scratchDir = ctx.getMRScratchDir();\n-\n-      // create the tez tmp dir\n       utils.createTezDir(scratchDir, conf);\n \n-      // jobConf will hold all the configuration for hadoop, tez, and hive\n-      JobConf jobConf = utils.createConfiguration(conf);\n+      // we need to get the user specified local resources for this dag\n+      String hiveJarDir = utils.getHiveJarDirectory(conf);\n+      List<LocalResource> additionalLr = utils.localizeTempFilesFromConf(hiveJarDir, conf);\n+      List<LocalResource> handlerLr = utils.localizeTempFiles(hiveJarDir, conf, inputOutputJars);\n+      if (handlerLr != null) {\n+        additionalLr.addAll(handlerLr);\n+      }\n+\n+      // If we have any jars from input format, we need to restart the session because\n+      // AM will need them; so, AM has to be restarted. What a mess...\n+      if (!session.hasResources(handlerLr)) {\n+        if (session.isOpen()) {\n+          LOG.info(\"Tez session being reopened to pass custom jars to AM\");\n+          session.close(false);\n+          session = TezSessionPoolManager.getInstance().getSession(null, conf, false);\n+          ss.setTezSession(session);\n+        }\n+        session.open(conf, additionalLr);\n+      }\n+      if (!session.isOpen()) {\n+        // can happen if the user sets the tez flag after the session was\n+        // established\n+        LOG.info(\"Tez session hasn't been created yet. Opening session\");\n+        session.open(conf);\n+      }\n \n       // unless already installed on all the cluster nodes, we'll have to\n       // localize hive-exec.jar as well.\n       LocalResource appJarLr = session.getAppJarLr();\n \n       // next we translate the TezWork to a Tez DAG\n-      DAG dag = build(jobConf, work, scratchDir, appJarLr, ctx);\n+      DAG dag = build(jobConf, work, scratchDir, appJarLr, additionalLr, ctx);\n \n       // submit will send the job to the cluster and start executing\n       client = submit(jobConf, dag, scratchDir, appJarLr, session);\n@@ -186,16 +204,13 @@ public int execute(DriverContext driverContext) {\n   }\n \n   DAG build(JobConf conf, TezWork work, Path scratchDir,\n-      LocalResource appJarLr, Context ctx)\n+      LocalResource appJarLr, List<LocalResource> additionalLr, Context ctx)\n       throws Exception {\n \n     perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_BUILD_DAG);\n     Map<BaseWork, Vertex> workToVertex = new HashMap<BaseWork, Vertex>();\n     Map<BaseWork, JobConf> workToConf = new HashMap<BaseWork, JobConf>();\n \n-    // we need to get the user specified local resources for this dag\n-    List<LocalResource> additionalLr = utils.localizeTempFiles(conf);\n-\n     // getAllWork returns a topologically sorted list, which we use to make\n     // sure that vertices are created before they are used in edges.\n     List<BaseWork> ws = work.getAllWork();\n@@ -299,7 +314,7 @@ DAGClient submit(JobConf conf, DAG dag, Path scratchDir,\n       sessionState.close(true);\n \n       // (re)open the session\n-      sessionState.open(sessionState.getSessionId(), this.conf);\n+      sessionState.open(this.conf);\n \n       console.printInfo(\"Session re-established.\");\n ",
                "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "sha": "c355d5aed2d2f9c8550c8ec56132b9fc68a80c30",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java",
                "patch": "@@ -845,6 +845,13 @@ public static void setKeyAndValueDescForTaskTree(Task<? extends Serializable> ta\n           setKeyAndValueDesc(work.getReduceWork(), op);\n         }\n       }\n+    } else if (task != null && (task.getWork() instanceof TezWork)) {\n+      TezWork work = (TezWork)task.getWork();\n+      for (BaseWork w : work.getAllWorkUnsorted()) {\n+        if (w instanceof MapWork) {\n+          ((MapWork)w).deriveExplainAttributes();\n+        }\n+      }\n     }\n \n     if (task.getChildTasks() == null) {",
                "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java",
                "sha": "f285312bab17a6d14be0a5cacc8ae4e830e0691f",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/plan/BaseWork.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/BaseWork.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/BaseWork.java",
                "patch": "@@ -28,6 +28,7 @@\n \n import org.apache.hadoop.hive.ql.exec.HashTableDummyOperator;\n import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.mapred.JobConf;\n \n /**\n  * BaseWork. Base class for any \"work\" that's being done on the cluster. Items like stats\n@@ -106,4 +107,6 @@ public void addDummyOp(HashTableDummyOperator dummyOp) {\n \n     return returnSet;\n   }\n+\n+  public abstract void configureJobConf(JobConf job);\n }",
                "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/plan/BaseWork.java",
                "sha": "4d3658fb0694b63fe05e3926bf63ceb7425908d9",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java",
                "patch": "@@ -172,6 +172,8 @@ public void setPathToPartitionInfo(\n \n   /**\n    * Derive additional attributes to be rendered by EXPLAIN.\n+   * TODO: this method is relied upon by custom input formats to set jobconf properties.\n+   *       This is madness? - This is Hive Storage Handlers!\n    */\n   public void deriveExplainAttributes() {\n     if (pathToPartitionInfo != null) {\n@@ -495,6 +497,7 @@ public String getSamplingTypeString() {\n         samplingType == 2 ? \"SAMPLING_ON_START\" : null;\n   }\n \n+  @Override\n   public void configureJobConf(JobConf job) {\n     for (PartitionDesc partition : aliasToPartnInfo.values()) {\n       PlanUtils.configureJobConf(partition.getTableDesc(), job);",
                "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java",
                "sha": "9945dea53e714424d27d6c67babe2798ab77be8b",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/plan/TezWork.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/TezWork.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/TezWork.java",
                "patch": "@@ -33,6 +33,7 @@\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.hive.ql.plan.TezEdgeProperty.EdgeType;\n+import org.apache.hadoop.mapred.JobConf;\n import org.apache.tez.dag.api.EdgeProperty;\n \n /**\n@@ -93,6 +94,10 @@ public String getName() {\n     return result;\n   }\n \n+  public Collection<BaseWork> getAllWorkUnsorted() {\n+    return workGraph.keySet();\n+  }\n+\n   private void visit(BaseWork child, Set<BaseWork> seen, List<BaseWork> result) {\n \n     if (seen.contains(child)) {\n@@ -271,6 +276,37 @@ public String getType() {\n     }\n     return result;\n   }\n+  \n+  private static final String MR_JAR_PROPERTY = \"tmpjars\";\n+  /**\n+   * Calls configureJobConf on instances of work that are part of this TezWork.\n+   * Uses the passed job configuration to extract \"tmpjars\" added by these, so that Tez\n+   * could add them to the job proper Tez way. This is a very hacky way but currently\n+   * there's no good way to get these JARs - both storage handler interface, and HBase\n+   * code, would have to change to get the list directly (right now it adds to tmpjars).\n+   * This will happen in 0.14 hopefully.\n+   * @param jobConf Job configuration.\n+   * @return List of files added to tmpjars by storage handlers.\n+   */\n+  public String[] configureJobConfAndExtractJars(JobConf jobConf) {\n+    String[] oldTmpJars = jobConf.getStrings(MR_JAR_PROPERTY);\n+    jobConf.setStrings(MR_JAR_PROPERTY, new String[0]);\n+    for (BaseWork work : workGraph.keySet()) {\n+      work.configureJobConf(jobConf);\n+    }\n+    String[] newTmpJars = jobConf.getStrings(MR_JAR_PROPERTY);\n+    if (oldTmpJars != null && (oldTmpJars.length != 0)) {\n+      if (newTmpJars != null && (newTmpJars.length != 0)) {\n+        String[] combinedTmpJars = new String[newTmpJars.length + oldTmpJars.length];\n+        System.arraycopy(oldTmpJars, 0, combinedTmpJars, 0, oldTmpJars.length);\n+        System.arraycopy(newTmpJars, 0, combinedTmpJars, oldTmpJars.length, newTmpJars.length);\n+        jobConf.setStrings(MR_JAR_PROPERTY, combinedTmpJars);\n+      } else {\n+        jobConf.setStrings(MR_JAR_PROPERTY, oldTmpJars);\n+      }\n+    }\n+    return newTmpJars;\n+   }\n \n   /**\n    * connect adds an edge between a and b. Both nodes have",
                "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/plan/TezWork.java",
                "sha": "7394588130861466cb98b5e53a74a729214397ec",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/plan/UnionWork.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/UnionWork.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/UnionWork.java",
                "patch": "@@ -28,6 +28,7 @@\n import org.apache.hadoop.hive.ql.plan.BaseWork;\n import org.apache.hadoop.hive.ql.exec.Operator;\n import org.apache.hadoop.hive.ql.exec.UnionOperator;\n+import org.apache.hadoop.mapred.JobConf;\n \n /**\n  * Simple wrapper for union all cases. All contributing work for a union all\n@@ -68,4 +69,7 @@ public void addUnionOperators(Collection<UnionOperator> unions) {\n   public Set<UnionOperator> getUnionOperators() {\n     return unionOperators;\n   }\n+\n+  public void configureJobConf(JobConf job) {\n+  }\n }",
                "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/plan/UnionWork.java",
                "sha": "5ef0e074b632ec74b4e546ffddba52719bc6b60d",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d",
                "deletions": 4,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java",
                "patch": "@@ -347,9 +347,9 @@ public static SessionState start(SessionState startSs) {\n         .equals(\"tez\") && (startSs.isHiveServerQuery == false)) {\n       try {\n         if (startSs.tezSessionState == null) {\n-          startSs.tezSessionState = new TezSessionState();\n+          startSs.tezSessionState = new TezSessionState(startSs.getSessionId());\n         }\n-        startSs.tezSessionState.open(startSs.getSessionId(), startSs.conf);\n+        startSs.tezSessionState.open(startSs.conf);\n       } catch (Exception e) {\n         throw new RuntimeException(e);\n       }\n@@ -815,10 +815,10 @@ public boolean delete_resource(ResourceType t, String value) {\n   }\n \n   public Set<String> list_resource(ResourceType t, List<String> filter) {\n-    if (resource_map.get(t) == null) {\n+    Set<String> orig = resource_map.get(t);\n+    if (orig == null) {\n       return null;\n     }\n-    Set<String> orig = resource_map.get(t);\n     if (filter == null) {\n       return orig;\n     } else {",
                "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java",
                "sha": "4785b7f1576c8deabb12ab4782d8dc53976bc64b",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezSessionPool.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezSessionPool.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d",
                "deletions": 10,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezSessionPool.java",
                "patch": "@@ -40,9 +40,9 @@ public TestTezSessionPoolManager() {\n     }\n \n     @Override\n-      public TezSessionState createSession() {\n-        return new TestTezSessionState();\n-      }\n+    public TezSessionState createSession(String sessionId) {\n+      return new TestTezSessionState(sessionId);\n+    }\n   }\n \n   @Before\n@@ -54,8 +54,8 @@ public void setUp() {\n     public void testGetNonDefaultSession() {\n       poolManager = new TestTezSessionPoolManager();\n       try {\n-        TezSessionState sessionState = poolManager.getSession(null, conf);\n-        TezSessionState sessionState1 = poolManager.getSession(sessionState, conf);\n+        TezSessionState sessionState = poolManager.getSession(null, conf, true);\n+        TezSessionState sessionState1 = poolManager.getSession(sessionState, conf, true);\n         if (sessionState1 != sessionState) {\n           fail();\n         }\n@@ -75,25 +75,25 @@ public void testSessionPoolGetInOrder() {\n         poolManager = new TestTezSessionPoolManager();\n         poolManager.setupPool(conf);\n         poolManager.startPool();\n-        TezSessionState sessionState = poolManager.getSession(null, conf);\n+        TezSessionState sessionState = poolManager.getSession(null, conf, true);\n         if (sessionState.getQueueName().compareTo(\"a\") != 0) {\n           fail();\n         }\n         poolManager.returnSession(sessionState);\n \n-        sessionState = poolManager.getSession(null, conf);\n+        sessionState = poolManager.getSession(null, conf, true);\n         if (sessionState.getQueueName().compareTo(\"b\") != 0) {\n           fail();\n         }\n         poolManager.returnSession(sessionState);\n \n-        sessionState = poolManager.getSession(null, conf);\n+        sessionState = poolManager.getSession(null, conf, true);\n         if (sessionState.getQueueName().compareTo(\"c\") != 0) {\n           fail();\n         }\n         poolManager.returnSession(sessionState);\n \n-        sessionState = poolManager.getSession(null, conf);\n+        sessionState = poolManager.getSession(null, conf, true);\n         if (sessionState.getQueueName().compareTo(\"a\") != 0) {\n           fail();\n         }\n@@ -118,7 +118,7 @@ public void run() {\n             tmpConf.set(\"tez.queue.name\", \"\");\n           }\n \n-          TezSessionState session = poolManager.getSession(null, tmpConf);\n+          TezSessionState session = poolManager.getSession(null, tmpConf, true);\n           Thread.sleep((random.nextInt(9) % 10) * 1000);\n           poolManager.returnSession(session);\n         } catch (Exception e) {",
                "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezSessionPool.java",
                "sha": "ad5a6e7ac218a1d16168b3a93e50dbbc6353dc69",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezSessionState.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezSessionState.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d",
                "deletions": 5,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezSessionState.java",
                "patch": "@@ -38,6 +38,11 @@\n   private String sessionId;\n   private HiveConf hiveConf;\n \n+  public TestTezSessionState(String sessionId) {\n+    super(sessionId);\n+    this.sessionId = sessionId;\n+  }\n+\n   @Override\n     public boolean isOpen() {\n       return open;\n@@ -48,11 +53,9 @@ public void setOpen(boolean open) {\n   }\n \n   @Override\n-    public void open(String sessionId, HiveConf conf) throws IOException,\n-           LoginException, URISyntaxException, TezException {\n-             this.sessionId = sessionId;\n-             this.hiveConf = conf;\n-    }\n+  public void open(HiveConf conf) {\n+    this.hiveConf = conf;\n+  }\n \n   @Override\n     public void close(boolean keepTmpDir) throws TezException, IOException {",
                "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezSessionState.java",
                "sha": "6ee6e42a979d58e1693f9f9ea85cec3842910d99",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java?ref=463546f4b03e29aaf9f55c11e33f13e53be1840d",
                "deletions": 3,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java",
                "patch": "@@ -33,6 +33,7 @@\n import java.util.ArrayList;\n import java.util.LinkedHashMap;\n import java.util.List;\n+import java.util.Map;\n \n import javax.security.auth.login.LoginException;\n \n@@ -176,7 +177,7 @@ public void tearDown() throws Exception {\n \n   @Test\n   public void testBuildDag() throws IllegalArgumentException, IOException, Exception {\n-    DAG dag = task.build(conf, work, path, appLr, new Context(conf));\n+    DAG dag = task.build(conf, work, path, appLr, null, new Context(conf));\n     for (BaseWork w: work.getAllWork()) {\n       Vertex v = dag.getVertex(w.getName());\n       assertNotNull(v);\n@@ -196,7 +197,7 @@ public void testBuildDag() throws IllegalArgumentException, IOException, Excepti\n \n   @Test\n   public void testEmptyWork() throws IllegalArgumentException, IOException, Exception {\n-    DAG dag = task.build(conf, new TezWork(\"\"), path, appLr, new Context(conf));\n+    DAG dag = task.build(conf, new TezWork(\"\"), path, appLr, null, new Context(conf));\n     assertEquals(dag.getVertices().size(), 0);\n   }\n \n@@ -206,7 +207,7 @@ public void testSubmit() throws LoginException, IllegalArgumentException,\n     DAG dag = new DAG(\"test\");\n     task.submit(conf, dag, path, appLr, sessionState);\n     // validate close/reopen\n-    verify(sessionState, times(1)).open(any(String.class), any(HiveConf.class));\n+    verify(sessionState, times(1)).open(any(HiveConf.class));\n     verify(sessionState, times(1)).close(eq(true));\n     verify(session, times(2)).submitDAG(any(DAG.class));\n   }",
                "raw_url": "https://github.com/apache/hive/raw/463546f4b03e29aaf9f55c11e33f13e53be1840d/ql/src/test/org/apache/hadoop/hive/ql/exec/tez/TestTezTask.java",
                "sha": "1793b58fc8a218e9d81cdc0b564d1739f3d637b7",
                "status": "modified"
            }
        ],
        "message": "HIVE-6739 : Hive HBase query fails on Tez due to missing jars and then due to NPE in getSplits (Sergey Shelukhin, reviewed by Vikram Dixit K)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1585602 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/a989afbdb18f8e7da707f0dda5e481befd3d62fc",
        "patched_files": [
            "MapWork.java",
            "SessionState.java",
            "DagUtils.java",
            "TezSessionState.java",
            "BaseWork.java",
            "GenMapRedUtils.java",
            "TezSessionPoolManager.java",
            "TezTask.java",
            "TezWork.java",
            "UnionWork.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestTezSessionPool.java",
            "TestTezTask.java",
            "TestTezWork.java",
            "TestSessionState.java",
            "TestTezSessionState.java"
        ]
    },
    "hive_48e4e04": {
        "bug_id": "hive_48e4e04",
        "commit": "https://github.com/apache/hive/commit/48e4e04c3b446f219c24b5fd0cf03c9e2e210f0c",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/48e4e04c3b446f219c24b5fd0cf03c9e2e210f0c/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java?ref=48e4e04c3b446f219c24b5fd0cf03c9e2e210f0c",
                "deletions": 4,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java",
                "patch": "@@ -797,10 +797,11 @@ private JSONObject outputPlan(Object work, PrintStream out,\n                 operator.getOperatorId());\n             if (!this.work.isUserLevelExplain() && this.work.isFormatted()\n                 && operator instanceof ReduceSinkOperator) {\n-              ((JSONObject) jsonOut.get(JSONObject.getNames(jsonOut)[0])).put(\n-                  OUTPUT_OPERATORS,\n-                  Arrays.toString(((ReduceSinkOperator) operator).getConf().getOutputOperators()\n-                      .toArray()));\n+              List<String> outputOperators = ((ReduceSinkOperator) operator).getConf().getOutputOperators();\n+              if (outputOperators != null) {\n+                ((JSONObject) jsonOut.get(JSONObject.getNames(jsonOut)[0])).put(OUTPUT_OPERATORS,\n+                    Arrays.toString(outputOperators.toArray()));\n+              }\n             }\n           }\n         }",
                "raw_url": "https://github.com/apache/hive/raw/48e4e04c3b446f219c24b5fd0cf03c9e2e210f0c/ql/src/java/org/apache/hadoop/hive/ql/exec/ExplainTask.java",
                "sha": "d35e3ba56a2395ad8665e8df7c29a0ada830a657",
                "status": "modified"
            }
        ],
        "message": "HIVE-16142: ATSHook NPE via LLAP (Pengcheng Xiong, reviewed by Ashutosh Chauhan)",
        "parent": "https://github.com/apache/hive/commit/35d707950ddd210c37533be3da51cea730bac881",
        "patched_files": [
            "ExplainTask.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestExplainTask.java"
        ]
    },
    "hive_49cc028": {
        "bug_id": "hive_49cc028",
        "commit": "https://github.com/apache/hive/commit/49cc02873fb7b1d43d5e1dbb3ef3435c877f61b2",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/49cc02873fb7b1d43d5e1dbb3ef3435c877f61b2/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=49cc02873fb7b1d43d5e1dbb3ef3435c877f61b2",
                "deletions": 7,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "patch": "@@ -3864,7 +3864,7 @@ static boolean isRegex(String pattern, HiveConf conf) {\n     }\n     selectStar = selectStar && exprList.getChildCount() == posn + 1;\n \n-    handleInsertStatementSpec(col_list, dest, out_rwsch, inputRR, qb, selExprList);\n+    out_rwsch = handleInsertStatementSpec(col_list, dest, out_rwsch, inputRR, qb, selExprList);\n \n     ArrayList<String> columnNames = new ArrayList<String>();\n     Map<String, ExprNodeDesc> colExprMap = new HashMap<String, ExprNodeDesc>();\n@@ -3909,14 +3909,14 @@ static boolean isRegex(String pattern, HiveConf conf) {\n    * @see #handleInsertStatementSpecPhase1(ASTNode, QBParseInfo, org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.Phase1Ctx)\n    * @throws SemanticException\n    */\n-  private void handleInsertStatementSpec(List<ExprNodeDesc> col_list, String dest,\n+  private RowResolver handleInsertStatementSpec(List<ExprNodeDesc> col_list, String dest,\n                                          RowResolver outputRR, RowResolver inputRR, QB qb,\n                                          ASTNode selExprList) throws SemanticException {\n     //(z,x)\n     List<String> targetTableSchema = qb.getParseInfo().getDestSchemaForClause(dest);//specified in the query\n     if(targetTableSchema == null) {\n       //no insert schema was specified\n-      return;\n+      return outputRR;\n     }\n     if(targetTableSchema.size() != col_list.size()) {\n       Table target = qb.getMetaData().getDestTableForAlias(dest);\n@@ -3959,6 +3959,7 @@ private void handleInsertStatementSpec(List<ExprNodeDesc> col_list, String dest,\n         }\n       }\n     }\n+    RowResolver newOutputRR = new RowResolver();\n     //now make the select produce <regular columns>,<dynamic partition columns> with\n     //where missing columns are NULL-filled\n     for(String f : targetTableColNames) {\n@@ -3967,7 +3968,7 @@ private void handleInsertStatementSpec(List<ExprNodeDesc> col_list, String dest,\n         new_col_list.add(targetCol2Projection.get(f));\n         ColumnInfo ci = targetCol2ColumnInfo.get(f);//todo: is this OK?\n         ci.setInternalName(getColumnInternalName(colListPos));\n-        newSchema.add(ci);\n+        newOutputRR.put(ci.getTabAlias(), ci.getInternalName(), ci);\n       }\n       else {\n         //add new 'synthetic' columns for projections not provided by Select\n@@ -3979,14 +3980,13 @@ private void handleInsertStatementSpec(List<ExprNodeDesc> col_list, String dest,\n         final String tableAlias = null;//this column doesn't come from any table\n         ColumnInfo colInfo = new ColumnInfo(getColumnInternalName(colListPos),\n           exp.getWritableObjectInspector(), tableAlias, false);\n-        newSchema.add(colInfo);\n-        outputRR.addMappingOnly(colInfo.getTabAlias(), colInfo.getInternalName(), colInfo);\n+        newOutputRR.put(colInfo.getTabAlias(), colInfo.getInternalName(), colInfo);\n       }\n       colListPos++;\n     }\n     col_list.clear();\n     col_list.addAll(new_col_list);\n-    outputRR.setRowSchema(new RowSchema(newSchema));\n+    return newOutputRR;\n   }\n   String recommendName(ExprNodeDesc exp, String colAlias) {\n     if (!colAlias.startsWith(autogenColAliasPrfxLbl)) {",
                "raw_url": "https://github.com/apache/hive/raw/49cc02873fb7b1d43d5e1dbb3ef3435c877f61b2/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "sha": "351c26770c2f78727d09fdb48c8495daf88fe202",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hive/blob/49cc02873fb7b1d43d5e1dbb3ef3435c877f61b2/ql/src/test/queries/clientpositive/insert_into_with_schema2.q",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/insert_into_with_schema2.q?ref=49cc02873fb7b1d43d5e1dbb3ef3435c877f61b2",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/insert_into_with_schema2.q",
                "patch": "@@ -21,3 +21,14 @@ insert into student_acid(grade, age) values(20, 2);\n insert into student_acid(age) values(22);\n \n select * from student_acid;\n+\n+set hive.exec.dynamic.partition.mode=nonstrict;\n+\n+drop table if exists acid_partitioned;\n+create table acid_partitioned (a int, c string)\n+  partitioned by (p int)\n+  clustered by (a) into 1 buckets;\n+\n+insert into acid_partitioned partition (p) (a,p) values(1,2);\n+\n+select * from acid_partitioned;",
                "raw_url": "https://github.com/apache/hive/raw/49cc02873fb7b1d43d5e1dbb3ef3435c877f61b2/ql/src/test/queries/clientpositive/insert_into_with_schema2.q",
                "sha": "a5352ec2f1d51cf0a2f863656d0b168c8487b6b4",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/hive/blob/49cc02873fb7b1d43d5e1dbb3ef3435c877f61b2/ql/src/test/results/clientpositive/insert_into_with_schema2.q.out",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/insert_into_with_schema2.q.out?ref=49cc02873fb7b1d43d5e1dbb3ef3435c877f61b2",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/insert_into_with_schema2.q.out",
                "patch": "@@ -96,3 +96,40 @@ POSTHOOK: Input: default@student_acid\n 1\tNULL\n 2\t20\n 22\tNULL\n+PREHOOK: query: drop table if exists acid_partitioned\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: drop table if exists acid_partitioned\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: create table acid_partitioned (a int, c string)\n+  partitioned by (p int)\n+  clustered by (a) into 1 buckets\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@acid_partitioned\n+POSTHOOK: query: create table acid_partitioned (a int, c string)\n+  partitioned by (p int)\n+  clustered by (a) into 1 buckets\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@acid_partitioned\n+PREHOOK: query: insert into acid_partitioned partition (p) (a,p) values(1,2)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@values__tmp__table__4\n+PREHOOK: Output: default@acid_partitioned\n+POSTHOOK: query: insert into acid_partitioned partition (p) (a,p) values(1,2)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@values__tmp__table__4\n+POSTHOOK: Output: default@acid_partitioned@p=2\n+POSTHOOK: Lineage: acid_partitioned PARTITION(p=2).a EXPRESSION [(values__tmp__table__4)values__tmp__table__4.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+POSTHOOK: Lineage: acid_partitioned PARTITION(p=2).c SIMPLE []\n+PREHOOK: query: select * from acid_partitioned\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@acid_partitioned\n+PREHOOK: Input: default@acid_partitioned@p=2\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from acid_partitioned\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@acid_partitioned\n+POSTHOOK: Input: default@acid_partitioned@p=2\n+#### A masked pattern was here ####\n+1\tNULL\t2",
                "raw_url": "https://github.com/apache/hive/raw/49cc02873fb7b1d43d5e1dbb3ef3435c877f61b2/ql/src/test/results/clientpositive/insert_into_with_schema2.q.out",
                "sha": "32e6e92bba3cdf8bd411371ec4e6d1c955c33bdb",
                "status": "modified"
            }
        ],
        "message": "HIVE-10828 - Insert with schema and dynamic partitions NullPointerException (Eugene Koifman, reviewed by Ashutosh Chauhan)",
        "parent": "https://github.com/apache/hive/commit/1293f3d389b73eea672805c49270124f44cbd687",
        "patched_files": [
            "SemanticAnalyzer.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestSemanticAnalyzer.java"
        ]
    },
    "hive_49d31f8": {
        "bug_id": "hive_49d31f8",
        "commit": "https://github.com/apache/hive/commit/49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java?ref=49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54",
                "deletions": 7,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java",
                "patch": "@@ -394,18 +394,16 @@ protected Void performDataRead() throws IOException {\n             counters.incrCounter(Counter.METADATA_CACHE_MISS);\n             ensureMetadataReader();\n             long startTimeHdfs = counters.startTimeCounter();\n-            stripeMetadata = new OrcStripeMetadata(\n-                stripeKey, metadataReader, stripe, stripeIncludes, sargColumns);\n+            stripeMetadata = new OrcStripeMetadata(new OrcBatchKey(fileId, stripeIx, 0),\n+                metadataReader, stripe, stripeIncludes, sargColumns);\n             counters.incrTimeCounter(Counter.HDFS_TIME_US, startTimeHdfs);\n             if (hasFileId && metadataCache != null) {\n               stripeMetadata = metadataCache.putStripeMetadata(stripeMetadata);\n               if (DebugUtils.isTraceOrcEnabled()) {\n                 LlapIoImpl.LOG.info(\"Caching stripe \" + stripeKey.stripeIx\n                     + \" metadata with includes: \" + DebugUtils.toString(stripeIncludes));\n               }\n-              stripeKey = new OrcBatchKey(fileId, -1, 0);\n             }\n-\n           }\n           consumer.setStripeMetadata(stripeMetadata);\n         }\n@@ -658,16 +656,15 @@ private OrcFileMetadata getOrReadFileMetadata() throws IOException {\n         StripeInformation si = fileMetadata.getStripes().get(stripeIx);\n         if (value == null) {\n           long startTime = counters.startTimeCounter();\n-          value = new OrcStripeMetadata(stripeKey, metadataReader, si, globalInc, sargColumns);\n+          value = new OrcStripeMetadata(new OrcBatchKey(fileId, stripeIx, 0),\n+              metadataReader, si, globalInc, sargColumns);\n           counters.incrTimeCounter(Counter.HDFS_TIME_US, startTime);\n           if (hasFileId && metadataCache != null) {\n             value = metadataCache.putStripeMetadata(value);\n             if (DebugUtils.isTraceOrcEnabled()) {\n               LlapIoImpl.LOG.info(\"Caching stripe \" + stripeKey.stripeIx\n                   + \" metadata with includes: \" + DebugUtils.toString(globalInc));\n             }\n-            // Create new key object to reuse for gets; we've used the old one to put in cache.\n-            stripeKey = new OrcBatchKey(fileId, 0, 0);\n           }\n         }\n         // We might have got an old value from cache; recheck it has indexes.",
                "raw_url": "https://github.com/apache/hive/raw/49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54/llap-server/src/java/org/apache/hadoop/hive/llap/io/encoded/OrcEncodedDataReader.java",
                "sha": "b36cf6499bcf443bb168c839e486822e05fe14e8",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java?ref=49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java",
                "patch": "@@ -106,7 +106,7 @@ public DiskRangeList createCacheChunk(MemoryBuffer buffer, long offset, long end\n   private ByteBufferAllocatorPool pool;\n   private boolean isDebugTracingEnabled;\n \n-  public EncodedReaderImpl(long fileId, List<OrcProto.Type> types, CompressionCodec codec,\n+  public EncodedReaderImpl(Long fileId, List<OrcProto.Type> types, CompressionCodec codec,\n       int bufferSize, long strideRate, DataCache cache, DataReader dataReader, PoolFactory pf)\n           throws IOException {\n     this.fileId = fileId;",
                "raw_url": "https://github.com/apache/hive/raw/49d31f8c699c7c0af5ee7f5f7e55abc5f3673a54/ql/src/java/org/apache/hadoop/hive/ql/io/orc/encoded/EncodedReaderImpl.java",
                "sha": "6cec80ee968a503c48cdd487fe11ab31cc2aa592",
                "status": "modified"
            }
        ],
        "message": "HIVE-12990 : LLAP: ORC cache NPE without FileID support (Sergey Shelukhin, reviewed by Prasanth Jayachandran)",
        "parent": "https://github.com/apache/hive/commit/38b172cdce32fa0b9014f16b44e8dc96bdc143d3",
        "patched_files": [
            "EncodedReaderImpl.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestEncodedReaderImpl.java"
        ]
    },
    "hive_4d62b46": {
        "bug_id": "hive_4d62b46",
        "commit": "https://github.com/apache/hive/commit/4d62b4621871c79de3210157dc33021d746b5b2b",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/hcatalog/core/src/main/java/org/apache/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/core/src/main/java/org/apache/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java?ref=4d62b4621871c79de3210157dc33021d746b5b2b",
                "deletions": 1,
                "filename": "hcatalog/core/src/main/java/org/apache/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java",
                "patch": "@@ -78,7 +78,7 @@ public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast)\n     case HiveParser.TOK_ALTERTABLE_PARTITION:\n       if (((ASTNode) ast.getChild(1)).getToken().getType() == HiveParser.TOK_ALTERTABLE_FILEFORMAT) {\n         return ast;\n-      } else if (((ASTNode) ast.getChild(1)).getToken().getType() == HiveParser.TOK_ALTERTABLE_ALTERPARTS_MERGEFILES) {\n+      } else if (((ASTNode) ast.getChild(1)).getToken().getType() == HiveParser.TOK_ALTERTABLE_MERGEFILES) {\n         // unsupported\n         throw new SemanticException(\"Operation not supported.\");\n       } else {",
                "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/hcatalog/core/src/main/java/org/apache/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java",
                "sha": "8bb40455c48d14e77187a1d2ff8156764e19fd1a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java?ref=4d62b4621871c79de3210157dc33021d746b5b2b",
                "deletions": 1,
                "filename": "hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java",
                "patch": "@@ -75,7 +75,7 @@ public ASTNode preAnalyze(HiveSemanticAnalyzerHookContext context, ASTNode ast)\n     case HiveParser.TOK_ALTERTABLE_PARTITION:\n       if (((ASTNode) ast.getChild(1)).getToken().getType() == HiveParser.TOK_ALTERTABLE_FILEFORMAT) {\n         return ast;\n-      } else if (((ASTNode) ast.getChild(1)).getToken().getType() == HiveParser.TOK_ALTERTABLE_ALTERPARTS_MERGEFILES) {\n+      } else if (((ASTNode) ast.getChild(1)).getToken().getType() == HiveParser.TOK_ALTERTABLE_MERGEFILES) {\n         // unsupported\n         throw new SemanticException(\"Operation not supported.\");\n       } else {",
                "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/hcatalog/core/src/main/java/org/apache/hive/hcatalog/cli/SemanticAnalysis/HCatSemanticAnalyzer.java",
                "sha": "75f54e2ed414ea1461bc61a2fa01c224de12cbbf",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java",
                "changes": 75,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/Driver.java?ref=4d62b4621871c79de3210157dc33021d746b5b2b",
                "deletions": 38,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/Driver.java",
                "patch": "@@ -527,55 +527,54 @@ private void doAuthorization(BaseSemanticAnalyzer sem)\n     SessionState ss = SessionState.get();\n     HiveOperation op = ss.getHiveOperation();\n     Hive db = sem.getDb();\n-    if(ss.isAuthorizationModeV2()){\n+    if (ss.isAuthorizationModeV2()) {\n       doAuthorizationV2(ss, op, inputs, outputs);\n       return;\n     }\n \n-    if (op != null) {\n-      if (op.equals(HiveOperation.CREATEDATABASE)) {\n-        ss.getAuthorizer().authorize(\n-            op.getInputRequiredPrivileges(), op.getOutputRequiredPrivileges());\n-      } else if (op.equals(HiveOperation.CREATETABLE_AS_SELECT)\n-          || op.equals(HiveOperation.CREATETABLE)) {\n-        ss.getAuthorizer().authorize(\n-            db.getDatabase(SessionState.get().getCurrentDatabase()), null,\n-            HiveOperation.CREATETABLE_AS_SELECT.getOutputRequiredPrivileges());\n-      } else {\n-        if (op.equals(HiveOperation.IMPORT)) {\n-          ImportSemanticAnalyzer isa = (ImportSemanticAnalyzer) sem;\n-          if (!isa.existsTable()) {\n-            ss.getAuthorizer().authorize(\n-                db.getDatabase(SessionState.get().getCurrentDatabase()), null,\n-                HiveOperation.CREATETABLE_AS_SELECT.getOutputRequiredPrivileges());\n-          }\n+    if (op == null) {\n+      throw new HiveException(\"Operation should not be null\");\n+    }\n+    if (op.equals(HiveOperation.CREATEDATABASE)) {\n+      ss.getAuthorizer().authorize(\n+          op.getInputRequiredPrivileges(), op.getOutputRequiredPrivileges());\n+    } else if (op.equals(HiveOperation.CREATETABLE_AS_SELECT)\n+        || op.equals(HiveOperation.CREATETABLE)) {\n+      ss.getAuthorizer().authorize(\n+          db.getDatabase(SessionState.get().getCurrentDatabase()), null,\n+          HiveOperation.CREATETABLE_AS_SELECT.getOutputRequiredPrivileges());\n+    } else {\n+      if (op.equals(HiveOperation.IMPORT)) {\n+        ImportSemanticAnalyzer isa = (ImportSemanticAnalyzer) sem;\n+        if (!isa.existsTable()) {\n+          ss.getAuthorizer().authorize(\n+              db.getDatabase(SessionState.get().getCurrentDatabase()), null,\n+              HiveOperation.CREATETABLE_AS_SELECT.getOutputRequiredPrivileges());\n         }\n       }\n-      if (outputs != null && outputs.size() > 0) {\n-        //do authorization for each output\n-        for (WriteEntity write : outputs) {\n-          if (write.getType() == Entity.Type.DATABASE) {\n-            ss.getAuthorizer().authorize(write.getDatabase(),\n-                null, op.getOutputRequiredPrivileges());\n-            continue;\n-          }\n-\n-          if (write.getType() == WriteEntity.Type.PARTITION) {\n-            Partition part = db.getPartition(write.getTable(), write\n-                .getPartition().getSpec(), false);\n-            if (part != null) {\n-              ss.getAuthorizer().authorize(write.getPartition(), null,\n-                      op.getOutputRequiredPrivileges());\n-              continue;\n-            }\n-          }\n+    }\n+    if (outputs != null && outputs.size() > 0) {\n+      for (WriteEntity write : outputs) {\n+        if (write.getType() == Entity.Type.DATABASE) {\n+          ss.getAuthorizer().authorize(write.getDatabase(),\n+              null, op.getOutputRequiredPrivileges());\n+          continue;\n+        }\n \n-          if (write.getTable() != null) {\n-            ss.getAuthorizer().authorize(write.getTable(), null,\n+        if (write.getType() == WriteEntity.Type.PARTITION) {\n+          Partition part = db.getPartition(write.getTable(), write\n+              .getPartition().getSpec(), false);\n+          if (part != null) {\n+            ss.getAuthorizer().authorize(write.getPartition(), null,\n                     op.getOutputRequiredPrivileges());\n+            continue;\n           }\n         }\n \n+        if (write.getTable() != null) {\n+          ss.getAuthorizer().authorize(write.getTable(), null,\n+                  op.getOutputRequiredPrivileges());\n+        }\n       }\n     }\n ",
                "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java",
                "sha": "cb0c1a5b720955db0103b7494175d9f42cfd4fd5",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java?ref=4d62b4621871c79de3210157dc33021d746b5b2b",
                "deletions": 5,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java",
                "patch": "@@ -233,11 +233,11 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {\n       ast = (ASTNode) ast.getChild(1);\n       if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_FILEFORMAT) {\n         analyzeAlterTableFileFormat(ast, tableName, partSpec);\n-      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_ALTERPARTS_PROTECTMODE) {\n+      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_PROTECTMODE) {\n         analyzeAlterTableProtectMode(ast, tableName, partSpec);\n       } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_LOCATION) {\n         analyzeAlterTableLocation(ast, tableName, partSpec);\n-      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_ALTERPARTS_MERGEFILES) {\n+      } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_MERGEFILES) {\n         analyzeAlterTablePartMergeFiles(tablePart, ast, tableName, partSpec);\n       } else if (ast.getToken().getType() == HiveParser.TOK_ALTERTABLE_SERIALIZER) {\n         analyzeAlterTableSerde(ast, tableName, partSpec);\n@@ -365,8 +365,8 @@ public void analyzeInternal(ASTNode ast) throws SemanticException {\n     case HiveParser.TOK_ALTERTABLE_DROPPARTS:\n       analyzeAlterTableDropParts(ast, false);\n       break;\n-    case HiveParser.TOK_ALTERTABLE_ALTERPARTS:\n-      analyzeAlterTableAlterParts(ast);\n+    case HiveParser.TOK_ALTERTABLE_PARTCOLTYPE:\n+      analyzeAlterTablePartColType(ast);\n       break;\n     case HiveParser.TOK_ALTERTABLE_PROPERTIES:\n       analyzeAlterTableProps(ast, false, false);\n@@ -2433,7 +2433,7 @@ private void analyzeAlterTableDropParts(ASTNode ast, boolean expectView)\n         dropTblDesc), conf));\n   }\n \n-  private void analyzeAlterTableAlterParts(ASTNode ast)\n+  private void analyzeAlterTablePartColType(ASTNode ast)\n       throws SemanticException {\n     // get table name\n     String tblName = getUnescapedName((ASTNode)ast.getChild(0));",
                "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java",
                "sha": "f4d9a83bf3c792bb5e7d18fcf3438a06fd1780f7",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g?ref=4d62b4621871c79de3210157dc33021d746b5b2b",
                "deletions": 7,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g",
                "patch": "@@ -138,8 +138,9 @@ TOK_ALTERTABLE_RENAMEPART;\n TOK_ALTERTABLE_REPLACECOLS;\n TOK_ALTERTABLE_ADDPARTS;\n TOK_ALTERTABLE_DROPPARTS;\n-TOK_ALTERTABLE_ALTERPARTS;\n-TOK_ALTERTABLE_ALTERPARTS_PROTECTMODE;\n+TOK_ALTERTABLE_PARTCOLTYPE;\n+TOK_ALTERTABLE_PROTECTMODE;\n+TOK_ALTERTABLE_MERGEFILES;\n TOK_ALTERTABLE_TOUCH;\n TOK_ALTERTABLE_ARCHIVE;\n TOK_ALTERTABLE_UNARCHIVE;\n@@ -285,7 +286,6 @@ TOK_DATABASEPROPERTIES;\n TOK_DATABASELOCATION;\n TOK_DBPROPLIST;\n TOK_ALTERDATABASE_PROPERTIES;\n-TOK_ALTERTABLE_ALTERPARTS_MERGEFILES;\n TOK_TABNAME;\n TOK_TABSRC;\n TOK_RESTRICT;\n@@ -900,8 +900,16 @@ alterTableStatementSuffix\n     | alterTblPartitionStatement\n     | alterStatementSuffixSkewedby\n     | alterStatementSuffixExchangePartition\n+    | alterStatementPartitionKeyType\n     ;\n \n+alterStatementPartitionKeyType\n+@init {msgs.push(\"alter partition key type\"); }\n+@after {msgs.pop();}\n+\t: identifier KW_PARTITION KW_COLUMN LPAREN columnNameType RPAREN\n+\t-> ^(TOK_ALTERTABLE_PARTCOLTYPE identifier columnNameType)\n+\t;\n+\n alterViewStatementSuffix\n @init { msgs.push(\"alter view statement\"); }\n @after { msgs.pop(); }\n@@ -1058,8 +1066,6 @@ alterTblPartitionStatement\n @after {msgs.pop();}\n   : tablePartitionPrefix alterTblPartitionStatementSuffix\n   -> ^(TOK_ALTERTABLE_PARTITION tablePartitionPrefix alterTblPartitionStatementSuffix)\n-  |Identifier KW_PARTITION KW_COLUMN LPAREN columnNameType RPAREN\n-  -> ^(TOK_ALTERTABLE_ALTERPARTS Identifier columnNameType)\n   ;\n \n alterTblPartitionStatementSuffix\n@@ -1151,7 +1157,7 @@ alterStatementSuffixProtectMode\n @init { msgs.push(\"alter partition protect mode statement\"); }\n @after { msgs.pop(); }\n     : alterProtectMode\n-    -> ^(TOK_ALTERTABLE_ALTERPARTS_PROTECTMODE alterProtectMode)\n+    -> ^(TOK_ALTERTABLE_PROTECTMODE alterProtectMode)\n     ;\n \n alterStatementSuffixRenamePart\n@@ -1165,7 +1171,7 @@ alterStatementSuffixMergeFiles\n @init { msgs.push(\"\"); }\n @after { msgs.pop(); }\n     : KW_CONCATENATE\n-    -> ^(TOK_ALTERTABLE_ALTERPARTS_MERGEFILES)\n+    -> ^(TOK_ALTERTABLE_MERGEFILES)\n     ;\n \n alterProtectMode",
                "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/parse/HiveParser.g",
                "sha": "216c361148a695d32fa1eb67e51fb14e54843abf",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java?ref=4d62b4621871c79de3210157dc33021d746b5b2b",
                "deletions": 3,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java",
                "patch": "@@ -102,11 +102,12 @@\n     commandType.put(HiveParser.TOK_ALTERTABLE_SKEWED, HiveOperation.ALTERTABLE_SKEWED);\n     commandType.put(HiveParser.TOK_ANALYZE, HiveOperation.ANALYZE_TABLE);\n     commandType.put(HiveParser.TOK_ALTERVIEW_RENAME, HiveOperation.ALTERVIEW_RENAME);\n+    commandType.put(HiveParser.TOK_ALTERTABLE_PARTCOLTYPE, HiveOperation.ALTERTABLE_PARTCOLTYPE);\n   }\n \n   static {\n     tablePartitionCommandType.put(\n-        HiveParser.TOK_ALTERTABLE_ALTERPARTS_PROTECTMODE,\n+        HiveParser.TOK_ALTERTABLE_PROTECTMODE,\n         new HiveOperation[] { HiveOperation.ALTERTABLE_PROTECTMODE,\n             HiveOperation.ALTERPARTITION_PROTECTMODE });\n     tablePartitionCommandType.put(HiveParser.TOK_ALTERTABLE_FILEFORMAT,\n@@ -115,7 +116,7 @@\n     tablePartitionCommandType.put(HiveParser.TOK_ALTERTABLE_LOCATION,\n         new HiveOperation[] { HiveOperation.ALTERTABLE_LOCATION,\n             HiveOperation.ALTERPARTITION_LOCATION });\n-    tablePartitionCommandType.put(HiveParser.TOK_ALTERTABLE_ALTERPARTS_MERGEFILES,\n+    tablePartitionCommandType.put(HiveParser.TOK_ALTERTABLE_MERGEFILES,\n         new HiveOperation[] {HiveOperation.ALTERTABLE_MERGEFILES,\n             HiveOperation.ALTERPARTITION_MERGEFILES });\n     tablePartitionCommandType.put(HiveParser.TOK_ALTERTABLE_SERIALIZER,\n@@ -172,6 +173,7 @@ public static BaseSemanticAnalyzer get(HiveConf conf, ASTNode tree)\n       case HiveParser.TOK_DROPTABLE_PROPERTIES:\n       case HiveParser.TOK_ALTERTABLE_SERIALIZER:\n       case HiveParser.TOK_ALTERTABLE_SERDEPROPERTIES:\n+      case HiveParser.TOK_ALTERTABLE_PARTCOLTYPE:\n       case HiveParser.TOK_ALTERINDEX_REBUILD:\n       case HiveParser.TOK_ALTERINDEX_PROPERTIES:\n       case HiveParser.TOK_ALTERVIEW_PROPERTIES:\n@@ -196,7 +198,6 @@ public static BaseSemanticAnalyzer get(HiveConf conf, ASTNode tree)\n       case HiveParser.TOK_ALTERTABLE_TOUCH:\n       case HiveParser.TOK_ALTERTABLE_ARCHIVE:\n       case HiveParser.TOK_ALTERTABLE_UNARCHIVE:\n-      case HiveParser.TOK_ALTERTABLE_ALTERPARTS:\n       case HiveParser.TOK_LOCKTABLE:\n       case HiveParser.TOK_UNLOCKTABLE:\n       case HiveParser.TOK_LOCKDB:",
                "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzerFactory.java",
                "sha": "2495c40e0c1a0f0f60e8fe12f8265723cdd51c9b",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java?ref=4d62b4621871c79de3210157dc33021d746b5b2b",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java",
                "patch": "@@ -106,6 +106,7 @@\n   ALTERTABLE_SKEWED(\"ALTERTABLE_SKEWED\", new Privilege[] {Privilege.ALTER_METADATA}, null),\n   ALTERTBLPART_SKEWED_LOCATION(\"ALTERTBLPART_SKEWED_LOCATION\",\n       new Privilege[] {Privilege.ALTER_DATA}, null),\n+  ALTERTABLE_PARTCOLTYPE(\"ALTERTABLE_PARTCOLTYPE\", new Privilege[] { Privilege.SELECT }, new Privilege[] { Privilege.ALTER_DATA }),\n   ALTERVIEW_RENAME(\"ALTERVIEW_RENAME\", new Privilege[] {Privilege.ALTER_METADATA}, null),\n   ;\n ",
                "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/plan/HiveOperation.java",
                "sha": "e1a3dce303df13165fc064bdb5a92ec57eba8aa6",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HiveOperationType.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HiveOperationType.java?ref=4d62b4621871c79de3210157dc33021d746b5b2b",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HiveOperationType.java",
                "patch": "@@ -49,6 +49,7 @@\n   ALTERTABLE_UNARCHIVE,\n   ALTERTABLE_PROPERTIES,\n   ALTERTABLE_SERIALIZER,\n+  ALTERTABLE_PARTCOLTYPE,\n   ALTERPARTITION_SERIALIZER,\n   ALTERTABLE_SERDEPROPERTIES,\n   ALTERPARTITION_SERDEPROPERTIES,",
                "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HiveOperationType.java",
                "sha": "0fcfe5252242c570ac37a700b2d0cf2a52e1e8e2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/test/results/clientnegative/alter_partition_coltype_2columns.q.out",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/alter_partition_coltype_2columns.q.out?ref=4d62b4621871c79de3210157dc33021d746b5b2b",
                "deletions": 1,
                "filename": "ql/src/test/results/clientnegative/alter_partition_coltype_2columns.q.out",
                "patch": "@@ -33,4 +33,4 @@ ts                  \tstring              \tNone\n \t \t \n dt                  \tstring              \tNone                \n ts                  \tstring              \tNone                \n-FAILED: ParseException line 4:50 mismatched input ',' expecting ) near 'int' in alter table partition statement\n+FAILED: ParseException line 4:50 mismatched input ',' expecting ) near 'int' in alter partition key type",
                "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/test/results/clientnegative/alter_partition_coltype_2columns.q.out",
                "sha": "c8f4021d7c47d51a4128b46bba53d6d70cbc52b3",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/test/results/clientpositive/alter_partition_coltype.q.out",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/alter_partition_coltype.q.out?ref=4d62b4621871c79de3210157dc33021d746b5b2b",
                "deletions": 6,
                "filename": "ql/src/test/results/clientpositive/alter_partition_coltype.q.out",
                "patch": "@@ -50,11 +50,11 @@ POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src\n 25\n PREHOOK: query: -- alter partition key column data type for dt column.\n alter table alter_coltype partition column (dt int)\n-PREHOOK: type: null\n+PREHOOK: type: ALTERTABLE_PARTCOLTYPE\n PREHOOK: Input: default@alter_coltype\n POSTHOOK: query: -- alter partition key column data type for dt column.\n alter table alter_coltype partition column (dt int)\n-POSTHOOK: type: null\n+POSTHOOK: type: ALTERTABLE_PARTCOLTYPE\n POSTHOOK: Input: default@alter_coltype\n POSTHOOK: Output: default@alter_coltype\n POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]\n@@ -240,22 +240,22 @@ POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src\n 0\n PREHOOK: query: -- alter partition key column data type for ts column.\n alter table alter_coltype partition column (ts double)\n-PREHOOK: type: null\n+PREHOOK: type: ALTERTABLE_PARTCOLTYPE\n PREHOOK: Input: default@alter_coltype\n POSTHOOK: query: -- alter partition key column data type for ts column.\n alter table alter_coltype partition column (ts double)\n-POSTHOOK: type: null\n+POSTHOOK: type: ALTERTABLE_PARTCOLTYPE\n POSTHOOK: Input: default@alter_coltype\n POSTHOOK: Output: default@alter_coltype\n POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]\n POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]\n POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]\n POSTHOOK: Lineage: alter_coltype PARTITION(dt=100x,ts=6:30pm).value SIMPLE [(src1)src1.FieldSchema(name:value, type:string, comment:default), ]\n PREHOOK: query: alter table alter_coltype partition column (dt string)\n-PREHOOK: type: null\n+PREHOOK: type: ALTERTABLE_PARTCOLTYPE\n PREHOOK: Input: default@alter_coltype\n POSTHOOK: query: alter table alter_coltype partition column (dt string)\n-POSTHOOK: type: null\n+POSTHOOK: type: ALTERTABLE_PARTCOLTYPE\n POSTHOOK: Input: default@alter_coltype\n POSTHOOK: Output: default@alter_coltype\n POSTHOOK: Lineage: alter_coltype PARTITION(dt=10,ts=3.0).key SIMPLE [(src1)src1.FieldSchema(name:key, type:string, comment:default), ]",
                "raw_url": "https://github.com/apache/hive/raw/4d62b4621871c79de3210157dc33021d746b5b2b/ql/src/test/results/clientpositive/alter_partition_coltype.q.out",
                "sha": "04b9b2ce5c3639d8a5fae29514b396b199c2c8c5",
                "status": "modified"
            }
        ],
        "message": "HIVE-6205 : alter <table> partition column throws NPE in authorization (Navis via Thejas Nair)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1561391 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/2684a3ff8e8324be6541b35771b0171907ded0d1",
        "patched_files": [
            "HiveOperationType.java",
            "SemanticAnalyzerFactory.java",
            "Driver.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestSemanticAnalyzerFactory.java",
            "TestDriver.java",
            "TestHiveOperationType.java"
        ]
    },
    "hive_4f683fa": {
        "bug_id": "hive_4f683fa",
        "commit": "https://github.com/apache/hive/commit/4f683fa1d6f756a5533e94ce95300b14582b8f19",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/4f683fa1d6f756a5533e94ce95300b14582b8f19/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=4f683fa1d6f756a5533e94ce95300b14582b8f19",
                "deletions": 2,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "patch": "@@ -1236,8 +1236,13 @@ public void getMetaData(QB qb, ReadEntity parentInput) throws SemanticException\n             sqAliasToCTEName.put(alias, cte_name);\n             continue;\n           }\n-          throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(qb\n-              .getParseInfo().getSrcForAlias(alias)));\n+          ASTNode src = qb.getParseInfo().getSrcForAlias(alias);\n+          if (null != src) {\n+            throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(src));\n+          } else {\n+            throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(alias));\n+          }\n+\n         }\n \n         // Disallow INSERT INTO on bucketized tables",
                "raw_url": "https://github.com/apache/hive/raw/4f683fa1d6f756a5533e94ce95300b14582b8f19/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "sha": "f80dd029bb6b220cd459d06a05de29dabded912a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/4f683fa1d6f756a5533e94ce95300b14582b8f19/ql/src/test/queries/clientnegative/analyze_non_existent_tbl.q",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/analyze_non_existent_tbl.q?ref=4f683fa1d6f756a5533e94ce95300b14582b8f19",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientnegative/analyze_non_existent_tbl.q",
                "patch": "@@ -0,0 +1 @@\n+analyze table nonexistent compute statistics;",
                "raw_url": "https://github.com/apache/hive/raw/4f683fa1d6f756a5533e94ce95300b14582b8f19/ql/src/test/queries/clientnegative/analyze_non_existent_tbl.q",
                "sha": "78a97019f192e698ab6669c0fa51feb5b3b8c9dd",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/4f683fa1d6f756a5533e94ce95300b14582b8f19/ql/src/test/results/clientnegative/analyze_non_existent_tbl.q.out",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/analyze_non_existent_tbl.q.out?ref=4f683fa1d6f756a5533e94ce95300b14582b8f19",
                "deletions": 0,
                "filename": "ql/src/test/results/clientnegative/analyze_non_existent_tbl.q.out",
                "patch": "@@ -0,0 +1 @@\n+FAILED: SemanticException [Error 10001]: Table not found nonexistent",
                "raw_url": "https://github.com/apache/hive/raw/4f683fa1d6f756a5533e94ce95300b14582b8f19/ql/src/test/results/clientnegative/analyze_non_existent_tbl.q.out",
                "sha": "ab2ecbed16291746346aaaf5e7e2b55dc064e995",
                "status": "added"
            }
        ],
        "message": "HIVE-6545 : analyze table throws NPE for non-existent tables. (Ashutosh Chauhan via Harish Butani)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1574257 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/bda54e30c61b3c8168fdc294ea06695cb8a2771c",
        "patched_files": [
            "SemanticAnalyzer.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestSemanticAnalyzer.java"
        ]
    },
    "hive_50177ef": {
        "bug_id": "hive_50177ef",
        "commit": "https://github.com/apache/hive/commit/50177ef69486730c10ee9460870eefe51050826b",
        "file": [
            {
                "additions": 88,
                "blob_url": "https://github.com/apache/hive/blob/50177ef69486730c10ee9460870eefe51050826b/ql/src/test/org/apache/hadoop/hive/ql/io/TestAcidInputFormat.java",
                "changes": 88,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/io/TestAcidInputFormat.java?ref=50177ef69486730c10ee9460870eefe51050826b",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/io/TestAcidInputFormat.java",
                "patch": "@@ -0,0 +1,88 @@\n+package org.apache.hadoop.hive.ql.io;\n+\n+import static org.hamcrest.CoreMatchers.is;\n+import static org.junit.Assert.assertThat;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n+import java.io.DataInput;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+\n+import org.apache.hadoop.hive.ql.io.AcidInputFormat.DeltaMetaData;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.mockito.Mock;\n+import org.mockito.runners.MockitoJUnitRunner;\n+\n+@RunWith(MockitoJUnitRunner.class)\n+public class TestAcidInputFormat {\n+\n+  @Mock\n+  private DataInput mockDataInput;\n+\n+  @Test\n+  public void testDeltaMetaDataReadFieldsNoStatementIds() throws Exception {\n+    when(mockDataInput.readLong()).thenReturn(1L, 2L);\n+    when(mockDataInput.readInt()).thenReturn(0);\n+\n+    DeltaMetaData deltaMetaData = new AcidInputFormat.DeltaMetaData();\n+    deltaMetaData.readFields(mockDataInput);\n+\n+    verify(mockDataInput, times(1)).readInt();\n+    assertThat(deltaMetaData.getMinTxnId(), is(1L));\n+    assertThat(deltaMetaData.getMaxTxnId(), is(2L));\n+    assertThat(deltaMetaData.getStmtIds().isEmpty(), is(true));\n+  }\n+\n+  @Test\n+  public void testDeltaMetaDataReadFieldsWithStatementIds() throws Exception {\n+    when(mockDataInput.readLong()).thenReturn(1L, 2L);\n+    when(mockDataInput.readInt()).thenReturn(2, 100, 101);\n+\n+    DeltaMetaData deltaMetaData = new AcidInputFormat.DeltaMetaData();\n+    deltaMetaData.readFields(mockDataInput);\n+\n+    verify(mockDataInput, times(3)).readInt();\n+    assertThat(deltaMetaData.getMinTxnId(), is(1L));\n+    assertThat(deltaMetaData.getMaxTxnId(), is(2L));\n+    assertThat(deltaMetaData.getStmtIds().size(), is(2));\n+    assertThat(deltaMetaData.getStmtIds().get(0), is(100));\n+    assertThat(deltaMetaData.getStmtIds().get(1), is(101));\n+  }\n+\n+  @Test\n+  public void testDeltaMetaConstructWithState() throws Exception {\n+    DeltaMetaData deltaMetaData = new AcidInputFormat.DeltaMetaData(2000L, 2001L, Arrays.asList(97, 98, 99));\n+\n+    assertThat(deltaMetaData.getMinTxnId(), is(2000L));\n+    assertThat(deltaMetaData.getMaxTxnId(), is(2001L));\n+    assertThat(deltaMetaData.getStmtIds().size(), is(3));\n+    assertThat(deltaMetaData.getStmtIds().get(0), is(97));\n+    assertThat(deltaMetaData.getStmtIds().get(1), is(98));\n+    assertThat(deltaMetaData.getStmtIds().get(2), is(99));\n+  }\n+\n+  @Test\n+  public void testDeltaMetaDataReadFieldsWithStatementIdsResetsState() throws Exception {\n+    when(mockDataInput.readLong()).thenReturn(1L, 2L);\n+    when(mockDataInput.readInt()).thenReturn(2, 100, 101);\n+\n+    List<Integer> statementIds = new ArrayList<>();\n+    statementIds.add(97);\n+    statementIds.add(98);\n+    statementIds.add(99);\n+    DeltaMetaData deltaMetaData = new AcidInputFormat.DeltaMetaData(2000L, 2001L, statementIds);\n+    deltaMetaData.readFields(mockDataInput);\n+\n+    verify(mockDataInput, times(3)).readInt();\n+    assertThat(deltaMetaData.getMinTxnId(), is(1L));\n+    assertThat(deltaMetaData.getMaxTxnId(), is(2L));\n+    assertThat(deltaMetaData.getStmtIds().size(), is(2));\n+    assertThat(deltaMetaData.getStmtIds().get(0), is(100));\n+    assertThat(deltaMetaData.getStmtIds().get(1), is(101));\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/hive/raw/50177ef69486730c10ee9460870eefe51050826b/ql/src/test/org/apache/hadoop/hive/ql/io/TestAcidInputFormat.java",
                "sha": "6a776701fc4228bb9cdfa16bcf2d027ca8456eb7",
                "status": "added"
            }
        ],
        "message": "HIVE-12202 NPE thrown when reading legacy ACID delta files(missed a file)(Elliot West via Eugene Koifman)",
        "parent": "https://github.com/apache/hive/commit/6577f55cd7f21568994638399f9c31bef578b5cc",
        "patched_files": [
            "AcidInputFormat.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestAcidInputFormat.java"
        ]
    },
    "hive_50610ce": {
        "bug_id": "hive_50610ce",
        "commit": "https://github.com/apache/hive/commit/50610ce3757506c5df96025ecaacaf04a566c245",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/50610ce3757506c5df96025ecaacaf04a566c245/itests/src/test/resources/testconfiguration.properties",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=50610ce3757506c5df96025ecaacaf04a566c245",
                "deletions": 0,
                "filename": "itests/src/test/resources/testconfiguration.properties",
                "patch": "@@ -29,6 +29,7 @@ minimr.query.files=auto_sortmerge_join_16.q,\\\n   list_bucket_dml_10.q,\\\n   load_fs2.q,\\\n   load_hdfs_file_with_space_in_the_name.q,\\\n+  non_native_window_udf.q, \\\n   optrstat_groupby.q,\\\n   parallel_orderby.q,\\\n   ql_rewrite_gbtoidx.q,\\",
                "raw_url": "https://github.com/apache/hive/raw/50610ce3757506c5df96025ecaacaf04a566c245/itests/src/test/resources/testconfiguration.properties",
                "sha": "4ab928056ae6d64a70f7363b00881a83221fabd4",
                "status": "modified"
            },
            {
                "additions": 77,
                "blob_url": "https://github.com/apache/hive/blob/50610ce3757506c5df96025ecaacaf04a566c245/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java",
                "changes": 81,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java?ref=50610ce3757506c5df96025ecaacaf04a566c245",
                "deletions": 4,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java",
                "patch": "@@ -20,10 +20,14 @@\n \n import java.util.AbstractList;\n import java.util.ArrayList;\n+import java.util.HashMap;\n import java.util.Iterator;\n import java.util.List;\n+import java.util.Map;\n \n import org.apache.commons.lang.ArrayUtils;\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.hive.common.type.HiveDecimal;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n@@ -60,10 +64,42 @@\n \n @SuppressWarnings(\"deprecation\")\n public class WindowingTableFunction extends TableFunctionEvaluator {\n+  public static final Log LOG =LogFactory.getLog(WindowingTableFunction.class.getName());\n+  static class WindowingFunctionInfoHelper {\n+    private boolean supportsWindow;\n+\n+    WindowingFunctionInfoHelper() {\n+    }\n+\n+    public WindowingFunctionInfoHelper(boolean supportsWindow) {\n+      this.supportsWindow = supportsWindow;\n+    }\n+\n+    public boolean isSupportsWindow() {\n+      return supportsWindow;\n+    }\n+    public void setSupportsWindow(boolean supportsWindow) {\n+      this.supportsWindow = supportsWindow;\n+    }\n+  }\n \n   StreamingState streamingState;\n   RankLimit rnkLimitDef;\n+\n+  // There is some information about the windowing functions that needs to be initialized\n+  // during query compilation time, and made available to during the map/reduce tasks via\n+  // plan serialization.\n+  Map<String, WindowingFunctionInfoHelper> windowingFunctionHelpers = null;\n   \n+  public Map<String, WindowingFunctionInfoHelper> getWindowingFunctionHelpers() {\n+    return windowingFunctionHelpers;\n+  }\n+\n+  public void setWindowingFunctionHelpers(\n+      Map<String, WindowingFunctionInfoHelper> windowingFunctionHelpers) {\n+    this.windowingFunctionHelpers = windowingFunctionHelpers;\n+  }\n+\n   @SuppressWarnings({ \"unchecked\", \"rawtypes\" })\n   @Override\n   public void execute(PTFPartitionIterator<Object> pItr, PTFPartition outP) throws HiveException {\n@@ -147,9 +183,8 @@ private boolean processWindow(WindowFunctionDef wFn) {\n   private boolean streamingPossible(Configuration cfg, WindowFunctionDef wFnDef)\n       throws HiveException {\n     WindowFrameDef wdwFrame = wFnDef.getWindowFrame();\n-    WindowFunctionInfo wFnInfo = FunctionRegistry.getWindowFunctionInfo(wFnDef\n-        .getName());\n \n+    WindowingFunctionInfoHelper wFnInfo = getWindowingFunctionInfoHelper(wFnDef.getName());\n     if (!wFnInfo.isSupportsWindow()) {\n       return true;\n     }\n@@ -259,6 +294,45 @@ private boolean streamingPossible(Configuration cfg, WindowFunctionDef wFnDef)\n     return new int[] {precedingSpan, followingSpan};\n   }\n \n+  private void initializeWindowingFunctionInfoHelpers() throws SemanticException {\n+    // getWindowFunctionInfo() cannot be called during map/reduce tasks. So cache necessary\n+    // values during query compilation, and rely on plan serialization to bring this info\n+    // to the object during the map/reduce tasks.\n+    if (windowingFunctionHelpers != null) {\n+      return;\n+    }\n+\n+    windowingFunctionHelpers = new HashMap<String, WindowingFunctionInfoHelper>();\n+    WindowTableFunctionDef tabDef = (WindowTableFunctionDef) getTableDef();\n+    for (int i = 0; i < tabDef.getWindowFunctions().size(); i++) {\n+      WindowFunctionDef wFn = tabDef.getWindowFunctions().get(i);\n+      GenericUDAFEvaluator fnEval = wFn.getWFnEval();\n+      WindowFunctionInfo wFnInfo = FunctionRegistry.getWindowFunctionInfo(wFn.getName());\n+      boolean supportsWindow = wFnInfo.isSupportsWindow();\n+      windowingFunctionHelpers.put(wFn.getName(), new WindowingFunctionInfoHelper(supportsWindow));\n+    }\n+  }\n+\n+  @Override\n+  protected void setOutputOI(StructObjectInspector outputOI) {\n+    super.setOutputOI(outputOI);\n+    // Call here because at this point the WindowTableFunctionDef has been set\n+    try {\n+      initializeWindowingFunctionInfoHelpers();\n+    } catch (SemanticException err) {\n+      throw new RuntimeException(\"Unexpected error while setting up windowing function\", err);\n+    }\n+  }\n+\n+  private WindowingFunctionInfoHelper getWindowingFunctionInfoHelper(String fnName) {\n+    WindowingFunctionInfoHelper wFnInfoHelper = windowingFunctionHelpers.get(fnName);\n+    if (wFnInfoHelper == null) {\n+      // Should not happen\n+      throw new RuntimeException(\"No cached WindowingFunctionInfoHelper for \" + fnName);\n+    }\n+    return wFnInfoHelper;\n+  }\n+\n   @Override\n   public void initializeStreaming(Configuration cfg,\n       StructObjectInspector inputOI, boolean isMapSide) throws HiveException {\n@@ -412,8 +486,7 @@ public void startPartition() throws HiveException {\n       if (fnEval instanceof ISupportStreamingModeForWindowing) {\n         fnEval.terminate(streamingState.aggBuffers[i]);\n \n-        WindowFunctionInfo wFnInfo = FunctionRegistry.getWindowFunctionInfo(wFn\n-            .getName());\n+        WindowingFunctionInfoHelper wFnInfo = getWindowingFunctionInfoHelper(wFn.getName());\n         if (!wFnInfo.isSupportsWindow()) {\n           numRowsRemaining = ((ISupportStreamingModeForWindowing) fnEval)\n               .getRowsRemainingAfterTerminate();",
                "raw_url": "https://github.com/apache/hive/raw/50610ce3757506c5df96025ecaacaf04a566c245/ql/src/java/org/apache/hadoop/hive/ql/udf/ptf/WindowingTableFunction.java",
                "sha": "d7817d90dce7c851affdf35aff65ce3de259c866",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hive/blob/50610ce3757506c5df96025ecaacaf04a566c245/ql/src/test/queries/clientpositive/non_native_window_udf.q",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/non_native_window_udf.q?ref=50610ce3757506c5df96025ecaacaf04a566c245",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/non_native_window_udf.q",
                "patch": "@@ -0,0 +1,11 @@\n+\n+create temporary function mylastval as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue';\n+\n+select  p_mfgr,p_name, p_size, \n+sum(p_size) over (distribute by p_mfgr sort by p_name rows between current row and current row) as s2, \n+first_value(p_size) over w1  as f, \n+last_value(p_size, false) over w1  as l,\n+mylastval(p_size, false) over w1  as m \n+from part \n+window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and 2 following);\n+",
                "raw_url": "https://github.com/apache/hive/raw/50610ce3757506c5df96025ecaacaf04a566c245/ql/src/test/queries/clientpositive/non_native_window_udf.q",
                "sha": "c3b88453d87b1b76c455026e4a1079f13342f52c",
                "status": "added"
            },
            {
                "additions": 52,
                "blob_url": "https://github.com/apache/hive/blob/50610ce3757506c5df96025ecaacaf04a566c245/ql/src/test/results/clientpositive/non_native_window_udf.q.out",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/non_native_window_udf.q.out?ref=50610ce3757506c5df96025ecaacaf04a566c245",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/non_native_window_udf.q.out",
                "patch": "@@ -0,0 +1,52 @@\n+PREHOOK: query: create temporary function mylastval as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue'\n+PREHOOK: type: CREATEFUNCTION\n+PREHOOK: Output: mylastval\n+POSTHOOK: query: create temporary function mylastval as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFLastValue'\n+POSTHOOK: type: CREATEFUNCTION\n+POSTHOOK: Output: mylastval\n+PREHOOK: query: select  p_mfgr,p_name, p_size, \n+sum(p_size) over (distribute by p_mfgr sort by p_name rows between current row and current row) as s2, \n+first_value(p_size) over w1  as f, \n+last_value(p_size, false) over w1  as l,\n+mylastval(p_size, false) over w1  as m \n+from part \n+window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and 2 following)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@part\n+#### A masked pattern was here ####\n+POSTHOOK: query: select  p_mfgr,p_name, p_size, \n+sum(p_size) over (distribute by p_mfgr sort by p_name rows between current row and current row) as s2, \n+first_value(p_size) over w1  as f, \n+last_value(p_size, false) over w1  as l,\n+mylastval(p_size, false) over w1  as m \n+from part \n+window w1 as (distribute by p_mfgr sort by p_name rows between 2 preceding and 2 following)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@part\n+#### A masked pattern was here ####\n+Manufacturer#1\talmond antique burnished rose metallic\t2\t2\t2\t34\t34\n+Manufacturer#1\talmond antique burnished rose metallic\t2\t2\t2\t6\t6\n+Manufacturer#1\talmond antique chartreuse lavender yellow\t34\t34\t2\t28\t28\n+Manufacturer#1\talmond antique salmon chartreuse burlywood\t6\t6\t2\t42\t42\n+Manufacturer#1\talmond aquamarine burnished black steel\t28\t28\t34\t42\t42\n+Manufacturer#1\talmond aquamarine pink moccasin thistle\t42\t42\t6\t42\t42\n+Manufacturer#2\talmond antique violet chocolate turquoise\t14\t14\t14\t2\t2\n+Manufacturer#2\talmond antique violet turquoise frosted\t40\t40\t14\t25\t25\n+Manufacturer#2\talmond aquamarine midnight light salmon\t2\t2\t14\t18\t18\n+Manufacturer#2\talmond aquamarine rose maroon antique\t25\t25\t40\t18\t18\n+Manufacturer#2\talmond aquamarine sandy cyan gainsboro\t18\t18\t2\t18\t18\n+Manufacturer#3\talmond antique chartreuse khaki white\t17\t17\t17\t19\t19\n+Manufacturer#3\talmond antique forest lavender goldenrod\t14\t14\t17\t1\t1\n+Manufacturer#3\talmond antique metallic orange dim\t19\t19\t17\t45\t45\n+Manufacturer#3\talmond antique misty red olive\t1\t1\t14\t45\t45\n+Manufacturer#3\talmond antique olive coral navajo\t45\t45\t19\t45\t45\n+Manufacturer#4\talmond antique gainsboro frosted violet\t10\t10\t10\t27\t27\n+Manufacturer#4\talmond antique violet mint lemon\t39\t39\t10\t7\t7\n+Manufacturer#4\talmond aquamarine floral ivory bisque\t27\t27\t10\t12\t12\n+Manufacturer#4\talmond aquamarine yellow dodger mint\t7\t7\t39\t12\t12\n+Manufacturer#4\talmond azure aquamarine papaya violet\t12\t12\t27\t12\t12\n+Manufacturer#5\talmond antique blue firebrick mint\t31\t31\t31\t2\t2\n+Manufacturer#5\talmond antique medium spring khaki\t6\t6\t31\t46\t46\n+Manufacturer#5\talmond antique sky peru orange\t2\t2\t31\t23\t23\n+Manufacturer#5\talmond aquamarine dodger light gainsboro\t46\t46\t6\t23\t23\n+Manufacturer#5\talmond azure blanched chiffon midnight\t23\t23\t2\t23\t23",
                "raw_url": "https://github.com/apache/hive/raw/50610ce3757506c5df96025ecaacaf04a566c245/ql/src/test/results/clientpositive/non_native_window_udf.q.out",
                "sha": "605e5b2456efc87282455e0d8d1a3ac3c66ecced",
                "status": "added"
            }
        ],
        "message": "HIVE-9073: NPE when using custom windowing UDAFs (Jason Dere, reviewed by Ashutosh Chauhan)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1671971 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/3ee63c1402e9efda6afa5a8d52785299a08b9a57",
        "patched_files": [
            "WindowingTableFunction.java",
            "non_native_window_udf.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "testconfiguration.java"
        ]
    },
    "hive_509308f": {
        "bug_id": "hive_509308f",
        "commit": "https://github.com/apache/hive/commit/509308f642f4af8eb44a9fb7f0f105198df9fac6",
        "file": [
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hive/blob/509308f642f4af8eb44a9fb7f0f105198df9fac6/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java?ref=509308f642f4af8eb44a9fb7f0f105198df9fac6",
                "deletions": 1,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java",
                "patch": "@@ -8539,7 +8539,17 @@ private String getPrimaryKeyConstraintName(String db_name, String tbl_name) thro\n     final String parent_tbl_name = parent_tbl_name_input;\n     final String foreign_db_name = foreign_db_name_input;\n     final String foreign_tbl_name = foreign_tbl_name_input;\n-    return new GetListHelper<SQLForeignKey>(foreign_db_name, foreign_tbl_name, allowSql, allowJdo) {\n+    final String db_name;\n+    final String tbl_name;\n+    if (foreign_tbl_name == null) {\n+      // The FK table name might be null if we are retrieving the constraint from the PK side\n+      db_name = parent_db_name_input;\n+      tbl_name = parent_tbl_name_input;\n+    } else {\n+      db_name = foreign_db_name_input;\n+      tbl_name = foreign_tbl_name_input;\n+    }\n+    return new GetListHelper<SQLForeignKey>(db_name, tbl_name, allowSql, allowJdo) {\n \n       @Override\n       protected List<SQLForeignKey> getSqlResult(GetHelper<List<SQLForeignKey>> ctx) throws MetaException {",
                "raw_url": "https://github.com/apache/hive/raw/509308f642f4af8eb44a9fb7f0f105198df9fac6/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java",
                "sha": "4676e15942d72b0db56bedf0ff30aa60964c28d8",
                "status": "modified"
            },
            {
                "additions": 46,
                "blob_url": "https://github.com/apache/hive/blob/509308f642f4af8eb44a9fb7f0f105198df9fac6/metastore/src/test/org/apache/hadoop/hive/metastore/TestObjectStore.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/test/org/apache/hadoop/hive/metastore/TestObjectStore.java?ref=509308f642f4af8eb44a9fb7f0f105198df9fac6",
                "deletions": 3,
                "filename": "metastore/src/test/org/apache/hadoop/hive/metastore/TestObjectStore.java",
                "patch": "@@ -63,6 +63,8 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import com.google.common.collect.ImmutableList;\n+\n import javax.jdo.Query;\n \n public class TestObjectStore {\n@@ -204,22 +206,63 @@ public void testDatabaseOps() throws MetaException, InvalidObjectException, NoSu\n   public void testTableOps() throws MetaException, InvalidObjectException, NoSuchObjectException, InvalidInputException {\n     Database db1 = new Database(DB1, \"description\", \"locationurl\", null);\n     objectStore.createDatabase(db1);\n-    StorageDescriptor sd = new StorageDescriptor(null, \"location\", null, null, false, 0, new SerDeInfo(\"SerDeName\", \"serializationLib\", null), null, null, null);\n+    StorageDescriptor sd1 = new StorageDescriptor(ImmutableList.of(new FieldSchema(\"pk_col\", \"double\", null)),\n+            \"location\", null, null, false, 0, new SerDeInfo(\"SerDeName\", \"serializationLib\", null),\n+            null, null, null);\n     HashMap<String,String> params = new HashMap<String,String>();\n     params.put(\"EXTERNAL\", \"false\");\n-    Table tbl1 = new Table(TABLE1, DB1, \"owner\", 1, 2, 3, sd, null, params, null, null, \"MANAGED_TABLE\");\n+    Table tbl1 = new Table(TABLE1, DB1, \"owner\", 1, 2, 3, sd1, null, params, null, null, \"MANAGED_TABLE\");\n     objectStore.createTable(tbl1);\n \n     List<String> tables = objectStore.getAllTables(DB1);\n     Assert.assertEquals(1, tables.size());\n     Assert.assertEquals(TABLE1, tables.get(0));\n \n-    Table newTbl1 = new Table(\"new\" + TABLE1, DB1, \"owner\", 1, 2, 3, sd, null, params, null, null, \"MANAGED_TABLE\");\n+    StorageDescriptor sd2 = new StorageDescriptor(ImmutableList.of(new FieldSchema(\"fk_col\", \"double\", null)),\n+            \"location\", null, null, false, 0, new SerDeInfo(\"SerDeName\", \"serializationLib\", null),\n+            null, null, null);\n+    Table newTbl1 = new Table(\"new\" + TABLE1, DB1, \"owner\", 1, 2, 3, sd2, null, params, null, null, \"MANAGED_TABLE\");\n     objectStore.alterTable(DB1, TABLE1, newTbl1);\n     tables = objectStore.getTables(DB1, \"new*\");\n     Assert.assertEquals(1, tables.size());\n     Assert.assertEquals(\"new\" + TABLE1, tables.get(0));\n \n+    objectStore.createTable(tbl1);\n+    tables = objectStore.getAllTables(DB1);\n+    Assert.assertEquals(2, tables.size());\n+\n+    List<SQLForeignKey> foreignKeys = objectStore.getForeignKeys(DB1, TABLE1, null, null);\n+    Assert.assertEquals(0, foreignKeys.size());\n+\n+    SQLPrimaryKey pk = new SQLPrimaryKey(DB1, TABLE1, \"pk_col\", 1,\n+            \"pk_const_1\", false, false, false);\n+    objectStore.addPrimaryKeys(ImmutableList.of(pk));\n+    SQLForeignKey fk = new SQLForeignKey(DB1, TABLE1, \"pk_col\",\n+            DB1, \"new\" + TABLE1, \"fk_col\", 1,\n+            0, 0, \"fk_const_1\", \"pk_const_1\", false, false, false);\n+    objectStore.addForeignKeys(ImmutableList.of(fk));\n+\n+    // Retrieve from PK side\n+    foreignKeys = objectStore.getForeignKeys(null, null, DB1, \"new\" + TABLE1);\n+    Assert.assertEquals(1, foreignKeys.size());\n+\n+    List<SQLForeignKey> fks = objectStore.getForeignKeys(null, null, DB1, \"new\" + TABLE1);\n+    if (fks != null) {\n+      for (SQLForeignKey fkcol : fks) {\n+        objectStore.dropConstraint(fkcol.getFktable_db(), fkcol.getFktable_name(), fkcol.getFk_name());\n+      }\n+    }\n+    // Retrieve from FK side\n+    foreignKeys = objectStore.getForeignKeys(DB1, TABLE1, null, null);\n+    Assert.assertEquals(0, foreignKeys.size());\n+    // Retrieve from PK side\n+    foreignKeys = objectStore.getForeignKeys(null, null, DB1, \"new\" + TABLE1);\n+    Assert.assertEquals(0, foreignKeys.size());\n+\n+    objectStore.dropTable(DB1, TABLE1);\n+    tables = objectStore.getAllTables(DB1);\n+    Assert.assertEquals(1, tables.size());\n+\n     objectStore.dropTable(DB1, \"new\" + TABLE1);\n     tables = objectStore.getAllTables(DB1);\n     Assert.assertEquals(0, tables.size());",
                "raw_url": "https://github.com/apache/hive/raw/509308f642f4af8eb44a9fb7f0f105198df9fac6/metastore/src/test/org/apache/hadoop/hive/metastore/TestObjectStore.java",
                "sha": "b28ea7359357406fcd0ffc01a864ff572ab5f278",
                "status": "modified"
            }
        ],
        "message": "HIVE-16788: ODBC call SQLForeignKeys leads to NPE if you use PK arguments rather than FK arguments (Jesus Camacho Rodriguez, reviewed by Ashutosh Chauhan)",
        "parent": "https://github.com/apache/hive/commit/8aee8d4f2b124fcfa093724b4de0a54287a8084f",
        "patched_files": [
            "ObjectStore.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestObjectStore.java"
        ]
    },
    "hive_50f52b7": {
        "bug_id": "hive_50f52b7",
        "commit": "https://github.com/apache/hive/commit/50f52b728f911634e03b8ff6251c15edf3b987cb",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hive/blob/50f52b728f911634e03b8ff6251c15edf3b987cb/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java?ref=50f52b728f911634e03b8ff6251c15edf3b987cb",
                "deletions": 1,
                "filename": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
                "patch": "@@ -1843,7 +1843,6 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n     TESTMODE_BUCKET_CODEC_VERSION(\"hive.test.bucketcodec.version\", 1,\n       \"For testing only.  Will make ACID subsystem write RecordIdentifier.bucketId in specified\\n\" +\n         \"format\", false),\n-    HIVE_QUERY_TIMESTAMP(\"hive.query.timestamp\", System.currentTimeMillis(), \"query execute time.\"),\n \n     HIVEMERGEMAPFILES(\"hive.merge.mapfiles\", true,\n         \"Merge small files at the end of a map-only job\"),",
                "raw_url": "https://github.com/apache/hive/raw/50f52b728f911634e03b8ff6251c15edf3b987cb/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
                "sha": "9df9cca278e9a31c024d570fab2d21c4c126b9b6",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hive/blob/50f52b728f911634e03b8ff6251c15edf3b987cb/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java?ref=50f52b728f911634e03b8ff6251c15edf3b987cb",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java",
                "patch": "@@ -1924,7 +1924,6 @@ public String getNextValuesTempTableSuffix() {\n    */\n   public void setupQueryCurrentTimestamp() {\n     queryCurrentTimestamp = new Timestamp(System.currentTimeMillis());\n-    sessionConf.setLongVar(ConfVars.HIVE_QUERY_TIMESTAMP, queryCurrentTimestamp.getTime());\n \n     // Provide a facility to set current timestamp during tests\n     if (sessionConf.getBoolVar(ConfVars.HIVE_IN_TEST)) {",
                "raw_url": "https://github.com/apache/hive/raw/50f52b728f911634e03b8ff6251c15edf3b987cb/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java",
                "sha": "6bb756cc08148ae4bb9c935f270579e8abeb717a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/50f52b728f911634e03b8ff6251c15edf3b987cb/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentDate.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentDate.java?ref=50f52b728f911634e03b8ff6251c15edf3b987cb",
                "deletions": 25,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentDate.java",
                "patch": "@@ -18,12 +18,8 @@\n package org.apache.hadoop.hive.ql.udf.generic;\n \n import java.sql.Date;\n-import java.sql.Timestamp;\n \n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.exec.Description;\n-import org.apache.hadoop.hive.ql.exec.MapredContext;\n import org.apache.hadoop.hive.ql.exec.UDFArgumentException;\n import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n@@ -43,13 +39,6 @@\n public class GenericUDFCurrentDate extends GenericUDF {\n \n   protected DateWritable currentDate;\n-  private Configuration conf;\n-\n-  @Override\n-  public void configure(MapredContext context) {\n-    super.configure(context);\n-    conf = context.getJobConf();\n-  }\n \n   @Override\n   public ObjectInspector initialize(ObjectInspector[] arguments)\n@@ -61,21 +50,8 @@ public ObjectInspector initialize(ObjectInspector[] arguments)\n     }\n \n     if (currentDate == null) {\n-      SessionState ss = SessionState.get();\n-      Timestamp queryTimestamp;\n-      if (ss == null) {\n-        if (conf == null) {\n-          queryTimestamp = new Timestamp(System.currentTimeMillis());\n-        } else {\n-          queryTimestamp = new Timestamp(\n-                  HiveConf.getLongVar(conf, HiveConf.ConfVars.HIVE_QUERY_TIMESTAMP));\n-        }\n-      } else {\n-        queryTimestamp = ss.getQueryCurrentTimestamp();\n-      }\n-\n       Date dateVal =\n-              Date.valueOf(queryTimestamp.toString().substring(0, 10));\n+          Date.valueOf(SessionState.get().getQueryCurrentTimestamp().toString().substring(0, 10));\n       currentDate = new DateWritable(dateVal);\n     }\n ",
                "raw_url": "https://github.com/apache/hive/raw/50f52b728f911634e03b8ff6251c15edf3b987cb/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentDate.java",
                "sha": "7d3c3f46aa1318cb1eaf152d3b9f0ab36ef00ff7",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/50f52b728f911634e03b8ff6251c15edf3b987cb/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentTimestamp.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentTimestamp.java?ref=50f52b728f911634e03b8ff6251c15edf3b987cb",
                "deletions": 25,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentTimestamp.java",
                "patch": "@@ -17,12 +17,7 @@\n  */\n package org.apache.hadoop.hive.ql.udf.generic;\n \n-import java.sql.Timestamp;\n-\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.exec.Description;\n-import org.apache.hadoop.hive.ql.exec.MapredContext;\n import org.apache.hadoop.hive.ql.exec.UDFArgumentException;\n import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n@@ -42,13 +37,6 @@\n public class GenericUDFCurrentTimestamp extends GenericUDF {\n \n   protected TimestampWritable currentTimestamp;\n-  private Configuration conf;\n-\n-  @Override\n-  public void configure(MapredContext context) {\n-    super.configure(context);\n-    conf = context.getJobConf();\n-  }\n \n   @Override\n   public ObjectInspector initialize(ObjectInspector[] arguments)\n@@ -60,19 +48,7 @@ public ObjectInspector initialize(ObjectInspector[] arguments)\n     }\n \n     if (currentTimestamp == null) {\n-      SessionState ss = SessionState.get();\n-      Timestamp queryTimestamp;\n-      if (ss == null) {\n-        if (conf == null) {\n-          queryTimestamp = new Timestamp(System.currentTimeMillis());\n-        } else {\n-          queryTimestamp = new Timestamp(\n-                  HiveConf.getLongVar(conf, HiveConf.ConfVars.HIVE_QUERY_TIMESTAMP));\n-        }\n-      } else {\n-        queryTimestamp = ss.getQueryCurrentTimestamp();\n-      }\n-      currentTimestamp = new TimestampWritable(queryTimestamp);\n+      currentTimestamp = new TimestampWritable(SessionState.get().getQueryCurrentTimestamp());\n     }\n \n     return PrimitiveObjectInspectorFactory.writableTimestampObjectInspector;",
                "raw_url": "https://github.com/apache/hive/raw/50f52b728f911634e03b8ff6251c15edf3b987cb/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentTimestamp.java",
                "sha": "9da51c84f51d3dae1eac46f8b1e7eef2e482e6c4",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/50f52b728f911634e03b8ff6251c15edf3b987cb/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUnixTimeStamp.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUnixTimeStamp.java?ref=50f52b728f911634e03b8ff6251c15edf3b987cb",
                "deletions": 26,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUnixTimeStamp.java",
                "patch": "@@ -18,11 +18,6 @@\n \n package org.apache.hadoop.hive.ql.udf.generic;\n \n-import java.sql.Timestamp;\n-\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.hive.conf.HiveConf;\n-import org.apache.hadoop.hive.ql.exec.MapredContext;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import org.apache.hadoop.hive.ql.exec.Description;\n@@ -42,34 +37,14 @@\n public class GenericUDFUnixTimeStamp extends GenericUDFToUnixTimeStamp {\n   private static final Logger LOG = LoggerFactory.getLogger(GenericUDFUnixTimeStamp.class);\n   private LongWritable currentTimestamp; // retValue is transient so store this separately.\n-  private Configuration conf;\n-\n-  @Override\n-  public void configure(MapredContext context) {\n-    super.configure(context);\n-    conf = context.getJobConf();\n-  }\n-\n   @Override\n   protected void initializeInput(ObjectInspector[] arguments) throws UDFArgumentException {\n     if (arguments.length > 0) {\n       super.initializeInput(arguments);\n     } else {\n       if (currentTimestamp == null) {\n         currentTimestamp = new LongWritable(0);\n-        SessionState ss = SessionState.get();\n-        Timestamp queryTimestamp;\n-        if (ss == null) {\n-          if (conf == null) {\n-            queryTimestamp = new Timestamp(System.currentTimeMillis());\n-          } else {\n-            queryTimestamp = new Timestamp(\n-                    HiveConf.getLongVar(conf, HiveConf.ConfVars.HIVE_QUERY_TIMESTAMP));\n-          }\n-        } else {\n-          queryTimestamp = ss.getQueryCurrentTimestamp();\n-        }\n-        setValueFromTs(currentTimestamp, queryTimestamp);\n+        setValueFromTs(currentTimestamp, SessionState.get().getQueryCurrentTimestamp());\n         String msg = \"unix_timestamp(void) is deprecated. Use current_timestamp instead.\";\n         SessionState.getConsole().printInfo(msg, false);\n       }",
                "raw_url": "https://github.com/apache/hive/raw/50f52b728f911634e03b8ff6251c15edf3b987cb/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUnixTimeStamp.java",
                "sha": "832983105f1f453a756a532cf21bcba8b3ae9fd0",
                "status": "modified"
            }
        ],
        "message": "Revert \"HIVE-13745: UDF current_date\u3001current_timestamp\u3001unix_timestamp NPE (Biao Wu, reviewed by Yongzhi Chen)\"\n\nThis reverts commit fb79870592d775cd836d5611e21ab1c7030aadba.",
        "parent": "https://github.com/apache/hive/commit/0d787cbc055eb237bcccd5fdbc144fb6b1d7d4ca",
        "patched_files": [
            "SessionState.java",
            "HiveConf.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestSessionState.java",
            "TestHiveConf.java"
        ]
    },
    "hive_5131046": {
        "bug_id": "hive_5131046",
        "commit": "https://github.com/apache/hive/commit/5131046ca795ecf958e4e24f163e6014588c2222",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/5131046ca795ecf958e4e24f163e6014588c2222/itests/src/test/resources/testconfiguration.properties",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=5131046ca795ecf958e4e24f163e6014588c2222",
                "deletions": 0,
                "filename": "itests/src/test/resources/testconfiguration.properties",
                "patch": "@@ -527,6 +527,7 @@ minillaplocal.query.files=\\\n   external_jdbc_table_partition.q,\\\n   external_jdbc_table_typeconversion.q,\\\n   fullouter_mapjoin_1_optimized.q,\\\n+  get_splits_0.q,\\\n   groupby2.q,\\\n   groupby_groupingset_bug.q,\\\n   hybridgrace_hashjoin_1.q,\\",
                "raw_url": "https://github.com/apache/hive/raw/5131046ca795ecf958e4e24f163e6014588c2222/itests/src/test/resources/testconfiguration.properties",
                "sha": "18e4f7f02c3e3667c8ab3ab09924571b82a73866",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hive/blob/5131046ca795ecf958e4e24f163e6014588c2222/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java?ref=5131046ca795ecf958e4e24f163e6014588c2222",
                "deletions": 9,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java",
                "patch": "@@ -545,22 +545,26 @@ public static String getKeywords(Set<String> excludes) {\n \n   public static RelNode parseQuery(HiveConf conf, String viewQuery)\n       throws SemanticException, IOException, ParseException {\n-    return getAnalyzer(conf).genLogicalPlan(parse(viewQuery));\n+    final Context ctx = new Context(conf);\n+    ctx.setIsLoadingMaterializedView(true);\n+    final ASTNode ast = parse(viewQuery, ctx);\n+    final CalcitePlanner analyzer = getAnalyzer(conf, ctx);\n+    return analyzer.genLogicalPlan(ast);\n   }\n \n   public static List<FieldSchema> parseQueryAndGetSchema(HiveConf conf, String viewQuery)\n       throws SemanticException, IOException, ParseException {\n-    final CalcitePlanner analyzer = getAnalyzer(conf);\n-    analyzer.genLogicalPlan(parse(viewQuery));\n+    final Context ctx = new Context(conf);\n+    ctx.setIsLoadingMaterializedView(true);\n+    final ASTNode ast = parse(viewQuery, ctx);\n+    final CalcitePlanner analyzer = getAnalyzer(conf, ctx);\n+    analyzer.genLogicalPlan(ast);\n     return analyzer.getResultSchema();\n   }\n \n-  private static CalcitePlanner getAnalyzer(HiveConf conf) throws SemanticException, IOException {\n-    final QueryState qs =\n-        new QueryState.Builder().withHiveConf(conf).build();\n-    CalcitePlanner analyzer = new CalcitePlanner(qs);\n-    Context ctx = new Context(conf);\n-    ctx.setIsLoadingMaterializedView(true);\n+  private static CalcitePlanner getAnalyzer(HiveConf conf, Context ctx) throws SemanticException {\n+    final QueryState qs = new QueryState.Builder().withHiveConf(conf).build();\n+    final CalcitePlanner analyzer = new CalcitePlanner(qs);\n     analyzer.initCtx(ctx);\n     analyzer.init(false);\n     return analyzer;",
                "raw_url": "https://github.com/apache/hive/raw/5131046ca795ecf958e4e24f163e6014588c2222/ql/src/java/org/apache/hadoop/hive/ql/parse/ParseUtils.java",
                "sha": "07c65af246f4a9d88d12c933f9638e5baa0f5cb5",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/5131046ca795ecf958e4e24f163e6014588c2222/ql/src/test/queries/clientpositive/get_splits_0.q",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/get_splits_0.q?ref=5131046ca795ecf958e4e24f163e6014588c2222",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/get_splits_0.q",
                "patch": "@@ -0,0 +1,3 @@\n+--! qt:dataset:src\n+select get_splits(\"SELECT * FROM src WHERE value in (SELECT value FROM src)\",0);\n+select get_splits(\"SELECT key AS `key 1`, value AS `value 1` FROM src\",0);",
                "raw_url": "https://github.com/apache/hive/raw/5131046ca795ecf958e4e24f163e6014588c2222/ql/src/test/queries/clientpositive/get_splits_0.q",
                "sha": "e585fda78fd4b35a1b5fdc3ae0740fdbecff2440",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hive/blob/5131046ca795ecf958e4e24f163e6014588c2222/ql/src/test/results/clientpositive/llap/get_splits_0.q.out",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/llap/get_splits_0.q.out?ref=5131046ca795ecf958e4e24f163e6014588c2222",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/llap/get_splits_0.q.out",
                "raw_url": "https://github.com/apache/hive/raw/5131046ca795ecf958e4e24f163e6014588c2222/ql/src/test/results/clientpositive/llap/get_splits_0.q.out",
                "sha": "e1ebe952975c089e1030076e3e26d77bf6799eb9",
                "status": "added"
            }
        ],
        "message": "HIVE-21041: NPE, ParseException in getting schema from logical plan (Teddy Choi, reviewed by Jesus Camacho Rodriguez)\n\nChange-Id: Iff9d9b02f934ed800f932ff916a59288a896f169",
        "parent": "https://github.com/apache/hive/commit/ef7c3963be035635ef75ee202b9826b9beb407d7",
        "patched_files": [
            "ParseUtils.java",
            "get_splits_0.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "testconfiguration.java"
        ]
    },
    "hive_534b447": {
        "bug_id": "hive_534b447",
        "commit": "https://github.com/apache/hive/commit/534b447046a0e66272635efb8429daf6f3e0e737",
        "file": [
            {
                "additions": 100,
                "blob_url": "https://github.com/apache/hive/blob/534b447046a0e66272635efb8429daf6f3e0e737/data/files/orc_create_people.txt",
                "changes": 100,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/data/files/orc_create_people.txt?ref=534b447046a0e66272635efb8429daf6f3e0e737",
                "deletions": 0,
                "filename": "data/files/orc_create_people.txt",
                "patch": "@@ -0,0 +1,100 @@\n+1\u0001Celeste\u0001Browning\u0001959-3763 Nec, Av.\u0001Ca\n+2\u0001Risa\u0001Yang\u0001P.O. Box 292, 8229 Porttitor Road\u0001Or\n+3\u0001Venus\u0001Sutton\u0001Ap #962-8021 Egestas Rd.\u0001Ca\n+4\u0001Gretchen\u0001Harrison\u0001P.O. Box 636, 8734 Magna Avenue\u0001Or\n+5\u0001Lani\u0001Irwin\u0001Ap #441-5911 Iaculis, Ave\u0001Ca\n+6\u0001Vera\u0001George\u0001409-1555 Vel, Ave\u0001Or\n+7\u0001Jessica\u0001Malone\u0001286-9779 Aliquam Road\u0001Ca\n+8\u0001Ann\u0001Chapman\u0001Ap #504-3915 Placerat Road\u0001Or\n+9\u0001Nigel\u0001Bartlett\u0001Ap #185-385 Diam Street\u0001Ca\n+10\u0001Azalia\u0001Jennings\u00015772 Diam St.\u0001Or\n+11\u0001Preston\u0001Cannon\u0001Ap #527-8769 Nunc Avenue\u0001Ca\n+12\u0001Allistair\u0001Vasquez\u00012562 Odio. St.\u0001Or\n+13\u0001Reed\u0001Hayes\u00015190 Elit Street\u0001Ca\n+14\u0001Elaine\u0001Barron\u0001P.O. Box 840, 8860 Sodales Av.\u0001Or\n+15\u0001Lydia\u0001Hood\u0001P.O. Box 698, 5666 Semper Road\u0001Ca\n+16\u0001Vance\u0001Maxwell\u0001298-3313 Malesuada Road\u0001Or\n+17\u0001Keiko\u0001Deleon\u0001P.O. Box 732, 5921 Massa. Av.\u0001Ca\n+18\u0001Dolan\u0001Kane\u0001Ap #906-3606 Ut Rd.\u0001Or\n+19\u0001Merritt\u0001Perkins\u0001P.O. Box 228, 7090 Egestas Street\u0001Ca\n+20\u0001Casey\u0001Salazar\u0001506-5065 Ut St.\u0001Or\n+21\u0001Samson\u0001Noel\u00011370 Ultrices, Road\u0001Ca\n+22\u0001Byron\u0001Walker\u0001P.O. Box 386, 8324 Tellus Ave\u0001Or\n+23\u0001Piper\u0001Singleton\u0001Ap #500-3561 Primis St.\u0001Ca\n+24\u0001Ria\u0001Mckinney\u00013080 Dui Rd.\u0001Or\n+25\u0001Rahim\u0001Stanley\u0001559-9016 Nascetur Street\u0001Ca\n+26\u0001Chloe\u0001Steele\u0001P.O. Box 766, 1628 Elit Street\u0001Or\n+27\u0001Paloma\u0001Ward\u0001Ap #390-3042 Ipsum Rd.\u0001Ca\n+28\u0001Roary\u0001Sherman\u0001Ap #409-6549 Metus St.\u0001Or\n+29\u0001Calvin\u0001Buckner\u00016378 Diam Avenue\u0001Ca\n+30\u0001Camille\u0001Good\u0001Ap #113-8659 Suspendisse St.\u0001Or\n+31\u0001Steel\u0001Ayala\u00015518 Justo St.\u0001Ca\n+32\u0001Josiah\u0001Gilbert\u0001Ap #149-6651 At, Av.\u0001Or\n+33\u0001Hamilton\u0001Cruz\u00014620 Tellus. Ave\u0001Ca\n+34\u0001Scarlet\u0001Santos\u0001586-1785 Velit. Av.\u0001Or\n+35\u0001Lewis\u0001Mcintyre\u0001629-6419 Ac Rd.\u0001Ca\n+36\u0001Arsenio\u0001Mejia\u0001P.O. Box 767, 8625 Justo Rd.\u0001Or\n+37\u0001Velma\u0001Haley\u00011377 At Rd.\u0001Ca\n+38\u0001Tatum\u0001Jennings\u0001829-7432 Posuere, Road\u0001Or\n+39\u0001Britanni\u0001Eaton\u00018811 Morbi Street\u0001Ca\n+40\u0001Aileen\u0001Jacobson\u0001P.O. Box 469, 2266 Dui, Rd.\u0001Or\n+41\u0001Kareem\u0001Ayala\u00012706 Ridiculus Street\u0001Ca\n+42\u0001Maite\u0001Rush\u00017592 Neque Road\u0001Or\n+43\u0001Signe\u0001Velasquez\u0001Ap #868-3039 Eget St.\u0001Ca\n+44\u0001Zorita\u0001Camacho\u0001P.O. Box 651, 3340 Quis Av.\u0001Or\n+45\u0001Glenna\u0001Curtis\u0001953-7965 Enim Ave\u0001Ca\n+46\u0001Quin\u0001Cortez\u00014898 Ridiculus St.\u0001Or\n+47\u0001Talon\u0001Dalton\u0001P.O. Box 408, 7597 Integer Rd.\u0001Ca\n+48\u0001Darryl\u0001Blankenship\u0001P.O. Box 771, 1471 Non Rd.\u0001Or\n+49\u0001Vernon\u0001Reyes\u0001P.O. Box 971, 7009 Vulputate Street\u0001Ca\n+50\u0001Tallulah\u0001Heath\u0001P.O. Box 865, 3697 Dis Ave\u0001Or\n+51\u0001Ciaran\u0001Olson\u00012721 Et St.\u0001Ca\n+52\u0001Orlando\u0001Witt\u0001P.O. Box 717, 1102 Nulla. Rd.\u0001Or\n+53\u0001Quinn\u0001Rice\u0001Ap #647-6627 Tristique Avenue\u0001Ca\n+54\u0001Wyatt\u0001Pickett\u0001Ap #128-3130 Vel, Rd.\u0001Or\n+55\u0001Emerald\u0001Copeland\u0001857-5119 Turpis Rd.\u0001Ca\n+56\u0001Jonas\u0001Quinn\u0001Ap #441-7183 Ligula. Street\u0001Or\n+57\u0001Willa\u0001Berg\u00016672 Velit Ave\u0001Ca\n+58\u0001Malik\u0001Lee\u0001998-9208 In Street\u0001Or\n+59\u0001Callie\u0001Medina\u00011620 Dui. Rd.\u0001Ca\n+60\u0001Luke\u0001Mason\u0001P.O. Box 143, 2070 Augue Rd.\u0001Or\n+61\u0001Shafira\u0001Estrada\u00018824 Ante Street\u0001Ca\n+62\u0001Elizabeth\u0001Rutledge\u0001315-6510 Sit St.\u0001Or\n+63\u0001Pandora\u0001Levine\u0001357-3596 Nibh. Ave\u0001Ca\n+64\u0001Hilel\u0001Prince\u0001845-1229 Sociosqu Rd.\u0001Or\n+65\u0001Rinah\u0001Torres\u0001Ap #492-9328 At St.\u0001Ca\n+66\u0001Yael\u0001Hobbs\u0001P.O. Box 477, 3896 In Street\u0001Or\n+67\u0001Nevada\u0001Nash\u0001P.O. Box 251, 1914 Tincidunt Road\u0001Ca\n+68\u0001Marny\u0001Huff\u0001P.O. Box 818, 6086 Ultricies St.\u0001Or\n+69\u0001Kimberley\u0001Miles\u0001Ap #893-3685 In Road\u0001Ca\n+70\u0001Duncan\u0001Fuller\u0001Ap #197-5216 Iaculis Street\u0001Or\n+71\u0001Yardley\u0001Leblanc\u0001P.O. Box 938, 1278 Sit Ave\u0001Ca\n+72\u0001Hamish\u0001Brewer\u0001Ap #854-781 Quisque St.\u0001Or\n+73\u0001Petra\u0001Moon\u0001453-6609 Curabitur Street\u0001Ca\n+74\u0001Reese\u0001Estrada\u0001Ap #382-3313 Malesuada St.\u0001Or\n+75\u0001Gage\u0001Higgins\u00017443 Eu Street\u0001Ca\n+76\u0001Zachery\u0001Camacho\u0001Ap #795-4143 Quam. St.\u0001Or\n+77\u0001Kelly\u0001Garner\u0001P.O. Box 895, 2843 Cras Rd.\u0001Ca\n+78\u0001Hanae\u0001Carr\u00019440 Amet St.\u0001Or\n+79\u0001Ann\u0001Alston\u0001884-7948 Dictum Road\u0001Ca\n+80\u0001Chancellor\u0001Cobb\u0001P.O. Box 889, 5978 Ac Avenue\u0001Or\n+81\u0001Dorothy\u0001Harrell\u00016974 Tristique Ave\u0001Ca\n+82\u0001Vaughan\u0001Leon\u00011610 Luctus Av.\u0001Or\n+83\u0001Wynne\u0001Jimenez\u0001321-9171 Felis. Avenue\u0001Ca\n+84\u0001Willa\u0001Mendoza\u0001489-182 Sed Av.\u0001Or\n+85\u0001Camden\u0001Goodwin\u00014579 Ante St.\u0001Ca\n+86\u0001Ifeoma\u0001French\u0001P.O. Box 160, 8769 Integer Road\u0001Or\n+87\u0001Ramona\u0001Strong\u00011666 Ridiculus Avenue\u0001Ca\n+88\u0001Brett\u0001Ramos\u0001Ap #579-9879 Et, Road\u0001Or\n+89\u0001Ulla\u0001Gray\u0001595-7066 Malesuada Road\u0001Ca\n+90\u0001Kevyn\u0001Mccall\u0001P.O. Box 968, 1420 Aenean Avenue\u0001Or\n+91\u0001Genevieve\u0001Wilkins\u0001908 Turpis. Street\u0001Ca\n+92\u0001Thane\u0001Oneil\u00016766 Lectus St.\u0001Or\n+93\u0001Mariko\u0001Cline\u0001P.O. Box 329, 5375 Ac St.\u0001Ca\n+94\u0001Lael\u0001Mclean\u0001500-7010 Sit St.\u0001Or\n+95\u0001Winifred\u0001Hopper\u0001Ap #140-8982 Velit Avenue\u0001Ca\n+96\u0001Rafael\u0001England\u0001P.O. Box 405, 7857 Eget Av.\u0001Or\n+97\u0001Dana\u0001Carter\u0001814-601 Purus. Av.\u0001Ca\n+98\u0001Juliet\u0001Battle\u0001Ap #535-1965 Cursus St.\u0001Or\n+99\u0001Wynter\u0001Vincent\u0001626-8492 Mollis Avenue\u0001Ca\n+100\u0001Wang\u0001Mitchell\u00014023 Lacinia. Ave\u0001Or",
                "raw_url": "https://github.com/apache/hive/raw/534b447046a0e66272635efb8429daf6f3e0e737/data/files/orc_create_people.txt",
                "sha": "884598981a13cfae7a7bfbbccaedf9d4c5b4083f",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/534b447046a0e66272635efb8429daf6f3e0e737/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java?ref=534b447046a0e66272635efb8429daf6f3e0e737",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java",
                "patch": "@@ -114,7 +114,7 @@\n         String[] neededColumnNames = columnNamesString.split(\",\");\n         int i = 0;\n         for(int columnId: types.get(0).getSubtypesList()) {\n-          if (includeColumn[columnId]) {\n+          if (includeColumn == null || includeColumn[columnId]) {\n             columnNames[columnId] = neededColumnNames[i++];\n           }\n         }",
                "raw_url": "https://github.com/apache/hive/raw/534b447046a0e66272635efb8429daf6f3e0e737/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java",
                "sha": "63b54abaf4e131d4a2fb19a0cc4997eb44236209",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/534b447046a0e66272635efb8429daf6f3e0e737/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java?ref=534b447046a0e66272635efb8429daf6f3e0e737",
                "deletions": 7,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java",
                "patch": "@@ -2129,13 +2129,18 @@ static TruthValue evaluatePredicate(OrcProto.ColumnStatistics index,\n     TruthValue[] leafValues = new TruthValue[sargLeaves.size()];\n     for(int rowGroup=0; rowGroup < result.length; ++rowGroup) {\n       for(int pred=0; pred < leafValues.length; ++pred) {\n-        OrcProto.ColumnStatistics stats =\n-            indexes[filterColumns[pred]].getEntry(rowGroup).getStatistics();\n-        leafValues[pred] = evaluatePredicate(stats, sargLeaves.get(pred));\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Stats = \" + stats);\n-          LOG.debug(\"Setting \" + sargLeaves.get(pred) + \" to \" +\n-              leafValues[pred]);\n+        if (filterColumns[pred] != -1) {\n+          OrcProto.ColumnStatistics stats =\n+              indexes[filterColumns[pred]].getEntry(rowGroup).getStatistics();\n+          leafValues[pred] = evaluatePredicate(stats, sargLeaves.get(pred));\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Stats = \" + stats);\n+            LOG.debug(\"Setting \" + sargLeaves.get(pred) + \" to \" +\n+                leafValues[pred]);\n+          }\n+        } else {\n+          // the column is a virtual column\n+          leafValues[pred] = TruthValue.YES_NO_NULL;\n         }\n       }\n       result[rowGroup] = sarg.evaluate(leafValues).isNotNeeded();",
                "raw_url": "https://github.com/apache/hive/raw/534b447046a0e66272635efb8429daf6f3e0e737/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java",
                "sha": "fe1845c519008a5b3e7e29345b29c6513d34704b",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/hive/blob/534b447046a0e66272635efb8429daf6f3e0e737/ql/src/test/queries/clientpositive/orc_create.q",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/orc_create.q?ref=534b447046a0e66272635efb8429daf6f3e0e737",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/orc_create.q",
                "patch": "@@ -1,6 +1,8 @@\n DROP TABLE orc_create;\n DROP TABLE orc_create_complex;\n DROP TABLE orc_create_staging;\n+DROP TABLE orc_create_people_staging;\n+DROP TABLE orc_create_people;\n \n CREATE TABLE orc_create_staging (\n   str STRING,\n@@ -38,6 +40,8 @@ set hive.default.fileformat=orc;\n CREATE TABLE orc_create (key INT, value STRING)\n    PARTITIONED BY (ds string);\n \n+set hive.default.fileformat=text;\n+\n DESCRIBE FORMATTED orc_create;\n \n CREATE TABLE orc_create_complex (\n@@ -61,6 +65,39 @@ SELECT mp from orc_create_complex;\n SELECT lst from orc_create_complex;\n SELECT strct from orc_create_complex;\n \n+CREATE TABLE orc_create_people_staging (\n+  id int,\n+  first_name string,\n+  last_name string,\n+  address string,\n+  state string);\n+\n+LOAD DATA LOCAL INPATH '../data/files/orc_create_people.txt'\n+  OVERWRITE INTO TABLE orc_create_people_staging;\n+\n+CREATE TABLE orc_create_people (\n+  id int,\n+  first_name string,\n+  last_name string,\n+  address string)\n+PARTITIONED BY (state string)\n+STORED AS orc;\n+\n+set hive.exec.dynamic.partition.mode=nonstrict;\n+\n+INSERT OVERWRITE TABLE orc_create_people PARTITION (state)\n+  SELECT * FROM orc_create_people_staging;\n+\n+SET hive.optimize.index.filter=true;\n+-- test predicate push down with partition pruning\n+SELECT COUNT(*) FROM orc_create_people where id < 10 and state = 'Ca';\n+\n+-- test predicate push down with no column projection\n+SELECT id, first_name, last_name, address\n+  FROM orc_create_people WHERE id > 90;\n+\n DROP TABLE orc_create;\n DROP TABLE orc_create_complex;\n DROP TABLE orc_create_staging;\n+DROP TABLE orc_create_people_staging;\n+DROP TABLE orc_create_people;",
                "raw_url": "https://github.com/apache/hive/raw/534b447046a0e66272635efb8429daf6f3e0e737/ql/src/test/queries/clientpositive/orc_create.q",
                "sha": "6aca5486445c809c54857929f5a17cb291f74093",
                "status": "modified"
            },
            {
                "additions": 210,
                "blob_url": "https://github.com/apache/hive/blob/534b447046a0e66272635efb8429daf6f3e0e737/ql/src/test/results/clientpositive/orc_create.q.out",
                "changes": 210,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/orc_create.q.out?ref=534b447046a0e66272635efb8429daf6f3e0e737",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/orc_create.q.out",
                "patch": "@@ -10,6 +10,14 @@ PREHOOK: query: DROP TABLE orc_create_staging\n PREHOOK: type: DROPTABLE\n POSTHOOK: query: DROP TABLE orc_create_staging\n POSTHOOK: type: DROPTABLE\n+PREHOOK: query: DROP TABLE orc_create_people_staging\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE orc_create_people_staging\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: DROP TABLE orc_create_people\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE orc_create_people\n+POSTHOOK: type: DROPTABLE\n PREHOOK: query: CREATE TABLE orc_create_staging (\n   str STRING,\n   mp  MAP<STRING,STRING>,\n@@ -398,6 +406,144 @@ POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_crea\n {\"a\":\"one\",\"b\":\"two\"}\n {\"a\":\"three\",\"b\":\"four\"}\n {\"a\":\"five\",\"b\":\"six\"}\n+PREHOOK: query: CREATE TABLE orc_create_people_staging (\n+  id int,\n+  first_name string,\n+  last_name string,\n+  address string,\n+  state string)\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: CREATE TABLE orc_create_people_staging (\n+  id int,\n+  first_name string,\n+  last_name string,\n+  address string,\n+  state string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@orc_create_people_staging\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+PREHOOK: query: LOAD DATA LOCAL INPATH '../data/files/orc_create_people.txt'\n+  OVERWRITE INTO TABLE orc_create_people_staging\n+PREHOOK: type: LOAD\n+PREHOOK: Output: default@orc_create_people_staging\n+POSTHOOK: query: LOAD DATA LOCAL INPATH '../data/files/orc_create_people.txt'\n+  OVERWRITE INTO TABLE orc_create_people_staging\n+POSTHOOK: type: LOAD\n+POSTHOOK: Output: default@orc_create_people_staging\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+PREHOOK: query: CREATE TABLE orc_create_people (\n+  id int,\n+  first_name string,\n+  last_name string,\n+  address string)\n+PARTITIONED BY (state string)\n+STORED AS orc\n+PREHOOK: type: CREATETABLE\n+POSTHOOK: query: CREATE TABLE orc_create_people (\n+  id int,\n+  first_name string,\n+  last_name string,\n+  address string)\n+PARTITIONED BY (state string)\n+STORED AS orc\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: default@orc_create_people\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+PREHOOK: query: INSERT OVERWRITE TABLE orc_create_people PARTITION (state)\n+  SELECT * FROM orc_create_people_staging\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people_staging\n+PREHOOK: Output: default@orc_create_people\n+POSTHOOK: query: INSERT OVERWRITE TABLE orc_create_people PARTITION (state)\n+  SELECT * FROM orc_create_people_staging\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people_staging\n+POSTHOOK: Output: default@orc_create_people@state=Ca\n+POSTHOOK: Output: default@orc_create_people@state=Or\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+PREHOOK: query: -- test predicate push down with partition pruning\n+SELECT COUNT(*) FROM orc_create_people where id < 10 and state = 'Ca'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Input: default@orc_create_people@state=Ca\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- test predicate push down with partition pruning\n+SELECT COUNT(*) FROM orc_create_people where id < 10 and state = 'Ca'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Input: default@orc_create_people@state=Ca\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+5\n+PREHOOK: query: -- test predicate push down with no column projection\n+SELECT id, first_name, last_name, address\n+  FROM orc_create_people WHERE id > 90\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Input: default@orc_create_people@state=Ca\n+PREHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- test predicate push down with no column projection\n+SELECT id, first_name, last_name, address\n+  FROM orc_create_people WHERE id > 90\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Input: default@orc_create_people@state=Ca\n+POSTHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+91\tGenevieve\tWilkins\t908 Turpis. Street\n+93\tMariko\tCline\tP.O. Box 329, 5375 Ac St.\n+95\tWinifred\tHopper\tAp #140-8982 Velit Avenue\n+97\tDana\tCarter\t814-601 Purus. Av.\n+99\tWynter\tVincent\t626-8492 Mollis Avenue\n+92\tThane\tOneil\t6766 Lectus St.\n+94\tLael\tMclean\t500-7010 Sit St.\n+96\tRafael\tEngland\tP.O. Box 405, 7857 Eget Av.\n+98\tJuliet\tBattle\tAp #535-1965 Cursus St.\n+100\tWang\tMitchell\t4023 Lacinia. Ave\n PREHOOK: query: DROP TABLE orc_create\n PREHOOK: type: DROPTABLE\n PREHOOK: Input: default@orc_create\n@@ -410,6 +556,14 @@ POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create\n POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n PREHOOK: query: DROP TABLE orc_create_complex\n PREHOOK: type: DROPTABLE\n PREHOOK: Input: default@orc_create_complex\n@@ -422,6 +576,14 @@ POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create\n POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n PREHOOK: query: DROP TABLE orc_create_staging\n PREHOOK: type: DROPTABLE\n PREHOOK: Input: default@orc_create_staging\n@@ -434,3 +596,51 @@ POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create\n POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+PREHOOK: query: DROP TABLE orc_create_people_staging\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@orc_create_people_staging\n+PREHOOK: Output: default@orc_create_people_staging\n+POSTHOOK: query: DROP TABLE orc_create_people_staging\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@orc_create_people_staging\n+POSTHOOK: Output: default@orc_create_people_staging\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+PREHOOK: query: DROP TABLE orc_create_people\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Output: default@orc_create_people\n+POSTHOOK: query: DROP TABLE orc_create_people\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Output: default@orc_create_people\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]",
                "raw_url": "https://github.com/apache/hive/raw/534b447046a0e66272635efb8429daf6f3e0e737/ql/src/test/results/clientpositive/orc_create.q.out",
                "sha": "03e1fb3fd2e503be9a79d7afe01d7a7e6fb12008",
                "status": "modified"
            }
        ],
        "message": "HIVE-5364 : NPE on some queries from partitioned orc table (Owen O'Malley via Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1529097 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/4adf1728e69e4f735c041dd551f5a65bde4f07d3",
        "patched_files": [
            "RecordReaderImpl.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestRecordReaderImpl.java"
        ]
    },
    "hive_53cf27b": {
        "bug_id": "hive_53cf27b",
        "commit": "https://github.com/apache/hive/commit/53cf27b412535b5d7c95c760971280c6a68e63ff",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hive/blob/53cf27b412535b5d7c95c760971280c6a68e63ff/cli/build.xml",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/cli/build.xml?ref=53cf27b412535b5d7c95c760971280c6a68e63ff",
                "deletions": 4,
                "filename": "cli/build.xml",
                "patch": "@@ -41,8 +41,4 @@ to call at top-level: ant deploy-contrib compile-core-test\n     </javac>\n   </target>\n \n-  <target name=\"test\">\n-    <echo message=\"Nothing to do!\"/>\n-  </target>\n-\n </project>",
                "raw_url": "https://github.com/apache/hive/raw/53cf27b412535b5d7c95c760971280c6a68e63ff/cli/build.xml",
                "sha": "add53e155f36f1fc46b55dbad76fc178a4440987",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/53cf27b412535b5d7c95c760971280c6a68e63ff/cli/ivy.xml",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/cli/ivy.xml?ref=53cf27b412535b5d7c95c760971280c6a68e63ff",
                "deletions": 0,
                "filename": "cli/ivy.xml",
                "patch": "@@ -28,5 +28,6 @@\n           <artifact name=\"hadoop\" type=\"source\" ext=\"tar.gz\"/>\n         </dependency>\n         <dependency org=\"commons-cli\" name=\"commons-cli\" rev=\"${commons-cli.version}\"/>\n+        <dependency org=\"org.mockito\" name=\"mockito-all\" rev=\"${mockito-all.version}\" />\n     </dependencies>\n </ivy-module>",
                "raw_url": "https://github.com/apache/hive/raw/53cf27b412535b5d7c95c760971280c6a68e63ff/cli/ivy.xml",
                "sha": "abe72329e2c7ea60b637726317aa51643d91ce3e",
                "status": "modified"
            },
            {
                "additions": 114,
                "blob_url": "https://github.com/apache/hive/blob/53cf27b412535b5d7c95c760971280c6a68e63ff/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java",
                "changes": 188,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java?ref=53cf27b412535b5d7c95c760971280c6a68e63ff",
                "deletions": 74,
                "filename": "cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java",
                "patch": "@@ -48,7 +48,6 @@\n import org.apache.hadoop.hive.common.LogUtils.LogInitializationException;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.api.FieldSchema;\n-import org.apache.hadoop.hive.metastore.api.Schema;\n import org.apache.hadoop.hive.ql.CommandNeedRetryException;\n import org.apache.hadoop.hive.ql.Driver;\n import org.apache.hadoop.hive.ql.exec.FunctionRegistry;\n@@ -83,7 +82,7 @@\n   public static final String HIVERCFILE = \".hiverc\";\n \n   private final LogHelper console;\n-  private final Configuration conf;\n+  private Configuration conf;\n \n   public CliDriver() {\n     SessionState ss = SessionState.get();\n@@ -94,10 +93,8 @@ public CliDriver() {\n \n   public int processCmd(String cmd) {\n     CliSessionState ss = (CliSessionState) SessionState.get();\n-\n     String cmd_trimmed = cmd.trim();\n-    String[] tokens = cmd_trimmed.split(\"\\\\s+\");\n-    String cmd_1 = cmd_trimmed.substring(tokens[0].length()).trim();\n+    String[] tokens = tokenizeCmd(cmd_trimmed);\n     int ret = 0;\n \n     if (cmd_trimmed.toLowerCase().equals(\"quit\") || cmd_trimmed.toLowerCase().equals(\"exit\")) {\n@@ -109,6 +106,8 @@ public int processCmd(String cmd) {\n       System.exit(0);\n \n     } else if (tokens[0].equalsIgnoreCase(\"source\")) {\n+      String cmd_1 = getFirstCmd(cmd_trimmed, tokens[0].length());\n+\n       File sourceFile = new File(cmd_1);\n       if (! sourceFile.isFile()){\n         console.printError(\"File: \"+ cmd_1 + \" is not a file.\");\n@@ -207,91 +206,132 @@ public int processCmd(String cmd) {\n         }\n       }\n     } else { // local mode\n-      CommandProcessor proc = CommandProcessorFactory.get(tokens[0], (HiveConf)conf);\n-      int tryCount = 0;\n-      boolean needRetry;\n+      CommandProcessor proc = CommandProcessorFactory.get(tokens[0], (HiveConf) conf);\n+      ret = processLocalCmd(cmd, proc, ss);\n+    }\n \n-      do {\n-        try {\n-          needRetry = false;\n-          if (proc != null) {\n-            if (proc instanceof Driver) {\n-              Driver qp = (Driver) proc;\n-              PrintStream out = ss.out;\n-              long start = System.currentTimeMillis();\n-              if (ss.getIsVerbose()) {\n-                out.println(cmd);\n-              }\n+    return ret;\n+  }\n \n-              qp.setTryCount(tryCount);\n-              ret = qp.run(cmd).getResponseCode();\n-              if (ret != 0) {\n-                qp.close();\n-                return ret;\n-              }\n+  /**\n+   * For testing purposes to inject Configuration dependency\n+   * @param conf to replace default\n+   */\n+  void setConf(Configuration conf) {\n+    this.conf = conf;\n+  }\n \n-              ArrayList<String> res = new ArrayList<String>();\n-\n-              if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_CLI_PRINT_HEADER)) {\n-                // Print the column names\n-                boolean first_col = true;\n-                Schema sc = qp.getSchema();\n-                for (FieldSchema fs : sc.getFieldSchemas()) {\n-                  if (!first_col) {\n-                    out.print('\\t');\n-                  }\n-                  out.print(fs.getName());\n-                  first_col = false;\n-                }\n-                out.println();\n-              }\n+  /**\n+   * Extract and clean up the first command in the input.\n+   */\n+  private String getFirstCmd(String cmd, int length) {\n+    return cmd.substring(length).trim();\n+  }\n+\n+  private String[] tokenizeCmd(String cmd) {\n+    return cmd.split(\"\\\\s+\");\n+  }\n+\n+  int processLocalCmd(String cmd, CommandProcessor proc, CliSessionState ss) {\n+    int tryCount = 0;\n+    boolean needRetry;\n+    int ret = 0;\n+\n+    do {\n+      try {\n+        needRetry = false;\n+        if (proc != null) {\n+          if (proc instanceof Driver) {\n+            Driver qp = (Driver) proc;\n+            PrintStream out = ss.out;\n+            long start = System.currentTimeMillis();\n+            if (ss.getIsVerbose()) {\n+              out.println(cmd);\n+            }\n \n-              try {\n-                while (qp.getResults(res)) {\n-                  for (String r : res) {\n-                    out.println(r);\n-                  }\n-                  res.clear();\n-                  if (out.checkError()) {\n-                    break;\n-                  }\n+            qp.setTryCount(tryCount);\n+            ret = qp.run(cmd).getResponseCode();\n+            if (ret != 0) {\n+              qp.close();\n+              return ret;\n+            }\n+\n+            ArrayList<String> res = new ArrayList<String>();\n+\n+            printHeader(qp, out);\n+\n+            try {\n+              while (qp.getResults(res)) {\n+                for (String r : res) {\n+                  out.println(r);\n+                }\n+                res.clear();\n+                if (out.checkError()) {\n+                  break;\n                 }\n-              } catch (IOException e) {\n-                console.printError(\"Failed with exception \" + e.getClass().getName() + \":\"\n-                    + e.getMessage(), \"\\n\"\n-                    + org.apache.hadoop.util.StringUtils.stringifyException(e));\n-                ret = 1;\n               }\n+            } catch (IOException e) {\n+              console.printError(\"Failed with exception \" + e.getClass().getName() + \":\"\n+                  + e.getMessage(), \"\\n\"\n+                  + org.apache.hadoop.util.StringUtils.stringifyException(e));\n+              ret = 1;\n+            }\n \n-              int cret = qp.close();\n-              if (ret == 0) {\n-                ret = cret;\n-              }\n+            int cret = qp.close();\n+            if (ret == 0) {\n+              ret = cret;\n+            }\n \n-              long end = System.currentTimeMillis();\n-              if (end > start) {\n-                double timeTaken = (end - start) / 1000.0;\n-                console.printInfo(\"Time taken: \" + timeTaken + \" seconds\", null);\n-              }\n+            long end = System.currentTimeMillis();\n+            if (end > start) {\n+              double timeTaken = (end - start) / 1000.0;\n+              console.printInfo(\"Time taken: \" + timeTaken + \" seconds\", null);\n+            }\n \n-            } else {\n-              if (ss.getIsVerbose()) {\n-                ss.out.println(tokens[0] + \" \" + cmd_1);\n-              }\n-              ret = proc.run(cmd_1).getResponseCode();\n+          } else {\n+            String firstToken = tokenizeCmd(cmd.trim())[0];\n+            String cmd_1 = getFirstCmd(cmd.trim(), firstToken.length());\n+\n+            if (ss.getIsVerbose()) {\n+              ss.out.println(firstToken + \" \" + cmd_1);\n             }\n+            ret = proc.run(cmd_1).getResponseCode();\n           }\n-        } catch (CommandNeedRetryException e) {\n-          console.printInfo(\"Retry query with a different approach...\");\n-          tryCount++;\n-          needRetry = true;\n         }\n-      } while (needRetry);\n-    }\n+      } catch (CommandNeedRetryException e) {\n+        console.printInfo(\"Retry query with a different approach...\");\n+        tryCount++;\n+        needRetry = true;\n+      }\n+    } while (needRetry);\n \n     return ret;\n   }\n \n+  /**\n+   * If enabled and applicable to this command, print the field headers\n+   * for the output.\n+   *\n+   * @param qp Driver that executed the command\n+   * @param out Printstream which to send output to\n+   */\n+  private void printHeader(Driver qp, PrintStream out) {\n+    List<FieldSchema> fieldSchemas = qp.getSchema().getFieldSchemas();\n+    if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_CLI_PRINT_HEADER)\n+          && fieldSchemas != null) {\n+      // Print the column names\n+      boolean first_col = true;\n+      for (FieldSchema fs : fieldSchemas) {\n+        if (!first_col) {\n+          out.print('\\t');\n+        }\n+        out.print(fs.getName());\n+        first_col = false;\n+      }\n+      out.println();\n+    }\n+  }\n+\n   public int processLine(String line) {\n     return processLine(line, false);\n   }",
                "raw_url": "https://github.com/apache/hive/raw/53cf27b412535b5d7c95c760971280c6a68e63ff/cli/src/java/org/apache/hadoop/hive/cli/CliDriver.java",
                "sha": "a689a2b7f92013b3910849014089b2d7be74f9ab",
                "status": "modified"
            },
            {
                "additions": 102,
                "blob_url": "https://github.com/apache/hive/blob/53cf27b412535b5d7c95c760971280c6a68e63ff/cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java",
                "changes": 102,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java?ref=53cf27b412535b5d7c95c760971280c6a68e63ff",
                "deletions": 0,
                "filename": "cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java",
                "patch": "@@ -0,0 +1,102 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.cli;\n+\n+import junit.framework.TestCase;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.metastore.api.FieldSchema;\n+import org.apache.hadoop.hive.metastore.api.Schema;\n+import org.apache.hadoop.hive.ql.CommandNeedRetryException;\n+import org.apache.hadoop.hive.ql.Driver;\n+import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;\n+\n+import java.io.PrintStream;\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import static org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import static org.mockito.Matchers.anyBoolean;\n+import static org.mockito.Matchers.anyString;\n+import static org.mockito.Matchers.eq;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.never;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n+// Cannot call class TestCliDriver since that's the name of the generated\n+// code for the script-based testing\n+public class TestCliDriverMethods extends TestCase {\n+\n+  // If the command has an associated schema, make sure it gets printed to use\n+  public void testThatCliDriverPrintsHeaderForCommandsWithSchema() throws CommandNeedRetryException {\n+    Schema mockSchema = mock(Schema.class);\n+    List<FieldSchema> fieldSchemas = new ArrayList<FieldSchema>();\n+    String fieldName = \"FlightOfTheConchords\";\n+    fieldSchemas.add(new FieldSchema(fieldName, \"type\", \"comment\"));\n+\n+    when(mockSchema.getFieldSchemas()).thenReturn(fieldSchemas);\n+\n+    PrintStream mockOut = headerPrintingTestDriver(mockSchema);\n+    // Should have printed out the header for the field schema\n+    verify(mockOut, times(1)).print(fieldName);\n+  }\n+\n+  // If the command has no schema, make sure nothing is printed\n+  public void testThatCliDriverPrintsNoHeaderForCommandsWithNoSchema() throws CommandNeedRetryException {\n+    Schema mockSchema = mock(Schema.class);\n+    when(mockSchema.getFieldSchemas()).thenReturn(null);\n+\n+    PrintStream mockOut = headerPrintingTestDriver(mockSchema);\n+    // Should not have tried to print any thing.\n+    verify(mockOut, never()).print(anyString());\n+  }\n+\n+  /**\n+   * Do the actual testing against a mocked CliDriver based on what type of schema\n+   * @param mockSchema Schema to throw against test\n+   * @return Output that would have been sent to the user\n+   * @throws CommandNeedRetryException won't actually be thrown\n+   */\n+  private PrintStream headerPrintingTestDriver(Schema mockSchema) throws CommandNeedRetryException {\n+    CliDriver cliDriver = new CliDriver();\n+\n+    // We want the driver to try to print the header...\n+    Configuration conf = mock(Configuration.class);\n+    when(conf.getBoolean(eq(ConfVars.HIVE_CLI_PRINT_HEADER.varname), anyBoolean())).thenReturn(true);\n+    cliDriver.setConf(conf);\n+\n+    Driver proc = mock(Driver.class);\n+\n+    CommandProcessorResponse cpr = mock(CommandProcessorResponse.class);\n+    when(cpr.getResponseCode()).thenReturn(0);\n+    when(proc.run(anyString())).thenReturn(cpr);\n+\n+    // and then see what happens based on the provided schema\n+    when(proc.getSchema()).thenReturn(mockSchema);\n+\n+    CliSessionState mockSS = mock(CliSessionState.class);\n+    PrintStream mockOut = mock(PrintStream.class);\n+\n+    mockSS.out = mockOut;\n+\n+    cliDriver.processLocalCmd(\"use default;\", proc, mockSS);\n+    return mockOut;\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/hive/raw/53cf27b412535b5d7c95c760971280c6a68e63ff/cli/src/test/org/apache/hadoop/hive/cli/TestCliDriverMethods.java",
                "sha": "22a0891607f4af0ff9f0438bfa396036938915a9",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/53cf27b412535b5d7c95c760971280c6a68e63ff/ql/src/test/queries/clientpositive/print_header.q",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/print_header.q?ref=53cf27b412535b5d7c95c760971280c6a68e63ff",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/print_header.q",
                "patch": "@@ -11,3 +11,4 @@ SELECT src1.key as k1, src1.value as v1,\n \n SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key LIMIT 10;\n \n+use default;",
                "raw_url": "https://github.com/apache/hive/raw/53cf27b412535b5d7c95c760971280c6a68e63ff/ql/src/test/queries/clientpositive/print_header.q",
                "sha": "3ca0340e2d2675d772a108c7faec7be1c4684f14",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/53cf27b412535b5d7c95c760971280c6a68e63ff/ql/src/test/results/clientpositive/print_header.q.out",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/print_header.q.out?ref=53cf27b412535b5d7c95c760971280c6a68e63ff",
                "deletions": 4,
                "filename": "ql/src/test/results/clientpositive/print_header.q.out",
                "patch": "@@ -7,7 +7,7 @@ PREHOOK: query: SELECT src1.key as k1, src1.value as v1,\n   LIMIT 10\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/tmp/pbutler/hive_2010-11-22_13-41-11_283_258399688654159660/-mr-10000\n+PREHOOK: Output: file:/var/folders/Uq/UqBVUjdIE+qhrtZHnLNzc++++TM/-Tmp-/jhoman/hive_2011-08-08_12-36-14_972_2863485419468463022/-mr-10000\n POSTHOOK: query: SELECT src1.key as k1, src1.value as v1, \n        src2.key as k2, src2.value as v2 FROM \n   (SELECT * FROM src WHERE src.key < 10) src1 \n@@ -17,7 +17,7 @@ POSTHOOK: query: SELECT src1.key as k1, src1.value as v1,\n   LIMIT 10\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/tmp/pbutler/hive_2010-11-22_13-41-11_283_258399688654159660/-mr-10000\n+POSTHOOK: Output: file:/var/folders/Uq/UqBVUjdIE+qhrtZHnLNzc++++TM/-Tmp-/jhoman/hive_2011-08-08_12-36-14_972_2863485419468463022/-mr-10000\n k1\tv1\tk2\tv2\n 0\tval_0\t0\tval_0\n 0\tval_0\t0\tval_0\n@@ -32,11 +32,11 @@ k1\tv1\tk2\tv2\n PREHOOK: query: SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key LIMIT 10\n PREHOOK: type: QUERY\n PREHOOK: Input: default@src\n-PREHOOK: Output: file:/tmp/pbutler/hive_2010-11-22_13-41-30_510_2595029549749893604/-mr-10000\n+PREHOOK: Output: file:/var/folders/Uq/UqBVUjdIE+qhrtZHnLNzc++++TM/-Tmp-/jhoman/hive_2011-08-08_12-36-28_635_5275683580628567895/-mr-10000\n POSTHOOK: query: SELECT src.key, sum(substr(src.value,5)) FROM src GROUP BY src.key LIMIT 10\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@src\n-POSTHOOK: Output: file:/tmp/pbutler/hive_2010-11-22_13-41-30_510_2595029549749893604/-mr-10000\n+POSTHOOK: Output: file:/var/folders/Uq/UqBVUjdIE+qhrtZHnLNzc++++TM/-Tmp-/jhoman/hive_2011-08-08_12-36-28_635_5275683580628567895/-mr-10000\n key\t_c1\n 0\t0.0\n 10\t10.0\n@@ -48,3 +48,7 @@ key\t_c1\n 111\t111.0\n 113\t226.0\n 114\t114.0\n+PREHOOK: query: use default\n+PREHOOK: type: SWITCHDATABASE\n+POSTHOOK: query: use default\n+POSTHOOK: type: SWITCHDATABASE",
                "raw_url": "https://github.com/apache/hive/raw/53cf27b412535b5d7c95c760971280c6a68e63ff/ql/src/test/results/clientpositive/print_header.q.out",
                "sha": "adcfb8f45b1285791c296463b8151f72ad84e98b",
                "status": "modified"
            }
        ],
        "message": "HIVE-2334. DESCRIBE TABLE causes NPE when hive.cli.print.header=true (Jakob Homan via cws)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1159742 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/e31c71efec509a367816995f6f99559ec2a9919e",
        "patched_files": [
            "CliDriver.java",
            "build.java",
            "ivy.java",
            "print_header.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestCliDriverMethods.java"
        ]
    },
    "hive_5404635": {
        "bug_id": "hive_5404635",
        "commit": "https://github.com/apache/hive/commit/54046353e022d7aa5f3656de7d2a66f15608466d",
        "file": [
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hive/blob/54046353e022d7aa5f3656de7d2a66f15608466d/ql/src/test/queries/clientpositive/stats_analyze_empty.q",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/stats_analyze_empty.q?ref=54046353e022d7aa5f3656de7d2a66f15608466d",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/stats_analyze_empty.q",
                "patch": "@@ -0,0 +1,18 @@\n+set hive.stats.autogather=true;\n+set hive.explain.user=true;\n+\n+drop table if exists testdeci2;\n+\n+create table testdeci2(\n+id int,\n+amount decimal(10,3),\n+sales_tax decimal(10,3),\n+item string)\n+stored as orc location '/tmp/testdeci2'\n+TBLPROPERTIES (\"transactional\"=\"false\")\n+;\n+\n+\n+analyze table testdeci2 compute statistics for columns;\n+\n+insert into table testdeci2 values(1,12.123,12345.123,'desk1'),(2,123.123,1234.123,'desk2');",
                "raw_url": "https://github.com/apache/hive/raw/54046353e022d7aa5f3656de7d2a66f15608466d/ql/src/test/queries/clientpositive/stats_analyze_empty.q",
                "sha": "6ea6125964fbf8fc7c16c8b80dbc38ecc12e72b6",
                "status": "added"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/hive/blob/54046353e022d7aa5f3656de7d2a66f15608466d/ql/src/test/results/clientpositive/stats_analyze_empty.q.out",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/stats_analyze_empty.q.out?ref=54046353e022d7aa5f3656de7d2a66f15608466d",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/stats_analyze_empty.q.out",
                "patch": "@@ -0,0 +1,48 @@\n+PREHOOK: query: drop table if exists testdeci2\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: drop table if exists testdeci2\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: create table testdeci2(\n+id int,\n+amount decimal(10,3),\n+sales_tax decimal(10,3),\n+item string)\n+#### A masked pattern was here ####\n+TBLPROPERTIES (\"transactional\"=\"false\")\n+PREHOOK: type: CREATETABLE\n+#### A masked pattern was here ####\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@testdeci2\n+POSTHOOK: query: create table testdeci2(\n+id int,\n+amount decimal(10,3),\n+sales_tax decimal(10,3),\n+item string)\n+#### A masked pattern was here ####\n+TBLPROPERTIES (\"transactional\"=\"false\")\n+POSTHOOK: type: CREATETABLE\n+#### A masked pattern was here ####\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@testdeci2\n+PREHOOK: query: analyze table testdeci2 compute statistics for columns\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@testdeci2\n+PREHOOK: Output: default@testdeci2\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table testdeci2 compute statistics for columns\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@testdeci2\n+POSTHOOK: Output: default@testdeci2\n+#### A masked pattern was here ####\n+PREHOOK: query: insert into table testdeci2 values(1,12.123,12345.123,'desk1'),(2,123.123,1234.123,'desk2')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: _dummy_database@_dummy_table\n+PREHOOK: Output: default@testdeci2\n+POSTHOOK: query: insert into table testdeci2 values(1,12.123,12345.123,'desk1'),(2,123.123,1234.123,'desk2')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: _dummy_database@_dummy_table\n+POSTHOOK: Output: default@testdeci2\n+POSTHOOK: Lineage: testdeci2.amount SCRIPT []\n+POSTHOOK: Lineage: testdeci2.id SCRIPT []\n+POSTHOOK: Lineage: testdeci2.item SCRIPT []\n+POSTHOOK: Lineage: testdeci2.sales_tax SCRIPT []",
                "raw_url": "https://github.com/apache/hive/raw/54046353e022d7aa5f3656de7d2a66f15608466d/ql/src/test/results/clientpositive/stats_analyze_empty.q.out",
                "sha": "6eb51e950dbcb24c6a4f9ce08837eb2ebd608cdc",
                "status": "added"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hive/blob/54046353e022d7aa5f3656de7d2a66f15608466d/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMerger.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMerger.java?ref=54046353e022d7aa5f3656de7d2a66f15608466d",
                "deletions": 6,
                "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMerger.java",
                "patch": "@@ -31,15 +31,15 @@ public void merge(ColumnStatisticsObj aggregateColStats, ColumnStatisticsObj new\n         (DecimalColumnStatsDataInspector) aggregateColStats.getStatsData().getDecimalStats();\n     DecimalColumnStatsDataInspector newData =\n         (DecimalColumnStatsDataInspector) newColStats.getStatsData().getDecimalStats();\n-    Decimal lowValue = aggregateData.getLowValue() != null\n-        && (aggregateData.getLowValue().compareTo(newData.getLowValue()) > 0) ? aggregateData\n-        .getLowValue() : newData.getLowValue();\n+\n+    Decimal lowValue = getMin(aggregateData.getLowValue(), newData.getLowValue());\n     aggregateData.setLowValue(lowValue);\n-    Decimal highValue = aggregateData.getHighValue() != null\n-        && (aggregateData.getHighValue().compareTo(newData.getHighValue()) > 0) ? aggregateData\n-        .getHighValue() : newData.getHighValue();\n+\n+    Decimal highValue = getMax(aggregateData.getHighValue(), newData.getHighValue());\n     aggregateData.setHighValue(highValue);\n+\n     aggregateData.setNumNulls(aggregateData.getNumNulls() + newData.getNumNulls());\n+\n     if (aggregateData.getNdvEstimator() == null || newData.getNdvEstimator() == null) {\n       aggregateData.setNumDVs(Math.max(aggregateData.getNumDVs(), newData.getNumDVs()));\n     } else {\n@@ -58,4 +58,28 @@ public void merge(ColumnStatisticsObj aggregateColStats, ColumnStatisticsObj new\n       aggregateData.setNumDVs(ndv);\n     }\n   }\n+\n+  Decimal getMax(Decimal firstValue, Decimal secondValue) {\n+    if (firstValue == null && secondValue == null) {\n+      return null;\n+    }\n+\n+    if (firstValue != null && secondValue != null) {\n+      return firstValue.compareTo(secondValue) > 0 ? firstValue : secondValue;\n+    }\n+\n+    return firstValue == null ? secondValue : firstValue;\n+  }\n+\n+  Decimal getMin(Decimal firstValue, Decimal secondValue) {\n+    if (firstValue == null && secondValue == null) {\n+      return null;\n+    }\n+\n+    if (firstValue != null && secondValue != null) {\n+      return firstValue.compareTo(secondValue) > 0 ? secondValue : firstValue;\n+    }\n+\n+    return firstValue == null ? secondValue : firstValue;\n+  }\n }",
                "raw_url": "https://github.com/apache/hive/raw/54046353e022d7aa5f3656de7d2a66f15608466d/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMerger.java",
                "sha": "517ca7259b7542c6d830b13b2a391c773862879d",
                "status": "modified"
            },
            {
                "additions": 242,
                "blob_url": "https://github.com/apache/hive/blob/54046353e022d7aa5f3656de7d2a66f15608466d/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMergerTest.java",
                "changes": 242,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMergerTest.java?ref=54046353e022d7aa5f3656de7d2a66f15608466d",
                "deletions": 0,
                "filename": "standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMergerTest.java",
                "patch": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+\n+package org.apache.hadoop.hive.metastore.columnstats.merge;\n+\n+import java.nio.ByteBuffer;\n+\n+import org.apache.hadoop.hive.metastore.annotation.MetastoreUnitTest;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsData;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n+import org.apache.hadoop.hive.metastore.api.Decimal;\n+import org.apache.hadoop.hive.metastore.columnstats.cache.DecimalColumnStatsDataInspector;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+@Category(MetastoreUnitTest.class)\n+public class DecimalColumnStatsMergerTest {\n+\n+  private static final Decimal DECIMAL_3 = getDecimal(3, 0);\n+  private static final Decimal DECIMAL_5 = getDecimal(5, 0);\n+  private static final Decimal DECIMAL_20 = getDecimal(2, 1);\n+\n+  private DecimalColumnStatsMerger merger = new DecimalColumnStatsMerger();\n+\n+  @Test\n+  public void testMergeNullMinMaxValues() {\n+    ColumnStatisticsObj objNulls = new ColumnStatisticsObj();\n+    createData(objNulls, null, null);\n+\n+    merger.merge(objNulls, objNulls);\n+\n+    Assert.assertNull(objNulls.getStatsData().getDecimalStats().getLowValue());\n+    Assert.assertNull(objNulls.getStatsData().getDecimalStats().getHighValue());\n+  }\n+\n+  @Test\n+  public void testMergeNonNullAndNullLowerValuesOldIsNull() {\n+    ColumnStatisticsObj oldObj = new ColumnStatisticsObj();\n+    createData(oldObj, null, null);\n+\n+    ColumnStatisticsObj newObj = new ColumnStatisticsObj();\n+    createData(newObj, DECIMAL_3, null);\n+\n+    merger.merge(oldObj, newObj);\n+\n+    Assert.assertEquals(DECIMAL_3, oldObj.getStatsData().getDecimalStats().getLowValue());\n+  }\n+\n+  @Test\n+  public void testMergeNonNullAndNullLowerValuesNewIsNull() {\n+    ColumnStatisticsObj oldObj = new ColumnStatisticsObj();\n+    createData(oldObj, DECIMAL_3, null);\n+\n+    ColumnStatisticsObj newObj = new ColumnStatisticsObj();\n+    createData(newObj, null, null);\n+\n+    merger.merge(oldObj, newObj);\n+\n+    Assert.assertEquals(DECIMAL_3, oldObj.getStatsData().getDecimalStats().getLowValue());\n+  }\n+\n+  @Test\n+  public void testMergeNonNullAndNullHigherValuesOldIsNull() {\n+    ColumnStatisticsObj oldObj = new ColumnStatisticsObj();\n+    createData(oldObj, null, null);\n+\n+    ColumnStatisticsObj newObj = new ColumnStatisticsObj();\n+    createData(newObj, null, DECIMAL_3);\n+\n+    merger.merge(oldObj, newObj);\n+\n+    Assert.assertEquals(DECIMAL_3, oldObj.getStatsData().getDecimalStats().getHighValue());\n+  }\n+\n+  @Test\n+  public void testMergeNonNullAndNullHigherValuesNewIsNull() {\n+    ColumnStatisticsObj oldObj = new ColumnStatisticsObj();\n+    createData(oldObj, null, DECIMAL_3);\n+\n+    ColumnStatisticsObj newObj = new ColumnStatisticsObj();\n+    createData(newObj, null, null);\n+\n+    merger.merge(oldObj, newObj);\n+\n+    Assert.assertEquals(DECIMAL_3, oldObj.getStatsData().getDecimalStats().getHighValue());\n+  }\n+\n+  @Test\n+  public void testMergeLowValuesFirstWins() {\n+    ColumnStatisticsObj oldObj = new ColumnStatisticsObj();\n+    createData(oldObj, DECIMAL_3, null);\n+\n+    ColumnStatisticsObj newObj = new ColumnStatisticsObj();\n+    createData(newObj, DECIMAL_5, null);\n+\n+    merger.merge(oldObj, newObj);\n+\n+    Assert.assertEquals(DECIMAL_3, oldObj.getStatsData().getDecimalStats().getLowValue());\n+  }\n+\n+  @Test\n+  public void testMergeLowValuesSecondWins() {\n+    ColumnStatisticsObj oldObj = new ColumnStatisticsObj();\n+    createData(oldObj, DECIMAL_5, null);\n+\n+    ColumnStatisticsObj newObj = new ColumnStatisticsObj();\n+    createData(newObj, DECIMAL_3, null);\n+\n+    merger.merge(oldObj, newObj);\n+\n+    Assert.assertEquals(DECIMAL_3, oldObj.getStatsData().getDecimalStats().getLowValue());\n+  }\n+\n+  @Test\n+  public void testMergeHighValuesFirstWins() {\n+    ColumnStatisticsObj oldObj = new ColumnStatisticsObj();\n+    createData(oldObj, null, DECIMAL_5);\n+\n+    ColumnStatisticsObj newObj = new ColumnStatisticsObj();\n+    createData(newObj, null, DECIMAL_3);\n+\n+    merger.merge(oldObj, newObj);\n+\n+    Assert.assertEquals(DECIMAL_5, oldObj.getStatsData().getDecimalStats().getHighValue());\n+  }\n+\n+  @Test\n+  public void testMergeHighValuesSecondWins() {\n+    ColumnStatisticsObj oldObj = new ColumnStatisticsObj();\n+    createData(oldObj, null, DECIMAL_3);\n+\n+    ColumnStatisticsObj newObj = new ColumnStatisticsObj();\n+    createData(newObj, null, DECIMAL_5);\n+\n+    merger.merge(oldObj, newObj);\n+\n+    Assert.assertEquals(DECIMAL_5, oldObj.getStatsData().getDecimalStats().getHighValue());\n+  }\n+\n+  @Test\n+  public void testDecimalCompareEqual() {\n+    Assert.assertTrue(DECIMAL_3.equals(DECIMAL_3));\n+  }\n+\n+  @Test\n+  public void testDecimalCompareDoesntEqual() {\n+    Assert.assertTrue(!DECIMAL_3.equals(DECIMAL_5));\n+  }\n+\n+  @Test\n+  public void testCompareSimple() {\n+    Assert.assertEquals(DECIMAL_5, merger.getMax(DECIMAL_3, DECIMAL_5));\n+  }\n+\n+  @Test\n+  public void testCompareSimpleFlipped() {\n+    Assert.assertEquals(DECIMAL_5, merger.getMax(DECIMAL_5, DECIMAL_3));\n+  }\n+\n+  @Test\n+  public void testCompareSimpleReversed() {\n+    Assert.assertEquals(DECIMAL_3, merger.getMin(DECIMAL_3, DECIMAL_5));\n+  }\n+\n+  @Test\n+  public void testCompareSimpleFlippedReversed() {\n+    Assert.assertEquals(DECIMAL_3, merger.getMin(DECIMAL_5, DECIMAL_3));\n+  }\n+\n+  /*\n+   * it should pass, but fails because of HIVE-19131, get back to this later!\n+   *\n+   * @Test public void testCompareUnscaledValue() { Assert.assertEquals(DECIMAL_20,\n+   * merger.compareValues(DECIMAL_3, DECIMAL_20)); }\n+   */\n+\n+  @Test\n+  public void testCompareNullsMin() {\n+    Assert.assertNull(merger.getMin(null, null));\n+  }\n+\n+  @Test\n+  public void testCompareNullsMax() {\n+    Assert.assertNull(merger.getMax(null, null));\n+  }\n+\n+  @Test\n+  public void testCompareFirstNullMin() {\n+    Assert.assertEquals(DECIMAL_3, merger.getMin(null, DECIMAL_3));\n+  }\n+\n+  @Test\n+  public void testCompareSecondNullMin() {\n+    Assert.assertEquals(DECIMAL_3, merger.getMin(DECIMAL_3, null));\n+  }\n+\n+  @Test\n+  public void testCompareFirstNullMax() {\n+    Assert.assertEquals(DECIMAL_3, merger.getMax(null, DECIMAL_3));\n+  }\n+\n+  @Test\n+  public void testCompareSecondNullMax() {\n+    Assert.assertEquals(DECIMAL_3, merger.getMax(DECIMAL_3, null));\n+  }\n+\n+  private static Decimal getDecimal(int number, int scale) {\n+    ByteBuffer bb = ByteBuffer.allocate(4);\n+    bb.asIntBuffer().put(number);\n+    return new Decimal(bb, (short) scale);\n+  }\n+\n+  private DecimalColumnStatsDataInspector createData(ColumnStatisticsObj objNulls, Decimal lowValue,\n+      Decimal highValue) {\n+    ColumnStatisticsData statisticsData = new ColumnStatisticsData();\n+    DecimalColumnStatsDataInspector data = new DecimalColumnStatsDataInspector();\n+\n+    statisticsData.setDecimalStats(data);\n+    objNulls.setStatsData(statisticsData);\n+\n+    data.setLowValue(lowValue);\n+    data.setHighValue(highValue);\n+    return data;\n+  }\n+}",
                "raw_url": "https://github.com/apache/hive/raw/54046353e022d7aa5f3656de7d2a66f15608466d/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/columnstats/merge/DecimalColumnStatsMergerTest.java",
                "sha": "3b74d1e381207cb71c2e5a36220db5201f85ac14",
                "status": "added"
            }
        ],
        "message": "HIVE-18946: Fix columnstats merge NPE (Laszlo Bodor via Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>",
        "parent": "https://github.com/apache/hive/commit/fa9e743e7afbcd6409a51955e39e4e2bb3c109d2",
        "patched_files": [
            "DecimalColumnStatsMerger.java",
            "stats_analyze_empty.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "DecimalColumnStatsMergerTest.java"
        ]
    },
    "hive_566544d": {
        "bug_id": "hive_566544d",
        "commit": "https://github.com/apache/hive/commit/566544d1b780e873132f84660878261935b500df",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/566544d1b780e873132f84660878261935b500df/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java?ref=566544d1b780e873132f84660878261935b500df",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java",
                "patch": "@@ -422,7 +422,7 @@\n   ACID_NO_SORTED_BUCKETS(10298, \"ACID insert, update, delete not supported on tables that are \" +\n       \"sorted, table {0}\", true),\n   ALTER_TABLE_TYPE_PARTIAL_PARTITION_SPEC_NO_SUPPORTED(10299,\n-      \"Alter table partition type {0} does not allow partial partition spec\"),\n+      \"Alter table partition type {0} does not allow partial partition spec\", true),\n \n   //========================== 20000 range starts here ========================//\n   SCRIPT_INIT_ERROR(20000, \"Unable to initialize custom script.\"),",
                "raw_url": "https://github.com/apache/hive/raw/566544d1b780e873132f84660878261935b500df/ql/src/java/org/apache/hadoop/hive/ql/ErrorMsg.java",
                "sha": "292c83cf967756ad3056f8a57de9590a0b00babf",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/566544d1b780e873132f84660878261935b500df/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java?ref=566544d1b780e873132f84660878261935b500df",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java",
                "patch": "@@ -1396,8 +1396,9 @@ private void addInputsOutputsAlterTable(String tableName, Map<String, String> pa\n         // Partial partition spec supplied. Make sure this is allowed.\n         if (desc == null\n             || !AlterTableDesc.doesAlterTableTypeSupportPartialPartitionSpec(desc.getOp())) {\n+          String alterTabletype = (desc != null) ? desc.getOp().name() : \"\";\n           throw new SemanticException(\n-              ErrorMsg.ALTER_TABLE_TYPE_PARTIAL_PARTITION_SPEC_NO_SUPPORTED, desc.getOp().name());\n+              ErrorMsg.ALTER_TABLE_TYPE_PARTIAL_PARTITION_SPEC_NO_SUPPORTED, alterTabletype);\n         } else if (!conf.getBoolVar(HiveConf.ConfVars.DYNAMICPARTITIONING)) {\n           throw new SemanticException(ErrorMsg.DYNAMIC_PARTITION_DISABLED);\n         }",
                "raw_url": "https://github.com/apache/hive/raw/566544d1b780e873132f84660878261935b500df/ql/src/java/org/apache/hadoop/hive/ql/parse/DDLSemanticAnalyzer.java",
                "sha": "f3c01a8b4b6ddff2af02f95c091305288e495b27",
                "status": "modified"
            }
        ],
        "message": "HIVE-8579: Guaranteed NPE in DDLSemanticAnalyzer (Jason Dere, reviewed by Vaibhav Gumashta)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1635354 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/8fb70dca8e0341b17147e9692fb234caecac6407",
        "patched_files": [
            "ErrorMsg.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestErrorMsg.java"
        ]
    },
    "hive_5708a0b": {
        "bug_id": "hive_5708a0b",
        "commit": "https://github.com/apache/hive/commit/5708a0b797bf12b4f61afaf0d343ea6bd9b237e2",
        "file": [
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hive/blob/5708a0b797bf12b4f61afaf0d343ea6bd9b237e2/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java?ref=5708a0b797bf12b4f61afaf0d343ea6bd9b237e2",
                "deletions": 13,
                "filename": "standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java",
                "patch": "@@ -812,7 +812,7 @@ private void updateTableColStats(RawStore rawStore, String catName, String dbNam\n       rawStore.openTransaction();\n       try {\n         Table table = rawStore.getTable(catName, dbName, tblName);\n-        if (!table.isSetPartitionKeys()) {\n+        if (table != null && !table.isSetPartitionKeys()) {\n           List<String> colNames = MetaStoreUtils.getColumnNamesForTable(table);\n           Deadline.startTimer(\"getTableColumnStatistics\");\n \n@@ -856,18 +856,20 @@ private void updateTablePartitionColStats(RawStore rawStore, String catName, Str\n       rawStore.openTransaction();\n       try {\n         Table table = rawStore.getTable(catName, dbName, tblName);\n-        List<String> colNames = MetaStoreUtils.getColumnNamesForTable(table);\n-        List<String> partNames = rawStore.listPartitionNames(catName, dbName, tblName, (short) -1);\n-        // Get partition column stats for this table\n-        Deadline.startTimer(\"getPartitionColumnStatistics\");\n-        List<ColumnStatistics> partitionColStats =\n-            rawStore.getPartitionColumnStatistics(catName, dbName, tblName, partNames, colNames);\n-        Deadline.stopTimer();\n-        sharedCache.refreshPartitionColStatsInCache(catName, dbName, tblName, partitionColStats);\n-        List<Partition> parts = rawStore.getPartitionsByNames(catName, dbName, tblName, partNames);\n-        // Also save partitions for consistency as they have the stats state.\n-        for (Partition part : parts) {\n-          sharedCache.alterPartitionInCache(catName, dbName, tblName, part.getValues(), part);\n+        if (table != null) {\n+          List<String> colNames = MetaStoreUtils.getColumnNamesForTable(table);\n+          List<String> partNames = rawStore.listPartitionNames(catName, dbName, tblName, (short) -1);\n+          // Get partition column stats for this table\n+          Deadline.startTimer(\"getPartitionColumnStatistics\");\n+          List<ColumnStatistics> partitionColStats =\n+                  rawStore.getPartitionColumnStatistics(catName, dbName, tblName, partNames, colNames);\n+          Deadline.stopTimer();\n+          sharedCache.refreshPartitionColStatsInCache(catName, dbName, tblName, partitionColStats);\n+          List<Partition> parts = rawStore.getPartitionsByNames(catName, dbName, tblName, partNames);\n+          // Also save partitions for consistency as they have the stats state.\n+          for (Partition part : parts) {\n+            sharedCache.alterPartitionInCache(catName, dbName, tblName, part.getValues(), part);\n+          }\n         }\n         committed = rawStore.commitTransaction();\n       } catch (MetaException | NoSuchObjectException e) {\n@@ -886,6 +888,9 @@ private static void updateTableAggregatePartitionColStats(RawStore rawStore, Str\n                                                        String tblName) {\n       try {\n         Table table = rawStore.getTable(catName, dbName, tblName);\n+        if (table == null) {\n+          return;\n+        }\n         List<String> partNames = rawStore.listPartitionNames(catName, dbName, tblName, (short) -1);\n         List<String> colNames = MetaStoreUtils.getColumnNamesForTable(table);\n         if ((partNames != null) && (partNames.size() > 0)) {",
                "raw_url": "https://github.com/apache/hive/raw/5708a0b797bf12b4f61afaf0d343ea6bd9b237e2/standalone-metastore/metastore-server/src/main/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java",
                "sha": "3564efe8ed42c1e5a18787a1012d6e1af90e4416",
                "status": "modified"
            }
        ],
        "message": "HIVE-21479: NPE during metastore cache update (Daniel Dai, reviewed by Zoltan Haindrich)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>",
        "parent": "https://github.com/apache/hive/commit/12f83719d940034dc8c6273e2772f6b30d07108e",
        "patched_files": [
            "CachedStore.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestCachedStore.java"
        ]
    },
    "hive_583a951": {
        "bug_id": "hive_583a951",
        "commit": "https://github.com/apache/hive/commit/583a9511ba8809d81595a5fa4da32ed2c2f8912e",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/itests/src/test/resources/testconfiguration.properties",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e",
                "deletions": 0,
                "filename": "itests/src/test/resources/testconfiguration.properties",
                "patch": "@@ -30,6 +30,7 @@ disabled.query.files=ql_rewrite_gbtoidx.q,\\\n   cbo_rp_subq_not_in.q,\\\n   cbo_rp_subq_exists.q,\\\n   orc_llap.q,\\\n+  min_structvalue.q,\\\n   ql_rewrite_gbtoidx_cbo_2.q,\\\n   rcfile_merge1.q,\\\n   smb_mapjoin_8.q,\\",
                "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/itests/src/test/resources/testconfiguration.properties",
                "sha": "efa690db10836b0c21723bfe51adb22f4ab53bac",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e",
                "deletions": 13,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java",
                "patch": "@@ -709,7 +709,7 @@ boolean canCBOHandleAst(ASTNode ast, QB qb, PreCboCtx cboCtx) {\n     boolean isSupportedRoot = root == HiveParser.TOK_QUERY || root == HiveParser.TOK_EXPLAIN\n         || qb.isCTAS() || qb.isMaterializedView();\n     // Queries without a source table currently are not supported by CBO\n-    boolean isSupportedType = (qb.getIsQuery() && !qb.containsQueryWithoutSourceTable())\n+    boolean isSupportedType = (qb.getIsQuery())\n         || qb.isCTAS() || qb.isMaterializedView() || cboCtx.type == PreCboCtx.Type.INSERT\n         || cboCtx.type == PreCboCtx.Type.MULTI_INSERT;\n     boolean noBadTokens = HiveCalciteUtil.validateASTForUnsupportedTokens(ast);\n@@ -4164,18 +4164,11 @@ private RelNode genLogicalPlan(QB qb, boolean outerMostQB,\n \n       if (aliasToRel.isEmpty()) {\n         // // This may happen for queries like select 1; (no source table)\n-        // We can do following which is same, as what Hive does.\n-        // With this, we will be able to generate Calcite plan.\n-        // qb.getMetaData().setSrcForAlias(DUMMY_TABLE, getDummyTable());\n-        // RelNode op = genTableLogicalPlan(DUMMY_TABLE, qb);\n-        // qb.addAlias(DUMMY_TABLE);\n-        // qb.setTabAlias(DUMMY_TABLE, DUMMY_TABLE);\n-        // aliasToRel.put(DUMMY_TABLE, op);\n-        // However, Hive trips later while trying to get Metadata for this dummy\n-        // table\n-        // So, for now lets just disable this. Anyway there is nothing much to\n-        // optimize in such cases.\n-        throw new CalciteSemanticException(\"Unsupported\", UnsupportedFeature.Others);\n+        qb.getMetaData().setSrcForAlias(DUMMY_TABLE, getDummyTable());\n+        qb.addAlias(DUMMY_TABLE);\n+        qb.setTabAlias(DUMMY_TABLE, DUMMY_TABLE);\n+        RelNode op = genTableLogicalPlan(DUMMY_TABLE, qb);\n+        aliasToRel.put(DUMMY_TABLE, op);\n \n       }\n ",
                "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java",
                "sha": "28953b9d030f33d1c26789453b97335f4e848a5b",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "patch": "@@ -2019,6 +2019,9 @@ private void getMetaData(QB qb, ReadEntity parentInput)\n       }\n \n       if (tab == null) {\n+        if(tabName.equals(DUMMY_DATABASE + \".\" + DUMMY_TABLE)) {\n+          continue;\n+        }\n         ASTNode src = qb.getParseInfo().getSrcForAlias(alias);\n         if (null != src) {\n           throw new SemanticException(ErrorMsg.INVALID_TABLE.getMsg(src));\n@@ -10611,6 +10614,9 @@ public Operator genPlan(QB qb, boolean skipAmbiguityCheck)\n \n     // Recurse over all the source tables\n     for (String alias : qb.getTabAliases()) {\n+      if(alias.equals(DUMMY_TABLE)) {\n+        continue;\n+      }\n       Operator op = genTablePlan(alias, qb);\n       aliasToOpInfo.put(alias, op);\n     }\n@@ -10738,7 +10744,7 @@ private void rewriteRRForSubQ(String alias, Operator operator, boolean skipAmbig\n     opParseCtx.get(operator).setRowResolver(newRR);\n   }\n \n-  private Table getDummyTable() throws SemanticException {\n+  protected Table getDummyTable() throws SemanticException {\n     Path dummyPath = createDummyFile();\n     Table desc = new Table(DUMMY_DATABASE, DUMMY_TABLE);\n     desc.getTTable().getSd().setLocation(dummyPath.toString());",
                "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "sha": "d56fd21c63cdce35613c02b115d3f2c4dcaca08e",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hive/blob/c5b3ccc41016afd94035637cb011eacbeb9e5893/ql/src/test/queries/clientnegative/subquery_missing_from.q",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/subquery_missing_from.q?ref=c5b3ccc41016afd94035637cb011eacbeb9e5893",
                "deletions": 1,
                "filename": "ql/src/test/queries/clientnegative/subquery_missing_from.q",
                "patch": "@@ -1 +0,0 @@\n-select * from src where src.key in (select key);\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hive/raw/c5b3ccc41016afd94035637cb011eacbeb9e5893/ql/src/test/queries/clientnegative/subquery_missing_from.q",
                "sha": "3b49ac6a0a8f00dbc26a4071042d6e05574f0767",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hive/blob/c5b3ccc41016afd94035637cb011eacbeb9e5893/ql/src/test/queries/clientnegative/subquery_select_no_source.q",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientnegative/subquery_select_no_source.q?ref=c5b3ccc41016afd94035637cb011eacbeb9e5893",
                "deletions": 2,
                "filename": "ql/src/test/queries/clientnegative/subquery_select_no_source.q",
                "patch": "@@ -1,2 +0,0 @@\n--- since CBO doesn't allow such queries we can not support subqueries here\n-explain select (select max(p_size) from part);",
                "raw_url": "https://github.com/apache/hive/raw/c5b3ccc41016afd94035637cb011eacbeb9e5893/ql/src/test/queries/clientnegative/subquery_select_no_source.q",
                "sha": "75cae51e6af638c5bca38c983bf7dc7161c30fee",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hive/blob/c5b3ccc41016afd94035637cb011eacbeb9e5893/ql/src/test/results/clientnegative/subquery_missing_from.q.out",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/subquery_missing_from.q.out?ref=c5b3ccc41016afd94035637cb011eacbeb9e5893",
                "deletions": 3,
                "filename": "ql/src/test/results/clientnegative/subquery_missing_from.q.out",
                "patch": "@@ -1,3 +0,0 @@\n-FAILED: SemanticException Line 0:-1 Invalid SubQuery expression 'key' in definition of SubQuery sq_1 [\n-src.key in (select key)\n-] used as sq_1 at Line 0:-1: From clause is missing in SubQuery.",
                "raw_url": "https://github.com/apache/hive/raw/c5b3ccc41016afd94035637cb011eacbeb9e5893/ql/src/test/results/clientnegative/subquery_missing_from.q.out",
                "sha": "b09a8e311f5e7b2ef0c35b28ade81dfb4033fed3",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hive/blob/c5b3ccc41016afd94035637cb011eacbeb9e5893/ql/src/test/results/clientnegative/subquery_select_no_source.q.out",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/subquery_select_no_source.q.out?ref=c5b3ccc41016afd94035637cb011eacbeb9e5893",
                "deletions": 1,
                "filename": "ql/src/test/results/clientnegative/subquery_select_no_source.q.out",
                "patch": "@@ -1 +0,0 @@\n-FAILED: CalciteSubquerySemanticException [Error 10249]: Unsupported SubQuery Expression  Currently SubQuery expressions are only allowed as Where and Having Clause predicates",
                "raw_url": "https://github.com/apache/hive/raw/c5b3ccc41016afd94035637cb011eacbeb9e5893/ql/src/test/results/clientnegative/subquery_select_no_source.q.out",
                "sha": "37c4e57813a61d4f6832df610265962fffc92240",
                "status": "removed"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/beeline/mapjoin2.q.out",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/beeline/mapjoin2.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/beeline/mapjoin2.q.out",
                "patch": "@@ -53,6 +53,7 @@ POSTHOOK: Input: default@tbl\n #### A masked pattern was here ####\n false\tfalse\ttrue\ttrue\n true\ttrue\tfalse\tfalse\n+Warning: Map Join MAPJOIN[9][bigTable=?] in task 'Stage-3:MAPRED' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table\n@@ -62,6 +63,7 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n #### A masked pattern was here ####\n 11\t1\t1\t0\t0\n+Warning: Map Join MAPJOIN[9][bigTable=?] in task 'Stage-3:MAPRED' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a left outer join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table\n@@ -71,6 +73,7 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n #### A masked pattern was here ####\n 11\t1\t1\t0\t0\n+Warning: Map Join MAPJOIN[9][bigTable=?] in task 'Stage-3:MAPRED' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a right outer join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table\n@@ -80,6 +83,7 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n #### A masked pattern was here ####\n 11\t1\t1\t0\t0\n+Warning: Shuffle Join JOIN[6][tables = [$hdt$_0, $hdt$_1]] in Stage 'Stage-1:MAPRED' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a full outer join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table",
                "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/beeline/mapjoin2.q.out",
                "sha": "7e7084160df06263534e64b490dcb20d810a26ec",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/beeline/select_dummy_source.q.out",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/beeline/select_dummy_source.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e",
                "deletions": 7,
                "filename": "ql/src/test/results/clientpositive/beeline/select_dummy_source.q.out",
                "patch": "@@ -89,13 +89,17 @@ STAGE PLANS:\n               UDTF Operator\n                 Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n                 function name: explode\n-                File Output Operator\n-                  compressed: false\n+                Select Operator\n+                  expressions: col (type: string)\n+                  outputColumnNames: _col0\n                   Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n-                  table:\n-                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n-                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                  File Output Operator\n+                    compressed: false\n+                    Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n+                    table:\n+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n \n   Stage: Stage-0\n     Fetch Operator\n@@ -204,7 +208,11 @@ STAGE PLANS:\n             UDTF Operator\n               Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n               function name: explode\n-              ListSink\n+              Select Operator\n+                expressions: col (type: string)\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n+                ListSink\n \n PREHOOK: query: select explode(array('a', 'b'))\n PREHOOK: type: QUERY",
                "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/beeline/select_dummy_source.q.out",
                "sha": "0b73e84e9aea6855e91fd547bfb7a973b55d6613",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/decimal_precision2.q.out",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/decimal_precision2.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e",
                "deletions": 5,
                "filename": "ql/src/test/results/clientpositive/decimal_precision2.q.out",
                "patch": "@@ -37,9 +37,9 @@ STAGE PLANS:\n           Row Limit Per Split: 1\n           Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE\n           Select Operator\n-            expressions: 100 (type: decimal(3,0))\n+            expressions: 100 (type: int)\n             outputColumnNames: _col0\n-            Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: COMPLETE\n+            Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE\n             ListSink\n \n PREHOOK: query: explain select 0.000BD\n@@ -59,9 +59,9 @@ STAGE PLANS:\n           Row Limit Per Split: 1\n           Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE\n           Select Operator\n-            expressions: 0 (type: decimal(1,0))\n+            expressions: 0 (type: int)\n             outputColumnNames: _col0\n-            Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: COMPLETE\n+            Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE\n             ListSink\n \n PREHOOK: query: explain select 0.100BD\n@@ -147,7 +147,7 @@ STAGE PLANS:\n           Row Limit Per Split: 1\n           Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE\n           Select Operator\n-            expressions: 69.0212249755859375 (type: decimal(27,20))\n+            expressions: 69.0212249755859375 (type: decimal(18,16))\n             outputColumnNames: _col0\n             Statistics: Num rows: 1 Data size: 112 Basic stats: COMPLETE Column stats: COMPLETE\n             ListSink",
                "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/decimal_precision2.q.out",
                "sha": "4ce7e1cca7d43da1a88b153e6c6cdeeff5b2adf7",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/llap/explainuser_1.q.out",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/llap/explainuser_1.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e",
                "deletions": 6,
                "filename": "ql/src/test/results/clientpositive/llap/explainuser_1.q.out",
                "patch": "@@ -5150,16 +5150,18 @@ PREHOOK: query: explain select explode(array('a', 'b'))\n PREHOOK: type: QUERY\n POSTHOOK: query: explain select explode(array('a', 'b'))\n POSTHOOK: type: QUERY\n-Plan not optimized by CBO.\n+Plan optimized by CBO.\n \n Stage-0\n   Fetch Operator\n     limit:-1\n-    UDTF Operator [UDTF_2]\n-      function name:explode\n-      Select Operator [SEL_1]\n-        Output:[\"_col0\"]\n-        TableScan [TS_0]\n+    Select Operator [SEL_3]\n+      Output:[\"_col0\"]\n+      UDTF Operator [UDTF_2]\n+        function name:explode\n+        Select Operator [SEL_1]\n+          Output:[\"_col0\"]\n+          TableScan [TS_0]\n \n PREHOOK: query: CREATE TABLE T1(key STRING, val STRING) STORED AS TEXTFILE\n PREHOOK: type: CREATETABLE",
                "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/llap/explainuser_1.q.out",
                "sha": "a47d791fad073baa52e6374252e0c734a9310b8c",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/llap/mapjoin2.q.out",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/llap/mapjoin2.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/llap/mapjoin2.q.out",
                "patch": "@@ -53,6 +53,7 @@ POSTHOOK: Input: default@tbl\n #### A masked pattern was here ####\n false\tfalse\ttrue\ttrue\n true\ttrue\tfalse\tfalse\n+Warning: Map Join MAPJOIN[9][bigTable=?] in task 'Map 1' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table\n@@ -62,6 +63,7 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n #### A masked pattern was here ####\n 11\t1\t1\t0\t0\n+Warning: Map Join MAPJOIN[9][bigTable=?] in task 'Map 1' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a left outer join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table\n@@ -71,6 +73,7 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n #### A masked pattern was here ####\n 11\t1\t1\t0\t0\n+Warning: Map Join MAPJOIN[9][bigTable=?] in task 'Map 2' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a right outer join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table\n@@ -80,6 +83,7 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n #### A masked pattern was here ####\n 11\t1\t1\t0\t0\n+Warning: Shuffle Join MERGEJOIN[9][tables = [$hdt$_0, $hdt$_1]] in Stage 'Reducer 2' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a full outer join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table",
                "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/llap/mapjoin2.q.out",
                "sha": "ce65c6ddbfb6b4cdb7c058a05c583835d0631e06",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/llap/select_dummy_source.q.out",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/llap/select_dummy_source.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e",
                "deletions": 2,
                "filename": "ql/src/test/results/clientpositive/llap/select_dummy_source.q.out",
                "patch": "@@ -82,7 +82,10 @@ STAGE PLANS:\n             outputColumnNames: _col0\n             UDTF Operator\n               function name: explode\n-              ListSink\n+              Select Operator\n+                expressions: col (type: string)\n+                outputColumnNames: _col0\n+                ListSink\n \n PREHOOK: query: select explode(array('a', 'b'))\n PREHOOK: type: QUERY\n@@ -178,7 +181,10 @@ STAGE PLANS:\n             outputColumnNames: _col0\n             UDTF Operator\n               function name: explode\n-              ListSink\n+              Select Operator\n+                expressions: col (type: string)\n+                outputColumnNames: _col0\n+                ListSink\n \n PREHOOK: query: select explode(array('a', 'b'))\n PREHOOK: type: QUERY",
                "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/llap/select_dummy_source.q.out",
                "sha": "b7f939fb8e8fd4138e447df9840583a4955ec035",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/mapjoin2.q.out",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/mapjoin2.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/mapjoin2.q.out",
                "patch": "@@ -53,6 +53,7 @@ POSTHOOK: Input: default@tbl\n #### A masked pattern was here ####\n false\tfalse\ttrue\ttrue\n true\ttrue\tfalse\tfalse\n+Warning: Map Join MAPJOIN[9][bigTable=?] in task 'Stage-3:MAPRED' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table\n@@ -62,6 +63,7 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n #### A masked pattern was here ####\n 11\t1\t1\t0\t0\n+Warning: Map Join MAPJOIN[9][bigTable=?] in task 'Stage-3:MAPRED' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a left outer join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table\n@@ -71,6 +73,7 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n #### A masked pattern was here ####\n 11\t1\t1\t0\t0\n+Warning: Map Join MAPJOIN[9][bigTable=?] in task 'Stage-3:MAPRED' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a right outer join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table\n@@ -80,6 +83,7 @@ POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n #### A masked pattern was here ####\n 11\t1\t1\t0\t0\n+Warning: Shuffle Join JOIN[6][tables = [$hdt$_0, $hdt$_1]] in Stage 'Stage-1:MAPRED' is a cross product\n PREHOOK: query: select a.key, a.a_one, b.b_one, a.a_zero, b.b_zero from ( SELECT 11 key, 0 confuse_you, 1 a_one, 0 a_zero ) a full outer join ( SELECT 11 key, 0 confuse_you, 1 b_one, 0 b_zero ) b on a.key = b.key\n PREHOOK: type: QUERY\n PREHOOK: Input: _dummy_database@_dummy_table",
                "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/mapjoin2.q.out",
                "sha": "7e7084160df06263534e64b490dcb20d810a26ec",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/select_dummy_source.q.out",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/select_dummy_source.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e",
                "deletions": 7,
                "filename": "ql/src/test/results/clientpositive/select_dummy_source.q.out",
                "patch": "@@ -89,13 +89,17 @@ STAGE PLANS:\n               UDTF Operator\n                 Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n                 function name: explode\n-                File Output Operator\n-                  compressed: false\n+                Select Operator\n+                  expressions: col (type: string)\n+                  outputColumnNames: _col0\n                   Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n-                  table:\n-                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n-                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                  File Output Operator\n+                    compressed: false\n+                    Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n+                    table:\n+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n \n   Stage: Stage-0\n     Fetch Operator\n@@ -204,7 +208,11 @@ STAGE PLANS:\n             UDTF Operator\n               Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n               function name: explode\n-              ListSink\n+              Select Operator\n+                expressions: col (type: string)\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE\n+                ListSink\n \n PREHOOK: query: select explode(array('a', 'b'))\n PREHOOK: type: QUERY",
                "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/select_dummy_source.q.out",
                "sha": "0b73e84e9aea6855e91fd547bfb7a973b55d6613",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/timestamptz_1.q.out",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/timestamptz_1.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e",
                "deletions": 1,
                "filename": "ql/src/test/results/clientpositive/timestamptz_1.q.out",
                "patch": "@@ -18,7 +18,7 @@ POSTHOOK: query: insert overwrite table tstz1 select cast('2016-01-03 12:26:34 A\n POSTHOOK: type: QUERY\n POSTHOOK: Input: _dummy_database@_dummy_table\n POSTHOOK: Output: default@tstz1\n-POSTHOOK: Lineage: tstz1.t EXPRESSION []\n+POSTHOOK: Lineage: tstz1.t SIMPLE []\n PREHOOK: query: select cast(t as string) from tstz1\n PREHOOK: type: QUERY\n PREHOOK: Input: default@tstz1",
                "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/timestamptz_1.q.out",
                "sha": "b4ef3e41ad814de385e094a8bda12242203469b6",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/udtf_stack.q.out",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/udtf_stack.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e",
                "deletions": 7,
                "filename": "ql/src/test/results/clientpositive/udtf_stack.q.out",
                "patch": "@@ -182,13 +182,17 @@ STAGE PLANS:\n               UDTF Operator\n                 Statistics: Num rows: 1 Data size: 185 Basic stats: COMPLETE Column stats: COMPLETE\n                 function name: stack\n-                File Output Operator\n-                  compressed: false\n-                  Statistics: Num rows: 1 Data size: 185 Basic stats: COMPLETE Column stats: COMPLETE\n-                  table:\n-                      input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n-                      output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n-                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                Select Operator\n+                  expressions: col0 (type: string), col1 (type: string), null (type: void)\n+                  outputColumnNames: _col0, _col1, _col2\n+                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE\n+                  File Output Operator\n+                    compressed: false\n+                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE\n+                    table:\n+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n \n   Stage: Stage-0\n     Fetch Operator",
                "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/udtf_stack.q.out",
                "sha": "3192a44e41d9063af2b17f7350795dc9d2834b19",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/vector_tablesample_rows.q.out",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/vector_tablesample_rows.q.out?ref=583a9511ba8809d81595a5fa4da32ed2c2f8912e",
                "deletions": 1,
                "filename": "ql/src/test/results/clientpositive/vector_tablesample_rows.q.out",
                "patch": "@@ -251,7 +251,7 @@ STAGE PLANS:\n             Select Operator\n               Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE\n               Group By Operator\n-                aggregations: count(1)\n+                aggregations: count()\n                 Group By Vectorization:\n                     groupByMode: HASH\n                     vectorOutput: false",
                "raw_url": "https://github.com/apache/hive/raw/583a9511ba8809d81595a5fa4da32ed2c2f8912e/ql/src/test/results/clientpositive/vector_tablesample_rows.q.out",
                "sha": "2d86d8c70834b1dbae3adf3a1d10a2188ffcd385",
                "status": "modified"
            }
        ],
        "message": "HIVE-17535 Select 1 EXCEPT Select 1 fails with NPE (Vineet Garg,reviewed by Ashutosh Chauhan)",
        "parent": "https://github.com/apache/hive/commit/c5b3ccc41016afd94035637cb011eacbeb9e5893",
        "patched_files": [
            "subquery_missing_from.java",
            "mapjoin2.java",
            "timestamptz_1.java",
            "CalcitePlanner.java",
            "vector_tablesample_rows.java",
            "decimal_precision2.java",
            "select_dummy_source.java",
            "subquery_select_no_source.java",
            "explainuser_1.java",
            "SemanticAnalyzer.java",
            "udtf_stack.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "testconfiguration.java",
            "TestSemanticAnalyzer.java"
        ]
    },
    "hive_5b2cbb5": {
        "bug_id": "hive_5b2cbb5",
        "commit": "https://github.com/apache/hive/commit/5b2cbb5489ffc672e1eb7ee40b8eaa50fd26115e",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/5b2cbb5489ffc672e1eb7ee40b8eaa50fd26115e/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFEvaluator.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFEvaluator.java?ref=5b2cbb5489ffc672e1eb7ee40b8eaa50fd26115e",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFEvaluator.java",
                "patch": "@@ -149,6 +149,7 @@ public ObjectInspector init(Mode m, ObjectInspector[] parameters) throws HiveExc\n     // This function should be overriden in every sub class\n     // And the sub class should call super.init(m, parameters) to get mode set.\n     mode = m;\n+    partitionEvaluator = null;\n     return null;\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/5b2cbb5489ffc672e1eb7ee40b8eaa50fd26115e/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDAFEvaluator.java",
                "sha": "b02ca0708b8526912129872a821ab1722d9b32c9",
                "status": "modified"
            },
            {
                "additions": 79,
                "blob_url": "https://github.com/apache/hive/blob/5b2cbb5489ffc672e1eb7ee40b8eaa50fd26115e/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDAFEvaluator.java",
                "changes": 79,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDAFEvaluator.java?ref=5b2cbb5489ffc672e1eb7ee40b8eaa50fd26115e",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDAFEvaluator.java",
                "patch": "@@ -0,0 +1,79 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.udf.generic;\n+\n+import org.apache.hadoop.hive.ql.exec.PTFPartition;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.ptf.PTFExpressionDef;\n+import org.apache.hadoop.hive.ql.plan.ptf.WindowFrameDef;\n+import org.apache.hadoop.hive.ql.udf.ptf.BasePartitionEvaluator;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.mockito.Answers;\n+import org.mockito.Mock;\n+import org.mockito.runners.MockitoJUnitRunner;\n+\n+import java.util.Collections;\n+import java.util.List;\n+\n+@RunWith(MockitoJUnitRunner.class)\n+public class TestGenericUDAFEvaluator {\n+\n+  @Mock(answer = Answers.CALLS_REAL_METHODS)\n+  private GenericUDAFEvaluator udafEvaluator;\n+\n+  @Mock\n+  private WindowFrameDef winFrame;\n+\n+  @Mock\n+  private PTFPartition partition1;\n+\n+  @Mock\n+  private ObjectInspector outputOI;\n+\n+  private List<PTFExpressionDef> parameters = Collections.emptyList();\n+\n+  @Test\n+  public void testGetPartitionWindowingEvaluatorWithoutInitCall() {\n+    BasePartitionEvaluator partition1Evaluator1 = udafEvaluator.getPartitionWindowingEvaluator(\n+        winFrame, partition1, parameters, outputOI);\n+\n+    BasePartitionEvaluator partition1Evaluator2 = udafEvaluator.getPartitionWindowingEvaluator(\n+        winFrame, partition1, parameters, outputOI);\n+\n+    Assert.assertEquals(partition1Evaluator1, partition1Evaluator2);\n+  }\n+\n+  @Test\n+  public void testGetPartitionWindowingEvaluatorWithInitCall() throws HiveException {\n+    BasePartitionEvaluator partition1Evaluator1 = udafEvaluator.getPartitionWindowingEvaluator(\n+        winFrame, partition1, parameters, outputOI);\n+\n+    udafEvaluator.init(GenericUDAFEvaluator.Mode.COMPLETE, null);\n+\n+    BasePartitionEvaluator newPartitionEvaluator = udafEvaluator.getPartitionWindowingEvaluator(\n+        winFrame, partition1, parameters, outputOI);\n+\n+    Assert.assertNotEquals(partition1Evaluator1, newPartitionEvaluator);\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/hive/raw/5b2cbb5489ffc672e1eb7ee40b8eaa50fd26115e/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDAFEvaluator.java",
                "sha": "0747fa15d0b80215155ec6ffcb5d0746a97abef9",
                "status": "added"
            }
        ],
        "message": "HIVE-18786 : NPE in Hive windowing functions (Dongwook Kwon via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>",
        "parent": "https://github.com/apache/hive/commit/1b3ac733f53598636870f4f7af09d2938fe0b09f",
        "patched_files": [
            "GenericUDAFEvaluator.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestGenericUDAFEvaluator.java"
        ]
    },
    "hive_5d94d4c": {
        "bug_id": "hive_5d94d4c",
        "commit": "https://github.com/apache/hive/commit/5d94d4c205bad635f157f1eedd782aeb88c27cf6",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/5d94d4c205bad635f157f1eedd782aeb88c27cf6/ql/src/java/org/apache/hadoop/hive/ql/io/orc/JsonFileDump.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/JsonFileDump.java?ref=5d94d4c205bad635f157f1eedd782aeb88c27cf6",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/JsonFileDump.java",
                "patch": "@@ -54,6 +54,9 @@ public static void printJsonMetaData(List<String> files,\n       Configuration conf,\n       List<Integer> rowIndexCols, boolean prettyPrint, boolean printTimeZone)\n       throws JSONException, IOException {\n+    if (files.isEmpty()) {\n+      return;\n+    }\n     JSONStringer writer = new JSONStringer();\n     boolean multiFile = files.size() > 1;\n     if (multiFile) {",
                "raw_url": "https://github.com/apache/hive/raw/5d94d4c205bad635f157f1eedd782aeb88c27cf6/ql/src/java/org/apache/hadoop/hive/ql/io/orc/JsonFileDump.java",
                "sha": "00de5451d2779af2f077c3c88d3135ba5c49c85d",
                "status": "modified"
            }
        ],
        "message": "HIVE-12673: Orcfiledump throws NPE when no files are available (Rajesh Balamohan reviewed by Prasanth Jayachandran)",
        "parent": "https://github.com/apache/hive/commit/4d67088efa9b6438ff178e0b55382f167be06925",
        "patched_files": [
            "JsonFileDump.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestJsonFileDump.java"
        ]
    },
    "hive_5db4c77": {
        "bug_id": "hive_5db4c77",
        "commit": "https://github.com/apache/hive/commit/5db4c77699ff2adeb30ef2d8ea038cfbd9035d99",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/5db4c77699ff2adeb30ef2d8ea038cfbd9035d99/ql/src/java/org/apache/hadoop/hive/ql/hooks/HiveProtoLoggingHook.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/hooks/HiveProtoLoggingHook.java?ref=5db4c77699ff2adeb30ef2d8ea038cfbd9035d99",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/hooks/HiveProtoLoggingHook.java",
                "patch": "@@ -305,10 +305,10 @@ private void writeEvent(HiveHookEventProto event) {\n       for (int retryCount = 0; retryCount <= MAX_RETRIES; ++retryCount) {\n         try {\n           if (eventPerFile) {\n-            LOG.debug(\"Event per file enabled. Closing proto event file: {}\", writer.getPath());\n             if (!maybeRolloverWriterForDay()) {\n               writer = logger.getWriter(logFileName + \"_\" + ++logFileCount);\n             }\n+            LOG.debug(\"Event per file enabled. New proto event file: {}\", writer.getPath());\n             writer.writeProto(event);\n             IOUtils.closeQuietly(writer);\n             writer = null;",
                "raw_url": "https://github.com/apache/hive/raw/5db4c77699ff2adeb30ef2d8ea038cfbd9035d99/ql/src/java/org/apache/hadoop/hive/ql/hooks/HiveProtoLoggingHook.java",
                "sha": "ec00ab6d6e9caf8f07169270ae5ba9f306e736e5",
                "status": "modified"
            }
        ],
        "message": "HIVE-21250 : NPE in HiveProtoLoggingHook for eventPerFile mode. (Harish JP, reviewd by Anishek Agarwal)",
        "parent": "https://github.com/apache/hive/commit/793f192de238874d32d4c2f5137e97ebf048cc70",
        "patched_files": [
            "HiveProtoLoggingHook.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveProtoLoggingHook.java"
        ]
    },
    "hive_63080f8": {
        "bug_id": "hive_63080f8",
        "commit": "https://github.com/apache/hive/commit/63080f82c02c2b7756be09209bf07f03280e4f9b",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hive/blob/63080f82c02c2b7756be09209bf07f03280e4f9b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/IMockUtils.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/IMockUtils.java?ref=63080f82c02c2b7756be09209bf07f03280e4f9b",
                "deletions": 0,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/IMockUtils.java",
                "patch": "@@ -25,6 +25,8 @@\n import org.apache.hadoop.hive.cli.CliSessionState;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.Driver;\n+import org.apache.hadoop.hive.ql.security.SessionStateConfigUserAuthenticator;\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.sqlstd.SQLStdHiveAuthorizerFactoryForTest;\n import org.apache.hadoop.hive.ql.session.SessionState;\n import org.mockito.Mock;\n import org.mockito.Mockito;\n@@ -107,6 +109,14 @@ protected void setupDriver() {\n         \"org.apache.hadoop.hive.metastore.hbase.HBaseStore\");\n     conf.setBoolVar(HiveConf.ConfVars.METASTORE_FASTPATH, true);\n     conf.setBoolVar(HiveConf.ConfVars.HIVE_SUPPORT_CONCURRENCY, false);\n+    // Setup so we can test SQL standard auth\n+    conf.setBoolVar(HiveConf.ConfVars.HIVE_TEST_AUTHORIZATION_SQLSTD_HS2_MODE, true);\n+    conf.setVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_MANAGER,\n+        SQLStdHiveAuthorizerFactoryForTest.class.getName());\n+    conf.setVar(HiveConf.ConfVars.HIVE_AUTHENTICATOR_MANAGER,\n+        SessionStateConfigUserAuthenticator.class.getName());\n+    conf.setBoolVar(HiveConf.ConfVars.HIVE_AUTHORIZATION_ENABLED, true);\n+    conf.setVar(HiveConf.ConfVars.USERS_IN_ADMIN_ROLE, System.getProperty(\"user.name\"));\n     HBaseReadWrite.setTestConnection(hconn);\n \n     SessionState.start(new CliSessionState(conf));",
                "raw_url": "https://github.com/apache/hive/raw/63080f82c02c2b7756be09209bf07f03280e4f9b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/IMockUtils.java",
                "sha": "c30ac34cb65d10ac9b0eb4a504fe9a385f6879b9",
                "status": "modified"
            },
            {
                "additions": 57,
                "blob_url": "https://github.com/apache/hive/blob/63080f82c02c2b7756be09209bf07f03280e4f9b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/TestHBaseMetastoreSql.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/TestHBaseMetastoreSql.java?ref=63080f82c02c2b7756be09209bf07f03280e4f9b",
                "deletions": 1,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/TestHBaseMetastoreSql.java",
                "patch": "@@ -73,7 +73,10 @@ public void insertIntoPartitionTable() throws Exception {\n   public void database() throws Exception {\n     CommandProcessorResponse rsp = driver.run(\"create database db\");\n     Assert.assertEquals(0, rsp.getResponseCode());\n-    rsp = driver.run(\"alter database db set owner user me\");\n+    rsp = driver.run(\"set role admin\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    // security doesn't let me change the properties\n+    rsp = driver.run(\"alter database db set dbproperties ('key' = 'value')\");\n     Assert.assertEquals(0, rsp.getResponseCode());\n     rsp = driver.run(\"drop database db\");\n     Assert.assertEquals(0, rsp.getResponseCode());\n@@ -124,5 +127,58 @@ public void partitionedTable() throws Exception {\n     Assert.assertEquals(0, rsp.getResponseCode());\n   }\n \n+  @Test\n+  public void role() throws Exception {\n+    CommandProcessorResponse rsp = driver.run(\"set role admin\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"create role role1\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"grant role1 to user fred with admin option\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"create role role2\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"grant role1 to role role2\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"show principals role1\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"show role grant role role1\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"show role grant user \" + System.getProperty(\"user.name\"));\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"show roles\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"revoke admin option for role1 from user fred\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"revoke role1 from user fred\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"revoke role1 from role role2\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"show current roles\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+\n+    rsp = driver.run(\"drop role role2\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"drop role role1\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+  }\n+\n+  @Test\n+  public void grant() throws Exception {\n+    CommandProcessorResponse rsp = driver.run(\"set role admin\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    rsp = driver.run(\"create role role3\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    driver.run(\"create table granttbl (c int)\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    driver.run(\"grant select on granttbl to \" + System.getProperty(\"user.name\"));\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    driver.run(\"grant select on granttbl to role3 with grant option\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    driver.run(\"revoke select on granttbl from \" + System.getProperty(\"user.name\"));\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+    driver.run(\"revoke grant option for select on granttbl from role3\");\n+    Assert.assertEquals(0, rsp.getResponseCode());\n+  }\n+\n \n }",
                "raw_url": "https://github.com/apache/hive/raw/63080f82c02c2b7756be09209bf07f03280e4f9b/itests/hive-unit/src/test/java/org/apache/hadoop/hive/metastore/hbase/TestHBaseMetastoreSql.java",
                "sha": "fe5e8e2789af748e91ff5d95832e5c23d79088b5",
                "status": "modified"
            },
            {
                "additions": 57,
                "blob_url": "https://github.com/apache/hive/blob/63080f82c02c2b7756be09209bf07f03280e4f9b/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java",
                "changes": 94,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java?ref=63080f82c02c2b7756be09209bf07f03280e4f9b",
                "deletions": 37,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java",
                "patch": "@@ -617,17 +617,22 @@ public PrincipalPrivilegeSet getUserPrivilegeSet(String userName, List<String> g\n       PrincipalPrivilegeSet pps = new PrincipalPrivilegeSet();\n       PrincipalPrivilegeSet global = getHBase().getGlobalPrivs();\n       if (global == null) return null;\n-      List<PrivilegeGrantInfo> pgi = global.getUserPrivileges().get(userName);\n-      if (pgi != null) {\n-        pps.putToUserPrivileges(userName, pgi);\n+      List<PrivilegeGrantInfo> pgi;\n+      if (global.getUserPrivileges() != null) {\n+        pgi = global.getUserPrivileges().get(userName);\n+        if (pgi != null) {\n+          pps.putToUserPrivileges(userName, pgi);\n+        }\n       }\n \n-      List<String> roles = getHBase().getUserRoles(userName);\n-      if (roles != null) {\n-        for (String role : roles) {\n-          pgi = global.getRolePrivileges().get(role);\n-          if (pgi != null) {\n-            pps.putToRolePrivileges(role, pgi);\n+      if (global.getRolePrivileges() != null) {\n+        List<String> roles = getHBase().getUserRoles(userName);\n+        if (roles != null) {\n+          for (String role : roles) {\n+            pgi = global.getRolePrivileges().get(role);\n+            if (pgi != null) {\n+              pps.putToRolePrivileges(role, pgi);\n+            }\n           }\n         }\n       }\n@@ -645,18 +650,25 @@ public PrincipalPrivilegeSet getDBPrivilegeSet(String dbName, String userName,\n     try {\n       PrincipalPrivilegeSet pps = new PrincipalPrivilegeSet();\n       Database db = getHBase().getDb(dbName);\n-      // Find the user privileges for this db\n-      List<PrivilegeGrantInfo> pgi = db.getPrivileges().getUserPrivileges().get(userName);\n-      if (pgi != null) {\n-        pps.putToUserPrivileges(userName, pgi);\n-      }\n-\n-      List<String> roles = getHBase().getUserRoles(userName);\n-      if (roles != null) {\n-        for (String role : roles) {\n-          pgi = db.getPrivileges().getRolePrivileges().get(role);\n+      if (db.getPrivileges() != null) {\n+        List<PrivilegeGrantInfo> pgi;\n+        // Find the user privileges for this db\n+        if (db.getPrivileges().getUserPrivileges() != null) {\n+          pgi = db.getPrivileges().getUserPrivileges().get(userName);\n           if (pgi != null) {\n-            pps.putToRolePrivileges(role, pgi);\n+            pps.putToUserPrivileges(userName, pgi);\n+          }\n+        }\n+\n+        if (db.getPrivileges().getRolePrivileges() != null) {\n+          List<String> roles = getHBase().getUserRoles(userName);\n+          if (roles != null) {\n+            for (String role : roles) {\n+              pgi = db.getPrivileges().getRolePrivileges().get(role);\n+              if (pgi != null) {\n+                pps.putToRolePrivileges(role, pgi);\n+              }\n+            }\n           }\n         }\n       }\n@@ -674,18 +686,24 @@ public PrincipalPrivilegeSet getTablePrivilegeSet(String dbName, String tableNam\n     try {\n       PrincipalPrivilegeSet pps = new PrincipalPrivilegeSet();\n       Table table = getHBase().getTable(dbName, tableName);\n-      // Find the user privileges for this db\n-      List<PrivilegeGrantInfo> pgi = table.getPrivileges().getUserPrivileges().get(userName);\n-      if (pgi != null) {\n-        pps.putToUserPrivileges(userName, pgi);\n-      }\n-\n-      List<String> roles = getHBase().getUserRoles(userName);\n-      if (roles != null) {\n-        for (String role : roles) {\n-          pgi = table.getPrivileges().getRolePrivileges().get(role);\n+      List<PrivilegeGrantInfo> pgi;\n+      if (table.getPrivileges() != null) {\n+        if (table.getPrivileges().getUserPrivileges() != null) {\n+          pgi = table.getPrivileges().getUserPrivileges().get(userName);\n           if (pgi != null) {\n-            pps.putToRolePrivileges(role, pgi);\n+            pps.putToUserPrivileges(userName, pgi);\n+          }\n+        }\n+\n+        if (table.getPrivileges().getRolePrivileges() != null) {\n+          List<String> roles = getHBase().getUserRoles(userName);\n+          if (roles != null) {\n+            for (String role : roles) {\n+              pgi = table.getPrivileges().getRolePrivileges().get(role);\n+              if (pgi != null) {\n+                pps.putToRolePrivileges(role, pgi);\n+              }\n+            }\n           }\n         }\n       }\n@@ -1068,12 +1086,14 @@ public Role getRole(String roleName) throws NoSuchObjectException {\n       List<RolePrincipalGrant> rpgs = new ArrayList<RolePrincipalGrant>(roles.size());\n       for (Role role : roles) {\n         HbaseMetastoreProto.RoleGrantInfoList grants = getHBase().getRolePrincipals(role.getRoleName());\n-        for (HbaseMetastoreProto.RoleGrantInfo grant : grants.getGrantInfoList()) {\n-          if (grant.getPrincipalType().equals(principalType) &&\n-              grant.getPrincipalName().equals(principalName)) {\n-            rpgs.add(new RolePrincipalGrant(role.getRoleName(), principalName, principalType,\n-                grant.getGrantOption(), (int)grant.getAddTime(), grant.getGrantor(),\n-                HBaseUtils.convertPrincipalTypes(grant.getGrantorType())));\n+        if (grants != null) {\n+          for (HbaseMetastoreProto.RoleGrantInfo grant : grants.getGrantInfoList()) {\n+            if (grant.getPrincipalType() == HBaseUtils.convertPrincipalTypes(principalType) &&\n+                grant.getPrincipalName().equals(principalName)) {\n+              rpgs.add(new RolePrincipalGrant(role.getRoleName(), principalName, principalType,\n+                  grant.getGrantOption(), (int) grant.getAddTime(), grant.getGrantor(),\n+                  HBaseUtils.convertPrincipalTypes(grant.getGrantorType())));\n+            }\n           }\n         }\n       }",
                "raw_url": "https://github.com/apache/hive/raw/63080f82c02c2b7756be09209bf07f03280e4f9b/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java",
                "sha": "159b9e729876d0da7c68cbc76866b9e2920fd2cb",
                "status": "modified"
            }
        ],
        "message": "HIVE-10018 Activating SQLStandardAuth results in NPE [hbase-metastore branch] (Alan Gates)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/hbase-metastore@1668746 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/10cfdd96b69084bf0da7b864113d073006147e77",
        "patched_files": [
            "HBaseStore.java",
            "IMockUtils.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHBaseMetastoreSql.java",
            "TestHBaseStore.java"
        ]
    },
    "hive_6433c3b": {
        "bug_id": "hive_6433c3b",
        "commit": "https://github.com/apache/hive/commit/6433c3b0b8e305c8dddc6ea2fe94cfcd5062e40d",
        "file": [
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hive/blob/6433c3b0b8e305c8dddc6ea2fe94cfcd5062e40d/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ThriftUnionObjectInspector.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ThriftUnionObjectInspector.java?ref=6433c3b0b8e305c8dddc6ea2fe94cfcd5062e40d",
                "deletions": 13,
                "filename": "serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ThriftUnionObjectInspector.java",
                "patch": "@@ -18,26 +18,27 @@\n \n package org.apache.hadoop.hive.serde2.objectinspector;\n \n-import com.google.common.primitives.UnsignedBytes;\n-import org.apache.hadoop.util.ReflectionUtils;\n-import org.apache.thrift.TFieldIdEnum;\n-import org.apache.thrift.TUnion;\n-import org.apache.thrift.meta_data.FieldMetaData;\n-\n import java.lang.reflect.Field;\n import java.lang.reflect.Type;\n import java.util.ArrayList;\n import java.util.List;\n import java.util.Map;\n \n+import org.apache.thrift.TFieldIdEnum;\n+import org.apache.thrift.TUnion;\n+import org.apache.thrift.meta_data.FieldMetaData;\n+\n+import com.google.common.primitives.UnsignedBytes;\n+\n /**\n  * Always use the ObjectInspectorFactory to create new ObjectInspector objects,\n  * instead of directly creating an instance of this class.\n  */\n public class ThriftUnionObjectInspector extends ReflectionStructObjectInspector implements UnionObjectInspector {\n \n   private static final String FIELD_METADATA_MAP = \"metaDataMap\";\n-  private  List<ObjectInspector> ois;\n+  private List<ObjectInspector> ois;\n+  private List<StandardStructObjectInspector.MyField> fields;\n \n   @Override\n   public boolean shouldIgnoreField(String name) {\n@@ -88,10 +89,14 @@ protected void init(Class<?> objectClass,\n \n     try {\n       final Map<? extends TFieldIdEnum, FieldMetaData> fieldMap = (Map<? extends TFieldIdEnum, FieldMetaData>) fieldMetaData.get(null);\n+      fields = new ArrayList<StandardStructObjectInspector.MyField>(fieldMap.size());\n       this.ois = new ArrayList<ObjectInspector>();\n       for(Map.Entry<? extends TFieldIdEnum, FieldMetaData> metadata : fieldMap.entrySet()) {\n-        final Type fieldType = ThriftObjectInspectorUtils.getFieldType(objectClass, metadata.getValue().fieldName);\n+        int fieldId = metadata.getKey().getThriftFieldId();\n+        String fieldName = metadata.getValue().fieldName;\n+        final Type fieldType = ThriftObjectInspectorUtils.getFieldType(objectClass, fieldName);\n         final ObjectInspector reflectionObjectInspector = ObjectInspectorFactory.getReflectionObjectInspector(fieldType, options);\n+        fields.add(new StandardStructObjectInspector.MyField(fieldId, fieldName, reflectionObjectInspector));\n         this.ois.add(reflectionObjectInspector);\n       }\n     } catch (IllegalAccessException e) {\n@@ -112,10 +117,5 @@ public Category getCategory() {\n   public String getTypeName() {\n     return ObjectInspectorUtils.getStandardUnionTypeName(this);\n   }\n-\n-  @Override\n-  public Object create() {\n-    return ReflectionUtils.newInstance(objectClass, null);\n-  }\n }\n ",
                "raw_url": "https://github.com/apache/hive/raw/6433c3b0b8e305c8dddc6ea2fe94cfcd5062e40d/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ThriftUnionObjectInspector.java",
                "sha": "600abbb2af19ae7e071d81bc8466ad8d1ab0204a",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/6433c3b0b8e305c8dddc6ea2fe94cfcd5062e40d/serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestThriftObjectInspectors.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestThriftObjectInspectors.java?ref=6433c3b0b8e305c8dddc6ea2fe94cfcd5062e40d",
                "deletions": 2,
                "filename": "serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestThriftObjectInspectors.java",
                "patch": "@@ -23,14 +23,15 @@\n import java.util.List;\n import java.util.Set;\n \n-import junit.framework.TestCase;\n-\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;\n import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n import org.apache.hadoop.hive.serde2.thrift.test.Complex;\n import org.apache.hadoop.hive.serde2.thrift.test.IntString;\n+import org.apache.hadoop.hive.serde2.thrift.test.PropValueUnion;\n import org.apache.hadoop.hive.serde2.thrift.test.SetIntString;\n \n+import junit.framework.TestCase;\n+\n /**\n  * TestThriftObjectInspectors.\n  *\n@@ -60,6 +61,11 @@ public void testThriftObjectInspectors() throws Throwable {\n         assertNull(soi.getStructFieldData(null, fields.get(i)));\n       }\n \n+      ObjectInspector oi = ObjectInspectorFactory\n+          .getReflectionObjectInspector(PropValueUnion.class,\n+          ObjectInspectorFactory.ObjectInspectorOptions.THRIFT);\n+      assertNotNull(oi.toString());\n+\n       // real object\n       Complex c = new Complex();\n       c.setAint(1);",
                "raw_url": "https://github.com/apache/hive/raw/6433c3b0b8e305c8dddc6ea2fe94cfcd5062e40d/serde/src/test/org/apache/hadoop/hive/serde2/objectinspector/TestThriftObjectInspectors.java",
                "sha": "85f2bd63fb86019ee76103cb6b4c3b64b836f730",
                "status": "modified"
            }
        ],
        "message": "HIVE-11580: ThriftUnionObjectInspector#toString throws NPE (Jimmy, reviewed by Chao)",
        "parent": "https://github.com/apache/hive/commit/5edbf31a755668b213c2a21fb8a4a2e902e081f6",
        "patched_files": [
            "ThriftUnionObjectInspector.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestThriftObjectInspectors.java"
        ]
    },
    "hive_6447f5c": {
        "bug_id": "hive_6447f5c",
        "commit": "https://github.com/apache/hive/commit/6447f5cd57d193c6ceb6aaf141fb12f29ac53cd4",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/6447f5cd57d193c6ceb6aaf141fb12f29ac53cd4/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFStringToMap.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFStringToMap.java?ref=6447f5cd57d193c6ceb6aaf141fb12f29ac53cd4",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFStringToMap.java",
                "patch": "@@ -83,11 +83,23 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen\n   public Object evaluate(DeferredObject[] arguments) throws HiveException {\n     ret.clear();\n     String text = (String) soi_text.convert(arguments[0].get());\n+    if (text == null) {\n+      return ret;\n+    }\n+\n     String delimiter1 = (soi_de1 == null) ?\n       default_de1 : (String) soi_de1.convert(arguments[1].get());\n     String delimiter2 = (soi_de2 == null) ?\n       default_de2 : (String) soi_de2.convert(arguments[2].get());\n \n+    if (delimiter1 == null) {\n+      delimiter1 = default_de1;\n+    }\n+\n+    if (delimiter2 == null) {\n+      delimiter2 = default_de2;\n+    }\n+\n     String[] keyValuePairs = text.split(delimiter1);\n \n     for (String keyValuePair : keyValuePairs) {",
                "raw_url": "https://github.com/apache/hive/raw/6447f5cd57d193c6ceb6aaf141fb12f29ac53cd4/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFStringToMap.java",
                "sha": "093f2a3d6130cca01d137ebe1eb3d3c8416c54d5",
                "status": "modified"
            },
            {
                "additions": 152,
                "blob_url": "https://github.com/apache/hive/blob/6447f5cd57d193c6ceb6aaf141fb12f29ac53cd4/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFStringToMap.java",
                "changes": 152,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFStringToMap.java?ref=6447f5cd57d193c6ceb6aaf141fb12f29ac53cd4",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFStringToMap.java",
                "patch": "@@ -0,0 +1,152 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.ql.udf.generic;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.LinkedHashMap;\n+import java.util.Map;\n+\n+import org.apache.hadoop.hive.ql.exec.UDFArgumentException;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredJavaObject;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredObject;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.junit.Test;\n+\n+public class TestGenericUDFStringToMap {\n+\n+  @Test\n+  public void testStringToMapWithCustomDelimiters() throws HiveException {\n+    GenericUDFStringToMap udf = new GenericUDFStringToMap();\n+    initGenericUDF(udf);\n+    Map<String, String> expResult = new LinkedHashMap<String, String>();\n+    expResult.put(\"key1\", \"value1\");\n+    expResult.put(\"key2\", \"value2\");\n+    expResult.put(\"key3\", \"value3\");\n+    runAndVerify(\"key1=value1;key2=value2;key3=value3\", \";\", \"=\", expResult, udf);\n+  }\n+\n+  @Test\n+  public void testStringToMapWithDefaultDelimiters() throws HiveException {\n+    GenericUDFStringToMap udf = new GenericUDFStringToMap();\n+    initGenericUDFWithNoDelimiters(udf);\n+    Map<String, String> expResult = new LinkedHashMap<String, String>();\n+    expResult.put(\"key1\", \"value1\");\n+    expResult.put(\"key2\", \"value2\");\n+    expResult.put(\"key3\", \"value3\");\n+    runAndVerify(\"key1:value1,key2:value2,key3:value3\", expResult, udf);\n+  }\n+\n+  @Test\n+  public void testStringToMapWithNullDelimiters() throws HiveException {\n+    GenericUDFStringToMap udf = new GenericUDFStringToMap();\n+    initGenericUDF(udf);\n+    Map<String, String> expResult = new LinkedHashMap<String, String>();\n+    expResult.put(\"key1\", \"value1\");\n+    expResult.put(\"key2\", \"value2\");\n+    expResult.put(\"key3\", \"value3\");\n+    runAndVerify(\"key1:value1,key2:value2,key3:value3\", null, null, expResult, udf);\n+  }\n+\n+  @Test\n+  public void testStringToMapWithNullText() throws HiveException {\n+    GenericUDFStringToMap udf = new GenericUDFStringToMap();\n+    initGenericUDFWithNoDelimiters(udf);\n+    Map<String, String> expResult = new LinkedHashMap<String, String>();\n+    runAndVerify(null, expResult, udf);\n+  }\n+\n+  @Test\n+  public void testStringToMapWithEmptyText() throws HiveException {\n+    GenericUDFStringToMap udf = new GenericUDFStringToMap();\n+    initGenericUDFWithNoDelimiters(udf);\n+    Map<String, String> expResult = new LinkedHashMap<String, String>();\n+    expResult.put(\"\", null);\n+    runAndVerify(\"\", expResult, udf);\n+  }\n+\n+  @Test\n+  public void testStringToMapNoKey() throws HiveException {\n+    GenericUDFStringToMap udf = new GenericUDFStringToMap();\n+    initGenericUDFWithNoDelimiters(udf);\n+    Map<String, String> expResult = new LinkedHashMap<String, String>();\n+    expResult.put(\"\", \"value\");\n+    runAndVerify(\":value\", expResult, udf);\n+  }\n+\n+  @Test\n+  public void testStringToMapNoValue() throws HiveException {\n+    GenericUDFStringToMap udf = new GenericUDFStringToMap();\n+    initGenericUDFWithNoDelimiters(udf);\n+    Map<String, String> expResult = new LinkedHashMap<String, String>();\n+    expResult.put(\"key\", \"\");\n+    runAndVerify(\"key:\", expResult, udf);\n+  }\n+\n+  @Test\n+  public void testStringToMapNotMatchingDelimiter() throws HiveException {\n+    GenericUDFStringToMap udf = new GenericUDFStringToMap();\n+    initGenericUDFWithNoDelimiters(udf);\n+    Map<String, String> expResult = new LinkedHashMap<String, String>();\n+    expResult.put(\"key=value\", null);\n+    runAndVerify(\"key=value\", expResult, udf);\n+  }\n+\n+  private void initGenericUDF(GenericUDFStringToMap udf)\n+      throws UDFArgumentException {\n+\n+    ObjectInspector valueOI0 = PrimitiveObjectInspectorFactory.javaStringObjectInspector;\n+    ObjectInspector valueOI1 = PrimitiveObjectInspectorFactory.javaStringObjectInspector;\n+    ObjectInspector valueOI2 = PrimitiveObjectInspectorFactory.javaStringObjectInspector;\n+    ObjectInspector[] arguments = { valueOI0, valueOI1, valueOI2 };\n+    udf.initialize(arguments);\n+  }\n+\n+  private void initGenericUDFWithNoDelimiters(GenericUDFStringToMap udf)\n+      throws UDFArgumentException {\n+\n+    ObjectInspector valueOI0 = PrimitiveObjectInspectorFactory.javaStringObjectInspector;\n+    ObjectInspector[] arguments = { valueOI0 };\n+    udf.initialize(arguments);\n+  }\n+\n+  private void runAndVerify(String text, String delimiter1, String delimiter2,\n+      Map<String, String> expResult, GenericUDF udf) throws HiveException {\n+\n+    DeferredObject valueObj0 = new DeferredJavaObject(text);\n+    DeferredObject valueObj1 = new DeferredJavaObject(delimiter1);\n+    DeferredObject valueObj2 = new DeferredJavaObject(delimiter2);\n+    DeferredObject[] args = { valueObj0, valueObj1, valueObj2 };\n+\n+    @SuppressWarnings(\"unchecked\")\n+    LinkedHashMap<Object, Object> output = (LinkedHashMap<Object, Object>) udf.evaluate(args);\n+    assertTrue(\"str_to_map() test\", expResult.equals(output));\n+  }\n+\n+  private void runAndVerify(String text, Map<String, String> expResult,\n+      GenericUDF udf) throws HiveException {\n+\n+    DeferredObject valueObj0 = new DeferredJavaObject(text);\n+    DeferredObject[] args = { valueObj0 };\n+    @SuppressWarnings(\"unchecked\")\n+    LinkedHashMap<Object, Object> output = (LinkedHashMap<Object, Object>) udf.evaluate(args);\n+    assertTrue(\"str_to_map() test\", expResult.equals(output));\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hive/raw/6447f5cd57d193c6ceb6aaf141fb12f29ac53cd4/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFStringToMap.java",
                "sha": "1653936284a437f553f435a480e70f0c6e761aab",
                "status": "added"
            }
        ],
        "message": "HIVE-12954: NPE with str_to_map on null strings (Marta Kuczora, reviewed by Aihua Xu)",
        "parent": "https://github.com/apache/hive/commit/89080f557ad95141e3752d6f5d43455dc0ffb2c6",
        "patched_files": [
            "GenericUDFStringToMap.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestGenericUDFStringToMap.java"
        ]
    },
    "hive_650c636": {
        "bug_id": "hive_650c636",
        "commit": "https://github.com/apache/hive/commit/650c6365cb9c03b2c48c2efc8185a5a293488d73",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/650c6365cb9c03b2c48c2efc8185a5a293488d73/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java?ref=650c6365cb9c03b2c48c2efc8185a5a293488d73",
                "deletions": 1,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java",
                "patch": "@@ -2029,7 +2029,9 @@ private void alterPartitionNoTxn(String dbname, String name, List<String> part_v\n     oldp.setValues(newp.getValues());\n     oldp.setPartitionName(newp.getPartitionName());\n     oldp.setParameters(newPart.getParameters());\n-    copyMSD(newp.getSd(), oldp.getSd());\n+    if (!TableType.VIRTUAL_VIEW.name().equals(oldp.getTable().getTableType())) {\n+      copyMSD(newp.getSd(), oldp.getSd());\n+    }\n     if (newp.getCreateTime() != oldp.getCreateTime()) {\n       oldp.setCreateTime(newp.getCreateTime());\n     }",
                "raw_url": "https://github.com/apache/hive/raw/650c6365cb9c03b2c48c2efc8185a5a293488d73/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java",
                "sha": "94a0a7f2291bf16ccee7aae3d281b2dc676749ce",
                "status": "modified"
            },
            {
                "additions": 100,
                "blob_url": "https://github.com/apache/hive/blob/650c6365cb9c03b2c48c2efc8185a5a293488d73/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java",
                "changes": 100,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java?ref=650c6365cb9c03b2c48c2efc8185a5a293488d73",
                "deletions": 0,
                "filename": "metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java",
                "patch": "@@ -586,6 +586,106 @@ public void testDropTable() throws Throwable {\n \n   }\n \n+  public void testAlterViewParititon() throws Throwable {\n+    String dbName = \"compdb\";\n+    String tblName = \"comptbl\";\n+    String viewName = \"compView\";\n+\n+    client.dropTable(dbName, tblName);\n+    silentDropDatabase(dbName);\n+    Database db = new Database();\n+    db.setName(dbName);\n+    db.setDescription(\"Alter Partition Test database\");\n+    client.createDatabase(db);\n+\n+    ArrayList<FieldSchema> cols = new ArrayList<FieldSchema>(2);\n+    cols.add(new FieldSchema(\"name\", serdeConstants.STRING_TYPE_NAME, \"\"));\n+    cols.add(new FieldSchema(\"income\", serdeConstants.INT_TYPE_NAME, \"\"));\n+\n+    Table tbl = new Table();\n+    tbl.setDbName(dbName);\n+    tbl.setTableName(tblName);\n+    StorageDescriptor sd = new StorageDescriptor();\n+    tbl.setSd(sd);\n+    sd.setCols(cols);\n+    sd.setCompressed(false);\n+    sd.setParameters(new HashMap<String, String>());\n+    sd.setSerdeInfo(new SerDeInfo());\n+    sd.getSerdeInfo().setName(tbl.getTableName());\n+    sd.getSerdeInfo().setParameters(new HashMap<String, String>());\n+    sd.getSerdeInfo().getParameters()\n+        .put(serdeConstants.SERIALIZATION_FORMAT, \"1\");\n+    sd.setSortCols(new ArrayList<Order>());\n+\n+    client.createTable(tbl);\n+\n+    if (isThriftClient) {\n+      // the createTable() above does not update the location in the 'tbl'\n+      // object when the client is a thrift client and the code below relies\n+      // on the location being present in the 'tbl' object - so get the table\n+      // from the metastore\n+      tbl = client.getTable(dbName, tblName);\n+    }\n+\n+    ArrayList<FieldSchema> viewCols = new ArrayList<FieldSchema>(1);\n+    viewCols.add(new FieldSchema(\"income\", serdeConstants.INT_TYPE_NAME, \"\"));\n+\n+    ArrayList<FieldSchema> viewPartitionCols = new ArrayList<FieldSchema>(1);\n+    viewPartitionCols.add(new FieldSchema(\"name\", serdeConstants.STRING_TYPE_NAME, \"\"));\n+\n+    Table view = new Table();\n+    view.setDbName(dbName);\n+    view.setTableName(viewName);\n+    view.setTableType(TableType.VIRTUAL_VIEW.name());\n+    view.setPartitionKeys(viewPartitionCols);\n+    view.setViewOriginalText(\"SELECT income, name FROM \" + tblName);\n+    view.setViewExpandedText(\"SELECT `\" + tblName + \"`.`income`, `\" + tblName +\n+        \"`.`name` FROM `\" + dbName + \"`.`\" + tblName + \"`\");\n+    StorageDescriptor viewSd = new StorageDescriptor();\n+    view.setSd(viewSd);\n+    viewSd.setCols(viewCols);\n+    viewSd.setCompressed(false);\n+    viewSd.setParameters(new HashMap<String, String>());\n+    viewSd.setSerdeInfo(new SerDeInfo());\n+    viewSd.getSerdeInfo().setParameters(new HashMap<String, String>());\n+\n+    client.createTable(view);\n+\n+    if (isThriftClient) {\n+      // the createTable() above does not update the location in the 'tbl'\n+      // object when the client is a thrift client and the code below relies\n+      // on the location being present in the 'tbl' object - so get the table\n+      // from the metastore\n+      view = client.getTable(dbName, viewName);\n+    }\n+\n+    List<String> vals = new ArrayList<String>(1);\n+    vals.add(\"abc\");\n+\n+    Partition part = new Partition();\n+    part.setDbName(dbName);\n+    part.setTableName(viewName);\n+    part.setValues(vals);\n+    part.setParameters(new HashMap<String, String>());\n+\n+    client.add_partition(part);\n+\n+    Partition part2 = client.getPartition(dbName, viewName, part.getValues());\n+\n+    part2.getParameters().put(\"a\", \"b\");\n+\n+    client.alter_partition(dbName, viewName, part2);\n+\n+    Partition part3 = client.getPartition(dbName, viewName, part.getValues());\n+    assertEquals(\"couldn't view alter partition\", part3.getParameters().get(\n+        \"a\"), \"b\");\n+\n+    client.dropTable(dbName, viewName);\n+\n+    client.dropTable(dbName, tblName);\n+\n+    client.dropDatabase(dbName);\n+  }\n \n   public void testAlterPartition() throws Throwable {\n ",
                "raw_url": "https://github.com/apache/hive/raw/650c6365cb9c03b2c48c2efc8185a5a293488d73/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveMetaStore.java",
                "sha": "b6bce7686e0d3b74dd43c54904fa1fea1e3b7d7e",
                "status": "modified"
            }
        ],
        "message": "HIVE-4079 Altering a view partition fails with NPE\n(Kevin Wilfong via namit)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1451173 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/0be7d2feaee56e4673ab5003fccdcb4bc04f2c21",
        "patched_files": [
            "HiveMetaStore.java",
            "ObjectStore.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveMetaStore.java"
        ]
    },
    "hive_69a793b": {
        "bug_id": "hive_69a793b",
        "commit": "https://github.com/apache/hive/commit/69a793b3475d51d61d69aa8ac5d5fee70b90bf80",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/69a793b3475d51d61d69aa8ac5d5fee70b90bf80/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java?ref=69a793b3475d51d61d69aa8ac5d5fee70b90bf80",
                "deletions": 1,
                "filename": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
                "patch": "@@ -2305,7 +2305,8 @@ public void setSparkConfigUpdated(boolean isSparkConfigUpdated) {\n     LLAP_USE_LRFU(\"hive.llap.io.use.lrfu\", false,\n         \"Whether ORC low-level cache should use LRFU cache policy instead of default (FIFO).\"),\n     LLAP_LRFU_LAMBDA(\"hive.llap.io.lrfu.lambda\", 0.01f,\n-        \"Lambda for ORC low-level cache LRFU cache policy.\"),\n+        \"Lambda for ORC low-level cache LRFU cache policy. Must be in [0, 1]. 0 makes LRFU\\n\" +\n+        \"behave like LFU, 1 makes it behave like LRU, values in between balance accordingly.\"),\n     LLAP_ORC_ENABLE_TIME_COUNTERS(\"hive.llap.io.orc.time.counters\", true,\n         \"Whether to enable time counters for LLAP IO layer (time spent in HDFS, etc.)\"),\n     LLAP_AUTO_ALLOW_UBER(\"hive.llap.auto.allow.uber\", true,",
                "raw_url": "https://github.com/apache/hive/raw/69a793b3475d51d61d69aa8ac5d5fee70b90bf80/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
                "sha": "f0650482b389a7cffc5b2aaef352de292b3d23d7",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/69a793b3475d51d61d69aa8ac5d5fee70b90bf80/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java?ref=69a793b3475d51d61d69aa8ac5d5fee70b90bf80",
                "deletions": 3,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java",
                "patch": "@@ -205,7 +205,8 @@ public long evictSomeBlocks(long memoryToReserve) {\n           listHead = listTail = null; // We have evicted the entire list.\n         } else {\n           // Splice the section that we have evicted out of the list.\n-          removeFromListUnderLock(nextCandidate.next, firstCandidate);\n+          // We have already updated the state above so no need to do that again.\n+          removeFromListUnderLockNoStateUpdate(nextCandidate.next, firstCandidate);\n         }\n       }\n     } finally {\n@@ -333,7 +334,6 @@ private void removeFromListAndUnlock(LlapCacheableBuffer buffer) {\n     try {\n       if (buffer.indexInHeap != LlapCacheableBuffer.IN_LIST) return;\n       removeFromListUnderLock(buffer);\n-      buffer.indexInHeap = LlapCacheableBuffer.NOT_IN_CACHE;\n     } finally {\n       listLock.unlock();\n     }\n@@ -350,9 +350,11 @@ private void removeFromListUnderLock(LlapCacheableBuffer buffer) {\n     } else {\n       buffer.prev.next = buffer.next;\n     }\n+    buffer.indexInHeap = LlapCacheableBuffer.NOT_IN_CACHE;\n   }\n \n-  private void removeFromListUnderLock(LlapCacheableBuffer from, LlapCacheableBuffer to) {\n+  private void removeFromListUnderLockNoStateUpdate(\n+      LlapCacheableBuffer from, LlapCacheableBuffer to) {\n     if (to == listTail) {\n       listTail = from.prev;\n     } else {",
                "raw_url": "https://github.com/apache/hive/raw/69a793b3475d51d61d69aa8ac5d5fee70b90bf80/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java",
                "sha": "76e7605acc669c8be0e190216691c0780e6ef0f2",
                "status": "modified"
            },
            {
                "additions": 50,
                "blob_url": "https://github.com/apache/hive/blob/69a793b3475d51d61d69aa8ac5d5fee70b90bf80/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java",
                "changes": 57,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java?ref=69a793b3475d51d61d69aa8ac5d5fee70b90bf80",
                "deletions": 7,
                "filename": "llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java",
                "patch": "@@ -17,17 +17,14 @@\n  */\n package org.apache.hadoop.hive.llap.cache;\n \n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertNotNull;\n-import static org.junit.Assert.assertNotSame;\n-import static org.junit.Assert.assertNull;\n-import static org.junit.Assert.assertSame;\n-import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.*;\n \n+import java.lang.reflect.Field;\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.List;\n import java.util.Random;\n+import java.util.concurrent.locks.ReentrantLock;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -41,6 +38,45 @@\n public class TestLowLevelLrfuCachePolicy {\n   private static final Log LOG = LogFactory.getLog(TestLowLevelLrfuCachePolicy.class);\n \n+  @Test\n+  public void testRegression_HIVE_12178() throws Exception {\n+    LOG.info(\"Testing wrong list status after eviction\");\n+    EvictionTracker et = new EvictionTracker();\n+    int memSize = 2, lambda = 1; // Set lambda to 1 so the heap size becomes 1 (LRU).\n+    Configuration conf = createConf(1, memSize, (double)lambda);\n+    final LowLevelLrfuCachePolicy lrfu = new LowLevelLrfuCachePolicy(conf);\n+    Field f = LowLevelLrfuCachePolicy.class.getDeclaredField(\"listLock\");\n+    f.setAccessible(true);\n+    ReentrantLock listLock = (ReentrantLock)f.get(lrfu);\n+    LowLevelCacheMemoryManager mm = new LowLevelCacheMemoryManager(conf, lrfu,\n+        LlapDaemonCacheMetrics.create(\"test\", \"1\"));\n+    lrfu.setEvictionListener(et);\n+    final LlapDataBuffer buffer1 = LowLevelCacheImpl.allocateFake();\n+    LlapDataBuffer buffer2 = LowLevelCacheImpl.allocateFake();\n+    assertTrue(cache(mm, lrfu, et, buffer1));\n+    assertTrue(cache(mm, lrfu, et, buffer2));\n+    // buffer2 is now in the heap, buffer1 is in the list. \"Use\" buffer1 again;\n+    // before we notify though, lock the list, so lock cannot remove it from the list.\n+    buffer1.incRef();\n+    assertEquals(LlapCacheableBuffer.IN_LIST, buffer1.indexInHeap);\n+    listLock.lock();\n+    try {\n+      Thread otherThread = new Thread(new Runnable() {\n+        public void run() {\n+          lrfu.notifyLock(buffer1);\n+        }\n+      });\n+      otherThread.start();\n+      otherThread.join();\n+    } finally {\n+      listLock.unlock();\n+    }\n+    // Now try to evict with locked buffer still in the list.\n+    mm.reserveMemory(1, false);\n+    assertSame(buffer2, et.evicted.get(0));\n+    unlock(lrfu, buffer1);\n+  }\n+\n   @Test\n   public void testHeapSize2() {\n     testHeapSize(2);\n@@ -100,13 +136,20 @@ public void testLfuExtreme() {\n     verifyOrder(mm, lfu, et, inserted);\n   }\n \n-  private Configuration createConf(int min, int heapSize) {\n+  private Configuration createConf(int min, int heapSize, Double lambda) {\n     Configuration conf = new Configuration();\n     conf.setInt(HiveConf.ConfVars.LLAP_ORC_CACHE_MIN_ALLOC.varname, min);\n     conf.setInt(HiveConf.ConfVars.LLAP_ORC_CACHE_MAX_SIZE.varname, heapSize);\n+    if (lambda != null) {\n+      conf.setDouble(HiveConf.ConfVars.LLAP_LRFU_LAMBDA.varname, lambda.doubleValue());\n+    }\n     return conf;\n   }\n \n+  private Configuration createConf(int min, int heapSize) {\n+    return createConf(min, heapSize, null);\n+  }\n+\n   @Test\n   public void testLruExtreme() {\n     int heapSize = 4;",
                "raw_url": "https://github.com/apache/hive/raw/69a793b3475d51d61d69aa8ac5d5fee70b90bf80/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestLowLevelLrfuCachePolicy.java",
                "sha": "bb815e3273883326fd175b9ef35cd8f5d6b94844",
                "status": "modified"
            }
        ],
        "message": "HIVE-12178 : LLAP: NPE in LRFU policy (Sergey Shelukhin, reviewed by Prasanth Jayachandran)",
        "parent": "https://github.com/apache/hive/commit/f6bd00244c1a6804e71b8294fa1588857ad736da",
        "patched_files": [
            "LowLevelLrfuCachePolicy.java",
            "HiveConf.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestLowLevelLrfuCachePolicy.java",
            "TestHiveConf.java"
        ]
    },
    "hive_6dff237": {
        "bug_id": "hive_6dff237",
        "commit": "https://github.com/apache/hive/commit/6dff2376b68835ba0972ebb89a9c16e443411517",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/6dff2376b68835ba0972ebb89a9c16e443411517/itests/src/test/resources/testconfiguration.properties",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=6dff2376b68835ba0972ebb89a9c16e443411517",
                "deletions": 0,
                "filename": "itests/src/test/resources/testconfiguration.properties",
                "patch": "@@ -204,6 +204,7 @@ minitez.query.files=bucket_map_join_tez1.q,\\\n   tez_schema_evolution.q,\\\n   tez_union.q,\\\n   tez_union_decimal.q,\\\n+  tez_union_group_by.q,\\\n   tez_smb_main.q,\\\n   tez_smb_1.q,\\\n   vectorized_dynamic_partition_pruning.q",
                "raw_url": "https://github.com/apache/hive/raw/6dff2376b68835ba0972ebb89a9c16e443411517/itests/src/test/resources/testconfiguration.properties",
                "sha": "3ea6bb53cf2b59e46fdf5639858038b625718366",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/6dff2376b68835ba0972ebb89a9c16e443411517/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java?ref=6dff2376b68835ba0972ebb89a9c16e443411517",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java",
                "patch": "@@ -46,6 +46,9 @@\n   public static <T> Set<T> findOperators(Collection<Operator<?>> starts, Class<T> clazz) {\n     Set<T> found = new HashSet<T>();\n     for (Operator<?> start : starts) {\n+      if (start == null) {\n+        continue;\n+      }\n       findOperators(start, clazz, found);\n     }\n     return found;",
                "raw_url": "https://github.com/apache/hive/raw/6dff2376b68835ba0972ebb89a9c16e443411517/ql/src/java/org/apache/hadoop/hive/ql/exec/OperatorUtils.java",
                "sha": "2bd40fa6782804804a3b02b312b9e2bfc3cd9e61",
                "status": "modified"
            },
            {
                "additions": 87,
                "blob_url": "https://github.com/apache/hive/blob/6dff2376b68835ba0972ebb89a9c16e443411517/ql/src/test/queries/clientpositive/tez_union_group_by.q",
                "changes": 87,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/tez_union_group_by.q?ref=6dff2376b68835ba0972ebb89a9c16e443411517",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/tez_union_group_by.q",
                "patch": "@@ -0,0 +1,87 @@\n+CREATE TABLE x\n+(\n+u bigint,\n+t string,\n+st string\n+)\n+PARTITIONED BY (date string)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\");\n+\n+CREATE TABLE y\n+(\n+u bigint\n+)\n+PARTITIONED BY (date string)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\");\n+\n+CREATE TABLE z\n+(\n+u bigint\n+)\n+PARTITIONED BY (date string)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\");\n+\n+CREATE TABLE v\n+(\n+t string, \n+st string,\n+id int\n+)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\");\n+\n+EXPLAIN \n+SELECT o.u, n.u\n+FROM \n+(\n+SELECT m.u, Min(date) as ft\n+FROM \n+(\n+SELECT u, date FROM x WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM y WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM z WHERE date < '2014-09-02' \n+) m\n+GROUP BY m.u\n+) n \n+LEFT OUTER JOIN\n+(\n+SELECT x.u\n+FROM x\n+JOIN v \n+ON (x.t = v.t AND x.st <=> v.st)\n+WHERE x.date >= '2014-03-04' AND x.date < '2014-09-03'\n+GROUP BY x.u\n+) o\n+ON n.u = o.u \n+WHERE n.u <> 0 AND n.ft <= '2014-09-02';\n+\n+SELECT o.u, n.u\n+FROM \n+(\n+SELECT m.u, Min(date) as ft\n+FROM \n+(\n+SELECT u, date FROM x WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM y WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM z WHERE date < '2014-09-02' \n+) m\n+GROUP BY m.u\n+) n \n+LEFT OUTER JOIN\n+(\n+SELECT x.u\n+FROM x\n+JOIN v \n+ON (x.t = v.t AND x.st <=> v.st)\n+WHERE x.date >= '2014-03-04' AND x.date < '2014-09-03'\n+GROUP BY x.u\n+) o\n+ON n.u = o.u \n+WHERE n.u <> 0 AND n.ft <= '2014-09-02';",
                "raw_url": "https://github.com/apache/hive/raw/6dff2376b68835ba0972ebb89a9c16e443411517/ql/src/test/queries/clientpositive/tez_union_group_by.q",
                "sha": "56e85833f1ce3d6a093d8be2c54d988c58aae071",
                "status": "added"
            },
            {
                "additions": 327,
                "blob_url": "https://github.com/apache/hive/blob/6dff2376b68835ba0972ebb89a9c16e443411517/ql/src/test/results/clientpositive/tez/tez_union_group_by.q.out",
                "changes": 327,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_union_group_by.q.out?ref=6dff2376b68835ba0972ebb89a9c16e443411517",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/tez/tez_union_group_by.q.out",
                "patch": "@@ -0,0 +1,327 @@\n+PREHOOK: query: CREATE TABLE x\n+(\n+u bigint,\n+t string,\n+st string\n+)\n+PARTITIONED BY (date string)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\")\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@x\n+POSTHOOK: query: CREATE TABLE x\n+(\n+u bigint,\n+t string,\n+st string\n+)\n+PARTITIONED BY (date string)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\")\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@x\n+PREHOOK: query: CREATE TABLE y\n+(\n+u bigint\n+)\n+PARTITIONED BY (date string)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\")\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@y\n+POSTHOOK: query: CREATE TABLE y\n+(\n+u bigint\n+)\n+PARTITIONED BY (date string)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\")\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@y\n+PREHOOK: query: CREATE TABLE z\n+(\n+u bigint\n+)\n+PARTITIONED BY (date string)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\")\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@z\n+POSTHOOK: query: CREATE TABLE z\n+(\n+u bigint\n+)\n+PARTITIONED BY (date string)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\")\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@z\n+PREHOOK: query: CREATE TABLE v\n+(\n+t string, \n+st string,\n+id int\n+)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\")\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@v\n+POSTHOOK: query: CREATE TABLE v\n+(\n+t string, \n+st string,\n+id int\n+)\n+STORED AS ORC \n+TBLPROPERTIES (\"orc.compress\"=\"ZLIB\")\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@v\n+PREHOOK: query: EXPLAIN \n+SELECT o.u, n.u\n+FROM \n+(\n+SELECT m.u, Min(date) as ft\n+FROM \n+(\n+SELECT u, date FROM x WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM y WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM z WHERE date < '2014-09-02' \n+) m\n+GROUP BY m.u\n+) n \n+LEFT OUTER JOIN\n+(\n+SELECT x.u\n+FROM x\n+JOIN v \n+ON (x.t = v.t AND x.st <=> v.st)\n+WHERE x.date >= '2014-03-04' AND x.date < '2014-09-03'\n+GROUP BY x.u\n+) o\n+ON n.u = o.u \n+WHERE n.u <> 0 AND n.ft <= '2014-09-02'\n+PREHOOK: type: QUERY\n+POSTHOOK: query: EXPLAIN \n+SELECT o.u, n.u\n+FROM \n+(\n+SELECT m.u, Min(date) as ft\n+FROM \n+(\n+SELECT u, date FROM x WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM y WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM z WHERE date < '2014-09-02' \n+) m\n+GROUP BY m.u\n+) n \n+LEFT OUTER JOIN\n+(\n+SELECT x.u\n+FROM x\n+JOIN v \n+ON (x.t = v.t AND x.st <=> v.st)\n+WHERE x.date >= '2014-03-04' AND x.date < '2014-09-03'\n+GROUP BY x.u\n+) o\n+ON n.u = o.u \n+WHERE n.u <> 0 AND n.ft <= '2014-09-02'\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 10 <- Union 7 (CONTAINS)\n+        Map 6 <- Union 7 (CONTAINS)\n+        Map 9 <- Union 7 (CONTAINS)\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE), Map 5 (SIMPLE_EDGE)\n+        Reducer 3 <- Reducer 2 (SIMPLE_EDGE)\n+        Reducer 4 <- Reducer 3 (SIMPLE_EDGE), Reducer 8 (SIMPLE_EDGE)\n+        Reducer 8 <- Union 7 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: v\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: t is not null (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: t (type: string), st (type: string)\n+                      sort order: ++\n+                      Map-reduce partition columns: t (type: string), st (type: string)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+        Map 10 \n+        Map 5 \n+        Map 6 \n+        Map 9 \n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                condition expressions:\n+                  0 {VALUE._col0}\n+                  1 \n+                nullSafes: [false, true]\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Select Operator\n+                  expressions: _col0 (type: bigint)\n+                  outputColumnNames: _col0\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Group By Operator\n+                    keys: _col0 (type: bigint)\n+                    mode: hash\n+                    outputColumnNames: _col0\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: _col0 (type: bigint)\n+                      sort order: +\n+                      Map-reduce partition columns: _col0 (type: bigint)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+        Reducer 3 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                keys: KEY._col0 (type: bigint)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Select Operator\n+                  expressions: _col0 (type: bigint)\n+                  outputColumnNames: _col0\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: _col0 (type: bigint)\n+                    sort order: +\n+                    Map-reduce partition columns: _col0 (type: bigint)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+        Reducer 4 \n+            Reduce Operator Tree:\n+              Join Operator\n+                condition map:\n+                     Left Outer Join0 to 1\n+                condition expressions:\n+                  0 {KEY.reducesinkkey0}\n+                  1 {KEY.reducesinkkey0}\n+                outputColumnNames: _col0, _col2\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Select Operator\n+                  expressions: _col2 (type: bigint), _col0 (type: bigint)\n+                  outputColumnNames: _col0, _col1\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  File Output Operator\n+                    compressed: false\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    table:\n+                        input format: org.apache.hadoop.mapred.TextInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Reducer 8 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: min(VALUE._col0)\n+                keys: KEY._col0 (type: bigint)\n+                mode: mergepartial\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Filter Operator\n+                  predicate: (_col1 <= '2014-09-02') (type: boolean)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Select Operator\n+                    expressions: _col0 (type: bigint)\n+                    outputColumnNames: _col0\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: _col0 (type: bigint)\n+                      sort order: +\n+                      Map-reduce partition columns: _col0 (type: bigint)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+        Union 7 \n+            Vertex: Union 7\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: SELECT o.u, n.u\n+FROM \n+(\n+SELECT m.u, Min(date) as ft\n+FROM \n+(\n+SELECT u, date FROM x WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM y WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM z WHERE date < '2014-09-02' \n+) m\n+GROUP BY m.u\n+) n \n+LEFT OUTER JOIN\n+(\n+SELECT x.u\n+FROM x\n+JOIN v \n+ON (x.t = v.t AND x.st <=> v.st)\n+WHERE x.date >= '2014-03-04' AND x.date < '2014-09-03'\n+GROUP BY x.u\n+) o\n+ON n.u = o.u \n+WHERE n.u <> 0 AND n.ft <= '2014-09-02'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@v\n+PREHOOK: Input: default@x\n+PREHOOK: Input: default@y\n+PREHOOK: Input: default@z\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT o.u, n.u\n+FROM \n+(\n+SELECT m.u, Min(date) as ft\n+FROM \n+(\n+SELECT u, date FROM x WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM y WHERE date < '2014-09-02' \n+UNION ALL\n+SELECT u, date FROM z WHERE date < '2014-09-02' \n+) m\n+GROUP BY m.u\n+) n \n+LEFT OUTER JOIN\n+(\n+SELECT x.u\n+FROM x\n+JOIN v \n+ON (x.t = v.t AND x.st <=> v.st)\n+WHERE x.date >= '2014-03-04' AND x.date < '2014-09-03'\n+GROUP BY x.u\n+) o\n+ON n.u = o.u \n+WHERE n.u <> 0 AND n.ft <= '2014-09-02'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@v\n+POSTHOOK: Input: default@x\n+POSTHOOK: Input: default@y\n+POSTHOOK: Input: default@z\n+#### A masked pattern was here ####",
                "raw_url": "https://github.com/apache/hive/raw/6dff2376b68835ba0972ebb89a9c16e443411517/ql/src/test/results/clientpositive/tez/tez_union_group_by.q.out",
                "sha": "b2a61a67d0515f4fbf782fc339d54b7564c4de6d",
                "status": "added"
            }
        ],
        "message": "HIVE-8227: NPE w/ hive on tez when doing unions on empty tables (Gunther Hagleitner, reviewed by Vikram Dixit K)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1627237 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/796fa713a7fc09f198ecba122232ba3c98264687",
        "patched_files": [
            "OperatorUtils.java",
            "tez_union_group_by.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "testconfiguration.java"
        ]
    },
    "hive_6fb647f": {
        "bug_id": "hive_6fb647f",
        "commit": "https://github.com/apache/hive/commit/6fb647f32ce4e393c4bfcd871821d4da166abaa0",
        "file": [
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hive/blob/6fb647f32ce4e393c4bfcd871821d4da166abaa0/metastore/src/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java?ref=6fb647f32ce4e393c4bfcd871821d4da166abaa0",
                "deletions": 18,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java",
                "patch": "@@ -85,6 +85,7 @@\n import org.apache.hadoop.hive.metastore.partition.spec.PartitionSpecProxy;\n import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.hadoop.util.StringUtils;\n import org.apache.hive.common.util.HiveStringUtils;\n import org.apache.thrift.TException;\n import org.slf4j.Logger;\n@@ -355,10 +356,10 @@ public void run() {\n             }\n           }\n         }\n-      } catch (MetaException e) {\n-        LOG.error(\"Updating CachedStore: error getting database names\", e);\n       } catch (InstantiationException | IllegalAccessException e) {\n         throw new RuntimeException(\"Cannot instantiate \" + rawStoreClassName, e);\n+      } catch (Exception e) {\n+        LOG.error(\"Updating CachedStore: error happen when refresh\", e);\n       } finally {\n         try {\n           if (rawStore != null) {\n@@ -460,15 +461,17 @@ private void updateTableColStats(RawStore rawStore, String dbName, String tblNam\n         ColumnStatistics tableColStats =\n             rawStore.getTableColumnStatistics(dbName, tblName, colNames);\n         Deadline.stopTimer();\n-        if (tableColStatsCacheLock.writeLock().tryLock()) {\n-          // Skip background updates if we detect change\n-          if (isTableColStatsCacheDirty.compareAndSet(true, false)) {\n-            LOG.debug(\"Skipping table column stats cache update; the table column stats list we \"\n-                + \"have is dirty.\");\n-            return;\n+        if (tableColStats != null) {\n+          if (tableColStatsCacheLock.writeLock().tryLock()) {\n+            // Skip background updates if we detect change\n+            if (isTableColStatsCacheDirty.compareAndSet(true, false)) {\n+              LOG.debug(\"Skipping table column stats cache update; the table column stats list we \"\n+                  + \"have is dirty.\");\n+              return;\n+            }\n+            SharedCache.refreshTableColStats(HiveStringUtils.normalizeIdentifier(dbName),\n+                HiveStringUtils.normalizeIdentifier(tblName), tableColStats.getStatsObj());\n           }\n-          SharedCache.refreshTableColStats(HiveStringUtils.normalizeIdentifier(dbName),\n-              HiveStringUtils.normalizeIdentifier(tblName), tableColStats.getStatsObj());\n         }\n       } catch (MetaException | NoSuchObjectException e) {\n         LOG.info(\"Updating CachedStore: unable to read table column stats of table: \" + tblName, e);\n@@ -486,15 +489,17 @@ private void updateTablePartitionColStats(RawStore rawStore, String dbName, Stri\n         Map<String, List<ColumnStatisticsObj>> colStatsPerPartition =\n             rawStore.getColStatsForTablePartitions(dbName, tblName);\n         Deadline.stopTimer();\n-        if (partitionColStatsCacheLock.writeLock().tryLock()) {\n-          // Skip background updates if we detect change\n-          if (isPartitionColStatsCacheDirty.compareAndSet(true, false)) {\n-            LOG.debug(\"Skipping partition column stats cache update; the partition column stats \"\n-                + \"list we have is dirty.\");\n-            return;\n+        if (colStatsPerPartition != null) {\n+          if (partitionColStatsCacheLock.writeLock().tryLock()) {\n+            // Skip background updates if we detect change\n+            if (isPartitionColStatsCacheDirty.compareAndSet(true, false)) {\n+              LOG.debug(\"Skipping partition column stats cache update; the partition column stats \"\n+                  + \"list we have is dirty.\");\n+              return;\n+            }\n+            SharedCache.refreshPartitionColStats(HiveStringUtils.normalizeIdentifier(dbName),\n+                HiveStringUtils.normalizeIdentifier(tblName), colStatsPerPartition);\n           }\n-          SharedCache.refreshPartitionColStats(HiveStringUtils.normalizeIdentifier(dbName),\n-              HiveStringUtils.normalizeIdentifier(tblName), colStatsPerPartition);\n         }\n       } catch (MetaException | NoSuchObjectException e) {\n         LOG.info(\"Updating CachedStore: unable to read partitions column stats of table: \"",
                "raw_url": "https://github.com/apache/hive/raw/6fb647f32ce4e393c4bfcd871821d4da166abaa0/metastore/src/java/org/apache/hadoop/hive/metastore/cache/CachedStore.java",
                "sha": "3ac4fe1604c7b0b455894b8e6293484e9226836e",
                "status": "modified"
            }
        ],
        "message": "HIVE-16848: NPE during CachedStore refresh (Daniel Dai, reviewed by Vaibhav Gumashta, Thejas Nair)",
        "parent": "https://github.com/apache/hive/commit/41f72dc3eda0e2744ea3787560ef12ec1d994038",
        "patched_files": [
            "CachedStore.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestCachedStore.java"
        ]
    },
    "hive_70cc5ef": {
        "bug_id": "hive_70cc5ef",
        "commit": "https://github.com/apache/hive/commit/70cc5eface64b5417916e42312befc022f4a06c0",
        "file": [
            {
                "additions": 62,
                "blob_url": "https://github.com/apache/hive/blob/70cc5eface64b5417916e42312befc022f4a06c0/beeline/src/java/org/apache/hive/beeline/BeeLine.java",
                "changes": 155,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/beeline/src/java/org/apache/hive/beeline/BeeLine.java?ref=70cc5eface64b5417916e42312befc022f4a06c0",
                "deletions": 93,
                "filename": "beeline/src/java/org/apache/hive/beeline/BeeLine.java",
                "patch": "@@ -23,12 +23,10 @@\n package org.apache.hive.beeline;\n \n import java.io.ByteArrayInputStream;\n-import java.io.ByteArrayOutputStream;\n import java.io.Closeable;\n import java.io.EOFException;\n import java.io.File;\n import java.io.FileInputStream;\n-import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.io.InputStream;\n import java.io.PrintStream;\n@@ -80,7 +78,6 @@\n import jline.console.completer.StringsCompleter;\n import jline.console.completer.FileNameCompleter;\n import jline.console.ConsoleReader;\n-import jline.console.history.History;\n import jline.console.history.FileHistory;\n \n import org.apache.commons.cli.CommandLine;\n@@ -146,7 +143,7 @@\n   private String dbName = null;\n   private String currentDatabase = null;\n \n-  private History history;\n+  private FileHistory history;\n   // Indicates if this instance of beeline is running in compatibility mode, or beeline mode\n   private boolean isBeeLine = true;\n \n@@ -517,14 +514,17 @@ public static void main(String[] args) throws IOException {\n   public static void mainWithInputRedirection(String[] args, InputStream inputStream)\n       throws IOException {\n     BeeLine beeLine = new BeeLine();\n-    int status = beeLine.begin(args, inputStream);\n+    try {\n+      int status = beeLine.begin(args, inputStream);\n \n-    if (!Boolean.getBoolean(BeeLineOpts.PROPERTY_NAME_EXIT)) {\n-        System.exit(status);\n+      if (!Boolean.getBoolean(BeeLineOpts.PROPERTY_NAME_EXIT)) {\n+          System.exit(status);\n+      }\n+    } finally {\n+      beeLine.close();\n     }\n   }\n \n-\n   public BeeLine() {\n     this(true);\n   }\n@@ -539,12 +539,11 @@ DatabaseConnection getDatabaseConnection() {\n \n \n   Connection getConnection() throws SQLException {\n-    if (getDatabaseConnections().current() == null) {\n-      throw new IllegalArgumentException(loc(\"no-current-connection\"));\n-    }\n-    if (getDatabaseConnections().current().getConnection() == null) {\n+    if (getDatabaseConnections().current() == null\n+        || getDatabaseConnections().current().getConnection() == null) {\n       throw new IllegalArgumentException(loc(\"no-current-connection\"));\n     }\n+\n     return getDatabaseConnections().current().getConnection();\n   }\n \n@@ -983,38 +982,36 @@ public int begin(String[] args, InputStream inputStream) throws IOException {\n       // nothing\n     }\n \n-    try {\n-      //this method also initializes the consoleReader which is\n-      //needed by initArgs for certain execution paths\n-      ConsoleReader reader = initializeConsoleReader(inputStream);\n-      if (isBeeLine) {\n-        int code = initArgs(args);\n-        if (code != 0) {\n-          return code;\n-        }\n-      } else {\n-        int code = initArgsFromCliVars(args);\n-        if (code != 0 || exit) {\n-          return code;\n-        }\n-        defaultConnect(false);\n-      }\n+    setupHistory();\n \n-      if (getOpts().isHelpAsked()) {\n-        return 0;\n-      }\n-      if (getOpts().getScriptFile() != null) {\n-        return executeFile(getOpts().getScriptFile());\n+    //this method also initializes the consoleReader which is\n+    //needed by initArgs for certain execution paths\n+    ConsoleReader reader = initializeConsoleReader(inputStream);\n+    if (isBeeLine) {\n+      int code = initArgs(args);\n+      if (code != 0) {\n+        return code;\n       }\n-      try {\n-        info(getApplicationTitle());\n-      } catch (Exception e) {\n-        // ignore\n+    } else {\n+      int code = initArgsFromCliVars(args);\n+      if (code != 0 || exit) {\n+        return code;\n       }\n-      return execute(reader, false);\n-    } finally {\n-        close();\n+      defaultConnect(false);\n+    }\n+\n+    if (getOpts().isHelpAsked()) {\n+      return 0;\n     }\n+    if (getOpts().getScriptFile() != null) {\n+      return executeFile(getOpts().getScriptFile());\n+    }\n+    try {\n+      info(getApplicationTitle());\n+    } catch (Exception e) {\n+      // ignore\n+    }\n+    return execute(reader, false);\n   }\n \n   /*\n@@ -1120,7 +1117,7 @@ public int defaultConnect(boolean exitOnError) {\n   }\n \n   private int executeFile(String fileName) {\n-    InputStream initStream = null;\n+    InputStream fileStream = null;\n     try {\n       if (!isBeeLine) {\n         org.apache.hadoop.fs.Path path = new org.apache.hadoop.fs.Path(fileName);\n@@ -1132,17 +1129,16 @@ private int executeFile(String fileName) {\n         } else {\n           fs = FileSystem.get(path.toUri(), conf);\n         }\n-        initStream = fs.open(path);\n+        fileStream = fs.open(path);\n       } else {\n-        initStream = new FileInputStream(fileName);\n+        fileStream = new FileInputStream(fileName);\n       }\n-      return execute(initializeConsoleReader(initStream), !getOpts().getForce());\n+      return execute(initializeConsoleReader(fileStream), !getOpts().getForce());\n     } catch (Throwable t) {\n       handleException(t);\n       return ERRNO_OTHER;\n     } finally {\n-      IOUtils.closeStream(initStream);\n-      consoleReader = null;\n+      IOUtils.closeStream(fileStream);\n       output(\"\");   // dummy new line\n     }\n   }\n@@ -1181,6 +1177,25 @@ public void close() {\n     commands.closeall(null);\n   }\n \n+  private void setupHistory() throws IOException {\n+    if (this.history != null) {\n+       return;\n+    }\n+\n+    this.history = new FileHistory(new File(getOpts().getHistoryFile()));\n+    // add shutdown hook to flush the history to history file\n+    ShutdownHookManager.addShutdownHook(new Runnable() {\n+      @Override\n+      public void run() {\n+        try {\n+          history.flush();\n+        } catch (IOException e) {\n+          error(e);\n+        }\n+      }\n+    });\n+  }\n+\n   public ConsoleReader initializeConsoleReader(InputStream inputStream) throws IOException {\n     if (inputStream != null) {\n       // ### NOTE: fix for sf.net bug 879425.\n@@ -1197,29 +1212,9 @@ public ConsoleReader initializeConsoleReader(InputStream inputStream) throws IOE\n     //disable the expandEvents for the purpose of backward compatibility\n     consoleReader.setExpandEvents(false);\n \n-    // setup history\n-    ByteArrayOutputStream hist = new ByteArrayOutputStream();\n-    if (new File(getOpts().getHistoryFile()).isFile()) {\n-      try {\n-        // save the current contents of the history buffer. This gets\n-        // around a bug in JLine where setting the output before the\n-        // input will clobber the history input, but setting the\n-        // input before the output will cause the previous commands\n-        // to not be saved to the buffer.\n-        try (FileInputStream historyIn = new FileInputStream(getOpts().getHistoryFile())) {\n-          int n;\n-          while ((n = historyIn.read()) != -1) {\n-            hist.write(n);\n-          }\n-        }\n-      } catch (Exception e) {\n-        handleException(e);\n-      }\n-    }\n-\n     try {\n       // now set the output for the history\n-      consoleReader.setHistory(new FileHistory(new File(getOpts().getHistoryFile())));\n+      consoleReader.setHistory(this.history);\n     } catch (Exception e) {\n       handleException(e);\n     }\n@@ -1228,32 +1223,6 @@ public ConsoleReader initializeConsoleReader(InputStream inputStream) throws IOE\n       // from script.. no need to load history and no need of completer, either\n       return consoleReader;\n     }\n-    try {\n-      // now load in the previous history\n-      if (hist != null) {\n-        History h = consoleReader.getHistory();\n-        if (!(h instanceof FileHistory)) {\n-          consoleReader.getHistory().add(hist.toString());\n-        }\n-      }\n-    } catch (Exception e) {\n-        handleException(e);\n-    }\n-\n-    // add shutdown hook to flush the history to history file\n-    ShutdownHookManager.addShutdownHook(new Runnable() {\n-        @Override\n-        public void run() {\n-            History h = consoleReader.getHistory();\n-            if (h instanceof FileHistory) {\n-                try {\n-                    ((FileHistory) h).flush();\n-                } catch (IOException e) {\n-                    error(e);\n-                }\n-            }\n-        }\n-    });\n \n     consoleReader.addCompleter(new BeeLineCompleter(this));\n     return consoleReader;",
                "raw_url": "https://github.com/apache/hive/raw/70cc5eface64b5417916e42312befc022f4a06c0/beeline/src/java/org/apache/hive/beeline/BeeLine.java",
                "sha": "65818dd48fa91b7cac2d3c07a3caa082b78e6365",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/70cc5eface64b5417916e42312befc022f4a06c0/beeline/src/java/org/apache/hive/beeline/cli/HiveCli.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/beeline/src/java/org/apache/hive/beeline/cli/HiveCli.java?ref=70cc5eface64b5417916e42312befc022f4a06c0",
                "deletions": 1,
                "filename": "beeline/src/java/org/apache/hive/beeline/cli/HiveCli.java",
                "patch": "@@ -32,6 +32,10 @@ public static void main(String[] args) throws IOException {\n \n   public int runWithArgs(String[] cmd, InputStream inputStream) throws IOException {\n     beeLine = new BeeLine(false);\n-    return beeLine.begin(cmd, inputStream);\n+    try {\n+      return beeLine.begin(cmd, inputStream);\n+    } finally {\n+      beeLine.close();\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/hive/raw/70cc5eface64b5417916e42312befc022f4a06c0/beeline/src/java/org/apache/hive/beeline/cli/HiveCli.java",
                "sha": "13fea293eb9df66d61cfd2fdf1a0c0626a571e1b",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/70cc5eface64b5417916e42312befc022f4a06c0/beeline/src/test/org/apache/hive/beeline/TestBeeLineHistory.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/beeline/src/test/org/apache/hive/beeline/TestBeeLineHistory.java?ref=70cc5eface64b5417916e42312befc022f4a06c0",
                "deletions": 1,
                "filename": "beeline/src/test/org/apache/hive/beeline/TestBeeLineHistory.java",
                "patch": "@@ -22,6 +22,7 @@\n import java.io.File;\n import java.io.PrintStream;\n import java.io.PrintWriter;\n+import java.lang.reflect.Method;\n \n import org.junit.AfterClass;\n import org.junit.Assert;\n@@ -58,11 +59,14 @@ public void testNumHistories() throws Exception {\n     BeeLine beeline = new BeeLine();\n     beeline.getOpts().setHistoryFile(fileName);\n     beeline.setOutputStream(ops);\n+    Method method = beeline.getClass().getDeclaredMethod(\"setupHistory\");\n+    method.setAccessible(true);\n+    method.invoke(beeline);\n     beeline.initializeConsoleReader(null);\n     beeline.dispatch(\"!history\");\n     String output = os.toString(\"UTF-8\");\n     int numHistories = output.split(\"\\n\").length;\n-    Assert.assertEquals(numHistories, 10);\n+    Assert.assertEquals(10, numHistories);\n     beeline.close();\n   }\n \n@@ -73,6 +77,9 @@ public void testHistory() throws Exception {\n     BeeLine beeline = new BeeLine();\n     beeline.getOpts().setHistoryFile(fileName);\n     beeline.setOutputStream(ops);\n+    Method method = beeline.getClass().getDeclaredMethod(\"setupHistory\");\n+    method.setAccessible(true);\n+    method.invoke(beeline);\n     beeline.initializeConsoleReader(null);\n     beeline.dispatch(\"!history\");\n     String output = os.toString(\"UTF-8\");",
                "raw_url": "https://github.com/apache/hive/raw/70cc5eface64b5417916e42312befc022f4a06c0/beeline/src/test/org/apache/hive/beeline/TestBeeLineHistory.java",
                "sha": "623e667b3cc588d6ada5432e87e23f5ab48c59b8",
                "status": "modified"
            }
        ],
        "message": "HIVE-15275 \"beeline -f <file>\" will throw NPE (Aihua Xu, reviewed by Vihang Karajgaonkar, Yongzhi Chen)",
        "parent": "https://github.com/apache/hive/commit/1a39cbfcaeda4392e231b70afa343f2862e91f26",
        "patched_files": [
            "BeeLine.java",
            "HiveCli.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestBeeLineHistory.java",
            "TestHiveCli.java"
        ]
    },
    "hive_70d6eef": {
        "bug_id": "hive_70d6eef",
        "commit": "https://github.com/apache/hive/commit/70d6eefe775453553ec804889c9319c6dd88f4cd",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hive/blob/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateAdd.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateAdd.java?ref=70d6eefe775453553ec804889c9319c6dd88f4cd",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateAdd.java",
                "patch": "@@ -96,6 +96,9 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen\n     ObjectInspector outputOI = PrimitiveObjectInspectorFactory.writableStringObjectInspector;\n     switch (inputType1) {\n     case STRING:\n+    case VARCHAR:\n+    case CHAR:\n+      inputType1 = PrimitiveCategory.STRING;\n       textConverter = ObjectInspectorConverters.getConverter(\n         (PrimitiveObjectInspector) arguments[0],\n         PrimitiveObjectInspectorFactory.writableStringObjectInspector);\n@@ -129,7 +132,14 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen\n   @Override\n   public Object evaluate(DeferredObject[] arguments) throws HiveException {\n \n+    if (arguments[0].get() == null) {\n+      return null;\n+    }\n     IntWritable toBeAdded = (IntWritable) intWritableConverter.convert(arguments[1].get());\n+    if (toBeAdded == null) {\n+      return null;\n+    }\n+\n     switch (inputType1) {\n     case STRING:\n       String dateString = textConverter.convert(arguments[0].get()).toString();",
                "raw_url": "https://github.com/apache/hive/raw/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateAdd.java",
                "sha": "3168385a4939c4a9217c356d5777da8b84a57485",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateDiff.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateDiff.java?ref=70d6eefe775453553ec804889c9319c6dd88f4cd",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateDiff.java",
                "patch": "@@ -113,9 +113,14 @@ private Date convertToDate(PrimitiveCategory inputType, Converter converter, Def\n     throws HiveException {\n     assert(converter != null);\n     assert(argument != null);\n+    if (argument.get() == null) {\n+      return null;\n+    }\n     Date date = new Date();\n     switch (inputType) {\n     case STRING:\n+    case VARCHAR:\n+    case CHAR:\n       String dateString = converter.convert(argument.get()).toString();\n       try {\n         date = formatter.parse(dateString);\n@@ -149,6 +154,8 @@ private Converter checkArguments(ObjectInspector[] arguments, int i) throws UDFA\n     Converter converter;\n     switch (inputType) {\n     case STRING:\n+    case VARCHAR:\n+    case CHAR:\n       converter = ObjectInspectorConverters.getConverter(\n         (PrimitiveObjectInspector) arguments[i],\n         PrimitiveObjectInspectorFactory.writableStringObjectInspector);",
                "raw_url": "https://github.com/apache/hive/raw/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateDiff.java",
                "sha": "5d8bd0d66580ca7d8a3577f7b5f069d7bcf044d2",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hive/blob/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateSub.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateSub.java?ref=70d6eefe775453553ec804889c9319c6dd88f4cd",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateSub.java",
                "patch": "@@ -96,6 +96,9 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen\n     ObjectInspector outputOI = PrimitiveObjectInspectorFactory.writableStringObjectInspector;\n     switch (inputType1) {\n     case STRING:\n+    case VARCHAR:\n+    case CHAR:\n+      inputType1 = PrimitiveCategory.STRING;\n       textConverter = ObjectInspectorConverters.getConverter(\n         (PrimitiveObjectInspector) arguments[0],\n         PrimitiveObjectInspectorFactory.writableStringObjectInspector);\n@@ -129,7 +132,14 @@ public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumen\n   @Override\n   public Object evaluate(DeferredObject[] arguments) throws HiveException {\n \n+    if (arguments[0].get() == null) {\n+      return null;\n+    }\n     IntWritable toBeSubed = (IntWritable) intWritableConverter.convert(arguments[1].get());\n+    if (toBeSubed == null) {\n+      return null;\n+    }\n+\n     switch (inputType1) {\n     case STRING:\n       String dateString = textConverter.convert(arguments[0].get()).toString();",
                "raw_url": "https://github.com/apache/hive/raw/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDateSub.java",
                "sha": "1685a86dc5053bcfadda0da940d0c338a2faa2c4",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hive/blob/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateAdd.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateAdd.java?ref=70d6eefe775453553ec804889c9319c6dd88f4cd",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateAdd.java",
                "patch": "@@ -47,6 +47,16 @@ public void testStringToDate() throws HiveException {\n     Text output = (Text) udf.evaluate(args);\n \n     assertEquals(\"date_add() test for STRING failed \", \"2009-07-22\", output.toString());\n+\n+    // Test with null args\n+    args = new DeferredObject[] { new DeferredJavaObject(null), valueObj2 };\n+    assertNull(\"date_add() 1st arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { valueObj1, new DeferredJavaObject(null) };\n+    assertNull(\"date_add() 2nd arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { new DeferredJavaObject(null), new DeferredJavaObject(null) };\n+    assertNull(\"date_add() both args null\", udf.evaluate(args));\n   }\n \n   public void testTimestampToDate() throws HiveException {\n@@ -63,6 +73,16 @@ public void testTimestampToDate() throws HiveException {\n     Text output = (Text) udf.evaluate(args);\n \n     assertEquals(\"date_add() test for TIMESTAMP failed \", \"2009-07-23\", output.toString());\n+\n+    // Test with null args\n+    args = new DeferredObject[] { new DeferredJavaObject(null), valueObj2 };\n+    assertNull(\"date_add() 1st arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { valueObj1, new DeferredJavaObject(null) };\n+    assertNull(\"date_add() 2nd arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { new DeferredJavaObject(null), new DeferredJavaObject(null) };\n+    assertNull(\"date_add() both args null\", udf.evaluate(args));\n   }\n \n   public void testDateWritablepToDate() throws HiveException {\n@@ -79,6 +99,16 @@ public void testDateWritablepToDate() throws HiveException {\n     Text output = (Text) udf.evaluate(args);\n \n     assertEquals(\"date_add() test for DATEWRITABLE failed \", \"2009-07-24\", output.toString());\n+\n+    // Test with null args\n+    args = new DeferredObject[] { new DeferredJavaObject(null), valueObj2 };\n+    assertNull(\"date_add() 1st arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { valueObj1, new DeferredJavaObject(null) };\n+    assertNull(\"date_add() 2nd arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { new DeferredJavaObject(null), new DeferredJavaObject(null) };\n+    assertNull(\"date_add() both args null\", udf.evaluate(args));\n   }\n \n }",
                "raw_url": "https://github.com/apache/hive/raw/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateAdd.java",
                "sha": "53b8f939ec260e2f04b92f6aac4d27c6ebd21680",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hive/blob/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateDiff.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateDiff.java?ref=70d6eefe775453553ec804889c9319c6dd88f4cd",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateDiff.java",
                "patch": "@@ -48,6 +48,16 @@ public void testStringToDate() throws HiveException {\n     IntWritable output = (IntWritable) udf.evaluate(args);\n \n     assertEquals(\"date_iff() test for STRING failed \", \"-2\", output.toString());\n+\n+    // Test with null args\n+    args = new DeferredObject[] { new DeferredJavaObject(null), valueObj2 };\n+    assertNull(\"date_add() 1st arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { valueObj1, new DeferredJavaObject(null) };\n+    assertNull(\"date_add() 2nd arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { new DeferredJavaObject(null), new DeferredJavaObject(null) };\n+    assertNull(\"date_add() both args null\", udf.evaluate(args));\n   }\n \n   public void testTimestampToDate() throws HiveException {\n@@ -65,6 +75,16 @@ public void testTimestampToDate() throws HiveException {\n     IntWritable output = (IntWritable) udf.evaluate(args);\n \n     assertEquals(\"datediff() test for TIMESTAMP failed \", \"3\", output.toString());\n+\n+    // Test with null args\n+    args = new DeferredObject[] { new DeferredJavaObject(null), valueObj2 };\n+    assertNull(\"date_add() 1st arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { valueObj1, new DeferredJavaObject(null) };\n+    assertNull(\"date_add() 2nd arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { new DeferredJavaObject(null), new DeferredJavaObject(null) };\n+    assertNull(\"date_add() both args null\", udf.evaluate(args));\n   }\n \n   public void testDateWritablepToDate() throws HiveException {\n@@ -81,6 +101,16 @@ public void testDateWritablepToDate() throws HiveException {\n     IntWritable output = (IntWritable) udf.evaluate(args);\n \n     assertEquals(\"datediff() test for DATEWRITABLE failed \", \"10\", output.toString());\n+\n+    // Test with null args\n+    args = new DeferredObject[] { new DeferredJavaObject(null), valueObj2 };\n+    assertNull(\"date_add() 1st arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { valueObj1, new DeferredJavaObject(null) };\n+    assertNull(\"date_add() 2nd arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { new DeferredJavaObject(null), new DeferredJavaObject(null) };\n+    assertNull(\"date_add() both args null\", udf.evaluate(args));\n   }\n \n }",
                "raw_url": "https://github.com/apache/hive/raw/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateDiff.java",
                "sha": "849e70c17a1e2c839bf857ca22b5b61cda6de9e1",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hive/blob/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateSub.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateSub.java?ref=70d6eefe775453553ec804889c9319c6dd88f4cd",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateSub.java",
                "patch": "@@ -47,6 +47,16 @@ public void testStringToDate() throws HiveException {\n     Text output = (Text) udf.evaluate(args);\n \n     assertEquals(\"date_sub() test for STRING failed \", \"2009-07-18\", output.toString());\n+\n+    // Test with null args\n+    args = new DeferredObject[] { new DeferredJavaObject(null), valueObj2 };\n+    assertNull(\"date_add() 1st arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { valueObj1, new DeferredJavaObject(null) };\n+    assertNull(\"date_add() 2nd arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { new DeferredJavaObject(null), new DeferredJavaObject(null) };\n+    assertNull(\"date_add() both args null\", udf.evaluate(args));\n   }\n \n   public void testTimestampToDate() throws HiveException {\n@@ -63,6 +73,16 @@ public void testTimestampToDate() throws HiveException {\n     Text output = (Text) udf.evaluate(args);\n \n     assertEquals(\"date_sub() test for TIMESTAMP failed \", \"2009-07-17\", output.toString());\n+\n+    // Test with null args\n+    args = new DeferredObject[] { new DeferredJavaObject(null), valueObj2 };\n+    assertNull(\"date_add() 1st arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { valueObj1, new DeferredJavaObject(null) };\n+    assertNull(\"date_add() 2nd arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { new DeferredJavaObject(null), new DeferredJavaObject(null) };\n+    assertNull(\"date_add() both args null\", udf.evaluate(args));\n   }\n \n   public void testDateWritablepToDate() throws HiveException {\n@@ -79,6 +99,16 @@ public void testDateWritablepToDate() throws HiveException {\n     Text output = (Text) udf.evaluate(args);\n \n     assertEquals(\"date_sub() test for DATEWRITABLE failed \", \"2009-07-16\", output.toString());\n+\n+    // Test with null args\n+    args = new DeferredObject[] { new DeferredJavaObject(null), valueObj2 };\n+    assertNull(\"date_add() 1st arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { valueObj1, new DeferredJavaObject(null) };\n+    assertNull(\"date_add() 2nd arg null\", udf.evaluate(args));\n+\n+    args = new DeferredObject[] { new DeferredJavaObject(null), new DeferredJavaObject(null) };\n+    assertNull(\"date_add() both args null\", udf.evaluate(args));\n   }\n \n }",
                "raw_url": "https://github.com/apache/hive/raw/70d6eefe775453553ec804889c9319c6dd88f4cd/ql/src/test/org/apache/hadoop/hive/ql/udf/TestGenericUDFDateSub.java",
                "sha": "d419ef28c66f2a4f2c95a1a78d0bdc337e5276c6",
                "status": "modified"
            }
        ],
        "message": "HIVE-6704 : date_add()/date_sub()/datediff() fail with NPE with null input (Jason Dere via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1580538 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/6745354ab2db240bd87f9a884814fff21b3146c5",
        "patched_files": [
            "GenericUDFDateSub.java",
            "GenericUDFDateAdd.java",
            "GenericUDFDateDiff.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestGenericUDFDateAdd.java",
            "TestGenericUDFDateDiff.java",
            "TestGenericUDFDateSub.java"
        ]
    },
    "hive_72c9d36": {
        "bug_id": "hive_72c9d36",
        "commit": "https://github.com/apache/hive/commit/72c9d3616f9ca836bc4fd6f2addf0edf7799cd46",
        "file": [
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hive/blob/72c9d3616f9ca836bc4fd6f2addf0edf7799cd46/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java?ref=72c9d3616f9ca836bc4fd6f2addf0edf7799cd46",
                "deletions": 8,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "patch": "@@ -242,7 +242,7 @@ public static void clearWork(Configuration conf) {\n     Path reducePath = getPlanPath(conf, REDUCE_PLAN_NAME);\n \n     // if the plan path hasn't been initialized just return, nothing to clean.\n-    if (mapPath == null || reducePath == null) {\n+    if (mapPath == null && reducePath == null) {\n       return;\n     }\n \n@@ -260,12 +260,7 @@ public static void clearWork(Configuration conf) {\n     } finally {\n       // where a single process works with multiple plans - we must clear\n       // the cache before working with the next plan.\n-      if (mapPath != null) {\n-        gWorkMap.remove(mapPath);\n-      }\n-      if (reducePath != null) {\n-        gWorkMap.remove(reducePath);\n-      }\n+      clearWorkMapForConf(conf);\n     }\n   }\n \n@@ -3314,7 +3309,19 @@ public static boolean isVectorMode(Configuration conf) {\n     return false;\n   }\n \n-    public static void clearWorkMap() {\n+  public static void clearWorkMapForConf(Configuration conf) {\n+    // Remove cached query plans for the current query only\n+    Path mapPath = getPlanPath(conf, MAP_PLAN_NAME);\n+    Path reducePath = getPlanPath(conf, REDUCE_PLAN_NAME);\n+    if (mapPath != null) {\n+      gWorkMap.remove(mapPath);\n+    }\n+    if (reducePath != null) {\n+      gWorkMap.remove(reducePath);\n+    }\n+  }\n+\n+  public static void clearWorkMap() {\n     gWorkMap.clear();\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/72c9d3616f9ca836bc4fd6f2addf0edf7799cd46/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "sha": "5e5cf97c7b8fefb204e45028af8bf1003437d25a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/72c9d3616f9ca836bc4fd6f2addf0edf7799cd46/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java?ref=72c9d3616f9ca836bc4fd6f2addf0edf7799cd46",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java",
                "patch": "@@ -393,7 +393,7 @@ private void addSplitsForGroup(List<Path> dirs, TableScanOperator tableScan, Job\n           currentTable, result);\n     }\n \n-    Utilities.clearWorkMap();\n+    Utilities.clearWorkMapForConf(job);\n     LOG.info(\"number of splits \" + result.size());\n     perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.GET_SPLITS);\n     return result.toArray(new HiveInputSplit[result.size()]);",
                "raw_url": "https://github.com/apache/hive/raw/72c9d3616f9ca836bc4fd6f2addf0edf7799cd46/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java",
                "sha": "61cc874dfebd8982fd5057371d605e8233b6ca79",
                "status": "modified"
            }
        ],
        "message": "HIVE-7210: NPE with \"No plan file found\" when running Driver instances on multiple threads (Jason Dere, reviewed by Gunther Hagleitner/Vikram Dixit)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1603344 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/cd05b4c1c4738e8e575ffbe6d542bb93ef626361",
        "patched_files": [
            "Utilities.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestUtilities.java"
        ]
    },
    "hive_735ba0d": {
        "bug_id": "hive_735ba0d",
        "commit": "https://github.com/apache/hive/commit/735ba0d872ddfbe0470497576904d721350548a4",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/735ba0d872ddfbe0470497576904d721350548a4/ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java?ref=735ba0d872ddfbe0470497576904d721350548a4",
                "deletions": 4,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java",
                "patch": "@@ -112,7 +112,7 @@ public void collect(HiveKey key, Object value) throws IOException {\n     return partitionKeys;\n   }\n \n-  public void writePartitionKeys(Path path, HiveConf conf, JobConf job) throws IOException {\n+  public void writePartitionKeys(Path path, JobConf job) throws IOException {\n     byte[][] partitionKeys = getPartitionKeys(job.getNumReduceTasks());\n     int numPartition = partitionKeys.length + 1;\n     if (numPartition != job.getNumReduceTasks()) {\n@@ -133,10 +133,11 @@ public void writePartitionKeys(Path path, HiveConf conf, JobConf job) throws IOE\n   }\n \n   // random sampling\n-  public static FetchOperator createSampler(FetchWork work, HiveConf conf, JobConf job,\n+  public static FetchOperator createSampler(FetchWork work, JobConf job,\n       Operator<?> operator) throws HiveException {\n-    int sampleNum = conf.getIntVar(HiveConf.ConfVars.HIVESAMPLINGNUMBERFORORDERBY);\n-    float samplePercent = conf.getFloatVar(HiveConf.ConfVars.HIVESAMPLINGPERCENTFORORDERBY);\n+    int sampleNum = HiveConf.getIntVar(job, HiveConf.ConfVars.HIVESAMPLINGNUMBERFORORDERBY);\n+    float samplePercent =\n+        HiveConf.getFloatVar(job, HiveConf.ConfVars.HIVESAMPLINGPERCENTFORORDERBY);\n     if (samplePercent < 0.0 || samplePercent > 1.0) {\n       throw new IllegalArgumentException(\"Percentile value must be within the range of 0 to 1.\");\n     }",
                "raw_url": "https://github.com/apache/hive/raw/735ba0d872ddfbe0470497576904d721350548a4/ql/src/java/org/apache/hadoop/hive/ql/exec/PartitionKeySampler.java",
                "sha": "dc1b601b732dc9cda814c41da6ae42d1179d4f56",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/735ba0d872ddfbe0470497576904d721350548a4/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java?ref=735ba0d872ddfbe0470497576904d721350548a4",
                "deletions": 6,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java",
                "patch": "@@ -376,7 +376,7 @@ public int execute(DriverContext driverContext) {\n \n       if (mWork.getSamplingType() > 0 && rWork != null && job.getNumReduceTasks() > 1) {\n         try {\n-          handleSampling(driverContext, mWork, job, conf);\n+          handleSampling(ctx, mWork, job);\n           job.setPartitionerClass(HiveTotalOrderPartitioner.class);\n         } catch (IllegalStateException e) {\n           console.printInfo(\"Not enough sampling data.. Rolling back to single reducer task\");\n@@ -496,7 +496,7 @@ public int execute(DriverContext driverContext) {\n     return (returnVal);\n   }\n \n-  private void handleSampling(DriverContext context, MapWork mWork, JobConf job, HiveConf conf)\n+  private void handleSampling(Context context, MapWork mWork, JobConf job)\n       throws Exception {\n     assert mWork.getAliasToWork().keySet().size() == 1;\n \n@@ -512,7 +512,7 @@ private void handleSampling(DriverContext context, MapWork mWork, JobConf job, H\n       inputPaths.add(new Path(path));\n     }\n \n-    Path tmpPath = context.getCtx().getExternalTmpPath(inputPaths.get(0));\n+    Path tmpPath = context.getExternalTmpPath(inputPaths.get(0));\n     Path partitionFile = new Path(tmpPath, \".partitions\");\n     ShimLoader.getHadoopShims().setTotalOrderPartitionFile(job, partitionFile);\n     PartitionKeySampler sampler = new PartitionKeySampler();\n@@ -541,9 +541,9 @@ private void handleSampling(DriverContext context, MapWork mWork, JobConf job, H\n       fetchWork.setSource(ts);\n \n       // random sampling\n-      FetchOperator fetcher = PartitionKeySampler.createSampler(fetchWork, conf, job, ts);\n+      FetchOperator fetcher = PartitionKeySampler.createSampler(fetchWork, job, ts);\n       try {\n-        ts.initialize(conf, new ObjectInspector[]{fetcher.getOutputObjectInspector()});\n+        ts.initialize(job, new ObjectInspector[]{fetcher.getOutputObjectInspector()});\n         OperatorUtils.setChildrenCollector(ts.getChildOperators(), sampler);\n         while (fetcher.pushRow()) { }\n       } finally {\n@@ -552,7 +552,7 @@ private void handleSampling(DriverContext context, MapWork mWork, JobConf job, H\n     } else {\n       throw new IllegalArgumentException(\"Invalid sampling type \" + mWork.getSamplingType());\n     }\n-    sampler.writePartitionKeys(partitionFile, conf, job);\n+    sampler.writePartitionKeys(partitionFile, job);\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hive/raw/735ba0d872ddfbe0470497576904d721350548a4/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java",
                "sha": "a2cf71281e8cd960e5f297638005da010b74d6ae",
                "status": "modified"
            }
        ],
        "message": "HIVE-10816: NPE in ExecDriver::handleSampling when submitted via child JVM (Rui reviewed by Xuefu)",
        "parent": "https://github.com/apache/hive/commit/8f9d964007f668f11084d55fe5608294edb3434f",
        "patched_files": [
            "ExecDriver.java",
            "PartitionKeySampler.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestPartitionKeySampler.java",
            "TestExecDriver.java"
        ]
    },
    "hive_7676cd9": {
        "bug_id": "hive_7676cd9",
        "commit": "https://github.com/apache/hive/commit/7676cd91a8612c55aff0cf30dcaba6342a369eaa",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java?ref=7676cd91a8612c55aff0cf30dcaba6342a369eaa",
                "deletions": 2,
                "filename": "serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java",
                "patch": "@@ -43,7 +43,7 @@\n import java.io.IOException;\n import java.nio.ByteBuffer;\n import java.util.ArrayList;\n-import java.util.Hashtable;\n+import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n \n@@ -246,7 +246,7 @@ private Object deserializeMap(Object datum, Schema mapSchema, MapTypeInfo column\n           throws AvroSerdeException {\n     // Avro only allows maps with Strings for keys, so we only have to worry\n     // about deserializing the values\n-    Map<String, Object> map = new Hashtable<String, Object>();\n+    Map<String, Object> map = new HashMap<String, Object>();\n     Map<Utf8, Object> mapDatum = (Map)datum;\n     Schema valueSchema = mapSchema.getValueType();\n     TypeInfo valueTypeInfo = columnType.getMapValueTypeInfo();",
                "raw_url": "https://github.com/apache/hive/raw/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroDeserializer.java",
                "sha": "c85ef15df43c16ff730eed5925ec7ec948782586",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerializer.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerializer.java?ref=7676cd91a8612c55aff0cf30dcaba6342a369eaa",
                "deletions": 2,
                "filename": "serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerializer.java",
                "patch": "@@ -39,7 +39,7 @@\n \n import java.nio.ByteBuffer;\n import java.util.ArrayList;\n-import java.util.Hashtable;\n+import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n \n@@ -229,7 +229,7 @@ private Object serializeMap(MapTypeInfo typeInfo, MapObjectInspector fieldOI, Ob\n     Map<?,?> map = fieldOI.getMap(structFieldData);\n     Schema valueType = schema.getValueType();\n \n-    Map<Object, Object> deserialized = new Hashtable<Object, Object>(fieldOI.getMapSize(structFieldData));\n+    Map<Object, Object> deserialized = new HashMap<Object, Object>(fieldOI.getMapSize(structFieldData));\n \n     for (Map.Entry<?, ?> entry : map.entrySet()) {\n       deserialized.put(serialize(mapKeyTypeInfo, mapKeyObjectInspector, entry.getKey(), null), // This works, but is a bit fragile.  Construct a single String schema?",
                "raw_url": "https://github.com/apache/hive/raw/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/java/org/apache/hadoop/hive/serde2/avro/AvroSerializer.java",
                "sha": "03c16f481e5d6008966f83a2e7078ccfab823d9a",
                "status": "modified"
            },
            {
                "additions": 52,
                "blob_url": "https://github.com/apache/hive/blob/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroDeserializer.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroDeserializer.java?ref=7676cd91a8612c55aff0cf30dcaba6342a369eaa",
                "deletions": 0,
                "filename": "serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroDeserializer.java",
                "patch": "@@ -36,6 +36,7 @@\n import java.util.Hashtable;\n import java.util.List;\n import java.util.Map;\n+import java.util.HashMap;\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNull;\n@@ -404,6 +405,57 @@ public void canDeserializeNullableTypes() throws IOException, SerDeException {\n     verifyNullableType(record, s, null);\n   }\n \n+  @Test\n+  public void canDeserializeMapWithNullablePrimitiveValues() throws SerDeException, IOException {\n+    Schema s = Schema.parse(TestAvroObjectInspectorGenerator.MAP_WITH_NULLABLE_PRIMITIVE_VALUE_TYPE_SCHEMA);\n+    GenericData.Record record = new GenericData.Record(s);\n+\n+    Map<String, Long> m = new HashMap<String, Long>();\n+    m.put(\"one\", 1l);\n+    m.put(\"two\", 2l);\n+    m.put(\"three\", 3l);\n+    m.put(\"mu\", null);\n+\n+    record.put(\"aMap\", m);\n+    assertTrue(GENERIC_DATA.validate(s, record));\n+    System.out.println(\"record = \" + record);\n+\n+    AvroGenericRecordWritable garw = Utils.serializeAndDeserializeRecord(record);\n+\n+    AvroObjectInspectorGenerator aoig = new AvroObjectInspectorGenerator(s);\n+\n+    AvroDeserializer de = new AvroDeserializer();\n+\n+    ArrayList<Object> row = (ArrayList<Object>)de.deserialize(aoig.getColumnNames(),\n+            aoig.getColumnTypes(), garw, s);\n+    assertEquals(1, row.size());\n+    Object theMapObject = row.get(0);\n+    assertTrue(theMapObject instanceof Map);\n+    Map theMap = (Map)theMapObject;\n+\n+    // Verify the raw object that's been created\n+    assertEquals(1l, theMap.get(\"one\"));\n+    assertEquals(2l, theMap.get(\"two\"));\n+    assertEquals(3l, theMap.get(\"three\"));\n+    assertTrue(theMap.containsKey(\"mu\"));\n+    assertEquals(null, theMap.get(\"mu\"));\n+\n+    // Verify that the provided object inspector can pull out these same values\n+    StandardStructObjectInspector oi =\n+            (StandardStructObjectInspector)aoig.getObjectInspector();\n+\n+    List<Object> z = oi.getStructFieldsDataAsList(row);\n+    assertEquals(1, z.size());\n+    StructField fieldRef = oi.getStructFieldRef(\"amap\");\n+\n+    Map theMap2 = (Map)oi.getStructFieldData(row, fieldRef);\n+    assertEquals(1l, theMap2.get(\"one\"));\n+    assertEquals(2l, theMap2.get(\"two\"));\n+    assertEquals(3l, theMap2.get(\"three\"));\n+    assertTrue(theMap2.containsKey(\"mu\"));\n+    assertEquals(null, theMap2.get(\"mu\"));\n+  }\n+\n   private void verifyNullableType(GenericData.Record record, Schema s,\n                                   String expected) throws SerDeException, IOException {\n     assertTrue(GENERIC_DATA.validate(s, record));",
                "raw_url": "https://github.com/apache/hive/raw/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroDeserializer.java",
                "sha": "5fe448eb2454810e8dfb97259a7adaae94e5b319",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hive/blob/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroObjectInspectorGenerator.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroObjectInspectorGenerator.java?ref=7676cd91a8612c55aff0cf30dcaba6342a369eaa",
                "deletions": 2,
                "filename": "serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroObjectInspectorGenerator.java",
                "patch": "@@ -142,6 +142,18 @@\n       \"    {\\\"name\\\":\\\"nullableString\\\", \\\"type\\\":[\\\"null\\\", \\\"string\\\"]}\\n\" +\n       \"  ]\\n\" +\n       \"}\";\n+  public static final String MAP_WITH_NULLABLE_PRIMITIVE_VALUE_TYPE_SCHEMA = \"{\\n\" +\n+      \"  \\\"namespace\\\": \\\"testing\\\",\\n\" +\n+      \"  \\\"name\\\": \\\"mapWithNullableUnionTest\\\",\\n\" +\n+      \"  \\\"type\\\": \\\"record\\\",\\n\" +\n+      \"  \\\"fields\\\": [\\n\" +\n+      \"    {\\n\" +\n+      \"      \\\"name\\\":\\\"aMap\\\",\\n\" +\n+      \"      \\\"type\\\":{\\\"type\\\":\\\"map\\\",\\n\" +\n+      \"      \\\"values\\\":[\\\"null\\\",\\\"long\\\"]}\\n\" +\n+      \"\\t}\\n\" +\n+      \"  ]\\n\" +\n+      \"}\";\n   public static final String BYTES_SCHEMA = \"{\\n\" +\n       \"  \\\"type\\\": \\\"record\\\", \\n\" +\n       \"  \\\"name\\\": \\\"bytesTest\\\",\\n\" +\n@@ -325,10 +337,19 @@ private void verifyColumnNames(String[] expectedColumnNames, List<String> column\n   public void canHandleMapsWithPrimitiveValueTypes() throws SerDeException {\n     Schema s = Schema.parse(MAP_WITH_PRIMITIVE_VALUE_TYPE);\n     AvroObjectInspectorGenerator aoig = new AvroObjectInspectorGenerator(s);\n-\n+    verifyMap(aoig, \"aMap\");\n+  }\n+ \n+  /**\n+   * Check a given AvroObjectInspectorGenerator to verify that it matches our test\n+   * schema's expected map.\n+   * @param aoig should already have been intitialized, may not be null\n+   * @param fieldName name of the contianed column, will always fail if null.\n+   */\n+  private void verifyMap(final AvroObjectInspectorGenerator aoig, final String fieldName) {\n     // Column names\n     assertEquals(1, aoig.getColumnNames().size());\n-    assertEquals(\"aMap\", aoig.getColumnNames().get(0));\n+    assertEquals(fieldName, aoig.getColumnNames().get(0));\n \n     // Column types\n     assertEquals(1, aoig.getColumnTypes().size());\n@@ -483,6 +504,13 @@ public void convertsNullableTypes() throws SerDeException {\n     assertEquals(PrimitiveObjectInspector.PrimitiveCategory.STRING, pti.getPrimitiveCategory());\n   }\n \n+  @Test // That Union[T, NULL] is converted to just T, within a Map\n+  public void convertsMapsWithNullablePrimitiveTypes() throws SerDeException {\n+    Schema s = Schema.parse(MAP_WITH_NULLABLE_PRIMITIVE_VALUE_TYPE_SCHEMA);\n+    AvroObjectInspectorGenerator aoig = new AvroObjectInspectorGenerator(s);\n+    verifyMap(aoig, \"aMap\");\n+  }\n+\n   @Test\n   public void objectInspectorsAreCached() throws SerDeException {\n     // Verify that Hive is caching the object inspectors for us.",
                "raw_url": "https://github.com/apache/hive/raw/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroObjectInspectorGenerator.java",
                "sha": "bc1a5c7305d978d745efb7d2f278d29278d5a08a",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hive/blob/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerializer.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerializer.java?ref=7676cd91a8612c55aff0cf30dcaba6342a369eaa",
                "deletions": 0,
                "filename": "serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerializer.java",
                "patch": "@@ -31,6 +31,7 @@\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.Hashtable;\n+import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n \n@@ -206,6 +207,21 @@ public void canSerializeNullableTypes() throws SerDeException, IOException {\n     assertNull(r.get(\"nullableint\"));\n   }\n \n+  @Test \n+  public void canSerializeMapsWithNullablePrimitiveValues() throws SerDeException, IOException {\n+    String field = \"{ \\\"name\\\":\\\"mapWithNulls\\\", \\\"type\\\": \" +\n+            \"{\\\"type\\\":\\\"map\\\", \\\"values\\\": [\\\"null\\\", \\\"boolean\\\"]} }\";\n+\n+    Map<String, Boolean> m = new HashMap<String, Boolean>();\n+    m.put(\"yes\", true);\n+    m.put(\"no\", false);\n+    m.put(\"maybe\", null);\n+    GenericRecord r = serializeAndDeserialize(field, \"mapWithNulls\", m);\n+\n+    Object result = r.get(\"mapWithNulls\");\n+    assertEquals(m, result);\n+  }\n+\n   @Test\n   public void canSerializeBytes() throws SerDeException, IOException {\n     String field = \"{ \\\"name\\\":\\\"bytes1\\\", \\\"type\\\":\\\"bytes\\\" }\";",
                "raw_url": "https://github.com/apache/hive/raw/7676cd91a8612c55aff0cf30dcaba6342a369eaa/serde/src/test/org/apache/hadoop/hive/serde2/avro/TestAvroSerializer.java",
                "sha": "2ba0e9ff2b6a4e1058f5d2b5d7b477afae8bb768",
                "status": "modified"
            }
        ],
        "message": "HIVE-3525. Avro Maps with Nullable Values fail with NPE (Sean Busbey via cws)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1399935 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/6b2fcdbdafb769d0015de5a442f955921451b371",
        "patched_files": [
            "AvroObjectInspectorGenerator.java",
            "AvroDeserializer.java",
            "AvroSerializer.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestAvroObjectInspectorGenerator.java",
            "TestAvroDeserializer.java",
            "TestAvroSerializer.java"
        ]
    },
    "hive_76f780a": {
        "bug_id": "hive_76f780a",
        "commit": "https://github.com/apache/hive/commit/76f780a545465d2ee674c896a0bd50540edd72e0",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/76f780a545465d2ee674c896a0bd50540edd72e0/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java?ref=76f780a545465d2ee674c896a0bd50540edd72e0",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java",
                "patch": "@@ -228,7 +228,7 @@ private void convertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcContext cont\n     @SuppressWarnings(\"unchecked\")\n     CommonMergeJoinOperator mergeJoinOp =\n         (CommonMergeJoinOperator) OperatorFactory.get(new CommonMergeJoinDesc(numBuckets,\n-            isSubQuery, mapJoinConversionPos, mapJoinDesc));\n+            isSubQuery, mapJoinConversionPos, mapJoinDesc), joinOp.getSchema());\n     OpTraits opTraits =\n         new OpTraits(joinOp.getOpTraits().getBucketColNames(), numBuckets, joinOp.getOpTraits()\n             .getSortCols());",
                "raw_url": "https://github.com/apache/hive/raw/76f780a545465d2ee674c896a0bd50540edd72e0/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java",
                "sha": "f231b0655454085ac447b2df891b1a54e3efed1f",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/76f780a545465d2ee674c896a0bd50540edd72e0/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MergeJoinProc.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MergeJoinProc.java?ref=76f780a545465d2ee674c896a0bd50540edd72e0",
                "deletions": 18,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/MergeJoinProc.java",
                "patch": "@@ -22,21 +22,6 @@\n import org.apache.hadoop.hive.ql.plan.TezWork.VertexType;\n \n public class MergeJoinProc implements NodeProcessor {\n-\n-  public Operator<? extends OperatorDesc> getLeafOperator(Operator<? extends OperatorDesc> op) {\n-    for (Operator<? extends OperatorDesc> childOp : op.getChildOperators()) {\n-      // FileSink or ReduceSink operators are used to create vertices. See\n-      // TezCompiler.\n-      if ((childOp instanceof ReduceSinkOperator) || (childOp instanceof FileSinkOperator)) {\n-        return childOp;\n-      } else {\n-        return getLeafOperator(childOp);\n-      }\n-    }\n-\n-    return null;\n-  }\n-\n   @Override\n   public Object\n       process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx, Object... nodeOutputs)\n@@ -60,13 +45,13 @@\n     // merge work already exists for this merge join operator, add the dummy store work to the\n     // merge work. Else create a merge work, add above work to the merge work\n     MergeJoinWork mergeWork = null;\n-    if (context.opMergeJoinWorkMap.containsKey(getLeafOperator(mergeJoinOp))) {\n+    if (context.opMergeJoinWorkMap.containsKey(mergeJoinOp)) {\n       // we already have the merge work corresponding to this merge join operator\n-      mergeWork = context.opMergeJoinWorkMap.get(getLeafOperator(mergeJoinOp));\n+      mergeWork = context.opMergeJoinWorkMap.get(mergeJoinOp);\n     } else {\n       mergeWork = new MergeJoinWork();\n       tezWork.add(mergeWork);\n-      context.opMergeJoinWorkMap.put(getLeafOperator(mergeJoinOp), mergeWork);\n+      context.opMergeJoinWorkMap.put(mergeJoinOp, mergeWork);\n     }\n \n     mergeWork.setMergeJoinOperator(mergeJoinOp);",
                "raw_url": "https://github.com/apache/hive/raw/76f780a545465d2ee674c896a0bd50540edd72e0/ql/src/java/org/apache/hadoop/hive/ql/optimizer/MergeJoinProc.java",
                "sha": "e3c87277c1c3df47bacdd673210cd961409b78ee",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/76f780a545465d2ee674c896a0bd50540edd72e0/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java?ref=76f780a545465d2ee674c896a0bd50540edd72e0",
                "deletions": 7,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java",
                "patch": "@@ -137,15 +137,15 @@ public Object process(Node nd, Stack<Node> stack,\n       // we are currently walking the big table side of the merge join. we need to create or hook up\n       // merge join work.\n       MergeJoinWork mergeJoinWork = null;\n-      if (context.opMergeJoinWorkMap.containsKey(operator)) {\n+      if (context.opMergeJoinWorkMap.containsKey(context.currentMergeJoinOperator)) {\n         // we have found a merge work corresponding to this closing operator. Hook up this work.\n-        mergeJoinWork = context.opMergeJoinWorkMap.get(operator);\n+        mergeJoinWork = context.opMergeJoinWorkMap.get(context.currentMergeJoinOperator);\n       } else {\n         // we need to create the merge join work\n         mergeJoinWork = new MergeJoinWork();\n         mergeJoinWork.setMergeJoinOperator(context.currentMergeJoinOperator);\n         tezWork.add(mergeJoinWork);\n-        context.opMergeJoinWorkMap.put(operator, mergeJoinWork);\n+        context.opMergeJoinWorkMap.put(context.currentMergeJoinOperator, mergeJoinWork);\n       }\n       // connect the work correctly.\n       mergeJoinWork.addMergedWork(work, null);\n@@ -334,10 +334,15 @@ public Object process(Node nd, Stack<Node> stack,\n           UnionWork unionWork = (UnionWork) followingWork;\n           int index = getMergeIndex(tezWork, unionWork, rs);\n           // guaranteed to be instance of MergeJoinWork if index is valid\n-          MergeJoinWork mergeJoinWork = (MergeJoinWork) tezWork.getChildren(unionWork).get(index);\n-          // disconnect the connection to union work and connect to merge work\n-          followingWork = mergeJoinWork;\n-          rWork = (ReduceWork) mergeJoinWork.getMainWork();\n+          BaseWork baseWork = tezWork.getChildren(unionWork).get(index);\n+          if (baseWork instanceof MergeJoinWork) {\n+            MergeJoinWork mergeJoinWork = (MergeJoinWork) baseWork;\n+            // disconnect the connection to union work and connect to merge work\n+            followingWork = mergeJoinWork;\n+            rWork = (ReduceWork) mergeJoinWork.getMainWork();\n+          } else {\n+            rWork = (ReduceWork) baseWork;\n+          }\n         } else {\n           rWork = (ReduceWork) followingWork;\n         }",
                "raw_url": "https://github.com/apache/hive/raw/76f780a545465d2ee674c896a0bd50540edd72e0/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java",
                "sha": "59a632776f811dafd2fcd23ec3264b7864535b8f",
                "status": "modified"
            }
        ],
        "message": "HIVE-8563: Running annotate_stats_join_pkfk.q in TestMiniTezCliDriver is causing NPE (Vikram Dixit K via Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1633986 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/ce7a8b7f53f9cd52ed5e7f0fb427e25ce32fe30f",
        "patched_files": [
            "GenTezWork.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestGenTezWork.java"
        ]
    },
    "hive_7fa8e37": {
        "bug_id": "hive_7fa8e37",
        "commit": "https://github.com/apache/hive/commit/7fa8e37fd13d9d6a4a4a5b2c72ce02d7c2d199ef",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/7fa8e37fd13d9d6a4a4a5b2c72ce02d7c2d199ef/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java?ref=7fa8e37fd13d9d6a4a4a5b2c72ce02d7c2d199ef",
                "deletions": 2,
                "filename": "llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java",
                "patch": "@@ -541,7 +541,7 @@ public void indicateError(Throwable t) {\n   @Override\n   public String getInProgressLogsUrl(TezTaskAttemptID attemptID, NodeId containerNodeId) {\n     String url = \"\";\n-    if (timelineServerUri != null) {\n+    if (timelineServerUri != null && containerNodeId != null) {\n       LlapNodeId llapNodeId = LlapNodeId.getInstance(containerNodeId.getHost(), containerNodeId.getPort());\n       BiMap<ContainerId, TezTaskAttemptID> biMap = entityTracker.getContainerAttemptMapForNode(llapNodeId);\n       ContainerId containerId = biMap.inverse().get(attemptID);\n@@ -559,7 +559,7 @@ public String getInProgressLogsUrl(TezTaskAttemptID attemptID, NodeId containerN\n   @Override\n   public String getCompletedLogsUrl(TezTaskAttemptID attemptID, NodeId containerNodeId) {\n     String url = \"\";\n-    if (timelineServerUri != null) {\n+    if (timelineServerUri != null && containerNodeId != null) {\n       LlapNodeId llapNodeId = LlapNodeId.getInstance(containerNodeId.getHost(), containerNodeId.getPort());\n       BiMap<ContainerId, TezTaskAttemptID> biMap = entityTracker.getContainerAttemptMapForNode(llapNodeId);\n       ContainerId containerId = biMap.inverse().get(attemptID);",
                "raw_url": "https://github.com/apache/hive/raw/7fa8e37fd13d9d6a4a4a5b2c72ce02d7c2d199ef/llap-tez/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java",
                "sha": "3aae7a42fac9f67cd3a7ce9e915b569ffd994115",
                "status": "modified"
            }
        ],
        "message": "HIVE-15992: LLAP: NPE in LlapTaskCommunicator.getCompletedLogsUrl for unsuccessful attempt (Rajesh Balamohan reviewed by Prasanth Jayachandran)",
        "parent": "https://github.com/apache/hive/commit/bda64ee87c74a06b3cf19b08c41d67f192f22018",
        "patched_files": [
            "LlapTaskCommunicator.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestLlapTaskCommunicator.java"
        ]
    },
    "hive_7faa8a1": {
        "bug_id": "hive_7faa8a1",
        "commit": "https://github.com/apache/hive/commit/7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3",
        "file": [
            {
                "additions": 66,
                "blob_url": "https://github.com/apache/hive/blob/7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3/itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHiveAuthFactory.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHiveAuthFactory.java?ref=7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3",
                "deletions": 0,
                "filename": "itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHiveAuthFactory.java",
                "patch": "@@ -0,0 +1,66 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hive.minikdc;\n+\n+import org.junit.Assert;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hive.service.auth.HiveAuthFactory;\n+import org.junit.AfterClass;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+\n+public class TestHiveAuthFactory {\n+  private static HiveConf hiveConf;\n+  private static MiniHiveKdc miniHiveKdc = null;\n+\n+  @BeforeClass\n+  public static void setUp() throws Exception {\n+    hiveConf = new HiveConf();\n+    miniHiveKdc = MiniHiveKdc.getMiniHiveKdc(hiveConf);\n+  }\n+\n+  @AfterClass\n+  public static void tearDown() throws Exception {\n+  }\n+\n+  /**\n+   * Verify that delegation token manager is started with no exception\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testStartTokenManager() throws Exception {\n+    hiveConf.setVar(ConfVars.HIVE_SERVER2_AUTHENTICATION, HiveAuthFactory.AuthTypes.KERBEROS.getAuthName());\n+    String principalName = miniHiveKdc.getFullHiveServicePrincipal();\n+    System.out.println(\"Principal: \" + principalName);\n+    \n+    hiveConf.setVar(ConfVars.HIVE_SERVER2_KERBEROS_PRINCIPAL, principalName);\n+    String keyTabFile = miniHiveKdc.getKeyTabFile(miniHiveKdc.getHiveServicePrincipal());\n+    System.out.println(\"keyTabFile: \" + keyTabFile);\n+    Assert.assertNotNull(keyTabFile);\n+    hiveConf.setVar(ConfVars.HIVE_SERVER2_KERBEROS_KEYTAB, keyTabFile);\n+\n+    System.out.println(\"rawStoreClassName =\" +  hiveConf.getVar(ConfVars.METASTORE_RAW_STORE_IMPL));\n+\n+    HiveAuthFactory authFactory = new HiveAuthFactory(hiveConf);\n+    Assert.assertNotNull(authFactory);\n+    Assert.assertEquals(\"org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge$Server$TUGIAssumingTransportFactory\", \n+        authFactory.getAuthTransFactory().getClass().getName());\n+  }\n+}",
                "raw_url": "https://github.com/apache/hive/raw/7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3/itests/hive-minikdc/src/test/java/org/apache/hive/minikdc/TestHiveAuthFactory.java",
                "sha": "a30ec7e9e5e079f7c13456c29082121da9b6c202",
                "status": "added"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3/service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java?ref=7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3",
                "deletions": 3,
                "filename": "service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java",
                "patch": "@@ -18,7 +18,6 @@\n package org.apache.hive.service.auth;\n \n import java.io.IOException;\n-import java.net.InetAddress;\n import java.net.InetSocketAddress;\n import java.net.UnknownHostException;\n import java.util.ArrayList;\n@@ -33,6 +32,9 @@\n \n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n+import org.apache.hadoop.hive.metastore.HiveMetaStore;\n+import org.apache.hadoop.hive.metastore.HiveMetaStore.HMSHandler;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n import org.apache.hadoop.hive.shims.HadoopShims.KerberosNameShim;\n import org.apache.hadoop.hive.shims.ShimLoader;\n import org.apache.hadoop.hive.thrift.HadoopThriftAuthBridge;\n@@ -108,8 +110,11 @@ public HiveAuthFactory(HiveConf conf) throws TTransportException {\n                         conf.getVar(ConfVars.HIVE_SERVER2_KERBEROS_PRINCIPAL));\n         // start delegation token manager\n         try {\n-          saslServer.startDelegationTokenSecretManager(conf, null, ServerMode.HIVESERVER2);\n-        } catch (IOException e) {\n+          HMSHandler baseHandler = new HiveMetaStore.HMSHandler(\n+              \"new db based metaserver\", conf, true);\n+          saslServer.startDelegationTokenSecretManager(conf, baseHandler.getMS(), ServerMode.HIVESERVER2);\n+        }\n+        catch (MetaException|IOException e) {\n           throw new TTransportException(\"Failed to start token manager\", e);\n         }\n       }",
                "raw_url": "https://github.com/apache/hive/raw/7faa8a1e3bb7d65bed93cb203c9ec11c3c92daf3/service/src/java/org/apache/hive/service/auth/HiveAuthFactory.java",
                "sha": "22c309fbfd941fb517370057ef38fbec903ce088",
                "status": "modified"
            }
        ],
        "message": "HIVE-9622 - Getting NPE when trying to restart HS2 when metastore is configured to use org.apache.hadoop.hive.thrift.DBTokenStore (Aihua via Brock)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1659302 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/c09884e0b662ecb635ccdb162d06ade2ea71d8b6",
        "patched_files": [
            "HiveAuthFactory.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveAuthFactory.java"
        ]
    },
    "hive_80aaa1e": {
        "bug_id": "hive_80aaa1e",
        "commit": "https://github.com/apache/hive/commit/80aaa1e65894400d16df608e48bf838238152ec8",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hive/blob/80aaa1e65894400d16df608e48bf838238152ec8/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=80aaa1e65894400d16df608e48bf838238152ec8",
                "deletions": 0,
                "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "patch": "@@ -3555,6 +3555,16 @@ private boolean drop_partition_common(RawStore ms, String db_name, String tbl_na\n       boolean isExternalTbl = false;\n       Map<String, String> transactionalListenerResponses = Collections.emptyMap();\n \n+      if (db_name == null) {\n+        throw new MetaException(\"The DB name cannot be null.\");\n+      }\n+      if (tbl_name == null) {\n+        throw new MetaException(\"The table name cannot be null.\");\n+      }\n+      if (part_vals == null) {\n+        throw new MetaException(\"The partition values cannot be null.\");\n+      }\n+\n       try {\n         ms.openTransaction();\n         part = ms.getPartition(db_name, tbl_name, part_vals);",
                "raw_url": "https://github.com/apache/hive/raw/80aaa1e65894400d16df608e48bf838238152ec8/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "sha": "84fac2dfa4f29c1cd67d6a06e1f31e6aad016c75",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hive/blob/80aaa1e65894400d16df608e48bf838238152ec8/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java?ref=80aaa1e65894400d16df608e48bf838238152ec8",
                "deletions": 0,
                "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java",
                "patch": "@@ -993,13 +993,23 @@ public boolean dropPartition(String db_name, String tbl_name,\n   @Override\n   public boolean dropPartition(String db_name, String tbl_name,\n       List<String> part_vals, PartitionDropOptions options) throws TException {\n+    if (options == null) {\n+      options = PartitionDropOptions.instance();\n+    }\n     return dropPartition(db_name, tbl_name, part_vals, options.deleteData,\n                          options.purgeData? getEnvironmentContextWithIfPurgeSet() : null);\n   }\n \n   public boolean dropPartition(String db_name, String tbl_name, List<String> part_vals,\n       boolean deleteData, EnvironmentContext envContext) throws NoSuchObjectException,\n       MetaException, TException {\n+    if (part_vals != null) {\n+      for (String partVal : part_vals) {\n+        if (partVal == null) {\n+          throw new MetaException(\"The partition value must not be null.\");\n+        }\n+      }\n+    }\n     return client.drop_partition_with_environment_context(db_name, tbl_name, part_vals, deleteData,\n         envContext);\n   }",
                "raw_url": "https://github.com/apache/hive/raw/80aaa1e65894400d16df608e48bf838238152ec8/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java",
                "sha": "0e561f82ff28242f1ec62dd0b141b6707506d461",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/80aaa1e65894400d16df608e48bf838238152ec8/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestDropPartitions.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestDropPartitions.java?ref=80aaa1e65894400d16df608e48bf838238152ec8",
                "deletions": 9,
                "filename": "standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestDropPartitions.java",
                "patch": "@@ -264,18 +264,13 @@ public void testDropPartitionNonExistingPartVals() throws Exception {\n     client.dropPartition(DB_NAME, TABLE_NAME, Lists.newArrayList(\"2017\", \"may\"), false);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testDropPartitionNullVal() throws Exception {\n \n     List<String> partVals = new ArrayList<>();\n     partVals.add(null);\n     partVals.add(null);\n-    try {\n-      client.dropPartition(DB_NAME, TABLE_NAME, partVals, false);\n-      Assert.fail(\"NullPointerException or NoSuchObjectException is expected to be thrown\");\n-    } catch (NullPointerException | NoSuchObjectException e) {\n-      // TODO: Should not throw NPE.\n-    }\n+    client.dropPartition(DB_NAME, TABLE_NAME, partVals, false);\n   }\n \n   @Test(expected = NoSuchObjectException.class)\n@@ -400,10 +395,13 @@ public void testDropPartitionPurgeSetInTable() throws Exception {\n     checkPartitionsAfterDelete(tableName, droppedPartitions, remainingPartitions, true, true);\n   }\n \n-  @Test(expected = NullPointerException.class)\n+  @Test\n   public void testDropPartitionNullPartDropOptions() throws Exception {\n-    // TODO: This should not throw NPE\n+\n     client.dropPartition(DB_NAME, TABLE_NAME, PARTITIONS[0].getValues(), null);\n+    List<Partition> droppedPartitions = Lists.newArrayList(PARTITIONS[0]);\n+    List<Partition> remainingPartitions = Lists.newArrayList(PARTITIONS[1], PARTITIONS[2]);\n+    checkPartitionsAfterDelete(TABLE_NAME, droppedPartitions, remainingPartitions, true, false);\n   }\n \n   // Tests for dropPartition(String db_name, String tbl_name, String name,",
                "raw_url": "https://github.com/apache/hive/raw/80aaa1e65894400d16df608e48bf838238152ec8/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestDropPartitions.java",
                "sha": "d2ba4be7c055ef3ab3ee76ec771066bb41003854",
                "status": "modified"
            }
        ],
        "message": "HIVE-18898: Fix NPEs in HiveMetastore.dropPartition method (Marta Kuczora, reviewed by Sahil Takiar, Alexander Kolbasov and Peter Vary)",
        "parent": "https://github.com/apache/hive/commit/901a3b3fb04a9d78c64e661ca47e7ebe184abfb8",
        "patched_files": [
            "HiveMetaStore.java",
            "HiveMetaStoreClient.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveMetaStore.java",
            "TestDropPartitions.java"
        ]
    },
    "hive_820db60": {
        "bug_id": "hive_820db60",
        "commit": "https://github.com/apache/hive/commit/820db608f2878dde1d9c7b3fa3fbfdb3564710d6",
        "file": [
            {
                "additions": 47,
                "blob_url": "https://github.com/apache/hive/blob/820db608f2878dde1d9c7b3fa3fbfdb3564710d6/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=820db608f2878dde1d9c7b3fa3fbfdb3564710d6",
                "deletions": 2,
                "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "patch": "@@ -6767,13 +6767,28 @@ private static MetaException newMetaException(Exception e) {\n     }\n \n     private void validateFunctionInfo(Function func) throws InvalidObjectException, MetaException {\n+      if (func == null) {\n+        throw new MetaException(\"Function cannot be null.\");\n+      }\n+      if (func.getFunctionName() == null) {\n+        throw new MetaException(\"Function name cannot be null.\");\n+      }\n+      if (func.getDbName() == null) {\n+        throw new MetaException(\"Database name in Function cannot be null.\");\n+      }\n       if (!MetaStoreUtils.validateName(func.getFunctionName(), null)) {\n         throw new InvalidObjectException(func.getFunctionName() + \" is not a valid object name\");\n       }\n       String className = func.getClassName();\n       if (className == null) {\n         throw new InvalidObjectException(\"Function class name cannot be null\");\n       }\n+      if (func.getOwnerType() == null) {\n+        throw new MetaException(\"Function owner type cannot be null.\");\n+      }\n+      if (func.getFunctionType() == null) {\n+        throw new MetaException(\"Function type cannot be null.\");\n+      }\n     }\n \n     @Override\n@@ -6826,11 +6841,17 @@ public void create_function(Function func) throws TException {\n     public void drop_function(String dbName, String funcName)\n         throws NoSuchObjectException, MetaException,\n         InvalidObjectException, InvalidInputException {\n+      if (funcName == null) {\n+        throw new MetaException(\"Function name cannot be null.\");\n+      }\n       boolean success = false;\n       Function func = null;\n       RawStore ms = getMS();\n       Map<String, String> transactionalListenerResponses = Collections.emptyMap();\n       String[] parsedDbName = parseDbName(dbName, conf);\n+      if (parsedDbName[DB_NAME] == null) {\n+        throw new MetaException(\"Database name cannot be null.\");\n+      }\n       try {\n         ms.openTransaction();\n         func = ms.getFunction(parsedDbName[CAT_NAME], parsedDbName[DB_NAME], funcName);\n@@ -6876,21 +6897,42 @@ public void drop_function(String dbName, String funcName)\n \n     @Override\n     public void alter_function(String dbName, String funcName, Function newFunc) throws TException {\n-      validateFunctionInfo(newFunc);\n+      String[] parsedDbName = parseDbName(dbName, conf);\n+      validateForAlterFunction(parsedDbName[DB_NAME], funcName, newFunc);\n       boolean success = false;\n       RawStore ms = getMS();\n-      String[] parsedDbName = parseDbName(dbName, conf);\n       try {\n         ms.openTransaction();\n         ms.alterFunction(parsedDbName[CAT_NAME], parsedDbName[DB_NAME], funcName, newFunc);\n         success = ms.commitTransaction();\n+      } catch (InvalidObjectException e) {\n+        // Throwing MetaException instead of InvalidObjectException as the InvalidObjectException\n+        // is not defined for the alter_function method in the Thrift interface.\n+        throwMetaException(e);\n       } finally {\n         if (!success) {\n           ms.rollbackTransaction();\n         }\n       }\n     }\n \n+    private void validateForAlterFunction(String dbName, String funcName, Function newFunc)\n+        throws MetaException {\n+      if (dbName == null || funcName == null) {\n+        throw new MetaException(\"Database and function name cannot be null.\");\n+      }\n+      try {\n+        validateFunctionInfo(newFunc);\n+      } catch (InvalidObjectException e) {\n+        // The validateFunctionInfo method is used by the create and alter function methods as well\n+        // and it can throw InvalidObjectException. But the InvalidObjectException is not defined\n+        // for the alter_function method in the Thrift interface, therefore a TApplicationException\n+        // will occur at the caller side. Re-throwing the InvalidObjectException as MetaException\n+        // would eliminate the TApplicationException at caller side.\n+        throw newMetaException(e);\n+      }\n+    }\n+\n     @Override\n     public List<String> get_functions(String dbName, String pattern)\n         throws MetaException {\n@@ -6938,6 +6980,9 @@ public GetAllFunctionsResponse get_all_functions()\n \n     @Override\n     public Function get_function(String dbName, String funcName) throws TException {\n+      if (dbName == null || funcName == null) {\n+        throw new MetaException(\"Database and function name cannot be null.\");\n+      }\n       startFunction(\"get_function\", \": \" + dbName + \".\" + funcName);\n \n       RawStore ms = getMS();",
                "raw_url": "https://github.com/apache/hive/raw/820db608f2878dde1d9c7b3fa3fbfdb3564710d6/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "sha": "65ca63c61f4cc7dfece9b656668f9f079f5ad97c",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/820db608f2878dde1d9c7b3fa3fbfdb3564710d6/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java?ref=820db608f2878dde1d9c7b3fa3fbfdb3564710d6",
                "deletions": 0,
                "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java",
                "patch": "@@ -2668,6 +2668,9 @@ public boolean isPartitionMarkedForEvent(String catName, String db_name, String\n \n   @Override\n   public void createFunction(Function func) throws TException {\n+    if (func == null) {\n+      throw new MetaException(\"Function cannot be null.\");\n+    }\n     if (!func.isSetCatName()) func.setCatName(getDefaultCatalog(conf));\n     client.create_function(func);\n   }",
                "raw_url": "https://github.com/apache/hive/raw/820db608f2878dde1d9c7b3fa3fbfdb3564710d6/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/HiveMetaStoreClient.java",
                "sha": "867771849b5f3ef2cf446cc3bca1a59f5f19a3e2",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hive/blob/820db608f2878dde1d9c7b3fa3fbfdb3564710d6/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestFunctions.java",
                "changes": 258,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestFunctions.java?ref=820db608f2878dde1d9c7b3fa3fbfdb3564710d6",
                "deletions": 217,
                "filename": "standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestFunctions.java",
                "patch": "@@ -37,9 +37,7 @@\n import org.apache.hadoop.hive.metastore.client.builder.DatabaseBuilder;\n import org.apache.hadoop.hive.metastore.client.builder.FunctionBuilder;\n import org.apache.hadoop.hive.metastore.minihms.AbstractMetaStoreService;\n-import org.apache.thrift.TApplicationException;\n import org.apache.thrift.TException;\n-import org.apache.thrift.transport.TTransportException;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -208,74 +206,39 @@ public void testCreateFunctionEmptyName() throws Exception {\n     client.createFunction(function);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n+  public void testCreateFunctionNullFunction() throws Exception {\n+    client.createFunction(null);\n+  }\n+\n+  @Test(expected = MetaException.class)\n   public void testCreateFunctionNullFunctionName() throws Exception {\n     Function function = testFunctions[0];\n     function.setFunctionName(null);\n-\n-    try {\n-      client.createFunction(function);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.createFunction(function);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testCreateFunctionNullDatabaseName() throws Exception {\n     Function function = testFunctions[0];\n     function.setDbName(null);\n-\n-    try {\n-      client.createFunction(function);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.createFunction(function);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testCreateFunctionNullOwnerType() throws Exception {\n     Function function = testFunctions[0];\n     function.setFunctionName(\"test_function_2\");\n     function.setOwnerType(null);\n-\n-    try {\n-      client.createFunction(function);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.createFunction(function);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testCreateFunctionNullFunctionType() throws Exception {\n     Function function = testFunctions[0];\n     function.setFunctionName(\"test_function_2\");\n     function.setFunctionType(null);\n-\n-    try {\n-      client.createFunction(function);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.createFunction(function);\n   }\n \n   @Test(expected = NoSuchObjectException.class)\n@@ -331,18 +294,9 @@ public void testGetFunctionNoSuchFunctionInThisDatabase() throws Exception {\n     client.getFunction(OTHER_DATABASE, function.getFunctionName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testGetFunctionNullDatabase() throws Exception {\n-    try {\n-      client.getFunction(null, OTHER_DATABASE);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws MetaException\n-      Assert.fail(\"Expected an NullPointerException or MetaException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (MetaException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.getFunction(null, OTHER_DATABASE);\n   }\n \n   @Test(expected = MetaException.class)\n@@ -371,32 +325,14 @@ public void testDropFunctionNoSuchFunctionInThisDatabase() throws Exception {\n     client.dropFunction(OTHER_DATABASE, function.getFunctionName());\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testDropFunctionNullDatabase() throws Exception {\n-    try {\n-      client.dropFunction(null, \"no_such_function\");\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.dropFunction(null, \"no_such_function\");\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testDropFunctionNullFunctionName() throws Exception {\n-    try {\n-      client.dropFunction(DEFAULT_DATABASE, null);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.dropFunction(DEFAULT_DATABASE, null);\n   }\n \n   @Test\n@@ -601,190 +537,78 @@ public void testAlterFunctionNoSuchFunctionInThisDatabase() throws Exception {\n     client.alterFunction(OTHER_DATABASE, originalFunction.getFunctionName(), newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionNullDatabase() throws Exception {\n     Function newFunction = getNewFunction();\n-\n-    try {\n-      client.alterFunction(null, OTHER_DATABASE, newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(null, OTHER_DATABASE, newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionNullFunctionName() throws Exception {\n     Function newFunction = getNewFunction();\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, null, newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, null, newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionNullFunction() throws Exception {\n     Function originalFunction = testFunctions[1];\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, originalFunction.getFunctionName(), null);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, originalFunction.getFunctionName(), null);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionInvalidNameInNew() throws Exception {\n     Function newFunction = getNewFunction();\n     newFunction.setFunctionName(\"test_function_2;\");\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // InvalidObjectException, remote throws TApplicationException\n-      Assert.fail(\"Expected an InvalidObjectException or TApplicationException to be thrown\");\n-    } catch (InvalidObjectException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TApplicationException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionEmptyNameInNew() throws Exception {\n     Function newFunction = getNewFunction();\n     newFunction.setFunctionName(\"\");\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // InvalidObjectException, remote throws TApplicationException\n-      Assert.fail(\"Expected an InvalidObjectException or TApplicationException to be thrown\");\n-    } catch (InvalidObjectException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TApplicationException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionNullClassInNew() throws Exception {\n     Function newFunction = getNewFunction();\n     newFunction.setClassName(null);\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // InvalidObjectException, remote throws TApplicationException\n-      Assert.fail(\"Expected an InvalidObjectException or TApplicationException to be thrown\");\n-    } catch (InvalidObjectException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TApplicationException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionNullFunctionNameInNew() throws Exception {\n     Function newFunction = getNewFunction();\n     newFunction.setFunctionName(null);\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionNullDatabaseNameInNew() throws Exception {\n     Function newFunction = getNewFunction();\n     newFunction.setDbName(null);\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionNullOwnerTypeInNew() throws Exception {\n     Function newFunction = getNewFunction();\n     newFunction.setOwnerType(null);\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionNullFunctionTypeInNew() throws Exception {\n     Function newFunction = getNewFunction();\n     newFunction.setFunctionType(null);\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // NullPointerException, remote throws TTransportException\n-      Assert.fail(\"Expected an NullPointerException or TTransportException to be thrown\");\n-    } catch (NullPointerException exception) {\n-      // Expected exception - Embedded MetaStore\n-    } catch (TTransportException exception) {\n-      // Expected exception - Remote MetaStore\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n   }\n \n-  @Test\n+  @Test(expected = MetaException.class)\n   public void testAlterFunctionNoSuchDatabaseInNew() throws Exception {\n     Function newFunction = getNewFunction();\n     newFunction.setDbName(\"no_such_database\");\n-\n-    try {\n-      client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n-      // TODO: Should have a check on the server side. Embedded metastore throws\n-      // InvalidObjectException, remote throws TApplicationException\n-      Assert.fail(\"Expected an InvalidObjectException or TApplicationException to be thrown\");\n-    } catch (InvalidObjectException exception) {\n-      // Expected exception - Embedded MetaStore\n-      exception.printStackTrace();\n-    } catch (TApplicationException exception) {\n-      // Expected exception - Remote MetaStore\n-      exception.printStackTrace();\n-    }\n+    client.alterFunction(DEFAULT_DATABASE, \"test_function_to_find_2\", newFunction);\n   }\n \n   @Test(expected = MetaException.class)",
                "raw_url": "https://github.com/apache/hive/raw/820db608f2878dde1d9c7b3fa3fbfdb3564710d6/standalone-metastore/src/test/java/org/apache/hadoop/hive/metastore/client/TestFunctions.java",
                "sha": "b5705f90a072b2f5da7a04bdf68ea81cf680f5b0",
                "status": "modified"
            }
        ],
        "message": "HIVE-19076: Fix NPE and TApplicationException in function related HiveMetastore methods (Marta Kuczora, via Peter Vary)",
        "parent": "https://github.com/apache/hive/commit/5eed779c611c7c766b69f992d76683c58b5772c9",
        "patched_files": [
            "HiveMetaStore.java",
            "HiveMetaStoreClient.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveMetaStore.java",
            "TestFunctions.java"
        ]
    },
    "hive_82b84ac": {
        "bug_id": "hive_82b84ac",
        "commit": "https://github.com/apache/hive/commit/82b84ac766f54c72a76d5e7fe8fd4fe67d264fe7",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/82b84ac766f54c72a76d5e7fe8fd4fe67d264fe7/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java?ref=82b84ac766f54c72a76d5e7fe8fd4fe67d264fe7",
                "deletions": 0,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java",
                "patch": "@@ -2661,6 +2661,10 @@ public GetHelper(String dbName, String tblName, boolean allowSql, boolean allowJ\n       // the fallback from failed SQL to JDO is not possible.\n       boolean isConfigEnabled = HiveConf.getBoolVar(getConf(), ConfVars.METASTORE_TRY_DIRECT_SQL)\n           && (HiveConf.getBoolVar(getConf(), ConfVars.METASTORE_TRY_DIRECT_SQL_DDL) || !isInTxn);\n+      if (isConfigEnabled && directSql == null) {\n+        directSql = new MetaStoreDirectSql(pm, getConf());\n+      }\n+\n       if (!allowJdo && isConfigEnabled && !directSql.isCompatibleDatastore()) {\n         throw new MetaException(\"SQL is not operational\"); // test path; SQL is enabled and broken.\n       }",
                "raw_url": "https://github.com/apache/hive/raw/82b84ac766f54c72a76d5e7fe8fd4fe67d264fe7/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java",
                "sha": "82de857d221889d2c62252aabc0741a83d5f5313",
                "status": "modified"
            }
        ],
        "message": "HIVE-14173: NPE was thrown after enabling directsql in the middle of session (Chaoyu Tang, reviewed by Sergey Shelukhin)",
        "parent": "https://github.com/apache/hive/commit/fad946bb2c11a9159b78426865975c0782e5f663",
        "patched_files": [
            "ObjectStore.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestObjectStore.java"
        ]
    },
    "hive_838da8c": {
        "bug_id": "hive_838da8c",
        "commit": "https://github.com/apache/hive/commit/838da8cb41b6b32b19b0ae8b793d5b7d4e6f613c",
        "file": [
            {
                "additions": 66,
                "blob_url": "https://github.com/apache/hive/blob/838da8cb41b6b32b19b0ae8b793d5b7d4e6f613c/llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java",
                "changes": 87,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java?ref=838da8cb41b6b32b19b0ae8b793d5b7d4e6f613c",
                "deletions": 21,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java",
                "patch": "@@ -118,6 +118,9 @@ public void allocateMultiple(LlapMemoryBuffer[] dest, int size)\n     }\n     // First try to quickly lock some of the correct-sized free lists and allocate from them.\n     int arenaCount = allocatedArenas.get();\n+    if (arenaCount < 0) {\n+      arenaCount = -arenaCount - 1; // Next arena is being allocated.\n+    }\n     long threadId = arenaCount > 1 ? Thread.currentThread().getId() : 0;\n     {\n       int startIndex = (int)(threadId % arenaCount), index = startIndex;\n@@ -317,18 +320,21 @@ private int allocateWithSplit(int arenaIx, int freeListIx,\n       FreeList freeList = freeLists[freeListIx];\n       int remaining = -1;\n       freeList.lock.lock();\n-      // TODO: write some comments for this method\n       try {\n+        // Try to allocate from target-sized free list, maybe we'll get lucky.\n         ix = allocateFromFreeListUnderLock(\n             arenaIx, freeList, freeListIx, dest, ix, allocationSize);\n         remaining = dest.length - ix;\n         if (remaining == 0) return ix;\n       } finally {\n         freeList.lock.unlock();\n       }\n-      byte headerData = makeHeader(freeListIx, true);\n-      int headerStep = 1 << freeListIx;\n-      int splitListIx = freeListIx + 1;\n+      byte headerData = makeHeader(freeListIx, true); // Header for newly allocated used blocks.\n+      int headerStep = 1 << freeListIx; // Number of headers (smallest blocks) per target block.\n+      int splitListIx = freeListIx + 1; // Next free list from which we will be splitting.\n+      // Each iteration of this loop tries to split blocks from one level of the free list into\n+      // target size blocks; if we cannot satisfy the allocation from the free list containing the\n+      // blocks of a particular size, we'll try to split yet larger blocks, until we run out.\n       while (remaining > 0 && splitListIx < freeLists.length) {\n         int splitWaysLog2 = (splitListIx - freeListIx);\n         assert splitWaysLog2 > 0;\n@@ -338,28 +344,33 @@ private int allocateWithSplit(int arenaIx, int freeListIx,\n         FreeList splitList = freeLists[splitListIx];\n         splitList.lock.lock();\n         try {\n-          int headerIx = splitList.listHead;\n+          int headerIx = splitList.listHead; // Index of the next free block to split.\n           while (headerIx >= 0 && remaining > 0) {\n             int origOffset = offsetFromHeaderIndex(headerIx), offset = origOffset;\n-            int toTake = Math.min(splitWays, remaining); // We split it splitWays and take toTake.\n+            // We will split the block at headerIx [splitWays] ways, and take [toTake] blocks,\n+            // which will leave [lastSplitBlocksRemaining] free blocks of target size.\n+            int toTake = Math.min(splitWays, remaining);\n             remaining -= toTake;\n             lastSplitBlocksRemaining = splitWays - toTake; // Whatever remains.\n-            // Take toTake blocks by splitting the block at origOffset.\n+            // Take toTake blocks by splitting the block at offset.\n             for (; toTake > 0; ++ix, --toTake, headerIx += headerStep, offset += allocationSize) {\n               headers[headerIx] = headerData;\n               // TODO: this could be done out of the lock, we only need to take the blocks out.\n               ((LlapDataBuffer)dest[ix]).initialize(arenaIx, data, offset, allocationSize);\n             }\n             lastSplitNextHeader = headerIx; // If anything remains, this is where it starts.\n-            headerIx = data.getInt(origOffset + 4); // Get next item from the free list.\n+            headerIx = getNextFreeListItem(origOffset);\n           }\n           replaceListHeadUnderLock(splitList, headerIx); // In the end, update free list head.\n         } finally {\n           splitList.lock.unlock();\n         }\n         if (remaining == 0) {\n-          // We have just obtained all we needed by splitting at lastSplitBlockOffset; now\n-          // we need to put the space remaining from that block into lower free lists.\n+          // We have just obtained all we needed by splitting some block; now we need\n+          // to put the space remaining from that block into lower free lists.\n+          // We'll put at most one block into each list, since 2 blocks can always be combined\n+          // to make a larger-level block. Each bit in the remaining target-sized blocks count\n+          // is one block in a list offset from target-sized list by bit index.\n           int newListIndex = freeListIx;\n           while (lastSplitBlocksRemaining > 0) {\n             if ((lastSplitBlocksRemaining & 1) == 1) {\n@@ -394,17 +405,43 @@ private void replaceListHeadUnderLock(FreeList freeList, int headerIx) {\n \n     private int allocateWithExpand(\n         int arenaIx, int freeListIx, LlapMemoryBuffer[] dest, int ix, int size) {\n-      if (data == null) {\n-        synchronized (this) {\n-          // Never goes from non-null to null, so this is the only place we need sync.\n-          if (data == null) {\n-            init();\n-            allocatedArenas.incrementAndGet();\n-            metrics.incrAllocatedArena();\n+      while (true) {\n+        int arenaCount = allocatedArenas.get(), allocArenaCount = arenaCount;\n+        if (arenaCount < 0)  {\n+          allocArenaCount = -arenaCount - 1; // Someone is allocating an arena.\n+        }\n+        if (allocArenaCount > arenaIx) {\n+          // Someone already allocated this arena; just do the usual thing.\n+          return allocateWithSplit(arenaIx, freeListIx, dest, ix, size);\n+        }\n+        if ((arenaIx + 1) == -arenaCount) {\n+          // Someone is allocating this arena. Wait a bit and recheck.\n+          try {\n+            synchronized (this) {\n+              this.wait(100);\n+            }\n+          } catch (InterruptedException e) {\n+            Thread.currentThread().interrupt(); // Restore interrupt, won't handle here.\n           }\n+          continue;\n         }\n+        // Either this arena is being allocated, or it is already allocated, or it is next. The\n+        // caller should not try to allocate another arena before waiting for the previous one.\n+        assert arenaCount == arenaIx :\n+          \"Arena count \" + arenaCount + \" but \" + arenaIx + \" is not being allocated\";\n+        if (!allocatedArenas.compareAndSet(arenaCount, -arenaCount - 1)) {\n+          continue; // CAS race, look again.\n+        }\n+        assert data == null;\n+        init();\n+        boolean isCommited = allocatedArenas.compareAndSet(-arenaCount - 1, arenaCount + 1);\n+        assert isCommited;\n+        synchronized (this) {\n+          this.notifyAll();\n+        }\n+        metrics.incrAllocatedArena();\n+        return allocateWithSplit(arenaIx, freeListIx, dest, ix, size);\n       }\n-      return allocateWithSplit(arenaIx, freeListIx, dest, ix, size);\n     }\n \n     public int offsetFromHeaderIndex(int lastSplitNextHeader) {\n@@ -418,14 +455,22 @@ public int allocateFromFreeListUnderLock(int arenaIx, FreeList freeList,\n         int offset = offsetFromHeaderIndex(current);\n         // Noone else has this either allocated or in a different free list; no sync needed.\n         headers[current] = makeHeader(freeListIx, true);\n-        current = data.getInt(offset + 4);\n+        current = getNextFreeListItem(offset);\n         ((LlapDataBuffer)dest[ix]).initialize(arenaIx, data, offset, size);\n         ++ix;\n       }\n       replaceListHeadUnderLock(freeList, current);\n       return ix;\n     }\n \n+    private int getPrevFreeListItem(int offset) {\n+      return data.getInt(offset);\n+    }\n+\n+    private int getNextFreeListItem(int offset) {\n+      return data.getInt(offset + 4);\n+    }\n+\n     private byte makeHeader(int freeListIx, boolean isInUse) {\n       return (byte)(((freeListIx + 1) << 1) | (isInUse ? 1 : 0));\n     }\n@@ -462,7 +507,7 @@ public void deallocate(LlapDataBuffer buffer) {\n     private void addBlockToFreeListUnderLock(FreeList freeList, int headerIx) {\n       if (freeList.listHead >= 0) {\n         int oldHeadOffset = offsetFromHeaderIndex(freeList.listHead);\n-        assert data.getInt(oldHeadOffset) == -1;\n+        assert getPrevFreeListItem(oldHeadOffset) == -1;\n         data.putInt(oldHeadOffset, headerIx);\n       }\n       int offset = offsetFromHeaderIndex(headerIx);\n@@ -473,7 +518,7 @@ private void addBlockToFreeListUnderLock(FreeList freeList, int headerIx) {\n \n     private void removeBlockFromFreeList(FreeList freeList, int headerIx) {\n       int bOffset = offsetFromHeaderIndex(headerIx),\n-          bpHeaderIx = data.getInt(bOffset), bnHeaderIx = data.getInt(bOffset + 4);\n+          bpHeaderIx = getPrevFreeListItem(bOffset), bnHeaderIx = getNextFreeListItem(bOffset);\n       if (freeList.listHead == headerIx) {\n         assert bpHeaderIx == -1;\n         freeList.listHead = bnHeaderIx;",
                "raw_url": "https://github.com/apache/hive/raw/838da8cb41b6b32b19b0ae8b793d5b7d4e6f613c/llap-server/src/java/org/apache/hadoop/hive/llap/cache/BuddyAllocator.java",
                "sha": "fca624917d58800d2a4ef354bcfe2f30bc29b402",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hive/blob/838da8cb41b6b32b19b0ae8b793d5b7d4e6f613c/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestBuddyAllocator.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestBuddyAllocator.java?ref=838da8cb41b6b32b19b0ae8b793d5b7d4e6f613c",
                "deletions": 0,
                "filename": "llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestBuddyAllocator.java",
                "patch": "@@ -136,6 +136,37 @@ public Void call() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testMTTArenas() {\n+    final int min = 3, max = 4, maxAlloc = 1 << max, minAllocCount = 2048, threadCount = 4;\n+    Configuration conf = createConf(1 << min, maxAlloc, maxAlloc, (1 << min) * minAllocCount);\n+    final BuddyAllocator a = new BuddyAllocator(conf, new DummyMemoryManager(),\n+        LlapDaemonCacheMetrics.create(\"test\", \"1\"));\n+    ExecutorService executor = Executors.newFixedThreadPool(threadCount);\n+    final CountDownLatch cdlIn = new CountDownLatch(threadCount), cdlOut = new CountDownLatch(1);\n+    Callable<Void> testCallable = new Callable<Void>() {\n+      public Void call() throws Exception {\n+        syncThreadStart(cdlIn, cdlOut);\n+        allocSameSize(a, minAllocCount / threadCount, min);\n+        return null;\n+      }\n+    };\n+    @SuppressWarnings(\"unchecked\")\n+    FutureTask<Void>[] allocTasks = new FutureTask[threadCount];\n+    for (int i = 0; i < threadCount; ++i) {\n+      allocTasks[i] = new FutureTask<>(testCallable);\n+      executor.execute(allocTasks[i]);\n+    }\n+    try {\n+      cdlIn.await(); // Wait for all threads to be ready.\n+      cdlOut.countDown(); // Release them at the same time.\n+      for (int i = 0; i < threadCount; ++i) {\n+        allocTasks[i].get();\n+      }\n+    } catch (Throwable t) {\n+      throw new RuntimeException(t);\n+    }\n+  }\n   private void syncThreadStart(final CountDownLatch cdlIn, final CountDownLatch cdlOut) {\n     cdlIn.countDown();\n     try {",
                "raw_url": "https://github.com/apache/hive/raw/838da8cb41b6b32b19b0ae8b793d5b7d4e6f613c/llap-server/src/test/org/apache/hadoop/hive/llap/cache/TestBuddyAllocator.java",
                "sha": "50d5e19094ad8960d61dcc183bc078d33599387a",
                "status": "modified"
            }
        ],
        "message": "HIVE-11200 : LLAP: Cache BuddyAllocator throws NPE (Sergey Shelukhin)",
        "parent": "https://github.com/apache/hive/commit/3b8fdc1dc7228fbd9107ee003d9dbe9e2dfb8692",
        "patched_files": [
            "BuddyAllocator.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestBuddyAllocator.java"
        ]
    },
    "hive_8412b37": {
        "bug_id": "hive_8412b37",
        "commit": "https://github.com/apache/hive/commit/8412b3748b7b384689a39b8747c7f8fd41e58f28",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/8412b3748b7b384689a39b8747c7f8fd41e58f28/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java?ref=8412b3748b7b384689a39b8747c7f8fd41e58f28",
                "deletions": 3,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java",
                "patch": "@@ -84,22 +84,26 @@ public void testCreateFunctionIncrementalReplication() throws Throwable {\n \n     primary.run(\"CREATE FUNCTION \" + primaryDbName\n         + \".testFunctionOne as 'hivemall.tools.string.StopwordUDF' \"\n-        + \"using jar  'ivy://io.github.myui:hivemall:0.4.0-2'\");\n+        + \"using jar  'ivy://io.github.myui:hivemall:0.4.0-2'\")\n+        .run(\"CREATE FUNCTION \" + primaryDbName\n+            + \".testFunctionTwo as 'org.apache.hadoop.hive.ql.udf.generic.GenericUDAFMax'\");\n \n     WarehouseInstance.Tuple incrementalDump =\n         primary.dump(primaryDbName, bootStrapDump.lastReplicationId);\n     replica.load(replicatedDbName, incrementalDump.dumpLocation)\n         .run(\"REPL STATUS \" + replicatedDbName)\n         .verifyResult(incrementalDump.lastReplicationId)\n         .run(\"SHOW FUNCTIONS LIKE '\" + replicatedDbName + \"%'\")\n-        .verifyResult(replicatedDbName + \".testFunctionOne\");\n+        .verifyResults(new String[] { replicatedDbName + \".testFunctionOne\",\n+                                      replicatedDbName + \".testFunctionTwo\" });\n \n     // Test the idempotent behavior of CREATE FUNCTION\n     replica.load(replicatedDbName, incrementalDump.dumpLocation)\n         .run(\"REPL STATUS \" + replicatedDbName)\n         .verifyResult(incrementalDump.lastReplicationId)\n         .run(\"SHOW FUNCTIONS LIKE '\" + replicatedDbName + \"%'\")\n-        .verifyResult(replicatedDbName + \".testFunctionOne\");\n+        .verifyResults(new String[] { replicatedDbName + \".testFunctionOne\",\n+                                      replicatedDbName + \".testFunctionTwo\" });\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hive/raw/8412b3748b7b384689a39b8747c7f8fd41e58f28/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java",
                "sha": "af5746ff48943d6e5808a861cc56ea2cf810432b",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hive/blob/8412b3748b7b384689a39b8747c7f8fd41e58f28/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FunctionSerializer.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FunctionSerializer.java?ref=8412b3748b7b384689a39b8747c7f8fd41e58f28",
                "deletions": 12,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FunctionSerializer.java",
                "patch": "@@ -51,18 +51,20 @@ public void writeTo(JsonWriter writer, ReplicationSpec additionalPropertiesProvi\n       throws SemanticException, IOException, MetaException {\n     TSerializer serializer = new TSerializer(new TJSONProtocol.Factory());\n     List<ResourceUri> resourceUris = new ArrayList<>();\n-    for (ResourceUri uri : function.getResourceUris()) {\n-      Path inputPath = new Path(uri.getUri());\n-      if (\"hdfs\".equals(inputPath.toUri().getScheme())) {\n-        FileSystem fileSystem = inputPath.getFileSystem(hiveConf);\n-        Path qualifiedUri = PathBuilder.fullyQualifiedHDFSUri(inputPath, fileSystem);\n-        // Initialize ReplChangeManager instance since we will require it to encode file URI.\n-        ReplChangeManager.getInstance(hiveConf);\n-        String checkSum = ReplChangeManager.checksumFor(qualifiedUri, fileSystem);\n-        String newFileUri = ReplChangeManager.encodeFileUri(qualifiedUri.toString(), checkSum, null);\n-        resourceUris.add(new ResourceUri(uri.getResourceType(), newFileUri));\n-      } else {\n-        resourceUris.add(uri);\n+    if (function.getResourceUris() != null) {\n+      for (ResourceUri uri : function.getResourceUris()) {\n+        Path inputPath = new Path(uri.getUri());\n+        if (\"hdfs\".equals(inputPath.toUri().getScheme())) {\n+          FileSystem fileSystem = inputPath.getFileSystem(hiveConf);\n+          Path qualifiedUri = PathBuilder.fullyQualifiedHDFSUri(inputPath, fileSystem);\n+          // Initialize ReplChangeManager instance since we will require it to encode file URI.\n+          ReplChangeManager.getInstance(hiveConf);\n+          String checkSum = ReplChangeManager.checksumFor(qualifiedUri, fileSystem);\n+          String newFileUri = ReplChangeManager.encodeFileUri(qualifiedUri.toString(), checkSum, null);\n+          resourceUris.add(new ResourceUri(uri.getResourceType(), newFileUri));\n+        } else {\n+          resourceUris.add(uri);\n+        }\n       }\n     }\n     Function copyObj = new Function(this.function);",
                "raw_url": "https://github.com/apache/hive/raw/8412b3748b7b384689a39b8747c7f8fd41e58f28/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/dump/io/FunctionSerializer.java",
                "sha": "576eb0699a8aa263f2a4c083a09e5bfef6f3fffc",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/8412b3748b7b384689a39b8747c7f8fd41e58f28/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateFunctionHandler.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateFunctionHandler.java?ref=8412b3748b7b384689a39b8747c7f8fd41e58f28",
                "deletions": 3,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateFunctionHandler.java",
                "patch": "@@ -129,9 +129,9 @@ private CreateFunctionDesc build() throws SemanticException {\n       // and not do them lazily. The reason being the function class used for transformations additionally\n       // also creates the corresponding replCopyTasks, which cannot be evaluated lazily. since the query\n       // plan needs to be complete before we execute and not modify it while execution in the driver.\n-      List<ResourceUri> transformedUris = ImmutableList.copyOf(\n-          Lists.transform(metadata.function.getResourceUris(), conversionFunction)\n-      );\n+      List<ResourceUri> transformedUris = (metadata.function.getResourceUris() == null)\n+              ? null\n+              : ImmutableList.copyOf(Lists.transform(metadata.function.getResourceUris(), conversionFunction));\n       replCopyTasks.addAll(conversionFunction.replCopyTasks);\n       String fullQualifiedFunctionName = FunctionUtils.qualifyFunctionName(\n           metadata.function.getFunctionName(), destinationDbName",
                "raw_url": "https://github.com/apache/hive/raw/8412b3748b7b384689a39b8747c7f8fd41e58f28/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/CreateFunctionHandler.java",
                "sha": "bc891f710a8bbfda05b9e80e2a3bfc5e94590e0d",
                "status": "modified"
            }
        ],
        "message": "HIVE-21992: REPL DUMP throws NPE when dumping Create Function event (Sankar Hariappan, reviewed by Mahesh Kumar Behera)\n\nSigned-off-by: Sankar Hariappan <sankarh@apache.org>",
        "parent": "https://github.com/apache/hive/commit/198ab0e862cdf33e8dff37bf24676ffcb392ed82",
        "patched_files": [
            "CreateFunctionHandler.java",
            "FunctionSerializer.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestReplicationScenariosAcrossInstances.java"
        ]
    },
    "hive_88e86b9": {
        "bug_id": "hive_88e86b9",
        "commit": "https://github.com/apache/hive/commit/88e86b98835b949f8410259c6dab0c802233bb73",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/88e86b98835b949f8410259c6dab0c802233bb73/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java?ref=88e86b98835b949f8410259c6dab0c802233bb73",
                "deletions": 1,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java",
                "patch": "@@ -1493,7 +1493,7 @@ public static Path getOriginalLocation(\n   }\n \n   public static boolean isNonNativeTable(Table table) {\n-    if (table == null) {\n+    if (table == null || table.getParameters() == null) {\n       return false;\n     }\n     return (table.getParameters().get(hive_metastoreConstants.META_TABLE_STORAGE) != null);",
                "raw_url": "https://github.com/apache/hive/raw/88e86b98835b949f8410259c6dab0c802233bb73/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java",
                "sha": "05ee3ace1e22c9da78f8f1441a6510a279b4db71",
                "status": "modified"
            }
        ],
        "message": "HIVE-15329: NullPointerException might occur when create table (Meilong Huang reviewed by Prasanth Jayachandran)",
        "parent": "https://github.com/apache/hive/commit/e43861a1023920b23c126733ceca273bc766c58a",
        "patched_files": [
            "MetaStoreUtils.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestMetaStoreUtils.java"
        ]
    },
    "hive_89e51e4": {
        "bug_id": "hive_89e51e4",
        "commit": "https://github.com/apache/hive/commit/89e51e4eb47c6b505bce1928d89c0e7e2975431c",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/89e51e4eb47c6b505bce1928d89c0e7e2975431c/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java?ref=89e51e4eb47c6b505bce1928d89c0e7e2975431c",
                "deletions": 22,
                "filename": "itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java",
                "patch": "@@ -43,7 +43,6 @@\n import java.util.Arrays;\n import java.util.Collection;\n import java.util.Deque;\n-import java.util.HashMap;\n import java.util.HashSet;\n import java.util.LinkedList;\n import java.util.List;\n@@ -64,19 +63,17 @@\n import org.apache.hadoop.hbase.zookeeper.MiniZooKeeperCluster;\n import org.apache.hadoop.hive.cli.CliDriver;\n import org.apache.hadoop.hive.cli.CliSessionState;\n+import org.apache.hadoop.hive.common.io.CachingPrintStream;\n import org.apache.hadoop.hive.common.io.DigestPrintStream;\n import org.apache.hadoop.hive.common.io.SortAndDigestPrintStream;\n import org.apache.hadoop.hive.common.io.SortPrintStream;\n-import org.apache.hadoop.hive.common.io.CachingPrintStream;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n import org.apache.hadoop.hive.metastore.MetaStoreUtils;\n import org.apache.hadoop.hive.metastore.api.Index;\n import org.apache.hadoop.hive.ql.exec.FunctionRegistry;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.Utilities;\n-import org.apache.hadoop.hive.ql.exec.vector.util.AllVectorTypesRecord;\n-import org.apache.hadoop.hive.ql.io.IgnoreKeyTextOutputFormat;\n import org.apache.hadoop.hive.ql.lockmgr.zookeeper.ZooKeeperHiveLockManager;\n import org.apache.hadoop.hive.ql.metadata.Hive;\n import org.apache.hadoop.hive.ql.metadata.Table;\n@@ -87,22 +84,14 @@\n import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;\n import org.apache.hadoop.hive.ql.parse.SemanticException;\n import org.apache.hadoop.hive.ql.session.SessionState;\n-import org.apache.hadoop.hive.serde.serdeConstants;\n-import org.apache.hadoop.hive.serde2.thrift.ThriftDeserializer;\n-import org.apache.hadoop.hive.serde2.thrift.test.Complex;\n import org.apache.hadoop.hive.shims.HadoopShims;\n import org.apache.hadoop.hive.shims.ShimLoader;\n-import org.apache.hadoop.mapred.SequenceFileInputFormat;\n-import org.apache.hadoop.mapred.SequenceFileOutputFormat;\n-import org.apache.hadoop.mapred.TextInputFormat;\n import org.apache.hadoop.util.Shell;\n import org.apache.hive.common.util.StreamPrinter;\n-import org.apache.thrift.protocol.TBinaryProtocol;\n import org.apache.tools.ant.BuildException;\n import org.apache.zookeeper.WatchedEvent;\n import org.apache.zookeeper.Watcher;\n import org.apache.zookeeper.ZooKeeper;\n-import org.junit.Assume;\n \n import com.google.common.collect.ImmutableList;\n \n@@ -145,8 +134,8 @@\n   private QTestSetup setup = null;\n   private boolean isSessionStateStarted = false;\n \n-  private String initScript;\n-  private String cleanupScript;\n+  private final String initScript;\n+  private final String cleanupScript;\n \n   static {\n     for (String srcTable : System.getProperty(\"test.src.tables\", \"\").trim().split(\",\")) {\n@@ -332,14 +321,6 @@ public QTestUtil(String outDir, String logDir, MiniClusterType clusterType,\n     HadoopShims shims = ShimLoader.getHadoopShims();\n     int numberOfDataNodes = 4;\n \n-    // can run tez tests only on hadoop 2\n-    if (clusterType == MiniClusterType.tez) {\n-      Assume.assumeTrue(ShimLoader.getMajorVersion().equals(\"0.23\"));\n-      // this is necessary temporarily - there's a probem with multi datanodes on MiniTezCluster\n-      // will be fixed in 0.3\n-      numberOfDataNodes = 1;\n-    }\n-\n     if (clusterType != MiniClusterType.none) {\n       dfs = shims.getMiniDfs(conf, numberOfDataNodes, true, null);\n       FileSystem fs = dfs.getFileSystem();",
                "raw_url": "https://github.com/apache/hive/raw/89e51e4eb47c6b505bce1928d89c0e7e2975431c/itests/util/src/main/java/org/apache/hadoop/hive/ql/QTestUtil.java",
                "sha": "78ea21dbf88ef3c5337b97a31b1422bc1202c374",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hive/blob/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java?ref=89e51e4eb47c6b505bce1928d89c0e7e2975431c",
                "deletions": 5,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "patch": "@@ -346,7 +346,7 @@ private EdgeProperty createEdgeProperty(TezEdgeProperty edgeProp, Configuration\n \n   /**\n    * Utility method to create a stripped down configuration for the MR partitioner.\n-   * \n+   *\n    * @param partitionerClassName\n    *          the real MR partitioner class name\n    * @param baseConf\n@@ -427,7 +427,7 @@ private Vertex createVertex(JobConf conf, MapWork mapWork,\n \n     // use tez to combine splits\n     boolean groupSplitsInInputInitializer;\n-    \n+\n     DataSourceDescriptor dataSource;\n \n     int numTasks = -1;\n@@ -462,11 +462,12 @@ private Vertex createVertex(JobConf conf, MapWork mapWork,\n       }\n     }\n \n-    // set up the operator plan. Before setting up Inputs since the config is updated.\n-    Utilities.setMapWork(conf, mapWork, mrScratchDir, false);\n-    \n     if (HiveConf.getBoolVar(conf, ConfVars.HIVE_AM_SPLIT_GENERATION)\n         && !mapWork.isUseOneNullRowInputFormat()) {\n+\n+      // set up the operator plan. (before setting up splits on the AM)\n+      Utilities.setMapWork(conf, mapWork, mrScratchDir, false);\n+\n       // if we're generating the splits in the AM, we just need to set\n       // the correct plugin.\n       if (groupSplitsInInputInitializer) {\n@@ -484,6 +485,9 @@ private Vertex createVertex(JobConf conf, MapWork mapWork,\n       dataSource = MRInputHelpers.configureMRInputWithLegacySplitGeneration(conf, new Path(tezDir,\n           \"split_\" + mapWork.getName().replaceAll(\" \", \"_\")), true);\n       numTasks = dataSource.getNumberOfShards();\n+\n+      // set up the operator plan. (after generating splits - that changes configs)\n+      Utilities.setMapWork(conf, mapWork, mrScratchDir, false);\n     }\n \n     UserPayload serializedConf = TezUtils.createUserPayloadFromConf(conf);",
                "raw_url": "https://github.com/apache/hive/raw/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java",
                "sha": "1ef6cc5340096bd7b692941151843c0dd8405f69",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hive/blob/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java?ref=89e51e4eb47c6b505bce1928d89c0e7e2975431c",
                "deletions": 15,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java",
                "patch": "@@ -64,6 +64,23 @@\n   protected static final String MAP_PLAN_KEY = \"__MAP_PLAN__\";\n   private MapWork mapWork;\n \n+  public MapRecordProcessor(JobConf jconf) {\n+    ObjectCache cache = ObjectCacheFactory.getCache(jconf);\n+    execContext.setJc(jconf);\n+    // create map and fetch operators\n+    mapWork = (MapWork) cache.retrieve(MAP_PLAN_KEY);\n+    if (mapWork == null) {\n+      mapWork = Utilities.getMapWork(jconf);\n+      cache.cache(MAP_PLAN_KEY, mapWork);\n+      l4j.info(\"Plan: \"+mapWork);\n+      for (String s: mapWork.getAliases()) {\n+        l4j.info(\"Alias: \"+s);\n+      }\n+    } else {\n+      Utilities.setMapWork(jconf, mapWork);\n+    }\n+  }\n+\n   @Override\n   void init(JobConf jconf, ProcessorContext processorContext, MRTaskReporter mrReporter,\n       Map<String, LogicalInput> inputs, Map<String, LogicalOutput> outputs) throws Exception {\n@@ -87,22 +104,7 @@ void init(JobConf jconf, ProcessorContext processorContext, MRTaskReporter mrRep\n       ((TezKVOutputCollector) outMap.get(outputEntry.getKey())).initialize();\n     }\n \n-    ObjectCache cache = ObjectCacheFactory.getCache(jconf);\n     try {\n-\n-      execContext.setJc(jconf);\n-      // create map and fetch operators\n-      mapWork = (MapWork) cache.retrieve(MAP_PLAN_KEY);\n-      if (mapWork == null) {\n-        mapWork = Utilities.getMapWork(jconf);\n-        cache.cache(MAP_PLAN_KEY, mapWork);\n-        l4j.info(\"Plan: \"+mapWork);\n-        for (String s: mapWork.getAliases()) {\n-          l4j.info(\"Alias: \"+s);\n-        }\n-      } else {\n-        Utilities.setMapWork(jconf, mapWork);\n-      }\n       if (mapWork.getVectorMode()) {\n         mapOp = new VectorMapOperator();\n       } else {",
                "raw_url": "https://github.com/apache/hive/raw/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java",
                "sha": "7556d7bfba81ac67db511e423c634e4125697e10",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java?ref=89e51e4eb47c6b505bce1928d89c0e7e2975431c",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java",
                "patch": "@@ -130,7 +130,7 @@ public void run(Map<String, LogicalInput> inputs, Map<String, LogicalOutput> out\n       LOG.info(\"Running task: \" + getContext().getUniqueIdentifier());\n \n       if (isMap) {\n-        rproc = new MapRecordProcessor();\n+        rproc = new MapRecordProcessor(jobConf);\n         MRInputLegacy mrInput = getMRInput(inputs);\n         try {\n           mrInput.init();\n@@ -201,6 +201,7 @@ void initialize() throws Exception {\n       this.writer = (KeyValueWriter) output.getWriter();\n     }\n \n+    @Override\n     public void collect(Object key, Object value) throws IOException {\n       writer.write(key, value);\n     }",
                "raw_url": "https://github.com/apache/hive/raw/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java",
                "sha": "8b023dcd6a27251dda528b7e82946d850e5e1802",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java?ref=89e51e4eb47c6b505bce1928d89c0e7e2975431c",
                "deletions": 2,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "patch": "@@ -134,7 +134,7 @@ public int execute(DriverContext driverContext) {\n       }\n \n       List<LocalResource> additionalLr = session.getLocalizedResources();\n-      \n+\n       // log which resources we're adding (apart from the hive exec)\n       if (LOG.isDebugEnabled()) {\n         if (additionalLr == null || additionalLr.size() == 0) {\n@@ -165,7 +165,7 @@ public int execute(DriverContext driverContext) {\n       counters = client.getDAGStatus(statusGetOpts).getDAGCounters();\n       TezSessionPoolManager.getInstance().returnSession(session);\n \n-      if (LOG.isInfoEnabled()) {\n+      if (LOG.isInfoEnabled() && counters != null) {\n         for (CounterGroup group: counters) {\n           LOG.info(group.getDisplayName() +\":\");\n           for (TezCounter counter: group) {",
                "raw_url": "https://github.com/apache/hive/raw/89e51e4eb47c6b505bce1928d89c0e7e2975431c/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "sha": "f4da332875be221cf668483f4b82bd5ebf32c39e",
                "status": "modified"
            }
        ],
        "message": "HIVE-7891: Fix NPE in split generation on Tez 0.5 (Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/tez@1619739 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/0754c55b488ba2795b61f1778ec617bd5c88aac8",
        "patched_files": [
            "TezTask.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestTezTask.java"
        ]
    },
    "hive_8b0b83f": {
        "bug_id": "hive_8b0b83f",
        "commit": "https://github.com/apache/hive/commit/8b0b83fd57553b4cb52129ff36c398e18230b649",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/8b0b83fd57553b4cb52129ff36c398e18230b649/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=8b0b83fd57553b4cb52129ff36c398e18230b649",
                "deletions": 2,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "patch": "@@ -4706,7 +4706,8 @@ public boolean drop_partition_by_name_with_environment_context(final String db_n\n     @Override\n     public Index add_index(final Index newIndex, final Table indexTable)\n         throws InvalidObjectException, AlreadyExistsException, MetaException, TException {\n-      startFunction(\"add_index\", \": \" + newIndex.toString() + \" \" + indexTable.toString());\n+      String tableName = indexTable != null ? indexTable.getTableName() : \"\";\n+      startFunction(\"add_index\", \": \" + newIndex.toString() + \" \" + tableName);\n       Index ret = null;\n       Exception ex = null;\n       try {\n@@ -4725,7 +4726,6 @@ public Index add_index(final Index newIndex, final Table indexTable)\n           throw newMetaException(e);\n         }\n       } finally {\n-        String tableName = indexTable != null ? indexTable.getTableName() : null;\n         endFunction(\"add_index\", ret != null, ex, tableName);\n       }\n       return ret;",
                "raw_url": "https://github.com/apache/hive/raw/8b0b83fd57553b4cb52129ff36c398e18230b649/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "sha": "58b9044930046758a83ee499692e5593cd82f9e0",
                "status": "modified"
            }
        ],
        "message": "HIVE-10495 : Hive index creation code throws NPE if index table is null (Bing Li via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>",
        "parent": "https://github.com/apache/hive/commit/6b87af7477219a3b62acb4b8ff4e614d45816d68",
        "patched_files": [
            "HiveMetaStore.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveMetaStore.java"
        ]
    },
    "hive_8b2cd2a": {
        "bug_id": "hive_8b2cd2a",
        "commit": "https://github.com/apache/hive/commit/8b2cd2abf2a32e42d24e60f1ac7a026af783dcbd",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/8b2cd2abf2a32e42d24e60f1ac7a026af783dcbd/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java?ref=8b2cd2abf2a32e42d24e60f1ac7a026af783dcbd",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java",
                "patch": "@@ -569,7 +569,7 @@ protected void createBucketFiles(FSPaths fsp) throws HiveException {\n       assert filesIdx == numFiles;\n \n       // in recent hadoop versions, use deleteOnExit to clean tmp files.\n-      if (isNativeTable) {\n+      if (isNativeTable && fs != null && fsp != null) {\n         autoDelete = fs.deleteOnExit(fsp.outPaths[0]);\n       }\n     } catch (Exception e) {",
                "raw_url": "https://github.com/apache/hive/raw/8b2cd2abf2a32e42d24e60f1ac7a026af783dcbd/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java",
                "sha": "2604d5d82fd5b4b67e6c7cb048c9840c4837d4b2",
                "status": "modified"
            }
        ],
        "message": "HIVE-11380: NPE when FileSinkOperator is not initialized (Yongzhi Chen, reviewed by Sergio Pena)",
        "parent": "https://github.com/apache/hive/commit/251991568c5e9e38b3480e9ef5dc972b9da112db",
        "patched_files": [
            "FileSinkOperator.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestFileSinkOperator.java"
        ]
    },
    "hive_90fabf1": {
        "bug_id": "hive_90fabf1",
        "commit": "https://github.com/apache/hive/commit/90fabf185f264508afd329d9e0bf847fc9ae19f1",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1",
                "deletions": 4,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java",
                "patch": "@@ -848,10 +848,10 @@ private boolean isStripeSatisfyPredicate(StripeStatistics stripeStatistics,\n \n             // column statistics at index 0 contains only the number of rows\n             ColumnStatistics stats = stripeStatistics.getColumnStatistics()[filterColumns[pred] + 1];\n-            Object minValue = getMin(stats);\n-            Object maxValue = getMax(stats);\n-            truthValues[pred] = RecordReaderImpl.evaluatePredicateRange(predLeaves.get(pred),\n-                minValue, maxValue);\n+            Object minValue = RecordReaderImpl.getMin(stats);\n+            Object maxValue = RecordReaderImpl.getMax(stats);\n+            PredicateLeaf predLeaf = predLeaves.get(pred);\n+            truthValues[pred] = RecordReaderImpl.evaluatePredicateRange(predLeaf, minValue, maxValue);\n           } else {\n \n             // parition column case.",
                "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java",
                "sha": "a425a01f0831218199fcad008855dd4d149996bc",
                "status": "modified"
            },
            {
                "additions": 99,
                "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java",
                "changes": 148,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1",
                "deletions": 49,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java",
                "patch": "@@ -31,16 +31,25 @@\n import java.util.TreeMap;\n \n import org.apache.commons.lang.builder.HashCodeBuilder;\n+import org.apache.commons.lang.StringUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FSDataInputStream;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hive.common.type.HiveChar;\n import org.apache.hadoop.hive.common.type.HiveDecimal;\n import org.apache.hadoop.hive.conf.HiveConf;\n import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.HIVE_ORC_ZEROCOPY;\n import org.apache.hadoop.hive.ql.exec.vector.*;\n+import org.apache.hadoop.hive.common.type.HiveVarchar;\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.ColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DecimalColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.VectorizedRowBatch;\n import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;\n import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;\n import org.apache.hadoop.hive.ql.io.sarg.SearchArgument.TruthValue;\n@@ -2165,57 +2174,47 @@ private static TreeReader createTreeReader(Path path,\n   }\n \n   /**\n-   * Get the minimum value out of an index entry.\n-   * @param index the index entry\n-   * @return the object for the minimum value or null if there isn't one\n+   * Get the maximum value out of an index entry.\n+   * @param index\n+   *          the index entry\n+   * @return the object for the maximum value or null if there isn't one\n    */\n-  static Object getMin(OrcProto.ColumnStatistics index) {\n-    if (index.hasIntStatistics()) {\n-      OrcProto.IntegerStatistics stat = index.getIntStatistics();\n-      if (stat.hasMinimum()) {\n-        return stat.getMinimum();\n-      }\n-    }\n-    if (index.hasStringStatistics()) {\n-      OrcProto.StringStatistics stat = index.getStringStatistics();\n-      if (stat.hasMinimum()) {\n-        return stat.getMinimum();\n-      }\n-    }\n-    if (index.hasDoubleStatistics()) {\n-      OrcProto.DoubleStatistics stat = index.getDoubleStatistics();\n-      if (stat.hasMinimum()) {\n-        return stat.getMinimum();\n-      }\n+  static Object getMax(ColumnStatistics index) {\n+    if (index instanceof IntegerColumnStatistics) {\n+      return ((IntegerColumnStatistics) index).getMaximum();\n+    } else if (index instanceof DoubleColumnStatistics) {\n+      return ((DoubleColumnStatistics) index).getMaximum();\n+    } else if (index instanceof StringColumnStatistics) {\n+      return ((StringColumnStatistics) index).getMaximum();\n+    } else if (index instanceof DateColumnStatistics) {\n+      return ((DateColumnStatistics) index).getMaximum();\n+    } else if (index instanceof DecimalColumnStatistics) {\n+      return ((DecimalColumnStatistics) index).getMaximum();\n+    } else {\n+      return null;\n     }\n-    return null;\n   }\n \n   /**\n-   * Get the maximum value out of an index entry.\n-   * @param index the index entry\n-   * @return the object for the maximum value or null if there isn't one\n+   * Get the minimum value out of an index entry.\n+   * @param index\n+   *          the index entry\n+   * @return the object for the minimum value or null if there isn't one\n    */\n-  static Object getMax(OrcProto.ColumnStatistics index) {\n-    if (index.hasIntStatistics()) {\n-      OrcProto.IntegerStatistics stat = index.getIntStatistics();\n-      if (stat.hasMaximum()) {\n-        return stat.getMaximum();\n-      }\n-    }\n-    if (index.hasStringStatistics()) {\n-      OrcProto.StringStatistics stat = index.getStringStatistics();\n-      if (stat.hasMaximum()) {\n-        return stat.getMaximum();\n-      }\n-    }\n-    if (index.hasDoubleStatistics()) {\n-      OrcProto.DoubleStatistics stat = index.getDoubleStatistics();\n-      if (stat.hasMaximum()) {\n-        return stat.getMaximum();\n-      }\n+  static Object getMin(ColumnStatistics index) {\n+    if (index instanceof IntegerColumnStatistics) {\n+      return ((IntegerColumnStatistics) index).getMinimum();\n+    } else if (index instanceof DoubleColumnStatistics) {\n+      return ((DoubleColumnStatistics) index).getMinimum();\n+    } else if (index instanceof StringColumnStatistics) {\n+      return ((StringColumnStatistics) index).getMinimum();\n+    } else if (index instanceof DateColumnStatistics) {\n+      return ((DateColumnStatistics) index).getMinimum();\n+    } else if (index instanceof DecimalColumnStatistics) {\n+      return ((DecimalColumnStatistics) index).getMinimum();\n+    } else {\n+      return null;\n     }\n-    return null;\n   }\n \n   /**\n@@ -2228,7 +2227,8 @@ static Object getMax(OrcProto.ColumnStatistics index) {\n    */\n   static TruthValue evaluatePredicate(OrcProto.ColumnStatistics index,\n                                PredicateLeaf predicate) {\n-    Object minValue = getMin(index);\n+    ColumnStatistics cs = ColumnStatisticsImpl.deserialize(index);\n+    Object minValue = getMin(cs);\n     // if we didn't have any values, everything must have been null\n     if (minValue == null) {\n       if (predicate.getOperator() == PredicateLeaf.Operator.IS_NULL) {\n@@ -2237,13 +2237,20 @@ static TruthValue evaluatePredicate(OrcProto.ColumnStatistics index,\n         return TruthValue.NULL;\n       }\n     }\n-    Object maxValue = getMax(index);\n+    Object maxValue = getMax(cs);\n     return evaluatePredicateRange(predicate, minValue, maxValue);\n   }\n \n-  static TruthValue evaluatePredicateRange(PredicateLeaf predicate, Object minValue,\n-      Object maxValue) {\n+  static TruthValue evaluatePredicateRange(PredicateLeaf predicate, Object min,\n+      Object max) {\n     Location loc;\n+\n+    // column statistics for char/varchar columns are stored as strings, so convert char/varchar\n+    // type predicates to string\n+    Object predObj = predicate.getLiteral();\n+    Object minValue = getPrimitiveObject(predObj, min);\n+    Object maxValue = getPrimitiveObject(predObj, max);\n+\n     switch (predicate.getOperator()) {\n       case NULL_SAFE_EQUALS:\n         loc = compareToRange((Comparable) predicate.getLiteral(),\n@@ -2288,6 +2295,8 @@ static TruthValue evaluatePredicateRange(PredicateLeaf predicate, Object minValu\n           // for a single value, look through to see if that value is in the\n           // set\n           for(Object arg: predicate.getLiteralList()) {\n+            minValue = getPrimitiveObject(arg, min);\n+            maxValue = getPrimitiveObject(arg, max);\n             loc = compareToRange((Comparable) arg, minValue, maxValue);\n             if (loc == Location.MIN) {\n               return TruthValue.YES_NULL;\n@@ -2297,6 +2306,8 @@ static TruthValue evaluatePredicateRange(PredicateLeaf predicate, Object minValu\n         } else {\n           // are all of the values outside of the range?\n           for(Object arg: predicate.getLiteralList()) {\n+            minValue = getPrimitiveObject(arg, min);\n+            maxValue = getPrimitiveObject(arg, max);\n             loc = compareToRange((Comparable) arg, minValue, maxValue);\n             if (loc == Location.MIN || loc == Location.MIDDLE ||\n                 loc == Location.MAX) {\n@@ -2307,9 +2318,16 @@ static TruthValue evaluatePredicateRange(PredicateLeaf predicate, Object minValu\n         }\n       case BETWEEN:\n         List<Object> args = predicate.getLiteralList();\n+        minValue = getPrimitiveObject(args.get(0), min);\n+        maxValue = getPrimitiveObject(args.get(0), max);\n+\n         loc = compareToRange((Comparable) args.get(0), minValue, maxValue);\n         if (loc == Location.BEFORE || loc == Location.MIN) {\n-          Location loc2 = compareToRange((Comparable) args.get(1), minValue,\n+          Object predObj2 = args.get(1);\n+          minValue = getPrimitiveObject(predObj2, min);\n+          maxValue = getPrimitiveObject(predObj2, max);\n+\n+          Location loc2 = compareToRange((Comparable) predObj2, minValue,\n               maxValue);\n           if (loc2 == Location.AFTER || loc2 == Location.MAX) {\n             return TruthValue.YES_NULL;\n@@ -2330,6 +2348,38 @@ static TruthValue evaluatePredicateRange(PredicateLeaf predicate, Object minValu\n     }\n   }\n \n+  private static Object getPrimitiveObject(Object predObj, Object obj) {\n+    if (obj instanceof DateWritable) {\n+      DateWritable dobj = (DateWritable) obj;\n+      if (predObj instanceof String || predObj instanceof HiveChar\n+          || predObj instanceof HiveVarchar) {\n+        return dobj.toString();\n+      }\n+    } else if (obj instanceof HiveDecimal) {\n+      HiveDecimal hdObj = (HiveDecimal) obj;\n+      if (predObj instanceof Float) {\n+        return hdObj.floatValue();\n+      } else if (predObj instanceof Double) {\n+        return hdObj.doubleValue();\n+      } else if (predObj instanceof Short) {\n+        return hdObj.shortValue();\n+      } else if (predObj instanceof Integer) {\n+        return hdObj.intValue();\n+      } else if (predObj instanceof Long) {\n+        return hdObj.longValue();\n+      } else if (predObj instanceof String || predObj instanceof HiveChar\n+          || predObj instanceof HiveVarchar) {\n+        // primitive type of char/varchar is Text (i.e trailing white spaces trimmed string)\n+        return StringUtils.stripEnd(hdObj.toString(), null);\n+      }\n+    } else if (obj instanceof String || obj instanceof HiveChar || obj instanceof HiveVarchar) {\n+      // primitive type of char/varchar is Text (i.e trailing white spaces trimmed string)\n+      return StringUtils.stripEnd(obj.toString(), null);\n+    }\n+\n+    return obj;\n+  }\n+\n   /**\n    * Pick the row groups that we need to load from the current stripe.\n    * @return an array with a boolean for each row group or null if all of the",
                "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java",
                "sha": "cfa78cb5f45f065e233b13b4fb880bd8f49e8662",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/PredicateLeaf.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/PredicateLeaf.java?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/sarg/PredicateLeaf.java",
                "patch": "@@ -45,7 +45,11 @@\n   public static enum Type {\n     INTEGER, // all of the integer types\n     FLOAT,   // float and double\n-    STRING\n+    STRING,\n+    DATE,\n+    DECIMAL,\n+    CHAR,\n+    VARCHAR\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/PredicateLeaf.java",
                "sha": "922b99f9331aa048c4b9990f428a8ed0aa991eab",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java",
                "patch": "@@ -283,11 +283,19 @@ Operator getOperator() {\n           case INT:\n           case LONG:\n             return PredicateLeaf.Type.INTEGER;\n+          case CHAR:\n+            return PredicateLeaf.Type.CHAR;\n+          case VARCHAR:\n+            return PredicateLeaf.Type.VARCHAR;\n           case STRING:\n             return PredicateLeaf.Type.STRING;\n           case FLOAT:\n           case DOUBLE:\n             return PredicateLeaf.Type.FLOAT;\n+          case DATE:\n+            return PredicateLeaf.Type.DATE;\n+          case DECIMAL:\n+            return PredicateLeaf.Type.DECIMAL;\n           default:\n         }\n       }",
                "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java",
                "sha": "4f26d3fade609d7e906ff54fa842551ce3dcd20b",
                "status": "modified"
            },
            {
                "additions": 253,
                "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestRecordReaderImpl.java",
                "changes": 287,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestRecordReaderImpl.java?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1",
                "deletions": 34,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestRecordReaderImpl.java",
                "patch": "@@ -18,21 +18,21 @@\n \n package org.apache.hadoop.hive.ql.io.orc;\n \n-import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;\n-import org.apache.hadoop.hive.ql.io.sarg.SearchArgument.TruthValue;\n-import org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl;\n-import org.junit.Test;\n-\n-import org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.Location;\n-\n-import java.util.ArrayList;\n-import java.util.List;\n-\n import static junit.framework.Assert.assertEquals;\n import static org.hamcrest.core.Is.is;\n import static org.junit.Assert.assertThat;\n import static org.junit.Assert.assertTrue;\n \n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.hadoop.hive.common.type.HiveDecimal;\n+import org.apache.hadoop.hive.ql.io.orc.RecordReaderImpl.Location;\n+import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;\n+import org.apache.hadoop.hive.ql.io.sarg.SearchArgument.TruthValue;\n+import org.apache.hadoop.hive.ql.io.sarg.TestSearchArgumentImpl;\n+import org.junit.Test;\n+\n public class TestRecordReaderImpl {\n \n   @Test\n@@ -75,26 +75,45 @@ public void testCompareToRangeString() throws Exception {\n         RecordReaderImpl.compareToRange(\"c\", \"b\", \"b\"));\n   }\n \n+  @Test\n+  public void testCompareToCharNeedConvert() throws Exception {\n+    assertEquals(Location.BEFORE,\n+        RecordReaderImpl.compareToRange(\"apple\", \"hello\", \"world\"));\n+    assertEquals(Location.AFTER,\n+        RecordReaderImpl.compareToRange(\"zombie\", \"hello\", \"world\"));\n+    assertEquals(Location.MIN,\n+        RecordReaderImpl.compareToRange(\"hello\", \"hello\", \"world\"));\n+    assertEquals(Location.MIDDLE,\n+        RecordReaderImpl.compareToRange(\"pilot\", \"hello\", \"world\"));\n+    assertEquals(Location.MAX,\n+        RecordReaderImpl.compareToRange(\"world\", \"hello\", \"world\"));\n+    assertEquals(Location.BEFORE,\n+        RecordReaderImpl.compareToRange(\"apple\", \"hello\", \"hello\"));\n+    assertEquals(Location.MIN,\n+        RecordReaderImpl.compareToRange(\"hello\", \"hello\", \"hello\"));\n+    assertEquals(Location.AFTER,\n+        RecordReaderImpl.compareToRange(\"zombie\", \"hello\", \"hello\"));\n+  }\n+\n   @Test\n   public void testGetMin() throws Exception {\n-    assertEquals(null, RecordReaderImpl.getMin(createIntStats(null, null)));\n-    assertEquals(10L, RecordReaderImpl.getMin(createIntStats(10L, 100L)));\n-    assertEquals(null, RecordReaderImpl.getMin(\n-        OrcProto.ColumnStatistics.newBuilder()\n-            .setDoubleStatistics(OrcProto.DoubleStatistics.newBuilder().build())\n-            .build()));\n-    assertEquals(10.0d, RecordReaderImpl.getMin(\n+    assertEquals(10L, RecordReaderImpl.getMin(ColumnStatisticsImpl.deserialize(createIntStats(10L, 100L))));\n+    assertEquals(10.0d, RecordReaderImpl.getMin(ColumnStatisticsImpl.deserialize(\n         OrcProto.ColumnStatistics.newBuilder()\n             .setDoubleStatistics(OrcProto.DoubleStatistics.newBuilder()\n-                .setMinimum(10.0d).setMaximum(100.0d).build()).build()));\n-    assertEquals(null, RecordReaderImpl.getMin(\n+                .setMinimum(10.0d).setMaximum(100.0d).build()).build())));\n+    assertEquals(null, RecordReaderImpl.getMin(ColumnStatisticsImpl.deserialize(\n         OrcProto.ColumnStatistics.newBuilder()\n             .setStringStatistics(OrcProto.StringStatistics.newBuilder().build())\n-            .build()));\n-    assertEquals(\"a\", RecordReaderImpl.getMin(\n+            .build())));\n+    assertEquals(\"a\", RecordReaderImpl.getMin(ColumnStatisticsImpl.deserialize(\n         OrcProto.ColumnStatistics.newBuilder()\n             .setStringStatistics(OrcProto.StringStatistics.newBuilder()\n-                .setMinimum(\"a\").setMaximum(\"b\").build()).build()));\n+                .setMinimum(\"a\").setMaximum(\"b\").build()).build())));\n+    assertEquals(\"hello\", RecordReaderImpl.getMin(ColumnStatisticsImpl\n+        .deserialize(createStringStats(\"hello\", \"world\"))));\n+    assertEquals(HiveDecimal.create(\"111.1\"), RecordReaderImpl.getMin(ColumnStatisticsImpl\n+        .deserialize(createDecimalStats(\"111.1\", \"112.1\"))));\n   }\n \n   private static OrcProto.ColumnStatistics createIntStats(Long min,\n@@ -111,26 +130,39 @@ public void testGetMin() throws Exception {\n         .setIntStatistics(intStats.build()).build();\n   }\n \n+  private static OrcProto.ColumnStatistics createStringStats(String min, String max) {\n+    OrcProto.StringStatistics.Builder strStats = OrcProto.StringStatistics.newBuilder();\n+    strStats.setMinimum(min);\n+    strStats.setMaximum(max);\n+    return OrcProto.ColumnStatistics.newBuilder().setStringStatistics(strStats.build()).build();\n+  }\n+\n+  private static OrcProto.ColumnStatistics createDecimalStats(String min, String max) {\n+    OrcProto.DecimalStatistics.Builder decStats = OrcProto.DecimalStatistics.newBuilder();\n+    decStats.setMinimum(min);\n+    decStats.setMaximum(max);\n+    return OrcProto.ColumnStatistics.newBuilder().setDecimalStatistics(decStats.build()).build();\n+  }\n+\n   @Test\n   public void testGetMax() throws Exception {\n-    assertEquals(null, RecordReaderImpl.getMax(createIntStats(null, null)));\n-    assertEquals(100L, RecordReaderImpl.getMax(createIntStats(10L, 100L)));\n-    assertEquals(null, RecordReaderImpl.getMax(\n-        OrcProto.ColumnStatistics.newBuilder()\n-            .setDoubleStatistics(OrcProto.DoubleStatistics.newBuilder().build())\n-            .build()));\n-    assertEquals(100.0d, RecordReaderImpl.getMax(\n+    assertEquals(100L, RecordReaderImpl.getMax(ColumnStatisticsImpl.deserialize(createIntStats(10L, 100L))));\n+    assertEquals(100.0d, RecordReaderImpl.getMax(ColumnStatisticsImpl.deserialize(\n         OrcProto.ColumnStatistics.newBuilder()\n             .setDoubleStatistics(OrcProto.DoubleStatistics.newBuilder()\n-                .setMinimum(10.0d).setMaximum(100.0d).build()).build()));\n-    assertEquals(null, RecordReaderImpl.getMax(\n+                .setMinimum(10.0d).setMaximum(100.0d).build()).build())));\n+    assertEquals(null, RecordReaderImpl.getMax(ColumnStatisticsImpl.deserialize(\n         OrcProto.ColumnStatistics.newBuilder()\n             .setStringStatistics(OrcProto.StringStatistics.newBuilder().build())\n-            .build()));\n-    assertEquals(\"b\", RecordReaderImpl.getMax(\n+            .build())));\n+    assertEquals(\"b\", RecordReaderImpl.getMax(ColumnStatisticsImpl.deserialize(\n         OrcProto.ColumnStatistics.newBuilder()\n             .setStringStatistics(OrcProto.StringStatistics.newBuilder()\n-                .setMinimum(\"a\").setMaximum(\"b\").build()).build()));\n+                .setMinimum(\"a\").setMaximum(\"b\").build()).build())));\n+    assertEquals(\"world\", RecordReaderImpl.getMax(ColumnStatisticsImpl\n+        .deserialize(createStringStats(\"hello\", \"world\"))));\n+    assertEquals(HiveDecimal.create(\"112.1\"), RecordReaderImpl.getMax(ColumnStatisticsImpl\n+        .deserialize(createDecimalStats(\"111.1\", \"112.1\"))));\n   }\n \n   @Test\n@@ -150,6 +182,37 @@ public void testEquals() throws Exception {\n         RecordReaderImpl.evaluatePredicate(createIntStats(0L, 10L), pred));\n     assertEquals(TruthValue.YES_NULL,\n         RecordReaderImpl.evaluatePredicate(createIntStats(15L, 15L), pred));\n+\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.EQUALS,\n+        PredicateLeaf.Type.CHAR, \"x\", \"b\", null);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n+\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.EQUALS,\n+        PredicateLeaf.Type.VARCHAR, \"x\", \"b\", null);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n+\n   }\n \n   @Test\n@@ -169,6 +232,36 @@ public void testNullSafeEquals() throws Exception {\n         RecordReaderImpl.evaluatePredicate(createIntStats(0L, 10L), pred));\n     assertEquals(TruthValue.YES_NO,\n         RecordReaderImpl.evaluatePredicate(createIntStats(15L, 15L), pred));\n+\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,\n+        PredicateLeaf.Type.CHAR, \"x\", \"hello\", null);\n+    assertEquals(TruthValue.NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"world\", \"zombie\"), pred));\n+    assertEquals(TruthValue.YES_NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"hello\", \"zombie\"), pred));\n+    assertEquals(TruthValue.YES_NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"apple\", \"zombie\"), pred));\n+    assertEquals(TruthValue.YES_NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"apple\", \"hello\"), pred));\n+    assertEquals(TruthValue.NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"apple\", \"banana\"), pred));\n+    assertEquals(TruthValue.YES_NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"hello\", \"hello\"), pred));\n+\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.NULL_SAFE_EQUALS,\n+        PredicateLeaf.Type.VARCHAR, \"x\", \"hello\", null);\n+    assertEquals(TruthValue.NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"world\", \"zombie\"), pred));\n+    assertEquals(TruthValue.YES_NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"hello\", \"zombie\"), pred));\n+    assertEquals(TruthValue.YES_NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"apple\", \"zombie\"), pred));\n+    assertEquals(TruthValue.YES_NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"apple\", \"hello\"), pred));\n+    assertEquals(TruthValue.NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"apple\", \"banana\"), pred));\n+    assertEquals(TruthValue.YES_NO,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"hello\", \"hello\"), pred));\n   }\n \n   @Test\n@@ -186,6 +279,36 @@ public void testLessThan() throws Exception {\n         RecordReaderImpl.evaluatePredicate(createIntStats(10L, 15L), lessThan));\n     assertEquals(TruthValue.YES_NULL,\n         RecordReaderImpl.evaluatePredicate(createIntStats(0L, 10L), lessThan));\n+\n+    PredicateLeaf pred = TestSearchArgumentImpl.createPredicateLeaf(\n+        PredicateLeaf.Operator.LESS_THAN, PredicateLeaf.Type.CHAR, \"x\", \"b\", null);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n+\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.LESS_THAN,\n+        PredicateLeaf.Type.VARCHAR, \"x\", \"b\", null);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n   }\n \n   @Test\n@@ -203,6 +326,36 @@ public void testLessThanEquals() throws Exception {\n         RecordReaderImpl.evaluatePredicate(createIntStats(10L, 15L), pred));\n     assertEquals(TruthValue.YES_NULL,\n         RecordReaderImpl.evaluatePredicate(createIntStats(0L, 10L), pred));\n+\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.LESS_THAN_EQUALS,\n+        PredicateLeaf.Type.CHAR, \"x\", \"b\", null);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n+\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.LESS_THAN_EQUALS,\n+        PredicateLeaf.Type.VARCHAR, \"x\", \"b\", null);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n   }\n \n   @Test\n@@ -221,6 +374,39 @@ public void testIn() throws Exception {\n         RecordReaderImpl.evaluatePredicate(createIntStats(10L, 30L), pred));\n     assertEquals(TruthValue.NO_NULL,\n         RecordReaderImpl.evaluatePredicate(createIntStats(12L, 18L), pred));\n+\n+    args.clear();\n+    args.add(\"a\");\n+    args.add(\"b\");\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.IN,\n+        PredicateLeaf.Type.CHAR, \"x\", null, args);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n+\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.IN,\n+        PredicateLeaf.Type.VARCHAR, \"x\", null, args);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n   }\n \n   @Test\n@@ -245,6 +431,39 @@ public void testBetween() throws Exception {\n         RecordReaderImpl.evaluatePredicate(createIntStats(10L, 20L), pred));\n     assertEquals(TruthValue.YES_NULL,\n         RecordReaderImpl.evaluatePredicate(createIntStats(12L, 18L), pred));\n+\n+    args.clear();\n+    args.add(\"a\");\n+    args.add(\"b\");\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.BETWEEN,\n+        PredicateLeaf.Type.CHAR, \"x\", null, args);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n+\n+    pred = TestSearchArgumentImpl.createPredicateLeaf(PredicateLeaf.Operator.BETWEEN,\n+        PredicateLeaf.Type.VARCHAR, \"x\", null, args);\n+    assertEquals(TruthValue.NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"c\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"d\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"b\"), pred));\n+    assertEquals(TruthValue.YES_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"a\", \"a\"), pred));\n+    assertEquals(TruthValue.YES_NO_NULL,\n+        RecordReaderImpl.evaluatePredicate(createStringStats(\"b\", \"b\"), pred));\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestRecordReaderImpl.java",
                "sha": "3595e0516f456135dd3e21bbc3a18469bd91cf71",
                "status": "modified"
            },
            {
                "additions": 76,
                "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/queries/clientpositive/orc_ppd_char.q",
                "changes": 76,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/orc_ppd_char.q?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/orc_ppd_char.q",
                "patch": "@@ -0,0 +1,76 @@\n+SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;\n+SET mapred.min.split.size=1000;\n+SET mapred.max.split.size=5000;\n+\n+create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\"); \n+\n+insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl;\n+\n+set hive.optimize.index.filter=false;\n+\n+-- char data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where c=\"apple\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c=\"apple\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c!=\"apple\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c!=\"apple\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c<\"hello\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c<\"hello\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c<=\"hello\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c<=\"hello\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c=\"apple \";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c=\"apple \";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c in (\"apple\", \"carrot\");\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c in (\"apple\", \"carrot\");\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c in (\"apple\", \"hello\");\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c in (\"apple\", \"hello\");\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c in (\"carrot\");\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c in (\"carrot\");\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c between \"apple\" and \"carrot\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c between \"apple\" and \"carrot\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c between \"apple\" and \"zombie\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c between \"apple\" and \"zombie\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where c between \"carrot\" and \"carrot1\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where c between \"carrot\" and \"carrot1\";\n+",
                "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/queries/clientpositive/orc_ppd_char.q",
                "sha": "1f5f54ae19ee8035505d5aaf264fded6c82b7514",
                "status": "added"
            },
            {
                "additions": 97,
                "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/queries/clientpositive/orc_ppd_date.q",
                "changes": 97,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/orc_ppd_date.q?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/orc_ppd_date.q",
                "patch": "@@ -0,0 +1,97 @@\n+SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;\n+SET mapred.min.split.size=1000;\n+SET mapred.max.split.size=5000;\n+\n+create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\"); \n+\n+insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl;\n+\n+-- date data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where da='1970-02-20';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da='1970-02-20';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as date);\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as date);\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as varchar(20));\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as varchar(20));\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da!='1970-02-20';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da!='1970-02-20';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da<'1970-02-27';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da<'1970-02-27';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da<'1970-02-29';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da<'1970-02-29';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da<'1970-02-15';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da<'1970-02-15';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da<='1970-02-20';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da<='1970-02-20';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da<='1970-02-27';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da<='1970-02-27';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-27' as date));\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-27' as date));\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da in (cast('1970-02-20' as date), cast('1970-02-27' as date));\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da in (cast('1970-02-20' as date), cast('1970-02-27' as date));\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-22' as date));\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-22' as date));\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-22';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-22';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-28';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-28';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where da between '1970-02-18' and '1970-02-19';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where da between '1970-02-18' and '1970-02-19';",
                "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/queries/clientpositive/orc_ppd_date.q",
                "sha": "c34be867e484f40dc074fd0579fa96bec40f7027",
                "status": "added"
            },
            {
                "additions": 151,
                "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/queries/clientpositive/orc_ppd_decimal.q",
                "changes": 151,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/orc_ppd_decimal.q?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/orc_ppd_decimal.q",
                "patch": "@@ -0,0 +1,151 @@\n+SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;\n+SET mapred.min.split.size=1000;\n+SET mapred.max.split.size=5000;\n+\n+create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\"); \n+\n+insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl;\n+\n+-- decimal data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where d=0.22;\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d=0.22;\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d='0.22';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d='0.22';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d=cast('0.22' as float);\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d=cast('0.22' as float);\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d!=0.22;\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d!=0.22;\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d!='0.22';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d!='0.22';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d!=cast('0.22' as float);\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d!=cast('0.22' as float);\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d<11.22;\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d<11.22;\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d<'11.22';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d<'11.22';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d<cast('11.22' as float);\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d<cast('11.22' as float);\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d<1;\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d<1;\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d<=11.22;\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d<=11.22;\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d<='11.22';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d<='11.22';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d<=cast('11.22' as float);\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d<=cast('11.22' as float);\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d<=12;\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d<=12;\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d in ('0.22', '1.0');\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d in ('0.22', '1.0');\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d in ('0.22', '11.22');\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d in ('0.22', '11.22');\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d in ('0.9', '1.0');\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d in ('0.9', '1.0');\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22);\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22);\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22, cast('11.22' as float));\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22, cast('11.22' as float));\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d between 0 and 1;\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d between 0 and 1;\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d between 0 and 1000;\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d between 0 and 1000;\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d between 0 and '2.0';\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d between 0 and '2.0';\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d between 0 and cast(3 as float);\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d between 0 and cast(3 as float);\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where d between 1 and cast(30 as char(10));\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where d between 1 and cast(30 as char(10));",
                "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/queries/clientpositive/orc_ppd_decimal.q",
                "sha": "a93590eacca01b2d752303eda2a1a8df6caacb47",
                "status": "added"
            },
            {
                "additions": 76,
                "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/queries/clientpositive/orc_ppd_varchar.q",
                "changes": 76,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/orc_ppd_varchar.q?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/orc_ppd_varchar.q",
                "patch": "@@ -0,0 +1,76 @@\n+SET hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;\n+SET mapred.min.split.size=1000;\n+SET mapred.max.split.size=5000;\n+\n+create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\"); \n+\n+insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl;\n+\n+set hive.optimize.index.filter=false;\n+\n+-- varchar data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where v=\"bee\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v=\"bee\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v!=\"bee\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v!=\"bee\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v<\"world\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v<\"world\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v<=\"world\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v<=\"world\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v=\"bee   \";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v=\"bee   \";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v in (\"bee\", \"orange\");\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v in (\"bee\", \"orange\");\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v in (\"bee\", \"world\");\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v in (\"bee\", \"world\");\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v in (\"orange\");\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v in (\"orange\");\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v between \"bee\" and \"orange\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v between \"bee\" and \"orange\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v between \"bee\" and \"zombie\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v between \"bee\" and \"zombie\";\n+\n+set hive.optimize.index.filter=false;\n+select sum(hash(*)) from newtypesorc where v between \"orange\" and \"pine\";\n+\n+set hive.optimize.index.filter=true;\n+select sum(hash(*)) from newtypesorc where v between \"orange\" and \"pine\";\n+",
                "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/queries/clientpositive/orc_ppd_varchar.q",
                "sha": "0fecc664e46db6bfdb8e1b270e3b016da68877ef",
                "status": "added"
            },
            {
                "additions": 307,
                "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/results/clientpositive/orc_ppd_char.q.out",
                "changes": 307,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/orc_ppd_char.q.out?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/orc_ppd_char.q.out",
                "patch": "@@ -0,0 +1,307 @@\n+PREHOOK: query: create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\")\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+POSTHOOK: query: create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\")\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@newtypesorc\n+PREHOOK: query: insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@newtypesorc\n+POSTHOOK: query: insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@newtypesorc\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+PREHOOK: query: -- char data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where c=\"apple\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- char data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where c=\"apple\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c=\"apple\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c=\"apple\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c!=\"apple\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c!=\"apple\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c!=\"apple\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c!=\"apple\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c<\"hello\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c<\"hello\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c<\"hello\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c<\"hello\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c<=\"hello\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c<=\"hello\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c<=\"hello\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c<=\"hello\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c=\"apple \"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c=\"apple \"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c=\"apple \"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c=\"apple \"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"apple\", \"carrot\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"apple\", \"carrot\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"apple\", \"carrot\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"apple\", \"carrot\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"apple\", \"hello\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"apple\", \"hello\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"apple\", \"hello\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"apple\", \"hello\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"carrot\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"carrot\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"carrot\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c in (\"carrot\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c between \"apple\" and \"carrot\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c between \"apple\" and \"carrot\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c between \"apple\" and \"carrot\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c between \"apple\" and \"carrot\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c between \"apple\" and \"zombie\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c between \"apple\" and \"zombie\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c between \"apple\" and \"zombie\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c between \"apple\" and \"zombie\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c between \"carrot\" and \"carrot1\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c between \"carrot\" and \"carrot1\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where c between \"carrot\" and \"carrot1\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where c between \"carrot\" and \"carrot1\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL",
                "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/results/clientpositive/orc_ppd_char.q.out",
                "sha": "6d92e09d942bc71f993875df0f84ac25f544a03a",
                "status": "added"
            },
            {
                "additions": 411,
                "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/results/clientpositive/orc_ppd_date.q.out",
                "changes": 411,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/orc_ppd_date.q.out?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/orc_ppd_date.q.out",
                "patch": "@@ -0,0 +1,411 @@\n+PREHOOK: query: create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\")\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+POSTHOOK: query: create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\")\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@newtypesorc\n+PREHOOK: query: insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@newtypesorc\n+POSTHOOK: query: insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@newtypesorc\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+PREHOOK: query: -- date data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where da='1970-02-20'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- date data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where da='1970-02-20'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da='1970-02-20'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da='1970-02-20'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as date)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as date)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as date)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as date)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as varchar(20))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as varchar(20))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as varchar(20))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da=cast('1970-02-20' as varchar(20))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da!='1970-02-20'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da!='1970-02-20'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da!='1970-02-20'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da!='1970-02-20'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-27'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-27'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-27'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-27'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-29'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-29'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-29'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-29'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-15'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-15'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-15'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<'1970-02-15'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<='1970-02-20'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<='1970-02-20'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<='1970-02-20'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<='1970-02-20'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<='1970-02-27'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<='1970-02-27'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da<='1970-02-27'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da<='1970-02-27'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-27' as date))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-27' as date))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-27' as date))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-27' as date))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-20' as date), cast('1970-02-27' as date))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-20' as date), cast('1970-02-27' as date))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-20' as date), cast('1970-02-27' as date))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-20' as date), cast('1970-02-27' as date))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-22' as date))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-22' as date))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-22' as date))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da in (cast('1970-02-21' as date), cast('1970-02-22' as date))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-28'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-28'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-28'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-19' and '1970-02-28'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-18' and '1970-02-19'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-18' and '1970-02-19'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-18' and '1970-02-19'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where da between '1970-02-18' and '1970-02-19'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL",
                "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/results/clientpositive/orc_ppd_date.q.out",
                "sha": "a3e5253d97a4be97876975a75c1e4d7fb87a42a0",
                "status": "added"
            },
            {
                "additions": 645,
                "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/results/clientpositive/orc_ppd_decimal.q.out",
                "changes": 645,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/orc_ppd_decimal.q.out?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/orc_ppd_decimal.q.out",
                "patch": "@@ -0,0 +1,645 @@\n+PREHOOK: query: create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\")\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+POSTHOOK: query: create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\")\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@newtypesorc\n+PREHOOK: query: insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@newtypesorc\n+POSTHOOK: query: insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@newtypesorc\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+PREHOOK: query: -- decimal data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where d=0.22\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- decimal data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where d=0.22\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d=0.22\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d=0.22\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d='0.22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d='0.22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d='0.22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d='0.22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d=cast('0.22' as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d=cast('0.22' as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d=cast('0.22' as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d=cast('0.22' as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d!=0.22\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d!=0.22\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d!=0.22\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d!=0.22\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d!='0.22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d!='0.22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d!='0.22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d!='0.22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d!=cast('0.22' as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d!=cast('0.22' as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d!=cast('0.22' as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d!=cast('0.22' as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<11.22\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<11.22\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<11.22\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<11.22\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<'11.22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<'11.22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<'11.22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<'11.22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<cast('11.22' as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<cast('11.22' as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<cast('11.22' as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<cast('11.22' as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<=11.22\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<=11.22\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<=11.22\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<=11.22\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<='11.22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<='11.22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<='11.22'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<='11.22'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<=cast('11.22' as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<=cast('11.22' as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<=cast('11.22' as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<=cast('11.22' as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<=12\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<=12\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d<=12\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d<=12\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.22', '1.0')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.22', '1.0')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.22', '1.0')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.22', '1.0')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.22', '11.22')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.22', '11.22')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.22', '11.22')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.22', '11.22')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', '1.0')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', '1.0')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', '1.0')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', '1.0')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22, cast('11.22' as float))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22, cast('11.22' as float))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22, cast('11.22' as float))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d in ('0.9', 0.22, cast('11.22' as float))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and 1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and 1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and 1000\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and 1000\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and 1000\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and 1000\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and '2.0'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and '2.0'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and '2.0'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and '2.0'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and cast(3 as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and cast(3 as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and cast(3 as float)\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 0 and cast(3 as float)\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 1 and cast(30 as char(10))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 1 and cast(30 as char(10))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where d between 1 and cast(30 as char(10))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where d between 1 and cast(30 as char(10))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500",
                "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/results/clientpositive/orc_ppd_decimal.q.out",
                "sha": "a1094b6819725da343ea71477af0e467cf77365d",
                "status": "added"
            },
            {
                "additions": 307,
                "blob_url": "https://github.com/apache/hive/blob/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/results/clientpositive/orc_ppd_varchar.q.out",
                "changes": 307,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/orc_ppd_varchar.q.out?ref=90fabf185f264508afd329d9e0bf847fc9ae19f1",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/orc_ppd_varchar.q.out",
                "patch": "@@ -0,0 +1,307 @@\n+PREHOOK: query: create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\")\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+POSTHOOK: query: create table newtypesorc(c char(10), v varchar(10), d decimal(5,3), da date) stored as orc tblproperties(\"orc.stripe.size\"=\"16777216\")\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@newtypesorc\n+PREHOOK: query: insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@newtypesorc\n+POSTHOOK: query: insert overwrite table newtypesorc select * from (select cast(\"apple\" as char(10)), cast(\"bee\" as varchar(10)), 0.22, cast(\"1970-02-20\" as date) from src src1 union all select cast(\"hello\" as char(10)), cast(\"world\" as varchar(10)), 11.22, cast(\"1970-02-27\" as date) from src src2) uniontbl\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@newtypesorc\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+PREHOOK: query: -- varchar data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where v=\"bee\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- varchar data types (EQUAL, NOT_EQUAL, LESS_THAN, LESS_THAN_EQUALS, IN, BETWEEN tests)\n+select sum(hash(*)) from newtypesorc where v=\"bee\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v=\"bee\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v=\"bee\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v!=\"bee\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v!=\"bee\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v!=\"bee\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v!=\"bee\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+334427804500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v<\"world\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v<\"world\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v<\"world\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v<\"world\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v<=\"world\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v<=\"world\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v<=\"world\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v<=\"world\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v=\"bee   \"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v=\"bee   \"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v=\"bee   \"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v=\"bee   \"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"bee\", \"orange\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"bee\", \"orange\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"bee\", \"orange\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"bee\", \"orange\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"bee\", \"world\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"bee\", \"world\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"bee\", \"world\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"bee\", \"world\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"orange\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"orange\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"orange\")\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v in (\"orange\")\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v between \"bee\" and \"orange\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v between \"bee\" and \"orange\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v between \"bee\" and \"orange\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v between \"bee\" and \"orange\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+-252951929000\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v between \"bee\" and \"zombie\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v between \"bee\" and \"zombie\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v between \"bee\" and \"zombie\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v between \"bee\" and \"zombie\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+81475875500\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v between \"orange\" and \"pine\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v between \"orange\" and \"pine\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL\n+PREHOOK: query: select sum(hash(*)) from newtypesorc where v between \"orange\" and \"pine\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select sum(hash(*)) from newtypesorc where v between \"orange\" and \"pine\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@newtypesorc\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: newtypesorc.c EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.d EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.da EXPRESSION []\n+POSTHOOK: Lineage: newtypesorc.v EXPRESSION []\n+NULL",
                "raw_url": "https://github.com/apache/hive/raw/90fabf185f264508afd329d9e0bf847fc9ae19f1/ql/src/test/results/clientpositive/orc_ppd_varchar.q.out",
                "sha": "0bce7605b436e0a43cf9a9e3ee53550d79c0e114",
                "status": "added"
            }
        ],
        "message": "HIVE-5950: ORC SARG creation fails with NPE for predicate conditions with decimal/date/char/varchar datatypes (Prasanth J via Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1574237 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/c192ecbb3e28610a46a0ecea6ba677ad0e467c3a",
        "patched_files": [
            "orc_ppd_varchar.java",
            "PredicateLeaf.java",
            "orc_ppd_char.java",
            "orc_ppd_date.java",
            "OrcInputFormat.java",
            "RecordReaderImpl.java",
            "SearchArgumentImpl.java",
            "orc_ppd_decimal.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestRecordReaderImpl.java",
            "TestSearchArgumentImpl.java"
        ]
    },
    "hive_9544297": {
        "bug_id": "hive_9544297",
        "commit": "https://github.com/apache/hive/commit/9544297803858557b776822296663c0438cf0723",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/9544297803858557b776822296663c0438cf0723/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java?ref=9544297803858557b776822296663c0438cf0723",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java",
                "patch": "@@ -831,9 +831,13 @@ public void close() throws IOException {\n     }\n \n     try {\n-      tezSessionState.close(false);\n+      if (tezSessionState != null) {\n+        tezSessionState.close(false);\n+      }\n     } catch (Exception e) {\n       LOG.info(\"Error closing tez session\", e);\n+    } finally {\n+      tezSessionState = null;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/9544297803858557b776822296663c0438cf0723/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java",
                "sha": "3e69f01c14cc1c314f9d77cc51061b3a2fd6ada4",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/9544297803858557b776822296663c0438cf0723/ql/src/test/org/apache/hadoop/hive/ql/session/TestSessionState.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/session/TestSessionState.java?ref=9544297803858557b776822296663c0438cf0723",
                "deletions": 1,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/session/TestSessionState.java",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.hive.ql.session;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n \n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.MetaStoreUtils;\n@@ -57,5 +58,11 @@ public void testgetDbName() throws Exception {\n \n   }\n \n-\n+  @Test\n+  public void testClose() throws Exception {\n+    SessionState ss = SessionState.get();\n+    assertNull(ss.getTezSession());\n+    ss.close();\n+    assertNull(ss.getTezSession());\n+  }\n }",
                "raw_url": "https://github.com/apache/hive/raw/9544297803858557b776822296663c0438cf0723/ql/src/test/org/apache/hadoop/hive/ql/session/TestSessionState.java",
                "sha": "d4e737fe85f6c6b565b1bba3b4eb573f17f5260c",
                "status": "modified"
            }
        ],
        "message": "HIVE-6097: Sessions on Tez NPE when quitting CLI (Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/tez@1553108 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/329d7f354a4ff3b669a2a1aa23b19469814307c2",
        "patched_files": [
            "SessionState.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestSessionState.java"
        ]
    },
    "hive_96f7960": {
        "bug_id": "hive_96f7960",
        "commit": "https://github.com/apache/hive/commit/96f7960c790fadb6bbc9d81fe095288241d86d8c",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/96f7960c790fadb6bbc9d81fe095288241d86d8c/itests/src/test/resources/testconfiguration.properties",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=96f7960c790fadb6bbc9d81fe095288241d86d8c",
                "deletions": 1,
                "filename": "itests/src/test/resources/testconfiguration.properties",
                "patch": "@@ -42,7 +42,8 @@ minitez.query.files=acid_vectorization_original_tez.q,\\\n   multi_count_distinct.q,\\\n   tez-tag.q,\\\n   tez_union_with_udf.q,\\\n-  tez_union_udtf.q\n+  tez_union_udtf.q,\\\n+  tez_complextype_with_null.q\n \n \n minillap.shared.query.files=insert_into1.q,\\",
                "raw_url": "https://github.com/apache/hive/raw/96f7960c790fadb6bbc9d81fe095288241d86d8c/itests/src/test/resources/testconfiguration.properties",
                "sha": "84c2042676332b307afc6a34f7bbc7a2279692ba",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/96f7960c790fadb6bbc9d81fe095288241d86d8c/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java?ref=96f7960c790fadb6bbc9d81fe095288241d86d8c",
                "deletions": 2,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java",
                "patch": "@@ -1345,11 +1345,11 @@ public static long getWritableSize(ObjectInspector oi, Object value) {\n     if (oi instanceof WritableStringObjectInspector) {\n       WritableStringObjectInspector woi = (WritableStringObjectInspector) oi;\n       return JavaDataModel.get().lengthForStringOfLength(\n-          woi.getPrimitiveWritableObject(value).getLength());\n+        value == null ? 0 : woi.getPrimitiveWritableObject(value).getLength());\n     } else if (oi instanceof WritableBinaryObjectInspector) {\n       WritableBinaryObjectInspector woi = (WritableBinaryObjectInspector) oi;\n       return JavaDataModel.get().lengthForByteArrayOfSize(\n-          woi.getPrimitiveWritableObject(value).getLength());\n+        value == null ? 0 : woi.getPrimitiveWritableObject(value).getLength());\n     } else if (oi instanceof WritableBooleanObjectInspector) {\n       return JavaDataModel.get().primitive1();\n     } else if (oi instanceof WritableByteObjectInspector) {",
                "raw_url": "https://github.com/apache/hive/raw/96f7960c790fadb6bbc9d81fe095288241d86d8c/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsUtils.java",
                "sha": "1795ae562603846a2562857ec30f8ad44d331dcc",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hive/blob/96f7960c790fadb6bbc9d81fe095288241d86d8c/ql/src/test/queries/clientpositive/tez_complextype_with_null.q",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/tez_complextype_with_null.q?ref=96f7960c790fadb6bbc9d81fe095288241d86d8c",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/tez_complextype_with_null.q",
                "patch": "@@ -0,0 +1,15 @@\n+CREATE TABLE complex1 (\n+    c0 int,\n+    c1 array<int>,\n+    c2 map<int, string>,\n+    c3 struct<f1:int,f2:string,f3:array<int>>,\n+    c4 array<struct<f1:int,f2:string,f3:array<int>>>);\n+\n+INSERT INTO complex1\n+    SELECT 3,\n+       array(1, 2, null),\n+       map(1, 'one', 2, null),\n+       named_struct('f1', cast(null as int), 'f2', cast(null as string), 'f3', array(1, 2, null)),\n+       array(named_struct('f1', 11, 'f2', 'two', 'f3', array(2, 3, 4)));\n+\n+select * from complex1;",
                "raw_url": "https://github.com/apache/hive/raw/96f7960c790fadb6bbc9d81fe095288241d86d8c/ql/src/test/queries/clientpositive/tez_complextype_with_null.q",
                "sha": "7a0f240cdb29fa1c61abb95951bbef919baa2207",
                "status": "added"
            },
            {
                "additions": 50,
                "blob_url": "https://github.com/apache/hive/blob/96f7960c790fadb6bbc9d81fe095288241d86d8c/ql/src/test/results/clientpositive/tez/tez_complextype_with_null.q.out",
                "changes": 50,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_complextype_with_null.q.out?ref=96f7960c790fadb6bbc9d81fe095288241d86d8c",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/tez/tez_complextype_with_null.q.out",
                "patch": "@@ -0,0 +1,50 @@\n+PREHOOK: query: CREATE TABLE complex1 (\n+    c0 int,\n+    c1 array<int>,\n+    c2 map<int, string>,\n+    c3 struct<f1:int,f2:string,f3:array<int>>,\n+    c4 array<struct<f1:int,f2:string,f3:array<int>>>)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@complex1\n+POSTHOOK: query: CREATE TABLE complex1 (\n+    c0 int,\n+    c1 array<int>,\n+    c2 map<int, string>,\n+    c3 struct<f1:int,f2:string,f3:array<int>>,\n+    c4 array<struct<f1:int,f2:string,f3:array<int>>>)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@complex1\n+PREHOOK: query: INSERT INTO complex1\n+    SELECT 3,\n+       array(1, 2, null),\n+       map(1, 'one', 2, null),\n+       named_struct('f1', cast(null as int), 'f2', cast(null as string), 'f3', array(1, 2, null)),\n+       array(named_struct('f1', 11, 'f2', 'two', 'f3', array(2, 3, 4)))\n+PREHOOK: type: QUERY\n+PREHOOK: Input: _dummy_database@_dummy_table\n+PREHOOK: Output: default@complex1\n+POSTHOOK: query: INSERT INTO complex1\n+    SELECT 3,\n+       array(1, 2, null),\n+       map(1, 'one', 2, null),\n+       named_struct('f1', cast(null as int), 'f2', cast(null as string), 'f3', array(1, 2, null)),\n+       array(named_struct('f1', 11, 'f2', 'two', 'f3', array(2, 3, 4)))\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: _dummy_database@_dummy_table\n+POSTHOOK: Output: default@complex1\n+POSTHOOK: Lineage: complex1.c0 SIMPLE []\n+POSTHOOK: Lineage: complex1.c1 EXPRESSION []\n+POSTHOOK: Lineage: complex1.c2 EXPRESSION []\n+POSTHOOK: Lineage: complex1.c3 EXPRESSION []\n+POSTHOOK: Lineage: complex1.c4 EXPRESSION []\n+PREHOOK: query: select * from complex1\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@complex1\n+PREHOOK: Output: hdfs://### HDFS PATH ###\n+POSTHOOK: query: select * from complex1\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@complex1\n+POSTHOOK: Output: hdfs://### HDFS PATH ###\n+3\t[1,2,null]\t{1:\"one\",2:null}\t{\"f1\":null,\"f2\":null,\"f3\":[1,2,null]}\t[{\"f1\":11,\"f2\":\"two\",\"f3\":[2,3,4]}]",
                "raw_url": "https://github.com/apache/hive/raw/96f7960c790fadb6bbc9d81fe095288241d86d8c/ql/src/test/results/clientpositive/tez/tez_complextype_with_null.q.out",
                "sha": "f20151d746c17b28be1f9dffd97d6bca12913d08",
                "status": "added"
            }
        ],
        "message": "HIVE-16587: NPE when inserting complex types with nested null values (Naresh P R, reviewed by Sankar Hariappan)\n\nSigned-off-by: Sankar Hariappan <sankarh@apache.org>",
        "parent": "https://github.com/apache/hive/commit/7fc5a88a149cf0767a5846cbb6ace22d8e99a63c",
        "patched_files": [
            "StatsUtils.java",
            "tez_complextype_with_null.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestStatsUtils.java",
            "testconfiguration.java"
        ]
    },
    "hive_97b4750": {
        "bug_id": "hive_97b4750",
        "commit": "https://github.com/apache/hive/commit/97b4750c6314eea9025b426e4df73f795b601927",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/97b4750c6314eea9025b426e4df73f795b601927/service/src/java/org/apache/hive/service/server/HiveServer2.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/service/src/java/org/apache/hive/service/server/HiveServer2.java?ref=97b4750c6314eea9025b426e4df73f795b601927",
                "deletions": 3,
                "filename": "service/src/java/org/apache/hive/service/server/HiveServer2.java",
                "patch": "@@ -317,7 +317,7 @@ public synchronized void stop() {\n       }\n     }\n     // Remove this server instance from ZooKeeper if dynamic service discovery is set\n-    if (hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_SUPPORT_DYNAMIC_SERVICE_DISCOVERY)) {\n+    if (hiveConf != null && hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_SUPPORT_DYNAMIC_SERVICE_DISCOVERY)) {\n       try {\n         removeServerInstanceFromZooKeeper();\n       } catch (Exception e) {\n@@ -326,7 +326,7 @@ public synchronized void stop() {\n     }\n     // There should already be an instance of the session pool manager.\n     // If not, ignoring is fine while stopping HiveServer2.\n-    if (hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_TEZ_INITIALIZE_DEFAULT_SESSIONS)) {\n+    if (hiveConf != null && hiveConf.getBoolVar(ConfVars.HIVE_SERVER2_TEZ_INITIALIZE_DEFAULT_SESSIONS)) {\n       try {\n         TezSessionPoolManager.getInstance().stop();\n       } catch (Exception e) {\n@@ -335,7 +335,7 @@ public synchronized void stop() {\n       }\n     }\n \n-    if (hiveConf.getVar(ConfVars.HIVE_EXECUTION_ENGINE).equals(\"spark\")) {\n+    if (hiveConf != null && hiveConf.getVar(ConfVars.HIVE_EXECUTION_ENGINE).equals(\"spark\")) {\n       try {\n         SparkSessionManagerImpl.getInstance().shutdown();\n       } catch(Exception ex) {",
                "raw_url": "https://github.com/apache/hive/raw/97b4750c6314eea9025b426e4df73f795b601927/service/src/java/org/apache/hive/service/server/HiveServer2.java",
                "sha": "4a4be975a0912eb0981c56cbade85e83d9e3c391",
                "status": "modified"
            }
        ],
        "message": "HIVE-9566: HiveServer2 fails to start with NullPointerException (Na via Xuefu)",
        "parent": "https://github.com/apache/hive/commit/8ed337749261ad78becb46a16a350ef23d9f305f",
        "patched_files": [
            "HiveServer2.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveServer2.java"
        ]
    },
    "hive_9c00ee0": {
        "bug_id": "hive_9c00ee0",
        "commit": "https://github.com/apache/hive/commit/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/beeline/src/java/org/apache/hive/beeline/ColorBuffer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/beeline/src/java/org/apache/hive/beeline/ColorBuffer.java?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a",
                "deletions": 0,
                "filename": "beeline/src/java/org/apache/hive/beeline/ColorBuffer.java",
                "patch": "@@ -78,6 +78,9 @@ ColorBuffer pad(ColorBuffer str, int len) {\n   }\n \n   ColorBuffer center(String str, int len) {\n+    if (str == null) {\n+      str = \"\";\n+    }\n     StringBuilder buf = new StringBuilder(str);\n     while (buf.length() < len) {\n       buf.append(\" \");",
                "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/beeline/src/java/org/apache/hive/beeline/ColorBuffer.java",
                "sha": "1730d492d77523dc80a937559dcf93ef707449ac",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/beeline/src/java/org/apache/hive/beeline/Commands.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/beeline/src/java/org/apache/hive/beeline/Commands.java?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a",
                "deletions": 1,
                "filename": "beeline/src/java/org/apache/hive/beeline/Commands.java",
                "patch": "@@ -1052,7 +1052,9 @@ private boolean executeInternal(String sql, boolean call) {\n             logThread.interrupt();\n           }\n           logThread.join(DEFAULT_QUERY_PROGRESS_THREAD_TIMEOUT);\n-          showRemainingLogsIfAny(stmnt);\n+          if (stmnt != null) {\n+            showRemainingLogsIfAny(stmnt);\n+          }\n         }\n         if (stmnt != null) {\n           stmnt.close();",
                "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/beeline/src/java/org/apache/hive/beeline/Commands.java",
                "sha": "fd0af2ca0c963d0dd2f3ea35c1dc45a179122eb2",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-1.2.0-to-2.0.0.mysql.sql",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-1.2.0-to-2.0.0.mysql.sql?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a",
                "deletions": 2,
                "filename": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-1.2.0-to-2.0.0.mysql.sql",
                "patch": "@@ -1,4 +1,4 @@\n-SELECT 'Upgrading MetaStore schema from 1.2.0 to 2.0.0' AS ' ';\n+SELECT 'Upgrading MetaStore schema from 1.2.0 to 2.0.0' AS MESSAGE;\n --SOURCE 021-HIVE-7018.mysql.sql;\n ALTER TABLE `TBLS` DROP FOREIGN KEY `TBLS_FK3`;\n ALTER TABLE `TBLS` DROP KEY `TBLS_N51`;\n@@ -71,5 +71,5 @@ CREATE TABLE AUX_TABLE (\n \n \n UPDATE VERSION SET SCHEMA_VERSION='2.0.0', VERSION_COMMENT='Hive release version 2.0.0' where VER_ID=1;\n-SELECT 'Finished upgrading MetaStore schema from 1.2.0 to 2.0.0' AS ' ';\n+SELECT 'Finished upgrading MetaStore schema from 1.2.0 to 2.0.0' AS MESSAGE;\n ",
                "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-1.2.0-to-2.0.0.mysql.sql",
                "sha": "20ddd1ada6858a4cccedd26d5f9bbf355fa7a123",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.0.0-to-2.1.0.mysql.sql",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.0.0-to-2.1.0.mysql.sql?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a",
                "deletions": 2,
                "filename": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.0.0-to-2.1.0.mysql.sql",
                "patch": "@@ -1,4 +1,4 @@\n-SELECT 'Upgrading MetaStore schema from 2.0.0 to 2.1.0' AS ' ';\n+SELECT 'Upgrading MetaStore schema from 2.0.0 to 2.1.0' AS MESSAGE;\n \n --SOURCE 034-HIVE-13076.mysql.sql;\n CREATE TABLE IF NOT EXISTS `KEY_CONSTRAINTS`\n@@ -38,5 +38,5 @@ ALTER TABLE COMPACTION_QUEUE ADD CQ_TBLPROPERTIES varchar(2048);\n ALTER TABLE COMPLETED_COMPACTIONS ADD CC_TBLPROPERTIES varchar(2048);\n \n UPDATE VERSION SET SCHEMA_VERSION='2.1.0', VERSION_COMMENT='Hive release version 2.1.0' where VER_ID=1;\n-SELECT 'Finished upgrading MetaStore schema from 2.0.0 to 2.1.0' AS ' ';\n+SELECT 'Finished upgrading MetaStore schema from 2.0.0 to 2.1.0' AS MESSAGE;\n ",
                "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.0.0-to-2.1.0.mysql.sql",
                "sha": "22a3c377b77081891967df50db29837e649ff7cb",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.1.0-to-2.2.0.mysql.sql",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.1.0-to-2.2.0.mysql.sql?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a",
                "deletions": 2,
                "filename": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.1.0-to-2.2.0.mysql.sql",
                "patch": "@@ -1,4 +1,4 @@\n-SELECT 'Upgrading MetaStore schema from 2.1.0 to 2.2.0' AS ' ';\n+SELECT 'Upgrading MetaStore schema from 2.1.0 to 2.2.0' AS MESSAGE;\n \n --SOURCE 037-HIVE-14496.mysql.sql;\n -- Step 1: Add the column allowing null\n@@ -39,5 +39,5 @@ ALTER TABLE TAB_COL_STATS MODIFY COLUMN_NAME varchar(767) CHARACTER SET latin1 C\n ALTER TABLE PART_COL_STATS MODIFY COLUMN_NAME varchar(767) CHARACTER SET latin1 COLLATE latin1_bin NOT NULL;\n \n UPDATE VERSION SET SCHEMA_VERSION='2.2.0', VERSION_COMMENT='Hive release version 2.2.0' where VER_ID=1;\n-SELECT 'Finished upgrading MetaStore schema from 2.1.0 to 2.2.0' AS ' ';\n+SELECT 'Finished upgrading MetaStore schema from 2.1.0 to 2.2.0' AS MESSAGE;\n ",
                "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.1.0-to-2.2.0.mysql.sql",
                "sha": "3346fe0bc2b3e0967531cdb34702fb152edbbdef",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.2.0-to-2.3.0.mysql.sql",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.2.0-to-2.3.0.mysql.sql?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a",
                "deletions": 2,
                "filename": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.2.0-to-2.3.0.mysql.sql",
                "patch": "@@ -1,8 +1,8 @@\n-SELECT 'Upgrading MetaStore schema from 2.2.0 to 2.3.0' AS ' ';\n+SELECT 'Upgrading MetaStore schema from 2.2.0 to 2.3.0' AS MESSAGE;\n \n --SOURCE 040-HIVE-16399.mysql.sql;\n CREATE INDEX TC_TXNID_INDEX ON TXN_COMPONENTS (TC_TXNID);\n \n UPDATE VERSION SET SCHEMA_VERSION='2.3.0', VERSION_COMMENT='Hive release version 2.3.0' where VER_ID=1;\n-SELECT 'Finished upgrading MetaStore schema from 2.2.0 to 2.3.0' AS ' ';\n+SELECT 'Finished upgrading MetaStore schema from 2.2.0 to 2.3.0' AS MESSAGE;\n ",
                "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.2.0-to-2.3.0.mysql.sql",
                "sha": "37e817b7cfd41ebcf566b338f85ede77ac9bc63c",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.3.0-to-3.0.0.mysql.sql",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.3.0-to-3.0.0.mysql.sql?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a",
                "deletions": 2,
                "filename": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.3.0-to-3.0.0.mysql.sql",
                "patch": "@@ -1,4 +1,4 @@\n-SELECT 'Upgrading MetaStore schema from 2.3.0 to 3.0.0' AS ' ';\n+SELECT 'Upgrading MetaStore schema from 2.3.0 to 3.0.0' AS MESSAGE;\n \n --SOURCE 041-HIVE-16556.mysql.sql;\n --\n@@ -323,4 +323,4 @@ ALTER TABLE `TBLS` ADD COLUMN `OWNER_TYPE` VARCHAR(10) CHARACTER SET latin1 COLL\n \n -- These lines need to be last.  Insert any changes above.\n UPDATE VERSION SET SCHEMA_VERSION='3.0.0', VERSION_COMMENT='Hive release version 3.0.0' where VER_ID=1;\n-SELECT 'Finished upgrading MetaStore schema from 2.3.0 to 3.0.0' AS ' ';\n+SELECT 'Finished upgrading MetaStore schema from 2.3.0 to 3.0.0' AS MESSAGE;",
                "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-2.3.0-to-3.0.0.mysql.sql",
                "sha": "7140c2af173d8feef8502e9ae75a296d3e9e1552",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.0.0-to-3.1.0.mysql.sql",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.0.0-to-3.1.0.mysql.sql?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a",
                "deletions": 2,
                "filename": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.0.0-to-3.1.0.mysql.sql",
                "patch": "@@ -1,4 +1,4 @@\n-SELECT 'Upgrading MetaStore schema from 3.0.0 to 3.1.0' AS ' ';\n+SELECT 'Upgrading MetaStore schema from 3.0.0 to 3.1.0' AS MESSAGE;\n   \n -- HIVE-19440\n ALTER TABLE `GLOBAL_PRIVS` ADD `AUTHORIZER` varchar(128) CHARACTER SET latin1 COLLATE latin1_bin DEFAULT NULL;\n@@ -53,4 +53,4 @@ CREATE TABLE MATERIALIZATION_REBUILD_LOCKS (\n \n -- These lines need to be last.  Insert any changes above.\n UPDATE VERSION SET SCHEMA_VERSION='3.1.0', VERSION_COMMENT='Hive release version 3.1.0' where VER_ID=1;\n-SELECT 'Finished upgrading MetaStore schema from 3.0.0 to 3.1.0' AS ' ';\n+SELECT 'Finished upgrading MetaStore schema from 3.0.0 to 3.1.0' AS MESSAGE;",
                "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.0.0-to-3.1.0.mysql.sql",
                "sha": "3eae9f2850e6d35576f7717a34a259f2b86d3ecf",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.1.0-to-3.2.0.mysql.sql",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.1.0-to-3.2.0.mysql.sql?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a",
                "deletions": 2,
                "filename": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.1.0-to-3.2.0.mysql.sql",
                "patch": "@@ -1,4 +1,4 @@\n-SELECT 'Upgrading MetaStore schema from 3.1.0 to 3.2.0' AS ' ';\n+SELECT 'Upgrading MetaStore schema from 3.1.0 to 3.2.0' AS MESSAGE;\n \n -- HIVE-19267\n CREATE TABLE TXN_WRITE_NOTIFICATION_LOG (\n@@ -25,5 +25,5 @@ ALTER TABLE `CTLGS` ADD `CREATE_TIME` INT(11);\n \n -- These lines need to be last.  Insert any changes above.\n UPDATE VERSION SET SCHEMA_VERSION='3.2.0', VERSION_COMMENT='Hive release version 3.2.0' where VER_ID=1;\n-SELECT 'Finished upgrading MetaStore schema from 3.1.0 to 3.2.0' AS ' ';\n+SELECT 'Finished upgrading MetaStore schema from 3.1.0 to 3.2.0' AS MESSAGE;\n ",
                "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.1.0-to-3.2.0.mysql.sql",
                "sha": "ebfb90c51e51101cdd2802eb6b06f87c6300d812",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.2.0-to-4.0.0.mysql.sql",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.2.0-to-4.0.0.mysql.sql?ref=9c00ee0d3043a7fdb7576baf8aea0805439eeb0a",
                "deletions": 2,
                "filename": "standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.2.0-to-4.0.0.mysql.sql",
                "patch": "@@ -1,4 +1,4 @@\n-SELECT 'Upgrading MetaStore schema from 3.2.0 to 4.0.0' AS ' ';\n+SELECT 'Upgrading MetaStore schema from 3.2.0 to 4.0.0' AS MESSAGE;\n \n -- HIVE-19416\n ALTER TABLE TBLS ADD WRITE_ID bigint DEFAULT 0;\n@@ -19,5 +19,5 @@ ALTER TABLE COLUMNS_V2 MODIFY COMMENT varchar(4000) CHARACTER SET latin1 COLLATE\n \n -- These lines need to be last.  Insert any changes above.\n UPDATE VERSION SET SCHEMA_VERSION='4.0.0', VERSION_COMMENT='Hive release version 4.0.0' where VER_ID=1;\n-SELECT 'Finished upgrading MetaStore schema from 3.2.0 to 4.0.0' AS ' ';\n+SELECT 'Finished upgrading MetaStore schema from 3.2.0 to 4.0.0' AS MESSAGE;\n ",
                "raw_url": "https://github.com/apache/hive/raw/9c00ee0d3043a7fdb7576baf8aea0805439eeb0a/standalone-metastore/metastore-server/src/main/sql/mysql/upgrade-3.2.0-to-4.0.0.mysql.sql",
                "sha": "296cb12ef080043855d627e5e76d9f8d160754bb",
                "status": "modified"
            }
        ],
        "message": "HIVE-21844 : HMS schema Upgrade Script is failing with NPE. (Mahesh Kumar Behera reviewed by  Sankar Hariappan)",
        "parent": "https://github.com/apache/hive/commit/ec3779978797051fdb345172536aafcd50f1b4ae",
        "patched_files": [
            "Commands.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestCommands.java"
        ]
    },
    "hive_a1b07ca": {
        "bug_id": "hive_a1b07ca",
        "commit": "https://github.com/apache/hive/commit/a1b07ca59db6f92bb574e136a580f0b2b6ac1490",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/a1b07ca59db6f92bb574e136a580f0b2b6ac1490/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java?ref=a1b07ca59db6f92bb574e136a580f0b2b6ac1490",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "patch": "@@ -114,7 +114,8 @@ public int execute(DriverContext driverContext) {\n \n       // if we don't have one yet create it.\n       if (session == null) {\n-        ss.setTezSession(new TezSessionState());\n+        session = new TezSessionState();\n+        ss.setTezSession(session);\n       }\n \n       // if it's not running start it.",
                "raw_url": "https://github.com/apache/hive/raw/a1b07ca59db6f92bb574e136a580f0b2b6ac1490/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezTask.java",
                "sha": "c6f431c35f37523983a1e6aa40e57cf8abab9cb7",
                "status": "modified"
            }
        ],
        "message": "HIVE-6231: NPE when switching to Tez execution mode after session has been initialized (Patch by Gunther Hagleitner, reviewed by Vikram Dixit K)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1560268 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/45a2616ab18a136b3873fba57c715ece0d09cbbe",
        "patched_files": [
            "TezTask.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestTezTask.java"
        ]
    },
    "hive_a496e58": {
        "bug_id": "hive_a496e58",
        "commit": "https://github.com/apache/hive/commit/a496e581152425773080aac48cf479e493cd5b74",
        "file": [
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hive/blob/a496e581152425773080aac48cf479e493cd5b74/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java?ref=a496e581152425773080aac48cf479e493cd5b74",
                "deletions": 12,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java",
                "patch": "@@ -117,6 +117,8 @@\n   protected transient JobConf job;\n   public static MemoryMXBean memoryMXBean;\n   protected HadoopJobExecHelper jobExecHelper;\n+  private transient boolean isShutdown = false;\n+  private transient boolean jobKilled = false;\n \n   protected static transient final Logger LOG = LoggerFactory.getLogger(ExecDriver.class);\n \n@@ -413,10 +415,7 @@ public int execute(DriverContext driverContext) {\n \n       if (driverContext.isShutdown()) {\n         LOG.warn(\"Task was cancelled\");\n-        if (rj != null) {\n-          rj.killJob();\n-          rj = null;\n-        }\n+        killJob();\n         return 5;\n       }\n \n@@ -449,7 +448,7 @@ public int execute(DriverContext driverContext) {\n \n         if (rj != null) {\n           if (returnVal != 0) {\n-            rj.killJob();\n+            killJob();\n           }\n           jobID = rj.getID().toString();\n         }\n@@ -857,22 +856,37 @@ public void logPlanProgress(SessionState ss) throws IOException {\n     ss.getHiveHistory().logPlanProgress(queryPlan);\n   }\n \n+  public boolean isTaskShutdown() {\n+    return isShutdown;\n+  }\n+\n   @Override\n   public void shutdown() {\n     super.shutdown();\n-    if (rj != null) {\n+    killJob();\n+    isShutdown = true;\n+  }\n+\n+  @Override\n+  public String getExternalHandle() {\n+    return this.jobID;\n+  }\n+\n+  private void killJob() {\n+    boolean needToKillJob = false;\n+    synchronized(this) {\n+      if (rj != null && !jobKilled) {\n+        jobKilled = true;\n+        needToKillJob = true;\n+      }\n+    }\n+    if (needToKillJob) {\n       try {\n         rj.killJob();\n       } catch (Exception e) {\n         LOG.warn(\"failed to kill job \" + rj.getID(), e);\n       }\n-      rj = null;\n     }\n   }\n-\n-  @Override\n-  public String getExternalHandle() {\n-    return this.jobID;\n-  }\n }\n ",
                "raw_url": "https://github.com/apache/hive/raw/a496e581152425773080aac48cf479e493cd5b74/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java",
                "sha": "20ecbcdc6fd7a224316ddfd9d3992c5b9cbe261d",
                "status": "modified"
            }
        ],
        "message": "HIVE-16433: Not nullify variable \"rj\" to avoid NPE due to race condition in ExecDriver (Zhihai Xu via Jimmy Xiang)",
        "parent": "https://github.com/apache/hive/commit/e5a6b30241c166c82d082effd72967dc25804f97",
        "patched_files": [
            "ExecDriver.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestExecDriver.java"
        ]
    },
    "hive_a6bfabe": {
        "bug_id": "hive_a6bfabe",
        "commit": "https://github.com/apache/hive/commit/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/itests/src/test/resources/testconfiguration.properties",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f",
                "deletions": 1,
                "filename": "itests/src/test/resources/testconfiguration.properties",
                "patch": "@@ -305,7 +305,8 @@ minitez.query.files=bucket_map_join_tez1.q,\\\n   tez_smb_main.q,\\\n   tez_smb_1.q,\\\n   vectorized_dynamic_partition_pruning.q,\\\n-  tez_multi_union.q\n+  tez_multi_union.q,\\\n+  tez_join.q\n \n encrypted.query.files=encryption_join_unencrypted_tbl.q,\\\n   encryption_insert_partition_static.q,\\",
                "raw_url": "https://github.com/apache/hive/raw/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/itests/src/test/resources/testconfiguration.properties",
                "sha": "91dcc03381d317decaad329031bb31b0b61e8bc4",
                "status": "modified"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/hive/blob/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java",
                "changes": 75,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java?ref=a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f",
                "deletions": 37,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java",
                "patch": "@@ -111,7 +111,7 @@\n         }\n \n         if (parentOp instanceof ReduceSinkOperator) {\n-          ReduceSinkOperator rs = (ReduceSinkOperator)parentOp;\n+          ReduceSinkOperator rs = (ReduceSinkOperator) parentOp;\n           estimatedBuckets = (estimatedBuckets < rs.getConf().getNumReducers()) ?\n               rs.getConf().getNumReducers() : estimatedBuckets;\n         }\n@@ -133,10 +133,10 @@\n       if (retval == null) {\n         return retval;\n       } else {\n-          // only case is full outer join with SMB enabled which is not possible. Convert to regular\n-          // join.\n-          convertJoinSMBJoin(joinOp, context, 0, 0, false, false);\n-          return null;\n+        // only case is full outer join with SMB enabled which is not possible. Convert to regular\n+        // join.\n+        convertJoinSMBJoin(joinOp, context, 0, 0, false, false);\n+        return null;\n       }\n     }\n \n@@ -160,8 +160,10 @@\n     }\n \n     MapJoinOperator mapJoinOp = convertJoinMapJoin(joinOp, context, mapJoinConversionPos);\n-    // map join operator by default has no bucket cols\n-    mapJoinOp.setOpTraits(new OpTraits(null, -1, null));\n+    // map join operator by default has no bucket cols and num of reduce sinks\n+    // reduced by 1\n+    mapJoinOp\n+        .setOpTraits(new OpTraits(null, -1, null, joinOp.getOpTraits().getNumReduceSinks()));\n     mapJoinOp.setStatistics(joinOp.getStatistics());\n     // propagate this change till the next RS\n     for (Operator<? extends OperatorDesc> childOp : mapJoinOp.getChildOperators()) {\n@@ -176,7 +178,8 @@ private Object checkAndConvertSMBJoin(OptimizeTezProcContext context, JoinOperat\n       TezBucketJoinProcCtx tezBucketJoinProcCtx) throws SemanticException {\n     // we cannot convert to bucket map join, we cannot convert to\n     // map join either based on the size. Check if we can convert to SMB join.\n-    if (context.conf.getBoolVar(HiveConf.ConfVars.HIVE_AUTO_SORTMERGE_JOIN) == false) {\n+    if ((context.conf.getBoolVar(HiveConf.ConfVars.HIVE_AUTO_SORTMERGE_JOIN) == false)\n+        || (joinOp.getOpTraits().getNumReduceSinks() >= 2)) {\n       convertJoinSMBJoin(joinOp, context, 0, 0, false, false);\n       return null;\n     }\n@@ -221,7 +224,7 @@ private Object checkAndConvertSMBJoin(OptimizeTezProcContext context, JoinOperat\n       convertJoinSMBJoin(joinOp, context, pos, 0, false, false);\n     }\n     return null;\n-}\n+  }\n \n   // replaces the join operator with a new CommonJoinOperator, removes the\n   // parent reduce sinks\n@@ -240,9 +243,9 @@ private void convertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcContext cont\n           new MapJoinDesc(\n                   MapJoinProcessor.getKeys(joinOp.getConf().isLeftInputJoin(),\n                   joinOp.getConf().getBaseSrc(), joinOp).getSecond(),\n-              null, joinDesc.getExprs(), null, null,\n-              joinDesc.getOutputColumnNames(), mapJoinConversionPos, joinDesc.getConds(),\n-              joinDesc.getFilters(), joinDesc.getNoOuterJoin(), null);\n+                  null, joinDesc.getExprs(), null, null,\n+                  joinDesc.getOutputColumnNames(), mapJoinConversionPos, joinDesc.getConds(),\n+                  joinDesc.getFilters(), joinDesc.getNoOuterJoin(), null);\n       mapJoinDesc.setNullSafes(joinDesc.getNullSafes());\n       mapJoinDesc.setFilterMap(joinDesc.getFilterMap());\n       mapJoinDesc.resetOrder();\n@@ -251,9 +254,9 @@ private void convertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcContext cont\n     CommonMergeJoinOperator mergeJoinOp =\n         (CommonMergeJoinOperator) OperatorFactory.get(new CommonMergeJoinDesc(numBuckets,\n             isSubQuery, mapJoinConversionPos, mapJoinDesc), joinOp.getSchema());\n-    OpTraits opTraits =\n-        new OpTraits(joinOp.getOpTraits().getBucketColNames(), numBuckets, joinOp.getOpTraits()\n-            .getSortCols());\n+    int numReduceSinks = joinOp.getOpTraits().getNumReduceSinks();\n+    OpTraits opTraits = new OpTraits(joinOp.getOpTraits().getBucketColNames(), numBuckets, joinOp\n+        .getOpTraits().getSortCols(), numReduceSinks);\n     mergeJoinOp.setOpTraits(opTraits);\n     mergeJoinOp.setStatistics(joinOp.getStatistics());\n \n@@ -289,8 +292,7 @@ private void convertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcContext cont\n \n     if (adjustParentsChildren) {\n       mergeJoinOp.getConf().setGenJoinKeys(true);\n-      List<Operator<? extends OperatorDesc>> newParentOpList =\n-          new ArrayList<Operator<? extends OperatorDesc>>();\n+      List<Operator<? extends OperatorDesc>> newParentOpList = new ArrayList<Operator<? extends OperatorDesc>>();\n       for (Operator<? extends OperatorDesc> parentOp : mergeJoinOp.getParentOperators()) {\n         for (Operator<? extends OperatorDesc> grandParentOp : parentOp.getParentOperators()) {\n           grandParentOp.getChildOperators().remove(parentOp);\n@@ -328,7 +330,8 @@ private void setAllChildrenTraitsToNull(Operator<? extends OperatorDesc> current\n     if (currentOp instanceof ReduceSinkOperator) {\n       return;\n     }\n-    currentOp.setOpTraits(new OpTraits(null, -1, null));\n+    currentOp.setOpTraits(new OpTraits(null, -1, null,\n+        currentOp.getOpTraits().getNumReduceSinks()));\n     for (Operator<? extends OperatorDesc> childOp : currentOp.getChildOperators()) {\n       if ((childOp instanceof ReduceSinkOperator) || (childOp instanceof GroupByOperator)) {\n         break;\n@@ -351,7 +354,7 @@ private boolean convertJoinBucketMapJoin(JoinOperator joinOp, OptimizeTezProcCon\n \n     // we can set the traits for this join operator\n     OpTraits opTraits = new OpTraits(joinOp.getOpTraits().getBucketColNames(),\n-        tezBucketJoinProcCtx.getNumBuckets(), null);\n+        tezBucketJoinProcCtx.getNumBuckets(), null, joinOp.getOpTraits().getNumReduceSinks());\n     mapJoinOp.setOpTraits(opTraits);\n     mapJoinOp.setStatistics(joinOp.getStatistics());\n     setNumberOfBucketsOnChildren(mapJoinOp);\n@@ -377,8 +380,7 @@ private boolean checkConvertJoinSMBJoin(JoinOperator joinOp, OptimizeTezProcCont\n \n     ReduceSinkOperator bigTableRS =\n         (ReduceSinkOperator) joinOp.getParentOperators().get(bigTablePosition);\n-    int numBuckets = bigTableRS.getParentOperators().get(0).getOpTraits()\n-            .getNumBuckets();\n+    int numBuckets = bigTableRS.getParentOperators().get(0).getOpTraits().getNumBuckets();\n \n     // the sort and bucket cols have to match on both sides for this\n     // transformation of the join operation\n@@ -425,13 +427,12 @@ private void setNumberOfBucketsOnChildren(Operator<? extends OperatorDesc> curre\n   }\n \n   /*\n-   * If the parent reduce sink of the big table side has the same emit key cols\n-   * as its parent, we can create a bucket map join eliminating the reduce sink.\n+   * If the parent reduce sink of the big table side has the same emit key cols as its parent, we\n+   * can create a bucket map join eliminating the reduce sink.\n    */\n   private boolean checkConvertJoinBucketMapJoin(JoinOperator joinOp,\n       OptimizeTezProcContext context, int bigTablePosition,\n-      TezBucketJoinProcCtx tezBucketJoinProcCtx)\n-  throws SemanticException {\n+      TezBucketJoinProcCtx tezBucketJoinProcCtx) throws SemanticException {\n     // bail on mux-operator because mux operator masks the emit keys of the\n     // constituent reduce sinks\n     if (!(joinOp.getParentOperators().get(0) instanceof ReduceSinkOperator)) {\n@@ -453,8 +454,8 @@ private boolean checkConvertJoinBucketMapJoin(JoinOperator joinOp,\n     }\n \n     /*\n-     * this is the case when the big table is a sub-query and is probably\n-     * already bucketed by the join column in say a group by operation\n+     * this is the case when the big table is a sub-query and is probably already bucketed by the\n+     * join column in say a group by operation\n      */\n     boolean isSubQuery = false;\n     if (numBuckets < 0) {\n@@ -492,7 +493,8 @@ private boolean checkColEquality(List<List<String>> grandParentColNames,\n           // all columns need to be at least a subset of the parentOfParent's bucket cols\n           ExprNodeDesc exprNodeDesc = colExprMap.get(colName);\n           if (exprNodeDesc instanceof ExprNodeColumnDesc) {\n-            if (((ExprNodeColumnDesc)exprNodeDesc).getColumn().equals(listBucketCols.get(colCount))) {\n+            if (((ExprNodeColumnDesc) exprNodeDesc).getColumn()\n+                .equals(listBucketCols.get(colCount))) {\n               colCount++;\n             } else {\n               break;\n@@ -562,14 +564,13 @@ public int getMapJoinConversionPos(JoinOperator joinOp, OptimizeTezProcContext c\n \n       Statistics currInputStat = parentOp.getStatistics();\n       if (currInputStat == null) {\n-        LOG.warn(\"Couldn't get statistics from: \"+parentOp);\n+        LOG.warn(\"Couldn't get statistics from: \" + parentOp);\n         return -1;\n       }\n \n       long inputSize = currInputStat.getDataSize();\n-      if ((bigInputStat == null) ||\n-          ((bigInputStat != null) &&\n-          (inputSize > bigInputStat.getDataSize()))) {\n+      if ((bigInputStat == null)\n+          || ((bigInputStat != null) && (inputSize > bigInputStat.getDataSize()))) {\n \n         if (bigTableFound) {\n           // cannot convert to map join; we've already chosen a big table\n@@ -639,11 +640,11 @@ public MapJoinOperator convertJoinMapJoin(JoinOperator joinOp, OptimizeTezProcCo\n       }\n     }\n \n-    //can safely convert the join to a map join.\n+    // can safely convert the join to a map join.\n     MapJoinOperator mapJoinOp =\n         MapJoinProcessor.convertJoinOpMapJoinOp(context.conf, joinOp,\n-                joinOp.getConf().isLeftInputJoin(), joinOp.getConf().getBaseSrc(),\n-                joinOp.getConf().getMapAliases(), bigTablePosition, true);\n+            joinOp.getConf().isLeftInputJoin(), joinOp.getConf().getBaseSrc(),\n+            joinOp.getConf().getMapAliases(), bigTablePosition, true);\n \n     Operator<? extends OperatorDesc> parentBigTableOp =\n         mapJoinOp.getParentOperators().get(bigTablePosition);\n@@ -667,7 +668,7 @@ public MapJoinOperator convertJoinMapJoin(JoinOperator joinOp, OptimizeTezProcCo\n             parentBigTableOp.getParentOperators().get(0));\n       }\n       parentBigTableOp.getParentOperators().get(0).removeChild(parentBigTableOp);\n-      for (Operator<? extends OperatorDesc> op : mapJoinOp.getParentOperators()) {\n+      for (Operator<? extends OperatorDesc>op : mapJoinOp.getParentOperators()) {\n         if (!(op.getChildOperators().contains(mapJoinOp))) {\n           op.getChildOperators().add(mapJoinOp);\n         }\n@@ -681,7 +682,7 @@ public MapJoinOperator convertJoinMapJoin(JoinOperator joinOp, OptimizeTezProcCo\n   private boolean hasDynamicPartitionBroadcast(Operator<?> parent) {\n     boolean hasDynamicPartitionPruning = false;\n \n-    for (Operator<?> op: parent.getChildOperators()) {\n+    for (Operator<?> op : parent.getChildOperators()) {\n       while (op != null) {\n         if (op instanceof AppMasterEventOperator && op.getConf() instanceof DynamicPruningEventDesc) {\n           // found dynamic partition pruning operator",
                "raw_url": "https://github.com/apache/hive/raw/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ConvertJoinMapJoin.java",
                "sha": "ce4312029ebc0cc01d3cfcca60cc9cc10a8e0f51",
                "status": "modified"
            },
            {
                "additions": 62,
                "blob_url": "https://github.com/apache/hive/blob/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java",
                "changes": 96,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java?ref=a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f",
                "deletions": 34,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java",
                "patch": "@@ -82,7 +82,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n   }\n \n   /*\n-   * Reduce sink operator is the de-facto operator \n+   * Reduce sink operator is the de-facto operator\n    * for determining keyCols (emit keys of a map phase)\n    */\n   public static class ReduceSinkRule implements NodeProcessor {\n@@ -106,34 +106,37 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n       List<List<String>> listBucketCols = new ArrayList<List<String>>();\n       listBucketCols.add(bucketCols);\n       int numBuckets = -1;\n+      int numReduceSinks = 1;\n       OpTraits parentOpTraits = rs.getParentOperators().get(0).getConf().getOpTraits();\n       if (parentOpTraits != null) {\n         numBuckets = parentOpTraits.getNumBuckets();\n+        numReduceSinks += parentOpTraits.getNumReduceSinks();\n       }\n-      OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listBucketCols);\n+      OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listBucketCols, numReduceSinks);\n       rs.setOpTraits(opTraits);\n       return null;\n     }\n   }\n \n   /*\n-   * Table scan has the table object and pruned partitions that has information such as\n-   * bucketing, sorting, etc. that is used later for optimization.\n+   * Table scan has the table object and pruned partitions that has information\n+   * such as bucketing, sorting, etc. that is used later for optimization.\n    */\n   public static class TableScanRule implements NodeProcessor {\n \n-    public boolean checkBucketedTable(Table tbl, \n-        ParseContext pGraphContext,\n+    public boolean checkBucketedTable(Table tbl, ParseContext pGraphContext,\n         PrunedPartitionList prunedParts) throws SemanticException {\n \n       if (tbl.isPartitioned()) {\n         List<Partition> partitions = prunedParts.getNotDeniedPartns();\n         // construct a mapping of (Partition->bucket file names) and (Partition -> bucket number)\n         if (!partitions.isEmpty()) {\n           for (Partition p : partitions) {\n-            List<String> fileNames =\n-                AbstractBucketJoinProc.getBucketFilePathsOfPartition(p.getDataLocation(), pGraphContext);\n-            // The number of files for the table should be same as number of buckets.\n+            List<String> fileNames = \n+                AbstractBucketJoinProc.getBucketFilePathsOfPartition(p.getDataLocation(), \n+                    pGraphContext);\n+            // The number of files for the table should be same as number of\n+            // buckets.\n             int bucketCount = p.getBucketCount();\n \n             if (fileNames.size() != 0 && fileNames.size() != bucketCount) {\n@@ -143,8 +146,9 @@ public boolean checkBucketedTable(Table tbl,\n         }\n       } else {\n \n-        List<String> fileNames =\n-            AbstractBucketJoinProc.getBucketFilePathsOfPartition(tbl.getDataLocation(), pGraphContext);\n+        List<String> fileNames = \n+            AbstractBucketJoinProc.getBucketFilePathsOfPartition(tbl.getDataLocation(), \n+                pGraphContext);\n         Integer num = new Integer(tbl.getNumBuckets());\n \n         // The number of files for the table should be same as number of buckets.\n@@ -183,7 +187,8 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         }\n         sortedColsList.add(sortCols);\n       }\n-      OpTraits opTraits = new OpTraits(bucketColsList, numBuckets, sortedColsList);\n+      // num reduce sinks hardcoded to 0 because TS has no parents\n+      OpTraits opTraits = new OpTraits(bucketColsList, numBuckets, sortedColsList, 0);\n       ts.setOpTraits(opTraits);\n       return null;\n     }\n@@ -208,17 +213,22 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n       }\n \n       List<List<String>> listBucketCols = new ArrayList<List<String>>();\n+      int numReduceSinks = 0;\n+      OpTraits parentOpTraits = gbyOp.getParentOperators().get(0).getOpTraits();\n+      if (parentOpTraits != null) {\n+        numReduceSinks = parentOpTraits.getNumReduceSinks();\n+      }\n       listBucketCols.add(gbyKeys);\n-      OpTraits opTraits = new OpTraits(listBucketCols, -1, listBucketCols);\n+      OpTraits opTraits = new OpTraits(listBucketCols, -1, listBucketCols, numReduceSinks);\n       gbyOp.setOpTraits(opTraits);\n       return null;\n     }\n   }\n \n   public static class SelectRule implements NodeProcessor {\n \n-    public List<List<String>> getConvertedColNames(List<List<String>> parentColNames,\n-        SelectOperator selOp) {\n+    public List<List<String>> getConvertedColNames(\n+        List<List<String>> parentColNames, SelectOperator selOp) {\n       List<List<String>> listBucketCols = new ArrayList<List<String>>();\n       if (selOp.getColumnExprMap() != null) {\n         if (parentColNames != null) {\n@@ -244,8 +254,8 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n     @Override\n     public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         Object... nodeOutputs) throws SemanticException {\n-      SelectOperator selOp = (SelectOperator)nd;\n-      List<List<String>> parentBucketColNames =\n+      SelectOperator selOp = (SelectOperator) nd;\n+      List<List<String>> parentBucketColNames = \n           selOp.getParentOperators().get(0).getOpTraits().getBucketColNames();\n \n       List<List<String>> listBucketCols = null;\n@@ -254,18 +264,21 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         if (parentBucketColNames != null) {\n           listBucketCols = getConvertedColNames(parentBucketColNames, selOp);\n         }\n-        List<List<String>> parentSortColNames = selOp.getParentOperators().get(0).getOpTraits()\n-            .getSortCols();\n+        List<List<String>> parentSortColNames = \n+            selOp.getParentOperators().get(0).getOpTraits().getSortCols();\n         if (parentSortColNames != null) {\n           listSortCols = getConvertedColNames(parentSortColNames, selOp);\n         }\n       }\n \n       int numBuckets = -1;\n-      if (selOp.getParentOperators().get(0).getOpTraits() != null) {\n-        numBuckets = selOp.getParentOperators().get(0).getOpTraits().getNumBuckets();\n+      int numReduceSinks = 0;\n+      OpTraits parentOpTraits = selOp.getParentOperators().get(0).getOpTraits();\n+      if (parentOpTraits != null) {\n+        numBuckets = parentOpTraits.getNumBuckets();\n+        numReduceSinks = parentOpTraits.getNumReduceSinks();\n       }\n-      OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listSortCols);\n+      OpTraits opTraits = new OpTraits(listBucketCols, numBuckets, listSortCols, numReduceSinks);\n       selOp.setOpTraits(opTraits);\n       return null;\n     }\n@@ -276,26 +289,31 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n     @Override\n     public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         Object... nodeOutputs) throws SemanticException {\n-      JoinOperator joinOp = (JoinOperator)nd;\n+      JoinOperator joinOp = (JoinOperator) nd;\n       List<List<String>> bucketColsList = new ArrayList<List<String>>();\n       List<List<String>> sortColsList = new ArrayList<List<String>>();\n       byte pos = 0;\n+      int numReduceSinks = 0; // will be set to the larger of the parents\n       for (Operator<? extends OperatorDesc> parentOp : joinOp.getParentOperators()) {\n         if (!(parentOp instanceof ReduceSinkOperator)) {\n           // can be mux operator\n           break;\n         }\n-        ReduceSinkOperator rsOp = (ReduceSinkOperator)parentOp;\n+        ReduceSinkOperator rsOp = (ReduceSinkOperator) parentOp;\n         if (rsOp.getOpTraits() == null) {\n           ReduceSinkRule rsRule = new ReduceSinkRule();\n           rsRule.process(rsOp, stack, procCtx, nodeOutputs);\n         }\n-        bucketColsList.add(getOutputColNames(joinOp, rsOp.getOpTraits().getBucketColNames(), pos));\n-        sortColsList.add(getOutputColNames(joinOp, rsOp.getOpTraits().getSortCols(), pos));\n+        OpTraits parentOpTraits = rsOp.getOpTraits();\n+        bucketColsList.add(getOutputColNames(joinOp, parentOpTraits.getBucketColNames(), pos));\n+        sortColsList.add(getOutputColNames(joinOp, parentOpTraits.getSortCols(), pos));\n+        if (parentOpTraits.getNumReduceSinks() > numReduceSinks) {\n+          numReduceSinks = parentOpTraits.getNumReduceSinks();\n+        }\n         pos++;\n       }\n \n-      joinOp.setOpTraits(new OpTraits(bucketColsList, -1, bucketColsList));\n+      joinOp.setOpTraits(new OpTraits(bucketColsList, -1, bucketColsList, numReduceSinks));\n       return null;\n     }\n \n@@ -311,7 +329,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         for (String colName : colNames) {\n           for (ExprNodeDesc exprNode : joinOp.getConf().getExprs().get(pos)) {\n             if (exprNode instanceof ExprNodeColumnDesc) {\n-              if(((ExprNodeColumnDesc)(exprNode)).getColumn().equals(colName)) {\n+              if (((ExprNodeColumnDesc) (exprNode)).getColumn().equals(colName)) {\n                 for (Entry<String, ExprNodeDesc> entry : joinOp.getColumnExprMap().entrySet()) {\n                   if (entry.getValue().isSame(exprNode)) {\n                     bucketColNames.add(entry.getKey());\n@@ -338,20 +356,30 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n   }\n \n   /*\n-   *  When we have operators that have multiple parents, it is not\n-   *  clear which parent's traits we need to propagate forward.\n+   * When we have operators that have multiple parents, it is not clear which\n+   * parent's traits we need to propagate forward.\n    */\n   public static class MultiParentRule implements NodeProcessor {\n \n     @Override\n     public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procCtx,\n         Object... nodeOutputs) throws SemanticException {\n-      OpTraits opTraits = new OpTraits(null, -1, null);\n       @SuppressWarnings(\"unchecked\")\n-      Operator<? extends OperatorDesc> operator = (Operator<? extends OperatorDesc>)nd;\n+      Operator<? extends OperatorDesc> operator = (Operator<? extends OperatorDesc>) nd;\n+\n+      int numReduceSinks = 0;\n+      for (Operator<?> parentOp : operator.getParentOperators()) {\n+        if (parentOp.getOpTraits() == null) {\n+          continue;\n+        }\n+        if (parentOp.getOpTraits().getNumReduceSinks() > numReduceSinks) {\n+          numReduceSinks = parentOp.getOpTraits().getNumReduceSinks();\n+        }\n+      }\n+      OpTraits opTraits = new OpTraits(null, -1, null, numReduceSinks);\n       operator.setOpTraits(opTraits);\n       return null;\n-    } \n+    }\n   }\n \n   public static NodeProcessor getTableScanRule() {\n@@ -361,7 +389,7 @@ public static NodeProcessor getTableScanRule() {\n   public static NodeProcessor getReduceSinkRule() {\n     return new ReduceSinkRule();\n   }\n-  \n+\n   public static NodeProcessor getSelectRule() {\n     return new SelectRule();\n   }",
                "raw_url": "https://github.com/apache/hive/raw/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/java/org/apache/hadoop/hive/ql/optimizer/metainfo/annotation/OpTraitsRulesProcFactory.java",
                "sha": "62428dbdd61d28b63d59388f00b3c0c0d328d593",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java?ref=a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java",
                "patch": "@@ -103,7 +103,7 @@\n     }\n \n     // we can set the traits for this join operator\n-    OpTraits opTraits = new OpTraits(bucketColNames, numBuckets, null);\n+    OpTraits opTraits = new OpTraits(bucketColNames, numBuckets, null, joinOp.getOpTraits().getNumReduceSinks());\n     mapJoinOp.setOpTraits(opTraits);\n     mapJoinOp.setStatistics(joinOp.getStatistics());\n     setNumberOfBucketsOnChildren(mapJoinOp);",
                "raw_url": "https://github.com/apache/hive/raw/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/java/org/apache/hadoop/hive/ql/optimizer/spark/SparkMapJoinOptimizer.java",
                "sha": "a455175efccf401b2286e32e731bc970de768f90",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java?ref=a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java",
                "patch": "@@ -25,11 +25,14 @@\n   List<List<String>> bucketColNames;\n   List<List<String>> sortColNames;\n   int numBuckets;\n+  int numReduceSinks;\n \n-  public OpTraits(List<List<String>> bucketColNames, int numBuckets, List<List<String>> sortColNames) {\n+  public OpTraits(List<List<String>> bucketColNames, int numBuckets,\n+      List<List<String>> sortColNames, int numReduceSinks) {\n     this.bucketColNames = bucketColNames;\n     this.numBuckets = numBuckets;\n     this.sortColNames = sortColNames;\n+    this.numReduceSinks = numReduceSinks;\n   }\n \n   public List<List<String>> getBucketColNames() {\n@@ -55,4 +58,12 @@ public void setSortColNames(List<List<String>> sortColNames) {\n   public List<List<String>> getSortCols() {\n     return sortColNames;\n   }\n+\n+  public void setNumReduceSinks(int numReduceSinks) {\n+    this.numReduceSinks = numReduceSinks;\n+  }\n+\n+  public int getNumReduceSinks() {\n+    return this.numReduceSinks;\n+  }\n }",
                "raw_url": "https://github.com/apache/hive/raw/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/java/org/apache/hadoop/hive/ql/plan/OpTraits.java",
                "sha": "a687a3d0b9ae439066a09550becc75be2bb2327e",
                "status": "modified"
            },
            {
                "additions": 43,
                "blob_url": "https://github.com/apache/hive/blob/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/test/queries/clientpositive/tez_join.q",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/tez_join.q?ref=a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/tez_join.q",
                "patch": "@@ -0,0 +1,43 @@\n+set hive.auto.convert.sortmerge.join = true;\n+\n+create table t1(\n+id string,\n+od string);\n+\n+create table t2(\n+id string,\n+od string);\n+\n+explain\n+select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od from t1 order by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od from t2 order by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id;\n+\n+select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od from t1 order by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od from t2 order by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id;\n+\n+explain\n+select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od, count(*) from t1 group by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od, count(*) from t2 group by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id;\n+\n+select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od, count(*) from t1 group by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od, count(*) from t2 group by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id;",
                "raw_url": "https://github.com/apache/hive/raw/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/test/queries/clientpositive/tez_join.q",
                "sha": "d35ec835c74dc925148a46ceda908a5e315006db",
                "status": "added"
            },
            {
                "additions": 320,
                "blob_url": "https://github.com/apache/hive/blob/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/test/results/clientpositive/tez/tez_join.q.out",
                "changes": 320,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_join.q.out?ref=a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/tez/tez_join.q.out",
                "patch": "@@ -0,0 +1,320 @@\n+PREHOOK: query: create table t1(\n+id string,\n+od string)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@t1\n+POSTHOOK: query: create table t1(\n+id string,\n+od string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@t1\n+PREHOOK: query: create table t2(\n+id string,\n+od string)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@t2\n+POSTHOOK: query: create table t2(\n+id string,\n+od string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@t2\n+PREHOOK: query: explain\n+select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od from t1 order by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od from t2 order by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od from t1 order by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od from t2 order by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE)\n+        Reducer 3 <- Reducer 2 (SIMPLE_EDGE), Reducer 5 (SIMPLE_EDGE)\n+        Reducer 5 <- Map 4 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: t1\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: id is not null (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Select Operator\n+                      expressions: id (type: string), od (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: string), _col1 (type: string)\n+                        sort order: ++\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: t2\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: id is not null (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Select Operator\n+                      expressions: id (type: string), od (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: string), _col1 (type: string)\n+                        sort order: ++\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Select Operator\n+                expressions: KEY.reducesinkkey0 (type: string)\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col0 (type: string)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+        Reducer 3 \n+            Reduce Operator Tree:\n+              Merge Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                keys:\n+                  0 _col0 (type: string)\n+                  1 _col0 (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Filter Operator\n+                  predicate: (_col0 = _col1) (type: boolean)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Select Operator\n+                    expressions: _col0 (type: string)\n+                    outputColumnNames: _col0\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    File Output Operator\n+                      compressed: false\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Reducer 5 \n+            Reduce Operator Tree:\n+              Select Operator\n+                expressions: KEY.reducesinkkey0 (type: string)\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col0 (type: string)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od from t1 order by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od from t2 order by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@t1\n+PREHOOK: Input: default@t2\n+#### A masked pattern was here ####\n+POSTHOOK: query: select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od from t1 order by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od from t2 order by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@t1\n+POSTHOOK: Input: default@t2\n+#### A masked pattern was here ####\n+PREHOOK: query: explain\n+select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od, count(*) from t1 group by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od, count(*) from t2 group by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od, count(*) from t1 group by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od, count(*) from t2 group by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE)\n+        Reducer 3 <- Reducer 2 (SIMPLE_EDGE), Reducer 5 (SIMPLE_EDGE)\n+        Reducer 5 <- Map 4 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: t1\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: id is not null (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count()\n+                      keys: id (type: string), od (type: string)\n+                      mode: hash\n+                      outputColumnNames: _col0, _col1, _col2\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: string), _col1 (type: string)\n+                        sort order: ++\n+                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                        value expressions: _col2 (type: bigint)\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: t2\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: id is not null (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count()\n+                      keys: id (type: string), od (type: string)\n+                      mode: hash\n+                      outputColumnNames: _col0, _col1, _col2\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: string), _col1 (type: string)\n+                        sort order: ++\n+                        Map-reduce partition columns: _col0 (type: string), _col1 (type: string)\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                        value expressions: _col2 (type: bigint)\n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                keys: KEY._col0 (type: string), KEY._col1 (type: string)\n+                mode: mergepartial\n+                outputColumnNames: _col0, _col1, _col2\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Select Operator\n+                  expressions: _col0 (type: string)\n+                  outputColumnNames: _col0\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: _col0 (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: _col0 (type: string)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+        Reducer 3 \n+            Reduce Operator Tree:\n+              Merge Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                keys:\n+                  0 _col0 (type: string)\n+                  1 _col0 (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Filter Operator\n+                  predicate: (_col0 = _col1) (type: boolean)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Select Operator\n+                    expressions: _col0 (type: string)\n+                    outputColumnNames: _col0\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    File Output Operator\n+                      compressed: false\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Reducer 5 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                keys: KEY._col0 (type: string), KEY._col1 (type: string)\n+                mode: mergepartial\n+                outputColumnNames: _col0, _col1, _col2\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Select Operator\n+                  expressions: _col0 (type: string)\n+                  outputColumnNames: _col0\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: _col0 (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: _col0 (type: string)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od, count(*) from t1 group by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od, count(*) from t2 group by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@t1\n+PREHOOK: Input: default@t2\n+#### A masked pattern was here ####\n+POSTHOOK: query: select vt1.id from\n+(select rt1.id from\n+(select t1.id, t1.od, count(*) from t1 group by t1.id, t1.od) rt1) vt1\n+join\n+(select rt2.id from\n+(select t2.id, t2.od, count(*) from t2 group by t2.id, t2.od) rt2) vt2\n+where vt1.id=vt2.id\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@t1\n+POSTHOOK: Input: default@t2\n+#### A masked pattern was here ####",
                "raw_url": "https://github.com/apache/hive/raw/a6bfabefb1a61d03e6029fbdeb3399fa5a9d611f/ql/src/test/results/clientpositive/tez/tez_join.q.out",
                "sha": "a051dc7cbe006aa99204a4a6f755bcd431d4649c",
                "status": "added"
            }
        ],
        "message": "HIVE-9886: Hive on tez: NPE when converting join to SMB in sub-query (Vikram Dixit K, reviewed by Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1665380 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/5461b9bbde3cf960ffbfc399491a55de7585fd79",
        "patched_files": [
            "tez_join.java",
            "OpTraits.java",
            "OpTraitsRulesProcFactory.java",
            "ConvertJoinMapJoin.java",
            "SparkMapJoinOptimizer.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "testconfiguration.java"
        ]
    },
    "hive_a9d35c7": {
        "bug_id": "hive_a9d35c7",
        "commit": "https://github.com/apache/hive/commit/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2",
                "deletions": 13,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java",
                "patch": "@@ -657,7 +657,7 @@ private int showGrantsV2(ShowGrantDesc showGrantDesc) throws HiveException {\n \n         PrivilegeGrantInfo grantInfo =\n             AuthorizationUtils.getThriftPrivilegeGrantInfo(priv, privInfo.getGrantorPrincipal(),\n-                privInfo.isGrantOption());\n+                privInfo.isGrantOption(), privInfo.getGrantTime());\n \n         //only grantInfo is used\n         HiveObjectPrivilege thriftObjectPriv = new HiveObjectPrivilege(new HiveObjectRef(\n@@ -674,18 +674,6 @@ private int showGrantsV2(ShowGrantDesc showGrantDesc) throws HiveException {\n     return 0;\n   }\n \n-  private static void sortPrivileges(List<HiveObjectPrivilege> privileges) {\n-    Collections.sort(privileges, new Comparator<HiveObjectPrivilege>() {\n-\n-      @Override\n-      public int compare(HiveObjectPrivilege one, HiveObjectPrivilege other) {\n-        return one.getGrantInfo().getPrivilege().compareTo(other.getGrantInfo().getPrivilege());\n-      }\n-\n-    });\n-\n-  }\n-\n   private int grantOrRevokePrivileges(List<PrincipalDesc> principals,\n       List<PrivilegeDesc> privileges, PrivilegeObjectDesc privSubjectDesc,\n       String grantor, PrincipalType grantorType, boolean grantOption, boolean isGrant)\n@@ -854,6 +842,7 @@ private int grantOrRevokePrivilegesV2(List<PrincipalDesc> principals,\n \n   private HivePrivilegeObject getHivePrivilegeObject(PrivilegeObjectDesc privSubjectDesc)\n       throws HiveException {\n+\n     String [] dbTable = Utilities.getDbTableName(privSubjectDesc.getObject());\n     return new HivePrivilegeObject(getPrivObjectType(privSubjectDesc), dbTable[0], dbTable[1]);\n   }\n@@ -877,6 +866,9 @@ private HivePrincipalType getHivePrincipalType(PrincipalType type) throws HiveEx\n   }\n \n   private HivePrivilegeObjectType getPrivObjectType(PrivilegeObjectDesc privSubjectDesc) {\n+    if (privSubjectDesc.getObject() == null) {\n+      return null;\n+    }\n     return privSubjectDesc.getTable() ? HivePrivilegeObjectType.TABLE_OR_VIEW : HivePrivilegeObjectType.DATABASE;\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java",
                "sha": "42df435f3e0fa8ec2c347c6a598e9c74fe0cffcf",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2",
                "deletions": 3,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "patch": "@@ -70,9 +70,6 @@\n import java.util.Random;\n import java.util.Set;\n import java.util.UUID;\n-import java.util.zip.Deflater;\n-import java.util.zip.DeflaterOutputStream;\n-import java.util.zip.InflaterInputStream;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.Future;\n@@ -81,6 +78,9 @@\n import java.util.concurrent.TimeUnit;\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n+import java.util.zip.Deflater;\n+import java.util.zip.DeflaterOutputStream;\n+import java.util.zip.InflaterInputStream;\n \n import org.antlr.runtime.CommonToken;\n import org.apache.commons.codec.binary.Base64;\n@@ -2029,6 +2029,9 @@ public static String formatBinaryString(byte[] array, int start, int length) {\n    * @throws HiveException\n    */\n   public static String[] getDbTableName(String dbtable) throws HiveException{\n+    if(dbtable == null){\n+      return new String[2];\n+    }\n     String[] names =  dbtable.split(\"\\\\.\");\n     switch (names.length) {\n     case 2:",
                "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "sha": "9a74fa5800841ad7cbf9beed4abca0981bacf161",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationUtils.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationUtils.java?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2",
                "deletions": 2,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationUtils.java",
                "patch": "@@ -108,12 +108,13 @@ public static PrincipalType getThriftPrincipalType(HivePrincipalType type) {\n    * @param privilege\n    * @param grantorPrincipal\n    * @param grantOption\n+   * @param grantTime\n    * @return\n    * @throws HiveException\n    */\n   public static PrivilegeGrantInfo getThriftPrivilegeGrantInfo(HivePrivilege privilege,\n-      HivePrincipal grantorPrincipal, boolean grantOption) throws HiveException {\n-    return new PrivilegeGrantInfo(privilege.getName(), 0 /* time gets added by server */,\n+      HivePrincipal grantorPrincipal, boolean grantOption, int grantTime) throws HiveException {\n+    return new PrivilegeGrantInfo(privilege.getName(), grantTime,\n         grantorPrincipal.getName(), getThriftPrincipalType(grantorPrincipal.getType()), grantOption);\n   }\n \n@@ -125,6 +126,9 @@ public static PrivilegeGrantInfo getThriftPrivilegeGrantInfo(HivePrivilege privi\n    * @throws HiveException\n    */\n   public static HiveObjectType getThriftHiveObjType(HivePrivilegeObjectType type) throws HiveException {\n+    if (type == null) {\n+      return null;\n+    }\n     switch(type){\n     case DATABASE:\n       return HiveObjectType.DATABASE;",
                "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/AuthorizationUtils.java",
                "sha": "5e2d12c57bae003ddac6dc9b5732c1f4df8516ec",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeInfo.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeInfo.java?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeInfo.java",
                "patch": "@@ -31,14 +31,17 @@\n   private final HivePrivilegeObject object;\n   private final HivePrincipal grantorPrincipal;\n   private final boolean grantOption;\n+  private final int grantTime;\n \n   public HivePrivilegeInfo(HivePrincipal principal, HivePrivilege privilege,\n-      HivePrivilegeObject object, HivePrincipal grantorPrincipal, boolean grantOption){\n+      HivePrivilegeObject object, HivePrincipal grantorPrincipal, boolean grantOption,\n+      int grantTime){\n     this.principal = principal;\n     this.privilege = privilege;\n     this.object = object;\n     this.grantorPrincipal = grantorPrincipal;\n     this.grantOption = grantOption;\n+    this.grantTime = grantTime;\n   }\n \n   public HivePrincipal getPrincipal() {\n@@ -61,5 +64,9 @@ public boolean isGrantOption() {\n     return grantOption;\n   }\n \n+  public int getGrantTime() {\n+    return grantTime;\n+  }\n+\n \n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/HivePrivilegeInfo.java",
                "sha": "0f91ccbc08614f459b4407c6033c673e27c98611",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2",
                "deletions": 5,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java",
                "patch": "@@ -35,15 +35,13 @@\n import org.apache.hadoop.fs.permission.FsAction;\n import org.apache.hadoop.hive.common.FileUtils;\n import org.apache.hadoop.hive.conf.HiveConf;\n-import org.apache.hadoop.hive.metastore.HiveMetaStore;\n import org.apache.hadoop.hive.metastore.IMetaStoreClient;\n import org.apache.hadoop.hive.metastore.MetaStoreUtils;\n import org.apache.hadoop.hive.metastore.api.Database;\n import org.apache.hadoop.hive.metastore.api.HiveObjectPrivilege;\n import org.apache.hadoop.hive.metastore.api.HiveObjectRef;\n import org.apache.hadoop.hive.metastore.api.HiveObjectType;\n import org.apache.hadoop.hive.metastore.api.MetaException;\n-import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n import org.apache.hadoop.hive.metastore.api.PrincipalPrivilegeSet;\n import org.apache.hadoop.hive.metastore.api.PrivilegeBag;\n import org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo;\n@@ -91,7 +89,7 @@ static PrivilegeBag getThriftPrivilegesBag(List<HivePrincipal> hivePrincipals,\n             + \" is not supported in sql standard authorization mode\");\n       }\n       PrivilegeGrantInfo grantInfo = getThriftPrivilegeGrantInfo(privilege, grantorPrincipal,\n-          grantOption);\n+          grantOption, 0 /*real grant time added by metastore*/);\n       for (HivePrincipal principal : hivePrincipals) {\n         HiveObjectPrivilege objPriv = new HiveObjectPrivilege(privObj, principal.getName(),\n             AuthorizationUtils.getThriftPrincipalType(principal.getType()), grantInfo);\n@@ -102,10 +100,11 @@ static PrivilegeBag getThriftPrivilegesBag(List<HivePrincipal> hivePrincipals,\n   }\n \n   static PrivilegeGrantInfo getThriftPrivilegeGrantInfo(HivePrivilege privilege,\n-      HivePrincipal grantorPrincipal, boolean grantOption) throws HiveAuthzPluginException {\n+      HivePrincipal grantorPrincipal, boolean grantOption, int grantTime)\n+          throws HiveAuthzPluginException {\n     try {\n       return AuthorizationUtils.getThriftPrivilegeGrantInfo(privilege, grantorPrincipal,\n-          grantOption);\n+          grantOption, grantTime);\n     } catch (HiveException e) {\n       throw new HiveAuthzPluginException(e);\n     }",
                "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLAuthorizationUtils.java",
                "sha": "03d12ca107cbcd3cf3f00f1517df50ad5c2c93ad",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAccessController.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAccessController.java?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2",
                "deletions": 46,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAccessController.java",
                "patch": "@@ -39,7 +39,6 @@\n import org.apache.hadoop.hive.metastore.api.PrivilegeGrantInfo;\n import org.apache.hadoop.hive.metastore.api.Role;\n import org.apache.hadoop.hive.metastore.api.RolePrincipalGrant;\n-import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hadoop.hive.ql.security.HiveAuthenticationProvider;\n import org.apache.hadoop.hive.ql.security.authorization.AuthorizationUtils;\n import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveAccessControlException;\n@@ -156,7 +155,7 @@ public void grantPrivileges(List<HivePrincipal> hivePrincipals,\n         metastoreClient, authenticator.getUserName(), getCurrentRoles(), isUserAdmin());\n \n     // grant\n-    PrivilegeBag privBag = getThriftPrivilegesBag(hivePrincipals, hivePrivileges, hivePrivObject,\n+    PrivilegeBag privBag = SQLAuthorizationUtils.getThriftPrivilegesBag(hivePrincipals, hivePrivileges, hivePrivObject,\n         grantorPrincipal, grantOption);\n     try {\n       metastoreClient.grant_privileges(privBag);\n@@ -188,49 +187,6 @@ public void grantPrivileges(List<HivePrincipal> hivePrincipals,\n     return new ArrayList<HivePrivilege>(hivePrivSet);\n   }\n \n-  /**\n-   * Create thrift privileges bag\n-   *\n-   * @param hivePrincipals\n-   * @param hivePrivileges\n-   * @param hivePrivObject\n-   * @param grantorPrincipal\n-   * @param grantOption\n-   * @return\n-   * @throws HiveAuthzPluginException\n-   */\n-  private PrivilegeBag getThriftPrivilegesBag(List<HivePrincipal> hivePrincipals,\n-      List<HivePrivilege> hivePrivileges, HivePrivilegeObject hivePrivObject,\n-      HivePrincipal grantorPrincipal, boolean grantOption) throws HiveAuthzPluginException {\n-\n-    HiveObjectRef privObj = SQLAuthorizationUtils.getThriftHiveObjectRef(hivePrivObject);\n-    PrivilegeBag privBag = new PrivilegeBag();\n-    for (HivePrivilege privilege : hivePrivileges) {\n-      if (privilege.getColumns() != null && privilege.getColumns().size() > 0) {\n-        throw new HiveAuthzPluginException(\"Privileges on columns not supported currently\"\n-            + \" in sql standard authorization mode\");\n-      }\n-\n-      PrivilegeGrantInfo grantInfo = getThriftPrivilegeGrantInfo(privilege, grantorPrincipal,\n-          grantOption);\n-      for (HivePrincipal principal : hivePrincipals) {\n-        HiveObjectPrivilege objPriv = new HiveObjectPrivilege(privObj, principal.getName(),\n-            AuthorizationUtils.getThriftPrincipalType(principal.getType()), grantInfo);\n-        privBag.addToPrivileges(objPriv);\n-      }\n-    }\n-    return privBag;\n-  }\n-\n-  private PrivilegeGrantInfo getThriftPrivilegeGrantInfo(HivePrivilege privilege,\n-      HivePrincipal grantorPrincipal, boolean grantOption) throws HiveAuthzPluginException {\n-    try {\n-      return AuthorizationUtils.getThriftPrivilegeGrantInfo(privilege, grantorPrincipal,\n-          grantOption);\n-    } catch (HiveException e) {\n-      throw new HiveAuthzPluginException(e);\n-    }\n-  }\n \n   @Override\n   public void revokePrivileges(List<HivePrincipal> hivePrincipals,\n@@ -430,7 +386,7 @@ public void revokeRole(List<HivePrincipal> hivePrincipals, List<String> roleName\n             AuthorizationUtils.getHivePrincipalType(msGrantInfo.getGrantorType()));\n \n         HivePrivilegeInfo resPrivInfo = new HivePrivilegeInfo(resPrincipal, resPrivilege,\n-            resPrivObj, grantorPrincipal, msGrantInfo.isGrantOption());\n+            resPrivObj, grantorPrincipal, msGrantInfo.isGrantOption(), msGrantInfo.getCreateTime());\n         resPrivInfos.add(resPrivInfo);\n       }\n       return resPrivInfos;",
                "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/java/org/apache/hadoop/hive/ql/security/authorization/plugin/sqlstd/SQLStdHiveAccessController.java",
                "sha": "fec5eae72c6ff5bcdd4da7f379c86f81edb185dc",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/test/queries/clientpositive/authorization_revoke_table_priv.q",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/authorization_revoke_table_priv.q?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2",
                "deletions": 1,
                "filename": "ql/src/test/queries/clientpositive/authorization_revoke_table_priv.q",
                "patch": "@@ -9,6 +9,7 @@ CREATE TABLE table_priv_rev(i int);\n -- grant insert privilege to user2\n GRANT INSERT ON table_priv_rev TO USER user2;\n SHOW GRANT USER user2 ON TABLE table_priv_rev;\n+SHOW GRANT USER user2 ON ALL;\n \n -- revoke insert privilege from user2\n REVOKE INSERT ON TABLE table_priv_rev FROM USER user2;\n@@ -18,6 +19,7 @@ SHOW GRANT USER user2 ON TABLE table_priv_rev;\n -- grant insert privilege to user2\n GRANT INSERT ON table_priv_rev TO USER user2;\n SHOW GRANT USER user2 ON TABLE table_priv_rev;\n+SHOW GRANT USER user2 ON ALL;\n \n -- grant select privilege to user2, with grant option\n GRANT SELECT ON table_priv_rev TO USER user2 WITH GRANT OPTION;\n@@ -31,10 +33,12 @@ SHOW GRANT USER user2 ON TABLE table_priv_rev;\n GRANT DELETE ON table_priv_rev TO USER user2;\n SHOW GRANT USER user2 ON TABLE table_priv_rev;\n \n+\n -- start revoking --\n -- revoke update privilege from user2\n REVOKE UPDATE ON TABLE table_priv_rev FROM USER user2;\n SHOW GRANT USER user2 ON TABLE table_priv_rev;\n+SHOW GRANT USER user2 ON ALL;\n \n -- revoke DELETE privilege from user2\n REVOKE DELETE ON TABLE table_priv_rev FROM USER user2;\n@@ -47,7 +51,7 @@ SHOW GRANT USER user2 ON TABLE table_priv_rev;\n -- revoke select privilege from user2\n REVOKE SELECT ON TABLE table_priv_rev FROM USER user2;\n SHOW GRANT USER user2 ON TABLE table_priv_rev;\n-\n+SHOW GRANT USER user2 ON ALL;\n \n -- grant all followed by revoke all\n GRANT ALL ON table_priv_rev TO USER user2;",
                "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/test/queries/clientpositive/authorization_revoke_table_priv.q",
                "sha": "2e384d7f1227efa111e4552ef27338d3d435c17c",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/test/queries/clientpositive/authorization_view_sqlstd.q",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/authorization_view_sqlstd.q?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/authorization_view_sqlstd.q",
                "patch": "@@ -12,6 +12,8 @@ create view vt1 as select i,k from t1;\n -- protecting certain rows\n create view vt2 as select * from t1 where i > 1;\n \n+show grant user user1 on all;\n+\n --view grant to user\n -- try with and without table keyword\n \n@@ -21,20 +23,24 @@ grant insert on table vt1 to user user3;\n show grant user user2 on table vt1;\n show grant user user3 on table vt1;\n \n+\n set user.name=user2;\n select * from vt1;\n \n set user.name=user1;\n \n grant all on table vt2 to user user2;\n show grant user user2 on table vt2;\n+show grant user user2 on all;\n \n revoke all on vt2 from user user2;\n show grant user user2 on table vt2;\n \n revoke select on table vt1 from user user2;\n show grant user user2 on table vt1;\n \n+show grant user user2 on all;\n+\n -- grant privileges on roles for view, after next statement\n show grant user user3 on table vt1;\n ",
                "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/test/queries/clientpositive/authorization_view_sqlstd.q",
                "sha": "915237acb93957cd0f061f10fca42c8a5e7602ea",
                "status": "modified"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/test/results/clientpositive/authorization_revoke_table_priv.q.out",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/authorization_revoke_table_priv.q.out?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/authorization_revoke_table_priv.q.out",
                "patch": "@@ -22,6 +22,11 @@ PREHOOK: type: SHOW_GRANT\n POSTHOOK: query: SHOW GRANT USER user2 ON TABLE table_priv_rev\n POSTHOOK: type: SHOW_GRANT\n default\ttable_priv_rev\t\t\tuser2\tUSER\tINSERT\tfalse\t-1\tuser1\n+PREHOOK: query: SHOW GRANT USER user2 ON ALL\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: SHOW GRANT USER user2 ON ALL\n+POSTHOOK: type: SHOW_GRANT\n+default\ttable_priv_rev\t\t\tuser2\tUSER\tINSERT\tfalse\t-1\tuser1\n PREHOOK: query: -- revoke insert privilege from user2\n REVOKE INSERT ON TABLE table_priv_rev FROM USER user2\n PREHOOK: type: REVOKE_PRIVILEGE\n@@ -49,6 +54,11 @@ PREHOOK: type: SHOW_GRANT\n POSTHOOK: query: SHOW GRANT USER user2 ON TABLE table_priv_rev\n POSTHOOK: type: SHOW_GRANT\n default\ttable_priv_rev\t\t\tuser2\tUSER\tINSERT\tfalse\t-1\tuser1\n+PREHOOK: query: SHOW GRANT USER user2 ON ALL\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: SHOW GRANT USER user2 ON ALL\n+POSTHOOK: type: SHOW_GRANT\n+default\ttable_priv_rev\t\t\tuser2\tUSER\tINSERT\tfalse\t-1\tuser1\n PREHOOK: query: -- grant select privilege to user2, with grant option\n GRANT SELECT ON table_priv_rev TO USER user2 WITH GRANT OPTION\n PREHOOK: type: GRANT_PRIVILEGE\n@@ -111,6 +121,13 @@ POSTHOOK: type: SHOW_GRANT\n default\ttable_priv_rev\t\t\tuser2\tUSER\tDELETE\tfalse\t-1\tuser1\n default\ttable_priv_rev\t\t\tuser2\tUSER\tINSERT\tfalse\t-1\tuser1\n default\ttable_priv_rev\t\t\tuser2\tUSER\tSELECT\ttrue\t-1\tuser1\n+PREHOOK: query: SHOW GRANT USER user2 ON ALL\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: SHOW GRANT USER user2 ON ALL\n+POSTHOOK: type: SHOW_GRANT\n+default\ttable_priv_rev\t\t\tuser2\tUSER\tDELETE\tfalse\t-1\tuser1\n+default\ttable_priv_rev\t\t\tuser2\tUSER\tINSERT\tfalse\t-1\tuser1\n+default\ttable_priv_rev\t\t\tuser2\tUSER\tSELECT\ttrue\t-1\tuser1\n PREHOOK: query: -- revoke DELETE privilege from user2\n REVOKE DELETE ON TABLE table_priv_rev FROM USER user2\n PREHOOK: type: REVOKE_PRIVILEGE\n@@ -150,6 +167,10 @@ PREHOOK: query: SHOW GRANT USER user2 ON TABLE table_priv_rev\n PREHOOK: type: SHOW_GRANT\n POSTHOOK: query: SHOW GRANT USER user2 ON TABLE table_priv_rev\n POSTHOOK: type: SHOW_GRANT\n+PREHOOK: query: SHOW GRANT USER user2 ON ALL\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: SHOW GRANT USER user2 ON ALL\n+POSTHOOK: type: SHOW_GRANT\n PREHOOK: query: -- grant all followed by revoke all\n GRANT ALL ON table_priv_rev TO USER user2\n PREHOOK: type: GRANT_PRIVILEGE",
                "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/test/results/clientpositive/authorization_revoke_table_priv.q.out",
                "sha": "907c8898b662be53bae7f6a64cd1f02c3f43f547",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/hive/blob/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/test/results/clientpositive/authorization_view_sqlstd.q.out",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/authorization_view_sqlstd.q.out?ref=a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/authorization_view_sqlstd.q.out",
                "patch": "@@ -23,6 +23,22 @@ create view vt2 as select * from t1 where i > 1\n POSTHOOK: type: CREATEVIEW\n POSTHOOK: Input: default@t1\n POSTHOOK: Output: default@vt2\n+PREHOOK: query: show grant user user1 on all\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: show grant user user1 on all\n+POSTHOOK: type: SHOW_GRANT\n+default\tt1\t\t\tuser1\tUSER\tDELETE\ttrue\t-1\tuser1\n+default\tt1\t\t\tuser1\tUSER\tINSERT\ttrue\t-1\tuser1\n+default\tt1\t\t\tuser1\tUSER\tSELECT\ttrue\t-1\tuser1\n+default\tt1\t\t\tuser1\tUSER\tUPDATE\ttrue\t-1\tuser1\n+default\tvt1\t\t\tuser1\tUSER\tDELETE\ttrue\t-1\tuser1\n+default\tvt1\t\t\tuser1\tUSER\tINSERT\ttrue\t-1\tuser1\n+default\tvt1\t\t\tuser1\tUSER\tSELECT\ttrue\t-1\tuser1\n+default\tvt1\t\t\tuser1\tUSER\tUPDATE\ttrue\t-1\tuser1\n+default\tvt2\t\t\tuser1\tUSER\tDELETE\ttrue\t-1\tuser1\n+default\tvt2\t\t\tuser1\tUSER\tINSERT\ttrue\t-1\tuser1\n+default\tvt2\t\t\tuser1\tUSER\tSELECT\ttrue\t-1\tuser1\n+default\tvt2\t\t\tuser1\tUSER\tUPDATE\ttrue\t-1\tuser1\n PREHOOK: query: --view grant to user\n -- try with and without table keyword\n \n@@ -75,6 +91,15 @@ default\tvt2\t\t\tuser2\tUSER\tDELETE\tfalse\t-1\tuser1\n default\tvt2\t\t\tuser2\tUSER\tINSERT\tfalse\t-1\tuser1\n default\tvt2\t\t\tuser2\tUSER\tSELECT\tfalse\t-1\tuser1\n default\tvt2\t\t\tuser2\tUSER\tUPDATE\tfalse\t-1\tuser1\n+PREHOOK: query: show grant user user2 on all\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: show grant user user2 on all\n+POSTHOOK: type: SHOW_GRANT\n+default\tvt1\t\t\tuser2\tUSER\tSELECT\tfalse\t-1\tuser1\n+default\tvt2\t\t\tuser2\tUSER\tDELETE\tfalse\t-1\tuser1\n+default\tvt2\t\t\tuser2\tUSER\tINSERT\tfalse\t-1\tuser1\n+default\tvt2\t\t\tuser2\tUSER\tSELECT\tfalse\t-1\tuser1\n+default\tvt2\t\t\tuser2\tUSER\tUPDATE\tfalse\t-1\tuser1\n PREHOOK: query: revoke all on vt2 from user user2\n PREHOOK: type: REVOKE_PRIVILEGE\n PREHOOK: Output: default@vt2\n@@ -95,6 +120,10 @@ PREHOOK: query: show grant user user2 on table vt1\n PREHOOK: type: SHOW_GRANT\n POSTHOOK: query: show grant user user2 on table vt1\n POSTHOOK: type: SHOW_GRANT\n+PREHOOK: query: show grant user user2 on all\n+PREHOOK: type: SHOW_GRANT\n+POSTHOOK: query: show grant user user2 on all\n+POSTHOOK: type: SHOW_GRANT\n PREHOOK: query: -- grant privileges on roles for view, after next statement\n show grant user user3 on table vt1\n PREHOOK: type: SHOW_GRANT",
                "raw_url": "https://github.com/apache/hive/raw/a9d35c747dcd7e0b1c16c68bcdc5e0c702a0d5a2/ql/src/test/results/clientpositive/authorization_view_sqlstd.q.out",
                "sha": "89186a5a1d5c43acdadd17007705a5bfd1a07278",
                "status": "modified"
            }
        ],
        "message": "HIVE-6567 : \"show grant ... on all\" fails with NPE (Thejas Nair, reviewed by Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1577428 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/871b2765ca7db24c65fd22a52241bd2b4f8ce6ca",
        "patched_files": [
            "Utilities.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestUtilities.java"
        ]
    },
    "hive_ae82715": {
        "bug_id": "hive_ae82715",
        "commit": "https://github.com/apache/hive/commit/ae82715f1014c4ed514441311b61ed1891e2a12b",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hive/blob/ae82715f1014c4ed514441311b61ed1891e2a12b/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java?ref=ae82715f1014c4ed514441311b61ed1891e2a12b",
                "deletions": 3,
                "filename": "jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java",
                "patch": "@@ -1023,9 +1023,6 @@ public String getQueryId() throws SQLException {\n       return client.GetQueryId(new TGetQueryIdReq(stmtHandle)).getQueryId();\n     } catch (TException e) {\n       throw new SQLException(e);\n-    } catch (Exception e) {\n-      // If concurrently the query is closed before we fetch queryID.\n-      return null;\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/hive/raw/ae82715f1014c4ed514441311b61ed1891e2a12b/jdbc/src/java/org/apache/hive/jdbc/HiveStatement.java",
                "sha": "20e9c3c7e01d2b8300fe92cfca29bf621588801c",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/ae82715f1014c4ed514441311b61ed1891e2a12b/service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java?ref=ae82715f1014c4ed514441311b61ed1891e2a12b",
                "deletions": 0,
                "filename": "service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java",
                "patch": "@@ -854,6 +854,9 @@ public TGetQueryIdResp GetQueryId(TGetQueryIdReq req) throws TException {\n       return new TGetQueryIdResp(cliService.getQueryId(req.getOperationHandle()));\n     } catch (HiveSQLException e) {\n       throw new TException(e);\n+    } catch (Exception e) {\n+      // If concurrently the query is closed before we fetch queryID.\n+      return new TGetQueryIdResp((String)null);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/ae82715f1014c4ed514441311b61ed1891e2a12b/service/src/java/org/apache/hive/service/cli/thrift/ThriftCLIService.java",
                "sha": "48f4fe29ec165424bc5891c6c6980c3e0a85d482",
                "status": "modified"
            }
        ],
        "message": "HIVE-21669: HS2 throws NPE when HiveStatement.getQueryId is invoked and query is closed concurrently (Sankar Hariappan, reviewed by Mahesh Kumar Behera)",
        "parent": "https://github.com/apache/hive/commit/0f8119fe797c5b596d22ec4eaaef8aeeb501ccae",
        "patched_files": [
            "HiveStatement.java",
            "ThriftCLIService.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveStatement.java",
            "ThriftCLIServiceTest.java"
        ]
    },
    "hive_af510fa": {
        "bug_id": "hive_af510fa",
        "commit": "https://github.com/apache/hive/commit/af510fa61b2203aca2c210aee9542f05e2692659",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/af510fa61b2203aca2c210aee9542f05e2692659/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistory.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistory.java?ref=af510fa61b2203aca2c210aee9542f05e2692659",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistory.java",
                "patch": "@@ -409,6 +409,9 @@ public void setTaskCounters(String queryId, String taskId, Counters ctrs) {\n \n   public void printRowCount(String queryId) {\n     QueryInfo ji = queryInfoMap.get(queryId);\n+    if (ji == null) {\n+      return;\n+    }\n     for (String tab : ji.rowCountMap.keySet()) {\n       console.printInfo(ji.rowCountMap.get(tab) + \" Rows loaded to \" + tab);\n     }\n@@ -420,7 +423,6 @@ public void printRowCount(String queryId) {\n    * @param queryId\n    */\n   public void endQuery(String queryId) {\n-\n     QueryInfo ji = queryInfoMap.get(queryId);\n     if (ji == null) {\n       return;",
                "raw_url": "https://github.com/apache/hive/raw/af510fa61b2203aca2c210aee9542f05e2692659/ql/src/java/org/apache/hadoop/hive/ql/history/HiveHistory.java",
                "sha": "7e80c2d6bd5689a5be4e8ce88be4a64750de7553",
                "status": "modified"
            }
        ],
        "message": "HIVE-3265. HiveHistory.printRowCount() throws NPE (Shreepadma Venugopalan via cws)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1378472 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/d59a476e6aee105d80ecc361f8656dd112f6fbc1",
        "patched_files": [
            "HiveHistory.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveHistory.java"
        ]
    },
    "hive_afd7b5b": {
        "bug_id": "hive_afd7b5b",
        "commit": "https://github.com/apache/hive/commit/afd7b5b38556f638782606edbe6850ef70e1c8bb",
        "file": [
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hive/blob/afd7b5b38556f638782606edbe6850ef70e1c8bb/jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java",
                "changes": 62,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java?ref=afd7b5b38556f638782606edbe6850ef70e1c8bb",
                "deletions": 21,
                "filename": "jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java",
                "patch": "@@ -760,36 +760,56 @@ SSLConnectionSocketFactory getTwoWaySSLSocketFactory() throws SQLException {\n   }\n \n   // Lookup the delegation token. First in the connection URL, then Configuration\n-  private String getClientDelegationToken(Map<String, String> jdbcConnConf)\n-      throws SQLException {\n+  private String getClientDelegationToken(Map<String, String> jdbcConnConf) throws SQLException {\n     String tokenStr = null;\n-    if (JdbcConnectionParams.AUTH_TOKEN.equalsIgnoreCase(jdbcConnConf.get(JdbcConnectionParams.AUTH_TYPE))) {\n-      // check delegation token in job conf if any\n+    if (!JdbcConnectionParams.AUTH_TOKEN.equalsIgnoreCase(jdbcConnConf.get(JdbcConnectionParams.AUTH_TYPE))) {\n+      return null;\n+    }\n+    DelegationTokenFetcher fetcher = new DelegationTokenFetcher();\n+    try {\n+      tokenStr = fetcher.getTokenStringFromFile();\n+    } catch (IOException e) {\n+      LOG.warn(\"Cannot get token from environment variable $HADOOP_TOKEN_FILE_LOCATION=\" +\n+              System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION));\n+    }\n+    if (tokenStr == null) {\n       try {\n-        if (System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION) != null) {\n-          try {\n-            Credentials cred = new Credentials();\n-            DataInputStream dis = new DataInputStream(new FileInputStream(System.getenv(UserGroupInformation\n-                    .HADOOP_TOKEN_FILE_LOCATION)));\n-            cred.readTokenStorageStream(dis);\n-            dis.close();\n-            Token<? extends TokenIdentifier> token = cred.getToken(new Text(\"hive\"));\n-            tokenStr = token.encodeToUrlString();\n-          } catch (IOException e) {\n-            LOG.warn(\"Cannot get token from environment variable $HADOOP_TOKEN_FILE_LOCATION=\" +\n-                    System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION));\n-          }\n-        }\n-        if (tokenStr == null) {\n-          tokenStr = SessionUtils.getTokenStrForm(HiveAuthConstants.HS2_CLIENT_TOKEN);\n-        }\n+        return fetcher.getTokenFromSession();\n       } catch (IOException e) {\n         throw new SQLException(\"Error reading token \", e);\n       }\n     }\n     return tokenStr;\n   }\n \n+  static class DelegationTokenFetcher {\n+    String getTokenStringFromFile() throws IOException {\n+      if (System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION) == null) {\n+        return null;\n+      }\n+      Credentials cred = new Credentials();\n+      try (DataInputStream dis = new DataInputStream(new FileInputStream(System.getenv(UserGroupInformation\n+              .HADOOP_TOKEN_FILE_LOCATION)))) {\n+        cred.readTokenStorageStream(dis);\n+      }\n+      return getTokenFromCredential(cred, \"hive\");\n+    }\n+\n+    String getTokenFromCredential(Credentials cred, String key) throws IOException {\n+      Token<? extends TokenIdentifier> token = cred.getToken(new Text(key));\n+      if (token == null) {\n+        LOG.warn(\"Delegation token with key: [hive] cannot be found.\");\n+        return null;\n+      }\n+      return token.encodeToUrlString();\n+    }\n+\n+    String getTokenFromSession() throws IOException {\n+      LOG.debug(\"Fetching delegation token from session.\");\n+      return SessionUtils.getTokenStrForm(HiveAuthConstants.HS2_CLIENT_TOKEN);\n+    }\n+  }\n+\n   private void openSession() throws SQLException {\n     TOpenSessionReq openReq = new TOpenSessionReq();\n ",
                "raw_url": "https://github.com/apache/hive/raw/afd7b5b38556f638782606edbe6850ef70e1c8bb/jdbc/src/java/org/apache/hive/jdbc/HiveConnection.java",
                "sha": "4c7119f112844ffcc01e7c3dcd628fa2d83684a1",
                "status": "modified"
            },
            {
                "additions": 60,
                "blob_url": "https://github.com/apache/hive/blob/afd7b5b38556f638782606edbe6850ef70e1c8bb/jdbc/src/test/org/apache/hive/jdbc/TestHiveConnection.java",
                "changes": 60,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/jdbc/src/test/org/apache/hive/jdbc/TestHiveConnection.java?ref=afd7b5b38556f638782606edbe6850ef70e1c8bb",
                "deletions": 0,
                "filename": "jdbc/src/test/org/apache/hive/jdbc/TestHiveConnection.java",
                "patch": "@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hive.jdbc;\n+\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.security.Credentials;\n+import org.apache.hadoop.security.token.Token;\n+import org.junit.Assert;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+\n+public class TestHiveConnection {\n+\n+  private static final String EXISTING_TOKEN = \"ExistingToken\";\n+  public static final String EXPECTED_TOKEN_STRING_FORM = \"AAAAAA\";\n+  private static HiveConnection.DelegationTokenFetcher fetcher;\n+\n+  @BeforeClass\n+  public static void init() {\n+    fetcher = new HiveConnection.DelegationTokenFetcher();\n+  }\n+\n+  @Test\n+  public void testIfNPEThrownWhileGettingDelegationToken() throws IOException {\n+    try {\n+      String tokenStr = fetcher.getTokenFromCredential(new Credentials(), \"hive\");\n+      Assert.assertEquals(\"Token with id: hive shall not be found.\", null, tokenStr);\n+    } catch (NullPointerException e) {\n+      Assert.fail(\"This NPE is not handled in the code elsewhere so user is not notified about it!\");\n+      e.printStackTrace();\n+    }\n+  }\n+\n+  @Test\n+  public void testIfGettingDelegationTokenFromCredentialWorks() throws IOException {\n+    Credentials creds = new Credentials();\n+    creds.addToken(new Text(EXISTING_TOKEN), new Token<>());\n+\n+    String tokenStr = fetcher.getTokenFromCredential(creds, EXISTING_TOKEN);\n+    Assert.assertEquals(\"Token string form is not as expected.\", EXPECTED_TOKEN_STRING_FORM, tokenStr);\n+  }\n+}",
                "raw_url": "https://github.com/apache/hive/raw/afd7b5b38556f638782606edbe6850ef70e1c8bb/jdbc/src/test/org/apache/hive/jdbc/TestHiveConnection.java",
                "sha": "bcd2608e1ba0101d94574cf3692a3cab66859a01",
                "status": "added"
            }
        ],
        "message": "HIVE-21507: Hive swallows NPE if no delegation token found (Denes Bodo, reviewed by Zoltan Haindrich, Daniel Dai)\n\nSigned-off-by: Zoltan Haindrich <kirk@rxd.hu>, Daniel Dai <daijyc@gmail.com>",
        "parent": "https://github.com/apache/hive/commit/6d74222521d2a1333990b9b3577ec9a7f7e619b8",
        "patched_files": [
            "HiveConnection.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveConnection.java"
        ]
    },
    "hive_b00621d": {
        "bug_id": "hive_b00621d",
        "commit": "https://github.com/apache/hive/commit/b00621da53a733d3903521683b0c0ccf1201a140",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/b00621da53a733d3903521683b0c0ccf1201a140/itests/src/test/resources/testconfiguration.properties",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=b00621da53a733d3903521683b0c0ccf1201a140",
                "deletions": 0,
                "filename": "itests/src/test/resources/testconfiguration.properties",
                "patch": "@@ -439,6 +439,7 @@ minillap.query.files=acid_bucket_pruning.q,\\\n   dynamic_partition_pruning_2.q,\\\n   tez_union_dynamic_partition.q,\\\n   load_fs2.q,\\\n+  llap_stats.q,\\\n   multi_count_distinct_null.q\n \n minillaplocal.query.files=acid_globallimit.q,\\",
                "raw_url": "https://github.com/apache/hive/raw/b00621da53a733d3903521683b0c0ccf1201a140/itests/src/test/resources/testconfiguration.properties",
                "sha": "e966959a5261f9f1bdfe3c0aab829ba38ee699c3",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hive/blob/b00621da53a733d3903521683b0c0ccf1201a140/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java?ref=b00621da53a733d3903521683b0c0ccf1201a140",
                "deletions": 4,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java",
                "patch": "@@ -87,6 +87,7 @@\n   private final boolean[] includedColumns;\n   private final ReadPipeline rp;\n   private final ExecutorService executor;\n+  private final int columnCount;\n \n   private TypeDescription fileSchema;\n \n@@ -119,6 +120,14 @@ public LlapRecordReader(JobConf job, FileSplit split, List<Integer> includedCols\n     MapWork mapWork = Utilities.getMapWork(job);\n     VectorizedRowBatchCtx ctx = mapWork.getVectorizedRowBatchCtx();\n     rbCtx = ctx != null ? ctx : LlapInputFormat.createFakeVrbCtx(mapWork);\n+    if (includedCols == null) {\n+      // Assume including everything means the VRB will have everything.\n+      this.columnCount = rbCtx.getRowColumnTypeInfos().length;\n+    } else {\n+      this.columnCount = columnIds.size();\n+    }\n+\n+\n \n     int partitionColumnCount = rbCtx.getPartitionColumnCount();\n     if (partitionColumnCount > 0) {\n@@ -159,7 +168,8 @@ public boolean init() {\n   private boolean checkOrcSchemaEvolution() {\n     SchemaEvolution schemaEvolution = new SchemaEvolution(\n         fileSchema, rp.getReaderSchema(), includedColumns);\n-    for (Integer colId : columnIds) {\n+    for (int i = 0; i < columnCount; ++i) {\n+      int colId = columnIds == null ? i : columnIds.get(i);\n       if (!schemaEvolution.isPPDSafeConversion(colId)) {\n         LlapIoImpl.LOG.warn(\"Unsupported schema evolution! Disabling Llap IO for {}\", split);\n         return false;\n@@ -197,14 +207,14 @@ public boolean next(NullWritable key, VectorizedRowBatch value) throws IOExcepti\n       counters.incrTimeCounter(LlapIOCounters.CONSUMER_TIME_NS, firstReturnTime);\n       return false;\n     }\n-    if (columnIds.size() != cvb.cols.length) {\n-      throw new RuntimeException(\"Unexpected number of columns, VRB has \" + columnIds.size()\n+    if (columnCount != cvb.cols.length) {\n+      throw new RuntimeException(\"Unexpected number of columns, VRB has \" + columnCount\n           + \" included, but the reader returned \" + cvb.cols.length);\n     }\n     // VRB was created from VrbCtx, so we already have pre-allocated column vectors\n     for (int i = 0; i < cvb.cols.length; ++i) {\n       // Return old CVs (if any) to caller. We assume these things all have the same schema.\n-      cvb.swapColumnVector(i, value.cols, columnIds.get(i));\n+      cvb.swapColumnVector(i, value.cols, columnIds == null ? i : columnIds.get(i));\n     }\n     value.selectedInUse = false;\n     value.size = cvb.size;",
                "raw_url": "https://github.com/apache/hive/raw/b00621da53a733d3903521683b0c0ccf1201a140/llap-server/src/java/org/apache/hadoop/hive/llap/io/api/impl/LlapRecordReader.java",
                "sha": "2f79828f416d19283b3f0ed52dfc3c827d48c85c",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hive/blob/b00621da53a733d3903521683b0c0ccf1201a140/ql/src/test/queries/clientpositive/llap_stats.q",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/llap_stats.q?ref=b00621da53a733d3903521683b0c0ccf1201a140",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/llap_stats.q",
                "patch": "@@ -0,0 +1,26 @@\n+set hive.mapred.mode=nonstrict;\n+SET hive.vectorized.execution.enabled=true;\n+set hive.exec.dynamic.partition.mode=nonstrict;\n+\n+SET hive.llap.io.enabled=false;\n+\n+SET hive.exec.orc.default.buffer.size=32768;\n+SET hive.exec.orc.default.row.index.stride=1000;\n+SET hive.optimize.index.filter=true;\n+set hive.auto.convert.join=false;\n+\n+DROP TABLE llap_stats;\n+\n+CREATE TABLE llap_stats(ctinyint TINYINT, csmallint SMALLINT) partitioned by (cint int) STORED AS ORC;\n+\n+insert into table llap_stats partition(cint)\n+select cint, ctinyint, csmallint from alltypesorc where cint is not null limit 10;\n+\n+select * from llap_stats;\n+\n+SET hive.llap.io.enabled=true;\n+\n+explain analyze table llap_stats partition (cint) compute statistics for columns;\n+analyze table llap_stats partition (cint) compute statistics for columns;\n+\n+DROP TABLE llap_stats;",
                "raw_url": "https://github.com/apache/hive/raw/b00621da53a733d3903521683b0c0ccf1201a140/ql/src/test/queries/clientpositive/llap_stats.q",
                "sha": "49b52bd4a6342b7fb7391846e06a477ff7c6f30f",
                "status": "added"
            },
            {
                "additions": 192,
                "blob_url": "https://github.com/apache/hive/blob/b00621da53a733d3903521683b0c0ccf1201a140/ql/src/test/results/clientpositive/llap/llap_stats.q.out",
                "changes": 192,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/llap/llap_stats.q.out?ref=b00621da53a733d3903521683b0c0ccf1201a140",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/llap/llap_stats.q.out",
                "patch": "@@ -0,0 +1,192 @@\n+PREHOOK: query: DROP TABLE llap_stats\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE llap_stats\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: CREATE TABLE llap_stats(ctinyint TINYINT, csmallint SMALLINT) partitioned by (cint int) STORED AS ORC\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@llap_stats\n+POSTHOOK: query: CREATE TABLE llap_stats(ctinyint TINYINT, csmallint SMALLINT) partitioned by (cint int) STORED AS ORC\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@llap_stats\n+PREHOOK: query: insert into table llap_stats partition(cint)\n+select cint, ctinyint, csmallint from alltypesorc where cint is not null limit 10\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@alltypesorc\n+PREHOOK: Output: default@llap_stats\n+POSTHOOK: query: insert into table llap_stats partition(cint)\n+select cint, ctinyint, csmallint from alltypesorc where cint is not null limit 10\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@alltypesorc\n+POSTHOOK: Output: default@llap_stats@cint=-13326\n+POSTHOOK: Output: default@llap_stats@cint=-15431\n+POSTHOOK: Output: default@llap_stats@cint=-15549\n+POSTHOOK: Output: default@llap_stats@cint=-15813\n+POSTHOOK: Output: default@llap_stats@cint=-4213\n+POSTHOOK: Output: default@llap_stats@cint=-7824\n+POSTHOOK: Output: default@llap_stats@cint=-9566\n+POSTHOOK: Output: default@llap_stats@cint=15007\n+POSTHOOK: Output: default@llap_stats@cint=4963\n+POSTHOOK: Output: default@llap_stats@cint=7021\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-13326).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-13326).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-15431).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-15431).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-15549).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-15549).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-15813).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-15813).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-4213).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-4213).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-7824).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-7824).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-9566).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=-9566).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=15007).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=15007).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=4963).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=4963).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=7021).csmallint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+POSTHOOK: Lineage: llap_stats PARTITION(cint=7021).ctinyint EXPRESSION [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+PREHOOK: query: select * from llap_stats\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@llap_stats\n+PREHOOK: Input: default@llap_stats@cint=-13326\n+PREHOOK: Input: default@llap_stats@cint=-15431\n+PREHOOK: Input: default@llap_stats@cint=-15549\n+PREHOOK: Input: default@llap_stats@cint=-15813\n+PREHOOK: Input: default@llap_stats@cint=-4213\n+PREHOOK: Input: default@llap_stats@cint=-7824\n+PREHOOK: Input: default@llap_stats@cint=-9566\n+PREHOOK: Input: default@llap_stats@cint=15007\n+PREHOOK: Input: default@llap_stats@cint=4963\n+PREHOOK: Input: default@llap_stats@cint=7021\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from llap_stats\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@llap_stats\n+POSTHOOK: Input: default@llap_stats@cint=-13326\n+POSTHOOK: Input: default@llap_stats@cint=-15431\n+POSTHOOK: Input: default@llap_stats@cint=-15549\n+POSTHOOK: Input: default@llap_stats@cint=-15813\n+POSTHOOK: Input: default@llap_stats@cint=-4213\n+POSTHOOK: Input: default@llap_stats@cint=-7824\n+POSTHOOK: Input: default@llap_stats@cint=-9566\n+POSTHOOK: Input: default@llap_stats@cint=15007\n+POSTHOOK: Input: default@llap_stats@cint=4963\n+POSTHOOK: Input: default@llap_stats@cint=7021\n+#### A masked pattern was here ####\n+-17\t-50\t-13326\n+-17\t-11\t-15431\n+-17\t61\t-15549\n+-17\t-28\t-15813\n+-17\tNULL\t-4213\n+-17\t27\t-7824\n+-17\t31\t-9566\n+-17\t-34\t15007\n+-17\t31\t4963\n+-17\t29\t7021\n+PREHOOK: query: explain analyze table llap_stats partition (cint) compute statistics for columns\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain analyze table llap_stats partition (cint) compute statistics for columns\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-0 is a root stage\n+  Stage-2 depends on stages: Stage-0\n+\n+STAGE PLANS:\n+  Stage: Stage-0\n+    Tez\n+#### A masked pattern was here ####\n+      Edges:\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: llap_stats\n+                  Statistics: Num rows: 10 Data size: 116 Basic stats: COMPLETE Column stats: PARTIAL\n+                  Select Operator\n+                    expressions: cint (type: int), ctinyint (type: tinyint), csmallint (type: smallint)\n+                    outputColumnNames: cint, ctinyint, csmallint\n+                    Statistics: Num rows: 10 Data size: 116 Basic stats: COMPLETE Column stats: PARTIAL\n+                    Group By Operator\n+                      aggregations: compute_stats(ctinyint, 16), compute_stats(csmallint, 16)\n+                      keys: cint (type: int)\n+                      mode: hash\n+                      outputColumnNames: _col0, _col1, _col2\n+                      Statistics: Num rows: 5 Data size: 4780 Basic stats: COMPLETE Column stats: PARTIAL\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: int)\n+                        sort order: +\n+                        Map-reduce partition columns: _col0 (type: int)\n+                        Statistics: Num rows: 5 Data size: 4780 Basic stats: COMPLETE Column stats: PARTIAL\n+                        value expressions: _col1 (type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>), _col2 (type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>)\n+            Execution mode: llap\n+            LLAP IO: all inputs\n+        Reducer 2 \n+            Execution mode: llap\n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: compute_stats(VALUE._col0), compute_stats(VALUE._col1)\n+                keys: KEY._col0 (type: int)\n+                mode: mergepartial\n+                outputColumnNames: _col0, _col1, _col2\n+                Statistics: Num rows: 5 Data size: 4820 Basic stats: COMPLETE Column stats: PARTIAL\n+                Select Operator\n+                  expressions: _col1 (type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,numdistinctvalues:bigint,ndvbitvector:string>), _col2 (type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,numdistinctvalues:bigint,ndvbitvector:string>), _col0 (type: int)\n+                  outputColumnNames: _col0, _col1, _col2\n+                  Statistics: Num rows: 5 Data size: 4820 Basic stats: COMPLETE Column stats: PARTIAL\n+                  File Output Operator\n+                    compressed: false\n+                    Statistics: Num rows: 5 Data size: 4820 Basic stats: COMPLETE Column stats: PARTIAL\n+                    table:\n+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-2\n+    Column Stats Work\n+      Column Stats Desc:\n+          Columns: ctinyint, csmallint\n+          Column Types: tinyint, smallint\n+          Table: default.llap_stats\n+\n+PREHOOK: query: analyze table llap_stats partition (cint) compute statistics for columns\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@llap_stats\n+PREHOOK: Input: default@llap_stats@cint=-13326\n+PREHOOK: Input: default@llap_stats@cint=-15431\n+PREHOOK: Input: default@llap_stats@cint=-15549\n+PREHOOK: Input: default@llap_stats@cint=-15813\n+PREHOOK: Input: default@llap_stats@cint=-4213\n+PREHOOK: Input: default@llap_stats@cint=-7824\n+PREHOOK: Input: default@llap_stats@cint=-9566\n+PREHOOK: Input: default@llap_stats@cint=15007\n+PREHOOK: Input: default@llap_stats@cint=4963\n+PREHOOK: Input: default@llap_stats@cint=7021\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table llap_stats partition (cint) compute statistics for columns\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@llap_stats\n+POSTHOOK: Input: default@llap_stats@cint=-13326\n+POSTHOOK: Input: default@llap_stats@cint=-15431\n+POSTHOOK: Input: default@llap_stats@cint=-15549\n+POSTHOOK: Input: default@llap_stats@cint=-15813\n+POSTHOOK: Input: default@llap_stats@cint=-4213\n+POSTHOOK: Input: default@llap_stats@cint=-7824\n+POSTHOOK: Input: default@llap_stats@cint=-9566\n+POSTHOOK: Input: default@llap_stats@cint=15007\n+POSTHOOK: Input: default@llap_stats@cint=4963\n+POSTHOOK: Input: default@llap_stats@cint=7021\n+#### A masked pattern was here ####\n+PREHOOK: query: DROP TABLE llap_stats\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@llap_stats\n+PREHOOK: Output: default@llap_stats\n+POSTHOOK: query: DROP TABLE llap_stats\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@llap_stats\n+POSTHOOK: Output: default@llap_stats",
                "raw_url": "https://github.com/apache/hive/raw/b00621da53a733d3903521683b0c0ccf1201a140/ql/src/test/results/clientpositive/llap/llap_stats.q.out",
                "sha": "f6921f13067352cf02e7a1a5e6d194b6d87f02d0",
                "status": "added"
            }
        ],
        "message": "HIVE-15649 : LLAP IO may NPE on all-column read (Sergey Shelukhin, reviewed by Prasanth Jayachandran)",
        "parent": "https://github.com/apache/hive/commit/3d71a90a72d850040d42f7a4ba55a7dc36924e5b",
        "patched_files": [
            "llap_stats.java",
            "LlapRecordReader.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "testconfiguration.java"
        ]
    },
    "hive_b1f5562": {
        "bug_id": "hive_b1f5562",
        "commit": "https://github.com/apache/hive/commit/b1f556296405814e6b492660c8f03bcb9d892d4c",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/data/files/test1.txt",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/data/files/test1.txt?ref=b1f556296405814e6b492660c8f03bcb9d892d4c",
                "deletions": 0,
                "filename": "data/files/test1.txt",
                "patch": "@@ -0,0 +1,5 @@\n+tom\u000115\n+john\u0001\n+mayr\u000140\n+\u000130\n+\u0001",
                "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/data/files/test1.txt",
                "sha": "0f8801ad3ab78236913eaf63316dd28b744eb08b",
                "status": "added"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java?ref=b1f556296405814e6b492660c8f03bcb9d892d4c",
                "deletions": 2,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java",
                "patch": "@@ -28,6 +28,7 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.common.FileUtils;\n import org.apache.hadoop.hive.common.StatsSetupConst;\n+import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.ErrorMsg;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hadoop.hive.ql.metadata.VirtualColumn;\n@@ -65,6 +66,8 @@\n   private transient int rowLimit = -1;\n   private transient int currCount = 0;\n \n+  private String defaultPartitionName;\n+\n   public TableDesc getTableDesc() {\n     return tableDesc;\n   }\n@@ -145,8 +148,9 @@ private void gatherStats(Object row) {\n             (StructObjectInspector) inputObjInspectors[0], ObjectInspectorCopyOption.WRITABLE);\n \n         for (Object o : writable) {\n-          assert (o != null && o.toString().length() > 0);\n-          values.add(o.toString());\n+          // It's possible that a parition column may have NULL value, in which case the row belongs\n+          // to the special partition, __HIVE_DEFAULT_PARTITION__.\n+          values.add(o == null ? defaultPartitionName : o.toString());\n         }\n         partitionSpecs = FileUtils.makePartName(conf.getPartColumns(), values);\n         LOG.info(\"Stats Gathering found a new partition spec = \" + partitionSpecs);\n@@ -205,6 +209,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {\n       jc = new JobConf(hconf);\n     }\n \n+    defaultPartitionName = HiveConf.getVar(hconf, HiveConf.ConfVars.DEFAULTPARTITIONNAME);\n     currentStat = null;\n     stats = new HashMap<String, Stat>();\n     if (conf.getPartColumns() == null || conf.getPartColumns().size() == 0) {",
                "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java",
                "sha": "58ed550f2af94c2ddcfa94ec08f4ef4bdca984ed",
                "status": "modified"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/queries/clientpositive/analyze_table_null_partition.q",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/analyze_table_null_partition.q?ref=b1f556296405814e6b492660c8f03bcb9d892d4c",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/analyze_table_null_partition.q",
                "patch": "@@ -0,0 +1,21 @@\n+SET hive.exec.dynamic.partition.mode=nonstrict;\n+\n+DROP TABLE IF EXISTS test1;\n+DROP TABLE IF EXISTS test2;\n+\n+CREATE TABLE test1(name string, age int);\n+CREATE TABLE test2(name string) PARTITIONED by (age int);\n+\n+LOAD DATA LOCAL INPATH '../../data/files/test1.txt' INTO TABLE test1;\n+FROM test1 INSERT OVERWRITE TABLE test2 PARTITION(age) SELECT test1.name, test1.age;\n+\n+ANALYZE TABLE test2 PARTITION(age) COMPUTE STATISTICS;\n+\n+-- To show stats. It doesn't show due to a bug.\n+DESC EXTENDED test2;\n+\n+-- Another way to show stats.\n+EXPLAIN EXTENDED select * from test2;\n+\n+DROP TABLE test1;\n+DROP TABLE test2;",
                "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/queries/clientpositive/analyze_table_null_partition.q",
                "sha": "2f7a893d33031cf329767b704707c0c2badcaf2f",
                "status": "added"
            },
            {
                "additions": 335,
                "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/analyze_table_null_partition.q.out",
                "changes": 335,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/analyze_table_null_partition.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/analyze_table_null_partition.q.out",
                "patch": "@@ -0,0 +1,335 @@\n+PREHOOK: query: DROP TABLE IF EXISTS test1\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE IF EXISTS test1\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: DROP TABLE IF EXISTS test2\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE IF EXISTS test2\n+POSTHOOK: type: DROPTABLE\n+PREHOOK: query: CREATE TABLE test1(name string, age int)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+POSTHOOK: query: CREATE TABLE test1(name string, age int)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@test1\n+PREHOOK: query: CREATE TABLE test2(name string) PARTITIONED by (age int)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+POSTHOOK: query: CREATE TABLE test2(name string) PARTITIONED by (age int)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@test2\n+PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/test1.txt' INTO TABLE test1\n+PREHOOK: type: LOAD\n+#### A masked pattern was here ####\n+PREHOOK: Output: default@test1\n+POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/test1.txt' INTO TABLE test1\n+POSTHOOK: type: LOAD\n+#### A masked pattern was here ####\n+POSTHOOK: Output: default@test1\n+PREHOOK: query: FROM test1 INSERT OVERWRITE TABLE test2 PARTITION(age) SELECT test1.name, test1.age\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@test1\n+PREHOOK: Output: default@test2\n+POSTHOOK: query: FROM test1 INSERT OVERWRITE TABLE test2 PARTITION(age) SELECT test1.name, test1.age\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@test1\n+POSTHOOK: Output: default@test2@age=15\n+POSTHOOK: Output: default@test2@age=30\n+POSTHOOK: Output: default@test2@age=40\n+POSTHOOK: Output: default@test2@age=__HIVE_DEFAULT_PARTITION__\n+POSTHOOK: Lineage: test2 PARTITION(age=15).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=30).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=40).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=__HIVE_DEFAULT_PARTITION__).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+PREHOOK: query: ANALYZE TABLE test2 PARTITION(age) COMPUTE STATISTICS\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@test2\n+PREHOOK: Input: default@test2@age=15\n+PREHOOK: Input: default@test2@age=30\n+PREHOOK: Input: default@test2@age=40\n+PREHOOK: Input: default@test2@age=__HIVE_DEFAULT_PARTITION__\n+PREHOOK: Output: default@test2\n+PREHOOK: Output: default@test2@age=15\n+PREHOOK: Output: default@test2@age=30\n+PREHOOK: Output: default@test2@age=40\n+PREHOOK: Output: default@test2@age=__HIVE_DEFAULT_PARTITION__\n+POSTHOOK: query: ANALYZE TABLE test2 PARTITION(age) COMPUTE STATISTICS\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@test2\n+POSTHOOK: Input: default@test2@age=15\n+POSTHOOK: Input: default@test2@age=30\n+POSTHOOK: Input: default@test2@age=40\n+POSTHOOK: Input: default@test2@age=__HIVE_DEFAULT_PARTITION__\n+POSTHOOK: Output: default@test2\n+POSTHOOK: Output: default@test2@age=15\n+POSTHOOK: Output: default@test2@age=30\n+POSTHOOK: Output: default@test2@age=40\n+POSTHOOK: Output: default@test2@age=__HIVE_DEFAULT_PARTITION__\n+POSTHOOK: Lineage: test2 PARTITION(age=15).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=30).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=40).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=__HIVE_DEFAULT_PARTITION__).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+PREHOOK: query: -- To show stats. It doesn't show due to a bug.\n+DESC EXTENDED test2\n+PREHOOK: type: DESCTABLE\n+PREHOOK: Input: default@test2\n+POSTHOOK: query: -- To show stats. It doesn't show due to a bug.\n+DESC EXTENDED test2\n+POSTHOOK: type: DESCTABLE\n+POSTHOOK: Input: default@test2\n+POSTHOOK: Lineage: test2 PARTITION(age=15).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=30).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=40).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=__HIVE_DEFAULT_PARTITION__).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+name                \tstring              \t                    \n+age                 \tint                 \t                    \n+\t \t \n+# Partition Information\t \t \n+# col_name            \tdata_type           \tcomment             \n+\t \t \n+age                 \tint                 \t                    \n+\t \t \n+#### A masked pattern was here ####\n+PREHOOK: query: -- Another way to show stats.\n+EXPLAIN EXTENDED select * from test2\n+PREHOOK: type: QUERY\n+POSTHOOK: query: -- Another way to show stats.\n+EXPLAIN EXTENDED select * from test2\n+POSTHOOK: type: QUERY\n+POSTHOOK: Lineage: test2 PARTITION(age=15).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=30).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=40).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=__HIVE_DEFAULT_PARTITION__).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+ABSTRACT SYNTAX TREE:\n+  \n+TOK_QUERY\n+   TOK_FROM\n+      TOK_TABREF\n+         TOK_TABNAME\n+            test2\n+   TOK_INSERT\n+      TOK_DESTINATION\n+         TOK_DIR\n+            TOK_TMP_FILE\n+      TOK_SELECT\n+         TOK_SELEXPR\n+            TOK_ALLCOLREF\n+\n+\n+STAGE DEPENDENCIES:\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Partition Description:\n+          Partition\n+            input format: org.apache.hadoop.mapred.TextInputFormat\n+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+            partition values:\n+              age 15\n+            properties:\n+              COLUMN_STATS_ACCURATE true\n+              bucket_count -1\n+              columns name\n+              columns.comments \n+              columns.types string\n+#### A masked pattern was here ####\n+              name default.test2\n+              numFiles 1\n+              numRows 1\n+              partition_columns age\n+              partition_columns.types int\n+              rawDataSize 3\n+              serialization.ddl struct test2 { string name}\n+              serialization.format 1\n+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              totalSize 4\n+#### A masked pattern was here ####\n+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+          \n+              input format: org.apache.hadoop.mapred.TextInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+              properties:\n+                bucket_count -1\n+                columns name\n+                columns.comments \n+                columns.types string\n+#### A masked pattern was here ####\n+                name default.test2\n+                partition_columns age\n+                partition_columns.types int\n+                serialization.ddl struct test2 { string name}\n+                serialization.format 1\n+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+#### A masked pattern was here ####\n+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              name: default.test2\n+            name: default.test2\n+          Partition\n+            input format: org.apache.hadoop.mapred.TextInputFormat\n+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+            partition values:\n+              age 30\n+            properties:\n+              COLUMN_STATS_ACCURATE true\n+              bucket_count -1\n+              columns name\n+              columns.comments \n+              columns.types string\n+#### A masked pattern was here ####\n+              name default.test2\n+              numFiles 1\n+              numRows 1\n+              partition_columns age\n+              partition_columns.types int\n+              rawDataSize 0\n+              serialization.ddl struct test2 { string name}\n+              serialization.format 1\n+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              totalSize 1\n+#### A masked pattern was here ####\n+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+          \n+              input format: org.apache.hadoop.mapred.TextInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+              properties:\n+                bucket_count -1\n+                columns name\n+                columns.comments \n+                columns.types string\n+#### A masked pattern was here ####\n+                name default.test2\n+                partition_columns age\n+                partition_columns.types int\n+                serialization.ddl struct test2 { string name}\n+                serialization.format 1\n+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+#### A masked pattern was here ####\n+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              name: default.test2\n+            name: default.test2\n+          Partition\n+            input format: org.apache.hadoop.mapred.TextInputFormat\n+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+            partition values:\n+              age 40\n+            properties:\n+              COLUMN_STATS_ACCURATE true\n+              bucket_count -1\n+              columns name\n+              columns.comments \n+              columns.types string\n+#### A masked pattern was here ####\n+              name default.test2\n+              numFiles 1\n+              numRows 1\n+              partition_columns age\n+              partition_columns.types int\n+              rawDataSize 4\n+              serialization.ddl struct test2 { string name}\n+              serialization.format 1\n+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              totalSize 5\n+#### A masked pattern was here ####\n+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+          \n+              input format: org.apache.hadoop.mapred.TextInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+              properties:\n+                bucket_count -1\n+                columns name\n+                columns.comments \n+                columns.types string\n+#### A masked pattern was here ####\n+                name default.test2\n+                partition_columns age\n+                partition_columns.types int\n+                serialization.ddl struct test2 { string name}\n+                serialization.format 1\n+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+#### A masked pattern was here ####\n+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              name: default.test2\n+            name: default.test2\n+          Partition\n+            input format: org.apache.hadoop.mapred.TextInputFormat\n+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+            partition values:\n+              age __HIVE_DEFAULT_PARTITION__\n+            properties:\n+              COLUMN_STATS_ACCURATE true\n+              bucket_count -1\n+              columns name\n+              columns.comments \n+              columns.types string\n+#### A masked pattern was here ####\n+              name default.test2\n+              numFiles 1\n+              numRows 2\n+              partition_columns age\n+              partition_columns.types int\n+              rawDataSize 4\n+              serialization.ddl struct test2 { string name}\n+              serialization.format 1\n+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              totalSize 6\n+#### A masked pattern was here ####\n+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+          \n+              input format: org.apache.hadoop.mapred.TextInputFormat\n+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+              properties:\n+                bucket_count -1\n+                columns name\n+                columns.comments \n+                columns.types string\n+#### A masked pattern was here ####\n+                name default.test2\n+                partition_columns age\n+                partition_columns.types int\n+                serialization.ddl struct test2 { string name}\n+                serialization.format 1\n+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+#### A masked pattern was here ####\n+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+              name: default.test2\n+            name: default.test2\n+      Processor Tree:\n+        TableScan\n+          alias: test2\n+          Statistics: Num rows: 5 Data size: 111 Basic stats: COMPLETE Column stats: NONE\n+          GatherStats: false\n+          Select Operator\n+            expressions: name (type: string), age (type: int)\n+            outputColumnNames: _col0, _col1\n+            Statistics: Num rows: 5 Data size: 111 Basic stats: COMPLETE Column stats: NONE\n+            ListSink\n+\n+PREHOOK: query: DROP TABLE test1\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@test1\n+PREHOOK: Output: default@test1\n+POSTHOOK: query: DROP TABLE test1\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@test1\n+POSTHOOK: Output: default@test1\n+POSTHOOK: Lineage: test2 PARTITION(age=15).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=30).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=40).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=__HIVE_DEFAULT_PARTITION__).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+PREHOOK: query: DROP TABLE test2\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@test2\n+PREHOOK: Output: default@test2\n+POSTHOOK: query: DROP TABLE test2\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@test2\n+POSTHOOK: Output: default@test2\n+POSTHOOK: Lineage: test2 PARTITION(age=15).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=30).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=40).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]\n+POSTHOOK: Lineage: test2 PARTITION(age=__HIVE_DEFAULT_PARTITION__).name SIMPLE [(test1)test1.FieldSchema(name:name, type:string, comment:null), ]",
                "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/analyze_table_null_partition.q.out",
                "sha": "605f5b44687494b63aced91dfbb5e30e6b6cc754",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/bucket_map_join_tez1.q.out",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/bucket_map_join_tez1.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c",
                "deletions": 1,
                "filename": "ql/src/test/results/clientpositive/tez/bucket_map_join_tez1.q.out",
                "patch": "@@ -535,7 +535,7 @@ STAGE PLANS:\n   Stage: Stage-1\n     Tez\n       Edges:\n-        Map 3 <- Map 1 (CUSTOM_EDGE), Map 2 (CUSTOM_EDGE)\n+        Map 3 <- Map 2 (CUSTOM_EDGE), Map 1 (CUSTOM_EDGE)\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 ",
                "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/bucket_map_join_tez1.q.out",
                "sha": "b4077354185f9338077eceb435ccad6589f3dad1",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/bucket_map_join_tez2.q.out",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/bucket_map_join_tez2.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c",
                "deletions": 1,
                "filename": "ql/src/test/results/clientpositive/tez/bucket_map_join_tez2.q.out",
                "patch": "@@ -124,7 +124,7 @@ STAGE PLANS:\n   Stage: Stage-1\n     Tez\n       Edges:\n-        Map 3 <- Map 2 (CUSTOM_EDGE), Map 1 (BROADCAST_EDGE)\n+        Map 3 <- Map 1 (BROADCAST_EDGE), Map 2 (CUSTOM_EDGE)\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 ",
                "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/bucket_map_join_tez2.q.out",
                "sha": "17d64c71ec1024ba7ee3833c47914e09ad130f70",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/count.q.out",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/count.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c",
                "deletions": 1,
                "filename": "ql/src/test/results/clientpositive/tez/count.q.out",
                "patch": "@@ -63,7 +63,7 @@ STAGE PLANS:\n                         sort order: +++\n                         Map-reduce partition columns: _col0 (type: int)\n                         Statistics: Num rows: 4 Data size: 78 Basic stats: COMPLETE Column stats: NONE\n-                        value expressions: _col3 (type: bigint), _col4 (type: bigint), _col5 (type: bigint)\n+                        value expressions: _col5 (type: bigint)\n         Reducer 2 \n             Reduce Operator Tree:\n               Group By Operator",
                "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/count.q.out",
                "sha": "31c12835bfbd0459495ad09658111c9b01cf6622",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/limit_pushdown.q.out",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/limit_pushdown.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c",
                "deletions": 4,
                "filename": "ql/src/test/results/clientpositive/tez/limit_pushdown.q.out",
                "patch": "@@ -481,7 +481,6 @@ STAGE PLANS:\n                         Map-reduce partition columns: _col0 (type: tinyint)\n                         Statistics: Num rows: 31436 Data size: 377237 Basic stats: COMPLETE Column stats: NONE\n                         TopN Hash Memory Usage: 0.3\n-                        value expressions: _col2 (type: bigint)\n         Reducer 2 \n             Reduce Operator Tree:\n               Group By Operator\n@@ -577,7 +576,6 @@ STAGE PLANS:\n                         Map-reduce partition columns: _col0 (type: tinyint)\n                         Statistics: Num rows: 1849 Data size: 377237 Basic stats: COMPLETE Column stats: NONE\n                         TopN Hash Memory Usage: 0.3\n-                        value expressions: _col3 (type: bigint), _col4 (type: bigint)\n         Reducer 2 \n             Reduce Operator Tree:\n               Group By Operator\n@@ -981,8 +979,8 @@ STAGE PLANS:\n                   alias: src\n                   Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n                   Select Operator\n-                    expressions: value (type: string), key (type: string)\n-                    outputColumnNames: value, key\n+                    expressions: key (type: string), value (type: string)\n+                    outputColumnNames: key, value\n                     Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n                     Reduce Output Operator\n                       key expressions: value (type: string)",
                "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/limit_pushdown.q.out",
                "sha": "a8b281aaa2b5f022c975e46289ff1c8b7b28f5dc",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out",
                "patch": "@@ -1,7 +1,9 @@\n PREHOOK: query: show partitions srcpart\n PREHOOK: type: SHOWPARTITIONS\n+PREHOOK: Input: default@srcpart\n POSTHOOK: query: show partitions srcpart\n POSTHOOK: type: SHOWPARTITIONS\n+POSTHOOK: Input: default@srcpart\n ds=2008-04-08/hr=11\n ds=2008-04-08/hr=12\n ds=2008-04-09/hr=11\n@@ -22,8 +24,10 @@ POSTHOOK: Output: database:default\n POSTHOOK: Output: default@nzhang_part2\n PREHOOK: query: describe extended nzhang_part1\n PREHOOK: type: DESCTABLE\n+PREHOOK: Input: default@nzhang_part1\n POSTHOOK: query: describe extended nzhang_part1\n POSTHOOK: type: DESCTABLE\n+POSTHOOK: Input: default@nzhang_part1\n key                 \tstring              \tdefault             \n value               \tstring              \tdefault             \n ds                  \tstring              \t                    \n@@ -187,8 +191,10 @@ POSTHOOK: Lineage: nzhang_part2 PARTITION(ds=2008-12-31,hr=12).key SIMPLE [(srcp\n POSTHOOK: Lineage: nzhang_part2 PARTITION(ds=2008-12-31,hr=12).value SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]\n PREHOOK: query: show partitions nzhang_part1\n PREHOOK: type: SHOWPARTITIONS\n+PREHOOK: Input: default@nzhang_part1\n POSTHOOK: query: show partitions nzhang_part1\n POSTHOOK: type: SHOWPARTITIONS\n+POSTHOOK: Input: default@nzhang_part1\n POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]\n POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]\n POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]\n@@ -201,8 +207,10 @@ ds=2008-04-08/hr=11\n ds=2008-04-08/hr=12\n PREHOOK: query: show partitions nzhang_part2\n PREHOOK: type: SHOWPARTITIONS\n+PREHOOK: Input: default@nzhang_part2\n POSTHOOK: query: show partitions nzhang_part2\n POSTHOOK: type: SHOWPARTITIONS\n+POSTHOOK: Input: default@nzhang_part2\n POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=11).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]\n POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=11).value SIMPLE [(srcpart)srcpart.FieldSchema(name:value, type:string, comment:default), ]\n POSTHOOK: Lineage: nzhang_part1 PARTITION(ds=2008-04-08,hr=12).key SIMPLE [(srcpart)srcpart.FieldSchema(name:key, type:string, comment:default), ]",
                "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out",
                "sha": "4638fe7d16bb796e08b58dadf470341e923f6259",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/mrr.q.out",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/mrr.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c",
                "deletions": 2,
                "filename": "ql/src/test/results/clientpositive/tez/mrr.q.out",
                "patch": "@@ -452,7 +452,6 @@ STAGE PLANS:\n                       sort order: ++\n                       Map-reduce partition columns: _col0 (type: string)\n                       Statistics: Num rows: 63 Data size: 6393 Basic stats: COMPLETE Column stats: NONE\n-                      value expressions: _col2 (type: bigint)\n         Reducer 3 \n             Reduce Operator Tree:\n               Group By Operator\n@@ -864,7 +863,6 @@ STAGE PLANS:\n                           sort order: ++\n                           Map-reduce partition columns: _col0 (type: string)\n                           Statistics: Num rows: 63 Data size: 6393 Basic stats: COMPLETE Column stats: NONE\n-                          value expressions: _col2 (type: bigint)\n         Reducer 3 \n             Reduce Operator Tree:\n               Group By Operator",
                "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/mrr.q.out",
                "sha": "70c219bb4701bbfac647d0b4a709dad276827302",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/tez_dml.q.out",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_dml.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c",
                "deletions": 7,
                "filename": "ql/src/test/results/clientpositive/tez/tez_dml.q.out",
                "patch": "@@ -879,10 +879,10 @@ POSTHOOK: Lineage: tmp_src_part PARTITION(d=5).c SIMPLE [(tmp_src)tmp_src.FieldS\n STAGE DEPENDENCIES:\n   Stage-2 is a root stage\n   Stage-3 depends on stages: Stage-2\n-  Stage-1 depends on stages: Stage-3\n-  Stage-4 depends on stages: Stage-1\n   Stage-0 depends on stages: Stage-3\n-  Stage-5 depends on stages: Stage-0\n+  Stage-4 depends on stages: Stage-0\n+  Stage-1 depends on stages: Stage-3\n+  Stage-5 depends on stages: Stage-1\n \n STAGE PLANS:\n   Stage: Stage-2\n@@ -928,28 +928,28 @@ STAGE PLANS:\n   Stage: Stage-3\n     Dependency Collection\n \n-  Stage: Stage-1\n+  Stage: Stage-0\n     Move Operator\n       tables:\n           replace: false\n           table:\n               input format: org.apache.hadoop.mapred.TextInputFormat\n               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-              name: default.odd\n+              name: default.even\n \n   Stage: Stage-4\n     Stats-Aggr Operator\n \n-  Stage: Stage-0\n+  Stage: Stage-1\n     Move Operator\n       tables:\n           replace: false\n           table:\n               input format: org.apache.hadoop.mapred.TextInputFormat\n               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-              name: default.even\n+              name: default.odd\n \n   Stage: Stage-5\n     Stats-Aggr Operator",
                "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/tez_dml.q.out",
                "sha": "d64c0dadd29ee8f028cdae912a7aa43c63e32264",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/tez_union.q.out",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_union.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c",
                "deletions": 22,
                "filename": "ql/src/test/results/clientpositive/tez/tez_union.q.out",
                "patch": "@@ -159,8 +159,8 @@ STAGE PLANS:\n                 TableScan\n                   alias: src\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n+                    expressions: key (type: string)\n+                    outputColumnNames: _col0\n                     Reduce Output Operator\n                       key expressions: _col0 (type: string)\n                       sort order: +\n@@ -170,8 +170,8 @@ STAGE PLANS:\n                 TableScan\n                   alias: src\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n+                    expressions: key (type: string)\n+                    outputColumnNames: _col0\n                     Reduce Output Operator\n                       key expressions: _col0 (type: string)\n                       sort order: +\n@@ -181,8 +181,8 @@ STAGE PLANS:\n                 TableScan\n                   alias: src\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n+                    expressions: key (type: string)\n+                    outputColumnNames: _col0\n                     Reduce Output Operator\n                       key expressions: _col0 (type: string)\n                       sort order: +\n@@ -192,8 +192,8 @@ STAGE PLANS:\n                 TableScan\n                   alias: src\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n+                    expressions: key (type: string)\n+                    outputColumnNames: _col0\n                     Reduce Output Operator\n                       key expressions: _col0 (type: string)\n                       sort order: +\n@@ -206,9 +206,9 @@ STAGE PLANS:\n                 condition expressions:\n                   0 \n                   1 \n-                Statistics: Num rows: 63 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n+                Statistics: Num rows: 127 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n                 Select Operator\n-                  Statistics: Num rows: 63 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 127 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n                   Group By Operator\n                     aggregations: count()\n                     mode: hash\n@@ -317,8 +317,8 @@ STAGE PLANS:\n                 TableScan\n                   alias: src\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n+                    expressions: key (type: string)\n+                    outputColumnNames: _col0\n                     Map Join Operator\n                       condition map:\n                            Inner Join 0 to 1\n@@ -343,8 +343,8 @@ STAGE PLANS:\n                 TableScan\n                   alias: src\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n+                    expressions: key (type: string)\n+                    outputColumnNames: _col0\n                     Map Join Operator\n                       condition map:\n                            Inner Join 0 to 1\n@@ -446,7 +446,7 @@ STAGE PLANS:\n       Edges:\n         Map 2 <- Map 1 (BROADCAST_EDGE), Union 3 (CONTAINS), Map 5 (BROADCAST_EDGE), Map 8 (BROADCAST_EDGE)\n         Map 7 <- Map 1 (BROADCAST_EDGE), Map 6 (BROADCAST_EDGE), Union 3 (CONTAINS), Map 8 (BROADCAST_EDGE)\n-        Map 9 <- Map 1 (BROADCAST_EDGE), Map 8 (BROADCAST_EDGE), Union 3 (CONTAINS), Map 10 (BROADCAST_EDGE)\n+        Map 9 <- Map 8 (BROADCAST_EDGE), Map 1 (BROADCAST_EDGE), Union 3 (CONTAINS), Map 10 (BROADCAST_EDGE)\n         Reducer 4 <- Union 3 (SIMPLE_EDGE)\n #### A masked pattern was here ####\n       Vertices:\n@@ -1074,8 +1074,8 @@ STAGE PLANS:\n                 TableScan\n                   alias: src\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n+                    expressions: key (type: string)\n+                    outputColumnNames: _col0\n                     Reduce Output Operator\n                       key expressions: _col0 (type: string)\n                       sort order: +\n@@ -1096,14 +1096,14 @@ STAGE PLANS:\n                       0 _col0 (type: string)\n                       1 key (type: string)\n                     outputColumnNames: _col0, _col2\n-                    Statistics: Num rows: 63 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 127 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n                     Select Operator\n                       expressions: _col0 (type: string), _col2 (type: string)\n                       outputColumnNames: _col0, _col1\n-                      Statistics: Num rows: 63 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n+                      Statistics: Num rows: 127 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n                       File Output Operator\n                         compressed: false\n-                        Statistics: Num rows: 63 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n+                        Statistics: Num rows: 127 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n                         table:\n                             input format: org.apache.hadoop.mapred.TextInputFormat\n                             output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n@@ -1113,8 +1113,8 @@ STAGE PLANS:\n                 TableScan\n                   alias: src\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n+                    expressions: key (type: string)\n+                    outputColumnNames: _col0\n                     Reduce Output Operator\n                       key expressions: _col0 (type: string)\n                       sort order: +",
                "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/tez_union.q.out",
                "sha": "4dc687cad3e8f13dcbbbc1e02c4e00349f4f14b4",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union2.q.out",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/union2.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c",
                "deletions": 7,
                "filename": "ql/src/test/results/clientpositive/tez/union2.q.out",
                "patch": "@@ -28,8 +28,6 @@ STAGE PLANS:\n                 TableScan\n                   alias: s1\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n                     Select Operator\n                       Group By Operator\n                         aggregations: count(1)\n@@ -43,8 +41,6 @@ STAGE PLANS:\n                 TableScan\n                   alias: s2\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n                     Select Operator\n                       Group By Operator\n                         aggregations: count(1)\n@@ -59,14 +55,14 @@ STAGE PLANS:\n                 aggregations: count(VALUE._col0)\n                 mode: mergepartial\n                 outputColumnNames: _col0\n-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n                 Select Operator\n                   expressions: _col0 (type: bigint)\n                   outputColumnNames: _col0\n-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n                   File Output Operator\n                     compressed: false\n-                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n                     table:\n                         input format: org.apache.hadoop.mapred.TextInputFormat\n                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat",
                "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union2.q.out",
                "sha": "a0c6a9d1c5897819b136b8aef83879fd1c72df45",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union3.q.out",
                "changes": 60,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/union3.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c",
                "deletions": 36,
                "filename": "ql/src/test/results/clientpositive/tez/union3.q.out",
                "patch": "@@ -54,82 +54,70 @@ STAGE PLANS:\n             Map Operator Tree:\n                 TableScan\n                   alias: src\n-                  Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n-                    Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                     Limit\n                       Number of rows: 1\n-                      Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                      Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                       Reduce Output Operator\n                         sort order: \n-                        Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n-                        value expressions: _col0 (type: string), _col1 (type: string)\n+                        Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n         Map 4 \n             Map Operator Tree:\n                 TableScan\n                   alias: src\n-                  Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n-                    Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                     Limit\n                       Number of rows: 1\n-                      Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                      Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                       Reduce Output Operator\n                         sort order: \n-                        Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n-                        value expressions: _col0 (type: string), _col1 (type: string)\n+                        Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n         Map 6 \n             Map Operator Tree:\n                 TableScan\n                   alias: src\n-                  Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n-                    Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                     Limit\n                       Number of rows: 1\n-                      Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                      Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                       Reduce Output Operator\n                         sort order: \n-                        Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n-                        value expressions: _col0 (type: string), _col1 (type: string)\n+                        Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n         Map 9 \n             Map Operator Tree:\n                 TableScan\n                   alias: src\n-                  Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n-                    Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                     Limit\n                       Number of rows: 1\n-                      Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                      Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                       Reduce Output Operator\n                         sort order: \n-                        Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n-                        value expressions: _col0 (type: string), _col1 (type: string)\n+                        Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n         Reducer 10 \n             Reduce Operator Tree:\n               Extract\n-                Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                 Limit\n                   Number of rows: 1\n-                  Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                   Select Operator\n                     expressions: 2 (type: int)\n                     outputColumnNames: _col0\n-                    Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                     Reduce Output Operator\n                       key expressions: _col0 (type: int)\n                       sort order: +\n                       Map-reduce partition columns: _col0 (type: int)\n-                      Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                      Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                       value expressions: _col0 (type: int)\n         Reducer 11 \n             Reduce Operator Tree:\n@@ -180,19 +168,19 @@ STAGE PLANS:\n         Reducer 7 \n             Reduce Operator Tree:\n               Extract\n-                Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                 Limit\n                   Number of rows: 1\n-                  Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                   Select Operator\n                     expressions: 1 (type: int)\n                     outputColumnNames: _col0\n-                    Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                     Reduce Output Operator\n                       key expressions: _col0 (type: int)\n                       sort order: +\n                       Map-reduce partition columns: _col0 (type: int)\n-                      Statistics: Num rows: 1 Data size: 200 Basic stats: COMPLETE Column stats: NONE\n+                      Statistics: Num rows: 0 Data size: 5812 Basic stats: PARTIAL Column stats: COMPLETE\n                       value expressions: _col0 (type: int)\n         Reducer 8 \n             Reduce Operator Tree:",
                "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union3.q.out",
                "sha": "b5ba13c22f2ddd3695e45e397851dc07373dcd7a",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union5.q.out",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/union5.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c",
                "deletions": 4,
                "filename": "ql/src/test/results/clientpositive/tez/union5.q.out",
                "patch": "@@ -64,8 +64,8 @@ STAGE PLANS:\n                 mode: mergepartial\n                 outputColumnNames: _col0\n                 Select Operator\n-                  expressions: 'tst1' (type: string), _col0 (type: bigint)\n-                  outputColumnNames: _col0, _col1\n+                  expressions: 'tst1' (type: string)\n+                  outputColumnNames: _col0\n                   Select Operator\n                     expressions: _col0 (type: string)\n                     outputColumnNames: _col0\n@@ -105,8 +105,8 @@ STAGE PLANS:\n                 mode: mergepartial\n                 outputColumnNames: _col0\n                 Select Operator\n-                  expressions: 'tst2' (type: string), _col0 (type: bigint)\n-                  outputColumnNames: _col0, _col1\n+                  expressions: 'tst2' (type: string)\n+                  outputColumnNames: _col0\n                   Select Operator\n                     expressions: _col0 (type: string)\n                     outputColumnNames: _col0",
                "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union5.q.out",
                "sha": "5c50f8e28e04a6a095e4b75730d81928e1a6fecf",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union7.q.out",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/union7.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c",
                "deletions": 4,
                "filename": "ql/src/test/results/clientpositive/tez/union7.q.out",
                "patch": "@@ -46,8 +46,8 @@ STAGE PLANS:\n                 TableScan\n                   alias: s2\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n+                    expressions: key (type: string)\n+                    outputColumnNames: _col0\n                     Select Operator\n                       expressions: _col0 (type: string)\n                       outputColumnNames: _col0\n@@ -68,8 +68,8 @@ STAGE PLANS:\n                 mode: mergepartial\n                 outputColumnNames: _col0\n                 Select Operator\n-                  expressions: 'tst1' (type: string), UDFToString(_col0) (type: string)\n-                  outputColumnNames: _col0, _col1\n+                  expressions: 'tst1' (type: string)\n+                  outputColumnNames: _col0\n                   Select Operator\n                     expressions: _col0 (type: string)\n                     outputColumnNames: _col0",
                "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union7.q.out",
                "sha": "38917f19de4b98c583035f21642b72af5b0ee469",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union9.q.out",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/union9.q.out?ref=b1f556296405814e6b492660c8f03bcb9d892d4c",
                "deletions": 9,
                "filename": "ql/src/test/results/clientpositive/tez/union9.q.out",
                "patch": "@@ -31,8 +31,6 @@ STAGE PLANS:\n                 TableScan\n                   alias: s1\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n                     Select Operator\n                       Group By Operator\n                         aggregations: count(1)\n@@ -46,8 +44,6 @@ STAGE PLANS:\n                 TableScan\n                   alias: s2\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n                     Select Operator\n                       Group By Operator\n                         aggregations: count(1)\n@@ -61,8 +57,6 @@ STAGE PLANS:\n                 TableScan\n                   alias: s3\n                   Select Operator\n-                    expressions: key (type: string), value (type: string)\n-                    outputColumnNames: _col0, _col1\n                     Select Operator\n                       Group By Operator\n                         aggregations: count(1)\n@@ -77,14 +71,14 @@ STAGE PLANS:\n                 aggregations: count(VALUE._col0)\n                 mode: mergepartial\n                 outputColumnNames: _col0\n-                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n                 Select Operator\n                   expressions: _col0 (type: bigint)\n                   outputColumnNames: _col0\n-                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n                   File Output Operator\n                     compressed: false\n-                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                    Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n                     table:\n                         input format: org.apache.hadoop.mapred.TextInputFormat\n                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat",
                "raw_url": "https://github.com/apache/hive/raw/b1f556296405814e6b492660c8f03bcb9d892d4c/ql/src/test/results/clientpositive/tez/union9.q.out",
                "sha": "04c5acb204d9da2decfc4d50ef6fad487218f518",
                "status": "modified"
            }
        ],
        "message": "HIVE-6984: Analyzing partitioned table with NULL values for the partition column failed with NPE (reviewed by Sergey)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1591796 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/cc2476929a811b2b5310600b421b52c366223070",
        "patched_files": [
            "union9.java",
            "TableScanOperator.java",
            "union5.java",
            "limit_pushdown.java",
            "bucket_map_join_tez1.java",
            "bucket_map_join_tez2.java",
            "mrr.java",
            "tez_union.java",
            "union3.java",
            "union2.java",
            "analyze_table_null_partition.java",
            "load_dyn_part1.java",
            "union7.java",
            "tez_dml.java",
            "count.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "test1.java"
        ]
    },
    "hive_b249f00": {
        "bug_id": "hive_b249f00",
        "commit": "https://github.com/apache/hive/commit/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/itests/src/test/resources/testconfiguration.properties",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "itests/src/test/resources/testconfiguration.properties",
                "patch": "@@ -313,6 +313,7 @@ minitez.query.files=bucket_map_join_tez1.q,\\\n   tez_schema_evolution.q,\\\n   tez_union.q,\\\n   tez_union2.q,\\\n+  tez_union_view.q,\\\n   tez_union_decimal.q,\\\n   tez_union_group_by.q,\\\n   tez_smb_main.q,\\",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/itests/src/test/resources/testconfiguration.properties",
                "sha": "0a5d839489e3ea79b2d69156c60a23371aaf2cce",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java",
                "patch": "@@ -365,6 +365,17 @@ private boolean validateMapWork(MapWork mapWork, boolean isTez) throws SemanticE\n       addMapWorkRules(opRules, vnp);\n       Dispatcher disp = new DefaultRuleDispatcher(vnp, opRules, null);\n       GraphWalker ogw = new DefaultGraphWalker(disp);\n+      if ((mapWork.getAliasToWork() == null) || (mapWork.getAliasToWork().size() == 0)) {\n+        return false;\n+      } else {\n+        for (Operator<?> op : mapWork.getAliasToWork().values()) {\n+          if (op == null) {\n+            LOG.warn(\"Map work has invalid aliases to work with. Fail validation!\");\n+            return false;\n+          }\n+        }\n+      }\n+\n       // iterator the mapper operator tree\n       ArrayList<Node> topNodes = new ArrayList<Node>();\n       topNodes.addAll(mapWork.getAliasToWork().values());",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java",
                "sha": "319aacbafedeeaedaeefe9db7c9f38867530bfc1",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 10,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java",
                "patch": "@@ -334,16 +334,8 @@ public void replaceRoots(Map<Operator<?>, Operator<?>> replacementMap) {\n   public Set<Operator<?>> getAllRootOperators() {\n     Set<Operator<?>> opSet = new LinkedHashSet<Operator<?>>();\n \n-    Map<String, ArrayList<String>> pa = getPathToAliases();\n-    if (pa != null) {\n-      for (List<String> ls : pa.values()) {\n-        for (String a : ls) {\n-          Operator<?> op = getAliasToWork().get(a);\n-          if (op != null ) {\n-            opSet.add(op);\n-          }\n-        }\n-      }\n+    for (Operator<?> op : getAliasToWork().values()) {\n+      opSet.add(op);\n     }\n     return opSet;\n   }",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/java/org/apache/hadoop/hive/ql/plan/MapWork.java",
                "sha": "3217df27bb5731a1dcd5db1ae17c5bdff2e3fbfc",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/queries/clientpositive/tez_union.q",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/tez_union.q?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/tez_union.q",
                "patch": "@@ -92,3 +92,21 @@ right outer join src s on u.key = s.key;\n \n select * from ut order by ukey, skey limit 20;\n drop table ut;\n+\n+set hive.vectorized.execution.enabled=true;\n+\n+create table TABLE1(EMP_NAME STRING, EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n+\n+create table table2 (EMP_NAME STRING) PARTITIONED BY (EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n+\n+CREATE OR REPLACE VIEW TABLE3 as select EMP_NAME, EMP_ID from TABLE1;\n+\n+explain formatted select count(*) from TABLE3;\n+\n+drop table table2;\n+\n+create table table2 (EMP_NAME STRING) PARTITIONED BY (EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n+\n+CREATE OR REPLACE VIEW TABLE3 as select EMP_NAME, EMP_ID from TABLE1 UNION ALL select EMP_NAME,EMP_ID from TABLE2;\n+\n+explain formatted select count(*) from TABLE3;",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/queries/clientpositive/tez_union.q",
                "sha": "96f58b21529a46b25f0dd06dba7d61fb6aaf0d7f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientnegative/join_nonexistent_part.q.out",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientnegative/join_nonexistent_part.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 1,
                "filename": "ql/src/test/results/clientnegative/join_nonexistent_part.q.out",
                "patch": "@@ -1,2 +1,2 @@\n-Warning: Shuffle Join JOIN[8][tables = [$hdt$_0]] in Stage 'Stage-1:MAPRED' is a cross product\n+Warning: Shuffle Join JOIN[8][tables = [$hdt$_0, $hdt$_1]] in Stage 'Stage-1:MAPRED' is a cross product\n Authorization failed:No privilege 'Select' found for inputs { database:default, table:srcpart, columnName:key}. Use SHOW GRANT to get more details.",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientnegative/join_nonexistent_part.q.out",
                "sha": "391dd0592611d7af8484c52efde3a50fb7dfa44d",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/annotate_stats_join.q.out",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/annotate_stats_join.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 16,
                "filename": "ql/src/test/results/clientpositive/annotate_stats_join.q.out",
                "patch": "@@ -439,22 +439,6 @@ STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n       Map Operator Tree:\n-          TableScan\n-            alias: e\n-            Statistics: Num rows: 48 Data size: 552 Basic stats: COMPLETE Column stats: COMPLETE\n-            Filter Operator\n-              predicate: deptid is not null (type: boolean)\n-              Statistics: Num rows: 48 Data size: 4752 Basic stats: COMPLETE Column stats: COMPLETE\n-              Select Operator\n-                expressions: lastname (type: string), deptid (type: int), locid (type: int)\n-                outputColumnNames: _col0, _col1, _col2\n-                Statistics: Num rows: 48 Data size: 4752 Basic stats: COMPLETE Column stats: COMPLETE\n-                Reduce Output Operator\n-                  key expressions: _col1 (type: int)\n-                  sort order: +\n-                  Map-reduce partition columns: _col1 (type: int)\n-                  Statistics: Num rows: 48 Data size: 4752 Basic stats: COMPLETE Column stats: COMPLETE\n-                  value expressions: _col0 (type: string), _col2 (type: int)\n           TableScan\n             alias: e\n             Statistics: Num rows: 48 Data size: 552 Basic stats: COMPLETE Column stats: COMPLETE\n@@ -487,6 +471,22 @@ STAGE PLANS:\n                   Map-reduce partition columns: _col0 (type: int)\n                   Statistics: Num rows: 6 Data size: 570 Basic stats: COMPLETE Column stats: COMPLETE\n                   value expressions: _col1 (type: string)\n+          TableScan\n+            alias: e\n+            Statistics: Num rows: 48 Data size: 552 Basic stats: COMPLETE Column stats: COMPLETE\n+            Filter Operator\n+              predicate: deptid is not null (type: boolean)\n+              Statistics: Num rows: 48 Data size: 4752 Basic stats: COMPLETE Column stats: COMPLETE\n+              Select Operator\n+                expressions: lastname (type: string), deptid (type: int), locid (type: int)\n+                outputColumnNames: _col0, _col1, _col2\n+                Statistics: Num rows: 48 Data size: 4752 Basic stats: COMPLETE Column stats: COMPLETE\n+                Reduce Output Operator\n+                  key expressions: _col1 (type: int)\n+                  sort order: +\n+                  Map-reduce partition columns: _col1 (type: int)\n+                  Statistics: Num rows: 48 Data size: 4752 Basic stats: COMPLETE Column stats: COMPLETE\n+                  value expressions: _col0 (type: string), _col2 (type: int)\n       Reduce Operator Tree:\n         Join Operator\n           condition map:",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/annotate_stats_join.q.out",
                "sha": "66e944b83665fb9c1272907e5e4f404ade46fcea",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/auto_join32.q.out",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/auto_join32.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/auto_join32.q.out",
                "patch": "@@ -391,6 +391,36 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: v\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: ((p = 'bar') and name is not null) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: name (type: string), registration (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Sorted Merge Bucket Map Join Operator\n+                  condition map:\n+                       Inner Join 0 to 1\n+                  keys:\n+                    0 _col0 (type: string)\n+                    1 _col0 (type: string)\n+                  outputColumnNames: _col1, _col3\n+                  Select Operator\n+                    expressions: _col3 (type: string), _col1 (type: string)\n+                    outputColumnNames: _col0, _col1\n+                    Group By Operator\n+                      aggregations: count(DISTINCT _col1)\n+                      keys: _col0 (type: string), _col1 (type: string)\n+                      mode: hash\n+                      outputColumnNames: _col0, _col1, _col2\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: string), _col1 (type: string)\n+                        sort order: ++\n+                        Map-reduce partition columns: _col0 (type: string)\n       Reduce Operator Tree:\n         Group By Operator\n           aggregations: count(DISTINCT KEY._col1:0._col0)",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/auto_join32.q.out",
                "sha": "bfc8be8599d73dd3a809e44fe515f8dd65a22557",
                "status": "modified"
            },
            {
                "additions": 88,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/bucketmapjoin1.q.out",
                "changes": 88,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/bucketmapjoin1.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/bucketmapjoin1.q.out",
                "patch": "@@ -125,6 +125,50 @@ STAGE PLANS:\n \n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: a\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: key is not null (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Map Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                keys:\n+                  0 key (type: int)\n+                  1 key (type: int)\n+                outputColumnNames: _col0, _col1, _col7\n+                Position of Big Table: 0\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                BucketMapJoin: true\n+                Select Operator\n+                  expressions: _col0 (type: int), _col1 (type: string), _col7 (type: string)\n+                  outputColumnNames: _col0, _col1, _col2\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  File Output Operator\n+                    compressed: false\n+                    GlobalTableId: 0\n+#### A masked pattern was here ####\n+                    NumFilesPerFileSink: 1\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+#### A masked pattern was here ####\n+                    table:\n+                        input format: org.apache.hadoop.mapred.TextInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                        properties:\n+                          columns _col0,_col1,_col2\n+                          columns.types int:string:string\n+                          escape.delim \\\n+                          hive.serialization.extend.additional.nesting.levels true\n+                          serialization.format 1\n+                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                    TotalFiles: 1\n+                    GatherStats: false\n+                    MultiFileSpray: false\n       Local Work:\n         Map Reduce Local Work\n \n@@ -249,6 +293,50 @@ STAGE PLANS:\n \n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: b\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: (key is not null and (ds = '2008-04-08')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Map Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                keys:\n+                  0 key (type: int)\n+                  1 key (type: int)\n+                outputColumnNames: _col0, _col1, _col7\n+                Position of Big Table: 1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                BucketMapJoin: true\n+                Select Operator\n+                  expressions: _col0 (type: int), _col1 (type: string), _col7 (type: string)\n+                  outputColumnNames: _col0, _col1, _col2\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  File Output Operator\n+                    compressed: false\n+                    GlobalTableId: 0\n+#### A masked pattern was here ####\n+                    NumFilesPerFileSink: 1\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+#### A masked pattern was here ####\n+                    table:\n+                        input format: org.apache.hadoop.mapred.TextInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                        properties:\n+                          columns _col0,_col1,_col2\n+                          columns.types int:string:string\n+                          escape.delim \\\n+                          hive.serialization.extend.additional.nesting.levels true\n+                          serialization.format 1\n+                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                    TotalFiles: 1\n+                    GatherStats: false\n+                    MultiFileSpray: false\n       Local Work:\n         Map Reduce Local Work\n ",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/bucketmapjoin1.q.out",
                "sha": "72f2a07524b33317726da843d980e51a23132b98",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/correlationoptimizer3.q.out",
                "changes": 60,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/correlationoptimizer3.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 30,
                "filename": "ql/src/test/results/clientpositive/correlationoptimizer3.q.out",
                "patch": "@@ -284,21 +284,6 @@ STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n       Map Operator Tree:\n-          TableScan\n-            alias: y\n-            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n-            Filter Operator\n-              predicate: key is not null (type: boolean)\n-              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n-              Select Operator\n-                expressions: key (type: string)\n-                outputColumnNames: _col0\n-                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n-                Reduce Output Operator\n-                  key expressions: _col0 (type: string)\n-                  sort order: +\n-                  Map-reduce partition columns: _col0 (type: string)\n-                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n           TableScan\n             alias: y\n             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n@@ -330,6 +315,21 @@ STAGE PLANS:\n                   Map-reduce partition columns: _col0 (type: string)\n                   Statistics: Num rows: 13 Data size: 99 Basic stats: COMPLETE Column stats: NONE\n                   value expressions: _col1 (type: string)\n+          TableScan\n+            alias: y\n+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+            Filter Operator\n+              predicate: key is not null (type: boolean)\n+              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string)\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col0 (type: string)\n+                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n           TableScan\n             alias: x\n             Statistics: Num rows: 25 Data size: 191 Basic stats: COMPLETE Column stats: NONE\n@@ -988,21 +988,6 @@ STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n       Map Operator Tree:\n-          TableScan\n-            alias: y\n-            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n-            Filter Operator\n-              predicate: key is not null (type: boolean)\n-              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n-              Select Operator\n-                expressions: key (type: string)\n-                outputColumnNames: _col0\n-                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n-                Reduce Output Operator\n-                  key expressions: _col0 (type: string)\n-                  sort order: +\n-                  Map-reduce partition columns: _col0 (type: string)\n-                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n           TableScan\n             alias: y\n             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n@@ -1034,6 +1019,21 @@ STAGE PLANS:\n                   Map-reduce partition columns: _col0 (type: string)\n                   Statistics: Num rows: 13 Data size: 99 Basic stats: COMPLETE Column stats: NONE\n                   value expressions: _col1 (type: string)\n+          TableScan\n+            alias: y\n+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+            Filter Operator\n+              predicate: key is not null (type: boolean)\n+              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string)\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col0 (type: string)\n+                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n           TableScan\n             alias: x\n             Statistics: Num rows: 25 Data size: 191 Basic stats: COMPLETE Column stats: NONE",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/correlationoptimizer3.q.out",
                "sha": "5389647fb276a3949c93ea0f20d64eb6733a91c7",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/correlationoptimizer6.q.out",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/correlationoptimizer6.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 16,
                "filename": "ql/src/test/results/clientpositive/correlationoptimizer6.q.out",
                "patch": "@@ -3031,22 +3031,6 @@ STAGE PLANS:\n                     Map-reduce partition columns: _col0 (type: string)\n                     Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n                     value expressions: _col1 (type: bigint)\n-          TableScan\n-            alias: x\n-            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n-            Filter Operator\n-              predicate: key is not null (type: boolean)\n-              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n-              Select Operator\n-                expressions: key (type: string), value (type: string)\n-                outputColumnNames: _col0, _col1\n-                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n-                Reduce Output Operator\n-                  key expressions: _col0 (type: string)\n-                  sort order: +\n-                  Map-reduce partition columns: _col0 (type: string)\n-                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n-                  value expressions: _col1 (type: string)\n           TableScan\n             alias: y\n             Statistics: Num rows: 25 Data size: 191 Basic stats: COMPLETE Column stats: NONE\n@@ -3069,6 +3053,22 @@ STAGE PLANS:\n                     Map-reduce partition columns: _col0 (type: string)\n                     Statistics: Num rows: 13 Data size: 99 Basic stats: COMPLETE Column stats: NONE\n                     value expressions: _col1 (type: bigint)\n+          TableScan\n+            alias: x\n+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+            Filter Operator\n+              predicate: key is not null (type: boolean)\n+              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string), value (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col0 (type: string)\n+                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                  value expressions: _col1 (type: string)\n       Reduce Operator Tree:\n         Demux Operator\n           Statistics: Num rows: 513 Data size: 5411 Basic stats: COMPLETE Column stats: NONE",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/correlationoptimizer6.q.out",
                "sha": "be518dc6684e52c2b0d388d472b89f5c85cf268f",
                "status": "modified"
            },
            {
                "additions": 54,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/groupby_sort_6.q.out",
                "changes": 54,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/groupby_sort_6.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/groupby_sort_6.q.out",
                "patch": "@@ -66,6 +66,33 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: t1\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: (ds = '1') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string)\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Group By Operator\n+                  aggregations: count(1)\n+                  keys: _col0 (type: string)\n+                  mode: hash\n+                  outputColumnNames: _col0, _col1\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: _col0 (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: _col0 (type: string)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    tag: -1\n+                    value expressions: _col1 (type: bigint)\n+                    auto parallelism: false\n       Needs Tagging: false\n       Reduce Operator Tree:\n         Group By Operator\n@@ -208,6 +235,33 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: t1\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: (ds = '1') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string)\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Group By Operator\n+                  aggregations: count(1)\n+                  keys: _col0 (type: string)\n+                  mode: hash\n+                  outputColumnNames: _col0, _col1\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: _col0 (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: _col0 (type: string)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    tag: -1\n+                    value expressions: _col1 (type: bigint)\n+                    auto parallelism: false\n       Needs Tagging: false\n       Reduce Operator Tree:\n         Group By Operator",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/groupby_sort_6.q.out",
                "sha": "c5cb8b9908cc25710ec57f21e5849e2069dedcda",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/input23.q.out",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/input23.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 2,
                "filename": "ql/src/test/results/clientpositive/input23.q.out",
                "patch": "@@ -1,4 +1,4 @@\n-Warning: Shuffle Join JOIN[9][tables = [$hdt$_0]] in Stage 'Stage-1:MAPRED' is a cross product\n+Warning: Shuffle Join JOIN[9][tables = [$hdt$_0, $hdt$_1]] in Stage 'Stage-1:MAPRED' is a cross product\n PREHOOK: query: explain extended\n  select * from srcpart a join srcpart b where a.ds = '2008-04-08' and a.hr = '11' and b.ds = '2008-04-08' and b.hr = '14' limit 5\n PREHOOK: type: QUERY\n@@ -79,6 +79,24 @@ STAGE PLANS:\n                 tag: 0\n                 value expressions: _col0 (type: string), _col1 (type: string)\n                 auto parallelism: false\n+          TableScan\n+            alias: a\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: ((ds = '2008-04-08') and (hr = '14')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string), value (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Reduce Output Operator\n+                  sort order: \n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  tag: 1\n+                  value expressions: _col0 (type: string), _col1 (type: string)\n+                  auto parallelism: false\n       Path -> Alias:\n #### A masked pattern was here ####\n       Path -> Partition:\n@@ -175,7 +193,7 @@ STAGE PLANS:\n       Processor Tree:\n         ListSink\n \n-Warning: Shuffle Join JOIN[9][tables = [$hdt$_0]] in Stage 'Stage-1:MAPRED' is a cross product\n+Warning: Shuffle Join JOIN[9][tables = [$hdt$_0, $hdt$_1]] in Stage 'Stage-1:MAPRED' is a cross product\n PREHOOK: query: select * from srcpart a join srcpart b where a.ds = '2008-04-08' and a.hr = '11' and b.ds = '2008-04-08' and b.hr = '14' limit 5\n PREHOOK: type: QUERY\n PREHOOK: Input: default@srcpart",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/input23.q.out",
                "sha": "73038c34cb24804bc1f3338e8864663f701bcac7",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/input26.q.out",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/input26.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/input26.q.out",
                "patch": "@@ -87,6 +87,24 @@ STAGE PLANS:\n \n   Stage: Stage-3\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: a\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: ((ds = '2008-04-08') and (hr = '14')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string), value (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Limit\n+                  Number of rows: 5\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Reduce Output Operator\n+                    sort order: \n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    value expressions: _col0 (type: string), _col1 (type: string)\n       Reduce Operator Tree:\n         Select Operator\n           expressions: VALUE._col0 (type: string), VALUE._col1 (type: string)",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/input26.q.out",
                "sha": "1b24aa672a3c37bd57dad18abeed827dcc244200",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/join_cond_pushdown_unqual2.q.out",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/join_cond_pushdown_unqual2.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 12,
                "filename": "ql/src/test/results/clientpositive/join_cond_pushdown_unqual2.q.out",
                "patch": "@@ -80,18 +80,6 @@ STAGE PLANS:\n                 Map-reduce partition columns: p_name (type: string)\n                 Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n                 value expressions: p_partkey (type: int), p_mfgr (type: string), p_brand (type: string), p_type (type: string), p_size (type: int), p_container (type: string), p_retailprice (type: double), p_comment (type: string)\n-          TableScan\n-            alias: p4\n-            Statistics: Num rows: 26 Data size: 3147 Basic stats: COMPLETE Column stats: NONE\n-            Filter Operator\n-              predicate: p_name is not null (type: boolean)\n-              Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n-              Reduce Output Operator\n-                key expressions: p_name (type: string)\n-                sort order: +\n-                Map-reduce partition columns: p_name (type: string)\n-                Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n-                value expressions: p_partkey (type: int), p_mfgr (type: string), p_brand (type: string), p_type (type: string), p_size (type: int), p_container (type: string), p_retailprice (type: double), p_comment (type: string)\n           TableScan\n             alias: p2\n             Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n@@ -116,6 +104,18 @@ STAGE PLANS:\n                 Map-reduce partition columns: p3_name (type: string)\n                 Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n                 value expressions: p3_partkey (type: int), p3_mfgr (type: string), p3_brand (type: string), p3_type (type: string), p3_size (type: int), p3_container (type: string), p3_retailprice (type: double), p3_comment (type: string)\n+          TableScan\n+            alias: p4\n+            Statistics: Num rows: 26 Data size: 3147 Basic stats: COMPLETE Column stats: NONE\n+            Filter Operator\n+              predicate: p_name is not null (type: boolean)\n+              Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n+              Reduce Output Operator\n+                key expressions: p_name (type: string)\n+                sort order: +\n+                Map-reduce partition columns: p_name (type: string)\n+                Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n+                value expressions: p_partkey (type: int), p_mfgr (type: string), p_brand (type: string), p_type (type: string), p_size (type: int), p_container (type: string), p_retailprice (type: double), p_comment (type: string)\n       Reduce Operator Tree:\n         Join Operator\n           condition map:",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/join_cond_pushdown_unqual2.q.out",
                "sha": "678ddb8c0a938832fcaaaace1218e9db8f1e0247",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/join_cond_pushdown_unqual4.q.out",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/join_cond_pushdown_unqual4.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 12,
                "filename": "ql/src/test/results/clientpositive/join_cond_pushdown_unqual4.q.out",
                "patch": "@@ -82,18 +82,6 @@ STAGE PLANS:\n                 Map-reduce partition columns: p_name (type: string)\n                 Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n                 value expressions: p_partkey (type: int), p_mfgr (type: string), p_brand (type: string), p_type (type: string), p_size (type: int), p_container (type: string), p_retailprice (type: double), p_comment (type: string)\n-          TableScan\n-            alias: p4\n-            Statistics: Num rows: 26 Data size: 3147 Basic stats: COMPLETE Column stats: NONE\n-            Filter Operator\n-              predicate: p_name is not null (type: boolean)\n-              Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n-              Reduce Output Operator\n-                key expressions: p_name (type: string)\n-                sort order: +\n-                Map-reduce partition columns: p_name (type: string)\n-                Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n-                value expressions: p_partkey (type: int), p_mfgr (type: string), p_brand (type: string), p_type (type: string), p_size (type: int), p_container (type: string), p_retailprice (type: double), p_comment (type: string)\n           TableScan\n             alias: p2\n             Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n@@ -118,6 +106,18 @@ STAGE PLANS:\n                 Map-reduce partition columns: p3_name (type: string)\n                 Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n                 value expressions: p3_partkey (type: int), p3_mfgr (type: string), p3_brand (type: string), p3_type (type: string), p3_size (type: int), p3_container (type: string), p3_retailprice (type: double), p3_comment (type: string)\n+          TableScan\n+            alias: p4\n+            Statistics: Num rows: 26 Data size: 3147 Basic stats: COMPLETE Column stats: NONE\n+            Filter Operator\n+              predicate: p_name is not null (type: boolean)\n+              Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n+              Reduce Output Operator\n+                key expressions: p_name (type: string)\n+                sort order: +\n+                Map-reduce partition columns: p_name (type: string)\n+                Statistics: Num rows: 13 Data size: 1573 Basic stats: COMPLETE Column stats: NONE\n+                value expressions: p_partkey (type: int), p_mfgr (type: string), p_brand (type: string), p_type (type: string), p_size (type: int), p_container (type: string), p_retailprice (type: double), p_comment (type: string)\n       Reduce Operator Tree:\n         Join Operator\n           condition map:",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/join_cond_pushdown_unqual4.q.out",
                "sha": "4668eb1d59e1b705c9562d857c3731bbb3fa01b5",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/join_view.q.out",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/join_view.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/join_view.q.out",
                "patch": "@@ -49,6 +49,31 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: invites\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: (ds = '2011-09-01') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Reduce Output Operator\n+                key expressions: '2011-09-01' (type: string)\n+                sort order: +\n+                Map-reduce partition columns: '2011-09-01' (type: string)\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                value expressions: bar (type: string)\n+          TableScan\n+            alias: invites2\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: (ds = '2011-09-01') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Reduce Output Operator\n+                key expressions: '2011-09-01' (type: string)\n+                sort order: +\n+                Map-reduce partition columns: '2011-09-01' (type: string)\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                value expressions: foo (type: int)\n       Reduce Operator Tree:\n         Join Operator\n           condition map:",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/join_view.q.out",
                "sha": "e703e0b33b5961221b32081b0148dc21ad40a1da",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/metadataonly1.q.out",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/metadataonly1.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/metadataonly1.q.out",
                "patch": "@@ -36,6 +36,26 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: test1\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Select Operator\n+              expressions: ds (type: string)\n+              outputColumnNames: _col0\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Group By Operator\n+                aggregations: max(_col0)\n+                mode: hash\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 84 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  sort order: \n+                  Statistics: Num rows: 1 Data size: 84 Basic stats: COMPLETE Column stats: NONE\n+                  tag: -1\n+                  value expressions: _col0 (type: string)\n+                  auto parallelism: false\n       Needs Tagging: false\n       Reduce Operator Tree:\n         Group By Operator",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/metadataonly1.q.out",
                "sha": "e55efd599a93dee4bf91900d542cdbaa08ae57a0",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/nullgroup5.q.out",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/nullgroup5.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/nullgroup5.q.out",
                "patch": "@@ -56,6 +56,25 @@ STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n       Map Operator Tree:\n+          TableScan\n+            alias: x\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: (ds = '2009-04-05') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string), value (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  File Output Operator\n+                    compressed: false\n+                    Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    table:\n+                        input format: org.apache.hadoop.mapred.TextInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n           TableScan\n             alias: y\n             Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/nullgroup5.q.out",
                "sha": "8a94d62f2be4ff0f594c14f5e8342fc8b83c66d9",
                "status": "modified"
            },
            {
                "additions": 80,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/optimize_nullscan.q.out",
                "changes": 122,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/optimize_nullscan.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 42,
                "filename": "ql/src/test/results/clientpositive/optimize_nullscan.q.out",
                "patch": "@@ -176,6 +176,29 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: srcpart\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: false (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Group By Operator\n+                aggregations: count(key)\n+                keys: key (type: string)\n+                mode: hash\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col0 (type: string)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  tag: -1\n+                  value expressions: _col1 (type: bigint)\n+                  auto parallelism: false\n       Needs Tagging: false\n       Reduce Operator Tree:\n         Group By Operator\n@@ -534,15 +557,6 @@ STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n       Map Operator Tree:\n-          TableScan\n-            GatherStats: false\n-            Reduce Output Operator\n-              key expressions: _col0 (type: string)\n-              sort order: +\n-              Map-reduce partition columns: _col0 (type: string)\n-              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n-              tag: 1\n-              auto parallelism: false\n           TableScan\n             alias: src\n             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n@@ -562,6 +576,15 @@ STAGE PLANS:\n                   Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n                   tag: 0\n                   auto parallelism: false\n+          TableScan\n+            GatherStats: false\n+            Reduce Output Operator\n+              key expressions: _col0 (type: string)\n+              sort order: +\n+              Map-reduce partition columns: _col0 (type: string)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              tag: 1\n+              auto parallelism: false\n       Path -> Alias:\n         -mr-10003default.src{} [a:src]\n #### A masked pattern was here ####\n@@ -1510,14 +1533,6 @@ STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n       Map Operator Tree:\n-          TableScan\n-            GatherStats: false\n-            Reduce Output Operator\n-              sort order: \n-              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n-              tag: 1\n-              value expressions: _col0 (type: string)\n-              auto parallelism: false\n           TableScan\n             alias: src\n             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n@@ -1536,6 +1551,14 @@ STAGE PLANS:\n                   tag: 0\n                   value expressions: _col0 (type: string)\n                   auto parallelism: false\n+          TableScan\n+            GatherStats: false\n+            Reduce Output Operator\n+              sort order: \n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              tag: 1\n+              value expressions: _col0 (type: string)\n+              auto parallelism: false\n       Path -> Alias:\n         -mr-10003default.src{} [a:src]\n #### A masked pattern was here ####\n@@ -1752,6 +1775,21 @@ STAGE PLANS:\n                 Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n                 tag: 0\n                 auto parallelism: false\n+          TableScan\n+            alias: srcpart\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: false (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Reduce Output Operator\n+                key expressions: key (type: string)\n+                sort order: +\n+                Map-reduce partition columns: key (type: string)\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                tag: 1\n+                auto parallelism: false\n       Path -> Alias:\n         -mr-10004default.src{} [null-subquery2:a-subquery2:src]\n       Path -> Partition:\n@@ -1831,31 +1869,6 @@ STAGE PLANS:\n   Stage: Stage-2\n     Map Reduce\n       Map Operator Tree:\n-          TableScan\n-            GatherStats: false\n-            Union\n-              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n-              File Output Operator\n-                compressed: false\n-                GlobalTableId: 0\n-#### A masked pattern was here ####\n-                NumFilesPerFileSink: 1\n-                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n-#### A masked pattern was here ####\n-                table:\n-                    input format: org.apache.hadoop.mapred.TextInputFormat\n-                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n-                    properties:\n-                      columns _col0\n-                      columns.types string\n-                      escape.delim \\\n-                      hive.serialization.extend.additional.nesting.levels true\n-                      serialization.format 1\n-                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-                TotalFiles: 1\n-                GatherStats: false\n-                MultiFileSpray: false\n           TableScan\n             alias: src\n             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n@@ -1891,6 +1904,31 @@ STAGE PLANS:\n                     TotalFiles: 1\n                     GatherStats: false\n                     MultiFileSpray: false\n+          TableScan\n+            GatherStats: false\n+            Union\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              File Output Operator\n+                compressed: false\n+                GlobalTableId: 0\n+#### A masked pattern was here ####\n+                NumFilesPerFileSink: 1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+#### A masked pattern was here ####\n+                table:\n+                    input format: org.apache.hadoop.mapred.TextInputFormat\n+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                    properties:\n+                      columns _col0\n+                      columns.types string\n+                      escape.delim \\\n+                      hive.serialization.extend.additional.nesting.levels true\n+                      serialization.format 1\n+                      serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                TotalFiles: 1\n+                GatherStats: false\n+                MultiFileSpray: false\n       Path -> Alias:\n         -mr-10003default.src{} [null-subquery1:a-subquery1:src]\n #### A masked pattern was here ####",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/optimize_nullscan.q.out",
                "sha": "1f4becf14aa10158a88db1e2c95b76ec068115bd",
                "status": "modified"
            },
            {
                "additions": 32,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/partition_boolexpr.q.out",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/partition_boolexpr.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/partition_boolexpr.q.out",
                "patch": "@@ -84,6 +84,22 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: srcpart\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: false (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Group By Operator\n+                aggregations: count(1)\n+                mode: hash\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  sort order: \n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  value expressions: _col0 (type: bigint)\n       Reduce Operator Tree:\n         Group By Operator\n           aggregations: count(VALUE._col0)\n@@ -253,6 +269,22 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: srcpart\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: false (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Group By Operator\n+                aggregations: count(1)\n+                mode: hash\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  sort order: \n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  value expressions: _col0 (type: bigint)\n       Reduce Operator Tree:\n         Group By Operator\n           aggregations: count(VALUE._col0)",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/partition_boolexpr.q.out",
                "sha": "cfd03e23ce354fdb8dde6f64b67af02e6025fa09",
                "status": "modified"
            },
            {
                "additions": 80,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/ppd_union_view.q.out",
                "changes": 80,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/ppd_union_view.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/ppd_union_view.q.out",
                "patch": "@@ -331,6 +331,45 @@ STAGE PLANS:\n   Stage: Stage-2\n     Map Reduce\n       Map Operator Tree:\n+          TableScan\n+            alias: t1_new\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: (ds = '2011-10-13') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string), value (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 1 Data size: 15 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: _col0 (type: string), _col1 (type: string), '2011-10-13' (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 1 Data size: 15 Basic stats: COMPLETE Column stats: NONE\n+                    File Output Operator\n+                      compressed: false\n+                      GlobalTableId: 0\n+#### A masked pattern was here ####\n+                      NumFilesPerFileSink: 1\n+                      Statistics: Num rows: 1 Data size: 15 Basic stats: COMPLETE Column stats: NONE\n+#### A masked pattern was here ####\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          properties:\n+                            columns _col0,_col1,_col2\n+                            columns.types string:string:string\n+                            escape.delim \\\n+                            hive.serialization.extend.additional.nesting.levels true\n+                            serialization.format 1\n+                            serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                      TotalFiles: 1\n+                      GatherStats: false\n+                      MultiFileSpray: false\n           TableScan\n             GatherStats: false\n             Union\n@@ -465,6 +504,47 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-3\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: t1_old\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: ((ds = '2011-10-15') and keymap is not null) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: keymap (type: string), value (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col0 (type: string)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  tag: 0\n+                  value expressions: _col1 (type: string)\n+                  auto parallelism: false\n+          TableScan\n+            alias: t1_mapping\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: (('2011-10-15' = ds) and keymap is not null) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: string), keymap (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col1 (type: string)\n+                  sort order: +\n+                  Map-reduce partition columns: _col1 (type: string)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  tag: 1\n+                  value expressions: _col0 (type: string)\n+                  auto parallelism: false\n       Needs Tagging: true\n       Reduce Operator Tree:\n         Join Operator",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/ppd_union_view.q.out",
                "sha": "a13ef7ae6e9412ad17d02ac43e202b8e9a63bd36",
                "status": "modified"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/reduce_deduplicate.q.out",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/reduce_deduplicate.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/reduce_deduplicate.q.out",
                "patch": "@@ -382,6 +382,40 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: complex_tbl_2\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: (ds = '2010-03-29') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: aet (type: string), aes (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Transform Operator\n+                  command: cat\n+                  output info:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      properties:\n+                        columns _col0,_col1,_col2,_col3,_col4,_col5,_col6\n+                        columns.types string,string,int,string,bigint,string,string\n+                        field.delim 9\n+                        serialization.format 9\n+                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: _col1 (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: _col1 (type: string)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    tag: -1\n+                    value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: int), _col3 (type: string), _col4 (type: bigint), _col5 (type: string), _col6 (type: string)\n+                    auto parallelism: false\n       Needs Tagging: false\n       Reduce Operator Tree:\n         Select Operator",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/reduce_deduplicate.q.out",
                "sha": "fa714b8a9c09761ce55f512a76e149563f0dfe6c",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/sample6.q.out",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/sample6.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/sample6.q.out",
                "patch": "@@ -3195,6 +3195,26 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: s\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: true\n+              predicate: (((hash(key) & 2147483647) % 2) = 0) (type: boolean)\n+              sampleDesc: BUCKET 1 OUT OF 2\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: key (type: int), value (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: int), _col1 (type: string)\n+                  sort order: ++\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  tag: -1\n+                  auto parallelism: false\n       Needs Tagging: false\n       Reduce Operator Tree:\n         Select Operator",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/sample6.q.out",
                "sha": "9891cbbeb963be1b3d7a30036464481b138fd155",
                "status": "modified"
            },
            {
                "additions": 80,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/smb_mapjoin9.q.out",
                "changes": 80,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/smb_mapjoin9.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/smb_mapjoin9.q.out",
                "patch": "@@ -112,6 +112,46 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: a\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: (key is not null and (ds = '2010-10-15')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Sorted Merge Bucket Map Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                keys:\n+                  0 key (type: int)\n+                  1 key (type: int)\n+                outputColumnNames: _col0, _col6, _col7\n+                Position of Big Table: 0\n+                Select Operator\n+                  expressions: _col6 (type: int), _col7 (type: string), '2010-10-15' (type: string), _col0 (type: int)\n+                  outputColumnNames: _col0, _col1, _col2, _col3\n+                  File Output Operator\n+                    compressed: false\n+                    GlobalTableId: 0\n+#### A masked pattern was here ####\n+                    NumFilesPerFileSink: 1\n+#### A masked pattern was here ####\n+                    table:\n+                        input format: org.apache.hadoop.mapred.TextInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                        properties:\n+                          columns _col0,_col1,_col2,_col3\n+                          columns.types int:string:string:int\n+                          escape.delim \\\n+                          hive.serialization.extend.additional.nesting.levels true\n+                          serialization.format 1\n+                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                    TotalFiles: 1\n+                    GatherStats: false\n+                    MultiFileSpray: false\n \n   Stage: Stage-0\n     Fetch Operator\n@@ -231,6 +271,46 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: b\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            GatherStats: false\n+            Filter Operator\n+              isSamplingPred: false\n+              predicate: (key is not null and (ds = '2010-10-15')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Sorted Merge Bucket Map Join Operator\n+                condition map:\n+                     Inner Join 0 to 1\n+                keys:\n+                  0 key (type: int)\n+                  1 key (type: int)\n+                outputColumnNames: _col0, _col6, _col7\n+                Position of Big Table: 1\n+                Select Operator\n+                  expressions: _col6 (type: int), _col7 (type: string), '2010-10-15' (type: string), _col0 (type: int)\n+                  outputColumnNames: _col0, _col1, _col2, _col3\n+                  File Output Operator\n+                    compressed: false\n+                    GlobalTableId: 0\n+#### A masked pattern was here ####\n+                    NumFilesPerFileSink: 1\n+#### A masked pattern was here ####\n+                    table:\n+                        input format: org.apache.hadoop.mapred.TextInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                        properties:\n+                          columns _col0,_col1,_col2,_col3\n+                          columns.types int:string:string:int\n+                          escape.delim \\\n+                          hive.serialization.extend.additional.nesting.levels true\n+                          serialization.format 1\n+                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                    TotalFiles: 1\n+                    GatherStats: false\n+                    MultiFileSpray: false\n \n   Stage: Stage-0\n     Fetch Operator",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/smb_mapjoin9.q.out",
                "sha": "9530be11299c683a19bcce90895adbf8e088370c",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/auto_join32.q.out",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/auto_join32.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/spark/auto_join32.q.out",
                "patch": "@@ -426,6 +426,32 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: (name is not null and (p = 'bar')) (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Sorted Merge Bucket Map Join Operator\n+                      condition map:\n+                           Inner Join 0 to 1\n+                      keys:\n+                        0 name (type: string)\n+                        1 name (type: string)\n+                      outputColumnNames: _col0, _col9\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Group By Operator\n+                        aggregations: count(DISTINCT _col9)\n+                        keys: _col0 (type: string), _col9 (type: string)\n+                        mode: hash\n+                        outputColumnNames: _col0, _col1, _col2\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                        Reduce Output Operator\n+                          key expressions: _col0 (type: string), _col1 (type: string)\n+                          sort order: ++\n+                          Map-reduce partition columns: _col0 (type: string)\n+                          Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n         Reducer 2 \n             Reduce Operator Tree:\n               Group By Operator",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/auto_join32.q.out",
                "sha": "c537b950d570be6dab5df5aedcb077a6a3451dbf",
                "status": "modified"
            },
            {
                "additions": 120,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/bucketmapjoin1.q.out",
                "changes": 120,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/bucketmapjoin1.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/spark/bucketmapjoin1.q.out",
                "patch": "@@ -101,6 +101,20 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 2 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: b\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: false\n+                    predicate: (key is not null and (ds = '2008-04-08')) (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Spark HashTable Sink Operator\n+                      keys:\n+                        0 key (type: int)\n+                        1 key (type: int)\n+                      Position of Big Table: 0\n             Local Work:\n               Map Reduce Local Work\n                 Bucket Mapjoin Context:\n@@ -112,6 +126,52 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: a\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: false\n+                    predicate: key is not null (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Map Join Operator\n+                      condition map:\n+                           Inner Join 0 to 1\n+                      keys:\n+                        0 key (type: int)\n+                        1 key (type: int)\n+                      outputColumnNames: _col0, _col1, _col7\n+                      input vertices:\n+                        1 Map 2\n+                      Position of Big Table: 0\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      BucketMapJoin: true\n+                      Select Operator\n+                        expressions: _col0 (type: int), _col1 (type: string), _col7 (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                        File Output Operator\n+                          compressed: false\n+                          GlobalTableId: 0\n+#### A masked pattern was here ####\n+                          NumFilesPerFileSink: 1\n+                          Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+#### A masked pattern was here ####\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              properties:\n+                                columns _col0,_col1,_col2\n+                                columns.types int:string:string\n+                                escape.delim \\\n+                                hive.serialization.extend.additional.nesting.levels true\n+                                serialization.format 1\n+                                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                          TotalFiles: 1\n+                          GatherStats: false\n+                          MultiFileSpray: false\n             Local Work:\n               Map Reduce Local Work\n                 Bucket Mapjoin Context:\n@@ -215,6 +275,20 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: a\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: false\n+                    predicate: key is not null (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Spark HashTable Sink Operator\n+                      keys:\n+                        0 key (type: int)\n+                        1 key (type: int)\n+                      Position of Big Table: 1\n             Local Work:\n               Map Reduce Local Work\n                 Bucket Mapjoin Context:\n@@ -226,6 +300,52 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 2 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: b\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: false\n+                    predicate: (key is not null and (ds = '2008-04-08')) (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Map Join Operator\n+                      condition map:\n+                           Inner Join 0 to 1\n+                      keys:\n+                        0 key (type: int)\n+                        1 key (type: int)\n+                      outputColumnNames: _col0, _col1, _col7\n+                      input vertices:\n+                        0 Map 1\n+                      Position of Big Table: 1\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      BucketMapJoin: true\n+                      Select Operator\n+                        expressions: _col0 (type: int), _col1 (type: string), _col7 (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                        File Output Operator\n+                          compressed: false\n+                          GlobalTableId: 0\n+#### A masked pattern was here ####\n+                          NumFilesPerFileSink: 1\n+                          Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+#### A masked pattern was here ####\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              properties:\n+                                columns _col0,_col1,_col2\n+                                columns.types int:string:string\n+                                escape.delim \\\n+                                hive.serialization.extend.additional.nesting.levels true\n+                                serialization.format 1\n+                                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                          TotalFiles: 1\n+                          GatherStats: false\n+                          MultiFileSpray: false\n             Local Work:\n               Map Reduce Local Work\n                 Bucket Mapjoin Context:",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/bucketmapjoin1.q.out",
                "sha": "44f4d0c045b9623a20964117b9f64214e9cc9231",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/join_view.q.out",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/join_view.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/spark/join_view.q.out",
                "patch": "@@ -54,7 +54,33 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: invites\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: (ds = '2011-09-01') (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: '2011-09-01' (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: '2011-09-01' (type: string)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      value expressions: bar (type: string)\n         Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: invites2\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: (ds = '2011-09-01') (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: '2011-09-01' (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: '2011-09-01' (type: string)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      value expressions: foo (type: int)\n         Reducer 2 \n             Reduce Operator Tree:\n               Join Operator",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/join_view.q.out",
                "sha": "f6e0542390889645edfb1845dc9983e30f23480f",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/optimize_nullscan.q.out",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/optimize_nullscan.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/spark/optimize_nullscan.q.out",
                "patch": "@@ -107,6 +107,29 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: srcpart\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: false\n+                    predicate: false (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count(key)\n+                      keys: key (type: string)\n+                      mode: hash\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: string)\n+                        sort order: +\n+                        Map-reduce partition columns: _col0 (type: string)\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                        tag: -1\n+                        value expressions: _col1 (type: bigint)\n+                        auto parallelism: false\n         Reducer 2 \n             Needs Tagging: false\n             Reduce Operator Tree:\n@@ -1654,6 +1677,22 @@ STAGE PLANS:\n             Truncated Path -> Alias:\n               -mr-10003default.src{} [src]\n         Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: srcpart\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: false\n+                    predicate: false (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: key (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: key (type: string)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      tag: 1\n+                      auto parallelism: false\n         Reducer 3 \n             Needs Tagging: true\n             Reduce Operator Tree:",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/optimize_nullscan.q.out",
                "sha": "843570e4db1b89afbd3d1684e6ee50c7d9723bc4",
                "status": "modified"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/reduce_deduplicate.q.out",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/reduce_deduplicate.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/spark/reduce_deduplicate.q.out",
                "patch": "@@ -393,6 +393,40 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: complex_tbl_2\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: false\n+                    predicate: (ds = '2010-03-29') (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Select Operator\n+                      expressions: aet (type: string), aes (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Transform Operator\n+                        command: cat\n+                        output info:\n+                            input format: org.apache.hadoop.mapred.TextInputFormat\n+                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                            properties:\n+                              columns _col0,_col1,_col2,_col3,_col4,_col5,_col6\n+                              columns.types string,string,int,string,bigint,string,string\n+                              field.delim 9\n+                              serialization.format 9\n+                              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                        Reduce Output Operator\n+                          key expressions: _col1 (type: string)\n+                          sort order: +\n+                          Map-reduce partition columns: _col1 (type: string)\n+                          Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                          tag: -1\n+                          value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: int), _col3 (type: string), _col4 (type: bigint), _col5 (type: string), _col6 (type: string)\n+                          auto parallelism: false\n         Reducer 2 \n             Needs Tagging: false\n             Reduce Operator Tree:",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/reduce_deduplicate.q.out",
                "sha": "8d3f56b2fb379acd1d887a77d2abdd778efc7cad",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/sample6.q.out",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/sample6.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/spark/sample6.q.out",
                "patch": "@@ -3081,6 +3081,26 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: true\n+                    predicate: (((hash(key) & 2147483647) % 2) = 0) (type: boolean)\n+                    sampleDesc: BUCKET 1 OUT OF 2\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Select Operator\n+                      expressions: key (type: int), value (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: int), _col1 (type: string)\n+                        sort order: ++\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                        tag: -1\n+                        auto parallelism: false\n         Reducer 2 \n             Needs Tagging: false\n             Reduce Operator Tree:",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/sample6.q.out",
                "sha": "4117732faa988a5c62647124218a9fe5796410bf",
                "status": "modified"
            },
            {
                "additions": 264,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/union_view.q.out",
                "changes": 264,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/spark/union_view.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/spark/union_view.q.out",
                "patch": "@@ -290,7 +290,43 @@ STAGE PLANS:\n                               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n                               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n         Map 2 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: ((key = 86) and (ds = '1')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '1')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '1' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n         Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: ((key = 86) and (ds = '1')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '1')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '1' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n \n   Stage: Stage-0\n     Fetch Operator\n@@ -308,6 +344,24 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: ((key = 86) and (ds = '2')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '2')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '2' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n         Map 2 \n             Map Operator Tree:\n                 TableScan\n@@ -328,6 +382,24 @@ STAGE PLANS:\n                               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n                               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n         Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: ((key = 86) and (ds = '2')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '2')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '2' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n \n   Stage: Stage-0\n     Fetch Operator\n@@ -345,7 +417,43 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: ((key = 86) and (ds = '3')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '3')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '3' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n         Map 2 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: ((key = 86) and (ds = '3')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '3')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '3' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n         Map 3 \n             Map Operator Tree:\n                 TableScan\n@@ -490,7 +598,37 @@ STAGE PLANS:\n                           sort order: \n                           value expressions: _col0 (type: bigint)\n         Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: (ds = '1') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '1') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n         Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: (ds = '1') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '1') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n         Reducer 2 \n             Reduce Operator Tree:\n               Group By Operator\n@@ -524,6 +662,21 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: (ds = '2') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '2') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n         Map 3 \n             Map Operator Tree:\n                 TableScan\n@@ -539,6 +692,21 @@ STAGE PLANS:\n                           sort order: \n                           value expressions: _col0 (type: bigint)\n         Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: (ds = '2') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '2') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n         Reducer 2 \n             Reduce Operator Tree:\n               Group By Operator\n@@ -572,7 +740,37 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: (ds = '3') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '3') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n         Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: (ds = '3') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '3') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n         Map 4 \n             Map Operator Tree:\n                 TableScan\n@@ -621,7 +819,43 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: ((key = 86) and (ds = '4')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '4')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '4' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n         Map 2 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: ((key = 86) and (ds = '4')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '4')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '4' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n         Map 3 \n             Map Operator Tree:\n                 TableScan\n@@ -661,7 +895,37 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: (ds = '4') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '4') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n         Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: (ds = '4') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '4') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n         Map 4 \n             Map Operator Tree:\n                 TableScan",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/spark/union_view.q.out",
                "sha": "2ca9e1332764e8b826c815aa3b5ce234cb826c29",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/dynamic_partition_pruning.q.out",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/dynamic_partition_pruning.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/tez/dynamic_partition_pruning.q.out",
                "patch": "@@ -2588,6 +2588,19 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: srcpart\n+                  filterExpr: ((ds is not null and hr is not null) and (hr = 13)) (type: boolean)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: ((ds is not null and hr is not null) and (hr = 13)) (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: ds (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: ds (type: string)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n         Map 5 \n             Map Operator Tree:\n                 TableScan\n@@ -4595,6 +4608,28 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: srcpart\n+                  filterExpr: ((ds is not null and hr is not null) and (hr = 13)) (type: boolean)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: ((ds is not null and hr is not null) and (hr = 13)) (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Map Join Operator\n+                      condition map:\n+                           Inner Join 0 to 1\n+                      keys:\n+                        0 ds (type: string)\n+                        1 ds (type: string)\n+                      input vertices:\n+                        1 Map 2\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: '13' (type: string)\n+                        sort order: +\n+                        Map-reduce partition columns: '13' (type: string)\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n         Map 2 \n             Map Operator Tree:\n                 TableScan",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/dynamic_partition_pruning.q.out",
                "sha": "9a04fa2d2132e025fa7c58389ae2a3ef82d337fb",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/metadataonly1.q.out",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/metadataonly1.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/tez/metadataonly1.q.out",
                "patch": "@@ -41,6 +41,26 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: test1\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Select Operator\n+                    expressions: ds (type: string)\n+                    outputColumnNames: _col0\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: max(_col0)\n+                      mode: hash\n+                      outputColumnNames: _col0\n+                      Statistics: Num rows: 1 Data size: 84 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        sort order: \n+                        Statistics: Num rows: 1 Data size: 84 Basic stats: COMPLETE Column stats: NONE\n+                        tag: -1\n+                        value expressions: _col0 (type: string)\n+                        auto parallelism: false\n         Reducer 2 \n             Needs Tagging: false\n             Reduce Operator Tree:",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/metadataonly1.q.out",
                "sha": "28503140784d331a82502dde2318a60229eca3a9",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/optimize_nullscan.q.out",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/optimize_nullscan.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/tez/optimize_nullscan.q.out",
                "patch": "@@ -104,6 +104,29 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: srcpart\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: false\n+                    predicate: false (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count(key)\n+                      keys: key (type: string)\n+                      mode: hash\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: _col0 (type: string)\n+                        sort order: +\n+                        Map-reduce partition columns: _col0 (type: string)\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                        tag: -1\n+                        value expressions: _col1 (type: bigint)\n+                        auto parallelism: true\n         Reducer 2 \n             Needs Tagging: false\n             Reduce Operator Tree:\n@@ -1656,6 +1679,22 @@ STAGE PLANS:\n             Truncated Path -> Alias:\n               -mr-10002default.src{} [src]\n         Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: srcpart\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  GatherStats: false\n+                  Filter Operator\n+                    isSamplingPred: false\n+                    predicate: false (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: key (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: key (type: string)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      tag: 1\n+                      auto parallelism: true\n         Reducer 4 \n             Needs Tagging: false\n             Reduce Operator Tree:",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/optimize_nullscan.q.out",
                "sha": "cca489e3deaa0d8e633be74a0cb19f3173dae0e7",
                "status": "modified"
            },
            {
                "additions": 65,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/tez_union.q.out",
                "changes": 65,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_union.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/tez/tez_union.q.out",
                "patch": "@@ -1253,3 +1253,68 @@ POSTHOOK: query: drop table ut\n POSTHOOK: type: DROPTABLE\n POSTHOOK: Input: default@ut\n POSTHOOK: Output: default@ut\n+PREHOOK: query: create table TABLE1(EMP_NAME STRING, EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@TABLE1\n+POSTHOOK: query: create table TABLE1(EMP_NAME STRING, EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@TABLE1\n+PREHOOK: query: create table table2 (EMP_NAME STRING) PARTITIONED BY (EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@table2\n+POSTHOOK: query: create table table2 (EMP_NAME STRING) PARTITIONED BY (EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@table2\n+PREHOOK: query: CREATE OR REPLACE VIEW TABLE3 as select EMP_NAME, EMP_ID from TABLE1\n+PREHOOK: type: CREATEVIEW\n+PREHOOK: Input: default@table1\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@TABLE3\n+POSTHOOK: query: CREATE OR REPLACE VIEW TABLE3 as select EMP_NAME, EMP_ID from TABLE1\n+POSTHOOK: type: CREATEVIEW\n+POSTHOOK: Input: default@table1\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@TABLE3\n+PREHOOK: query: explain formatted select count(*) from TABLE3\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain formatted select count(*) from TABLE3\n+POSTHOOK: type: QUERY\n+#### A masked pattern was here ####\n+PREHOOK: query: drop table table2\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@table2\n+PREHOOK: Output: default@table2\n+POSTHOOK: query: drop table table2\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@table2\n+POSTHOOK: Output: default@table2\n+PREHOOK: query: create table table2 (EMP_NAME STRING) PARTITIONED BY (EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@table2\n+POSTHOOK: query: create table table2 (EMP_NAME STRING) PARTITIONED BY (EMP_ID INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@table2\n+PREHOOK: query: CREATE OR REPLACE VIEW TABLE3 as select EMP_NAME, EMP_ID from TABLE1 UNION ALL select EMP_NAME,EMP_ID from TABLE2\n+PREHOOK: type: CREATEVIEW\n+PREHOOK: Input: default@table1\n+PREHOOK: Input: default@table2\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@TABLE3\n+POSTHOOK: query: CREATE OR REPLACE VIEW TABLE3 as select EMP_NAME, EMP_ID from TABLE1 UNION ALL select EMP_NAME,EMP_ID from TABLE2\n+POSTHOOK: type: CREATEVIEW\n+POSTHOOK: Input: default@table1\n+POSTHOOK: Input: default@table2\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@TABLE3\n+POSTHOOK: Output: default@table3\n+PREHOOK: query: explain formatted select count(*) from TABLE3\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain formatted select count(*) from TABLE3\n+POSTHOOK: type: QUERY\n+#### A masked pattern was here ####",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/tez_union.q.out",
                "sha": "6f6e8cab6e2e87b904965e28f8325daaccd7d4cc",
                "status": "modified"
            },
            {
                "additions": 67,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/tez_union_group_by.q.out",
                "changes": 67,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_union_group_by.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/tez/tez_union_group_by.q.out",
                "patch": "@@ -156,6 +156,24 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: x\n+                  Filter Operator\n+                    predicate: ((date < '2014-09-02') and (u <> 0)) (type: boolean)\n+                    Select Operator\n+                      expressions: u (type: bigint), date (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Group By Operator\n+                        aggregations: min(_col1)\n+                        keys: _col0 (type: bigint)\n+                        mode: hash\n+                        outputColumnNames: _col0, _col1\n+                        Reduce Output Operator\n+                          key expressions: _col0 (type: bigint)\n+                          sort order: +\n+                          Map-reduce partition columns: _col0 (type: bigint)\n+                          value expressions: _col1 (type: string)\n         Map 10 \n             Map Operator Tree:\n                 TableScan\n@@ -170,8 +188,57 @@ STAGE PLANS:\n                       Map-reduce partition columns: t (type: string), st (type: string)\n                       Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n         Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: y\n+                  Filter Operator\n+                    predicate: ((date < '2014-09-02') and (u <> 0)) (type: boolean)\n+                    Select Operator\n+                      expressions: u (type: bigint), date (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Group By Operator\n+                        aggregations: min(_col1)\n+                        keys: _col0 (type: bigint)\n+                        mode: hash\n+                        outputColumnNames: _col0, _col1\n+                        Reduce Output Operator\n+                          key expressions: _col0 (type: bigint)\n+                          sort order: +\n+                          Map-reduce partition columns: _col0 (type: bigint)\n+                          value expressions: _col1 (type: string)\n         Map 6 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: z\n+                  Filter Operator\n+                    predicate: ((date < '2014-09-02') and (u <> 0)) (type: boolean)\n+                    Select Operator\n+                      expressions: u (type: bigint), date (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Group By Operator\n+                        aggregations: min(_col1)\n+                        keys: _col0 (type: bigint)\n+                        mode: hash\n+                        outputColumnNames: _col0, _col1\n+                        Reduce Output Operator\n+                          key expressions: _col0 (type: bigint)\n+                          sort order: +\n+                          Map-reduce partition columns: _col0 (type: bigint)\n+                          value expressions: _col1 (type: string)\n         Map 7 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: x\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: (((t is not null and (date >= '2014-03-04')) and (date < '2014-09-03')) and (u <> 0)) (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: t (type: string), st (type: string)\n+                      sort order: ++\n+                      Map-reduce partition columns: t (type: string), st (type: string)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      value expressions: u (type: bigint)\n         Reducer 3 \n             Reduce Operator Tree:\n               Group By Operator",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/tez_union_group_by.q.out",
                "sha": "654b34bcb0d4152462ec1423fb4dffe2566ecf99",
                "status": "modified"
            },
            {
                "additions": 1004,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/union_view.q.out",
                "changes": 1004,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/union_view.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/tez/union_view.q.out",
                "patch": "@@ -0,0 +1,1004 @@\n+PREHOOK: query: CREATE TABLE src_union_1 (key int, value string) PARTITIONED BY (ds string)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@src_union_1\n+POSTHOOK: query: CREATE TABLE src_union_1 (key int, value string) PARTITIONED BY (ds string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@src_union_1\n+PREHOOK: query: CREATE INDEX src_union_1_key_idx ON TABLE src_union_1(key) AS 'COMPACT' WITH DEFERRED REBUILD\n+PREHOOK: type: CREATEINDEX\n+PREHOOK: Input: default@src_union_1\n+POSTHOOK: query: CREATE INDEX src_union_1_key_idx ON TABLE src_union_1(key) AS 'COMPACT' WITH DEFERRED REBUILD\n+POSTHOOK: type: CREATEINDEX\n+POSTHOOK: Input: default@src_union_1\n+POSTHOOK: Output: default@default__src_union_1_src_union_1_key_idx__\n+PREHOOK: query: CREATE TABLE src_union_2 (key int, value string) PARTITIONED BY (ds string, part_1 string)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@src_union_2\n+POSTHOOK: query: CREATE TABLE src_union_2 (key int, value string) PARTITIONED BY (ds string, part_1 string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@src_union_2\n+PREHOOK: query: CREATE INDEX src_union_2_key_idx ON TABLE src_union_2(key) AS 'COMPACT' WITH DEFERRED REBUILD\n+PREHOOK: type: CREATEINDEX\n+PREHOOK: Input: default@src_union_2\n+POSTHOOK: query: CREATE INDEX src_union_2_key_idx ON TABLE src_union_2(key) AS 'COMPACT' WITH DEFERRED REBUILD\n+POSTHOOK: type: CREATEINDEX\n+POSTHOOK: Input: default@src_union_2\n+POSTHOOK: Output: default@default__src_union_2_src_union_2_key_idx__\n+PREHOOK: query: CREATE TABLE src_union_3(key int, value string) PARTITIONED BY (ds string, part_1 string, part_2 string)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@src_union_3\n+POSTHOOK: query: CREATE TABLE src_union_3(key int, value string) PARTITIONED BY (ds string, part_1 string, part_2 string)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@src_union_3\n+PREHOOK: query: CREATE INDEX src_union_3_key_idx ON TABLE src_union_3(key) AS 'COMPACT' WITH DEFERRED REBUILD\n+PREHOOK: type: CREATEINDEX\n+PREHOOK: Input: default@src_union_3\n+POSTHOOK: query: CREATE INDEX src_union_3_key_idx ON TABLE src_union_3(key) AS 'COMPACT' WITH DEFERRED REBUILD\n+POSTHOOK: type: CREATEINDEX\n+POSTHOOK: Input: default@src_union_3\n+POSTHOOK: Output: default@default__src_union_3_src_union_3_key_idx__\n+STAGE DEPENDENCIES:\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        TableScan\n+          alias: src_union_1\n+          filterExpr: ((key = 86) and (ds = '1')) (type: boolean)\n+          Filter Operator\n+            predicate: (key = 86) (type: boolean)\n+            Select Operator\n+              expressions: 86 (type: int), value (type: string), '1' (type: string)\n+              outputColumnNames: _col0, _col1, _col2\n+              ListSink\n+\n+STAGE DEPENDENCIES:\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        TableScan\n+          alias: src_union_2\n+          filterExpr: ((key = 86) and (ds = '2')) (type: boolean)\n+          Filter Operator\n+            predicate: (key = 86) (type: boolean)\n+            Select Operator\n+              expressions: 86 (type: int), value (type: string), '2' (type: string)\n+              outputColumnNames: _col0, _col1, _col2\n+              ListSink\n+\n+STAGE DEPENDENCIES:\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        TableScan\n+          alias: src_union_3\n+          filterExpr: ((key = 86) and (ds = '3')) (type: boolean)\n+          Filter Operator\n+            predicate: (key = 86) (type: boolean)\n+            Select Operator\n+              expressions: 86 (type: int), value (type: string), '3' (type: string)\n+              outputColumnNames: _col0, _col1, _col2\n+              ListSink\n+\n+86\tval_86\t1\n+86\tval_86\t2\n+86\tval_86\t2\n+86\tval_86\t3\n+86\tval_86\t3\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: (ds = '1') (type: boolean)\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count(1)\n+                      mode: hash\n+                      outputColumnNames: _col0\n+                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        sort order: \n+                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                        value expressions: _col0 (type: bigint)\n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: (ds = '2') (type: boolean)\n+                  Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count(1)\n+                      mode: hash\n+                      outputColumnNames: _col0\n+                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        sort order: \n+                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                        value expressions: _col0 (type: bigint)\n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: (ds = '3') (type: boolean)\n+                  Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count(1)\n+                      mode: hash\n+                      outputColumnNames: _col0\n+                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        sort order: \n+                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                        value expressions: _col0 (type: bigint)\n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+500\n+1000\n+1000\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 3 <- Union 2 (CONTAINS)\n+        Map 4 <- Union 2 (CONTAINS)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: ((key = 86) and (ds = '1')) (type: boolean)\n+                  Filter Operator\n+                    predicate: (key = 86) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '1' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: ((key = 86) and (ds = '1')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '1')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '1' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: ((key = 86) and (ds = '1')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '1')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '1' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 3 <- Union 2 (CONTAINS)\n+        Map 4 <- Union 2 (CONTAINS)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: ((key = 86) and (ds = '2')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '2')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '2' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: ((key = 86) and (ds = '2')) (type: boolean)\n+                  Filter Operator\n+                    predicate: (key = 86) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '2' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: ((key = 86) and (ds = '2')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '2')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '2' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 3 <- Union 2 (CONTAINS)\n+        Map 4 <- Union 2 (CONTAINS)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: ((key = 86) and (ds = '3')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '3')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '3' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: ((key = 86) and (ds = '3')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '3')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '3' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: ((key = 86) and (ds = '3')) (type: boolean)\n+                  Filter Operator\n+                    predicate: (key = 86) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '3' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 4 <- Union 2 (CONTAINS)\n+        Map 5 <- Union 2 (CONTAINS)\n+        Reducer 3 <- Union 2 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: ((key = 86) and ds is not null) (type: boolean)\n+                  Filter Operator\n+                    predicate: (key = 86) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string), ds (type: string)\n+                      outputColumnNames: _col1, _col2\n+                      Select Operator\n+                        expressions: _col1 (type: string), _col2 (type: string)\n+                        outputColumnNames: _col1, _col2\n+                        Reduce Output Operator\n+                          key expressions: _col2 (type: string)\n+                          sort order: +\n+                          value expressions: _col1 (type: string)\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: ((key = 86) and ds is not null) (type: boolean)\n+                  Filter Operator\n+                    predicate: (key = 86) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string), ds (type: string)\n+                      outputColumnNames: _col1, _col2\n+                      Select Operator\n+                        expressions: _col1 (type: string), _col2 (type: string)\n+                        outputColumnNames: _col1, _col2\n+                        Reduce Output Operator\n+                          key expressions: _col2 (type: string)\n+                          sort order: +\n+                          value expressions: _col1 (type: string)\n+        Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: ((key = 86) and ds is not null) (type: boolean)\n+                  Filter Operator\n+                    predicate: (key = 86) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string), ds (type: string)\n+                      outputColumnNames: _col1, _col2\n+                      Select Operator\n+                        expressions: _col1 (type: string), _col2 (type: string)\n+                        outputColumnNames: _col1, _col2\n+                        Reduce Output Operator\n+                          key expressions: _col2 (type: string)\n+                          sort order: +\n+                          value expressions: _col1 (type: string)\n+        Reducer 3 \n+            Reduce Operator Tree:\n+              Select Operator\n+                expressions: 86 (type: int), VALUE._col1 (type: string), KEY.reducesinkkey0 (type: string)\n+                outputColumnNames: _col0, _col1, _col2\n+                Statistics: Num rows: 1250 Data size: 13280 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1250 Data size: 13280 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+86\tval_86\t1\n+86\tval_86\t2\n+86\tval_86\t2\n+86\tval_86\t3\n+86\tval_86\t3\n+86\tval_86\t1\n+86\tval_86\t2\n+86\tval_86\t2\n+86\tval_86\t3\n+86\tval_86\t3\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 4 <- Union 2 (CONTAINS)\n+        Map 5 <- Union 2 (CONTAINS)\n+        Reducer 3 <- Union 2 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: (ds = '1') (type: boolean)\n+                  Select Operator\n+                    Select Operator\n+                      Group By Operator\n+                        aggregations: count(1)\n+                        mode: hash\n+                        outputColumnNames: _col0\n+                        Reduce Output Operator\n+                          sort order: \n+                          value expressions: _col0 (type: bigint)\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: (ds = '1') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '1') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n+        Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: (ds = '1') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '1') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n+        Reducer 3 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 4 <- Union 2 (CONTAINS)\n+        Map 5 <- Union 2 (CONTAINS)\n+        Reducer 3 <- Union 2 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: (ds = '2') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '2') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: (ds = '2') (type: boolean)\n+                  Select Operator\n+                    Select Operator\n+                      Group By Operator\n+                        aggregations: count(1)\n+                        mode: hash\n+                        outputColumnNames: _col0\n+                        Reduce Output Operator\n+                          sort order: \n+                          value expressions: _col0 (type: bigint)\n+        Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: (ds = '2') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '2') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n+        Reducer 3 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 4 <- Union 2 (CONTAINS)\n+        Map 5 <- Union 2 (CONTAINS)\n+        Reducer 3 <- Union 2 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: (ds = '3') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '3') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: (ds = '3') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '3') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n+        Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: (ds = '3') (type: boolean)\n+                  Select Operator\n+                    Select Operator\n+                      Group By Operator\n+                        aggregations: count(1)\n+                        mode: hash\n+                        outputColumnNames: _col0\n+                        Reduce Output Operator\n+                          sort order: \n+                          value expressions: _col0 (type: bigint)\n+        Reducer 3 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+500\n+1000\n+1000\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 3 <- Union 2 (CONTAINS)\n+        Map 4 <- Union 2 (CONTAINS)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: ((key = 86) and (ds = '4')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '4')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '4' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: ((key = 86) and (ds = '4')) (type: boolean)\n+                  Filter Operator\n+                    predicate: ((key = 86) and (ds = '4')) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '4' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: ((key = 86) and (ds = '4')) (type: boolean)\n+                  Filter Operator\n+                    predicate: (key = 86) (type: boolean)\n+                    Select Operator\n+                      expressions: value (type: string)\n+                      outputColumnNames: _col1\n+                      Select Operator\n+                        expressions: 86 (type: int), _col1 (type: string), '4' (type: string)\n+                        outputColumnNames: _col0, _col1, _col2\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+86\tval_86\t4\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 4 <- Union 2 (CONTAINS)\n+        Map 5 <- Union 2 (CONTAINS)\n+        Reducer 3 <- Union 2 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_1\n+                  filterExpr: (ds = '4') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '4') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_2\n+                  filterExpr: (ds = '4') (type: boolean)\n+                  Filter Operator\n+                    predicate: (ds = '4') (type: boolean)\n+                    Select Operator\n+                      Select Operator\n+                        Group By Operator\n+                          aggregations: count(1)\n+                          mode: hash\n+                          outputColumnNames: _col0\n+                          Reduce Output Operator\n+                            sort order: \n+                            value expressions: _col0 (type: bigint)\n+        Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src_union_3\n+                  filterExpr: (ds = '4') (type: boolean)\n+                  Select Operator\n+                    Select Operator\n+                      Group By Operator\n+                        aggregations: count(1)\n+                        mode: hash\n+                        outputColumnNames: _col0\n+                        Reduce Output Operator\n+                          sort order: \n+                          value expressions: _col0 (type: bigint)\n+        Reducer 3 \n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: count(VALUE._col0)\n+                mode: mergepartial\n+                outputColumnNames: _col0\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+500",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/union_view.q.out",
                "sha": "ae6d7c8849269df0729a49f00dad1fc59ff3862c",
                "status": "added"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/vectorized_dynamic_partition_pruning.q.out",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/vectorized_dynamic_partition_pruning.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/tez/vectorized_dynamic_partition_pruning.q.out",
                "patch": "@@ -2620,6 +2620,19 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: srcpart\n+                  filterExpr: ((ds is not null and hr is not null) and (hr = 13)) (type: boolean)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: ((ds is not null and hr is not null) and (hr = 13)) (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: ds (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: ds (type: string)\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n         Map 5 \n             Map Operator Tree:\n                 TableScan\n@@ -4652,6 +4665,28 @@ STAGE PLANS:\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: srcpart\n+                  filterExpr: ((ds is not null and hr is not null) and (hr = 13)) (type: boolean)\n+                  Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                  Filter Operator\n+                    predicate: ((ds is not null and hr is not null) and (hr = 13)) (type: boolean)\n+                    Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                    Map Join Operator\n+                      condition map:\n+                           Inner Join 0 to 1\n+                      keys:\n+                        0 ds (type: string)\n+                        1 ds (type: string)\n+                      input vertices:\n+                        1 Map 2\n+                      Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                      Reduce Output Operator\n+                        key expressions: '13' (type: string)\n+                        sort order: +\n+                        Map-reduce partition columns: '13' (type: string)\n+                        Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n         Map 2 \n             Map Operator Tree:\n                 TableScan",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/tez/vectorized_dynamic_partition_pruning.q.out",
                "sha": "a8b25dba663e90979ffd5f25e57583b30a5f1854",
                "status": "modified"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/union30.q.out",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/union30.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 21,
                "filename": "ql/src/test/results/clientpositive/union30.q.out",
                "patch": "@@ -95,27 +95,6 @@ STAGE PLANS:\n   Stage: Stage-2\n     Map Reduce\n       Map Operator Tree:\n-          TableScan\n-            alias: src\n-            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n-            Select Operator\n-              expressions: key (type: string), value (type: string)\n-              outputColumnNames: _col0, _col1\n-              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n-              Union\n-                Statistics: Num rows: 1500 Data size: 15936 Basic stats: COMPLETE Column stats: NONE\n-                Select Operator\n-                  expressions: UDFToInteger(_col0) (type: int), _col1 (type: string)\n-                  outputColumnNames: _col0, _col1\n-                  Statistics: Num rows: 1500 Data size: 15936 Basic stats: COMPLETE Column stats: NONE\n-                  File Output Operator\n-                    compressed: false\n-                    Statistics: Num rows: 1500 Data size: 15936 Basic stats: COMPLETE Column stats: NONE\n-                    table:\n-                        input format: org.apache.hadoop.mapred.TextInputFormat\n-                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n-                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-                        name: default.union_subq_union\n           TableScan\n             alias: src\n             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n@@ -167,6 +146,27 @@ STAGE PLANS:\n                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n                       serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n                       name: default.union_subq_union\n+          TableScan\n+            alias: src\n+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+            Select Operator\n+              expressions: key (type: string), value (type: string)\n+              outputColumnNames: _col0, _col1\n+              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+              Union\n+                Statistics: Num rows: 1500 Data size: 15936 Basic stats: COMPLETE Column stats: NONE\n+                Select Operator\n+                  expressions: UDFToInteger(_col0) (type: int), _col1 (type: string)\n+                  outputColumnNames: _col0, _col1\n+                  Statistics: Num rows: 1500 Data size: 15936 Basic stats: COMPLETE Column stats: NONE\n+                  File Output Operator\n+                    compressed: false\n+                    Statistics: Num rows: 1500 Data size: 15936 Basic stats: COMPLETE Column stats: NONE\n+                    table:\n+                        input format: org.apache.hadoop.mapred.TextInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                        name: default.union_subq_union\n \n   Stage: Stage-8\n     Conditional Operator",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/union30.q.out",
                "sha": "4529074118aa114fbe50c78d4b65d489a99ec18c",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/union_lateralview.q.out",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/union_lateralview.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 8,
                "filename": "ql/src/test/results/clientpositive/union_lateralview.q.out",
                "patch": "@@ -101,14 +101,6 @@ STAGE PLANS:\n                             Map-reduce partition columns: _col1 (type: string)\n                             Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE\n                             value expressions: _col0 (type: int), _col2 (type: string)\n-          TableScan\n-            alias: b\n-            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n-            Reduce Output Operator\n-              key expressions: key (type: string)\n-              sort order: +\n-              Map-reduce partition columns: key (type: string)\n-              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n           TableScan\n             alias: srcpart\n             Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n@@ -157,6 +149,14 @@ STAGE PLANS:\n                             Map-reduce partition columns: _col1 (type: string)\n                             Statistics: Num rows: 2000 Data size: 21248 Basic stats: COMPLETE Column stats: NONE\n                             value expressions: _col0 (type: int), _col2 (type: string)\n+          TableScan\n+            alias: b\n+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+            Reduce Output Operator\n+              key expressions: key (type: string)\n+              sort order: +\n+              Map-reduce partition columns: key (type: string)\n+              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n       Reduce Operator Tree:\n         Join Operator\n           condition map:",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/union_lateralview.q.out",
                "sha": "734c1f42cca6d13d31e1ee28b036fece10ddd065",
                "status": "modified"
            },
            {
                "additions": 210,
                "blob_url": "https://github.com/apache/hive/blob/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/union_view.q.out",
                "changes": 210,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/union_view.q.out?ref=b249f00d5e44cc4c7d4d70db0d2a949c343c11c9",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/union_view.q.out",
                "patch": "@@ -503,6 +503,54 @@ STAGE PLANS:\n                           input format: org.apache.hadoop.mapred.TextInputFormat\n                           output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n                           serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+          TableScan\n+            alias: src_union_2\n+            filterExpr: ((key = 86) and (ds = '1')) (type: boolean)\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: ((key = 86) and (ds = '1')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: value (type: string)\n+                outputColumnNames: _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: 86 (type: int), _col1 (type: string), '1' (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                    File Output Operator\n+                      compressed: false\n+                      Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+          TableScan\n+            alias: src_union_3\n+            filterExpr: ((key = 86) and (ds = '1')) (type: boolean)\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: ((key = 86) and (ds = '1')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: value (type: string)\n+                outputColumnNames: _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: 86 (type: int), _col1 (type: string), '1' (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                    File Output Operator\n+                      compressed: false\n+                      Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n \n   Stage: Stage-0\n     Fetch Operator\n@@ -570,6 +618,30 @@ STAGE PLANS:\n                       sort order: +\n                       Statistics: Num rows: 1250 Data size: 13280 Basic stats: COMPLETE Column stats: NONE\n                       value expressions: _col1 (type: string)\n+          TableScan\n+            alias: src_union_1\n+            filterExpr: ((key = 86) and (ds = '2')) (type: boolean)\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: ((key = 86) and (ds = '2')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: value (type: string)\n+                outputColumnNames: _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: 86 (type: int), _col1 (type: string), '2' (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    File Output Operator\n+                      compressed: false\n+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n           TableScan\n             alias: src_union_2\n             filterExpr: ((key = 86) and ds is not null) (type: boolean)\n@@ -965,6 +1037,30 @@ STAGE PLANS:\n                           input format: org.apache.hadoop.mapred.TextInputFormat\n                           output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n                           serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+          TableScan\n+            alias: src_union_3\n+            filterExpr: ((key = 86) and (ds = '2')) (type: boolean)\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: ((key = 86) and (ds = '2')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: value (type: string)\n+                outputColumnNames: _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: 86 (type: int), _col1 (type: string), '2' (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    File Output Operator\n+                      compressed: false\n+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n \n   Stage: Stage-0\n     Fetch Operator\n@@ -1005,6 +1101,54 @@ STAGE PLANS:\n   Stage: Stage-1\n     Map Reduce\n       Map Operator Tree:\n+          TableScan\n+            alias: src_union_1\n+            filterExpr: ((key = 86) and (ds = '3')) (type: boolean)\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: ((key = 86) and (ds = '3')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: value (type: string)\n+                outputColumnNames: _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: 86 (type: int), _col1 (type: string), '3' (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    File Output Operator\n+                      compressed: false\n+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+          TableScan\n+            alias: src_union_2\n+            filterExpr: ((key = 86) and (ds = '3')) (type: boolean)\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: ((key = 86) and (ds = '3')) (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                expressions: value (type: string)\n+                outputColumnNames: _col1\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: 86 (type: int), _col1 (type: string), '3' (type: string)\n+                    outputColumnNames: _col0, _col1, _col2\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    File Output Operator\n+                      compressed: false\n+                      Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                      table:\n+                          input format: org.apache.hadoop.mapred.TextInputFormat\n+                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n           TableScan\n             alias: src_union_3\n             filterExpr: (ds = '4') (type: boolean)\n@@ -1024,6 +1168,72 @@ STAGE PLANS:\n                       sort order: \n                       Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n                       value expressions: _col0 (type: bigint)\n+          TableScan\n+            alias: src_union_2\n+            filterExpr: (ds = '1') (type: boolean)\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: (ds = '1') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count(1)\n+                      mode: hash\n+                      outputColumnNames: _col0\n+                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        sort order: \n+                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                        value expressions: _col0 (type: bigint)\n+          TableScan\n+            alias: src_union_3\n+            filterExpr: (ds = '1') (type: boolean)\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: (ds = '1') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count(1)\n+                      mode: hash\n+                      outputColumnNames: _col0\n+                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        sort order: \n+                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                        value expressions: _col0 (type: bigint)\n+          TableScan\n+            alias: src_union_3\n+            filterExpr: (ds = '2') (type: boolean)\n+            Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+            Filter Operator\n+              predicate: (ds = '2') (type: boolean)\n+              Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+              Select Operator\n+                Statistics: Num rows: 0 Data size: 0 Basic stats: NONE Column stats: NONE\n+                Union\n+                  Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    Statistics: Num rows: 1000 Data size: 10624 Basic stats: COMPLETE Column stats: NONE\n+                    Group By Operator\n+                      aggregations: count(1)\n+                      mode: hash\n+                      outputColumnNames: _col0\n+                      Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                      Reduce Output Operator\n+                        sort order: \n+                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: NONE\n+                        value expressions: _col0 (type: bigint)\n       Reduce Operator Tree:\n         Group By Operator\n           aggregations: count(VALUE._col0)",
                "raw_url": "https://github.com/apache/hive/raw/b249f00d5e44cc4c7d4d70db0d2a949c343c11c9/ql/src/test/results/clientpositive/union_view.q.out",
                "sha": "68854b3cad1a6cd711ccaa0aa862b325f9561cae",
                "status": "modified"
            }
        ],
        "message": "HIVE-10273: Union with partition tables which have no data fails with NPE (Vikram Dixit, reviewed by Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1673937 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/b7ed17fe74d8f2572b7b359d1723d5de6a5ac425",
        "patched_files": [
            "partition_boolexpr.java",
            "nullgroup5.java",
            "reduce_deduplicate.java",
            "ppd_union_view.java",
            "tez_union_group_by.java",
            "smb_mapjoin9.java",
            "groupby_sort_6.java",
            "annotate_stats_join.java",
            "join_view.java",
            "correlationoptimizer6.java",
            "union30.java",
            "bucketmapjoin1.java",
            "join_cond_pushdown_unqual4.java",
            "dynamic_partition_pruning.java",
            "union_lateralview.java",
            "join_cond_pushdown_unqual2.java",
            "MapWork.java",
            "optimize_nullscan.java",
            "input23.java",
            "vectorized_dynamic_partition_pruning.java",
            "input26.java",
            "Vectorizer.java",
            "correlationoptimizer3.java",
            "tez_union.java",
            "join_nonexistent_part.java",
            "union_view.java",
            "metadataonly1.java",
            "auto_join32.java",
            "sample6.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "testconfiguration.java",
            "TestVectorizer.java"
        ]
    },
    "hive_b2efd1a": {
        "bug_id": "hive_b2efd1a",
        "commit": "https://github.com/apache/hive/commit/b2efd1a695ddaf073bbe0bf311075e542e1b865b",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/b2efd1a695ddaf073bbe0bf311075e542e1b865b/itests/src/test/resources/testconfiguration.properties",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=b2efd1a695ddaf073bbe0bf311075e542e1b865b",
                "deletions": 0,
                "filename": "itests/src/test/resources/testconfiguration.properties",
                "patch": "@@ -383,6 +383,7 @@ minitez.query.files=bucket_map_join_tez1.q,\\\n   hybridgrace_hashjoin_1.q,\\\n   hybridgrace_hashjoin_2.q,\\\n   mapjoin_decimal.q,\\\n+  mergejoin_3way.q,\\\n   lvj_mapjoin.q,\\\n   llapdecider.q,\\\n   mrr.q,\\",
                "raw_url": "https://github.com/apache/hive/raw/b2efd1a695ddaf073bbe0bf311075e542e1b865b/itests/src/test/resources/testconfiguration.properties",
                "sha": "ed84c0f76602e35f57565b5ac233e0f704538111",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/b2efd1a695ddaf073bbe0bf311075e542e1b865b/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonMergeJoinOperator.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonMergeJoinOperator.java?ref=b2efd1a695ddaf073bbe0bf311075e542e1b865b",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/CommonMergeJoinOperator.java",
                "patch": "@@ -462,7 +462,7 @@ private void joinFinalLeftData() throws HiveException {\n     while (dataInCache) {\n       for (byte pos = 0; pos < order.length; pos++) {\n         if (this.foundNextKeyGroup[pos] && this.nextKeyWritables[pos] != null) {\n-          promoteNextGroupToCandidate(pos);\n+          fetchNextGroup(pos);\n         }\n       }\n       joinOneGroup();",
                "raw_url": "https://github.com/apache/hive/raw/b2efd1a695ddaf073bbe0bf311075e542e1b865b/ql/src/java/org/apache/hadoop/hive/ql/exec/CommonMergeJoinOperator.java",
                "sha": "1cbd13d543a5928a2ac60b16a87a76885c8beb3e",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hive/blob/b2efd1a695ddaf073bbe0bf311075e542e1b865b/ql/src/test/queries/clientpositive/mergejoin_3way.q",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/mergejoin_3way.q?ref=b2efd1a695ddaf073bbe0bf311075e542e1b865b",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/mergejoin_3way.q",
                "patch": "@@ -0,0 +1,15 @@\n+set hive.auto.convert.join=false;\n+set hive.cbo.enable=false;\n+\n+select\n+  a.key, b.value, c.value\n+from\n+  src a,\n+  src1 b,\n+  src1 c\n+where\n+  a.key = b.key and a.key = c.key\n+  and b.key != '' and b.value != ''\n+  and a.value > 'wal_6789'\n+  and c.value > 'wal_6789'\n+;",
                "raw_url": "https://github.com/apache/hive/raw/b2efd1a695ddaf073bbe0bf311075e542e1b865b/ql/src/test/queries/clientpositive/mergejoin_3way.q",
                "sha": "4c50f1de86f2e72ae7655e60bd19930bb451dab1",
                "status": "added"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hive/blob/b2efd1a695ddaf073bbe0bf311075e542e1b865b/ql/src/test/results/clientpositive/tez/mergejoin_3way.q.out",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/mergejoin_3way.q.out?ref=b2efd1a695ddaf073bbe0bf311075e542e1b865b",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/tez/mergejoin_3way.q.out",
                "patch": "@@ -0,0 +1,30 @@\n+PREHOOK: query: select\n+  a.key, b.value, c.value\n+from\n+  src a,\n+  src1 b,\n+  src1 c\n+where\n+  a.key = b.key and a.key = c.key\n+  and b.key != '' and b.value != ''\n+  and a.value > 'wal_6789'\n+  and c.value > 'wal_6789'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Input: default@src1\n+#### A masked pattern was here ####\n+POSTHOOK: query: select\n+  a.key, b.value, c.value\n+from\n+  src a,\n+  src1 b,\n+  src1 c\n+where\n+  a.key = b.key and a.key = c.key\n+  and b.key != '' and b.value != ''\n+  and a.value > 'wal_6789'\n+  and c.value > 'wal_6789'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Input: default@src1\n+#### A masked pattern was here ####",
                "raw_url": "https://github.com/apache/hive/raw/b2efd1a695ddaf073bbe0bf311075e542e1b865b/ql/src/test/results/clientpositive/tez/mergejoin_3way.q.out",
                "sha": "e644051ec93fa4450df48202460908a1b3c425bb",
                "status": "added"
            }
        ],
        "message": "HIVE-12563: NullPointerException with 3-way Tez merge join (Jason Dere, reviewed by Gunther Hagleitner)",
        "parent": "https://github.com/apache/hive/commit/3c8b9c27b18758b2b982ce3a65214bfb0e27314a",
        "patched_files": [
            "CommonMergeJoinOperator.java",
            "mergejoin_3way.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "testconfiguration.java"
        ]
    },
    "hive_b61e6b5": {
        "bug_id": "hive_b61e6b5",
        "commit": "https://github.com/apache/hive/commit/b61e6b52b54c9f8914aa6e4e042ff2921ce6a947",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/b61e6b52b54c9f8914aa6e4e042ff2921ce6a947/itests/src/test/resources/testconfiguration.properties",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=b61e6b52b54c9f8914aa6e4e042ff2921ce6a947",
                "deletions": 0,
                "filename": "itests/src/test/resources/testconfiguration.properties",
                "patch": "@@ -139,6 +139,7 @@ minitez.query.files.shared=alter_merge_2_orc.q,\\\n   orc_merge6.q,\\\n   orc_merge7.q,\\\n   orc_merge8.q,\\\n+  orc_merge9.q,\\\n   orc_merge_incompat1.q,\\\n   orc_merge_incompat2.q,\\\n   orc_vectorization_ppd.q,\\",
                "raw_url": "https://github.com/apache/hive/raw/b61e6b52b54c9f8914aa6e4e042ff2921ce6a947/itests/src/test/resources/testconfiguration.properties",
                "sha": "97715fc4e0548d661e61397ca95bf990204acf73",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/hive/blob/b61e6b52b54c9f8914aa6e4e042ff2921ce6a947/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MergeFileRecordProcessor.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MergeFileRecordProcessor.java?ref=b61e6b52b54c9f8914aa6e4e042ff2921ce6a947",
                "deletions": 13,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MergeFileRecordProcessor.java",
                "patch": "@@ -17,9 +17,8 @@\n  */\n package org.apache.hadoop.hive.ql.exec.tez;\n \n-import java.io.IOException;\n+import java.util.List;\n import java.util.Map;\n-import java.util.Map.Entry;\n import java.util.concurrent.Callable;\n \n import org.apache.commons.logging.Log;\n@@ -41,11 +40,14 @@\n import org.apache.hadoop.util.StringUtils;\n import org.apache.tez.mapreduce.input.MRInputLegacy;\n import org.apache.tez.mapreduce.processor.MRTaskReporter;\n+import org.apache.tez.runtime.api.Input;\n import org.apache.tez.runtime.api.LogicalInput;\n import org.apache.tez.runtime.api.LogicalOutput;\n import org.apache.tez.runtime.api.ProcessorContext;\n import org.apache.tez.runtime.library.api.KeyValueReader;\n \n+import com.google.common.collect.Lists;\n+\n /**\n  * Record processor for fast merging of files.\n  */\n@@ -219,22 +221,36 @@ private boolean processRow(Object key, Object value) {\n   }\n \n   private MRInputLegacy getMRInput(Map<String, LogicalInput> inputs) throws Exception {\n-    // there should be only one MRInput\n-    MRInputLegacy theMRInput = null;\n-    LOG.info(\"VDK: the inputs are: \" + inputs);\n-    for (Entry<String, LogicalInput> inp : inputs.entrySet()) {\n-      if (inp.getValue() instanceof MRInputLegacy) {\n-        if (theMRInput != null) {\n+    LOG.info(\"The inputs are: \" + inputs);\n+\n+    // start the mr input and wait for ready event. number of MRInput is expected to be 1\n+    List<Input> li = Lists.newArrayList();\n+    int numMRInputs = 0;\n+    for (LogicalInput inp : inputs.values()) {\n+      if (inp instanceof MRInputLegacy) {\n+        numMRInputs++;\n+        if (numMRInputs > 1) {\n           throw new IllegalArgumentException(\"Only one MRInput is expected\");\n         }\n-        // a better logic would be to find the alias\n-        theMRInput = (MRInputLegacy) inp.getValue();\n+        inp.start();\n+        li.add(inp);\n       } else {\n-        throw new IOException(\"Expecting only one input of type MRInputLegacy. Found type: \"\n-            + inp.getClass().getCanonicalName());\n+        throw new IllegalArgumentException(\"Expecting only one input of type MRInputLegacy.\" +\n+            \" Found type: \" + inp.getClass().getCanonicalName());\n       }\n     }\n-    theMRInput.init();\n+\n+    // typically alter table .. concatenate is run on only one partition/one table,\n+    // so it doesn't matter if we wait for all inputs or any input to be ready.\n+    processorContext.waitForAnyInputReady(li);\n+\n+    final MRInputLegacy theMRInput;\n+    if (li.size() == 1) {\n+      theMRInput = (MRInputLegacy) li.get(0);\n+      theMRInput.init();\n+    } else {\n+      throw new IllegalArgumentException(\"MRInputs count is expected to be 1\");\n+    }\n \n     return theMRInput;\n   }",
                "raw_url": "https://github.com/apache/hive/raw/b61e6b52b54c9f8914aa6e4e042ff2921ce6a947/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MergeFileRecordProcessor.java",
                "sha": "fce152306ad2f4df85e1b29083773f15911decc9",
                "status": "modified"
            },
            {
                "additions": 186,
                "blob_url": "https://github.com/apache/hive/blob/b61e6b52b54c9f8914aa6e4e042ff2921ce6a947/ql/src/test/results/clientpositive/tez/orc_merge9.q.out",
                "changes": 186,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/orc_merge9.q.out?ref=b61e6b52b54c9f8914aa6e4e042ff2921ce6a947",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/tez/orc_merge9.q.out",
                "patch": "@@ -0,0 +1,186 @@\n+PREHOOK: query: create table ts_merge (\n+userid bigint,\n+string1 string,\n+subtype double,\n+decimal1 decimal(38,18),\n+ts timestamp\n+) stored as orc\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@ts_merge\n+POSTHOOK: query: create table ts_merge (\n+userid bigint,\n+string1 string,\n+subtype double,\n+decimal1 decimal(38,18),\n+ts timestamp\n+) stored as orc\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@ts_merge\n+PREHOOK: query: load data local inpath '../../data/files/orc_split_elim.orc' overwrite into table ts_merge\n+PREHOOK: type: LOAD\n+#### A masked pattern was here ####\n+PREHOOK: Output: default@ts_merge\n+POSTHOOK: query: load data local inpath '../../data/files/orc_split_elim.orc' overwrite into table ts_merge\n+POSTHOOK: type: LOAD\n+#### A masked pattern was here ####\n+POSTHOOK: Output: default@ts_merge\n+PREHOOK: query: load data local inpath '../../data/files/orc_split_elim.orc' into table ts_merge\n+PREHOOK: type: LOAD\n+#### A masked pattern was here ####\n+PREHOOK: Output: default@ts_merge\n+POSTHOOK: query: load data local inpath '../../data/files/orc_split_elim.orc' into table ts_merge\n+POSTHOOK: type: LOAD\n+#### A masked pattern was here ####\n+POSTHOOK: Output: default@ts_merge\n+Found 2 items\n+#### A masked pattern was here ####\n+PREHOOK: query: select count(*) from ts_merge\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@ts_merge\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from ts_merge\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@ts_merge\n+#### A masked pattern was here ####\n+50000\n+PREHOOK: query: alter table ts_merge concatenate\n+PREHOOK: type: ALTER_TABLE_MERGE\n+PREHOOK: Input: default@ts_merge\n+PREHOOK: Output: default@ts_merge\n+POSTHOOK: query: alter table ts_merge concatenate\n+POSTHOOK: type: ALTER_TABLE_MERGE\n+POSTHOOK: Input: default@ts_merge\n+POSTHOOK: Output: default@ts_merge\n+PREHOOK: query: select count(*) from ts_merge\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@ts_merge\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from ts_merge\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@ts_merge\n+#### A masked pattern was here ####\n+50000\n+Found 1 items\n+#### A masked pattern was here ####\n+PREHOOK: query: -- incompatible merge test (stripe statistics missing)\n+\n+create table a_merge like alltypesorc\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@a_merge\n+POSTHOOK: query: -- incompatible merge test (stripe statistics missing)\n+\n+create table a_merge like alltypesorc\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@a_merge\n+PREHOOK: query: insert overwrite table a_merge select * from alltypesorc\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@alltypesorc\n+PREHOOK: Output: default@a_merge\n+POSTHOOK: query: insert overwrite table a_merge select * from alltypesorc\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@alltypesorc\n+POSTHOOK: Output: default@a_merge\n+POSTHOOK: Lineage: a_merge.cbigint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cboolean1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cboolean1, type:boolean, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cboolean2 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cboolean2, type:boolean, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cfloat SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cfloat, type:float, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: a_merge.csmallint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:csmallint, type:smallint, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cstring1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cstring2 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring2, type:string, comment:null), ]\n+POSTHOOK: Lineage: a_merge.ctimestamp1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp1, type:timestamp, comment:null), ]\n+POSTHOOK: Lineage: a_merge.ctimestamp2 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp2, type:timestamp, comment:null), ]\n+POSTHOOK: Lineage: a_merge.ctinyint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+PREHOOK: query: load data local inpath '../../data/files/alltypesorc' into table a_merge\n+PREHOOK: type: LOAD\n+#### A masked pattern was here ####\n+PREHOOK: Output: default@a_merge\n+POSTHOOK: query: load data local inpath '../../data/files/alltypesorc' into table a_merge\n+POSTHOOK: type: LOAD\n+#### A masked pattern was here ####\n+POSTHOOK: Output: default@a_merge\n+Found 2 items\n+#### A masked pattern was here ####\n+PREHOOK: query: select count(*) from a_merge\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@a_merge\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from a_merge\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@a_merge\n+#### A masked pattern was here ####\n+24576\n+PREHOOK: query: alter table a_merge concatenate\n+PREHOOK: type: ALTER_TABLE_MERGE\n+PREHOOK: Input: default@a_merge\n+PREHOOK: Output: default@a_merge\n+POSTHOOK: query: alter table a_merge concatenate\n+POSTHOOK: type: ALTER_TABLE_MERGE\n+POSTHOOK: Input: default@a_merge\n+POSTHOOK: Output: default@a_merge\n+PREHOOK: query: select count(*) from a_merge\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@a_merge\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from a_merge\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@a_merge\n+#### A masked pattern was here ####\n+24576\n+Found 2 items\n+#### A masked pattern was here ####\n+PREHOOK: query: insert into table a_merge select * from alltypesorc\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@alltypesorc\n+PREHOOK: Output: default@a_merge\n+POSTHOOK: query: insert into table a_merge select * from alltypesorc\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@alltypesorc\n+POSTHOOK: Output: default@a_merge\n+POSTHOOK: Lineage: a_merge.cbigint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cbigint, type:bigint, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cboolean1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cboolean1, type:boolean, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cboolean2 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cboolean2, type:boolean, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cfloat SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cfloat, type:float, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: a_merge.csmallint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:csmallint, type:smallint, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cstring1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+POSTHOOK: Lineage: a_merge.cstring2 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring2, type:string, comment:null), ]\n+POSTHOOK: Lineage: a_merge.ctimestamp1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp1, type:timestamp, comment:null), ]\n+POSTHOOK: Lineage: a_merge.ctimestamp2 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp2, type:timestamp, comment:null), ]\n+POSTHOOK: Lineage: a_merge.ctinyint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctinyint, type:tinyint, comment:null), ]\n+Found 3 items\n+#### A masked pattern was here ####\n+PREHOOK: query: select count(*) from a_merge\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@a_merge\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from a_merge\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@a_merge\n+#### A masked pattern was here ####\n+36864\n+PREHOOK: query: alter table a_merge concatenate\n+PREHOOK: type: ALTER_TABLE_MERGE\n+PREHOOK: Input: default@a_merge\n+PREHOOK: Output: default@a_merge\n+POSTHOOK: query: alter table a_merge concatenate\n+POSTHOOK: type: ALTER_TABLE_MERGE\n+POSTHOOK: Input: default@a_merge\n+POSTHOOK: Output: default@a_merge\n+PREHOOK: query: select count(*) from a_merge\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@a_merge\n+#### A masked pattern was here ####\n+POSTHOOK: query: select count(*) from a_merge\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@a_merge\n+#### A masked pattern was here ####\n+36864\n+Found 2 items\n+#### A masked pattern was here ####",
                "raw_url": "https://github.com/apache/hive/raw/b61e6b52b54c9f8914aa6e4e042ff2921ce6a947/ql/src/test/results/clientpositive/tez/orc_merge9.q.out",
                "sha": "bdf0fd30332f7790f18ad5b242154641d14f0c4e",
                "status": "added"
            }
        ],
        "message": "HIVE-11221: In Tez mode, alter table concatenate orc files can intermittently fail with NPE (Prasanth Jayachandran reviewed by Vikram Dixit)",
        "parent": "https://github.com/apache/hive/commit/d89a7d1e7fe7fb51aeb514e4357ae149158b2a34",
        "patched_files": [
            "MergeFileRecordProcessor.java",
            "orc_merge9.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "testconfiguration.java"
        ]
    },
    "hive_b650b79": {
        "bug_id": "hive_b650b79",
        "commit": "https://github.com/apache/hive/commit/b650b798ffb589800e19320595c54f5793cd1e40",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/b650b798ffb589800e19320595c54f5793cd1e40/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java?ref=b650b798ffb589800e19320595c54f5793cd1e40",
                "deletions": 3,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java",
                "patch": "@@ -254,9 +254,13 @@ static void dedupLockObjects(List<HiveLockObj> lockObjects) {\n \n   private HiveLockMode getWriteEntityLockMode (WriteEntity we) {\n     HiveLockMode lockMode = we.isComplete() ? HiveLockMode.EXCLUSIVE : HiveLockMode.SHARED;\n-    //but the writeEntity is complete in DDL operations, and we need check its writeType to\n-    //to determine the lockMode\n-    switch (we.getWriteType()) {\n+    //but the writeEntity is complete in DDL operations, instead DDL sets the writeType, so\n+    //we use it to determine its lockMode, and first we check if the writeType was set\n+    WriteEntity.WriteType writeType = we.getWriteType();\n+    if (writeType == null) {\n+      return lockMode;\n+    }\n+    switch (writeType) {\n       case DDL_EXCLUSIVE:\n         return HiveLockMode.EXCLUSIVE;\n       case DDL_SHARED:",
                "raw_url": "https://github.com/apache/hive/raw/b650b798ffb589800e19320595c54f5793cd1e40/ql/src/java/org/apache/hadoop/hive/ql/lockmgr/DummyTxnManager.java",
                "sha": "8fdac5e29e723695a00e99751303ed54e6a9e5e5",
                "status": "modified"
            }
        ],
        "message": "HIVE-9330 : DummyTxnManager will throw NPE if WriteEntity writeType has not been set (Chaoyu Tang via Szehon)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1652897 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/ec3c2f86bc2399cab3127463846efcd44d7efea2",
        "patched_files": [
            "DummyTxnManager.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestDummyTxnManager.java"
        ]
    },
    "hive_bd21f89": {
        "bug_id": "hive_bd21f89",
        "commit": "https://github.com/apache/hive/commit/bd21f890e21f3baa8b715cb4203405de65b2a30b",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/bd21f890e21f3baa8b715cb4203405de65b2a30b/itests/src/test/resources/testconfiguration.properties",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=bd21f890e21f3baa8b715cb4203405de65b2a30b",
                "deletions": 0,
                "filename": "itests/src/test/resources/testconfiguration.properties",
                "patch": "@@ -555,6 +555,7 @@ minillaplocal.query.files=\\\n   materialized_view_describe.q,\\\n   materialized_view_drop.q,\\\n   materialized_view_rebuild.q,\\\n+  materialized_view_rewrite_empty.q,\\\n   materialized_view_rewrite_1.q,\\\n   materialized_view_rewrite_2.q,\\\n   materialized_view_rewrite_3.q,\\",
                "raw_url": "https://github.com/apache/hive/raw/bd21f890e21f3baa8b715cb4203405de65b2a30b/itests/src/test/resources/testconfiguration.properties",
                "sha": "3ed2cf398a8edde959ccec75d51878724518a72b",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hive/blob/bd21f890e21f3baa8b715cb4203405de65b2a30b/ql/src/test/queries/clientpositive/materialized_view_rewrite_empty.q",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/materialized_view_rewrite_empty.q?ref=bd21f890e21f3baa8b715cb4203405de65b2a30b",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/materialized_view_rewrite_empty.q",
                "patch": "@@ -0,0 +1,28 @@\n+-- SORT_QUERY_RESULTS\n+\n+set hive.vectorized.execution.enabled=false;\n+set hive.support.concurrency=true;\n+set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;\n+set hive.strict.checks.cartesian.product=false;\n+set hive.stats.fetch.column.stats=true;\n+set hive.materializedview.rewriting=true;\n+\n+create table emps_mv_rewrite_empty (\n+  empid int,\n+  deptno int,\n+  name varchar(256),\n+  salary float,\n+  commission int)\n+stored as orc TBLPROPERTIES ('transactional'='true');\n+analyze table emps_mv_rewrite_empty compute statistics for columns;\n+\n+create materialized view emps_mv_rewrite_empty_mv1 as\n+select * from emps_mv_rewrite_empty where empid < 150;\n+\n+explain\n+select * from emps_mv_rewrite_empty where empid < 120;\n+\n+select * from emps_mv_rewrite_empty where empid < 120;\n+\n+drop materialized view emps_mv_rewrite_empty_mv1;\n+drop table emps_mv_rewrite_empty;",
                "raw_url": "https://github.com/apache/hive/raw/bd21f890e21f3baa8b715cb4203405de65b2a30b/ql/src/test/queries/clientpositive/materialized_view_rewrite_empty.q",
                "sha": "e5daa8dc7820752b4d55cc92f5a6b56bfd6706a4",
                "status": "added"
            },
            {
                "additions": 89,
                "blob_url": "https://github.com/apache/hive/blob/bd21f890e21f3baa8b715cb4203405de65b2a30b/ql/src/test/results/clientpositive/llap/materialized_view_rewrite_empty.q.out",
                "changes": 89,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/llap/materialized_view_rewrite_empty.q.out?ref=bd21f890e21f3baa8b715cb4203405de65b2a30b",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/llap/materialized_view_rewrite_empty.q.out",
                "patch": "@@ -0,0 +1,89 @@\n+PREHOOK: query: create table emps_mv_rewrite_empty (\n+  empid int,\n+  deptno int,\n+  name varchar(256),\n+  salary float,\n+  commission int)\n+stored as orc TBLPROPERTIES ('transactional'='true')\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@emps_mv_rewrite_empty\n+POSTHOOK: query: create table emps_mv_rewrite_empty (\n+  empid int,\n+  deptno int,\n+  name varchar(256),\n+  salary float,\n+  commission int)\n+stored as orc TBLPROPERTIES ('transactional'='true')\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@emps_mv_rewrite_empty\n+PREHOOK: query: analyze table emps_mv_rewrite_empty compute statistics for columns\n+PREHOOK: type: ANALYZE_TABLE\n+PREHOOK: Input: default@emps_mv_rewrite_empty\n+PREHOOK: Output: default@emps_mv_rewrite_empty\n+#### A masked pattern was here ####\n+POSTHOOK: query: analyze table emps_mv_rewrite_empty compute statistics for columns\n+POSTHOOK: type: ANALYZE_TABLE\n+POSTHOOK: Input: default@emps_mv_rewrite_empty\n+POSTHOOK: Output: default@emps_mv_rewrite_empty\n+#### A masked pattern was here ####\n+PREHOOK: query: create materialized view emps_mv_rewrite_empty_mv1 as\n+select * from emps_mv_rewrite_empty where empid < 150\n+PREHOOK: type: CREATE_MATERIALIZED_VIEW\n+PREHOOK: Input: default@emps_mv_rewrite_empty\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@emps_mv_rewrite_empty_mv1\n+POSTHOOK: query: create materialized view emps_mv_rewrite_empty_mv1 as\n+select * from emps_mv_rewrite_empty where empid < 150\n+POSTHOOK: type: CREATE_MATERIALIZED_VIEW\n+POSTHOOK: Input: default@emps_mv_rewrite_empty\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@emps_mv_rewrite_empty_mv1\n+PREHOOK: query: explain\n+select * from emps_mv_rewrite_empty where empid < 120\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select * from emps_mv_rewrite_empty where empid < 120\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        TableScan\n+          alias: emps_mv_rewrite_empty\n+          Filter Operator\n+            predicate: (empid < 120) (type: boolean)\n+            Select Operator\n+              expressions: empid (type: int), deptno (type: int), name (type: varchar(256)), salary (type: float), commission (type: int)\n+              outputColumnNames: _col0, _col1, _col2, _col3, _col4\n+              ListSink\n+\n+PREHOOK: query: select * from emps_mv_rewrite_empty where empid < 120\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@emps_mv_rewrite_empty\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from emps_mv_rewrite_empty where empid < 120\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@emps_mv_rewrite_empty\n+#### A masked pattern was here ####\n+PREHOOK: query: drop materialized view emps_mv_rewrite_empty_mv1\n+PREHOOK: type: DROP_MATERIALIZED_VIEW\n+PREHOOK: Input: default@emps_mv_rewrite_empty_mv1\n+PREHOOK: Output: default@emps_mv_rewrite_empty_mv1\n+POSTHOOK: query: drop materialized view emps_mv_rewrite_empty_mv1\n+POSTHOOK: type: DROP_MATERIALIZED_VIEW\n+POSTHOOK: Input: default@emps_mv_rewrite_empty_mv1\n+POSTHOOK: Output: default@emps_mv_rewrite_empty_mv1\n+PREHOOK: query: drop table emps_mv_rewrite_empty\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@emps_mv_rewrite_empty\n+PREHOOK: Output: default@emps_mv_rewrite_empty\n+POSTHOOK: query: drop table emps_mv_rewrite_empty\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@emps_mv_rewrite_empty\n+POSTHOOK: Output: default@emps_mv_rewrite_empty",
                "raw_url": "https://github.com/apache/hive/raw/bd21f890e21f3baa8b715cb4203405de65b2a30b/ql/src/test/results/clientpositive/llap/materialized_view_rewrite_empty.q.out",
                "sha": "b33d8c3f2d3b0e9eae01b7b878b564350b1a5961",
                "status": "added"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hive/blob/bd21f890e21f3baa8b715cb4203405de65b2a30b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/MaterializationsInvalidationCache.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/MaterializationsInvalidationCache.java?ref=bd21f890e21f3baa8b715cb4203405de65b2a30b",
                "deletions": 0,
                "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/MaterializationsInvalidationCache.java",
                "patch": "@@ -360,6 +360,15 @@ private void enrichWithInvalidationInfo(Materialization materialization) {\n \n       final ConcurrentSkipListMap<Long, Long> usedTableModifications =\n           tableModifications.get(qNameTableUsed);\n+      if (usedTableModifications == null) {\n+        // This is not necessarily an error, since the table may be empty. To be safe,\n+        // instead of including this materialized view, we just log the information and\n+        // skip it (if table is really empty, it will not matter for performance anyway).\n+        LOG.warn(\"No information found in invalidation cache for table {}, possible tables are: {}\",\n+            qNameTableUsed, tableModifications.keySet());\n+        materialization.setInvalidationTime(Long.MIN_VALUE);\n+        return;\n+      }\n       final ConcurrentSkipListSet<Long> usedUDTableModifications =\n           updateDeleteTableModifications.get(qNameTableUsed);\n       final Entry<Long, Long> tn = usedTableModifications.higherEntry(tableMaterializationTxnList.getHighWatermark());",
                "raw_url": "https://github.com/apache/hive/raw/bd21f890e21f3baa8b715cb4203405de65b2a30b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/MaterializationsInvalidationCache.java",
                "sha": "fc644f0b637616a670a3585dc4978fb917828747",
                "status": "modified"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/hive/blob/bd21f890e21f3baa8b715cb4203405de65b2a30b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java?ref=bd21f890e21f3baa8b715cb4203405de65b2a30b",
                "deletions": 19,
                "filename": "standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java",
                "patch": "@@ -838,11 +838,7 @@ public void commitTxn(CommitTxnRequest rqst)\n     throws NoSuchTxnException, TxnAbortedException, MetaException {\n     MaterializationsRebuildLockHandler materializationsRebuildLockHandler =\n         MaterializationsRebuildLockHandler.get();\n-    String fullyQualifiedName = null;\n-    String dbName = null;\n-    String tblName = null;\n-    long writeId = 0L;\n-    long timestamp = 0L;\n+    List<TransactionRegistryInfo> txnComponents = new ArrayList<>();\n     boolean isUpdateDelete = false;\n     long txnid = rqst.getTxnid();\n     long sourceTxnId = -1;\n@@ -1007,12 +1003,10 @@ public void commitTxn(CommitTxnRequest rqst)\n         s = \"select ctc_database, ctc_table, ctc_writeid, ctc_timestamp from COMPLETED_TXN_COMPONENTS where ctc_txnid = \" + txnid;\n         LOG.debug(\"Going to extract table modification information for invalidation cache <\" + s + \">\");\n         rs = stmt.executeQuery(s);\n-        if (rs.next()) {\n-          dbName = rs.getString(1);\n-          tblName = rs.getString(2);\n-          fullyQualifiedName = Warehouse.getQualifiedName(dbName, tblName);\n-          writeId = rs.getLong(3);\n-          timestamp = rs.getTimestamp(4, Calendar.getInstance(TimeZone.getTimeZone(\"UTC\"))).getTime();\n+        while (rs.next()) {\n+          // We only enter in this loop if the transaction actually affected any table\n+          txnComponents.add(new TransactionRegistryInfo(rs.getString(1), rs.getString(2),\n+              rs.getLong(3), rs.getTimestamp(4, Calendar.getInstance(TimeZone.getTimeZone(\"UTC\"))).getTime()));\n         }\n         s = \"delete from TXN_COMPONENTS where tc_txnid = \" + txnid;\n         LOG.debug(\"Going to execute update <\" + s + \">\");\n@@ -1042,18 +1036,22 @@ public void commitTxn(CommitTxnRequest rqst)\n \n         MaterializationsInvalidationCache materializationsInvalidationCache =\n             MaterializationsInvalidationCache.get();\n-        if (materializationsInvalidationCache.containsMaterialization(dbName, tblName) &&\n-            !materializationsRebuildLockHandler.readyToCommitResource(dbName, tblName, txnid)) {\n-          throw new MetaException(\n-              \"Another process is rebuilding the materialized view \" + fullyQualifiedName);\n+        for (TransactionRegistryInfo info : txnComponents) {\n+          if (materializationsInvalidationCache.containsMaterialization(info.dbName, info.tblName) &&\n+              !materializationsRebuildLockHandler.readyToCommitResource(info.dbName, info.tblName, txnid)) {\n+            throw new MetaException(\n+                \"Another process is rebuilding the materialized view \" + info.fullyQualifiedName);\n+          }\n         }\n         LOG.debug(\"Going to commit\");\n         close(rs);\n         dbConn.commit();\n \n         // Update registry with modifications\n-        materializationsInvalidationCache.notifyTableModification(\n-            dbName, tblName, writeId, timestamp, isUpdateDelete);\n+        for (TransactionRegistryInfo info : txnComponents) {\n+          materializationsInvalidationCache.notifyTableModification(\n+              info.dbName, info.tblName, info.writeId, info.timestamp, isUpdateDelete);\n+        }\n       } catch (SQLException e) {\n         LOG.debug(\"Going to rollback\");\n         rollbackDBConn(dbConn);\n@@ -1064,8 +1062,8 @@ public void commitTxn(CommitTxnRequest rqst)\n         close(commitIdRs);\n         close(lockHandle, stmt, dbConn);\n         unlockInternal();\n-        if (fullyQualifiedName != null) {\n-          materializationsRebuildLockHandler.unlockResource(dbName, tblName, txnid);\n+        for (TransactionRegistryInfo info : txnComponents) {\n+          materializationsRebuildLockHandler.unlockResource(info.dbName, info.tblName, txnid);\n         }\n       }\n     } catch (RetryException e) {\n@@ -4783,4 +4781,21 @@ public boolean isWrapperFor(Class<?> iface) throws SQLException {\n       throw new UnsupportedOperationException();\n     }\n   };\n+\n+  private class TransactionRegistryInfo {\n+    final String dbName;\n+    final String tblName;\n+    final String fullyQualifiedName;\n+    final long writeId;\n+    final long timestamp;\n+\n+    public TransactionRegistryInfo (String dbName, String tblName, long writeId, long timestamp) {\n+      this.dbName = dbName;\n+      this.tblName = tblName;\n+      this.fullyQualifiedName = Warehouse.getQualifiedName(dbName, tblName);\n+      this.writeId = writeId;\n+      this.timestamp = timestamp;\n+    }\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hive/raw/bd21f890e21f3baa8b715cb4203405de65b2a30b/standalone-metastore/src/main/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java",
                "sha": "361ede54efcd739516285e7b6a85e1122e8b870a",
                "status": "modified"
            }
        ],
        "message": "HIVE-19884 : Invalidation cache may throw NPE when there is no data in table used by materialized view (Jesus Camacho Rodriguez via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>",
        "parent": "https://github.com/apache/hive/commit/5a9a328a8129eb8bd116158e06cf37a259cf32c6",
        "patched_files": [
            "materialized_view_rewrite_empty.java",
            "MaterializationsInvalidationCache.java",
            "TxnHandler.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "testconfiguration.java",
            "TestTxnHandler.java"
        ]
    },
    "hive_bef879d": {
        "bug_id": "hive_bef879d",
        "commit": "https://github.com/apache/hive/commit/bef879d0a3e1827bffbd278a883e721124ee0eea",
        "file": [
            {
                "additions": 60,
                "blob_url": "https://github.com/apache/hive/blob/bef879d0a3e1827bffbd278a883e721124ee0eea/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java",
                "changes": 75,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java?ref=bef879d0a3e1827bffbd278a883e721124ee0eea",
                "deletions": 15,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java",
                "patch": "@@ -140,7 +140,9 @@ public void notifyUnlock(LlapCacheableBuffer buffer) {\n       } else if (heapSize == heap.length) {\n         // The buffer is not in the (full) heap. Demote the top item of the heap into the list.\n         LlapCacheableBuffer demoted = heap[0];\n-        synchronized (listLock) {\n+        listLock.lock();\n+        try {\n+          assert demoted.indexInHeap == 0; // Noone could have moved it, we have the heap lock.\n           demoted.indexInHeap = LlapCacheableBuffer.IN_LIST;\n           demoted.prev = null;\n           if (listHead != null) {\n@@ -151,6 +153,8 @@ public void notifyUnlock(LlapCacheableBuffer buffer) {\n             listHead = listTail = demoted;\n             demoted.next = null;\n           }\n+        } finally {\n+          listLock.unlock();\n         }\n         // Now insert the buffer in its place and restore heap property.\n         buffer.indexInHeap = 0;\n@@ -340,44 +344,62 @@ private void removeFromListAndUnlock(LlapCacheableBuffer buffer) {\n   }\n \n   private void removeFromListUnderLock(LlapCacheableBuffer buffer) {\n-    if (buffer == listTail) {\n+    buffer.indexInHeap = LlapCacheableBuffer.NOT_IN_CACHE;\n+    boolean isTail = buffer == listTail, isHead = buffer == listHead;\n+    if ((isTail != (buffer.next == null)) || (isHead != (buffer.prev == null))) {\n+      debugDumpListOnError(buffer);\n+      throw new AssertionError(\"LRFU list is corrupted.\");\n+    }\n+    if (isTail) {\n       listTail = buffer.prev;\n     } else {\n       buffer.next.prev = buffer.prev;\n     }\n-    if (buffer == listHead) {\n+    if (isHead) {\n       listHead = buffer.next;\n     } else {\n       buffer.prev.next = buffer.next;\n     }\n-    buffer.indexInHeap = LlapCacheableBuffer.NOT_IN_CACHE;\n   }\n \n   private void removeFromListUnderLockNoStateUpdate(\n       LlapCacheableBuffer from, LlapCacheableBuffer to) {\n-    if (to == listTail) {\n+    boolean isToTail = to == listTail, isFromHead = from == listHead;\n+    if ((isToTail != (to.next == null)) || (isFromHead != (from.prev == null))) {\n+      debugDumpListOnError(from, to);\n+      throw new AssertionError(\"LRFU list is corrupted.\");\n+    }\n+    if (isToTail) {\n       listTail = from.prev;\n     } else {\n       to.next.prev = from.prev;\n     }\n-    if (from == listHead) {\n+    if (isFromHead) {\n       listHead = to.next;\n     } else {\n       from.prev.next = to.next;\n     }\n   }\n \n-  public String debugDumpHeap() {\n-    StringBuilder result = new StringBuilder(\"List: \");\n-    if (listHead == null) {\n-      result.append(\"<empty>\");\n-    } else {\n-      LlapCacheableBuffer listItem = listHead;\n-      while (listItem != null) {\n-        result.append(listItem.toStringForCache()).append(\" -> \");\n-        listItem = listItem.next;\n+  private void debugDumpListOnError(LlapCacheableBuffer... buffers) {\n+    // Hopefully this will be helpful in case of NPEs.\n+    StringBuilder listDump = new StringBuilder(\"Invalid list removal. List: \");\n+    try {\n+      dumpList(listDump, listHead, listTail);\n+      int i = 0;\n+      for (LlapCacheableBuffer buffer : buffers) {\n+        listDump.append(\"; list from the buffer #\").append(i).append(\" being removed: \");\n+        dumpList(listDump, buffer, null);\n       }\n+    } catch (Throwable t) {\n+      LlapIoImpl.LOG.error(\"Error dumping the lists on error\", t);\n     }\n+    LlapIoImpl.LOG.error(listDump.toString());\n+  }\n+\n+  public String debugDumpHeap() {\n+    StringBuilder result = new StringBuilder(\"List: \");\n+    dumpList(result, listHead, listTail);\n     result.append(\"\\nHeap:\");\n     if (heapSize == 0) {\n       result.append(\" <empty>\\n\");\n@@ -421,6 +443,29 @@ public String debugDumpHeap() {\n     return result.toString();\n   }\n \n+  private static void dumpList(StringBuilder result,\n+      LlapCacheableBuffer listHeadLocal, LlapCacheableBuffer listTailLocal) {\n+    if (listHeadLocal == null) {\n+      result.append(\"<empty>\");\n+      return;\n+    }\n+    LlapCacheableBuffer listItem = listHeadLocal;\n+    while (listItem.prev != null) {\n+      listItem = listItem.prev;  // To detect incorrect lists.\n+    }\n+    while (listItem != null) {\n+      result.append(listItem.toStringForCache());\n+      if (listItem == listTailLocal) {\n+        result.append(\"(tail)\"); // To detect incorrect lists.\n+      }\n+      if (listItem == listHeadLocal) {\n+        result.append(\"(head)\"); // To detect incorrect lists.\n+      }\n+      result.append(\" -> \");\n+      listItem = listItem.next;\n+    }\n+  }\n+\n   @Override\n   public String debugDumpForOom() {\n     String result = debugDumpHeap();",
                "raw_url": "https://github.com/apache/hive/raw/bef879d0a3e1827bffbd278a883e721124ee0eea/llap-server/src/java/org/apache/hadoop/hive/llap/cache/LowLevelLrfuCachePolicy.java",
                "sha": "40cb92d24550489bb9df653e179700594fcb9679",
                "status": "modified"
            }
        ],
        "message": "HIVE-12557 : NPE while removing entry in LRFU cache (Sergey Shelukhin, reviewed by Prasanth Jayachandran)",
        "parent": "https://github.com/apache/hive/commit/1d02ab578dbd47103a70710abd4d949ea8cea9d2",
        "patched_files": [
            "LowLevelLrfuCachePolicy.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestLowLevelLrfuCachePolicy.java"
        ]
    },
    "hive_c156b32": {
        "bug_id": "hive_c156b32",
        "commit": "https://github.com/apache/hive/commit/c156b32b49aeb5943e45a68fc7600c9244afb128",
        "file": [
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hive/blob/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java?ref=c156b32b49aeb5943e45a68fc7600c9244afb128",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java",
                "patch": "@@ -26,10 +26,12 @@\n import org.apache.hadoop.hive.common.ValidReadTxnList;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.apache.hadoop.hive.metastore.api.Partition;\n import org.apache.hadoop.hive.metastore.api.ShowLocksRequest;\n import org.apache.hadoop.hive.metastore.api.ShowLocksResponse;\n import org.apache.hadoop.hive.metastore.api.ShowLocksResponseElement;\n import org.apache.hadoop.hive.metastore.api.StorageDescriptor;\n+import org.apache.hadoop.hive.metastore.api.Table;\n import org.apache.hadoop.hive.metastore.txn.CompactionInfo;\n import org.apache.hadoop.hive.ql.io.AcidUtils;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -183,7 +185,23 @@ public void run() {\n   private void clean(CompactionInfo ci) throws MetaException {\n     LOG.info(\"Starting cleaning for \" + ci.getFullPartitionName());\n     try {\n-      StorageDescriptor sd = resolveStorageDescriptor(resolveTable(ci), resolvePartition(ci));\n+      Table t = resolveTable(ci);\n+      if (t == null) {\n+        // The table was dropped before we got around to cleaning it.\n+        LOG.info(\"Unable to find table \" + ci.getFullTableName() + \", assuming it was dropped\");\n+        return;\n+      }\n+      Partition p = null;\n+      if (ci.partName != null) {\n+        p = resolvePartition(ci);\n+        if (p == null) {\n+          // The partition was dropped before we got around to cleaning it.\n+          LOG.info(\"Unable to find partition \" + ci.getFullPartitionName() +\n+              \", assuming it was dropped\");\n+          return;\n+        }\n+      }\n+      StorageDescriptor sd = resolveStorageDescriptor(t, p);\n       final String location = sd.getLocation();\n \n       // Create a bogus validTxnList with a high water mark set to MAX_LONG and no open",
                "raw_url": "https://github.com/apache/hive/raw/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Cleaner.java",
                "sha": "16d2c81333dbc2e3a74b28018933f127f9b146fb",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorThread.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorThread.java?ref=c156b32b49aeb5943e45a68fc7600c9244afb128",
                "deletions": 5,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorThread.java",
                "patch": "@@ -32,13 +32,13 @@\n import org.apache.hadoop.hive.metastore.api.Table;\n import org.apache.hadoop.hive.metastore.txn.CompactionInfo;\n import org.apache.hadoop.hive.metastore.txn.CompactionTxnHandler;\n-import org.apache.hadoop.hive.metastore.txn.TxnHandler;\n import org.apache.hadoop.security.AccessControlException;\n import org.apache.hadoop.security.UserGroupInformation;\n \n import java.io.IOException;\n import java.security.PrivilegedExceptionAction;\n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.List;\n import java.util.concurrent.atomic.AtomicBoolean;\n \n@@ -105,13 +105,15 @@ protected Table resolveTable(CompactionInfo ci) throws MetaException {\n    * one partition.\n    */\n   protected Partition resolvePartition(CompactionInfo ci) throws Exception {\n-    Partition p = null;\n     if (ci.partName != null) {\n-      List<String> names = new ArrayList<String>(1);\n-      names.add(ci.partName);\n       List<Partition> parts = null;\n       try {\n-        parts = rs.getPartitionsByNames(ci.dbname, ci.tableName, names);\n+        parts = rs.getPartitionsByNames(ci.dbname, ci.tableName,\n+            Collections.singletonList(ci.partName));\n+        if (parts == null || parts.size() == 0) {\n+          // The partition got dropped before we went looking for it.\n+          return null;\n+        }\n       } catch (Exception e) {\n         LOG.error(\"Unable to find partition \" + ci.getFullPartitionName() + \", \" + e.getMessage());\n         throw e;",
                "raw_url": "https://github.com/apache/hive/raw/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/CompactorThread.java",
                "sha": "38cd95ea305edd26bb425b1555ce125930aa3de3",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java?ref=c156b32b49aeb5943e45a68fc7600c9244afb128",
                "deletions": 3,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java",
                "patch": "@@ -85,13 +85,13 @@ public void run() {\n           LOG.debug(\"Found \" + potentials.size() + \" potential compactions, \" +\n               \"checking to see if we should compact any of them\");\n           for (CompactionInfo ci : potentials) {\n-            LOG.debug(\"Checking to see if we should compact \" + ci.getFullPartitionName());\n+            LOG.info(\"Checking to see if we should compact \" + ci.getFullPartitionName());\n             try {\n               Table t = resolveTable(ci);\n               if (t == null) {\n                 // Most likely this means it's a temp table\n-                LOG.debug(\"Can't find table \" + ci.getFullTableName() + \", assuming it's a temp \" +\n-                    \"table and moving on.\");\n+                LOG.info(\"Can't find table \" + ci.getFullTableName() + \", assuming it's a temp \" +\n+                    \"table or has been dropped and moving on.\");\n                 continue;\n               }\n \n@@ -121,6 +121,11 @@ public void run() {\n \n               // Figure out who we should run the file operations as\n               Partition p = resolvePartition(ci);\n+              if (p == null && ci.partName != null) {\n+                LOG.info(\"Can't find partition \" + ci.getFullPartitionName() +\n+                    \", assuming it has been dropped and moving on.\");\n+                continue;\n+              }\n               StorageDescriptor sd = resolveStorageDescriptor(t, p);\n               String runAs = findUserToRunAs(sd.getLocation(), t);\n ",
                "raw_url": "https://github.com/apache/hive/raw/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Initiator.java",
                "sha": "847d75199d6d614bd17ea852a4e3e87bf6911be7",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hive/blob/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java?ref=c156b32b49aeb5943e45a68fc7600c9244afb128",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java",
                "patch": "@@ -94,6 +94,12 @@ public void run() {\n         Table t1 = null;\n         try {\n           t1 = resolveTable(ci);\n+          if (t1 == null) {\n+            LOG.info(\"Unable to find table \" + ci.getFullTableName() +\n+                \", assuming it was dropped and moving on.\");\n+            txnHandler.markCleaned(ci);\n+            continue;\n+          }\n         } catch (MetaException e) {\n           txnHandler.markCleaned(ci);\n           continue;\n@@ -106,6 +112,12 @@ public void run() {\n         Partition p = null;\n         try {\n           p = resolvePartition(ci);\n+          if (p == null && ci.partName != null) {\n+            LOG.info(\"Unable to find partition \" + ci.getFullPartitionName() +\n+                \", assuming it was dropped and moving on.\");\n+            txnHandler.markCleaned(ci);\n+            continue;\n+          }\n         } catch (Exception e) {\n           txnHandler.markCleaned(ci);\n           continue;",
                "raw_url": "https://github.com/apache/hive/raw/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/java/org/apache/hadoop/hive/ql/txn/compactor/Worker.java",
                "sha": "f26225a72c34252c8fdf615bd34b59532376c5de",
                "status": "modified"
            },
            {
                "additions": 54,
                "blob_url": "https://github.com/apache/hive/blob/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCleaner.java",
                "changes": 56,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCleaner.java?ref=c156b32b49aeb5943e45a68fc7600c9244afb128",
                "deletions": 2,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCleaner.java",
                "patch": "@@ -17,17 +17,17 @@\n  */\n package org.apache.hadoop.hive.ql.txn.compactor;\n \n-import junit.framework.Assert;\n+import org.junit.Assert;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.conf.HiveConf;\n-import org.apache.hadoop.hive.metastore.MetaStoreThread;\n import org.apache.hadoop.hive.metastore.api.*;\n import org.apache.hadoop.hive.metastore.txn.CompactionInfo;\n import org.junit.Test;\n \n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.List;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicBoolean;\n@@ -428,4 +428,56 @@ public void cleanupAfterMajorPartitionCompactionNoBase() throws Exception {\n     Assert.assertEquals(1, paths.size());\n     Assert.assertEquals(\"base_25\", paths.get(0).getName());\n   }\n+\n+  @Test\n+  public void droppedTable() throws Exception {\n+    Table t = newTable(\"default\", \"dt\", false);\n+\n+    addDeltaFile(t, null, 1L, 22L, 22);\n+    addDeltaFile(t, null, 23L, 24L, 2);\n+    addBaseFile(t, null, 25L, 25);\n+\n+    burnThroughTransactions(25);\n+\n+    CompactionRequest rqst = new CompactionRequest(\"default\", \"dt\", CompactionType.MINOR);\n+    txnHandler.compact(rqst);\n+    CompactionInfo ci = txnHandler.findNextToCompact(\"fred\");\n+    txnHandler.markCompacted(ci);\n+    txnHandler.setRunAs(ci.id, System.getProperty(\"user.name\"));\n+\n+    ms.dropTable(\"default\", \"dt\");\n+\n+    startCleaner();\n+\n+    // Check there are no compactions requests left.\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(0, rsp.getCompactsSize());\n+  }\n+\n+  @Test\n+  public void droppedPartition() throws Exception {\n+    Table t = newTable(\"default\", \"dp\", true);\n+    Partition p = newPartition(t, \"today\");\n+\n+    addDeltaFile(t, p, 1L, 22L, 22);\n+    addDeltaFile(t, p, 23L, 24L, 2);\n+    addBaseFile(t, p, 25L, 25);\n+\n+    burnThroughTransactions(25);\n+\n+    CompactionRequest rqst = new CompactionRequest(\"default\", \"dp\", CompactionType.MAJOR);\n+    rqst.setPartitionname(\"ds=today\");\n+    txnHandler.compact(rqst);\n+    CompactionInfo ci = txnHandler.findNextToCompact(\"fred\");\n+    txnHandler.markCompacted(ci);\n+    txnHandler.setRunAs(ci.id, System.getProperty(\"user.name\"));\n+\n+    ms.dropPartition(\"default\", \"dp\", Collections.singletonList(\"today\"), true);\n+\n+    startCleaner();\n+\n+    // Check there are no compactions requests left.\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    Assert.assertEquals(0, rsp.getCompactsSize());\n+  }\n }",
                "raw_url": "https://github.com/apache/hive/raw/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestCleaner.java",
                "sha": "ffdbb9aacf87610d1c340e97fb715e9c1c25152d",
                "status": "modified"
            },
            {
                "additions": 62,
                "blob_url": "https://github.com/apache/hive/blob/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestInitiator.java",
                "changes": 63,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestInitiator.java?ref=c156b32b49aeb5943e45a68fc7600c9244afb128",
                "deletions": 1,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestInitiator.java",
                "patch": "@@ -17,7 +17,7 @@\n  */\n package org.apache.hadoop.hive.ql.txn.compactor;\n \n-import junit.framework.Assert;\n+import org.junit.Assert;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.hive.conf.HiveConf;\n@@ -27,6 +27,7 @@\n import org.junit.Test;\n \n import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n@@ -653,4 +654,64 @@ public void noCompactTableDynamicPartitioning() throws Exception {\n     Assert.assertEquals(0, compacts.size());\n   }\n \n+  @Test\n+  public void dropTable() throws Exception {\n+    Table t = newTable(\"default\", \"dt\", false);\n+\n+    addBaseFile(t, null, 20L, 20);\n+    addDeltaFile(t, null, 21L, 22L, 2);\n+    addDeltaFile(t, null, 23L, 24L, 2);\n+\n+    burnThroughTransactions(23);\n+\n+    long txnid = openTxn();\n+    LockComponent comp = new LockComponent(LockType.SHARED_WRITE, LockLevel.PARTITION, \"default\");\n+    comp.setTablename(\"dt\");\n+    List<LockComponent> components = new ArrayList<LockComponent>(1);\n+    components.add(comp);\n+    LockRequest req = new LockRequest(components, \"me\", \"localhost\");\n+    req.setTxnid(txnid);\n+    LockResponse res = txnHandler.lock(req);\n+    txnHandler.commitTxn(new CommitTxnRequest(txnid));\n+\n+    ms.dropTable(\"default\", \"dt\");\n+\n+    startInitiator();\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    List<ShowCompactResponseElement> compacts = rsp.getCompacts();\n+    Assert.assertEquals(0, compacts.size());\n+  }\n+\n+  @Test\n+  public void dropPartition() throws Exception {\n+    Table t = newTable(\"default\", \"dp\", true);\n+    Partition p = newPartition(t, \"today\");\n+\n+    addBaseFile(t, p, 20L, 20);\n+    addDeltaFile(t, p, 21L, 22L, 2);\n+    addDeltaFile(t, p, 23L, 24L, 2);\n+\n+    burnThroughTransactions(23);\n+\n+    long txnid = openTxn();\n+    LockComponent comp = new LockComponent(LockType.SHARED_WRITE, LockLevel.PARTITION, \"default\");\n+    comp.setTablename(\"dp\");\n+    comp.setPartitionname(\"ds=today\");\n+    List<LockComponent> components = new ArrayList<LockComponent>(1);\n+    components.add(comp);\n+    LockRequest req = new LockRequest(components, \"me\", \"localhost\");\n+    req.setTxnid(txnid);\n+    LockResponse res = txnHandler.lock(req);\n+    txnHandler.commitTxn(new CommitTxnRequest(txnid));\n+\n+    ms.dropPartition(\"default\", \"dp\", Collections.singletonList(\"today\"), true);\n+\n+    startInitiator();\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    List<ShowCompactResponseElement> compacts = rsp.getCompacts();\n+    Assert.assertEquals(0, compacts.size());\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hive/raw/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestInitiator.java",
                "sha": "00b13de0992eee9b1aa7144bb2f9ae6549e4e55e",
                "status": "modified"
            },
            {
                "additions": 45,
                "blob_url": "https://github.com/apache/hive/blob/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java",
                "changes": 45,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java?ref=c156b32b49aeb5943e45a68fc7600c9244afb128",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java",
                "patch": "@@ -29,6 +29,7 @@\n import java.io.*;\n import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.List;\n@@ -799,4 +800,48 @@ public void majorWithAborted() throws Exception {\n     Assert.assertEquals(\"delta_23_25\", stat[3].getPath().getName());\n     Assert.assertEquals(\"delta_26_27\", stat[4].getPath().getName());\n   }\n+\n+  @Test\n+  public void droppedTable() throws Exception {\n+    Table t = newTable(\"default\", \"dt\", false);\n+\n+    addDeltaFile(t, null, 1L, 2L, 2);\n+    addDeltaFile(t, null, 3L, 4L, 2);\n+    burnThroughTransactions(4);\n+\n+    CompactionRequest rqst = new CompactionRequest(\"default\", \"dt\", CompactionType.MAJOR);\n+    txnHandler.compact(rqst);\n+\n+    ms.dropTable(\"default\", \"dt\");\n+\n+    startWorker();\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    List<ShowCompactResponseElement> compacts = rsp.getCompacts();\n+    Assert.assertEquals(0, compacts.size());\n+  }\n+\n+  @Test\n+  public void droppedPartition() throws Exception {\n+    Table t = newTable(\"default\", \"dp\", true);\n+    Partition p = newPartition(t, \"today\");\n+\n+    addBaseFile(t, p, 20L, 20);\n+    addDeltaFile(t, p, 21L, 22L, 2);\n+    addDeltaFile(t, p, 23L, 24L, 2);\n+\n+    burnThroughTransactions(25);\n+\n+    CompactionRequest rqst = new CompactionRequest(\"default\", \"dp\", CompactionType.MINOR);\n+    rqst.setPartitionname(\"ds=today\");\n+    txnHandler.compact(rqst);\n+\n+    ms.dropPartition(\"default\", \"dp\", Collections.singletonList(\"today\"), true);\n+\n+    startWorker();\n+\n+    ShowCompactResponse rsp = txnHandler.showCompact(new ShowCompactRequest());\n+    List<ShowCompactResponseElement> compacts = rsp.getCompacts();\n+    Assert.assertEquals(0, compacts.size());\n+  }\n }",
                "raw_url": "https://github.com/apache/hive/raw/c156b32b49aeb5943e45a68fc7600c9244afb128/ql/src/test/org/apache/hadoop/hive/ql/txn/compactor/TestWorker.java",
                "sha": "bebac541265a32d518e634f09b347b4ea4416f00",
                "status": "modified"
            }
        ],
        "message": "HIVE-10595 Dropping a table can cause NPEs in the compactor (Alan Gates, reviewed by Eugene Koifman)",
        "parent": "https://github.com/apache/hive/commit/72088ca7c39136dd68dbbefb3897cd7abfdd982c",
        "patched_files": [
            "Cleaner.java",
            "CompactorThread.java",
            "Worker.java",
            "Initiator.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestCleaner.java",
            "TestInitiator.java",
            "TestWorker.java"
        ]
    },
    "hive_c20b086": {
        "bug_id": "hive_c20b086",
        "commit": "https://github.com/apache/hive/commit/c20b0866824ff5ae7ce8ca0e2ae236a5af908917",
        "file": [
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java",
                "changes": 62,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917",
                "deletions": 28,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java",
                "patch": "@@ -38,6 +38,7 @@\n import org.apache.hadoop.hive.llap.metrics.LlapDaemonExecutorMetrics;\n import org.apache.hadoop.hive.llap.protocol.LlapTaskUmbilicalProtocol;\n import org.apache.hadoop.hive.llap.tezplugins.Converters;\n+import org.apache.hadoop.hive.ql.io.IOContextMap;\n import org.apache.hadoop.ipc.RPC;\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.security.Credentials;\n@@ -198,38 +199,43 @@ public LlapTaskUmbilicalProtocol run() throws Exception {\n         new AtomicLong(0),\n         request.getContainerIdString());\n \n-    synchronized (this) {\n-      if (shouldRunTask) {\n-        taskRunner = new TezTaskRunner2(conf, taskUgi, fragmentInfo.getLocalDirs(),\n-            taskSpec,\n-            request.getAppAttemptNumber(),\n-            serviceConsumerMetadata, envMap, startedInputsMap, taskReporter, executor,\n-            objectRegistry,\n-            pid,\n-            executionContext, memoryAvailable);\n-      }\n-    }\n-    if (taskRunner == null) {\n-      LOG.info(\"Not starting task {} since it was killed earlier\", taskSpec.getTaskAttemptID());\n-      return new TaskRunner2Result(EndReason.KILL_REQUESTED, null, false);\n-    }\n-\n+    String attemptId = fragmentInfo.getFragmentIdentifierString();\n+    IOContextMap.setThreadAttemptId(attemptId);\n     try {\n-      TaskRunner2Result result = taskRunner.run();\n-      if (result.isContainerShutdownRequested()) {\n-        LOG.warn(\"Unexpected container shutdown requested while running task. Ignoring\");\n+      synchronized (this) {\n+        if (shouldRunTask) {\n+          taskRunner = new TezTaskRunner2(conf, taskUgi, fragmentInfo.getLocalDirs(),\n+              taskSpec,\n+              request.getAppAttemptNumber(),\n+              serviceConsumerMetadata, envMap, startedInputsMap, taskReporter, executor,\n+              objectRegistry,\n+              pid,\n+              executionContext, memoryAvailable);\n+        }\n+      }\n+      if (taskRunner == null) {\n+        LOG.info(\"Not starting task {} since it was killed earlier\", taskSpec.getTaskAttemptID());\n+        return new TaskRunner2Result(EndReason.KILL_REQUESTED, null, false);\n       }\n-      isCompleted.set(true);\n-      return result;\n \n-    } finally {\n-      // TODO Fix UGI and FS Handling. Closing UGI here causes some errors right now.\n-      //        FileSystem.closeAllForUGI(taskUgi);\n-      LOG.info(\"ExecutionTime for Container: \" + request.getContainerIdString() + \"=\" +\n-          runtimeWatch.stop().elapsedMillis());\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(\"canFinish post completion: \" + taskSpec.getTaskAttemptID() + \": \" + canFinish());\n+      try {\n+        TaskRunner2Result result = taskRunner.run();\n+        if (result.isContainerShutdownRequested()) {\n+          LOG.warn(\"Unexpected container shutdown requested while running task. Ignoring\");\n+        }\n+        isCompleted.set(true);\n+        return result;\n+      } finally {\n+        // TODO Fix UGI and FS Handling. Closing UGI here causes some errors right now.\n+        //        FileSystem.closeAllForUGI(taskUgi);\n+        LOG.info(\"ExecutionTime for Container: \" + request.getContainerIdString() + \"=\" +\n+            runtimeWatch.stop().elapsedMillis());\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"canFinish post completion: \" + taskSpec.getTaskAttemptID() + \": \" + canFinish());\n+        }\n       }\n+    } finally {\n+      IOContextMap.clearThreadAttempt(attemptId);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/impl/TaskRunnerCallable.java",
                "sha": "1a125cbb98fb03ebf451a8bd5b7e39e26f5ec26e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java",
                "patch": "@@ -25,6 +25,7 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.io.IOContext;\n+import org.apache.hadoop.hive.ql.io.IOContextMap;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hadoop.hive.ql.plan.FilterDesc;\n import org.apache.hadoop.hive.ql.plan.api.OperatorType;\n@@ -61,7 +62,7 @@ protected void initializeOp(Configuration hconf) throws HiveException {\n       }\n \n       conditionInspector = null;\n-      ioContext = IOContext.get(hconf);\n+      ioContext = IOContextMap.get(hconf);\n     } catch (Throwable e) {\n       throw new HiveException(e);\n     }",
                "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/exec/FilterOperator.java",
                "sha": "0e7e79dc864a63aaf69e37d90dcf41cc33826739",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecMapperContext.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecMapperContext.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917",
                "deletions": 7,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecMapperContext.java",
                "patch": "@@ -22,8 +22,8 @@\n import org.apache.commons.logging.Log;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.ql.exec.FetchOperator;\n-import org.apache.hadoop.hive.ql.exec.Utilities;\n import org.apache.hadoop.hive.ql.io.IOContext;\n+import org.apache.hadoop.hive.ql.io.IOContextMap;\n import org.apache.hadoop.hive.ql.plan.MapredLocalWork;\n import org.apache.hadoop.mapred.JobConf;\n \n@@ -63,11 +63,11 @@ public void setCurrentBigBucketFile(String currentBigBucketFile) {\n \n   public ExecMapperContext(JobConf jc) {\n     this.jc = jc;\n-    ioCxt = IOContext.get(jc);\n+    ioCxt = IOContextMap.get(jc);\n   }\n \n   public void clear() {\n-    IOContext.clear();\n+    IOContextMap.clear();\n     ioCxt = null;\n   }\n \n@@ -151,8 +151,4 @@ public void setFetchOperators(Map<String, FetchOperator> fetchOperators) {\n   public IOContext getIoCxt() {\n     return ioCxt;\n   }\n-\n-  public void setIoCxt(IOContext ioCxt) {\n-    this.ioCxt = ioCxt;\n-  }\n }",
                "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecMapperContext.java",
                "sha": "fc5abfef583e9a2c4c6a0866b30421b724dd8415",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java",
                "patch": "@@ -201,7 +201,7 @@ public Object call() {\n             mergeMapOp.setChildren(jconf);\n \n             DummyStoreOperator dummyOp = getJoinParentOp(mergeMapOp);\n-\t          mapOp.setConnectedOperators(mergeMapWork.getTag(), dummyOp);\n+            mapOp.setConnectedOperators(mergeMapWork.getTag(), dummyOp);\n \n             mergeMapOp.passExecContext(new ExecMapperContext(jconf));\n             mergeMapOp.initializeLocalWork(jconf);",
                "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/MapRecordProcessor.java",
                "sha": "e205f1ebaf14434a92957538d443c33678987a6e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/io/HiveContextAwareRecordReader.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/HiveContextAwareRecordReader.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/HiveContextAwareRecordReader.java",
                "patch": "@@ -162,7 +162,7 @@ protected void updateIOContext()\n   }\n \n   public IOContext getIOContext() {\n-    return IOContext.get(jobConf);\n+    return IOContextMap.get(jobConf);\n   }\n \n   private void initIOContext(long startPos, boolean isBlockPointer,",
                "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/io/HiveContextAwareRecordReader.java",
                "sha": "738ca9ce32b4b7a6bfd5f239fd12aa989724c67f",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java",
                "patch": "@@ -264,7 +264,6 @@ public static boolean canWrapForLlap(Class<? extends InputFormat> inputFormatCla\n \n   public RecordReader getRecordReader(InputSplit split, JobConf job,\n       Reporter reporter) throws IOException {\n-\n     HiveInputSplit hsplit = (HiveInputSplit) split;\n \n     InputSplit inputSplit = hsplit.getInputSplit();",
                "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/io/HiveInputFormat.java",
                "sha": "0d9b6445f143e9abab18cad0124eda11d6538be4",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/io/IOContext.java",
                "changes": 55,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/IOContext.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917",
                "deletions": 55,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/IOContext.java",
                "patch": "@@ -18,13 +18,7 @@\n \n package org.apache.hadoop.hive.ql.io;\n \n-import java.util.HashMap;\n-import java.util.Map;\n-\n-import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.Path;\n-import org.apache.hadoop.hive.conf.HiveConf;\n-import org.apache.hadoop.hive.ql.exec.Utilities;\n \n /**\n  * IOContext basically contains the position information of the current\n@@ -35,55 +29,6 @@\n  * nextBlockStart refers the end of current row and beginning of next row.\n  */\n public class IOContext {\n-  public static final String DEFAULT_CONTEXT = \"\";\n-\n-  private static final ThreadLocal<Map<String,IOContext>> threadLocalMap\n-      = new ThreadLocal<Map<String,IOContext>>() {\n-    @Override\n-    protected synchronized Map<String,IOContext> initialValue() {\n-      Map<String, IOContext> map = new HashMap<String, IOContext>(); \n-      map.put(DEFAULT_CONTEXT, new IOContext());\n-      return map;\n-    }\n-  };\n-\n-  /**\n-   * Spark uses this thread local TODO: no it doesn't?\n-   */\n-  private static final ThreadLocal<IOContext> threadLocal = new ThreadLocal<IOContext>(){\n-    @Override\n-    protected IOContext initialValue() { return new IOContext(); }\n-  };\n-\n-  private static IOContext get() {\n-      return IOContext.threadLocalMap.get().get(DEFAULT_CONTEXT);\n-  }\n-\n-  /**\n-   * Tez and MR use this map but are single threaded per JVM thus no synchronization is required.\n-   */\n-  private static final Map<String, IOContext> inputNameIOContextMap = new HashMap<String, IOContext>();\n-\n-  public static IOContext get(Configuration conf) {\n-    String inputName = conf.get(Utilities.INPUT_NAME);\n-    Map<String, IOContext> inputNameIOContextMap = threadLocalMap.get();\n-\n-    if (inputName == null) {\n-      inputName = DEFAULT_CONTEXT;\n-    }\n-\n-    if (!inputNameIOContextMap.containsKey(inputName)) {\n-      IOContext ioContext = new IOContext();\n-      inputNameIOContextMap.put(inputName, ioContext);\n-    }\n-\n-    return inputNameIOContextMap.get(inputName);\n-  }\n-\n-  public static void clear() {\n-      threadLocal.remove();\n-  }\n-\n   private long currentBlockStart;\n   private long nextBlockStart;\n   private long currentRow;",
                "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/io/IOContext.java",
                "sha": "019db8db83af4bbb3c9f2209cae52669befe6695",
                "status": "modified"
            },
            {
                "additions": 117,
                "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/io/IOContextMap.java",
                "changes": 117,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/IOContextMap.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/IOContextMap.java",
                "patch": "@@ -0,0 +1,117 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io;\n+\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+\n+/**\n+ * There used to be a global static map of IOContext-s inside IOContext (Hive style!).\n+ * Unfortunately, due to variety of factors, this is now a giant fustercluck.\n+ * 1) Spark doesn't apparently care about multiple inputs, but has multiple threads, so one\n+ *    threadlocal IOContext was added for it.\n+ * 2) LLAP has lots of tasks in the same process so globals no longer cut it either.\n+ * 3) However, Tez runs 2+ threads for one task (e.g. TezTaskEventRouter and TezChild), and these\n+ *    surprisingly enough need the same context. Tez, in its infinite wisdom, doesn't allow them\n+ *    to communicate in any way nor provide any shared context.\n+ * So we are going to...\n+ * 1) Keep the good ol' global map for MR and Tez. Hive style!\n+ * 2) Keep the threadlocal for Spark. Hive style!\n+ * 3) Create inheritable (TADA!) threadlocal with attemptId, only set in LLAP; that will propagate\n+ *    to all the little Tez threads, and we will keep a map per attempt. Hive style squared!\n+ */\n+public class IOContextMap {\n+  public static final String DEFAULT_CONTEXT = \"\";\n+  private static final Log LOG = LogFactory.getLog(IOContextMap.class);\n+\n+  /** Used for Tez and MR */\n+  private static final ConcurrentHashMap<String, IOContext> globalMap =\n+      new ConcurrentHashMap<String, IOContext>();\n+\n+  /** Used for Spark */\n+  private static final ThreadLocal<IOContext> sparkThreadLocal = new ThreadLocal<IOContext>(){\n+    @Override\n+    protected IOContext initialValue() { return new IOContext(); }\n+  };\n+\n+  /** Used for Tez+LLAP */\n+  private static final ConcurrentHashMap<String, ConcurrentHashMap<String, IOContext>> attemptMap =\n+      new ConcurrentHashMap<String, ConcurrentHashMap<String, IOContext>>();\n+\n+  // TODO: This depends on Tez creating separate threads, as it does now. If that changes, some\n+  //       other way to propagate/find out attempt ID would be needed (e.g. see TEZ-2587).\n+  private static final InheritableThreadLocal<String> threadAttemptId =\n+      new InheritableThreadLocal<>();\n+\n+  public static void setThreadAttemptId(String attemptId) {\n+    assert attemptId != null;\n+    threadAttemptId.set(attemptId);\n+  }\n+\n+  public static void clearThreadAttempt(String attemptId) {\n+    assert attemptId != null;\n+    String attemptIdCheck = threadAttemptId.get();\n+    if (!attemptId.equals(attemptIdCheck)) {\n+      LOG.error(\"Thread is clearing context for \"\n+          + attemptId + \", but \" + attemptIdCheck + \" expected\");\n+    }\n+    attemptMap.remove(attemptId);\n+    threadAttemptId.remove();\n+  }\n+\n+  public static IOContext get(Configuration conf) {\n+    if (HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_EXECUTION_ENGINE).equals(\"spark\")) {\n+      return sparkThreadLocal.get();\n+    }\n+    String inputName = conf.get(Utilities.INPUT_NAME);\n+    if (inputName == null) {\n+      inputName = DEFAULT_CONTEXT;\n+    }\n+    String attemptId = threadAttemptId.get();\n+    ConcurrentHashMap<String, IOContext> map;\n+    if (attemptId == null) {\n+      map = globalMap;\n+    } else {\n+      map = attemptMap.get(attemptId);\n+      if (map == null) {\n+        map = new ConcurrentHashMap<>();\n+        ConcurrentHashMap<String, IOContext> oldMap = attemptMap.putIfAbsent(attemptId, map);\n+        if (oldMap != null) {\n+          map = oldMap;\n+        }\n+      }\n+    }\n+\n+    IOContext ioContext = map.get(inputName);\n+    if (ioContext != null) return ioContext;\n+    ioContext = new IOContext();\n+    IOContext oldContext = map.putIfAbsent(inputName, ioContext);\n+    return (oldContext == null) ? ioContext : oldContext;\n+  }\n+\n+  public static void clear() {\n+    sparkThreadLocal.remove();\n+    globalMap.clear();\n+  }\n+}",
                "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/java/org/apache/hadoop/hive/ql/io/IOContextMap.java",
                "sha": "57e7e2a910023ce2f4a700c2a3dc6b7654fb489a",
                "status": "added"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917",
                "deletions": 1,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java",
                "patch": "@@ -33,6 +33,7 @@\n import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.Driver;\n import org.apache.hadoop.hive.ql.io.IOContext;\n+import org.apache.hadoop.hive.ql.io.IOContextMap;\n import org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory;\n import org.apache.hadoop.hive.ql.plan.CollectDesc;\n import org.apache.hadoop.hive.ql.plan.ExprNodeConstantDesc;\n@@ -272,7 +273,7 @@ public void testMapOperator() throws Throwable {\n       JobConf hconf = new JobConf(TestOperators.class);\n       HiveConf.setVar(hconf, HiveConf.ConfVars.HADOOPMAPFILENAME,\n           \"hdfs:///testDir/testFile\");\n-      IOContext.get(hconf).setInputPath(\n+      IOContextMap.get(hconf).setInputPath(\n           new Path(\"hdfs:///testDir/testFile\"));\n \n       // initialize pathToAliases",
                "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/test/org/apache/hadoop/hive/ql/exec/TestOperators.java",
                "sha": "c3a36c021127935790a90eff7136b91b0cb77e4e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/test/org/apache/hadoop/hive/ql/io/TestHiveBinarySearchRecordReader.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/io/TestHiveBinarySearchRecordReader.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917",
                "deletions": 1,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/io/TestHiveBinarySearchRecordReader.java",
                "patch": "@@ -116,7 +116,7 @@ public void doClose() throws IOException {\n \n   private void resetIOContext() {\n     conf.set(Utilities.INPUT_NAME, \"TestHiveBinarySearchRecordReader\");\n-    ioContext = IOContext.get(conf);\n+    ioContext = IOContextMap.get(conf);\n     ioContext.setUseSorted(false);\n     ioContext.setBinarySearching(false);\n     ioContext.setEndBinarySearch(false);",
                "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/test/org/apache/hadoop/hive/ql/io/TestHiveBinarySearchRecordReader.java",
                "sha": "9dc4f5b8ad64b75111f5d39a008e697ba5bb18e9",
                "status": "modified"
            },
            {
                "additions": 207,
                "blob_url": "https://github.com/apache/hive/blob/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/test/org/apache/hadoop/hive/ql/io/TestIOContextMap.java",
                "changes": 207,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/io/TestIOContextMap.java?ref=c20b0866824ff5ae7ce8ca0e2ae236a5af908917",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/io/TestIOContextMap.java",
                "patch": "@@ -0,0 +1,207 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.io;\n+\n+import static org.junit.Assert.*;\n+\n+import java.util.Set;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CountDownLatch;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.FutureTask;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.Utilities;\n+import org.junit.Test;\n+\n+import com.google.common.collect.Sets;\n+\n+public class TestIOContextMap {\n+\n+  private void syncThreadStart(final CountDownLatch cdlIn, final CountDownLatch cdlOut) {\n+    cdlIn.countDown();\n+    try {\n+      cdlOut.await();\n+    } catch (InterruptedException e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n+\n+  @Test\n+  public void testMRTezGlobalMap() throws Exception {\n+    // Tests concurrent modification, and that results are the same per input across threads\n+    // but different between inputs.\n+    final int THREAD_COUNT = 2, ITER_COUNT = 1000;\n+    final AtomicInteger countdown = new AtomicInteger(ITER_COUNT);\n+    final CountDownLatch phase1End = new CountDownLatch(THREAD_COUNT);\n+    final IOContext[] results = new IOContext[ITER_COUNT];\n+    ExecutorService executor = Executors.newFixedThreadPool(THREAD_COUNT);\n+    final CountDownLatch cdlIn = new CountDownLatch(THREAD_COUNT), cdlOut = new CountDownLatch(1);\n+\n+    @SuppressWarnings(\"unchecked\")\n+    FutureTask<Void>[] tasks = new FutureTask[THREAD_COUNT];\n+    for (int i = 0; i < tasks.length; ++i) {\n+      tasks[i] = new FutureTask<Void>(new Callable<Void>() {\n+        public Void call() throws Exception {\n+          Configuration conf = new Configuration();\n+          syncThreadStart(cdlIn, cdlOut);\n+          // Phase 1 - create objects.\n+          while (true) {\n+            int nextIx = countdown.decrementAndGet();\n+            if (nextIx < 0) break;\n+            conf.set(Utilities.INPUT_NAME, \"Input \" + nextIx);\n+            results[nextIx] = IOContextMap.get(conf);\n+            if (nextIx == 0) break;\n+          }\n+          phase1End.countDown();\n+          phase1End.await();\n+          // Phase 2 - verify we get the expected objects created by all threads.\n+          for (int i = 0; i < ITER_COUNT; ++i) {\n+            conf.set(Utilities.INPUT_NAME, \"Input \" + i);\n+            IOContext ctx = IOContextMap.get(conf);\n+            assertSame(results[i], ctx);\n+          }\n+          return null;\n+        }\n+      });\n+      executor.execute(tasks[i]);\n+    }\n+\n+    cdlIn.await(); // Wait for all threads to be ready.\n+    cdlOut.countDown(); // Release them at the same time.\n+    for (int i = 0; i < tasks.length; ++i) {\n+      tasks[i].get();\n+    }\n+    Set<IOContext> resultSet = Sets.newIdentityHashSet();\n+    for (int i = 0; i < results.length; ++i) {\n+      assertTrue(resultSet.add(results[i])); // All the objects must be different.\n+    }\n+  }\n+\n+  @Test\n+  public void testTezLlapAttemptMap() throws Exception {\n+    // Tests that different threads get the same object per attempt per input, and different\n+    // between attempts/inputs; that attempt is inherited between threads; and that clearing\n+    // the attempt produces a different result.\n+    final int THREAD_COUNT = 2, ITER_COUNT = 1000, ATTEMPT_COUNT = 3;\n+    final AtomicInteger countdown = new AtomicInteger(ITER_COUNT);\n+    final IOContext[] results = new IOContext[ITER_COUNT * ATTEMPT_COUNT];\n+    ExecutorService executor = Executors.newFixedThreadPool(THREAD_COUNT);\n+    final CountDownLatch cdlIn = new CountDownLatch(THREAD_COUNT), cdlOut = new CountDownLatch(1);\n+\n+    @SuppressWarnings(\"unchecked\")\n+    FutureTask<Void>[] tasks = new FutureTask[THREAD_COUNT];\n+    for (int i = 0; i < tasks.length; ++i) {\n+      tasks[i] = new FutureTask<Void>(new Callable<Void>() {\n+        public Void call() throws Exception {\n+          final Configuration conf = new Configuration(), conf2 = new Configuration();\n+          syncThreadStart(cdlIn, cdlOut);\n+          while (true) {\n+            int nextIx = countdown.decrementAndGet();\n+            if (nextIx < 0) break;\n+            String input1 = \"Input \" + nextIx;\n+            conf.set(Utilities.INPUT_NAME, input1);\n+            for (int j = 0; j < ATTEMPT_COUNT; ++j) {\n+              String attemptId = \"Attempt \" + nextIx + \":\" + j;\n+              IOContextMap.setThreadAttemptId(attemptId);\n+              final IOContext r1 = results[(nextIx * ATTEMPT_COUNT) + j] = IOContextMap.get(conf);\n+              // For some attempts, check inheritance.\n+              if ((nextIx % (ITER_COUNT / 10)) == 0) {\n+                String input2 = \"Input2 \" + nextIx;\n+                conf2.set(Utilities.INPUT_NAME, input2);\n+                final AtomicReference<IOContext> ref2 = new AtomicReference<>();\n+                Thread t = new Thread(new Runnable() {\n+                  public void run() {\n+                    assertSame(r1, IOContextMap.get(conf));\n+                    ref2.set(IOContextMap.get(conf2));\n+                  }\n+                });\n+                t.start();\n+                t.join();\n+                assertSame(ref2.get(), IOContextMap.get(conf2));\n+              }\n+              // Don't clear the attempt ID, or the stuff will be cleared.\n+            }\n+            if (nextIx == 0) break;\n+          }\n+          return null;\n+        }\n+      });\n+      executor.execute(tasks[i]);\n+    }\n+\n+    cdlIn.await(); // Wait for all threads to be ready.\n+    cdlOut.countDown(); // Release them at the same time.\n+    for (int i = 0; i < tasks.length; ++i) {\n+      tasks[i].get();\n+    }\n+    Configuration conf = new Configuration();\n+    Set<IOContext> resultSet = Sets.newIdentityHashSet();\n+    for (int i = 0; i < ITER_COUNT; ++i) {\n+      conf.set(Utilities.INPUT_NAME, \"Input \" + i);\n+      for (int j = 0; j < ATTEMPT_COUNT; ++j) {\n+        String attemptId = \"Attempt \" + i + \":\" + j;\n+        IOContext result = results[(i * ATTEMPT_COUNT) + j];\n+        assertTrue(resultSet.add(result)); // All the objects must be different.\n+        IOContextMap.setThreadAttemptId(attemptId);\n+        assertSame(result, IOContextMap.get(conf)); // Matching result for attemptId + input.\n+        IOContextMap.clearThreadAttempt(attemptId);\n+        IOContextMap.setThreadAttemptId(attemptId);\n+        assertNotSame(result, IOContextMap.get(conf)); // Different result after clearing.\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testSparkThreadLocal() throws Exception {\n+    // Test that input name does not change IOContext returned, and that each thread gets its own.\n+    final Configuration conf1 = new Configuration();\n+    conf1.set(HiveConf.ConfVars.HIVE_EXECUTION_ENGINE.varname, \"spark\");\n+    final Configuration conf2 = new Configuration(conf1);\n+    conf2.set(Utilities.INPUT_NAME, \"Other input\");\n+    final int THREAD_COUNT = 2;\n+    ExecutorService executor = Executors.newFixedThreadPool(THREAD_COUNT);\n+    final CountDownLatch cdlIn = new CountDownLatch(THREAD_COUNT), cdlOut = new CountDownLatch(1);\n+    @SuppressWarnings(\"unchecked\")\n+    FutureTask<IOContext>[] tasks = new FutureTask[THREAD_COUNT];\n+    for (int i = 0; i < tasks.length; ++i) {\n+      tasks[i] = new FutureTask<IOContext>(new Callable<IOContext>() {\n+        public IOContext call() throws Exception {\n+          syncThreadStart(cdlIn, cdlOut);\n+          IOContext c1 = IOContextMap.get(conf1), c2 = IOContextMap.get(conf2);\n+          assertSame(c1, c2);\n+          return c1;\n+        }\n+      });\n+      executor.execute(tasks[i]);\n+    }\n+\n+    cdlIn.await(); // Wait for all threads to be ready.\n+    cdlOut.countDown(); // Release them at the same time.\n+    Set<IOContext> results = Sets.newIdentityHashSet();\n+    for (int i = 0; i < tasks.length; ++i) {\n+      assertTrue(results.add(tasks[i].get())); // All the objects must be different.\n+    }\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/hive/raw/c20b0866824ff5ae7ce8ca0e2ae236a5af908917/ql/src/test/org/apache/hadoop/hive/ql/io/TestIOContextMap.java",
                "sha": "dad55360d5f72bd1b645283b59a2c515bc2040ec",
                "status": "added"
            }
        ],
        "message": "HIVE-11015 : LLAP: MiniTez tez_smb_main, tez_bmj_schema_evolution fail with NPE (Sergey Shelukhin, reviewed by Vikram Dixit K)",
        "parent": "https://github.com/apache/hive/commit/638597af7f11f22a3896df7a2ba19bbba7e699f8",
        "patched_files": [
            "FilterOperator.java",
            "IOContext.java",
            "MapRecordProcessor.java",
            "HiveContextAwareRecordReader.java",
            "ExecMapperContext.java",
            "HiveInputFormat.java",
            "TaskRunnerCallable.java",
            "IOContextMap.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestOperators.java",
            "TestHiveBinarySearchRecordReader.java",
            "TestIOContextMap.java"
        ]
    },
    "hive_c29038a": {
        "bug_id": "hive_c29038a",
        "commit": "https://github.com/apache/hive/commit/c29038af9bc237bc82b83abb4f1370017a8cd379",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/c29038af9bc237bc82b83abb4f1370017a8cd379/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDe.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDe.java?ref=c29038af9bc237bc82b83abb4f1370017a8cd379",
                "deletions": 2,
                "filename": "druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDe.java",
                "patch": "@@ -357,9 +357,13 @@ protected SegmentAnalysis submitMetadataRequest(String address, SegmentMetadataQ\n     assert values.size() > granularityFieldIndex;\n     Preconditions.checkArgument(\n         fields.get(granularityFieldIndex).getFieldName().equals(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME));\n-    value.put(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME,\n+\n+    Timestamp timestamp =\n         ((TimestampObjectInspector) fields.get(granularityFieldIndex).getFieldObjectInspector())\n-            .getPrimitiveJavaObject(values.get(granularityFieldIndex)).toEpochMilli());\n+            .getPrimitiveJavaObject(values.get(granularityFieldIndex));\n+    Preconditions.checkNotNull(timestamp, \"Timestamp column cannot have null value\");\n+    value.put(Constants.DRUID_TIMESTAMP_GRANULARITY_COL_NAME, timestamp.toEpochMilli());\n+\n     if (values.size() == columns.length + 2) {\n       // Then partition number if any.\n       final int partitionNumPos = granularityFieldIndex + 1;",
                "raw_url": "https://github.com/apache/hive/raw/c29038af9bc237bc82b83abb4f1370017a8cd379/druid-handler/src/java/org/apache/hadoop/hive/druid/serde/DruidSerDe.java",
                "sha": "cf37e37c1517d35f7e40e169a447ae36cc3bc202",
                "status": "modified"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/hive/blob/c29038af9bc237bc82b83abb4f1370017a8cd379/druid-handler/src/test/org/apache/hadoop/hive/druid/serde/TestDruidSerDe.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/druid-handler/src/test/org/apache/hadoop/hive/druid/serde/TestDruidSerDe.java?ref=c29038af9bc237bc82b83abb4f1370017a8cd379",
                "deletions": 0,
                "filename": "druid-handler/src/test/org/apache/hadoop/hive/druid/serde/TestDruidSerDe.java",
                "patch": "@@ -74,6 +74,7 @@\n import org.apache.hadoop.io.NullWritable;\n import org.apache.hadoop.io.Text;\n import org.junit.Before;\n+import org.junit.Rule;\n import org.junit.Test;\n \n import com.fasterxml.jackson.core.JsonParseException;\n@@ -90,6 +91,7 @@\n import io.druid.query.select.SelectResultValue;\n import io.druid.query.timeseries.TimeseriesResultValue;\n import io.druid.query.topn.TopNResultValue;\n+import org.junit.rules.ExpectedException;\n \n /**\n  * Basic tests for Druid SerDe. The examples are taken from Druid 0.9.1.1\n@@ -860,6 +862,38 @@ public void testDruidObjectSerializer()\n     serializeObject(tbl, serDe, ROW_OBJECT, DRUID_WRITABLE);\n   }\n \n+  @Rule\n+  public ExpectedException expectedEx = ExpectedException.none();\n+\n+  @Test\n+  public void testDruidObjectSerializerwithNullTimestamp()\n+      throws Exception {\n+    // Create, initialize, and test the SerDe\n+    DruidSerDe serDe = new DruidSerDe();\n+    Configuration conf = new Configuration();\n+    Properties tbl;\n+    // Mixed source (all types)\n+    tbl = createPropertiesSource(COLUMN_NAMES, COLUMN_TYPES);\n+    SerDeUtils.initializeSerDe(serDe, conf, tbl, null);\n+    Object[] row = new Object[] {\n+        null,\n+        new Text(\"dim1_val\"),\n+        new HiveCharWritable(new HiveChar(\"dim2_v\", 6)),\n+        new HiveVarcharWritable(new HiveVarchar(\"dim3_val\", 8)),\n+        new DoubleWritable(10669.3D),\n+        new FloatWritable(10669.45F),\n+        new LongWritable(1113939),\n+        new IntWritable(1112123),\n+        new ShortWritable((short) 12),\n+        new ByteWritable((byte) 0),\n+        null // granularity\n+    };\n+    expectedEx.expect(NullPointerException.class);\n+    expectedEx.expectMessage(\"Timestamp column cannot have null value\");\n+    // should fail as timestamp is null\n+    serializeObject(tbl, serDe, row, DRUID_WRITABLE);\n+  }\n+\n   private static Properties createPropertiesSource(String columnNames, String columnTypes) {\n     Properties tbl = new Properties();\n ",
                "raw_url": "https://github.com/apache/hive/raw/c29038af9bc237bc82b83abb4f1370017a8cd379/druid-handler/src/test/org/apache/hadoop/hive/druid/serde/TestDruidSerDe.java",
                "sha": "acde2394d7fb10cfe6f8c1c1349361450f155afb",
                "status": "modified"
            }
        ],
        "message": "HIVE-20698 : Add better message for NPE when inserting rows with null timestamp to druid (Nishant Bangarwa via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>",
        "parent": "https://github.com/apache/hive/commit/9d522216972598a38b6750eb9b5d4af1b79cd6ba",
        "patched_files": [
            "DruidSerDe.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestDruidSerDe.java"
        ]
    },
    "hive_c2e335f": {
        "bug_id": "hive_c2e335f",
        "commit": "https://github.com/apache/hive/commit/c2e335fc0b4a8144d8d93ff10e9191432ae6547e",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/c2e335fc0b4a8144d8d93ff10e9191432ae6547e/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java?ref=c2e335fc0b4a8144d8d93ff10e9191432ae6547e",
                "deletions": 2,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java",
                "patch": "@@ -294,8 +294,13 @@ public int execute(DriverContext driverContext) {\n             //'sourcePath' result of 'select ...' part of CTAS statement\n             assert lfd.getIsDfsDir();\n             FileSystem srcFs = sourcePath.getFileSystem(conf);\n-            List<Path> newFiles = new ArrayList<>();\n-            Hive.moveAcidFiles(srcFs, srcFs.globStatus(sourcePath), targetPath, newFiles);\n+            FileStatus[] srcs = srcFs.globStatus(sourcePath);\n+            if(srcs != null) {\n+              List<Path> newFiles = new ArrayList<>();\n+              Hive.moveAcidFiles(srcFs, srcs, targetPath, newFiles);\n+            } else {\n+              LOG.debug(\"No files found to move from \" + sourcePath + \" to \" + targetPath);\n+            }\n           }\n           else {\n             moveFile(sourcePath, targetPath, lfd.getIsDfsDir());",
                "raw_url": "https://github.com/apache/hive/raw/c2e335fc0b4a8144d8d93ff10e9191432ae6547e/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java",
                "sha": "4e804ba04b2ee916f7867a8a4a439d7c467f2523",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hive/blob/c2e335fc0b4a8144d8d93ff10e9191432ae6547e/ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java?ref=c2e335fc0b4a8144d8d93ff10e9191432ae6547e",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java",
                "patch": "@@ -178,6 +178,7 @@ public void testNoBuckets() throws Exception {\n    */\n   @Test\n   public void testCTAS() throws Exception {\n+    runStatementOnDriver(\"drop table if exists myctas\");\n     int[][] values = {{1,2},{3,4}};\n     runStatementOnDriver(\"insert into \" + Table.NONACIDORCTBL +  makeValuesClause(values));\n     runStatementOnDriver(\"create table myctas stored as ORC TBLPROPERTIES ('transactional\" +\n@@ -221,6 +222,16 @@ public void testCTAS() throws Exception {\n     };\n     checkExpected(rs, expected4, \"Unexpected row count after ctas from union distinct query\");\n   }\n+  @Test\n+  public void testCtasEmpty() throws Exception {\n+    MetastoreConf.setBoolVar(hiveConf, MetastoreConf.ConfVars.CREATE_TABLES_AS_ACID, true);\n+    runStatementOnDriver(\"drop table if exists myctas\");\n+    runStatementOnDriver(\"create table myctas stored as ORC as\" +\n+        \" select a, b from \" + Table.NONACIDORCTBL);\n+    List<String> rs = runStatementOnDriver(\"select ROW__ID, a, b, INPUT__FILE__NAME\" +\n+        \" from myctas order by ROW__ID\");\n+  }\n+\n   /**\n    * Insert into unbucketed acid table from union all query\n    * Union All is flattend so nested subdirs are created and acid move drops them since",
                "raw_url": "https://github.com/apache/hive/raw/c2e335fc0b4a8144d8d93ff10e9191432ae6547e/ql/src/test/org/apache/hadoop/hive/ql/TestTxnNoBuckets.java",
                "sha": "3c6b6be1ba6c5c4b522f31e90dda0b63d0bb8d3a",
                "status": "modified"
            }
        ],
        "message": "HIVE-18606 CTAS on empty table throws NPE from org.apache.hadoop.hive.ql.exec.MoveTask (Eugene Koifman, reviewed by Sergey Shelukhin)",
        "parent": "https://github.com/apache/hive/commit/464a3f61a0c4a1c4e44a1ce427f604295534e969",
        "patched_files": [
            "MoveTask.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestTxnNoBuckets.java"
        ]
    },
    "hive_c50a627": {
        "bug_id": "hive_c50a627",
        "commit": "https://github.com/apache/hive/commit/c50a627fe687027d08413a7aed9ec83e101b9ec2",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/c50a627fe687027d08413a7aed9ec83e101b9ec2/common/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/common/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java?ref=c50a627fe687027d08413a7aed9ec83e101b9ec2",
                "deletions": 4,
                "filename": "common/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java",
                "patch": "@@ -254,16 +254,16 @@ public static BigDecimal enforcePrecisionScale(BigDecimal bd, int maxPrecision,\n       return null;\n     }\n \n+    if (bd.scale() > maxScale) {\n+      bd = bd.setScale(maxScale, RoundingMode.HALF_UP);\n+    }\n+\n     int maxIntDigits = maxPrecision - maxScale;\n     int intDigits = bd.precision() - bd.scale();\n     if (intDigits > maxIntDigits) {\n       return null;\n     }\n \n-    if (bd.scale() > maxScale) {\n-      bd = bd.setScale(maxScale, RoundingMode.HALF_UP);\n-    }\n-\n     return bd;\n   }\n }",
                "raw_url": "https://github.com/apache/hive/raw/c50a627fe687027d08413a7aed9ec83e101b9ec2/common/src/java/org/apache/hadoop/hive/common/type/HiveDecimal.java",
                "sha": "ad48f69c6fc72e09fd0ce03e50a658d8a3e4a342",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/c50a627fe687027d08413a7aed9ec83e101b9ec2/common/src/test/org/apache/hadoop/hive/common/type/TestHiveDecimal.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/common/src/test/org/apache/hadoop/hive/common/type/TestHiveDecimal.java?ref=c50a627fe687027d08413a7aed9ec83e101b9ec2",
                "deletions": 0,
                "filename": "common/src/test/org/apache/hadoop/hive/common/type/TestHiveDecimal.java",
                "patch": "@@ -68,6 +68,13 @@ public void testPrecisionScaleEnforcement() {\n     Assert.assertEquals(\"0.02\", HiveDecimal.enforcePrecisionScale(new BigDecimal(\"0.015\"), 3, 2).toString());\n     Assert.assertEquals(\"0.01\", HiveDecimal.enforcePrecisionScale(new BigDecimal(\"0.0145\"), 3, 2).toString());\n \n+    // Rounding numbers that increase int digits\n+    Assert.assertEquals(\"10\",\n+        HiveDecimal.enforcePrecisionScale(new BigDecimal(\"9.5\"), 2, 0).toString());\n+    Assert.assertNull(HiveDecimal.enforcePrecisionScale(new BigDecimal(\"9.5\"), 1, 0));\n+    Assert.assertEquals(\"9\",\n+        HiveDecimal.enforcePrecisionScale(new BigDecimal(\"9.4\"), 1, 0).toString());\n+\n     // Integers with no scale values are not modified (zeros are not null)\n     Assert.assertEquals(\"0\", HiveDecimal.enforcePrecisionScale(new BigDecimal(\"0\"), 1, 0).toString());\n     Assert.assertEquals(\"30\", HiveDecimal.enforcePrecisionScale(new BigDecimal(\"30\"), 2, 0).toString());",
                "raw_url": "https://github.com/apache/hive/raw/c50a627fe687027d08413a7aed9ec83e101b9ec2/common/src/test/org/apache/hadoop/hive/common/type/TestHiveDecimal.java",
                "sha": "46a73f2eba706bf413dde31a26e302b3e4c7bd65",
                "status": "modified"
            }
        ],
        "message": "HIVE-8008: NPE while reading null decimal value (Chao via Xuefu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1623261 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/3bebb7898ec5496502bffcda7993523da0585aa1",
        "patched_files": [
            "HiveDecimal.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveDecimal.java"
        ]
    },
    "hive_c6bc891": {
        "bug_id": "hive_c6bc891",
        "commit": "https://github.com/apache/hive/commit/c6bc891dae978c8e9bdd1b60538b5096c7ab4798",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java?ref=c6bc891dae978c8e9bdd1b60538b5096c7ab4798",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java",
                "patch": "@@ -105,6 +105,7 @@ public DDLTask() {\n     super();\n   }\n \n+  @Override\n   public void initialize(HiveConf conf, QueryPlan queryPlan, DriverContext ctx) {\n     super.initialize(conf, queryPlan, ctx);\n     this.conf = conf;",
                "raw_url": "https://github.com/apache/hive/raw/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java",
                "sha": "666bd33aa70b71de7c40c111513fe65d8cbeb301",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java?ref=c6bc891dae978c8e9bdd1b60538b5096c7ab4798",
                "deletions": 2,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java",
                "patch": "@@ -52,6 +52,7 @@\n import org.apache.hadoop.hive.ql.exec.FileSinkOperator.RecordWriter;\n import org.apache.hadoop.hive.ql.history.HiveHistory.Keys;\n import org.apache.hadoop.hive.ql.io.*;\n+import org.apache.hadoop.hive.ql.DriverContext;\n import org.apache.hadoop.hive.ql.QueryPlan;\n import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;\n import org.apache.hadoop.hive.ql.session.SessionState;\n@@ -107,8 +108,9 @@ private void initializeFiles(String prop, String files) {\n   /**\n    * Initialization when invoked from QL\n    */\n-  public void initialize(HiveConf conf, QueryPlan queryPlan) {\n-    super.initialize(conf, queryPlan, null);\n+  @Override\n+  public void initialize(HiveConf conf, QueryPlan queryPlan, DriverContext driverContext) {\n+    super.initialize(conf, queryPlan, driverContext);\n     job = new JobConf(conf, ExecDriver.class);\n     // NOTE: initialize is only called if it is in non-local mode.\n     // In case it's in non-local mode, we need to move the SessionState files",
                "raw_url": "https://github.com/apache/hive/raw/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/ExecDriver.java",
                "sha": "315cec491337e375a9ca2ba55b893f4941e8108e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java?ref=c6bc891dae978c8e9bdd1b60538b5096c7ab4798",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java",
                "patch": "@@ -51,6 +51,7 @@ public FetchTask() {\n  \t  super();\n  \t}\n  \t\n+ \t@Override\n   public void initialize (HiveConf conf, QueryPlan queryPlan, DriverContext ctx) {\n     super.initialize(conf, queryPlan, ctx);\n     ",
                "raw_url": "https://github.com/apache/hive/raw/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/FetchTask.java",
                "sha": "b56fed061110704d213746e12607822defc12fb7",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionTask.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionTask.java?ref=c6bc891dae978c8e9bdd1b60538b5096c7ab4798",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionTask.java",
                "patch": "@@ -45,6 +45,7 @@ public FunctionTask() {\n     super();\n   }\n   \n+  @Override\n   public void initialize(HiveConf conf, QueryPlan queryPlan, DriverContext ctx) {\n     super.initialize(conf, queryPlan, ctx);\n     this.conf = conf;",
                "raw_url": "https://github.com/apache/hive/raw/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/FunctionTask.java",
                "sha": "5eb4f205026979a7ab0969001d8f9844a3551b7a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java?ref=c6bc891dae978c8e9bdd1b60538b5096c7ab4798",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java",
                "patch": "@@ -68,7 +68,7 @@ public Task() {\n     this.taskCounters = new HashMap<String, Long>();\n   }\n \n-  public void initialize (HiveConf conf, QueryPlan queryPlan, DriverContext driverContext) {\n+  public void initialize(HiveConf conf, QueryPlan queryPlan, DriverContext driverContext) {\n     this.queryPlan = queryPlan;\n     isdone = false;\n     started = false;",
                "raw_url": "https://github.com/apache/hive/raw/c6bc891dae978c8e9bdd1b60538b5096c7ab4798/ql/src/java/org/apache/hadoop/hive/ql/exec/Task.java",
                "sha": "e767ededd4442fbe28804a04ce5705441ca73c6a",
                "status": "modified"
            }
        ],
        "message": "HIVE-1064: NPE when operating HIVE CLI in distributed mode (Carl Steinbach via Ning Zhang)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hive/trunk@901473 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/b306976522482371a6cbe52df7a152605f8e3c88",
        "patched_files": [
            "ExecDriver.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestExecDriver.java"
        ]
    },
    "hive_c7685c9": {
        "bug_id": "hive_c7685c9",
        "commit": "https://github.com/apache/hive/commit/c7685c931e3ca1e57d7fc232cba49905ebf874db",
        "file": [
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/hive/blob/c7685c931e3ca1e57d7fc232cba49905ebf874db/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java",
                "changes": 63,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java?ref=c7685c931e3ca1e57d7fc232cba49905ebf874db",
                "deletions": 25,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java",
                "patch": "@@ -35,9 +35,7 @@\n import org.apache.hadoop.hive.ql.exec.vector.expressions.ConstantVectorExpression;\n import org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprAndExpr;\n import org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprOrExpr;\n-import org.apache.hadoop.hive.ql.exec.vector.expressions.FilterNotExpr;\n import org.apache.hadoop.hive.ql.exec.vector.expressions.IdentityExpression;\n-import org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsFalse;\n import org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNotNull;\n import org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsNull;\n import org.apache.hadoop.hive.ql.exec.vector.expressions.SelectColumnIsTrue;\n@@ -159,7 +157,7 @@ int allocateOutputColumn(String columnType) {\n     private int allocateOutputColumnInternal(String columnType) {\n       for (int i = 0; i < outputColCount; i++) {\n         if (usedOutputColumns.contains(i) ||\n-            !(outputColumnsTypes)[i].equals(columnType)) {\n+            !(outputColumnsTypes)[i].equalsIgnoreCase(columnType)) {\n           continue;\n         }\n         //Use i\n@@ -324,13 +322,15 @@ private VectorExpression getUnaryMinusExpression(List<ExprNodeDesc> childExprLis\n       colType = v1.getOutputType();\n     } else if (childExpr instanceof ExprNodeColumnDesc) {\n       ExprNodeColumnDesc colDesc = (ExprNodeColumnDesc) childExpr;\n-      inputCol = columnMap.get(colDesc.getColumn());\n+      inputCol = getInputColumnIndex(colDesc.getColumn());\n       colType = colDesc.getTypeString();\n     } else {\n       throw new HiveException(\"Expression not supported: \"+childExpr);\n     }\n-    int outputCol = ocm.allocateOutputColumn(colType);\n-    String className = getNormalizedTypeName(colType) + \"colUnaryMinus\";\n+    String outputColumnType = getNormalizedTypeName(colType);\n+    int outputCol = ocm.allocateOutputColumn(outputColumnType);\n+    String className = \"org.apache.hadoop.hive.ql.exec.vector.expressions.gen.\"\n+       + outputColumnType + \"ColUnaryMinus\";\n     VectorExpression expr;\n     try {\n       expr = (VectorExpression) Class.forName(className).\n@@ -685,16 +685,7 @@ private VectorExpression getVectorExpression(GenericUDFOPOr udf,\n \n   private VectorExpression getVectorExpression(GenericUDFOPNot udf,\n       List<ExprNodeDesc> childExpr) throws HiveException {\n-    ExprNodeDesc expr = childExpr.get(0);\n-    if (expr instanceof ExprNodeColumnDesc) {\n-      ExprNodeColumnDesc colDesc = (ExprNodeColumnDesc) expr;\n-      int inputCol = getInputColumnIndex(colDesc.getColumn());\n-      VectorExpression ve = new SelectColumnIsFalse(inputCol);\n-      return ve;\n-    } else {\n-      VectorExpression ve = getVectorExpression(expr);\n-      return new FilterNotExpr(ve);\n-    }\n+    throw new HiveException(\"Not is not supported\");\n   }\n \n   private VectorExpression getVectorExpression(GenericUDFOPAnd udf,\n@@ -916,7 +907,8 @@ private VectorExpression getVectorBinaryComparisonFilterExpression(String\n     return expr;\n   }\n \n-  private String getNormalizedTypeName(String colType) {\n+  private String getNormalizedTypeName(String colType) throws HiveException {\n+    validateInputType(colType);\n     String normalizedType = null;\n     if (colType.equalsIgnoreCase(\"Double\") || colType.equalsIgnoreCase(\"Float\")) {\n       normalizedType = \"Double\";\n@@ -929,7 +921,7 @@ private String getNormalizedTypeName(String colType) {\n   }\n \n   private String getFilterColumnColumnExpressionClassName(String colType1,\n-      String colType2, String opName) {\n+      String colType2, String opName) throws HiveException {\n     StringBuilder b = new StringBuilder();\n     b.append(\"org.apache.hadoop.hive.ql.exec.vector.expressions.gen.\");\n     if (opType.equals(OperatorType.FILTER)) {\n@@ -944,7 +936,7 @@ private String getFilterColumnColumnExpressionClassName(String colType1,\n   }\n \n   private String getFilterColumnScalarExpressionClassName(String colType, String\n-      scalarType, String opName) {\n+      scalarType, String opName) throws HiveException {\n     StringBuilder b = new StringBuilder();\n     b.append(\"org.apache.hadoop.hive.ql.exec.vector.expressions.gen.\");\n     if (opType.equals(OperatorType.FILTER)) {\n@@ -959,7 +951,7 @@ private String getFilterColumnScalarExpressionClassName(String colType, String\n   }\n \n   private String getFilterScalarColumnExpressionClassName(String colType, String\n-      scalarType, String opName) {\n+      scalarType, String opName) throws HiveException {\n     StringBuilder b = new StringBuilder();\n     b.append(\"org.apache.hadoop.hive.ql.exec.vector.expressions.gen.\");\n     if (opType.equals(OperatorType.FILTER)) {\n@@ -974,7 +966,7 @@ private String getFilterScalarColumnExpressionClassName(String colType, String\n   }\n \n   private String getBinaryColumnScalarExpressionClassName(String colType,\n-      String scalarType, String method) {\n+      String scalarType, String method) throws HiveException {\n     StringBuilder b = new StringBuilder();\n     String normColType = getNormalizedTypeName(colType);\n     String normScalarType = getNormalizedTypeName(scalarType);\n@@ -993,7 +985,7 @@ private String getBinaryColumnScalarExpressionClassName(String colType,\n   }\n \n   private String getBinaryScalarColumnExpressionClassName(String colType,\n-      String scalarType, String method) {\n+      String scalarType, String method) throws HiveException {\n     StringBuilder b = new StringBuilder();\n     String normColType = getNormalizedTypeName(colType);\n     String normScalarType = getNormalizedTypeName(scalarType);\n@@ -1012,7 +1004,7 @@ private String getBinaryScalarColumnExpressionClassName(String colType,\n   }\n \n   private String getBinaryColumnColumnExpressionClassName(String colType1,\n-      String colType2, String method) {\n+      String colType2, String method) throws HiveException {\n     StringBuilder b = new StringBuilder();\n     String normColType1 = getNormalizedTypeName(colType1);\n     String normColType2 = getNormalizedTypeName(colType2);\n@@ -1030,7 +1022,10 @@ private String getBinaryColumnColumnExpressionClassName(String colType1,\n     return b.toString();\n   }\n \n-  private String getOutputColType(String inputType1, String inputType2, String method) {\n+  private String getOutputColType(String inputType1, String inputType2, String method)\n+      throws HiveException {\n+    validateInputType(inputType1);\n+    validateInputType(inputType2);\n     if (method.equalsIgnoreCase(\"divide\") || inputType1.equalsIgnoreCase(\"double\") ||\n         inputType2.equalsIgnoreCase(\"double\") || inputType1.equalsIgnoreCase(\"float\") ||\n         inputType2.equalsIgnoreCase(\"float\")) {\n@@ -1044,7 +1039,25 @@ private String getOutputColType(String inputType1, String inputType2, String met\n     }\n   }\n \n-  private String getOutputColType(String inputType, String method) {\n+  private void validateInputType(String inputType) throws HiveException {\n+    if (! (inputType.equalsIgnoreCase(\"float\") ||\n+        inputType.equalsIgnoreCase(\"double\") ||\n+        inputType.equalsIgnoreCase(\"string\") ||\n+        inputType.equalsIgnoreCase(\"tinyint\") ||\n+            inputType.equalsIgnoreCase(\"smallint\") ||\n+            inputType.equalsIgnoreCase(\"short\") ||\n+            inputType.equalsIgnoreCase(\"byte\") ||\n+            inputType.equalsIgnoreCase(\"int\") ||\n+            inputType.equalsIgnoreCase(\"long\") ||\n+            inputType.equalsIgnoreCase(\"bigint\") ||\n+            inputType.equalsIgnoreCase(\"boolean\") ||\n+            inputType.equalsIgnoreCase(\"timestamp\") ) ) {\n+      throw new HiveException(\"Unsupported input type: \"+inputType);\n+    }\n+  }\n+\n+  private String getOutputColType(String inputType, String method) throws HiveException {\n+    validateInputType(inputType);\n     if (inputType.equalsIgnoreCase(\"float\") || inputType.equalsIgnoreCase(\"double\")) {\n       return \"double\";\n     } else if (inputType.equalsIgnoreCase(\"string\")) {",
                "raw_url": "https://github.com/apache/hive/raw/c7685c931e3ca1e57d7fc232cba49905ebf874db/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizationContext.java",
                "sha": "16b45ddbc0f1cdf9681a9481b85afb3c9ca5d4cf",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hive/blob/c7685c931e3ca1e57d7fc232cba49905ebf874db/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorizationContext.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorizationContext.java?ref=c7685c931e3ca1e57d7fc232cba49905ebf874db",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorizationContext.java",
                "patch": "@@ -11,13 +11,15 @@\n import org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprAndExpr;\n import org.apache.hadoop.hive.ql.exec.vector.expressions.FilterExprOrExpr;\n import org.apache.hadoop.hive.ql.exec.vector.expressions.VectorExpression;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.DoubleColUnaryMinus;\n import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterDoubleColLessDoubleScalar;\n import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterLongColGreaterLongScalar;\n import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.FilterStringColGreaterStringScalar;\n import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColAddLongColumn;\n import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColModuloLongColumn;\n import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColMultiplyLongColumn;\n import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColSubtractLongColumn;\n+import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongColUnaryMinus;\n import org.apache.hadoop.hive.ql.exec.vector.expressions.gen.LongScalarSubtractLongColumn;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n@@ -28,6 +30,7 @@\n import org.apache.hadoop.hive.ql.udf.UDFOPMinus;\n import org.apache.hadoop.hive.ql.udf.UDFOPMod;\n import org.apache.hadoop.hive.ql.udf.UDFOPMultiply;\n+import org.apache.hadoop.hive.ql.udf.UDFOPNegative;\n import org.apache.hadoop.hive.ql.udf.UDFOPPlus;\n import org.apache.hadoop.hive.ql.udf.generic.GenericUDF;\n import org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge;\n@@ -287,4 +290,42 @@ public void testFilterWithNegativeScalar() throws HiveException {\n \n     assertTrue(ve instanceof FilterLongColGreaterLongScalar);\n   }\n+\n+  @Test\n+  public void testUnaryMinusColumnLong() throws HiveException {\n+    ExprNodeColumnDesc col1Expr = new  ExprNodeColumnDesc(Integer.class, \"col1\", \"table\", false);\n+    ExprNodeGenericFuncDesc negExprDesc = new ExprNodeGenericFuncDesc();\n+    GenericUDF gudf = new GenericUDFBridge(\"-\", true, UDFOPNegative.class);\n+    negExprDesc.setGenericUDF(gudf);\n+    List<ExprNodeDesc> children = new ArrayList<ExprNodeDesc>(1);\n+    children.add(col1Expr);\n+    negExprDesc.setChildExprs(children);\n+    Map<String, Integer> columnMap = new HashMap<String, Integer>();\n+    columnMap.put(\"col1\", 1);\n+    VectorizationContext vc = new VectorizationContext(columnMap, 1);\n+    vc.setOperatorType(OperatorType.SELECT);\n+\n+    VectorExpression ve = vc.getVectorExpression(negExprDesc);\n+\n+    assertTrue( ve instanceof LongColUnaryMinus);\n+  }\n+\n+  @Test\n+  public void testUnaryMinusColumnDouble() throws HiveException {\n+    ExprNodeColumnDesc col1Expr = new  ExprNodeColumnDesc(Float.class, \"col1\", \"table\", false);\n+    ExprNodeGenericFuncDesc negExprDesc = new ExprNodeGenericFuncDesc();\n+    GenericUDF gudf = new GenericUDFBridge(\"-\", true, UDFOPNegative.class);\n+    negExprDesc.setGenericUDF(gudf);\n+    List<ExprNodeDesc> children = new ArrayList<ExprNodeDesc>(1);\n+    children.add(col1Expr);\n+    negExprDesc.setChildExprs(children);\n+    Map<String, Integer> columnMap = new HashMap<String, Integer>();\n+    columnMap.put(\"col1\", 1);\n+    VectorizationContext vc = new VectorizationContext(columnMap, 1);\n+    vc.setOperatorType(OperatorType.SELECT);\n+\n+    VectorExpression ve = vc.getVectorExpression(negExprDesc);\n+\n+    assertTrue( ve instanceof DoubleColUnaryMinus);\n+  }\n }",
                "raw_url": "https://github.com/apache/hive/raw/c7685c931e3ca1e57d7fc232cba49905ebf874db/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/TestVectorizationContext.java",
                "sha": "f7bfce16ec4ebe607ff9baaf80b797907d16318d",
                "status": "modified"
            }
        ],
        "message": "HIVE-4744 : Unary Minus Expression Throwing java.lang.NullPointerException (Jitendra Nath Pandey via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/vectorization@1494786 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/70195fe39dd3fe0960852ab2de5aa84e17957f6c",
        "patched_files": [
            "VectorizationContext.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestVectorizationContext.java"
        ]
    },
    "hive_c7d2643": {
        "bug_id": "hive_c7d2643",
        "commit": "https://github.com/apache/hive/commit/c7d2643e0025d6fa16744872c7a131f4b5a377e6",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/c7d2643e0025d6fa16744872c7a131f4b5a377e6/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/RelOptHiveTable.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/RelOptHiveTable.java?ref=c7d2643e0025d6fa16744872c7a131f4b5a377e6",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/RelOptHiveTable.java",
                "patch": "@@ -131,6 +131,9 @@ public double getRowCount() {\n       }\n     }\n \n+    if (rowCount == -1)\n+      noColsMissingStats.getAndIncrement();\n+\n     return rowCount;\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/c7d2643e0025d6fa16744872c7a131f4b5a377e6/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/RelOptHiveTable.java",
                "sha": "ddef37be78d2ba64332d6b5eb963b98df4223290",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/c7d2643e0025d6fa16744872c7a131f4b5a377e6/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/reloperators/HiveProjectRel.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/reloperators/HiveProjectRel.java?ref=c7d2643e0025d6fa16744872c7a131f4b5a377e6",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/reloperators/HiveProjectRel.java",
                "patch": "@@ -86,7 +86,7 @@ public static HiveProjectRel create(RelNode child, List<? extends RexNode> exps,\n     RelOptCluster cluster = child.getCluster();\n \n     // 1 Ensure columnNames are unique - OPTIQ-411\n-    if (!Util.isDistinct(fieldNames)) {\n+    if (fieldNames != null && !Util.isDistinct(fieldNames)) {\n       String msg = \"Select list contains multiple expressions with the same name.\" + fieldNames;\n       throw new OptiqSemanticException(msg);\n     }",
                "raw_url": "https://github.com/apache/hive/raw/c7d2643e0025d6fa16744872c7a131f4b5a377e6/ql/src/java/org/apache/hadoop/hive/ql/optimizer/optiq/reloperators/HiveProjectRel.java",
                "sha": "c643aa46c484210fe348602ae41bfe2deccca2c5",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hive/blob/c7d2643e0025d6fa16744872c7a131f4b5a377e6/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=c7d2643e0025d6fa16744872c7a131f4b5a377e6",
                "deletions": 10,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "patch": "@@ -12139,6 +12139,7 @@ private boolean canHandleQuery(QB qbToChk, boolean topLevelQB) {\n     // 1. If top level QB is query then everything below it must also be Query\n     // 2. Nested Subquery will return false for qbToChk.getIsQuery()\n     if ((!topLevelQB || qbToChk.getIsQuery())\n+        && (!conf.getBoolVar(ConfVars.HIVE_IN_TEST) || conf.getVar(ConfVars.HIVEMAPREDMODE).equalsIgnoreCase(\"nonstrict\"))\n         && (!topLevelQB || (queryProperties.getJoinCount() > 1) || conf.getBoolVar(ConfVars.HIVE_IN_TEST))\n         && !queryProperties.hasClusterBy() && !queryProperties.hasDistributeBy()\n         && !queryProperties.hasSortBy() && !queryProperties.hasPTF()\n@@ -13870,7 +13871,6 @@ private RelNode genLogicalPlan(QB qb) throws SemanticException {\n         aliasToRel.put(tableAlias, op);\n       }\n \n-\n       if (aliasToRel.isEmpty()) {\n         //// This may happen for queries like select 1; (no source table)\n         // We can do following which is same, as what Hive does.\n@@ -13910,7 +13910,15 @@ private RelNode genLogicalPlan(QB qb) throws SemanticException {\n       selectRel = genSelectLogicalPlan(qb, srcRel);\n       srcRel = (selectRel == null) ? srcRel : selectRel;\n \n-      // 6. Incase this QB corresponds to subquery then modify its RR to point\n+      // 6. Build Rel for OB Clause\n+      obRel = genOBLogicalPlan(qb, srcRel);\n+      srcRel = (obRel == null) ? srcRel : obRel;\n+\n+      // 7. Build Rel for Limit Clause\n+      limitRel = genLimitLogicalPlan(qb, srcRel);\n+      srcRel = (limitRel == null) ? srcRel : limitRel;\n+\n+      // 8. Incase this QB corresponds to subquery then modify its RR to point\n       // to subquery alias\n       // TODO: cleanup this\n       if (qb.getParseInfo().getAlias() != null) {\n@@ -13932,14 +13940,6 @@ private RelNode genLogicalPlan(QB qb) throws SemanticException {\n         relToHiveColNameOptiqPosMap.put(srcRel, buildHiveToOptiqColumnMap(newRR, srcRel));\n       }\n \n-      // 7. Build Rel for OB Clause\n-      obRel = genOBLogicalPlan(qb, srcRel);\n-      srcRel = (obRel == null) ? srcRel : obRel;\n-\n-      // 8. Build Rel for Limit Clause\n-      limitRel = genLimitLogicalPlan(qb, srcRel);\n-      srcRel = (limitRel == null) ? srcRel : limitRel;\n-\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(\"Created Plan for Query Block \" + qb.getId());\n       }",
                "raw_url": "https://github.com/apache/hive/raw/c7d2643e0025d6fa16744872c7a131f4b5a377e6/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "sha": "0daad5bfea23f76c846e17f7c7d5f4940a1cce46",
                "status": "modified"
            }
        ],
        "message": "HIVE-8166 : CBO: 1) Bailout in strict mode 2) OB,LIMIT RR table alias is same as that of sub query 3) If RowCount Not found then fall back to non cbo 4)Fix NPE in unique col name check (John Pullokkaran via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/cbo@1625852 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/88e790ad42a829ba0492e4ecfaf05dc750798a0c",
        "patched_files": [
            "SemanticAnalyzer.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestSemanticAnalyzer.java"
        ]
    },
    "hive_c81bd2e": {
        "bug_id": "hive_c81bd2e",
        "commit": "https://github.com/apache/hive/commit/c81bd2ee50772d2434af04f580d8ddc4d2480746",
        "file": [
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hive/blob/c81bd2ee50772d2434af04f580d8ddc4d2480746/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java?ref=c81bd2ee50772d2434af04f580d8ddc4d2480746",
                "deletions": 8,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java",
                "patch": "@@ -17,14 +17,6 @@\n  */\n package org.apache.hadoop.hive.ql.io.orc;\n \n-import java.io.DataInput;\n-import java.io.DataOutput;\n-import java.io.IOException;\n-import java.util.ArrayList;\n-import java.util.HashMap;\n-import java.util.List;\n-import java.util.Map;\n-\n import org.apache.hadoop.hive.common.type.HiveDecimal;\n import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;\n@@ -43,6 +35,14 @@\n import org.apache.hadoop.hive.serde2.typeinfo.UnionTypeInfo;\n import org.apache.hadoop.io.Writable;\n \n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n final public class OrcStruct implements Writable {\n \n   private Object[] fields;\n@@ -346,17 +346,26 @@ public ObjectInspector getMapValueObjectInspector() {\n \n     @Override\n     public Object getMapValueElement(Object map, Object key) {\n+      if (map == null) {\n+        return null;\n+      }\n       return ((Map) map).get(key);\n     }\n \n     @Override\n     @SuppressWarnings(\"unchecked\")\n     public Map<Object, Object> getMap(Object map) {\n+      if (map == null) {\n+        return null;\n+      }\n       return (Map) map;\n     }\n \n     @Override\n     public int getMapSize(Object map) {\n+      if (map == null) {\n+        return -1;\n+      }\n       return ((Map) map).size();\n     }\n \n@@ -429,17 +438,26 @@ public ObjectInspector getListElementObjectInspector() {\n \n     @Override\n     public Object getListElement(Object list, int i) {\n+      if (list == null) {\n+        return null;\n+      }\n       return ((List) list).get(i);\n     }\n \n     @Override\n     public int getListLength(Object list) {\n+      if (list == null) {\n+        return -1;\n+      }\n       return ((List) list).size();\n     }\n \n     @Override\n     @SuppressWarnings(\"unchecked\")\n     public List<?> getList(Object list) {\n+      if (list == null) {\n+        return null;\n+      }\n       return (List) list;\n     }\n ",
                "raw_url": "https://github.com/apache/hive/raw/c81bd2ee50772d2434af04f580d8ddc4d2480746/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java",
                "sha": "2fbdf543d8345a4beed6a953c473a9a9f747c569",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/c81bd2ee50772d2434af04f580d8ddc4d2480746/ql/src/test/queries/clientpositive/orc_null_check.q",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/orc_null_check.q?ref=c81bd2ee50772d2434af04f580d8ddc4d2480746",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/orc_null_check.q",
                "patch": "@@ -0,0 +1,8 @@\n+create table listtable(l array<string>);\n+create table listtable_orc(l array<string>) stored as orc;\n+\n+insert overwrite table listtable select array(null) from src;\n+insert overwrite table listtable_orc select * from listtable;\n+\n+select size(l) from listtable_orc limit 10;\n+",
                "raw_url": "https://github.com/apache/hive/raw/c81bd2ee50772d2434af04f580d8ddc4d2480746/ql/src/test/queries/clientpositive/orc_null_check.q",
                "sha": "2cb119024ab2e5e1677acd13a5e8f5f0d4dd69e7",
                "status": "added"
            },
            {
                "additions": 52,
                "blob_url": "https://github.com/apache/hive/blob/c81bd2ee50772d2434af04f580d8ddc4d2480746/ql/src/test/results/clientpositive/orc_null_check.q.out",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/orc_null_check.q.out?ref=c81bd2ee50772d2434af04f580d8ddc4d2480746",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/orc_null_check.q.out",
                "patch": "@@ -0,0 +1,52 @@\n+PREHOOK: query: create table listtable(l array<string>)\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@listtable\n+POSTHOOK: query: create table listtable(l array<string>)\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@listtable\n+PREHOOK: query: create table listtable_orc(l array<string>) stored as orc\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@listtable_orc\n+POSTHOOK: query: create table listtable_orc(l array<string>) stored as orc\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@listtable_orc\n+PREHOOK: query: insert overwrite table listtable select array(null) from src\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@listtable\n+POSTHOOK: query: insert overwrite table listtable select array(null) from src\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@listtable\n+POSTHOOK: Lineage: listtable.l EXPRESSION []\n+PREHOOK: query: insert overwrite table listtable_orc select * from listtable\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@listtable\n+PREHOOK: Output: default@listtable_orc\n+POSTHOOK: query: insert overwrite table listtable_orc select * from listtable\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@listtable\n+POSTHOOK: Output: default@listtable_orc\n+POSTHOOK: Lineage: listtable_orc.l SIMPLE [(listtable)listtable.FieldSchema(name:l, type:array<string>, comment:null), ]\n+PREHOOK: query: select size(l) from listtable_orc limit 10\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@listtable_orc\n+#### A masked pattern was here ####\n+POSTHOOK: query: select size(l) from listtable_orc limit 10\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@listtable_orc\n+#### A masked pattern was here ####\n+-1\n+-1\n+-1\n+-1\n+-1\n+-1\n+-1\n+-1\n+-1\n+-1",
                "raw_url": "https://github.com/apache/hive/raw/c81bd2ee50772d2434af04f580d8ddc4d2480746/ql/src/test/results/clientpositive/orc_null_check.q.out",
                "sha": "093fdff7269fa059b93fbf2dbd5460fb1e2d486c",
                "status": "added"
            }
        ],
        "message": "HIVE-9111: Potential NPE in OrcStruct for list and map types (Prasanth Jayachandran reviewed by Vikram Dixit)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1646046 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/15ea88cf1788de4b920a8477a1e2f87f6ccb1520",
        "patched_files": [
            "OrcStruct.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestOrcStruct.java"
        ]
    },
    "hive_cbd0925": {
        "bug_id": "hive_cbd0925",
        "commit": "https://github.com/apache/hive/commit/cbd09253d6178dcd2f5f4043644570769dd52453",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/cbd09253d6178dcd2f5f4043644570769dd52453/itests/src/test/resources/testconfiguration.properties",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=cbd09253d6178dcd2f5f4043644570769dd52453",
                "deletions": 1,
                "filename": "itests/src/test/resources/testconfiguration.properties",
                "patch": "@@ -352,7 +352,8 @@ encrypted.query.files=encryption_join_unencrypted_tbl.q,\\\n   encryption_unencrypted_nonhdfs_external_tables.q \\\n   encryption_move_tbl.q \\\n   encryption_drop_table.q \\\n-  encryption_insert_values.q\n+  encryption_insert_values.q \\\n+  encryption_drop_view.q\n \n beeline.positive.exclude=add_part_exist.q,\\\n   alter1.q,\\",
                "raw_url": "https://github.com/apache/hive/raw/cbd09253d6178dcd2f5f4043644570769dd52453/itests/src/test/resources/testconfiguration.properties",
                "sha": "ae03283616b27bf09b0c46822e122ed6f329f2b5",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/cbd09253d6178dcd2f5f4043644570769dd52453/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=cbd09253d6178dcd2f5f4043644570769dd52453",
                "deletions": 1,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "patch": "@@ -1535,7 +1535,8 @@ private boolean drop_table_core(final RawStore ms, final String dbname, final St\n           }\n         }\n \n-        if(!ifPurge) {\n+        // tblPath will be null when tbl is a view. We skip the following if block in that case.\n+        if(tblPath != null && !ifPurge) {\n           String trashInterval = hiveConf.get(\"fs.trash.interval\");\n           boolean trashEnabled = trashInterval != null && trashInterval.length() > 0\n             && Float.parseFloat(trashInterval) > 0;",
                "raw_url": "https://github.com/apache/hive/raw/cbd09253d6178dcd2f5f4043644570769dd52453/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "sha": "d81c85696bac1f119380598f2ab16c7c48d73c06",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/cbd09253d6178dcd2f5f4043644570769dd52453/ql/src/test/queries/clientpositive/encryption_drop_view.q",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/encryption_drop_view.q?ref=cbd09253d6178dcd2f5f4043644570769dd52453",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/encryption_drop_view.q",
                "patch": "@@ -0,0 +1,6 @@\n+DROP TABLE IF EXISTS dve_encrypted_table PURGE;\n+CREATE TABLE dve_encrypted_table (key INT, value STRING) LOCATION '${hiveconf:hive.metastore.warehouse.dir}/default/dve_encrypted_table';\n+CRYPTO CREATE_KEY --keyName key_128 --bitLength 128;\n+CRYPTO CREATE_ZONE --keyName key_128 --path ${hiveconf:hive.metastore.warehouse.dir}/default/dve_encrypted_table;\n+CREATE VIEW dve_view AS SELECT * FROM dve_encrypted_table;\n+DROP VIEW dve_view;",
                "raw_url": "https://github.com/apache/hive/raw/cbd09253d6178dcd2f5f4043644570769dd52453/ql/src/test/queries/clientpositive/encryption_drop_view.q",
                "sha": "911583af08badbb90e64fb00c02a796e54671637",
                "status": "added"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/hive/blob/cbd09253d6178dcd2f5f4043644570769dd52453/ql/src/test/results/clientpositive/encrypted/encryption_drop_view.q.out",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/encrypted/encryption_drop_view.q.out?ref=cbd09253d6178dcd2f5f4043644570769dd52453",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/encrypted/encryption_drop_view.q.out",
                "patch": "@@ -0,0 +1,34 @@\n+PREHOOK: query: DROP TABLE IF EXISTS dve_encrypted_table PURGE\n+PREHOOK: type: DROPTABLE\n+POSTHOOK: query: DROP TABLE IF EXISTS dve_encrypted_table PURGE\n+POSTHOOK: type: DROPTABLE\n+#### A masked pattern was here ####\n+PREHOOK: type: CREATETABLE\n+#### A masked pattern was here ####\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@dve_encrypted_table\n+#### A masked pattern was here ####\n+POSTHOOK: type: CREATETABLE\n+#### A masked pattern was here ####\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@dve_encrypted_table\n+Encryption key created: 'key_128'\n+Encryption zone created: '/build/ql/test/data/warehouse/default/dve_encrypted_table' using key: 'key_128'\n+PREHOOK: query: CREATE VIEW dve_view AS SELECT * FROM dve_encrypted_table\n+PREHOOK: type: CREATEVIEW\n+PREHOOK: Input: default@dve_encrypted_table\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@dve_view\n+POSTHOOK: query: CREATE VIEW dve_view AS SELECT * FROM dve_encrypted_table\n+POSTHOOK: type: CREATEVIEW\n+POSTHOOK: Input: default@dve_encrypted_table\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@dve_view\n+PREHOOK: query: DROP VIEW dve_view\n+PREHOOK: type: DROPVIEW\n+PREHOOK: Input: default@dve_view\n+PREHOOK: Output: default@dve_view\n+POSTHOOK: query: DROP VIEW dve_view\n+POSTHOOK: type: DROPVIEW\n+POSTHOOK: Input: default@dve_view\n+POSTHOOK: Output: default@dve_view",
                "raw_url": "https://github.com/apache/hive/raw/cbd09253d6178dcd2f5f4043644570769dd52453/ql/src/test/results/clientpositive/encrypted/encryption_drop_view.q.out",
                "sha": "7958b391d863d6662796f36db4b31a31242af4d5",
                "status": "added"
            }
        ],
        "message": "HIVE-10801 : 'drop view' fails throwing java.lang.NullPointerException (Hari Subramaniyan, reviewed by Eugene Koifman)",
        "parent": "https://github.com/apache/hive/commit/a00bf4f8765bd87bce20166767cc53747f1801a0",
        "patched_files": [
            "HiveMetaStore.java",
            "encryption_drop_view.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveMetaStore.java",
            "testconfiguration.java"
        ]
    },
    "hive_cce0e37": {
        "bug_id": "hive_cce0e37",
        "commit": "https://github.com/apache/hive/commit/cce0e3777d178e68f38a0c9335d44a12fff42a6b",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/cce0e3777d178e68f38a0c9335d44a12fff42a6b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java?ref=cce0e3777d178e68f38a0c9335d44a12fff42a6b",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java",
                "patch": "@@ -3291,7 +3291,9 @@ private static void copyFiles(final HiveConf conf, final FileSystem destFs,\n         try {\n           files = srcFs.listStatus(src.getPath(), FileUtils.HIDDEN_FILES_PATH_FILTER);\n         } catch (IOException e) {\n-          pool.shutdownNow();\n+          if (null != pool) {\n+            pool.shutdownNow();\n+          }\n           throw new HiveException(e);\n         }\n       } else {",
                "raw_url": "https://github.com/apache/hive/raw/cce0e3777d178e68f38a0c9335d44a12fff42a6b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java",
                "sha": "466188130184516459dbd307e9cd0cf22844b46d",
                "status": "modified"
            }
        ],
        "message": "HIVE-19265 : Potential NPE and hiding actual exception in Hive#copyFiles (Igor Kryvenko via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>",
        "parent": "https://github.com/apache/hive/commit/f0199500f00ae58cf1a9f73f5baebdc5d5eca417",
        "patched_files": [
            "Hive.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHive.java"
        ]
    },
    "hive_cd39b40": {
        "bug_id": "hive_cd39b40",
        "commit": "https://github.com/apache/hive/commit/cd39b40b5bfe4cbf33b97a5498888d99396ca44e",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hive/blob/cd39b40b5bfe4cbf33b97a5498888d99396ca44e/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=cd39b40b5bfe4cbf33b97a5498888d99396ca44e",
                "deletions": 7,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "patch": "@@ -47,7 +47,6 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hive.common.FileUtils;\n-import org.apache.hadoop.hive.common.JavaUtils;\n import org.apache.hadoop.hive.common.ObjectPair;\n import org.apache.hadoop.hive.common.StatsSetupConst;\n import org.apache.hadoop.hive.common.StatsSetupConst.StatDB;\n@@ -5866,7 +5865,7 @@ private Operator genFileSinkPlan(String dest, QB qb, Operator input)\n       if (!isNonNativeTable) {\n         AcidUtils.Operation acidOp = getAcidType(table_desc.getOutputFileFormatClass());\n         if (acidOp != AcidUtils.Operation.NOT_ACID) {\n-          checkIfAcidAndOverwriting(qb, table_desc);\n+          checkAcidConstraints(qb, table_desc);\n         }\n         ltd = new LoadTableDesc(queryTmpdir,table_desc, dpCtx, acidOp);\n         ltd.setReplace(!qb.getParseInfo().isInsertIntoTable(dest_tab.getDbName(),\n@@ -5973,7 +5972,7 @@ private Operator genFileSinkPlan(String dest, QB qb, Operator input)\n           dest_part.isStoredAsSubDirectories(), conf);\n       AcidUtils.Operation acidOp = getAcidType(table_desc.getOutputFileFormatClass());\n       if (acidOp != AcidUtils.Operation.NOT_ACID) {\n-        checkIfAcidAndOverwriting(qb, table_desc);\n+        checkAcidConstraints(qb, table_desc);\n       }\n       ltd = new LoadTableDesc(queryTmpdir, table_desc, dest_part.getSpec(), acidOp);\n       ltd.setReplace(!qb.getParseInfo().isInsertIntoTable(dest_tab.getDbName(),\n@@ -6233,15 +6232,19 @@ private Operator genFileSinkPlan(String dest, QB qb, Operator input)\n     return output;\n   }\n \n-  // Check if we are overwriting any tables.  If so, throw an exception as that is not allowed\n-  // when using an Acid compliant txn manager and operating on an acid table.\n-  private void checkIfAcidAndOverwriting(QB qb, TableDesc tableDesc) throws SemanticException {\n+  // Check constraints on acid tables.  This includes\n+  // * no insert overwrites\n+  // * no use of vectorization\n+  private void checkAcidConstraints(QB qb, TableDesc tableDesc) throws SemanticException {\n     String tableName = tableDesc.getTableName();\n     if (!qb.getParseInfo().isInsertIntoTable(tableName)) {\n       LOG.debug(\"Couldn't find table \" + tableName + \" in insertIntoTable\");\n       throw new SemanticException(ErrorMsg.NO_INSERT_OVERWRITE_WITH_ACID.getMsg());\n     }\n-\n+    if (conf.getBoolVar(ConfVars.HIVE_VECTORIZATION_ENABLED)) {\n+      LOG.info(\"Turning off vectorization for acid write operation\");\n+      conf.setBoolVar(ConfVars.HIVE_VECTORIZATION_ENABLED, false);\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hive/raw/cd39b40b5bfe4cbf33b97a5498888d99396ca44e/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "sha": "db2ad3f3645315906f196ac08284c7ed7c491985",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hive/blob/cd39b40b5bfe4cbf33b97a5498888d99396ca44e/ql/src/test/queries/clientpositive/acid_vectorization.q",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/acid_vectorization.q?ref=cd39b40b5bfe4cbf33b97a5498888d99396ca44e",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/acid_vectorization.q",
                "patch": "@@ -0,0 +1,16 @@\n+set hive.support.concurrency=true;\n+set hive.txn.manager=org.apache.hadoop.hive.ql.lockmgr.DbTxnManager;\n+set hive.input.format=org.apache.hadoop.hive.ql.io.HiveInputFormat;\n+set hive.enforce.bucketing=true;\n+set hive.exec.dynamic.partition.mode=nonstrict;\n+set hive.vectorized.execution.enabled=true;\n+set hive.mapred.supports.subdirectories=true;\n+\n+CREATE TABLE acid_vectorized(a INT, b STRING) CLUSTERED BY(a) INTO 2 BUCKETS STORED AS ORC;\n+insert into table acid_vectorized select cint, cstring1 from alltypesorc where cint is not null order by cint limit 10;\n+set hive.vectorized.execution.enabled=true;\n+insert into table acid_vectorized values (1, 'bar');\n+set hive.vectorized.execution.enabled=true;\n+update acid_vectorized set b = 'foo' where b = 'bar';\n+set hive.vectorized.execution.enabled=true;\n+delete from acid_vectorized where b = 'foo';",
                "raw_url": "https://github.com/apache/hive/raw/cd39b40b5bfe4cbf33b97a5498888d99396ca44e/ql/src/test/queries/clientpositive/acid_vectorization.q",
                "sha": "9d91d880e07e98962fb2f4a0e23a6b02591f2901",
                "status": "added"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/hive/blob/cd39b40b5bfe4cbf33b97a5498888d99396ca44e/ql/src/test/results/clientpositive/acid_vectorization.q.out",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/acid_vectorization.q.out?ref=cd39b40b5bfe4cbf33b97a5498888d99396ca44e",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/acid_vectorization.q.out",
                "patch": "@@ -0,0 +1,44 @@\n+PREHOOK: query: CREATE TABLE acid_vectorized(a INT, b STRING) CLUSTERED BY(a) INTO 2 BUCKETS STORED AS ORC\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: CREATE TABLE acid_vectorized(a INT, b STRING) CLUSTERED BY(a) INTO 2 BUCKETS STORED AS ORC\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@acid_vectorized\n+PREHOOK: query: insert into table acid_vectorized select cint, cstring1 from alltypesorc where cint is not null order by cint limit 10\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@alltypesorc\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: insert into table acid_vectorized select cint, cstring1 from alltypesorc where cint is not null order by cint limit 10\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@alltypesorc\n+POSTHOOK: Output: default@acid_vectorized\n+POSTHOOK: Lineage: acid_vectorized.a SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: acid_vectorized.b SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+PREHOOK: query: insert into table acid_vectorized values (1, 'bar')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@values__tmp__table__1\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: insert into table acid_vectorized values (1, 'bar')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@values__tmp__table__1\n+POSTHOOK: Output: default@acid_vectorized\n+POSTHOOK: Lineage: acid_vectorized.a EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+POSTHOOK: Lineage: acid_vectorized.b SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col2, type:string, comment:), ]\n+PREHOOK: query: update acid_vectorized set b = 'foo' where b = 'bar'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@acid_vectorized\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: update acid_vectorized set b = 'foo' where b = 'bar'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@acid_vectorized\n+POSTHOOK: Output: default@acid_vectorized\n+PREHOOK: query: delete from acid_vectorized where b = 'foo'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@acid_vectorized\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: delete from acid_vectorized where b = 'foo'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@acid_vectorized\n+POSTHOOK: Output: default@acid_vectorized",
                "raw_url": "https://github.com/apache/hive/raw/cd39b40b5bfe4cbf33b97a5498888d99396ca44e/ql/src/test/results/clientpositive/acid_vectorization.q.out",
                "sha": "4a9d19f5529128303fc748cebcef23b8a560b8fa",
                "status": "added"
            }
        ],
        "message": "HIVE-8104 Insert statements against ACID tables NPE when vectorization is on (Alan Gates reviewed by Eugene Koifman)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1625866 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/cb933c10ea021cfa66922f470ec5530983f2028a",
        "patched_files": [
            "SemanticAnalyzer.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestSemanticAnalyzer.java"
        ]
    },
    "hive_cf42684": {
        "bug_id": "hive_cf42684",
        "commit": "https://github.com/apache/hive/commit/cf4268487a5d65346b79994a2bfada70b20c428e",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/cf4268487a5d65346b79994a2bfada70b20c428e/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java?ref=cf4268487a5d65346b79994a2bfada70b20c428e",
                "deletions": 0,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java",
                "patch": "@@ -122,6 +122,7 @@ public void testTokenStorage() throws Exception {\n \n     assertTrue(ts.removeToken(tokenId));\n     assertEquals(0, ts.getAllDelegationTokenIdentifiers().size());\n+    assertNull(ts.getToken(tokenId));\n   }\n \n   public void testAclNoAuth() throws Exception {",
                "raw_url": "https://github.com/apache/hive/raw/cf4268487a5d65346b79994a2bfada70b20c428e/itests/hive-unit/src/test/java/org/apache/hadoop/hive/thrift/TestZooKeeperTokenStore.java",
                "sha": "65a10e344e3fb24e5efd3b051bb1eaf26b0179a3",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hive/blob/cf4268487a5d65346b79994a2bfada70b20c428e/shims/common/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/shims/common/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java?ref=cf4268487a5d65346b79994a2bfada70b20c428e",
                "deletions": 1,
                "filename": "shims/common/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java",
                "patch": "@@ -330,7 +330,6 @@ public void run() {\n       } catch (Throwable t) {\n         LOGGER.error(\"ExpiredTokenRemover thread received unexpected exception. \"\n             + t, t);\n-        Runtime.getRuntime().exit(-1);\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/hive/raw/cf4268487a5d65346b79994a2bfada70b20c428e/shims/common/src/main/java/org/apache/hadoop/hive/thrift/TokenStoreDelegationTokenSecretManager.java",
                "sha": "87b418ebf29f8744a57bdb809a1d0fec7ed47ab5",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/cf4268487a5d65346b79994a2bfada70b20c428e/shims/common/src/main/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/shims/common/src/main/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java?ref=cf4268487a5d65346b79994a2bfada70b20c428e",
                "deletions": 0,
                "filename": "shims/common/src/main/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java",
                "patch": "@@ -396,6 +396,10 @@ public boolean removeToken(DelegationTokenIdentifier tokenIdentifier) {\n   @Override\n   public DelegationTokenInformation getToken(DelegationTokenIdentifier tokenIdentifier) {\n     byte[] tokenBytes = zkGetData(getTokenPath(tokenIdentifier));\n+    if(tokenBytes == null) {\n+      // The token is already removed.\n+      return null;\n+    }\n     try {\n       return HiveDelegationTokenSupport.decodeDelegationTokenInformation(tokenBytes);\n     } catch (Exception ex) {",
                "raw_url": "https://github.com/apache/hive/raw/cf4268487a5d65346b79994a2bfada70b20c428e/shims/common/src/main/java/org/apache/hadoop/hive/thrift/ZooKeeperTokenStore.java",
                "sha": "528e55d28cb7f56b0bdc4edb4f829acb6d6a61ca",
                "status": "modified"
            }
        ],
        "message": "HIVE-13090 : Hive metastore crashes on NPE with ZooKeeperTokenStore (Piotr Wikie\u0142, Thejas Nair, reviewed by Ashutosh Chauhan)",
        "parent": "https://github.com/apache/hive/commit/e732616392ddf5139f4d32bfb9fc51f352114887",
        "patched_files": [
            "TokenStoreDelegationTokenSecretManager.java",
            "ZooKeeperTokenStore.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestZooKeeperTokenStore.java"
        ]
    },
    "hive_d00edc7": {
        "bug_id": "hive_d00edc7",
        "commit": "https://github.com/apache/hive/commit/d00edc7b5ae5227484f9da610b021e13f60f5812",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/d00edc7b5ae5227484f9da610b021e13f60f5812/common/src/java/org/apache/hadoop/hive/common/FileUtils.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/common/src/java/org/apache/hadoop/hive/common/FileUtils.java?ref=d00edc7b5ae5227484f9da610b021e13f60f5812",
                "deletions": 0,
                "filename": "common/src/java/org/apache/hadoop/hive/common/FileUtils.java",
                "patch": "@@ -649,6 +649,11 @@ public static void checkDeletePermission(Path path, Configuration conf, String u\n     //   if a user is a super user. Also super users running hive queries is not a common\n     //   use case. super users can also do a chown to be able to drop the file\n \n+    if(path == null) {\n+      // no file/dir to be deleted\n+      return;\n+    }\n+\n     final FileSystem fs = path.getFileSystem(conf);\n     if (!fs.exists(path)) {\n       // no file/dir to be deleted",
                "raw_url": "https://github.com/apache/hive/raw/d00edc7b5ae5227484f9da610b021e13f60f5812/common/src/java/org/apache/hadoop/hive/common/FileUtils.java",
                "sha": "79819b89c6821cac9cf7f601daadfa93f87a9045",
                "status": "modified"
            },
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/hive/blob/d00edc7b5ae5227484f9da610b021e13f60f5812/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestStorageBasedMetastoreAuthorizationDrops.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestStorageBasedMetastoreAuthorizationDrops.java?ref=d00edc7b5ae5227484f9da610b021e13f60f5812",
                "deletions": 1,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestStorageBasedMetastoreAuthorizationDrops.java",
                "patch": "@@ -96,6 +96,7 @@ protected void setUp() throws Exception {\n     driver = new Driver(clientHiveConf);\n \n     setupFakeUser();\n+    InjectableDummyAuthenticator.injectMode(false);\n   }\n \n \n@@ -159,6 +160,38 @@ private void dropTableByOtherUser(String perm, int expectedRet) throws Exception\n     assertEquals(expectedRet, resp.getResponseCode());\n   }\n \n+  /**\n+   * Drop view should not be blocked by SBA. View will not have any location to drop.\n+   * @throws Exception\n+   */\n+  public void testDropView() throws Exception {\n+    String dbName = getTestDbName();\n+    String tblName = getTestTableName();\n+    String viewName = \"view\" + tblName;\n+    setPermissions(clientHiveConf.getVar(ConfVars.METASTOREWAREHOUSE), \"-rwxrwxrwx\");\n+\n+    CommandProcessorResponse resp = driver.run(\"create database \" + dbName);\n+    assertEquals(0, resp.getResponseCode());\n+    Database db = msc.getDatabase(dbName);\n+    validateCreateDb(db, dbName);\n+\n+    setPermissions(db.getLocationUri(), \"-rwxrwxrwt\");\n+\n+    String dbDotTable = dbName + \".\" + tblName;\n+    resp = driver.run(\"create table \" + dbDotTable + \"(i int)\");\n+    assertEquals(0, resp.getResponseCode());\n+\n+    String dbDotView = dbName + \".\" + viewName;\n+    resp = driver.run(\"create view \" + dbDotView + \" as select * from \" +  dbDotTable);\n+    assertEquals(0, resp.getResponseCode());\n+\n+    resp = driver.run(\"drop view \" + dbDotView);\n+    assertEquals(0, resp.getResponseCode());\n+\n+    resp = driver.run(\"drop table \" + dbDotTable);\n+    assertEquals(0, resp.getResponseCode());\n+  }\n+\n \n   public void testDropPartition() throws Exception {\n     dropPartitionByOtherUser(\"-rwxrwxrwx\", 0);\n@@ -202,7 +235,6 @@ private void setupFakeUser() {\n \n     InjectableDummyAuthenticator.injectUserName(fakeUser);\n     InjectableDummyAuthenticator.injectGroupNames(fakeGroupNames);\n-    InjectableDummyAuthenticator.injectMode(true);\n   }\n \n   private String setupUser() {",
                "raw_url": "https://github.com/apache/hive/raw/d00edc7b5ae5227484f9da610b021e13f60f5812/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/security/TestStorageBasedMetastoreAuthorizationDrops.java",
                "sha": "6cf8565a3753a6bf92d6ffaf6aeb975c51e9c031",
                "status": "modified"
            }
        ],
        "message": "HIVE-7987 : Storage based authorization  - NPE for drop view (Thejas Nair, reviewed by Jason Dere)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1622767 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/94af26453ece19e170b2a072b38392a305bd5668",
        "patched_files": [
            "FileUtils.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestStorageBasedMetastoreAuthorizationDrops.java"
        ]
    },
    "hive_d2bea05": {
        "bug_id": "hive_d2bea05",
        "commit": "https://github.com/apache/hive/commit/d2bea05454f0e10a0a0675f082d9da036e382b27",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/d2bea05454f0e10a0a0675f082d9da036e382b27/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/CHANGES.txt?ref=d2bea05454f0e10a0a0675f082d9da036e382b27",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -79,6 +79,9 @@ Trunk -  Unreleased\n     HIVE-778. add describe for div\n     (Ning Zhang via namit)\n \n+    HIVE-755. Driver NullPointerException when calling getResults without first compiling\n+    (Eric Hwang via namit)\n+\n Release 0.4.0 -  Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hive/raw/d2bea05454f0e10a0a0675f082d9da036e382b27/CHANGES.txt",
                "sha": "a4ee97751a046babf899c75e199da03592067aef",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hive/blob/d2bea05454f0e10a0a0675f082d9da036e382b27/service/src/java/org/apache/hadoop/hive/service/HiveServer.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/service/src/java/org/apache/hadoop/hive/service/HiveServer.java?ref=d2bea05454f0e10a0a0675f082d9da036e382b27",
                "deletions": 2,
                "filename": "service/src/java/org/apache/hadoop/hive/service/HiveServer.java",
                "patch": "@@ -72,6 +72,11 @@\n      * Stores state per connection\n      */\n     private SessionState session;\n+    \n+    /**\n+     * Flag that indicates whether the last executed command was a Hive query\n+     */\n+    private boolean isHiveQuery;\n \n     public static final Log LOG = LogFactory.getLog(HiveServer.class.getName());\n \n@@ -81,6 +86,7 @@\n     public HiveServerHandler() throws MetaException {\n       super(HiveServer.class.getName());\n \n+      isHiveQuery = false;\n       SessionState session = new SessionState(new HiveConf(SessionState.class));\n       SessionState.start(session);\n       session.in = null;\n@@ -107,8 +113,10 @@ public void execute(String cmd) throws HiveServerException, TException {\n         CommandProcessor proc = CommandProcessorFactory.get(tokens[0]);\n         if(proc != null) {\n           if (proc instanceof Driver) {\n+        \t  isHiveQuery = true;\n             ret = driver.run(cmd);\n           } else {\n+        \t  isHiveQuery = false;\n             ret = proc.run(cmd_1);\n           }\n         }\n@@ -164,6 +172,10 @@ public HiveClusterStatus getClusterStatus() throws HiveServerException, TExcepti\n      * Return the Hive schema of the query result\n      */\n     public Schema getSchema() throws HiveServerException, TException {\n+      if (!isHiveQuery)\n+        // Return empty schema if the last command was not a Hive query\n+        return new Schema();\t\n+    \t\n       try {\n         Schema schema = driver.getSchema();\n         if (schema == null) {\n@@ -183,6 +195,10 @@ public Schema getSchema() throws HiveServerException, TException {\n      * Return the Thrift schema of the query result\n      */\n     public Schema getThriftSchema() throws HiveServerException, TException {\n+      if (!isHiveQuery)\n+        // Return empty schema if the last command was not a Hive query\n+        return new Schema();\n+    \t\n       try {\n         Schema schema = driver.getThriftSchema();\n         if (schema == null) {\n@@ -205,8 +221,12 @@ public Schema getThriftSchema() throws HiveServerException, TException {\n      * @return the next row in a query result set. null if there is no more row to fetch.\n      */\n     public String fetchOne() throws HiveServerException, TException {\n-      driver.setMaxRows(1);\n+      if (!isHiveQuery)\n+        // Return no results if the last command was not a Hive query\n+        return \"\";\n+      \n       Vector<String> result = new Vector<String>();\n+      driver.setMaxRows(1);\n       try {\n         if (driver.getResults(result)) {\n           return result.get(0);\n@@ -234,7 +254,11 @@ public String fetchOne() throws HiveServerException, TException {\n       if (numRows < 0) {\n         throw new HiveServerException(\"Invalid argument for number of rows: \" + numRows);\n       } \n-      Vector<String> result = new Vector<String>();\n+      if (!isHiveQuery)\n+      \t// Return no results if the last command was not a Hive query\n+        return new Vector<String>();\n+      \n+      Vector<String> result = new Vector<String>();      \n       driver.setMaxRows(numRows);\n       try {\n         driver.getResults(result);\n@@ -253,6 +277,10 @@ public String fetchOne() throws HiveServerException, TException {\n      * to the client. Decide whether the buffering should be done in the client.\n      */\n     public List<String> fetchAll() throws HiveServerException, TException {\n+      if (!isHiveQuery)\n+        // Return no results if the last command was not a Hive query\n+        return new Vector<String>();\n+      \n       Vector<String> rows = new Vector<String>();\n       Vector<String> result = new Vector<String>();\n       try {",
                "raw_url": "https://github.com/apache/hive/raw/d2bea05454f0e10a0a0675f082d9da036e382b27/service/src/java/org/apache/hadoop/hive/service/HiveServer.java",
                "sha": "395dd624e6c8bc474718a1422daf3e5ae38c3b18",
                "status": "modified"
            },
            {
                "additions": 47,
                "blob_url": "https://github.com/apache/hive/blob/d2bea05454f0e10a0a0675f082d9da036e382b27/service/src/test/org/apache/hadoop/hive/service/TestHiveServer.java",
                "changes": 47,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/service/src/test/org/apache/hadoop/hive/service/TestHiveServer.java?ref=d2bea05454f0e10a0a0675f082d9da036e382b27",
                "deletions": 0,
                "filename": "service/src/test/org/apache/hadoop/hive/service/TestHiveServer.java",
                "patch": "@@ -115,6 +115,53 @@ public void notestExecute() throws Exception {\n     transport.close();\n   }\n \n+  public void testNonHiveCommand() throws Exception {\n+    try {\n+      client.execute(\"drop table \" + tableName);\n+    } catch (Exception ex) {\n+    }\n+\n+    client.execute(\"create table \" + tableName + \" (num int)\");\n+    client.execute(\"load data local inpath '\" + dataFilePath.toString() + \"' into table \" + tableName);\n+    \n+    // Command not part of HiveQL -  verify no results\n+    client.execute(\"SET hive.mapred.mode = nonstrict\");\n+    \n+    Schema schema = client.getSchema();\n+    assertEquals(schema.getFieldSchemasSize(), 0);\n+    assertEquals(schema.getPropertiesSize(), 0);\n+    \n+    Schema thriftschema = client.getThriftSchema();\n+    assertEquals(thriftschema.getFieldSchemasSize(), 0);\n+    assertEquals(thriftschema.getPropertiesSize(), 0);\n+    \n+    assertEquals(client.fetchOne(), \"\");\n+    assertEquals(client.fetchN(10).size(), 0);\n+    assertEquals(client.fetchAll().size(), 0);\n+    \n+    // Execute Hive query and fetch\n+    client.execute(\"select * from \" + tableName + \" limit 10\");\n+    String row = client.fetchOne();\n+    \n+    // Re-execute command not part of HiveQL - verify still no results\n+    client.execute(\"SET hive.mapred.mode = nonstrict\");\n+    \n+    schema = client.getSchema();\n+    assertEquals(schema.getFieldSchemasSize(), 0);\n+    assertEquals(schema.getPropertiesSize(), 0);\n+    \n+    thriftschema = client.getThriftSchema();\n+    assertEquals(thriftschema.getFieldSchemasSize(), 0);\n+    assertEquals(thriftschema.getPropertiesSize(), 0);\n+    \n+    assertEquals(client.fetchOne(), \"\");\n+    assertEquals(client.fetchN(10).size(), 0);\n+    assertEquals(client.fetchAll().size(), 0);\n+\n+    // Cleanup\n+    client.execute(\"drop table \" + tableName);\n+  }\n+  \n   /**\n    * Test metastore call\n    */",
                "raw_url": "https://github.com/apache/hive/raw/d2bea05454f0e10a0a0675f082d9da036e382b27/service/src/test/org/apache/hadoop/hive/service/TestHiveServer.java",
                "sha": "fdf90eb4817a513d129e6517ee4f87c520b4983b",
                "status": "modified"
            }
        ],
        "message": "HIVE-755. Driver NullPointerException when calling getResults without first compiling\n(Eric Hwang via namit)\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hive/trunk@810290 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/99e59863087d67c4ef1f20e95756ef652cfb20a4",
        "patched_files": [
            "HiveServer.java",
            "CHANGES.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveServer.java"
        ]
    },
    "hive_d375b39": {
        "bug_id": "hive_d375b39",
        "commit": "https://github.com/apache/hive/commit/d375b3977d42936b3b00888e2b1bacf736e8ac3e",
        "file": [
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e",
                "deletions": 2,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java",
                "patch": "@@ -123,6 +123,9 @@ private void setupMRLegacyConfigs(TezProcessorContext processorContext) {\n   @Override\n   public void run(Map<String, LogicalInput> inputs, Map<String, LogicalOutput> outputs)\n       throws Exception {\n+    \n+    Exception processingException = null;\n+    \n     try{\n       perfLogger.PerfLogBegin(CLASS_NAME, PerfLogger.TEZ_RUN_PROCESSOR);\n       // in case of broadcast-join read the broadcast edge inputs\n@@ -160,9 +163,20 @@ public void run(Map<String, LogicalInput> inputs, Map<String, LogicalOutput> out\n \n       //done - output does not need to be committed as hive does not use outputcommitter\n       perfLogger.PerfLogEnd(CLASS_NAME, PerfLogger.TEZ_RUN_PROCESSOR);\n+    } catch (Exception e) {\n+      processingException = e;\n     } finally {\n-      if(rproc != null){\n-        rproc.close();\n+      try {\n+        if(rproc != null){\n+          rproc.close();\n+        }\n+      } catch (Exception e) {\n+        if (processingException == null) {\n+          processingException = e;\n+        }\n+      }\n+      if (processingException != null) {\n+        throw processingException;\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/TezProcessor.java",
                "sha": "803903256b5986e75cf193c5cb73fd27ccdec701",
                "status": "modified"
            },
            {
                "additions": 78,
                "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java",
                "changes": 144,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e",
                "deletions": 66,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java",
                "patch": "@@ -42,6 +42,7 @@\n import org.apache.hadoop.hive.ql.plan.HashTableDummyDesc;\n import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n import org.apache.hadoop.hive.ql.plan.PlanUtils;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n import org.apache.hadoop.hive.ql.plan.TableDesc;\n import org.apache.hadoop.hive.ql.plan.TezWork;\n import org.apache.hadoop.hive.ql.plan.TezWork.EdgeType;\n@@ -63,12 +64,16 @@\n   public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext, Object... nodeOutputs)\n       throws SemanticException {\n     GenTezProcContext context = (GenTezProcContext) procContext;\n+    MapJoinOperator mapJoinOp = (MapJoinOperator)nd;\n+\n+    if (stack.size() < 2 || !(stack.get(stack.size() - 2) instanceof ReduceSinkOperator)) {\n+      context.currentMapJoinOperators.add(mapJoinOp);\n+      return null;\n+    }\n+\n     context.preceedingWork = null;\n     context.currentRootOperator = null;\n \n-    MapJoinOperator mapJoinOp = (MapJoinOperator)nd;\n-    Operator<? extends OperatorDesc> childOp = mapJoinOp.getChildOperators().get(0);\n-\n     ReduceSinkOperator parentRS = (ReduceSinkOperator)stack.get(stack.size() - 2);\n \n     // remember the original parent list before we start modifying it.\n@@ -77,66 +82,72 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext,\n       context.mapJoinParentMap.put(mapJoinOp, parents);\n     }\n \n-    BaseWork myWork = null;\n-\n-    while (childOp != null) {\n-      if ((childOp instanceof ReduceSinkOperator) || (childOp instanceof FileSinkOperator)) {\n-        /*\n-         *  if there was a pre-existing work generated for the big-table mapjoin side,\n-         *  we need to hook the work generated for the RS (associated with the RS-MJ pattern)\n-         *  with the pre-existing work.\n-         *\n-         *  Otherwise, we need to associate that the reduce sink/file sink down the MJ path\n-         *  to be linked to the RS work (associated with the RS-MJ pattern).\n-         *\n-         */\n-\n-        myWork = context.operatorWorkMap.get(childOp);\n-        BaseWork parentWork = context.operatorWorkMap.get(parentRS);\n-\n-        // set the link between mapjoin and parent vertex\n-        int pos = context.mapJoinParentMap.get(mapJoinOp).indexOf(parentRS);\n-        if (pos == -1) {\n-          throw new SemanticException(\"Cannot find position of parent in mapjoin\");\n-        }\n-        LOG.debug(\"Mapjoin \"+mapJoinOp+\", pos: \"+pos+\" --> \"+parentWork.getName());\n-        mapJoinOp.getConf().getParentToInput().put(pos, parentWork.getName());\n-\n-        if (myWork != null) {\n-          // link the work with the work associated with the reduce sink that triggered this rule\n-          TezWork tezWork = context.currentTask.getWork();\n-          tezWork.connect(parentWork, myWork, EdgeType.BROADCAST_EDGE);\n-\n-          // remember the output name of the reduce sink\n-          parentRS.getConf().setOutputName(myWork.getName());\n-          context.connectedReduceSinks.add(parentRS);\n+    List<BaseWork> mapJoinWork = null;\n+\n+    /*\n+     *  if there was a pre-existing work generated for the big-table mapjoin side,\n+     *  we need to hook the work generated for the RS (associated with the RS-MJ pattern)\n+     *  with the pre-existing work.\n+     *\n+     *  Otherwise, we need to associate that the mapjoin op\n+     *  to be linked to the RS work (associated with the RS-MJ pattern).\n+     *\n+     */\n+    mapJoinWork = context.mapJoinWorkMap.get(mapJoinOp);\n+    BaseWork parentWork;\n+    if (context.unionWorkMap.containsKey(parentRS)) {\n+      parentWork = context.unionWorkMap.get(parentRS);\n+    } else {\n+      assert context.childToWorkMap.get(parentRS).size() == 1;\n+      parentWork = context.childToWorkMap.get(parentRS).get(0);\n+    }\n \n+    // set the link between mapjoin and parent vertex\n+    int pos = context.mapJoinParentMap.get(mapJoinOp).indexOf(parentRS);\n+    if (pos == -1) {\n+      throw new SemanticException(\"Cannot find position of parent in mapjoin\");\n+    }\n+    LOG.debug(\"Mapjoin \"+mapJoinOp+\", pos: \"+pos+\" --> \"+parentWork.getName());\n+    mapJoinOp.getConf().getParentToInput().put(pos, parentWork.getName());\n+\n+    if (mapJoinWork != null) {\n+      for (BaseWork myWork: mapJoinWork) {\n+        // link the work with the work associated with the reduce sink that triggered this rule\n+        TezWork tezWork = context.currentTask.getWork();\n+        LOG.debug(\"connecting \"+parentWork.getName()+\" with \"+myWork.getName());\n+        tezWork.connect(parentWork, myWork, EdgeType.BROADCAST_EDGE);\n+        \n+        ReduceSinkOperator r = null;\n+        if (parentRS.getConf().getOutputName() != null) {\n+          LOG.debug(\"Cloning reduce sink for multi-child broadcast edge\");\n+          // we've already set this one up. Need to clone for the next work.\n+          r = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n+              (ReduceSinkDesc) parentRS.getConf().clone(), parentRS.getParentOperators());\n+          context.clonedReduceSinks.add(r);\n         } else {\n-          List<BaseWork> linkWorkList = context.linkOpWithWorkMap.get(childOp);\n-          if (linkWorkList == null) {\n-            linkWorkList = new ArrayList<BaseWork>();\n-          }\n-          linkWorkList.add(parentWork);\n-          context.linkOpWithWorkMap.put(childOp, linkWorkList);\n-\n-          List<ReduceSinkOperator> reduceSinks \n-            = context.linkWorkWithReduceSinkMap.get(parentWork);\n-          if (reduceSinks == null) {\n-            reduceSinks = new ArrayList<ReduceSinkOperator>();\n-          }\n-          reduceSinks.add(parentRS);\n-          context.linkWorkWithReduceSinkMap.put(parentWork, reduceSinks);\n+          r = parentRS;\n         }\n-\n-        break;\n+        // remember the output name of the reduce sink\n+        r.getConf().setOutputName(myWork.getName());\n+        context.connectedReduceSinks.add(r);\n       }\n+    }\n \n-      if ((childOp.getChildOperators() != null) && (childOp.getChildOperators().size() >= 1)) {\n-        childOp = childOp.getChildOperators().get(0);\n-      } else {\n-        break;\n-      }\n+    // remember in case we need to connect additional work later\n+    List<BaseWork> linkWorkList = context.linkOpWithWorkMap.get(mapJoinOp);\n+    if (linkWorkList == null) {\n+      linkWorkList = new ArrayList<BaseWork>();\n+    }\n+    linkWorkList.add(parentWork);\n+    context.linkOpWithWorkMap.put(mapJoinOp, linkWorkList);\n+    \n+    List<ReduceSinkOperator> reduceSinks \n+      = context.linkWorkWithReduceSinkMap.get(parentWork);\n+    if (reduceSinks == null) {\n+      reduceSinks = new ArrayList<ReduceSinkOperator>();\n     }\n+    reduceSinks.add(parentRS);\n+    context.linkWorkWithReduceSinkMap.put(parentWork, reduceSinks);\n \n     // create the dummy operators\n     List<Operator<? extends OperatorDesc>> dummyOperators =\n@@ -178,17 +189,18 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx procContext,\n \n     // the \"work\" needs to know about the dummy operators. They have to be separately initialized\n     // at task startup\n-    if (myWork != null) {\n-      myWork.addDummyOp(dummyOp);\n-    } else {\n-      List<Operator<?>> dummyList = dummyOperators;\n-      if (context.linkChildOpWithDummyOp.containsKey(childOp)) {\n-        dummyList = context.linkChildOpWithDummyOp.get(childOp);\n+    if (mapJoinWork != null) {\n+      for (BaseWork myWork: mapJoinWork) {\n+        myWork.addDummyOp(dummyOp);\n+      }\n+    }\n+    if (context.linkChildOpWithDummyOp.containsKey(mapJoinOp)) {\n+      for (Operator<?> op: context.linkChildOpWithDummyOp.get(mapJoinOp)) {\n+        dummyOperators.add(op);\n       }\n-      dummyList.add(dummyOp);\n-      context.linkChildOpWithDummyOp.put(childOp, dummyList);\n     }\n+    context.linkChildOpWithDummyOp.put(mapJoinOp, dummyOperators);\n+\n     return true;\n   }\n-\n }",
                "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/optimizer/ReduceSinkMapJoinProc.java",
                "sha": "135bb4df5252b3ddc21483f237a68e2ab5ddc515",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezProcContext.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezProcContext.java?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e",
                "deletions": 3,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezProcContext.java",
                "patch": "@@ -95,12 +95,15 @@\n   // map join work\n   public final Map<BaseWork, List<ReduceSinkOperator>> linkWorkWithReduceSinkMap;\n \n-  // a map that maintains operator (file-sink or reduce-sink) to work mapping\n-  public final Map<Operator<?>, BaseWork> operatorWorkMap;\n+  // map that says which mapjoin belongs to which work item\n+  public final Map<MapJoinOperator, List<BaseWork>> mapJoinWorkMap;\n \n   // a map to keep track of which root generated which work\n   public final Map<Operator<?>, BaseWork> rootToWorkMap;\n \n+  // a map to keep track of which child generated with work\n+  public final Map<Operator<?>, List<BaseWork>> childToWorkMap;\n+\n   // we need to keep the original list of operators in the map join to know\n   // what position in the mapjoin the different parent work items will have.\n   public final Map<MapJoinOperator, List<Operator<?>>> mapJoinParentMap;\n@@ -111,10 +114,14 @@\n   // used to group dependent tasks for multi table inserts\n   public final DependencyCollectionTask dependencyTask;\n \n+  // remember map joins as we encounter them.\n+  public final Set<MapJoinOperator> currentMapJoinOperators;\n+\n   // used to hook up unions\n   public final Map<Operator<?>, BaseWork> unionWorkMap;\n   public final List<UnionOperator> currentUnionOperators;\n   public final Set<BaseWork> workWithUnionOperators;\n+  public final Set<ReduceSinkOperator> clonedReduceSinks;\n \n   // we link filesink that will write to the same final location\n   public final Map<Path, List<FileSinkDesc>> linkedFileSinks;\n@@ -139,15 +146,18 @@ public GenTezProcContext(HiveConf conf, ParseContext parseContext,\n     this.leafOperatorToFollowingWork = new HashMap<Operator<?>, BaseWork>();\n     this.linkOpWithWorkMap = new HashMap<Operator<?>, List<BaseWork>>();\n     this.linkWorkWithReduceSinkMap = new HashMap<BaseWork, List<ReduceSinkOperator>>();\n-    this.operatorWorkMap = new HashMap<Operator<?>, BaseWork>();\n+    this.mapJoinWorkMap = new HashMap<MapJoinOperator, List<BaseWork>>();\n     this.rootToWorkMap = new HashMap<Operator<?>, BaseWork>();\n+    this.childToWorkMap = new HashMap<Operator<?>, List<BaseWork>>();\n     this.mapJoinParentMap = new HashMap<MapJoinOperator, List<Operator<?>>>();\n+    this.currentMapJoinOperators = new HashSet<MapJoinOperator>();\n     this.linkChildOpWithDummyOp = new HashMap<Operator<?>, List<Operator<?>>>();\n     this.dependencyTask = (DependencyCollectionTask)\n         TaskFactory.get(new DependencyCollectionWork(), conf);\n     this.unionWorkMap = new HashMap<Operator<?>, BaseWork>();\n     this.currentUnionOperators = new LinkedList<UnionOperator>();\n     this.workWithUnionOperators = new HashSet<BaseWork>();\n+    this.clonedReduceSinks = new HashSet<ReduceSinkOperator>();\n     this.linkedFileSinks = new HashMap<Path, List<FileSinkDesc>>();\n     this.fileSinkSet = new HashSet<FileSinkOperator>();\n     this.connectedReduceSinks = new HashSet<ReduceSinkOperator>();",
                "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezProcContext.java",
                "sha": "ec21aa8e07dd872d32fe7286b2e7538e8e628a8a",
                "status": "modified"
            },
            {
                "additions": 75,
                "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java",
                "changes": 108,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e",
                "deletions": 33,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java",
                "patch": "@@ -29,7 +29,9 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.hive.ql.exec.FileSinkOperator;\n import org.apache.hadoop.hive.ql.exec.HashTableDummyOperator;\n+import org.apache.hadoop.hive.ql.exec.MapJoinOperator;\n import org.apache.hadoop.hive.ql.exec.Operator;\n+import org.apache.hadoop.hive.ql.exec.OperatorFactory;\n import org.apache.hadoop.hive.ql.exec.ReduceSinkOperator;\n import org.apache.hadoop.hive.ql.exec.TableScanOperator;\n import org.apache.hadoop.hive.ql.exec.UnionOperator;\n@@ -41,6 +43,7 @@\n import org.apache.hadoop.hive.ql.plan.BaseWork;\n import org.apache.hadoop.hive.ql.plan.MapWork;\n import org.apache.hadoop.hive.ql.plan.OperatorDesc;\n+import org.apache.hadoop.hive.ql.plan.ReduceSinkDesc;\n import org.apache.hadoop.hive.ql.plan.ReduceWork;\n import org.apache.hadoop.hive.ql.plan.TezWork;\n import org.apache.hadoop.hive.ql.plan.UnionWork;\n@@ -87,6 +90,12 @@ public Object process(Node nd, Stack<Node> stack,\n     LOG.debug(\"Root operator: \" + root);\n     LOG.debug(\"Leaf operator: \" + operator);\n \n+    if (context.clonedReduceSinks.contains(operator)) {\n+      // if we're visiting a terminal we've created ourselves,\n+      // just skip and keep going\n+      return null;\n+    }\n+\n     TezWork tezWork = context.currentTask.getWork();\n \n     // Right now the work graph is pretty simple. If there is no\n@@ -112,42 +121,75 @@ public Object process(Node nd, Stack<Node> stack,\n       }\n       context.rootToWorkMap.put(root, work);\n     }\n-    context.operatorWorkMap.put(operator, work);\n-\n-    /*\n-     * this happens in case of map join operations.\n-     * The tree looks like this:\n-     *\n-     *       RS <--- we are here perhaps\n-     *       |\n-     *    MapJoin\n-     *    /     \\\n-     *  RS       TS\n-     *  /\n-     * TS\n-     *\n-     * If we are at the RS pointed above, and we may have already visited the\n-     * RS following the TS, we have already generated work for the TS-RS.\n-     * We need to hook the current work to this generated work.\n-     */\n-    List<BaseWork> linkWorkList = context.linkOpWithWorkMap.get(operator);\n-    if (linkWorkList != null) {\n-      if (context.linkChildOpWithDummyOp.containsKey(operator)) {\n-        for (Operator<?> dummy: context.linkChildOpWithDummyOp.get(operator)) {\n-          work.addDummyOp((HashTableDummyOperator) dummy);\n+\n+    if (!context.childToWorkMap.containsKey(operator)) {\n+      List<BaseWork> workItems = new LinkedList<BaseWork>();\n+      workItems.add(work);\n+      context.childToWorkMap.put(operator, workItems);\n+    } else {\n+      context.childToWorkMap.get(operator).add(work);\n+    }\n+\n+    // remember which mapjoin operator links with which work\n+    if (!context.currentMapJoinOperators.isEmpty()) {\n+      for (MapJoinOperator mj: context.currentMapJoinOperators) {\n+        LOG.debug(\"Processing map join: \" + mj);\n+        // remember the mapping in case we scan another branch of the \n+        // mapjoin later\n+        if (!context.mapJoinWorkMap.containsKey(mj)) {\n+          List<BaseWork> workItems = new LinkedList<BaseWork>();\n+          workItems.add(work);\n+          context.mapJoinWorkMap.put(mj, workItems);\n+        } else {\n+          context.mapJoinWorkMap.get(mj).add(work);\n         }\n-      }\n-      for (BaseWork parentWork : linkWorkList) {\n-        tezWork.connect(parentWork, work, EdgeType.BROADCAST_EDGE);\n-\n-        // need to set up output name for reduce sink now that we know the name\n-        // of the downstream work\n-        for (ReduceSinkOperator r:\n-               context.linkWorkWithReduceSinkMap.get(parentWork)) {\n-          r.getConf().setOutputName(work.getName());\n-          context.connectedReduceSinks.add(r);\n+\n+        /*\n+         * this happens in case of map join operations.\n+         * The tree looks like this:\n+         *\n+         *        RS <--- we are here perhaps\n+         *        |\n+         *     MapJoin\n+         *     /     \\\n+         *   RS       TS\n+         *  /\n+         * TS\n+         *\n+         * If we are at the RS pointed above, and we may have already visited the\n+         * RS following the TS, we have already generated work for the TS-RS.\n+         * We need to hook the current work to this generated work.\n+         */\n+        List<BaseWork> linkWorkList = context.linkOpWithWorkMap.get(mj);\n+        if (linkWorkList != null) {\n+          if (context.linkChildOpWithDummyOp.containsKey(mj)) {\n+            for (Operator<?> dummy: context.linkChildOpWithDummyOp.get(mj)) {\n+              work.addDummyOp((HashTableDummyOperator) dummy);\n+            }\n+          }\n+          for (BaseWork parentWork : linkWorkList) {\n+            LOG.debug(\"connecting \"+parentWork.getName()+\" with \"+work.getName());\n+            tezWork.connect(parentWork, work, EdgeType.BROADCAST_EDGE);\n+\n+            // need to set up output name for reduce sink now that we know the name\n+            // of the downstream work\n+            for (ReduceSinkOperator r:\n+                   context.linkWorkWithReduceSinkMap.get(parentWork)) {\n+              if (r.getConf().getOutputName() != null) {\n+                LOG.debug(\"Cloning reduce sink for multi-child broadcast edge\");\n+                // we've already set this one up. Need to clone for the next work.\n+                r = (ReduceSinkOperator) OperatorFactory.getAndMakeChild(\n+                    (ReduceSinkDesc)r.getConf().clone(), r.getParentOperators());\n+                context.clonedReduceSinks.add(r);\n+              }\n+              r.getConf().setOutputName(work.getName());\n+              context.connectedReduceSinks.add(r);\n+            }\n+          }\n         }\n       }\n+      // clear out the set. we don't need it anymore.\n+      context.currentMapJoinOperators.clear();\n     }\n \n     // This is where we cut the tree as described above. We also remember that",
                "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/parse/GenTezWork.java",
                "sha": "f0b3fdf612c876db4297839ab195c66c25e485a7",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e",
                "deletions": 2,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java",
                "patch": "@@ -143,7 +143,6 @@ protected void generateTaskTree(List<Task<? extends Serializable>> rootTasks, Pa\n         genTezWork);\n \n     opRules.put(new RuleRegExp(\"No more walking on ReduceSink-MapJoin\",\n-        ReduceSinkOperator.getOperatorName() + \"%\" +\n         MapJoinOperator.getOperatorName() + \"%\"), new ReduceSinkMapJoinProc());\n \n     opRules.put(new RuleRegExp(\"Split Work + Move/Merge - FileSink\",\n@@ -154,7 +153,7 @@ protected void generateTaskTree(List<Task<? extends Serializable>> rootTasks, Pa\n         TableScanOperator.getOperatorName() + \"%\"),\n         new ProcessAnalyzeTable(GenTezUtils.getUtils()));\n \n-    opRules.put(new RuleRegExp(\"Handle union\",\n+    opRules.put(new RuleRegExp(\"Remember union\",\n         UnionOperator.getOperatorName() + \"%\"), new NodeProcessor()\n     {\n       @Override",
                "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/parse/TezCompiler.java",
                "sha": "e1ce79a41bf475ba2b0fcd98d8e5ab7329db6d0a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceSinkDesc.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceSinkDesc.java?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceSinkDesc.java",
                "patch": "@@ -132,6 +132,7 @@ public Object clone() {\n     desc.setValueSerializeInfo((TableDesc) getValueSerializeInfo().clone());\n     desc.setNumBuckets(numBuckets);\n     desc.setBucketCols(bucketCols);\n+    desc.setStatistics(this.getStatistics());\n     return desc;\n   }\n ",
                "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/java/org/apache/hadoop/hive/ql/plan/ReduceSinkDesc.java",
                "sha": "157d07283ba9d752108c860f4ba5f04cc7130382",
                "status": "modified"
            },
            {
                "additions": 74,
                "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/queries/clientpositive/tez_union.q",
                "changes": 75,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/tez_union.q?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e",
                "deletions": 1,
                "filename": "ql/src/test/queries/clientpositive/tez_union.q",
                "patch": "@@ -5,17 +5,90 @@ select s1.key as key, s1.value as value from src s1 join src s3 on s1.key=s3.key\n UNION  ALL  \n select s2.key as key, s2.value as value from src s2;\n \n+create table ut as\n select s1.key as key, s1.value as value from src s1 join src s3 on s1.key=s3.key\n UNION  ALL  \n select s2.key as key, s2.value as value from src s2;\n \n+select * from ut order by key, value limit 20;\n+drop table ut;\n+\n set hive.auto.convert.join=false;\n \n explain\n with u as (select * from src union all select * from src)\n select count(*) from (select u1.key as k1, u2.key as k2 from\n u as u1 join u as u2 on (u1.key = u2.key)) a;\n \n+create table ut as\n with u as (select * from src union all select * from src)\n-select count(*) from (select u1.key as k1, u2.key as k2 from\n+select count(*) as cnt from (select u1.key as k1, u2.key as k2 from\n u as u1 join u as u2 on (u1.key = u2.key)) a;\n+\n+select * from ut order by cnt limit 20;\n+drop table ut;\n+\n+set hive.auto.convert.join=true;\n+\n+explain select s1.key as skey, u1.key as ukey from\n+src s1\n+join (select * from src union all select * from src) u1 on s1.key = u1.key;\n+\n+create table ut as\n+select s1.key as skey, u1.key as ukey from\n+src s1\n+join (select * from src union all select * from src) u1 on s1.key = u1.key;\n+\n+select * from ut order by skey, ukey limit 20;\n+drop table ut;\n+\n+explain select s1.key as skey, u1.key as ukey, s8.key as lkey from \n+src s1\n+join (select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+      union all select s4.key from src s4 join src s5 on s4.key = s5.key\n+      union all select s6.key from src s6 join src s7 on s6.key = s7.key) u1 on (s1.key = u1.key)\n+join src s8 on (u1.key = s8.key)\n+order by lkey;\n+\n+create table ut as\n+select s1.key as skey, u1.key as ukey, s8.key as lkey from \n+src s1\n+join (select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+      union all select s4.key from src s4 join src s5 on s4.key = s5.key\n+      union all select s6.key from src s6 join src s7 on s6.key = s7.key) u1 on (s1.key = u1.key)\n+join src s8 on (u1.key = s8.key)\n+order by lkey;\n+\n+select * from ut order by skey, ukey, lkey limit 100;\n+\n+drop table ut;\n+\n+explain\n+select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+union all select s4.key from src s4 join src s5 on s4.key = s5.key;\n+\n+create table ut as\n+select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+union all select s4.key from src s4 join src s5 on s4.key = s5.key;\n+\n+select * from ut order by key limit 30;\n+\n+drop table ut;\n+\n+explain\n+select * from\n+(select * from src union all select * from src) u\n+left outer join src s on u.key = s.key;\n+\n+explain\n+select u.key as ukey, s.key as skey from\n+(select * from src union all select * from src) u\n+right outer join src s on u.key = s.key;\n+\n+create table ut as\n+select u.key as ukey, s.key as skey from\n+(select * from src union all select * from src) u\n+right outer join src s on u.key = s.key;\n+\n+select * from ut order by ukey, skey limit 20;\n+drop table ut;\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/queries/clientpositive/tez_union.q",
                "sha": "f80d94c4a15fd1288ea6b0845b2896de438e0c23",
                "status": "modified"
            },
            {
                "additions": 1882,
                "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out",
                "changes": 3741,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e",
                "deletions": 1859,
                "filename": "ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out",
                "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/load_dyn_part1.q.out",
                "sha": "ea0f1b9b4dbecef7b80d5f15396b3991c41ed8ca",
                "status": "modified"
            },
            {
                "additions": 942,
                "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/load_dyn_part3.q.out",
                "changes": 1872,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/load_dyn_part3.q.out?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e",
                "deletions": 930,
                "filename": "ql/src/test/results/clientpositive/tez/load_dyn_part3.q.out",
                "patch": "@@ -44,6 +44,8 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Tez\n+      Edges:\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE)\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n@@ -55,14 +57,24 @@ STAGE PLANS:\n                     expressions: key (type: string), value (type: string), ds (type: string), hr (type: string)\n                     outputColumnNames: _col0, _col1, _col2, _col3\n                     Statistics: Num rows: 116 Data size: 23248 Basic stats: COMPLETE Column stats: NONE\n-                    File Output Operator\n-                      compressed: false\n+                    Reduce Output Operator\n+                      key expressions: _col2 (type: string), _col3 (type: string)\n+                      sort order: ++\n+                      Map-reduce partition columns: _col2 (type: string), _col3 (type: string)\n                       Statistics: Num rows: 116 Data size: 23248 Basic stats: COMPLETE Column stats: NONE\n-                      table:\n-                          input format: org.apache.hadoop.mapred.TextInputFormat\n-                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n-                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-                          name: default.nzhang_part3\n+                      value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)\n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Extract\n+                Statistics: Num rows: 116 Data size: 23248 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 116 Data size: 23248 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                      name: default.nzhang_part3\n \n   Stage: Stage-2\n     Dependency Collection\n@@ -634,1006 +646,1006 @@ POSTHOOK: Lineage: nzhang_part3 PARTITION(ds=2008-04-09,hr=12).value SIMPLE [(sr\n 400\tval_400\t2008-04-08\t11\n 200\tval_200\t2008-04-08\t11\n 97\tval_97\t2008-04-08\t11\n-238\tval_238\t2008-04-08\t12\n-86\tval_86\t2008-04-08\t12\n-311\tval_311\t2008-04-08\t12\n-27\tval_27\t2008-04-08\t12\n-165\tval_165\t2008-04-08\t12\n-409\tval_409\t2008-04-08\t12\n-255\tval_255\t2008-04-08\t12\n-278\tval_278\t2008-04-08\t12\n-98\tval_98\t2008-04-08\t12\n-484\tval_484\t2008-04-08\t12\n-265\tval_265\t2008-04-08\t12\n-193\tval_193\t2008-04-08\t12\n-401\tval_401\t2008-04-08\t12\n-150\tval_150\t2008-04-08\t12\n-273\tval_273\t2008-04-08\t12\n-224\tval_224\t2008-04-08\t12\n-369\tval_369\t2008-04-08\t12\n-66\tval_66\t2008-04-08\t12\n-128\tval_128\t2008-04-08\t12\n-213\tval_213\t2008-04-08\t12\n-146\tval_146\t2008-04-08\t12\n-406\tval_406\t2008-04-08\t12\n-429\tval_429\t2008-04-08\t12\n-374\tval_374\t2008-04-08\t12\n+97\tval_97\t2008-04-08\t12\n+200\tval_200\t2008-04-08\t12\n+400\tval_400\t2008-04-08\t12\n+403\tval_403\t2008-04-08\t12\n+169\tval_169\t2008-04-08\t12\n+90\tval_90\t2008-04-08\t12\n+126\tval_126\t2008-04-08\t12\n+222\tval_222\t2008-04-08\t12\n+477\tval_477\t2008-04-08\t12\n+414\tval_414\t2008-04-08\t12\n+194\tval_194\t2008-04-08\t12\n+307\tval_307\t2008-04-08\t12\n+348\tval_348\t2008-04-08\t12\n 152\tval_152\t2008-04-08\t12\n-469\tval_469\t2008-04-08\t12\n-145\tval_145\t2008-04-08\t12\n-495\tval_495\t2008-04-08\t12\n+448\tval_448\t2008-04-08\t12\n 37\tval_37\t2008-04-08\t12\n-327\tval_327\t2008-04-08\t12\n+28\tval_28\t2008-04-08\t12\n+84\tval_84\t2008-04-08\t12\n+315\tval_315\t2008-04-08\t12\n+469\tval_469\t2008-04-08\t12\n+97\tval_97\t2008-04-08\t12\n+344\tval_344\t2008-04-08\t12\n 281\tval_281\t2008-04-08\t12\n-277\tval_277\t2008-04-08\t12\n-209\tval_209\t2008-04-08\t12\n-15\tval_15\t2008-04-08\t12\n-82\tval_82\t2008-04-08\t12\n-403\tval_403\t2008-04-08\t12\n-166\tval_166\t2008-04-08\t12\n-417\tval_417\t2008-04-08\t12\n-430\tval_430\t2008-04-08\t12\n-252\tval_252\t2008-04-08\t12\n-292\tval_292\t2008-04-08\t12\n-219\tval_219\t2008-04-08\t12\n-287\tval_287\t2008-04-08\t12\n-153\tval_153\t2008-04-08\t12\n-193\tval_193\t2008-04-08\t12\n-338\tval_338\t2008-04-08\t12\n-446\tval_446\t2008-04-08\t12\n-459\tval_459\t2008-04-08\t12\n-394\tval_394\t2008-04-08\t12\n-237\tval_237\t2008-04-08\t12\n-482\tval_482\t2008-04-08\t12\n-174\tval_174\t2008-04-08\t12\n-413\tval_413\t2008-04-08\t12\n-494\tval_494\t2008-04-08\t12\n+183\tval_183\t2008-04-08\t12\n+273\tval_273\t2008-04-08\t12\n+18\tval_18\t2008-04-08\t12\n+167\tval_167\t2008-04-08\t12\n+348\tval_348\t2008-04-08\t12\n+285\tval_285\t2008-04-08\t12\n+186\tval_186\t2008-04-08\t12\n+362\tval_362\t2008-04-08\t12\n+458\tval_458\t2008-04-08\t12\n+146\tval_146\t2008-04-08\t12\n+498\tval_498\t2008-04-08\t12\n+341\tval_341\t2008-04-08\t12\n+9\tval_9\t2008-04-08\t12\n+298\tval_298\t2008-04-08\t12\n+100\tval_100\t2008-04-08\t12\n+492\tval_492\t2008-04-08\t12\n+462\tval_462\t2008-04-08\t12\n+18\tval_18\t2008-04-08\t12\n+379\tval_379\t2008-04-08\t12\n+384\tval_384\t2008-04-08\t12\n+67\tval_67\t2008-04-08\t12\n+134\tval_134\t2008-04-08\t12\n+26\tval_26\t2008-04-08\t12\n+256\tval_256\t2008-04-08\t12\n+384\tval_384\t2008-04-08\t12\n+407\tval_407\t2008-04-08\t12\n+421\tval_421\t2008-04-08\t12\n+401\tval_401\t2008-04-08\t12\n+375\tval_375\t2008-04-08\t12\n+454\tval_454\t2008-04-08\t12\n+189\tval_189\t2008-04-08\t12\n+175\tval_175\t2008-04-08\t12\n+133\tval_133\t2008-04-08\t12\n+406\tval_406\t2008-04-08\t12\n+233\tval_233\t2008-04-08\t12\n+462\tval_462\t2008-04-08\t12\n+214\tval_214\t2008-04-08\t12\n+172\tval_172\t2008-04-08\t12\n+353\tval_353\t2008-04-08\t12\n+136\tval_136\t2008-04-08\t12\n+83\tval_83\t2008-04-08\t12\n+480\tval_480\t2008-04-08\t12\n+265\tval_265\t2008-04-08\t12\n+249\tval_249\t2008-04-08\t12\n 207\tval_207\t2008-04-08\t12\n+460\tval_460\t2008-04-08\t12\n+493\tval_493\t2008-04-08\t12\n+333\tval_333\t2008-04-08\t12\n+317\tval_317\t2008-04-08\t12\n+310\tval_310\t2008-04-08\t12\n+468\tval_468\t2008-04-08\t12\n+178\tval_178\t2008-04-08\t12\n+478\tval_478\t2008-04-08\t12\n+230\tval_230\t2008-04-08\t12\n+277\tval_277\t2008-04-08\t12\n+325\tval_325\t2008-04-08\t12\n+323\tval_323\t2008-04-08\t12\n+443\tval_443\t2008-04-08\t12\n+169\tval_169\t2008-04-08\t12\n+429\tval_429\t2008-04-08\t12\n+120\tval_120\t2008-04-08\t12\n+444\tval_444\t2008-04-08\t12\n 199\tval_199\t2008-04-08\t12\n-466\tval_466\t2008-04-08\t12\n-208\tval_208\t2008-04-08\t12\n-174\tval_174\t2008-04-08\t12\n-399\tval_399\t2008-04-08\t12\n-396\tval_396\t2008-04-08\t12\n-247\tval_247\t2008-04-08\t12\n 417\tval_417\t2008-04-08\t12\n-489\tval_489\t2008-04-08\t12\n-162\tval_162\t2008-04-08\t12\n-377\tval_377\t2008-04-08\t12\n-397\tval_397\t2008-04-08\t12\n-309\tval_309\t2008-04-08\t12\n-365\tval_365\t2008-04-08\t12\n-266\tval_266\t2008-04-08\t12\n+305\tval_305\t2008-04-08\t12\n+479\tval_479\t2008-04-08\t12\n+248\tval_248\t2008-04-08\t12\n+360\tval_360\t2008-04-08\t12\n 439\tval_439\t2008-04-08\t12\n-342\tval_342\t2008-04-08\t12\n-367\tval_367\t2008-04-08\t12\n-325\tval_325\t2008-04-08\t12\n-167\tval_167\t2008-04-08\t12\n-195\tval_195\t2008-04-08\t12\n-475\tval_475\t2008-04-08\t12\n-17\tval_17\t2008-04-08\t12\n-113\tval_113\t2008-04-08\t12\n-155\tval_155\t2008-04-08\t12\n-203\tval_203\t2008-04-08\t12\n-339\tval_339\t2008-04-08\t12\n-0\tval_0\t2008-04-08\t12\n-455\tval_455\t2008-04-08\t12\n-128\tval_128\t2008-04-08\t12\n-311\tval_311\t2008-04-08\t12\n-316\tval_316\t2008-04-08\t12\n-57\tval_57\t2008-04-08\t12\n-302\tval_302\t2008-04-08\t12\n-205\tval_205\t2008-04-08\t12\n-149\tval_149\t2008-04-08\t12\n+237\tval_237\t2008-04-08\t12\n+491\tval_491\t2008-04-08\t12\n+200\tval_200\t2008-04-08\t12\n+414\tval_414\t2008-04-08\t12\n+119\tval_119\t2008-04-08\t12\n 438\tval_438\t2008-04-08\t12\n-345\tval_345\t2008-04-08\t12\n-129\tval_129\t2008-04-08\t12\n-170\tval_170\t2008-04-08\t12\n-20\tval_20\t2008-04-08\t12\n-489\tval_489\t2008-04-08\t12\n-157\tval_157\t2008-04-08\t12\n-378\tval_378\t2008-04-08\t12\n-221\tval_221\t2008-04-08\t12\n-92\tval_92\t2008-04-08\t12\n-111\tval_111\t2008-04-08\t12\n-47\tval_47\t2008-04-08\t12\n-72\tval_72\t2008-04-08\t12\n-4\tval_4\t2008-04-08\t12\n-280\tval_280\t2008-04-08\t12\n-35\tval_35\t2008-04-08\t12\n-427\tval_427\t2008-04-08\t12\n-277\tval_277\t2008-04-08\t12\n-208\tval_208\t2008-04-08\t12\n-356\tval_356\t2008-04-08\t12\n-399\tval_399\t2008-04-08\t12\n-169\tval_169\t2008-04-08\t12\n+163\tval_163\t2008-04-08\t12\n+70\tval_70\t2008-04-08\t12\n+104\tval_104\t2008-04-08\t12\n+255\tval_255\t2008-04-08\t12\n+351\tval_351\t2008-04-08\t12\n+24\tval_24\t2008-04-08\t12\n+291\tval_291\t2008-04-08\t12\n+480\tval_480\t2008-04-08\t12\n+397\tval_397\t2008-04-08\t12\n+70\tval_70\t2008-04-08\t12\n+5\tval_5\t2008-04-08\t12\n 382\tval_382\t2008-04-08\t12\n-498\tval_498\t2008-04-08\t12\n-125\tval_125\t2008-04-08\t12\n-386\tval_386\t2008-04-08\t12\n-437\tval_437\t2008-04-08\t12\n-469\tval_469\t2008-04-08\t12\n-192\tval_192\t2008-04-08\t12\n-286\tval_286\t2008-04-08\t12\n 187\tval_187\t2008-04-08\t12\n-176\tval_176\t2008-04-08\t12\n-54\tval_54\t2008-04-08\t12\n-459\tval_459\t2008-04-08\t12\n-51\tval_51\t2008-04-08\t12\n-138\tval_138\t2008-04-08\t12\n-103\tval_103\t2008-04-08\t12\n-239\tval_239\t2008-04-08\t12\n-213\tval_213\t2008-04-08\t12\n-216\tval_216\t2008-04-08\t12\n-430\tval_430\t2008-04-08\t12\n-278\tval_278\t2008-04-08\t12\n-176\tval_176\t2008-04-08\t12\n-289\tval_289\t2008-04-08\t12\n-221\tval_221\t2008-04-08\t12\n-65\tval_65\t2008-04-08\t12\n-318\tval_318\t2008-04-08\t12\n-332\tval_332\t2008-04-08\t12\n-311\tval_311\t2008-04-08\t12\n-275\tval_275\t2008-04-08\t12\n-137\tval_137\t2008-04-08\t12\n-241\tval_241\t2008-04-08\t12\n-83\tval_83\t2008-04-08\t12\n-333\tval_333\t2008-04-08\t12\n-180\tval_180\t2008-04-08\t12\n-284\tval_284\t2008-04-08\t12\n+424\tval_424\t2008-04-08\t12\n+164\tval_164\t2008-04-08\t12\n+431\tval_431\t2008-04-08\t12\n+125\tval_125\t2008-04-08\t12\n+298\tval_298\t2008-04-08\t12\n+478\tval_478\t2008-04-08\t12\n+454\tval_454\t2008-04-08\t12\n+431\tval_431\t2008-04-08\t12\n+164\tval_164\t2008-04-08\t12\n+217\tval_217\t2008-04-08\t12\n+201\tval_201\t2008-04-08\t12\n+396\tval_396\t2008-04-08\t12\n 12\tval_12\t2008-04-08\t12\n-230\tval_230\t2008-04-08\t12\n-181\tval_181\t2008-04-08\t12\n-67\tval_67\t2008-04-08\t12\n-260\tval_260\t2008-04-08\t12\n-404\tval_404\t2008-04-08\t12\n-384\tval_384\t2008-04-08\t12\n-489\tval_489\t2008-04-08\t12\n-353\tval_353\t2008-04-08\t12\n-373\tval_373\t2008-04-08\t12\n-272\tval_272\t2008-04-08\t12\n-138\tval_138\t2008-04-08\t12\n-217\tval_217\t2008-04-08\t12\n-84\tval_84\t2008-04-08\t12\n+424\tval_424\t2008-04-08\t12\n 348\tval_348\t2008-04-08\t12\n+262\tval_262\t2008-04-08\t12\n+203\tval_203\t2008-04-08\t12\n+90\tval_90\t2008-04-08\t12\n+258\tval_258\t2008-04-08\t12\n+114\tval_114\t2008-04-08\t12\n+401\tval_401\t2008-04-08\t12\n+406\tval_406\t2008-04-08\t12\n+190\tval_190\t2008-04-08\t12\n+409\tval_409\t2008-04-08\t12\n+406\tval_406\t2008-04-08\t12\n+257\tval_257\t2008-04-08\t12\n+105\tval_105\t2008-04-08\t12\n+53\tval_53\t2008-04-08\t12\n+483\tval_483\t2008-04-08\t12\n+403\tval_403\t2008-04-08\t12\n+175\tval_175\t2008-04-08\t12\n+366\tval_366\t2008-04-08\t12\n 466\tval_466\t2008-04-08\t12\n-58\tval_58\t2008-04-08\t12\n-8\tval_8\t2008-04-08\t12\n-411\tval_411\t2008-04-08\t12\n-230\tval_230\t2008-04-08\t12\n-208\tval_208\t2008-04-08\t12\n-348\tval_348\t2008-04-08\t12\n-24\tval_24\t2008-04-08\t12\n+104\tval_104\t2008-04-08\t12\n+335\tval_335\t2008-04-08\t12\n+321\tval_321\t2008-04-08\t12\n+193\tval_193\t2008-04-08\t12\n+44\tval_44\t2008-04-08\t12\n+80\tval_80\t2008-04-08\t12\n+235\tval_235\t2008-04-08\t12\n+331\tval_331\t2008-04-08\t12\n+283\tval_283\t2008-04-08\t12\n+35\tval_35\t2008-04-08\t12\n+2\tval_2\t2008-04-08\t12\n+280\tval_280\t2008-04-08\t12\n 463\tval_463\t2008-04-08\t12\n-431\tval_431\t2008-04-08\t12\n-179\tval_179\t2008-04-08\t12\n-172\tval_172\t2008-04-08\t12\n-42\tval_42\t2008-04-08\t12\n-129\tval_129\t2008-04-08\t12\n-158\tval_158\t2008-04-08\t12\n-119\tval_119\t2008-04-08\t12\n-496\tval_496\t2008-04-08\t12\n-0\tval_0\t2008-04-08\t12\n-322\tval_322\t2008-04-08\t12\n-197\tval_197\t2008-04-08\t12\n-468\tval_468\t2008-04-08\t12\n-393\tval_393\t2008-04-08\t12\n-454\tval_454\t2008-04-08\t12\n-100\tval_100\t2008-04-08\t12\n-298\tval_298\t2008-04-08\t12\n-199\tval_199\t2008-04-08\t12\n+469\tval_469\t2008-04-08\t12\n+229\tval_229\t2008-04-08\t12\n+316\tval_316\t2008-04-08\t12\n+202\tval_202\t2008-04-08\t12\n+432\tval_432\t2008-04-08\t12\n+467\tval_467\t2008-04-08\t12\n+128\tval_128\t2008-04-08\t12\n+438\tval_438\t2008-04-08\t12\n+244\tval_244\t2008-04-08\t12\n+5\tval_5\t2008-04-08\t12\n 191\tval_191\t2008-04-08\t12\n-418\tval_418\t2008-04-08\t12\n-96\tval_96\t2008-04-08\t12\n-26\tval_26\t2008-04-08\t12\n-165\tval_165\t2008-04-08\t12\n-327\tval_327\t2008-04-08\t12\n+288\tval_288\t2008-04-08\t12\n+401\tval_401\t2008-04-08\t12\n+480\tval_480\t2008-04-08\t12\n+487\tval_487\t2008-04-08\t12\n+70\tval_70\t2008-04-08\t12\n+263\tval_263\t2008-04-08\t12\n+256\tval_256\t2008-04-08\t12\n+223\tval_223\t2008-04-08\t12\n+116\tval_116\t2008-04-08\t12\n+485\tval_485\t2008-04-08\t12\n+239\tval_239\t2008-04-08\t12\n+219\tval_219\t2008-04-08\t12\n+274\tval_274\t2008-04-08\t12\n+167\tval_167\t2008-04-08\t12\n+344\tval_344\t2008-04-08\t12\n+367\tval_367\t2008-04-08\t12\n+216\tval_216\t2008-04-08\t12\n+113\tval_113\t2008-04-08\t12\n+296\tval_296\t2008-04-08\t12\n+103\tval_103\t2008-04-08\t12\n+368\tval_368\t2008-04-08\t12\n+33\tval_33\t2008-04-08\t12\n 230\tval_230\t2008-04-08\t12\n-205\tval_205\t2008-04-08\t12\n-120\tval_120\t2008-04-08\t12\n-131\tval_131\t2008-04-08\t12\n-51\tval_51\t2008-04-08\t12\n-404\tval_404\t2008-04-08\t12\n-43\tval_43\t2008-04-08\t12\n-436\tval_436\t2008-04-08\t12\n-156\tval_156\t2008-04-08\t12\n-469\tval_469\t2008-04-08\t12\n+69\tval_69\t2008-04-08\t12\n+342\tval_342\t2008-04-08\t12\n+74\tval_74\t2008-04-08\t12\n+76\tval_76\t2008-04-08\t12\n 468\tval_468\t2008-04-08\t12\n-308\tval_308\t2008-04-08\t12\n+64\tval_64\t2008-04-08\t12\n+209\tval_209\t2008-04-08\t12\n+30\tval_30\t2008-04-08\t12\n+453\tval_453\t2008-04-08\t12\n+138\tval_138\t2008-04-08\t12\n+228\tval_228\t2008-04-08\t12\n+218\tval_218\t2008-04-08\t12\n+449\tval_449\t2008-04-08\t12\n+149\tval_149\t2008-04-08\t12\n+492\tval_492\t2008-04-08\t12\n+223\tval_223\t2008-04-08\t12\n+41\tval_41\t2008-04-08\t12\n+76\tval_76\t2008-04-08\t12\n+78\tval_78\t2008-04-08\t12\n+458\tval_458\t2008-04-08\t12\n+489\tval_489\t2008-04-08\t12\n+119\tval_119\t2008-04-08\t12\n+430\tval_430\t2008-04-08\t12\n+321\tval_321\t2008-04-08\t12\n+42\tval_42\t2008-04-08\t12\n+195\tval_195\t2008-04-08\t12\n+160\tval_160\t2008-04-08\t12\n+498\tval_498\t2008-04-08\t12\n+322\tval_322\t2008-04-08\t12\n+472\tval_472\t2008-04-08\t12\n+143\tval_143\t2008-04-08\t12\n+233\tval_233\t2008-04-08\t12\n+229\tval_229\t2008-04-08\t12\n+34\tval_34\t2008-04-08\t12\n+168\tval_168\t2008-04-08\t12\n+11\tval_11\t2008-04-08\t12\n 95\tval_95\t2008-04-08\t12\n-196\tval_196\t2008-04-08\t12\n-288\tval_288\t2008-04-08\t12\n-481\tval_481\t2008-04-08\t12\n-457\tval_457\t2008-04-08\t12\n-98\tval_98\t2008-04-08\t12\n-282\tval_282\t2008-04-08\t12\n-197\tval_197\t2008-04-08\t12\n-187\tval_187\t2008-04-08\t12\n-318\tval_318\t2008-04-08\t12\n-318\tval_318\t2008-04-08\t12\n-409\tval_409\t2008-04-08\t12\n-470\tval_470\t2008-04-08\t12\n-137\tval_137\t2008-04-08\t12\n-369\tval_369\t2008-04-08\t12\n-316\tval_316\t2008-04-08\t12\n-169\tval_169\t2008-04-08\t12\n-413\tval_413\t2008-04-08\t12\n-85\tval_85\t2008-04-08\t12\n-77\tval_77\t2008-04-08\t12\n-0\tval_0\t2008-04-08\t12\n-490\tval_490\t2008-04-08\t12\n-87\tval_87\t2008-04-08\t12\n-364\tval_364\t2008-04-08\t12\n-179\tval_179\t2008-04-08\t12\n-118\tval_118\t2008-04-08\t12\n-134\tval_134\t2008-04-08\t12\n+336\tval_336\t2008-04-08\t12\n+35\tval_35\t2008-04-08\t12\n+58\tval_58\t2008-04-08\t12\n 395\tval_395\t2008-04-08\t12\n-282\tval_282\t2008-04-08\t12\n-138\tval_138\t2008-04-08\t12\n-238\tval_238\t2008-04-08\t12\n-419\tval_419\t2008-04-08\t12\n-15\tval_15\t2008-04-08\t12\n-118\tval_118\t2008-04-08\t12\n-72\tval_72\t2008-04-08\t12\n-90\tval_90\t2008-04-08\t12\n-307\tval_307\t2008-04-08\t12\n-19\tval_19\t2008-04-08\t12\n-435\tval_435\t2008-04-08\t12\n-10\tval_10\t2008-04-08\t12\n-277\tval_277\t2008-04-08\t12\n-273\tval_273\t2008-04-08\t12\n-306\tval_306\t2008-04-08\t12\n-224\tval_224\t2008-04-08\t12\n-309\tval_309\t2008-04-08\t12\n-389\tval_389\t2008-04-08\t12\n-327\tval_327\t2008-04-08\t12\n+317\tval_317\t2008-04-08\t12\n+396\tval_396\t2008-04-08\t12\n+402\tval_402\t2008-04-08\t12\n+497\tval_497\t2008-04-08\t12\n+5\tval_5\t2008-04-08\t12\n+226\tval_226\t2008-04-08\t12\n+177\tval_177\t2008-04-08\t12\n+452\tval_452\t2008-04-08\t12\n 242\tval_242\t2008-04-08\t12\n-369\tval_369\t2008-04-08\t12\n-392\tval_392\t2008-04-08\t12\n-272\tval_272\t2008-04-08\t12\n-331\tval_331\t2008-04-08\t12\n 401\tval_401\t2008-04-08\t12\n+331\tval_331\t2008-04-08\t12\n+272\tval_272\t2008-04-08\t12\n+392\tval_392\t2008-04-08\t12\n+369\tval_369\t2008-04-08\t12\n 242\tval_242\t2008-04-08\t12\n-452\tval_452\t2008-04-08\t12\n-177\tval_177\t2008-04-08\t12\n-226\tval_226\t2008-04-08\t12\n-5\tval_5\t2008-04-08\t12\n-497\tval_497\t2008-04-08\t12\n-402\tval_402\t2008-04-08\t12\n-396\tval_396\t2008-04-08\t12\n-317\tval_317\t2008-04-08\t12\n+327\tval_327\t2008-04-08\t12\n+389\tval_389\t2008-04-08\t12\n+309\tval_309\t2008-04-08\t12\n+224\tval_224\t2008-04-08\t12\n+306\tval_306\t2008-04-08\t12\n+273\tval_273\t2008-04-08\t12\n+277\tval_277\t2008-04-08\t12\n+10\tval_10\t2008-04-08\t12\n+435\tval_435\t2008-04-08\t12\n+19\tval_19\t2008-04-08\t12\n+307\tval_307\t2008-04-08\t12\n+90\tval_90\t2008-04-08\t12\n+72\tval_72\t2008-04-08\t12\n+118\tval_118\t2008-04-08\t12\n+15\tval_15\t2008-04-08\t12\n+419\tval_419\t2008-04-08\t12\n+238\tval_238\t2008-04-08\t12\n+138\tval_138\t2008-04-08\t12\n+282\tval_282\t2008-04-08\t12\n 395\tval_395\t2008-04-08\t12\n-58\tval_58\t2008-04-08\t12\n-35\tval_35\t2008-04-08\t12\n-336\tval_336\t2008-04-08\t12\n-95\tval_95\t2008-04-08\t12\n-11\tval_11\t2008-04-08\t12\n-168\tval_168\t2008-04-08\t12\n-34\tval_34\t2008-04-08\t12\n-229\tval_229\t2008-04-08\t12\n-233\tval_233\t2008-04-08\t12\n-143\tval_143\t2008-04-08\t12\n-472\tval_472\t2008-04-08\t12\n-322\tval_322\t2008-04-08\t12\n-498\tval_498\t2008-04-08\t12\n-160\tval_160\t2008-04-08\t12\n-195\tval_195\t2008-04-08\t12\n-42\tval_42\t2008-04-08\t12\n-321\tval_321\t2008-04-08\t12\n-430\tval_430\t2008-04-08\t12\n-119\tval_119\t2008-04-08\t12\n-489\tval_489\t2008-04-08\t12\n-458\tval_458\t2008-04-08\t12\n-78\tval_78\t2008-04-08\t12\n-76\tval_76\t2008-04-08\t12\n-41\tval_41\t2008-04-08\t12\n-223\tval_223\t2008-04-08\t12\n-492\tval_492\t2008-04-08\t12\n-149\tval_149\t2008-04-08\t12\n-449\tval_449\t2008-04-08\t12\n-218\tval_218\t2008-04-08\t12\n-228\tval_228\t2008-04-08\t12\n-138\tval_138\t2008-04-08\t12\n-453\tval_453\t2008-04-08\t12\n-30\tval_30\t2008-04-08\t12\n-209\tval_209\t2008-04-08\t12\n-64\tval_64\t2008-04-08\t12\n+134\tval_134\t2008-04-08\t12\n+118\tval_118\t2008-04-08\t12\n+179\tval_179\t2008-04-08\t12\n+364\tval_364\t2008-04-08\t12\n+87\tval_87\t2008-04-08\t12\n+490\tval_490\t2008-04-08\t12\n+0\tval_0\t2008-04-08\t12\n+77\tval_77\t2008-04-08\t12\n+85\tval_85\t2008-04-08\t12\n+413\tval_413\t2008-04-08\t12\n+169\tval_169\t2008-04-08\t12\n+316\tval_316\t2008-04-08\t12\n+369\tval_369\t2008-04-08\t12\n+137\tval_137\t2008-04-08\t12\n+470\tval_470\t2008-04-08\t12\n+409\tval_409\t2008-04-08\t12\n+318\tval_318\t2008-04-08\t12\n+318\tval_318\t2008-04-08\t12\n+187\tval_187\t2008-04-08\t12\n+197\tval_197\t2008-04-08\t12\n+282\tval_282\t2008-04-08\t12\n+98\tval_98\t2008-04-08\t12\n+457\tval_457\t2008-04-08\t12\n+481\tval_481\t2008-04-08\t12\n+288\tval_288\t2008-04-08\t12\n+196\tval_196\t2008-04-08\t12\n+95\tval_95\t2008-04-08\t12\n+308\tval_308\t2008-04-08\t12\n 468\tval_468\t2008-04-08\t12\n-76\tval_76\t2008-04-08\t12\n-74\tval_74\t2008-04-08\t12\n-342\tval_342\t2008-04-08\t12\n-69\tval_69\t2008-04-08\t12\n+469\tval_469\t2008-04-08\t12\n+156\tval_156\t2008-04-08\t12\n+436\tval_436\t2008-04-08\t12\n+43\tval_43\t2008-04-08\t12\n+404\tval_404\t2008-04-08\t12\n+51\tval_51\t2008-04-08\t12\n+131\tval_131\t2008-04-08\t12\n+120\tval_120\t2008-04-08\t12\n+205\tval_205\t2008-04-08\t12\n 230\tval_230\t2008-04-08\t12\n-33\tval_33\t2008-04-08\t12\n-368\tval_368\t2008-04-08\t12\n-103\tval_103\t2008-04-08\t12\n-296\tval_296\t2008-04-08\t12\n-113\tval_113\t2008-04-08\t12\n-216\tval_216\t2008-04-08\t12\n-367\tval_367\t2008-04-08\t12\n-344\tval_344\t2008-04-08\t12\n-167\tval_167\t2008-04-08\t12\n-274\tval_274\t2008-04-08\t12\n-219\tval_219\t2008-04-08\t12\n-239\tval_239\t2008-04-08\t12\n-485\tval_485\t2008-04-08\t12\n-116\tval_116\t2008-04-08\t12\n-223\tval_223\t2008-04-08\t12\n-256\tval_256\t2008-04-08\t12\n-263\tval_263\t2008-04-08\t12\n-70\tval_70\t2008-04-08\t12\n-487\tval_487\t2008-04-08\t12\n-480\tval_480\t2008-04-08\t12\n-401\tval_401\t2008-04-08\t12\n-288\tval_288\t2008-04-08\t12\n+327\tval_327\t2008-04-08\t12\n+165\tval_165\t2008-04-08\t12\n+26\tval_26\t2008-04-08\t12\n+96\tval_96\t2008-04-08\t12\n+418\tval_418\t2008-04-08\t12\n 191\tval_191\t2008-04-08\t12\n-5\tval_5\t2008-04-08\t12\n-244\tval_244\t2008-04-08\t12\n-438\tval_438\t2008-04-08\t12\n-128\tval_128\t2008-04-08\t12\n-467\tval_467\t2008-04-08\t12\n-432\tval_432\t2008-04-08\t12\n-202\tval_202\t2008-04-08\t12\n-316\tval_316\t2008-04-08\t12\n-229\tval_229\t2008-04-08\t12\n-469\tval_469\t2008-04-08\t12\n+199\tval_199\t2008-04-08\t12\n+298\tval_298\t2008-04-08\t12\n+100\tval_100\t2008-04-08\t12\n+454\tval_454\t2008-04-08\t12\n+393\tval_393\t2008-04-08\t12\n+468\tval_468\t2008-04-08\t12\n+197\tval_197\t2008-04-08\t12\n+322\tval_322\t2008-04-08\t12\n+0\tval_0\t2008-04-08\t12\n+496\tval_496\t2008-04-08\t12\n+119\tval_119\t2008-04-08\t12\n+158\tval_158\t2008-04-08\t12\n+129\tval_129\t2008-04-08\t12\n+42\tval_42\t2008-04-08\t12\n+172\tval_172\t2008-04-08\t12\n+179\tval_179\t2008-04-08\t12\n+431\tval_431\t2008-04-08\t12\n 463\tval_463\t2008-04-08\t12\n-280\tval_280\t2008-04-08\t12\n-2\tval_2\t2008-04-08\t12\n-35\tval_35\t2008-04-08\t12\n-283\tval_283\t2008-04-08\t12\n-331\tval_331\t2008-04-08\t12\n-235\tval_235\t2008-04-08\t12\n-80\tval_80\t2008-04-08\t12\n-44\tval_44\t2008-04-08\t12\n-193\tval_193\t2008-04-08\t12\n-321\tval_321\t2008-04-08\t12\n-335\tval_335\t2008-04-08\t12\n-104\tval_104\t2008-04-08\t12\n+24\tval_24\t2008-04-08\t12\n+348\tval_348\t2008-04-08\t12\n+208\tval_208\t2008-04-08\t12\n+230\tval_230\t2008-04-08\t12\n+411\tval_411\t2008-04-08\t12\n+8\tval_8\t2008-04-08\t12\n+58\tval_58\t2008-04-08\t12\n 466\tval_466\t2008-04-08\t12\n-366\tval_366\t2008-04-08\t12\n-175\tval_175\t2008-04-08\t12\n-403\tval_403\t2008-04-08\t12\n-483\tval_483\t2008-04-08\t12\n-53\tval_53\t2008-04-08\t12\n-105\tval_105\t2008-04-08\t12\n-257\tval_257\t2008-04-08\t12\n-406\tval_406\t2008-04-08\t12\n-409\tval_409\t2008-04-08\t12\n-190\tval_190\t2008-04-08\t12\n-406\tval_406\t2008-04-08\t12\n-401\tval_401\t2008-04-08\t12\n-114\tval_114\t2008-04-08\t12\n-258\tval_258\t2008-04-08\t12\n-90\tval_90\t2008-04-08\t12\n-203\tval_203\t2008-04-08\t12\n-262\tval_262\t2008-04-08\t12\n 348\tval_348\t2008-04-08\t12\n-424\tval_424\t2008-04-08\t12\n-12\tval_12\t2008-04-08\t12\n-396\tval_396\t2008-04-08\t12\n-201\tval_201\t2008-04-08\t12\n+84\tval_84\t2008-04-08\t12\n 217\tval_217\t2008-04-08\t12\n-164\tval_164\t2008-04-08\t12\n-431\tval_431\t2008-04-08\t12\n-454\tval_454\t2008-04-08\t12\n-478\tval_478\t2008-04-08\t12\n-298\tval_298\t2008-04-08\t12\n-125\tval_125\t2008-04-08\t12\n-431\tval_431\t2008-04-08\t12\n-164\tval_164\t2008-04-08\t12\n-424\tval_424\t2008-04-08\t12\n+138\tval_138\t2008-04-08\t12\n+272\tval_272\t2008-04-08\t12\n+373\tval_373\t2008-04-08\t12\n+353\tval_353\t2008-04-08\t12\n+489\tval_489\t2008-04-08\t12\n+384\tval_384\t2008-04-08\t12\n+404\tval_404\t2008-04-08\t12\n+260\tval_260\t2008-04-08\t12\n+67\tval_67\t2008-04-08\t12\n+181\tval_181\t2008-04-08\t12\n+230\tval_230\t2008-04-08\t12\n+12\tval_12\t2008-04-08\t12\n+284\tval_284\t2008-04-08\t12\n+180\tval_180\t2008-04-08\t12\n+333\tval_333\t2008-04-08\t12\n+83\tval_83\t2008-04-08\t12\n+241\tval_241\t2008-04-08\t12\n+137\tval_137\t2008-04-08\t12\n+275\tval_275\t2008-04-08\t12\n+311\tval_311\t2008-04-08\t12\n+332\tval_332\t2008-04-08\t12\n+318\tval_318\t2008-04-08\t12\n+65\tval_65\t2008-04-08\t12\n+221\tval_221\t2008-04-08\t12\n+289\tval_289\t2008-04-08\t12\n+176\tval_176\t2008-04-08\t12\n+278\tval_278\t2008-04-08\t12\n+430\tval_430\t2008-04-08\t12\n+216\tval_216\t2008-04-08\t12\n+213\tval_213\t2008-04-08\t12\n+239\tval_239\t2008-04-08\t12\n+103\tval_103\t2008-04-08\t12\n+138\tval_138\t2008-04-08\t12\n+51\tval_51\t2008-04-08\t12\n+459\tval_459\t2008-04-08\t12\n+54\tval_54\t2008-04-08\t12\n+176\tval_176\t2008-04-08\t12\n 187\tval_187\t2008-04-08\t12\n+286\tval_286\t2008-04-08\t12\n+192\tval_192\t2008-04-08\t12\n+469\tval_469\t2008-04-08\t12\n+437\tval_437\t2008-04-08\t12\n+386\tval_386\t2008-04-08\t12\n+125\tval_125\t2008-04-08\t12\n+498\tval_498\t2008-04-08\t12\n 382\tval_382\t2008-04-08\t12\n-5\tval_5\t2008-04-08\t12\n-70\tval_70\t2008-04-08\t12\n-397\tval_397\t2008-04-08\t12\n-480\tval_480\t2008-04-08\t12\n-291\tval_291\t2008-04-08\t12\n-24\tval_24\t2008-04-08\t12\n-351\tval_351\t2008-04-08\t12\n-255\tval_255\t2008-04-08\t12\n-104\tval_104\t2008-04-08\t12\n-70\tval_70\t2008-04-08\t12\n-163\tval_163\t2008-04-08\t12\n-438\tval_438\t2008-04-08\t12\n-119\tval_119\t2008-04-08\t12\n-414\tval_414\t2008-04-08\t12\n-200\tval_200\t2008-04-08\t12\n-491\tval_491\t2008-04-08\t12\n-237\tval_237\t2008-04-08\t12\n-439\tval_439\t2008-04-08\t12\n-360\tval_360\t2008-04-08\t12\n-248\tval_248\t2008-04-08\t12\n-479\tval_479\t2008-04-08\t12\n-305\tval_305\t2008-04-08\t12\n-417\tval_417\t2008-04-08\t12\n-199\tval_199\t2008-04-08\t12\n-444\tval_444\t2008-04-08\t12\n-120\tval_120\t2008-04-08\t12\n-429\tval_429\t2008-04-08\t12\n 169\tval_169\t2008-04-08\t12\n-443\tval_443\t2008-04-08\t12\n-323\tval_323\t2008-04-08\t12\n-325\tval_325\t2008-04-08\t12\n+399\tval_399\t2008-04-08\t12\n+356\tval_356\t2008-04-08\t12\n+208\tval_208\t2008-04-08\t12\n 277\tval_277\t2008-04-08\t12\n-230\tval_230\t2008-04-08\t12\n-478\tval_478\t2008-04-08\t12\n-178\tval_178\t2008-04-08\t12\n-468\tval_468\t2008-04-08\t12\n-310\tval_310\t2008-04-08\t12\n-317\tval_317\t2008-04-08\t12\n-333\tval_333\t2008-04-08\t12\n-493\tval_493\t2008-04-08\t12\n-460\tval_460\t2008-04-08\t12\n-207\tval_207\t2008-04-08\t12\n-249\tval_249\t2008-04-08\t12\n-265\tval_265\t2008-04-08\t12\n-480\tval_480\t2008-04-08\t12\n-83\tval_83\t2008-04-08\t12\n-136\tval_136\t2008-04-08\t12\n-353\tval_353\t2008-04-08\t12\n-172\tval_172\t2008-04-08\t12\n-214\tval_214\t2008-04-08\t12\n-462\tval_462\t2008-04-08\t12\n-233\tval_233\t2008-04-08\t12\n-406\tval_406\t2008-04-08\t12\n-133\tval_133\t2008-04-08\t12\n-175\tval_175\t2008-04-08\t12\n-189\tval_189\t2008-04-08\t12\n-454\tval_454\t2008-04-08\t12\n-375\tval_375\t2008-04-08\t12\n-401\tval_401\t2008-04-08\t12\n-421\tval_421\t2008-04-08\t12\n-407\tval_407\t2008-04-08\t12\n-384\tval_384\t2008-04-08\t12\n-256\tval_256\t2008-04-08\t12\n-26\tval_26\t2008-04-08\t12\n-134\tval_134\t2008-04-08\t12\n-67\tval_67\t2008-04-08\t12\n-384\tval_384\t2008-04-08\t12\n-379\tval_379\t2008-04-08\t12\n-18\tval_18\t2008-04-08\t12\n-462\tval_462\t2008-04-08\t12\n-492\tval_492\t2008-04-08\t12\n-100\tval_100\t2008-04-08\t12\n-298\tval_298\t2008-04-08\t12\n-9\tval_9\t2008-04-08\t12\n-341\tval_341\t2008-04-08\t12\n-498\tval_498\t2008-04-08\t12\n-146\tval_146\t2008-04-08\t12\n-458\tval_458\t2008-04-08\t12\n-362\tval_362\t2008-04-08\t12\n-186\tval_186\t2008-04-08\t12\n-285\tval_285\t2008-04-08\t12\n-348\tval_348\t2008-04-08\t12\n+427\tval_427\t2008-04-08\t12\n+35\tval_35\t2008-04-08\t12\n+280\tval_280\t2008-04-08\t12\n+4\tval_4\t2008-04-08\t12\n+72\tval_72\t2008-04-08\t12\n+47\tval_47\t2008-04-08\t12\n+111\tval_111\t2008-04-08\t12\n+92\tval_92\t2008-04-08\t12\n+221\tval_221\t2008-04-08\t12\n+378\tval_378\t2008-04-08\t12\n+157\tval_157\t2008-04-08\t12\n+489\tval_489\t2008-04-08\t12\n+20\tval_20\t2008-04-08\t12\n+170\tval_170\t2008-04-08\t12\n+129\tval_129\t2008-04-08\t12\n+345\tval_345\t2008-04-08\t12\n+438\tval_438\t2008-04-08\t12\n+149\tval_149\t2008-04-08\t12\n+205\tval_205\t2008-04-08\t12\n+302\tval_302\t2008-04-08\t12\n+57\tval_57\t2008-04-08\t12\n+316\tval_316\t2008-04-08\t12\n+311\tval_311\t2008-04-08\t12\n+128\tval_128\t2008-04-08\t12\n+455\tval_455\t2008-04-08\t12\n+0\tval_0\t2008-04-08\t12\n+339\tval_339\t2008-04-08\t12\n+203\tval_203\t2008-04-08\t12\n+155\tval_155\t2008-04-08\t12\n+113\tval_113\t2008-04-08\t12\n+17\tval_17\t2008-04-08\t12\n+475\tval_475\t2008-04-08\t12\n+195\tval_195\t2008-04-08\t12\n 167\tval_167\t2008-04-08\t12\n-18\tval_18\t2008-04-08\t12\n-273\tval_273\t2008-04-08\t12\n-183\tval_183\t2008-04-08\t12\n+325\tval_325\t2008-04-08\t12\n+367\tval_367\t2008-04-08\t12\n+342\tval_342\t2008-04-08\t12\n+439\tval_439\t2008-04-08\t12\n+266\tval_266\t2008-04-08\t12\n+365\tval_365\t2008-04-08\t12\n+309\tval_309\t2008-04-08\t12\n+397\tval_397\t2008-04-08\t12\n+377\tval_377\t2008-04-08\t12\n+162\tval_162\t2008-04-08\t12\n+489\tval_489\t2008-04-08\t12\n+417\tval_417\t2008-04-08\t12\n+247\tval_247\t2008-04-08\t12\n+396\tval_396\t2008-04-08\t12\n+399\tval_399\t2008-04-08\t12\n+174\tval_174\t2008-04-08\t12\n+208\tval_208\t2008-04-08\t12\n+466\tval_466\t2008-04-08\t12\n+199\tval_199\t2008-04-08\t12\n+207\tval_207\t2008-04-08\t12\n+494\tval_494\t2008-04-08\t12\n+413\tval_413\t2008-04-08\t12\n+174\tval_174\t2008-04-08\t12\n+482\tval_482\t2008-04-08\t12\n+237\tval_237\t2008-04-08\t12\n+394\tval_394\t2008-04-08\t12\n+459\tval_459\t2008-04-08\t12\n+446\tval_446\t2008-04-08\t12\n+338\tval_338\t2008-04-08\t12\n+193\tval_193\t2008-04-08\t12\n+153\tval_153\t2008-04-08\t12\n+287\tval_287\t2008-04-08\t12\n+219\tval_219\t2008-04-08\t12\n+292\tval_292\t2008-04-08\t12\n+252\tval_252\t2008-04-08\t12\n+430\tval_430\t2008-04-08\t12\n+417\tval_417\t2008-04-08\t12\n+166\tval_166\t2008-04-08\t12\n+403\tval_403\t2008-04-08\t12\n+82\tval_82\t2008-04-08\t12\n+15\tval_15\t2008-04-08\t12\n+209\tval_209\t2008-04-08\t12\n+277\tval_277\t2008-04-08\t12\n 281\tval_281\t2008-04-08\t12\n-344\tval_344\t2008-04-08\t12\n-97\tval_97\t2008-04-08\t12\n-469\tval_469\t2008-04-08\t12\n-315\tval_315\t2008-04-08\t12\n-84\tval_84\t2008-04-08\t12\n-28\tval_28\t2008-04-08\t12\n+327\tval_327\t2008-04-08\t12\n 37\tval_37\t2008-04-08\t12\n-448\tval_448\t2008-04-08\t12\n+495\tval_495\t2008-04-08\t12\n+145\tval_145\t2008-04-08\t12\n+469\tval_469\t2008-04-08\t12\n 152\tval_152\t2008-04-08\t12\n-348\tval_348\t2008-04-08\t12\n-307\tval_307\t2008-04-08\t12\n-194\tval_194\t2008-04-08\t12\n-414\tval_414\t2008-04-08\t12\n-477\tval_477\t2008-04-08\t12\n-222\tval_222\t2008-04-08\t12\n-126\tval_126\t2008-04-08\t12\n-90\tval_90\t2008-04-08\t12\n-169\tval_169\t2008-04-08\t12\n-403\tval_403\t2008-04-08\t12\n-400\tval_400\t2008-04-08\t12\n-200\tval_200\t2008-04-08\t12\n-97\tval_97\t2008-04-08\t12\n+374\tval_374\t2008-04-08\t12\n+429\tval_429\t2008-04-08\t12\n+406\tval_406\t2008-04-08\t12\n+146\tval_146\t2008-04-08\t12\n+213\tval_213\t2008-04-08\t12\n+128\tval_128\t2008-04-08\t12\n+66\tval_66\t2008-04-08\t12\n+369\tval_369\t2008-04-08\t12\n+224\tval_224\t2008-04-08\t12\n+273\tval_273\t2008-04-08\t12\n+150\tval_150\t2008-04-08\t12\n+401\tval_401\t2008-04-08\t12\n+193\tval_193\t2008-04-08\t12\n+265\tval_265\t2008-04-08\t12\n+484\tval_484\t2008-04-08\t12\n+98\tval_98\t2008-04-08\t12\n+278\tval_278\t2008-04-08\t12\n+255\tval_255\t2008-04-08\t12\n+409\tval_409\t2008-04-08\t12\n+165\tval_165\t2008-04-08\t12\n+27\tval_27\t2008-04-08\t12\n+311\tval_311\t2008-04-08\t12\n+86\tval_86\t2008-04-08\t12\n+238\tval_238\t2008-04-08\t12\n 238\tval_238\t2008-04-09\t11\n-86\tval_86\t2008-04-09\t11\n-311\tval_311\t2008-04-09\t11\n-27\tval_27\t2008-04-09\t11\n-165\tval_165\t2008-04-09\t11\n-409\tval_409\t2008-04-09\t11\n-255\tval_255\t2008-04-09\t11\n-278\tval_278\t2008-04-09\t11\n-98\tval_98\t2008-04-09\t11\n-484\tval_484\t2008-04-09\t11\n-265\tval_265\t2008-04-09\t11\n-193\tval_193\t2008-04-09\t11\n-401\tval_401\t2008-04-09\t11\n-150\tval_150\t2008-04-09\t11\n-273\tval_273\t2008-04-09\t11\n-224\tval_224\t2008-04-09\t11\n-369\tval_369\t2008-04-09\t11\n-66\tval_66\t2008-04-09\t11\n-128\tval_128\t2008-04-09\t11\n-213\tval_213\t2008-04-09\t11\n-146\tval_146\t2008-04-09\t11\n-406\tval_406\t2008-04-09\t11\n-429\tval_429\t2008-04-09\t11\n-374\tval_374\t2008-04-09\t11\n+97\tval_97\t2008-04-09\t11\n+200\tval_200\t2008-04-09\t11\n+400\tval_400\t2008-04-09\t11\n+403\tval_403\t2008-04-09\t11\n+169\tval_169\t2008-04-09\t11\n+90\tval_90\t2008-04-09\t11\n+126\tval_126\t2008-04-09\t11\n+222\tval_222\t2008-04-09\t11\n+477\tval_477\t2008-04-09\t11\n+414\tval_414\t2008-04-09\t11\n+194\tval_194\t2008-04-09\t11\n+307\tval_307\t2008-04-09\t11\n+348\tval_348\t2008-04-09\t11\n 152\tval_152\t2008-04-09\t11\n-469\tval_469\t2008-04-09\t11\n-145\tval_145\t2008-04-09\t11\n-495\tval_495\t2008-04-09\t11\n+448\tval_448\t2008-04-09\t11\n 37\tval_37\t2008-04-09\t11\n-327\tval_327\t2008-04-09\t11\n+28\tval_28\t2008-04-09\t11\n+84\tval_84\t2008-04-09\t11\n+315\tval_315\t2008-04-09\t11\n+469\tval_469\t2008-04-09\t11\n+97\tval_97\t2008-04-09\t11\n+344\tval_344\t2008-04-09\t11\n 281\tval_281\t2008-04-09\t11\n-277\tval_277\t2008-04-09\t11\n-209\tval_209\t2008-04-09\t11\n-15\tval_15\t2008-04-09\t11\n-82\tval_82\t2008-04-09\t11\n-403\tval_403\t2008-04-09\t11\n-166\tval_166\t2008-04-09\t11\n-417\tval_417\t2008-04-09\t11\n-430\tval_430\t2008-04-09\t11\n-252\tval_252\t2008-04-09\t11\n-292\tval_292\t2008-04-09\t11\n-219\tval_219\t2008-04-09\t11\n-287\tval_287\t2008-04-09\t11\n-153\tval_153\t2008-04-09\t11\n-193\tval_193\t2008-04-09\t11\n-338\tval_338\t2008-04-09\t11\n-446\tval_446\t2008-04-09\t11\n-459\tval_459\t2008-04-09\t11\n-394\tval_394\t2008-04-09\t11\n-237\tval_237\t2008-04-09\t11\n-482\tval_482\t2008-04-09\t11\n-174\tval_174\t2008-04-09\t11\n-413\tval_413\t2008-04-09\t11\n-494\tval_494\t2008-04-09\t11\n-207\tval_207\t2008-04-09\t11\n-199\tval_199\t2008-04-09\t11\n-466\tval_466\t2008-04-09\t11\n-208\tval_208\t2008-04-09\t11\n-174\tval_174\t2008-04-09\t11\n-399\tval_399\t2008-04-09\t11\n-396\tval_396\t2008-04-09\t11\n-247\tval_247\t2008-04-09\t11\n-417\tval_417\t2008-04-09\t11\n-489\tval_489\t2008-04-09\t11\n-162\tval_162\t2008-04-09\t11\n-377\tval_377\t2008-04-09\t11\n-397\tval_397\t2008-04-09\t11\n-309\tval_309\t2008-04-09\t11\n-365\tval_365\t2008-04-09\t11\n-266\tval_266\t2008-04-09\t11\n-439\tval_439\t2008-04-09\t11\n-342\tval_342\t2008-04-09\t11\n-367\tval_367\t2008-04-09\t11\n-325\tval_325\t2008-04-09\t11\n+183\tval_183\t2008-04-09\t11\n+273\tval_273\t2008-04-09\t11\n+18\tval_18\t2008-04-09\t11\n 167\tval_167\t2008-04-09\t11\n-195\tval_195\t2008-04-09\t11\n-475\tval_475\t2008-04-09\t11\n-17\tval_17\t2008-04-09\t11\n-113\tval_113\t2008-04-09\t11\n-155\tval_155\t2008-04-09\t11\n-203\tval_203\t2008-04-09\t11\n-339\tval_339\t2008-04-09\t11\n-0\tval_0\t2008-04-09\t11\n-455\tval_455\t2008-04-09\t11\n-128\tval_128\t2008-04-09\t11\n-311\tval_311\t2008-04-09\t11\n-316\tval_316\t2008-04-09\t11\n-57\tval_57\t2008-04-09\t11\n-302\tval_302\t2008-04-09\t11\n-205\tval_205\t2008-04-09\t11\n-149\tval_149\t2008-04-09\t11\n-438\tval_438\t2008-04-09\t11\n-345\tval_345\t2008-04-09\t11\n-129\tval_129\t2008-04-09\t11\n-170\tval_170\t2008-04-09\t11\n-20\tval_20\t2008-04-09\t11\n-489\tval_489\t2008-04-09\t11\n-157\tval_157\t2008-04-09\t11\n-378\tval_378\t2008-04-09\t11\n-221\tval_221\t2008-04-09\t11\n-92\tval_92\t2008-04-09\t11\n-111\tval_111\t2008-04-09\t11\n-47\tval_47\t2008-04-09\t11\n-72\tval_72\t2008-04-09\t11\n-4\tval_4\t2008-04-09\t11\n-280\tval_280\t2008-04-09\t11\n-35\tval_35\t2008-04-09\t11\n-427\tval_427\t2008-04-09\t11\n+348\tval_348\t2008-04-09\t11\n+285\tval_285\t2008-04-09\t11\n+186\tval_186\t2008-04-09\t11\n+362\tval_362\t2008-04-09\t11\n+458\tval_458\t2008-04-09\t11\n+146\tval_146\t2008-04-09\t11\n+498\tval_498\t2008-04-09\t11\n+341\tval_341\t2008-04-09\t11\n+9\tval_9\t2008-04-09\t11\n+298\tval_298\t2008-04-09\t11\n+100\tval_100\t2008-04-09\t11\n+492\tval_492\t2008-04-09\t11\n+462\tval_462\t2008-04-09\t11\n+18\tval_18\t2008-04-09\t11\n+379\tval_379\t2008-04-09\t11\n+384\tval_384\t2008-04-09\t11\n+67\tval_67\t2008-04-09\t11\n+134\tval_134\t2008-04-09\t11\n+26\tval_26\t2008-04-09\t11\n+256\tval_256\t2008-04-09\t11\n+384\tval_384\t2008-04-09\t11\n+407\tval_407\t2008-04-09\t11\n+421\tval_421\t2008-04-09\t11\n+401\tval_401\t2008-04-09\t11\n+375\tval_375\t2008-04-09\t11\n+454\tval_454\t2008-04-09\t11\n+189\tval_189\t2008-04-09\t11\n+175\tval_175\t2008-04-09\t11\n+133\tval_133\t2008-04-09\t11\n+406\tval_406\t2008-04-09\t11\n+233\tval_233\t2008-04-09\t11\n+462\tval_462\t2008-04-09\t11\n+214\tval_214\t2008-04-09\t11\n+172\tval_172\t2008-04-09\t11\n+353\tval_353\t2008-04-09\t11\n+136\tval_136\t2008-04-09\t11\n+83\tval_83\t2008-04-09\t11\n+480\tval_480\t2008-04-09\t11\n+265\tval_265\t2008-04-09\t11\n+249\tval_249\t2008-04-09\t11\n+207\tval_207\t2008-04-09\t11\n+460\tval_460\t2008-04-09\t11\n+493\tval_493\t2008-04-09\t11\n+333\tval_333\t2008-04-09\t11\n+317\tval_317\t2008-04-09\t11\n+310\tval_310\t2008-04-09\t11\n+468\tval_468\t2008-04-09\t11\n+178\tval_178\t2008-04-09\t11\n+478\tval_478\t2008-04-09\t11\n+230\tval_230\t2008-04-09\t11\n 277\tval_277\t2008-04-09\t11\n-208\tval_208\t2008-04-09\t11\n-356\tval_356\t2008-04-09\t11\n-399\tval_399\t2008-04-09\t11\n+325\tval_325\t2008-04-09\t11\n+323\tval_323\t2008-04-09\t11\n+443\tval_443\t2008-04-09\t11\n 169\tval_169\t2008-04-09\t11\n+429\tval_429\t2008-04-09\t11\n+120\tval_120\t2008-04-09\t11\n+444\tval_444\t2008-04-09\t11\n+199\tval_199\t2008-04-09\t11\n+417\tval_417\t2008-04-09\t11\n+305\tval_305\t2008-04-09\t11\n+479\tval_479\t2008-04-09\t11\n+248\tval_248\t2008-04-09\t11\n+360\tval_360\t2008-04-09\t11\n+439\tval_439\t2008-04-09\t11\n+237\tval_237\t2008-04-09\t11\n+491\tval_491\t2008-04-09\t11\n+200\tval_200\t2008-04-09\t11\n+414\tval_414\t2008-04-09\t11\n+119\tval_119\t2008-04-09\t11\n+438\tval_438\t2008-04-09\t11\n+163\tval_163\t2008-04-09\t11\n+70\tval_70\t2008-04-09\t11\n+104\tval_104\t2008-04-09\t11\n+255\tval_255\t2008-04-09\t11\n+351\tval_351\t2008-04-09\t11\n+24\tval_24\t2008-04-09\t11\n+291\tval_291\t2008-04-09\t11\n+480\tval_480\t2008-04-09\t11\n+397\tval_397\t2008-04-09\t11\n+70\tval_70\t2008-04-09\t11\n+5\tval_5\t2008-04-09\t11\n 382\tval_382\t2008-04-09\t11\n-498\tval_498\t2008-04-09\t11\n-125\tval_125\t2008-04-09\t11\n-386\tval_386\t2008-04-09\t11\n-437\tval_437\t2008-04-09\t11\n-469\tval_469\t2008-04-09\t11\n-192\tval_192\t2008-04-09\t11\n-286\tval_286\t2008-04-09\t11\n 187\tval_187\t2008-04-09\t11\n-176\tval_176\t2008-04-09\t11\n-54\tval_54\t2008-04-09\t11\n-459\tval_459\t2008-04-09\t11\n-51\tval_51\t2008-04-09\t11\n-138\tval_138\t2008-04-09\t11\n-103\tval_103\t2008-04-09\t11\n-239\tval_239\t2008-04-09\t11\n-213\tval_213\t2008-04-09\t11\n-216\tval_216\t2008-04-09\t11\n-430\tval_430\t2008-04-09\t11\n-278\tval_278\t2008-04-09\t11\n-176\tval_176\t2008-04-09\t11\n-289\tval_289\t2008-04-09\t11\n-221\tval_221\t2008-04-09\t11\n-65\tval_65\t2008-04-09\t11\n-318\tval_318\t2008-04-09\t11\n-332\tval_332\t2008-04-09\t11\n-311\tval_311\t2008-04-09\t11\n-275\tval_275\t2008-04-09\t11\n-137\tval_137\t2008-04-09\t11\n-241\tval_241\t2008-04-09\t11\n-83\tval_83\t2008-04-09\t11\n-333\tval_333\t2008-04-09\t11\n-180\tval_180\t2008-04-09\t11\n-284\tval_284\t2008-04-09\t11\n-12\tval_12\t2008-04-09\t11\n-230\tval_230\t2008-04-09\t11\n-181\tval_181\t2008-04-09\t11\n-67\tval_67\t2008-04-09\t11\n-260\tval_260\t2008-04-09\t11\n-404\tval_404\t2008-04-09\t11\n-384\tval_384\t2008-04-09\t11\n-489\tval_489\t2008-04-09\t11\n-353\tval_353\t2008-04-09\t11\n-373\tval_373\t2008-04-09\t11\n-272\tval_272\t2008-04-09\t11\n-138\tval_138\t2008-04-09\t11\n+424\tval_424\t2008-04-09\t11\n+164\tval_164\t2008-04-09\t11\n+431\tval_431\t2008-04-09\t11\n+125\tval_125\t2008-04-09\t11\n+298\tval_298\t2008-04-09\t11\n+478\tval_478\t2008-04-09\t11\n+454\tval_454\t2008-04-09\t11\n+431\tval_431\t2008-04-09\t11\n+164\tval_164\t2008-04-09\t11\n 217\tval_217\t2008-04-09\t11\n-84\tval_84\t2008-04-09\t11\n+201\tval_201\t2008-04-09\t11\n+396\tval_396\t2008-04-09\t11\n+12\tval_12\t2008-04-09\t11\n+424\tval_424\t2008-04-09\t11\n 348\tval_348\t2008-04-09\t11\n+262\tval_262\t2008-04-09\t11\n+203\tval_203\t2008-04-09\t11\n+90\tval_90\t2008-04-09\t11\n+258\tval_258\t2008-04-09\t11\n+114\tval_114\t2008-04-09\t11\n+401\tval_401\t2008-04-09\t11\n+406\tval_406\t2008-04-09\t11\n+190\tval_190\t2008-04-09\t11\n+409\tval_409\t2008-04-09\t11\n+406\tval_406\t2008-04-09\t11\n+257\tval_257\t2008-04-09\t11\n+105\tval_105\t2008-04-09\t11\n+53\tval_53\t2008-04-09\t11\n+483\tval_483\t2008-04-09\t11\n+403\tval_403\t2008-04-09\t11\n+175\tval_175\t2008-04-09\t11\n+366\tval_366\t2008-04-09\t11\n 466\tval_466\t2008-04-09\t11\n-58\tval_58\t2008-04-09\t11\n-8\tval_8\t2008-04-09\t11\n-411\tval_411\t2008-04-09\t11\n-230\tval_230\t2008-04-09\t11\n-208\tval_208\t2008-04-09\t11\n-348\tval_348\t2008-04-09\t11\n-24\tval_24\t2008-04-09\t11\n+104\tval_104\t2008-04-09\t11\n+335\tval_335\t2008-04-09\t11\n+321\tval_321\t2008-04-09\t11\n+193\tval_193\t2008-04-09\t11\n+44\tval_44\t2008-04-09\t11\n+80\tval_80\t2008-04-09\t11\n+235\tval_235\t2008-04-09\t11\n+331\tval_331\t2008-04-09\t11\n+283\tval_283\t2008-04-09\t11\n+35\tval_35\t2008-04-09\t11\n+2\tval_2\t2008-04-09\t11\n+280\tval_280\t2008-04-09\t11\n 463\tval_463\t2008-04-09\t11\n-431\tval_431\t2008-04-09\t11\n-179\tval_179\t2008-04-09\t11\n-172\tval_172\t2008-04-09\t11\n-42\tval_42\t2008-04-09\t11\n-129\tval_129\t2008-04-09\t11\n-158\tval_158\t2008-04-09\t11\n-119\tval_119\t2008-04-09\t11\n-496\tval_496\t2008-04-09\t11\n-0\tval_0\t2008-04-09\t11\n-322\tval_322\t2008-04-09\t11\n-197\tval_197\t2008-04-09\t11\n-468\tval_468\t2008-04-09\t11\n-393\tval_393\t2008-04-09\t11\n-454\tval_454\t2008-04-09\t11\n-100\tval_100\t2008-04-09\t11\n-298\tval_298\t2008-04-09\t11\n-199\tval_199\t2008-04-09\t11\n+469\tval_469\t2008-04-09\t11\n+229\tval_229\t2008-04-09\t11\n+316\tval_316\t2008-04-09\t11\n+202\tval_202\t2008-04-09\t11\n+432\tval_432\t2008-04-09\t11\n+467\tval_467\t2008-04-09\t11\n+128\tval_128\t2008-04-09\t11\n+438\tval_438\t2008-04-09\t11\n+244\tval_244\t2008-04-09\t11\n+5\tval_5\t2008-04-09\t11\n 191\tval_191\t2008-04-09\t11\n-418\tval_418\t2008-04-09\t11\n-96\tval_96\t2008-04-09\t11\n-26\tval_26\t2008-04-09\t11\n-165\tval_165\t2008-04-09\t11\n-327\tval_327\t2008-04-09\t11\n+288\tval_288\t2008-04-09\t11\n+401\tval_401\t2008-04-09\t11\n+480\tval_480\t2008-04-09\t11\n+487\tval_487\t2008-04-09\t11\n+70\tval_70\t2008-04-09\t11\n+263\tval_263\t2008-04-09\t11\n+256\tval_256\t2008-04-09\t11\n+223\tval_223\t2008-04-09\t11\n+116\tval_116\t2008-04-09\t11\n+485\tval_485\t2008-04-09\t11\n+239\tval_239\t2008-04-09\t11\n+219\tval_219\t2008-04-09\t11\n+274\tval_274\t2008-04-09\t11\n+167\tval_167\t2008-04-09\t11\n+344\tval_344\t2008-04-09\t11\n+367\tval_367\t2008-04-09\t11\n+216\tval_216\t2008-04-09\t11\n+113\tval_113\t2008-04-09\t11\n+296\tval_296\t2008-04-09\t11\n+103\tval_103\t2008-04-09\t11\n+368\tval_368\t2008-04-09\t11\n+33\tval_33\t2008-04-09\t11\n 230\tval_230\t2008-04-09\t11\n-205\tval_205\t2008-04-09\t11\n-120\tval_120\t2008-04-09\t11\n-131\tval_131\t2008-04-09\t11\n-51\tval_51\t2008-04-09\t11\n-404\tval_404\t2008-04-09\t11\n-43\tval_43\t2008-04-09\t11\n-436\tval_436\t2008-04-09\t11\n-156\tval_156\t2008-04-09\t11\n-469\tval_469\t2008-04-09\t11\n+69\tval_69\t2008-04-09\t11\n+342\tval_342\t2008-04-09\t11\n+74\tval_74\t2008-04-09\t11\n+76\tval_76\t2008-04-09\t11\n 468\tval_468\t2008-04-09\t11\n-308\tval_308\t2008-04-09\t11\n-95\tval_95\t2008-04-09\t11\n-196\tval_196\t2008-04-09\t11\n-288\tval_288\t2008-04-09\t11\n-481\tval_481\t2008-04-09\t11\n-457\tval_457\t2008-04-09\t11\n-98\tval_98\t2008-04-09\t11\n-282\tval_282\t2008-04-09\t11\n-197\tval_197\t2008-04-09\t11\n-187\tval_187\t2008-04-09\t11\n-318\tval_318\t2008-04-09\t11\n-318\tval_318\t2008-04-09\t11\n-409\tval_409\t2008-04-09\t11\n-470\tval_470\t2008-04-09\t11\n-137\tval_137\t2008-04-09\t11\n-369\tval_369\t2008-04-09\t11\n-316\tval_316\t2008-04-09\t11\n-169\tval_169\t2008-04-09\t11\n-413\tval_413\t2008-04-09\t11\n-85\tval_85\t2008-04-09\t11\n-77\tval_77\t2008-04-09\t11\n-0\tval_0\t2008-04-09\t11\n-490\tval_490\t2008-04-09\t11\n-87\tval_87\t2008-04-09\t11\n-364\tval_364\t2008-04-09\t11\n-179\tval_179\t2008-04-09\t11\n-118\tval_118\t2008-04-09\t11\n-134\tval_134\t2008-04-09\t11\n+64\tval_64\t2008-04-09\t11\n+209\tval_209\t2008-04-09\t11\n+30\tval_30\t2008-04-09\t11\n+453\tval_453\t2008-04-09\t11\n+138\tval_138\t2008-04-09\t11\n+228\tval_228\t2008-04-09\t11\n+218\tval_218\t2008-04-09\t11\n+449\tval_449\t2008-04-09\t11\n+149\tval_149\t2008-04-09\t11\n+492\tval_492\t2008-04-09\t11\n+223\tval_223\t2008-04-09\t11\n+41\tval_41\t2008-04-09\t11\n+76\tval_76\t2008-04-09\t11\n+78\tval_78\t2008-04-09\t11\n+458\tval_458\t2008-04-09\t11\n+489\tval_489\t2008-04-09\t11\n+119\tval_119\t2008-04-09\t11\n+430\tval_430\t2008-04-09\t11\n+321\tval_321\t2008-04-09\t11\n+42\tval_42\t2008-04-09\t11\n+195\tval_195\t2008-04-09\t11\n+160\tval_160\t2008-04-09\t11\n+498\tval_498\t2008-04-09\t11\n+322\tval_322\t2008-04-09\t11\n+472\tval_472\t2008-04-09\t11\n+143\tval_143\t2008-04-09\t11\n+233\tval_233\t2008-04-09\t11\n+229\tval_229\t2008-04-09\t11\n+34\tval_34\t2008-04-09\t11\n+168\tval_168\t2008-04-09\t11\n+11\tval_11\t2008-04-09\t11\n+95\tval_95\t2008-04-09\t11\n+336\tval_336\t2008-04-09\t11\n+35\tval_35\t2008-04-09\t11\n+58\tval_58\t2008-04-09\t11\n 395\tval_395\t2008-04-09\t11\n-282\tval_282\t2008-04-09\t11\n-138\tval_138\t2008-04-09\t11\n-238\tval_238\t2008-04-09\t11\n-419\tval_419\t2008-04-09\t11\n-15\tval_15\t2008-04-09\t11\n-118\tval_118\t2008-04-09\t11\n-72\tval_72\t2008-04-09\t11\n-90\tval_90\t2008-04-09\t11\n-307\tval_307\t2008-04-09\t11\n-19\tval_19\t2008-04-09\t11\n-435\tval_435\t2008-04-09\t11\n-10\tval_10\t2008-04-09\t11\n-277\tval_277\t2008-04-09\t11\n-273\tval_273\t2008-04-09\t11\n-306\tval_306\t2008-04-09\t11\n-224\tval_224\t2008-04-09\t11\n-309\tval_309\t2008-04-09\t11\n-389\tval_389\t2008-04-09\t11\n-327\tval_327\t2008-04-09\t11\n+317\tval_317\t2008-04-09\t11\n+396\tval_396\t2008-04-09\t11\n+402\tval_402\t2008-04-09\t11\n+497\tval_497\t2008-04-09\t11\n+5\tval_5\t2008-04-09\t11\n+226\tval_226\t2008-04-09\t11\n+177\tval_177\t2008-04-09\t11\n+452\tval_452\t2008-04-09\t11\n 242\tval_242\t2008-04-09\t11\n-369\tval_369\t2008-04-09\t11\n-392\tval_392\t2008-04-09\t11\n-272\tval_272\t2008-04-09\t11\n-331\tval_331\t2008-04-09\t11\n 401\tval_401\t2008-04-09\t11\n+331\tval_331\t2008-04-09\t11\n+272\tval_272\t2008-04-09\t11\n+392\tval_392\t2008-04-09\t11\n+369\tval_369\t2008-04-09\t11\n 242\tval_242\t2008-04-09\t11\n-452\tval_452\t2008-04-09\t11\n-177\tval_177\t2008-04-09\t11\n-226\tval_226\t2008-04-09\t11\n-5\tval_5\t2008-04-09\t11\n-497\tval_497\t2008-04-09\t11\n-402\tval_402\t2008-04-09\t11\n-396\tval_396\t2008-04-09\t11\n-317\tval_317\t2008-04-09\t11\n+327\tval_327\t2008-04-09\t11\n+389\tval_389\t2008-04-09\t11\n+309\tval_309\t2008-04-09\t11\n+224\tval_224\t2008-04-09\t11\n+306\tval_306\t2008-04-09\t11\n+273\tval_273\t2008-04-09\t11\n+277\tval_277\t2008-04-09\t11\n+10\tval_10\t2008-04-09\t11\n+435\tval_435\t2008-04-09\t11\n+19\tval_19\t2008-04-09\t11\n+307\tval_307\t2008-04-09\t11\n+90\tval_90\t2008-04-09\t11\n+72\tval_72\t2008-04-09\t11\n+118\tval_118\t2008-04-09\t11\n+15\tval_15\t2008-04-09\t11\n+419\tval_419\t2008-04-09\t11\n+238\tval_238\t2008-04-09\t11\n+138\tval_138\t2008-04-09\t11\n+282\tval_282\t2008-04-09\t11\n 395\tval_395\t2008-04-09\t11\n-58\tval_58\t2008-04-09\t11\n-35\tval_35\t2008-04-09\t11\n-336\tval_336\t2008-04-09\t11\n+134\tval_134\t2008-04-09\t11\n+118\tval_118\t2008-04-09\t11\n+179\tval_179\t2008-04-09\t11\n+364\tval_364\t2008-04-09\t11\n+87\tval_87\t2008-04-09\t11\n+490\tval_490\t2008-04-09\t11\n+0\tval_0\t2008-04-09\t11\n+77\tval_77\t2008-04-09\t11\n+85\tval_85\t2008-04-09\t11\n+413\tval_413\t2008-04-09\t11\n+169\tval_169\t2008-04-09\t11\n+316\tval_316\t2008-04-09\t11\n+369\tval_369\t2008-04-09\t11\n+137\tval_137\t2008-04-09\t11\n+470\tval_470\t2008-04-09\t11\n+409\tval_409\t2008-04-09\t11\n+318\tval_318\t2008-04-09\t11\n+318\tval_318\t2008-04-09\t11\n+187\tval_187\t2008-04-09\t11\n+197\tval_197\t2008-04-09\t11\n+282\tval_282\t2008-04-09\t11\n+98\tval_98\t2008-04-09\t11\n+457\tval_457\t2008-04-09\t11\n+481\tval_481\t2008-04-09\t11\n+288\tval_288\t2008-04-09\t11\n+196\tval_196\t2008-04-09\t11\n 95\tval_95\t2008-04-09\t11\n-11\tval_11\t2008-04-09\t11\n-168\tval_168\t2008-04-09\t11\n-34\tval_34\t2008-04-09\t11\n-229\tval_229\t2008-04-09\t11\n-233\tval_233\t2008-04-09\t11\n-143\tval_143\t2008-04-09\t11\n-472\tval_472\t2008-04-09\t11\n-322\tval_322\t2008-04-09\t11\n-498\tval_498\t2008-04-09\t11\n-160\tval_160\t2008-04-09\t11\n-195\tval_195\t2008-04-09\t11\n-42\tval_42\t2008-04-09\t11\n-321\tval_321\t2008-04-09\t11\n-430\tval_430\t2008-04-09\t11\n-119\tval_119\t2008-04-09\t11\n-489\tval_489\t2008-04-09\t11\n-458\tval_458\t2008-04-09\t11\n-78\tval_78\t2008-04-09\t11\n-76\tval_76\t2008-04-09\t11\n-41\tval_41\t2008-04-09\t11\n-223\tval_223\t2008-04-09\t11\n-492\tval_492\t2008-04-09\t11\n-149\tval_149\t2008-04-09\t11\n-449\tval_449\t2008-04-09\t11\n-218\tval_218\t2008-04-09\t11\n-228\tval_228\t2008-04-09\t11\n-138\tval_138\t2008-04-09\t11\n-453\tval_453\t2008-04-09\t11\n-30\tval_30\t2008-04-09\t11\n-209\tval_209\t2008-04-09\t11\n-64\tval_64\t2008-04-09\t11\n+308\tval_308\t2008-04-09\t11\n 468\tval_468\t2008-04-09\t11\n-76\tval_76\t2008-04-09\t11\n-74\tval_74\t2008-04-09\t11\n-342\tval_342\t2008-04-09\t11\n-69\tval_69\t2008-04-09\t11\n+469\tval_469\t2008-04-09\t11\n+156\tval_156\t2008-04-09\t11\n+436\tval_436\t2008-04-09\t11\n+43\tval_43\t2008-04-09\t11\n+404\tval_404\t2008-04-09\t11\n+51\tval_51\t2008-04-09\t11\n+131\tval_131\t2008-04-09\t11\n+120\tval_120\t2008-04-09\t11\n+205\tval_205\t2008-04-09\t11\n 230\tval_230\t2008-04-09\t11\n-33\tval_33\t2008-04-09\t11\n-368\tval_368\t2008-04-09\t11\n-103\tval_103\t2008-04-09\t11\n-296\tval_296\t2008-04-09\t11\n-113\tval_113\t2008-04-09\t11\n-216\tval_216\t2008-04-09\t11\n-367\tval_367\t2008-04-09\t11\n-344\tval_344\t2008-04-09\t11\n-167\tval_167\t2008-04-09\t11\n-274\tval_274\t2008-04-09\t11\n-219\tval_219\t2008-04-09\t11\n-239\tval_239\t2008-04-09\t11\n-485\tval_485\t2008-04-09\t11\n-116\tval_116\t2008-04-09\t11\n-223\tval_223\t2008-04-09\t11\n-256\tval_256\t2008-04-09\t11\n-263\tval_263\t2008-04-09\t11\n-70\tval_70\t2008-04-09\t11\n-487\tval_487\t2008-04-09\t11\n-480\tval_480\t2008-04-09\t11\n-401\tval_401\t2008-04-09\t11\n-288\tval_288\t2008-04-09\t11\n+327\tval_327\t2008-04-09\t11\n+165\tval_165\t2008-04-09\t11\n+26\tval_26\t2008-04-09\t11\n+96\tval_96\t2008-04-09\t11\n+418\tval_418\t2008-04-09\t11\n 191\tval_191\t2008-04-09\t11\n-5\tval_5\t2008-04-09\t11\n-244\tval_244\t2008-04-09\t11\n-438\tval_438\t2008-04-09\t11\n-128\tval_128\t2008-04-09\t11\n-467\tval_467\t2008-04-09\t11\n-432\tval_432\t2008-04-09\t11\n-202\tval_202\t2008-04-09\t11\n-316\tval_316\t2008-04-09\t11\n-229\tval_229\t2008-04-09\t11\n-469\tval_469\t2008-04-09\t11\n+199\tval_199\t2008-04-09\t11\n+298\tval_298\t2008-04-09\t11\n+100\tval_100\t2008-04-09\t11\n+454\tval_454\t2008-04-09\t11\n+393\tval_393\t2008-04-09\t11\n+468\tval_468\t2008-04-09\t11\n+197\tval_197\t2008-04-09\t11\n+322\tval_322\t2008-04-09\t11\n+0\tval_0\t2008-04-09\t11\n+496\tval_496\t2008-04-09\t11\n+119\tval_119\t2008-04-09\t11\n+158\tval_158\t2008-04-09\t11\n+129\tval_129\t2008-04-09\t11\n+42\tval_42\t2008-04-09\t11\n+172\tval_172\t2008-04-09\t11\n+179\tval_179\t2008-04-09\t11\n+431\tval_431\t2008-04-09\t11\n 463\tval_463\t2008-04-09\t11\n-280\tval_280\t2008-04-09\t11\n-2\tval_2\t2008-04-09\t11\n-35\tval_35\t2008-04-09\t11\n-283\tval_283\t2008-04-09\t11\n-331\tval_331\t2008-04-09\t11\n-235\tval_235\t2008-04-09\t11\n-80\tval_80\t2008-04-09\t11\n-44\tval_44\t2008-04-09\t11\n-193\tval_193\t2008-04-09\t11\n-321\tval_321\t2008-04-09\t11\n-335\tval_335\t2008-04-09\t11\n-104\tval_104\t2008-04-09\t11\n+24\tval_24\t2008-04-09\t11\n+348\tval_348\t2008-04-09\t11\n+208\tval_208\t2008-04-09\t11\n+230\tval_230\t2008-04-09\t11\n+411\tval_411\t2008-04-09\t11\n+8\tval_8\t2008-04-09\t11\n+58\tval_58\t2008-04-09\t11\n 466\tval_466\t2008-04-09\t11\n-366\tval_366\t2008-04-09\t11\n-175\tval_175\t2008-04-09\t11\n-403\tval_403\t2008-04-09\t11\n-483\tval_483\t2008-04-09\t11\n-53\tval_53\t2008-04-09\t11\n-105\tval_105\t2008-04-09\t11\n-257\tval_257\t2008-04-09\t11\n-406\tval_406\t2008-04-09\t11\n-409\tval_409\t2008-04-09\t11\n-190\tval_190\t2008-04-09\t11\n-406\tval_406\t2008-04-09\t11\n-401\tval_401\t2008-04-09\t11\n-114\tval_114\t2008-04-09\t11\n-258\tval_258\t2008-04-09\t11\n-90\tval_90\t2008-04-09\t11\n-203\tval_203\t2008-04-09\t11\n-262\tval_262\t2008-04-09\t11\n 348\tval_348\t2008-04-09\t11\n-424\tval_424\t2008-04-09\t11\n+84\tval_84\t2008-04-09\t11\n+217\tval_217\t2008-04-09\t11\n+138\tval_138\t2008-04-09\t11\n+272\tval_272\t2008-04-09\t11\n+373\tval_373\t2008-04-09\t11\n+353\tval_353\t2008-04-09\t11\n+489\tval_489\t2008-04-09\t11\n+384\tval_384\t2008-04-09\t11\n+404\tval_404\t2008-04-09\t11\n+260\tval_260\t2008-04-09\t11\n+67\tval_67\t2008-04-09\t11\n+181\tval_181\t2008-04-09\t11\n+230\tval_230\t2008-04-09\t11\n 12\tval_12\t2008-04-09\t11\n-396\tval_396\t2008-04-09\t11\n-201\tval_201\t2008-04-09\t11\n-217\tval_217\t2008-04-09\t11\n-164\tval_164\t2008-04-09\t11\n-431\tval_431\t2008-04-09\t11\n-454\tval_454\t2008-04-09\t11\n-478\tval_478\t2008-04-09\t11\n-298\tval_298\t2008-04-09\t11\n-125\tval_125\t2008-04-09\t11\n-431\tval_431\t2008-04-09\t11\n-164\tval_164\t2008-04-09\t11\n-424\tval_424\t2008-04-09\t11\n+284\tval_284\t2008-04-09\t11\n+180\tval_180\t2008-04-09\t11\n+333\tval_333\t2008-04-09\t11\n+83\tval_83\t2008-04-09\t11\n+241\tval_241\t2008-04-09\t11\n+137\tval_137\t2008-04-09\t11\n+275\tval_275\t2008-04-09\t11\n+311\tval_311\t2008-04-09\t11\n+332\tval_332\t2008-04-09\t11\n+318\tval_318\t2008-04-09\t11\n+65\tval_65\t2008-04-09\t11\n+221\tval_221\t2008-04-09\t11\n+289\tval_289\t2008-04-09\t11\n+176\tval_176\t2008-04-09\t11\n+278\tval_278\t2008-04-09\t11\n+430\tval_430\t2008-04-09\t11\n+216\tval_216\t2008-04-09\t11\n+213\tval_213\t2008-04-09\t11\n+239\tval_239\t2008-04-09\t11\n+103\tval_103\t2008-04-09\t11\n+138\tval_138\t2008-04-09\t11\n+51\tval_51\t2008-04-09\t11\n+459\tval_459\t2008-04-09\t11\n+54\tval_54\t2008-04-09\t11\n+176\tval_176\t2008-04-09\t11\n 187\tval_187\t2008-04-09\t11\n+286\tval_286\t2008-04-09\t11\n+192\tval_192\t2008-04-09\t11\n+469\tval_469\t2008-04-09\t11\n+437\tval_437\t2008-04-09\t11\n+386\tval_386\t2008-04-09\t11\n+125\tval_125\t2008-04-09\t11\n+498\tval_498\t2008-04-09\t11\n 382\tval_382\t2008-04-09\t11\n-5\tval_5\t2008-04-09\t11\n-70\tval_70\t2008-04-09\t11\n-397\tval_397\t2008-04-09\t11\n-480\tval_480\t2008-04-09\t11\n-291\tval_291\t2008-04-09\t11\n-24\tval_24\t2008-04-09\t11\n-351\tval_351\t2008-04-09\t11\n-255\tval_255\t2008-04-09\t11\n-104\tval_104\t2008-04-09\t11\n-70\tval_70\t2008-04-09\t11\n-163\tval_163\t2008-04-09\t11\n+169\tval_169\t2008-04-09\t11\n+399\tval_399\t2008-04-09\t11\n+356\tval_356\t2008-04-09\t11\n+208\tval_208\t2008-04-09\t11\n+277\tval_277\t2008-04-09\t11\n+427\tval_427\t2008-04-09\t11\n+35\tval_35\t2008-04-09\t11\n+280\tval_280\t2008-04-09\t11\n+4\tval_4\t2008-04-09\t11\n+72\tval_72\t2008-04-09\t11\n+47\tval_47\t2008-04-09\t11\n+111\tval_111\t2008-04-09\t11\n+92\tval_92\t2008-04-09\t11\n+221\tval_221\t2008-04-09\t11\n+378\tval_378\t2008-04-09\t11\n+157\tval_157\t2008-04-09\t11\n+489\tval_489\t2008-04-09\t11\n+20\tval_20\t2008-04-09\t11\n+170\tval_170\t2008-04-09\t11\n+129\tval_129\t2008-04-09\t11\n+345\tval_345\t2008-04-09\t11\n 438\tval_438\t2008-04-09\t11\n-119\tval_119\t2008-04-09\t11\n-414\tval_414\t2008-04-09\t11\n-200\tval_200\t2008-04-09\t11\n-491\tval_491\t2008-04-09\t11\n-237\tval_237\t2008-04-09\t11\n+149\tval_149\t2008-04-09\t11\n+205\tval_205\t2008-04-09\t11\n+302\tval_302\t2008-04-09\t11\n+57\tval_57\t2008-04-09\t11\n+316\tval_316\t2008-04-09\t11\n+311\tval_311\t2008-04-09\t11\n+128\tval_128\t2008-04-09\t11\n+455\tval_455\t2008-04-09\t11\n+0\tval_0\t2008-04-09\t11\n+339\tval_339\t2008-04-09\t11\n+203\tval_203\t2008-04-09\t11\n+155\tval_155\t2008-04-09\t11\n+113\tval_113\t2008-04-09\t11\n+17\tval_17\t2008-04-09\t11\n+475\tval_475\t2008-04-09\t11\n+195\tval_195\t2008-04-09\t11\n+167\tval_167\t2008-04-09\t11\n+325\tval_325\t2008-04-09\t11\n+367\tval_367\t2008-04-09\t11\n+342\tval_342\t2008-04-09\t11\n 439\tval_439\t2008-04-09\t11\n-360\tval_360\t2008-04-09\t11\n-248\tval_248\t2008-04-09\t11\n-479\tval_479\t2008-04-09\t11\n-305\tval_305\t2008-04-09\t11\n+266\tval_266\t2008-04-09\t11\n+365\tval_365\t2008-04-09\t11\n+309\tval_309\t2008-04-09\t11\n+397\tval_397\t2008-04-09\t11\n+377\tval_377\t2008-04-09\t11\n+162\tval_162\t2008-04-09\t11\n+489\tval_489\t2008-04-09\t11\n 417\tval_417\t2008-04-09\t11\n+247\tval_247\t2008-04-09\t11\n+396\tval_396\t2008-04-09\t11\n+399\tval_399\t2008-04-09\t11\n+174\tval_174\t2008-04-09\t11\n+208\tval_208\t2008-04-09\t11\n+466\tval_466\t2008-04-09\t11\n 199\tval_199\t2008-04-09\t11\n-444\tval_444\t2008-04-09\t11\n-120\tval_120\t2008-04-09\t11\n-429\tval_429\t2008-04-09\t11\n-169\tval_169\t2008-04-09\t11\n-443\tval_443\t2008-04-09\t11\n-323\tval_323\t2008-04-09\t11\n-325\tval_325\t2008-04-09\t11\n-277\tval_277\t2008-04-09\t11\n-230\tval_230\t2008-04-09\t11\n-478\tval_478\t2008-04-09\t11\n-178\tval_178\t2008-04-09\t11\n-468\tval_468\t2008-04-09\t11\n-310\tval_310\t2008-04-09\t11\n-317\tval_317\t2008-04-09\t11\n-333\tval_333\t2008-04-09\t11\n-493\tval_493\t2008-04-09\t11\n-460\tval_460\t2008-04-09\t11\n 207\tval_207\t2008-04-09\t11\n-249\tval_249\t2008-04-09\t11\n-265\tval_265\t2008-04-09\t11\n-480\tval_480\t2008-04-09\t11\n-83\tval_83\t2008-04-09\t11\n-136\tval_136\t2008-04-09\t11\n-353\tval_353\t2008-04-09\t11\n-172\tval_172\t2008-04-09\t11\n-214\tval_214\t2008-04-09\t11\n-462\tval_462\t2008-04-09\t11\n-233\tval_233\t2008-04-09\t11\n-406\tval_406\t2008-04-09\t11\n-133\tval_133\t2008-04-09\t11\n-175\tval_175\t2008-04-09\t11\n-189\tval_189\t2008-04-09\t11\n-454\tval_454\t2008-04-09\t11\n-375\tval_375\t2008-04-09\t11\n-401\tval_401\t2008-04-09\t11\n-421\tval_421\t2008-04-09\t11\n-407\tval_407\t2008-04-09\t11\n-384\tval_384\t2008-04-09\t11\n-256\tval_256\t2008-04-09\t11\n-26\tval_26\t2008-04-09\t11\n-134\tval_134\t2008-04-09\t11\n-67\tval_67\t2008-04-09\t11\n-384\tval_384\t2008-04-09\t11\n-379\tval_379\t2008-04-09\t11\n-18\tval_18\t2008-04-09\t11\n-462\tval_462\t2008-04-09\t11\n-492\tval_492\t2008-04-09\t11\n-100\tval_100\t2008-04-09\t11\n-298\tval_298\t2008-04-09\t11\n-9\tval_9\t2008-04-09\t11\n-341\tval_341\t2008-04-09\t11\n-498\tval_498\t2008-04-09\t11\n-146\tval_146\t2008-04-09\t11\n-458\tval_458\t2008-04-09\t11\n-362\tval_362\t2008-04-09\t11\n-186\tval_186\t2008-04-09\t11\n-285\tval_285\t2008-04-09\t11\n-348\tval_348\t2008-04-09\t11\n-167\tval_167\t2008-04-09\t11\n-18\tval_18\t2008-04-09\t11\n-273\tval_273\t2008-04-09\t11\n-183\tval_183\t2008-04-09\t11\n+494\tval_494\t2008-04-09\t11\n+413\tval_413\t2008-04-09\t11\n+174\tval_174\t2008-04-09\t11\n+482\tval_482\t2008-04-09\t11\n+237\tval_237\t2008-04-09\t11\n+394\tval_394\t2008-04-09\t11\n+459\tval_459\t2008-04-09\t11\n+446\tval_446\t2008-04-09\t11\n+338\tval_338\t2008-04-09\t11\n+193\tval_193\t2008-04-09\t11\n+153\tval_153\t2008-04-09\t11\n+287\tval_287\t2008-04-09\t11\n+219\tval_219\t2008-04-09\t11\n+292\tval_292\t2008-04-09\t11\n+252\tval_252\t2008-04-09\t11\n+430\tval_430\t2008-04-09\t11\n+417\tval_417\t2008-04-09\t11\n+166\tval_166\t2008-04-09\t11\n+403\tval_403\t2008-04-09\t11\n+82\tval_82\t2008-04-09\t11\n+15\tval_15\t2008-04-09\t11\n+209\tval_209\t2008-04-09\t11\n+277\tval_277\t2008-04-09\t11\n 281\tval_281\t2008-04-09\t11\n-344\tval_344\t2008-04-09\t11\n-97\tval_97\t2008-04-09\t11\n-469\tval_469\t2008-04-09\t11\n-315\tval_315\t2008-04-09\t11\n-84\tval_84\t2008-04-09\t11\n-28\tval_28\t2008-04-09\t11\n+327\tval_327\t2008-04-09\t11\n 37\tval_37\t2008-04-09\t11\n-448\tval_448\t2008-04-09\t11\n+495\tval_495\t2008-04-09\t11\n+145\tval_145\t2008-04-09\t11\n+469\tval_469\t2008-04-09\t11\n 152\tval_152\t2008-04-09\t11\n-348\tval_348\t2008-04-09\t11\n-307\tval_307\t2008-04-09\t11\n-194\tval_194\t2008-04-09\t11\n-414\tval_414\t2008-04-09\t11\n-477\tval_477\t2008-04-09\t11\n-222\tval_222\t2008-04-09\t11\n-126\tval_126\t2008-04-09\t11\n-90\tval_90\t2008-04-09\t11\n-169\tval_169\t2008-04-09\t11\n-403\tval_403\t2008-04-09\t11\n-400\tval_400\t2008-04-09\t11\n-200\tval_200\t2008-04-09\t11\n-97\tval_97\t2008-04-09\t11\n+374\tval_374\t2008-04-09\t11\n+429\tval_429\t2008-04-09\t11\n+406\tval_406\t2008-04-09\t11\n+146\tval_146\t2008-04-09\t11\n+213\tval_213\t2008-04-09\t11\n+128\tval_128\t2008-04-09\t11\n+66\tval_66\t2008-04-09\t11\n+369\tval_369\t2008-04-09\t11\n+224\tval_224\t2008-04-09\t11\n+273\tval_273\t2008-04-09\t11\n+150\tval_150\t2008-04-09\t11\n+401\tval_401\t2008-04-09\t11\n+193\tval_193\t2008-04-09\t11\n+265\tval_265\t2008-04-09\t11\n+484\tval_484\t2008-04-09\t11\n+98\tval_98\t2008-04-09\t11\n+278\tval_278\t2008-04-09\t11\n+255\tval_255\t2008-04-09\t11\n+409\tval_409\t2008-04-09\t11\n+165\tval_165\t2008-04-09\t11\n+27\tval_27\t2008-04-09\t11\n+311\tval_311\t2008-04-09\t11\n+86\tval_86\t2008-04-09\t11\n 238\tval_238\t2008-04-09\t12\n 86\tval_86\t2008-04-09\t12\n 311\tval_311\t2008-04-09\t12",
                "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/load_dyn_part3.q.out",
                "sha": "75de746b7edfd2c6fdfc0f20e6ae08d6a5452877",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/mrr.q.out",
                "changes": 50,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/mrr.q.out?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e",
                "deletions": 25,
                "filename": "ql/src/test/results/clientpositive/tez/mrr.q.out",
                "patch": "@@ -1338,24 +1338,27 @@ STAGE PLANS:\n                 mode: mergepartial\n                 outputColumnNames: _col0, _col1\n                 Statistics: Num rows: 14 Data size: 2805 Basic stats: COMPLETE Column stats: NONE\n-                Select Operator\n-                  expressions: _col0 (type: string), _col1 (type: bigint)\n-                  outputColumnNames: _col0, _col1\n-                  Statistics: Num rows: 14 Data size: 2805 Basic stats: COMPLETE Column stats: NONE\n-                  Reduce Output Operator\n-                    key expressions: _col1 (type: bigint)\n-                    sort order: +\n-                    Statistics: Num rows: 14 Data size: 2805 Basic stats: COMPLETE Column stats: NONE\n-                    value expressions: _col0 (type: string), _col1 (type: bigint)\n+                Filter Operator\n+                  predicate: (_col1 > 1) (type: boolean)\n+                  Statistics: Num rows: 4 Data size: 801 Basic stats: COMPLETE Column stats: NONE\n+                  Select Operator\n+                    expressions: _col0 (type: string), _col1 (type: bigint)\n+                    outputColumnNames: _col0, _col1\n+                    Statistics: Num rows: 4 Data size: 801 Basic stats: COMPLETE Column stats: NONE\n+                    Reduce Output Operator\n+                      key expressions: _col1 (type: bigint)\n+                      sort order: +\n+                      Statistics: Num rows: 4 Data size: 801 Basic stats: COMPLETE Column stats: NONE\n+                      value expressions: _col0 (type: string), _col1 (type: bigint)\n         Reducer 11 \n             Reduce Operator Tree:\n               Extract\n-                Statistics: Num rows: 14 Data size: 2805 Basic stats: COMPLETE Column stats: NONE\n+                Statistics: Num rows: 4 Data size: 801 Basic stats: COMPLETE Column stats: NONE\n                 Reduce Output Operator\n                   key expressions: _col0 (type: string)\n                   sort order: +\n                   Map-reduce partition columns: _col0 (type: string)\n-                  Statistics: Num rows: 14 Data size: 2805 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 4 Data size: 801 Basic stats: COMPLETE Column stats: NONE\n                   value expressions: _col0 (type: string), _col1 (type: bigint)\n         Reducer 2 \n             Reduce Operator Tree:\n@@ -1396,25 +1399,22 @@ STAGE PLANS:\n                   2 {VALUE._col0} {VALUE._col1}\n                 outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5\n                 Statistics: Num rows: 30 Data size: 6171 Basic stats: COMPLETE Column stats: NONE\n-                Filter Operator\n-                  predicate: (_col1 > 1) (type: boolean)\n-                  Statistics: Num rows: 10 Data size: 2057 Basic stats: COMPLETE Column stats: NONE\n-                  Select Operator\n-                    expressions: _col0 (type: string), _col1 (type: bigint), _col2 (type: string), _col3 (type: bigint), _col4 (type: string), _col5 (type: bigint)\n-                    outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5\n-                    Statistics: Num rows: 10 Data size: 2057 Basic stats: COMPLETE Column stats: NONE\n-                    Reduce Output Operator\n-                      key expressions: _col0 (type: string)\n-                      sort order: +\n-                      Statistics: Num rows: 10 Data size: 2057 Basic stats: COMPLETE Column stats: NONE\n-                      value expressions: _col0 (type: string), _col1 (type: bigint), _col2 (type: string), _col3 (type: bigint), _col4 (type: string), _col5 (type: bigint)\n+                Select Operator\n+                  expressions: _col0 (type: string), _col1 (type: bigint), _col2 (type: string), _col3 (type: bigint), _col4 (type: string), _col5 (type: bigint)\n+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5\n+                  Statistics: Num rows: 30 Data size: 6171 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: _col0 (type: string)\n+                    sort order: +\n+                    Statistics: Num rows: 30 Data size: 6171 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: _col0 (type: string), _col1 (type: bigint), _col2 (type: string), _col3 (type: bigint), _col4 (type: string), _col5 (type: bigint)\n         Reducer 5 \n             Reduce Operator Tree:\n               Extract\n-                Statistics: Num rows: 10 Data size: 2057 Basic stats: COMPLETE Column stats: NONE\n+                Statistics: Num rows: 30 Data size: 6171 Basic stats: COMPLETE Column stats: NONE\n                 File Output Operator\n                   compressed: false\n-                  Statistics: Num rows: 10 Data size: 2057 Basic stats: COMPLETE Column stats: NONE\n+                  Statistics: Num rows: 30 Data size: 6171 Basic stats: COMPLETE Column stats: NONE\n                   table:\n                       input format: org.apache.hadoop.mapred.TextInputFormat\n                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat",
                "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/mrr.q.out",
                "sha": "29aab50e65eeefad1d823549e3ea734ff9afb02a",
                "status": "modified"
            },
            {
                "additions": 327,
                "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/tez_dml.q.out",
                "changes": 642,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_dml.q.out?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e",
                "deletions": 315,
                "filename": "ql/src/test/results/clientpositive/tez/tez_dml.q.out",
                "patch": "@@ -436,6 +436,8 @@ STAGE DEPENDENCIES:\n STAGE PLANS:\n   Stage: Stage-1\n     Tez\n+      Edges:\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE)\n #### A masked pattern was here ####\n       Vertices:\n         Map 1 \n@@ -447,14 +449,24 @@ STAGE PLANS:\n                     expressions: value (type: string), cnt (type: bigint)\n                     outputColumnNames: _col0, _col1\n                     Statistics: Num rows: 309 Data size: 2718 Basic stats: COMPLETE Column stats: NONE\n-                    File Output Operator\n-                      compressed: false\n+                    Reduce Output Operator\n+                      key expressions: _col1 (type: bigint)\n+                      sort order: +\n+                      Map-reduce partition columns: _col1 (type: bigint)\n                       Statistics: Num rows: 309 Data size: 2718 Basic stats: COMPLETE Column stats: NONE\n-                      table:\n-                          input format: org.apache.hadoop.mapred.TextInputFormat\n-                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n-                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-                          name: default.tmp_src_part\n+                      value expressions: _col0 (type: string), _col1 (type: bigint)\n+        Reducer 2 \n+            Reduce Operator Tree:\n+              Extract\n+                Statistics: Num rows: 309 Data size: 2718 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 309 Data size: 2718 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+                      name: default.tmp_src_part\n \n   Stage: Stage-2\n     Dependency Collection\n@@ -515,314 +527,314 @@ POSTHOOK: Lineage: tmp_src_part PARTITION(d=3).c SIMPLE [(tmp_src)tmp_src.FieldS\n POSTHOOK: Lineage: tmp_src_part PARTITION(d=4).c SIMPLE [(tmp_src)tmp_src.FieldSchema(name:value, type:string, comment:null), ]\n POSTHOOK: Lineage: tmp_src_part PARTITION(d=5).c SIMPLE [(tmp_src)tmp_src.FieldSchema(name:value, type:string, comment:null), ]\n val_490\t1\n-val_287\t1\n-val_286\t1\n-val_285\t1\n-val_284\t1\n-val_283\t1\n-val_114\t1\n-val_487\t1\n-val_485\t1\n-val_28\t1\n-val_484\t1\n-val_181\t1\n-val_275\t1\n-val_274\t1\n-val_183\t1\n-val_483\t1\n-val_27\t1\n-val_266\t1\n-val_482\t1\n-val_263\t1\n-val_262\t1\n-val_260\t1\n-val_481\t1\n-val_258\t1\n-val_257\t1\n-val_116\t1\n-val_479\t1\n-val_252\t1\n-val_249\t1\n-val_248\t1\n-val_247\t1\n-val_244\t1\n-val_92\t1\n-val_241\t1\n-val_477\t1\n-val_475\t1\n-val_472\t1\n-val_470\t1\n-val_235\t1\n-val_47\t1\n-val_186\t1\n-val_126\t1\n-val_228\t1\n-val_226\t1\n-val_131\t1\n-val_467\t1\n-val_222\t1\n-val_133\t1\n-val_82\t1\n-val_218\t1\n-val_80\t1\n-val_460\t1\n-val_214\t1\n-val_8\t1\n-val_78\t1\n-val_189\t1\n-val_457\t1\n-val_455\t1\n-val_136\t1\n-val_202\t1\n-val_201\t1\n-val_453\t1\n-val_20\t1\n-val_2\t1\n-val_19\t1\n-val_452\t1\n-val_196\t1\n-val_449\t1\n-val_194\t1\n-val_190\t1\n-val_192\t1\n-val_448\t1\n-val_446\t1\n-val_444\t1\n-val_443\t1\n-val_44\t1\n-val_77\t1\n-val_143\t1\n-val_437\t1\n-val_436\t1\n-val_435\t1\n-val_432\t1\n-val_145\t1\n-val_150\t1\n-val_43\t1\n-val_10\t1\n-val_427\t1\n-val_74\t1\n-val_421\t1\n-val_9\t1\n-val_419\t1\n-val_418\t1\n-val_153\t1\n-val_105\t1\n-val_69\t1\n-val_411\t1\n-val_41\t1\n-val_155\t1\n-val_407\t1\n-val_156\t1\n-val_87\t1\n-val_157\t1\n-val_402\t1\n-val_158\t1\n-val_400\t1\n-val_4\t1\n-val_66\t1\n-val_65\t1\n-val_160\t1\n-val_64\t1\n-val_394\t1\n-val_393\t1\n-val_392\t1\n-val_389\t1\n-val_386\t1\n-val_162\t1\n-val_86\t1\n-val_379\t1\n-val_378\t1\n-val_377\t1\n-val_375\t1\n-val_374\t1\n-val_373\t1\n-val_57\t1\n-val_163\t1\n-val_368\t1\n-val_54\t1\n-val_366\t1\n-val_365\t1\n-val_364\t1\n-val_362\t1\n-val_360\t1\n-val_356\t1\n-val_53\t1\n-val_351\t1\n-val_166\t1\n-val_168\t1\n-val_345\t1\n-val_85\t1\n-val_11\t1\n-val_341\t1\n-val_34\t1\n-val_339\t1\n-val_338\t1\n-val_336\t1\n-val_335\t1\n-val_111\t1\n-val_332\t1\n-val_497\t1\n-val_33\t1\n-val_17\t1\n-val_496\t1\n-val_323\t1\n-val_495\t1\n-val_494\t1\n-val_170\t1\n-val_493\t1\n-val_177\t1\n-val_315\t1\n-val_178\t1\n-val_310\t1\n-val_96\t1\n-val_308\t1\n-val_491\t1\n-val_306\t1\n-val_305\t1\n-val_302\t1\n-val_30\t1\n-val_180\t1\n-val_296\t1\n-val_292\t1\n-val_291\t1\n val_289\t1\n-val_98\t2\n-val_97\t2\n-val_95\t2\n-val_84\t2\n-val_83\t2\n-val_76\t2\n-val_72\t2\n-val_67\t2\n-val_58\t2\n-val_51\t2\n-val_492\t2\n-val_478\t2\n-val_463\t2\n-val_462\t2\n-val_459\t2\n-val_458\t2\n-val_439\t2\n-val_429\t2\n-val_424\t2\n-val_42\t2\n-val_414\t2\n-val_413\t2\n-val_404\t2\n-val_399\t2\n-val_397\t2\n-val_395\t2\n-val_382\t2\n-val_37\t2\n-val_367\t2\n-val_353\t2\n-val_344\t2\n-val_342\t2\n-val_333\t2\n-val_331\t2\n-val_325\t2\n-val_322\t2\n-val_321\t2\n-val_317\t2\n-val_309\t2\n-val_307\t2\n-val_288\t2\n-val_282\t2\n-val_281\t2\n-val_280\t2\n-val_278\t2\n-val_272\t2\n-val_265\t2\n-val_26\t2\n-val_256\t2\n-val_255\t2\n-val_242\t2\n-val_24\t2\n-val_239\t2\n-val_238\t2\n-val_237\t2\n-val_233\t2\n-val_229\t2\n-val_224\t2\n-val_223\t2\n-val_221\t2\n-val_219\t2\n-val_217\t2\n-val_216\t2\n-val_213\t2\n-val_209\t2\n-val_207\t2\n-val_205\t2\n-val_203\t2\n-val_200\t2\n-val_197\t2\n-val_195\t2\n-val_191\t2\n-val_18\t2\n-val_179\t2\n-val_176\t2\n-val_175\t2\n-val_174\t2\n-val_172\t2\n-val_165\t2\n-val_164\t2\n-val_152\t2\n-val_15\t2\n-val_149\t2\n-val_146\t2\n-val_137\t2\n-val_134\t2\n-val_129\t2\n-val_125\t2\n-val_120\t2\n-val_12\t2\n-val_118\t2\n-val_113\t2\n-val_104\t2\n-val_103\t2\n+val_291\t1\n+val_292\t1\n+val_296\t1\n+val_180\t1\n+val_30\t1\n+val_302\t1\n+val_305\t1\n+val_306\t1\n+val_491\t1\n+val_308\t1\n+val_96\t1\n+val_310\t1\n+val_178\t1\n+val_315\t1\n+val_177\t1\n+val_493\t1\n+val_170\t1\n+val_494\t1\n+val_495\t1\n+val_323\t1\n+val_496\t1\n+val_17\t1\n+val_33\t1\n+val_497\t1\n+val_332\t1\n+val_111\t1\n+val_335\t1\n+val_336\t1\n+val_338\t1\n+val_339\t1\n+val_34\t1\n+val_341\t1\n+val_11\t1\n+val_85\t1\n+val_345\t1\n+val_168\t1\n+val_166\t1\n+val_351\t1\n+val_53\t1\n+val_356\t1\n+val_360\t1\n+val_362\t1\n+val_364\t1\n+val_365\t1\n+val_366\t1\n+val_54\t1\n+val_368\t1\n+val_163\t1\n+val_57\t1\n+val_373\t1\n+val_374\t1\n+val_375\t1\n+val_377\t1\n+val_378\t1\n+val_379\t1\n+val_86\t1\n+val_162\t1\n+val_386\t1\n+val_389\t1\n+val_392\t1\n+val_393\t1\n+val_394\t1\n+val_64\t1\n+val_160\t1\n+val_65\t1\n+val_66\t1\n+val_4\t1\n+val_400\t1\n+val_158\t1\n+val_402\t1\n+val_157\t1\n+val_87\t1\n+val_156\t1\n+val_407\t1\n+val_155\t1\n+val_41\t1\n+val_411\t1\n+val_69\t1\n+val_105\t1\n+val_153\t1\n+val_418\t1\n+val_419\t1\n+val_9\t1\n+val_421\t1\n+val_74\t1\n+val_427\t1\n+val_10\t1\n+val_43\t1\n+val_150\t1\n+val_145\t1\n+val_432\t1\n+val_435\t1\n+val_436\t1\n+val_437\t1\n+val_143\t1\n+val_77\t1\n+val_44\t1\n+val_443\t1\n+val_444\t1\n+val_446\t1\n+val_448\t1\n+val_192\t1\n+val_190\t1\n+val_194\t1\n+val_449\t1\n+val_196\t1\n+val_452\t1\n+val_19\t1\n+val_2\t1\n+val_20\t1\n+val_453\t1\n+val_201\t1\n+val_202\t1\n+val_136\t1\n+val_455\t1\n+val_457\t1\n+val_189\t1\n+val_78\t1\n+val_8\t1\n+val_214\t1\n+val_460\t1\n+val_80\t1\n+val_218\t1\n+val_82\t1\n+val_133\t1\n+val_222\t1\n+val_467\t1\n+val_131\t1\n+val_226\t1\n+val_228\t1\n+val_126\t1\n+val_186\t1\n+val_47\t1\n+val_235\t1\n+val_470\t1\n+val_472\t1\n+val_475\t1\n+val_477\t1\n+val_241\t1\n+val_92\t1\n+val_244\t1\n+val_247\t1\n+val_248\t1\n+val_249\t1\n+val_252\t1\n+val_479\t1\n+val_116\t1\n+val_257\t1\n+val_258\t1\n+val_481\t1\n+val_260\t1\n+val_262\t1\n+val_263\t1\n+val_482\t1\n+val_266\t1\n+val_27\t1\n+val_483\t1\n+val_183\t1\n+val_274\t1\n+val_275\t1\n+val_181\t1\n+val_484\t1\n+val_28\t1\n+val_485\t1\n+val_487\t1\n+val_114\t1\n+val_283\t1\n+val_284\t1\n+val_285\t1\n+val_286\t1\n+val_287\t1\n+val_84\t2\n+val_95\t2\n+val_97\t2\n+val_98\t2\n val_100\t2\n-val_498\t3\n-val_369\t3\n-val_384\t3\n+val_103\t2\n+val_104\t2\n+val_113\t2\n+val_118\t2\n+val_12\t2\n+val_120\t2\n+val_125\t2\n+val_129\t2\n+val_134\t2\n+val_137\t2\n+val_146\t2\n+val_149\t2\n+val_15\t2\n+val_152\t2\n+val_164\t2\n+val_165\t2\n+val_172\t2\n+val_174\t2\n+val_175\t2\n+val_176\t2\n+val_179\t2\n+val_18\t2\n+val_191\t2\n+val_195\t2\n+val_197\t2\n+val_200\t2\n+val_203\t2\n+val_205\t2\n+val_207\t2\n+val_209\t2\n+val_213\t2\n+val_216\t2\n+val_217\t2\n+val_219\t2\n+val_221\t2\n+val_223\t2\n+val_224\t2\n+val_229\t2\n+val_233\t2\n+val_237\t2\n+val_238\t2\n+val_239\t2\n+val_24\t2\n+val_242\t2\n+val_255\t2\n+val_256\t2\n+val_26\t2\n+val_265\t2\n+val_272\t2\n+val_278\t2\n+val_280\t2\n+val_281\t2\n+val_282\t2\n+val_288\t2\n+val_307\t2\n+val_309\t2\n+val_317\t2\n+val_321\t2\n+val_322\t2\n+val_325\t2\n+val_331\t2\n+val_333\t2\n+val_342\t2\n+val_344\t2\n+val_353\t2\n+val_367\t2\n+val_37\t2\n+val_382\t2\n+val_395\t2\n+val_397\t2\n+val_399\t2\n+val_404\t2\n+val_413\t2\n+val_414\t2\n+val_42\t2\n+val_424\t2\n+val_429\t2\n+val_439\t2\n+val_458\t2\n+val_459\t2\n+val_462\t2\n+val_463\t2\n+val_478\t2\n+val_492\t2\n+val_51\t2\n+val_58\t2\n+val_67\t2\n+val_72\t2\n+val_76\t2\n+val_83\t2\n val_396\t3\n-val_403\t3\n-val_409\t3\n-val_417\t3\n-val_5\t3\n-val_430\t3\n-val_70\t3\n-val_119\t3\n-val_0\t3\n-val_431\t3\n-val_438\t3\n-val_480\t3\n-val_193\t3\n-val_199\t3\n-val_208\t3\n-val_187\t3\n-val_273\t3\n-val_298\t3\n-val_454\t3\n-val_311\t3\n-val_316\t3\n-val_466\t3\n-val_90\t3\n-val_128\t3\n-val_318\t3\n-val_327\t3\n-val_167\t3\n+val_384\t3\n+val_369\t3\n+val_498\t3\n val_35\t3\n-val_468\t4\n-val_489\t4\n+val_167\t3\n+val_327\t3\n+val_318\t3\n+val_128\t3\n+val_90\t3\n+val_466\t3\n+val_316\t3\n+val_311\t3\n+val_454\t3\n+val_298\t3\n+val_273\t3\n+val_187\t3\n+val_208\t3\n+val_199\t3\n+val_193\t3\n+val_480\t3\n+val_438\t3\n+val_431\t3\n+val_0\t3\n+val_119\t3\n+val_70\t3\n+val_430\t3\n+val_5\t3\n+val_417\t3\n+val_409\t3\n+val_403\t3\n val_406\t4\n-val_169\t4\n-val_138\t4\n+val_489\t4\n+val_468\t4\n val_277\t4\n-val_469\t5\n-val_401\t5\n-val_230\t5\n+val_138\t4\n+val_169\t4\n val_348\t5\n+val_230\t5\n+val_401\t5\n+val_469\t5\n PREHOOK: query: -- multi insert\n CREATE TABLE even (c int, d string)\n PREHOOK: type: CREATETABLE\n@@ -867,10 +879,10 @@ POSTHOOK: Lineage: tmp_src_part PARTITION(d=5).c SIMPLE [(tmp_src)tmp_src.FieldS\n STAGE DEPENDENCIES:\n   Stage-2 is a root stage\n   Stage-3 depends on stages: Stage-2\n-  Stage-0 depends on stages: Stage-3\n-  Stage-4 depends on stages: Stage-0\n   Stage-1 depends on stages: Stage-3\n-  Stage-5 depends on stages: Stage-1\n+  Stage-4 depends on stages: Stage-1\n+  Stage-0 depends on stages: Stage-3\n+  Stage-5 depends on stages: Stage-0\n \n STAGE PLANS:\n   Stage: Stage-2\n@@ -916,28 +928,28 @@ STAGE PLANS:\n   Stage: Stage-3\n     Dependency Collection\n \n-  Stage: Stage-0\n+  Stage: Stage-1\n     Move Operator\n       tables:\n           replace: false\n           table:\n               input format: org.apache.hadoop.mapred.TextInputFormat\n               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-              name: default.even\n+              name: default.odd\n \n   Stage: Stage-4\n     Stats-Aggr Operator\n \n-  Stage: Stage-1\n+  Stage: Stage-0\n     Move Operator\n       tables:\n           replace: false\n           table:\n               input format: org.apache.hadoop.mapred.TextInputFormat\n               output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n               serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n-              name: default.odd\n+              name: default.even\n \n   Stage: Stage-5\n     Stats-Aggr Operator",
                "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/tez_dml.q.out",
                "sha": "6b13942472c7a5d8690038a8f85e49c372401c99",
                "status": "modified"
            },
            {
                "additions": 952,
                "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/tez_union.q.out",
                "changes": 2477,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/tez_union.q.out?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e",
                "deletions": 1525,
                "filename": "ql/src/test/results/clientpositive/tez/tez_union.q.out",
                "patch": "@@ -79,1546 +79,55 @@ STAGE PLANS:\n     Fetch Operator\n       limit: -1\n \n-PREHOOK: query: select s1.key as key, s1.value as value from src s1 join src s3 on s1.key=s3.key\n+PREHOOK: query: create table ut as\n+select s1.key as key, s1.value as value from src s1 join src s3 on s1.key=s3.key\n UNION  ALL  \n select s2.key as key, s2.value as value from src s2\n-PREHOOK: type: QUERY\n+PREHOOK: type: CREATETABLE_AS_SELECT\n PREHOOK: Input: default@src\n-#### A masked pattern was here ####\n-POSTHOOK: query: select s1.key as key, s1.value as value from src s1 join src s3 on s1.key=s3.key\n+POSTHOOK: query: create table ut as\n+select s1.key as key, s1.value as value from src s1 join src s3 on s1.key=s3.key\n UNION  ALL  \n select s2.key as key, s2.value as value from src s2\n-POSTHOOK: type: QUERY\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: select * from ut order by key, value limit 20\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@ut\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from ut order by key, value limit 20\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@ut\n #### A masked pattern was here ####\n-238\tval_238\n-238\tval_238\n-86\tval_86\n-311\tval_311\n-311\tval_311\n-311\tval_311\n-27\tval_27\n-165\tval_165\n-165\tval_165\n-409\tval_409\n-409\tval_409\n-409\tval_409\n-255\tval_255\n-255\tval_255\n-278\tval_278\n-278\tval_278\n-98\tval_98\n-98\tval_98\n-484\tval_484\n-265\tval_265\n-265\tval_265\n-193\tval_193\n-193\tval_193\n-193\tval_193\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-150\tval_150\n-273\tval_273\n-273\tval_273\n-273\tval_273\n-224\tval_224\n-224\tval_224\n-369\tval_369\n-369\tval_369\n-369\tval_369\n-66\tval_66\n-128\tval_128\n-128\tval_128\n-128\tval_128\n-213\tval_213\n-213\tval_213\n-146\tval_146\n-146\tval_146\n-406\tval_406\n-406\tval_406\n-406\tval_406\n-406\tval_406\n-429\tval_429\n-429\tval_429\n-374\tval_374\n-152\tval_152\n-152\tval_152\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-145\tval_145\n-495\tval_495\n-37\tval_37\n-37\tval_37\n-327\tval_327\n-327\tval_327\n-327\tval_327\n-281\tval_281\n-281\tval_281\n-277\tval_277\n-277\tval_277\n-277\tval_277\n-277\tval_277\n-209\tval_209\n-209\tval_209\n-15\tval_15\n-15\tval_15\n-82\tval_82\n-403\tval_403\n-403\tval_403\n-403\tval_403\n-166\tval_166\n-417\tval_417\n-417\tval_417\n-417\tval_417\n-430\tval_430\n-430\tval_430\n-430\tval_430\n-252\tval_252\n-292\tval_292\n-219\tval_219\n-219\tval_219\n-287\tval_287\n-153\tval_153\n-193\tval_193\n-193\tval_193\n-193\tval_193\n-338\tval_338\n-446\tval_446\n-459\tval_459\n-459\tval_459\n-394\tval_394\n-237\tval_237\n-237\tval_237\n-482\tval_482\n-174\tval_174\n-174\tval_174\n-413\tval_413\n-413\tval_413\n-494\tval_494\n-207\tval_207\n-207\tval_207\n-199\tval_199\n-199\tval_199\n-199\tval_199\n-466\tval_466\n-466\tval_466\n-466\tval_466\n-208\tval_208\n-208\tval_208\n-208\tval_208\n-174\tval_174\n-174\tval_174\n-399\tval_399\n-399\tval_399\n-396\tval_396\n-396\tval_396\n-396\tval_396\n-247\tval_247\n-417\tval_417\n-417\tval_417\n-417\tval_417\n-489\tval_489\n-489\tval_489\n-489\tval_489\n-489\tval_489\n-162\tval_162\n-377\tval_377\n-397\tval_397\n-397\tval_397\n-309\tval_309\n-309\tval_309\n-365\tval_365\n-266\tval_266\n-439\tval_439\n-439\tval_439\n-342\tval_342\n-342\tval_342\n-367\tval_367\n-367\tval_367\n-325\tval_325\n-325\tval_325\n-167\tval_167\n-167\tval_167\n-167\tval_167\n-195\tval_195\n-195\tval_195\n-475\tval_475\n-17\tval_17\n-113\tval_113\n-113\tval_113\n-155\tval_155\n-203\tval_203\n-203\tval_203\n-339\tval_339\n 0\tval_0\n 0\tval_0\n 0\tval_0\n-455\tval_455\n-128\tval_128\n-128\tval_128\n-128\tval_128\n-311\tval_311\n-311\tval_311\n-311\tval_311\n-316\tval_316\n-316\tval_316\n-316\tval_316\n-57\tval_57\n-302\tval_302\n-205\tval_205\n-205\tval_205\n-149\tval_149\n-149\tval_149\n-438\tval_438\n-438\tval_438\n-438\tval_438\n-345\tval_345\n-129\tval_129\n-129\tval_129\n-170\tval_170\n-20\tval_20\n-489\tval_489\n-489\tval_489\n-489\tval_489\n-489\tval_489\n-157\tval_157\n-378\tval_378\n-221\tval_221\n-221\tval_221\n-92\tval_92\n-111\tval_111\n-47\tval_47\n-72\tval_72\n-72\tval_72\n-4\tval_4\n-280\tval_280\n-280\tval_280\n-35\tval_35\n-35\tval_35\n-35\tval_35\n-427\tval_427\n-277\tval_277\n-277\tval_277\n-277\tval_277\n-277\tval_277\n-208\tval_208\n-208\tval_208\n-208\tval_208\n-356\tval_356\n-399\tval_399\n-399\tval_399\n-169\tval_169\n-169\tval_169\n-169\tval_169\n-169\tval_169\n-382\tval_382\n-382\tval_382\n-498\tval_498\n-498\tval_498\n-498\tval_498\n-125\tval_125\n-125\tval_125\n-386\tval_386\n-437\tval_437\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-192\tval_192\n-286\tval_286\n-187\tval_187\n-187\tval_187\n-187\tval_187\n-176\tval_176\n-176\tval_176\n-54\tval_54\n-459\tval_459\n-459\tval_459\n-51\tval_51\n-51\tval_51\n-138\tval_138\n-138\tval_138\n-138\tval_138\n-138\tval_138\n-103\tval_103\n-103\tval_103\n-239\tval_239\n-239\tval_239\n-213\tval_213\n-213\tval_213\n-216\tval_216\n-216\tval_216\n-430\tval_430\n-430\tval_430\n-430\tval_430\n-278\tval_278\n-278\tval_278\n-176\tval_176\n-176\tval_176\n-289\tval_289\n-221\tval_221\n-221\tval_221\n-65\tval_65\n-318\tval_318\n-318\tval_318\n-318\tval_318\n-332\tval_332\n-311\tval_311\n-311\tval_311\n-311\tval_311\n-275\tval_275\n-137\tval_137\n-137\tval_137\n-241\tval_241\n-83\tval_83\n-83\tval_83\n-333\tval_333\n-333\tval_333\n-180\tval_180\n-284\tval_284\n-12\tval_12\n-12\tval_12\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-181\tval_181\n-67\tval_67\n-67\tval_67\n-260\tval_260\n-404\tval_404\n-404\tval_404\n-384\tval_384\n-384\tval_384\n-384\tval_384\n-489\tval_489\n-489\tval_489\n-489\tval_489\n-489\tval_489\n-353\tval_353\n-353\tval_353\n-373\tval_373\n-272\tval_272\n-272\tval_272\n-138\tval_138\n-138\tval_138\n-138\tval_138\n-138\tval_138\n-217\tval_217\n-217\tval_217\n-84\tval_84\n-84\tval_84\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-466\tval_466\n-466\tval_466\n-466\tval_466\n-58\tval_58\n-58\tval_58\n-8\tval_8\n-411\tval_411\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-208\tval_208\n-208\tval_208\n-208\tval_208\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-24\tval_24\n-24\tval_24\n-463\tval_463\n-463\tval_463\n-431\tval_431\n-431\tval_431\n-431\tval_431\n-179\tval_179\n-179\tval_179\n-172\tval_172\n-172\tval_172\n-42\tval_42\n-42\tval_42\n-129\tval_129\n-129\tval_129\n-158\tval_158\n-119\tval_119\n-119\tval_119\n-119\tval_119\n-496\tval_496\n 0\tval_0\n 0\tval_0\n 0\tval_0\n-322\tval_322\n-322\tval_322\n-197\tval_197\n-197\tval_197\n-468\tval_468\n-468\tval_468\n-468\tval_468\n-468\tval_468\n-393\tval_393\n-454\tval_454\n-454\tval_454\n-454\tval_454\n-100\tval_100\n-100\tval_100\n-298\tval_298\n-298\tval_298\n-298\tval_298\n-199\tval_199\n-199\tval_199\n-199\tval_199\n-191\tval_191\n-191\tval_191\n-418\tval_418\n-96\tval_96\n-26\tval_26\n-26\tval_26\n-165\tval_165\n-165\tval_165\n-327\tval_327\n-327\tval_327\n-327\tval_327\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-205\tval_205\n-205\tval_205\n-120\tval_120\n-120\tval_120\n-131\tval_131\n-51\tval_51\n-51\tval_51\n-404\tval_404\n-404\tval_404\n-43\tval_43\n-436\tval_436\n-156\tval_156\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-468\tval_468\n-468\tval_468\n-468\tval_468\n-468\tval_468\n-308\tval_308\n-95\tval_95\n-95\tval_95\n-196\tval_196\n-288\tval_288\n-288\tval_288\n-481\tval_481\n-457\tval_457\n-98\tval_98\n-98\tval_98\n-282\tval_282\n-282\tval_282\n-197\tval_197\n-197\tval_197\n-187\tval_187\n-187\tval_187\n-187\tval_187\n-318\tval_318\n-318\tval_318\n-318\tval_318\n-318\tval_318\n-318\tval_318\n-318\tval_318\n-409\tval_409\n-409\tval_409\n-409\tval_409\n-470\tval_470\n-137\tval_137\n-137\tval_137\n-369\tval_369\n-369\tval_369\n-369\tval_369\n-316\tval_316\n-316\tval_316\n-316\tval_316\n-169\tval_169\n-169\tval_169\n-169\tval_169\n-169\tval_169\n-413\tval_413\n-413\tval_413\n-85\tval_85\n-77\tval_77\n 0\tval_0\n 0\tval_0\n 0\tval_0\n-490\tval_490\n-87\tval_87\n-364\tval_364\n-179\tval_179\n-179\tval_179\n-118\tval_118\n-118\tval_118\n-134\tval_134\n-134\tval_134\n-395\tval_395\n-395\tval_395\n-282\tval_282\n-282\tval_282\n-138\tval_138\n-138\tval_138\n-138\tval_138\n-138\tval_138\n-238\tval_238\n-238\tval_238\n-419\tval_419\n-15\tval_15\n-15\tval_15\n-118\tval_118\n-118\tval_118\n-72\tval_72\n-72\tval_72\n-90\tval_90\n-90\tval_90\n-90\tval_90\n-307\tval_307\n-307\tval_307\n-19\tval_19\n-435\tval_435\n-10\tval_10\n-277\tval_277\n-277\tval_277\n-277\tval_277\n-277\tval_277\n-273\tval_273\n-273\tval_273\n-273\tval_273\n-306\tval_306\n-224\tval_224\n-224\tval_224\n-309\tval_309\n-309\tval_309\n-389\tval_389\n-327\tval_327\n-327\tval_327\n-327\tval_327\n-242\tval_242\n-242\tval_242\n-369\tval_369\n-369\tval_369\n-369\tval_369\n-392\tval_392\n-272\tval_272\n-272\tval_272\n-331\tval_331\n-331\tval_331\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-242\tval_242\n-242\tval_242\n-452\tval_452\n-177\tval_177\n-226\tval_226\n-5\tval_5\n-5\tval_5\n-5\tval_5\n-497\tval_497\n-402\tval_402\n-396\tval_396\n-396\tval_396\n-396\tval_396\n-317\tval_317\n-317\tval_317\n-395\tval_395\n-395\tval_395\n-58\tval_58\n-58\tval_58\n-35\tval_35\n-35\tval_35\n-35\tval_35\n-336\tval_336\n-95\tval_95\n-95\tval_95\n-11\tval_11\n-168\tval_168\n-34\tval_34\n-229\tval_229\n-229\tval_229\n-233\tval_233\n-233\tval_233\n-143\tval_143\n-472\tval_472\n-322\tval_322\n-322\tval_322\n-498\tval_498\n-498\tval_498\n-498\tval_498\n-160\tval_160\n-195\tval_195\n-195\tval_195\n-42\tval_42\n-42\tval_42\n-321\tval_321\n-321\tval_321\n-430\tval_430\n-430\tval_430\n-430\tval_430\n-119\tval_119\n-119\tval_119\n-119\tval_119\n-489\tval_489\n-489\tval_489\n-489\tval_489\n-489\tval_489\n-458\tval_458\n-458\tval_458\n-78\tval_78\n-76\tval_76\n-76\tval_76\n-41\tval_41\n-223\tval_223\n-223\tval_223\n-492\tval_492\n-492\tval_492\n-149\tval_149\n-149\tval_149\n-449\tval_449\n-218\tval_218\n-228\tval_228\n-138\tval_138\n-138\tval_138\n-138\tval_138\n-138\tval_138\n-453\tval_453\n-30\tval_30\n-209\tval_209\n-209\tval_209\n-64\tval_64\n-468\tval_468\n-468\tval_468\n-468\tval_468\n-468\tval_468\n-76\tval_76\n-76\tval_76\n-74\tval_74\n-342\tval_342\n-342\tval_342\n-69\tval_69\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-33\tval_33\n-368\tval_368\n-103\tval_103\n-103\tval_103\n-296\tval_296\n-113\tval_113\n-113\tval_113\n-216\tval_216\n-216\tval_216\n-367\tval_367\n-367\tval_367\n-344\tval_344\n-344\tval_344\n-167\tval_167\n-167\tval_167\n-167\tval_167\n-274\tval_274\n-219\tval_219\n-219\tval_219\n-239\tval_239\n-239\tval_239\n-485\tval_485\n-116\tval_116\n-223\tval_223\n-223\tval_223\n-256\tval_256\n-256\tval_256\n-263\tval_263\n-70\tval_70\n-70\tval_70\n-70\tval_70\n-487\tval_487\n-480\tval_480\n-480\tval_480\n-480\tval_480\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-288\tval_288\n-288\tval_288\n-191\tval_191\n-191\tval_191\n-5\tval_5\n-5\tval_5\n-5\tval_5\n-244\tval_244\n-438\tval_438\n-438\tval_438\n-438\tval_438\n-128\tval_128\n-128\tval_128\n-128\tval_128\n-467\tval_467\n-432\tval_432\n-202\tval_202\n-316\tval_316\n-316\tval_316\n-316\tval_316\n-229\tval_229\n-229\tval_229\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-463\tval_463\n-463\tval_463\n-280\tval_280\n-280\tval_280\n-2\tval_2\n-35\tval_35\n-35\tval_35\n-35\tval_35\n-283\tval_283\n-331\tval_331\n-331\tval_331\n-235\tval_235\n-80\tval_80\n-44\tval_44\n-193\tval_193\n-193\tval_193\n-193\tval_193\n-321\tval_321\n-321\tval_321\n-335\tval_335\n-104\tval_104\n-104\tval_104\n-466\tval_466\n-466\tval_466\n-466\tval_466\n-366\tval_366\n-175\tval_175\n-175\tval_175\n-403\tval_403\n-403\tval_403\n-403\tval_403\n-483\tval_483\n-53\tval_53\n-105\tval_105\n-257\tval_257\n-406\tval_406\n-406\tval_406\n-406\tval_406\n-406\tval_406\n-409\tval_409\n-409\tval_409\n-409\tval_409\n-190\tval_190\n-406\tval_406\n-406\tval_406\n-406\tval_406\n-406\tval_406\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-114\tval_114\n-258\tval_258\n-90\tval_90\n-90\tval_90\n-90\tval_90\n-203\tval_203\n-203\tval_203\n-262\tval_262\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-424\tval_424\n-424\tval_424\n-12\tval_12\n-12\tval_12\n-396\tval_396\n-396\tval_396\n-396\tval_396\n-201\tval_201\n-217\tval_217\n-217\tval_217\n-164\tval_164\n-164\tval_164\n-431\tval_431\n-431\tval_431\n-431\tval_431\n-454\tval_454\n-454\tval_454\n-454\tval_454\n-478\tval_478\n-478\tval_478\n-298\tval_298\n-298\tval_298\n-298\tval_298\n-125\tval_125\n-125\tval_125\n-431\tval_431\n-431\tval_431\n-431\tval_431\n-164\tval_164\n-164\tval_164\n-424\tval_424\n-424\tval_424\n-187\tval_187\n-187\tval_187\n-187\tval_187\n-382\tval_382\n-382\tval_382\n-5\tval_5\n-5\tval_5\n-5\tval_5\n-70\tval_70\n-70\tval_70\n-70\tval_70\n-397\tval_397\n-397\tval_397\n-480\tval_480\n-480\tval_480\n-480\tval_480\n-291\tval_291\n-24\tval_24\n-24\tval_24\n-351\tval_351\n-255\tval_255\n-255\tval_255\n-104\tval_104\n-104\tval_104\n-70\tval_70\n-70\tval_70\n-70\tval_70\n-163\tval_163\n-438\tval_438\n-438\tval_438\n-438\tval_438\n-119\tval_119\n-119\tval_119\n-119\tval_119\n-414\tval_414\n-414\tval_414\n-200\tval_200\n-200\tval_200\n-491\tval_491\n-237\tval_237\n-237\tval_237\n-439\tval_439\n-439\tval_439\n-360\tval_360\n-248\tval_248\n-479\tval_479\n-305\tval_305\n-417\tval_417\n-417\tval_417\n-417\tval_417\n-199\tval_199\n-199\tval_199\n-199\tval_199\n-444\tval_444\n-120\tval_120\n-120\tval_120\n-429\tval_429\n-429\tval_429\n-169\tval_169\n-169\tval_169\n-169\tval_169\n-169\tval_169\n-443\tval_443\n-323\tval_323\n-325\tval_325\n-325\tval_325\n-277\tval_277\n-277\tval_277\n-277\tval_277\n-277\tval_277\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-230\tval_230\n-478\tval_478\n-478\tval_478\n-178\tval_178\n-468\tval_468\n-468\tval_468\n-468\tval_468\n-468\tval_468\n-310\tval_310\n-317\tval_317\n-317\tval_317\n-333\tval_333\n-333\tval_333\n-493\tval_493\n-460\tval_460\n-207\tval_207\n-207\tval_207\n-249\tval_249\n-265\tval_265\n-265\tval_265\n-480\tval_480\n-480\tval_480\n-480\tval_480\n-83\tval_83\n-83\tval_83\n-136\tval_136\n-353\tval_353\n-353\tval_353\n-172\tval_172\n-172\tval_172\n-214\tval_214\n-462\tval_462\n-462\tval_462\n-233\tval_233\n-233\tval_233\n-406\tval_406\n-406\tval_406\n-406\tval_406\n-406\tval_406\n-133\tval_133\n-175\tval_175\n-175\tval_175\n-189\tval_189\n-454\tval_454\n-454\tval_454\n-454\tval_454\n-375\tval_375\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-401\tval_401\n-421\tval_421\n-407\tval_407\n-384\tval_384\n-384\tval_384\n-384\tval_384\n-256\tval_256\n-256\tval_256\n-26\tval_26\n-26\tval_26\n-134\tval_134\n-134\tval_134\n-67\tval_67\n-67\tval_67\n-384\tval_384\n-384\tval_384\n-384\tval_384\n-379\tval_379\n-18\tval_18\n-18\tval_18\n-462\tval_462\n-462\tval_462\n-492\tval_492\n-492\tval_492\n-100\tval_100\n-100\tval_100\n-298\tval_298\n-298\tval_298\n-298\tval_298\n-9\tval_9\n-341\tval_341\n-498\tval_498\n-498\tval_498\n-498\tval_498\n-146\tval_146\n-146\tval_146\n-458\tval_458\n-458\tval_458\n-362\tval_362\n-186\tval_186\n-285\tval_285\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-167\tval_167\n-167\tval_167\n-167\tval_167\n-18\tval_18\n-18\tval_18\n-273\tval_273\n-273\tval_273\n-273\tval_273\n-183\tval_183\n-281\tval_281\n-281\tval_281\n-344\tval_344\n-344\tval_344\n-97\tval_97\n-97\tval_97\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-469\tval_469\n-315\tval_315\n-84\tval_84\n-84\tval_84\n-28\tval_28\n-37\tval_37\n-37\tval_37\n-448\tval_448\n-152\tval_152\n-152\tval_152\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-348\tval_348\n-307\tval_307\n-307\tval_307\n-194\tval_194\n-414\tval_414\n-414\tval_414\n-477\tval_477\n-222\tval_222\n-126\tval_126\n-90\tval_90\n-90\tval_90\n-90\tval_90\n-169\tval_169\n-169\tval_169\n-169\tval_169\n-169\tval_169\n-403\tval_403\n-403\tval_403\n-403\tval_403\n-400\tval_400\n-200\tval_200\n-200\tval_200\n-97\tval_97\n-97\tval_97\n-238\tval_238\n-86\tval_86\n-311\tval_311\n-27\tval_27\n-165\tval_165\n-409\tval_409\n-255\tval_255\n-278\tval_278\n-98\tval_98\n-484\tval_484\n-265\tval_265\n-193\tval_193\n-401\tval_401\n-150\tval_150\n-273\tval_273\n-224\tval_224\n-369\tval_369\n-66\tval_66\n-128\tval_128\n-213\tval_213\n-146\tval_146\n-406\tval_406\n-429\tval_429\n-374\tval_374\n-152\tval_152\n-469\tval_469\n-145\tval_145\n-495\tval_495\n-37\tval_37\n-327\tval_327\n-281\tval_281\n-277\tval_277\n-209\tval_209\n-15\tval_15\n-82\tval_82\n-403\tval_403\n-166\tval_166\n-417\tval_417\n-430\tval_430\n-252\tval_252\n-292\tval_292\n-219\tval_219\n-287\tval_287\n-153\tval_153\n-193\tval_193\n-338\tval_338\n-446\tval_446\n-459\tval_459\n-394\tval_394\n-237\tval_237\n-482\tval_482\n-174\tval_174\n-413\tval_413\n-494\tval_494\n-207\tval_207\n-199\tval_199\n-466\tval_466\n-208\tval_208\n-174\tval_174\n-399\tval_399\n-396\tval_396\n-247\tval_247\n-417\tval_417\n-489\tval_489\n-162\tval_162\n-377\tval_377\n-397\tval_397\n-309\tval_309\n-365\tval_365\n-266\tval_266\n-439\tval_439\n-342\tval_342\n-367\tval_367\n-325\tval_325\n-167\tval_167\n-195\tval_195\n-475\tval_475\n-17\tval_17\n-113\tval_113\n-155\tval_155\n-203\tval_203\n-339\tval_339\n 0\tval_0\n-455\tval_455\n-128\tval_128\n-311\tval_311\n-316\tval_316\n-57\tval_57\n-302\tval_302\n-205\tval_205\n-149\tval_149\n-438\tval_438\n-345\tval_345\n-129\tval_129\n-170\tval_170\n-20\tval_20\n-489\tval_489\n-157\tval_157\n-378\tval_378\n-221\tval_221\n-92\tval_92\n-111\tval_111\n-47\tval_47\n-72\tval_72\n-4\tval_4\n-280\tval_280\n-35\tval_35\n-427\tval_427\n-277\tval_277\n-208\tval_208\n-356\tval_356\n-399\tval_399\n-169\tval_169\n-382\tval_382\n-498\tval_498\n-125\tval_125\n-386\tval_386\n-437\tval_437\n-469\tval_469\n-192\tval_192\n-286\tval_286\n-187\tval_187\n-176\tval_176\n-54\tval_54\n-459\tval_459\n-51\tval_51\n-138\tval_138\n-103\tval_103\n-239\tval_239\n-213\tval_213\n-216\tval_216\n-430\tval_430\n-278\tval_278\n-176\tval_176\n-289\tval_289\n-221\tval_221\n-65\tval_65\n-318\tval_318\n-332\tval_332\n-311\tval_311\n-275\tval_275\n-137\tval_137\n-241\tval_241\n-83\tval_83\n-333\tval_333\n-180\tval_180\n-284\tval_284\n-12\tval_12\n-230\tval_230\n-181\tval_181\n-67\tval_67\n-260\tval_260\n-404\tval_404\n-384\tval_384\n-489\tval_489\n-353\tval_353\n-373\tval_373\n-272\tval_272\n-138\tval_138\n-217\tval_217\n-84\tval_84\n-348\tval_348\n-466\tval_466\n-58\tval_58\n-8\tval_8\n-411\tval_411\n-230\tval_230\n-208\tval_208\n-348\tval_348\n-24\tval_24\n-463\tval_463\n-431\tval_431\n-179\tval_179\n-172\tval_172\n-42\tval_42\n-129\tval_129\n-158\tval_158\n-119\tval_119\n-496\tval_496\n 0\tval_0\n-322\tval_322\n-197\tval_197\n-468\tval_468\n-393\tval_393\n-454\tval_454\n-100\tval_100\n-298\tval_298\n-199\tval_199\n-191\tval_191\n-418\tval_418\n-96\tval_96\n-26\tval_26\n-165\tval_165\n-327\tval_327\n-230\tval_230\n-205\tval_205\n-120\tval_120\n-131\tval_131\n-51\tval_51\n-404\tval_404\n-43\tval_43\n-436\tval_436\n-156\tval_156\n-469\tval_469\n-468\tval_468\n-308\tval_308\n-95\tval_95\n-196\tval_196\n-288\tval_288\n-481\tval_481\n-457\tval_457\n-98\tval_98\n-282\tval_282\n-197\tval_197\n-187\tval_187\n-318\tval_318\n-318\tval_318\n-409\tval_409\n-470\tval_470\n-137\tval_137\n-369\tval_369\n-316\tval_316\n-169\tval_169\n-413\tval_413\n-85\tval_85\n-77\tval_77\n 0\tval_0\n-490\tval_490\n-87\tval_87\n-364\tval_364\n-179\tval_179\n-118\tval_118\n-134\tval_134\n-395\tval_395\n-282\tval_282\n-138\tval_138\n-238\tval_238\n-419\tval_419\n-15\tval_15\n-118\tval_118\n-72\tval_72\n-90\tval_90\n-307\tval_307\n-19\tval_19\n-435\tval_435\n 10\tval_10\n-277\tval_277\n-273\tval_273\n-306\tval_306\n-224\tval_224\n-309\tval_309\n-389\tval_389\n-327\tval_327\n-242\tval_242\n-369\tval_369\n-392\tval_392\n-272\tval_272\n-331\tval_331\n-401\tval_401\n-242\tval_242\n-452\tval_452\n-177\tval_177\n-226\tval_226\n-5\tval_5\n-497\tval_497\n-402\tval_402\n-396\tval_396\n-317\tval_317\n-395\tval_395\n-58\tval_58\n-35\tval_35\n-336\tval_336\n-95\tval_95\n-11\tval_11\n-168\tval_168\n-34\tval_34\n-229\tval_229\n-233\tval_233\n-143\tval_143\n-472\tval_472\n-322\tval_322\n-498\tval_498\n-160\tval_160\n-195\tval_195\n-42\tval_42\n-321\tval_321\n-430\tval_430\n-119\tval_119\n-489\tval_489\n-458\tval_458\n-78\tval_78\n-76\tval_76\n-41\tval_41\n-223\tval_223\n-492\tval_492\n-149\tval_149\n-449\tval_449\n-218\tval_218\n-228\tval_228\n-138\tval_138\n-453\tval_453\n-30\tval_30\n-209\tval_209\n-64\tval_64\n-468\tval_468\n-76\tval_76\n-74\tval_74\n-342\tval_342\n-69\tval_69\n-230\tval_230\n-33\tval_33\n-368\tval_368\n-103\tval_103\n-296\tval_296\n-113\tval_113\n-216\tval_216\n-367\tval_367\n-344\tval_344\n-167\tval_167\n-274\tval_274\n-219\tval_219\n-239\tval_239\n-485\tval_485\n-116\tval_116\n-223\tval_223\n-256\tval_256\n-263\tval_263\n-70\tval_70\n-487\tval_487\n-480\tval_480\n-401\tval_401\n-288\tval_288\n-191\tval_191\n-5\tval_5\n-244\tval_244\n-438\tval_438\n-128\tval_128\n-467\tval_467\n-432\tval_432\n-202\tval_202\n-316\tval_316\n-229\tval_229\n-469\tval_469\n-463\tval_463\n-280\tval_280\n-2\tval_2\n-35\tval_35\n-283\tval_283\n-331\tval_331\n-235\tval_235\n-80\tval_80\n-44\tval_44\n-193\tval_193\n-321\tval_321\n-335\tval_335\n-104\tval_104\n-466\tval_466\n-366\tval_366\n-175\tval_175\n-403\tval_403\n-483\tval_483\n-53\tval_53\n-105\tval_105\n-257\tval_257\n-406\tval_406\n-409\tval_409\n-190\tval_190\n-406\tval_406\n-401\tval_401\n-114\tval_114\n-258\tval_258\n-90\tval_90\n-203\tval_203\n-262\tval_262\n-348\tval_348\n-424\tval_424\n-12\tval_12\n-396\tval_396\n-201\tval_201\n-217\tval_217\n-164\tval_164\n-431\tval_431\n-454\tval_454\n-478\tval_478\n-298\tval_298\n-125\tval_125\n-431\tval_431\n-164\tval_164\n-424\tval_424\n-187\tval_187\n-382\tval_382\n-5\tval_5\n-70\tval_70\n-397\tval_397\n-480\tval_480\n-291\tval_291\n-24\tval_24\n-351\tval_351\n-255\tval_255\n-104\tval_104\n-70\tval_70\n-163\tval_163\n-438\tval_438\n-119\tval_119\n-414\tval_414\n-200\tval_200\n-491\tval_491\n-237\tval_237\n-439\tval_439\n-360\tval_360\n-248\tval_248\n-479\tval_479\n-305\tval_305\n-417\tval_417\n-199\tval_199\n-444\tval_444\n-120\tval_120\n-429\tval_429\n-169\tval_169\n-443\tval_443\n-323\tval_323\n-325\tval_325\n-277\tval_277\n-230\tval_230\n-478\tval_478\n-178\tval_178\n-468\tval_468\n-310\tval_310\n-317\tval_317\n-333\tval_333\n-493\tval_493\n-460\tval_460\n-207\tval_207\n-249\tval_249\n-265\tval_265\n-480\tval_480\n-83\tval_83\n-136\tval_136\n-353\tval_353\n-172\tval_172\n-214\tval_214\n-462\tval_462\n-233\tval_233\n-406\tval_406\n-133\tval_133\n-175\tval_175\n-189\tval_189\n-454\tval_454\n-375\tval_375\n-401\tval_401\n-421\tval_421\n-407\tval_407\n-384\tval_384\n-256\tval_256\n-26\tval_26\n-134\tval_134\n-67\tval_67\n-384\tval_384\n-379\tval_379\n-18\tval_18\n-462\tval_462\n-492\tval_492\n+10\tval_10\n+100\tval_100\n+100\tval_100\n+100\tval_100\n 100\tval_100\n-298\tval_298\n-9\tval_9\n-341\tval_341\n-498\tval_498\n-146\tval_146\n-458\tval_458\n-362\tval_362\n-186\tval_186\n-285\tval_285\n-348\tval_348\n-167\tval_167\n-18\tval_18\n-273\tval_273\n-183\tval_183\n-281\tval_281\n-344\tval_344\n-97\tval_97\n-469\tval_469\n-315\tval_315\n-84\tval_84\n-28\tval_28\n-37\tval_37\n-448\tval_448\n-152\tval_152\n-348\tval_348\n-307\tval_307\n-194\tval_194\n-414\tval_414\n-477\tval_477\n-222\tval_222\n-126\tval_126\n-90\tval_90\n-169\tval_169\n-403\tval_403\n-400\tval_400\n-200\tval_200\n-97\tval_97\n+100\tval_100\n+100\tval_100\n+PREHOOK: query: drop table ut\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@ut\n+PREHOOK: Output: default@ut\n+POSTHOOK: query: drop table ut\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@ut\n+POSTHOOK: Output: default@ut\n PREHOOK: query: explain\n with u as (select * from src union all select * from src)\n select count(*) from (select u1.key as k1, u2.key as k2 from\n@@ -1736,16 +245,934 @@ STAGE PLANS:\n     Fetch Operator\n       limit: -1\n \n-PREHOOK: query: with u as (select * from src union all select * from src)\n-select count(*) from (select u1.key as k1, u2.key as k2 from\n+PREHOOK: query: create table ut as\n+with u as (select * from src union all select * from src)\n+select count(*) as cnt from (select u1.key as k1, u2.key as k2 from\n+u as u1 join u as u2 on (u1.key = u2.key)) a\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@src\n+POSTHOOK: query: create table ut as\n+with u as (select * from src union all select * from src)\n+select count(*) as cnt from (select u1.key as k1, u2.key as k2 from\n u as u1 join u as u2 on (u1.key = u2.key)) a\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: select * from ut order by cnt limit 20\n PREHOOK: type: QUERY\n+PREHOOK: Input: default@ut\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from ut order by cnt limit 20\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@ut\n+#### A masked pattern was here ####\n+4112\n+PREHOOK: query: drop table ut\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@ut\n+PREHOOK: Output: default@ut\n+POSTHOOK: query: drop table ut\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@ut\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: explain select s1.key as skey, u1.key as ukey from\n+src s1\n+join (select * from src union all select * from src) u1 on s1.key = u1.key\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain select s1.key as skey, u1.key as ukey from\n+src s1\n+join (select * from src union all select * from src) u1 on s1.key = u1.key\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 2 <- Map 1 (BROADCAST_EDGE), Union 3 (CONTAINS)\n+        Map 4 <- Map 1 (BROADCAST_EDGE), Union 3 (CONTAINS)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s1\n+                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string)\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string)\n+        Map 2 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string)\n+                    outputColumnNames: _col0, _col1\n+                    Map Join Operator\n+                      condition map:\n+                           Inner Join 0 to 1\n+                      condition expressions:\n+                        0 {key}\n+                        1 {_col0}\n+                      keys:\n+                        0 key (type: string)\n+                        1 _col0 (type: string)\n+                      outputColumnNames: _col0, _col4\n+                      Select Operator\n+                        expressions: _col0 (type: string), _col4 (type: string)\n+                        outputColumnNames: _col0, _col1\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string)\n+                    outputColumnNames: _col0, _col1\n+                    Map Join Operator\n+                      condition map:\n+                           Inner Join 0 to 1\n+                      condition expressions:\n+                        0 {key}\n+                        1 {_col0}\n+                      keys:\n+                        0 key (type: string)\n+                        1 _col0 (type: string)\n+                      outputColumnNames: _col0, _col4\n+                      Select Operator\n+                        expressions: _col0 (type: string), _col4 (type: string)\n+                        outputColumnNames: _col0, _col1\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 3 \n+            Vertex: Union 3\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+PREHOOK: query: create table ut as\n+select s1.key as skey, u1.key as ukey from\n+src s1\n+join (select * from src union all select * from src) u1 on s1.key = u1.key\n+PREHOOK: type: CREATETABLE_AS_SELECT\n PREHOOK: Input: default@src\n+POSTHOOK: query: create table ut as\n+select s1.key as skey, u1.key as ukey from\n+src s1\n+join (select * from src union all select * from src) u1 on s1.key = u1.key\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: select * from ut order by skey, ukey limit 20\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@ut\n #### A masked pattern was here ####\n-POSTHOOK: query: with u as (select * from src union all select * from src)\n-select count(*) from (select u1.key as k1, u2.key as k2 from\n-u as u1 join u as u2 on (u1.key = u2.key)) a\n+POSTHOOK: query: select * from ut order by skey, ukey limit 20\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@ut\n+#### A masked pattern was here ####\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+10\t10\n+10\t10\n+PREHOOK: query: drop table ut\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@ut\n+PREHOOK: Output: default@ut\n+POSTHOOK: query: drop table ut\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@ut\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: explain select s1.key as skey, u1.key as ukey, s8.key as lkey from \n+src s1\n+join (select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+      union all select s4.key from src s4 join src s5 on s4.key = s5.key\n+      union all select s6.key from src s6 join src s7 on s6.key = s7.key) u1 on (s1.key = u1.key)\n+join src s8 on (u1.key = s8.key)\n+order by lkey\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain select s1.key as skey, u1.key as ukey, s8.key as lkey from \n+src s1\n+join (select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+      union all select s4.key from src s4 join src s5 on s4.key = s5.key\n+      union all select s6.key from src s6 join src s7 on s6.key = s7.key) u1 on (s1.key = u1.key)\n+join src s8 on (u1.key = s8.key)\n+order by lkey\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 2 <- Map 1 (BROADCAST_EDGE), Union 3 (CONTAINS), Map 5 (BROADCAST_EDGE), Map 8 (BROADCAST_EDGE)\n+        Map 7 <- Map 1 (BROADCAST_EDGE), Map 6 (BROADCAST_EDGE), Union 3 (CONTAINS), Map 8 (BROADCAST_EDGE)\n+        Map 9 <- Map 1 (BROADCAST_EDGE), Map 8 (BROADCAST_EDGE), Union 3 (CONTAINS), Map 10 (BROADCAST_EDGE)\n+        Reducer 4 <- Union 3 (SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s1\n+                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string)\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string)\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string)\n+        Map 10 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s5\n+                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+        Map 2 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s2\n+                  Map Join Operator\n+                    condition map:\n+                         Inner Join 0 to 1\n+                    condition expressions:\n+                      0 {key}\n+                      1 \n+                    keys:\n+                      0 key (type: string)\n+                      1 key (type: string)\n+                    outputColumnNames: _col0\n+                    Select Operator\n+                      expressions: _col0 (type: string)\n+                      outputColumnNames: _col0\n+                      Map Join Operator\n+                        condition map:\n+                             Inner Join 0 to 1\n+                             Inner Join 1 to 2\n+                        condition expressions:\n+                          0 {key}\n+                          1 {_col0}\n+                          2 {key}\n+                        keys:\n+                          0 key (type: string)\n+                          1 _col0 (type: string)\n+                          2 key (type: string)\n+                        outputColumnNames: _col0, _col4, _col5\n+                        Select Operator\n+                          expressions: _col0 (type: string), _col4 (type: string), _col5 (type: string)\n+                          outputColumnNames: _col0, _col1, _col2\n+                          Reduce Output Operator\n+                            key expressions: _col2 (type: string)\n+                            sort order: +\n+                            value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)\n+        Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s3\n+                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+        Map 6 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s7\n+                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+        Map 7 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s6\n+                  Map Join Operator\n+                    condition map:\n+                         Inner Join 0 to 1\n+                    condition expressions:\n+                      0 {key}\n+                      1 \n+                    keys:\n+                      0 key (type: string)\n+                      1 key (type: string)\n+                    outputColumnNames: _col0\n+                    Select Operator\n+                      expressions: _col0 (type: string)\n+                      outputColumnNames: _col0\n+                      Map Join Operator\n+                        condition map:\n+                             Inner Join 0 to 1\n+                             Inner Join 1 to 2\n+                        condition expressions:\n+                          0 {key}\n+                          1 {_col0}\n+                          2 {key}\n+                        keys:\n+                          0 key (type: string)\n+                          1 _col0 (type: string)\n+                          2 key (type: string)\n+                        outputColumnNames: _col0, _col4, _col5\n+                        Select Operator\n+                          expressions: _col0 (type: string), _col4 (type: string), _col5 (type: string)\n+                          outputColumnNames: _col0, _col1, _col2\n+                          Reduce Output Operator\n+                            key expressions: _col2 (type: string)\n+                            sort order: +\n+                            value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)\n+        Map 8 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s8\n+                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string)\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string)\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string)\n+        Map 9 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s4\n+                  Map Join Operator\n+                    condition map:\n+                         Inner Join 0 to 1\n+                    condition expressions:\n+                      0 {key}\n+                      1 \n+                    keys:\n+                      0 key (type: string)\n+                      1 key (type: string)\n+                    outputColumnNames: _col0\n+                    Select Operator\n+                      expressions: _col0 (type: string)\n+                      outputColumnNames: _col0\n+                      Map Join Operator\n+                        condition map:\n+                             Inner Join 0 to 1\n+                             Inner Join 1 to 2\n+                        condition expressions:\n+                          0 {key}\n+                          1 {_col0}\n+                          2 {key}\n+                        keys:\n+                          0 key (type: string)\n+                          1 _col0 (type: string)\n+                          2 key (type: string)\n+                        outputColumnNames: _col0, _col4, _col5\n+                        Select Operator\n+                          expressions: _col0 (type: string), _col4 (type: string), _col5 (type: string)\n+                          outputColumnNames: _col0, _col1, _col2\n+                          Reduce Output Operator\n+                            key expressions: _col2 (type: string)\n+                            sort order: +\n+                            value expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string)\n+        Reducer 4 \n+            Reduce Operator Tree:\n+              Extract\n+                Statistics: Num rows: 415 Data size: 42193 Basic stats: COMPLETE Column stats: NONE\n+                File Output Operator\n+                  compressed: false\n+                  Statistics: Num rows: 415 Data size: 42193 Basic stats: COMPLETE Column stats: NONE\n+                  table:\n+                      input format: org.apache.hadoop.mapred.TextInputFormat\n+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 3 \n+            Vertex: Union 3\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+PREHOOK: query: create table ut as\n+select s1.key as skey, u1.key as ukey, s8.key as lkey from \n+src s1\n+join (select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+      union all select s4.key from src s4 join src s5 on s4.key = s5.key\n+      union all select s6.key from src s6 join src s7 on s6.key = s7.key) u1 on (s1.key = u1.key)\n+join src s8 on (u1.key = s8.key)\n+order by lkey\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@src\n+POSTHOOK: query: create table ut as\n+select s1.key as skey, u1.key as ukey, s8.key as lkey from \n+src s1\n+join (select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+      union all select s4.key from src s4 join src s5 on s4.key = s5.key\n+      union all select s6.key from src s6 join src s7 on s6.key = s7.key) u1 on (s1.key = u1.key)\n+join src s8 on (u1.key = s8.key)\n+order by lkey\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: select * from ut order by skey, ukey, lkey limit 100\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@ut\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from ut order by skey, ukey, lkey limit 100\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@ut\n+#### A masked pattern was here ####\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+0\t0\t0\n+PREHOOK: query: drop table ut\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@ut\n+PREHOOK: Output: default@ut\n+POSTHOOK: query: drop table ut\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@ut\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: explain\n+select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+union all select s4.key from src s4 join src s5 on s4.key = s5.key\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+union all select s4.key from src s4 join src s5 on s4.key = s5.key\n POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS), Map 3 (BROADCAST_EDGE)\n+        Map 4 <- Union 2 (CONTAINS), Map 5 (BROADCAST_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s2\n+                  Map Join Operator\n+                    condition map:\n+                         Inner Join 0 to 1\n+                    condition expressions:\n+                      0 {key}\n+                      1 \n+                    keys:\n+                      0 key (type: string)\n+                      1 key (type: string)\n+                    outputColumnNames: _col0\n+                    Select Operator\n+                      expressions: _col0 (type: string)\n+                      outputColumnNames: _col0\n+                      Select Operator\n+                        expressions: _col0 (type: string)\n+                        outputColumnNames: _col0\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s3\n+                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s4\n+                  Map Join Operator\n+                    condition map:\n+                         Inner Join 0 to 1\n+                    condition expressions:\n+                      0 {key}\n+                      1 \n+                    keys:\n+                      0 key (type: string)\n+                      1 key (type: string)\n+                    outputColumnNames: _col0\n+                    Select Operator\n+                      expressions: _col0 (type: string)\n+                      outputColumnNames: _col0\n+                      Select Operator\n+                        expressions: _col0 (type: string)\n+                        outputColumnNames: _col0\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s5\n+                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+PREHOOK: query: create table ut as\n+select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+union all select s4.key from src s4 join src s5 on s4.key = s5.key\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@src\n+POSTHOOK: query: create table ut as\n+select s2.key as key from src s2 join src s3 on s2.key = s3.key\n+union all select s4.key from src s4 join src s5 on s4.key = s5.key\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: select * from ut order by key limit 30\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@ut\n #### A masked pattern was here ####\n-4112\n+POSTHOOK: query: select * from ut order by key limit 30\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@ut\n+#### A masked pattern was here ####\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+0\n+10\n+10\n+100\n+100\n+100\n+100\n+100\n+100\n+100\n+100\n+103\n+103\n+PREHOOK: query: drop table ut\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@ut\n+PREHOOK: Output: default@ut\n+POSTHOOK: query: drop table ut\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@ut\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: explain\n+select * from\n+(select * from src union all select * from src) u\n+left outer join src s on u.key = s.key\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select * from\n+(select * from src union all select * from src) u\n+left outer join src s on u.key = s.key\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS), Map 3 (BROADCAST_EDGE)\n+        Map 4 <- Map 3 (BROADCAST_EDGE), Union 2 (CONTAINS)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string)\n+                    outputColumnNames: _col0, _col1\n+                    Map Join Operator\n+                      condition map:\n+                           Left Outer Join0 to 1\n+                      condition expressions:\n+                        0 {_col0} {_col1}\n+                        1 {key} {value}\n+                      keys:\n+                        0 _col0 (type: string)\n+                        1 key (type: string)\n+                      outputColumnNames: _col0, _col1, _col2, _col3\n+                      Select Operator\n+                        expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)\n+                        outputColumnNames: _col0, _col1, _col2, _col3\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s\n+                  Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string), value (type: string)\n+                  Reduce Output Operator\n+                    key expressions: key (type: string)\n+                    sort order: +\n+                    Map-reduce partition columns: key (type: string)\n+                    Statistics: Num rows: 29 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                    value expressions: key (type: string), value (type: string)\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string)\n+                    outputColumnNames: _col0, _col1\n+                    Map Join Operator\n+                      condition map:\n+                           Left Outer Join0 to 1\n+                      condition expressions:\n+                        0 {_col0} {_col1}\n+                        1 {key} {value}\n+                      keys:\n+                        0 _col0 (type: string)\n+                        1 key (type: string)\n+                      outputColumnNames: _col0, _col1, _col2, _col3\n+                      Select Operator\n+                        expressions: _col0 (type: string), _col1 (type: string), _col2 (type: string), _col3 (type: string)\n+                        outputColumnNames: _col0, _col1, _col2, _col3\n+                        File Output Operator\n+                          compressed: false\n+                          table:\n+                              input format: org.apache.hadoop.mapred.TextInputFormat\n+                              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+PREHOOK: query: explain\n+select u.key as ukey, s.key as skey from\n+(select * from src union all select * from src) u\n+right outer join src s on u.key = s.key\n+PREHOOK: type: QUERY\n+POSTHOOK: query: explain\n+select u.key as ukey, s.key as skey from\n+(select * from src union all select * from src) u\n+right outer join src s on u.key = s.key\n+POSTHOOK: type: QUERY\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+      Edges:\n+        Map 1 <- Union 2 (CONTAINS)\n+        Map 3 <- Union 2 (BROADCAST_EDGE)\n+        Map 4 <- Union 2 (CONTAINS)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string)\n+                    outputColumnNames: _col0, _col1\n+                    Reduce Output Operator\n+                      key expressions: _col0 (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: _col0 (type: string)\n+                      value expressions: _col0 (type: string)\n+        Map 3 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: s\n+                  Statistics: Num rows: 58 Data size: 5812 Basic stats: COMPLETE Column stats: NONE\n+                  Map Join Operator\n+                    condition map:\n+                         Right Outer Join0 to 1\n+                    condition expressions:\n+                      0 {_col0}\n+                      1 {key}\n+                    keys:\n+                      0 _col0 (type: string)\n+                      1 key (type: string)\n+                    outputColumnNames: _col0, _col2\n+                    Statistics: Num rows: 63 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n+                    Select Operator\n+                      expressions: _col0 (type: string), _col2 (type: string)\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 63 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n+                      File Output Operator\n+                        compressed: false\n+                        Statistics: Num rows: 63 Data size: 12786 Basic stats: COMPLETE Column stats: NONE\n+                        table:\n+                            input format: org.apache.hadoop.mapred.TextInputFormat\n+                            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Map 4 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: src\n+                  Select Operator\n+                    expressions: key (type: string), value (type: string)\n+                    outputColumnNames: _col0, _col1\n+                    Reduce Output Operator\n+                      key expressions: _col0 (type: string)\n+                      sort order: +\n+                      Map-reduce partition columns: _col0 (type: string)\n+                      value expressions: _col0 (type: string)\n+        Union 2 \n+            Vertex: Union 2\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+\n+PREHOOK: query: create table ut as\n+select u.key as ukey, s.key as skey from\n+(select * from src union all select * from src) u\n+right outer join src s on u.key = s.key\n+PREHOOK: type: CREATETABLE_AS_SELECT\n+PREHOOK: Input: default@src\n+POSTHOOK: query: create table ut as\n+select u.key as ukey, s.key as skey from\n+(select * from src union all select * from src) u\n+right outer join src s on u.key = s.key\n+POSTHOOK: type: CREATETABLE_AS_SELECT\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@ut\n+PREHOOK: query: select * from ut order by ukey, skey limit 20\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@ut\n+#### A masked pattern was here ####\n+POSTHOOK: query: select * from ut order by ukey, skey limit 20\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@ut\n+#### A masked pattern was here ####\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+0\t0\n+10\t10\n+10\t10\n+PREHOOK: query: drop table ut\n+PREHOOK: type: DROPTABLE\n+PREHOOK: Input: default@ut\n+PREHOOK: Output: default@ut\n+POSTHOOK: query: drop table ut\n+POSTHOOK: type: DROPTABLE\n+POSTHOOK: Input: default@ut\n+POSTHOOK: Output: default@ut",
                "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/tez_union.q.out",
                "sha": "4c0d57cc1ab2f93a7f9a6dbb2e7e6f2cd2e133c8",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hive/blob/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/vectorization_15.q.out",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/vectorization_15.q.out?ref=d375b3977d42936b3b00888e2b1bacf736e8ac3e",
                "deletions": 19,
                "filename": "ql/src/test/results/clientpositive/tez/vectorization_15.q.out",
                "patch": "@@ -77,32 +77,32 @@ NULL\ttrue\t10419.0\t10\tNULL\t-721614386\tNULL\tNULL\t7.2161435972E8\t10419.0\t828862.706\n NULL\ttrue\t14519.0\t100xJdkyc\tNULL\t729277608\tNULL\tNULL\t-7.2927763428E8\t14519.0\t1155030.007\tNULL\tNULL\tNULL\t-23.0\tNULL\t0.0\tNULL\tNULL\t7.2927763428E8\t0.0\n -62.0\tNULL\t15601.0\tNULL\t-62\tNULL\t1969-12-31 16:00:09.889\t0.0\tNULL\t15601.0\t1241106.353\t33.0\t0.0\t0.0\t-23.0\t62\tNULL\tNULL\t-23\tNULL\tNULL\n -51.0\tNULL\t-200.0\tNULL\t-51\tNULL\t1969-12-31 15:59:55.423\t0.0\tNULL\t-200.0\t-15910.599999999999\t33.0\t0.0\t0.0\t-23.0\t51\tNULL\tNULL\t-23\tNULL\tNULL\n--51.0\tfalse\tNULL\t10\t-51\t1058319346\t1969-12-31 16:00:08.451\t0.0\t-1.05831937228E9\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t1.058319397E9\t-23\t1.05831937228E9\t0.0\n--51.0\tfalse\tNULL\t10TYIE5S35U6dj3N\t-51\t-469581869\t1969-12-31 16:00:08.451\t0.0\t4.6958184272E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-4.69581818E8\t-23\t-4.6958184272E8\t0.0\n--51.0\tfalse\tNULL\t1Lh6Uoq3WhNtOqQHu7WN7U\t-51\t-352637533\t1969-12-31 16:00:08.451\t0.0\t3.5263750672E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-3.52637482E8\t-23\t-3.5263750672E8\t0.0\n--51.0\ttrue\tNULL\t04Y1mA17\t-51\t-114647521\t1969-12-31 16:00:08.451\t0.0\t1.1464749472E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-1.1464747E8\t-23\t-1.1464749472E8\t0.0\n+-51.0\tfalse\tNULL\t10\t-51\t1058319346\t1969-12-31 16:00:08.451\t0.0\t-1.05831937228E9\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t1.05831942E9\t-23\t1.05831937228E9\t0.0\n+-51.0\tfalse\tNULL\t10TYIE5S35U6dj3N\t-51\t-469581869\t1969-12-31 16:00:08.451\t0.0\t4.6958184272E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-4.69581792E8\t-23\t-4.6958184272E8\t0.0\n+-51.0\tfalse\tNULL\t1Lh6Uoq3WhNtOqQHu7WN7U\t-51\t-352637533\t1969-12-31 16:00:08.451\t0.0\t3.5263750672E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-3.52637472E8\t-23\t-3.5263750672E8\t0.0\n+-51.0\ttrue\tNULL\t04Y1mA17\t-51\t-114647521\t1969-12-31 16:00:08.451\t0.0\t1.1464749472E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-1.14647472E8\t-23\t-1.1464749472E8\t0.0\n -51.0\ttrue\tNULL\t10Wu570aLPO0p02P17FeH\t-51\t405338893\t1969-12-31 16:00:08.451\t0.0\t-4.0533891928E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t4.05338944E8\t-23\t4.0533891928E8\t0.0\n -51.0\ttrue\tNULL\t3cQp060\t-51\t-226923315\t1969-12-31 16:00:08.451\t0.0\t2.2692328872E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-2.26923264E8\t-23\t-2.2692328872E8\t0.0\n--51.0\ttrue\tNULL\t8EPG0Xi307qd\t-51\t-328662044\t1969-12-31 16:00:08.451\t0.0\t3.2866201772E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-3.28661993E8\t-23\t-3.2866201772E8\t0.0\n--51.0\ttrue\tNULL\t8iHtdkJ6d\t-51\t1006818344\t1969-12-31 16:00:08.451\t0.0\t-1.00681837028E9\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t1.006818395E9\t-23\t1.00681837028E9\t0.0\n--51.0\ttrue\tNULL\tQiOcvR0kt6r7f0R7fiPxQTCU\t-51\t266531954\t1969-12-31 16:00:08.451\t0.0\t-2.6653198028E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t2.66532005E8\t-23\t2.6653198028E8\t0.0\n--51.0\ttrue\tNULL\tYbpj38RTTYl7CnJXPNx1g4C\t-51\t-370919370\t1969-12-31 16:00:08.451\t0.0\t3.7091934372E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-3.70919319E8\t-23\t-3.7091934372E8\t0.0\n+-51.0\ttrue\tNULL\t8EPG0Xi307qd\t-51\t-328662044\t1969-12-31 16:00:08.451\t0.0\t3.2866201772E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-3.28661984E8\t-23\t-3.2866201772E8\t0.0\n+-51.0\ttrue\tNULL\t8iHtdkJ6d\t-51\t1006818344\t1969-12-31 16:00:08.451\t0.0\t-1.00681837028E9\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t1.00681843E9\t-23\t1.00681837028E9\t0.0\n+-51.0\ttrue\tNULL\tQiOcvR0kt6r7f0R7fiPxQTCU\t-51\t266531954\t1969-12-31 16:00:08.451\t0.0\t-2.6653198028E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t2.66532E8\t-23\t2.6653198028E8\t0.0\n+-51.0\ttrue\tNULL\tYbpj38RTTYl7CnJXPNx1g4C\t-51\t-370919370\t1969-12-31 16:00:08.451\t0.0\t3.7091934372E8\tNULL\tNULL\t33.0\t0.0\t0.0\tNULL\t51\t0.0\t-3.70919296E8\t-23\t-3.7091934372E8\t0.0\n -48.0\tNULL\t-7196.0\tNULL\t-48\tNULL\t1969-12-31 16:00:06.337\t0.0\tNULL\t-7196.0\t-572463.388\t33.0\t0.0\t0.0\t-23.0\t48\tNULL\tNULL\t-23\tNULL\tNULL\n -6.0\tNULL\t-200.0\tNULL\t-6\tNULL\t1969-12-31 15:59:56.094\t0.0\tNULL\t-200.0\t-15910.599999999999\t3.0\t0.0\t0.0\t-23.0\t6\tNULL\tNULL\t-5\tNULL\tNULL\n 5.0\tNULL\t15601.0\tNULL\t5\tNULL\t1969-12-31 16:00:00.959\t0.0\tNULL\t15601.0\t1241106.353\t3.0\t0.0\t0.0\t-23.0\t-5\tNULL\tNULL\t-3\tNULL\tNULL\n-8.0\tfalse\tNULL\t10V3pN5r5lI2qWl2lG103\t8\t-362835731\t1969-12-31 16:00:15.892\t0.0\t3.6283570472E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-3.62835739E8\t-7\t-3.6283570472E8\t0.0\n-8.0\tfalse\tNULL\t10c4qt584m5y6uWT\t8\t-183000142\t1969-12-31 16:00:15.892\t0.0\t1.8300011572E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-1.8300015E8\t-7\t-1.8300011572E8\t0.0\n+8.0\tfalse\tNULL\t10V3pN5r5lI2qWl2lG103\t8\t-362835731\t1969-12-31 16:00:15.892\t0.0\t3.6283570472E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-3.62835744E8\t-7\t-3.6283570472E8\t0.0\n+8.0\tfalse\tNULL\t10c4qt584m5y6uWT\t8\t-183000142\t1969-12-31 16:00:15.892\t0.0\t1.8300011572E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-1.8300016E8\t-7\t-1.8300011572E8\t0.0\n 8.0\tfalse\tNULL\t8GloEukQ0c68JDmnYL53\t8\t-722873402\t1969-12-31 16:00:15.892\t0.0\t7.2287337572E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-7.2287341E8\t-7\t-7.2287337572E8\t0.0\n 8.0\tfalse\tNULL\tkA0XH5C5\t8\t-503903864\t1969-12-31 16:00:15.892\t0.0\t5.0390383772E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-5.03903872E8\t-7\t-5.0390383772E8\t0.0\n-8.0\ttrue\tNULL\t100VTM7PEW8GH1uE\t8\t88129338\t1969-12-31 16:00:15.892\t0.0\t-8.812936428E7\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t8.812933E7\t-7\t8.812936428E7\t0.0\n-8.0\ttrue\tNULL\t1062158y\t8\t-1005155523\t1969-12-31 16:00:15.892\t0.0\t1.00515549672E9\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-1.005155531E9\t-7\t-1.00515549672E9\t0.0\n-8.0\ttrue\tNULL\t1063cEnGjSal\t8\t-624769630\t1969-12-31 16:00:15.892\t0.0\t6.2476960372E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-6.24769638E8\t-7\t-6.2476960372E8\t0.0\n-8.0\ttrue\tNULL\t4kMasVoB7lX1wc5i64bNk\t8\t683567667\t1969-12-31 16:00:15.892\t0.0\t-6.8356769328E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t6.83567659E8\t-7\t6.8356769328E8\t0.0\n-8.0\ttrue\tNULL\tXH6I7A417\t8\t436627202\t1969-12-31 16:00:15.892\t0.0\t-4.3662722828E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t4.36627194E8\t-7\t4.3662722828E8\t0.0\n-11.0\tfalse\tNULL\t10pO8p1LNx4Y\t11\t271296824\t1969-12-31 16:00:02.351\t0.0\t-2.7129685028E8\tNULL\tNULL\t0.0\t0.0\t0.0\tNULL\t-11\t0.0\t2.71296813E8\t-1\t2.7129685028E8\t0.0\n-11.0\tfalse\tNULL\t1H6wGP\t11\t-560827082\t1969-12-31 16:00:02.351\t0.0\t5.6082705572E8\tNULL\tNULL\t0.0\t0.0\t0.0\tNULL\t-11\t0.0\t-5.60827093E8\t-1\t-5.6082705572E8\t0.0\n-11.0\tfalse\tNULL\t2a7V63IL7jK3o\t11\t-325931647\t1969-12-31 16:00:02.351\t0.0\t3.2593162072E8\tNULL\tNULL\t0.0\t0.0\t0.0\tNULL\t-11\t0.0\t-3.25931658E8\t-1\t-3.2593162072E8\t0.0\n-11.0\ttrue\tNULL\t10\t11\t92365813\t1969-12-31 16:00:02.351\t0.0\t-9.236583928E7\tNULL\tNULL\t0.0\t0.0\t0.0\tNULL\t-11\t0.0\t9.2365802E7\t-1\t9.236583928E7\t0.0\n+8.0\ttrue\tNULL\t100VTM7PEW8GH1uE\t8\t88129338\t1969-12-31 16:00:15.892\t0.0\t-8.812936428E7\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t8.8129328E7\t-7\t8.812936428E7\t0.0\n+8.0\ttrue\tNULL\t1062158y\t8\t-1005155523\t1969-12-31 16:00:15.892\t0.0\t1.00515549672E9\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-1.00515552E9\t-7\t-1.00515549672E9\t0.0\n+8.0\ttrue\tNULL\t1063cEnGjSal\t8\t-624769630\t1969-12-31 16:00:15.892\t0.0\t6.2476960372E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t-6.247696E8\t-7\t-6.2476960372E8\t0.0\n+8.0\ttrue\tNULL\t4kMasVoB7lX1wc5i64bNk\t8\t683567667\t1969-12-31 16:00:15.892\t0.0\t-6.8356769328E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t6.8356768E8\t-7\t6.8356769328E8\t0.0\n+8.0\ttrue\tNULL\tXH6I7A417\t8\t436627202\t1969-12-31 16:00:15.892\t0.0\t-4.3662722828E8\tNULL\tNULL\t1.0\t0.0\t0.0\tNULL\t-8\t0.0\t4.366272E8\t-7\t4.3662722828E8\t0.0\n+11.0\tfalse\tNULL\t10pO8p1LNx4Y\t11\t271296824\t1969-12-31 16:00:02.351\t0.0\t-2.7129685028E8\tNULL\tNULL\t0.0\t0.0\t0.0\tNULL\t-11\t0.0\t2.71296832E8\t-1\t2.7129685028E8\t0.0\n+11.0\tfalse\tNULL\t1H6wGP\t11\t-560827082\t1969-12-31 16:00:02.351\t0.0\t5.6082705572E8\tNULL\tNULL\t0.0\t0.0\t0.0\tNULL\t-11\t0.0\t-5.6082707E8\t-1\t-5.6082705572E8\t0.0\n+11.0\tfalse\tNULL\t2a7V63IL7jK3o\t11\t-325931647\t1969-12-31 16:00:02.351\t0.0\t3.2593162072E8\tNULL\tNULL\t0.0\t0.0\t0.0\tNULL\t-11\t0.0\t-3.25931648E8\t-1\t-3.2593162072E8\t0.0\n+11.0\ttrue\tNULL\t10\t11\t92365813\t1969-12-31 16:00:02.351\t0.0\t-9.236583928E7\tNULL\tNULL\t0.0\t0.0\t0.0\tNULL\t-11\t0.0\t9.2365808E7\t-1\t9.236583928E7\t0.0\n 21.0\tNULL\t15601.0\tNULL\t21\tNULL\t1969-12-31 16:00:14.256\t0.0\tNULL\t15601.0\t1241106.353\t12.0\t0.0\t0.0\t-23.0\t-21\tNULL\tNULL\t-2\tNULL\tNULL\n 32.0\tNULL\t-200.0\tNULL\t32\tNULL\t1969-12-31 16:00:02.445\t0.0\tNULL\t-200.0\t-15910.599999999999\t1.0\t0.0\t0.0\t-23.0\t-32\tNULL\tNULL\t-23\tNULL\tNULL\n 36.0\tNULL\t-200.0\tNULL\t36\tNULL\t1969-12-31 16:00:00.554\t0.0\tNULL\t-200.0\t-15910.599999999999\t33.0\t0.0\t0.0\t-23.0\t-36\tNULL\tNULL\t-23\tNULL\tNULL",
                "raw_url": "https://github.com/apache/hive/raw/d375b3977d42936b3b00888e2b1bacf736e8ac3e/ql/src/test/results/clientpositive/tez/vectorization_15.q.out",
                "sha": "717a3cb73cb8f6db5734fb14b9293f721fc8a11f",
                "status": "modified"
            }
        ],
        "message": "HIVE-6753: Unions on Tez NPE when there's a mapjoin the union work (Gunther Hagleitner, reviewed by Vikram Dixit K)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1582488 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/1569a0e7a1d72e182ababb43f5f2b14d8bf8494d",
        "patched_files": [
            "GenTezWork.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestGenTezWork.java"
        ]
    },
    "hive_d5a22ee": {
        "bug_id": "hive_d5a22ee",
        "commit": "https://github.com/apache/hive/commit/d5a22eec75e07b8b669cd524f0ca3a85db116830",
        "file": [
            {
                "additions": 49,
                "blob_url": "https://github.com/apache/hive/blob/d5a22eec75e07b8b669cd524f0ca3a85db116830/metastore/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandlerNegative.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandlerNegative.java?ref=d5a22eec75e07b8b669cd524f0ca3a85db116830",
                "deletions": 0,
                "filename": "metastore/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandlerNegative.java",
                "patch": "@@ -0,0 +1,49 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hive.metastore.txn;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.metastore.api.MetaException;\n+import org.junit.Test;\n+\n+public class TestTxnHandlerNegative {\n+  static final private Log LOG = LogFactory.getLog(TestTxnHandlerNegative.class);\n+\n+  /**\n+   * this intentionally sets a bad URL for connection to test error handling logic\n+   * in TxnHandler\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testBadConnection() throws Exception {\n+    HiveConf conf = new HiveConf();\n+    conf.setVar(HiveConf.ConfVars.METASTORECONNECTURLKEY, \"blah\");\n+    TxnHandler txnHandler1 = new TxnHandler(conf);\n+    MetaException e = null;\n+    try {\n+      txnHandler1.getOpenTxns();\n+    }\n+    catch(MetaException ex) {\n+      LOG.info(\"Expected error: \" + ex.getMessage(), ex);\n+      e = ex;\n+    }\n+    assert e != null : \"did not get exception\";\n+  }\n+}",
                "raw_url": "https://github.com/apache/hive/raw/d5a22eec75e07b8b669cd524f0ca3a85db116830/metastore/src/test/org/apache/hadoop/hive/metastore/txn/TestTxnHandlerNegative.java",
                "sha": "abceaf321db790508f897537f1f7e5f5ac070576",
                "status": "added"
            }
        ],
        "message": "HIVE-9404 NPE in org.apache.hadoop.hive.metastore.txn.TxnHandler.determineDatabaseProduct() (Eugene Koifman, reviewed by Alan Gates)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1653337 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/ec0eb1b65ff2457537c254d0691bb763677950d6",
        "patched_files": [],
        "repo": "hive",
        "unit_tests": [
            "TestTxnHandlerNegative.java"
        ]
    },
    "hive_d9fae04": {
        "bug_id": "hive_d9fae04",
        "commit": "https://github.com/apache/hive/commit/d9fae049305e20ec8a72e581a2fc938028523402",
        "file": [
            {
                "additions": 55,
                "blob_url": "https://github.com/apache/hive/blob/d9fae049305e20ec8a72e581a2fc938028523402/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java",
                "changes": 55,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java?ref=d9fae049305e20ec8a72e581a2fc938028523402",
                "deletions": 0,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java",
                "patch": "@@ -620,4 +620,59 @@ public void testReplLoadFromSourceUsingWithClause() throws Throwable {\n             .run(\"show functions like '\" + replicatedDbName + \"*'\")\n             .verifyResult(null);\n   }\n+\n+  @Test\n+  public void testIncrementalReplWithEventsBatchHavingDropCreateTable() throws Throwable {\n+    // Bootstrap dump with empty db\n+    WarehouseInstance.Tuple bootstrapTuple = primary.dump(primaryDbName, null);\n+\n+    // Bootstrap load in replica\n+    replica.load(replicatedDbName, bootstrapTuple.dumpLocation)\n+            .status(replicatedDbName)\n+            .verifyResult(bootstrapTuple.lastReplicationId);\n+\n+    // First incremental dump\n+    WarehouseInstance.Tuple firstIncremental = primary.run(\"use \" + primaryDbName)\n+            .run(\"create table table1 (i int)\")\n+            .run(\"create table table2 (id int) partitioned by (country string)\")\n+            .run(\"insert into table1 values (1)\")\n+            .run(\"insert into table2 partition(country='india') values(1)\")\n+            .dump(primaryDbName, bootstrapTuple.lastReplicationId);\n+\n+    // Second incremental dump\n+    WarehouseInstance.Tuple secondIncremental = primary.run(\"use \" + primaryDbName)\n+            .run(\"drop table table1\")\n+            .run(\"drop table table2\")\n+            .run(\"create table table2 (id int) partitioned by (country string)\")\n+            .run(\"alter table table2 add partition(country='india')\")\n+            .run(\"alter table table2 drop partition(country='india')\")\n+            .run(\"insert into table2 partition(country='us') values(2)\")\n+            .run(\"create table table1 (i int)\")\n+            .run(\"insert into table1 values (2)\")\n+            .dump(primaryDbName, firstIncremental.lastReplicationId);\n+\n+    // First incremental load\n+    replica.load(replicatedDbName, firstIncremental.dumpLocation)\n+            .status(replicatedDbName)\n+            .verifyResult(firstIncremental.lastReplicationId)\n+            .run(\"use \" + replicatedDbName)\n+            .run(\"show tables\")\n+            .verifyResults(new String[] {\"table1\", \"table2\"})\n+            .run(\"select * from table1\")\n+            .verifyResults(new String[] {\"1\"})\n+            .run(\"select id from table2 order by id\")\n+            .verifyResults(new String[] {\"1\"});\n+\n+    // Second incremental load\n+    replica.load(replicatedDbName, secondIncremental.dumpLocation)\n+            .status(replicatedDbName)\n+            .verifyResult(secondIncremental.lastReplicationId)\n+            .run(\"use \" + replicatedDbName)\n+            .run(\"show tables\")\n+            .verifyResults(new String[] {\"table1\", \"table2\"})\n+            .run(\"select * from table1\")\n+            .verifyResults(new String[] {\"2\"})\n+            .run(\"select id from table2 order by id\")\n+            .verifyResults(new String[] {\"2\"});\n+  }\n }",
                "raw_url": "https://github.com/apache/hive/raw/d9fae049305e20ec8a72e581a2fc938028523402/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/TestReplicationScenariosAcrossInstances.java",
                "sha": "70e1aa7f3aaa0d4e95ffefae47ad891f405a9637",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/d9fae049305e20ec8a72e581a2fc938028523402/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java?ref=d9fae049305e20ec8a72e581a2fc938028523402",
                "deletions": 0,
                "filename": "itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java",
                "patch": "@@ -231,6 +231,11 @@ WarehouseInstance load(String replicatedDbName, String dumpLocation, List<String\n     return run(replLoadCmd);\n   }\n \n+  WarehouseInstance status(String replicatedDbName) throws Throwable {\n+    String replStatusCmd = \"REPL STATUS \" + replicatedDbName;\n+    return run(replStatusCmd);\n+  }\n+\n   WarehouseInstance status(String replicatedDbName, List<String> withClauseOptions) throws Throwable {\n     String replStatusCmd = \"REPL STATUS \" + replicatedDbName;\n     if (!withClauseOptions.isEmpty()) {",
                "raw_url": "https://github.com/apache/hive/raw/d9fae049305e20ec8a72e581a2fc938028523402/itests/hive-unit/src/test/java/org/apache/hadoop/hive/ql/parse/WarehouseInstance.java",
                "sha": "accdc1ff0824dd6aac9827e8dbe7ea4b7c56e2f4",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/d9fae049305e20ec8a72e581a2fc938028523402/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java?ref=d9fae049305e20ec8a72e581a2fc938028523402",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java",
                "patch": "@@ -4522,6 +4522,12 @@ private void dropPartitions(Hive db, Table tbl, DropTableDesc dropTbl) throws Hi\n       // for dropping. Thus, we need a way to push this filter (replicationSpec.allowEventReplacementInto)\n       // to the  metastore to allow it to do drop a partition or not, depending on a Predicate on the\n       // parameter key values.\n+\n+      if (tbl == null) {\n+        // If table is missing, then partitions are also would've been dropped. Just no-op.\n+        return;\n+      }\n+\n       for (DropTableDesc.PartSpec partSpec : dropTbl.getPartSpecs()){\n         List<Partition> partitions = new ArrayList<>();\n         try {\n@@ -4551,7 +4557,7 @@ private void dropPartitions(Hive db, Table tbl, DropTableDesc dropTbl) throws Hi\n       console.printInfo(\"Dropped the partition \" + partition.getName());\n       // We have already locked the table, don't lock the partitions.\n       addIfAbsentByName(new WriteEntity(partition, WriteEntity.WriteType.DDL_NO_LOCK));\n-    };\n+    }\n   }\n \n   private void dropTable(Hive db, Table tbl, DropTableDesc dropTbl) throws HiveException {",
                "raw_url": "https://github.com/apache/hive/raw/d9fae049305e20ec8a72e581a2fc938028523402/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java",
                "sha": "61a04326ac51d6423001eb8cfbe6707decf5416a",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hive/blob/d9fae049305e20ec8a72e581a2fc938028523402/ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java?ref=d9fae049305e20ec8a72e581a2fc938028523402",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java",
                "patch": "@@ -34,6 +34,7 @@\n import org.apache.hadoop.hive.ql.metadata.Hive;\n import org.apache.hadoop.hive.ql.metadata.Partition;\n import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.parse.repl.DumpType;\n import org.apache.hadoop.hive.ql.parse.repl.dump.Utils;\n import org.apache.hadoop.hive.ql.parse.repl.dump.io.DBSerializer;\n import org.apache.hadoop.hive.ql.parse.repl.dump.io.JsonWriter;\n@@ -92,6 +93,7 @@\n     private List<Task<? extends Serializable>> tasks;\n     private Logger LOG;\n     private Context ctx;\n+    private DumpType eventType = DumpType.EVENT_UNKNOWN;\n \n     public HiveConf getConf() {\n       return conf;\n@@ -121,6 +123,14 @@ public Context getCtx() {\n       return ctx;\n     }\n \n+    public void setEventType(DumpType eventType) {\n+      this.eventType = eventType;\n+    }\n+\n+    public DumpType getEventType() {\n+      return eventType;\n+    }\n+\n     public SemanticAnalyzerWrapperContext(HiveConf conf, Hive db,\n                                           HashSet<ReadEntity> inputs,\n                                           HashSet<WriteEntity> outputs,",
                "raw_url": "https://github.com/apache/hive/raw/d9fae049305e20ec8a72e581a2fc938028523402/ql/src/java/org/apache/hadoop/hive/ql/parse/EximUtil.java",
                "sha": "0d2fafb83b9bc008958cb11c5feb88c884e10425",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hive/blob/d9fae049305e20ec8a72e581a2fc938028523402/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java?ref=d9fae049305e20ec8a72e581a2fc938028523402",
                "deletions": 3,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java",
                "patch": "@@ -47,12 +47,14 @@\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hadoop.hive.ql.metadata.InvalidTableException;\n import org.apache.hadoop.hive.ql.metadata.Table;\n+import org.apache.hadoop.hive.ql.parse.repl.DumpType;\n import org.apache.hadoop.hive.ql.parse.repl.load.MetaData;\n import org.apache.hadoop.hive.ql.parse.repl.load.UpdatedMetaDataTracker;\n import org.apache.hadoop.hive.ql.plan.AddPartitionDesc;\n import org.apache.hadoop.hive.ql.plan.CopyWork;\n import org.apache.hadoop.hive.ql.plan.ImportTableDesc;\n import org.apache.hadoop.hive.ql.plan.DDLWork;\n+import org.apache.hadoop.hive.ql.plan.DropTableDesc;\n import org.apache.hadoop.hive.ql.plan.LoadTableDesc;\n import org.apache.hadoop.hive.ql.plan.LoadTableDesc.LoadFileType;\n import org.apache.hadoop.hive.ql.plan.MoveWork;\n@@ -438,6 +440,13 @@ private static boolean isAcid(Long writeId) {\n     return tableDesc.getCreateTableTask(x.getInputs(), x.getOutputs(), x.getConf());\n   }\n \n+  private static Task<?> dropTableTask(Table table, EximUtil.SemanticAnalyzerWrapperContext x,\n+                                       ReplicationSpec replicationSpec) {\n+    DropTableDesc dropTblDesc = new DropTableDesc(table.getTableName(), table.getTableType(),\n+            true, false, replicationSpec);\n+    return TaskFactory.get(new DDLWork(x.getInputs(), x.getOutputs(), dropTblDesc), x.getConf());\n+  }\n+\n   private static Task<? extends Serializable> alterTableTask(ImportTableDesc tableDesc,\n       EximUtil.SemanticAnalyzerWrapperContext x, ReplicationSpec replicationSpec) {\n     tableDesc.setReplaceMode(true);\n@@ -912,7 +921,7 @@ private static void createReplImportTasks(\n       UpdatedMetaDataTracker updatedMetadata)\n       throws HiveException, URISyntaxException, IOException, MetaException {\n \n-    Task<?> dr = null;\n+    Task<?> dropTblTask = null;\n     WriteEntity.WriteType lockType = WriteEntity.WriteType.DDL_NO_LOCK;\n \n     // Normally, on import, trying to create a table or a partition in a db that does not yet exist\n@@ -934,6 +943,15 @@ private static void createReplImportTasks(\n                 tblDesc.getDatabaseName(), tblDesc.getTableName());\n         return;\n       }\n+\n+      // If the table exists and we found a valid create table event, then need to drop the table first\n+      // and then create it. This case is possible if the event sequence is drop_table(t1) -> create_table(t1).\n+      // We need to drop here to handle the case where the previous incremental load created the table but\n+      // didn't set the last repl ID due to some failure.\n+      if (x.getEventType() == DumpType.EVENT_CREATE_TABLE) {\n+        dropTblTask = dropTableTask(table, x, replicationSpec);\n+        table = null;\n+      }\n     } else {\n       // If table doesn't exist, allow creating a new one only if the database state is older than the update.\n       if ((parentDb != null) && (!replicationSpec.allowReplacementInto(parentDb.getParameters()))) {\n@@ -1000,8 +1018,15 @@ private static void createReplImportTasks(\n           t.addDependentTask(loadTable(fromURI, table, true, new Path(tblDesc.getLocation()), replicationSpec, x, writeId, stmtId, isSourceMm));\n         }\n       }\n-      // Simply create\n-      x.getTasks().add(t);\n+\n+      if (dropTblTask != null) {\n+        // Drop first and then create\n+        dropTblTask.addDependentTask(t);\n+        x.getTasks().add(dropTblTask);\n+      } else {\n+        // Simply create\n+        x.getTasks().add(t);\n+      }\n     } else {\n       // Table existed, and is okay to replicate into, not dropping and re-creating.\n       if (table.isPartitioned()) {",
                "raw_url": "https://github.com/apache/hive/raw/d9fae049305e20ec8a72e581a2fc938028523402/ql/src/java/org/apache/hadoop/hive/ql/parse/ImportSemanticAnalyzer.java",
                "sha": "832f660079e44785a240aa938287ed2ea770f98d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/d9fae049305e20ec8a72e581a2fc938028523402/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/TableHandler.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/TableHandler.java?ref=d9fae049305e20ec8a72e581a2fc938028523402",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/TableHandler.java",
                "patch": "@@ -36,6 +36,7 @@\n           new EximUtil.SemanticAnalyzerWrapperContext(\n               context.hiveConf, context.db, readEntitySet, writeEntitySet, importTasks, context.log,\n               context.nestedContext);\n+      x.setEventType(context.dmd.getDumpType());\n \n       // REPL LOAD is not partition level. It is always DB or table level. So, passing null for partition specs.\n       // Also, REPL LOAD doesn't support external table and hence no location set as well.",
                "raw_url": "https://github.com/apache/hive/raw/d9fae049305e20ec8a72e581a2fc938028523402/ql/src/java/org/apache/hadoop/hive/ql/parse/repl/load/message/TableHandler.java",
                "sha": "7f6e80a7d156d7940fe4e24481b6a8b9abd80866",
                "status": "modified"
            }
        ],
        "message": "HIVE-19130: NPE is thrown when REPL LOAD applied drop partition event (Sankar Hariappan, reviewed by Mahesh Kumar Behera, Thejas M Nair)",
        "parent": "https://github.com/apache/hive/commit/244ca8e5c3192acd017d691ccdbaf0fa06c9fe39",
        "patched_files": [
            "ImportSemanticAnalyzer.java",
            "DDLTask.java",
            "EximUtil.java",
            "WarehouseInstance.java",
            "TableHandler.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestReplicationScenariosAcrossInstances.java",
            "TestEximUtil.java"
        ]
    },
    "hive_daca6a6": {
        "bug_id": "hive_daca6a6",
        "commit": "https://github.com/apache/hive/commit/daca6a6dedafd862bcf271a8cf117100378d3f57",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hive/blob/daca6a6dedafd862bcf271a8cf117100378d3f57/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java?ref=daca6a6dedafd862bcf271a8cf117100378d3f57",
                "deletions": 2,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java",
                "patch": "@@ -29,6 +29,8 @@\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.hive.common.type.Decimal128;\n import org.apache.hadoop.hive.common.type.HiveDecimal;\n@@ -63,7 +65,9 @@\n  * with the partition column.\n  */\n public class VectorizedRowBatchCtx {\n-  \n+\n+  private static final Log LOG = LogFactory.getLog(VectorizedRowBatchCtx.class.getName());\n+\n   // OI for raw row data (EG without partition cols)\n   private StructObjectInspector rawRowOI;\n \n@@ -223,6 +227,9 @@ public void init(Configuration hiveConf, FileSplit split) throws ClassNotFoundEx\n                   convert(partSpec.get(key));              \n           partitionTypes.put(key, TypeInfoFactory.getPrimitiveTypeInfo(partKeyTypes[i]).getPrimitiveCategory());\n         }\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Partition column: name: \" + key + \", value: \" + objectVal + \", type: \" + partitionTypes.get(key));\n+        }\n         partitionValues.put(key, objectVal);\n         partObjectInspectors.add(objectInspector);\n       }\n@@ -263,7 +270,7 @@ public VectorizedRowBatch createVectorizedRowBatch() throws HiveException\n       // in the included list.\n       if ((colsToInclude == null) || colsToInclude.contains(j)\n           || ((partitionValues != null) &&\n-              (partitionValues.get(fieldRefs.get(j).getFieldName()) != null))) {\n+              partitionValues.containsKey(fieldRefs.get(j).getFieldName()))) {\n         ObjectInspector foi = fieldRefs.get(j).getFieldObjectInspector();\n         switch (foi.getCategory()) {\n         case PRIMITIVE: {",
                "raw_url": "https://github.com/apache/hive/raw/daca6a6dedafd862bcf271a8cf117100378d3f57/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/VectorizedRowBatchCtx.java",
                "sha": "4d5ed40eb19158c7883231849466deef3bfe07c9",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hive/blob/daca6a6dedafd862bcf271a8cf117100378d3f57/ql/src/test/queries/clientpositive/vector_non_string_partition.q",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/vector_non_string_partition.q?ref=daca6a6dedafd862bcf271a8cf117100378d3f57",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/vector_non_string_partition.q",
                "patch": "@@ -0,0 +1,17 @@\n+SET hive.vectorized.execution.enabled=true;\n+CREATE TABLE non_string_part(cint INT, cstring1 STRING, cdouble DOUBLE, ctimestamp1 TIMESTAMP) PARTITIONED BY (ctinyint tinyint) STORED AS ORC;\n+SET hive.exec.dynamic.partition.mode=nonstrict;\n+SET hive.exec.dynamic.partition=true;\n+\n+INSERT OVERWRITE TABLE non_string_part PARTITION(ctinyint) SELECT cint, cstring1, cdouble, ctimestamp1, ctinyint fROM alltypesorc \n+WHERE ctinyint IS NULL AND cdouble IS NOT NULL ORDER BY cdouble;\n+\n+SHOW PARTITIONS non_string_part;\n+\n+EXPLAIN SELECT cint, ctinyint FROM non_string_part WHERE cint > 0 ORDER BY cint LIMIT 10;\n+\n+SELECT cint, ctinyint FROM non_string_part WHERE cint > 0 ORDER BY cint LIMIT 10;\n+\n+EXPLAIN SELECT cint, cstring1 FROM non_string_part WHERE cint > 0 ORDER BY cint, cstring1 LIMIT 10;\n+\n+SELECT cint, cstring1 FROM non_string_part WHERE cint > 0 ORDER BY cint, cstring1 LIMIT 10;",
                "raw_url": "https://github.com/apache/hive/raw/daca6a6dedafd862bcf271a8cf117100378d3f57/ql/src/test/queries/clientpositive/vector_non_string_partition.q",
                "sha": "fc1dc6d3b89f31b48131c25df4a04eccd8829ffa",
                "status": "added"
            },
            {
                "additions": 180,
                "blob_url": "https://github.com/apache/hive/blob/daca6a6dedafd862bcf271a8cf117100378d3f57/ql/src/test/results/clientpositive/vector_non_string_partition.q.out",
                "changes": 180,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/vector_non_string_partition.q.out?ref=daca6a6dedafd862bcf271a8cf117100378d3f57",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/vector_non_string_partition.q.out",
                "patch": "@@ -0,0 +1,180 @@\n+PREHOOK: query: CREATE TABLE non_string_part(cint INT, cstring1 STRING, cdouble DOUBLE, ctimestamp1 TIMESTAMP) PARTITIONED BY (ctinyint tinyint) STORED AS ORC\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+POSTHOOK: query: CREATE TABLE non_string_part(cint INT, cstring1 STRING, cdouble DOUBLE, ctimestamp1 TIMESTAMP) PARTITIONED BY (ctinyint tinyint) STORED AS ORC\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@non_string_part\n+PREHOOK: query: INSERT OVERWRITE TABLE non_string_part PARTITION(ctinyint) SELECT cint, cstring1, cdouble, ctimestamp1, ctinyint fROM alltypesorc \n+WHERE ctinyint IS NULL AND cdouble IS NOT NULL ORDER BY cdouble\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@alltypesorc\n+PREHOOK: Output: default@non_string_part\n+POSTHOOK: query: INSERT OVERWRITE TABLE non_string_part PARTITION(ctinyint) SELECT cint, cstring1, cdouble, ctimestamp1, ctinyint fROM alltypesorc \n+WHERE ctinyint IS NULL AND cdouble IS NOT NULL ORDER BY cdouble\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@alltypesorc\n+POSTHOOK: Output: default@non_string_part@ctinyint=__HIVE_DEFAULT_PARTITION__\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cstring1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).ctimestamp1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp1, type:timestamp, comment:null), ]\n+PREHOOK: query: SHOW PARTITIONS non_string_part\n+PREHOOK: type: SHOWPARTITIONS\n+PREHOOK: Input: default@non_string_part\n+POSTHOOK: query: SHOW PARTITIONS non_string_part\n+POSTHOOK: type: SHOWPARTITIONS\n+POSTHOOK: Input: default@non_string_part\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cstring1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).ctimestamp1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp1, type:timestamp, comment:null), ]\n+ctinyint=__HIVE_DEFAULT_PARTITION__\n+PREHOOK: query: EXPLAIN SELECT cint, ctinyint FROM non_string_part WHERE cint > 0 ORDER BY cint LIMIT 10\n+PREHOOK: type: QUERY\n+POSTHOOK: query: EXPLAIN SELECT cint, ctinyint FROM non_string_part WHERE cint > 0 ORDER BY cint LIMIT 10\n+POSTHOOK: type: QUERY\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cstring1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).ctimestamp1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp1, type:timestamp, comment:null), ]\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: non_string_part\n+            Statistics: Num rows: 3073 Data size: 339150 Basic stats: COMPLETE Column stats: NONE\n+            Filter Operator\n+              predicate: (cint > 0) (type: boolean)\n+              Statistics: Num rows: 1024 Data size: 113013 Basic stats: COMPLETE Column stats: NONE\n+              Select Operator\n+                expressions: cint (type: int), ctinyint (type: tinyint)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 1024 Data size: 113013 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: int)\n+                  sort order: +\n+                  Statistics: Num rows: 1024 Data size: 113013 Basic stats: COMPLETE Column stats: NONE\n+                  value expressions: _col0 (type: int), _col1 (type: tinyint)\n+      Execution mode: vectorized\n+      Reduce Operator Tree:\n+        Extract\n+          Statistics: Num rows: 1024 Data size: 113013 Basic stats: COMPLETE Column stats: NONE\n+          Limit\n+            Number of rows: 10\n+            Statistics: Num rows: 10 Data size: 1100 Basic stats: COMPLETE Column stats: NONE\n+            File Output Operator\n+              compressed: false\n+              Statistics: Num rows: 10 Data size: 1100 Basic stats: COMPLETE Column stats: NONE\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: 10\n+\n+PREHOOK: query: SELECT cint, ctinyint FROM non_string_part WHERE cint > 0 ORDER BY cint LIMIT 10\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@non_string_part\n+PREHOOK: Input: default@non_string_part@ctinyint=__HIVE_DEFAULT_PARTITION__\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT cint, ctinyint FROM non_string_part WHERE cint > 0 ORDER BY cint LIMIT 10\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@non_string_part\n+POSTHOOK: Input: default@non_string_part@ctinyint=__HIVE_DEFAULT_PARTITION__\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cstring1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).ctimestamp1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp1, type:timestamp, comment:null), ]\n+762\tNULL\n+762\tNULL\n+6981\tNULL\n+6981\tNULL\n+6981\tNULL\n+86028\tNULL\n+504142\tNULL\n+799471\tNULL\n+1248059\tNULL\n+1286921\tNULL\n+PREHOOK: query: EXPLAIN SELECT cint, cstring1 FROM non_string_part WHERE cint > 0 ORDER BY cint, cstring1 LIMIT 10\n+PREHOOK: type: QUERY\n+POSTHOOK: query: EXPLAIN SELECT cint, cstring1 FROM non_string_part WHERE cint > 0 ORDER BY cint, cstring1 LIMIT 10\n+POSTHOOK: type: QUERY\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cstring1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).ctimestamp1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp1, type:timestamp, comment:null), ]\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 is a root stage\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Map Reduce\n+      Map Operator Tree:\n+          TableScan\n+            alias: non_string_part\n+            Statistics: Num rows: 3073 Data size: 339150 Basic stats: COMPLETE Column stats: NONE\n+            Filter Operator\n+              predicate: (cint > 0) (type: boolean)\n+              Statistics: Num rows: 1024 Data size: 113013 Basic stats: COMPLETE Column stats: NONE\n+              Select Operator\n+                expressions: cint (type: int), cstring1 (type: string)\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 1024 Data size: 113013 Basic stats: COMPLETE Column stats: NONE\n+                Reduce Output Operator\n+                  key expressions: _col0 (type: int), _col1 (type: string)\n+                  sort order: ++\n+                  Statistics: Num rows: 1024 Data size: 113013 Basic stats: COMPLETE Column stats: NONE\n+                  value expressions: _col0 (type: int), _col1 (type: string)\n+      Execution mode: vectorized\n+      Reduce Operator Tree:\n+        Extract\n+          Statistics: Num rows: 1024 Data size: 113013 Basic stats: COMPLETE Column stats: NONE\n+          Limit\n+            Number of rows: 10\n+            Statistics: Num rows: 10 Data size: 1100 Basic stats: COMPLETE Column stats: NONE\n+            File Output Operator\n+              compressed: false\n+              Statistics: Num rows: 10 Data size: 1100 Basic stats: COMPLETE Column stats: NONE\n+              table:\n+                  input format: org.apache.hadoop.mapred.TextInputFormat\n+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\n+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: 10\n+\n+PREHOOK: query: SELECT cint, cstring1 FROM non_string_part WHERE cint > 0 ORDER BY cint, cstring1 LIMIT 10\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@non_string_part\n+PREHOOK: Input: default@non_string_part@ctinyint=__HIVE_DEFAULT_PARTITION__\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT cint, cstring1 FROM non_string_part WHERE cint > 0 ORDER BY cint, cstring1 LIMIT 10\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@non_string_part\n+POSTHOOK: Input: default@non_string_part@ctinyint=__HIVE_DEFAULT_PARTITION__\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cdouble SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cdouble, type:double, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cint SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).cstring1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+POSTHOOK: Lineage: non_string_part PARTITION(ctinyint=__HIVE_DEFAULT_PARTITION__).ctimestamp1 SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:ctimestamp1, type:timestamp, comment:null), ]\n+762\t3WsVeqb28VWEEOLI8ail\n+762\t40ks5556SV\n+6981\t1FNNhmiFLGw425NA13g\n+6981\to5mb0QP5Y48Qd4vdB0\n+6981\tsF2CRfgt2K\n+86028\tT2o8XRFAL0HC4ikDQnfoCymw\n+504142\tPlOxor04p5cvVl\n+799471\t2fu24\n+1248059\tUhps6mMh3IfHB3j7yH62K\n+1286921\tODLrXI8882q8LS8",
                "raw_url": "https://github.com/apache/hive/raw/daca6a6dedafd862bcf271a8cf117100378d3f57/ql/src/test/results/clientpositive/vector_non_string_partition.q.out",
                "sha": "7cd26258255a7e3dfa934f74244da4307d4fed7a",
                "status": "added"
            }
        ],
        "message": "HIVE-6841: Vectorized execution throws NPE for partitioning columns with __HIVE_DEFAULT_PARTITION__ (reviewd by Hari, Ashutosh)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1585548 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/fffdf11f26b1a5306af4770c581d0267cdbd7334",
        "patched_files": [
            "VectorizedRowBatchCtx.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestVectorizedRowBatchCtx.java"
        ]
    },
    "hive_e1ef225": {
        "bug_id": "hive_e1ef225",
        "commit": "https://github.com/apache/hive/commit/e1ef225bafd066b63dbe48a3496fa5b3fa39a6dc",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/e1ef225bafd066b63dbe48a3496fa5b3fa39a6dc/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java?ref=e1ef225bafd066b63dbe48a3496fa5b3fa39a6dc",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java",
                "patch": "@@ -207,11 +207,17 @@ public StructField getStructFieldRef(String s) {\n \n     @Override\n     public Object getStructFieldData(Object object, StructField field) {\n+      if (object == null) {\n+        return null;\n+      }\n       return ((OrcStruct) object).fields[((Field) field).offset];\n     }\n \n     @Override\n     public List<Object> getStructFieldsDataAsList(Object object) {\n+      if (object == null) {\n+        return null;\n+      }\n       OrcStruct struct = (OrcStruct) object;\n       List<Object> result = new ArrayList<Object>(struct.fields.length);\n       for (Object child: struct.fields) {",
                "raw_url": "https://github.com/apache/hive/raw/e1ef225bafd066b63dbe48a3496fa5b3fa39a6dc/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcStruct.java",
                "sha": "685b386606cec0e85eb3bf5fa0e7cf0e4fd6f88f",
                "status": "modified"
            }
        ],
        "message": "HIVE-6716: ORC struct throws NPE for tables with inner structs having null values (Prasanth J via Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1581007 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/a39be7254a38c010154e7ebd65e8282db804ba80",
        "patched_files": [
            "OrcStruct.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestOrcStruct.java"
        ]
    },
    "hive_e8221ab": {
        "bug_id": "hive_e8221ab",
        "commit": "https://github.com/apache/hive/commit/e8221ab0e147580441e89a516c2570a3acaa8e17",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/e8221ab0e147580441e89a516c2570a3acaa8e17/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java?ref=e8221ab0e147580441e89a516c2570a3acaa8e17",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java",
                "patch": "@@ -573,7 +573,7 @@ static ExpressionTree pushDownNot(ExpressionTree root) {\n      * @param expr The expression to clean up\n      * @return The cleaned up expression\n      */\n-    ExpressionTree foldMaybe(ExpressionTree expr) {\n+    static ExpressionTree foldMaybe(ExpressionTree expr) {\n       if (expr.children != null) {\n         for(int i=0; i < expr.children.size(); ++i) {\n           ExpressionTree child = foldMaybe(expr.children.get(i));\n@@ -594,6 +594,9 @@ ExpressionTree foldMaybe(ExpressionTree expr) {\n             expr.children.set(i, child);\n           }\n         }\n+        if (expr.children.isEmpty()) {\n+          return new ExpressionTree(TruthValue.YES_NO_NULL);\n+        }\n       }\n       return expr;\n     }",
                "raw_url": "https://github.com/apache/hive/raw/e8221ab0e147580441e89a516c2570a3acaa8e17/ql/src/java/org/apache/hadoop/hive/ql/io/sarg/SearchArgumentImpl.java",
                "sha": "1663d785d412d4a92252fef29ae68fc30b22f6cc",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hive/blob/e8221ab0e147580441e89a516c2570a3acaa8e17/ql/src/test/org/apache/hadoop/hive/ql/io/sarg/TestSearchArgumentImpl.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/io/sarg/TestSearchArgumentImpl.java?ref=e8221ab0e147580441e89a516c2570a3acaa8e17",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/io/sarg/TestSearchArgumentImpl.java",
                "patch": "@@ -146,6 +146,31 @@ public void testFlatten() throws Exception {\n         ).toString());\n   }\n \n+  @Test\n+  public void testFoldMaybe() throws Exception {\n+    assertEquals(\"(and leaf-1)\",\n+        ExpressionBuilder.foldMaybe(and(leaf(1),\n+            constant(TruthValue.YES_NO_NULL))).toString());\n+    assertEquals(\"(and leaf-1 leaf-2)\",\n+        ExpressionBuilder.foldMaybe(and(leaf(1),\n+            constant(TruthValue.YES_NO_NULL), leaf(2))).toString());\n+    assertEquals(\"(and leaf-1 leaf-2)\",\n+        ExpressionBuilder.foldMaybe(and(constant(TruthValue.YES_NO_NULL),\n+            leaf(1), leaf(2), constant(TruthValue.YES_NO_NULL))).toString());\n+    assertEquals(\"YES_NO_NULL\",\n+        ExpressionBuilder.foldMaybe(and(constant(TruthValue.YES_NO_NULL),\n+            constant(TruthValue.YES_NO_NULL))).toString());\n+    assertEquals(\"YES_NO_NULL\",\n+        ExpressionBuilder.foldMaybe(or(leaf(1),\n+            constant(TruthValue.YES_NO_NULL))).toString());\n+    assertEquals(\"(or leaf-1 (and leaf-2))\",\n+        ExpressionBuilder.foldMaybe(or(leaf(1),\n+            and(leaf(2), constant(TruthValue.YES_NO_NULL)))).toString());\n+    assertEquals(\"(and leaf-1)\",\n+        ExpressionBuilder.foldMaybe(and(or(leaf(2),\n+            constant(TruthValue.YES_NO_NULL)), leaf(1))).toString());\n+  }\n+\n   @Test\n   public void testCNF() throws Exception {\n     assertEquals(\"leaf-1\", ExpressionBuilder.convertToCNF(leaf(1)).toString());",
                "raw_url": "https://github.com/apache/hive/raw/e8221ab0e147580441e89a516c2570a3acaa8e17/ql/src/test/org/apache/hadoop/hive/ql/io/sarg/TestSearchArgumentImpl.java",
                "sha": "6a2a56a831cd682b00dc63a904b79643867ba273",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hive/blob/e8221ab0e147580441e89a516c2570a3acaa8e17/ql/src/test/queries/clientpositive/orc_create.q",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/orc_create.q?ref=e8221ab0e147580441e89a516c2570a3acaa8e17",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/orc_create.q",
                "patch": "@@ -92,6 +92,15 @@ SET hive.optimize.index.filter=true;\n -- test predicate push down with partition pruning\n SELECT COUNT(*) FROM orc_create_people where id < 10 and state = 'Ca';\n \n+-- test predicate push down\n+SELECT COUNT(*) FROM orc_create_people where id = 50;\n+SELECT COUNT(*) FROM orc_create_people where id between 10 and 20;\n+SELECT COUNT(*) FROM orc_create_people where id > 10 and id < 100;\n+SELECT COUNT(*) FROM orc_create_people where (id + 1) = 20;\n+SELECT COUNT(*) FROM orc_create_people where (id + 10) < 200;\n+SELECT COUNT(*) FROM orc_create_people where id < 30  or first_name = \"Rafael\";\n+SELECT COUNT(*) FROM orc_create_people where length(substr(first_name, 1, 2)) <= 2 and last_name like '%';\n+\n -- test predicate push down with no column projection\n SELECT id, first_name, last_name, address\n   FROM orc_create_people WHERE id > 90;",
                "raw_url": "https://github.com/apache/hive/raw/e8221ab0e147580441e89a516c2570a3acaa8e17/ql/src/test/queries/clientpositive/orc_create.q",
                "sha": "823727d5edc604f79714c968b4528407acafad84",
                "status": "modified"
            },
            {
                "additions": 177,
                "blob_url": "https://github.com/apache/hive/blob/e8221ab0e147580441e89a516c2570a3acaa8e17/ql/src/test/results/clientpositive/orc_create.q.out",
                "changes": 177,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/orc_create.q.out?ref=e8221ab0e147580441e89a516c2570a3acaa8e17",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/orc_create.q.out",
                "patch": "@@ -506,6 +506,183 @@ POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc\n POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n 5\n+PREHOOK: query: -- test predicate push down\n+SELECT COUNT(*) FROM orc_create_people where id = 50\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Input: default@orc_create_people@state=Ca\n+PREHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: query: -- test predicate push down\n+SELECT COUNT(*) FROM orc_create_people where id = 50\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Input: default@orc_create_people@state=Ca\n+POSTHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+1\n+PREHOOK: query: SELECT COUNT(*) FROM orc_create_people where id between 10 and 20\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Input: default@orc_create_people@state=Ca\n+PREHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT COUNT(*) FROM orc_create_people where id between 10 and 20\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Input: default@orc_create_people@state=Ca\n+POSTHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+11\n+PREHOOK: query: SELECT COUNT(*) FROM orc_create_people where id > 10 and id < 100\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Input: default@orc_create_people@state=Ca\n+PREHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT COUNT(*) FROM orc_create_people where id > 10 and id < 100\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Input: default@orc_create_people@state=Ca\n+POSTHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+89\n+PREHOOK: query: SELECT COUNT(*) FROM orc_create_people where (id + 1) = 20\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Input: default@orc_create_people@state=Ca\n+PREHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT COUNT(*) FROM orc_create_people where (id + 1) = 20\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Input: default@orc_create_people@state=Ca\n+POSTHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+1\n+PREHOOK: query: SELECT COUNT(*) FROM orc_create_people where (id + 10) < 200\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Input: default@orc_create_people@state=Ca\n+PREHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT COUNT(*) FROM orc_create_people where (id + 10) < 200\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Input: default@orc_create_people@state=Ca\n+POSTHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+100\n+PREHOOK: query: SELECT COUNT(*) FROM orc_create_people where id < 30  or first_name = \"Rafael\"\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Input: default@orc_create_people@state=Ca\n+PREHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT COUNT(*) FROM orc_create_people where id < 30  or first_name = \"Rafael\"\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Input: default@orc_create_people@state=Ca\n+POSTHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+30\n+PREHOOK: query: SELECT COUNT(*) FROM orc_create_people where length(substr(first_name, 1, 2)) <= 2 and last_name like '%'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@orc_create_people\n+PREHOOK: Input: default@orc_create_people@state=Ca\n+PREHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: query: SELECT COUNT(*) FROM orc_create_people where length(substr(first_name, 1, 2)) <= 2 and last_name like '%'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@orc_create_people\n+POSTHOOK: Input: default@orc_create_people@state=Ca\n+POSTHOOK: Input: default@orc_create_people@state=Or\n+#### A masked pattern was here ####\n+POSTHOOK: Lineage: orc_create_complex.lst SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:lst, type:array<string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.mp SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:mp, type:map<string,string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.str SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:str, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_complex.strct SIMPLE [(orc_create_staging)orc_create_staging.FieldSchema(name:strct, type:struct<A:string,B:string>, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Ca).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).address SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:address, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).first_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:first_name, type:string, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).id SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:id, type:int, comment:null), ]\n+POSTHOOK: Lineage: orc_create_people PARTITION(state=Or).last_name SIMPLE [(orc_create_people_staging)orc_create_people_staging.FieldSchema(name:last_name, type:string, comment:null), ]\n+100\n PREHOOK: query: -- test predicate push down with no column projection\n SELECT id, first_name, last_name, address\n   FROM orc_create_people WHERE id > 90",
                "raw_url": "https://github.com/apache/hive/raw/e8221ab0e147580441e89a516c2570a3acaa8e17/ql/src/test/results/clientpositive/orc_create.q.out",
                "sha": "259520ed458ef8d589ba7fab2a5f4753744fc2d1",
                "status": "modified"
            }
        ],
        "message": "HIVE-5580. Predicate pushdown predicates with an and-operator between \nnon-SARGable predicates cause a NPE. (omalley)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1550010 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/a93fe77129d10cbd4ec37ae320b8ae32e9305f2f",
        "patched_files": [
            "orc_create.java",
            "SearchArgumentImpl.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestSearchArgumentImpl.java"
        ]
    },
    "hive_e91f69e": {
        "bug_id": "hive_e91f69e",
        "commit": "https://github.com/apache/hive/commit/e91f69e213f6ee78ad8299cda81079104f7141bb",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/e91f69e213f6ee78ad8299cda81079104f7141bb/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java?ref=e91f69e213f6ee78ad8299cda81079104f7141bb",
                "deletions": 5,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "patch": "@@ -387,7 +387,10 @@ private static BaseWork getBaseWork(Configuration conf, String name) {\n \n       path = getPlanPath(conf, name);\n       LOG.info(\"PLAN PATH = \" + path);\n-      assert path != null;\n+      if (path == null) { // Map/reduce plan may not be generated\n+        return null;\n+      }\n+\n       BaseWork gWork = gWorkMap.get(conf).get(path);\n       if (gWork == null) {\n         Path localPath = path;\n@@ -443,12 +446,11 @@ private static BaseWork getBaseWork(Configuration conf, String name) {\n       return gWork;\n     } catch (FileNotFoundException fnf) {\n       // happens. e.g.: no reduce work.\n-      LOG.debug(\"File not found: \" + fnf.getMessage());\n-      LOG.info(\"No plan file found: \"+path);\n+      LOG.debug(\"No plan file found: \" + path, fnf);\n       return null;\n     } catch (Exception e) {\n-      String msg = \"Failed to load plan: \" + path + \": \" + e;\n-      LOG.error(msg, e);\n+      String msg = \"Failed to load plan: \" + path;\n+      LOG.error(\"Failed to load plan: \" + path, e);\n       throw new RuntimeException(msg, e);\n     } finally {\n       SerializationUtilities.releaseKryo(kryo);",
                "raw_url": "https://github.com/apache/hive/raw/e91f69e213f6ee78ad8299cda81079104f7141bb/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java",
                "sha": "528d663e701eaa13e2cd7da328973637457ff642",
                "status": "modified"
            }
        ],
        "message": "HIVE-13855: select INPUT__FILE__NAME throws NPE exception (Aihua Xu, reviewed by Yongzhi Chen)",
        "parent": "https://github.com/apache/hive/commit/e459a67283900393a79e4f69853103cc4fd8a726",
        "patched_files": [
            "Utilities.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestUtilities.java"
        ]
    },
    "hive_e9860bb": {
        "bug_id": "hive_e9860bb",
        "commit": "https://github.com/apache/hive/commit/e9860bb80b7a2adeee7732b0dba60cb4d789e249",
        "file": [
            {
                "additions": 65,
                "blob_url": "https://github.com/apache/hive/blob/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/registry/impl/LlapYarnRegistryImpl.java",
                "changes": 110,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/registry/impl/LlapYarnRegistryImpl.java?ref=e9860bb80b7a2adeee7732b0dba60cb4d789e249",
                "deletions": 45,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/daemon/registry/impl/LlapYarnRegistryImpl.java",
                "patch": "@@ -19,15 +19,16 @@\n import java.net.UnknownHostException;\n import java.util.HashMap;\n import java.util.HashSet;\n+import java.util.LinkedHashMap;\n import java.util.Map;\n import java.util.Set;\n import java.util.UUID;\n import java.util.concurrent.Executors;\n import java.util.concurrent.ScheduledExecutorService;\n import java.util.concurrent.TimeUnit;\n \n+import com.google.common.util.concurrent.ThreadFactoryBuilder;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.PathNotFoundException;\n import org.apache.hadoop.hive.llap.configuration.LlapConfiguration;\n import org.apache.hadoop.hive.llap.daemon.registry.ServiceInstance;\n import org.apache.hadoop.hive.llap.daemon.registry.ServiceInstanceSet;\n@@ -37,7 +38,6 @@\n import org.apache.hadoop.registry.client.binding.RegistryTypeUtils;\n import org.apache.hadoop.registry.client.binding.RegistryUtils;\n import org.apache.hadoop.registry.client.binding.RegistryUtils.ServiceRecordMarshal;\n-import org.apache.hadoop.registry.client.exceptions.InvalidRecordException;\n import org.apache.hadoop.registry.client.impl.zk.RegistryOperationsService;\n import org.apache.hadoop.registry.client.types.AddressTypes;\n import org.apache.hadoop.registry.client.types.Endpoint;\n@@ -55,9 +55,10 @@\n   private static final Logger LOG = Logger.getLogger(LlapYarnRegistryImpl.class);\n \n   private RegistryOperationsService client;\n-  private String instanceName;\n-  private Configuration conf;\n-  private ServiceRecordMarshal encoder;\n+  private final String instanceName;\n+  private final Configuration conf;\n+  private final ServiceRecordMarshal encoder;\n+  private final String path;\n \n   private final DynamicServiceInstanceSet instances = new DynamicServiceInstanceSet();\n \n@@ -68,7 +69,8 @@\n \n   private final static String SERVICE_CLASS = \"org-apache-hive\";\n \n-  final ScheduledExecutorService refresher = Executors.newScheduledThreadPool(1);\n+  final ScheduledExecutorService refresher = Executors.newScheduledThreadPool(1,\n+      new ThreadFactoryBuilder().setDaemon(true).setNameFormat(\"LlapYarnRegistryRefresher\").build());\n   final long refreshDelay;\n \n   static {\n@@ -90,6 +92,8 @@ public LlapYarnRegistryImpl(String instanceName, Configuration conf) {\n     // registry reference\n     client = (RegistryOperationsService) RegistryOperationsFactory.createInstance(conf);\n     encoder = new RegistryUtils.ServiceRecordMarshal();\n+    this.path = RegistryPathUtils.join(RegistryUtils.componentPath(RegistryUtils.currentUser(),\n+        SERVICE_CLASS, instanceName, \"workers\"), \"worker-\");\n     refreshDelay =\n         conf.getInt(LlapConfiguration.LLAP_DAEMON_SERVICE_REFRESH_INTERVAL,\n             LlapConfiguration.LLAP_DAEMON_SERVICE_REFRESH_INTERVAL_DEFAULT);\n@@ -114,8 +118,7 @@ public Endpoint getShuffleEndpoint() {\n   }\n \n   private final String getPath() {\n-    return RegistryPathUtils.join(RegistryUtils.componentPath(RegistryUtils.currentUser(),\n-        SERVICE_CLASS, instanceName, \"workers\"), \"worker-\");\n+    return this.path;\n   }\n \n   @Override\n@@ -199,7 +202,8 @@ public boolean isAlive() {\n     }\n \n     public void kill() {\n-      LOG.info(\"Killing \" + this);\n+      // May be possible to generate a notification back to the scheduler from here.\n+      LOG.info(\"Killing service instance: \" + this);\n       this.alive = false;\n     }\n \n@@ -217,74 +221,90 @@ public Resource getResource() {\n \n     @Override\n     public String toString() {\n-      return \"DynamicServiceInstance [alive=\" + alive + \", host=\" + host + \":\" + rpcPort + \"]\";\n+      return \"DynamicServiceInstance [alive=\" + alive + \", host=\" + host + \":\" + rpcPort + \" with resources=\" + getResource() +\"]\";\n     }\n+\n+    // Relying on the identity hashCode and equality, since refreshing instances retains the old copy\n+    // of an already known instance.\n   }\n \n   private class DynamicServiceInstanceSet implements ServiceInstanceSet {\n \n-    Map<String, ServiceInstance> instances;\n+    // LinkedHashMap to retain iteration order.\n+    private final Map<String, ServiceInstance> instances = new LinkedHashMap<>();\n \n     @Override\n-    public Map<String, ServiceInstance> getAll() {\n-      return instances;\n+    public synchronized Map<String, ServiceInstance> getAll() {\n+      // Return a copy. Instances may be modified during a refresh.\n+      return new LinkedHashMap<>(instances);\n     }\n \n     @Override\n-    public ServiceInstance getInstance(String name) {\n+    public synchronized ServiceInstance getInstance(String name) {\n       return instances.get(name);\n     }\n \n     @Override\n-    public synchronized void refresh() throws IOException {\n+    public  void refresh() throws IOException {\n       /* call this from wherever */\n       Map<String, ServiceInstance> freshInstances = new HashMap<String, ServiceInstance>();\n \n       String path = getPath();\n       Map<String, ServiceRecord> records =\n           RegistryUtils.listServiceRecords(client, RegistryPathUtils.parentOf(path));\n-      Set<String> latestKeys = new HashSet<String>();\n-      LOG.info(\"Starting to refresh ServiceInstanceSet \" + System.identityHashCode(this));\n-      for (ServiceRecord rec : records.values()) {\n-        ServiceInstance instance = new DynamicServiceInstance(rec);\n-        if (instance != null) {\n-          if (instances != null && instances.containsKey(instance.getWorkerIdentity()) == false) {\n-            // add a new object\n-            freshInstances.put(instance.getWorkerIdentity(), instance);\n-            if (LOG.isInfoEnabled()) {\n-              LOG.info(\"Adding new worker \" + instance.getWorkerIdentity() + \" which mapped to \"\n-                  + instance);\n+      // Synchronize after reading the service records from the external service (ZK)\n+      synchronized (this) {\n+        Set<String> latestKeys = new HashSet<String>();\n+        LOG.info(\"Starting to refresh ServiceInstanceSet \" + System.identityHashCode(this));\n+        for (ServiceRecord rec : records.values()) {\n+          ServiceInstance instance = new DynamicServiceInstance(rec);\n+          if (instance != null) {\n+            if (instances != null && instances.containsKey(instance.getWorkerIdentity()) == false) {\n+              // add a new object\n+              freshInstances.put(instance.getWorkerIdentity(), instance);\n+              if (LOG.isInfoEnabled()) {\n+                LOG.info(\"Adding new worker \" + instance.getWorkerIdentity() + \" which mapped to \"\n+                    + instance);\n+              }\n+            } else {\n+              if (LOG.isDebugEnabled()) {\n+                LOG.debug(\"Retaining running worker \" + instance.getWorkerIdentity() +\n+                    \" which mapped to \" + instance);\n+              }\n             }\n-          } else if (LOG.isDebugEnabled()) {\n-            LOG.debug(\"Retaining running worker \" + instance.getWorkerIdentity() + \" which mapped to \" + instance);\n           }\n+          latestKeys.add(instance.getWorkerIdentity());\n         }\n-        latestKeys.add(instance.getWorkerIdentity());\n-      }\n \n-      if (instances != null) {\n-        // deep-copy before modifying\n-        Set<String> oldKeys = new HashSet(instances.keySet());\n-        if (oldKeys.removeAll(latestKeys)) {\n-          for (String k : oldKeys) {\n-            // this is so that people can hold onto ServiceInstance references as placeholders for tasks\n-            final DynamicServiceInstance dead = (DynamicServiceInstance) instances.get(k);\n-            dead.kill();\n-            if (LOG.isInfoEnabled()) {\n-              LOG.info(\"Deleting dead worker \" + k + \" which mapped to \" + dead);\n+        if (instances != null) {\n+          // deep-copy before modifying\n+          Set<String> oldKeys = new HashSet(instances.keySet());\n+          if (oldKeys.removeAll(latestKeys)) {\n+            // This is all the records which have not checked in, and are effectively dead.\n+            for (String k : oldKeys) {\n+              // this is so that people can hold onto ServiceInstance references as placeholders for tasks\n+              final DynamicServiceInstance dead = (DynamicServiceInstance) instances.get(k);\n+              dead.kill();\n+              if (LOG.isInfoEnabled()) {\n+                LOG.info(\"Deleting dead worker \" + k + \" which mapped to \" + dead);\n+              }\n             }\n           }\n+          // oldKeys contains the set of dead instances at this point.\n+          this.instances.keySet().removeAll(oldKeys);\n+          this.instances.putAll(freshInstances);\n+        } else {\n+          this.instances.putAll(freshInstances);\n         }\n-        this.instances.keySet().removeAll(oldKeys);\n-        this.instances.putAll(freshInstances);\n-      } else {\n-        this.instances = freshInstances;\n       }\n     }\n \n     @Override\n-    public Set<ServiceInstance> getByHost(String host) {\n+    public synchronized Set<ServiceInstance> getByHost(String host) {\n+      // TODO Maybe store this as a map which is populated during construction, to avoid walking\n+      // the map on each request.\n       Set<ServiceInstance> byHost = new HashSet<ServiceInstance>();\n+\n       for (ServiceInstance i : instances.values()) {\n         if (host.equals(i.getHost())) {\n           // all hosts in instances should be alive in this impl",
                "raw_url": "https://github.com/apache/hive/raw/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/java/org/apache/hadoop/hive/llap/daemon/registry/impl/LlapYarnRegistryImpl.java",
                "sha": "a9033694ae1457775d75d3a4fc438a0b69115cb4",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java?ref=e9860bb80b7a2adeee7732b0dba60cb4d789e249",
                "deletions": 1,
                "filename": "llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java",
                "patch": "@@ -218,7 +218,6 @@ public void setResponse(SubmitWorkResponseProto response) {\n \n           @Override\n           public void indicateError(Throwable t) {\n-            LOG.info(\"Failed to run task: \" + taskSpec.getTaskAttemptID() + \" on containerId: \" + containerId, t);\n             if (t instanceof ServiceException) {\n               ServiceException se = (ServiceException) t;\n               t = se.getCause();\n@@ -228,21 +227,29 @@ public void indicateError(Throwable t) {\n               String message = re.toString();\n               // RejectedExecutions from the remote service treated as KILLED\n               if (message.contains(RejectedExecutionException.class.getName())) {\n+                LOG.info(\n+                    \"Unable to run task: \" + taskSpec.getTaskAttemptID() + \" on containerId: \" +\n+                        containerId + \", Service Busy\");\n                 getTaskCommunicatorContext().taskKilled(taskSpec.getTaskAttemptID(),\n                     TaskAttemptEndReason.SERVICE_BUSY, \"Service Busy\");\n               } else {\n                 // All others from the remote service cause the task to FAIL.\n+                LOG.info(\"Failed to run task: \" + taskSpec.getTaskAttemptID() + \" on containerId: \" + containerId, t);\n                 getTaskCommunicatorContext()\n                     .taskFailed(taskSpec.getTaskAttemptID(), TaskAttemptEndReason.OTHER,\n                         t.toString());\n               }\n             } else {\n               // Exception from the RPC layer - communication failure, consider as KILLED / service down.\n               if (t instanceof IOException) {\n+                LOG.info(\n+                    \"Unable to run task: \" + taskSpec.getTaskAttemptID() + \" on containerId: \" +\n+                        containerId + \", Communication Error\");\n                 getTaskCommunicatorContext().taskKilled(taskSpec.getTaskAttemptID(),\n                     TaskAttemptEndReason.COMMUNICATION_ERROR, \"Communication Error\");\n               } else {\n                 // Anything else is a FAIL.\n+                LOG.info(\"Failed to run task: \" + taskSpec.getTaskAttemptID() + \" on containerId: \" + containerId, t);\n                 getTaskCommunicatorContext()\n                     .taskFailed(taskSpec.getTaskAttemptID(), TaskAttemptEndReason.OTHER,\n                         t.getMessage());",
                "raw_url": "https://github.com/apache/hive/raw/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/java/org/apache/hadoop/hive/llap/tezplugins/LlapTaskCommunicator.java",
                "sha": "d35b04ae743c34601dce154b9cd766cdbf9a1f32",
                "status": "modified"
            },
            {
                "additions": 54,
                "blob_url": "https://github.com/apache/hive/blob/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/java/org/apache/tez/dag/app/rm/ContainerFactory.java",
                "changes": 54,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/tez/dag/app/rm/ContainerFactory.java?ref=e9860bb80b7a2adeee7732b0dba60cb4d789e249",
                "deletions": 0,
                "filename": "llap-server/src/java/org/apache/tez/dag/app/rm/ContainerFactory.java",
                "patch": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ *  you may not use this file except in compliance with the License.\n+ *  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing, software\n+ *  distributed under the License is distributed on an \"AS IS\" BASIS,\n+ *  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ *  See the License for the specific language governing permissions and\n+ *  limitations under the License.\n+ */\n+\n+package org.apache.tez.dag.app.rm;\n+\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.api.records.Container;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.NodeId;\n+import org.apache.hadoop.yarn.api.records.Priority;\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.tez.dag.app.AppContext;\n+\n+class ContainerFactory {\n+  final ApplicationAttemptId customAppAttemptId;\n+  AtomicLong nextId;\n+\n+  public ContainerFactory(AppContext appContext, long appIdLong) {\n+    this.nextId = new AtomicLong(1);\n+    ApplicationId appId =\n+        ApplicationId.newInstance(appIdLong, appContext.getApplicationAttemptId()\n+            .getApplicationId().getId());\n+    this.customAppAttemptId =\n+        ApplicationAttemptId.newInstance(appId, appContext.getApplicationAttemptId()\n+            .getAttemptId());\n+  }\n+\n+  public Container createContainer(Resource capability, Priority priority, String hostname,\n+      int port) {\n+    ContainerId containerId =\n+        ContainerId.newContainerId(customAppAttemptId, nextId.getAndIncrement());\n+    NodeId nodeId = NodeId.newInstance(hostname, port);\n+    String nodeHttpAddress = \"hostname:0\"; // TODO: include UI ports\n+\n+    Container container =\n+        Container.newInstance(containerId, nodeId, nodeHttpAddress, capability, priority, null);\n+\n+    return container;\n+  }\n+}",
                "raw_url": "https://github.com/apache/hive/raw/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/java/org/apache/tez/dag/app/rm/ContainerFactory.java",
                "sha": "1ab3d152f2c07e044a3e7c53f3fe144b6e9527ba",
                "status": "added"
            },
            {
                "additions": 69,
                "blob_url": "https://github.com/apache/hive/blob/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/java/org/apache/tez/dag/app/rm/LlapTaskSchedulerService.java",
                "changes": 135,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/java/org/apache/tez/dag/app/rm/LlapTaskSchedulerService.java?ref=e9860bb80b7a2adeee7732b0dba60cb4d789e249",
                "deletions": 66,
                "filename": "llap-server/src/java/org/apache/tez/dag/app/rm/LlapTaskSchedulerService.java",
                "patch": "@@ -21,6 +21,7 @@\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.Iterator;\n+import java.util.LinkedHashMap;\n import java.util.LinkedList;\n import java.util.List;\n import java.util.Map;\n@@ -38,7 +39,6 @@\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.concurrent.atomic.AtomicInteger;\n-import java.util.concurrent.atomic.AtomicLong;\n import java.util.concurrent.locks.ReentrantReadWriteLock;\n \n import org.apache.commons.logging.Log;\n@@ -48,8 +48,6 @@\n import org.apache.hadoop.hive.llap.daemon.registry.ServiceInstance;\n import org.apache.hadoop.hive.llap.daemon.registry.ServiceInstanceSet;\n import org.apache.hadoop.hive.llap.daemon.registry.impl.LlapRegistryService;\n-import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n-import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.NodeId;\n@@ -78,15 +76,11 @@\n   // interface into the registry service\n   private ServiceInstanceSet activeInstances;\n \n+  // Tracks all instances, including ones which have been disabled in the past.\n+  // LinkedHashMap to provide the same iteration order when selecting a random host.\n   @VisibleForTesting\n-  final Map<ServiceInstance, NodeInfo> instanceToNodeMap = new HashMap<>();\n-  \n-  @VisibleForTesting\n-  final Set<ServiceInstance> instanceBlackList = new HashSet<ServiceInstance>();\n-\n-  @VisibleForTesting\n-  // Tracks currently allocated containers.\n-  final Map<ContainerId, String> containerToInstanceMap = new HashMap<>();\n+  final Map<ServiceInstance, NodeInfo> instanceToNodeMap = new LinkedHashMap<>();\n+  // TODO Ideally, remove elements from this once it's known that no tasks are linked to the instance (all deallocated)\n \n   // Tracks tasks which could not be allocated immediately.\n   @VisibleForTesting\n@@ -100,8 +94,9 @@ public int compare(Priority o1, Priority o2) {\n   // Tracks running and queued tasks. Cleared after a task completes.\n   private final ConcurrentMap<Object, TaskInfo> knownTasks = new ConcurrentHashMap<>();\n \n+  // Queue for disabled nodes. Nodes make it out of this queue when their expiration timeout is hit.\n   @VisibleForTesting\n-  final DelayQueue<NodeInfo> disabledNodes = new DelayQueue<>();\n+  final DelayQueue<NodeInfo> disabledNodesQueue = new DelayQueue<>();\n \n   private final ContainerFactory containerFactory;\n   private final Random random = new Random();\n@@ -263,9 +258,9 @@ public Resource getAvailableResources() {\n     int vcores = 0;\n     readLock.lock();\n     try {\n-      for (ServiceInstance inst : instanceToNodeMap.keySet()) {\n-        if (inst.isAlive()) {\n-          Resource r = inst.getResource();\n+      for (Entry<ServiceInstance, NodeInfo> entry : instanceToNodeMap.entrySet()) {\n+        if (entry.getKey().isAlive() && !entry.getValue().isDisabled()) {\n+          Resource r = entry.getKey().getResource();\n           memory += r.getMemory();\n           vcores += r.getVirtualCores();\n         }\n@@ -375,8 +370,6 @@ public boolean deallocateTask(Object task, boolean taskSucceeded, TaskAttemptEnd\n         }\n         return false;\n       }\n-      String hostForContainer = containerToInstanceMap.remove(taskInfo.containerId);\n-      assert hostForContainer != null;\n       ServiceInstance assignedInstance = taskInfo.assignedInstance;\n       assert assignedInstance != null;\n \n@@ -410,6 +403,8 @@ public boolean deallocateTask(Object task, boolean taskSucceeded, TaskAttemptEnd\n   @Override\n   public Object deallocateContainer(ContainerId containerId) {\n     LOG.info(\"DEBUG: Ignoring deallocateContainer for containerId: \" + containerId);\n+    // Containers are not being tracked for re-use.\n+    // This is safe to ignore since a deallocate task should have come in earlier.\n     return null;\n   }\n \n@@ -435,7 +430,7 @@ TaskSchedulerAppCallback createAppCallbackDelegate(TaskSchedulerAppCallback real\n   }\n \n   /**\n-   * @param requestedHosts the list of preferred hosts. null implies any host\n+   * @param request the list of preferred hosts. null implies any host\n    * @return\n    */\n   private ServiceInstance selectHost(TaskInfo request) {\n@@ -444,6 +439,9 @@ private ServiceInstance selectHost(TaskInfo request) {\n     try {\n       // Check if any hosts are active.\n       if (getAvailableResources().getMemory() <= 0) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Refreshing instances since total memory is 0\");\n+        }\n         refreshInstances();\n       }\n \n@@ -453,49 +451,65 @@ private ServiceInstance selectHost(TaskInfo request) {\n       }\n \n       if (requestedHosts != null) {\n+        int prefHostCount = -1;\n         for (String host : requestedHosts) {\n+          prefHostCount++;\n           // Pick the first host always. Weak attempt at cache affinity.\n           Set<ServiceInstance> instances = activeInstances.getByHost(host);\n           if (!instances.isEmpty()) {\n             for (ServiceInstance inst : instances) {\n-              if (inst.isAlive() && instanceToNodeMap.containsKey(inst)) {\n-                // only allocate from the \"available\" list\n+              NodeInfo nodeInfo = instanceToNodeMap.get(inst);\n+              if (inst.isAlive() && nodeInfo != null && !nodeInfo.isDisabled()) {\n                 // TODO Change this to work off of what we think is remaining capacity for an\n                 // instance\n-                LOG.info(\"Assigning \" + inst + \" when looking for \" + host);\n+                LOG.info(\n+                    \"Assigning \" + inst + \" when looking for \" + host + \". FirstRequestedHost=\" +\n+                        (prefHostCount == 0));\n                 return inst;\n               }\n             }\n           }\n         }\n       }\n       /* fall through - miss in locality (random scheduling) */\n-      ServiceInstance[] all = instanceToNodeMap.keySet().toArray(new ServiceInstance[0]);\n+      Entry<ServiceInstance, NodeInfo> [] all = instanceToNodeMap.entrySet().toArray(new Entry[instanceToNodeMap.size()]);\n       // Check again\n       if (all.length > 0) {\n         int n = random.nextInt(all.length);\n         // start at random offset and iterate whole list\n         for (int i = 0; i < all.length; i++) {\n-          ServiceInstance inst = all[(i + n) % all.length];\n-          if (inst.isAlive()) {\n-            LOG.info(\"Assigning \" + inst + \" when looking for any host\");\n-            return inst;\n+          Entry<ServiceInstance, NodeInfo> inst = all[(i + n) % all.length];\n+          if (inst.getKey().isAlive() && !inst.getValue().isDisabled()) {\n+            LOG.info(\"Assigning \" + inst + \" when looking for any host, from #hosts=\" + all.length);\n+            return inst.getKey();\n           }\n         }\n       }\n     } finally {\n       readLock.unlock();\n     }\n \n+    // TODO Ideally, each refresh operation should addNodes if they don't already exist.\n+    // Even better would be to get notifications from the service impl when a node gets added or removed.\n+    // Instead of having to walk through the entire list. The computation of a node getting added or\n+    // removed already exists in the DynamicRegistry implementation.\n+\n+\n+    // This will only happen if no allocations are possible, which means all other nodes have\n+    // been blacklisted.\n+    // TODO Look for new nodes more often. See comment above.\n+\n     /* check again whether nodes are disabled or just missing */\n     writeLock.lock();\n     try {\n       for (ServiceInstance inst : activeInstances.getAll().values()) {\n-        if (inst.isAlive() && instanceBlackList.contains(inst) == false\n-            && instanceToNodeMap.containsKey(inst) == false) {\n+        if (inst.isAlive() && instanceToNodeMap.containsKey(inst) == false) {\n           /* that's a good node, not added to the allocations yet */\n+          LOG.info(\"Found a new node: \" + inst + \". Adding to node list and disabling to trigger scheduling\");\n           addNode(inst, new NodeInfo(inst, BACKOFF_FACTOR, clock));\n           // mark it as disabled to let the pending tasks go there\n+          // TODO If disabling the instance, have it wake up immediately instead of waiting.\n+          // Ideally get rid of this requirement, by having all tasks allocated via a queue.\n           disableInstance(inst, true);\n         }\n       }\n@@ -515,19 +529,22 @@ private void refreshInstances() {\n   }\n \n   private void addNode(ServiceInstance inst, NodeInfo node) {\n+    LOG.info(\"Adding node: \" + inst);\n     instanceToNodeMap.put(inst, node);\n+    // TODO Trigger a scheduling run each time a new node is added.\n   }\n \n   private void reenableDisabledNode(NodeInfo nodeInfo) {\n     writeLock.lock();\n     try {\n       if (!nodeInfo.isBusy()) {\n+        // If the node being re-enabled was not marked busy previously, then it was disabled due to\n+        // some other failure. Refresh the service list to see if it's been removed permanently.\n         refreshInstances();\n       }\n+      LOG.info(\"Attempting to re-enable node: \" + nodeInfo.host);\n       if (nodeInfo.host.isAlive()) {\n         nodeInfo.enableNode();\n-        instanceBlackList.remove(nodeInfo.host);\n-        instanceToNodeMap.put(nodeInfo.host, nodeInfo);\n       } else {\n         if (LOG.isInfoEnabled()) {\n           LOG.info(\"Removing dead node \" + nodeInfo);\n@@ -541,19 +558,18 @@ private void reenableDisabledNode(NodeInfo nodeInfo) {\n   private void disableInstance(ServiceInstance instance, boolean busy) {\n     writeLock.lock();\n     try {\n-      NodeInfo nodeInfo = instanceToNodeMap.remove(instance);\n-      if (nodeInfo == null) {\n+      NodeInfo nodeInfo = instanceToNodeMap.get(instance);\n+      if (nodeInfo == null || nodeInfo.isDisabled()) {\n         if (LOG.isDebugEnabled()) {\n           LOG.debug(\"Node: \" + instance + \" already disabled, or invalid. Not doing anything.\");\n         }\n       } else {\n-        instanceBlackList.add(instance);\n         nodeInfo.disableNode(nodeReEnableTimeout);\n         nodeInfo.setBusy(busy); // daemon failure vs daemon busy\n         // TODO: handle task to container map events in case of hard failures\n-        disabledNodes.add(nodeInfo);\n+        disabledNodesQueue.add(nodeInfo);\n         if (LOG.isInfoEnabled()) {\n-          LOG.info(\"Disabling instance \" + instance + \" for \" + nodeReEnableTimeout + \" seconds\");\n+          LOG.info(\"Disabling instance \" + instance + \" for \" + nodeReEnableTimeout + \" milli-seconds\");\n         }\n       }\n     } finally {\n@@ -640,7 +656,6 @@ private boolean scheduleTask(TaskInfo taskInfo) {\n             host.getHost());\n         taskInfo.setAssignmentInfo(host, container.getId());\n         knownTasks.putIfAbsent(taskInfo.task, taskInfo);\n-        containerToInstanceMap.put(container.getId(), host.getWorkerIdentity());\n       } finally {\n         writeLock.unlock();\n       }\n@@ -660,7 +675,7 @@ public Void call() {\n       while (!isShutdown.get() && !Thread.currentThread().isInterrupted()) {\n         try {\n           while (true) {\n-            NodeInfo nodeInfo = disabledNodes.take();\n+            NodeInfo nodeInfo = disabledNodesQueue.take();\n             // A node became available. Enable the node and try scheduling.\n             reenableDisabledNode(nodeInfo);\n             schedulePendingTasks();\n@@ -694,8 +709,12 @@ public void shutdown() {\n     private long numSuccessfulTasks = 0;\n     private long numSuccessfulTasksAtLastBlacklist = -1;\n     float cumulativeBackoffFactor = 1.0f;\n+    // A node could be disabled for reasons other than being busy.\n+    private boolean disabled = false;\n+    // If disabled, the node could be marked as busy.\n     private boolean busy;\n \n+\n     NodeInfo(ServiceInstance host, float backoffFactor, Clock clock) {\n       this.host = host;\n       constBackOffFactor = backoffFactor;\n@@ -704,10 +723,12 @@ public void shutdown() {\n \n     void enableNode() {\n       expireTimeMillis = -1;\n+      disabled = false;\n     }\n \n     void disableNode(long duration) {\n       long currentTime = clock.getTime();\n+      disabled = true;\n       if (numSuccessfulTasksAtLastBlacklist == numSuccessfulTasks) {\n         // Blacklisted again, without any progress. Will never kick in for the first run.\n         cumulativeBackoffFactor = cumulativeBackoffFactor * constBackOffFactor;\n@@ -721,7 +742,12 @@ void disableNode(long duration) {\n     }\n \n     void registerTaskSuccess() {\n-      this.busy = false; // if a task exited, we might have free slots\n+      // TODO If a task succeeds, we may have free slots. Mark the node as !busy. Ideally take it out\n+      // of the queue for more allocations.\n+      // For now, not chanigng the busy status,\n+\n+      // this.busy = false;\n+      // this.disabled = false;\n       numSuccessfulTasks++;\n     }\n \n@@ -733,9 +759,13 @@ public boolean isBusy() {\n       return busy;\n     }\n \n+    public boolean isDisabled() {\n+      return disabled;\n+    }\n+\n     @Override\n     public long getDelay(TimeUnit unit) {\n-      return expireTimeMillis - clock.getTime();\n+      return unit.convert(expireTimeMillis - clock.getTime(), TimeUnit.MILLISECONDS);\n     }\n \n     @Override\n@@ -869,31 +899,4 @@ void setAssignmentInfo(ServiceInstance instance, ContainerId containerId) {\n     }\n   }\n \n-  static class ContainerFactory {\n-    final ApplicationAttemptId customAppAttemptId;\n-    AtomicLong nextId;\n-\n-    public ContainerFactory(AppContext appContext, long appIdLong) {\n-      this.nextId = new AtomicLong(1);\n-      ApplicationId appId =\n-          ApplicationId.newInstance(appIdLong, appContext.getApplicationAttemptId()\n-              .getApplicationId().getId());\n-      this.customAppAttemptId =\n-          ApplicationAttemptId.newInstance(appId, appContext.getApplicationAttemptId()\n-              .getAttemptId());\n-    }\n-\n-    public Container createContainer(Resource capability, Priority priority, String hostname,\n-        int port) {\n-      ContainerId containerId =\n-          ContainerId.newContainerId(customAppAttemptId, nextId.getAndIncrement());\n-      NodeId nodeId = NodeId.newInstance(hostname, port);\n-      String nodeHttpAddress = \"hostname:0\"; // TODO: include UI ports\n-\n-      Container container =\n-          Container.newInstance(containerId, nodeId, nodeHttpAddress, capability, priority, null);\n-\n-      return container;\n-    }\n-  }\n }",
                "raw_url": "https://github.com/apache/hive/raw/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/java/org/apache/tez/dag/app/rm/LlapTaskSchedulerService.java",
                "sha": "39ad5521cd02a538f6c458e0065f2b0c2081e2ef",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/test/org/apache/tez/dag/app/rm/TestLlapTaskSchedulerService.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/llap-server/src/test/org/apache/tez/dag/app/rm/TestLlapTaskSchedulerService.java?ref=e9860bb80b7a2adeee7732b0dba60cb4d789e249",
                "deletions": 6,
                "filename": "llap-server/src/test/org/apache/tez/dag/app/rm/TestLlapTaskSchedulerService.java",
                "patch": "@@ -22,9 +22,7 @@\n import static org.mockito.Matchers.any;\n import static org.mockito.Matchers.eq;\n import static org.mockito.Mockito.doReturn;\n-import static org.mockito.Mockito.doAnswer;\n import static org.mockito.Mockito.mock;\n-import static org.mockito.Mockito.never;\n import static org.mockito.Mockito.reset;\n import static org.mockito.Mockito.times;\n import static org.mockito.Mockito.verify;\n@@ -38,15 +36,13 @@\n import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.api.records.Priority;\n import org.apache.hadoop.yarn.api.records.Resource;\n-import org.apache.hadoop.yarn.util.Clock;\n import org.apache.hadoop.yarn.util.SystemClock;\n import org.apache.tez.dag.api.TaskAttemptEndReason;\n import org.apache.tez.dag.app.AppContext;\n import org.apache.tez.dag.app.ControlledClock;\n import org.apache.tez.dag.app.rm.TaskSchedulerService.TaskSchedulerAppCallback;\n import org.junit.Test;\n import org.mockito.ArgumentCaptor;\n-import org.mortbay.log.Log;\n \n public class TestLlapTaskSchedulerService {\n \n@@ -112,7 +108,7 @@ public void testNodeDisabled() {\n       // Verify that the node is blacklisted\n       assertEquals(1, tsWrapper.ts.dagStats.numRejectedTasks);\n       assertEquals(2, tsWrapper.ts.instanceToNodeMap.size());\n-      LlapTaskSchedulerService.NodeInfo disabledNodeInfo = tsWrapper.ts.disabledNodes.peek();\n+      LlapTaskSchedulerService.NodeInfo disabledNodeInfo = tsWrapper.ts.disabledNodesQueue.peek();\n       assertNotNull(disabledNodeInfo);\n       assertEquals(HOST1, disabledNodeInfo.host.getHost());\n       assertEquals((10000l), disabledNodeInfo.getDelay(TimeUnit.NANOSECONDS));\n@@ -164,7 +160,7 @@ public void testNodeReEnabled() throws InterruptedException {\n       // Verify that the node is blacklisted\n       assertEquals(3, tsWrapper.ts.dagStats.numRejectedTasks);\n       assertEquals(0, tsWrapper.ts.instanceToNodeMap.size());\n-      assertEquals(3, tsWrapper.ts.disabledNodes.size());\n+      assertEquals(3, tsWrapper.ts.disabledNodesQueue.size());\n \n \n       Object task4 = new Object();",
                "raw_url": "https://github.com/apache/hive/raw/e9860bb80b7a2adeee7732b0dba60cb4d789e249/llap-server/src/test/org/apache/tez/dag/app/rm/TestLlapTaskSchedulerService.java",
                "sha": "b116af0357c459ae27b2a3bfe91297e0ed543a36",
                "status": "modified"
            }
        ],
        "message": "HIVE-10408. Fix NPE in scheduler in case of rejected tasks. (Siddharth Seth)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/llap@1675176 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/fa57c1733c6f08bc9fc840026dd7b7521f60a226",
        "patched_files": [
            "ContainerFactory.java",
            "LlapYarnRegistryImpl.java",
            "LlapTaskCommunicator.java",
            "LlapTaskSchedulerService.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestLlapTaskSchedulerService.java"
        ]
    },
    "hive_eb2b7b8": {
        "bug_id": "hive_eb2b7b8",
        "commit": "https://github.com/apache/hive/commit/eb2b7b81f815238cc2b67d701d45aa7618fc8d13",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/eb2b7b81f815238cc2b67d701d45aa7618fc8d13/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java?ref=eb2b7b81f815238cc2b67d701d45aa7618fc8d13",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java",
                "patch": "@@ -1911,6 +1911,9 @@ private boolean isBigTableOnlyResults(MapJoinDesc desc) {\n \n   private boolean onExpressionHasNullSafes(MapJoinDesc desc) {\n     boolean[] nullSafes = desc.getNullSafes();\n+    if (nullSafes == null) {\n+\treturn false;\n+    }\n     for (boolean nullSafe : nullSafes) {\n       if (nullSafe) {\n         return true;",
                "raw_url": "https://github.com/apache/hive/raw/eb2b7b81f815238cc2b67d701d45aa7618fc8d13/ql/src/java/org/apache/hadoop/hive/ql/optimizer/physical/Vectorizer.java",
                "sha": "ee080aad9c365aa73be4c518fc136e43158edf27",
                "status": "modified"
            }
        ],
        "message": "HIVE-12798 : CBO: Calcite Operator To Hive Operator (Calcite Return Path): MiniTezCliDriver.vector* queries failures due to NPE in Vectorizer.onExpressionHasNullSafes() (Hari Subramaniyan, reviewed by Matt McCline )",
        "parent": "https://github.com/apache/hive/commit/c8f15f7b802fa0c2f2426a3b29093aba4aebc57f",
        "patched_files": [
            "Vectorizer.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestVectorizer.java"
        ]
    },
    "hive_ec0eb1b": {
        "bug_id": "hive_ec0eb1b",
        "commit": "https://github.com/apache/hive/commit/ec0eb1b65ff2457537c254d0691bb763677950d6",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ec0eb1b65ff2457537c254d0691bb763677950d6/metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java?ref=ec0eb1b65ff2457537c254d0691bb763677950d6",
                "deletions": 1,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java",
                "patch": "@@ -902,7 +902,7 @@ protected void checkRetryable(Connection conn,\n     // so I've tried to capture the different error messages (there appear to be fewer different\n     // error messages than SQL states).\n     // Derby and newer MySQL driver use the new SQLTransactionRollbackException\n-    if (dbProduct == null) {\n+    if (dbProduct == null && conn != null) {\n       determineDatabaseProduct(conn);\n     }\n     if (e instanceof SQLTransactionRollbackException ||",
                "raw_url": "https://github.com/apache/hive/raw/ec0eb1b65ff2457537c254d0691bb763677950d6/metastore/src/java/org/apache/hadoop/hive/metastore/txn/TxnHandler.java",
                "sha": "ca6464ee52822f63e34f0e14b55ce100f96a6ff0",
                "status": "modified"
            }
        ],
        "message": "HIVE-9404 NPE in org.apache.hadoop.hive.metastore.txn.TxnHandler.determineDatabaseProduct() (Eugene Koifman, reviewed by Alan Gates)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1653336 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/66054149b8c3d3cefe7a1d15708f4de9b63901c2",
        "patched_files": [
            "TxnHandler.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestTxnHandler.java"
        ]
    },
    "hive_ec4673b": {
        "bug_id": "hive_ec4673b",
        "commit": "https://github.com/apache/hive/commit/ec4673bbc61b2555ef2d992266055a331443b4d6",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/ec4673bbc61b2555ef2d992266055a331443b4d6/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFAbs.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFAbs.java?ref=ec4673bbc61b2555ef2d992266055a331443b4d6",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFAbs.java",
                "patch": "@@ -130,6 +130,9 @@ public Object evaluate(DeferredObject[] arguments) throws HiveException {\n     case STRING:\n     case DOUBLE:\n       valObject = inputConverter.convert(valObject);\n+      if (valObject == null) {\n+        return null;\n+      }\n       resultDouble.set(Math.abs(((DoubleWritable) valObject).get()));\n       return resultDouble;\n     case DECIMAL:",
                "raw_url": "https://github.com/apache/hive/raw/ec4673bbc61b2555ef2d992266055a331443b4d6/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFAbs.java",
                "sha": "a8e278683a0b6c6b79b392535bc668cd27f32c1d",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/ec4673bbc61b2555ef2d992266055a331443b4d6/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFAbs.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFAbs.java?ref=ec4673bbc61b2555ef2d992266055a331443b4d6",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFAbs.java",
                "patch": "@@ -133,6 +133,12 @@ public void testText() throws HiveException {\n     output = (DoubleWritable) udf.evaluate(args);\n \n     assertEquals(\"abs() test for String failed \", \"123.45\", output.toString());\n+\n+    valueObj = new DeferredJavaObject(new Text(\"foo\"));\n+    args[0] = valueObj;\n+    output = (DoubleWritable) udf.evaluate(args);\n+\n+    assertEquals(\"abs() test for String failed \", null, output);\n   }\n \n   public void testHiveDecimal() throws HiveException {",
                "raw_url": "https://github.com/apache/hive/raw/ec4673bbc61b2555ef2d992266055a331443b4d6/ql/src/test/org/apache/hadoop/hive/ql/udf/generic/TestGenericUDFAbs.java",
                "sha": "6dbb33f9feaff8d06a88b8db603dd207ac0ab5d7",
                "status": "modified"
            }
        ],
        "message": "HIVE-14658 : UDF abs throws NPE when input arg type is string (Niklaus Xiao via Ashutosh Chauhan)\n\nSigned-off-by: Ashutosh Chauhan <hashutosh@apache.org>",
        "parent": "https://github.com/apache/hive/commit/20824f27b1649ae8c2670a177463dae0f188183c",
        "patched_files": [
            "GenericUDFAbs.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestGenericUDFAbs.java"
        ]
    },
    "hive_ec953bd": {
        "bug_id": "hive_ec953bd",
        "commit": "https://github.com/apache/hive/commit/ec953bdc45f69118b84e0d4789f58f11e5207f1b",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java?ref=ec953bdc45f69118b84e0d4789f58f11e5207f1b",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java",
                "patch": "@@ -1134,7 +1134,7 @@ public float getProgress() throws IOException {\n \n       @Override\n       public ObjectInspector getObjectInspector() {\n-        return ((StructObjectInspector) reader.getObjectInspector())\n+        return ((StructObjectInspector) records.getObjectInspector())\n             .getAllStructFieldRefs().get(OrcRecordUpdater.ROW)\n             .getFieldObjectInspector();\n       }",
                "raw_url": "https://github.com/apache/hive/raw/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcInputFormat.java",
                "sha": "b2b1a4132d3a78222118f686621d7b5d17cc4ecb",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hive/blob/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java?ref=ec953bdc45f69118b84e0d4789f58f11e5207f1b",
                "deletions": 2,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java",
                "patch": "@@ -28,6 +28,7 @@\n import org.apache.hadoop.hive.ql.io.AcidInputFormat;\n import org.apache.hadoop.hive.ql.io.AcidUtils;\n import org.apache.hadoop.hive.ql.io.RecordIdentifier;\n+import org.apache.hadoop.hive.ql.metadata.VirtualColumn;\n import org.apache.hadoop.hive.serde.serdeConstants;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;\n@@ -37,9 +38,10 @@\n import org.apache.hadoop.io.LongWritable;\n \n import java.io.IOException;\n+import java.util.ArrayDeque;\n import java.util.ArrayList;\n import java.util.Arrays;\n-import java.util.Collections;\n+import java.util.Deque;\n import java.util.List;\n import java.util.Map;\n import java.util.TreeMap;\n@@ -627,8 +629,16 @@ public ObjectInspector getObjectInspector() {\n \n     // Parse the configuration parameters\n     ArrayList<String> columnNames = new ArrayList<String>();\n+    Deque<Integer> virtualColumns = new ArrayDeque<Integer>();\n     if (columnNameProperty != null && columnNameProperty.length() > 0) {\n-      Collections.addAll(columnNames, columnNameProperty.split(\",\"));\n+      String[] colNames = columnNameProperty.split(\",\");\n+      for (int i = 0; i < colNames.length; i++) {\n+        if (VirtualColumn.VIRTUAL_COLUMN_NAMES.contains(colNames[i])) {\n+          virtualColumns.addLast(i);\n+        } else {\n+          columnNames.add(colNames[i]);\n+        }\n+      }\n     }\n     if (columnTypeProperty == null) {\n       // Default type: all string\n@@ -644,6 +654,9 @@ public ObjectInspector getObjectInspector() {\n \n     ArrayList<TypeInfo> fieldTypes =\n         TypeInfoUtils.getTypeInfosFromTypeString(columnTypeProperty);\n+    while (virtualColumns.size() > 0) {\n+      fieldTypes.remove(virtualColumns.removeLast());\n+    }\n     StructTypeInfo rowType = new StructTypeInfo();\n     rowType.setAllStructFieldNames(columnNames);\n     rowType.setAllStructFieldTypeInfos(fieldTypes);",
                "raw_url": "https://github.com/apache/hive/raw/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/java/org/apache/hadoop/hive/ql/io/orc/OrcRawRecordMerger.java",
                "sha": "fd97cb93037ab51f4f64f0fce462c437bac698d8",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hive/blob/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java?ref=ec953bdc45f69118b84e0d4789f58f11e5207f1b",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java",
                "patch": "@@ -73,6 +73,7 @@\n import org.apache.hadoop.hive.ql.plan.MapWork;\n import org.apache.hadoop.hive.ql.plan.PartitionDesc;\n import org.apache.hadoop.hive.ql.plan.TableDesc;\n+import org.apache.hadoop.hive.serde.serdeConstants;\n import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;\n import org.apache.hadoop.hive.serde2.SerDe;\n import org.apache.hadoop.hive.serde2.SerDeUtils;\n@@ -1266,6 +1267,8 @@ JobConf createMockExecutionEnvironment(Path workDir,\n     }\n     conf.set(\"hive.io.file.readcolumn.ids\", columnIds.toString());\n     conf.set(\"partition_columns\", \"p\");\n+    conf.set(serdeConstants.LIST_COLUMNS, columnNames.toString());\n+    conf.set(serdeConstants.LIST_COLUMN_TYPES, columnTypes.toString());\n     MockFileSystem fs = (MockFileSystem) warehouseDir.getFileSystem(conf);\n     fs.clear();\n ",
                "raw_url": "https://github.com/apache/hive/raw/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java",
                "sha": "a15a7a7951b5ea35c7d5787ee5357c2f9773658c",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/test/queries/clientpositive/acid_vectorization.q",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/acid_vectorization.q?ref=ec953bdc45f69118b84e0d4789f58f11e5207f1b",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/acid_vectorization.q",
                "patch": "@@ -12,3 +12,5 @@ set hive.vectorized.execution.enabled=true;\n update acid_vectorized set b = 'foo' where b = 'bar';\n set hive.vectorized.execution.enabled=true;\n delete from acid_vectorized where b = 'foo';\n+set hive.vectorized.execution.enabled=true;\n+select a, b from acid_vectorized order by a, b;",
                "raw_url": "https://github.com/apache/hive/raw/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/test/queries/clientpositive/acid_vectorization.q",
                "sha": "4b114125e77830989dc9fccd70c1afe9ed981408",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hive/blob/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/test/results/clientpositive/acid_vectorization.q.out",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/acid_vectorization.q.out?ref=ec953bdc45f69118b84e0d4789f58f11e5207f1b",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/acid_vectorization.q.out",
                "patch": "@@ -42,3 +42,21 @@ POSTHOOK: query: delete from acid_vectorized where b = 'foo'\n POSTHOOK: type: QUERY\n POSTHOOK: Input: default@acid_vectorized\n POSTHOOK: Output: default@acid_vectorized\n+PREHOOK: query: select a, b from acid_vectorized order by a, b\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@acid_vectorized\n+#### A masked pattern was here ####\n+POSTHOOK: query: select a, b from acid_vectorized order by a, b\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@acid_vectorized\n+#### A masked pattern was here ####\n+-1073279343\toj1YrV5Wa\n+-1073051226\tA34p7oRr2WvUJNf\n+-1072910839\t0iqrc5\n+-1072081801\tdPkN74F7\n+-1072076362\t2uLyD28144vklju213J1mr\n+-1071480828\taw724t8c5558x2xneC624\n+-1071363017\tAnj0oF\n+-1070883071\t0ruyd6Y50JpdGRf6HqD\n+-1070551679\tiUR3Q\n+-1069736047\tk17Am8uPHWk02cEf1jet",
                "raw_url": "https://github.com/apache/hive/raw/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/test/results/clientpositive/acid_vectorization.q.out",
                "sha": "1792979156ec361c85882ac8b6968e93d42b5f31",
                "status": "modified"
            },
            {
                "additions": 62,
                "blob_url": "https://github.com/apache/hive/blob/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/test/results/clientpositive/tez/acid_vectorization.q.out",
                "changes": 62,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/tez/acid_vectorization.q.out?ref=ec953bdc45f69118b84e0d4789f58f11e5207f1b",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/tez/acid_vectorization.q.out",
                "patch": "@@ -0,0 +1,62 @@\n+PREHOOK: query: CREATE TABLE acid_vectorized(a INT, b STRING) CLUSTERED BY(a) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true')\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: CREATE TABLE acid_vectorized(a INT, b STRING) CLUSTERED BY(a) INTO 2 BUCKETS STORED AS ORC TBLPROPERTIES ('transactional'='true')\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@acid_vectorized\n+PREHOOK: query: insert into table acid_vectorized select cint, cstring1 from alltypesorc where cint is not null order by cint limit 10\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@alltypesorc\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: insert into table acid_vectorized select cint, cstring1 from alltypesorc where cint is not null order by cint limit 10\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@alltypesorc\n+POSTHOOK: Output: default@acid_vectorized\n+POSTHOOK: Lineage: acid_vectorized.a SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cint, type:int, comment:null), ]\n+POSTHOOK: Lineage: acid_vectorized.b SIMPLE [(alltypesorc)alltypesorc.FieldSchema(name:cstring1, type:string, comment:null), ]\n+PREHOOK: query: insert into table acid_vectorized values (1, 'bar')\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@values__tmp__table__1\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: insert into table acid_vectorized values (1, 'bar')\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@values__tmp__table__1\n+POSTHOOK: Output: default@acid_vectorized\n+POSTHOOK: Lineage: acid_vectorized.a EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]\n+POSTHOOK: Lineage: acid_vectorized.b SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col2, type:string, comment:), ]\n+PREHOOK: query: update acid_vectorized set b = 'foo' where b = 'bar'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@acid_vectorized\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: update acid_vectorized set b = 'foo' where b = 'bar'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@acid_vectorized\n+POSTHOOK: Output: default@acid_vectorized\n+PREHOOK: query: delete from acid_vectorized where b = 'foo'\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@acid_vectorized\n+PREHOOK: Output: default@acid_vectorized\n+POSTHOOK: query: delete from acid_vectorized where b = 'foo'\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@acid_vectorized\n+POSTHOOK: Output: default@acid_vectorized\n+PREHOOK: query: select a, b from acid_vectorized order by a, b\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@acid_vectorized\n+#### A masked pattern was here ####\n+POSTHOOK: query: select a, b from acid_vectorized order by a, b\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@acid_vectorized\n+#### A masked pattern was here ####\n+-1073279343\toj1YrV5Wa\n+-1073051226\tA34p7oRr2WvUJNf\n+-1072910839\t0iqrc5\n+-1072081801\tdPkN74F7\n+-1072076362\t2uLyD28144vklju213J1mr\n+-1071480828\taw724t8c5558x2xneC624\n+-1071363017\tAnj0oF\n+-1070883071\t0ruyd6Y50JpdGRf6HqD\n+-1070551679\tiUR3Q\n+-1069736047\tk17Am8uPHWk02cEf1jet",
                "raw_url": "https://github.com/apache/hive/raw/ec953bdc45f69118b84e0d4789f58f11e5207f1b/ql/src/test/results/clientpositive/tez/acid_vectorization.q.out",
                "sha": "1792979156ec361c85882ac8b6968e93d42b5f31",
                "status": "added"
            }
        ],
        "message": "HIVE-8332 Reading an ACID table with vectorization on results in NPE (Alan Gates, reviewed by Owen O'Malley)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1631536 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/8cb8121ae707b1652b1959f23ae9d28f7f51b34c",
        "patched_files": [
            "OrcInputFormat.java",
            "acid_vectorization.java",
            "OrcRawRecordMerger.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestOrcRawRecordMerger.java",
            "TestInputOutputFormat.java"
        ]
    },
    "hive_f229faf": {
        "bug_id": "hive_f229faf",
        "commit": "https://github.com/apache/hive/commit/f229fafcbdb9b9f1e48df74b35f0199ae6a193f8",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/f229fafcbdb9b9f1e48df74b35f0199ae6a193f8/itests/src/test/resources/testconfiguration.properties",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/itests/src/test/resources/testconfiguration.properties?ref=f229fafcbdb9b9f1e48df74b35f0199ae6a193f8",
                "deletions": 0,
                "filename": "itests/src/test/resources/testconfiguration.properties",
                "patch": "@@ -517,6 +517,7 @@ minillaplocal.query.files=\\\n   dynamic_semijoin_reduction_2.q,\\\n   dynamic_semijoin_reduction_3.q,\\\n   dynamic_semijoin_reduction_4.q,\\\n+  dynamic_semijoin_reduction_on_aggcol.q,\\\n   dynamic_semijoin_reduction_sw.q,\\\n   dynpart_sort_opt_vectorization.q,\\\n   dynpart_sort_optimization.q,\\",
                "raw_url": "https://github.com/apache/hive/raw/f229fafcbdb9b9f1e48df74b35f0199ae6a193f8/itests/src/test/resources/testconfiguration.properties",
                "sha": "3f1ce7cf298f22ab374f59610726610be0a30b82",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hive/blob/f229fafcbdb9b9f1e48df74b35f0199ae6a193f8/ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java?ref=f229fafcbdb9b9f1e48df74b35f0199ae6a193f8",
                "deletions": 9,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java",
                "patch": "@@ -604,16 +604,11 @@ private boolean generateSemiJoinOperatorPlan(DynamicListContext ctx, ParseContex\n     // Create the column expr map\n     Map<String, ExprNodeDesc> colExprMap = new HashMap<String, ExprNodeDesc>();\n     ExprNodeDesc exprNode = null;\n-    if ( parentOfRS.getColumnExprMap() != null) {\n-      exprNode = parentOfRS.getColumnExprMap().get(internalColName).clone();\n-    } else {\n-      exprNode = new ExprNodeColumnDesc(columnInfo);\n-    }\n-\n-    if (exprNode instanceof ExprNodeColumnDesc) {\n-      ExprNodeColumnDesc encd = (ExprNodeColumnDesc) exprNode;\n-      encd.setColumn(internalColName);\n+    if (columnInfo == null) {\n+      LOG.debug(\"No ColumnInfo found in {} for {}\", parentOfRS.getOperatorId(), internalColName);\n+      return false;\n     }\n+    exprNode = new ExprNodeColumnDesc(columnInfo);\n     colExprMap.put(internalColName, exprNode);\n \n     // Create the Select Operator",
                "raw_url": "https://github.com/apache/hive/raw/f229fafcbdb9b9f1e48df74b35f0199ae6a193f8/ql/src/java/org/apache/hadoop/hive/ql/optimizer/DynamicPartitionPruningOptimization.java",
                "sha": "6686273b1458ab42a433fa8ba4987fbcf6358409",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hive/blob/f229fafcbdb9b9f1e48df74b35f0199ae6a193f8/ql/src/test/queries/clientpositive/dynamic_semijoin_reduction_on_aggcol.q",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/queries/clientpositive/dynamic_semijoin_reduction_on_aggcol.q?ref=f229fafcbdb9b9f1e48df74b35f0199ae6a193f8",
                "deletions": 0,
                "filename": "ql/src/test/queries/clientpositive/dynamic_semijoin_reduction_on_aggcol.q",
                "patch": "@@ -0,0 +1,17 @@\n+--! qt:dataset:src\n+set hive.explain.user=false;\n+set hive.tez.dynamic.partition.pruning=true;\n+set hive.tez.dynamic.semijoin.reduction=true;\n+set hive.tez.bigtable.minsize.semijoin.reduction=1;\n+set hive.tez.min.bloom.filter.entries=1;\n+\n+create table dynamic_semijoin_reduction_on_aggcol(id int, outcome string, eventid int) stored as orc;\n+insert into dynamic_semijoin_reduction_on_aggcol select key, value, key from src;\n+\n+explain select a.id, b.outcome from (select id, max(eventid) as event_id_max from dynamic_semijoin_reduction_on_aggcol where id = 0 group by id) a \n+LEFT OUTER JOIN dynamic_semijoin_reduction_on_aggcol b \n+on a.event_id_max = b.eventid;\n+\n+select a.id, b.outcome from (select id, max(eventid) as event_id_max from dynamic_semijoin_reduction_on_aggcol where id = 0 group by id) a \n+LEFT OUTER JOIN dynamic_semijoin_reduction_on_aggcol b \n+on a.event_id_max = b.eventid;",
                "raw_url": "https://github.com/apache/hive/raw/f229fafcbdb9b9f1e48df74b35f0199ae6a193f8/ql/src/test/queries/clientpositive/dynamic_semijoin_reduction_on_aggcol.q",
                "sha": "e7c8db3e7780c45ee19252bd5626d38436ca27f7",
                "status": "added"
            },
            {
                "additions": 183,
                "blob_url": "https://github.com/apache/hive/blob/f229fafcbdb9b9f1e48df74b35f0199ae6a193f8/ql/src/test/results/clientpositive/llap/dynamic_semijoin_reduction_on_aggcol.q.out",
                "changes": 183,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/results/clientpositive/llap/dynamic_semijoin_reduction_on_aggcol.q.out?ref=f229fafcbdb9b9f1e48df74b35f0199ae6a193f8",
                "deletions": 0,
                "filename": "ql/src/test/results/clientpositive/llap/dynamic_semijoin_reduction_on_aggcol.q.out",
                "patch": "@@ -0,0 +1,183 @@\n+PREHOOK: query: create table dynamic_semijoin_reduction_on_aggcol(id int, outcome string, eventid int) stored as orc\n+PREHOOK: type: CREATETABLE\n+PREHOOK: Output: database:default\n+PREHOOK: Output: default@dynamic_semijoin_reduction_on_aggcol\n+POSTHOOK: query: create table dynamic_semijoin_reduction_on_aggcol(id int, outcome string, eventid int) stored as orc\n+POSTHOOK: type: CREATETABLE\n+POSTHOOK: Output: database:default\n+POSTHOOK: Output: default@dynamic_semijoin_reduction_on_aggcol\n+PREHOOK: query: insert into dynamic_semijoin_reduction_on_aggcol select key, value, key from src\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@src\n+PREHOOK: Output: default@dynamic_semijoin_reduction_on_aggcol\n+POSTHOOK: query: insert into dynamic_semijoin_reduction_on_aggcol select key, value, key from src\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@src\n+POSTHOOK: Output: default@dynamic_semijoin_reduction_on_aggcol\n+POSTHOOK: Lineage: dynamic_semijoin_reduction_on_aggcol.eventid EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: dynamic_semijoin_reduction_on_aggcol.id EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]\n+POSTHOOK: Lineage: dynamic_semijoin_reduction_on_aggcol.outcome SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]\n+PREHOOK: query: explain select a.id, b.outcome from (select id, max(eventid) as event_id_max from dynamic_semijoin_reduction_on_aggcol where id = 0 group by id) a \n+LEFT OUTER JOIN dynamic_semijoin_reduction_on_aggcol b \n+on a.event_id_max = b.eventid\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@dynamic_semijoin_reduction_on_aggcol\n+#### A masked pattern was here ####\n+POSTHOOK: query: explain select a.id, b.outcome from (select id, max(eventid) as event_id_max from dynamic_semijoin_reduction_on_aggcol where id = 0 group by id) a \n+LEFT OUTER JOIN dynamic_semijoin_reduction_on_aggcol b \n+on a.event_id_max = b.eventid\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@dynamic_semijoin_reduction_on_aggcol\n+#### A masked pattern was here ####\n+STAGE DEPENDENCIES:\n+  Stage-1 is a root stage\n+  Stage-0 depends on stages: Stage-1\n+\n+STAGE PLANS:\n+  Stage: Stage-1\n+    Tez\n+#### A masked pattern was here ####\n+      Edges:\n+        Map 5 <- Reducer 4 (BROADCAST_EDGE)\n+        Reducer 2 <- Map 1 (SIMPLE_EDGE)\n+        Reducer 3 <- Map 5 (SIMPLE_EDGE), Reducer 2 (SIMPLE_EDGE)\n+        Reducer 4 <- Reducer 2 (CUSTOM_SIMPLE_EDGE)\n+#### A masked pattern was here ####\n+      Vertices:\n+        Map 1 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: dynamic_semijoin_reduction_on_aggcol\n+                  filterExpr: (id = 0) (type: boolean)\n+                  Statistics: Num rows: 500 Data size: 4000 Basic stats: COMPLETE Column stats: COMPLETE\n+                  Filter Operator\n+                    predicate: (id = 0) (type: boolean)\n+                    Statistics: Num rows: 2 Data size: 16 Basic stats: COMPLETE Column stats: COMPLETE\n+                    Select Operator\n+                      expressions: eventid (type: int)\n+                      outputColumnNames: _col1\n+                      Statistics: Num rows: 2 Data size: 16 Basic stats: COMPLETE Column stats: COMPLETE\n+                      Group By Operator\n+                        aggregations: max(_col1)\n+                        keys: true (type: boolean)\n+                        minReductionHashAggr: 0.5\n+                        mode: hash\n+                        outputColumnNames: _col0, _col1\n+                        Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n+                        Reduce Output Operator\n+                          key expressions: _col0 (type: boolean)\n+                          sort order: +\n+                          Map-reduce partition columns: _col0 (type: boolean)\n+                          Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n+                          value expressions: _col1 (type: int)\n+            Execution mode: vectorized, llap\n+            LLAP IO: all inputs\n+        Map 5 \n+            Map Operator Tree:\n+                TableScan\n+                  alias: b\n+                  filterExpr: (eventid is not null and (eventid BETWEEN DynamicValue(RS_11_dynamic_semijoin_reduction_on_aggcol__col1_min) AND DynamicValue(RS_11_dynamic_semijoin_reduction_on_aggcol__col1_max) and in_bloom_filter(eventid, DynamicValue(RS_11_dynamic_semijoin_reduction_on_aggcol__col1_bloom_filter)))) (type: boolean)\n+                  Statistics: Num rows: 500 Data size: 47500 Basic stats: COMPLETE Column stats: COMPLETE\n+                  Filter Operator\n+                    predicate: ((eventid BETWEEN DynamicValue(RS_11_dynamic_semijoin_reduction_on_aggcol__col1_min) AND DynamicValue(RS_11_dynamic_semijoin_reduction_on_aggcol__col1_max) and in_bloom_filter(eventid, DynamicValue(RS_11_dynamic_semijoin_reduction_on_aggcol__col1_bloom_filter))) and eventid is not null) (type: boolean)\n+                    Statistics: Num rows: 500 Data size: 47500 Basic stats: COMPLETE Column stats: COMPLETE\n+                    Select Operator\n+                      expressions: outcome (type: string), eventid (type: int)\n+                      outputColumnNames: _col0, _col1\n+                      Statistics: Num rows: 500 Data size: 47500 Basic stats: COMPLETE Column stats: COMPLETE\n+                      Reduce Output Operator\n+                        key expressions: _col1 (type: int)\n+                        sort order: +\n+                        Map-reduce partition columns: _col1 (type: int)\n+                        Statistics: Num rows: 500 Data size: 47500 Basic stats: COMPLETE Column stats: COMPLETE\n+                        value expressions: _col0 (type: string)\n+            Execution mode: vectorized, llap\n+            LLAP IO: all inputs\n+        Reducer 2 \n+            Execution mode: vectorized, llap\n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: max(VALUE._col0)\n+                keys: KEY._col0 (type: boolean)\n+                mode: mergepartial\n+                outputColumnNames: _col0, _col1\n+                Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE\n+                Select Operator\n+                  expressions: _col1 (type: int)\n+                  outputColumnNames: _col0\n+                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE\n+                  Reduce Output Operator\n+                    key expressions: _col0 (type: int)\n+                    sort order: +\n+                    Map-reduce partition columns: _col0 (type: int)\n+                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE\n+                  Select Operator\n+                    expressions: _col0 (type: int)\n+                    outputColumnNames: _col0\n+                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE\n+                    Group By Operator\n+                      aggregations: min(_col0), max(_col0), bloom_filter(_col0, expectedEntries=1)\n+                      minReductionHashAggr: 0.0\n+                      mode: hash\n+                      outputColumnNames: _col0, _col1, _col2\n+                      Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: COMPLETE\n+                      Reduce Output Operator\n+                        sort order: \n+                        Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: COMPLETE\n+                        value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: binary)\n+        Reducer 3 \n+            Execution mode: llap\n+            Reduce Operator Tree:\n+              Merge Join Operator\n+                condition map:\n+                     Left Outer Join 0 to 1\n+                keys:\n+                  0 _col0 (type: int)\n+                  1 _col1 (type: int)\n+                outputColumnNames: _col1\n+                Statistics: Num rows: 2 Data size: 182 Basic stats: COMPLETE Column stats: COMPLETE\n+                Select Operator\n+                  expressions: 0 (type: int), _col1 (type: string)\n+                  outputColumnNames: _col0, _col1\n+                  Statistics: Num rows: 2 Data size: 190 Basic stats: COMPLETE Column stats: COMPLETE\n+                  File Output Operator\n+                    compressed: false\n+                    Statistics: Num rows: 2 Data size: 190 Basic stats: COMPLETE Column stats: COMPLETE\n+                    table:\n+                        input format: org.apache.hadoop.mapred.SequenceFileInputFormat\n+                        output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat\n+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\n+        Reducer 4 \n+            Execution mode: vectorized, llap\n+            Reduce Operator Tree:\n+              Group By Operator\n+                aggregations: min(VALUE._col0), max(VALUE._col1), bloom_filter(VALUE._col2, expectedEntries=1)\n+                mode: final\n+                outputColumnNames: _col0, _col1, _col2\n+                Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: COMPLETE\n+                Reduce Output Operator\n+                  sort order: \n+                  Statistics: Num rows: 1 Data size: 12 Basic stats: COMPLETE Column stats: COMPLETE\n+                  value expressions: _col0 (type: int), _col1 (type: int), _col2 (type: binary)\n+\n+  Stage: Stage-0\n+    Fetch Operator\n+      limit: -1\n+      Processor Tree:\n+        ListSink\n+\n+PREHOOK: query: select a.id, b.outcome from (select id, max(eventid) as event_id_max from dynamic_semijoin_reduction_on_aggcol where id = 0 group by id) a \n+LEFT OUTER JOIN dynamic_semijoin_reduction_on_aggcol b \n+on a.event_id_max = b.eventid\n+PREHOOK: type: QUERY\n+PREHOOK: Input: default@dynamic_semijoin_reduction_on_aggcol\n+#### A masked pattern was here ####\n+POSTHOOK: query: select a.id, b.outcome from (select id, max(eventid) as event_id_max from dynamic_semijoin_reduction_on_aggcol where id = 0 group by id) a \n+LEFT OUTER JOIN dynamic_semijoin_reduction_on_aggcol b \n+on a.event_id_max = b.eventid\n+POSTHOOK: type: QUERY\n+POSTHOOK: Input: default@dynamic_semijoin_reduction_on_aggcol\n+#### A masked pattern was here ####\n+0\tval_0\n+0\tval_0\n+0\tval_0",
                "raw_url": "https://github.com/apache/hive/raw/f229fafcbdb9b9f1e48df74b35f0199ae6a193f8/ql/src/test/results/clientpositive/llap/dynamic_semijoin_reduction_on_aggcol.q.out",
                "sha": "9f2af56f427def847d5fc8f6d7fe1f9e6e896b65",
                "status": "added"
            }
        ],
        "message": "HIVE-21799: NullPointerException in DynamicPartitionPruningOptimization, when join key is on aggregation column (Jason Dere, reviewed by Vineet Garg)",
        "parent": "https://github.com/apache/hive/commit/1a649cde5d1620560eec7363338f8c91307d24ac",
        "patched_files": [
            "DynamicPartitionPruningOptimization.java",
            "dynamic_semijoin_reduction_on_aggcol.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "testconfiguration.java"
        ]
    },
    "hive_f3b2c70": {
        "bug_id": "hive_f3b2c70",
        "commit": "https://github.com/apache/hive/commit/f3b2c702238f5592a6743adbe5aee42bbec9f4e6",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hive/blob/f3b2c702238f5592a6743adbe5aee42bbec9f4e6/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java?ref=f3b2c702238f5592a6743adbe5aee42bbec9f4e6",
                "deletions": 3,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "patch": "@@ -12125,9 +12125,6 @@ private AggregateCall convertAgg(AggInfo agg, RelNode input,\n       List<Integer> argList = new ArrayList<Integer>();\n       RelDataType type = TypeConverter.convert(agg.m_returnType,\n           this.m_cluster.getTypeFactory());\n-      if (aggregation.equals(SqlStdOperatorTable.AVG)) {\n-        type = type.getField(\"sum\", false).getType();\n-      }\n \n       // TODO: Does HQL allows expressions as aggregate args or can it only be\n       // projections from child?",
                "raw_url": "https://github.com/apache/hive/raw/f3b2c702238f5592a6743adbe5aee42bbec9f4e6/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java",
                "sha": "c7c76e520d8b4ba3edb5cf883b0f9c5b76dd7bd4",
                "status": "modified"
            }
        ],
        "message": "HIVE-7310: Turning CBO on results in NPE on some queries (Laljo John Pullokkaran via Gunther Hagleitner)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/cbo@1606276 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/e34ca1c02edb3b771c2976c75d57dc5212284637",
        "patched_files": [
            "SemanticAnalyzer.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestSemanticAnalyzer.java"
        ]
    },
    "hive_f50f088": {
        "bug_id": "hive_f50f088",
        "commit": "https://github.com/apache/hive/commit/f50f08876a6b84f66d5be52ac1727c62c2900fc5",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/f50f08876a6b84f66d5be52ac1727c62c2900fc5/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java?ref=f50f08876a6b84f66d5be52ac1727c62c2900fc5",
                "deletions": 6,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java",
                "patch": "@@ -2364,20 +2364,21 @@ static TruthValue evaluatePredicate(OrcProto.ColumnStatistics index,\n                                       PredicateLeaf predicate) {\n     ColumnStatistics cs = ColumnStatisticsImpl.deserialize(index);\n     Object minValue = getMin(cs);\n+    Object maxValue = getMax(cs);\n+    return evaluatePredicateRange(predicate, minValue, maxValue);\n+  }\n+\n+  static TruthValue evaluatePredicateRange(PredicateLeaf predicate, Object min,\n+      Object max) {\n     // if we didn't have any values, everything must have been null\n-    if (minValue == null) {\n+    if (min == null) {\n       if (predicate.getOperator() == PredicateLeaf.Operator.IS_NULL) {\n         return TruthValue.YES;\n       } else {\n         return TruthValue.NULL;\n       }\n     }\n-    Object maxValue = getMax(cs);\n-    return evaluatePredicateRange(predicate, minValue, maxValue);\n-  }\n \n-  static TruthValue evaluatePredicateRange(PredicateLeaf predicate, Object min,\n-      Object max) {\n     Location loc;\n     try {\n       // Predicate object and stats object can be one of the following base types",
                "raw_url": "https://github.com/apache/hive/raw/f50f08876a6b84f66d5be52ac1727c62c2900fc5/ql/src/java/org/apache/hadoop/hive/ql/io/orc/RecordReaderImpl.java",
                "sha": "a6a0ec1a0e022efd23fddadeb44d5dfff8c092d7",
                "status": "modified"
            },
            {
                "additions": 125,
                "blob_url": "https://github.com/apache/hive/blob/f50f08876a6b84f66d5be52ac1727c62c2900fc5/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java",
                "changes": 147,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java?ref=f50f08876a6b84f66d5be52ac1727c62c2900fc5",
                "deletions": 22,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java",
                "patch": "@@ -21,27 +21,6 @@\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertTrue;\n \n-import java.io.DataInput;\n-import java.io.DataOutput;\n-import java.io.FileNotFoundException;\n-import java.io.IOException;\n-import java.net.URI;\n-import java.net.URISyntaxException;\n-import java.sql.Date;\n-import java.sql.Timestamp;\n-import java.text.SimpleDateFormat;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.Collections;\n-import java.util.HashMap;\n-import java.util.LinkedHashMap;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Properties;\n-import java.util.Set;\n-import java.util.TimeZone;\n-import java.util.TreeSet;\n-\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.BlockLocation;\n import org.apache.hadoop.fs.FSDataInputStream;\n@@ -66,9 +45,9 @@\n import org.apache.hadoop.hive.ql.io.HiveInputFormat;\n import org.apache.hadoop.hive.ql.io.HiveOutputFormat;\n import org.apache.hadoop.hive.ql.io.InputFormatChecker;\n-import org.apache.hadoop.hive.ql.io.sarg.SearchArgumentFactory;\n import org.apache.hadoop.hive.ql.io.sarg.PredicateLeaf;\n import org.apache.hadoop.hive.ql.io.sarg.SearchArgument;\n+import org.apache.hadoop.hive.ql.io.sarg.SearchArgumentFactory;\n import org.apache.hadoop.hive.ql.plan.MapWork;\n import org.apache.hadoop.hive.ql.plan.PartitionDesc;\n import org.apache.hadoop.hive.ql.plan.TableDesc;\n@@ -104,6 +83,27 @@\n import org.junit.Test;\n import org.junit.rules.TestName;\n \n+import java.io.DataInput;\n+import java.io.DataOutput;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.net.URI;\n+import java.net.URISyntaxException;\n+import java.sql.Date;\n+import java.sql.Timestamp;\n+import java.text.SimpleDateFormat;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.LinkedHashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Properties;\n+import java.util.Set;\n+import java.util.TimeZone;\n+import java.util.TreeSet;\n+\n public class TestInputOutputFormat {\n \n   Path workDir = new Path(System.getProperty(\"test.tmp.dir\",\"target/tmp\"));\n@@ -1032,6 +1032,24 @@ public void testInOutFormat() throws Exception {\n     reader.close();\n   }\n \n+  static class SimpleRow implements Writable {\n+    Text z;\n+\n+    public SimpleRow(Text t) {\n+      this.z = t;\n+    }\n+\n+    @Override\n+    public void write(DataOutput dataOutput) throws IOException {\n+      throw new UnsupportedOperationException(\"unsupported\");\n+    }\n+\n+    @Override\n+    public void readFields(DataInput dataInput) throws IOException {\n+      throw new UnsupportedOperationException(\"unsupported\");\n+    }\n+  }\n+\n   static class NestedRow implements Writable {\n     int z;\n     MyRow r;\n@@ -1685,4 +1703,89 @@ public void testSetSearchArgument() throws Exception {\n     assertEquals(\"cost\", leaves.get(0).getColumnName());\n     assertEquals(PredicateLeaf.Operator.IS_NULL, leaves.get(0).getOperator());\n   }\n+\n+  @Test\n+  @SuppressWarnings(\"unchecked,deprecation\")\n+  public void testSplitElimination() throws Exception {\n+    Properties properties = new Properties();\n+    StructObjectInspector inspector;\n+    synchronized (TestOrcFile.class) {\n+      inspector = (StructObjectInspector)\n+          ObjectInspectorFactory.getReflectionObjectInspector(NestedRow.class,\n+              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n+    }\n+    SerDe serde = new OrcSerde();\n+    OutputFormat<?, ?> outFormat = new OrcOutputFormat();\n+    conf.setInt(\"mapred.max.split.size\", 50);\n+    RecordWriter writer =\n+        outFormat.getRecordWriter(fs, conf, testFilePath.toString(),\n+            Reporter.NULL);\n+    writer.write(NullWritable.get(),\n+        serde.serialize(new NestedRow(1,2,3), inspector));\n+    writer.write(NullWritable.get(),\n+        serde.serialize(new NestedRow(4,5,6), inspector));\n+    writer.write(NullWritable.get(),\n+        serde.serialize(new NestedRow(7,8,9), inspector));\n+    writer.close(Reporter.NULL);\n+    serde = new OrcSerde();\n+    SearchArgument sarg =\n+        SearchArgumentFactory.newBuilder()\n+            .startAnd()\n+            .lessThan(\"z\", new Integer(0))\n+            .end()\n+            .build();\n+    conf.set(\"sarg.pushdown\", sarg.toKryo());\n+    conf.set(\"hive.io.file.readcolumn.names\", \"z,r\");\n+    properties.setProperty(\"columns\", \"z,r\");\n+    properties.setProperty(\"columns.types\", \"int:struct<x:int,y:int>\");\n+    SerDeUtils.initializeSerDe(serde, conf, properties, null);\n+    inspector = (StructObjectInspector) serde.getObjectInspector();\n+    InputFormat<?,?> in = new OrcInputFormat();\n+    FileInputFormat.setInputPaths(conf, testFilePath.toString());\n+    InputSplit[] splits = in.getSplits(conf, 1);\n+    assertEquals(0, splits.length);\n+  }\n+\n+  @Test\n+  @SuppressWarnings(\"unchecked,deprecation\")\n+  public void testSplitEliminationNullStats() throws Exception {\n+    Properties properties = new Properties();\n+    StructObjectInspector inspector;\n+    synchronized (TestOrcFile.class) {\n+      inspector = (StructObjectInspector)\n+          ObjectInspectorFactory.getReflectionObjectInspector(SimpleRow.class,\n+              ObjectInspectorFactory.ObjectInspectorOptions.JAVA);\n+    }\n+    SerDe serde = new OrcSerde();\n+    OutputFormat<?, ?> outFormat = new OrcOutputFormat();\n+    conf.setInt(\"mapred.max.split.size\", 50);\n+    RecordWriter writer =\n+        outFormat.getRecordWriter(fs, conf, testFilePath.toString(),\n+            Reporter.NULL);\n+    writer.write(NullWritable.get(),\n+        serde.serialize(new SimpleRow(null), inspector));\n+    writer.write(NullWritable.get(),\n+        serde.serialize(new SimpleRow(null), inspector));\n+    writer.write(NullWritable.get(),\n+        serde.serialize(new SimpleRow(null), inspector));\n+    writer.close(Reporter.NULL);\n+    serde = new OrcSerde();\n+    SearchArgument sarg =\n+        SearchArgumentFactory.newBuilder()\n+            .startAnd()\n+            .lessThan(\"z\", new String(\"foo\"))\n+            .end()\n+            .build();\n+    conf.set(\"sarg.pushdown\", sarg.toKryo());\n+    conf.set(\"hive.io.file.readcolumn.names\", \"z\");\n+    properties.setProperty(\"columns\", \"z\");\n+    properties.setProperty(\"columns.types\", \"string\");\n+    SerDeUtils.initializeSerDe(serde, conf, properties, null);\n+    inspector = (StructObjectInspector) serde.getObjectInspector();\n+    InputFormat<?,?> in = new OrcInputFormat();\n+    FileInputFormat.setInputPaths(conf, testFilePath.toString());\n+    InputSplit[] splits = in.getSplits(conf, 1);\n+    assertEquals(0, splits.length);\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hive/raw/f50f08876a6b84f66d5be52ac1727c62c2900fc5/ql/src/test/org/apache/hadoop/hive/ql/io/orc/TestInputOutputFormat.java",
                "sha": "d1acd88797dd361415eddbaf9b46df10487d1807",
                "status": "modified"
            }
        ],
        "message": "HIVE-8778: ORC split elimination can cause NPE when column statistics is null (Prasanth J reviewed by Gopal V)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1637416 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/f03ea3976b4f0ad8608128886643a5d47671ec93",
        "patched_files": [
            "RecordReaderImpl.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestInputOutputFormat.java",
            "TestRecordReaderImpl.java"
        ]
    },
    "hive_f5ad79b": {
        "bug_id": "hive_f5ad79b",
        "commit": "https://github.com/apache/hive/commit/f5ad79b3fcc17f5584f0c3dd695ca6e8181b0c84",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/f5ad79b3fcc17f5584f0c3dd695ca6e8181b0c84/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java?ref=f5ad79b3fcc17f5584f0c3dd695ca6e8181b0c84",
                "deletions": 2,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java",
                "patch": "@@ -42,8 +42,8 @@\n   private byte[] bytesValue = null;\n   private String typeString;\n \n-  private transient Type type;\n-  private transient int bytesValueLength = 0;\n+  private Type type;\n+  private int bytesValueLength = 0;\n \n   public ConstantVectorExpression() {\n     super();",
                "raw_url": "https://github.com/apache/hive/raw/f5ad79b3fcc17f5584f0c3dd695ca6e8181b0c84/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/ConstantVectorExpression.java",
                "sha": "119b4b9d4d6cc0c0d5cf3961009f760dae3558a8",
                "status": "modified"
            }
        ],
        "message": "HIVE-5526 - NPE in ConstantVectorExpression.evaluate(vrg) (Remus Rusanu via Brock Noland)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/trunk@1532044 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/7083f4f3cbeecdb9b2ed984b8aa47537c37fe7f6",
        "patched_files": [
            "ConstantVectorExpression.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestConstantVectorExpression.java"
        ]
    },
    "hive_f766b8f": {
        "bug_id": "hive_f766b8f",
        "commit": "https://github.com/apache/hive/commit/f766b8fe1dd332a31ed04ef8ff53a3136c87ea3c",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hive/blob/f766b8fe1dd332a31ed04ef8ff53a3136c87ea3c/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=f766b8fe1dd332a31ed04ef8ff53a3136c87ea3c",
                "deletions": 3,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "patch": "@@ -4614,9 +4614,12 @@ private boolean drop_index_by_name_core(final RawStore ms,\n           deleteTableData(tblPath);\n           // ok even if the data is not deleted\n         }\n-        for (MetaStoreEventListener listener : listeners) {\n-          DropIndexEvent dropIndexEvent = new DropIndexEvent(index, success, this);\n-          listener.onDropIndex(dropIndexEvent);\n+        // Skip the event listeners if the index is NULL\n+        if (index != null) {\n+          for (MetaStoreEventListener listener : listeners) {\n+            DropIndexEvent dropIndexEvent = new DropIndexEvent(index, success, this);\n+            listener.onDropIndex(dropIndexEvent);\n+          }\n         }\n       }\n       return success;",
                "raw_url": "https://github.com/apache/hive/raw/f766b8fe1dd332a31ed04ef8ff53a3136c87ea3c/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "sha": "f8c3c4e48db0df9d6c18801bcd61f9e5dc6eb7c2",
                "status": "modified"
            }
        ],
        "message": "HIVE-15778: DROP INDEX (non-existent) throws NPE when using DbNotificationListener (Vamsee Yarlagadda, reviewed by Aihua Xu)",
        "parent": "https://github.com/apache/hive/commit/7cca0978af944b4a76dd40e014e628f82f43c42f",
        "patched_files": [
            "HiveMetaStore.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveMetaStore.java"
        ]
    },
    "hive_fb35bae": {
        "bug_id": "hive_fb35bae",
        "commit": "https://github.com/apache/hive/commit/fb35bae5ae2fad93de3deef9023f52cba8e4783b",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hive/blob/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java?ref=fb35bae5ae2fad93de3deef9023f52cba8e4783b",
                "deletions": 3,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java",
                "patch": "@@ -126,7 +126,8 @@\n   public ExecDriver() {\n     super();\n     console = new LogHelper(LOG);\n-    this.jobExecHelper = new HadoopJobExecHelper(queryState, job, console, this, this);\n+    job = new JobConf(ExecDriver.class);\n+    this.jobExecHelper = new HadoopJobExecHelper(job, console, this, this);\n   }\n \n   @Override\n@@ -175,7 +176,7 @@ public void initialize(QueryState queryState, QueryPlan queryPlan, DriverContext\n     initializeFiles(\"tmparchives\", getResource(conf, SessionState.ResourceType.ARCHIVE));\n \n     conf.stripHiddenConfigurations(job);\n-    this.jobExecHelper = new HadoopJobExecHelper(queryState, job, console, this, this);\n+    this.jobExecHelper = new HadoopJobExecHelper(job, console, this, this);\n   }\n \n   /**\n@@ -185,7 +186,7 @@ public ExecDriver(MapredWork plan, JobConf job, boolean isSilent) throws HiveExc\n     setWork(plan);\n     this.job = job;\n     console = new LogHelper(LOG, isSilent);\n-    this.jobExecHelper = new HadoopJobExecHelper(queryState, job, console, this, this);\n+    this.jobExecHelper = new HadoopJobExecHelper(job, console, this, this);\n   }\n \n   /**\n@@ -671,6 +672,7 @@ public static void main(String[] args) throws IOException, HiveException {\n     String queryId = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEQUERYID, \"\").trim();\n     if(queryId.isEmpty()) {\n       queryId = \"unknown-\" + System.currentTimeMillis();\n+      HiveConf.setVar(conf, HiveConf.ConfVars.HIVEQUERYID, queryId);\n     }\n     System.setProperty(HiveConf.ConfVars.HIVEQUERYID.toString(), queryId);\n ",
                "raw_url": "https://github.com/apache/hive/raw/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java",
                "sha": "285f9ad76e07b66dc338c5d33859ff47ac08d9a8",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java?ref=fb35bae5ae2fad93de3deef9023f52cba8e4783b",
                "deletions": 10,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java",
                "patch": "@@ -34,7 +34,6 @@\n import org.apache.hadoop.hive.conf.HiveConf.ConfVars;\n import org.apache.hadoop.hive.ql.Context;\n import org.apache.hadoop.hive.ql.MapRedStats;\n-import org.apache.hadoop.hive.ql.QueryState;\n import org.apache.hadoop.hive.ql.exec.Operator;\n import org.apache.hadoop.hive.ql.exec.Task;\n import org.apache.hadoop.hive.ql.exec.TaskHandle;\n@@ -78,7 +77,7 @@\n   public transient JobID jobId;\n   private final LogHelper console;\n   private final HadoopJobExecHook callBackObj;\n-  private final QueryState queryState;\n+  private final String queryId;\n \n   /**\n    * Update counters relevant to this task.\n@@ -139,9 +138,9 @@ public void setJobId(JobID jobId) {\n     this.jobId = jobId;\n   }\n \n-  public HadoopJobExecHelper(QueryState queryState, JobConf job, LogHelper console,\n+  public HadoopJobExecHelper(JobConf job, LogHelper console,\n       Task<? extends Serializable> task, HadoopJobExecHook hookCallBack) {\n-    this.queryState = queryState;\n+    this.queryId = HiveConf.getVar(job, HiveConf.ConfVars.HIVEQUERYID, \"unknown-\" + System.currentTimeMillis());\n     this.job = job;\n     this.console = console;\n     this.task = task;\n@@ -259,7 +258,6 @@ private MapRedStats progress(ExecDriverTaskHandle th) throws IOException, LockEx\n \n           String logMapper;\n           String logReducer;\n-          String queryId = queryState.getQueryId();\n           TaskReport[] mappers = jc.getMapTaskReports(rj.getID());\n           if (mappers == null) {\n             logMapper = \"no information for number of mappers; \";\n@@ -364,11 +362,11 @@ private MapRedStats progress(ExecDriverTaskHandle th) throws IOException, LockEx\n       String output = report.toString();\n       SessionState ss = SessionState.get();\n       if (ss != null) {\n-        ss.getHiveHistory().setTaskCounters(queryState.getQueryId(), getId(), ctrs);\n-        ss.getHiveHistory().setTaskProperty(queryState.getQueryId(), getId(),\n+        ss.getHiveHistory().setTaskCounters(queryId, getId(), ctrs);\n+        ss.getHiveHistory().setTaskProperty(queryId, getId(),\n             Keys.TASK_HADOOP_PROGRESS, output);\n         if (ss.getConf().getBoolVar(HiveConf.ConfVars.HIVE_LOG_INCREMENTAL_PLAN_PROGRESS)) {\n-          ss.getHiveHistory().progressTask(queryState.getQueryId(), this.task);\n+          ss.getHiveHistory().progressTask(queryId, this.task);\n           this.callBackObj.logPlanProgress(ss);\n         }\n       }\n@@ -397,7 +395,7 @@ private MapRedStats progress(ExecDriverTaskHandle th) throws IOException, LockEx\n       } else {\n         SessionState ss = SessionState.get();\n         if (ss != null) {\n-          ss.getHiveHistory().setTaskCounters(queryState.getQueryId(), getId(), ctrs);\n+          ss.getHiveHistory().setTaskCounters(queryId, getId(), ctrs);\n         }\n         success = rj.isSuccessful();\n       }\n@@ -441,7 +439,7 @@ public void jobInfo(RunningJob rj) {\n       console.printInfo(\"Job running in-process (local Hadoop)\");\n     } else {\n       if (SessionState.get() != null) {\n-        SessionState.get().getHiveHistory().setTaskProperty(queryState.getQueryId(),\n+        SessionState.get().getHiveHistory().setTaskProperty(queryId,\n             getId(), Keys.TASK_HADOOP_ID, rj.getID().toString());\n       }\n       console.printInfo(getJobStartMsg(rj.getID()) + \", Tracking URL = \"",
                "raw_url": "https://github.com/apache/hive/raw/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/HadoopJobExecHelper.java",
                "sha": "cfb4a2816817925fe7351f7d17bd8ab35742890b",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java?ref=fb35bae5ae2fad93de3deef9023f52cba8e4783b",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java",
                "patch": "@@ -128,7 +128,7 @@ public void initialize(QueryState queryState, QueryPlan queryPlan, DriverContext\n     job = new JobConf(conf, ExecDriver.class);\n     execContext = new ExecMapperContext(job);\n     //we don't use the HadoopJobExecHooks for local tasks\n-    this.jobExecHelper = new HadoopJobExecHelper(queryState, job, console, this, null);\n+    this.jobExecHelper = new HadoopJobExecHelper(job, console, this, null);\n   }\n \n   public static String now() {",
                "raw_url": "https://github.com/apache/hive/raw/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapredLocalTask.java",
                "sha": "23a13d66c2dc1434c96164a7e2a8869fd5b4fcae",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileTask.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileTask.java?ref=fb35bae5ae2fad93de3deef9023f52cba8e4783b",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileTask.java",
                "patch": "@@ -65,7 +65,7 @@ public void initialize(QueryState queryState, QueryPlan queryPlan,\n       DriverContext driverContext, CompilationOpContext opContext) {\n     super.initialize(queryState, queryPlan, driverContext, opContext);\n     job = new JobConf(conf, MergeFileTask.class);\n-    jobExecHelper = new HadoopJobExecHelper(queryState, job, this.console, this, this);\n+    jobExecHelper = new HadoopJobExecHelper(job, this.console, this, this);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hive/raw/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/io/merge/MergeFileTask.java",
                "sha": "376bab2c39f775a6d6c931f55f60c90c87e41957",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java?ref=fb35bae5ae2fad93de3deef9023f52cba8e4783b",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java",
                "patch": "@@ -89,7 +89,7 @@ public void initialize(QueryState queryState, QueryPlan queryPlan,\n       DriverContext driverContext, CompilationOpContext opContext) {\n     super.initialize(queryState, queryPlan, driverContext, opContext);\n     job = new JobConf(conf, PartialScanTask.class);\n-    jobExecHelper = new HadoopJobExecHelper(queryState, job, this.console, this, this);\n+    jobExecHelper = new HadoopJobExecHelper(job, this.console, this, this);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hive/raw/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java",
                "sha": "6131581b1965adccd3f49f8fd730648acffbc78e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/truncate/ColumnTruncateTask.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/truncate/ColumnTruncateTask.java?ref=fb35bae5ae2fad93de3deef9023f52cba8e4783b",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/truncate/ColumnTruncateTask.java",
                "patch": "@@ -64,7 +64,7 @@ public void initialize(QueryState queryState, QueryPlan queryPlan,\n       DriverContext driverContext, CompilationOpContext opContext) {\n     super.initialize(queryState, queryPlan, driverContext, opContext);\n     job = new JobConf(conf, ColumnTruncateTask.class);\n-    jobExecHelper = new HadoopJobExecHelper(queryState, job, this.console, this, this);\n+    jobExecHelper = new HadoopJobExecHelper(job, this.console, this, this);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hive/raw/fb35bae5ae2fad93de3deef9023f52cba8e4783b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/truncate/ColumnTruncateTask.java",
                "sha": "2d29afcbac0fcc6ce7e9554ad78d2b98127103a5",
                "status": "modified"
            }
        ],
        "message": "HIVE-14109: query execuction throws NPE when hive.exec.submitviachild is set to true (Aihua Xu, reviewed by Sergio Pe\u00f1a)",
        "parent": "https://github.com/apache/hive/commit/72cea13e4d968fad86be733c1f1aa65aafbb1fc4",
        "patched_files": [
            "ExecDriver.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestExecDriver.java"
        ]
    },
    "hive_fb4141c": {
        "bug_id": "hive_fb4141c",
        "commit": "https://github.com/apache/hive/commit/fb4141cd83cc449e5fe980b43f14cdab981b0c8e",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hive/blob/fb4141cd83cc449e5fe980b43f14cdab981b0c8e/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriterFactory.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriterFactory.java?ref=fb4141cd83cc449e5fe980b43f14cdab981b0c8e",
                "deletions": 6,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriterFactory.java",
                "patch": "@@ -43,6 +43,7 @@\n import org.apache.hadoop.io.FloatWritable;\n import org.apache.hadoop.io.IntWritable;\n import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.NullWritable;\n import org.apache.hadoop.io.Text;\n \n /**\n@@ -112,11 +113,11 @@ public Object writeValue(ColumnVector column, int row) throws HiveException {\n       } else if (!lcv.noNulls && !lcv.isRepeating && !lcv.isNull[row]) {\n         return writeValue(lcv.vector[row]);\n       } else if (!lcv.noNulls && !lcv.isRepeating && lcv.isNull[row]) {\n-        return null;\n+        return NullWritable.get();\n       } else if (!lcv.noNulls && lcv.isRepeating && !lcv.isNull[0]) {\n         return writeValue(lcv.vector[0]);\n       } else if (!lcv.noNulls && lcv.isRepeating && lcv.isNull[0]) {\n-        return null;\n+        return NullWritable.get();\n       }\n       throw new HiveException(\n         String.format(\n@@ -140,11 +141,11 @@ public Object writeValue(ColumnVector column, int row) throws HiveException {\n       } else if (!dcv.noNulls && !dcv.isRepeating && !dcv.isNull[row]) {\n         return writeValue(dcv.vector[row]);\n       } else if (!dcv.noNulls && !dcv.isRepeating && dcv.isNull[row]) {\n-        return null;\n+        return NullWritable.get();\n       } else if (!dcv.noNulls && dcv.isRepeating && !dcv.isNull[0]) {\n         return writeValue(dcv.vector[0]);\n       } else if (!dcv.noNulls && dcv.isRepeating && dcv.isNull[0]) {\n-        return null;\n+        return NullWritable.get();\n       }\n       throw new HiveException(\n         String.format(\n@@ -168,11 +169,11 @@ public Object writeValue(ColumnVector column, int row) throws HiveException {\n       } else if (!bcv.noNulls && !bcv.isRepeating && !bcv.isNull[row]) {\n         return writeValue(bcv.vector[row], bcv.start[row], bcv.length[row]);\n       } else if (!bcv.noNulls && !bcv.isRepeating && bcv.isNull[row]) {\n-        return null;\n+        return NullWritable.get();\n       } else if (!bcv.noNulls && bcv.isRepeating && !bcv.isNull[0]) {\n         return writeValue(bcv.vector[0], bcv.start[0], bcv.length[0]);\n       } else if (!bcv.noNulls && bcv.isRepeating && bcv.isNull[0]) {\n-        return null;\n+        return NullWritable.get();\n       }\n       throw new HiveException(\n         String.format(",
                "raw_url": "https://github.com/apache/hive/raw/fb4141cd83cc449e5fe980b43f14cdab981b0c8e/ql/src/java/org/apache/hadoop/hive/ql/exec/vector/expressions/VectorExpressionWriterFactory.java",
                "sha": "8379385fc15ff8713c6713208971e2e0a729abbe",
                "status": "modified"
            },
            {
                "additions": 206,
                "blob_url": "https://github.com/apache/hive/blob/fb4141cd83cc449e5fe980b43f14cdab981b0c8e/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorExpressionWriters.java",
                "changes": 206,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorExpressionWriters.java?ref=fb4141cd83cc449e5fe980b43f14cdab981b0c8e",
                "deletions": 0,
                "filename": "ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorExpressionWriters.java",
                "patch": "@@ -0,0 +1,206 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.hive.ql.exec.vector.expressions;\n+\n+\n+import java.sql.Timestamp;\n+import java.util.Random;\n+\n+import junit.framework.Assert;\n+\n+import org.apache.hadoop.hive.ql.exec.vector.BytesColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.DoubleColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.LongColumnVector;\n+import org.apache.hadoop.hive.ql.exec.vector.TimestampUtils;\n+import org.apache.hadoop.hive.ql.exec.vector.util.VectorizedRowGroupGenUtil;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeColumnDesc;\n+import org.apache.hadoop.hive.ql.plan.ExprNodeDesc;\n+import org.apache.hadoop.hive.serde2.io.ByteWritable;\n+import org.apache.hadoop.hive.serde2.io.DoubleWritable;\n+import org.apache.hadoop.hive.serde2.io.ShortWritable;\n+import org.apache.hadoop.hive.serde2.io.TimestampWritable;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;\n+import org.apache.hadoop.io.BooleanWritable;\n+import org.apache.hadoop.io.FloatWritable;\n+import org.apache.hadoop.io.IntWritable;\n+import org.apache.hadoop.io.LongWritable;\n+import org.apache.hadoop.io.NullWritable;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.io.Writable;\n+import org.junit.Test;\n+\n+public class TestVectorExpressionWriters {\n+\n+  private final int vectorSize = 5;\n+\n+  private VectorExpressionWriter getWriter(TypeInfo colTypeInfo) throws HiveException {\n+    ExprNodeDesc columnDesc = new ExprNodeColumnDesc();\n+    columnDesc.setTypeInfo(colTypeInfo);\n+    VectorExpressionWriter vew = VectorExpressionWriterFactory\n+        .genVectorExpressionWritable(columnDesc);\n+    return vew;\n+  }\n+\n+  private Writable getWritableValue(TypeInfo ti, double value) {\n+    if (ti.equals(TypeInfoFactory.floatTypeInfo)) {\n+      return new FloatWritable((float) value);\n+    } else if (ti.equals(TypeInfoFactory.doubleTypeInfo)) {\n+      return new DoubleWritable(value);\n+    }\n+    return null;\n+  }\n+\n+  private Writable getWritableValue(TypeInfo ti, byte[] value) {\n+    if (ti.equals(TypeInfoFactory.stringTypeInfo)) {\n+      return new Text(value);\n+    }\n+    return null;\n+  }\n+\n+  private Writable getWritableValue(TypeInfo ti, long value) {\n+    if (ti.equals(TypeInfoFactory.byteTypeInfo)) {\n+      return new ByteWritable((byte) value);\n+    } else if (ti.equals(TypeInfoFactory.shortTypeInfo)) {\n+      return new ShortWritable((short) value);\n+    } else if (ti.equals(TypeInfoFactory.intTypeInfo)) {\n+      return new IntWritable( (int) value);\n+    } else if (ti.equals(TypeInfoFactory.longTypeInfo)) {\n+      return new LongWritable( (long) value);\n+    } else if (ti.equals(TypeInfoFactory.booleanTypeInfo)) {\n+      return new BooleanWritable( value == 0 ? false : true);\n+    } else if (ti.equals(TypeInfoFactory.timestampTypeInfo)) {\n+      Timestamp ts = new Timestamp(value);\n+      TimestampUtils.assignTimeInNanoSec(value, ts);\n+      TimestampWritable tw = new TimestampWritable(ts);\n+      return tw;\n+    }\n+    return null;\n+  }\n+\n+  private void testWriterDouble(TypeInfo type) throws HiveException {\n+    DoubleColumnVector dcv = VectorizedRowGroupGenUtil.generateDoubleColumnVector(true, false,\n+        this.vectorSize, new Random(10));\n+    dcv.isNull[2] = true;\n+    VectorExpressionWriter vew = getWriter(type);\n+    for (int i = 0; i < vectorSize; i++) {\n+      Writable w = (Writable) vew.writeValue(dcv, i);\n+      if (!(w instanceof NullWritable)) {\n+        Writable expected = getWritableValue(type, dcv.vector[i]);\n+        Assert.assertEquals(expected, w);\n+      } else {\n+        Assert.assertTrue(dcv.isNull[i]);\n+      }\n+    }\n+  }\n+\n+  private void testWriterLong(TypeInfo type) throws HiveException {\n+    LongColumnVector lcv = VectorizedRowGroupGenUtil.generateLongColumnVector(true, false,\n+        vectorSize, new Random(10));\n+    lcv.isNull[3] = true;\n+    VectorExpressionWriter vew = getWriter(type);\n+    for (int i = 0; i < vectorSize; i++) {\n+      Writable w = (Writable) vew.writeValue(lcv, i);\n+      if (!(w instanceof NullWritable)) {\n+        Writable expected = getWritableValue(type, lcv.vector[i]);\n+        if (expected instanceof TimestampWritable) {\n+          TimestampWritable t1 = (TimestampWritable) expected;\n+          TimestampWritable t2 = (TimestampWritable) w;\n+          Assert.assertTrue(t1.getNanos() == t2.getNanos());\n+          Assert.assertTrue(t1.getSeconds() == t2.getSeconds());\n+          continue;\n+        }\n+        Assert.assertEquals(expected, w);\n+      } else {\n+        Assert.assertTrue(lcv.isNull[i]);\n+      }\n+    }\n+  }\n+\n+  private void testWriterBytes(TypeInfo type) throws HiveException {\n+    Text t1 = new Text(\"alpha\");\n+    Text t2 = new Text(\"beta\");\n+    BytesColumnVector bcv = new BytesColumnVector(vectorSize);\n+    bcv.noNulls = false;\n+    bcv.initBuffer();\n+    bcv.setVal(0, t1.getBytes(), 0, t1.getLength());\n+    bcv.isNull[1] = true;\n+    bcv.setVal(2, t2.getBytes(), 0, t2.getLength());\n+    bcv.isNull[3] = true;\n+    bcv.setVal(4, t1.getBytes(), 0, t1.getLength());\n+    VectorExpressionWriter vew = getWriter(type);\n+    for (int i = 0; i < vectorSize; i++) {\n+      Writable w = (Writable) vew.writeValue(bcv, i);\n+      if (!(w instanceof NullWritable)) {\n+        byte [] val = new byte[bcv.length[i]];\n+        System.arraycopy(bcv.vector[i], bcv.start[i], val, 0, bcv.length[i]);\n+        Writable expected = getWritableValue(type, val);\n+        Assert.assertEquals(expected, w);\n+      } else {\n+        Assert.assertTrue(bcv.isNull[i]);\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testVectorExpressionWriterDouble() throws HiveException {\n+    testWriterDouble(TypeInfoFactory.doubleTypeInfo);\n+  }\n+\n+  @Test\n+  public void testVectorExpressionWriterFloat() throws HiveException {\n+    testWriterDouble(TypeInfoFactory.floatTypeInfo);\n+  }\n+\n+  @Test\n+  public void testVectorExpressionWriterLong() throws HiveException {\n+    testWriterLong(TypeInfoFactory.longTypeInfo);\n+  }\n+\n+  @Test\n+  public void testVectorExpressionWriterInt() throws HiveException {\n+    testWriterLong(TypeInfoFactory.intTypeInfo);\n+  }\n+\n+  @Test\n+  public void testVectorExpressionWriterShort() throws HiveException {\n+    testWriterLong(TypeInfoFactory.shortTypeInfo);\n+  }\n+\n+  @Test\n+  public void testVectorExpressionWriterBoolean() throws HiveException {\n+    testWriterLong(TypeInfoFactory.booleanTypeInfo);\n+  }\n+\n+  @Test\n+  public void testVectorExpressionWriterTimestamp() throws HiveException {\n+    testWriterLong(TypeInfoFactory.timestampTypeInfo);\n+  }\n+\n+  @Test\n+  public void testVectorExpressionWriterBye() throws HiveException {\n+    testWriterLong(TypeInfoFactory.byteTypeInfo);\n+  }\n+\n+  @Test\n+  public void testVectorExpressionWriterBytes() throws HiveException {\n+    testWriterBytes(TypeInfoFactory.stringTypeInfo);\n+  }\n+}",
                "raw_url": "https://github.com/apache/hive/raw/fb4141cd83cc449e5fe980b43f14cdab981b0c8e/ql/src/test/org/apache/hadoop/hive/ql/exec/vector/expressions/TestVectorExpressionWriters.java",
                "sha": "cde5b1f8e6850302b0e54d6e5927841217aad982",
                "status": "added"
            }
        ],
        "message": "HIVE-4688 : NPE in writing null values. (Jitendra Nath Pandey via Ashutosh Chauhan)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hive/branches/vectorization@1491154 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hive/commit/10814d9f04d9e500171452f77be50b7cbe8db84d",
        "patched_files": [
            "VectorExpressionWriterFactory.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestVectorExpressionWriters.java"
        ]
    },
    "hive_fb79870": {
        "bug_id": "hive_fb79870",
        "commit": "https://github.com/apache/hive/commit/fb79870592d775cd836d5611e21ab1c7030aadba",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/fb79870592d775cd836d5611e21ab1c7030aadba/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java?ref=fb79870592d775cd836d5611e21ab1c7030aadba",
                "deletions": 0,
                "filename": "common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
                "patch": "@@ -1843,6 +1843,7 @@ private static void populateLlapDaemonVarsSet(Set<String> llapDaemonVarsSetLocal\n     TESTMODE_BUCKET_CODEC_VERSION(\"hive.test.bucketcodec.version\", 1,\n       \"For testing only.  Will make ACID subsystem write RecordIdentifier.bucketId in specified\\n\" +\n         \"format\", false),\n+    HIVE_QUERY_TIMESTAMP(\"hive.query.timestamp\", System.currentTimeMillis(), \"query execute time.\"),\n \n     HIVEMERGEMAPFILES(\"hive.merge.mapfiles\", true,\n         \"Merge small files at the end of a map-only job\"),",
                "raw_url": "https://github.com/apache/hive/raw/fb79870592d775cd836d5611e21ab1c7030aadba/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java",
                "sha": "44b9eb2824c1d0c475fc56d8737e023513f49d78",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hive/blob/fb79870592d775cd836d5611e21ab1c7030aadba/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java?ref=fb79870592d775cd836d5611e21ab1c7030aadba",
                "deletions": 0,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java",
                "patch": "@@ -1924,6 +1924,7 @@ public String getNextValuesTempTableSuffix() {\n    */\n   public void setupQueryCurrentTimestamp() {\n     queryCurrentTimestamp = new Timestamp(System.currentTimeMillis());\n+    sessionConf.setLongVar(ConfVars.HIVE_QUERY_TIMESTAMP, queryCurrentTimestamp.getTime());\n \n     // Provide a facility to set current timestamp during tests\n     if (sessionConf.getBoolVar(ConfVars.HIVE_IN_TEST)) {",
                "raw_url": "https://github.com/apache/hive/raw/fb79870592d775cd836d5611e21ab1c7030aadba/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java",
                "sha": "9f65a771f95a7c0bd3fdb4e56e47c0fc70235850",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hive/blob/fb79870592d775cd836d5611e21ab1c7030aadba/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentDate.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentDate.java?ref=fb79870592d775cd836d5611e21ab1c7030aadba",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentDate.java",
                "patch": "@@ -18,8 +18,12 @@\n package org.apache.hadoop.hive.ql.udf.generic;\n \n import java.sql.Date;\n+import java.sql.Timestamp;\n \n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.exec.Description;\n+import org.apache.hadoop.hive.ql.exec.MapredContext;\n import org.apache.hadoop.hive.ql.exec.UDFArgumentException;\n import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n@@ -39,6 +43,13 @@\n public class GenericUDFCurrentDate extends GenericUDF {\n \n   protected DateWritable currentDate;\n+  private Configuration conf;\n+\n+  @Override\n+  public void configure(MapredContext context) {\n+    super.configure(context);\n+    conf = context.getJobConf();\n+  }\n \n   @Override\n   public ObjectInspector initialize(ObjectInspector[] arguments)\n@@ -50,8 +61,21 @@ public ObjectInspector initialize(ObjectInspector[] arguments)\n     }\n \n     if (currentDate == null) {\n+      SessionState ss = SessionState.get();\n+      Timestamp queryTimestamp;\n+      if (ss == null) {\n+        if (conf == null) {\n+          queryTimestamp = new Timestamp(System.currentTimeMillis());\n+        } else {\n+          queryTimestamp = new Timestamp(\n+                  HiveConf.getLongVar(conf, HiveConf.ConfVars.HIVE_QUERY_TIMESTAMP));\n+        }\n+      } else {\n+        queryTimestamp = ss.getQueryCurrentTimestamp();\n+      }\n+\n       Date dateVal =\n-          Date.valueOf(SessionState.get().getQueryCurrentTimestamp().toString().substring(0, 10));\n+              Date.valueOf(queryTimestamp.toString().substring(0, 10));\n       currentDate = new DateWritable(dateVal);\n     }\n ",
                "raw_url": "https://github.com/apache/hive/raw/fb79870592d775cd836d5611e21ab1c7030aadba/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentDate.java",
                "sha": "91fd08f13e5cdc28cc80acffea0599e14a45a96e",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hive/blob/fb79870592d775cd836d5611e21ab1c7030aadba/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentTimestamp.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentTimestamp.java?ref=fb79870592d775cd836d5611e21ab1c7030aadba",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentTimestamp.java",
                "patch": "@@ -17,7 +17,12 @@\n  */\n package org.apache.hadoop.hive.ql.udf.generic;\n \n+import java.sql.Timestamp;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n import org.apache.hadoop.hive.ql.exec.Description;\n+import org.apache.hadoop.hive.ql.exec.MapredContext;\n import org.apache.hadoop.hive.ql.exec.UDFArgumentException;\n import org.apache.hadoop.hive.ql.exec.UDFArgumentLengthException;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n@@ -37,6 +42,13 @@\n public class GenericUDFCurrentTimestamp extends GenericUDF {\n \n   protected TimestampWritable currentTimestamp;\n+  private Configuration conf;\n+\n+  @Override\n+  public void configure(MapredContext context) {\n+    super.configure(context);\n+    conf = context.getJobConf();\n+  }\n \n   @Override\n   public ObjectInspector initialize(ObjectInspector[] arguments)\n@@ -48,7 +60,19 @@ public ObjectInspector initialize(ObjectInspector[] arguments)\n     }\n \n     if (currentTimestamp == null) {\n-      currentTimestamp = new TimestampWritable(SessionState.get().getQueryCurrentTimestamp());\n+      SessionState ss = SessionState.get();\n+      Timestamp queryTimestamp;\n+      if (ss == null) {\n+        if (conf == null) {\n+          queryTimestamp = new Timestamp(System.currentTimeMillis());\n+        } else {\n+          queryTimestamp = new Timestamp(\n+                  HiveConf.getLongVar(conf, HiveConf.ConfVars.HIVE_QUERY_TIMESTAMP));\n+        }\n+      } else {\n+        queryTimestamp = ss.getQueryCurrentTimestamp();\n+      }\n+      currentTimestamp = new TimestampWritable(queryTimestamp);\n     }\n \n     return PrimitiveObjectInspectorFactory.writableTimestampObjectInspector;",
                "raw_url": "https://github.com/apache/hive/raw/fb79870592d775cd836d5611e21ab1c7030aadba/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFCurrentTimestamp.java",
                "sha": "ca43840e372a26accda20386ef4c8679310783fe",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hive/blob/fb79870592d775cd836d5611e21ab1c7030aadba/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUnixTimeStamp.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUnixTimeStamp.java?ref=fb79870592d775cd836d5611e21ab1c7030aadba",
                "deletions": 1,
                "filename": "ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUnixTimeStamp.java",
                "patch": "@@ -18,6 +18,11 @@\n \n package org.apache.hadoop.hive.ql.udf.generic;\n \n+import java.sql.Timestamp;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.hadoop.hive.ql.exec.MapredContext;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import org.apache.hadoop.hive.ql.exec.Description;\n@@ -37,14 +42,34 @@\n public class GenericUDFUnixTimeStamp extends GenericUDFToUnixTimeStamp {\n   private static final Logger LOG = LoggerFactory.getLogger(GenericUDFUnixTimeStamp.class);\n   private LongWritable currentTimestamp; // retValue is transient so store this separately.\n+  private Configuration conf;\n+\n+  @Override\n+  public void configure(MapredContext context) {\n+    super.configure(context);\n+    conf = context.getJobConf();\n+  }\n+\n   @Override\n   protected void initializeInput(ObjectInspector[] arguments) throws UDFArgumentException {\n     if (arguments.length > 0) {\n       super.initializeInput(arguments);\n     } else {\n       if (currentTimestamp == null) {\n         currentTimestamp = new LongWritable(0);\n-        setValueFromTs(currentTimestamp, SessionState.get().getQueryCurrentTimestamp());\n+        SessionState ss = SessionState.get();\n+        Timestamp queryTimestamp;\n+        if (ss == null) {\n+          if (conf == null) {\n+            queryTimestamp = new Timestamp(System.currentTimeMillis());\n+          } else {\n+            queryTimestamp = new Timestamp(\n+                    HiveConf.getLongVar(conf, HiveConf.ConfVars.HIVE_QUERY_TIMESTAMP));\n+          }\n+        } else {\n+          queryTimestamp = ss.getQueryCurrentTimestamp();\n+        }\n+        setValueFromTs(currentTimestamp, queryTimestamp);\n         String msg = \"unix_timestamp(void) is deprecated. Use current_timestamp instead.\";\n         SessionState.getConsole().printInfo(msg, false);\n       }",
                "raw_url": "https://github.com/apache/hive/raw/fb79870592d775cd836d5611e21ab1c7030aadba/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFUnixTimeStamp.java",
                "sha": "6ce72f77037d49571eb1bc5fb647bed0559119cf",
                "status": "modified"
            }
        ],
        "message": "HIVE-13745: UDF current_date\u3001current_timestamp\u3001unix_timestamp NPE (Biao Wu, reviewed by Yongzhi Chen)",
        "parent": "https://github.com/apache/hive/commit/68b66a64f0d9b0d587a7ce1e085a0e8e45253adb",
        "patched_files": [
            "SessionState.java",
            "HiveConf.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestSessionState.java",
            "TestHiveConf.java"
        ]
    },
    "hive_fbbb7cf": {
        "bug_id": "hive_fbbb7cf",
        "commit": "https://github.com/apache/hive/commit/fbbb7cf1fa5691037243a6db3993f294ffb00eeb",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hive/blob/fbbb7cf1fa5691037243a6db3993f294ffb00eeb/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java?ref=fbbb7cf1fa5691037243a6db3993f294ffb00eeb",
                "deletions": 2,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "patch": "@@ -4281,8 +4281,8 @@ public TableStatsResult get_table_statistics_req(TableStatsRequest request)\n       }\n       try {\n         ColumnStatistics cs = getMS().getTableColumnStatistics(dbName, tblName, lowerCaseColNames);\n-        result = new TableStatsResult(\n-            cs == null ? Lists.<ColumnStatisticsObj>newArrayList() : cs.getStatsObj());\n+        result = new TableStatsResult((cs == null || cs.getStatsObj() == null)\n+            ? Lists.<ColumnStatisticsObj>newArrayList() : cs.getStatsObj());\n       } finally {\n         endFunction(\"get_table_statistics_req: \", result == null, null, tblName);\n       }",
                "raw_url": "https://github.com/apache/hive/raw/fbbb7cf1fa5691037243a6db3993f294ffb00eeb/metastore/src/java/org/apache/hadoop/hive/metastore/HiveMetaStore.java",
                "sha": "df64124536a576300b579a20e34a706d4b4cbdcd",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hive/blob/fbbb7cf1fa5691037243a6db3993f294ffb00eeb/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hive/contents/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java?ref=fbbb7cf1fa5691037243a6db3993f294ffb00eeb",
                "deletions": 0,
                "filename": "metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java",
                "patch": "@@ -34,6 +34,7 @@\n import org.apache.hadoop.hive.metastore.Warehouse;\n import org.apache.hadoop.hive.metastore.api.AggrStats;\n import org.apache.hadoop.hive.metastore.api.ColumnStatistics;\n+import org.apache.hadoop.hive.metastore.api.ColumnStatisticsObj;\n import org.apache.hadoop.hive.metastore.api.CurrentNotificationEventId;\n import org.apache.hadoop.hive.metastore.api.Database;\n import org.apache.hadoop.hive.metastore.api.FieldSchema;\n@@ -1693,9 +1694,11 @@ public AggrStats get_aggr_stats_for(String dbName, String tblName, List<String>\n       partVals.add(partNameToVals(partName));\n     }\n     boolean commit = false;\n+    boolean hasAnyStats = false;\n     openTransaction();\n     try {\n       AggrStats aggrStats = new AggrStats();\n+      aggrStats.setPartsFound(0);\n       for (String colName : colNames) {\n         try {\n           AggrStats oneCol =\n@@ -1704,6 +1707,7 @@ public AggrStats get_aggr_stats_for(String dbName, String tblName, List<String>\n             assert oneCol.getColStatsSize() == 1;\n             aggrStats.setPartsFound(oneCol.getPartsFound());\n             aggrStats.addToColStats(oneCol.getColStats().get(0));\n+            hasAnyStats = true;\n           }\n         } catch (CacheLoader.InvalidCacheLoadException e) {\n           LOG.debug(\"Found no stats for column \" + colName);\n@@ -1712,6 +1716,10 @@ public AggrStats get_aggr_stats_for(String dbName, String tblName, List<String>\n         }\n       }\n       commit = true;\n+      if (!hasAnyStats) {\n+        // Set the required field.\n+        aggrStats.setColStats(new ArrayList<ColumnStatisticsObj>());\n+      }\n       return aggrStats;\n     } catch (IOException e) {\n       LOG.error(\"Unable to fetch aggregate column statistics\", e);",
                "raw_url": "https://github.com/apache/hive/raw/fbbb7cf1fa5691037243a6db3993f294ffb00eeb/metastore/src/java/org/apache/hadoop/hive/metastore/hbase/HBaseStore.java",
                "sha": "df0fac31ad2bce2a49626c6cc8305496a8629030",
                "status": "modified"
            }
        ],
        "message": "HIVE-11636 NPE in stats conversion with HBase metastore (Sergey Shelukhin via gates)",
        "parent": "https://github.com/apache/hive/commit/e150af9457079c87c267094f3861528286e951ea",
        "patched_files": [
            "HiveMetaStore.java",
            "HBaseStore.java"
        ],
        "repo": "hive",
        "unit_tests": [
            "TestHiveMetaStore.java",
            "TestHBaseStore.java"
        ]
    }
}