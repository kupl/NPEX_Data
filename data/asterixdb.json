{
    "asterixdb_0d0a113": {
        "bug_id": "asterixdb_0d0a113",
        "commit": "https://github.com/apache/asterixdb/commit/0d0a113f40014384bcf54b68235581286c9b2c2b",
        "file": [
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/asterixdb/asterix-app/src/main/java/org/apache/asterix/api/common/AsterixHyracksIntegrationUtil.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/main/java/org/apache/asterix/api/common/AsterixHyracksIntegrationUtil.java?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 1,
                "filename": "asterixdb/asterix-app/src/main/java/org/apache/asterix/api/common/AsterixHyracksIntegrationUtil.java",
                "patch": "@@ -64,6 +64,8 @@ private LoggerHolder() {\n     public NodeControllerService[] ncs = new NodeControllerService[0];\n     public IHyracksClientConnection hcc;\n \n+    private static final String DEFAULT_STORAGE_PATH = joinPath(\"target\", \"io\", \"dir\");\n+    private static String storagePath = DEFAULT_STORAGE_PATH;\n     private ConfigManager configManager;\n     private List<String> nodeNames;\n \n@@ -217,8 +219,16 @@ public void stopCC(boolean terminateNCService) throws Exception {\n         }\n     }\n \n+    public static void setStoragePath(String path) {\n+        storagePath = path;\n+    }\n+\n+    public static void restoreDefaultStoragePath() {\n+        storagePath = DEFAULT_STORAGE_PATH;\n+    }\n+\n     protected String getDefaultStoragePath() {\n-        return joinPath(\"target\", \"io\", \"dir\");\n+        return storagePath;\n     }\n \n     public void removeTestStorageFiles() {",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/asterixdb/asterix-app/src/main/java/org/apache/asterix/api/common/AsterixHyracksIntegrationUtil.java",
                "sha": "279976548d3e77bd3cef1dd7c563f1398fd42443",
                "status": "modified"
            },
            {
                "additions": 173,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/asterixdb/asterix-app/src/test/java/org/apache/asterix/test/storage/DiskIsFullTest.java",
                "changes": 173,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/test/java/org/apache/asterix/test/storage/DiskIsFullTest.java?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 0,
                "filename": "asterixdb/asterix-app/src/test/java/org/apache/asterix/test/storage/DiskIsFullTest.java",
                "patch": "@@ -0,0 +1,173 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.asterix.test.storage;\n+\n+import static org.apache.hyracks.util.StorageUtil.StorageUnit.MEGABYTE;\n+\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import org.apache.asterix.api.common.AsterixHyracksIntegrationUtil;\n+import org.apache.asterix.app.bootstrap.TestNodeController;\n+import org.apache.asterix.app.data.gen.TupleGenerator;\n+import org.apache.asterix.app.data.gen.TupleGenerator.GenerationFunction;\n+import org.apache.asterix.common.config.DatasetConfig.DatasetType;\n+import org.apache.asterix.common.dataflow.LSMInsertDeleteOperatorNodePushable;\n+import org.apache.asterix.common.exceptions.ExceptionUtils;\n+import org.apache.asterix.common.transactions.ITransactionContext;\n+import org.apache.asterix.external.util.DataflowUtils;\n+import org.apache.asterix.file.StorageComponentProvider;\n+import org.apache.asterix.metadata.entities.Dataset;\n+import org.apache.asterix.metadata.entities.Index;\n+import org.apache.asterix.metadata.entities.InternalDatasetDetails;\n+import org.apache.asterix.metadata.entities.InternalDatasetDetails.PartitioningStrategy;\n+import org.apache.asterix.om.types.ARecordType;\n+import org.apache.asterix.om.types.BuiltinType;\n+import org.apache.asterix.om.types.IAType;\n+import org.apache.asterix.test.common.TestHelper;\n+import org.apache.commons.lang3.SystemUtils;\n+import org.apache.hyracks.api.comm.VSizeFrame;\n+import org.apache.hyracks.api.context.IHyracksTaskContext;\n+import org.apache.hyracks.api.exceptions.ErrorCode;\n+import org.apache.hyracks.api.exceptions.HyracksDataException;\n+import org.apache.hyracks.dataflow.common.comm.io.FrameTupleAppender;\n+import org.apache.hyracks.dataflow.common.data.accessors.ITupleReference;\n+import org.apache.hyracks.storage.am.lsm.common.impls.NoMergePolicyFactory;\n+import org.apache.hyracks.util.DiskUtil;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+public class DiskIsFullTest {\n+\n+    private static final IAType[] KEY_TYPES = { BuiltinType.AINT32 };\n+    private static final ARecordType RECORD_TYPE = new ARecordType(\"TestRecordType\", new String[] { \"key\", \"value\" },\n+            new IAType[] { BuiltinType.AINT32, BuiltinType.AINT64 }, false);\n+    private static final GenerationFunction[] RECORD_GEN_FUNCTION =\n+            { GenerationFunction.DETERMINISTIC, GenerationFunction.DETERMINISTIC };\n+    private static final boolean[] UNIQUE_RECORD_FIELDS = { true, false };\n+    private static final ARecordType META_TYPE = null;\n+    private static final GenerationFunction[] META_GEN_FUNCTION = null;\n+    private static final boolean[] UNIQUE_META_FIELDS = null;\n+    private static final int[] KEY_INDEXES = { 0 };\n+    private static final int[] KEY_INDICATOR = { Index.RECORD_INDICATOR };\n+    private static final List<Integer> KEY_INDICATOR_LIST = Arrays.asList(new Integer[] { Index.RECORD_INDICATOR });\n+    private static final int DATASET_ID = 101;\n+    private static final String DATAVERSE_NAME = \"TestDV\";\n+    private static final String DATASET_NAME = \"TestDS\";\n+    private static final String DATA_TYPE_NAME = \"DUMMY\";\n+    private static final String NODE_GROUP_NAME = \"DEFAULT\";\n+    private static final String TEST_DISK_NAME = \"asterixdb_ram_disk\";\n+    private boolean shouldRun = true;\n+\n+    @Before\n+    public void setUp() throws Exception {\n+        if (!SystemUtils.IS_OS_MAC) {\n+            System.out.println(\"Skipping test \" + DiskIsFullTest.class.getName() + \" due to unsupported OS\");\n+            shouldRun = false;\n+            return;\n+        }\n+        System.out.println(\"SetUp: \");\n+        TestHelper.deleteExistingInstanceFiles();\n+        // create RAM disk\n+        final Path ramDiskRoot = DiskUtil.mountRamDisk(TEST_DISK_NAME, 4, MEGABYTE);\n+        // Use RAM disk for storage\n+        AsterixHyracksIntegrationUtil.setStoragePath(ramDiskRoot.toAbsolutePath().toString());\n+    }\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (!shouldRun) {\n+            return;\n+        }\n+        System.out.println(\"TearDown\");\n+        TestHelper.deleteExistingInstanceFiles();\n+        DiskUtil.unmountRamDisk(TEST_DISK_NAME);\n+        AsterixHyracksIntegrationUtil.restoreDefaultStoragePath();\n+    }\n+\n+    @Test\n+    public void testDiskIsFull() {\n+        if (!shouldRun) {\n+            return;\n+        }\n+        HyracksDataException expectedException =\n+                HyracksDataException.create(ErrorCode.CANNOT_MODIFY_INDEX_DISK_IS_FULL);\n+        try {\n+            TestNodeController nc = new TestNodeController(null, false);\n+            nc.init();\n+            StorageComponentProvider storageManager = new StorageComponentProvider();\n+            List<List<String>> partitioningKeys = new ArrayList<>();\n+            partitioningKeys.add(Collections.singletonList(\"key\"));\n+            Dataset dataset =\n+                    new Dataset(DATAVERSE_NAME, DATASET_NAME, DATAVERSE_NAME, DATA_TYPE_NAME, NODE_GROUP_NAME, null,\n+                            null,\n+                            new InternalDatasetDetails(null, PartitioningStrategy.HASH, partitioningKeys, null, null,\n+                                    null, false, null, false), null, DatasetType.INTERNAL, DATASET_ID, 0);\n+            try {\n+                nc.createPrimaryIndex(dataset, KEY_TYPES, RECORD_TYPE, META_TYPE, new NoMergePolicyFactory(), null,\n+                        null, storageManager, KEY_INDEXES, KEY_INDICATOR_LIST);\n+                IHyracksTaskContext ctx = nc.createTestContext(false);\n+                nc.newJobId();\n+                ITransactionContext txnCtx = nc.getTransactionManager().getTransactionContext(nc.getTxnJobId(), true);\n+                // Prepare insert operation\n+                LSMInsertDeleteOperatorNodePushable insertOp =\n+                        nc.getInsertPipeline(ctx, dataset, KEY_TYPES, RECORD_TYPE, META_TYPE,\n+                                new NoMergePolicyFactory(), null, null, KEY_INDEXES, KEY_INDICATOR_LIST, storageManager)\n+                                .getLeft();\n+                insertOp.open();\n+                TupleGenerator tupleGenerator =\n+                        new TupleGenerator(RECORD_TYPE, META_TYPE, KEY_INDEXES, KEY_INDICATOR, RECORD_GEN_FUNCTION,\n+                                UNIQUE_RECORD_FIELDS, META_GEN_FUNCTION, UNIQUE_META_FIELDS);\n+                VSizeFrame frame = new VSizeFrame(ctx);\n+                FrameTupleAppender tupleAppender = new FrameTupleAppender(frame);\n+                // Insert records until disk becomes full\n+                int tupleCount = 100000;\n+                while (tupleCount > 0) {\n+                    ITupleReference tuple = tupleGenerator.next();\n+                    try {\n+                        DataflowUtils.addTupleToFrame(tupleAppender, tuple, insertOp);\n+                    } catch (Throwable t) {\n+                        final Throwable rootCause = ExceptionUtils.getRootCause(t);\n+                        rootCause.printStackTrace();\n+                        if (rootCause instanceof HyracksDataException) {\n+                            HyracksDataException cause = (HyracksDataException) rootCause;\n+                            Assert.assertEquals(cause.getErrorCode(), expectedException.getErrorCode());\n+                            Assert.assertEquals(cause.getMessage(), expectedException.getMessage());\n+                            return;\n+                        } else {\n+                            break;\n+                        }\n+                    }\n+                    tupleCount--;\n+                }\n+                Assert.fail(\"Expected exception (\" + expectedException + \") was not thrown\");\n+            } finally {\n+                nc.deInit();\n+            }\n+        } catch (Throwable e) {\n+            e.printStackTrace();\n+            Assert.fail(\"Expected exception (\" + expectedException + \") was not thrown\");\n+        }\n+    }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/asterixdb/asterix-app/src/test/java/org/apache/asterix/test/storage/DiskIsFullTest.java",
                "sha": "58697a99bc9fa19b1d59e2aadad8931a7a619208",
                "status": "added"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/asterixdb/asterix-transactions/src/main/java/org/apache/asterix/transaction/management/resource/PersistentLocalResourceRepository.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-transactions/src/main/java/org/apache/asterix/transaction/management/resource/PersistentLocalResourceRepository.java?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 3,
                "filename": "asterixdb/asterix-transactions/src/main/java/org/apache/asterix/transaction/management/resource/PersistentLocalResourceRepository.java",
                "patch": "@@ -18,6 +18,8 @@\n  */\n package org.apache.asterix.transaction.management.resource;\n \n+import static org.apache.hyracks.api.exceptions.ErrorCode.CANNOT_CREATE_FILE;\n+\n import java.io.File;\n import java.io.FileInputStream;\n import java.io.FileOutputStream;\n@@ -190,10 +192,12 @@ public synchronized void insert(LocalResource resource) throws HyracksDataExcept\n         FileReference resourceFile = ioManager.resolve(relativePath);\n         if (resourceFile.getFile().exists()) {\n             throw new HyracksDataException(\"Duplicate resource: \" + resourceFile.getAbsolutePath());\n-        } else {\n-            resourceFile.getFile().getParentFile().mkdirs();\n         }\n-        resourceCache.put(resource.getPath(), resource);\n+\n+        final File parent = resourceFile.getFile().getParentFile();\n+        if (!parent.exists() && !parent.mkdirs()) {\n+            throw HyracksDataException.create(CANNOT_CREATE_FILE, parent.getAbsolutePath());\n+        }\n \n         try (FileOutputStream fos = new FileOutputStream(resourceFile.getFile());\n                 ObjectOutputStream oosToFos = new ObjectOutputStream(fos)) {\n@@ -203,6 +207,8 @@ public synchronized void insert(LocalResource resource) throws HyracksDataExcept\n             throw new HyracksDataException(e);\n         }\n \n+        resourceCache.put(resource.getPath(), resource);\n+\n         //if replication enabled, send resource metadata info to remote nodes\n         if (isReplicationEnabled) {\n             createReplicationJob(ReplicationOperation.REPLICATE, resourceFile);",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/asterixdb/asterix-transactions/src/main/java/org/apache/asterix/transaction/management/resource/PersistentLocalResourceRepository.java",
                "sha": "b117cf1d6d1f40ae4558e53fad0489926833d778",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/exceptions/ErrorCode.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/exceptions/ErrorCode.java?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 0,
                "filename": "hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/exceptions/ErrorCode.java",
                "patch": "@@ -121,6 +121,7 @@\n     public static final int FOUND_MULTIPLE_TRANSACTIONS = 85;\n     public static final int UNRECOGNIZED_INDEX_COMPONENT_FILE = 86;\n     public static final int UNEQUAL_NUM_FILTERS_TREES = 87;\n+    public static final int CANNOT_MODIFY_INDEX_DISK_IS_FULL = 88;\n \n     // Compilation error codes.\n     public static final int RULECOLLECTION_NOT_INSTANCE_OF_LIST = 10000;",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/exceptions/ErrorCode.java",
                "sha": "e6fbc6f265cc4e0a0971d40b1db520e772bf3bc7",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-api/src/main/resources/errormsg/en.properties",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-api/src/main/resources/errormsg/en.properties?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 0,
                "filename": "hyracks-fullstack/hyracks/hyracks-api/src/main/resources/errormsg/en.properties",
                "patch": "@@ -104,5 +104,6 @@\n 85 = Found more than one transaction file in %1$s\n 86 = Found an unrecognized index file %1$s\n 87 = Unequal number of trees and filters found in %1$s\n+88 = Cannot modify index (Disk is full)\n \n 10000 = The given rule collection %1$s is not an instance of the List class.",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-api/src/main/resources/errormsg/en.properties",
                "sha": "d2e05e381c8bded26478c7abebfaffd4a3a2d1d1",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-dataflow-std/src/main/java/org/apache/hyracks/dataflow/std/misc/MaterializerTaskState.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-dataflow-std/src/main/java/org/apache/hyracks/dataflow/std/misc/MaterializerTaskState.java?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 10,
                "filename": "hyracks-fullstack/hyracks/hyracks-dataflow-std/src/main/java/org/apache/hyracks/dataflow/std/misc/MaterializerTaskState.java",
                "patch": "@@ -66,28 +66,35 @@ public void open(IHyracksTaskContext ctx) throws HyracksDataException {\n     }\n \n     public void close() throws HyracksDataException {\n-        out.close();\n+        if (out != null) {\n+            out.close();\n+        }\n     }\n \n     public void appendFrame(ByteBuffer buffer) throws HyracksDataException {\n         out.nextFrame(buffer);\n     }\n \n     public void writeOut(IFrameWriter writer, IFrame frame, boolean failed) throws HyracksDataException {\n-        RunFileReader in = out.createReader();\n+        RunFileReader in = null;\n+        if (out != null) {\n+            in = out.createReader();\n+        }\n         writer.open();\n         try {\n             if (failed) {\n                 writer.fail();\n                 return;\n             }\n-            in.open();\n-            try {\n-                while (in.nextFrame(frame)) {\n-                    writer.nextFrame(frame.getBuffer());\n+            if (in != null) {\n+                in.open();\n+                try {\n+                    while (in.nextFrame(frame)) {\n+                        writer.nextFrame(frame.getBuffer());\n+                    }\n+                } finally {\n+                    in.close();\n                 }\n-            } finally {\n-                in.close();\n             }\n         } catch (Exception e) {\n             writer.fail();\n@@ -96,10 +103,10 @@ public void writeOut(IFrameWriter writer, IFrame frame, boolean failed) throws H\n             try {\n                 writer.close();\n             } finally {\n-                if (numConsumers.decrementAndGet() == 0) {\n+                if (numConsumers.decrementAndGet() == 0 && out != null) {\n                     out.getFileReference().delete();\n                 }\n             }\n         }\n     }\n-}\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-dataflow-std/src/main/java/org/apache/hyracks/dataflow/std/misc/MaterializerTaskState.java",
                "sha": "31cbaad111562a950b3ec3a486552d0f24bcb6f1",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-storage-am-lsm-common/src/main/java/org/apache/hyracks/storage/am/lsm/common/impls/AbstractLSMMemoryComponent.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-storage-am-lsm-common/src/main/java/org/apache/hyracks/storage/am/lsm/common/impls/AbstractLSMMemoryComponent.java?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 0,
                "filename": "hyracks-fullstack/hyracks/hyracks-storage-am-lsm-common/src/main/java/org/apache/hyracks/storage/am/lsm/common/impls/AbstractLSMMemoryComponent.java",
                "patch": "@@ -144,6 +144,11 @@ public void threadExit(LSMOperationType opType, boolean failedOperation, boolean\n                     throw new IllegalStateException(\"Flush sees an illegal LSM memory compoenent state: \" + state);\n                 }\n                 readerCount--;\n+                if (failedOperation) {\n+                    // if flush failed, return the component state to READABLE_UNWRITABLE\n+                    state = ComponentState.READABLE_UNWRITABLE;\n+                    return;\n+                }\n                 if (readerCount == 0) {\n                     state = ComponentState.INACTIVE;\n                 } else {",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-storage-am-lsm-common/src/main/java/org/apache/hyracks/storage/am/lsm/common/impls/AbstractLSMMemoryComponent.java",
                "sha": "1ee68d991223fdb082b677286173c437d1cd243f",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-storage-am-lsm-common/src/main/java/org/apache/hyracks/storage/am/lsm/common/impls/LSMHarness.java",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-storage-am-lsm-common/src/main/java/org/apache/hyracks/storage/am/lsm/common/impls/LSMHarness.java?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 2,
                "filename": "hyracks-fullstack/hyracks/hyracks-storage-am-lsm-common/src/main/java/org/apache/hyracks/storage/am/lsm/common/impls/LSMHarness.java",
                "patch": "@@ -47,6 +47,7 @@\n import org.apache.hyracks.storage.am.lsm.common.api.ILSMIndex;\n import org.apache.hyracks.storage.am.lsm.common.api.ILSMIndexAccessor;\n import org.apache.hyracks.storage.am.lsm.common.api.ILSMIndexOperationContext;\n+import org.apache.hyracks.storage.am.lsm.common.api.ILSMMemoryComponent;\n import org.apache.hyracks.storage.am.lsm.common.api.ILSMMergePolicy;\n import org.apache.hyracks.storage.am.lsm.common.api.ILSMOperationTracker;\n import org.apache.hyracks.storage.am.lsm.common.api.LSMOperationType;\n@@ -131,6 +132,10 @@ protected boolean getAndEnterComponents(ILSMIndexOperationContext ctx, LSMOperat\n                     // Flush and merge operations should never reach this wait call, because they are always try operations.\n                     // If they fail to enter the components, then it means that there are an ongoing flush/merge operation on\n                     // the same components, so they should not proceed.\n+                    if (opType == LSMOperationType.MODIFICATION) {\n+                        // before waiting, make sure the index is in a modifiable state to avoid waiting forever.\n+                        ensureIndexModifiable();\n+                    }\n                     opTracker.wait();\n                 } catch (InterruptedException e) {\n                     throw new HyracksDataException(e);\n@@ -186,6 +191,7 @@ protected boolean enterComponents(ILSMIndexOperationContext ctx, LSMOperationTyp\n                 break;\n             case MERGE:\n                 lsmIndex.getIOOperationCallback().beforeOperation(LSMOperationType.MERGE);\n+                break;\n             default:\n                 break;\n         }\n@@ -498,15 +504,17 @@ public void flush(ILSMIndexOperationContext ctx, ILSMIOOperation operation) thro\n         }\n \n         ILSMDiskComponent newComponent = null;\n+        boolean failedOperation = false;\n         try {\n             newComponent = lsmIndex.flush(operation);\n             operation.getCallback().afterOperation(LSMOperationType.FLUSH, null, newComponent);\n             lsmIndex.markAsValid(newComponent);\n         } catch (Throwable e) {\n+            failedOperation = true;\n             e.printStackTrace();\n             throw e;\n         } finally {\n-            exitComponents(ctx, LSMOperationType.FLUSH, newComponent, false);\n+            exitComponents(ctx, LSMOperationType.FLUSH, newComponent, failedOperation);\n             operation.getCallback().afterFinalize(LSMOperationType.FLUSH, newComponent);\n         }\n         if (LOGGER.isLoggable(Level.INFO)) {\n@@ -545,15 +553,17 @@ public void merge(ILSMIndexOperationContext ctx, ILSMIOOperation operation) thro\n         }\n \n         ILSMDiskComponent newComponent = null;\n+        boolean failedOperation = false;\n         try {\n             newComponent = lsmIndex.merge(operation);\n             operation.getCallback().afterOperation(LSMOperationType.MERGE, ctx.getComponentHolder(), newComponent);\n             lsmIndex.markAsValid(newComponent);\n         } catch (Throwable e) {\n+            failedOperation = true;\n             e.printStackTrace();\n             throw e;\n         } finally {\n-            exitComponents(ctx, LSMOperationType.MERGE, newComponent, false);\n+            exitComponents(ctx, LSMOperationType.MERGE, newComponent, failedOperation);\n             operation.getCallback().afterFinalize(LSMOperationType.MERGE, newComponent);\n         }\n         if (LOGGER.isLoggable(Level.INFO)) {\n@@ -660,4 +670,23 @@ public void batchOperate(ILSMIndexOperationContext ctx, FrameTupleAccessor acces\n             exit(ctx);\n         }\n     }\n+\n+    /***\n+     * Ensures the index is in a modifiable state\n+     * @throws HyracksDataException if the index is not in a modifiable state\n+     */\n+    private void ensureIndexModifiable() throws HyracksDataException {\n+        // find if there is any memory component which is in a writable state or eventually will be in a writable state\n+        for (ILSMMemoryComponent memoryComponent : lsmIndex.getMemoryComponents()) {\n+            switch (memoryComponent.getState()) {\n+                case INACTIVE:\n+                case READABLE_WRITABLE:\n+                case READABLE_UNWRITABLE_FLUSHING:\n+                    return;\n+                default:\n+                    // continue to the next component\n+            }\n+        }\n+        throw HyracksDataException.create(ErrorCode.CANNOT_MODIFY_INDEX_DISK_IS_FULL);\n+    }\n }",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-storage-am-lsm-common/src/main/java/org/apache/hyracks/storage/am/lsm/common/impls/LSMHarness.java",
                "sha": "8ff907a30f74e38f4972f24ee85a9408570f034f",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-util/pom.xml",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-util/pom.xml?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 0,
                "filename": "hyracks-fullstack/hyracks/hyracks-util/pom.xml",
                "patch": "@@ -64,6 +64,10 @@\n       <groupId>com.fasterxml.jackson.core</groupId>\n       <artifactId>jackson-core</artifactId>\n     </dependency>\n+    <dependency>\n+      <groupId>org.apache.commons</groupId>\n+      <artifactId>commons-lang3</artifactId>\n+    </dependency>\n   </dependencies>\n \n </project>",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-util/pom.xml",
                "sha": "3b03fce8a5dfdd305d7e8caafab865051c5d959c",
                "status": "modified"
            },
            {
                "additions": 126,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-util/src/main/java/org/apache/hyracks/util/DiskUtil.java",
                "changes": 126,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-util/src/main/java/org/apache/hyracks/util/DiskUtil.java?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 0,
                "filename": "hyracks-fullstack/hyracks/hyracks-util/src/main/java/org/apache/hyracks/util/DiskUtil.java",
                "patch": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.hyracks.util;\n+\n+import java.io.BufferedReader;\n+import java.io.IOException;\n+import java.io.InputStreamReader;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n+\n+import org.apache.commons.lang3.SystemUtils;\n+\n+public class DiskUtil {\n+\n+    private static final Logger LOGGER = Logger.getLogger(DiskUtil.class.getName());\n+\n+    private DiskUtil() {\n+        throw new AssertionError(\"Util class should not be initialized.\");\n+    }\n+\n+    /**\n+     * Mounts a RAM disk\n+     *\n+     * @param name\n+     * @param size\n+     * @param unit\n+     * @return The root of the mounted disk\n+     * @throws IOException\n+     * @throws InterruptedException\n+     */\n+    public static Path mountRamDisk(String name, int size, StorageUtil.StorageUnit unit)\n+            throws IOException, InterruptedException {\n+        if (SystemUtils.IS_OS_MAC) {\n+            return mountMacRamDisk(name, (StorageUtil.getIntSizeInBytes(size, unit) * 2) / StorageUtil.BASE);\n+        } else if (SystemUtils.IS_OS_LINUX) {\n+            return mountLinuxRamDisk(name, size + unit.getLinuxUnitTypeInLetter());\n+        }\n+        throw new UnsupportedOperationException(\"Unsupported OS: \" + System.getProperty(\"os.name\"));\n+    }\n+\n+    /**\n+     * Unmounts a disk\n+     *\n+     * @param name\n+     * @throws IOException\n+     * @throws InterruptedException\n+     */\n+    public static void unmountRamDisk(String name) throws IOException, InterruptedException {\n+        if (SystemUtils.IS_OS_MAC) {\n+            unmountMacRamDisk(name);\n+        } else if (SystemUtils.IS_OS_LINUX) {\n+            unmountLinuxRamDisk(name);\n+        }\n+    }\n+\n+    private static Path mountMacRamDisk(String name, long size) throws IOException, InterruptedException {\n+        final String cmd = \"diskutil erasevolume HFS+ '\" + name + \"' `hdiutil attach -nomount ram://\" + size + \"`\";\n+        final ProcessBuilder pb = new ProcessBuilder(\"/bin/sh\", \"-c\", cmd);\n+        final Process p = pb.start();\n+        watchProcess(p);\n+        p.waitFor();\n+        return Paths.get(\"/Volumes\", name);\n+    }\n+\n+    private static void unmountMacRamDisk(String name) throws InterruptedException, IOException {\n+        final String cmd = \"diskutil unmount \" + name;\n+        final ProcessBuilder pb = new ProcessBuilder(\"/bin/sh\", \"-c\", cmd);\n+        final Process p = pb.start();\n+        watchProcess(p);\n+        p.waitFor();\n+    }\n+\n+    private static Path mountLinuxRamDisk(String name, String size) throws IOException, InterruptedException {\n+        Path root = Paths.get(\"/tmp\", name);\n+        if (!Files.exists(root)) {\n+            Files.createFile(root);\n+        }\n+        final String cmd = \"mount -o size=\" + size + \" -t tmpfs none /tmp/\" + name;\n+        final ProcessBuilder pb = new ProcessBuilder(\"bash\", \"-c\", cmd);\n+        final Process p = pb.start();\n+        watchProcess(p);\n+        p.waitFor();\n+        return root;\n+    }\n+\n+    private static void unmountLinuxRamDisk(String name) throws InterruptedException, IOException {\n+        final String cmd = \"umount /tmp/\" + name;\n+        final ProcessBuilder pb = new ProcessBuilder(\"bash\", \"-c\", cmd);\n+        final Process p = pb.start();\n+        watchProcess(p);\n+        p.waitFor();\n+    }\n+\n+    private static void watchProcess(Process p) {\n+        new Thread(() -> {\n+            final BufferedReader input = new BufferedReader(new InputStreamReader(p.getInputStream()));\n+            String line;\n+            try {\n+                while ((line = input.readLine()) != null) {\n+                    LOGGER.info(line);\n+                }\n+            } catch (IOException e) {\n+                LOGGER.log(Level.WARNING, e.getMessage(), e);\n+            }\n+        }).start();\n+    }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-util/src/main/java/org/apache/hyracks/util/DiskUtil.java",
                "sha": "9a65d720a4e85b9470ce1ea8fc3dc1c44c90603b",
                "status": "added"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/asterixdb/blob/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-util/src/main/java/org/apache/hyracks/util/StorageUtil.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-util/src/main/java/org/apache/hyracks/util/StorageUtil.java?ref=0d0a113f40014384bcf54b68235581286c9b2c2b",
                "deletions": 8,
                "filename": "hyracks-fullstack/hyracks/hyracks-util/src/main/java/org/apache/hyracks/util/StorageUtil.java",
                "patch": "@@ -23,17 +23,18 @@\n \n public class StorageUtil {\n \n-    private static final int BASE = 1024;\n+    public static final int BASE = 1024;\n \n     public enum StorageUnit {\n-        BYTE(\"B\", 1),\n-        KILOBYTE(\"KB\", BASE),\n-        MEGABYTE(\"MB\", KILOBYTE.multiplier * BASE),\n-        GIGABYTE(\"GB\", MEGABYTE.multiplier * BASE),\n-        TERABYTE(\"TB\", GIGABYTE.multiplier * BASE),\n-        PETABYTE(\"PB\", TERABYTE.multiplier * BASE);\n+        BYTE(\"B\", \"b\", 1),\n+        KILOBYTE(\"KB\", \"kb\", BASE),\n+        MEGABYTE(\"MB\", \"m\", KILOBYTE.multiplier * BASE),\n+        GIGABYTE(\"GB\", \"g\", MEGABYTE.multiplier * BASE),\n+        TERABYTE(\"TB\", \"t\", GIGABYTE.multiplier * BASE),\n+        PETABYTE(\"PB\", \"p\", TERABYTE.multiplier * BASE);\n \n         private final String unitTypeInLetter;\n+        private final String linuxUnitTypeInLetter;\n         private final long multiplier;\n         private static final Map<String, StorageUnit> SUFFIX_TO_UNIT_MAP = new HashMap<>();\n \n@@ -43,8 +44,9 @@\n             }\n         }\n \n-        StorageUnit(String unitTypeInLetter, long multiplier) {\n+        StorageUnit(String unitTypeInLetter, String linuxUnitTypeInLetter, long multiplier) {\n             this.unitTypeInLetter = unitTypeInLetter;\n+            this.linuxUnitTypeInLetter = linuxUnitTypeInLetter;\n             this.multiplier = multiplier;\n         }\n \n@@ -57,6 +59,10 @@ public double toBytes(double value) {\n             return value * multiplier;\n         }\n \n+        public String getLinuxUnitTypeInLetter() {\n+            return linuxUnitTypeInLetter;\n+        }\n+\n         public static StorageUnit lookupBySuffix(String name) {\n             return SUFFIX_TO_UNIT_MAP.get(name);\n         }",
                "raw_url": "https://github.com/apache/asterixdb/raw/0d0a113f40014384bcf54b68235581286c9b2c2b/hyracks-fullstack/hyracks/hyracks-util/src/main/java/org/apache/hyracks/util/StorageUtil.java",
                "sha": "dbfe6f9503fb67090ec2fbbb5450bfb2ea2fe207",
                "status": "modified"
            }
        ],
        "message": "[ASTERIXDB-1995][STO] Abort write txn when index cannot be flushed\n\n- user model changes: no\n- storage format changes: no\n- interface changes: no\n\nDetails:\n- Fix LSM memory component state transition on flush/merge failure\n- When index cannot be flushed, abort waiting threads\n- Prevent NPE in MateralizerTaskState when file creation fails\n- Check parent dirs creation for index metadata file\n\nChange-Id: I28592c30c788f4a6f44db8b47a84bc77f6b3f8f3\nReviewed-on: https://asterix-gerrit.ics.uci.edu/1896\nSonar-Qube: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nTested-by: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nBAD: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nIntegration-Tests: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: abdullah alamoudi <bamousaa@gmail.com>",
        "parent": "https://github.com/apache/asterixdb/commit/8b077a536aa7c1bb69f31067ace8f136ffdf5182",
        "repo": "asterixdb",
        "unit_tests": [
            "PersistentLocalResourceRepositoryTest.java"
        ]
    },
    "asterixdb_4eaaff5": {
        "bug_id": "asterixdb_4eaaff5",
        "commit": "https://github.com/apache/asterixdb/commit/4eaaff59f6ff53bd6a4adbe0127b7127cbbdd7d4",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/asterixdb/blob/4eaaff59f6ff53bd6a4adbe0127b7127cbbdd7d4/hyracks-fullstack/hyracks/hyracks-http/src/main/java/org/apache/hyracks/http/server/HttpServer.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-http/src/main/java/org/apache/hyracks/http/server/HttpServer.java?ref=4eaaff59f6ff53bd6a4adbe0127b7127cbbdd7d4",
                "deletions": 2,
                "filename": "hyracks-fullstack/hyracks/hyracks-http/src/main/java/org/apache/hyracks/http/server/HttpServer.java",
                "patch": "@@ -333,8 +333,10 @@ protected void doStop() throws InterruptedException {\n         } catch (Exception e) {\n             LOGGER.log(Level.ERROR, \"Error while shutting down http server executor\", e);\n         }\n-        channel.close();\n-        channel.closeFuture().sync();\n+        if (channel != null) {\n+            channel.close();\n+            channel.closeFuture().sync();\n+        }\n     }\n \n     public IServlet getServlet(FullHttpRequest request) {",
                "raw_url": "https://github.com/apache/asterixdb/raw/4eaaff59f6ff53bd6a4adbe0127b7127cbbdd7d4/hyracks-fullstack/hyracks/hyracks-http/src/main/java/org/apache/hyracks/http/server/HttpServer.java",
                "sha": "d9902da942921e10e7da92818644bcb17a50104a",
                "status": "modified"
            }
        ],
        "message": "[NO ISSUE][OTH] Ensure HttpServer Channel is Initialized\n\n- user model changes: no\n- storage format changes: no\n- interface changes: no\n\nDetails:\n- Ensure the HttpServer channel is initialized when stopping\n  the server to avoid NPE.\n\nChange-Id: I5b7403e80f6118f99be46d166c6cfbee8d4305ac\nReviewed-on: https://asterix-gerrit.ics.uci.edu/3389\nContrib: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nTested-by: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nIntegration-Tests: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: Dmitry Lychagin <dmitry.lychagin@couchbase.com>",
        "parent": "https://github.com/apache/asterixdb/commit/a93a5c6fe4f83cef973f5caaa540723c96e29ba2",
        "repo": "asterixdb",
        "unit_tests": [
            "HttpServerTest.java"
        ]
    },
    "asterixdb_54a5070": {
        "bug_id": "asterixdb_54a5070",
        "commit": "https://github.com/apache/asterixdb/commit/54a507007e08cd84652774263bd7e1fe9ede8a0f",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/54a507007e08cd84652774263bd7e1fe9ede8a0f/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/executor/JobExecutor.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/executor/JobExecutor.java?ref=54a507007e08cd84652774263bd7e1fe9ede8a0f",
                "deletions": 1,
                "filename": "hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/executor/JobExecutor.java",
                "patch": "@@ -521,7 +521,7 @@ private void startTasks(Map<String, List<TaskAttemptDescriptor>> taskAttemptMap)\n         }\n     }\n \n-    private void abortJob(List<Exception> exceptions) {\n+    public void abortJob(List<Exception> exceptions) {\n         Set<TaskCluster> inProgressTaskClustersCopy = new HashSet<>(inProgressTaskClusters);\n         for (TaskCluster tc : inProgressTaskClustersCopy) {\n             abortTaskCluster(findLastTaskClusterAttempt(tc), TaskClusterAttempt.TaskClusterStatus.ABORTED);",
                "raw_url": "https://github.com/apache/asterixdb/raw/54a507007e08cd84652774263bd7e1fe9ede8a0f/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/executor/JobExecutor.java",
                "sha": "f18a9179b1c7b3c60804ee20858849059b89ccdf",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/asterixdb/blob/54a507007e08cd84652774263bd7e1fe9ede8a0f/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/job/JobManager.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/job/JobManager.java?ref=54a507007e08cd84652774263bd7e1fe9ede8a0f",
                "deletions": 3,
                "filename": "hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/job/JobManager.java",
                "patch": "@@ -45,7 +45,6 @@\n import org.apache.hyracks.control.cc.cluster.INodeManager;\n import org.apache.hyracks.control.cc.scheduler.FIFOJobQueue;\n import org.apache.hyracks.control.cc.scheduler.IJobQueue;\n-import org.apache.hyracks.control.cc.work.JobCleanupWork;\n import org.apache.hyracks.control.common.controllers.CCConfig;\n \n import com.fasterxml.jackson.databind.ObjectMapper;\n@@ -318,8 +317,12 @@ private void executeJobInternal(JobRun run) {\n         try {\n             run.getExecutor().startJob();\n         } catch (Exception e) {\n-            ccs.getWorkQueue().schedule(new JobCleanupWork(ccs.getJobManager(), run.getJobId(), JobStatus.FAILURE,\n-                    Collections.singletonList(e)));\n+            LOGGER.log(Level.SEVERE, \"Aborting \" + run.getJobId() + \" due to failure during job start\", e);\n+            final List<Exception> exceptions = Collections.singletonList(e);\n+            // fail the job then abort it\n+            run.setStatus(JobStatus.FAILURE, exceptions);\n+            // abort job will trigger JobCleanupWork\n+            run.getExecutor().abortJob(exceptions);\n         }\n     }\n ",
                "raw_url": "https://github.com/apache/asterixdb/raw/54a507007e08cd84652774263bd7e1fe9ede8a0f/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/job/JobManager.java",
                "sha": "abf1d5793eb3fb1425184881b2690270f1cc3ecc",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/asterixdb/blob/54a507007e08cd84652774263bd7e1fe9ede8a0f/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/work/JobCleanupWork.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/work/JobCleanupWork.java?ref=54a507007e08cd84652774263bd7e1fe9ede8a0f",
                "deletions": 2,
                "filename": "hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/work/JobCleanupWork.java",
                "patch": "@@ -18,6 +18,7 @@\n  */\n package org.apache.hyracks.control.cc.work;\n \n+import java.util.ArrayList;\n import java.util.List;\n import java.util.logging.Level;\n import java.util.logging.Logger;\n@@ -55,8 +56,12 @@ public void run() {\n         } catch (HyracksException e) {\n             // Fail the job with the caught exception during final completion.\n             JobRun run = jobManager.get(jobId);\n-            run.getExceptions().add(e);\n-            run.setStatus(JobStatus.FAILURE, run.getExceptions());\n+            List<Exception> completionException = new ArrayList<>();\n+            if (run.getExceptions() != null && !run.getExceptions().isEmpty()) {\n+                completionException.addAll(run.getExceptions());\n+            }\n+            completionException.add(0, e);\n+            run.setStatus(JobStatus.FAILURE, completionException);\n         }\n     }\n ",
                "raw_url": "https://github.com/apache/asterixdb/raw/54a507007e08cd84652774263bd7e1fe9ede8a0f/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/work/JobCleanupWork.java",
                "sha": "502ac50e510e04c84611e73d7e60c56ad66c04d9",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/asterixdb/blob/54a507007e08cd84652774263bd7e1fe9ede8a0f/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/work/JobletCleanupNotificationWork.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/work/JobletCleanupNotificationWork.java?ref=54a507007e08cd84652774263bd7e1fe9ede8a0f",
                "deletions": 2,
                "filename": "hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/work/JobletCleanupNotificationWork.java",
                "patch": "@@ -18,6 +18,8 @@\n  */\n package org.apache.hyracks.control.cc.work;\n \n+import java.util.ArrayList;\n+import java.util.List;\n import java.util.Set;\n import java.util.logging.Level;\n import java.util.logging.Logger;\n@@ -67,8 +69,12 @@ public void runWork() {\n                 jobManager.finalComplete(run);\n             } catch (HyracksException e) {\n                 // Fail the job with the caught exception during final completion.\n-                run.getExceptions().add(e);\n-                run.setStatus(JobStatus.FAILURE, run.getExceptions());\n+                List<Exception> completionException = new ArrayList<>();\n+                if (run.getExceptions() != null && !run.getExceptions().isEmpty()) {\n+                    completionException.addAll(run.getExceptions());\n+                }\n+                completionException.add(0, e);\n+                run.setStatus(JobStatus.FAILURE, completionException);\n             }\n         }\n     }",
                "raw_url": "https://github.com/apache/asterixdb/raw/54a507007e08cd84652774263bd7e1fe9ede8a0f/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-cc/src/main/java/org/apache/hyracks/control/cc/work/JobletCleanupNotificationWork.java",
                "sha": "5bf721b394925fa162d43fe65a4a713cf461657e",
                "status": "modified"
            }
        ],
        "message": "[ASTERIXDB-2003][FAIL] Abort jobs failing during job start\n\n- user model changes: no\n- storage format changes: no\n- interface changes: no\n\nDetails:\n- Prevent NPE or unmodifiable list in JobCleanupWork and\n  JobletCleanupNotificationWork.\n- Abort job if a failure happens during job start\n\nChange-Id: If6fe4ed9084270f9f22ee4b4c71936d679c8b883\nReviewed-on: https://asterix-gerrit.ics.uci.edu/1904\nSonar-Qube: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: Yingyi Bu <buyingyi@gmail.com>\nTested-by: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nBAD: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nIntegration-Tests: Jenkins <jenkins@fulliautomatix.ics.uci.edu>",
        "parent": "https://github.com/apache/asterixdb/commit/0d0a113f40014384bcf54b68235581286c9b2c2b",
        "repo": "asterixdb",
        "unit_tests": [
            "JobManagerTest.java"
        ]
    },
    "asterixdb_89e6a93": {
        "bug_id": "asterixdb_89e6a93",
        "commit": "https://github.com/apache/asterixdb/commit/89e6a93277205a9dbc76c18e249919a745d224d2",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/asterixdb/blob/89e6a93277205a9dbc76c18e249919a745d224d2/hyracks-fullstack/hyracks/hyracks-storage-am-lsm-btree/src/main/java/org/apache/hyracks/storage/am/lsm/btree/impls/LSMBTreePointSearchCursor.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-storage-am-lsm-btree/src/main/java/org/apache/hyracks/storage/am/lsm/btree/impls/LSMBTreePointSearchCursor.java?ref=89e6a93277205a9dbc76c18e249919a745d224d2",
                "deletions": 3,
                "filename": "hyracks-fullstack/hyracks/hyracks-storage-am-lsm-btree/src/main/java/org/apache/hyracks/storage/am/lsm/btree/impls/LSMBTreePointSearchCursor.java",
                "patch": "@@ -214,9 +214,7 @@ public void next() throws HyracksDataException {\n     public void close() throws HyracksDataException {\n         if (lsmHarness != null) {\n             try {\n-                for (int i = 0; i < rangeCursors.length; i++) {\n-                    rangeCursors[i].close();\n-                }\n+                closeCursors();\n                 rangeCursors = null;\n             } finally {\n                 lsmHarness.endSearch(opCtx);\n@@ -265,4 +263,13 @@ public boolean isExclusiveLatchNodes() {\n         return false;\n     }\n \n+    private void closeCursors() throws HyracksDataException {\n+        if (rangeCursors != null) {\n+            for (int i = 0; i < rangeCursors.length; ++i) {\n+                if (rangeCursors[i] != null) {\n+                    rangeCursors[i].close();\n+                }\n+            }\n+        }\n+    }\n }",
                "raw_url": "https://github.com/apache/asterixdb/raw/89e6a93277205a9dbc76c18e249919a745d224d2/hyracks-fullstack/hyracks/hyracks-storage-am-lsm-btree/src/main/java/org/apache/hyracks/storage/am/lsm/btree/impls/LSMBTreePointSearchCursor.java",
                "sha": "0f7aa38826cf6e122aaab0193a0f9b91f1e4a0a0",
                "status": "modified"
            }
        ],
        "message": "[ASTERIXDB-2135][IDX] Prevent NPE in LSMBTreePointSearchCursor\n\n- user model changes: no\n- storage format changes: no\n- interface changes: no\n\nDetails:\n- Prevent NPE on LSMBTreePointSearchCursor.close\n\nChange-Id: I062c1200d9c5a1a574a1ccdb32be0ac011406d92\nReviewed-on: https://asterix-gerrit.ics.uci.edu/2081\nSonar-Qube: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nTested-by: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nContrib: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nIntegration-Tests: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: Ian Maxon <imaxon@apache.org>",
        "parent": "https://github.com/apache/asterixdb/commit/536c707dc2ffc92a9e2f331b8765697367d9ab3a",
        "repo": "asterixdb",
        "unit_tests": [
            "LSMBTreePointSearchCursorTest.java"
        ]
    },
    "asterixdb_d237f0c": {
        "bug_id": "asterixdb_d237f0c",
        "commit": "https://github.com/apache/asterixdb/commit/d237f0c22db8ee316d553b2254f711f328d5aff8",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/d237f0c22db8ee316d553b2254f711f328d5aff8/asterixdb/asterix-app/src/main/java/org/apache/asterix/messaging/MessagingChannelWriteInterface.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/main/java/org/apache/asterix/messaging/MessagingChannelWriteInterface.java?ref=d237f0c22db8ee316d553b2254f711f328d5aff8",
                "deletions": 1,
                "filename": "asterixdb/asterix-app/src/main/java/org/apache/asterix/messaging/MessagingChannelWriteInterface.java",
                "patch": "@@ -54,7 +54,7 @@ public void write(IConnectionWriterState writerState) throws NetException {\n             ecodeSent = true;\n             ccb.reportLocalEOS();\n             adjustChannelWritability();\n-        } else if (eos && !eosSent) {\n+        } else if (isPendingCloseWrite()) {\n             writerState.getCommand().setChannelId(channelId);\n             writerState.getCommand().setCommandType(MuxDemuxCommand.CommandType.CLOSE_CHANNEL);\n             writerState.getCommand().setData(0);",
                "raw_url": "https://github.com/apache/asterixdb/raw/d237f0c22db8ee316d553b2254f711f328d5aff8/asterixdb/asterix-app/src/main/java/org/apache/asterix/messaging/MessagingChannelWriteInterface.java",
                "sha": "84f7831d496d428343a381f8aeceee0c244c0e93",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/d237f0c22db8ee316d553b2254f711f328d5aff8/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/partitions/PartitionManager.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/partitions/PartitionManager.java?ref=d237f0c22db8ee316d553b2254f711f328d5aff8",
                "deletions": 0,
                "filename": "hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/partitions/PartitionManager.java",
                "patch": "@@ -107,6 +107,7 @@ public synchronized IPartition getPartition(PartitionId pid) {\n     public synchronized void registerPartitionRequest(PartitionId partitionId, NetworkOutputChannel writer) {\n         if (failedJobsCache.getIfPresent(partitionId.getJobId()) != null) {\n             writer.abort(AbstractChannelWriteInterface.REMOTE_ERROR_CODE);\n+            return;\n         }\n         List<IPartition> pList = availablePartitionMap.get(partitionId);\n         if (pList != null && !pList.isEmpty()) {",
                "raw_url": "https://github.com/apache/asterixdb/raw/d237f0c22db8ee316d553b2254f711f328d5aff8/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/partitions/PartitionManager.java",
                "sha": "7c8fb3471a0d2465ca6c9ce6d7a2741bfcd4a713",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/asterixdb/blob/d237f0c22db8ee316d553b2254f711f328d5aff8/hyracks-fullstack/hyracks/hyracks-net/src/main/java/org/apache/hyracks/net/protocols/muxdemux/AbstractChannelWriteInterface.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-net/src/main/java/org/apache/hyracks/net/protocols/muxdemux/AbstractChannelWriteInterface.java?ref=d237f0c22db8ee316d553b2254f711f328d5aff8",
                "deletions": 1,
                "filename": "hyracks-fullstack/hyracks/hyracks-net/src/main/java/org/apache/hyracks/net/protocols/muxdemux/AbstractChannelWriteInterface.java",
                "patch": "@@ -75,7 +75,7 @@ private boolean computeWritability() {\n         if (writableDataPresent) {\n             return credits > 0;\n         }\n-        if (eos && !eosSent) {\n+        if (isPendingCloseWrite()) {\n             return true;\n         }\n         return ecode.get() == REMOTE_ERROR_CODE && !ecodeSent;\n@@ -116,6 +116,10 @@ public int getCredits() {\n         return credits;\n     }\n \n+    protected boolean isPendingCloseWrite() {\n+        return eos && !eosSent && !ecodeSent;\n+    }\n+\n     private class CloseableBufferAcceptor implements ICloseableBufferAcceptor {\n         @Override\n         public void accept(ByteBuffer buffer) {",
                "raw_url": "https://github.com/apache/asterixdb/raw/d237f0c22db8ee316d553b2254f711f328d5aff8/hyracks-fullstack/hyracks/hyracks-net/src/main/java/org/apache/hyracks/net/protocols/muxdemux/AbstractChannelWriteInterface.java",
                "sha": "5c927f95e3ca1ced89b99f2684e11b6f95f99484",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/d237f0c22db8ee316d553b2254f711f328d5aff8/hyracks-fullstack/hyracks/hyracks-net/src/main/java/org/apache/hyracks/net/protocols/muxdemux/FullFrameChannelWriteInterface.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-net/src/main/java/org/apache/hyracks/net/protocols/muxdemux/FullFrameChannelWriteInterface.java?ref=d237f0c22db8ee316d553b2254f711f328d5aff8",
                "deletions": 1,
                "filename": "hyracks-fullstack/hyracks/hyracks-net/src/main/java/org/apache/hyracks/net/protocols/muxdemux/FullFrameChannelWriteInterface.java",
                "patch": "@@ -58,7 +58,7 @@ public void write(IConnectionWriterState writerState) throws NetException {\n             ecodeSent = true;\n             ccb.reportLocalEOS();\n             adjustChannelWritability();\n-        } else if (eos && !eosSent) {\n+        } else if (isPendingCloseWrite()) {\n             writerState.getCommand().setChannelId(channelId);\n             writerState.getCommand().setCommandType(MuxDemuxCommand.CommandType.CLOSE_CHANNEL);\n             writerState.getCommand().setData(0);",
                "raw_url": "https://github.com/apache/asterixdb/raw/d237f0c22db8ee316d553b2254f711f328d5aff8/hyracks-fullstack/hyracks/hyracks-net/src/main/java/org/apache/hyracks/net/protocols/muxdemux/FullFrameChannelWriteInterface.java",
                "sha": "3f4618bdc9728293345a1688c57975c586dc3a2d",
                "status": "modified"
            }
        ],
        "message": "[NO ISSUE][NET] Ensure CLOSE Is Not Sent After Channel ERROR\n\n- user model changes: no\n- storage format changes: no\n- interface changes: no\n\nDetails:\n- Currently it is possible to send network channel\n  CLOSE command after a channel ERROR was sent. When this\n  happens and the channel was recycled to be reused\n  on the receiver side, the CLOSE command will result\n  in NPE. There is no need to send a CLOSE command\n  after an ERROR command because when an ERROR command\n  is received, it is treated as ERROR + CLOSE on the\n  receiver side.\n- Avoid registering partition requests for failed jobs.\n\nChange-Id: I17a769a46f4d13220adb22dd255e56dc4ccc458d\nReviewed-on: https://asterix-gerrit.ics.uci.edu/2954\nSonar-Qube: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nTested-by: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nIntegration-Tests: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: Murtadha Hubail <mhubail@apache.org>\nReviewed-by: Michael Blow <mblow@apache.org>",
        "parent": "https://github.com/apache/asterixdb/commit/3a6846942cde2f6e97ab53e01e14a7025ca04814",
        "repo": "asterixdb",
        "unit_tests": [
            "PartitionManagerTest.java"
        ]
    },
    "asterixdb_da0a4e5": {
        "bug_id": "asterixdb_da0a4e5",
        "commit": "https://github.com/apache/asterixdb/commit/da0a4e56bffa7c7cb5fd14b5e4d21b4bee8dc5e8",
        "file": [
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/asterixdb/blob/da0a4e56bffa7c7cb5fd14b5e4d21b4bee8dc5e8/hyracks/hyracks-dataflow-std/src/main/java/edu/uci/ics/hyracks/dataflow/std/join/InMemoryHashJoin.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks/hyracks-dataflow-std/src/main/java/edu/uci/ics/hyracks/dataflow/std/join/InMemoryHashJoin.java?ref=da0a4e56bffa7c7cb5fd14b5e4d21b4bee8dc5e8",
                "deletions": 1,
                "filename": "hyracks/hyracks-dataflow-std/src/main/java/edu/uci/ics/hyracks/dataflow/std/join/InMemoryHashJoin.java",
                "patch": "@@ -119,7 +119,16 @@ public void join(ByteBuffer buffer, IFrameWriter writer) throws HyracksDataExcep\n                     accessorBuild.reset(buffers.get(bIndex));\n                     int c = tpComparator.compare(accessorProbe, i, accessorBuild, tIndex);\n                     if (c == 0) {\n-                    \tboolean predEval = ( (predEvaluator == null) || predEvaluator.evaluate(accessorProbe, i, accessorBuild, tIndex) );\n+                    \tboolean predEval = evaluatePredicate(i, tIndex);\n+\t\t\t\t\t\t/*\n+                    \ttry {\n+\t\t\t\t\t\t\tpredEval = ( (predEvaluator == null) || predEvaluator.evaluate(accessorProbe, i, accessorBuild, tIndex) );\n+\t\t\t\t\t\t} catch (ArrayIndexOutOfBoundsException e) {\n+\t\t\t\t\t\t\tSystem.out.println(\"Hit Array Index out of bound - now we swap\");\n+\t\t\t\t\t\t\te.printStackTrace();\n+\t\t\t\t\t\t\tpredEval = predEvaluator.evaluate(accessorBuild, i, accessorProbe, tIndex);\n+\t\t\t\t\t\t}\n+\t\t\t\t\t\t*/\n                     \tif(predEval){\n                     \t\tmatchFound = true;\n                             appendToResult(i, tIndex, writer);\n@@ -155,6 +164,15 @@ private void flushFrame(ByteBuffer buffer, IFrameWriter writer) throws HyracksDa\n         buffer.position(0);\n         buffer.limit(buffer.capacity());\n     }\n+    \n+    private boolean evaluatePredicate(int tIx1, int tIx2){\n+    \tif(reverseOutputOrder){\t\t//Role Reversal Optimization is triggered\n+    \t\treturn ( (predEvaluator == null) || predEvaluator.evaluate(accessorBuild, tIx2, accessorProbe, tIx1) );\n+    \t}\n+    \telse {\n+    \t\treturn ( (predEvaluator == null) || predEvaluator.evaluate(accessorProbe, tIx1, accessorBuild, tIx2) );\n+    \t}\n+    }\n \n     private void appendToResult(int probeSidetIx, int buildSidetIx, IFrameWriter writer) throws HyracksDataException {\n         if (!reverseOutputOrder) {",
                "raw_url": "https://github.com/apache/asterixdb/raw/da0a4e56bffa7c7cb5fd14b5e4d21b4bee8dc5e8/hyracks/hyracks-dataflow-std/src/main/java/edu/uci/ics/hyracks/dataflow/std/join/InMemoryHashJoin.java",
                "sha": "f44d2f1101c7a5b72475a575bea0ceb5c8036d98",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/asterixdb/blob/da0a4e56bffa7c7cb5fd14b5e4d21b4bee8dc5e8/hyracks/hyracks-dataflow-std/src/main/java/edu/uci/ics/hyracks/dataflow/std/join/NestedLoopJoin.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks/hyracks-dataflow-std/src/main/java/edu/uci/ics/hyracks/dataflow/std/join/NestedLoopJoin.java?ref=da0a4e56bffa7c7cb5fd14b5e4d21b4bee8dc5e8",
                "deletions": 1,
                "filename": "hyracks/hyracks-dataflow-std/src/main/java/edu/uci/ics/hyracks/dataflow/std/join/NestedLoopJoin.java",
                "patch": "@@ -49,6 +49,8 @@\n     private final boolean isLeftOuter;\n     private final ArrayTupleBuilder nullTupleBuilder;\n     private final IPredicateEvaluator predEvaluator;\n+    private boolean isReversed;\t\t//Added for handling correct calling for predicate-evaluator upon recursive calls (in OptimizedHybridHashJoin) that cause role-reversal\n+\n     \n     public NestedLoopJoin(IHyracksTaskContext ctx, FrameTupleAccessor accessor0, FrameTupleAccessor accessor1,\n             ITuplePairComparator comparators, int memSize, IPredicateEvaluator predEval, boolean isLeftOuter, INullWriter[] nullWriters1)\n@@ -63,6 +65,7 @@ public NestedLoopJoin(IHyracksTaskContext ctx, FrameTupleAccessor accessor0, Fra\n         this.outBuffers = new ArrayList<ByteBuffer>();\n         this.memSize = memSize;\n         this.predEvaluator = predEval;\n+        this.isReversed = false;\n         this.ctx = ctx;\n \n         this.isLeftOuter = isLeftOuter;\n@@ -133,7 +136,7 @@ private void blockJoin(ByteBuffer outerBuffer, ByteBuffer innerBuffer, IFrameWri\n             boolean matchFound = false;\n             for (int j = 0; j < tupleCount1; ++j) {\n                 int c = compare(accessorOuter, i, accessorInner, j);\n-                boolean prdEval = (predEvaluator == null) || (predEvaluator.evaluate(accessorOuter, i, accessorInner, j));\n+                boolean prdEval = evaluatePredicate(i, j);\n                 if (c == 0 && prdEval) {\n                 \tmatchFound = true;\n                     if (!appender.appendConcat(accessorOuter, i, accessorInner, j)) {\n@@ -165,6 +168,15 @@ private void blockJoin(ByteBuffer outerBuffer, ByteBuffer innerBuffer, IFrameWri\n             }\n         }\n     }\n+    \n+    private boolean evaluatePredicate(int tIx1, int tIx2){\n+    \tif(isReversed){\t\t//Role Reversal Optimization is triggered\n+    \t\treturn ( (predEvaluator == null) || predEvaluator.evaluate(accessorInner, tIx2, accessorOuter, tIx1) );\n+    \t}\n+    \telse {\n+    \t\treturn ( (predEvaluator == null) || predEvaluator.evaluate(accessorOuter, tIx1, accessorInner, tIx2) );\n+    \t}\n+    }\n \n     public void closeCache() throws HyracksDataException {\n         if (runFileWriter != null) {\n@@ -206,4 +218,8 @@ private int compare(FrameTupleAccessor accessor0, int tIndex0, FrameTupleAccesso\n         }\n         return 0;\n     }\n+    \n+    public void setIsReversed(boolean b){\n+    \tthis.isReversed = b;\n+    }\n }",
                "raw_url": "https://github.com/apache/asterixdb/raw/da0a4e56bffa7c7cb5fd14b5e4d21b4bee8dc5e8/hyracks/hyracks-dataflow-std/src/main/java/edu/uci/ics/hyracks/dataflow/std/join/NestedLoopJoin.java",
                "sha": "979ef5912d2adbe54657e7946782cfd983e9e785",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/asterixdb/blob/da0a4e56bffa7c7cb5fd14b5e4d21b4bee8dc5e8/hyracks/hyracks-dataflow-std/src/main/java/edu/uci/ics/hyracks/dataflow/std/join/OptimizedHybridHashJoin.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks/hyracks-dataflow-std/src/main/java/edu/uci/ics/hyracks/dataflow/std/join/OptimizedHybridHashJoin.java?ref=da0a4e56bffa7c7cb5fd14b5e4d21b4bee8dc5e8",
                "deletions": 2,
                "filename": "hyracks/hyracks-dataflow-std/src/main/java/edu/uci/ics/hyracks/dataflow/std/join/OptimizedHybridHashJoin.java",
                "patch": "@@ -99,6 +99,7 @@\n     private int freeFramesCounter; //Used for partition tuning\n     \n     private boolean isTableEmpty;\t//Added for handling the case, where build side is empty (tableSize is 0)\n+    private boolean isReversed;\t\t//Added for handling correct calling for predicate-evaluator upon recursive calls that cause role-reversal\n     \n     public OptimizedHybridHashJoin(IHyracksTaskContext ctx, int memForJoin, int numOfPartitions, String rel0Name,\n             String rel1Name, int[] keys0, int[] keys1, IBinaryComparator[] comparators, RecordDescriptor buildRd,\n@@ -125,6 +126,7 @@ public OptimizedHybridHashJoin(IHyracksTaskContext ctx, int memForJoin, int numO\n         this.predEvaluator = predEval;\n         this.isLeftOuter = false;\n         this.nullWriters1 = null;\n+        this.isReversed = false;\n \n     }\n \n@@ -153,7 +155,8 @@ public OptimizedHybridHashJoin(IHyracksTaskContext ctx, int memForJoin, int numO\n         \n         this.predEvaluator = predEval;\n         this.isLeftOuter = isLeftOuter;\n-\n+        this.isReversed = false;\n+        \n         this.nullWriters1 = isLeftOuter ? new INullWriter[nullWriterFactories1.length] : null;\n         if (isLeftOuter) {\n             for (int i = 0; i < nullWriterFactories1.length; i++) {\n@@ -441,7 +444,7 @@ private void createInMemoryJoiner(int inMemTupCount) throws HyracksDataException\n         this.inMemJoiner = new InMemoryHashJoin(ctx, inMemTupCount,\n                 new FrameTupleAccessor(ctx.getFrameSize(), probeRd), probeHpc, new FrameTupleAccessor(\n                         ctx.getFrameSize(), buildRd), buildHpc, new FrameTuplePairComparator(probeKeys, buildKeys,\n-                        comparators), isLeftOuter, nullWriters1, table, predEvaluator);\n+                        comparators), isLeftOuter, nullWriters1, table, predEvaluator, isReversed);\n     }\n \n     private void cacheInMemJoin() throws HyracksDataException {\n@@ -639,4 +642,8 @@ public String debugGetStats() {\n     public boolean isTableEmpty() {\n         return this.isTableEmpty;\n     }\n+    \n+    public void setIsReversed(boolean b){\n+    \tthis.isReversed = b;\n+    }\n }",
                "raw_url": "https://github.com/apache/asterixdb/raw/da0a4e56bffa7c7cb5fd14b5e4d21b4bee8dc5e8/hyracks/hyracks-dataflow-std/src/main/java/edu/uci/ics/hyracks/dataflow/std/join/OptimizedHybridHashJoin.java",
                "sha": "6bc810ef02a982a784c7023ad8f4ac13979022d0",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/asterixdb/blob/da0a4e56bffa7c7cb5fd14b5e4d21b4bee8dc5e8/hyracks/hyracks-dataflow-std/src/main/java/edu/uci/ics/hyracks/dataflow/std/join/OptimizedHybridHashJoinOperatorDescriptor.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks/hyracks-dataflow-std/src/main/java/edu/uci/ics/hyracks/dataflow/std/join/OptimizedHybridHashJoinOperatorDescriptor.java?ref=da0a4e56bffa7c7cb5fd14b5e4d21b4bee8dc5e8",
                "deletions": 14,
                "filename": "hyracks/hyracks-dataflow-std/src/main/java/edu/uci/ics/hyracks/dataflow/std/join/OptimizedHybridHashJoinOperatorDescriptor.java",
                "patch": "@@ -19,6 +19,8 @@\n import java.io.IOException;\n import java.nio.ByteBuffer;\n import java.util.BitSet;\n+import java.util.logging.Level;\n+import java.util.logging.Logger;\n \n import edu.uci.ics.hyracks.api.context.IHyracksTaskContext;\n import edu.uci.ics.hyracks.api.dataflow.ActivityId;\n@@ -117,6 +119,8 @@\n     \n     private final boolean isLeftOuter;\n     private final INullWriterFactory[] nullWriterFactories1;\n+    \n+    private static final Logger LOGGER = Logger.getLogger(OptimizedHybridHashJoinOperatorDescriptor.class.getName());\n \n     public OptimizedHybridHashJoinOperatorDescriptor(IOperatorDescriptorRegistry spec, int memsize, int inputsize0,\n             double factor, int[] keys0, int[] keys1, IBinaryHashFunctionFamily[] hashFunctionGeneratorFactories,\n@@ -139,8 +143,6 @@ public OptimizedHybridHashJoinOperatorDescriptor(IOperatorDescriptorRegistry spe\n         this.predEvaluatorFactory = predEvaluatorFactory;\n         this.isLeftOuter = isLeftOuter;\n         this.nullWriterFactories1 = nullWriterFactories1;\n-        \n-\n     }\n \n     public OptimizedHybridHashJoinOperatorDescriptor(IOperatorDescriptorRegistry spec, int memsize, int inputsize0,\n@@ -207,7 +209,7 @@ private int getNumberOfPartitions(int memorySize, int buildSize, double factor,\n     }\n \n     public static class BuildAndPartitionTaskState extends AbstractStateObject {\n-\n+    \t\n         private int memForJoin;\n         private int numOfPartitions;\n         private OptimizedHybridHashJoin hybridHJ;\n@@ -303,6 +305,7 @@ public void nextFrame(ByteBuffer buffer) throws HyracksDataException {\n                 public void close() throws HyracksDataException {\n                     state.hybridHJ.closeBuild();\n                     ctx.setStateObject(state);\n+                    LOGGER.log(Level.WARNING, \"OptimizedHybridHashJoin closed its build phase\");\n                 }\n \n                 @Override\n@@ -323,7 +326,7 @@ public void fail() throws HyracksDataException {\n      * Hybrid Hash Join recursively on them.\n      */\n     private class ProbeAndJoinActivityNode extends AbstractActivityNode {\n-\n+    \t\n         private static final long serialVersionUID = 1L;\n \n         private final ActivityId buildAid;\n@@ -423,9 +426,11 @@ private void joinPartitionPair(OptimizedHybridHashJoin ohhj, RunFileReader build\n                             hashFunctionGeneratorFactories).createPartitioner(level);\n                     ITuplePartitionComputer buildHpc = new FieldHashPartitionComputerFamily(buildKeys,\n                             hashFunctionGeneratorFactories).createPartitioner(level);\n-\n+                    \n                     long buildPartSize = ohhj.getBuildPartitionSize(pid) / ctx.getFrameSize();\n                     long probePartSize = ohhj.getProbePartitionSize(pid) / ctx.getFrameSize();\n+                    \n+                    LOGGER.log(Level.WARNING,\"Joining Partition Pairs (pid \"+pid+\") - (level \"+level+\") - BuildSize:\\t\"+buildPartSize+\"\\tProbeSize:\\t\"+probePartSize+\" - MemForJoin \"+(state.memForJoin));\n \n                     //Apply in-Mem HJ if possible\n                     if ((buildPartSize < state.memForJoin) || (probePartSize < state.memForJoin)) {\n@@ -460,7 +465,7 @@ private void joinPartitionPair(OptimizedHybridHashJoin ohhj, RunFileReader build\n                     else {\n                         OptimizedHybridHashJoin rHHj;\n                         if (isLeftOuter || buildPartSize < probePartSize) { //Build Side is smaller\n-\n+                        \tLOGGER.log(Level.WARNING,\"\\tApply RecursiveHHJ for (pid \"+pid+\") - (level \"+level+\") [buildSize is smaller]\");\n                             int n = getNumberOfPartitions(state.memForJoin, (int) buildPartSize, fudgeFactor,\n                                     nPartitions);\n                            \n@@ -503,6 +508,7 @@ private void joinPartitionPair(OptimizedHybridHashJoin ohhj, RunFileReader build\n                                 }\n \n                             } else { //Switch to NLJ (Further recursion seems not to be useful)\n+                            \tLOGGER.log(Level.WARNING,\"\\tSwitched to NLJ for (pid \"+pid+\") - (level \"+level+\") (reverse false) [coming from buildSize was smaller]\");\n                                 for (int rPid = rPStatus.nextSetBit(0); rPid >= 0; rPid = rPStatus.nextSetBit(rPid + 1)) {\n                                     RunFileReader rbrfw = rHHj.getBuildRFReader(rPid);\n                                     RunFileReader rprfw = rHHj.getProbeRFReader(rPid);\n@@ -515,19 +521,21 @@ private void joinPartitionPair(OptimizedHybridHashJoin ohhj, RunFileReader build\n                                     int probeSideInTups = rHHj.getProbePartitionSizeInTup(rPid);\n                                     if (isLeftOuter || buildSideInTups < probeSideInTups) {\n                                         applyNestedLoopJoin(probeRd, buildRd, state.memForJoin, rbrfw, rprfw,\n-                                                nljComparator0);\n+                                                nljComparator0, false);\n                                     } else {\n                                         applyNestedLoopJoin(buildRd, probeRd, state.memForJoin, rprfw, rbrfw,\n-                                                nljComparator1);\n+                                                nljComparator1, false);\n                                     }\n                                 }\n                             }\n                         } else { //Role Reversal (Probe Side is smaller)\n+                        \tLOGGER.log(Level.WARNING,\"\\tApply RecursiveHHJ for (pid \"+pid+\") - (level \"+level+\") WITH REVERSAL [probeSize is smaller]\");\n                             int n = getNumberOfPartitions(state.memForJoin, (int) probePartSize, fudgeFactor,\n                                     nPartitions);\n                             \n                             rHHj = new OptimizedHybridHashJoin(ctx, state.memForJoin, n, BUILD_REL, PROBE_REL,\n                                     buildKeys, probeKeys, comparators, buildRd, probeRd, buildHpc, probeHpc, predEvaluator);\n+                            rHHj.setIsReversed(true);\t//Added to use predicateEvaluator (for inMemoryHashJoin) correctly\n \n                             probeSideReader.open();\n                             rHHj.initBuild();\n@@ -561,7 +569,8 @@ private void joinPartitionPair(OptimizedHybridHashJoin ohhj, RunFileReader build\n                                     joinPartitionPair(rHHj, rprfw, rbrfw, rPid, afterMax, (level + 1));\n                                 }\n                             } else { //Switch to NLJ (Further recursion seems not to be effective)\n-                                for (int rPid = rPStatus.nextSetBit(0); rPid >= 0; rPid = rPStatus.nextSetBit(rPid + 1)) {\n+                            \tLOGGER.log(Level.WARNING,\"\\tSwitched to NLJ for (pid \"+pid+\") - (level \"+level+\") (reverse true) [coming from probeSize was smaller]\");\n+                            \tfor (int rPid = rPStatus.nextSetBit(0); rPid >= 0; rPid = rPStatus.nextSetBit(rPid + 1)) {\n                                     RunFileReader rbrfw = rHHj.getBuildRFReader(rPid);\n                                     RunFileReader rprfw = rHHj.getProbeRFReader(rPid);\n                                     \n@@ -573,10 +582,10 @@ private void joinPartitionPair(OptimizedHybridHashJoin ohhj, RunFileReader build\n                                     long probeSideSize = rprfw.getFileSize();\n                                     if (buildSideSize > probeSideSize) {\n                                         applyNestedLoopJoin(buildRd, probeRd, state.memForJoin, rbrfw, rprfw,\n-                                                nljComparator1);\n+                                                nljComparator1, true);\n                                     } else {\n                                         applyNestedLoopJoin(probeRd, buildRd, state.memForJoin, rprfw, rbrfw,\n-                                                nljComparator0);\n+                                                nljComparator0, true);\n                                     }\n                                 }\n                             }\n@@ -590,7 +599,7 @@ private void applyInMemHashJoin(int[] bKeys, int[] pKeys, int tabSize, RecordDes\n                         RecordDescriptor probeRDesc, ITuplePartitionComputer hpcRepLarger,\n                         ITuplePartitionComputer hpcRepSmaller, RunFileReader bReader, RunFileReader pReader, boolean reverse, int pid)\n                         throws HyracksDataException {\n-\n+                \tLOGGER.log(Level.WARNING,\"\\t(pid \"+pid+\") - applyInMemHashJoin (reversal \"+reverse+\")\");\n                     ISerializableTable table = new SerializableHashTable(tabSize, ctx);\n                     InMemoryHashJoin joiner = new InMemoryHashJoin(ctx, tabSize, new FrameTupleAccessor(\n                             ctx.getFrameSize(), probeRDesc), hpcRepLarger, new FrameTupleAccessor(ctx.getFrameSize(),\n@@ -619,9 +628,9 @@ private void applyInMemHashJoin(int[] bKeys, int[] pKeys, int tabSize, RecordDes\n                 }\n \n                 private void applyNestedLoopJoin(RecordDescriptor outerRd, RecordDescriptor innerRd, int memorySize,\n-                        RunFileReader outerReader, RunFileReader innerReader, ITuplePairComparator nljComparator)\n+                        RunFileReader outerReader, RunFileReader innerReader, ITuplePairComparator nljComparator, boolean reverse)\n                         throws HyracksDataException {\n-\n+                \t\n                     NestedLoopJoin nlj = new NestedLoopJoin(ctx, new FrameTupleAccessor(ctx.getFrameSize(), outerRd),\n                             new FrameTupleAccessor(ctx.getFrameSize(), innerRd), nljComparator, memorySize, predEvaluator, false, null);\n ",
                "raw_url": "https://github.com/apache/asterixdb/raw/da0a4e56bffa7c7cb5fd14b5e4d21b4bee8dc5e8/hyracks/hyracks-dataflow-std/src/main/java/edu/uci/ics/hyracks/dataflow/std/join/OptimizedHybridHashJoinOperatorDescriptor.java",
                "sha": "2d3185f401d5fb2b379cc7bcb1f6e74152b2ae48",
                "status": "modified"
            }
        ],
        "message": "Changes to fix NPE in tpch SF=1 with OptzHHJ",
        "parent": "https://github.com/apache/asterixdb/commit/0f63d8bf4dc96d106ef237d227807edfbda0046b",
        "repo": "asterixdb",
        "unit_tests": [
            "OptimizedHybridHashJoinTest.java"
        ]
    },
    "asterixdb_e0d8e50": {
        "bug_id": "asterixdb_e0d8e50",
        "commit": "https://github.com/apache/asterixdb/commit/e0d8e5078f90823e8dd51052317a7da1c08cc9f9",
        "file": [
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0d8e5078f90823e8dd51052317a7da1c08cc9f9/asterixdb/asterix-app/src/main/java/org/apache/asterix/app/message/ExecuteStatementRequestMessage.java",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/main/java/org/apache/asterix/app/message/ExecuteStatementRequestMessage.java?ref=e0d8e5078f90823e8dd51052317a7da1c08cc9f9",
                "deletions": 7,
                "filename": "asterixdb/asterix-app/src/main/java/org/apache/asterix/app/message/ExecuteStatementRequestMessage.java",
                "patch": "@@ -92,6 +92,11 @@ public void handle(ICcApplicationContext ccAppCtx) throws HyracksDataException,\n         ClusterControllerService ccSrv = (ClusterControllerService) ccSrvContext.getControllerService();\n         CCApplication ccApp = (CCApplication) ccSrv.getApplication();\n         CCMessageBroker messageBroker = (CCMessageBroker) ccSrvContext.getMessageBroker();\n+        final String rejectionReason = getRejectionReason(ccSrv);\n+        if (rejectionReason != null) {\n+            sendRejection(rejectionReason, messageBroker);\n+            return;\n+        }\n         CCExtensionManager ccExtMgr = (CCExtensionManager) ccAppCtx.getExtensionManager();\n         ILangCompilationProvider compilationProvider = ccExtMgr.getCompilationProvider(lang);\n         IStorageComponentProvider storageComponentProvider = ccAppCtx.getStorageComponentProvider();\n@@ -100,16 +105,9 @@ public void handle(ICcApplicationContext ccAppCtx) throws HyracksDataException,\n \n         ccSrv.getExecutor().submit(() -> {\n             ExecuteStatementResponseMessage responseMsg = new ExecuteStatementResponseMessage(requestMessageId);\n-\n             try {\n-                final IClusterManagementWork.ClusterState clusterState = ClusterStateManager.INSTANCE.getState();\n-                if (clusterState != IClusterManagementWork.ClusterState.ACTIVE) {\n-                    throw new IllegalStateException(\"Cannot execute request, cluster is \" + clusterState);\n-                }\n-\n                 IParser parser = compilationProvider.getParserFactory().createParser(statementsText);\n                 List<Statement> statements = parser.parse();\n-\n                 StringWriter outWriter = new StringWriter(256);\n                 PrintWriter outPrinter = new PrintWriter(outWriter);\n                 SessionOutput.ResultDecorator resultPrefix = ResultUtil.createPreResultDecorator();\n@@ -148,6 +146,27 @@ public void handle(ICcApplicationContext ccAppCtx) throws HyracksDataException,\n         });\n     }\n \n+    private String getRejectionReason(ClusterControllerService ccSrv) {\n+        if (ccSrv.getNodeManager().getNodeControllerState(requestNodeId) == null) {\n+            return \"Node is not registerted with the CC\";\n+        }\n+        final IClusterManagementWork.ClusterState clusterState = ClusterStateManager.INSTANCE.getState();\n+        if (clusterState != IClusterManagementWork.ClusterState.ACTIVE) {\n+            return \"Cannot execute request, cluster is \" + clusterState;\n+        }\n+        return null;\n+    }\n+\n+    private void sendRejection(String reason, CCMessageBroker messageBroker) {\n+        ExecuteStatementResponseMessage responseMsg = new ExecuteStatementResponseMessage(requestMessageId);\n+        responseMsg.setError(new Exception(reason));\n+        try {\n+            messageBroker.sendApplicationMessageToNC(responseMsg, requestNodeId);\n+        } catch (Exception e) {\n+            LOGGER.log(Level.WARNING, e.toString(), e);\n+        }\n+    }\n+\n     @Override\n     public String toString() {\n         return String.format(\"%s(id=%s, from=%s): %s\", getClass().getSimpleName(), requestMessageId, requestNodeId,",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0d8e5078f90823e8dd51052317a7da1c08cc9f9/asterixdb/asterix-app/src/main/java/org/apache/asterix/app/message/ExecuteStatementRequestMessage.java",
                "sha": "9faa9e9a09265664248143e1a46b44d76cb2e7fc",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0d8e5078f90823e8dd51052317a7da1c08cc9f9/asterixdb/asterix-app/src/main/java/org/apache/asterix/messaging/CCMessageBroker.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/main/java/org/apache/asterix/messaging/CCMessageBroker.java?ref=e0d8e5078f90823e8dd51052317a7da1c08cc9f9",
                "deletions": 1,
                "filename": "asterixdb/asterix-app/src/main/java/org/apache/asterix/messaging/CCMessageBroker.java",
                "patch": "@@ -69,7 +69,13 @@ public void receivedMessage(IMessage message, String nodeId) throws Exception {\n     public void sendApplicationMessageToNC(INcAddressedMessage msg, String nodeId) throws Exception {\n         INodeManager nodeManager = ccs.getNodeManager();\n         NodeControllerState state = nodeManager.getNodeControllerState(nodeId);\n-        state.getNodeController().sendApplicationMessageToNC(JavaSerializationUtils.serialize(msg), null, nodeId);\n+        if (state != null) {\n+            state.getNodeController().sendApplicationMessageToNC(JavaSerializationUtils.serialize(msg), null, nodeId);\n+        } else {\n+            if (LOGGER.isLoggable(Level.WARNING)) {\n+                LOGGER.warning(\"Couldn't send message to unregistered node (\" + nodeId + \")\");\n+            }\n+        }\n     }\n \n     @Override",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0d8e5078f90823e8dd51052317a7da1c08cc9f9/asterixdb/asterix-app/src/main/java/org/apache/asterix/messaging/CCMessageBroker.java",
                "sha": "0eade416d072d0927f5b7ddaec045defbf7387e5",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0d8e5078f90823e8dd51052317a7da1c08cc9f9/asterixdb/asterix-common/src/main/java/org/apache/asterix/common/cluster/IClusterStateManager.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-common/src/main/java/org/apache/asterix/common/cluster/IClusterStateManager.java?ref=e0d8e5078f90823e8dd51052317a7da1c08cc9f9",
                "deletions": 1,
                "filename": "asterixdb/asterix-common/src/main/java/org/apache/asterix/common/cluster/IClusterStateManager.java",
                "patch": "@@ -100,11 +100,20 @@ boolean waitForState(ClusterState waitForState, long timeout, TimeUnit unit)\n \n     /**\n      * Register the specified node partitions with the specified nodeId with this cluster state manager\n+     * then calls {@link IClusterStateManager#refreshState()}\n+     *\n+     * @param nodeId\n+     * @param nodePartitions\n+     * @throws AsterixException\n      */\n     void registerNodePartitions(String nodeId, ClusterPartition[] nodePartitions) throws AsterixException;\n \n     /**\n      * De-register the specified node's partitions from this cluster state manager\n+     * then calls {@link IClusterStateManager#refreshState()}\n+     *\n+     * @param nodeId\n+     * @throws HyracksDataException\n      */\n-    void deregisterNodePartitions(String nodeId);\n+    void deregisterNodePartitions(String nodeId) throws HyracksDataException;\n }",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0d8e5078f90823e8dd51052317a7da1c08cc9f9/asterixdb/asterix-common/src/main/java/org/apache/asterix/common/cluster/IClusterStateManager.java",
                "sha": "30675cd631ccbcbca68def98685009d07fd82829",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0d8e5078f90823e8dd51052317a7da1c08cc9f9/asterixdb/asterix-runtime/src/main/java/org/apache/asterix/runtime/utils/ClusterStateManager.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-runtime/src/main/java/org/apache/asterix/runtime/utils/ClusterStateManager.java?ref=e0d8e5078f90823e8dd51052317a7da1c08cc9f9",
                "deletions": 1,
                "filename": "asterixdb/asterix-runtime/src/main/java/org/apache/asterix/runtime/utils/ClusterStateManager.java",
                "patch": "@@ -154,6 +154,12 @@ public synchronized void updateClusterPartition(Integer partitionNum, String act\n     @Override\n     public synchronized void refreshState() throws HyracksDataException {\n         resetClusterPartitionConstraint();\n+        if (clusterPartitions.isEmpty()) {\n+            LOGGER.info(\"Cluster does not have any registered partitions\");\n+            setState(ClusterState.UNUSABLE);\n+            return;\n+        }\n+\n         for (ClusterPartition p : clusterPartitions.values()) {\n             if (!p.isActive()) {\n                 setState(ClusterState.UNUSABLE);\n@@ -368,10 +374,16 @@ public synchronized void registerNodePartitions(String nodeId, ClusterPartition[\n             clusterPartitions.put(nodePartition.getPartitionId(), nodePartition);\n         }\n         node2PartitionsMap.put(nodeId, nodePartitions);\n+        //TODO fix exception propagation from refreshState\n+        try {\n+            refreshState();\n+        } catch (HyracksDataException e) {\n+            throw new AsterixException(e);\n+        }\n     }\n \n     @Override\n-    public synchronized void deregisterNodePartitions(String nodeId) {\n+    public synchronized void deregisterNodePartitions(String nodeId) throws HyracksDataException {\n         ClusterPartition[] nodePartitions = node2PartitionsMap.remove(nodeId);\n         if (nodePartitions == null) {\n             LOGGER.info(\"deregisterNodePartitions unknown node \" + nodeId + \" (already removed?)\");\n@@ -382,6 +394,7 @@ public synchronized void deregisterNodePartitions(String nodeId) {\n             for (ClusterPartition nodePartition : nodePartitions) {\n                 clusterPartitions.remove(nodePartition.getPartitionId());\n             }\n+            refreshState();\n         }\n     }\n ",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0d8e5078f90823e8dd51052317a7da1c08cc9f9/asterixdb/asterix-runtime/src/main/java/org/apache/asterix/runtime/utils/ClusterStateManager.java",
                "sha": "8156a231d3ae1529282964c6f2a2ed10ebe68d4a",
                "status": "modified"
            }
        ],
        "message": "[ASTERIXDB-2019][CLUS] Update cluster state on partitions changes\n\n- user model changes: no\n- storage format changes: no\n- interface changes: no\n\nDetails:\n- Set the cluster to UNUSABLE when no partitions are registered\n- Update cluster state after partitions register/de-register\n- Reject unregistered nodes queries on CC\n- Avoid NPE when trying to send to a node that was de-registered\n\nChange-Id: I7d11733a1dcd86136e157d80517bff4abcfc776b\nReviewed-on: https://asterix-gerrit.ics.uci.edu/1918\nSonar-Qube: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nTested-by: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nContrib: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nIntegration-Tests: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: Michael Blow <mblow@apache.org>",
        "parent": "https://github.com/apache/asterixdb/commit/f94f63d3f8e52cdb099cce365b9fb053050969cf",
        "repo": "asterixdb",
        "unit_tests": [
            "ClusterStateManagerTest.java"
        ]
    },
    "asterixdb_e0e85a3": {
        "bug_id": "asterixdb_e0e85a3",
        "commit": "https://github.com/apache/asterixdb/commit/e0e85a39e612d9f6c91da5ac6080c741e582a6b5",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/asterixdb/asterix-app/src/main/java/org/apache/asterix/hyracks/bootstrap/CCApplication.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/main/java/org/apache/asterix/hyracks/bootstrap/CCApplication.java?ref=e0e85a39e612d9f6c91da5ac6080c741e582a6b5",
                "deletions": 1,
                "filename": "asterixdb/asterix-app/src/main/java/org/apache/asterix/hyracks/bootstrap/CCApplication.java",
                "patch": "@@ -123,7 +123,7 @@ public void start(IServiceContext serviceCtx, String[] args) throws Exception {\n                 .create(ClusterProperties.INSTANCE.getCluster(), repStrategy, ccServiceCtx);\n         ExternalLibraryUtils.setUpExternaLibraries(libraryManager, false);\n         componentProvider = new StorageComponentProvider();\n-        GlobalRecoveryManager.instantiate((HyracksConnection) getHcc(), componentProvider);\n+        GlobalRecoveryManager.instantiate(ccServiceCtx, getHcc(), componentProvider);\n         appCtx = new CcApplicationContext(ccServiceCtx, getHcc(), libraryManager, resourceIdManager,\n                 () -> MetadataManager.INSTANCE, GlobalRecoveryManager.instance(), ftStrategy,\n                 new ActiveLifecycleListener());",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/asterixdb/asterix-app/src/main/java/org/apache/asterix/hyracks/bootstrap/CCApplication.java",
                "sha": "bf7d5ebfa66547f29f4c653c2d0bd0e819fa3a1d",
                "status": "modified"
            },
            {
                "additions": 126,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/asterixdb/asterix-app/src/main/java/org/apache/asterix/hyracks/bootstrap/GlobalRecoveryManager.java",
                "changes": 250,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/main/java/org/apache/asterix/hyracks/bootstrap/GlobalRecoveryManager.java?ref=e0e85a39e612d9f6c91da5ac6080c741e582a6b5",
                "deletions": 124,
                "filename": "asterixdb/asterix-app/src/main/java/org/apache/asterix/hyracks/bootstrap/GlobalRecoveryManager.java",
                "patch": "@@ -45,7 +45,8 @@\n import org.apache.asterix.metadata.utils.ExternalIndexingOperations;\n import org.apache.asterix.metadata.utils.MetadataConstants;\n import org.apache.asterix.runtime.utils.ClusterStateManager;\n-import org.apache.hyracks.api.client.HyracksConnection;\n+import org.apache.hyracks.api.application.ICCServiceContext;\n+import org.apache.hyracks.api.client.IHyracksClientConnection;\n import org.apache.hyracks.api.job.JobId;\n import org.apache.hyracks.api.job.JobSpecification;\n \n@@ -55,10 +56,13 @@\n     private static GlobalRecoveryManager instance;\n     private static ClusterState state;\n     private final IStorageComponentProvider componentProvider;\n-    private HyracksConnection hcc;\n+    private final ICCServiceContext ccServiceCtx;\n+    private IHyracksClientConnection hcc;\n \n-    private GlobalRecoveryManager(HyracksConnection hcc, IStorageComponentProvider componentProvider) {\n+    private GlobalRecoveryManager(ICCServiceContext ccServiceCtx, IHyracksClientConnection hcc,\n+                                  IStorageComponentProvider componentProvider) {\n         setState(ClusterState.UNUSABLE);\n+        this.ccServiceCtx = ccServiceCtx;\n         this.hcc = hcc;\n         this.componentProvider = componentProvider;\n     }\n@@ -97,142 +101,140 @@ public void startGlobalRecovery(ICcApplicationContext appCtx) {\n         final ClusterState newState = ClusterStateManager.INSTANCE.getState();\n         boolean needToRecover = !newState.equals(state) && (newState == ClusterState.ACTIVE);\n         if (needToRecover) {\n-            Thread recoveryThread = new Thread(new Runnable() {\n-                @Override\n-                public void run() {\n-                    LOGGER.info(\"Starting AsterixDB's Global Recovery\");\n-                    MetadataTransactionContext mdTxnCtx = null;\n-                    try {\n-                        Thread.sleep(4000);\n-                        MetadataManager.INSTANCE.init();\n-                        // Loop over datasets\n-                        mdTxnCtx = MetadataManager.INSTANCE.beginTransaction();\n-                        List<Dataverse> dataverses = MetadataManager.INSTANCE.getDataverses(mdTxnCtx);\n-                        for (Dataverse dataverse : dataverses) {\n-                            if (!dataverse.getDataverseName().equals(MetadataConstants.METADATA_DATAVERSE_NAME)) {\n-                                MetadataProvider metadataProvider =\n-                                        new MetadataProvider(appCtx, dataverse, componentProvider);\n-                                try {\n-                                    List<Dataset> datasets = MetadataManager.INSTANCE.getDataverseDatasets(mdTxnCtx,\n-                                            dataverse.getDataverseName());\n-                                    for (Dataset dataset : datasets) {\n-                                        if (dataset.getDatasetType() == DatasetType.EXTERNAL) {\n-                                            // External dataset\n-                                            // Get indexes\n-                                            List<Index> indexes = MetadataManager.INSTANCE.getDatasetIndexes(mdTxnCtx,\n-                                                    dataset.getDataverseName(), dataset.getDatasetName());\n-                                            // Get the state of the dataset\n-                                            ExternalDatasetDetails dsd =\n-                                                    (ExternalDatasetDetails) dataset.getDatasetDetails();\n-                                            TransactionState datasetState = dsd.getState();\n-                                            if (!indexes.isEmpty()) {\n-                                                if (datasetState == TransactionState.BEGIN) {\n-                                                    List<ExternalFile> files = MetadataManager.INSTANCE\n-                                                            .getDatasetExternalFiles(mdTxnCtx, dataset);\n-                                                    // if persumed abort, roll backward\n-                                                    // 1. delete all pending files\n-                                                    for (ExternalFile file : files) {\n-                                                        if (file.getPendingOp() != ExternalFilePendingOp.NO_OP) {\n-                                                            MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx, file);\n-                                                        }\n-                                                    }\n-                                                }\n-                                                // 2. clean artifacts in NCs\n-                                                metadataProvider.setMetadataTxnContext(mdTxnCtx);\n-                                                JobSpecification jobSpec = ExternalIndexingOperations\n-                                                        .buildAbortOp(dataset, indexes, metadataProvider);\n-                                                executeHyracksJob(jobSpec);\n-                                                // 3. correct the dataset state\n-                                                ((ExternalDatasetDetails) dataset.getDatasetDetails())\n-                                                        .setState(TransactionState.COMMIT);\n-                                                MetadataManager.INSTANCE.updateDataset(mdTxnCtx, dataset);\n-                                                MetadataManager.INSTANCE.commitTransaction(mdTxnCtx);\n-                                                mdTxnCtx = MetadataManager.INSTANCE.beginTransaction();\n-                                            } else if (datasetState == TransactionState.READY_TO_COMMIT) {\n-                                                List<ExternalFile> files = MetadataManager.INSTANCE\n-                                                        .getDatasetExternalFiles(mdTxnCtx, dataset);\n-                                                // if ready to commit, roll forward\n-                                                // 1. commit indexes in NCs\n-                                                metadataProvider.setMetadataTxnContext(mdTxnCtx);\n-                                                JobSpecification jobSpec = ExternalIndexingOperations\n-                                                        .buildRecoverOp(dataset, indexes, metadataProvider);\n-                                                executeHyracksJob(jobSpec);\n-                                                // 2. add pending files in metadata\n-                                                for (ExternalFile file : files) {\n-                                                    if (file.getPendingOp() == ExternalFilePendingOp.ADD_OP) {\n-                                                        MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx, file);\n-                                                        file.setPendingOp(ExternalFilePendingOp.NO_OP);\n-                                                        MetadataManager.INSTANCE.addExternalFile(mdTxnCtx, file);\n-                                                    } else if (file.getPendingOp() == ExternalFilePendingOp.DROP_OP) {\n-                                                        // find original file\n-                                                        for (ExternalFile originalFile : files) {\n-                                                            if (originalFile.getFileName().equals(file.getFileName())) {\n-                                                                MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx,\n-                                                                        file);\n-                                                                MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx,\n-                                                                        originalFile);\n-                                                                break;\n-                                                            }\n-                                                        }\n-                                                    } else if (file.getPendingOp() == ExternalFilePendingOp.APPEND_OP) {\n-                                                        // find original file\n-                                                        for (ExternalFile originalFile : files) {\n-                                                            if (originalFile.getFileName().equals(file.getFileName())) {\n-                                                                MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx,\n-                                                                        file);\n-                                                                MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx,\n-                                                                        originalFile);\n-                                                                originalFile.setSize(file.getSize());\n-                                                                MetadataManager.INSTANCE.addExternalFile(mdTxnCtx,\n-                                                                        originalFile);\n-                                                            }\n-                                                        }\n-                                                    }\n-                                                    // 3. correct the dataset state\n-                                                    ((ExternalDatasetDetails) dataset.getDatasetDetails())\n-                                                            .setState(TransactionState.COMMIT);\n-                                                    MetadataManager.INSTANCE.updateDataset(mdTxnCtx, dataset);\n-                                                    MetadataManager.INSTANCE.commitTransaction(mdTxnCtx);\n-                                                    mdTxnCtx = MetadataManager.INSTANCE.beginTransaction();\n-                                                }\n-                                            }\n-                                        }\n-                                    }\n-                                } finally {\n-                                    metadataProvider.getLocks().unlock();\n-                                }\n-                            }\n-                        }\n-                        MetadataManager.INSTANCE.commitTransaction(mdTxnCtx);\n-                    } catch (Exception e) {\n-                        // This needs to be fixed <-- Needs to shutdown the system -->\n-                        /*\n-                         * Note: Throwing this illegal state exception will terminate this thread\n-                         * and feeds listeners will not be notified.\n-                         */\n-                        LOGGER.log(Level.SEVERE, \"Global recovery was not completed successfully: \", e);\n+            setState(newState);\n+            ccServiceCtx.getControllerService().getExecutor().submit(() -> {\n+                LOGGER.info(\"Starting Global Recovery\");\n+                MetadataTransactionContext mdTxnCtx = null;\n+                try {\n+                    MetadataManager.INSTANCE.init();\n+                    // Loop over datasets\n+                    mdTxnCtx = MetadataManager.INSTANCE.beginTransaction();\n+                    for (Dataverse dataverse : MetadataManager.INSTANCE.getDataverses(mdTxnCtx)) {\n+                        mdTxnCtx = recoverDataset(appCtx, mdTxnCtx, dataverse);\n+                    }\n+                    MetadataManager.INSTANCE.commitTransaction(mdTxnCtx);\n+                } catch (Exception e) {\n+                    // This needs to be fixed <-- Needs to shutdown the system -->\n+                    /*\n+                     * Note: Throwing this illegal state exception will terminate this thread\n+                     * and feeds listeners will not be notified.\n+                     */\n+                    LOGGER.log(Level.SEVERE, \"Global recovery was not completed successfully: \", e);\n+                    if (mdTxnCtx != null) {\n                         try {\n                             MetadataManager.INSTANCE.abortTransaction(mdTxnCtx);\n                         } catch (Exception e1) {\n                             LOGGER.log(Level.SEVERE, \"Exception in aborting\", e1);\n+                            e1.addSuppressed(e);\n                             throw new IllegalStateException(e1);\n                         }\n                     }\n-                    ClusterStateManager.INSTANCE.setGlobalRecoveryCompleted(true);\n-                    LOGGER.info(\"Global Recovery Completed\");\n                 }\n-            }, \"RecoveryThread\");\n-            setState(newState);\n-            recoveryThread.start();\n+                ClusterStateManager.INSTANCE.setGlobalRecoveryCompleted(true);\n+                LOGGER.info(\"Global Recovery Completed\");\n+            });\n+        }\n+    }\n+\n+    private MetadataTransactionContext recoverDataset(ICcApplicationContext appCtx, MetadataTransactionContext mdTxnCtx,\n+                                                      Dataverse dataverse)\n+            throws Exception {\n+        if (!dataverse.getDataverseName().equals(MetadataConstants.METADATA_DATAVERSE_NAME)) {\n+            MetadataProvider metadataProvider = new MetadataProvider(appCtx, dataverse, componentProvider);\n+            try {\n+                List<Dataset> datasets = MetadataManager.INSTANCE.getDataverseDatasets(mdTxnCtx,\n+                        dataverse.getDataverseName());\n+                for (Dataset dataset : datasets) {\n+                    if (dataset.getDatasetType() == DatasetType.EXTERNAL) {\n+                        // External dataset\n+                        // Get indexes\n+                        List<Index> indexes = MetadataManager.INSTANCE.getDatasetIndexes(mdTxnCtx,\n+                                dataset.getDataverseName(), dataset.getDatasetName());\n+                        // Get the state of the dataset\n+                        ExternalDatasetDetails dsd = (ExternalDatasetDetails) dataset.getDatasetDetails();\n+                        TransactionState datasetState = dsd.getState();\n+                        if (!indexes.isEmpty()) {\n+                            if (datasetState == TransactionState.BEGIN) {\n+                                List<ExternalFile> files = MetadataManager.INSTANCE.getDatasetExternalFiles(mdTxnCtx,\n+                                        dataset);\n+                                // if persumed abort, roll backward\n+                                // 1. delete all pending files\n+                                for (ExternalFile file : files) {\n+                                    if (file.getPendingOp() != ExternalFilePendingOp.NO_OP) {\n+                                        MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx, file);\n+                                    }\n+                                }\n+                            }\n+                            // 2. clean artifacts in NCs\n+                            metadataProvider.setMetadataTxnContext(mdTxnCtx);\n+                            JobSpecification jobSpec = ExternalIndexingOperations.buildAbortOp(dataset, indexes,\n+                                    metadataProvider);\n+                            executeHyracksJob(jobSpec);\n+                            // 3. correct the dataset state\n+                            ((ExternalDatasetDetails) dataset.getDatasetDetails()).setState(TransactionState.COMMIT);\n+                            MetadataManager.INSTANCE.updateDataset(mdTxnCtx, dataset);\n+                            MetadataManager.INSTANCE.commitTransaction(mdTxnCtx);\n+                            mdTxnCtx = MetadataManager.INSTANCE.beginTransaction();\n+                        } else if (datasetState == TransactionState.READY_TO_COMMIT) {\n+                            List<ExternalFile> files = MetadataManager.INSTANCE.getDatasetExternalFiles(mdTxnCtx,\n+                                    dataset);\n+                            // if ready to commit, roll forward\n+                            // 1. commit indexes in NCs\n+                            metadataProvider.setMetadataTxnContext(mdTxnCtx);\n+                            JobSpecification jobSpec = ExternalIndexingOperations.buildRecoverOp(dataset, indexes,\n+                                    metadataProvider);\n+                            executeHyracksJob(jobSpec);\n+                            // 2. add pending files in metadata\n+                            for (ExternalFile file : files) {\n+                                if (file.getPendingOp() == ExternalFilePendingOp.ADD_OP) {\n+                                    MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx, file);\n+                                    file.setPendingOp(ExternalFilePendingOp.NO_OP);\n+                                    MetadataManager.INSTANCE.addExternalFile(mdTxnCtx, file);\n+                                } else if (file.getPendingOp() == ExternalFilePendingOp.DROP_OP) {\n+                                    // find original file\n+                                    for (ExternalFile originalFile : files) {\n+                                        if (originalFile.getFileName().equals(file.getFileName())) {\n+                                            MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx, file);\n+                                            MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx, originalFile);\n+                                            break;\n+                                        }\n+                                    }\n+                                } else if (file.getPendingOp() == ExternalFilePendingOp.APPEND_OP) {\n+                                    // find original file\n+                                    for (ExternalFile originalFile : files) {\n+                                        if (originalFile.getFileName().equals(file.getFileName())) {\n+                                            MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx, file);\n+                                            MetadataManager.INSTANCE.dropExternalFile(mdTxnCtx, originalFile);\n+                                            originalFile.setSize(file.getSize());\n+                                            MetadataManager.INSTANCE.addExternalFile(mdTxnCtx, originalFile);\n+                                        }\n+                                    }\n+                                }\n+                                // 3. correct the dataset state\n+                                ((ExternalDatasetDetails) dataset.getDatasetDetails())\n+                                        .setState(TransactionState.COMMIT);\n+                                MetadataManager.INSTANCE.updateDataset(mdTxnCtx, dataset);\n+                                MetadataManager.INSTANCE.commitTransaction(mdTxnCtx);\n+                                mdTxnCtx = MetadataManager.INSTANCE.beginTransaction();\n+                            }\n+                        }\n+                    }\n+                }\n+            } finally {\n+                metadataProvider.getLocks().unlock();\n+            }\n         }\n+\n+        return mdTxnCtx;\n     }\n \n     public static GlobalRecoveryManager instance() {\n         return instance;\n     }\n \n-    public static synchronized void instantiate(HyracksConnection hcc, IStorageComponentProvider componentProvider) {\n-        instance = new GlobalRecoveryManager(hcc, componentProvider);\n+    public static synchronized void instantiate(ICCServiceContext ccServiceCtx, IHyracksClientConnection hcc,\n+                                                IStorageComponentProvider componentProvider) {\n+        instance = new GlobalRecoveryManager(ccServiceCtx, hcc, componentProvider);\n     }\n \n     public static synchronized void setState(ClusterState state) {",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/asterixdb/asterix-app/src/main/java/org/apache/asterix/hyracks/bootstrap/GlobalRecoveryManager.java",
                "sha": "1816a25292d317ac232f7790cd7bbc6f902bafad",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/asterixdb/asterix-app/src/test/java/org/apache/asterix/test/common/TestExecutor.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-app/src/test/java/org/apache/asterix/test/common/TestExecutor.java?ref=e0e85a39e612d9f6c91da5ac6080c741e582a6b5",
                "deletions": 4,
                "filename": "asterixdb/asterix-app/src/test/java/org/apache/asterix/test/common/TestExecutor.java",
                "patch": "@@ -45,6 +45,7 @@\n import java.util.concurrent.Executors;\n import java.util.concurrent.Future;\n import java.util.concurrent.TimeUnit;\n+import java.util.function.Predicate;\n import java.util.logging.Level;\n import java.util.logging.Logger;\n import java.util.regex.Matcher;\n@@ -404,7 +405,12 @@ private static void writeOutputToFile(File actualFile, InputStream resultStream)\n     }\n \n     protected HttpResponse executeAndCheckHttpRequest(HttpUriRequest method) throws Exception {\n-        return checkResponse(executeHttpRequest(method));\n+        return checkResponse(executeHttpRequest(method), code -> code == HttpStatus.SC_OK);\n+    }\n+\n+    protected HttpResponse executeAndCheckHttpRequest(HttpUriRequest method, Predicate<Integer> responseCodeValidator)\n+            throws Exception {\n+        return checkResponse(executeHttpRequest(method), responseCodeValidator);\n     }\n \n     protected HttpResponse executeHttpRequest(HttpUriRequest method) throws Exception {\n@@ -418,8 +424,9 @@ protected HttpResponse executeHttpRequest(HttpUriRequest method) throws Exceptio\n         }\n     }\n \n-    protected HttpResponse checkResponse(HttpResponse httpResponse) throws Exception {\n-        if (httpResponse.getStatusLine().getStatusCode() != HttpStatus.SC_OK) {\n+    protected HttpResponse checkResponse(HttpResponse httpResponse, Predicate<Integer> responseCodeValidator)\n+            throws Exception {\n+        if (!responseCodeValidator.test(httpResponse.getStatusLine().getStatusCode())) {\n             String errorBody = EntityUtils.toString(httpResponse.getEntity());\n             String exceptionMsg;\n             try {\n@@ -582,8 +589,13 @@ public InputStream executeJSONGet(OutputFormat fmt, URI uri) throws Exception {\n     }\n \n     public InputStream executeJSONPost(OutputFormat fmt, URI uri) throws Exception {\n+        return executeJSONPost(fmt, uri, code -> code == HttpStatus.SC_OK);\n+    }\n+\n+    public InputStream executeJSONPost(OutputFormat fmt, URI uri, Predicate<Integer> responseCodeValidator)\n+            throws Exception {\n         HttpUriRequest request = constructPostMethod(uri, fmt, new ArrayList<>());\n-        HttpResponse response = executeAndCheckHttpRequest(request);\n+        HttpResponse response = executeAndCheckHttpRequest(request, responseCodeValidator);\n         return response.getEntity().getContent();\n     }\n ",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/asterixdb/asterix-app/src/test/java/org/apache/asterix/test/common/TestExecutor.java",
                "sha": "e88f64716a95b13c21ca2a1eeb347c7dbc89374f",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/asterixdb/asterix-runtime/src/main/java/org/apache/asterix/runtime/utils/ClusterStateManager.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/asterixdb/asterix-runtime/src/main/java/org/apache/asterix/runtime/utils/ClusterStateManager.java?ref=e0e85a39e612d9f6c91da5ac6080c741e582a6b5",
                "deletions": 12,
                "filename": "asterixdb/asterix-runtime/src/main/java/org/apache/asterix/runtime/utils/ClusterStateManager.java",
                "patch": "@@ -26,6 +26,7 @@\n import java.util.Map;\n import java.util.Set;\n import java.util.SortedMap;\n+import java.util.TreeSet;\n import java.util.concurrent.TimeUnit;\n import java.util.logging.Level;\n import java.util.logging.Logger;\n@@ -41,6 +42,7 @@\n import org.apache.hyracks.api.config.IOption;\n import org.apache.hyracks.api.exceptions.HyracksDataException;\n import org.apache.hyracks.api.exceptions.HyracksException;\n+import org.apache.hyracks.control.cc.ClusterControllerService;\n import org.apache.hyracks.control.common.controllers.NCConfig;\n \n import com.fasterxml.jackson.databind.ObjectMapper;\n@@ -303,24 +305,27 @@ public synchronized ObjectNode getClusterStateDescription() {\n         stateDescription.put(\"metadata_node\", currentMetadataNode);\n         ArrayNode ncs = om.createArrayNode();\n         stateDescription.set(\"ncs\", ncs);\n-        for (Map.Entry<String, ClusterPartition[]> entry : node2PartitionsMap.entrySet()) {\n+        for (String node : new TreeSet<>(((ClusterControllerService) appCtx.getServiceContext().getControllerService())\n+                .getNodeManager().getAllNodeIds())) {\n             ObjectNode nodeJSON = om.createObjectNode();\n-            nodeJSON.put(\"node_id\", entry.getKey());\n+            nodeJSON.put(\"node_id\", node);\n             boolean allActive = true;\n             boolean anyActive = false;\n             Set<Map<String, Object>> partitions = new HashSet<>();\n-            for (ClusterPartition part : entry.getValue()) {\n-                HashMap<String, Object> partition = new HashMap<>();\n-                partition.put(\"partition_id\", \"partition_\" + part.getPartitionId());\n-                partition.put(\"active\", part.isActive());\n-                partitions.add(partition);\n-                allActive = allActive && part.isActive();\n-                if (allActive) {\n-                    anyActive = true;\n+            if (node2PartitionsMap.containsKey(node)) {\n+                for (ClusterPartition part : node2PartitionsMap.get(node)) {\n+                    HashMap<String, Object> partition = new HashMap<>();\n+                    partition.put(\"partition_id\", \"partition_\" + part.getPartitionId());\n+                    partition.put(\"active\", part.isActive());\n+                    partitions.add(partition);\n+                    allActive = allActive && part.isActive();\n+                    if (allActive) {\n+                        anyActive = true;\n+                    }\n                 }\n             }\n-            nodeJSON.put(\"state\", failedNodes.contains(entry.getKey()) ? \"FAILED\"\n-                    : allActive ? \"ACTIVE\" : anyActive ? \"PARTIALLY_ACTIVE\" : \"INACTIVE\");\n+            nodeJSON.put(\"state\", failedNodes.contains(node) ? \"FAILED\"\n+                    : allActive && anyActive ? \"ACTIVE\" : anyActive ? \"PARTIALLY_ACTIVE\" : \"INACTIVE\");\n             nodeJSON.putPOJO(\"partitions\", partitions);\n             ncs.add(nodeJSON);\n         }",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/asterixdb/asterix-runtime/src/main/java/org/apache/asterix/runtime/utils/ClusterStateManager.java",
                "sha": "48937f8d9da33688e42a8fff76e7cfc24e001a64",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/application/ICCServiceContext.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/application/ICCServiceContext.java?ref=e0e85a39e612d9f6c91da5ac6080c741e582a6b5",
                "deletions": 0,
                "filename": "hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/application/ICCServiceContext.java",
                "patch": "@@ -22,6 +22,7 @@\n \n import org.apache.hyracks.api.context.ICCContext;\n import org.apache.hyracks.api.job.IJobLifecycleListener;\n+import org.apache.hyracks.api.service.IControllerService;\n \n /**\n  * Service Context at the Cluster Controller for an application.",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/application/ICCServiceContext.java",
                "sha": "94ebcfe01cddf021d02635393f147a6af251d679",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/lifecycle/LifeCycleComponentManager.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/lifecycle/LifeCycleComponentManager.java?ref=e0e85a39e612d9f6c91da5ac6080c741e582a6b5",
                "deletions": 24,
                "filename": "hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/lifecycle/LifeCycleComponentManager.java",
                "patch": "@@ -43,24 +43,19 @@\n     private boolean configured;\n \n     public LifeCycleComponentManager() {\n-        components = new ArrayList<ILifeCycleComponent>();\n+        components = new ArrayList<>();\n         stopInitiated = false;\n         configured = false;\n         stopped = false;\n     }\n \n     @Override\n     public void uncaughtException(Thread t, Throwable e) {\n-        if (LOGGER.isLoggable(Level.SEVERE)) {\n-            LOGGER.severe(\"Uncaught Exception from thread \" + t.getName() + \" message: \" + e.getMessage());\n-            e.printStackTrace();\n-        }\n+        LOGGER.log(Level.SEVERE, \"Uncaught Exception from thread \" + t.getName(), e);\n         try {\n             stopAll(true);\n         } catch (IOException e1) {\n-            if (LOGGER.isLoggable(Level.SEVERE)) {\n-                LOGGER.severe(\"Exception in stopping Asterix. \" + e1.getMessage());\n-            }\n+            LOGGER.log(Level.SEVERE, \"Exception in stopping instance\", e1);\n         }\n     }\n \n@@ -79,31 +74,25 @@ public void startAll() {\n     @Override\n     public synchronized void stopAll(boolean dumpState) throws IOException {\n         if (LOGGER.isLoggable(Level.INFO)) {\n-            LOGGER.severe(\"Attempting to stop \" + this);\n+            LOGGER.info(\"Attempting to stop \" + this);\n         }\n         if (stopped) {\n-            if (LOGGER.isLoggable(Level.INFO)) {\n-                LOGGER.severe(\"Lifecycle management was already stopped\");\n-            }\n+            LOGGER.info(\"Lifecycle management was already stopped\");\n             return;\n         }\n         if (stopInitiated) {\n-            if (LOGGER.isLoggable(Level.INFO)) {\n-                LOGGER.severe(\"Stop already in progress\");\n-            }\n+            LOGGER.info(\"Stop already in progress\");\n             return;\n         }\n         if (!configured) {\n             if (LOGGER.isLoggable(Level.SEVERE)) {\n-                LOGGER.severe(\"Lifecycle management not configured\" + this);\n+                LOGGER.severe(\"Lifecycle management not configured \" + this);\n             }\n             return;\n         }\n \n         stopInitiated = true;\n-        if (LOGGER.isLoggable(Level.SEVERE)) {\n-            LOGGER.severe(\"Stopping Asterix instance\");\n-        }\n+        LOGGER.severe(\"Stopping instance\");\n \n         FileOutputStream componentDumpStream = null;\n         String componentDumpPath = null;\n@@ -120,14 +109,12 @@ public synchronized void stopAll(boolean dumpState) throws IOException {\n                     componentDumpStream = new FileOutputStream(f);\n                 }\n                 if (LOGGER.isLoggable(Level.INFO)) {\n-                    LOGGER.info(\"Stopping component instance \" + component.getClass().getName() + \" dump state \"\n-                            + dumpState + \" dump path \" + componentDumpPath);\n+                    LOGGER.info(\"Stopping component instance \" + component.getClass().getName() + \"; dump state: \"\n+                            + dumpState + \", dump path: \" + componentDumpPath);\n                 }\n                 component.stop(dumpState, componentDumpStream);\n             } catch (Exception e) {\n-                if (LOGGER.isLoggable(Level.SEVERE)) {\n-                    LOGGER.severe(\"Exception in stopping component \" + component.getClass().getName() + e.getMessage());\n-                }\n+                LOGGER.log(Level.SEVERE, \"Exception in stopping component \" + component.getClass().getName(), e);\n             } finally {\n                 if (componentDumpStream != null) {\n                     componentDumpStream.close();",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/hyracks-fullstack/hyracks/hyracks-api/src/main/java/org/apache/hyracks/api/lifecycle/LifeCycleComponentManager.java",
                "sha": "4674f9a1b6eb948137cf2593445e8d6d1a7475d7",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/NCDriver.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/NCDriver.java?ref=e0e85a39e612d9f6c91da5ac6080c741e582a6b5",
                "deletions": 5,
                "filename": "hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/NCDriver.java",
                "patch": "@@ -44,12 +44,7 @@ public static void main(String[] args) {\n             application.registerConfig(configManager);\n             NCConfig ncConfig = new NCConfig(nodeId, configManager);\n             final NodeControllerService ncService = new NodeControllerService(ncConfig, application);\n-            if (LOGGER.isLoggable(Level.INFO)) {\n-                LOGGER.info(\"Setting uncaught exception handler \" + ncService.getLifeCycleComponentManager());\n-            }\n-            Thread.currentThread().setUncaughtExceptionHandler(ncService.getLifeCycleComponentManager());\n             ncService.start();\n-            Runtime.getRuntime().addShutdownHook(new NCShutdownHook(ncService));\n             while (true) {\n                 Thread.sleep(10000);\n             }",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/NCDriver.java",
                "sha": "11df079a7dbe721077b1160813746e97cccac95d",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/asterixdb/blob/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/NodeControllerService.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/asterixdb/contents/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/NodeControllerService.java?ref=e0e85a39e612d9f6c91da5ac6080c741e582a6b5",
                "deletions": 0,
                "filename": "hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/NodeControllerService.java",
                "patch": "@@ -256,6 +256,11 @@ private void init() throws Exception {\n     @Override\n     public void start() throws Exception {\n         LOGGER.log(Level.INFO, \"Starting NodeControllerService\");\n+        if (LOGGER.isLoggable(Level.INFO)) {\n+            LOGGER.info(\"Setting uncaught exception handler \" + getLifeCycleComponentManager());\n+        }\n+        Thread.currentThread().setUncaughtExceptionHandler(getLifeCycleComponentManager());\n+        Runtime.getRuntime().addShutdownHook(new NCShutdownHook(this));\n         ipc = new IPCSystem(new InetSocketAddress(ncConfig.getClusterListenAddress(), ncConfig.getClusterListenPort()),\n                 new NodeControllerIPCI(this), new CCNCFunctions.SerializerDeserializer());\n         ipc.start();",
                "raw_url": "https://github.com/apache/asterixdb/raw/e0e85a39e612d9f6c91da5ac6080c741e582a6b5/hyracks-fullstack/hyracks/hyracks-control/hyracks-control-nc/src/main/java/org/apache/hyracks/control/nc/NodeControllerService.java",
                "sha": "be24dbe8f462581b6e2ce494d4b0ca6f0c9bc85f",
                "status": "modified"
            }
        ],
        "message": "Cleanup Logging, Report Joined Nodes, Misc Cleanup\n\n- Minor refactoring of NodeControllerService startup\n- Cleanup logging in GlobalRecoveryManager / LifeCycleComponentManager\n- Enable TestExecutor to accept non-200 status codes\n- Use ExecutorService for GlobalRecovery thread\n- Eliminate NPE when metadata node goes down before global recovery\n  starts\n\nChange-Id: I87b6b45e1a0cdc7a8b77d80b4e603d927aa60b8a\nReviewed-on: https://asterix-gerrit.ics.uci.edu/1706\nTested-by: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nBAD: Jenkins <jenkins@fulliautomatix.ics.uci.edu>\nReviewed-by: Till Westmann <tillw@apache.org>\nIntegration-Tests: Jenkins <jenkins@fulliautomatix.ics.uci.edu>",
        "parent": "https://github.com/apache/asterixdb/commit/6c6479d0e3f3f774703c8e9afa28259d9bd78bf7",
        "repo": "asterixdb",
        "unit_tests": [
            "ClusterStateManagerTest.java"
        ]
    }
}