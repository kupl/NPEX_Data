{
    "hadoop_0753031": {
        "bug_id": "hadoop_0753031",
        "commit": "https://github.com/apache/hadoop/commit/07530314c2130ecd1525682c59bf51f15b82c024",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/07530314c2130ecd1525682c59bf51f15b82c024/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/IpcException.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/IpcException.java?ref=07530314c2130ecd1525682c59bf51f15b82c024",
                "deletions": 3,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/IpcException.java",
                "patch": "@@ -26,9 +26,8 @@\n  */\n public class IpcException extends IOException {\n   private static final long serialVersionUID = 1L;\n-  \n-  final String errMsg;\n+\n   public IpcException(final String err) {\n-    errMsg = err; \n+    super(err);\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/07530314c2130ecd1525682c59bf51f15b82c024/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/IpcException.java",
                "sha": "61c42b80887f0f941885e52bb918b94b4f90d265",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/07530314c2130ecd1525682c59bf51f15b82c024/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java?ref=07530314c2130ecd1525682c59bf51f15b82c024",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "patch": "@@ -2155,7 +2155,7 @@ private void doSaslReply(Message message) throws IOException {\n     private void doSaslReply(Exception ioe) throws IOException {\n       setupResponse(authFailedCall,\n           RpcStatusProto.FATAL, RpcErrorCodeProto.FATAL_UNAUTHORIZED,\n-          null, ioe.getClass().getName(), ioe.getLocalizedMessage());\n+          null, ioe.getClass().getName(), ioe.toString());\n       sendResponse(authFailedCall);\n     }\n \n@@ -2550,7 +2550,8 @@ private void processOneRpc(ByteBuffer bb)\n         final RpcCall call = new RpcCall(this, callId, retry);\n         setupResponse(call,\n             rse.getRpcStatusProto(), rse.getRpcErrorCodeProto(), null,\n-            t.getClass().getName(), t.getMessage());\n+            t.getClass().getName(),\n+            t.getMessage() != null ? t.getMessage() : t.toString());\n         sendResponse(call);\n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/07530314c2130ecd1525682c59bf51f15b82c024/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "sha": "14fc2de530ce6cf2fe4550380c2bfec774ba14e4",
                "status": "modified"
            }
        ],
        "message": "HADOOP-9844. NPE when trying to create an error message response of SASL RPC\n\nThis closes #55\n\nChange-Id: I10a20380565fa89762f4aa564b2f1c83b9aeecdc\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>",
        "parent": "https://github.com/apache/hadoop/commit/98653ecccb80a7d793b2d27c81aebd3347f64b3c",
        "repo": "hadoop",
        "unit_tests": [
            "TestServer.java"
        ]
    },
    "hadoop_0cc98ae": {
        "bug_id": "hadoop_0cc98ae",
        "commit": "https://github.com/apache/hadoop/commit/0cc98ae0ec69419ded066f3f7decf59728b35e9d",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/0cc98ae0ec69419ded066f3f7decf59728b35e9d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java?ref=0cc98ae0ec69419ded066f3f7decf59728b35e9d",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "patch": "@@ -367,6 +367,8 @@ protected void serviceInit(Configuration conf) throws Exception {\n     \n     this.aclsManager = new ApplicationACLsManager(conf);\n \n+    this.dirsHandler = new LocalDirsHandlerService(metrics);\n+\n     boolean isDistSchedulingEnabled =\n         conf.getBoolean(YarnConfiguration.DIST_SCHEDULING_ENABLED,\n             YarnConfiguration.DEFAULT_DIST_SCHEDULING_ENABLED);\n@@ -390,7 +392,6 @@ protected void serviceInit(Configuration conf) throws Exception {\n     // NodeManager level dispatcher\n     this.dispatcher = new AsyncDispatcher(\"NM Event dispatcher\");\n \n-    dirsHandler = new LocalDirsHandlerService(metrics);\n     nodeHealthChecker =\n         new NodeHealthCheckerService(\n             getNodeHealthScriptRunner(conf), dirsHandler);",
                "raw_url": "https://github.com/apache/hadoop/raw/0cc98ae0ec69419ded066f3f7decf59728b35e9d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "sha": "bddc7c34ee7afd05a10babf81e762e32008f8cca",
                "status": "modified"
            }
        ],
        "message": "YARN-7396. NPE when accessing container logs due to null dirsHandler. Contributed by Jonathan Hung",
        "parent": "https://github.com/apache/hadoop/commit/7a49ddfdde2e2a7b407f4a62a42d97bfe456075a",
        "repo": "hadoop",
        "unit_tests": [
            "TestNodeManager.java"
        ]
    },
    "hadoop_1415ad3": {
        "bug_id": "hadoop_1415ad3",
        "commit": "https://github.com/apache/hadoop/commit/1415ad3800d117b4fff6ad0ef281acc7051a0bcf",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/1415ad3800d117b4fff6ad0ef281acc7051a0bcf/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java?ref=1415ad3800d117b4fff6ad0ef281acc7051a0bcf",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "patch": "@@ -584,6 +584,7 @@ public boolean hasKerberosCredentials() {\n   @InterfaceAudience.Public\n   @InterfaceStability.Evolving\n   public static UserGroupInformation getCurrentUser() throws IOException {\n+    ensureInitialized();\n     AccessControlContext context = AccessController.getContext();\n     Subject subject = Subject.getSubject(context);\n     if (subject == null || subject.getPrincipals(User.class).isEmpty()) {\n@@ -670,6 +671,7 @@ public static UserGroupInformation getUGIFromSubject(Subject subject)\n   @InterfaceAudience.Public\n   @InterfaceStability.Evolving\n   public static UserGroupInformation getLoginUser() throws IOException {\n+    ensureInitialized();\n     UserGroupInformation loginUser = loginUserRef.get();\n     // a potential race condition exists only for the initial creation of\n     // the login user.  there's no need to penalize all subsequent calls",
                "raw_url": "https://github.com/apache/hadoop/raw/1415ad3800d117b4fff6ad0ef281acc7051a0bcf/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "sha": "884380c43f236f365598139f5bf13a55ead30c42",
                "status": "modified"
            }
        ],
        "message": "HADOOP-16707. NPE in UGI.getCurrentUser in ITestAbfsIdentityTransformer setup.\n\nContributed by Steve Loughran.\r\n\r\nChange-Id: I38fdba2fa70e534d78b15e61de19368912588b0c",
        "parent": "https://github.com/apache/hadoop/commit/dfc61d8ea5971e9742af77fcae2cae28011ae0c9",
        "repo": "hadoop",
        "unit_tests": [
            "TestUserGroupInformation.java"
        ]
    },
    "hadoop_14384f5": {
        "bug_id": "hadoop_14384f5",
        "commit": "https://github.com/apache/hadoop/commit/14384f5b5142a98a10ce4bffadeb13e89bda9365",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/14384f5b5142a98a10ce4bffadeb13e89bda9365/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=14384f5b5142a98a10ce4bffadeb13e89bda9365",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -482,6 +482,9 @@ Release 2.8.0 - UNRELEASED\n     HDFS-7939. Two fsimage_rollback_* files are created which are not deleted\n     after rollback. (J.Andreina via vinayakumarb)\n \n+    HDFS-8111. NPE thrown when invalid FSImage filename given for\n+    'hdfs oiv_legacy' cmd ( surendra singh lilhore via vinayakumarb )\n+\n Release 2.7.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/14384f5b5142a98a10ce4bffadeb13e89bda9365/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "abbfe6ae4e397ab25521f12df9b00a80a705d4ba",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/14384f5b5142a98a10ce4bffadeb13e89bda9365/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewer.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewer.java?ref=14384f5b5142a98a10ce4bffadeb13e89bda9365",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewer.java",
                "patch": "@@ -137,7 +137,11 @@ public void go() throws IOException  {\n       done = true;\n     } finally {\n       if (!done) {\n-        LOG.error(\"image loading failed at offset \" + tracker.getPos());\n+        if (tracker != null) {\n+          LOG.error(\"image loading failed at offset \" + tracker.getPos());\n+        } else {\n+          LOG.error(\"Failed to load image file.\");\n+        }\n       }\n       IOUtils.cleanup(LOG, in, tracker);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/14384f5b5142a98a10ce4bffadeb13e89bda9365/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewer.java",
                "sha": "7f81ba8e67fa6622b5e879eca44ff6bcf4d0417f",
                "status": "modified"
            }
        ],
        "message": "HDFS-8111. NPE thrown when invalid FSImage filename given for 'hdfs oiv_legacy' cmd ( Contributed by surendra singh lilhore )",
        "parent": "https://github.com/apache/hadoop/commit/f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b",
        "repo": "hadoop",
        "unit_tests": [
            "TestOfflineImageViewer.java"
        ]
    },
    "hadoop_1fbb662": {
        "bug_id": "hadoop_1fbb662",
        "commit": "https://github.com/apache/hadoop/commit/1fbb662c7092d08a540acff7e92715693412e486",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/1fbb662c7092d08a540acff7e92715693412e486/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=1fbb662c7092d08a540acff7e92715693412e486",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -4487,8 +4487,12 @@ private void scanAndCompactStorages() throws InterruptedException {\n         for (int i = 0; i < datanodesAndStorages.size(); i += 2) {\n           namesystem.writeLock();\n           try {\n-            DatanodeStorageInfo storage = datanodeManager.\n-                getDatanode(datanodesAndStorages.get(i)).\n+            final DatanodeDescriptor dn = datanodeManager.\n+                getDatanode(datanodesAndStorages.get(i));\n+            if (dn == null) {\n+              continue;\n+            }\n+            final DatanodeStorageInfo storage = dn.\n                 getStorageInfo(datanodesAndStorages.get(i + 1));\n             if (storage != null) {\n               boolean aborted =",
                "raw_url": "https://github.com/apache/hadoop/raw/1fbb662c7092d08a540acff7e92715693412e486/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "e83cbc6ef4a266939307a3b13b46c9b5765d07c9",
                "status": "modified"
            }
        ],
        "message": "HDFS-12363. Possible NPE in BlockManager$StorageInfoDefragmenter#scanAndCompactStorages. Contributed by Xiao Chen",
        "parent": "https://github.com/apache/hadoop/commit/7ecc6dbed62c80397f71949bee41dcd03065755c",
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockManager.java"
        ]
    },
    "hadoop_2044967": {
        "bug_id": "hadoop_2044967",
        "commit": "https://github.com/apache/hadoop/commit/2044967e7581f00c3f6378860426a69078faf694",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/2044967e7581f00c3f6378860426a69078faf694/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/YarnCLI.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/YarnCLI.java?ref=2044967e7581f00c3f6378860426a69078faf694",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/YarnCLI.java",
                "patch": "@@ -72,6 +72,10 @@ public void setClient(YarnClient client) {\n   }\n \n   public void stop() {\n-    this.client.stop();\n+    // this.client may be null when it is called before\n+    // invoking `createAndStartYarnClient`\n+    if (this.client != null) {\n+      this.client.stop();\n+    }\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/2044967e7581f00c3f6378860426a69078faf694/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/YarnCLI.java",
                "sha": "c1e02d5fd1ea0ff54c0cb35de9498a5f92276488",
                "status": "modified"
            }
        ],
        "message": "YARN-9246 NPE when executing a command yarn node -status or -states without additional arguments. Contributed by Masahiro Tanaka",
        "parent": "https://github.com/apache/hadoop/commit/194f0b49fb8ae3b876dde82f04f906f0ab0f5bdf",
        "repo": "hadoop",
        "unit_tests": [
            "TestYarnCLI.java"
        ]
    },
    "hadoop_229472c": {
        "bug_id": "hadoop_229472c",
        "commit": "https://github.com/apache/hadoop/commit/229472cea7920194c48f5294bf763a8bee2ade63",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/229472cea7920194c48f5294bf763a8bee2ade63/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=229472cea7920194c48f5294bf763a8bee2ade63",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -439,6 +439,9 @@ Release 2.3.0 - UNRELEASED\n \n     HADOOP-10100. MiniKDC shouldn't use apacheds-all artifact. (rkanter via tucu)\n \n+    HADOOP-10107. Server.getNumOpenConnections may throw NPE. (Kihwal Lee via\n+    jing9)\n+\n Release 2.2.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/229472cea7920194c48f5294bf763a8bee2ade63/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "2b9aeb88a5a759ab34e1eccab28d261d53a65826",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/229472cea7920194c48f5294bf763a8bee2ade63/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java?ref=229472cea7920194c48f5294bf763a8bee2ade63",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "patch": "@@ -2109,6 +2109,7 @@ protected Server(String bindAddress, int port,\n     // Start the listener here and let it bind to the port\n     listener = new Listener();\n     this.port = listener.getAddress().getPort();    \n+    connectionManager = new ConnectionManager();\n     this.rpcMetrics = RpcMetrics.create(this);\n     this.rpcDetailedMetrics = RpcDetailedMetrics.create(this.port);\n     this.tcpNoDelay = conf.getBoolean(\n@@ -2117,7 +2118,6 @@ protected Server(String bindAddress, int port,\n \n     // Create the responder here\n     responder = new Responder();\n-    connectionManager = new ConnectionManager();\n     \n     if (secretManager != null) {\n       SaslRpcServer.init(conf);",
                "raw_url": "https://github.com/apache/hadoop/raw/229472cea7920194c48f5294bf763a8bee2ade63/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "sha": "7fb395cdb056029d85a7940a7e7fbedd941a5a4c",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10107. Server.getNumOpenConnections may throw NPE. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1543335 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/aa002344d0466a62672eae73cdb2bb2ae7c19a72",
        "repo": "hadoop",
        "unit_tests": [
            "TestServer.java"
        ]
    },
    "hadoop_22d7d1f": {
        "bug_id": "hadoop_22d7d1f",
        "commit": "https://github.com/apache/hadoop/commit/22d7d1f8bfe64ee04a7611b004ece8a4d4e81ea4",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/22d7d1f8bfe64ee04a7611b004ece8a4d4e81ea4/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/impl/ZookeeperFederationStateStore.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/impl/ZookeeperFederationStateStore.java?ref=22d7d1f8bfe64ee04a7611b004ece8a4d4e81ea4",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/impl/ZookeeperFederationStateStore.java",
                "patch": "@@ -422,6 +422,10 @@ public GetSubClusterPoliciesConfigurationsResponse getPoliciesConfigurations(\n     try {\n       for (String child : zkManager.getChildren(policiesZNode)) {\n         SubClusterPolicyConfiguration policy = getPolicy(child);\n+        if (policy == null) {\n+          LOG.warn(\"Policy for queue: {} does not exist.\", child);\n+          continue;\n+        }\n         result.add(policy);\n       }\n     } catch (Exception e) {",
                "raw_url": "https://github.com/apache/hadoop/raw/22d7d1f8bfe64ee04a7611b004ece8a4d4e81ea4/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/impl/ZookeeperFederationStateStore.java",
                "sha": "39f498c73a21e8bc4b8804952741b83590aeb91a",
                "status": "modified"
            }
        ],
        "message": "YARN-9601.Potential NPE in ZookeeperFederationStateStore#getPoliciesConfigurations (#908) Contributed by hunshenshi.",
        "parent": "https://github.com/apache/hadoop/commit/b0131bc265453051820e54908e70d39433c227ab",
        "repo": "hadoop",
        "unit_tests": [
            "TestZookeeperFederationStateStore.java"
        ]
    },
    "hadoop_251f528": {
        "bug_id": "hadoop_251f528",
        "commit": "https://github.com/apache/hadoop/commit/251f528814c4a4647cac0af6effb9a73135db180",
        "file": [
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/251f528814c4a4647cac0af6effb9a73135db180/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java?ref=251f528814c4a4647cac0af6effb9a73135db180",
                "deletions": 14,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "patch": "@@ -44,6 +44,7 @@\n import org.apache.hadoop.yarn.server.api.protocolrecords.NMContainerStatus;\n import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n import org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRunningOnNodeEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent;\n@@ -737,21 +738,22 @@ public void transition(RMContainerImpl container, RMContainerEvent event) {\n \n     private static void updateAttemptMetrics(RMContainerImpl container) {\n       Resource resource = container.getContainer().getResource();\n-      RMAppAttempt rmAttempt = container.rmContext.getRMApps()\n-          .get(container.getApplicationAttemptId().getApplicationId())\n-          .getCurrentAppAttempt();\n-\n-      if (rmAttempt != null) {\n-        long usedMillis = container.finishTime - container.creationTime;\n-        rmAttempt.getRMAppAttemptMetrics()\n-            .updateAggregateAppResourceUsage(resource, usedMillis);\n-        // If this is a preempted container, update preemption metrics\n-        if (ContainerExitStatus.PREEMPTED == container.finishedStatus\n-            .getExitStatus()) {\n+      RMApp app = container.rmContext.getRMApps()\n+          .get(container.getApplicationAttemptId().getApplicationId());\n+      if (app != null) {\n+        RMAppAttempt rmAttempt = app.getCurrentAppAttempt();\n+        if (rmAttempt != null) {\n+          long usedMillis = container.finishTime - container.creationTime;\n           rmAttempt.getRMAppAttemptMetrics()\n-              .updatePreemptionInfo(resource, container);\n-          rmAttempt.getRMAppAttemptMetrics()\n-              .updateAggregatePreemptedAppResourceUsage(resource, usedMillis);\n+              .updateAggregateAppResourceUsage(resource, usedMillis);\n+          // If this is a preempted container, update preemption metrics\n+          if (ContainerExitStatus.PREEMPTED == container.finishedStatus\n+              .getExitStatus()) {\n+            rmAttempt.getRMAppAttemptMetrics()\n+                .updatePreemptionInfo(resource, container);\n+            rmAttempt.getRMAppAttemptMetrics()\n+                .updateAggregatePreemptedAppResourceUsage(resource, usedMillis);\n+          }\n         }\n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/251f528814c4a4647cac0af6effb9a73135db180/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "sha": "b5c8e7cb8e975bd66b587c0c8c776a2b033a0f7c",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/251f528814c4a4647cac0af6effb9a73135db180/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java?ref=251f528814c4a4647cac0af6effb9a73135db180",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java",
                "patch": "@@ -1241,12 +1241,13 @@ public void incNumAllocatedContainers(NodeType containerType,\n       return;\n     }\n \n-    RMAppAttempt attempt =\n-        rmContext.getRMApps().get(attemptId.getApplicationId())\n-          .getCurrentAppAttempt();\n-    if (attempt != null) {\n-      attempt.getRMAppAttemptMetrics().incNumAllocatedContainers(containerType,\n-        requestType);\n+    RMApp app = rmContext.getRMApps().get(attemptId.getApplicationId());\n+    if (app != null) {\n+      RMAppAttempt attempt = app.getCurrentAppAttempt();\n+      if (attempt != null) {\n+        attempt.getRMAppAttemptMetrics()\n+            .incNumAllocatedContainers(containerType, requestType);\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/251f528814c4a4647cac0af6effb9a73135db180/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java",
                "sha": "005569cf2520751528b05cc317aed9585788e3ad",
                "status": "modified"
            }
        ],
        "message": "YARN-8222. Fix potential NPE when gets RMApp from RM context. Contributed by Tao Yang.",
        "parent": "https://github.com/apache/hadoop/commit/3265b55119d39ecbda6d75be04a9a1bf59c631f1",
        "repo": "hadoop",
        "unit_tests": [
            "TestSchedulerApplicationAttempt.java"
        ]
    },
    "hadoop_296c5de": {
        "bug_id": "hadoop_296c5de",
        "commit": "https://github.com/apache/hadoop/commit/296c5de0cfee88389cf9f90263280b2034e54cd5",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/296c5de0cfee88389cf9f90263280b2034e54cd5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java?ref=296c5de0cfee88389cf9f90263280b2034e54cd5",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "patch": "@@ -247,7 +247,9 @@ public RMContainerImpl(Container container,\n        YarnConfiguration\n                  .DEFAULT_APPLICATION_HISTORY_SAVE_NON_AM_CONTAINER_META_INFO);\n \n-    rmContext.getRMApplicationHistoryWriter().containerStarted(this);\n+    if (container.getId() != null) {\n+      rmContext.getRMApplicationHistoryWriter().containerStarted(this);\n+    }\n \n     // If saveNonAMContainerMetaInfo is true, store system metrics for all\n     // containers. If false, and if this container is marked as the AM, metrics\n@@ -892,6 +894,9 @@ public void setContainerId(ContainerId containerId) {\n     // container creation event to timeline service when id assigned.\n     container.setId(containerId);\n \n+    if (containerId != null) {\n+      rmContext.getRMApplicationHistoryWriter().containerStarted(this);\n+    }\n     // If saveNonAMContainerMetaInfo is true, store system metrics for all\n     // containers. If false, and if this container is marked as the AM, metrics\n     // will still be published for this container, but that calculation happens",
                "raw_url": "https://github.com/apache/hadoop/raw/296c5de0cfee88389cf9f90263280b2034e54cd5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "sha": "f5d8b5b063344eabc7a6fc46b842dd5f223353d6",
                "status": "modified"
            }
        ],
        "message": "YARN-5873. RM crashes with NPE if generic application history is enabled. Contributed by Varun Saxena.",
        "parent": "https://github.com/apache/hadoop/commit/04014c4c739bb4e3bc3fdf9299abc0f47521e8fd",
        "repo": "hadoop",
        "unit_tests": [
            "TestRMContainerImpl.java"
        ]
    },
    "hadoop_303c8dc": {
        "bug_id": "hadoop_303c8dc",
        "commit": "https://github.com/apache/hadoop/commit/303c8dc9b6c853c0939ea9ba14388897cc258071",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/303c8dc9b6c853c0939ea9ba14388897cc258071/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=303c8dc9b6c853c0939ea9ba14388897cc258071",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -3870,9 +3870,13 @@ private void clearCorruptLazyPersistFiles()\n         while (it.hasNext()) {\n           Block b = it.next();\n           BlockInfo blockInfo = blockManager.getStoredBlock(b);\n-          BlockCollection bc = getBlockCollection(blockInfo);\n-          if (bc.getStoragePolicyID() == lpPolicy.getId()) {\n-            filesToDelete.add(bc);\n+          if (blockInfo == null) {\n+            LOG.info(\"Cannot find block info for block \" + b);\n+          } else {\n+            BlockCollection bc = getBlockCollection(blockInfo);\n+            if (bc.getStoragePolicyID() == lpPolicy.getId()) {\n+              filesToDelete.add(bc);\n+            }\n           }\n         }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/303c8dc9b6c853c0939ea9ba14388897cc258071/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "997fd920c9d76c87302ae4da00cd53f4d9ac4a2b",
                "status": "modified"
            }
        ],
        "message": "Fix NPE in LazyPersistFileScrubber. Contributed by Inigo Goiri.",
        "parent": "https://github.com/apache/hadoop/commit/d81372dfad32488e7c46ffcfccdf0aa26bee04a5",
        "repo": "hadoop",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_38d5ca2": {
        "bug_id": "hadoop_38d5ca2",
        "commit": "https://github.com/apache/hadoop/commit/38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -126,6 +126,9 @@ Release 2.4.1 - UNRELEASED\n     YARN-1928. Fixed a race condition in TestAMRMRPCNodeUpdates which caused it\n     to fail occassionally. (Zhijie Shen via vinodkv)\n \n+    YARN-1934. Fixed a potential NPE in ZKRMStateStore caused by handling\n+    Disconnected event from ZK. (Karthik Kambatla via jianhe)\n+\n Release 2.4.0 - 2014-04-07 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344/hadoop-yarn-project/CHANGES.txt",
                "sha": "ed12a7bc8acf7df38decfbfa771b2a7dea07b881",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop/blob/38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java?ref=38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344",
                "deletions": 11,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "patch": "@@ -280,10 +280,9 @@ public String run() throws KeeperException, InterruptedException {\n     }\n   }\n \n-  private void logRootNodeAcls(String prefix) throws KeeperException,\n-      InterruptedException {\n+  private void logRootNodeAcls(String prefix) throws Exception {\n     Stat getStat = new Stat();\n-    List<ACL> getAcls = zkClient.getACL(zkRootNodePath, getStat);\n+    List<ACL> getAcls = getACLWithRetries(zkRootNodePath, getStat);\n \n     StringBuilder builder = new StringBuilder();\n     builder.append(prefix);\n@@ -363,7 +362,7 @@ protected synchronized void storeVersion() throws Exception {\n     String versionNodePath = getNodePath(zkRootNodePath, VERSION_NODE);\n     byte[] data =\n         ((RMStateVersionPBImpl) CURRENT_VERSION_INFO).getProto().toByteArray();\n-    if (zkClient.exists(versionNodePath, true) != null) {\n+    if (existsWithRetries(versionNodePath, true) != null) {\n       setDataWithRetries(versionNodePath, data, -1);\n     } else {\n       createWithRetries(versionNodePath, data, zkAcl, CreateMode.PERSISTENT);\n@@ -374,7 +373,7 @@ protected synchronized void storeVersion() throws Exception {\n   protected synchronized RMStateVersion loadVersion() throws Exception {\n     String versionNodePath = getNodePath(zkRootNodePath, VERSION_NODE);\n \n-    if (zkClient.exists(versionNodePath, true) != null) {\n+    if (existsWithRetries(versionNodePath, true) != null) {\n       byte[] data = getDataWithRetries(versionNodePath, true);\n       RMStateVersion version =\n           new RMStateVersionPBImpl(RMStateVersionProto.parseFrom(data));\n@@ -442,7 +441,8 @@ private void loadRMSequentialNumberState(RMState rmState) throws Exception {\n   }\n \n   private void loadRMDelegationTokenState(RMState rmState) throws Exception {\n-    List<String> childNodes = zkClient.getChildren(delegationTokensRootPath, true);\n+    List<String> childNodes =\n+        getChildrenWithRetries(delegationTokensRootPath, true);\n     for (String childNodeName : childNodes) {\n       String childNodePath =\n           getNodePath(delegationTokensRootPath, childNodeName);\n@@ -567,7 +567,7 @@ public synchronized void updateApplicationStateInternal(ApplicationId appId,\n     }\n     byte[] appStateData = appStateDataPB.getProto().toByteArray();\n \n-    if (zkClient.exists(nodeUpdatePath, true) != null) {\n+    if (existsWithRetries(nodeUpdatePath, true) != null) {\n       setDataWithRetries(nodeUpdatePath, appStateData, -1);\n     } else {\n       createWithRetries(nodeUpdatePath, appStateData, zkAcl,\n@@ -610,7 +610,7 @@ public synchronized void updateApplicationAttemptStateInternal(\n     }\n     byte[] attemptStateData = attemptStateDataPB.getProto().toByteArray();\n \n-    if (zkClient.exists(nodeUpdatePath, true) != null) {\n+    if (existsWithRetries(nodeUpdatePath, true) != null) {\n       setDataWithRetries(nodeUpdatePath, attemptStateData, -1);\n     } else {\n       createWithRetries(nodeUpdatePath, attemptStateData, zkAcl,\n@@ -661,7 +661,7 @@ protected synchronized void removeRMDelegationTokenState(\n       LOG.debug(\"Removing RMDelegationToken_\"\n           + rmDTIdentifier.getSequenceNumber());\n     }\n-    if (zkClient.exists(nodeRemovePath, true) != null) {\n+    if (existsWithRetries(nodeRemovePath, true) != null) {\n       opList.add(Op.delete(nodeRemovePath, -1));\n     } else {\n       LOG.info(\"Attempted to delete a non-existing znode \" + nodeRemovePath);\n@@ -677,7 +677,7 @@ protected void updateRMDelegationTokenAndSequenceNumberInternal(\n     String nodeRemovePath =\n         getNodePath(delegationTokensRootPath, DELEGATION_TOKEN_PREFIX\n             + rmDTIdentifier.getSequenceNumber());\n-    if (zkClient.exists(nodeRemovePath, true) == null) {\n+    if (existsWithRetries(nodeRemovePath, true) == null) {\n       // in case znode doesn't exist\n       addStoreOrUpdateOps(\n           opList, rmDTIdentifier, renewDate, latestSequenceNumber, false);\n@@ -760,7 +760,7 @@ protected synchronized void removeRMDTMasterKeyState(\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Removing RMDelegationKey_\" + delegationKey.getKeyId());\n     }\n-    if (zkClient.exists(nodeRemovePath, true) != null) {\n+    if (existsWithRetries(nodeRemovePath, true) != null) {\n       doMultiWithRetries(Op.delete(nodeRemovePath, -1));\n     } else {\n       LOG.info(\"Attempted to delete a non-existing znode \" + nodeRemovePath);\n@@ -891,6 +891,16 @@ public void setDataWithRetries(final String path, final byte[] data,\n     }.runWithRetries();\n   }\n \n+  private List<ACL> getACLWithRetries(\n+      final String path, final Stat stat) throws Exception {\n+    return new ZKAction<List<ACL>>() {\n+      @Override\n+      public List<ACL> run() throws KeeperException, InterruptedException {\n+        return zkClient.getACL(path, stat);\n+      }\n+    }.runWithRetries();\n+  }\n+\n   private List<String> getChildrenWithRetries(\n       final String path, final boolean watch) throws Exception {\n     return new ZKAction<List<String>>() {\n@@ -901,6 +911,16 @@ public void setDataWithRetries(final String path, final byte[] data,\n     }.runWithRetries();\n   }\n \n+  private Stat existsWithRetries(\n+      final String path, final boolean watch) throws Exception {\n+    return new ZKAction<Stat>() {\n+      @Override\n+      Stat run() throws KeeperException, InterruptedException {\n+        return zkClient.exists(path, watch);\n+      }\n+    }.runWithRetries();\n+  }\n+\n   /**\n    * Helper class that periodically attempts creating a znode to ensure that\n    * this RM continues to be the Active.",
                "raw_url": "https://github.com/apache/hadoop/raw/38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "sha": "9b15bb21e7e62a1dab06a6799dd5a39621c06a4f",
                "status": "modified"
            }
        ],
        "message": "YARN-1934. Fixed a potential NPE in ZKRMStateStore caused by handling Disconnected event from ZK. Contributed by Karthik Kambatla.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1587776 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/9887d7f2502f814200317fa0e516d3ca29ba5ae4",
        "repo": "hadoop",
        "unit_tests": [
            "TestZKRMStateStore.java"
        ]
    },
    "hadoop_3ab2959": {
        "bug_id": "hadoop_3ab2959",
        "commit": "https://github.com/apache/hadoop/commit/3ab295994a4e7870a1f68d742d26c3ac44546fa5",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/3ab295994a4e7870a1f68d742d26c3ac44546fa5/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=3ab295994a4e7870a1f68d742d26c3ac44546fa5",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -19,3 +19,5 @@ HDFS-2231. Configuration changes for HA namenode. (suresh)\n HDFS-2418. Change ConfiguredFailoverProxyProvider to take advantage of HDFS-2231. (atm)\n \n HDFS-2393. Mark appropriate methods of ClientProtocol with the idempotent annotation. (atm)\n+\n+HDFS-2523. Small NN fixes to include HAServiceProtocol and prevent NPE on shutdown. (todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/3ab295994a4e7870a1f68d742d26c3ac44546fa5/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "37e6e4acac308b051fc5814e9a4d03cc957ddcf4",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/3ab295994a4e7870a1f68d742d26c3ac44546fa5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=3ab295994a4e7870a1f68d742d26c3ac44546fa5",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -572,7 +572,9 @@ public void stop() {\n       stopRequested = true;\n     }\n     try {\n-      state.exitState(haContext);\n+      if (state != null) {\n+        state.exitState(haContext);\n+      }\n     } catch (ServiceFailedException e) {\n       LOG.warn(\"Encountered exception while exiting state \", e);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/3ab295994a4e7870a1f68d742d26c3ac44546fa5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "4eb080105f050c928e37e4d27e2071eafa6753b7",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/3ab295994a4e7870a1f68d742d26c3ac44546fa5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java?ref=3ab295994a4e7870a1f68d742d26c3ac44546fa5",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "patch": "@@ -39,6 +39,7 @@\n import org.apache.hadoop.fs.permission.PermissionStatus;\n import static org.apache.hadoop.hdfs.DFSConfigKeys.*;\n \n+import org.apache.hadoop.ha.HAServiceProtocol;\n import org.apache.hadoop.ha.HealthCheckFailedException;\n import org.apache.hadoop.ha.ServiceFailedException;\n import org.apache.hadoop.hdfs.HDFSPolicyProvider;\n@@ -156,6 +157,7 @@ public NameNodeRpcServer(Configuration conf, NameNode nn)\n     this.server.addProtocol(RefreshAuthorizationPolicyProtocol.class, this);\n     this.server.addProtocol(RefreshUserMappingsProtocol.class, this);\n     this.server.addProtocol(GetUserMappingsProtocol.class, this);\n+    this.server.addProtocol(HAServiceProtocol.class, this);\n     \n \n     // set service-level authorization security policy\n@@ -225,6 +227,8 @@ public long getProtocolVersion(String protocol,\n       return RefreshUserMappingsProtocol.versionID;\n     } else if (protocol.equals(GetUserMappingsProtocol.class.getName())){\n       return GetUserMappingsProtocol.versionID;\n+    } else if (protocol.equals(HAServiceProtocol.class.getName())) {\n+      return HAServiceProtocol.versionID;\n     } else {\n       throw new IOException(\"Unknown protocol to name node: \" + protocol);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/3ab295994a4e7870a1f68d742d26c3ac44546fa5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "sha": "6546b8fe06b07bfe73b85c60f3e143e84e847717",
                "status": "modified"
            }
        ],
        "message": "HDFS-2523. Small NN fixes to include HAServiceProtocol and prevent NPE on shutdown. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1195753 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/b4992f671d36e35fd874958ffbc9e66abc29a725",
        "repo": "hadoop",
        "unit_tests": [
            "TestNameNodeRpcServer.java"
        ]
    },
    "hadoop_3b00eae": {
        "bug_id": "hadoop_3b00eae",
        "commit": "https://github.com/apache/hadoop/commit/3b00eaea256d252be3361a7d9106b88756fcb9ba",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/3b00eaea256d252be3361a7d9106b88756fcb9ba/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=3b00eaea256d252be3361a7d9106b88756fcb9ba",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -1211,6 +1211,9 @@ Release 2.8.0 - UNRELEASED\n     HDFS-8948. Use GenericTestUtils to set log levels in TestPread and\n     TestReplaceDatanodeOnFailure. (Mingliang Liu via wheat9)\n \n+    HDFS-8932. NPE thrown in NameNode when try to get TotalSyncCount metric\n+    before editLogStream initialization. (Surendra Singh Lilhore via xyao)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/3b00eaea256d252be3361a7d9106b88756fcb9ba/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "7aadcc60337a497c271f62f09e74f7c23066b0f5",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/3b00eaea256d252be3361a7d9106b88756fcb9ba/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java?ref=3b00eaea256d252be3361a7d9106b88756fcb9ba",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
                "patch": "@@ -1692,10 +1692,14 @@ private JournalManager createJournal(URI uri) {\n   }\n \n   /**\n-   +   * Return total number of syncs happened on this edit log.\n-   +   * @return long - count\n-   +   */\n+   * Return total number of syncs happened on this edit log.\n+   * @return long - count\n+   */\n   public long getTotalSyncCount() {\n-    return editLogStream.getNumSync();\n+    if (editLogStream != null) {\n+      return editLogStream.getNumSync();\n+    } else {\n+      return 0;\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/3b00eaea256d252be3361a7d9106b88756fcb9ba/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
                "sha": "faaea633448cf6f5b16df7025c7e014629cb9dc5",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/3b00eaea256d252be3361a7d9106b88756fcb9ba/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=3b00eaea256d252be3361a7d9106b88756fcb9ba",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -7295,7 +7295,12 @@ public long getTotalSyncCount() {\n   @Metric({\"TotalSyncTimes\",\n               \"Total time spend in sync operation on various edit logs\"})\n   public String getTotalSyncTimes() {\n-    return fsImage.editLog.getJournalSet().getSyncTimes();\n+    JournalSet journalSet = fsImage.editLog.getJournalSet();\n+    if (journalSet != null) {\n+      return journalSet.getSyncTimes();\n+    } else {\n+      return \"\";\n+    }\n   }\n }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/3b00eaea256d252be3361a7d9106b88756fcb9ba/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "3c3ef0b9797cc703fd7f27b11838af6781133c8d",
                "status": "modified"
            }
        ],
        "message": "HDFS-8932. NPE thrown in NameNode when try to get TotalSyncCount metric before editLogStream initialization. Contributed by Surendra Singh Lilhore",
        "parent": "https://github.com/apache/hadoop/commit/66d0c81d8f4e200a5051c8df87be890c9ad8772e",
        "repo": "hadoop",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_44809b8": {
        "bug_id": "hadoop_44809b8",
        "commit": "https://github.com/apache/hadoop/commit/44809b80814d5520a73d5609d0f73a13eb2360ac",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/44809b80814d5520a73d5609d0f73a13eb2360ac/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=44809b80814d5520a73d5609d0f73a13eb2360ac",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -469,6 +469,9 @@ Release 2.8.0 - UNRELEASED\n     HADOOP-10027. *Compressor_deflateBytesDirect passes instance instead of\n     jclass to GetStaticObjectField. (Hui Zheng via cnauroth)\n \n+    HADOOP-11724. DistCp throws NPE when the target directory is root.\n+    (Lei Eddy Xu via Yongjun Zhang) \n+\n Release 2.7.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/44809b80814d5520a73d5609d0f73a13eb2360ac/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "2e26b0a48e13b514dd5ee8f9ac6a5e977c9a2e6c",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/44809b80814d5520a73d5609d0f73a13eb2360ac/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java?ref=44809b80814d5520a73d5609d0f73a13eb2360ac",
                "deletions": 0,
                "filename": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java",
                "patch": "@@ -133,6 +133,9 @@ private void cleanupTempFiles(JobContext context) {\n   private void deleteAttemptTempFiles(Path targetWorkPath,\n                                       FileSystem targetFS,\n                                       String jobId) throws IOException {\n+    if (targetWorkPath == null) {\n+      return;\n+    }\n \n     FileStatus[] tempFiles = targetFS.globStatus(\n         new Path(targetWorkPath, \".distcp.tmp.\" + jobId.replaceAll(\"job\",\"attempt\") + \"*\"));",
                "raw_url": "https://github.com/apache/hadoop/raw/44809b80814d5520a73d5609d0f73a13eb2360ac/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java",
                "sha": "9ec57f426aa13944452c5fea73082af0b649caea",
                "status": "modified"
            }
        ],
        "message": "HADOOP-11724. DistCp throws NPE when the target directory is root. (Lei Eddy Xu via Yongjun Zhang)",
        "parent": "https://github.com/apache/hadoop/commit/fc1031af749435dc95efea6745b1b2300ce29446",
        "repo": "hadoop",
        "unit_tests": [
            "TestCopyCommitter.java"
        ]
    },
    "hadoop_4530f45": {
        "bug_id": "hadoop_4530f45",
        "commit": "https://github.com/apache/hadoop/commit/4530f4500d308c9cefbcc5990769c04bd061ad87",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/4530f4500d308c9cefbcc5990769c04bd061ad87/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java?ref=4530f4500d308c9cefbcc5990769c04bd061ad87",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "patch": "@@ -526,9 +526,11 @@ protected void serviceStop() throws Exception {\n       DefaultMetricsSystem.shutdown();\n \n       // Cleanup ResourcePluginManager\n-      ResourcePluginManager rpm = context.getResourcePluginManager();\n-      if (rpm != null) {\n-        rpm.cleanup();\n+      if (null != context) {\n+        ResourcePluginManager rpm = context.getResourcePluginManager();\n+        if (rpm != null) {\n+          rpm.cleanup();\n+        }\n       }\n     } finally {\n       // YARN-3641: NM's services stop get failed shouldn't block the",
                "raw_url": "https://github.com/apache/hadoop/raw/4530f4500d308c9cefbcc5990769c04bd061ad87/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "sha": "9eff3a9213e3156aaa56d9c58730ac6ce1c633eb",
                "status": "modified"
            }
        ],
        "message": "YARN-9507. Fix NPE in NodeManager#serviceStop on startup failure. Contributed by Bilwa S T.",
        "parent": "https://github.com/apache/hadoop/commit/21852494815e7314e0873c3963a54457ac2aab28",
        "repo": "hadoop",
        "unit_tests": [
            "TestNodeManager.java"
        ]
    },
    "hadoop_47c41e7": {
        "bug_id": "hadoop_47c41e7",
        "commit": "https://github.com/apache/hadoop/commit/47c41e7ac7e6b905a58550f8899f629c1cf8b138",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/47c41e7ac7e6b905a58550f8899f629c1cf8b138/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java?ref=47c41e7ac7e6b905a58550f8899f629c1cf8b138",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java",
                "patch": "@@ -539,9 +539,14 @@ public boolean cancel() {\n    */\n   private boolean skipTokenRenewal(Token<?> token)\n       throws IOException {\n+\n     @SuppressWarnings(\"unchecked\")\n-    Text renewer = ((Token<AbstractDelegationTokenIdentifier>)token).\n-        decodeIdentifier().getRenewer();\n+    AbstractDelegationTokenIdentifier identifier =\n+        ((Token<AbstractDelegationTokenIdentifier>) token).decodeIdentifier();\n+    if (identifier == null) {\n+      return false;\n+    }\n+    Text renewer = identifier.getRenewer();\n     return (renewer != null && renewer.toString().equals(\"\"));\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/47c41e7ac7e6b905a58550f8899f629c1cf8b138/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java",
                "sha": "fd12f11c78999a7ba9a95e9e164562c13138a457",
                "status": "modified"
            }
        ],
        "message": "YARN-5048. DelegationTokenRenewer#skipTokenRenewal may throw NPE (Jian He via Yongjun Zhang)",
        "parent": "https://github.com/apache/hadoop/commit/6957e4569996734b1b176e04df5a03d000bed5b7",
        "repo": "hadoop",
        "unit_tests": [
            "TestDelegationTokenRenewer.java"
        ]
    },
    "hadoop_4922394": {
        "bug_id": "hadoop_4922394",
        "commit": "https://github.com/apache/hadoop/commit/492239424a3ace9868b6154f44a0f18fa5318235",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/492239424a3ace9868b6154f44a0f18fa5318235/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=492239424a3ace9868b6154f44a0f18fa5318235",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -129,6 +129,9 @@ Release 2.8.0 - UNRELEASED\n \n     YARN-3412. RM tests should use MockRM where possible. (kasha)\n \n+    YARN-3425. NPE from RMNodeLabelsManager.serviceStop when \n+    NodeLabelsManager.serviceInit failed. (Bibin A Chundatt via wangda)\n+\n Release 2.7.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/492239424a3ace9868b6154f44a0f18fa5318235/hadoop-yarn-project/CHANGES.txt",
                "sha": "f5dc39d2c6c163ad1d0fec1574b19c81cdc9815a",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/492239424a3ace9868b6154f44a0f18fa5318235/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/CommonNodeLabelsManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/CommonNodeLabelsManager.java?ref=492239424a3ace9868b6154f44a0f18fa5318235",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/CommonNodeLabelsManager.java",
                "patch": "@@ -258,7 +258,9 @@ protected void serviceStart() throws Exception {\n   // for UT purpose\n   protected void stopDispatcher() {\n     AsyncDispatcher asyncDispatcher = (AsyncDispatcher) dispatcher;\n-    asyncDispatcher.stop();\n+    if (null != asyncDispatcher) {\n+      asyncDispatcher.stop();\n+    }\n   }\n   \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/492239424a3ace9868b6154f44a0f18fa5318235/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/CommonNodeLabelsManager.java",
                "sha": "fe3816474e615342ed560a87f0a9ffdaea3ceb40",
                "status": "modified"
            }
        ],
        "message": "YARN-3425. NPE from RMNodeLabelsManager.serviceStop when NodeLabelsManager.serviceInit failed. (Bibin A Chundatt via wangda)",
        "parent": "https://github.com/apache/hadoop/commit/2e79f1c2125517586c165a84e99d3c4d38ca0938",
        "repo": "hadoop",
        "unit_tests": [
            "TestCommonNodeLabelsManager.java"
        ]
    },
    "hadoop_4b9f044": {
        "bug_id": "hadoop_4b9f044",
        "commit": "https://github.com/apache/hadoop/commit/4b9f0443cb0e35747e0c4ec5f416175b42164a60",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/4b9f0443cb0e35747e0c4ec5f416175b42164a60/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=4b9f0443cb0e35747e0c4ec5f416175b42164a60",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -248,6 +248,9 @@ Release 0.23.6 - UNRELEASED\n     YARN-280. RM does not reject app submission with invalid tokens \n     (Daryn Sharp via tgraves)\n \n+    YARN-225. Proxy Link in RM UI thows NPE in Secure mode \n+    (Devaraj K via bobby)\n+\n Release 0.23.5 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/4b9f0443cb0e35747e0c4ec5f416175b42164a60/hadoop-yarn-project/CHANGES.txt",
                "sha": "ed4f895cfacc5776d9999665c87c150dbc80c7a8",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/4b9f0443cb0e35747e0c4ec5f416175b42164a60/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java?ref=4b9f0443cb0e35747e0c4ec5f416175b42164a60",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java",
                "patch": "@@ -254,11 +254,14 @@ protected void doGet(HttpServletRequest req, HttpServletResponse resp)\n       \n       if(securityEnabled) {\n         String cookieName = getCheckCookieName(id); \n-        for(Cookie c: req.getCookies()) {\n-          if(cookieName.equals(c.getName())) {\n-            userWasWarned = true;\n-            userApproved = userApproved || Boolean.valueOf(c.getValue());\n-            break;\n+        Cookie[] cookies = req.getCookies();\n+        if (cookies != null) {\n+          for (Cookie c : cookies) {\n+            if (cookieName.equals(c.getName())) {\n+              userWasWarned = true;\n+              userApproved = userApproved || Boolean.valueOf(c.getValue());\n+              break;\n+            }\n           }\n         }\n       }",
                "raw_url": "https://github.com/apache/hadoop/raw/4b9f0443cb0e35747e0c4ec5f416175b42164a60/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java",
                "sha": "7f6bba14e98e09d7634d9d6cc712868c191d126a",
                "status": "modified"
            }
        ],
        "message": "YARN-225. Proxy Link in RM UI thows NPE in Secure mode (Devaraj K via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1426515 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/0fa9c7a825f444d50c89b986bacea7a547e4ab8b",
        "repo": "hadoop",
        "unit_tests": [
            "TestWebAppProxyServlet.java"
        ]
    },
    "hadoop_4c7a6c6": {
        "bug_id": "hadoop_4c7a6c6",
        "commit": "https://github.com/apache/hadoop/commit/4c7a6c6c3f5879a2f79080753074f078943f8392",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/4c7a6c6c3f5879a2f79080753074f078943f8392/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=4c7a6c6c3f5879a2f79080753074f078943f8392",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -3681,7 +3681,8 @@ public boolean isInStartupSafeMode() {\n \n   @Override\n   public boolean isPopulatingReplQueues() {\n-    if (!haContext.getState().shouldPopulateReplQueues()) {\n+    if (haContext != null && // null during startup!\n+        !haContext.getState().shouldPopulateReplQueues()) {\n       return false;\n     }\n     // safeMode is volatile, and may be set to null at any time",
                "raw_url": "https://github.com/apache/hadoop/raw/4c7a6c6c3f5879a2f79080753074f078943f8392/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "258cb53186ad15632cf9899da3f0304a189f942c",
                "status": "modified"
            }
        ],
        "message": "Amend HDFS-2795. Fix PersistBlocks failure due to an NPE in isPopulatingReplQueues()\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1232510 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/0c1450ca5d922b5bf713bb8bb17459dc11a97330",
        "repo": "hadoop",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_4d779e0": {
        "bug_id": "hadoop_4d779e0",
        "commit": "https://github.com/apache/hadoop/commit/4d779e088a30f958c9788366e0e251476cb18410",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/4d779e088a30f958c9788366e0e251476cb18410/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=4d779e088a30f958c9788366e0e251476cb18410",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -149,3 +149,5 @@ HDFS-2845. SBN should not allow browsing of the file system via web UI. (Bikas S\n HDFS-2742. HA: observed dataloss in replication stress test. (todd via eli)\n \n HDFS-2870. Fix log level for block debug info in processMisReplicatedBlocks (todd)\n+\n+HDFS-2859. LOCAL_ADDRESS_MATCHER.match has NPE when called from DFSUtil.getSuffixIDs when the host is incorrect (Bikas Saha via todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/4d779e088a30f958c9788366e0e251476cb18410/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "7a4ef27f19513e6bb421cf8ad4d79f49e29b0397",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/4d779e088a30f958c9788366e0e251476cb18410/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java?ref=4d779e088a30f958c9788366e0e251476cb18410",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "patch": "@@ -61,6 +61,8 @@\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.net.NodeBase;\n import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n \n import com.google.common.base.Joiner;\n import com.google.common.collect.Lists;\n@@ -69,6 +71,8 @@\n \n @InterfaceAudience.Private\n public class DFSUtil {\n+  private static final Log LOG = LogFactory.getLog(DFSUtil.class.getName());\n+  \n   private DFSUtil() { /* Hidden constructor */ }\n   private static final ThreadLocal<Random> RANDOM = new ThreadLocal<Random>() {\n     @Override\n@@ -935,9 +939,10 @@ private static String getNameServiceId(Configuration conf, String addressKey) {\n         try {\n           s = NetUtils.createSocketAddr(addr);\n         } catch (Exception e) {\n+          LOG.warn(\"Exception in creating socket address\", e);\n           continue;\n         }\n-        if (matcher.match(s)) {\n+        if (!s.isUnresolved() && matcher.match(s)) {\n           nameserviceId = nsId;\n           namenodeId = nnId;\n           found++;",
                "raw_url": "https://github.com/apache/hadoop/raw/4d779e088a30f958c9788366e0e251476cb18410/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "sha": "c9ccf9f38c7cafd26872165b4890710acc96be21",
                "status": "modified"
            }
        ],
        "message": "HDFS-2859. LOCAL_ADDRESS_MATCHER.match has NPE when called from DFSUtil.getSuffixIDs when the host is incorrect. Contributed by Bikas Saha.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1239356 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/048c416beb42ad27cf0e82b144da1d99e50c62b1",
        "repo": "hadoop",
        "unit_tests": [
            "TestDFSUtil.java"
        ]
    },
    "hadoop_5c8d907": {
        "bug_id": "hadoop_5c8d907",
        "commit": "https://github.com/apache/hadoop/commit/5c8d90763c52f6bf5224b59738739bd2d1a4b4b8",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/5c8d90763c52f6bf5224b59738739bd2d1a4b4b8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java?ref=5c8d90763c52f6bf5224b59738739bd2d1a4b4b8",
                "deletions": 4,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java",
                "patch": "@@ -586,12 +586,14 @@ public InetAddress getByName(String host) throws UnknownHostException {\n    *       hadoop.security.token.service.use_ip=false \n    */\n   protected static class QualifiedHostResolver implements HostResolver {\n-    private List<String> searchDomains;\n+    private List<String> searchDomains = new ArrayList<>();\n     {\n       ResolverConfig resolverConfig = ResolverConfig.getCurrentConfig();\n-      searchDomains = new ArrayList<>();\n-      for (Name name : resolverConfig.searchPath()) {\n-        searchDomains.add(name.toString());\n+      Name[] names = resolverConfig.searchPath();\n+      if (names != null) {\n+        for (Name name : names) {\n+          searchDomains.add(name.toString());\n+        }\n       }\n     }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/5c8d90763c52f6bf5224b59738739bd2d1a4b4b8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java",
                "sha": "2313119bfec2eb6a87ed9d259f5b02ae89b374ec",
                "status": "modified"
            }
        ],
        "message": "HADOOP-15764. Addendum patch: Fix NPE in SecurityUtil.",
        "parent": "https://github.com/apache/hadoop/commit/2a5d4315bfe13c12cacc7718537077bf9abb22e2",
        "repo": "hadoop",
        "unit_tests": [
            "TestSecurityUtil.java"
        ]
    },
    "hadoop_5e093f0": {
        "bug_id": "hadoop_5e093f0",
        "commit": "https://github.com/apache/hadoop/commit/5e093f0d400f82f67d9b2d24253c79e4a5abacf9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/5e093f0d400f82f67d9b2d24253c79e4a5abacf9/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=5e093f0d400f82f67d9b2d24253c79e4a5abacf9",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -290,6 +290,9 @@ Release 2.7.1 - UNRELEASED\n     YARN-3522. Fixed DistributedShell to instantiate TimeLineClient as the\n     correct user. (Zhijie Shen via jianhe)\n \n+    YARN-3537. NPE when NodeManager.serviceInit fails and stopRecoveryStore\n+    invoked (Brahma Reddy Battula via jlowe)\n+\n Release 2.7.0 - 2015-04-20\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/5e093f0d400f82f67d9b2d24253c79e4a5abacf9/hadoop-yarn-project/CHANGES.txt",
                "sha": "001396fa80b65a39c4137c42e42bfd58cb14694d",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/5e093f0d400f82f67d9b2d24253c79e4a5abacf9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java?ref=5e093f0d400f82f67d9b2d24253c79e4a5abacf9",
                "deletions": 12,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "patch": "@@ -178,18 +178,20 @@ private void initAndStartRecoveryStore(Configuration conf)\n   }\n \n   private void stopRecoveryStore() throws IOException {\n-    nmStore.stop();\n-    if (null != context) {\n-      if (context.getDecommissioned() && nmStore.canRecover()) {\n-        LOG.info(\"Removing state store due to decommission\");\n-        Configuration conf = getConfig();\n-        Path recoveryRoot =\n-            new Path(conf.get(YarnConfiguration.NM_RECOVERY_DIR));\n-        LOG.info(\"Removing state store at \" + recoveryRoot\n-            + \" due to decommission\");\n-        FileSystem recoveryFs = FileSystem.getLocal(conf);\n-        if (!recoveryFs.delete(recoveryRoot, true)) {\n-          LOG.warn(\"Unable to delete \" + recoveryRoot);\n+    if (null != nmStore) {\n+      nmStore.stop();\n+      if (null != context) {\n+        if (context.getDecommissioned() && nmStore.canRecover()) {\n+          LOG.info(\"Removing state store due to decommission\");\n+          Configuration conf = getConfig();\n+          Path recoveryRoot =\n+              new Path(conf.get(YarnConfiguration.NM_RECOVERY_DIR));\n+          LOG.info(\"Removing state store at \" + recoveryRoot\n+              + \" due to decommission\");\n+          FileSystem recoveryFs = FileSystem.getLocal(conf);\n+          if (!recoveryFs.delete(recoveryRoot, true)) {\n+            LOG.warn(\"Unable to delete \" + recoveryRoot);\n+          }\n         }\n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/5e093f0d400f82f67d9b2d24253c79e4a5abacf9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "sha": "6718b53f7b70943808e1f2b1f2c1f14e639ec3f1",
                "status": "modified"
            }
        ],
        "message": "YARN-3537. NPE when NodeManager.serviceInit fails and stopRecoveryStore invoked. Contributed by Brahma Reddy Battula",
        "parent": "https://github.com/apache/hadoop/commit/5ce3a77f3c00aeabcd791c3373dd3c8c25160ce2",
        "repo": "hadoop",
        "unit_tests": [
            "TestNodeManager.java"
        ]
    },
    "hadoop_60cbcff": {
        "bug_id": "hadoop_60cbcff",
        "commit": "https://github.com/apache/hadoop/commit/60cbcff2f7363e5cc386284981cac67abc965ee7",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/60cbcff2f7363e5cc386284981cac67abc965ee7/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=60cbcff2f7363e5cc386284981cac67abc965ee7",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -270,6 +270,8 @@ Trunk (Unreleased)\n \n     HDFS-7581. HDFS documentation needs updating post-shell rewrite (aw)\n \n+    HDFS-7606. Fix potential NPE in INodeFile.getBlocks(). (Byron Wong via shv)\n+\n Release 2.7.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/60cbcff2f7363e5cc386284981cac67abc965ee7/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "6af1a52324d292c0d3ee474666def12ca8844c75",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/60cbcff2f7363e5cc386284981cac67abc965ee7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java?ref=60cbcff2f7363e5cc386284981cac67abc965ee7",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
                "patch": "@@ -432,7 +432,7 @@ final long getBlockDiskspace() {\n       return snapshotBlocks;\n     // Blocks are not in the current snapshot\n     // Find next snapshot with blocks present or return current file blocks\n-    snapshotBlocks = getDiffs().findLaterSnapshotBlocks(diff.getSnapshotId());\n+    snapshotBlocks = getDiffs().findLaterSnapshotBlocks(snapshot);\n     return (snapshotBlocks == null) ? getBlocks() : snapshotBlocks;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/60cbcff2f7363e5cc386284981cac67abc965ee7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
                "sha": "e871bdc59b7f262728f3c670a4e75d0c6f8c5c76",
                "status": "modified"
            }
        ],
        "message": "HDFS-7606. Fix potential NPE in INodeFile.getBlocks(). Contributed by Byron Wong.",
        "parent": "https://github.com/apache/hadoop/commit/ec4389cf7270cff4cc96313b4190422ea7c70ced",
        "repo": "hadoop",
        "unit_tests": [
            "TestINodeFile.java"
        ]
    },
    "hadoop_614af50": {
        "bug_id": "hadoop_614af50",
        "commit": "https://github.com/apache/hadoop/commit/614af50625a8495812dce8da59db0e1aef40b1c0",
        "file": [
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop/blob/614af50625a8495812dce8da59db0e1aef40b1c0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java?ref=614af50625a8495812dce8da59db0e1aef40b1c0",
                "deletions": 10,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "patch": "@@ -1040,20 +1040,27 @@ public SchedulerNode getNode(NodeId nodeId) {\n     for (Map.Entry<ApplicationId, ContainerStatus> c : updateExistContainers) {\n       SchedulerApplication<T> app = applications.get(c.getKey());\n       ContainerId containerId = c.getValue().getContainerId();\n+      if (app == null || app.getCurrentAppAttempt() == null) {\n+        continue;\n+      }\n+      RMContainer rmContainer\n+          = app.getCurrentAppAttempt().getRMContainer(containerId);\n+      if (rmContainer == null) {\n+        continue;\n+      }\n+      // exposed ports are already set for the container, skip\n+      if (rmContainer.getExposedPorts() != null &&\n+          rmContainer.getExposedPorts().size() > 0) {\n+        continue;\n+      }\n+\n       String strExposedPorts = c.getValue().getExposedPorts();\n-      Map<String, List<Map<String, String>>> exposedPorts = null;\n       if (null != strExposedPorts && !strExposedPorts.isEmpty()) {\n         Gson gson = new Gson();\n-        exposedPorts = gson.fromJson(strExposedPorts,\n+        Map<String, List<Map<String, String>>> exposedPorts =\n+            gson.fromJson(strExposedPorts,\n             new TypeToken<Map<String, List<Map<String, String>>>>()\n-            {}.getType());\n-      }\n-\n-      RMContainer rmContainer\n-          = app.getCurrentAppAttempt().getRMContainer(containerId);\n-      if (null != rmContainer &&\n-          (null == rmContainer.getExposedPorts()\n-              || rmContainer.getExposedPorts().size() == 0)) {\n+                {}.getType());\n         LOG.info(\"update exist container \" + containerId.getContainerId()\n             + \", strExposedPorts = \" + strExposedPorts);\n         rmContainer.setExposedPorts(exposedPorts);",
                "raw_url": "https://github.com/apache/hadoop/raw/614af50625a8495812dce8da59db0e1aef40b1c0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "sha": "a798b97af5fa4ade6531c71a34c8405973359536",
                "status": "modified"
            }
        ],
        "message": "YARN-9179. Fix NPE in AbstractYarnScheduler#updateNewContainerInfo.",
        "parent": "https://github.com/apache/hadoop/commit/05c84ab01c08d37457bb5aa40df61f45c7ed5fd4",
        "repo": "hadoop",
        "unit_tests": [
            "TestAbstractYarnScheduler.java"
        ]
    },
    "hadoop_6285cbb": {
        "bug_id": "hadoop_6285cbb",
        "commit": "https://github.com/apache/hadoop/commit/6285cbb4990736ed25cea739c421fcfe0cd9d591",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/6285cbb4990736ed25cea739c421fcfe0cd9d591/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/CHANGES.txt?ref=6285cbb4990736ed25cea739c421fcfe0cd9d591",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -289,6 +289,9 @@ Trunk (unreleased changes)\n     HADOOP-6991.  Fix SequenceFile::Reader to honor file lengths and call\n     openFile (cdouglas via omalley)\n \n+    HADOOP-7011.  Fix KerberosName.main() to not throw an NPE.\n+    (Aaron T. Myers via tomwhite)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop/raw/6285cbb4990736ed25cea739c421fcfe0cd9d591/CHANGES.txt",
                "sha": "e607a0c1062c4eef025df0df61256d286a37a9af",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/6285cbb4990736ed25cea739c421fcfe0cd9d591/src/java/org/apache/hadoop/security/KerberosName.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/org/apache/hadoop/security/KerberosName.java?ref=6285cbb4990736ed25cea739c421fcfe0cd9d591",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/security/KerberosName.java",
                "patch": "@@ -399,9 +399,10 @@ static void printRules() throws IOException {\n   }\n \n   public static void main(String[] args) throws Exception {\n+    setConfiguration(new Configuration());\n     for(String arg: args) {\n       KerberosName name = new KerberosName(arg);\n       System.out.println(\"Name: \" + name + \" to \" + name.getShortName());\n     }\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/6285cbb4990736ed25cea739c421fcfe0cd9d591/src/java/org/apache/hadoop/security/KerberosName.java",
                "sha": "b533cd22f77d6dbf0aaa0a42edf6338dc6d987c6",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7011.  Fix KerberosName.main() to not throw an NPE.  Contributed by Aaron T. Myers.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1028938 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/0c462b223f151208ff7bd5148cee0e436c23d795",
        "repo": "hadoop",
        "unit_tests": [
            "TestKerberosName.java"
        ]
    },
    "hadoop_6604131": {
        "bug_id": "hadoop_6604131",
        "commit": "https://github.com/apache/hadoop/commit/660413165aa25815bbba66ac2195b0ae17184844",
        "file": [
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/660413165aa25815bbba66ac2195b0ae17184844/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java?ref=660413165aa25815bbba66ac2195b0ae17184844",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "patch": "@@ -620,13 +620,17 @@ private void setAppCollectorsMapToResponse(\n     Map<ApplicationId, RMApp> rmApps = rmContext.getRMApps();\n     // Set collectors for all running apps on this node.\n     for (ApplicationId appId : runningApps) {\n-      AppCollectorData appCollectorData = rmApps.get(appId).getCollectorData();\n-      if (appCollectorData != null) {\n-        liveAppCollectorsMap.put(appId, appCollectorData);\n-      } else {\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Collector for applicaton: \" + appId +\n-              \" hasn't registered yet!\");\n+      RMApp app = rmApps.get(appId);\n+      if (app != null) {\n+        AppCollectorData appCollectorData = rmApps.get(appId)\n+            .getCollectorData();\n+        if (appCollectorData != null) {\n+          liveAppCollectorsMap.put(appId, appCollectorData);\n+        } else {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Collector for applicaton: \" + appId +\n+                \" hasn't registered yet!\");\n+          }\n         }\n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/660413165aa25815bbba66ac2195b0ae17184844/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "sha": "cc47e02cb19d53a1f149c594262db56682d1352b",
                "status": "modified"
            }
        ],
        "message": "YARN-6801. NPE in RM while setting collectors map in NodeHeartbeatResponse. Contributed by Vrushali C.",
        "parent": "https://github.com/apache/hadoop/commit/ac7f52df83d2b4758e7debe9416be7db0ec69d2b",
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceTrackerService.java"
        ]
    },
    "hadoop_69dd284": {
        "bug_id": "hadoop_69dd284",
        "commit": "https://github.com/apache/hadoop/commit/69dd2844527f4d6fba99a13ed25538055e0613dd",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/69dd2844527f4d6fba99a13ed25538055e0613dd/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=69dd2844527f4d6fba99a13ed25538055e0613dd",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -1833,6 +1833,8 @@ Release 0.23.0 - Unreleased\n     MAPREDUCE-3258. Fixed AM & JobHistory web-ui to display counters properly.\n     (Siddharth Seth via acmurthy)\n \n+    MAPREDUCE-3290. Fixed a NPE in ClientRMService. (acmurthy) \n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/69dd2844527f4d6fba99a13ed25538055e0613dd/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "c9d61a6ec79ccd272013d739e90575c699f2c4a3",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/69dd2844527f4d6fba99a13ed25538055e0613dd/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java?ref=69dd2844527f4d6fba99a13ed25538055e0613dd",
                "deletions": 4,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "patch": "@@ -59,6 +59,7 @@\n import org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext;\n import org.apache.hadoop.yarn.api.records.NodeReport;\n import org.apache.hadoop.yarn.api.records.QueueInfo;\n+import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.YarnClusterMetrics;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.exceptions.YarnRemoteException;\n@@ -67,10 +68,12 @@\n import org.apache.hadoop.yarn.ipc.RPCUtil;\n import org.apache.hadoop.yarn.ipc.YarnRPC;\n import org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger.AuditConstants;\n+import org.apache.hadoop.yarn.server.resourcemanager.resource.Resources;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNodeReport;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.YarnScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.security.authorize.RMPolicyProvider;\n import org.apache.hadoop.yarn.server.security.ApplicationACLsManager;\n@@ -396,10 +399,18 @@ private NodeReport createNodeReports(RMNode rmNode) {\n     report.setRackName(rmNode.getRackName());\n     report.setCapability(rmNode.getTotalCapability());\n     report.setNodeHealthStatus(rmNode.getNodeHealthStatus());\n-    org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNodeReport schedulerNodeReport = scheduler\n-        .getNodeReport(rmNode.getNodeID());\n-    report.setUsed(schedulerNodeReport.getUsedResource());\n-    report.setNumContainers(schedulerNodeReport.getNumContainers());\n+    \n+    SchedulerNodeReport schedulerNodeReport = \n+        scheduler.getNodeReport(rmNode.getNodeID());\n+    Resource used = Resources.none();\n+    int numContainers = 0;\n+    if (schedulerNodeReport != null) {\n+      used = schedulerNodeReport.getUsedResource();\n+      numContainers = schedulerNodeReport.getNumContainers();\n+    } \n+    report.setUsed(used);\n+    report.setNumContainers(numContainers);\n+\n     return report;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/69dd2844527f4d6fba99a13ed25538055e0613dd/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "sha": "b19c1c17c5e6656c934d4d92ceb72cd6120b6042",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3290. Fixed a NPE in ClientRMService.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1190162 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/34e6de8f856648b4d74997410d9cc8da4010d7c9",
        "repo": "hadoop",
        "unit_tests": [
            "TestClientRMService.java"
        ]
    },
    "hadoop_747bafa": {
        "bug_id": "hadoop_747bafa",
        "commit": "https://github.com/apache/hadoop/commit/747bafaf969857b66233a8b4660590bdd712ed7d",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/747bafaf969857b66233a8b4660590bdd712ed7d/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java?ref=747bafaf969857b66233a8b4660590bdd712ed7d",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java",
                "patch": "@@ -323,7 +323,7 @@ synchronized void updateStatus() throws IOException {\n       this.status = ugi.doAs(new PrivilegedExceptionAction<JobStatus>() {\n         @Override\n         public JobStatus run() throws IOException, InterruptedException {\n-          return cluster.getClient().getJobStatus(status.getJobID());\n+          return cluster.getClient().getJobStatus(getJobID());\n         }\n       });\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/747bafaf969857b66233a8b4660590bdd712ed7d/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java",
                "sha": "84d90deff6f15793e121394d271cc3a2e045dbf0",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-6852. Job#updateStatus() failed with NPE due to race condition. Contributed by Junping Du",
        "parent": "https://github.com/apache/hadoop/commit/eeca8b0c4e2804b0fee5b012ea14b58383425ec3",
        "repo": "hadoop",
        "unit_tests": [
            "TestJob.java"
        ]
    },
    "hadoop_76b94c2": {
        "bug_id": "hadoop_76b94c2",
        "commit": "https://github.com/apache/hadoop/commit/76b94c274fe9775efcfd51c676d80c88a4f7fdb9",
        "file": [
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/76b94c274fe9775efcfd51c676d80c88a4f7fdb9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/FairCallQueue.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/FairCallQueue.java?ref=76b94c274fe9775efcfd51c676d80c88a4f7fdb9",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/FairCallQueue.java",
                "patch": "@@ -377,9 +377,21 @@ public void setDelegate(FairCallQueue<? extends Schedulable> obj) {\n       this.revisionNumber++;\n     }\n \n+    /**\n+     * Fetch the current call queue from the weak reference delegate. If there\n+     * is no delegate, or the delegate is empty, this will return null.\n+     */\n+    private FairCallQueue<? extends Schedulable> getCallQueue() {\n+      WeakReference<FairCallQueue<? extends Schedulable>> ref = this.delegate;\n+      if (ref == null) {\n+        return null;\n+      }\n+      return ref.get();\n+    }\n+\n     @Override\n     public int[] getQueueSizes() {\n-      FairCallQueue<? extends Schedulable> obj = this.delegate.get();\n+      FairCallQueue<? extends Schedulable> obj = getCallQueue();\n       if (obj == null) {\n         return new int[]{};\n       }\n@@ -389,7 +401,7 @@ public void setDelegate(FairCallQueue<? extends Schedulable> obj) {\n \n     @Override\n     public long[] getOverflowedCalls() {\n-      FairCallQueue<? extends Schedulable> obj = this.delegate.get();\n+      FairCallQueue<? extends Schedulable> obj = getCallQueue();\n       if (obj == null) {\n         return new long[]{};\n       }",
                "raw_url": "https://github.com/apache/hadoop/raw/76b94c274fe9775efcfd51c676d80c88a4f7fdb9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/FairCallQueue.java",
                "sha": "b4e953948c657b8d443c0af6a31d603a5b7fd051",
                "status": "modified"
            }
        ],
        "message": "HADOOP-16345. Fix a potential NPE when instantiating FairCallQueue metrics. Contributed by Erik Krogen.",
        "parent": "https://github.com/apache/hadoop/commit/4e38dafde4dce8cd8c368783a291e830f06e1def",
        "repo": "hadoop",
        "unit_tests": [
            "TestFairCallQueue.java"
        ]
    },
    "hadoop_7ca7fda": {
        "bug_id": "hadoop_7ca7fda",
        "commit": "https://github.com/apache/hadoop/commit/7ca7fdadc491290abb0949dbb56cba8e66de9862",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/7ca7fdadc491290abb0949dbb56cba8e66de9862/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=7ca7fdadc491290abb0949dbb56cba8e66de9862",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -1697,6 +1697,9 @@ Release 0.23.0 - Unreleased\n \n     MAPREDUCE-2693. Fix NPE in job-blacklisting. (Hitesh Shah via acmurthy) \n \n+    MAPREDUCE-3208. Fix NPE task/container log appenders. (liangzhwa via\n+    acmurthy) \n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/7ca7fdadc491290abb0949dbb56cba8e66de9862/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "cf1c7d1696a3da0f1ddf516536a82303b8f196ac",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/7ca7fdadc491290abb0949dbb56cba8e66de9862/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLogAppender.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLogAppender.java?ref=7ca7fdadc491290abb0949dbb56cba8e66de9862",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLogAppender.java",
                "patch": "@@ -93,7 +93,9 @@ public void append(LoggingEvent event) {\n   }\n   \n   public void flush() {\n-    qw.flush();\n+    if (qw != null) {\n+      qw.flush();\n+    }\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/7ca7fdadc491290abb0949dbb56cba8e66de9862/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLogAppender.java",
                "sha": "0b79837f62d57a93f88a1b45a3ef9795a2ee0f0a",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/7ca7fdadc491290abb0949dbb56cba8e66de9862/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/ContainerLogAppender.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/ContainerLogAppender.java?ref=7ca7fdadc491290abb0949dbb56cba8e66de9862",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/ContainerLogAppender.java",
                "patch": "@@ -65,7 +65,9 @@ public void append(LoggingEvent event) {\n   }\n   \n   public void flush() {\n-    qw.flush();\n+    if (qw != null) {\n+      qw.flush();\n+    }\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/7ca7fdadc491290abb0949dbb56cba8e66de9862/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/ContainerLogAppender.java",
                "sha": "7f09c175b6b234a8624d6a04433ef23345393fb9",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3208. Fix NPE task/container log appenders. Contributed by liangzhwa.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1186542 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/74748ec62570f92d57dbad3ba4cca47402990db5",
        "repo": "hadoop",
        "unit_tests": [
            "TestContainerLogAppender.java"
        ]
    },
    "hadoop_7d06806": {
        "bug_id": "hadoop_7d06806",
        "commit": "https://github.com/apache/hadoop/commit/7d06806dfdeb3252ac0defe23e8c468eabfa8b5e",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/7d06806dfdeb3252ac0defe23e8c468eabfa8b5e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java?ref=7d06806dfdeb3252ac0defe23e8c468eabfa8b5e",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "patch": "@@ -1273,8 +1273,6 @@ synchronized void transitionToStandby(boolean initialize)\n   protected void serviceStart() throws Exception {\n     if (this.rmContext.isHAEnabled()) {\n       transitionToStandby(false);\n-    } else {\n-      transitionToActive();\n     }\n \n     startWepApp();\n@@ -1284,6 +1282,11 @@ protected void serviceStart() throws Exception {\n       WebAppUtils.setRMWebAppPort(conf, port);\n     }\n     super.serviceStart();\n+\n+    // Non HA case, start after RM services are started.\n+    if (!this.rmContext.isHAEnabled()) {\n+      transitionToActive();\n+    }\n   }\n   \n   protected void doSecureLogin() throws IOException {",
                "raw_url": "https://github.com/apache/hadoop/raw/7d06806dfdeb3252ac0defe23e8c468eabfa8b5e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "sha": "05745ec272e1f72d924376688782c1bc4a71495b",
                "status": "modified"
            }
        ],
        "message": "YARN-6827. [ATS1/1.5] NPE exception while publishing recovering applications into ATS during RM restart. Contributed by Rohith Sharma K S.",
        "parent": "https://github.com/apache/hadoop/commit/c6d7d3eb059c7539db7d00586e181ec44da13557",
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceManager.java"
        ]
    },
    "hadoop_8c7f6b2": {
        "bug_id": "hadoop_8c7f6b2",
        "commit": "https://github.com/apache/hadoop/commit/8c7f6b2d4df2e5ca7b766db68213b778d28f198b",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/8c7f6b2d4df2e5ca7b766db68213b778d28f198b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java?ref=8c7f6b2d4df2e5ca7b766db68213b778d28f198b",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "patch": "@@ -350,8 +350,10 @@ static void handleComponentInstanceRelaunch(ComponentInstance compInstance,\n         // record in ATS\n         LOG.info(\"Publishing component instance status {} {} \",\n             event.getContainerId(), containerState);\n+        int exitStatus = failureBeforeLaunch || event.getStatus() == null ?\n+            ContainerExitStatus.INVALID : event.getStatus().getExitStatus();\n         compInstance.serviceTimelinePublisher.componentInstanceFinished(\n-            event.getContainerId(), event.getStatus().getExitStatus(),\n+            event.getContainerId(), exitStatus,\n             containerState, containerDiag);\n       }\n \n@@ -366,8 +368,10 @@ static void handleComponentInstanceRelaunch(ComponentInstance compInstance,\n \n       if (compInstance.timelineServiceEnabled) {\n         // record in ATS\n+        int exitStatus = failureBeforeLaunch || event.getStatus() == null ?\n+            ContainerExitStatus.INVALID : event.getStatus().getExitStatus();\n         compInstance.serviceTimelinePublisher.componentInstanceFinished(\n-            event.getContainerId(), event.getStatus().getExitStatus(),\n+            event.getContainerId(), exitStatus,\n             containerState, containerDiag);\n       }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/8c7f6b2d4df2e5ca7b766db68213b778d28f198b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "sha": "66c298d337ca9a8da98370edb2de3c878a1455db",
                "status": "modified"
            }
        ],
        "message": "YARN-9197.  Add safe guard against NPE for component instance failure.\n            Contributed by kyungwan nam",
        "parent": "https://github.com/apache/hadoop/commit/dacc1a759e3ba3eca000cbacc6145b231253b174",
        "repo": "hadoop",
        "unit_tests": [
            "TestComponentInstance.java"
        ]
    },
    "hadoop_8f9661d": {
        "bug_id": "hadoop_8f9661d",
        "commit": "https://github.com/apache/hadoop/commit/8f9661da4823bfbb243e430252ec1bb5780ecbfc",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=8f9661da4823bfbb243e430252ec1bb5780ecbfc",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -509,6 +509,10 @@ Release 0.23.0 - Unreleased\n     HADOOP-7360. Preserve relative paths that do not contain globs in FsShell.\n     (Daryn Sharp and Kihwal Lee via szetszwo)\n \n+    HADOOP-7771. FsShell -copyToLocal, -get, etc. commands throw NPE if the\n+    destination directory does not exist.  (John George and Daryn Sharp\n+    via szetszwo)\n+\n   OPTIMIZATIONS\n   \n     HADOOP-7333. Performance improvement in PureJavaCrc32. (Eric Caspole",
                "raw_url": "https://github.com/apache/hadoop/raw/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "0b0e1beb76248ba3b27554efd8cb2bd9b239c27d",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java?ref=8f9661da4823bfbb243e430252ec1bb5780ecbfc",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java",
                "patch": "@@ -55,6 +55,7 @@\n   protected int exitCode = 0;\n   protected int numErrors = 0;\n   protected boolean recursive = false;\n+  private int depth = 0;\n   protected ArrayList<Exception> exceptions = new ArrayList<Exception>();\n \n   private static final Log LOG = LogFactory.getLog(Command.class);\n@@ -86,6 +87,10 @@ protected boolean isRecursive() {\n     return recursive;\n   }\n \n+  protected int getDepth() {\n+    return depth;\n+  }\n+  \n   /** \n    * Execute the command on the input path\n    * \n@@ -269,6 +274,7 @@ protected void processArgument(PathData item) throws IOException {\n   protected void processPathArgument(PathData item) throws IOException {\n     // null indicates that the call is not via recursion, ie. there is\n     // no parent directory that was expanded\n+    depth = 0;\n     processPaths(null, item);\n   }\n   \n@@ -326,7 +332,12 @@ protected void processPath(PathData item) throws IOException {\n    *  @throws IOException if anything goes wrong...\n    */\n   protected void recursePath(PathData item) throws IOException {\n-    processPaths(item, item.getDirectoryContents());\n+    try {\n+      depth++;\n+      processPaths(item, item.getDirectoryContents());\n+    } finally {\n+      depth--;\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java",
                "sha": "b24d47e02b1027b5decd17b36d9ab8a71b589875",
                "status": "modified"
            },
            {
                "additions": 116,
                "blob_url": "https://github.com/apache/hadoop/blob/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java",
                "changes": 142,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java?ref=8f9661da4823bfbb243e430252ec1bb5780ecbfc",
                "deletions": 26,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java",
                "patch": "@@ -20,13 +20,18 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.io.InputStream;\n import java.util.LinkedList;\n \n+import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.shell.PathExceptions.PathExistsException;\n import org.apache.hadoop.fs.shell.PathExceptions.PathIOException;\n+import org.apache.hadoop.fs.shell.PathExceptions.PathIsDirectoryException;\n import org.apache.hadoop.fs.shell.PathExceptions.PathIsNotDirectoryException;\n import org.apache.hadoop.fs.shell.PathExceptions.PathNotFoundException;\n+import org.apache.hadoop.fs.shell.PathExceptions.PathOperationException;\n+import org.apache.hadoop.io.IOUtils;\n \n /**\n  * Provides: argument processing to ensure the destination is valid\n@@ -106,51 +111,136 @@ protected void processArguments(LinkedList<PathData> args)\n   }\n \n   @Override\n-  protected void processPaths(PathData parent, PathData ... items)\n+  protected void processPathArgument(PathData src)\n   throws IOException {\n+    if (src.stat.isDirectory() && src.fs.equals(dst.fs)) {\n+      PathData target = getTargetPath(src);\n+      String srcPath = src.fs.makeQualified(src.path).toString();\n+      String dstPath = dst.fs.makeQualified(target.path).toString();\n+      if (dstPath.equals(srcPath)) {\n+        PathIOException e = new PathIOException(src.toString(),\n+            \"are identical\");\n+        e.setTargetPath(dstPath.toString());\n+        throw e;\n+      }\n+      if (dstPath.startsWith(srcPath+Path.SEPARATOR)) {\n+        PathIOException e = new PathIOException(src.toString(),\n+            \"is a subdirectory of itself\");\n+        e.setTargetPath(target.toString());\n+        throw e;\n+      }\n+    }\n+    super.processPathArgument(src);\n+  }\n+\n+  @Override\n+  protected void processPath(PathData src) throws IOException {\n+    processPath(src, getTargetPath(src));\n+  }\n+  \n+  /**\n+   * Called with a source and target destination pair\n+   * @param src for the operation\n+   * @param target for the operation\n+   * @throws IOException if anything goes wrong\n+   */\n+  protected void processPath(PathData src, PathData dst) throws IOException {\n+    if (src.stat.isSymlink()) {\n+      // TODO: remove when FileContext is supported, this needs to either\n+      // copy the symlink or deref the symlink\n+      throw new PathOperationException(src.toString());        \n+    } else if (src.stat.isFile()) {\n+      copyFileToTarget(src, dst);\n+    } else if (src.stat.isDirectory() && !isRecursive()) {\n+      throw new PathIsDirectoryException(src.toString());\n+    }\n+  }\n+\n+  @Override\n+  protected void recursePath(PathData src) throws IOException {\n     PathData savedDst = dst;\n     try {\n       // modify dst as we descend to append the basename of the\n       // current directory being processed\n-      if (parent != null) dst = dst.getPathDataForChild(parent);\n-      super.processPaths(parent, items);\n+      dst = getTargetPath(src);\n+      if (dst.exists) {\n+        if (!dst.stat.isDirectory()) {\n+          throw new PathIsNotDirectoryException(dst.toString());\n+        }\n+      } else {\n+        if (!dst.fs.mkdirs(dst.path)) {\n+          // too bad we have no clue what failed\n+          PathIOException e = new PathIOException(dst.toString());\n+          e.setOperation(\"mkdir\");\n+          throw e;\n+        }    \n+        dst.refreshStatus(); // need to update stat to know it exists now\n+      }      \n+      super.recursePath(src);\n     } finally {\n       dst = savedDst;\n     }\n   }\n   \n-  @Override\n-  protected void processPath(PathData src) throws IOException {\n+  protected PathData getTargetPath(PathData src) throws IOException {\n     PathData target;\n-    // if the destination is a directory, make target a child path,\n-    // else use the destination as-is\n-    if (dst.exists && dst.stat.isDirectory()) {\n+    // on the first loop, the dst may be directory or a file, so only create\n+    // a child path if dst is a dir; after recursion, it's always a dir\n+    if ((getDepth() > 0) || (dst.exists && dst.stat.isDirectory())) {\n       target = dst.getPathDataForChild(src);\n     } else {\n       target = dst;\n     }\n-    if (target.exists && !overwrite) {\n+    return target;\n+  }\n+  \n+  /**\n+   * Copies the source file to the target.\n+   * @param src item to copy\n+   * @param target where to copy the item\n+   * @throws IOException if copy fails\n+   */ \n+  protected void copyFileToTarget(PathData src, PathData target) throws IOException {\n+    copyStreamToTarget(src.fs.open(src.path), target);\n+  }\n+  \n+  /**\n+   * Copies the stream contents to a temporary file.  If the copy is\n+   * successful, the temporary file will be renamed to the real path,\n+   * else the temporary file will be deleted.\n+   * @param in the input stream for the copy\n+   * @param target where to store the contents of the stream\n+   * @throws IOException if copy fails\n+   */ \n+  protected void copyStreamToTarget(InputStream in, PathData target)\n+  throws IOException {\n+    if (target.exists && (target.stat.isDirectory() || !overwrite)) {\n       throw new PathExistsException(target.toString());\n     }\n-\n-    try { \n-      // invoke processPath with both a source and resolved target\n-      processPath(src, target);\n-    } catch (PathIOException e) {\n-      // add the target unless it already has one\n-      if (e.getTargetPath() == null) {\n+    PathData tempFile = null;\n+    try {\n+      tempFile = target.createTempFile(target+\"._COPYING_\");\n+      FSDataOutputStream out = target.fs.create(tempFile.path, true);\n+      IOUtils.copyBytes(in, out, getConf(), true);\n+      // the rename method with an option to delete the target is deprecated\n+      if (target.exists && !target.fs.delete(target.path, false)) {\n+        // too bad we don't know why it failed\n+        PathIOException e = new PathIOException(target.toString());\n+        e.setOperation(\"delete\");\n+        throw e;\n+      }\n+      if (!tempFile.fs.rename(tempFile.path, target.path)) {\n+        // too bad we don't know why it failed\n+        PathIOException e = new PathIOException(tempFile.toString());\n+        e.setOperation(\"rename\");\n         e.setTargetPath(target.toString());\n+        throw e;\n+      }\n+      tempFile = null;\n+    } finally {\n+      if (tempFile != null) {\n+        tempFile.fs.delete(tempFile.path, false);\n       }\n-      throw e;\n     }\n   }\n-\n-  /**\n-   * Called with a source and target destination pair\n-   * @param src for the operation\n-   * @param target for the operation\n-   * @throws IOException if anything goes wrong\n-   */\n-  protected abstract void processPath(PathData src, PathData target)\n-  throws IOException;\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java",
                "sha": "6b3b40389f9f72528cdc248ee23af4af559df101",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java?ref=8f9661da4823bfbb243e430252ec1bb5780ecbfc",
                "deletions": 73,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java",
                "patch": "@@ -26,13 +26,7 @@\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.fs.ChecksumFileSystem;\n-import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.FileUtil;\n-import org.apache.hadoop.fs.LocalFileSystem;\n-import org.apache.hadoop.fs.shell.PathExceptions.PathExistsException;\n-import org.apache.hadoop.fs.shell.PathExceptions.PathIOException;\n-import org.apache.hadoop.fs.shell.PathExceptions.PathOperationException;\n-import org.apache.hadoop.io.IOUtils;\n \n /** Various commands for copy files */\n @InterfaceAudience.Private\n@@ -95,18 +89,10 @@ protected void processOptions(LinkedList<String> args) throws IOException {\n       CommandFormat cf = new CommandFormat(2, Integer.MAX_VALUE, \"f\");\n       cf.parse(args);\n       setOverwrite(cf.getOpt(\"f\"));\n+      // should have a -r option\n+      setRecursive(true);\n       getRemoteDestination(args);\n     }\n-\n-    @Override\n-    protected void processPath(PathData src, PathData target)\n-    throws IOException {\n-      if (!FileUtil.copy(src.fs, src.path, target.fs, target.path, false, overwrite, getConf())) {\n-        // we have no idea what the error is...  FileUtils masks it and in\n-        // some cases won't even report an error\n-        throw new PathIOException(src.toString());\n-      }\n-    }\n   }\n   \n   /** \n@@ -126,7 +112,6 @@ protected void processPath(PathData src, PathData target)\n      * It must be at least three characters long, required by\n      * {@link java.io.File#createTempFile(String, String, File)}.\n      */\n-    private static final String COPYTOLOCAL_PREFIX = \"_copyToLocal_\";\n     private boolean copyCrc;\n     private boolean verifyChecksum;\n \n@@ -144,7 +129,7 @@ protected void processOptions(LinkedList<String> args)\n     }\n \n     @Override\n-    protected void processPath(PathData src, PathData target)\n+    protected void copyFileToTarget(PathData src, PathData target)\n     throws IOException {\n       src.fs.setVerifyChecksum(verifyChecksum);\n \n@@ -153,41 +138,10 @@ protected void processPath(PathData src, PathData target)\n         copyCrc = false;\n       }      \n \n-      if (src.stat.isFile()) {\n-        // copy the file and maybe its crc\n-        copyFileToLocal(src, target);\n-        if (copyCrc) {\n-          copyFileToLocal(src.getChecksumFile(), target.getChecksumFile());\n-        }\n-      } else if (src.stat.isDirectory()) {\n-        // create the remote directory structure locally\n-        if (!target.toFile().mkdirs()) {\n-          throw new PathIOException(target.toString());\n-        }\n-      } else {\n-        throw new PathOperationException(src.toString());\n-      }\n-    }\n-\n-    private void copyFileToLocal(PathData src, PathData target)\n-    throws IOException {\n-      File targetFile = target.toFile();\n-      File tmpFile = FileUtil.createLocalTempFile(\n-          targetFile, COPYTOLOCAL_PREFIX, true);\n-      // too bad we can't tell exactly why it failed...\n-      if (!FileUtil.copy(src.fs, src.path, tmpFile, false, getConf())) {\n-        PathIOException e = new PathIOException(src.toString());\n-        e.setOperation(\"copy\");\n-        e.setTargetPath(tmpFile.toString());\n-        throw e;\n-      }\n-\n-      // too bad we can't tell exactly why it failed...\n-      if (!tmpFile.renameTo(targetFile)) {\n-        PathIOException e = new PathIOException(tmpFile.toString());\n-        e.setOperation(\"rename\");\n-        e.setTargetPath(targetFile.toString());\n-        throw e;\n+      super.copyFileToTarget(src, target);\n+      if (copyCrc) {\n+        // should we delete real file if crc copy fails?\n+        super.copyFileToTarget(src.getChecksumFile(), target.getChecksumFile());\n       }\n     }\n   }\n@@ -208,6 +162,8 @@ protected void processOptions(LinkedList<String> args) throws IOException {\n       cf.parse(args);\n       setOverwrite(cf.getOpt(\"f\"));\n       getRemoteDestination(args);\n+      // should have a -r option\n+      setRecursive(true);\n     }\n \n     // commands operating on local paths have no need for glob expansion\n@@ -223,30 +179,11 @@ protected void processArguments(LinkedList<PathData> args)\n     throws IOException {\n       // NOTE: this logic should be better, mimics previous implementation\n       if (args.size() == 1 && args.get(0).toString().equals(\"-\")) {\n-        if (dst.exists && !overwrite) {\n-          throw new PathExistsException(dst.toString());\n-        }\n-        copyFromStdin();\n+        copyStreamToTarget(System.in, getTargetPath(args.get(0)));\n         return;\n       }\n       super.processArguments(args);\n     }\n-\n-    @Override\n-    protected void processPath(PathData src, PathData target)\n-    throws IOException {\n-      target.fs.copyFromLocalFile(false, overwrite, src.path, target.path);\n-    }\n-\n-    /** Copies from stdin to the destination file. */\n-    protected void copyFromStdin() throws IOException {\n-      FSDataOutputStream out = dst.fs.create(dst.path); \n-      try {\n-        IOUtils.copyBytes(System.in, out, getConf(), false);\n-      } finally {\n-        out.close();\n-      }\n-    }\n   }\n \n   public static class CopyFromLocal extends Put {",
                "raw_url": "https://github.com/apache/hadoop/raw/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java",
                "sha": "066e5fdb899df7f517fb508451caf3a86f7caf68",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java?ref=8f9661da4823bfbb243e430252ec1bb5780ecbfc",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java",
                "patch": "@@ -182,6 +182,19 @@ public PathData getChecksumFile() throws IOException {\n     return new PathData(srcFs.getRawFileSystem(), srcPath.toString());\n   }\n \n+  /**\n+   * Returns a temporary file for this PathData with the given extension.\n+   * The file will be deleted on exit.\n+   * @param extension for the temporary file\n+   * @return PathData\n+   * @throws IOException shouldn't happen\n+   */\n+  public PathData createTempFile(String extension) throws IOException {\n+    PathData tmpFile = new PathData(fs, uri+\"._COPYING_\");\n+    fs.deleteOnExit(tmpFile.path);\n+    return tmpFile;\n+  }\n+\n   /**\n    * Returns a list of PathData objects of the items contained in the given\n    * directory.",
                "raw_url": "https://github.com/apache/hadoop/raw/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java",
                "sha": "a3c88f1f2af2736442e7db320919298356c681b4",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7771. FsShell -copyToLocal, -get, etc. commands throw NPE if the destination directory does not exist.  Contributed by John George and Daryn Sharp\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1195760 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/bb8fd6a2670d00d562673f32a55d3d0dd7aaa69c",
        "repo": "hadoop",
        "unit_tests": [
            "TestPathData.java"
        ]
    },
    "hadoop_9478484": {
        "bug_id": "hadoop_9478484",
        "commit": "https://github.com/apache/hadoop/commit/94784848456a92a6502f3a3c0074e44fba4b19c9",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/94784848456a92a6502f3a3c0074e44fba4b19c9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcScheduler.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcScheduler.java?ref=94784848456a92a6502f3a3c0074e44fba4b19c9",
                "deletions": 7,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcScheduler.java",
                "patch": "@@ -206,11 +206,6 @@ public DecayRpcScheduler(int numLevels, String ns, Configuration conf) {\n     this.backOffResponseTimeThresholds =\n         parseBackOffResponseTimeThreshold(ns, conf, numLevels);\n \n-    // Setup delay timer\n-    Timer timer = new Timer();\n-    DecayTask task = new DecayTask(this, timer);\n-    timer.scheduleAtFixedRate(task, decayPeriodMillis, decayPeriodMillis);\n-\n     // Setup response time metrics\n     responseTimeTotalInCurrWindow = new AtomicLongArray(numLevels);\n     responseTimeCountInCurrWindow = new AtomicLongArray(numLevels);\n@@ -223,6 +218,11 @@ public DecayRpcScheduler(int numLevels, String ns, Configuration conf) {\n     Preconditions.checkArgument(topUsersCount > 0,\n         \"the number of top users for scheduler metrics must be at least 1\");\n \n+    // Setup delay timer\n+    Timer timer = new Timer();\n+    DecayTask task = new DecayTask(this, timer);\n+    timer.scheduleAtFixedRate(task, decayPeriodMillis, decayPeriodMillis);\n+\n     MetricsProxy prox = MetricsProxy.getInstance(ns, numLevels);\n     prox.setDelegate(this);\n     prox.registerMetrics2Source(ns);\n@@ -821,9 +821,10 @@ private void addTopNCallerSummary(MetricsRecordBuilder rb) {\n     final int topCallerCount = 10;\n     TopN topNCallers = getTopCallers(topCallerCount);\n     Map<Object, Integer> decisions = scheduleCacheRef.get();\n-    for (int i=0; i < topNCallers.size(); i++) {\n+    final int actualCallerCount = topNCallers.size();\n+    for (int i = 0; i < actualCallerCount; i++) {\n       NameValuePair entry =  topNCallers.poll();\n-      String topCaller = \"Top.\" + (topCallerCount - i) + \".\" +\n+      String topCaller = \"Top.\" + (actualCallerCount - i) + \".\" +\n           \"Caller(\" + entry.getName() + \")\";\n       String topCallerVolume = topCaller + \".Volume\";\n       String topCallerPriority = topCaller + \".Priority\";",
                "raw_url": "https://github.com/apache/hadoop/raw/94784848456a92a6502f3a3c0074e44fba4b19c9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcScheduler.java",
                "sha": "3443d0394ad75a44e8506398762fcb622e727f05",
                "status": "modified"
            }
        ],
        "message": "HADOOP-13159. Fix potential NPE in Metrics2 source for DecayRpcScheduler. Contributed by Xiaoyu Yao.",
        "parent": "https://github.com/apache/hadoop/commit/0c6726e20d9503589b21123f30757ddfbd405dde",
        "repo": "hadoop",
        "unit_tests": [
            "TestDecayRpcScheduler.java"
        ]
    },
    "hadoop_9e35571": {
        "bug_id": "hadoop_9e35571",
        "commit": "https://github.com/apache/hadoop/commit/9e355719653c5e7b48b601090634882e4f29a743",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/9e355719653c5e7b48b601090634882e4f29a743/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=9e355719653c5e7b48b601090634882e4f29a743",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -782,6 +782,8 @@ Release 2.6.0 - UNRELEASED\n     HDFS-7096. Fix TestRpcProgramNfs3 to use DFS_ENCRYPTION_KEY_PROVIDER_URI\n     (clamb via cmccabe)\n \n+    HDFS-7046. HA NN can NPE upon transition to active. (kihwal)\n+\n     BREAKDOWN OF HDFS-6134 AND HADOOP-10150 SUBTASKS AND RELATED JIRAS\n   \n       HDFS-6387. HDFS CLI admin tool for creating & deleting an",
                "raw_url": "https://github.com/apache/hadoop/raw/9e355719653c5e7b48b601090634882e4f29a743/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "8c45d699c08a6fd5bc2abdea5733dd93667440bc",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/9e355719653c5e7b48b601090634882e4f29a743/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=9e355719653c5e7b48b601090634882e4f29a743",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -1156,8 +1156,9 @@ void startActiveServices() throws IOException {\n       cacheManager.startMonitorThread();\n       blockManager.getDatanodeManager().setShouldSendCachingCommands(true);\n     } finally {\n-      writeUnlock();\n       startingActiveService = false;\n+      checkSafeMode();\n+      writeUnlock();\n     }\n   }\n \n@@ -5570,6 +5571,9 @@ private void checkMode() {\n       // Have to have write-lock since leaving safemode initializes\n       // repl queues, which requires write lock\n       assert hasWriteLock();\n+      if (inTransitionToActive()) {\n+        return;\n+      }\n       // if smmthread is already running, the block threshold must have been \n       // reached before, there is no need to enter the safe mode again\n       if (smmthread == null && needEnter()) {",
                "raw_url": "https://github.com/apache/hadoop/raw/9e355719653c5e7b48b601090634882e4f29a743/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "4dc278667d1261e5128d1cc6029d6ab4dc7c05ff",
                "status": "modified"
            }
        ],
        "message": "HDFS-7046. HA NN can NPE upon transition to active. Contributed by\nKihwal Lee.",
        "parent": "https://github.com/apache/hadoop/commit/adf0b67a7104bd457b20c95ff78dd48753dcd699",
        "repo": "hadoop",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_a446ad2": {
        "bug_id": "hadoop_a446ad2",
        "commit": "https://github.com/apache/hadoop/commit/a446ad2c26359beb2b5367195de4257fbae648c6",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/a446ad2c26359beb2b5367195de4257fbae648c6/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=a446ad2c26359beb2b5367195de4257fbae648c6",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -481,6 +481,8 @@ Release 0.23.3 - UNRELEASED\n     MAPREDUCE-4237. TestNodeStatusUpdater can fail if localhost has a domain\n     associated with it (bobby)\n \n+    MAPREDUCE-4233. NPE can happen in RMNMNodeInfo. (bobby)\n+\n Release 0.23.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/a446ad2c26359beb2b5367195de4257fbae648c6/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "ff4664675d44bb97f2aff87288a3d2c7127d3307",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hadoop/blob/a446ad2c26359beb2b5367195de4257fbae648c6/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java?ref=a446ad2c26359beb2b5367195de4257fbae648c6",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java",
                "patch": "@@ -21,22 +21,28 @@\n import java.io.File;\n import java.io.IOException;\n import java.util.Iterator;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.yarn.api.records.NodeId;\n+import org.apache.hadoop.yarn.server.resourcemanager.MockNodes;\n import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n import org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;\n import org.codehaus.jackson.JsonNode;\n import org.codehaus.jackson.map.ObjectMapper;\n import org.junit.AfterClass;\n import org.junit.Assert;\n import org.junit.BeforeClass;\n import org.junit.Test;\n+import static org.mockito.Mockito.*;\n \n public class TestRMNMInfo {\n   private static final Log LOG = LogFactory.getLog(TestRMNMInfo.class);\n@@ -116,14 +122,47 @@ public void testRMNMInfo() throws Exception {\n               n.get(\"HealthStatus\").getValueAsText().contains(\"Healthy\"));\n       Assert.assertNotNull(n.get(\"LastHealthUpdate\"));\n       Assert.assertNotNull(n.get(\"HealthReport\"));\n-      Assert.assertNotNull(n.get(\"NumContainersMB\"));\n+      Assert.assertNotNull(n.get(\"NumContainers\"));\n       Assert.assertEquals(\n               n.get(\"NodeId\") + \": Unexpected number of used containers\",\n-              0, n.get(\"NumContainersMB\").getValueAsInt());\n+              0, n.get(\"NumContainers\").getValueAsInt());\n       Assert.assertEquals(\n               n.get(\"NodeId\") + \": Unexpected amount of used memory\",\n               0, n.get(\"UsedMemoryMB\").getValueAsInt());\n       Assert.assertNotNull(n.get(\"AvailableMemoryMB\"));\n     }\n   }\n+  \n+  @Test\n+  public void testRMNMInfoMissmatch() throws Exception {\n+    RMContext rmc = mock(RMContext.class);\n+    ResourceScheduler rms = mock(ResourceScheduler.class);\n+    ConcurrentMap<NodeId, RMNode> map = new ConcurrentHashMap<NodeId, RMNode>();\n+    RMNode node = MockNodes.newNodeInfo(1, MockNodes.newResource(4 * 1024));\n+    map.put(node.getNodeID(), node);\n+    when(rmc.getRMNodes()).thenReturn(map);\n+    \n+    RMNMInfo rmInfo = new RMNMInfo(rmc,rms);\n+    String liveNMs = rmInfo.getLiveNodeManagers();\n+    ObjectMapper mapper = new ObjectMapper();\n+    JsonNode jn = mapper.readTree(liveNMs);\n+    Assert.assertEquals(\"Unexpected number of live nodes:\",\n+                                               1, jn.size());\n+    Iterator<JsonNode> it = jn.iterator();\n+    while (it.hasNext()) {\n+      JsonNode n = it.next();\n+      Assert.assertNotNull(n.get(\"HostName\"));\n+      Assert.assertNotNull(n.get(\"Rack\"));\n+      Assert.assertTrue(\"Node \" + n.get(\"NodeId\") + \" should be RUNNING\",\n+              n.get(\"State\").getValueAsText().contains(\"RUNNING\"));\n+      Assert.assertNotNull(n.get(\"NodeHTTPAddress\"));\n+      Assert.assertTrue(\"Node \" + n.get(\"NodeId\") + \" should be Healthy\",\n+              n.get(\"HealthStatus\").getValueAsText().contains(\"Healthy\"));\n+      Assert.assertNotNull(n.get(\"LastHealthUpdate\"));\n+      Assert.assertNotNull(n.get(\"HealthReport\"));\n+      Assert.assertNull(n.get(\"NumContainers\"));\n+      Assert.assertNull(n.get(\"UsedMemoryMB\"));\n+      Assert.assertNull(n.get(\"AvailableMemoryMB\"));\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/a446ad2c26359beb2b5367195de4257fbae648c6/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java",
                "sha": "4ee485644d91ad5b1fcffde9846301c36d237353",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/a446ad2c26359beb2b5367195de4257fbae648c6/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java?ref=a446ad2c26359beb2b5367195de4257fbae648c6",
                "deletions": 4,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java",
                "patch": "@@ -93,10 +93,12 @@ public String getLiveNodeManagers() {\n                         ni.getNodeHealthStatus().getLastHealthReportTime());\n         info.put(\"HealthReport\",\n                         ni.getNodeHealthStatus().getHealthReport());\n-        info.put(\"NumContainersMB\", report.getNumContainers());\n-        info.put(\"UsedMemoryMB\", report.getUsedResource().getMemory());\n-        info.put(\"AvailableMemoryMB\",\n-                                report.getAvailableResource().getMemory());\n+        if(report != null) {\n+          info.put(\"NumContainers\", report.getNumContainers());\n+          info.put(\"UsedMemoryMB\", report.getUsedResource().getMemory());\n+          info.put(\"AvailableMemoryMB\",\n+              report.getAvailableResource().getMemory());\n+        }\n \n         nodesInfo.add(info);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/a446ad2c26359beb2b5367195de4257fbae648c6/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java",
                "sha": "0db42e40ec0e08d72413048956baf0393062157d",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4233. NPE can happen in RMNMNodeInfo. (bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1337363 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/f092e9fc8a2b58c755ecfc6828cc3e2af624b90b",
        "repo": "hadoop",
        "unit_tests": [
            "TestRMNMInfo.java"
        ]
    },
    "hadoop_a4f62a2": {
        "bug_id": "hadoop_a4f62a2",
        "commit": "https://github.com/apache/hadoop/commit/a4f62a2d58d4e00cda3632411c6abda6eaa12a0e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/a4f62a2d58d4e00cda3632411c6abda6eaa12a0e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=a4f62a2d58d4e00cda3632411c6abda6eaa12a0e",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -2308,6 +2308,9 @@ Release 2.8.0 - UNRELEASED\n     HDFS-9387. Fix namenodeUri parameter parsing in NNThroughputBenchmark.\n     (Mingliang Liu via xyao)\n \n+    HDFS-9421. NNThroughputBenchmark replication test NPE with -namenode option.\n+    (Mingliang Liu via xyao)\n+\n Release 2.7.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/a4f62a2d58d4e00cda3632411c6abda6eaa12a0e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "d76253a908a73012071f3b8a5eb38ff395cd2cb3",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop/blob/a4f62a2d58d4e00cda3632411c6abda6eaa12a0e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java?ref=a4f62a2d58d4e00cda3632411c6abda6eaa12a0e",
                "deletions": 9,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java",
                "patch": "@@ -106,9 +106,9 @@\n  * By default the refresh is never called.</li>\n  * <li>-keepResults do not clean up the name-space after execution.</li>\n  * <li>-useExisting do not recreate the name-space, use existing data.</li>\n- * <li>-namenode will run the test against a namenode in another\n- * process or on another host. If you use this option, the namenode\n- * must have dfs.namenode.fs-limits.min-block-size set to 16.</li>\n+ * <li>-namenode will run the test (except {@link ReplicationStats}) against a\n+ * namenode in another process or on another host. If you use this option,\n+ * the namenode must have dfs.namenode.fs-limits.min-block-size set to 16.</li>\n  * </ol>\n  * \n  * The benchmark first generates inputs for each thread so that the\n@@ -126,8 +126,9 @@\n   private static final String GENERAL_OPTIONS_USAGE = \n     \"     [-keepResults] | [-logLevel L] | [-UGCacheRefreshCount G] |\" +\n     \" [-namenode <namenode URI>]\\n\" +\n-    \"     If using -namenode, set the namenode's\" +\n-    \"         dfs.namenode.fs-limits.min-block-size to 16.\";\n+    \"     If using -namenode, set the namenode's \" +\n+    \"dfs.namenode.fs-limits.min-block-size to 16. Replication test does not \" +\n+        \"support -namenode.\";\n \n   static Configuration config;\n   static NameNode nameNode;\n@@ -1471,13 +1472,22 @@ public int run(String[] aArgs) throws Exception {\n         ops.add(opStat);\n       }\n       if(runAll || ReplicationStats.OP_REPLICATION_NAME.equals(type)) {\n-        opStat = new ReplicationStats(args);\n-        ops.add(opStat);\n+        if (namenodeUri != null || args.contains(\"-namenode\")) {\n+          LOG.warn(\"The replication test is ignored as it does not support \" +\n+              \"standalone namenode in another process or on another host. \" +\n+              \"Please run replication test without -namenode argument.\");\n+        } else {\n+          opStat = new ReplicationStats(args);\n+          ops.add(opStat);\n+        }\n       }\n       if(runAll || CleanAllStats.OP_CLEAN_NAME.equals(type)) {\n         opStat = new CleanAllStats(args);\n         ops.add(opStat);\n       }\n+      if (ops.isEmpty()) {\n+        printUsage();\n+      }\n \n       if (namenodeUri == null) {\n         nameNode = NameNode.createNameNode(argv, config);\n@@ -1501,8 +1511,6 @@ public int run(String[] aArgs) throws Exception {\n             DFSTestUtil.getRefreshUserMappingsProtocolProxy(config, nnUri);\n         getBlockPoolId(dfs);\n       }\n-      if(ops.size() == 0)\n-        printUsage();\n       // run each benchmark\n       for(OperationStatsBase op : ops) {\n         LOG.info(\"Starting benchmark: \" + op.getOpName());",
                "raw_url": "https://github.com/apache/hadoop/raw/a4f62a2d58d4e00cda3632411c6abda6eaa12a0e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java",
                "sha": "affbe2f9af54d24dadaa4e31b5f018aee1efc084",
                "status": "modified"
            }
        ],
        "message": "HDFS-9421. NNThroughputBenchmark replication test NPE with -namenode option. Contributed by Mingliang Liu.",
        "parent": "https://github.com/apache/hadoop/commit/2701f2d2558f3ade879539f3f7bedf749709f2f1",
        "repo": "hadoop",
        "unit_tests": [
            "TestNNThroughputBenchmark.java"
        ]
    },
    "hadoop_a8d60f4": {
        "bug_id": "hadoop_a8d60f4",
        "commit": "https://github.com/apache/hadoop/commit/a8d60f4190a3a5f7a88c04f30bf61052c53f2b44",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/a8d60f4190a3a5f7a88c04f30bf61052c53f2b44/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=a8d60f4190a3a5f7a88c04f30bf61052c53f2b44",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -148,6 +148,8 @@ Trunk (Unreleased)\n \n   BUG FIXES\n \n+    HADOOP-8419. Fixed GzipCode NPE reset for IBM JDK. (Yu Li via eyang)\n+\n     HADOOP-9041. FsUrlStreamHandlerFactory could cause an infinite loop in\n     FileSystem initialization. (Yanbo Liang and Radim Kolar via llu)\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/a8d60f4190a3a5f7a88c04f30bf61052c53f2b44/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "1d9febccf2eccfb1751aa062dded300ba7728be2",
                "status": "modified"
            },
            {
                "additions": 62,
                "blob_url": "https://github.com/apache/hadoop/blob/a8d60f4190a3a5f7a88c04f30bf61052c53f2b44/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/GzipCodec.java",
                "changes": 64,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/GzipCodec.java?ref=a8d60f4190a3a5f7a88c04f30bf61052c53f2b44",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/GzipCodec.java",
                "patch": "@@ -40,14 +40,74 @@\n   protected static class GzipOutputStream extends CompressorStream {\n \n     private static class ResetableGZIPOutputStream extends GZIPOutputStream {\n-      \n+      private static final int TRAILER_SIZE = 8;\n+      public static final String JVMVendor= System.getProperty(\"java.vendor\");\n+      public static final String JVMVersion= System.getProperty(\"java.version\");\n+      private static final boolean HAS_BROKEN_FINISH =\n+          (JVMVendor.contains(\"IBM\") && JVMVersion.contains(\"1.6.0\"));\n+\n       public ResetableGZIPOutputStream(OutputStream out) throws IOException {\n         super(out);\n       }\n-      \n+\n       public void resetState() throws IOException {\n         def.reset();\n       }\n+\n+      /**\n+       * Override this method for HADOOP-8419.\n+       * Override because IBM implementation calls def.end() which\n+       * causes problem when reseting the stream for reuse.\n+       *\n+       */\n+      @Override\n+      public void finish() throws IOException {\n+        if (HAS_BROKEN_FINISH) {\n+          if (!def.finished()) {\n+            def.finish();\n+            while (!def.finished()) {\n+              int i = def.deflate(this.buf, 0, this.buf.length);\n+              if ((def.finished()) && (i <= this.buf.length - TRAILER_SIZE)) {\n+                writeTrailer(this.buf, i);\n+                i += TRAILER_SIZE;\n+                out.write(this.buf, 0, i);\n+\n+                return;\n+              }\n+              if (i > 0) {\n+                out.write(this.buf, 0, i);\n+              }\n+            }\n+\n+            byte[] arrayOfByte = new byte[TRAILER_SIZE];\n+            writeTrailer(arrayOfByte, 0);\n+            out.write(arrayOfByte);\n+          }\n+        } else {\n+          super.finish();\n+        }\n+      }\n+\n+      /** re-implement for HADOOP-8419 because the relative method in jdk is invisible */\n+      private void writeTrailer(byte[] paramArrayOfByte, int paramInt)\n+        throws IOException {\n+        writeInt((int)this.crc.getValue(), paramArrayOfByte, paramInt);\n+        writeInt(this.def.getTotalIn(), paramArrayOfByte, paramInt + 4);\n+      }\n+\n+      /** re-implement for HADOOP-8419 because the relative method in jdk is invisible */\n+      private void writeInt(int paramInt1, byte[] paramArrayOfByte, int paramInt2)\n+        throws IOException {\n+        writeShort(paramInt1 & 0xFFFF, paramArrayOfByte, paramInt2);\n+        writeShort(paramInt1 >> 16 & 0xFFFF, paramArrayOfByte, paramInt2 + 2);\n+      }\n+\n+      /** re-implement for HADOOP-8419 because the relative method in jdk is invisible */\n+      private void writeShort(int paramInt1, byte[] paramArrayOfByte, int paramInt2)\n+        throws IOException {\n+        paramArrayOfByte[paramInt2] = (byte)(paramInt1 & 0xFF);\n+        paramArrayOfByte[(paramInt2 + 1)] = (byte)(paramInt1 >> 8 & 0xFF);\n+      }\n     }\n \n     public GzipOutputStream(OutputStream out) throws IOException {",
                "raw_url": "https://github.com/apache/hadoop/raw/a8d60f4190a3a5f7a88c04f30bf61052c53f2b44/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/GzipCodec.java",
                "sha": "6ac692c14e7e97cc97beaeeb54a1e9b239e3b914",
                "status": "modified"
            }
        ],
        "message": "HADOOP-8419. Fixed GzipCode NPE reset for IBM JDK. (Yu Li via eyang)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1431739 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/be5509c53743a0beddda3f5798e72b919e797bd0",
        "repo": "hadoop",
        "unit_tests": [
            "TestGzipCodec.java"
        ]
    },
    "hadoop_aac5472": {
        "bug_id": "hadoop_aac5472",
        "commit": "https://github.com/apache/hadoop/commit/aac547249134ec4f9d5229d28e4fdeef0f6f0dbb",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/aac547249134ec4f9d5229d28e4fdeef0f6f0dbb/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/CHANGES.txt?ref=aac547249134ec4f9d5229d28e4fdeef0f6f0dbb",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -100,6 +100,9 @@ Trunk (unreleased changes)\n     HADOOP-7131. Exceptions thrown by Text methods should include the causing\n     exception. (Uma Maheswara Rao G via todd)\n \n+    HADOOP-6912. Guard against NPE when calling UGI.isLoginKeytabBased().\n+    (Kan Zhang via jitendra)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/aac547249134ec4f9d5229d28e4fdeef0f6f0dbb/CHANGES.txt",
                "sha": "ead2d3f88a9ba1f5b25e5526666fa9d823086f02",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/aac547249134ec4f9d5229d28e4fdeef0f6f0dbb/src/java/org/apache/hadoop/security/UserGroupInformation.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/org/apache/hadoop/security/UserGroupInformation.java?ref=aac547249134ec4f9d5229d28e4fdeef0f6f0dbb",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/security/UserGroupInformation.java",
                "patch": "@@ -811,8 +811,8 @@ private boolean hasSufficientTimeElapsed(long now) {\n    * Did the login happen via keytab\n    * @return true or false\n    */\n-  public synchronized static boolean isLoginKeytabBased() {\n-    return loginUser.isKeytab;\n+  public synchronized static boolean isLoginKeytabBased() throws IOException {\n+    return getLoginUser().isKeytab;\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/aac547249134ec4f9d5229d28e4fdeef0f6f0dbb/src/java/org/apache/hadoop/security/UserGroupInformation.java",
                "sha": "085ce61719eefec5cd06738b846867a1335f08cd",
                "status": "modified"
            }
        ],
        "message": "HADOOP-6912. Guard against NPE when calling UGI.isLoginKeytabBased(). Contributed by Kan Zhang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1079068 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/69fe37a007045c3a3cf3b2b410e1ad14717fdb76",
        "repo": "hadoop",
        "unit_tests": [
            "TestUserGroupInformation.java"
        ]
    },
    "hadoop_ac5ae00": {
        "bug_id": "hadoop_ac5ae00",
        "commit": "https://github.com/apache/hadoop/commit/ac5ae0065a127ac150a887fa6c6f3cffd86ef733",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/ac5ae0065a127ac150a887fa6c6f3cffd86ef733/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java?ref=ac5ae0065a127ac150a887fa6c6f3cffd86ef733",
                "deletions": 6,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "patch": "@@ -2282,12 +2282,14 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,\n       if (memBlockInfo.getGenerationStamp() != diskGS) {\n         File memMetaFile = FsDatasetUtil.getMetaFile(diskFile, \n             memBlockInfo.getGenerationStamp());\n-        if (memMetaFile.exists()) {\n-          if (memMetaFile.compareTo(diskMetaFile) != 0) {\n-            LOG.warn(\"Metadata file in memory \"\n-                + memMetaFile.getAbsolutePath()\n-                + \" does not match file found by scan \"\n-                + (diskMetaFile == null? null: diskMetaFile.getAbsolutePath()));\n+        if (fileIoProvider.exists(vol, memMetaFile)) {\n+          String warningPrefix = \"Metadata file in memory \"\n+              + memMetaFile.getAbsolutePath()\n+              + \" does not match file found by scan \";\n+          if (!diskMetaFileExists) {\n+            LOG.warn(warningPrefix + \"null\");\n+          } else if (memMetaFile.compareTo(diskMetaFile) != 0) {\n+            LOG.warn(warningPrefix + diskMetaFile.getAbsolutePath());\n           }\n         } else {\n           // Metadata file corresponding to block in memory is missing",
                "raw_url": "https://github.com/apache/hadoop/raw/ac5ae0065a127ac150a887fa6c6f3cffd86ef733/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "sha": "aff19ce97e0c008722ad6a33bd6a38ae91220d76",
                "status": "modified"
            }
        ],
        "message": "HDFS-11476. Fix NPE in FsDatasetImpl#checkAndUpdate. Contributed by Xiaobing Zhou.",
        "parent": "https://github.com/apache/hadoop/commit/2148b83993fd8ce73bcbc7677c57ee5028a59cd4",
        "repo": "hadoop",
        "unit_tests": [
            "TestFsDatasetImpl.java"
        ]
    },
    "hadoop_b4097b9": {
        "bug_id": "hadoop_b4097b9",
        "commit": "https://github.com/apache/hadoop/commit/b4097b96a39bad6214b01989e7f2fb37dad70793",
        "file": [
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/hadoop/blob/b4097b96a39bad6214b01989e7f2fb37dad70793/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/RollingLevelDBTimelineStore.java",
                "changes": 55,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/RollingLevelDBTimelineStore.java?ref=b4097b96a39bad6214b01989e7f2fb37dad70793",
                "deletions": 26,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/RollingLevelDBTimelineStore.java",
                "patch": "@@ -793,39 +793,42 @@ private TimelineEntities getEntityByTime(byte[] base, String entityType,\n             entity = getEntity(entityId, entityType, startTime, queryFields,\n                 iterator, key, kp.getOffset());\n           }\n-          // determine if the retrieved entity matches the provided secondary\n-          // filters, and if so add it to the list of entities to return\n-          boolean filterPassed = true;\n-          if (secondaryFilters != null) {\n-            for (NameValuePair filter : secondaryFilters) {\n-              Object v = entity.getOtherInfo().get(filter.getName());\n-              if (v == null) {\n-                Set<Object> vs = entity.getPrimaryFilters()\n-                    .get(filter.getName());\n-                if (vs == null || !vs.contains(filter.getValue())) {\n+\n+          if (entity != null) {\n+            // determine if the retrieved entity matches the provided secondary\n+            // filters, and if so add it to the list of entities to return\n+            boolean filterPassed = true;\n+            if (secondaryFilters != null) {\n+              for (NameValuePair filter : secondaryFilters) {\n+                Object v = entity.getOtherInfo().get(filter.getName());\n+                if (v == null) {\n+                  Set<Object> vs = entity.getPrimaryFilters()\n+                          .get(filter.getName());\n+                  if (vs == null || !vs.contains(filter.getValue())) {\n+                    filterPassed = false;\n+                    break;\n+                  }\n+                } else if (!v.equals(filter.getValue())) {\n                   filterPassed = false;\n                   break;\n                 }\n-              } else if (!v.equals(filter.getValue())) {\n-                filterPassed = false;\n-                break;\n               }\n             }\n-          }\n-          if (filterPassed) {\n-            if (entity.getDomainId() == null) {\n-              entity.setDomainId(DEFAULT_DOMAIN_ID);\n-            }\n-            if (checkAcl == null || checkAcl.check(entity)) {\n-              // Remove primary filter and other info if they are added for\n-              // matching secondary filters\n-              if (addPrimaryFilters) {\n-                entity.setPrimaryFilters(null);\n+            if (filterPassed) {\n+              if (entity.getDomainId() == null) {\n+                entity.setDomainId(DEFAULT_DOMAIN_ID);\n               }\n-              if (addOtherInfo) {\n-                entity.setOtherInfo(null);\n+              if (checkAcl == null || checkAcl.check(entity)) {\n+                // Remove primary filter and other info if they are added for\n+                // matching secondary filters\n+                if (addPrimaryFilters) {\n+                  entity.setPrimaryFilters(null);\n+                }\n+                if (addOtherInfo) {\n+                  entity.setOtherInfo(null);\n+                }\n+                entities.addEntity(entity);\n               }\n-              entities.addEntity(entity);\n             }\n           }\n         }",
                "raw_url": "https://github.com/apache/hadoop/raw/b4097b96a39bad6214b01989e7f2fb37dad70793/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/RollingLevelDBTimelineStore.java",
                "sha": "e85505f73e877128a52bd24b0668469b55a5e097",
                "status": "modified"
            }
        ],
        "message": "YARN-9744. RollingLevelDBTimelineStore.getEntityByTime fails with NPE. Contributed by Prabhu Joseph.",
        "parent": "https://github.com/apache/hadoop/commit/0b507d2ddf132985b43b4e2d3ad11d7fd2d90cd3",
        "repo": "hadoop",
        "unit_tests": [
            "TestRollingLevelDBTimelineStore.java"
        ]
    },
    "hadoop_c020b62": {
        "bug_id": "hadoop_c020b62",
        "commit": "https://github.com/apache/hadoop/commit/c020b62cf8de1f3baadc9d2f3410640ef7880543",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c020b62cf8de1f3baadc9d2f3410640ef7880543/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=c020b62cf8de1f3baadc9d2f3410640ef7880543",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -707,6 +707,9 @@ Release 2.8.0 - UNRELEASED\n     YARN-3982. container-executor parsing of container-executor.cfg broken in\n     trunk and branch-2. (Varun Vasudev via xgong)\n \n+    YARN-3919. NPEs' while stopping service after exception during\n+    CommonNodeLabelsManager#start. (varun saxane via rohithsharmaks)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/c020b62cf8de1f3baadc9d2f3410640ef7880543/hadoop-yarn-project/CHANGES.txt",
                "sha": "8e8a76b7c87b8eb9a2d2bd9eb6d27dddacc621c6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c020b62cf8de1f3baadc9d2f3410640ef7880543/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java?ref=c020b62cf8de1f3baadc9d2f3410640ef7880543",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java",
                "patch": "@@ -139,7 +139,8 @@ protected void serviceStop() throws Exception {\n       blockNewEvents = true;\n       LOG.info(\"AsyncDispatcher is draining to stop, igonring any new events.\");\n       synchronized (waitForDrained) {\n-        while (!drained && eventHandlingThread.isAlive()) {\n+        while (!drained && eventHandlingThread != null\n+            && eventHandlingThread.isAlive()) {\n           waitForDrained.wait(1000);\n           LOG.info(\"Waiting for AsyncDispatcher to drain. Thread state is :\" +\n               eventHandlingThread.getState());",
                "raw_url": "https://github.com/apache/hadoop/raw/c020b62cf8de1f3baadc9d2f3410640ef7880543/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java",
                "sha": "f6701128ac625fed1dd8ddbcccb6a7ea206a40f3",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c020b62cf8de1f3baadc9d2f3410640ef7880543/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/FileSystemNodeLabelsStore.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/FileSystemNodeLabelsStore.java?ref=c020b62cf8de1f3baadc9d2f3410640ef7880543",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/FileSystemNodeLabelsStore.java",
                "patch": "@@ -33,6 +33,7 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.LocalFileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.yarn.api.records.NodeId;\n import org.apache.hadoop.yarn.api.records.NodeLabel;\n@@ -92,12 +93,7 @@ public void init(Configuration conf) throws Exception {\n \n   @Override\n   public void close() throws IOException {\n-    try {\n-      fs.close();\n-      editlogOs.close();\n-    } catch (IOException e) {\n-      LOG.warn(\"Exception happened whiling shutting down,\", e);\n-    }\n+    IOUtils.cleanup(LOG, fs, editlogOs);\n   }\n \n   private void setFileSystem(Configuration conf) throws IOException {",
                "raw_url": "https://github.com/apache/hadoop/raw/c020b62cf8de1f3baadc9d2f3410640ef7880543/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/FileSystemNodeLabelsStore.java",
                "sha": "20dc67c10e4a4068eb122076a32a441a98f3eda1",
                "status": "modified"
            }
        ],
        "message": "YARN-3919. NPEs' while stopping service after exception during CommonNodeLabelsManager#start. (varun saxena via rohithsharmaks)",
        "parent": "https://github.com/apache/hadoop/commit/5205a330b387d2e133ee790b9fe7d5af3cd8bccc",
        "repo": "hadoop",
        "unit_tests": [
            "TestFileSystemNodeLabelsStore.java"
        ]
    },
    "hadoop_c0a903d": {
        "bug_id": "hadoop_c0a903d",
        "commit": "https://github.com/apache/hadoop/commit/c0a903da22c65294b232c7530a6a684ee93daba4",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c0a903da22c65294b232c7530a6a684ee93daba4/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=c0a903da22c65294b232c7530a6a684ee93daba4",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -525,6 +525,9 @@ Release 2.4.0 - UNRELEASED\n     HDFS-6040. fix DFSClient issue without libhadoop.so and some other\n     ShortCircuitShm cleanups (cmccabe)\n \n+    HDFS-6047 TestPread NPE inside in DFSInputStream hedgedFetchBlockByteRange\n+    (stack)\n+\n   BREAKDOWN OF HDFS-5698 SUBTASKS AND RELATED JIRAS\n \n     HDFS-5717. Save FSImage header in protobuf. (Haohui Mai via jing9)",
                "raw_url": "https://github.com/apache/hadoop/raw/c0a903da22c65294b232c7530a6a684ee93daba4/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "acd5f15ae89902dd1eb04f86547b0e58e1cb793b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/c0a903da22c65294b232c7530a6a684ee93daba4/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java?ref=c0a903da22c65294b232c7530a6a684ee93daba4",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "patch": "@@ -1177,8 +1177,11 @@ private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n           // exception already handled in the call method. getFirstToComplete\n           // will remove the failing future from the list. nothing more to do.\n         }\n-        // We got here if exception.  Ignore this node on next go around.\n-        ignored.add(chosenNode.info);\n+        // We got here if exception.  Ignore this node on next go around IFF\n+        // we found a chosenNode to hedge read against.\n+        if (chosenNode != null && chosenNode.info != null) {\n+          ignored.add(chosenNode.info);\n+        }\n       }\n       // executed if we get an error from a data node\n       block = getBlockAt(block.getStartOffset(), false);",
                "raw_url": "https://github.com/apache/hadoop/raw/c0a903da22c65294b232c7530a6a684ee93daba4/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "sha": "3705a2fd4f353b3074cc01d750440d26ce8da993",
                "status": "modified"
            }
        ],
        "message": "HDFS-6047 TestPread NPE inside in DFSInputStream hedgedFetchBlockByteRange (stack)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1574205 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/88245b6a41171f939b22186c533ea2bc7994f9b3",
        "repo": "hadoop",
        "unit_tests": [
            "TestDFSInputStream.java"
        ]
    },
    "hadoop_c4382e7": {
        "bug_id": "hadoop_c4382e7",
        "commit": "https://github.com/apache/hadoop/commit/c4382e7565447277e716c22dd20053113e0732cb",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c4382e7565447277e716c22dd20053113e0732cb/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=c4382e7565447277e716c22dd20053113e0732cb",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -561,6 +561,8 @@ Release 2.1.0-beta - UNRELEASED\n \n     HDFS-4382. Fix typo MAX_NOT_CHANGED_INTERATIONS. (Ted Yu via suresh)\n \n+    HDFS-4840. ReplicationMonitor gets NPE during shutdown. (kihwal)\n+\n   BREAKDOWN OF HDFS-347 SUBTASKS AND RELATED JIRAS\n \n     HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes.",
                "raw_url": "https://github.com/apache/hadoop/raw/c4382e7565447277e716c22dd20053113e0732cb/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "a85341ec5480ca225f9dfb62af0e0a0ffabd4601",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/c4382e7565447277e716c22dd20053113e0732cb/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=c4382e7565447277e716c22dd20053113e0732cb",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -3094,10 +3094,15 @@ public void run() {\n           computeDatanodeWork();\n           processPendingReplications();\n           Thread.sleep(replicationRecheckInterval);\n-        } catch (InterruptedException ie) {\n-          LOG.warn(\"ReplicationMonitor thread received InterruptedException.\", ie);\n-          break;\n         } catch (Throwable t) {\n+          if (!namesystem.isRunning()) {\n+            LOG.info(\"Stopping ReplicationMonitor.\");\n+            if (!(t instanceof InterruptedException)) {\n+              LOG.info(\"ReplicationMonitor received an exception\"\n+                  + \" while shutting down.\", t);\n+            }\n+            break;\n+          }\n           LOG.fatal(\"ReplicationMonitor thread received Runtime exception. \", t);\n           terminate(1, t);\n         }",
                "raw_url": "https://github.com/apache/hadoop/raw/c4382e7565447277e716c22dd20053113e0732cb/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "46809da5cb1dca4eaeaef0696b662f86e6bdd197",
                "status": "modified"
            }
        ],
        "message": "HDFS-4840. ReplicationMonitor gets NPE during shutdown. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1489634 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/af65d6f80ee095f8a7652244511d02ce5584a160",
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockManager.java"
        ]
    },
    "hadoop_c684f2b": {
        "bug_id": "hadoop_c684f2b",
        "commit": "https://github.com/apache/hadoop/commit/c684f2b007a4808dafbe1c1d3ce01758e281d329",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c684f2b007a4808dafbe1c1d3ce01758e281d329/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=c684f2b007a4808dafbe1c1d3ce01758e281d329",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -227,6 +227,9 @@ Release 2.9.0 - UNRELEASED\n     YARN-4651. Document movetoqueue option in 'YARN Commands'\n     (Takashi Ohnishi via rohithsharmaks)\n \n+    YARN-4729. SchedulerApplicationAttempt#getTotalRequiredResources can throw \n+    an NPE. (kasha)\n+\n Release 2.8.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/c684f2b007a4808dafbe1c1d3ce01758e281d329/hadoop-yarn-project/CHANGES.txt",
                "sha": "7f26f8e63cda9d19ac4d07eec0a910e412a5d630",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c684f2b007a4808dafbe1c1d3ce01758e281d329/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java?ref=c684f2b007a4808dafbe1c1d3ce01758e281d329",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java",
                "patch": "@@ -244,7 +244,8 @@ public synchronized ResourceRequest getResourceRequest(Priority priority,\n   }\n \n   public synchronized int getTotalRequiredResources(Priority priority) {\n-    return getResourceRequest(priority, ResourceRequest.ANY).getNumContainers();\n+    ResourceRequest request = getResourceRequest(priority, ResourceRequest.ANY);\n+    return request == null ? 0 : request.getNumContainers();\n   }\n \n   public synchronized Resource getResource(Priority priority) {",
                "raw_url": "https://github.com/apache/hadoop/raw/c684f2b007a4808dafbe1c1d3ce01758e281d329/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java",
                "sha": "254200972cbb71aff59569c4d2c50df28390de0d",
                "status": "modified"
            }
        ],
        "message": "YARN-4729. SchedulerApplicationAttempt#getTotalRequiredResources can throw an NPE. (kasha)",
        "parent": "https://github.com/apache/hadoop/commit/dbbfc58c33fd1d2f7abae1784c2d78b7438825e2",
        "repo": "hadoop",
        "unit_tests": [
            "TestSchedulerApplicationAttempt.java"
        ]
    },
    "hadoop_c9cb6a5": {
        "bug_id": "hadoop_c9cb6a5",
        "commit": "https://github.com/apache/hadoop/commit/c9cb6a5960ad335a3ee93a6ee219eae5aad372f9",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c9cb6a5960ad335a3ee93a6ee219eae5aad372f9/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=c9cb6a5960ad335a3ee93a6ee219eae5aad372f9",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -863,6 +863,8 @@ Release 2.8.0 - UNRELEASED\n     YARN-4135. Improve the assertion message in MockRM while failing after waiting for the state.\n     (Nijel S F via rohithsharmaks)\n \n+    YARN-4167. NPE on RMActiveServices#serviceStop when store is null. (Bibin A Chundatt via rohithsharmaks)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/c9cb6a5960ad335a3ee93a6ee219eae5aad372f9/hadoop-yarn-project/CHANGES.txt",
                "sha": "a3dfb85445ab887596e48830f8ba686ce32cc482",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c9cb6a5960ad335a3ee93a6ee219eae5aad372f9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java?ref=c9cb6a5960ad335a3ee93a6ee219eae5aad372f9",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "patch": "@@ -605,7 +605,9 @@ protected void serviceStop() throws Exception {\n       if (rmContext != null) {\n         RMStateStore store = rmContext.getStateStore();\n         try {\n-          store.close();\n+          if (null != store) {\n+            store.close();\n+          }\n         } catch (Exception e) {\n           LOG.error(\"Error closing store.\", e);\n         }",
                "raw_url": "https://github.com/apache/hadoop/raw/c9cb6a5960ad335a3ee93a6ee219eae5aad372f9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "sha": "d1f339a1a848082dd2dcef8b13c34a5e2da2620a",
                "status": "modified"
            }
        ],
        "message": "YARN-4167. NPE on RMActiveServices#serviceStop when store is null. (Bibin A Chundatt via rohithsharmaks)",
        "parent": "https://github.com/apache/hadoop/commit/3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501",
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceManager.java"
        ]
    },
    "hadoop_c9dd2ca": {
        "bug_id": "hadoop_c9dd2ca",
        "commit": "https://github.com/apache/hadoop/commit/c9dd2cada055c0beffd04bad0ded8324f66ad1b7",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c9dd2cada055c0beffd04bad0ded8324f66ad1b7/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=c9dd2cada055c0beffd04bad0ded8324f66ad1b7",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -598,6 +598,9 @@ Release 2.8.0 - UNRELEASED\n     YARN-2194. Fix bug causing CGroups functionality to fail on RHEL7.\n     (Wei Yan via vvasudev)\n \n+    YARN-3892. Fixed NPE on RMStateStore#serviceStop when\n+    CapacityScheduler#serviceInit fails. (Bibin A Chundatt via jianhe)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/c9dd2cada055c0beffd04bad0ded8324f66ad1b7/hadoop-yarn-project/CHANGES.txt",
                "sha": "8b5cc0cd9ba6f7ace5feb0c8dfd27eb6cfc2bff1",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c9dd2cada055c0beffd04bad0ded8324f66ad1b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java?ref=c9dd2cada055c0beffd04bad0ded8324f66ad1b7",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "patch": "@@ -40,6 +40,7 @@\n import org.apache.hadoop.classification.InterfaceAudience.Private;\n import org.apache.hadoop.classification.InterfaceStability.Unstable;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.security.token.delegation.DelegationKey;\n import org.apache.hadoop.util.ZKUtil;\n import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n@@ -312,7 +313,7 @@ protected synchronized void closeInternal() throws Exception {\n       verifyActiveStatusThread.interrupt();\n       verifyActiveStatusThread.join(1000);\n     }\n-    curatorFramework.close();\n+    IOUtils.closeStream(curatorFramework);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/c9dd2cada055c0beffd04bad0ded8324f66ad1b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "sha": "8f096d882aaf14adf29a5235f481904bbade6376",
                "status": "modified"
            }
        ],
        "message": "YARN-3892. Fixed NPE on RMStateStore#serviceStop when CapacityScheduler#serviceInit fails. Contributed by Bibin A Chundatt",
        "parent": "https://github.com/apache/hadoop/commit/c0b8e4e5b5083631ed22d8d36c8992df7d34303c",
        "repo": "hadoop",
        "unit_tests": [
            "TestZKRMStateStore.java"
        ]
    },
    "hadoop_d6c8bad": {
        "bug_id": "hadoop_d6c8bad",
        "commit": "https://github.com/apache/hadoop/commit/d6c8bad86964dbad3cc810914f786c7c4722227a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/d6c8bad86964dbad3cc810914f786c7c4722227a/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=d6c8bad86964dbad3cc810914f786c7c4722227a",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -940,6 +940,8 @@ Release 2.8.0 - UNRELEASED\n     YARN-4255. container-executor does not clean up docker operation command files.\n     (Sidharta Seethana via vvasudev)\n \n+    YARN-4250. NPE in AppSchedulingInfo#isRequestLabelChanged. (Brahma Reddy Battula via rohithsharmaks)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/d6c8bad86964dbad3cc810914f786c7c4722227a/hadoop-yarn-project/CHANGES.txt",
                "sha": "de5fbe3c53b88d10931a9243a1aeb78a7c7c7b25",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/d6c8bad86964dbad3cc810914f786c7c4722227a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java?ref=d6c8bad86964dbad3cc810914f786c7c4722227a",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java",
                "patch": "@@ -417,7 +417,15 @@ private boolean isRequestLabelChanged(ResourceRequest requestOne,\n       ResourceRequest requestTwo) {\n     String requestOneLabelExp = requestOne.getNodeLabelExpression();\n     String requestTwoLabelExp = requestTwo.getNodeLabelExpression();\n-    return (!(requestOneLabelExp.equals(requestTwoLabelExp)));\n+    // First request label expression can be null and second request\n+    // is not null then we have to consider it as changed.\n+    if ((null == requestOneLabelExp) && (null != requestTwoLabelExp)) {\n+      return true;\n+    }\n+    // If the label is not matching between both request when\n+    // requestOneLabelExp is not null.\n+    return ((null != requestOneLabelExp) && !(requestOneLabelExp\n+        .equals(requestTwoLabelExp)));\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/d6c8bad86964dbad3cc810914f786c7c4722227a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java",
                "sha": "31560867a6266c0716b86e30649bedd391d6f261",
                "status": "modified"
            }
        ],
        "message": "YARN-4250. NPE in AppSchedulingInfo#isRequestLabelChanged. (Brahma Reddy Battula via rohithsharmaks)",
        "parent": "https://github.com/apache/hadoop/commit/da1ee078f9d3c2c25c51d0b392b0925821c42ad3",
        "repo": "hadoop",
        "unit_tests": [
            "TestAppSchedulingInfo.java"
        ]
    },
    "hadoop_d9852eb": {
        "bug_id": "hadoop_d9852eb",
        "commit": "https://github.com/apache/hadoop/commit/d9852eb5897a25323ab0302c2c0decb61d310e5e",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/d9852eb5897a25323ab0302c2c0decb61d310e5e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java?ref=d9852eb5897a25323ab0302c2c0decb61d310e5e",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "patch": "@@ -1198,6 +1198,7 @@ public Service getStatus(String serviceName)\n     ServiceApiUtil.validateNameFormat(serviceName, getConfig());\n     Service appSpec = new Service();\n     appSpec.setName(serviceName);\n+    appSpec.setState(ServiceState.STOPPED);\n     ApplicationId currentAppId = getAppId(serviceName);\n     if (currentAppId == null) {\n       LOG.info(\"Service {} does not have an application ID\", serviceName);",
                "raw_url": "https://github.com/apache/hadoop/raw/d9852eb5897a25323ab0302c2c0decb61d310e5e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "sha": "0ab332280f2447a16347dfd1b81b2ba2fa399140",
                "status": "modified"
            }
        ],
        "message": "YARN-8357.  Fixed NPE when YARN service is saved and not deployed.\n            Contributed by Chandni Singh",
        "parent": "https://github.com/apache/hadoop/commit/7ff5a40218241ad2380595175a493794129a7402",
        "repo": "hadoop",
        "unit_tests": [
            "ServiceClientTest.java",
            "TestServiceClient.java"
        ]
    },
    "hadoop_db334bb": {
        "bug_id": "hadoop_db334bb",
        "commit": "https://github.com/apache/hadoop/commit/db334bb8625da97c7e518cbcf477530c7ba7001e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/db334bb8625da97c7e518cbcf477530c7ba7001e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=db334bb8625da97c7e518cbcf477530c7ba7001e",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -783,6 +783,9 @@ Release 2.6.1 - UNRELEASED\n     HDFS-7443. Datanode upgrade to BLOCKID_BASED_LAYOUT fails if duplicate\n     block files are present in the same volume (cmccabe)\n \n+    HDFS-3443. Fix NPE when namenode transition to active during startup by\n+    adding checkNNStartup() in NameNodeRpcServer.  (Vinayakumar B via szetszwo)\n+\n Release 2.6.0 - 2014-11-18\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/db334bb8625da97c7e518cbcf477530c7ba7001e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "0a301f8373328bff0268b07ae38e79dfb3eaeb92",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/db334bb8625da97c7e518cbcf477530c7ba7001e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=db334bb8625da97c7e518cbcf477530c7ba7001e",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -79,6 +79,7 @@\n import java.util.Arrays;\n import java.util.Collection;\n import java.util.List;\n+import java.util.concurrent.atomic.AtomicBoolean;\n \n import static org.apache.hadoop.fs.CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY;\n import static org.apache.hadoop.fs.CommonConfigurationKeysPublic.FS_TRASH_INTERVAL_DEFAULT;\n@@ -265,6 +266,7 @@ public long getProtocolVersion(String protocol,\n   private final boolean haEnabled;\n   private final HAContext haContext;\n   protected final boolean allowStaleStandbyReads;\n+  private AtomicBoolean started = new AtomicBoolean(false); \n \n   \n   /** httpServer */\n@@ -775,6 +777,7 @@ protected NameNode(Configuration conf, NamenodeRole role)\n       this.stop();\n       throw e;\n     }\n+    this.started.set(true);\n   }\n \n   protected HAState createHAState(StartupOption startOpt) {\n@@ -1743,7 +1746,14 @@ public boolean isStandbyState() {\n   public boolean isActiveState() {\n     return (state.equals(ACTIVE_STATE));\n   }\n-  \n+\n+  /**\n+   * Returns whether the NameNode is completely started\n+   */\n+  boolean isStarted() {\n+    return this.started.get();\n+  }\n+\n   /**\n    * Check that a request to change this node's HA state is valid.\n    * In particular, verifies that, if auto failover is enabled, non-forced",
                "raw_url": "https://github.com/apache/hadoop/raw/db334bb8625da97c7e518cbcf477530c7ba7001e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "fea7c62be4c2d4888ea3b63b53056c9814944557",
                "status": "modified"
            },
            {
                "additions": 151,
                "blob_url": "https://github.com/apache/hadoop/blob/db334bb8625da97c7e518cbcf477530c7ba7001e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "changes": 187,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java?ref=db334bb8625da97c7e518cbcf477530c7ba7001e",
                "deletions": 36,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "patch": "@@ -69,7 +69,6 @@\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.DFSUtil;\n import org.apache.hadoop.hdfs.HDFSPolicyProvider;\n-import org.apache.hadoop.hdfs.inotify.Event;\n import org.apache.hadoop.hdfs.inotify.EventBatch;\n import org.apache.hadoop.hdfs.inotify.EventBatchList;\n import org.apache.hadoop.hdfs.protocol.AclException;\n@@ -479,12 +478,14 @@ public BlocksWithLocations getBlocks(DatanodeInfo datanode, long size)\n       throw new IllegalArgumentException(\n         \"Unexpected not positive size: \"+size);\n     }\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.getBlockManager().getBlocks(datanode, size); \n   }\n \n   @Override // NamenodeProtocol\n   public ExportedBlockKeys getBlockKeys() throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.getBlockManager().getBlockKeys();\n   }\n@@ -493,6 +494,7 @@ public ExportedBlockKeys getBlockKeys() throws IOException {\n   public void errorReport(NamenodeRegistration registration,\n                           int errorCode, \n                           String msg) throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.UNCHECKED);\n     namesystem.checkSuperuserPrivilege();\n     verifyRequest(registration);\n@@ -505,6 +507,7 @@ public void errorReport(NamenodeRegistration registration,\n   @Override // NamenodeProtocol\n   public NamenodeRegistration registerSubordinateNamenode(\n       NamenodeRegistration registration) throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     verifyLayoutVersion(registration.getVersion());\n     NamenodeRegistration myRegistration = nn.setRegistration();\n@@ -514,7 +517,8 @@ public NamenodeRegistration registerSubordinateNamenode(\n \n   @Override // NamenodeProtocol\n   public NamenodeCommand startCheckpoint(NamenodeRegistration registration)\n-  throws IOException {\n+      throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     verifyRequest(registration);\n     if(!nn.isRole(NamenodeRole.NAMENODE))\n@@ -537,6 +541,7 @@ public NamenodeCommand startCheckpoint(NamenodeRegistration registration)\n   @Override // NamenodeProtocol\n   public void endCheckpoint(NamenodeRegistration registration,\n                             CheckpointSignature sig) throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n@@ -554,18 +559,21 @@ public void endCheckpoint(NamenodeRegistration registration,\n   @Override // ClientProtocol\n   public Token<DelegationTokenIdentifier> getDelegationToken(Text renewer)\n       throws IOException {\n+    checkNNStartup();\n     return namesystem.getDelegationToken(renewer);\n   }\n \n   @Override // ClientProtocol\n   public long renewDelegationToken(Token<DelegationTokenIdentifier> token)\n       throws InvalidToken, IOException {\n+    checkNNStartup();\n     return namesystem.renewDelegationToken(token);\n   }\n \n   @Override // ClientProtocol\n   public void cancelDelegationToken(Token<DelegationTokenIdentifier> token)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.cancelDelegationToken(token);\n   }\n   \n@@ -574,13 +582,15 @@ public LocatedBlocks getBlockLocations(String src,\n                                           long offset, \n                                           long length) \n       throws IOException {\n+    checkNNStartup();\n     metrics.incrGetBlockLocations();\n     return namesystem.getBlockLocations(getClientMachine(), \n                                         src, offset, length);\n   }\n   \n   @Override // ClientProtocol\n   public FsServerDefaults getServerDefaults() throws IOException {\n+    checkNNStartup();\n     return namesystem.getServerDefaults();\n   }\n \n@@ -590,6 +600,7 @@ public HdfsFileStatus create(String src, FsPermission masked,\n       boolean createParent, short replication, long blockSize, \n       CryptoProtocolVersion[] supportedVersions)\n       throws IOException {\n+    checkNNStartup();\n     String clientMachine = getClientMachine();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n@@ -624,6 +635,7 @@ public HdfsFileStatus create(String src, FsPermission masked,\n   @Override // ClientProtocol\n   public LastBlockWithStatus append(String src, String clientName) \n       throws IOException {\n+    checkNNStartup();\n     String clientMachine = getClientMachine();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.append: file \"\n@@ -649,36 +661,42 @@ public LastBlockWithStatus append(String src, String clientName)\n \n   @Override // ClientProtocol\n   public boolean recoverLease(String src, String clientName) throws IOException {\n+    checkNNStartup();\n     String clientMachine = getClientMachine();\n     return namesystem.recoverLease(src, clientName, clientMachine);\n   }\n \n   @Override // ClientProtocol\n   public boolean setReplication(String src, short replication) \n-    throws IOException {  \n+    throws IOException {\n+    checkNNStartup();\n     return namesystem.setReplication(src, replication);\n   }\n \n   @Override\n   public void setStoragePolicy(String src, String policyName)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.setStoragePolicy(src, policyName);\n   }\n \n   @Override\n   public BlockStoragePolicy[] getStoragePolicies() throws IOException {\n+    checkNNStartup();\n     return namesystem.getStoragePolicies();\n   }\n \n   @Override // ClientProtocol\n   public void setPermission(String src, FsPermission permissions)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.setPermission(src, permissions);\n   }\n \n   @Override // ClientProtocol\n   public void setOwner(String src, String username, String groupname)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.setOwner(src, username, groupname);\n   }\n   \n@@ -687,6 +705,7 @@ public LocatedBlock addBlock(String src, String clientName,\n       ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n       String[] favoredNodes)\n       throws IOException {\n+    checkNNStartup();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \" + src\n           + \" fileId=\" + fileId + \" for \" + clientName);\n@@ -714,6 +733,7 @@ public LocatedBlock getAdditionalDatanode(final String src,\n       final DatanodeInfo[] excludes,\n       final int numAdditionalNodes, final String clientName\n       ) throws IOException {\n+    checkNNStartup();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getAdditionalDatanode: src=\" + src\n           + \", fileId=\" + fileId\n@@ -742,6 +762,7 @@ public LocatedBlock getAdditionalDatanode(final String src,\n   @Override // ClientProtocol\n   public void abandonBlock(ExtendedBlock b, long fileId, String src,\n         String holder) throws IOException {\n+    checkNNStartup();\n     if(stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*BLOCK* NameNode.abandonBlock: \"\n           +b+\" of file \"+src);\n@@ -755,6 +776,7 @@ public void abandonBlock(ExtendedBlock b, long fileId, String src,\n   public boolean complete(String src, String clientName,\n                           ExtendedBlock last,  long fileId)\n       throws IOException {\n+    checkNNStartup();\n     if(stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.complete: \"\n           + src + \" fileId=\" + fileId +\" for \" + clientName);\n@@ -770,12 +792,14 @@ public boolean complete(String src, String clientName,\n    */\n   @Override // ClientProtocol, DatanodeProtocol\n   public void reportBadBlocks(LocatedBlock[] blocks) throws IOException {\n+    checkNNStartup();\n     namesystem.reportBadBlocks(blocks);\n   }\n \n   @Override // ClientProtocol\n   public LocatedBlock updateBlockForPipeline(ExtendedBlock block, String clientName)\n       throws IOException {\n+    checkNNStartup();\n     return namesystem.updateBlockForPipeline(block, clientName);\n   }\n \n@@ -784,6 +808,7 @@ public LocatedBlock updateBlockForPipeline(ExtendedBlock block, String clientNam\n   public void updatePipeline(String clientName, ExtendedBlock oldBlock,\n       ExtendedBlock newBlock, DatanodeID[] newNodes, String[] newStorageIDs)\n       throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -805,19 +830,22 @@ public void commitBlockSynchronization(ExtendedBlock block,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.commitBlockSynchronization(block, newgenerationstamp,\n         newlength, closeFile, deleteblock, newtargets, newtargetstorages);\n   }\n   \n   @Override // ClientProtocol\n   public long getPreferredBlockSize(String filename) \n       throws IOException {\n+    checkNNStartup();\n     return namesystem.getPreferredBlockSize(filename);\n   }\n     \n   @Deprecated\n   @Override // ClientProtocol\n   public boolean rename(String src, String dst) throws IOException {\n+    checkNNStartup();\n     if(stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.rename: \" + src + \" to \" + dst);\n     }\n@@ -845,6 +873,7 @@ public boolean rename(String src, String dst) throws IOException {\n   \n   @Override // ClientProtocol\n   public void concat(String trg, String[] src) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -862,6 +891,7 @@ public void concat(String trg, String[] src) throws IOException {\n   @Override // ClientProtocol\n   public void rename2(String src, String dst, Options.Rename... options)\n       throws IOException {\n+    checkNNStartup();\n     if(stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.rename: \" + src + \" to \" + dst);\n     }\n@@ -886,6 +916,7 @@ public void rename2(String src, String dst, Options.Rename... options)\n   @Override // ClientProtocol\n   public boolean truncate(String src, long newLength, String clientName)\n       throws IOException {\n+    checkNNStartup();\n     if(stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.truncate: \" + src + \" to \" +\n           newLength);\n@@ -901,6 +932,7 @@ public boolean truncate(String src, long newLength, String clientName)\n \n   @Override // ClientProtocol\n   public boolean delete(String src, boolean recursive) throws IOException {\n+    checkNNStartup();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* Namenode.delete: src=\" + src\n           + \", recursive=\" + recursive);\n@@ -935,6 +967,7 @@ private boolean checkPathLength(String src) {\n   @Override // ClientProtocol\n   public boolean mkdirs(String src, FsPermission masked, boolean createParent)\n       throws IOException {\n+    checkNNStartup();\n     if(stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.mkdirs: \" + src);\n     }\n@@ -949,12 +982,14 @@ public boolean mkdirs(String src, FsPermission masked, boolean createParent)\n \n   @Override // ClientProtocol\n   public void renewLease(String clientName) throws IOException {\n+    checkNNStartup();\n     namesystem.renewLease(clientName);        \n   }\n \n   @Override // ClientProtocol\n   public DirectoryListing getListing(String src, byte[] startAfter,\n       boolean needLocation) throws IOException {\n+    checkNNStartup();\n     DirectoryListing files = namesystem.getListing(\n         src, startAfter, needLocation);\n     if (files != null) {\n@@ -966,44 +1001,51 @@ public DirectoryListing getListing(String src, byte[] startAfter,\n \n   @Override // ClientProtocol\n   public HdfsFileStatus getFileInfo(String src)  throws IOException {\n+    checkNNStartup();\n     metrics.incrFileInfoOps();\n     return namesystem.getFileInfo(src, true);\n   }\n   \n   @Override // ClientProtocol\n   public boolean isFileClosed(String src) throws IOException{\n+    checkNNStartup();\n     return namesystem.isFileClosed(src);\n   }\n   \n   @Override // ClientProtocol\n-  public HdfsFileStatus getFileLinkInfo(String src) throws IOException { \n+  public HdfsFileStatus getFileLinkInfo(String src) throws IOException {\n+    checkNNStartup();\n     metrics.incrFileInfoOps();\n     return namesystem.getFileInfo(src, false);\n   }\n   \n   @Override // ClientProtocol\n   public long[] getStats() throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.READ);\n     return namesystem.getStats();\n   }\n \n   @Override // ClientProtocol\n   public DatanodeInfo[] getDatanodeReport(DatanodeReportType type)\n   throws IOException {\n+    checkNNStartup();\n     DatanodeInfo results[] = namesystem.datanodeReport(type);\n     return results;\n   }\n     \n   @Override // ClientProtocol\n   public DatanodeStorageReport[] getDatanodeStorageReport(\n       DatanodeReportType type) throws IOException {\n+    checkNNStartup();\n     final DatanodeStorageReport[] reports = namesystem.getDatanodeStorageReport(type);\n     return reports;\n   }\n \n   @Override // ClientProtocol\n   public boolean setSafeMode(SafeModeAction action, boolean isChecked)\n       throws IOException {\n+    checkNNStartup();\n     OperationCategory opCategory = OperationCategory.UNCHECKED;\n     if (isChecked) {\n       if (action == SafeModeAction.SAFEMODE_GET) {\n@@ -1018,11 +1060,13 @@ public boolean setSafeMode(SafeModeAction action, boolean isChecked)\n \n   @Override // ClientProtocol\n   public boolean restoreFailedStorage(String arg) throws IOException { \n+    checkNNStartup();\n     return namesystem.restoreFailedStorage(arg);\n   }\n \n   @Override // ClientProtocol\n   public void saveNamespace() throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -1038,50 +1082,58 @@ public void saveNamespace() throws IOException {\n   \n   @Override // ClientProtocol\n   public long rollEdits() throws AccessControlException, IOException {\n+    checkNNStartup();\n     CheckpointSignature sig = namesystem.rollEditLog();\n     return sig.getCurSegmentTxId();\n   }\n \n   @Override // ClientProtocol\n   public void refreshNodes() throws IOException {\n+    checkNNStartup();\n     namesystem.refreshNodes();\n   }\n \n   @Override // NamenodeProtocol\n   public long getTransactionID() throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.UNCHECKED);\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.getFSImage().getLastAppliedOrWrittenTxId();\n   }\n   \n   @Override // NamenodeProtocol\n   public long getMostRecentCheckpointTxId() throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.UNCHECKED);\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.getFSImage().getMostRecentCheckpointTxId();\n   }\n   \n   @Override // NamenodeProtocol\n   public CheckpointSignature rollEditLog() throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.rollEditLog();\n   }\n   \n   @Override // NamenodeProtocol\n   public RemoteEditLogManifest getEditLogManifest(long sinceTxId)\n-  throws IOException {\n+      throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.READ);\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.getEditLog().getEditLogManifest(sinceTxId);\n   }\n     \n   @Override // ClientProtocol\n   public void finalizeUpgrade() throws IOException {\n+    checkNNStartup();\n     namesystem.finalizeUpgrade();\n   }\n \n   @Override // ClientProtocol\n   public RollingUpgradeInfo rollingUpgrade(RollingUpgradeAction action) throws IOException {\n+    checkNNStartup();\n     LOG.info(\"rollingUpgrade \" + action);\n     switch(action) {\n     case QUERY:\n@@ -1098,12 +1150,14 @@ public RollingUpgradeInfo rollingUpgrade(RollingUpgradeAction action) throws IOE\n \n   @Override // ClientProtocol\n   public void metaSave(String filename) throws IOException {\n+    checkNNStartup();\n     namesystem.metaSave(filename);\n   }\n \n   @Override // ClientProtocol\n   public CorruptFileBlocks listCorruptFileBlocks(String path, String cookie)\n       throws IOException {\n+    checkNNStartup();\n     String[] cookieTab = new String[] { cookie };\n     Collection<FSNamesystem.CorruptFileBlockInfo> fbs =\n       namesystem.listCorruptFileBlocks(path, cookieTab);\n@@ -1124,36 +1178,42 @@ public CorruptFileBlocks listCorruptFileBlocks(String path, String cookie)\n    */\n   @Override // ClientProtocol\n   public void setBalancerBandwidth(long bandwidth) throws IOException {\n+    checkNNStartup();\n     namesystem.setBalancerBandwidth(bandwidth);\n   }\n   \n   @Override // ClientProtocol\n   public ContentSummary getContentSummary(String path) throws IOException {\n+    checkNNStartup();\n     return namesystem.getContentSummary(path);\n   }\n \n   @Override // ClientProtocol\n   public void setQuota(String path, long namespaceQuota, long diskspaceQuota) \n       throws IOException {\n+    checkNNStartup();\n     namesystem.setQuota(path, namespaceQuota, diskspaceQuota);\n   }\n   \n   @Override // ClientProtocol\n   public void fsync(String src, long fileId, String clientName,\n                     long lastBlockLength)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.fsync(src, fileId, clientName, lastBlockLength);\n   }\n \n   @Override // ClientProtocol\n   public void setTimes(String src, long mtime, long atime) \n       throws IOException {\n+    checkNNStartup();\n     namesystem.setTimes(src, mtime, atime);\n   }\n \n   @Override // ClientProtocol\n   public void createSymlink(String target, String link, FsPermission dirPerms,\n       boolean createParent) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -1184,6 +1244,7 @@ public void createSymlink(String target, String link, FsPermission dirPerms,\n \n   @Override // ClientProtocol\n   public String getLinkTarget(String path) throws IOException {\n+    checkNNStartup();\n     metrics.incrGetLinkTargetOps();\n     HdfsFileStatus stat = null;\n     try {\n@@ -1206,6 +1267,7 @@ public String getLinkTarget(String path) throws IOException {\n   @Override // DatanodeProtocol\n   public DatanodeRegistration registerDatanode(DatanodeRegistration nodeReg)\n       throws IOException {\n+    checkNNStartup();\n     verifySoftwareVersion(nodeReg);\n     namesystem.registerDatanode(nodeReg);\n     return nodeReg;\n@@ -1216,6 +1278,7 @@ public HeartbeatResponse sendHeartbeat(DatanodeRegistration nodeReg,\n       StorageReport[] report, long dnCacheCapacity, long dnCacheUsed,\n       int xmitsInProgress, int xceiverCount,\n       int failedVolumes) throws IOException {\n+    checkNNStartup();\n     verifyRequest(nodeReg);\n     return namesystem.handleHeartbeat(nodeReg, report,\n         dnCacheCapacity, dnCacheUsed, xceiverCount, xmitsInProgress,\n@@ -1225,6 +1288,7 @@ public HeartbeatResponse sendHeartbeat(DatanodeRegistration nodeReg,\n   @Override // DatanodeProtocol\n   public DatanodeCommand blockReport(DatanodeRegistration nodeReg,\n       String poolId, StorageBlockReport[] reports) throws IOException {\n+    checkNNStartup();\n     verifyRequest(nodeReg);\n     if(blockStateChangeLog.isDebugEnabled()) {\n       blockStateChangeLog.debug(\"*BLOCK* NameNode.blockReport: \"\n@@ -1256,6 +1320,7 @@ public DatanodeCommand blockReport(DatanodeRegistration nodeReg,\n   @Override\n   public DatanodeCommand cacheReport(DatanodeRegistration nodeReg,\n       String poolId, List<Long> blockIds) throws IOException {\n+    checkNNStartup();\n     verifyRequest(nodeReg);\n     if (blockStateChangeLog.isDebugEnabled()) {\n       blockStateChangeLog.debug(\"*BLOCK* NameNode.cacheReport: \"\n@@ -1268,6 +1333,7 @@ public DatanodeCommand cacheReport(DatanodeRegistration nodeReg,\n   @Override // DatanodeProtocol\n   public void blockReceivedAndDeleted(DatanodeRegistration nodeReg, String poolId,\n       StorageReceivedDeletedBlocks[] receivedAndDeletedBlocks) throws IOException {\n+    checkNNStartup();\n     verifyRequest(nodeReg);\n     metrics.incrBlockReceivedAndDeletedOps();\n     if(blockStateChangeLog.isDebugEnabled()) {\n@@ -1283,6 +1349,7 @@ public void blockReceivedAndDeleted(DatanodeRegistration nodeReg, String poolId,\n   @Override // DatanodeProtocol\n   public void errorReport(DatanodeRegistration nodeReg,\n                           int errorCode, String msg) throws IOException { \n+    checkNNStartup();\n     String dnName = \n        (nodeReg == null) ? \"Unknown DataNode\" : nodeReg.toString();\n \n@@ -1304,6 +1371,7 @@ public void errorReport(DatanodeRegistration nodeReg,\n     \n   @Override // DatanodeProtocol, NamenodeProtocol\n   public NamespaceInfo versionRequest() throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.getNamespaceInfo();\n   }\n@@ -1328,6 +1396,7 @@ private void verifyRequest(NodeRegistration nodeReg) throws IOException {\n \n   @Override // RefreshAuthorizationPolicyProtocol\n   public void refreshServiceAcl() throws IOException {\n+    checkNNStartup();\n     if (!serviceAuthEnabled) {\n       throw new AuthorizationException(\"Service Level Authorization not enabled!\");\n     }\n@@ -1378,28 +1447,32 @@ public void refreshCallQueue() {\n   }\n \n   @Override // HAServiceProtocol\n-  public synchronized void monitorHealth() \n-      throws HealthCheckFailedException, AccessControlException {\n+  public synchronized void monitorHealth() throws HealthCheckFailedException,\n+      AccessControlException, IOException {\n+    checkNNStartup();\n     nn.monitorHealth();\n   }\n   \n   @Override // HAServiceProtocol\n   public synchronized void transitionToActive(StateChangeRequestInfo req) \n-      throws ServiceFailedException, AccessControlException {\n+      throws ServiceFailedException, AccessControlException, IOException {\n+    checkNNStartup();\n     nn.checkHaStateChange(req);\n     nn.transitionToActive();\n   }\n   \n   @Override // HAServiceProtocol\n   public synchronized void transitionToStandby(StateChangeRequestInfo req) \n-      throws ServiceFailedException, AccessControlException {\n+      throws ServiceFailedException, AccessControlException, IOException {\n+    checkNNStartup();\n     nn.checkHaStateChange(req);\n     nn.transitionToStandby();\n   }\n \n   @Override // HAServiceProtocol\n   public synchronized HAServiceStatus getServiceStatus() \n-      throws AccessControlException, ServiceFailedException {\n+      throws AccessControlException, ServiceFailedException, IOException {\n+    checkNNStartup();\n     return nn.getServiceStatus();\n   }\n \n@@ -1456,12 +1529,14 @@ private static String getClientMachine() {\n \n   @Override\n   public DataEncryptionKey getDataEncryptionKey() throws IOException {\n+    checkNNStartup();\n     return namesystem.getBlockManager().generateDataEncryptionKey();\n   }\n \n   @Override\n   public String createSnapshot(String snapshotRoot, String snapshotName)\n       throws IOException {\n+    checkNNStartup();\n     if (!checkPathLength(snapshotRoot)) {\n       throw new IOException(\"createSnapshot: Pathname too long.  Limit \"\n           + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n@@ -1486,6 +1561,7 @@ public String createSnapshot(String snapshotRoot, String snapshotName)\n   @Override\n   public void deleteSnapshot(String snapshotRoot, String snapshotName)\n       throws IOException {\n+    checkNNStartup();\n     metrics.incrDeleteSnapshotOps();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n@@ -1503,20 +1579,24 @@ public void deleteSnapshot(String snapshotRoot, String snapshotName)\n   @Override\n   // Client Protocol\n   public void allowSnapshot(String snapshotRoot) throws IOException {\n+    checkNNStartup();\n     metrics.incrAllowSnapshotOps();\n     namesystem.allowSnapshot(snapshotRoot);\n   }\n \n   @Override\n   // Client Protocol\n   public void disallowSnapshot(String snapshot) throws IOException {\n+    checkNNStartup();\n     metrics.incrDisAllowSnapshotOps();\n     namesystem.disallowSnapshot(snapshot);\n   }\n \n   @Override\n+  // ClientProtocol\n   public void renameSnapshot(String snapshotRoot, String snapshotOldName,\n       String snapshotNewName) throws IOException {\n+    checkNNStartup();\n     if (snapshotNewName == null || snapshotNewName.isEmpty()) {\n       throw new IOException(\"The new snapshot name is null or empty.\");\n     }\n@@ -1538,24 +1618,27 @@ public void renameSnapshot(String snapshotRoot, String snapshotOldName,\n   @Override // Client Protocol\n   public SnapshottableDirectoryStatus[] getSnapshottableDirListing()\n       throws IOException {\n+    checkNNStartup();\n     SnapshottableDirectoryStatus[] status = namesystem\n         .getSnapshottableDirListing();\n     metrics.incrListSnapshottableDirOps();\n     return status;\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public SnapshotDiffReport getSnapshotDiffReport(String snapshotRoot,\n       String earlierSnapshotName, String laterSnapshotName) throws IOException {\n+    checkNNStartup();\n     SnapshotDiffReport report = namesystem.getSnapshotDiffReport(snapshotRoot,\n         earlierSnapshotName, laterSnapshotName);\n     metrics.incrSnapshotDiffReportOps();\n     return report;\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public long addCacheDirective(\n       CacheDirectiveInfo path, EnumSet<CacheFlag> flags) throws IOException {\n+    checkNNStartup();\n     CacheEntryWithPayload cacheEntry = RetryCache.waitForCompletion\n       (retryCache, null);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n@@ -1573,9 +1656,10 @@ public long addCacheDirective(\n     return ret;\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void modifyCacheDirective(\n       CacheDirectiveInfo directive, EnumSet<CacheFlag> flags) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return;\n@@ -1590,8 +1674,9 @@ public void modifyCacheDirective(\n     }\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void removeCacheDirective(long id) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return;\n@@ -1605,17 +1690,19 @@ public void removeCacheDirective(long id) throws IOException {\n     }\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public BatchedEntries<CacheDirectiveEntry> listCacheDirectives(long prevId,\n       CacheDirectiveInfo filter) throws IOException {\n+    checkNNStartup();\n     if (filter == null) {\n       filter = new CacheDirectiveInfo.Builder().build();\n     }\n     return namesystem.listCacheDirectives(prevId, filter);\n   }\n \n-  @Override\n+  @Override //ClientProtocol\n   public void addCachePool(CachePoolInfo info) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -1629,8 +1716,9 @@ public void addCachePool(CachePoolInfo info) throws IOException {\n     }\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void modifyCachePool(CachePoolInfo info) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -1644,8 +1732,9 @@ public void modifyCachePool(CachePoolInfo info) throws IOException {\n     }\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void removeCachePool(String cachePoolName) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return;\n@@ -1659,47 +1748,55 @@ public void removeCachePool(String cachePoolName) throws IOException {\n     }\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public BatchedEntries<CachePoolEntry> listCachePools(String prevKey)\n       throws IOException {\n+    checkNNStartup();\n     return namesystem.listCachePools(prevKey != null ? prevKey : \"\");\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void modifyAclEntries(String src, List<AclEntry> aclSpec)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.modifyAclEntries(src, aclSpec);\n   }\n \n-  @Override\n+  @Override // ClienProtocol\n   public void removeAclEntries(String src, List<AclEntry> aclSpec)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.removeAclEntries(src, aclSpec);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void removeDefaultAcl(String src) throws IOException {\n+    checkNNStartup();\n     namesystem.removeDefaultAcl(src);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void removeAcl(String src) throws IOException {\n+    checkNNStartup();\n     namesystem.removeAcl(src);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void setAcl(String src, List<AclEntry> aclSpec) throws IOException {\n+    checkNNStartup();\n     namesystem.setAcl(src, aclSpec);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public AclStatus getAclStatus(String src) throws IOException {\n+    checkNNStartup();\n     return namesystem.getAclStatus(src);\n   }\n   \n-  @Override\n+  @Override // ClientProtocol\n   public void createEncryptionZone(String src, String keyName)\n     throws IOException {\n+    checkNNStartup();\n     final CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return;\n@@ -1713,21 +1810,24 @@ public void createEncryptionZone(String src, String keyName)\n     }\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public EncryptionZone getEZForPath(String src)\n     throws IOException {\n+    checkNNStartup();\n     return namesystem.getEZForPath(src);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public BatchedEntries<EncryptionZone> listEncryptionZones(\n       long prevId) throws IOException {\n+    checkNNStartup();\n     return namesystem.listEncryptionZones(prevId);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void setXAttr(String src, XAttr xAttr, EnumSet<XAttrSetFlag> flag)\n       throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -1741,19 +1841,22 @@ public void setXAttr(String src, XAttr xAttr, EnumSet<XAttrSetFlag> flag)\n     }\n   }\n   \n-  @Override\n+  @Override // ClientProtocol\n   public List<XAttr> getXAttrs(String src, List<XAttr> xAttrs) \n       throws IOException {\n+    checkNNStartup();\n     return namesystem.getXAttrs(src, xAttrs);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public List<XAttr> listXAttrs(String src) throws IOException {\n+    checkNNStartup();\n     return namesystem.listXAttrs(src);\n   }\n   \n-  @Override\n+  @Override // ClientProtocol\n   public void removeXAttr(String src, XAttr xAttr) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -1767,13 +1870,21 @@ public void removeXAttr(String src, XAttr xAttr) throws IOException {\n     }\n   }\n \n-  @Override\n+  private void checkNNStartup() throws IOException {\n+    if (!this.nn.isStarted()) {\n+      throw new IOException(this.nn.getRole() + \" still not started\");\n+    }\n+  }\n+\n+  @Override // ClientProtocol\n   public void checkAccess(String path, FsAction mode) throws IOException {\n+    checkNNStartup();\n     namesystem.checkAccess(path, mode);\n   }\n \n   @Override // ClientProtocol\n   public long getCurrentEditLogTxid() throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.READ); // only active\n     namesystem.checkSuperuserPrivilege();\n     // if it's not yet open for write, we may be in the process of transitioning\n@@ -1802,6 +1913,7 @@ private static FSEditLogOp readOp(EditLogInputStream elis)\n \n   @Override // ClientProtocol\n   public EventBatchList getEditsFromTxid(long txid) throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.READ); // only active\n     namesystem.checkSuperuserPrivilege();\n     int maxEventsPerRPC = nn.conf.getInt(\n@@ -1885,20 +1997,23 @@ public EventBatchList getEditsFromTxid(long txid) throws IOException {\n     return new EventBatchList(batches, firstSeenTxid, maxSeenTxid, syncTxid);\n   }\n \n-  @Override\n+  @Override // TraceAdminProtocol\n   public SpanReceiverInfo[] listSpanReceivers() throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     return nn.spanReceiverHost.listSpanReceivers();\n   }\n \n-  @Override\n+  @Override // TraceAdminProtocol\n   public long addSpanReceiver(SpanReceiverInfo info) throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     return nn.spanReceiverHost.addSpanReceiver(info);\n   }\n \n-  @Override\n+  @Override // TraceAdminProtocol\n   public void removeSpanReceiver(long id) throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     nn.spanReceiverHost.removeSpanReceiver(id);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/db334bb8625da97c7e518cbcf477530c7ba7001e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "sha": "a3ac455ee3ad5349298f4798020baeb3885df38f",
                "status": "modified"
            }
        ],
        "message": "HDFS-3443. Fix NPE when namenode transition to active during startup by adding checkNNStartup() in NameNodeRpcServer.  Contributed by Vinayakumar B",
        "parent": "https://github.com/apache/hadoop/commit/39c1bcf7d9c9331d25ca0aded85a293df04e0b52",
        "repo": "hadoop",
        "unit_tests": [
            "TestNameNodeRpcServer.java"
        ]
    },
    "hadoop_dd57c20": {
        "bug_id": "hadoop_dd57c20",
        "commit": "https://github.com/apache/hadoop/commit/dd57c2047bfd21910acc38c98153eedf1db75169",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=dd57c2047bfd21910acc38c98153eedf1db75169",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -319,6 +319,9 @@ Release 2.7.0 - UNRELEASED\n     YARN-2958. Made RMStateStore not update the last sequence number when updating the\n     delegation token. (Varun Saxena via zjshen)\n \n+    YARN-2978. Fixed potential NPE while getting queue info. (Varun Saxena via\n+    jianhe)\n+\n Release 2.6.0 - 2014-11-18\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/CHANGES.txt",
                "sha": "2f7b07cfacb52efe641f2194699b1db83dd9b352",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/AbstractCSQueue.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/AbstractCSQueue.java?ref=dd57c2047bfd21910acc38c98153eedf1db75169",
                "deletions": 11,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/AbstractCSQueue.java",
                "patch": "@@ -65,7 +65,6 @@\n   RMNodeLabelsManager labelManager;\n   String defaultLabelExpression;\n   Resource usedResources = Resources.createResource(0, 0);\n-  QueueInfo queueInfo;\n   Map<String, Float> absoluteCapacityByNodeLabels;\n   Map<String, Float> capacitiyByNodeLabels;\n   Map<String, Resource> usedResourcesByNodeLabels = new HashMap<String, Resource>();\n@@ -87,7 +86,6 @@ public AbstractCSQueue(CapacitySchedulerContext cs,\n     this.parent = parent;\n     this.queueName = queueName;\n     this.resourceCalculator = cs.getResourceCalculator();\n-    this.queueInfo = recordFactory.newRecordInstance(QueueInfo.class);\n     \n     // must be called after parent and queueName is set\n     this.metrics = old != null ? old.getMetrics() :\n@@ -99,9 +97,7 @@ public AbstractCSQueue(CapacitySchedulerContext cs,\n     this.accessibleLabels = cs.getConfiguration().getAccessibleNodeLabels(getQueuePath());\n     this.defaultLabelExpression = cs.getConfiguration()\n         .getDefaultNodeLabelExpression(getQueuePath());\n-    \n-    this.queueInfo.setQueueName(queueName);\n-    \n+\n     // inherit from parent if labels not set\n     if (this.accessibleLabels == null && parent != null) {\n       this.accessibleLabels = parent.getAccessibleNodeLabels();\n@@ -280,12 +276,6 @@ synchronized void setupQueueConfigs(Resource clusterResource, float capacity,\n     this.capacitiyByNodeLabels = new HashMap<String, Float>(nodeLabelCapacities);\n     this.maxCapacityByNodeLabels =\n         new HashMap<String, Float>(maximumNodeLabelCapacities);\n-    \n-    this.queueInfo.setAccessibleNodeLabels(this.accessibleLabels);\n-    this.queueInfo.setCapacity(this.capacity);\n-    this.queueInfo.setMaximumCapacity(this.maximumCapacity);\n-    this.queueInfo.setQueueState(this.state);\n-    this.queueInfo.setDefaultNodeLabelExpression(this.defaultLabelExpression);\n \n     // Update metrics\n     CSQueueUtils.updateQueueStatistics(\n@@ -330,6 +320,18 @@ synchronized void setupQueueConfigs(Resource clusterResource, float capacity,\n     this.reservationsContinueLooking = reservationContinueLooking;\n   }\n   \n+  protected QueueInfo getQueueInfo() {\n+    QueueInfo queueInfo = recordFactory.newRecordInstance(QueueInfo.class);\n+    queueInfo.setQueueName(queueName);\n+    queueInfo.setAccessibleNodeLabels(accessibleLabels);\n+    queueInfo.setCapacity(capacity);\n+    queueInfo.setMaximumCapacity(maximumCapacity);\n+    queueInfo.setQueueState(state);\n+    queueInfo.setDefaultNodeLabelExpression(defaultLabelExpression);\n+    queueInfo.setCurrentCapacity(getUsedCapacity());\n+    return queueInfo;\n+  }\n+  \n   @Private\n   public Resource getMaximumAllocation() {\n     return maximumAllocation;",
                "raw_url": "https://github.com/apache/hadoop/raw/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/AbstractCSQueue.java",
                "sha": "fec3a567744612905cb323d27e1ac9184b60ac07",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java?ref=dd57c2047bfd21910acc38c98153eedf1db75169",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "patch": "@@ -163,8 +163,6 @@ public LeafQueue(CapacitySchedulerContext cs,\n         CSQueueUtils.computeMaxActiveApplicationsPerUser(\n             maxActiveAppsUsingAbsCap, userLimit, userLimitFactor);\n \n-    this.queueInfo.setChildQueues(new ArrayList<QueueInfo>());\n-\n     QueueState state = cs.getConfiguration().getState(getQueuePath());\n \n     Map<QueueACL, AccessControlList> acls = \n@@ -235,14 +233,14 @@ protected synchronized void setupQueueConfigs(\n         this.defaultLabelExpression)) {\n       throw new IOException(\"Invalid default label expression of \"\n           + \" queue=\"\n-          + queueInfo.getQueueName()\n+          + getQueueName()\n           + \" doesn't have permission to access all labels \"\n           + \"in default label expression. labelExpression of resource request=\"\n           + (this.defaultLabelExpression == null ? \"\"\n               : this.defaultLabelExpression)\n           + \". Queue labels=\"\n-          + (queueInfo.getAccessibleNodeLabels() == null ? \"\" : StringUtils.join(queueInfo\n-              .getAccessibleNodeLabels().iterator(), ',')));\n+          + (getAccessibleNodeLabels() == null ? \"\" : StringUtils.join(\n+              getAccessibleNodeLabels().iterator(), ',')));\n     }\n     \n     this.nodeLocalityDelay = nodeLocalityDelay;\n@@ -433,7 +431,7 @@ public synchronized float getUserLimitFactor() {\n   @Override\n   public synchronized QueueInfo getQueueInfo(\n       boolean includeChildQueues, boolean recursive) {\n-    queueInfo.setCurrentCapacity(usedCapacity);\n+    QueueInfo queueInfo = getQueueInfo();\n     return queueInfo;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "sha": "dd710695f186a387c37e3f7e237f9add53372966",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java?ref=dd57c2047bfd21910acc38c98153eedf1db75169",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java",
                "patch": "@@ -110,8 +110,6 @@ public ParentQueue(CapacitySchedulerContext cs,\n     Map<QueueACL, AccessControlList> acls = \n       cs.getConfiguration().getAcls(getQueuePath());\n \n-    this.queueInfo.setChildQueues(new ArrayList<QueueInfo>());\n-\n     setupQueueConfigs(cs.getClusterResource(), capacity, absoluteCapacity,\n         maximumCapacity, absoluteMaxCapacity, state, acls, accessibleLabels,\n         defaultLabelExpression, capacitiyByNodeLabels, maxCapacityByNodeLabels, \n@@ -206,7 +204,7 @@ public String getQueuePath() {\n   @Override\n   public synchronized QueueInfo getQueueInfo( \n       boolean includeChildQueues, boolean recursive) {\n-    queueInfo.setCurrentCapacity(usedCapacity);\n+    QueueInfo queueInfo = getQueueInfo();\n \n     List<QueueInfo> childQueuesInfo = new ArrayList<QueueInfo>();\n     if (includeChildQueues) {",
                "raw_url": "https://github.com/apache/hadoop/raw/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java",
                "sha": "f820ccab929a905c78ccd1cf8422fb78f58d82e4",
                "status": "modified"
            }
        ],
        "message": "YARN-2978. Fixed potential NPE while getting queue info. Contributed by Varun Saxena",
        "parent": "https://github.com/apache/hadoop/commit/d02fb53750bc592c23ba470ae82eb6f47d9a00ec",
        "repo": "hadoop",
        "unit_tests": [
            "TestParentQueue.java"
        ]
    },
    "hadoop_dd852f5": {
        "bug_id": "hadoop_dd852f5",
        "commit": "https://github.com/apache/hadoop/commit/dd852f5b8c8fe9e52d15987605f36b5b60f02701",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/dd852f5b8c8fe9e52d15987605f36b5b60f02701/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=dd852f5b8c8fe9e52d15987605f36b5b60f02701",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -153,6 +153,9 @@ Release 2.8.0 - UNRELEASED\n \n     YARN-3110. Few issues in ApplicationHistory web ui. (Naganarasimha G R via xgong)\n \n+    YARN-3457. NPE when NodeManager.serviceInit fails and stopRecoveryStore called.\n+    (Bibin A Chundatt via ozawa)\n+\n Release 2.7.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/dd852f5b8c8fe9e52d15987605f36b5b60f02701/hadoop-yarn-project/CHANGES.txt",
                "sha": "d5f6ce0c4e4a12079e5ca90e6d8a1ca6e42f7885",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/dd852f5b8c8fe9e52d15987605f36b5b60f02701/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java?ref=dd852f5b8c8fe9e52d15987605f36b5b60f02701",
                "deletions": 10,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "patch": "@@ -177,16 +177,18 @@ private void initAndStartRecoveryStore(Configuration conf)\n \n   private void stopRecoveryStore() throws IOException {\n     nmStore.stop();\n-    if (context.getDecommissioned() && nmStore.canRecover()) {\n-      LOG.info(\"Removing state store due to decommission\");\n-      Configuration conf = getConfig();\n-      Path recoveryRoot = new Path(\n-          conf.get(YarnConfiguration.NM_RECOVERY_DIR));\n-      LOG.info(\"Removing state store at \" + recoveryRoot\n-          + \" due to decommission\");\n-      FileSystem recoveryFs = FileSystem.getLocal(conf);\n-      if (!recoveryFs.delete(recoveryRoot, true)) {\n-        LOG.warn(\"Unable to delete \" + recoveryRoot);\n+    if (null != context) {\n+      if (context.getDecommissioned() && nmStore.canRecover()) {\n+        LOG.info(\"Removing state store due to decommission\");\n+        Configuration conf = getConfig();\n+        Path recoveryRoot =\n+            new Path(conf.get(YarnConfiguration.NM_RECOVERY_DIR));\n+        LOG.info(\"Removing state store at \" + recoveryRoot\n+            + \" due to decommission\");\n+        FileSystem recoveryFs = FileSystem.getLocal(conf);\n+        if (!recoveryFs.delete(recoveryRoot, true)) {\n+          LOG.warn(\"Unable to delete \" + recoveryRoot);\n+        }\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/dd852f5b8c8fe9e52d15987605f36b5b60f02701/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "sha": "9831fc42c46a9ece18a7987e91c69f0fbfc12aa4",
                "status": "modified"
            }
        ],
        "message": "YARN-3457. NPE when NodeManager.serviceInit fails and stopRecoveryStore called. Contributed by Bibin A Chundatt.",
        "parent": "https://github.com/apache/hadoop/commit/ab04ff9efe632b4eca6faca7407ac35e00e6a379",
        "repo": "hadoop",
        "unit_tests": [
            "TestNodeManager.java"
        ]
    },
    "hadoop_f39f8c5": {
        "bug_id": "hadoop_f39f8c5",
        "commit": "https://github.com/apache/hadoop/commit/f39f8c57344ede533ca4363c98230f3a0c401a76",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt?ref=f39f8c57344ede533ca4363c98230f3a0c401a76",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "patch": "@@ -45,3 +45,5 @@ IMPROVEMENTS:\n \n     HDFS-5390. Send one incremental block report per storage directory.\n     (Arpit Agarwal)\n+\n+    HDFS-5401. Fix NPE in Directory Scanner. (Arpit Agarwal)",
                "raw_url": "https://github.com/apache/hadoop/raw/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "sha": "cd139d4845e203e0125d72371d510352118fa6bc",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java?ref=f39f8c57344ede533ca4363c98230f3a0c401a76",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.commons.logging.Log;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n+import org.apache.hadoop.hdfs.StorageType;\n import org.apache.hadoop.hdfs.protocol.Block;\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n@@ -180,10 +181,11 @@ public String toString() {\n     }\n   }\n   \n-  void reportBadBlocks(ExtendedBlock block) {\n+  void reportBadBlocks(ExtendedBlock block,\n+                       String storageUuid, StorageType storageType) {\n     checkBlock(block);\n     for (BPServiceActor actor : bpServices) {\n-      actor.reportBadBlocks(block);\n+      actor.reportBadBlocks(block, storageUuid, storageType);\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "sha": "5d584616df335258b426878141001405130adb92",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java?ref=f39f8c57344ede533ca4363c98230f3a0c401a76",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
                "patch": "@@ -28,6 +28,7 @@\n import org.apache.commons.logging.Log;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.hdfs.DFSUtil;\n+import org.apache.hadoop.hdfs.StorageType;\n import org.apache.hadoop.hdfs.protocol.BlockListAsLongs;\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n@@ -237,12 +238,18 @@ void scheduleBlockReport(long delay) {\n     resetBlockReportTime = true; // reset future BRs for randomness\n   }\n \n-  void reportBadBlocks(ExtendedBlock block) {\n+  void reportBadBlocks(ExtendedBlock block,\n+      String storageUuid, StorageType storageType) {\n     if (bpRegistration == null) {\n       return;\n     }\n     DatanodeInfo[] dnArr = { new DatanodeInfo(bpRegistration) };\n-    LocatedBlock[] blocks = { new LocatedBlock(block, dnArr) }; \n+    String[] uuids = { storageUuid };\n+    StorageType[] types = { storageType };\n+    // TODO: Corrupt flag is set to false for compatibility. We can probably\n+    // set it to true here.\n+    LocatedBlock[] blocks = {\n+        new LocatedBlock(block, dnArr, uuids, types, -1, false) };\n     \n     try {\n       bpNamenode.reportBadBlocks(blocks);  ",
                "raw_url": "https://github.com/apache/hadoop/raw/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
                "sha": "172fb0fc30ec6cd3d5fdbe7dc2459e8c7d25cf48",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java?ref=f39f8c57344ede533ca4363c98230f3a0c401a76",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "patch": "@@ -559,7 +559,9 @@ public void notifyNamenodeDeletedBlock(ExtendedBlock block, String storageUuid)\n    */\n   public void reportBadBlocks(ExtendedBlock block) throws IOException{\n     BPOfferService bpos = getBPOSForBlock(block);\n-    bpos.reportBadBlocks(block);\n+    FsVolumeSpi volume = getFSDataset().getVolume(block);\n+    bpos.reportBadBlocks(\n+        block, volume.getStorageID(), volume.getStorageType());\n   }\n \n   /**\n@@ -1265,8 +1267,10 @@ private void transferBlock(ExtendedBlock block, DatanodeInfo xferTargets[])\n     // Check if NN recorded length matches on-disk length \n     long onDiskLength = data.getLength(block);\n     if (block.getNumBytes() > onDiskLength) {\n+      FsVolumeSpi volume = getFSDataset().getVolume(block);\n       // Shorter on-disk len indicates corruption so report NN the corrupt block\n-      bpos.reportBadBlocks(block);\n+      bpos.reportBadBlocks(\n+          block, volume.getStorageID(), volume.getStorageType());\n       LOG.warn(\"Can't replicate block \" + block\n           + \" because on-disk length \" + onDiskLength \n           + \" is shorter than NameNode recorded length \" + block.getNumBytes());",
                "raw_url": "https://github.com/apache/hadoop/raw/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "sha": "318d2f3705a05cad964d1b9b68ed52b7f4f3237a",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java?ref=f39f8c57344ede533ca4363c98230f3a0c401a76",
                "deletions": 16,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "patch": "@@ -198,7 +198,9 @@ public LengthInputStream getMetaDataInputStream(ExtendedBlock b)\n   //                 two maps. This might require some refactoring\n   //                 rewrite of FsDatasetImpl.\n   final ReplicaMap volumeMap;\n-  final Map<FsVolumeImpl, ReplicaMap> perVolumeReplicaMap;\n+\n+  // Map from StorageID to ReplicaMap.\n+  final Map<String, ReplicaMap> perVolumeReplicaMap;\n \n \n   // Used for synchronizing access to usage stats\n@@ -249,7 +251,7 @@ public LengthInputStream getMetaDataInputStream(ExtendedBlock b)\n       LOG.info(\"Added volume - \" + dir + \", StorageType: \" + storageType);\n     }\n     volumeMap = new ReplicaMap(this);\n-    perVolumeReplicaMap = new HashMap<FsVolumeImpl, ReplicaMap>();\n+    perVolumeReplicaMap = new HashMap<String, ReplicaMap>();\n \n     @SuppressWarnings(\"unchecked\")\n     final VolumeChoosingPolicy<FsVolumeImpl> blockChooserImpl =\n@@ -628,7 +630,7 @@ private synchronized ReplicaBeingWritten append(String bpid,\n     \n     // Replace finalized replica by a RBW replica in replicas map\n     volumeMap.add(bpid, newReplicaInfo);\n-    perVolumeReplicaMap.get(v).add(bpid, newReplicaInfo);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(bpid, newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n@@ -759,7 +761,7 @@ public synchronized ReplicaInPipeline createRbw(ExtendedBlock b)\n     ReplicaBeingWritten newReplicaInfo = new ReplicaBeingWritten(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile());\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n-    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), newReplicaInfo);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(b.getBlockPoolId(), newReplicaInfo);\n     return newReplicaInfo;\n   }\n   \n@@ -878,7 +880,7 @@ public synchronized ReplicaInPipeline convertTemporaryToRbw(\n     rbw.setBytesAcked(visible);\n     // overwrite the RBW in the volume map\n     volumeMap.add(b.getBlockPoolId(), rbw);\n-    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), rbw);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(b.getBlockPoolId(), rbw);\n     return rbw;\n   }\n \n@@ -898,7 +900,7 @@ public synchronized ReplicaInPipeline createTemporary(ExtendedBlock b)\n     ReplicaInPipeline newReplicaInfo = new ReplicaInPipeline(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile());\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n-    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), newReplicaInfo);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(b.getBlockPoolId(), newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n@@ -967,7 +969,8 @@ private synchronized FinalizedReplica finalizeReplica(String bpid,\n       newReplicaInfo = new FinalizedReplica(replicaInfo, v, dest.getParentFile());\n     }\n     volumeMap.add(bpid, newReplicaInfo);\n-    perVolumeReplicaMap.get(newReplicaInfo.getVolume()).add(bpid, newReplicaInfo);\n+    perVolumeReplicaMap.get(newReplicaInfo.getVolume().getStorageID())\n+        .add(bpid, newReplicaInfo);\n     return newReplicaInfo;\n   }\n \n@@ -981,7 +984,7 @@ public synchronized void unfinalizeBlock(ExtendedBlock b) throws IOException {\n     if (replicaInfo != null && replicaInfo.getState() == ReplicaState.TEMPORARY) {\n       // remove from volumeMap\n       volumeMap.remove(b.getBlockPoolId(), b.getLocalBlock());\n-      perVolumeReplicaMap.get((FsVolumeImpl) replicaInfo.getVolume())\n+      perVolumeReplicaMap.get(replicaInfo.getVolume().getStorageID())\n           .remove(b.getBlockPoolId(), b.getLocalBlock());\n       \n       // delete the on-disk temp file\n@@ -1064,7 +1067,7 @@ public BlockListAsLongs getBlockReport(String bpid) {\n         new HashMap<String, BlockListAsLongs>();\n \n     for (FsVolumeImpl v : getVolumes()) {\n-      ReplicaMap rMap = perVolumeReplicaMap.get(v);\n+      ReplicaMap rMap = perVolumeReplicaMap.get(v.getStorageID());\n       BlockListAsLongs blockList = getBlockReportWithReplicaMap(bpid, rMap);\n       blockReportMap.put(v.getStorageID(), blockList);\n     }\n@@ -1212,7 +1215,7 @@ public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n-        perVolumeReplicaMap.get(v).remove(bpid, invalidBlks[i]);\n+        perVolumeReplicaMap.get(v.getStorageID()).remove(bpid, invalidBlks[i]);\n       }\n \n       // Delete the block asynchronously to make sure we can do it fast enough\n@@ -1274,7 +1277,8 @@ public void checkDataDir() throws DiskErrorException {\n               LOG.warn(\"Removing replica \" + bpid + \":\" + b.getBlockId()\n                   + \" on failed volume \" + fv.getCurrentDir().getAbsolutePath());\n               ib.remove();\n-              perVolumeReplicaMap.get(fv).remove(bpid, b.getBlockId());\n+              perVolumeReplicaMap.get(fv.getStorageID())\n+                  .remove(bpid, b.getBlockId());\n               removedBlocks++;\n             }\n           }\n@@ -1391,8 +1395,7 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,\n           // Block is in memory and not on the disk\n           // Remove the block from volumeMap\n           volumeMap.remove(bpid, blockId);\n-          perVolumeReplicaMap.get((FsVolumeImpl) memBlockInfo.getVolume())\n-              .remove(bpid, blockId);\n+          perVolumeReplicaMap.get(vol.getStorageID()).remove(bpid, blockId);\n           final DataBlockScanner blockScanner = datanode.getBlockScanner();\n           if (blockScanner != null) {\n             blockScanner.deleteBlock(bpid, new Block(blockId));\n@@ -1416,8 +1419,8 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,\n         ReplicaInfo diskBlockInfo = new FinalizedReplica(blockId, \n             diskFile.length(), diskGS, vol, diskFile.getParentFile());\n         volumeMap.add(bpid, diskBlockInfo);\n-        perVolumeReplicaMap.get((FsVolumeImpl) memBlockInfo.getVolume()).\n-            remove(bpid, diskBlockInfo);\n+        perVolumeReplicaMap.get(vol.getStorageID())\n+            .remove(bpid, diskBlockInfo);\n         final DataBlockScanner blockScanner = datanode.getBlockScanner();\n         if (blockScanner != null) {\n           blockScanner.addBlock(new ExtendedBlock(bpid, diskBlockInfo));\n@@ -1695,7 +1698,7 @@ public synchronized void addBlockPool(String bpid, Configuration conf)\n \n     // TODO: Avoid the double scan.\n     for (FsVolumeImpl v : getVolumes()) {\n-      ReplicaMap rMap = perVolumeReplicaMap.get(v);\n+      ReplicaMap rMap = perVolumeReplicaMap.get(v.getStorageID());\n       rMap.initBlockPool(bpid);\n       volumes.getVolumeMap(bpid, v, rMap);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "sha": "9077c40a8367a9f4bdddc10e439a1e95920ef481",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java?ref=f39f8c57344ede533ca4363c98230f3a0c401a76",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java",
                "patch": "@@ -90,13 +90,13 @@ long getRemaining() throws IOException {\n     return remaining;\n   }\n     \n-  void initializeReplicaMaps(Map<FsVolumeImpl, ReplicaMap> perVolumeReplicaMap,\n+  void initializeReplicaMaps(Map<String, ReplicaMap> perVolumeReplicaMap,\n                              ReplicaMap globalReplicaMap,\n                              Object mutex) throws IOException {\n     for (FsVolumeImpl v : volumes) {\n       ReplicaMap rMap = new ReplicaMap(mutex);\n       v.getVolumeMap(rMap);\n-      perVolumeReplicaMap.put(v, rMap);\n+      perVolumeReplicaMap.put(v.getStorageID(), rMap);\n       globalReplicaMap.addAll(rMap);\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java",
                "sha": "671996718be2eea6db9b076cf8a5aed4de36705e",
                "status": "modified"
            }
        ],
        "message": "HDFS-5401. Fix NPE in Directory Scanner.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1535158 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/b442fe92fbaeb6bd891b94c50d6086a46d4af4ac",
        "repo": "hadoop",
        "unit_tests": [
            "TestFsVolumeList.java"
        ]
    },
    "hadoop_f5756a2": {
        "bug_id": "hadoop_f5756a2",
        "commit": "https://github.com/apache/hadoop/commit/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -543,6 +543,8 @@ Trunk (Unreleased)\n     HADOOP-12638. UnsatisfiedLinkError while checking ISA-L in checknative\n     command. (Kai Sasaki via Colin P. McCabe)\n \n+    HADOOP-12615. Fix NPE in MiniKMS.start(). (Wei-Chiu Chuang via zhz)\n+\n   OPTIMIZATIONS\n \n     HADOOP-7761. Improve the performance of raw comparisons. (todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "3c98eadad054bd20bdfcbd7d9636e9e9f4c98113",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hadoop/blob/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ThreadUtil.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ThreadUtil.java?ref=f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ThreadUtil.java",
                "patch": "@@ -22,6 +22,9 @@\n \n import org.apache.hadoop.classification.InterfaceStability;\n \n+import java.io.IOException;\n+import java.io.InputStream;\n+\n @InterfaceStability.Evolving\n public class ThreadUtil {\n   \n@@ -46,4 +49,31 @@ public static void sleepAtLeastIgnoreInterrupts(long millis) {\n       }\n     }\n   }\n+\n+  /**\n+   * Convenience method that returns a resource as inputstream from the\n+   * classpath.\n+   * <p>\n+   * It first attempts to use the Thread's context classloader and if not\n+   * set it uses the class' classloader.\n+   *\n+   * @param resourceName resource to retrieve.\n+   *\n+   * @throws IOException thrown if resource cannot be loaded\n+   * @return inputstream with the resource.\n+   */\n+  public static InputStream getResourceAsStream(String resourceName)\n+      throws IOException {\n+    ClassLoader cl = Thread.currentThread().getContextClassLoader();\n+    if (cl == null) {\n+      throw new IOException(\"Can not read resource file '\" + resourceName +\n+          \"' because class loader of the current thread is null\");\n+    }\n+    InputStream is = cl.getResourceAsStream(resourceName);\n+    if (is == null) {\n+      throw new IOException(\"Can not read resource file '\" +\n+          resourceName + \"'\");\n+    }\n+    return is;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ThreadUtil.java",
                "sha": "ab7b5fdeddbd56e94998c0effb1ab86a085ea70e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/VersionInfo.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/VersionInfo.java?ref=f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
                "deletions": 5,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/VersionInfo.java",
                "patch": "@@ -43,11 +43,7 @@ protected VersionInfo(String component) {\n     String versionInfoFile = component + \"-version-info.properties\";\n     InputStream is = null;\n     try {\n-      is = Thread.currentThread().getContextClassLoader()\n-        .getResourceAsStream(versionInfoFile);\n-      if (is == null) {\n-        throw new IOException(\"Resource not found\");\n-      }\n+      is = ThreadUtil.getResourceAsStream(versionInfoFile);\n       info.load(is);\n     } catch (IOException ex) {\n       LogFactory.getLog(getClass()).warn(\"Could not read '\" +",
                "raw_url": "https://github.com/apache/hadoop/raw/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/VersionInfo.java",
                "sha": "dc8d36959a0867cc2cbcab46688d749c3466fdb1",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hadoop/blob/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/MiniKMS.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/MiniKMS.java?ref=f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
                "deletions": 10,
                "filename": "hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/MiniKMS.java",
                "patch": "@@ -23,6 +23,7 @@\n import org.apache.hadoop.crypto.key.kms.KMSRESTConstants;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.security.ssl.SslSocketConnectorSecure;\n+import org.apache.hadoop.util.ThreadUtil;\n import org.mortbay.jetty.Connector;\n import org.mortbay.jetty.Server;\n import org.mortbay.jetty.security.SslSocketConnector;\n@@ -34,6 +35,7 @@\n import java.io.InputStream;\n import java.io.OutputStream;\n import java.io.Writer;\n+import java.io.IOException;\n import java.net.InetAddress;\n import java.net.MalformedURLException;\n import java.net.ServerSocket;\n@@ -149,16 +151,26 @@ public MiniKMS(String kmsConfDir, String log4ConfFile, String keyStore,\n     this.inPort = inPort;\n   }\n \n+  private void copyResource(String inputResourceName, File outputFile) throws\n+      IOException {\n+    InputStream is = null;\n+    OutputStream os = null;\n+    try {\n+      is = ThreadUtil.getResourceAsStream(inputResourceName);\n+      os = new FileOutputStream(outputFile);\n+      IOUtils.copy(is, os);\n+    } finally {\n+      IOUtils.closeQuietly(is);\n+      IOUtils.closeQuietly(os);\n+    }\n+  }\n+\n   public void start() throws Exception {\n     ClassLoader cl = Thread.currentThread().getContextClassLoader();\n     System.setProperty(KMSConfiguration.KMS_CONFIG_DIR, kmsConfDir);\n     File aclsFile = new File(kmsConfDir, \"kms-acls.xml\");\n     if (!aclsFile.exists()) {\n-      InputStream is = cl.getResourceAsStream(\"mini-kms-acls-default.xml\");\n-      OutputStream os = new FileOutputStream(aclsFile);\n-      IOUtils.copy(is, os);\n-      is.close();\n-      os.close();\n+      copyResource(\"mini-kms-acls-default.xml\", aclsFile);\n     }\n     File coreFile = new File(kmsConfDir, \"core-site.xml\");\n     if (!coreFile.exists()) {\n@@ -195,11 +207,7 @@ public void start() throws Exception {\n           \"/kms-webapp/WEB-INF\");\n       webInf.mkdirs();\n       new File(webInf, \"web.xml\").delete();\n-      InputStream is = cl.getResourceAsStream(\"kms-webapp/WEB-INF/web.xml\");\n-      OutputStream os = new FileOutputStream(new File(webInf, \"web.xml\"));\n-      IOUtils.copy(is, os);\n-      is.close();\n-      os.close();\n+      copyResource(\"kms-webapp/WEB-INF/web.xml\", new File(webInf, \"web.xml\"));\n       webappPath = webInf.getParentFile().getAbsolutePath();\n     } else {\n       webappPath = cl.getResource(\"kms-webapp\").getPath();",
                "raw_url": "https://github.com/apache/hadoop/raw/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/MiniKMS.java",
                "sha": "f520edfa51d390cae77068a50d0e7d2be4746af5",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMSAudit.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMSAudit.java?ref=f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
                "deletions": 4,
                "filename": "hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMSAudit.java",
                "patch": "@@ -19,11 +19,15 @@\n \n import java.io.ByteArrayOutputStream;\n import java.io.FilterOutputStream;\n+import java.io.InputStream;\n+import java.io.IOException;\n import java.io.OutputStream;\n import java.io.PrintStream;\n \n import org.apache.hadoop.crypto.key.kms.server.KMS.KMSOp;\n+import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.util.ThreadUtil;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.PropertyConfigurator;\n import org.junit.After;\n@@ -52,15 +56,17 @@ public void setOutputStream(OutputStream out) {\n   }\n \n   @Before\n-  public void setUp() {\n+  public void setUp() throws IOException {\n     originalOut = System.err;\n     memOut = new ByteArrayOutputStream();\n     filterOut = new FilterOut(memOut);\n     capturedOut = new PrintStream(filterOut);\n     System.setErr(capturedOut);\n-    PropertyConfigurator.configure(Thread.currentThread().\n-        getContextClassLoader()\n-        .getResourceAsStream(\"log4j-kmsaudit.properties\"));\n+    InputStream is =\n+        ThreadUtil.getResourceAsStream(\"log4j-kmsaudit.properties\");\n+    PropertyConfigurator.configure(is);\n+    IOUtils.closeStream(is);\n+\n     this.kmsAudit = new KMSAudit(1000);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMSAudit.java",
                "sha": "7e1c0d7b08ec209fded079985ca929c95f797ed7",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/hadoop/blob/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-minikdc/src/main/java/org/apache/hadoop/minikdc/MiniKdc.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-minikdc/src/main/java/org/apache/hadoop/minikdc/MiniKdc.java?ref=f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
                "deletions": 3,
                "filename": "hadoop-common-project/hadoop-minikdc/src/main/java/org/apache/hadoop/minikdc/MiniKdc.java",
                "patch": "@@ -61,6 +61,7 @@\n import java.io.FileInputStream;\n import java.io.InputStream;\n import java.io.InputStreamReader;\n+import java.io.IOException;\n import java.io.StringReader;\n import java.lang.reflect.Method;\n import java.net.InetAddress;\n@@ -389,6 +390,32 @@ private void initDirectoryService() throws Exception {\n     ds.getAdminSession().add(entry);\n   }\n \n+  /**\n+   * Convenience method that returns a resource as inputstream from the\n+   * classpath.\n+   * <p>\n+   * It first attempts to use the Thread's context classloader and if not\n+   * set it uses the class' classloader.\n+   *\n+   * @param resourceName resource to retrieve.\n+   *\n+   * @throws IOException thrown if resource cannot be loaded\n+   * @return inputstream with the resource.\n+   */\n+  public static InputStream getResourceAsStream(String resourceName)\n+      throws IOException {\n+    ClassLoader cl = Thread.currentThread().getContextClassLoader();\n+    if (cl == null) {\n+      cl = MiniKdc.class.getClassLoader();\n+    }\n+    InputStream is = cl.getResourceAsStream(resourceName);\n+    if (is == null) {\n+      throw new IOException(\"Can not read resource file '\" +\n+          resourceName + \"'\");\n+    }\n+    return is;\n+  }\n+\n   private void initKDCServer() throws Exception {\n     String orgName= conf.getProperty(ORG_NAME);\n     String orgDomain = conf.getProperty(ORG_DOMAIN);\n@@ -400,8 +427,7 @@ private void initKDCServer() throws Exception {\n     map.put(\"3\", orgDomain.toUpperCase(Locale.ENGLISH));\n     map.put(\"4\", bindAddress);\n \n-    ClassLoader cl = Thread.currentThread().getContextClassLoader();\n-    InputStream is1 = cl.getResourceAsStream(\"minikdc.ldiff\");\n+    InputStream is1 = getResourceAsStream(\"minikdc.ldiff\");\n \n     SchemaManager schemaManager = ds.getSchemaManager();\n     LdifReader reader = null;\n@@ -443,7 +469,7 @@ private void initKDCServer() throws Exception {\n     kdc.start();\n \n     StringBuilder sb = new StringBuilder();\n-    InputStream is2 = cl.getResourceAsStream(\"minikdc-krb5.conf\");\n+    InputStream is2 = getResourceAsStream(\"minikdc-krb5.conf\");\n \n     BufferedReader r = null;\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-minikdc/src/main/java/org/apache/hadoop/minikdc/MiniKdc.java",
                "sha": "a5253c4521715cc911a29003685f196bce456ad6",
                "status": "modified"
            }
        ],
        "message": "HADOOP-12615. Fix NPE in MiniKMS.start(). Contributed by Wei-Chiu Chuang.\n\nChange-Id: Ie3e148bd1401618b1737a577957298bf622891f4",
        "parent": "https://github.com/apache/hadoop/commit/5104077e1f431ad3675d0b1c5c3cf53936902d8e",
        "repo": "hadoop",
        "unit_tests": [
            "TestMiniKdc.java"
        ]
    },
    "hadoop_f9e36de": {
        "bug_id": "hadoop_f9e36de",
        "commit": "https://github.com/apache/hadoop/commit/f9e36dea96f592d09f159e521379e426e7f07ec9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/f9e36dea96f592d09f159e521379e426e7f07ec9/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=f9e36dea96f592d09f159e521379e426e7f07ec9",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -99,6 +99,9 @@ Release 2.9.0 - UNRELEASED\n     YARN-2934. Improve handling of container's stderr.\n     (Naganarasimha G R via gera)\n \n+    YARN-4530. LocalizedResource trigger a NPE Cause the NodeManager exit\n+    (tangshangwen via rohithsharmaks)\n+\n Release 2.8.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/f9e36dea96f592d09f159e521379e426e7f07ec9/hadoop-yarn-project/CHANGES.txt",
                "sha": "80a3ed1dd270719218e99ba5966206fdc132ba33",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/f9e36dea96f592d09f159e521379e426e7f07ec9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java?ref=f9e36dea96f592d09f159e521379e426e7f07ec9",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "patch": "@@ -878,12 +878,12 @@ public void run() {\n             Future<Path> completed = queue.take();\n             LocalizerResourceRequestEvent assoc = pending.remove(completed);\n             try {\n-              Path local = completed.get();\n               if (null == assoc) {\n                 LOG.error(\"Localized unknown resource to \" + completed);\n                 // TODO delete\n                 return;\n               }\n+              Path local = completed.get();\n               LocalResourceRequest key = assoc.getResource().getRequest();\n               publicRsrc.handle(new ResourceLocalizedEvent(key, local, FileUtil\n                 .getDU(new File(local.toUri()))));",
                "raw_url": "https://github.com/apache/hadoop/raw/f9e36dea96f592d09f159e521379e426e7f07ec9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "sha": "c0c2e8e4fbcc69cc1b20aa6ba95f2bbff9dfe939",
                "status": "modified"
            }
        ],
        "message": "YARN-4530. LocalizedResource trigger a NPE Cause the NodeManager exit. (tangshangwen via rohithsharmaks)",
        "parent": "https://github.com/apache/hadoop/commit/4e4b3a8465a8433e78e015cb1ce7e0dc1ebeb523",
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceLocalizationService.java"
        ]
    },
    "hadoop_fe35103": {
        "bug_id": "hadoop_fe35103",
        "commit": "https://github.com/apache/hadoop/commit/fe35103591ece0209f8345aba5544313e45a073c",
        "file": [
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hadoop/blob/fe35103591ece0209f8345aba5544313e45a073c/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java?ref=fe35103591ece0209f8345aba5544313e45a073c",
                "deletions": 18,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
                "patch": "@@ -585,33 +585,38 @@ public void setCheckpointID(TaskID taskId, TaskCheckpointID cid) {\n   private void coalesceStatusUpdate(TaskAttemptId yarnAttemptID,\n       TaskAttemptStatus taskAttemptStatus,\n       AtomicReference<TaskAttemptStatus> lastStatusRef) {\n-    boolean asyncUpdatedNeeded = false;\n-    TaskAttemptStatus lastStatus = lastStatusRef.get();\n-\n-    if (lastStatus == null) {\n-      lastStatusRef.set(taskAttemptStatus);\n-      asyncUpdatedNeeded = true;\n-    } else {\n-      List<TaskAttemptId> oldFetchFailedMaps =\n-          taskAttemptStatus.fetchFailedMaps;\n-\n-      // merge fetchFailedMaps from the previous update\n-      if (lastStatus.fetchFailedMaps != null) {\n+    List<TaskAttemptId> fetchFailedMaps = taskAttemptStatus.fetchFailedMaps;\n+    TaskAttemptStatus lastStatus = null;\n+    boolean done = false;\n+    while (!done) {\n+      lastStatus = lastStatusRef.get();\n+      if (lastStatus != null && lastStatus.fetchFailedMaps != null) {\n+        // merge fetchFailedMaps from the previous update\n         if (taskAttemptStatus.fetchFailedMaps == null) {\n           taskAttemptStatus.fetchFailedMaps = lastStatus.fetchFailedMaps;\n         } else {\n-          taskAttemptStatus.fetchFailedMaps.addAll(lastStatus.fetchFailedMaps);\n+          taskAttemptStatus.fetchFailedMaps =\n+              new ArrayList<>(lastStatus.fetchFailedMaps.size() +\n+                  fetchFailedMaps.size());\n+          taskAttemptStatus.fetchFailedMaps.addAll(\n+              lastStatus.fetchFailedMaps);\n+          taskAttemptStatus.fetchFailedMaps.addAll(\n+              fetchFailedMaps);\n         }\n       }\n \n-      if (!lastStatusRef.compareAndSet(lastStatus, taskAttemptStatus)) {\n-        // update failed - async dispatcher has processed it in the meantime\n-        taskAttemptStatus.fetchFailedMaps = oldFetchFailedMaps;\n-        lastStatusRef.set(taskAttemptStatus);\n-        asyncUpdatedNeeded = true;\n+      // lastStatusRef may be changed by either the AsyncDispatcher when\n+      // it processes the update, or by another IPC server handler\n+      done = lastStatusRef.compareAndSet(lastStatus, taskAttemptStatus);\n+      if (!done) {\n+        LOG.info(\"TaskAttempt \" + yarnAttemptID +\n+            \": lastStatusRef changed by another thread, retrying...\");\n+        // let's revert taskAttemptStatus.fetchFailedMaps\n+        taskAttemptStatus.fetchFailedMaps = fetchFailedMaps;\n       }\n     }\n \n+    boolean asyncUpdatedNeeded = (lastStatus == null);\n     if (asyncUpdatedNeeded) {\n       context.getEventHandler().handle(\n           new TaskAttemptStatusUpdateEvent(taskAttemptStatus.id,",
                "raw_url": "https://github.com/apache/hadoop/raw/fe35103591ece0209f8345aba5544313e45a073c/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
                "sha": "556c90c4412173dbb9f9975be9ed53c29ef6dd77",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-7028. Concurrent task progress updates causing NPE in Application Master. Contributed by Gergo Repas",
        "parent": "https://github.com/apache/hadoop/commit/c9bf813c9a6c018d14f2bef49ba086ec0e60c761",
        "repo": "hadoop",
        "unit_tests": [
            "TestTaskAttemptListenerImpl.java"
        ]
    }
}