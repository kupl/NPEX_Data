{
    "hadoop_0247cb6": {
        "bug_id": "hadoop_0247cb6",
        "commit": "https://github.com/apache/hadoop/commit/0247cb6318507afe06816e337a19f396afc53efa",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/0247cb6318507afe06816e337a19f396afc53efa/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java?ref=0247cb6318507afe06816e337a19f396afc53efa",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java",
                "patch": "@@ -598,6 +598,11 @@ private ShortCircuitReplicaInfo requestFileDescriptors(DomainPeer peer,\n       sock.recvFileInputStreams(fis, buf, 0, buf.length);\n       ShortCircuitReplica replica = null;\n       try {\n+        if (fis[0] == null || fis[1] == null) {\n+          throw new IOException(\"the datanode \" + datanode + \" failed to \" +\n+              \"pass a file descriptor (might have reached open file limit).\");\n+        }\n+\n         ExtendedBlockId key =\n             new ExtendedBlockId(block.getBlockId(), block.getBlockPoolId());\n         if (buf[0] == USE_RECEIPT_VERIFICATION.getNumber()) {",
                "raw_url": "https://github.com/apache/hadoop/raw/0247cb6318507afe06816e337a19f396afc53efa/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java",
                "sha": "ce4318531a33c88440d79cfc54c8bd70fd43689f",
                "status": "modified"
            },
            {
                "additions": 89,
                "blob_url": "https://github.com/apache/hadoop/blob/0247cb6318507afe06816e337a19f396afc53efa/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/shortcircuit/TestShortCircuitCache.java",
                "changes": 89,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/shortcircuit/TestShortCircuitCache.java?ref=0247cb6318507afe06816e337a19f396afc53efa",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/shortcircuit/TestShortCircuitCache.java",
                "patch": "@@ -42,6 +42,10 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FSDataInputStream;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.hdfs.ClientContext;\n+import org.apache.hadoop.hdfs.DFSClient;\n+import org.apache.hadoop.hdfs.DFSUtilClient;\n+import org.apache.hadoop.hdfs.PeerCache;\n import org.apache.hadoop.hdfs.client.impl.BlockReaderFactory;\n import org.apache.hadoop.hdfs.client.impl.BlockReaderTestUtil;\n import org.apache.hadoop.hdfs.DFSInputStream;\n@@ -50,10 +54,12 @@\n import org.apache.hadoop.hdfs.ExtendedBlockId;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.hdfs.client.HdfsClientConfigKeys;\n+import org.apache.hadoop.hdfs.client.impl.DfsClientConf;\n import org.apache.hadoop.hdfs.net.DomainPeer;\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo.DatanodeInfoBuilder;\n import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n+import org.apache.hadoop.hdfs.protocol.LocatedBlock;\n import org.apache.hadoop.hdfs.server.datanode.BlockMetadataHeader;\n import org.apache.hadoop.hdfs.server.datanode.DataNodeFaultInjector;\n import org.apache.hadoop.hdfs.server.datanode.ShortCircuitRegistry;\n@@ -66,9 +72,11 @@\n import org.apache.hadoop.hdfs.shortcircuit.ShortCircuitShm.Slot;\n import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.ipc.RetriableException;\n+import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.net.unix.DomainSocket;\n import org.apache.hadoop.net.unix.TemporarySocketDirectory;\n import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n+import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.test.GenericTestUtils;\n import org.apache.hadoop.util.DataChecksum;\n import org.apache.hadoop.util.Time;\n@@ -819,4 +827,85 @@ public void testFetchOrCreateRetries() throws Exception {\n         .fetch(Mockito.eq(extendedBlockId), Mockito.any());\n     }\n   }\n+\n+  @Test\n+  public void testRequestFileDescriptorsWhenULimit() throws Exception {\n+    TemporarySocketDirectory sockDir = new TemporarySocketDirectory();\n+    Configuration conf = createShortCircuitConf(\n+        \"testRequestFileDescriptorsWhenULimit\", sockDir);\n+\n+    final short replicas = 1;\n+    final int fileSize = 3;\n+    final String testFile = \"/testfile\";\n+\n+    try (MiniDFSCluster cluster =\n+        new MiniDFSCluster.Builder(conf).numDataNodes(replicas).build()) {\n+\n+      cluster.waitActive();\n+\n+      DistributedFileSystem fs = cluster.getFileSystem();\n+      DFSTestUtil.createFile(fs, new Path(testFile), fileSize, replicas, 0L);\n+\n+      LocatedBlock blk = new DFSClient(DFSUtilClient.getNNAddress(conf), conf)\n+          .getLocatedBlocks(testFile, 0, fileSize).get(0);\n+\n+      ClientContext clientContext = Mockito.mock(ClientContext.class);\n+      Mockito.when(clientContext.getPeerCache()).thenAnswer(\n+          (Answer<PeerCache>) peerCacheCall -> {\n+            PeerCache peerCache = new PeerCache(10, Long.MAX_VALUE);\n+            DomainPeer peer = Mockito.spy(getDomainPeerToDn(conf));\n+            peerCache.put(blk.getLocations()[0], peer);\n+\n+            Mockito.when(peer.getDomainSocket()).thenAnswer(\n+                (Answer<DomainSocket>) domainSocketCall -> {\n+                  DomainSocket domainSocket = Mockito.mock(DomainSocket.class);\n+                  Mockito.when(domainSocket\n+                      .recvFileInputStreams(\n+                          Mockito.any(FileInputStream[].class),\n+                          Mockito.any(byte[].class),\n+                          Mockito.anyInt(),\n+                          Mockito.anyInt())\n+                  ).thenAnswer(\n+                      // we are mocking the FileOutputStream array with nulls\n+                      (Answer<Void>) recvFileInputStreamsCall -> null\n+                  );\n+                  return domainSocket;\n+                }\n+            );\n+\n+            return peerCache;\n+          });\n+\n+      Mockito.when(clientContext.getShortCircuitCache()).thenAnswer(\n+          (Answer<ShortCircuitCache>) shortCircuitCacheCall -> {\n+            ShortCircuitCache cache = Mockito.mock(ShortCircuitCache.class);\n+            Mockito.when(cache.allocShmSlot(\n+                Mockito.any(DatanodeInfo.class),\n+                Mockito.any(DomainPeer.class),\n+                Mockito.any(MutableBoolean.class),\n+                Mockito.any(ExtendedBlockId.class),\n+                Mockito.anyString()))\n+                .thenAnswer((Answer<Slot>) call -> null);\n+\n+            return cache;\n+          }\n+      );\n+\n+      DatanodeInfo[] nodes = blk.getLocations();\n+\n+      try {\n+        Assert.assertNull(new BlockReaderFactory(new DfsClientConf(conf))\n+            .setInetSocketAddress(NetUtils.createSocketAddr(nodes[0]\n+                .getXferAddr()))\n+            .setClientCacheContext(clientContext)\n+            .setDatanodeInfo(blk.getLocations()[0])\n+            .setBlock(blk.getBlock())\n+            .setBlockToken(new Token())\n+            .createShortCircuitReplicaInfo());\n+      } catch (NullPointerException ex) {\n+        Assert.fail(\"Should not throw NPE when the native library is unable \" +\n+            \"to create new files!\");\n+      }\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/0247cb6318507afe06816e337a19f396afc53efa/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/shortcircuit/TestShortCircuitCache.java",
                "sha": "ac29c3c33f7624ba717823ee7c01daf0e8bc950a",
                "status": "modified"
            }
        ],
        "message": "HDFS-13121. NPE when request file descriptors when SC read. Contributed by Zsolt Venczel.",
        "parent": "https://github.com/apache/hadoop/commit/061b168529a9cd5d6a3a482c890bacdb49186368",
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockReaderFactory.java"
        ]
    },
    "hadoop_0753031": {
        "bug_id": "hadoop_0753031",
        "commit": "https://github.com/apache/hadoop/commit/07530314c2130ecd1525682c59bf51f15b82c024",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/07530314c2130ecd1525682c59bf51f15b82c024/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/IpcException.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/IpcException.java?ref=07530314c2130ecd1525682c59bf51f15b82c024",
                "deletions": 3,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/IpcException.java",
                "patch": "@@ -26,9 +26,8 @@\n  */\n public class IpcException extends IOException {\n   private static final long serialVersionUID = 1L;\n-  \n-  final String errMsg;\n+\n   public IpcException(final String err) {\n-    errMsg = err; \n+    super(err);\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/07530314c2130ecd1525682c59bf51f15b82c024/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/IpcException.java",
                "sha": "61c42b80887f0f941885e52bb918b94b4f90d265",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/07530314c2130ecd1525682c59bf51f15b82c024/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java?ref=07530314c2130ecd1525682c59bf51f15b82c024",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "patch": "@@ -2155,7 +2155,7 @@ private void doSaslReply(Message message) throws IOException {\n     private void doSaslReply(Exception ioe) throws IOException {\n       setupResponse(authFailedCall,\n           RpcStatusProto.FATAL, RpcErrorCodeProto.FATAL_UNAUTHORIZED,\n-          null, ioe.getClass().getName(), ioe.getLocalizedMessage());\n+          null, ioe.getClass().getName(), ioe.toString());\n       sendResponse(authFailedCall);\n     }\n \n@@ -2550,7 +2550,8 @@ private void processOneRpc(ByteBuffer bb)\n         final RpcCall call = new RpcCall(this, callId, retry);\n         setupResponse(call,\n             rse.getRpcStatusProto(), rse.getRpcErrorCodeProto(), null,\n-            t.getClass().getName(), t.getMessage());\n+            t.getClass().getName(),\n+            t.getMessage() != null ? t.getMessage() : t.toString());\n         sendResponse(call);\n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/07530314c2130ecd1525682c59bf51f15b82c024/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "sha": "14fc2de530ce6cf2fe4550380c2bfec774ba14e4",
                "status": "modified"
            }
        ],
        "message": "HADOOP-9844. NPE when trying to create an error message response of SASL RPC\n\nThis closes #55\n\nChange-Id: I10a20380565fa89762f4aa564b2f1c83b9aeecdc\nSigned-off-by: Akira Ajisaka <aajisaka@apache.org>",
        "parent": "https://github.com/apache/hadoop/commit/98653ecccb80a7d793b2d27c81aebd3347f64b3c",
        "repo": "hadoop",
        "unit_tests": [
            "TestServer.java"
        ]
    },
    "hadoop_08d5060": {
        "bug_id": "hadoop_08d5060",
        "commit": "https://github.com/apache/hadoop/commit/08d5060605af81a3d6048044176dc656c0dad56c",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/08d5060605af81a3d6048044176dc656c0dad56c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/AllocationTagsManager.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/AllocationTagsManager.java?ref=08d5060605af81a3d6048044176dc656c0dad56c",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/AllocationTagsManager.java",
                "patch": "@@ -115,6 +115,11 @@ private void addTag(T type, String tag) {\n \n     private void removeTagFromInnerMap(Map<String, Long> innerMap, String tag) {\n       Long count = innerMap.get(tag);\n+      if (count == null) {\n+        LOG.warn(\"Trying to remove tags, however the tag \" + tag\n+            + \" no longer exists on this node/rack.\");\n+        return;\n+      }\n       if (count > 1) {\n         innerMap.put(tag, count - 1);\n       } else {",
                "raw_url": "https://github.com/apache/hadoop/raw/08d5060605af81a3d6048044176dc656c0dad56c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/AllocationTagsManager.java",
                "sha": "6f160b6363a767cee0f52cee58ed868ea983cede",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/hadoop/blob/08d5060605af81a3d6048044176dc656c0dad56c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestAllocationTagsManager.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestAllocationTagsManager.java?ref=08d5060605af81a3d6048044176dc656c0dad56c",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestAllocationTagsManager.java",
                "patch": "@@ -22,6 +22,7 @@\n \n import com.google.common.collect.ImmutableSet;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.NodeId;\n import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.server.resourcemanager.MockNodes;\n@@ -38,6 +39,7 @@\n import org.mockito.Mockito;\n \n import java.util.List;\n+import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n \n@@ -60,6 +62,41 @@ public void setup() {\n     rmContext = rm.getRMContext();\n   }\n \n+  @Test\n+  public void testMultipleAddRemoveContainer() {\n+    AllocationTagsManager atm = new AllocationTagsManager(rmContext);\n+\n+    NodeId nodeId = NodeId.fromString(\"host1:123\");\n+    ContainerId cid1 = TestUtils.getMockContainerId(1, 1);\n+    ContainerId cid2 = TestUtils.getMockContainerId(1, 2);\n+    ContainerId cid3 = TestUtils.getMockContainerId(1, 3);\n+    Set<String> tags1 = ImmutableSet.of(\"mapper\", \"reducer\");\n+    Set<String> tags2 = ImmutableSet.of(\"mapper\");\n+    Set<String> tags3 = ImmutableSet.of(\"zk\");\n+\n+    // node - mapper : 2\n+    //      - reduce : 1\n+    atm.addContainer(nodeId, cid1, tags1);\n+    atm.addContainer(nodeId, cid2, tags2);\n+    atm.addContainer(nodeId, cid3, tags3);\n+    Assert.assertEquals(2L,\n+        (long) atm.getAllocationTagsWithCount(nodeId).get(\"mapper\"));\n+    Assert.assertEquals(1L,\n+        (long) atm.getAllocationTagsWithCount(nodeId).get(\"reducer\"));\n+\n+    // remove container1\n+    atm.removeContainer(nodeId, cid1, tags1);\n+    Assert.assertEquals(1L,\n+        (long) atm.getAllocationTagsWithCount(nodeId).get(\"mapper\"));\n+    Assert.assertNull(atm.getAllocationTagsWithCount(nodeId).get(\"reducer\"));\n+\n+    // remove the same container again, the reducer no longer exists,\n+    // make sure there is no NPE here\n+    atm.removeContainer(nodeId, cid1, tags1);\n+    Assert.assertNull(atm.getAllocationTagsWithCount(nodeId).get(\"mapper\"));\n+    Assert.assertNull(atm.getAllocationTagsWithCount(nodeId).get(\"reducer\"));\n+  }\n+\n   @Test\n   public void testAllocationTagsManagerSimpleCases()\n       throws InvalidAllocationTagsQueryException {",
                "raw_url": "https://github.com/apache/hadoop/raw/08d5060605af81a3d6048044176dc656c0dad56c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestAllocationTagsManager.java",
                "sha": "9095ac1291c73385899b5ba4cd79d1dc9ccf3bed",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop/blob/08d5060605af81a3d6048044176dc656c0dad56c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestPlacementConstraintsUtil.java",
                "changes": 51,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestPlacementConstraintsUtil.java?ref=08d5060605af81a3d6048044176dc656c0dad56c",
                "deletions": 25,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestPlacementConstraintsUtil.java",
                "patch": "@@ -163,6 +163,11 @@ private ContainerId newContainerId(ApplicationId appId) {\n         ApplicationAttemptId.newInstance(appId, 0), 0);\n   }\n \n+  private ContainerId newContainerId(ApplicationId appId, int containerId) {\n+    return ContainerId.newContainerId(\n+        ApplicationAttemptId.newInstance(appId, 0), containerId);\n+  }\n+\n   private SchedulerNode newSchedulerNode(String hostname, String rackName,\n       NodeId nodeId) {\n     SchedulerNode node = mock(SchedulerNode.class);\n@@ -271,12 +276,10 @@ public void testMultiTagsPlacementConstraints()\n     SchedulerNode schedulerNode3 =newSchedulerNode(n3_r2.getHostName(),\n         n3_r2.getRackName(), n3_r2.getNodeID());\n \n-    ContainerId ca = ContainerId\n-        .newContainerId(ApplicationAttemptId.newInstance(appId1, 0), 0);\n+    ContainerId ca = newContainerId(appId1, 0);\n     tm.addContainer(n0_r1.getNodeID(), ca, ImmutableSet.of(\"A\"));\n \n-    ContainerId cb = ContainerId\n-        .newContainerId(ApplicationAttemptId.newInstance(appId1, 0), 0);\n+    ContainerId cb = newContainerId(appId1, 1);\n     tm.addContainer(n1_r1.getNodeID(), cb, ImmutableSet.of(\"B\"));\n \n     // n0 and n1 has A/B so they cannot satisfy the PC\n@@ -297,11 +300,9 @@ public void testMultiTagsPlacementConstraints()\n      * n2: A(1), B(1)\n      * n3:\n      */\n-    ContainerId ca1 = ContainerId\n-        .newContainerId(ApplicationAttemptId.newInstance(appId1, 0), 0);\n+    ContainerId ca1 = newContainerId(appId1, 2);\n     tm.addContainer(n2_r2.getNodeID(), ca1, ImmutableSet.of(\"A\"));\n-    ContainerId cb1 = ContainerId\n-        .newContainerId(ApplicationAttemptId.newInstance(appId1, 0), 0);\n+    ContainerId cb1 = newContainerId(appId1, 3);\n     tm.addContainer(n2_r2.getNodeID(), cb1, ImmutableSet.of(\"B\"));\n \n     // Only n2 has both A and B so only it can satisfy the PC\n@@ -468,9 +469,9 @@ public void testORConstraintAssignment()\n      *  n3: \"\"\n      */\n     tm.addContainer(n0r1.getNodeID(),\n-        newContainerId(appId1), ImmutableSet.of(\"hbase-m\"));\n+        newContainerId(appId1, 1), ImmutableSet.of(\"hbase-m\"));\n     tm.addContainer(n2r2.getNodeID(),\n-        newContainerId(appId1), ImmutableSet.of(\"hbase-rs\"));\n+        newContainerId(appId1, 2), ImmutableSet.of(\"hbase-rs\"));\n     Assert.assertEquals(1L, tm.getAllocationTagsWithCount(n0r1.getNodeID())\n         .get(\"hbase-m\").longValue());\n     Assert.assertEquals(1L, tm.getAllocationTagsWithCount(n2r2.getNodeID())\n@@ -504,7 +505,7 @@ public void testORConstraintAssignment()\n      *  n3: hbase-rs(1)\n      */\n     tm.addContainer(n3r2.getNodeID(),\n-        newContainerId(appId1), ImmutableSet.of(\"hbase-rs\"));\n+        newContainerId(appId1, 2), ImmutableSet.of(\"hbase-rs\"));\n     // n3 is qualified now because it is allocated with hbase-rs tag\n     Assert.assertTrue(PlacementConstraintsUtil.canSatisfyConstraints(appId1,\n         createSchedulingRequest(sourceTag1), schedulerNode3, pcm, tm));\n@@ -518,7 +519,7 @@ public void testORConstraintAssignment()\n      */\n     // Place\n     tm.addContainer(n2r2.getNodeID(),\n-        newContainerId(appId1), ImmutableSet.of(\"spark\"));\n+        newContainerId(appId1, 3), ImmutableSet.of(\"spark\"));\n     // According to constraint, \"zk\" is allowed to be placed on a node\n     // has \"hbase-m\" tag OR a node has both \"hbase-rs\" and \"spark\" tags.\n     Assert.assertTrue(PlacementConstraintsUtil.canSatisfyConstraints(appId1,\n@@ -552,9 +553,9 @@ public void testANDConstraintAssignment()\n      *  n3: \"\"\n      */\n     tm.addContainer(n0r1.getNodeID(),\n-        newContainerId(appId1), ImmutableSet.of(\"hbase-m\"));\n+        newContainerId(appId1, 0), ImmutableSet.of(\"hbase-m\"));\n     tm.addContainer(n2r2.getNodeID(),\n-        newContainerId(appId1), ImmutableSet.of(\"hbase-m\"));\n+        newContainerId(appId1, 1), ImmutableSet.of(\"hbase-m\"));\n     Assert.assertEquals(1L, tm.getAllocationTagsWithCount(n0r1.getNodeID())\n         .get(\"hbase-m\").longValue());\n     Assert.assertEquals(1L, tm.getAllocationTagsWithCount(n2r2.getNodeID())\n@@ -589,7 +590,7 @@ public void testANDConstraintAssignment()\n      */\n     for (int i=0; i<4; i++) {\n       tm.addContainer(n1r1.getNodeID(),\n-          newContainerId(appId1), ImmutableSet.of(\"spark\"));\n+          newContainerId(appId1, i+2), ImmutableSet.of(\"spark\"));\n     }\n     Assert.assertEquals(4L, tm.getAllocationTagsWithCount(n1r1.getNodeID())\n         .get(\"spark\").longValue());\n@@ -633,19 +634,19 @@ public void testGlobalAppConstraints()\n      *  n3: \"\"\n      */\n     tm.addContainer(n0r1.getNodeID(),\n-        newContainerId(application1), ImmutableSet.of(\"A\"));\n+        newContainerId(application1, 0), ImmutableSet.of(\"A\"));\n     tm.addContainer(n0r1.getNodeID(),\n-        newContainerId(application2), ImmutableSet.of(\"A\"));\n+        newContainerId(application2, 1), ImmutableSet.of(\"A\"));\n     tm.addContainer(n1r1.getNodeID(),\n-        newContainerId(application3), ImmutableSet.of(\"A\"));\n+        newContainerId(application3, 2), ImmutableSet.of(\"A\"));\n     tm.addContainer(n1r1.getNodeID(),\n-        newContainerId(application3), ImmutableSet.of(\"A\"));\n+        newContainerId(application3, 3), ImmutableSet.of(\"A\"));\n     tm.addContainer(n1r1.getNodeID(),\n-        newContainerId(application3), ImmutableSet.of(\"A\"));\n+        newContainerId(application3, 4), ImmutableSet.of(\"A\"));\n     tm.addContainer(n2r2.getNodeID(),\n-        newContainerId(application1), ImmutableSet.of(\"A\"));\n+        newContainerId(application1, 5), ImmutableSet.of(\"A\"));\n     tm.addContainer(n2r2.getNodeID(),\n-        newContainerId(application1), ImmutableSet.of(\"A\"));\n+        newContainerId(application1, 6), ImmutableSet.of(\"A\"));\n \n     SchedulerNode schedulerNode0 = newSchedulerNode(n0r1.getHostName(),\n         n0r1.getRackName(), n0r1.getNodeID());\n@@ -888,9 +889,9 @@ public void testInterAppConstraintsByAppID()\n      *  n3: \"\"\n      */\n     tm.addContainer(n0r1.getNodeID(),\n-        newContainerId(application1), ImmutableSet.of(\"hbase-m\"));\n+        newContainerId(application1, 0), ImmutableSet.of(\"hbase-m\"));\n     tm.addContainer(n2r2.getNodeID(),\n-        newContainerId(application1), ImmutableSet.of(\"hbase-m\"));\n+        newContainerId(application1, 1), ImmutableSet.of(\"hbase-m\"));\n     Assert.assertEquals(1L, tm.getAllocationTagsWithCount(n0r1.getNodeID())\n         .get(\"hbase-m\").longValue());\n     Assert.assertEquals(1L, tm.getAllocationTagsWithCount(n2r2.getNodeID())\n@@ -958,7 +959,7 @@ public void testInterAppConstraintsByAppID()\n      *  n3: \"\"\n      */\n     tm.addContainer(n0r1.getNodeID(),\n-        newContainerId(application3), ImmutableSet.of(\"hbase-m\"));\n+        newContainerId(application3, 0), ImmutableSet.of(\"hbase-m\"));\n \n     // Anti-affinity to self/hbase-m\n     Assert.assertFalse(PlacementConstraintsUtil",
                "raw_url": "https://github.com/apache/hadoop/raw/08d5060605af81a3d6048044176dc656c0dad56c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/constraint/TestPlacementConstraintsUtil.java",
                "sha": "5dbdc8a1b9468964a8d3474d467a4d20bffeca5d",
                "status": "modified"
            }
        ],
        "message": "YARN-8521. NPE in AllocationTagsManager when a container is removed more than once. Contributed by Weiwei Yang.",
        "parent": "https://github.com/apache/hadoop/commit/f5dbbfe2e97a8c11e3df0f95ae4a493f11fdbc28",
        "repo": "hadoop",
        "unit_tests": [
            "TestAllocationTagsManager.java"
        ]
    },
    "hadoop_0ab3f9d": {
        "bug_id": "hadoop_0ab3f9d",
        "commit": "https://github.com/apache/hadoop/commit/0ab3f9d56465bf31668159c562305a3b8222004c",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=0ab3f9d56465bf31668159c562305a3b8222004c",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -1240,6 +1240,9 @@ Release 2.8.0 - UNRELEASED\n     HADOOP-11628. SPNEGO auth does not work with CNAMEs in JDK8.\n     (Daryn Sharp via stevel).\n \n+    HADOOP-10941. Proxy user verification NPEs if remote host is unresolvable.\n+    (Benoy Antony via stevel).\n+\n   OPTIMIZATIONS\n \n     HADOOP-12051. ProtobufRpcEngine.invoke() should use Exception.toString()",
                "raw_url": "https://github.com/apache/hadoop/raw/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "2c9e86a5f0ea63a8114c1b83c547b9835511906e",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java?ref=0ab3f9d56465bf31668159c562305a3b8222004c",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java",
                "patch": "@@ -108,6 +108,10 @@ public Configuration getConf() {\n   public void authorize(UserGroupInformation user, \n       String remoteAddress) throws AuthorizationException {\n     \n+    if (user == null) {\n+      throw new IllegalArgumentException(\"user is null.\");\n+    }\n+\n     UserGroupInformation realUser = user.getRealUser();\n     if (realUser == null) {\n       return;",
                "raw_url": "https://github.com/apache/hadoop/raw/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java",
                "sha": "26cd7ab2614a61100223fc6b3cb45879695d3ec6",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/MachineList.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/MachineList.java?ref=0ab3f9d56465bf31668159c562305a3b8222004c",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/MachineList.java",
                "patch": "@@ -18,7 +18,6 @@\n package org.apache.hadoop.util;\n \n import java.net.InetAddress;\n-\n import java.net.UnknownHostException;\n import java.util.ArrayList;\n import java.util.Collection;\n@@ -141,6 +140,10 @@ public boolean includes(String ipAddress) {\n       return true;\n     }\n     \n+    if (ipAddress == null) {\n+      throw new IllegalArgumentException(\"ipAddress is null.\");\n+    }\n+\n     //check in the set of ipAddresses\n     if ((ipAddresses != null) && ipAddresses.contains(ipAddress)) {\n       return true;",
                "raw_url": "https://github.com/apache/hadoop/raw/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/MachineList.java",
                "sha": "2e6c079d0f2470699ab6cbe282a98e47e8c27697",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/hadoop/blob/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/authorize/TestProxyUsers.java",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/authorize/TestProxyUsers.java?ref=0ab3f9d56465bf31668159c562305a3b8222004c",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/authorize/TestProxyUsers.java",
                "patch": "@@ -334,6 +334,45 @@ public void testIPRange() {\n     assertNotAuthorized(proxyUserUgi, \"10.221.0.0\");\n   }\n \n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNullUser() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(\n+        DefaultImpersonationProvider.getTestProvider().\n+            getProxySuperuserGroupConfKey(REAL_USER_NAME),\n+        \"*\");\n+    conf.set(\n+        DefaultImpersonationProvider.getTestProvider().\n+            getProxySuperuserIpConfKey(REAL_USER_NAME),\n+        PROXY_IP_RANGE);\n+    ProxyUsers.refreshSuperUserGroupsConfiguration(conf);\n+    // user is null\n+    ProxyUsers.authorize(null, \"10.222.0.0\");\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNullIpAddress() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.set(\n+        DefaultImpersonationProvider.getTestProvider().\n+            getProxySuperuserGroupConfKey(REAL_USER_NAME),\n+        \"*\");\n+    conf.set(\n+        DefaultImpersonationProvider.getTestProvider().\n+            getProxySuperuserIpConfKey(REAL_USER_NAME),\n+        PROXY_IP_RANGE);\n+    ProxyUsers.refreshSuperUserGroupsConfiguration(conf);\n+\n+    // First try proxying a group that's allowed\n+    UserGroupInformation realUserUgi = UserGroupInformation\n+        .createRemoteUser(REAL_USER_NAME);\n+    UserGroupInformation proxyUserUgi = UserGroupInformation.createProxyUserForTesting(\n+        PROXY_USER_NAME, realUserUgi, GROUP_NAMES);\n+\n+    // remote address is null\n+    ProxyUsers.authorize(proxyUserUgi, null);\n+  }\n+\n   @Test\n   public void testWithDuplicateProxyGroups() throws Exception {\n     Configuration conf = new Configuration();",
                "raw_url": "https://github.com/apache/hadoop/raw/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/authorize/TestProxyUsers.java",
                "sha": "577f11b92961a2b70b059b84badd83dd1f1dfa54",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestMachineList.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestMachineList.java?ref=0ab3f9d56465bf31668159c562305a3b8222004c",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestMachineList.java",
                "patch": "@@ -176,7 +176,15 @@ public void testCIDRs() {\n \n     //test for exclusion with an unknown IP\n     assertFalse(ml.includes(\"10.119.103.111\"));\n+  }\n+\n+  @Test(expected = IllegalArgumentException.class)\n+  public void testNullIpAddress() {\n+    //create MachineList with a list of of ip ranges specified in CIDR format\n+    MachineList ml = new MachineList(CIDR_LIST);\n \n+    //test for exclusion with a null IP\n+    assertFalse(ml.includes(null));\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/0ab3f9d56465bf31668159c562305a3b8222004c/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestMachineList.java",
                "sha": "d721c29530f1769d8f0a0c75982f6a6f07873a01",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10941. Proxy user verification NPEs if remote host is unresolvable (Benoy Antony via stevel).",
        "parent": "https://github.com/apache/hadoop/commit/e286512a7143427f2975ec92cdc4fad0a093a456",
        "repo": "hadoop",
        "unit_tests": [
            "TestDefaultImpersonationProvider.java",
            "TestMachineList.java"
        ]
    },
    "hadoop_0ad6cdd": {
        "bug_id": "hadoop_0ad6cdd",
        "commit": "https://github.com/apache/hadoop/commit/0ad6cdd03149eefbbbeba5df633bae65d4ee8916",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt?ref=0ad6cdd03149eefbbbeba5df633bae65d4ee8916",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "patch": "@@ -69,3 +69,5 @@ HDFS-5535 subtasks:\n     HDFS-5987. Fix findbugs warnings in Rolling Upgrade branch. (seztszwo via\n     Arpit Agarwal)\n \n+    HDFS-5992. Fix NPE in MD5FileUtils and update editsStored for\n+    TestOfflineEditsViewer.  (szetszwo)",
                "raw_url": "https://github.com/apache/hadoop/raw/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "sha": "4c567902c863adaa5719b632106cb992a0247b66",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/MD5FileUtils.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/MD5FileUtils.java?ref=0ad6cdd03149eefbbbeba5df633bae65d4ee8916",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/MD5FileUtils.java",
                "patch": "@@ -20,6 +20,7 @@\n import java.io.BufferedReader;\n import java.io.File;\n import java.io.FileInputStream;\n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.io.InputStream;\n import java.io.InputStreamReader;\n@@ -72,10 +73,6 @@ public static void verifySavedMD5(File dataFile, MD5Hash expectedMD5)\n    *   where group(1) is the md5 string and group(2) is the data file path.\n    */\n   private static Matcher readStoredMd5(File md5File) throws IOException {\n-    if (!md5File.exists()) {\n-      return null;\n-    }\n-    \n     BufferedReader reader =\n         new BufferedReader(new InputStreamReader(new FileInputStream(\n             md5File), Charsets.UTF_8));\n@@ -105,6 +102,10 @@ private static Matcher readStoredMd5(File md5File) throws IOException {\n    */\n   public static MD5Hash readStoredMd5ForFile(File dataFile) throws IOException {\n     final File md5File = getDigestFileForFile(dataFile);\n+    if (!md5File.exists()) {\n+      return null;\n+    }\n+\n     final Matcher matcher = readStoredMd5(md5File);\n     String storedHash = matcher.group(1);\n     File referencedFile = new File(matcher.group(2));\n@@ -165,6 +166,10 @@ private static void saveMD5File(File dataFile, String digestString)\n   public static void renameMD5File(File oldDataFile, File newDataFile)\n       throws IOException {\n     final File fromFile = getDigestFileForFile(oldDataFile);\n+    if (!fromFile.exists()) {\n+      throw new FileNotFoundException(fromFile + \" does not exist.\");\n+    }\n+\n     final String digestString = readStoredMd5(fromFile).group(1);\n     saveMD5File(newDataFile, digestString);\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/util/MD5FileUtils.java",
                "sha": "d87ffbf3154236c316020cdb0520469379d87d12",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/OfflineEditsViewerHelper.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/OfflineEditsViewerHelper.java?ref=0ad6cdd03149eefbbbeba5df633bae65d4ee8916",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/OfflineEditsViewerHelper.java",
                "patch": "@@ -130,7 +130,10 @@ private CheckpointSignature runOperations() throws IOException {\n     DFSTestUtil.runOperations(cluster, dfs, cluster.getConfiguration(0),\n         dfs.getDefaultBlockSize(), 0);\n \n+    // OP_ROLLING_UPGRADE_START\n     cluster.getNamesystem().getEditLog().logStartRollingUpgrade(Time.now());\n+    // OP_ROLLING_UPGRADE_FINALIZE\n+    cluster.getNamesystem().getEditLog().logFinalizeRollingUpgrade(Time.now());\n \n     // Force a roll so we get an OP_END_LOG_SEGMENT txn\n     return cluster.getNameNodeRpc().rollEditLog();",
                "raw_url": "https://github.com/apache/hadoop/raw/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/OfflineEditsViewerHelper.java",
                "sha": "c6364b174d0b0763c271e09fff004d0b2abdcab0",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TestOfflineEditsViewer.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TestOfflineEditsViewer.java?ref=0ad6cdd03149eefbbbeba5df633bae65d4ee8916",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TestOfflineEditsViewer.java",
                "patch": "@@ -95,6 +95,7 @@ public void testGenerated() throws IOException {\n     // edits generated by nnHelper (MiniDFSCluster), should have all op codes\n     // binary, XML, reparsed binary\n     String edits = nnHelper.generateEdits();\n+    LOG.info(\"Generated edits=\" + edits);\n     String editsParsedXml = folder.newFile(\"editsParsed.xml\").getAbsolutePath();\n     String editsReparsed = folder.newFile(\"editsParsed\").getAbsolutePath();\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/offlineEditsViewer/TestOfflineEditsViewer.java",
                "sha": "19d98ab6988a92753296f65452c519f0fb878653",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored?ref=0ad6cdd03149eefbbbeba5df633bae65d4ee8916",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored",
                "raw_url": "https://github.com/apache/hadoop/raw/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored",
                "sha": "6c17f4af4a9ec863a56a6b27dec867a38eb6210e",
                "status": "modified"
            },
            {
                "additions": 102,
                "blob_url": "https://github.com/apache/hadoop/blob/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored.xml",
                "changes": 578,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored.xml?ref=0ad6cdd03149eefbbbeba5df633bae65d4ee8916",
                "deletions": 476,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored.xml",
                "patch": "@@ -1,10 +1,6 @@\n <?xml version=\"1.0\" encoding=\"UTF-8\"?>\n <EDITS>\n-<<<<<<< .working\n-  <EDITS_VERSION>-50</EDITS_VERSION>\n-=======\n-  <EDITS_VERSION>-51</EDITS_VERSION>\n->>>>>>> .merge-right.r1559304\n+  <EDITS_VERSION>-55</EDITS_VERSION>\n   <RECORD>\n     <OPCODE>OP_START_LOG_SEGMENT</OPCODE>\n     <DATA>\n@@ -17,13 +13,8 @@\n       <TXID>2</TXID>\n       <DELEGATION_KEY>\n         <KEY_ID>1</KEY_ID>\n-<<<<<<< .working\n-        <EXPIRY_DATE>1389421314720</EXPIRY_DATE>\n-        <KEY>d2a03d66ebfac521</KEY>\n-=======\n-        <EXPIRY_DATE>1390519460949</EXPIRY_DATE>\n-        <KEY>dc8d30edc97df67d</KEY>\n->>>>>>> .merge-right.r1559304\n+        <EXPIRY_DATE>1393648283650</EXPIRY_DATE>\n+        <KEY>76e6d2854a753680</KEY>\n       </DELEGATION_KEY>\n     </DATA>\n   </RECORD>\n@@ -33,13 +24,8 @@\n       <TXID>3</TXID>\n       <DELEGATION_KEY>\n         <KEY_ID>2</KEY_ID>\n-<<<<<<< .working\n-        <EXPIRY_DATE>1389421314722</EXPIRY_DATE>\n-        <KEY>ef94532092f55aef</KEY>\n-=======\n-        <EXPIRY_DATE>1390519460952</EXPIRY_DATE>\n-        <KEY>096bc20b6debed03</KEY>\n->>>>>>> .merge-right.r1559304\n+        <EXPIRY_DATE>1393648283653</EXPIRY_DATE>\n+        <KEY>939fb7b875c956cd</KEY>\n       </DELEGATION_KEY>\n     </DATA>\n   </RECORD>\n@@ -51,36 +37,18 @@\n       <INODEID>16386</INODEID>\n       <PATH>/file_create</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115316</MTIME>\n-      <ATIME>1388730115316</ATIME>\n-=======\n-      <MTIME>1389828264873</MTIME>\n-      <ATIME>1389828264873</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084379</MTIME>\n+      <ATIME>1392957084379</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>6</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>9</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>7</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -91,22 +59,13 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_create</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115327</MTIME>\n-      <ATIME>1388730115316</ATIME>\n-=======\n-      <MTIME>1389828265699</MTIME>\n-      <ATIME>1389828264873</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084397</MTIME>\n+      <ATIME>1392957084379</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -119,15 +78,9 @@\n       <LENGTH>0</LENGTH>\n       <SRC>/file_create</SRC>\n       <DST>/file_moved</DST>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115331</TIMESTAMP>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>8</RPC_CALLID>\n-=======\n-      <TIMESTAMP>1389828265705</TIMESTAMP>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>11</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084400</TIMESTAMP>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>9</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -136,15 +89,9 @@\n       <TXID>7</TXID>\n       <LENGTH>0</LENGTH>\n       <PATH>/file_moved</PATH>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115336</TIMESTAMP>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>9</RPC_CALLID>\n-=======\n-      <TIMESTAMP>1389828265712</TIMESTAMP>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>12</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084413</TIMESTAMP>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>10</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -154,17 +101,9 @@\n       <LENGTH>0</LENGTH>\n       <INODEID>16387</INODEID>\n       <PATH>/directory_mkdir</PATH>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115342</TIMESTAMP>\n-=======\n-      <TIMESTAMP>1389828265722</TIMESTAMP>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084419</TIMESTAMP>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>493</MODE>\n       </PERMISSION_STATUS>\n@@ -197,13 +136,8 @@\n       <TXID>12</TXID>\n       <SNAPSHOTROOT>/directory_mkdir</SNAPSHOTROOT>\n       <SNAPSHOTNAME>snapshot1</SNAPSHOTNAME>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>14</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>17</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>15</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -213,13 +147,8 @@\n       <SNAPSHOTROOT>/directory_mkdir</SNAPSHOTROOT>\n       <SNAPSHOTOLDNAME>snapshot1</SNAPSHOTOLDNAME>\n       <SNAPSHOTNEWNAME>snapshot2</SNAPSHOTNEWNAME>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>15</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>18</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>16</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -228,13 +157,8 @@\n       <TXID>14</TXID>\n       <SNAPSHOTROOT>/directory_mkdir</SNAPSHOTROOT>\n       <SNAPSHOTNAME>snapshot2</SNAPSHOTNAME>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>16</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>19</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>17</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -245,36 +169,18 @@\n       <INODEID>16388</INODEID>\n       <PATH>/file_create</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115362</MTIME>\n-      <ATIME>1388730115362</ATIME>\n-=======\n-      <MTIME>1389828265757</MTIME>\n-      <ATIME>1389828265757</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084440</MTIME>\n+      <ATIME>1392957084440</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>17</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>20</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>18</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -285,22 +191,13 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_create</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115363</MTIME>\n-      <ATIME>1388730115362</ATIME>\n-=======\n-      <MTIME>1389828265759</MTIME>\n-      <ATIME>1389828265757</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084441</MTIME>\n+      <ATIME>1392957084440</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -356,19 +253,10 @@\n       <LENGTH>0</LENGTH>\n       <SRC>/file_create</SRC>\n       <DST>/file_moved</DST>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115378</TIMESTAMP>\n-=======\n-      <TIMESTAMP>1389828265782</TIMESTAMP>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084455</TIMESTAMP>\n       <OPTIONS>NONE</OPTIONS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>24</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>27</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>25</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -379,36 +267,18 @@\n       <INODEID>16389</INODEID>\n       <PATH>/file_concat_target</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115382</MTIME>\n-      <ATIME>1388730115382</ATIME>\n-=======\n-      <MTIME>1389828265787</MTIME>\n-      <ATIME>1389828265787</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084459</MTIME>\n+      <ATIME>1392957084459</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>26</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>29</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>27</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -513,13 +383,8 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_concat_target</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115461</MTIME>\n-      <ATIME>1388730115382</ATIME>\n-=======\n-      <MTIME>1389828266540</MTIME>\n-      <ATIME>1389828265787</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084525</MTIME>\n+      <ATIME>1392957084459</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n@@ -539,11 +404,7 @@\n         <GENSTAMP>1003</GENSTAMP>\n       </BLOCK>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -557,36 +418,18 @@\n       <INODEID>16390</INODEID>\n       <PATH>/file_concat_0</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115463</MTIME>\n-      <ATIME>1388730115463</ATIME>\n-=======\n-      <MTIME>1389828266544</MTIME>\n-      <ATIME>1389828266544</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084527</MTIME>\n+      <ATIME>1392957084527</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>39</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>41</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>40</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -691,13 +534,8 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_concat_0</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115477</MTIME>\n-      <ATIME>1388730115463</ATIME>\n-=======\n-      <MTIME>1389828266569</MTIME>\n-      <ATIME>1389828266544</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084542</MTIME>\n+      <ATIME>1392957084527</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n@@ -717,11 +555,7 @@\n         <GENSTAMP>1006</GENSTAMP>\n       </BLOCK>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -735,36 +569,18 @@\n       <INODEID>16391</INODEID>\n       <PATH>/file_concat_1</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115479</MTIME>\n-      <ATIME>1388730115479</ATIME>\n-=======\n-      <MTIME>1389828266572</MTIME>\n-      <ATIME>1389828266572</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084544</MTIME>\n+      <ATIME>1392957084544</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>51</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>53</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>52</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -869,13 +685,8 @@\n       <INODEID>0</INODEID>\n       <PATH>/file_concat_1</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115495</MTIME>\n-      <ATIME>1388730115479</ATIME>\n-=======\n-      <MTIME>1389828266599</MTIME>\n-      <ATIME>1389828266572</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084559</MTIME>\n+      <ATIME>1392957084544</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n@@ -895,11 +706,7 @@\n         <GENSTAMP>1009</GENSTAMP>\n       </BLOCK>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n@@ -911,22 +718,13 @@\n       <TXID>56</TXID>\n       <LENGTH>0</LENGTH>\n       <TRG>/file_concat_target</TRG>\n-<<<<<<< .working\n-      <TIMESTAMP>1388730115498</TIMESTAMP>\n-=======\n-      <TIMESTAMP>1389828266603</TIMESTAMP>\n->>>>>>> .merge-right.r1559304\n+      <TIMESTAMP>1392957084561</TIMESTAMP>\n       <SOURCES>\n         <SOURCE1>/file_concat_0</SOURCE1>\n         <SOURCE2>/file_concat_1</SOURCE2>\n       </SOURCES>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>62</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>64</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>63</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -937,190 +735,37 @@\n       <INODEID>16392</INODEID>\n       <PATH>/file_symlink</PATH>\n       <VALUE>/file_concat_target</VALUE>\n-<<<<<<< .working\n-      <MTIME>1388730115502</MTIME>\n-      <ATIME>1388730115502</ATIME>\n-=======\n-      <MTIME>1389828266633</MTIME>\n-      <ATIME>1389828266633</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084564</MTIME>\n+      <ATIME>1392957084564</ATIME>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>511</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>63</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>66</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>64</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n     <OPCODE>OP_ADD</OPCODE>\n     <DATA>\n       <TXID>58</TXID>\n-<<<<<<< .working\n-      <DELEGATION_TOKEN_IDENTIFIER>\n-        <KIND>HDFS_DELEGATION_TOKEN</KIND>\n-        <SEQUENCE_NUMBER>1</SEQUENCE_NUMBER>\n-        <OWNER>szetszwo</OWNER>\n-        <RENEWER>JobTracker</RENEWER>\n-        <REALUSER></REALUSER>\n-        <ISSUE_DATE>1388730115505</ISSUE_DATE>\n-        <MAX_DATE>1389334915505</MAX_DATE>\n-        <MASTER_KEY_ID>2</MASTER_KEY_ID>\n-      </DELEGATION_TOKEN_IDENTIFIER>\n-      <EXPIRY_TIME>1388816515505</EXPIRY_TIME>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_RENEW_DELEGATION_TOKEN</OPCODE>\n-    <DATA>\n-      <TXID>59</TXID>\n-      <DELEGATION_TOKEN_IDENTIFIER>\n-        <KIND>HDFS_DELEGATION_TOKEN</KIND>\n-        <SEQUENCE_NUMBER>1</SEQUENCE_NUMBER>\n-        <OWNER>szetszwo</OWNER>\n-        <RENEWER>JobTracker</RENEWER>\n-        <REALUSER></REALUSER>\n-        <ISSUE_DATE>1388730115505</ISSUE_DATE>\n-        <MAX_DATE>1389334915505</MAX_DATE>\n-        <MASTER_KEY_ID>2</MASTER_KEY_ID>\n-      </DELEGATION_TOKEN_IDENTIFIER>\n-      <EXPIRY_TIME>1388816515564</EXPIRY_TIME>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_CANCEL_DELEGATION_TOKEN</OPCODE>\n-    <DATA>\n-      <TXID>60</TXID>\n-      <DELEGATION_TOKEN_IDENTIFIER>\n-        <KIND>HDFS_DELEGATION_TOKEN</KIND>\n-        <SEQUENCE_NUMBER>1</SEQUENCE_NUMBER>\n-        <OWNER>szetszwo</OWNER>\n-        <RENEWER>JobTracker</RENEWER>\n-        <REALUSER></REALUSER>\n-        <ISSUE_DATE>1388730115505</ISSUE_DATE>\n-        <MAX_DATE>1389334915505</MAX_DATE>\n-        <MASTER_KEY_ID>2</MASTER_KEY_ID>\n-      </DELEGATION_TOKEN_IDENTIFIER>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_ADD_CACHE_POOL</OPCODE>\n-    <DATA>\n-      <TXID>61</TXID>\n-      <POOLNAME>poolparty</POOLNAME>\n-      <OWNERNAME>szetszwo</OWNERNAME>\n-      <GROUPNAME>staff</GROUPNAME>\n-      <MODE>493</MODE>\n-      <LIMIT>9223372036854775807</LIMIT>\n-      <MAXRELATIVEEXPIRY>2305843009213693951</MAXRELATIVEEXPIRY>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>67</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_MODIFY_CACHE_POOL</OPCODE>\n-    <DATA>\n-      <TXID>62</TXID>\n-      <POOLNAME>poolparty</POOLNAME>\n-      <OWNERNAME>carlton</OWNERNAME>\n-      <GROUPNAME>party</GROUPNAME>\n-      <MODE>448</MODE>\n-      <LIMIT>1989</LIMIT>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>68</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_ADD_CACHE_DIRECTIVE</OPCODE>\n-    <DATA>\n-      <TXID>63</TXID>\n-      <ID>1</ID>\n-      <PATH>/bar</PATH>\n-      <REPLICATION>1</REPLICATION>\n-      <POOL>poolparty</POOL>\n-      <EXPIRATION>2305844397943809533</EXPIRATION>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>69</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_MODIFY_CACHE_DIRECTIVE</OPCODE>\n-    <DATA>\n-      <TXID>64</TXID>\n-      <ID>1</ID>\n-      <PATH>/bar2</PATH>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>70</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_REMOVE_CACHE_DIRECTIVE</OPCODE>\n-    <DATA>\n-      <TXID>65</TXID>\n-      <ID>1</ID>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>71</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_REMOVE_CACHE_POOL</OPCODE>\n-    <DATA>\n-      <TXID>66</TXID>\n-      <POOLNAME>poolparty</POOLNAME>\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>72</RPC_CALLID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-    <OPCODE>OP_ADD</OPCODE>\n-    <DATA>\n-      <TXID>67</TXID>\n-=======\n->>>>>>> .merge-right.r1559304\n       <LENGTH>0</LENGTH>\n       <INODEID>16393</INODEID>\n       <PATH>/hard-lease-recovery-test</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730115596</MTIME>\n-      <ATIME>1388730115596</ATIME>\n-=======\n-      <MTIME>1389828266637</MTIME>\n-      <ATIME>1389828266637</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957084567</MTIME>\n+      <ATIME>1392957084567</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n-<<<<<<< .working\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_381408282_1</CLIENT_NAME>\n-=======\n-      <CLIENT_NAME>DFSClient_NONMAPREDUCE_16108824_1</CLIENT_NAME>\n->>>>>>> .merge-right.r1559304\n+      <CLIENT_NAME>DFSClient_NONMAPREDUCE_-1178237747_1</CLIENT_NAME>\n       <CLIENT_MACHINE>127.0.0.1</CLIENT_MACHINE>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n-<<<<<<< .working\n-      <RPC_CLIENTID>8205c453-0c7f-4b25-955a-7786e56bce86</RPC_CLIENTID>\n-      <RPC_CALLID>73</RPC_CALLID>\n-=======\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>67</RPC_CALLID>\n->>>>>>> .merge-right.r1559304\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>65</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -1175,38 +820,22 @@\n   <RECORD>\n     <OPCODE>OP_REASSIGN_LEASE</OPCODE>\n     <DATA>\n-<<<<<<< .working\n-      <TXID>73</TXID>\n-      <LEASEHOLDER>DFSClient_NONMAPREDUCE_381408282_1</LEASEHOLDER>\n-=======\n       <TXID>64</TXID>\n-      <LEASEHOLDER>DFSClient_NONMAPREDUCE_16108824_1</LEASEHOLDER>\n->>>>>>> .merge-right.r1559304\n+      <LEASEHOLDER>DFSClient_NONMAPREDUCE_-1178237747_1</LEASEHOLDER>\n       <PATH>/hard-lease-recovery-test</PATH>\n       <NEWHOLDER>HDFS_NameNode</NEWHOLDER>\n     </DATA>\n   </RECORD>\n   <RECORD>\n-<<<<<<< .working\n-    <OPCODE>OP_CLOSE</OPCODE>\n-    <DATA>\n-      <TXID>74</TXID>\n-=======\n     <OPCODE>OP_CLOSE</OPCODE>\n     <DATA>\n       <TXID>65</TXID>\n->>>>>>> .merge-right.r1559304\n       <LENGTH>0</LENGTH>\n       <INODEID>0</INODEID>\n       <PATH>/hard-lease-recovery-test</PATH>\n       <REPLICATION>1</REPLICATION>\n-<<<<<<< .working\n-      <MTIME>1388730118281</MTIME>\n-      <ATIME>1388730115596</ATIME>\n-=======\n-      <MTIME>1389828269751</MTIME>\n-      <ATIME>1389828266637</ATIME>\n->>>>>>> .merge-right.r1559304\n+      <MTIME>1392957087263</MTIME>\n+      <ATIME>1392957084567</ATIME>\n       <BLOCKSIZE>512</BLOCKSIZE>\n       <CLIENT_NAME></CLIENT_NAME>\n       <CLIENT_MACHINE></CLIENT_MACHINE>\n@@ -1216,36 +845,24 @@\n         <GENSTAMP>1011</GENSTAMP>\n       </BLOCK>\n       <PERMISSION_STATUS>\n-<<<<<<< .working\n         <USERNAME>szetszwo</USERNAME>\n-=======\n-        <USERNAME>jing</USERNAME>\n->>>>>>> .merge-right.r1559304\n         <GROUPNAME>supergroup</GROUPNAME>\n         <MODE>420</MODE>\n       </PERMISSION_STATUS>\n     </DATA>\n   </RECORD>\n   <RECORD>\n-<<<<<<< .working\n-    <OPCODE>OP_UPGRADE_MARKER</OPCODE>\n-    <DATA>\n-      <TXID>75</TXID>\n-    </DATA>\n-  </RECORD>\n-  <RECORD>\n-=======\n     <OPCODE>OP_ADD_CACHE_POOL</OPCODE>\n     <DATA>\n       <TXID>66</TXID>\n       <POOLNAME>pool1</POOLNAME>\n-      <OWNERNAME>jing</OWNERNAME>\n+      <OWNERNAME>szetszwo</OWNERNAME>\n       <GROUPNAME>staff</GROUPNAME>\n       <MODE>493</MODE>\n       <LIMIT>9223372036854775807</LIMIT>\n       <MAXRELATIVEEXPIRY>2305843009213693951</MAXRELATIVEEXPIRY>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>74</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>72</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -1254,8 +871,8 @@\n       <TXID>67</TXID>\n       <POOLNAME>pool1</POOLNAME>\n       <LIMIT>99</LIMIT>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>75</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>73</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -1266,9 +883,9 @@\n       <PATH>/path</PATH>\n       <REPLICATION>1</REPLICATION>\n       <POOL>pool1</POOL>\n-      <EXPIRATION>2305844399041964876</EXPIRATION>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>76</RPC_CALLID>\n+      <EXPIRATION>2305844402170781554</EXPIRATION>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>74</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n@@ -1277,44 +894,53 @@\n       <TXID>69</TXID>\n       <ID>1</ID>\n       <REPLICATION>2</REPLICATION>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>77</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>75</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n     <OPCODE>OP_REMOVE_CACHE_DIRECTIVE</OPCODE>\n     <DATA>\n       <TXID>70</TXID>\n       <ID>1</ID>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>78</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>76</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n     <OPCODE>OP_REMOVE_CACHE_POOL</OPCODE>\n     <DATA>\n       <TXID>71</TXID>\n       <POOLNAME>pool1</POOLNAME>\n-      <RPC_CLIENTID>b5928e80-e373-4807-a688-f94483d08ce5</RPC_CLIENTID>\n-      <RPC_CALLID>79</RPC_CALLID>\n+      <RPC_CLIENTID>ad7d1b9e-e5d3-4d8d-ae1a-060f579be11e</RPC_CLIENTID>\n+      <RPC_CALLID>77</RPC_CALLID>\n     </DATA>\n   </RECORD>\n   <RECORD>\n->>>>>>> .merge-right.r1559304\n-    <OPCODE>OP_END_LOG_SEGMENT</OPCODE>\n+    <OPCODE>OP_SET_ACL</OPCODE>\n     <DATA>\n-<<<<<<< .working\n-      <TXID>76</TXID>\n-=======\n       <TXID>72</TXID>\n->>>>>>> .merge-right.r1559304\n+      <SRC>/file_concat_target</SRC>\n     </DATA>\n   </RECORD>\n   <RECORD>\n-    <OPCODE>OP_SET_ACL</OPCODE>\n+    <OPCODE>OP_ROLLING_UPGRADE_START</OPCODE>\n     <DATA>\n       <TXID>73</TXID>\n-      <SRC>/file_set_acl</SRC>\n+      <STARTTIME>1392957087621</STARTTIME>\n+    </DATA>\n+  </RECORD>\n+  <RECORD>\n+    <OPCODE>OP_ROLLING_UPGRADE_FINALIZE</OPCODE>\n+    <DATA>\n+      <TXID>74</TXID>\n+      <FINALIZETIME>1392957087621</FINALIZETIME>\n+    </DATA>\n+  </RECORD>\n+  <RECORD>\n+    <OPCODE>OP_END_LOG_SEGMENT</OPCODE>\n+    <DATA>\n+      <TXID>75</TXID>\n     </DATA>\n   </RECORD>\n </EDITS>",
                "raw_url": "https://github.com/apache/hadoop/raw/0ad6cdd03149eefbbbeba5df633bae65d4ee8916/hadoop-hdfs-project/hadoop-hdfs/src/test/resources/editsStored.xml",
                "sha": "b3115591d0327d924a4eb098147c8da374327d5b",
                "status": "modified"
            }
        ],
        "message": "HDFS-5992. Fix NPE in MD5FileUtils and update editsStored for TestOfflineEditsViewer.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1570690 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/37afb4d6834f04c511a977d6b4205930293aa4a0",
        "repo": "hadoop",
        "unit_tests": [
            "TestMD5FileUtils.java"
        ]
    },
    "hadoop_0cc98ae": {
        "bug_id": "hadoop_0cc98ae",
        "commit": "https://github.com/apache/hadoop/commit/0cc98ae0ec69419ded066f3f7decf59728b35e9d",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/0cc98ae0ec69419ded066f3f7decf59728b35e9d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java?ref=0cc98ae0ec69419ded066f3f7decf59728b35e9d",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "patch": "@@ -367,6 +367,8 @@ protected void serviceInit(Configuration conf) throws Exception {\n     \n     this.aclsManager = new ApplicationACLsManager(conf);\n \n+    this.dirsHandler = new LocalDirsHandlerService(metrics);\n+\n     boolean isDistSchedulingEnabled =\n         conf.getBoolean(YarnConfiguration.DIST_SCHEDULING_ENABLED,\n             YarnConfiguration.DEFAULT_DIST_SCHEDULING_ENABLED);\n@@ -390,7 +392,6 @@ protected void serviceInit(Configuration conf) throws Exception {\n     // NodeManager level dispatcher\n     this.dispatcher = new AsyncDispatcher(\"NM Event dispatcher\");\n \n-    dirsHandler = new LocalDirsHandlerService(metrics);\n     nodeHealthChecker =\n         new NodeHealthCheckerService(\n             getNodeHealthScriptRunner(conf), dirsHandler);",
                "raw_url": "https://github.com/apache/hadoop/raw/0cc98ae0ec69419ded066f3f7decf59728b35e9d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "sha": "bddc7c34ee7afd05a10babf81e762e32008f8cca",
                "status": "modified"
            }
        ],
        "message": "YARN-7396. NPE when accessing container logs due to null dirsHandler. Contributed by Jonathan Hung",
        "parent": "https://github.com/apache/hadoop/commit/7a49ddfdde2e2a7b407f4a62a42d97bfe456075a",
        "repo": "hadoop",
        "unit_tests": [
            "TestNodeManager.java"
        ]
    },
    "hadoop_0d89859": {
        "bug_id": "hadoop_0d89859",
        "commit": "https://github.com/apache/hadoop/commit/0d89859b51157078cc504ac81dc8aa75ce6b1782",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/0d89859b51157078cc504ac81dc8aa75ce6b1782/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=0d89859b51157078cc504ac81dc8aa75ce6b1782",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -266,6 +266,9 @@ Release 2.7.0 - UNRELEASED\n     YARN-2920. Changed CapacityScheduler to kill containers on nodes where\n     node labels are changed. (Wangda Tan via jianhe)\n \n+    YARN-2340. Fixed NPE when queue is stopped during RM restart.\n+    (Rohith Sharmaks via jianhe)\n+\n Release 2.6.0 - 2014-11-18\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/0d89859b51157078cc504ac81dc8aa75ce6b1782/hadoop-yarn-project/CHANGES.txt",
                "sha": "cb2fc24ae23de919aeaf111bc642edce236117c5",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/0d89859b51157078cc504ac81dc8aa75ce6b1782/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java?ref=0d89859b51157078cc504ac81dc8aa75ce6b1782",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "patch": "@@ -706,11 +706,14 @@ private synchronized void addApplication(ApplicationId applicationId,\n     try {\n       queue.submitApplication(applicationId, user, queueName);\n     } catch (AccessControlException ace) {\n-      LOG.info(\"Failed to submit application \" + applicationId + \" to queue \"\n-          + queueName + \" from user \" + user, ace);\n-      this.rmContext.getDispatcher().getEventHandler()\n-          .handle(new RMAppRejectedEvent(applicationId, ace.toString()));\n-      return;\n+      // Ignore the exception for recovered app as the app was previously accepted\n+      if (!isAppRecovering) {\n+        LOG.info(\"Failed to submit application \" + applicationId + \" to queue \"\n+            + queueName + \" from user \" + user, ace);\n+        this.rmContext.getDispatcher().getEventHandler()\n+            .handle(new RMAppRejectedEvent(applicationId, ace.toString()));\n+        return;\n+      }\n     }\n     // update the metrics\n     queue.getMetrics().submitApp(user);",
                "raw_url": "https://github.com/apache/hadoop/raw/0d89859b51157078cc504ac81dc8aa75ce6b1782/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "sha": "3648c5436d7a014d4fad12ff340aa8ee3d4bdac3",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/0d89859b51157078cc504ac81dc8aa75ce6b1782/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java?ref=0d89859b51157078cc504ac81dc8aa75ce6b1782",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java",
                "patch": "@@ -500,6 +500,8 @@ public void testCapacitySchedulerRecovery() throws Exception {\n     rm1.clearQueueMetrics(app1_2);\n     rm1.clearQueueMetrics(app2);\n \n+    csConf.set(\"yarn.scheduler.capacity.root.Default.QueueB.state\", \"STOPPED\");\n+\n     // Re-start RM\n     rm2 = new MockRM(csConf, memStore);\n     rm2.start();",
                "raw_url": "https://github.com/apache/hadoop/raw/0d89859b51157078cc504ac81dc8aa75ce6b1782/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java",
                "sha": "842eaecad3202d1816163eda70495a0c5ec65bc7",
                "status": "modified"
            }
        ],
        "message": "YARN-2340. Fixed NPE when queue is stopped during RM restart. Contributed by Rohith Sharmaks",
        "parent": "https://github.com/apache/hadoop/commit/fdf042dfffa4d2474e3cac86cfb8fe9ee4648beb",
        "repo": "hadoop",
        "unit_tests": [
            "TestCapacityScheduler.java"
        ]
    },
    "hadoop_0d9804d": {
        "bug_id": "hadoop_0d9804d",
        "commit": "https://github.com/apache/hadoop/commit/0d9804dcef2eab5ebf84667d9ca49bb035d9a731",
        "file": [
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop/blob/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/clientrm/RouterClientRMService.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/clientrm/RouterClientRMService.java?ref=0d9804dcef2eab5ebf84667d9ca49bb035d9a731",
                "deletions": 27,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/clientrm/RouterClientRMService.java",
                "patch": "@@ -430,13 +430,15 @@ public GetAllResourceTypeInfoResponse getResourceTypeInfo(\n     return pipeline.getRootInterceptor().getResourceTypeInfo(request);\n   }\n \n-  private RequestInterceptorChainWrapper getInterceptorChain()\n+  @VisibleForTesting\n+  protected RequestInterceptorChainWrapper getInterceptorChain()\n       throws IOException {\n     String user = UserGroupInformation.getCurrentUser().getUserName();\n-    if (!userPipelineMap.containsKey(user)) {\n-      initializePipeline(user);\n+    RequestInterceptorChainWrapper chain = userPipelineMap.get(user);\n+    if (chain != null && chain.getRootInterceptor() != null) {\n+      return chain;\n     }\n-    return userPipelineMap.get(user);\n+    return initializePipeline(user);\n   }\n \n   /**\n@@ -503,36 +505,33 @@ protected ClientRequestInterceptor createRequestInterceptorChain() {\n    *\n    * @param user\n    */\n-  private void initializePipeline(String user) {\n-    RequestInterceptorChainWrapper chainWrapper = null;\n+  private RequestInterceptorChainWrapper initializePipeline(String user) {\n     synchronized (this.userPipelineMap) {\n       if (this.userPipelineMap.containsKey(user)) {\n         LOG.info(\"Request to start an already existing user: {}\"\n             + \" was received, so ignoring.\", user);\n-        return;\n+        return userPipelineMap.get(user);\n       }\n \n-      chainWrapper = new RequestInterceptorChainWrapper();\n-      this.userPipelineMap.put(user, chainWrapper);\n-    }\n-\n-    // We register the pipeline instance in the map first and then initialize it\n-    // later because chain initialization can be expensive and we would like to\n-    // release the lock as soon as possible to prevent other applications from\n-    // blocking when one application's chain is initializing\n-    LOG.info(\"Initializing request processing pipeline for application \"\n-        + \"for the user: {}\", user);\n-\n-    try {\n-      ClientRequestInterceptor interceptorChain =\n-          this.createRequestInterceptorChain();\n-      interceptorChain.init(user);\n-      chainWrapper.init(interceptorChain);\n-    } catch (Exception e) {\n-      synchronized (this.userPipelineMap) {\n-        this.userPipelineMap.remove(user);\n+      RequestInterceptorChainWrapper chainWrapper =\n+          new RequestInterceptorChainWrapper();\n+      try {\n+        // We should init the pipeline instance after it is created and then\n+        // add to the map, to ensure thread safe.\n+        LOG.info(\"Initializing request processing pipeline for application \"\n+            + \"for the user: {}\", user);\n+\n+        ClientRequestInterceptor interceptorChain =\n+            this.createRequestInterceptorChain();\n+        interceptorChain.init(user);\n+        chainWrapper.init(interceptorChain);\n+      } catch (Exception e) {\n+        LOG.error(\"Init ClientRequestInterceptor error for user: \" + user, e);\n+        throw e;\n       }\n-      throw e;\n+\n+      this.userPipelineMap.put(user, chainWrapper);\n+      return chainWrapper;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/clientrm/RouterClientRMService.java",
                "sha": "bbb8047d98d3c0775fa6354248256aae6a1de3f9",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hadoop/blob/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/rmadmin/RouterRMAdminService.java",
                "changes": 51,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/rmadmin/RouterRMAdminService.java?ref=0d9804dcef2eab5ebf84667d9ca49bb035d9a731",
                "deletions": 26,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/rmadmin/RouterRMAdminService.java",
                "patch": "@@ -165,13 +165,15 @@ protected void serviceStop() throws Exception {\n     return interceptorClassNames;\n   }\n \n-  private RequestInterceptorChainWrapper getInterceptorChain()\n+  @VisibleForTesting\n+  protected RequestInterceptorChainWrapper getInterceptorChain()\n       throws IOException {\n     String user = UserGroupInformation.getCurrentUser().getUserName();\n-    if (!userPipelineMap.containsKey(user)) {\n-      initializePipeline(user);\n+    RequestInterceptorChainWrapper chain = userPipelineMap.get(user);\n+    if (chain != null && chain.getRootInterceptor() != null) {\n+      return chain;\n     }\n-    return userPipelineMap.get(user);\n+    return initializePipeline(user);\n   }\n \n   /**\n@@ -239,35 +241,32 @@ protected RMAdminRequestInterceptor createRequestInterceptorChain() {\n    *\n    * @param user\n    */\n-  private void initializePipeline(String user) {\n-    RequestInterceptorChainWrapper chainWrapper = null;\n+  private RequestInterceptorChainWrapper initializePipeline(String user) {\n     synchronized (this.userPipelineMap) {\n       if (this.userPipelineMap.containsKey(user)) {\n         LOG.info(\"Request to start an already existing user: {}\"\n             + \" was received, so ignoring.\", user);\n-        return;\n+        return userPipelineMap.get(user);\n       }\n \n-      chainWrapper = new RequestInterceptorChainWrapper();\n-      this.userPipelineMap.put(user, chainWrapper);\n-    }\n-\n-    // We register the pipeline instance in the map first and then initialize it\n-    // later because chain initialization can be expensive and we would like to\n-    // release the lock as soon as possible to prevent other applications from\n-    // blocking when one application's chain is initializing\n-    LOG.info(\"Initializing request processing pipeline for the user: {}\", user);\n-\n-    try {\n-      RMAdminRequestInterceptor interceptorChain =\n-          this.createRequestInterceptorChain();\n-      interceptorChain.init(user);\n-      chainWrapper.init(interceptorChain);\n-    } catch (Exception e) {\n-      synchronized (this.userPipelineMap) {\n-        this.userPipelineMap.remove(user);\n+      RequestInterceptorChainWrapper chainWrapper =\n+          new RequestInterceptorChainWrapper();\n+      try {\n+        // We should init the pipeline instance after it is created and then\n+        // add to the map, to ensure thread safe.\n+        LOG.info(\"Initializing request processing pipeline for user: {}\", user);\n+\n+        RMAdminRequestInterceptor interceptorChain =\n+            this.createRequestInterceptorChain();\n+        interceptorChain.init(user);\n+        chainWrapper.init(interceptorChain);\n+      } catch (Exception e) {\n+        LOG.error(\"Init RMAdminRequestInterceptor error for user: \" + user, e);\n+        throw e;\n       }\n-      throw e;\n+\n+      this.userPipelineMap.put(user, chainWrapper);\n+      return chainWrapper;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/rmadmin/RouterRMAdminService.java",
                "sha": "ef30613f50c4bf250742e2d8787e292008385dfe",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hadoop/blob/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/RouterWebServices.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/RouterWebServices.java?ref=0d9804dcef2eab5ebf84667d9ca49bb035d9a731",
                "deletions": 25,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/RouterWebServices.java",
                "patch": "@@ -173,10 +173,11 @@ protected RequestInterceptorChainWrapper getInterceptorChain(\n     } catch (IOException e) {\n       LOG.error(\"Cannot get user: {}\", e.getMessage());\n     }\n-    if (!userPipelineMap.containsKey(user)) {\n-      initializePipeline(user);\n+    RequestInterceptorChainWrapper chain = userPipelineMap.get(user);\n+    if (chain != null && chain.getRootInterceptor() != null) {\n+      return chain;\n     }\n-    return userPipelineMap.get(user);\n+    return initializePipeline(user);\n   }\n \n   /**\n@@ -242,35 +243,32 @@ protected RESTRequestInterceptor createRequestInterceptorChain() {\n    *\n    * @param user\n    */\n-  private void initializePipeline(String user) {\n-    RequestInterceptorChainWrapper chainWrapper = null;\n+  private RequestInterceptorChainWrapper initializePipeline(String user) {\n     synchronized (this.userPipelineMap) {\n       if (this.userPipelineMap.containsKey(user)) {\n         LOG.info(\"Request to start an already existing user: {}\"\n             + \" was received, so ignoring.\", user);\n-        return;\n+        return userPipelineMap.get(user);\n       }\n \n-      chainWrapper = new RequestInterceptorChainWrapper();\n-      this.userPipelineMap.put(user, chainWrapper);\n-    }\n-\n-    // We register the pipeline instance in the map first and then initialize it\n-    // later because chain initialization can be expensive and we would like to\n-    // release the lock as soon as possible to prevent other applications from\n-    // blocking when one application's chain is initializing\n-    LOG.info(\"Initializing request processing pipeline for the user: {}\", user);\n-\n-    try {\n-      RESTRequestInterceptor interceptorChain =\n-          this.createRequestInterceptorChain();\n-      interceptorChain.init(user);\n-      chainWrapper.init(interceptorChain);\n-    } catch (Exception e) {\n-      synchronized (this.userPipelineMap) {\n-        this.userPipelineMap.remove(user);\n+      RequestInterceptorChainWrapper chainWrapper =\n+          new RequestInterceptorChainWrapper();\n+      try {\n+        // We should init the pipeline instance after it is created and then\n+        // add to the map, to ensure thread safe.\n+        LOG.info(\"Initializing request processing pipeline for user: {}\", user);\n+\n+        RESTRequestInterceptor interceptorChain =\n+            this.createRequestInterceptorChain();\n+        interceptorChain.init(user);\n+        chainWrapper.init(interceptorChain);\n+      } catch (Exception e) {\n+        LOG.error(\"Init RESTRequestInterceptor error for user: \" + user, e);\n+        throw e;\n       }\n-      throw e;\n+\n+      this.userPipelineMap.put(user, chainWrapper);\n+      return chainWrapper;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/main/java/org/apache/hadoop/yarn/server/router/webapp/RouterWebServices.java",
                "sha": "49de588ba851d7597e470353dac02b23d860707f",
                "status": "modified"
            },
            {
                "additions": 60,
                "blob_url": "https://github.com/apache/hadoop/blob/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/clientrm/TestRouterClientRMService.java",
                "changes": 60,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/clientrm/TestRouterClientRMService.java?ref=0d9804dcef2eab5ebf84667d9ca49bb035d9a731",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/clientrm/TestRouterClientRMService.java",
                "patch": "@@ -19,8 +19,10 @@\n package org.apache.hadoop.yarn.server.router.clientrm;\n \n import java.io.IOException;\n+import java.security.PrivilegedExceptionAction;\n import java.util.Map;\n \n+import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.yarn.api.protocolrecords.GetClusterMetricsResponse;\n import org.apache.hadoop.yarn.api.protocolrecords.GetClusterNodeLabelsResponse;\n import org.apache.hadoop.yarn.api.protocolrecords.GetClusterNodesResponse;\n@@ -207,4 +209,62 @@ public void testUsersChainMapWithLRUCache()\n     Assert.assertNull(\"test2 should have been evicted\", chain);\n   }\n \n+  /**\n+   * This test validates if the ClientRequestInterceptor chain for the user\n+   * can build and init correctly when a multi-client process begins to\n+   * request RouterClientRMService for the same user simultaneously.\n+   */\n+  @Test\n+  public void testClientPipelineConcurrent() throws InterruptedException {\n+    final String user = \"test1\";\n+\n+    /*\n+     * ClientTestThread is a thread to simulate a client request to get a\n+     * ClientRequestInterceptor for the user.\n+     */\n+    class ClientTestThread extends Thread {\n+      private ClientRequestInterceptor interceptor;\n+      @Override public void run() {\n+        try {\n+          interceptor = pipeline();\n+        } catch (IOException | InterruptedException e) {\n+          e.printStackTrace();\n+        }\n+      }\n+      private ClientRequestInterceptor pipeline()\n+          throws IOException, InterruptedException {\n+        return UserGroupInformation.createRemoteUser(user).doAs(\n+            new PrivilegedExceptionAction<ClientRequestInterceptor>() {\n+              @Override\n+              public ClientRequestInterceptor run() throws Exception {\n+                RequestInterceptorChainWrapper wrapper =\n+                    getRouterClientRMService().getInterceptorChain();\n+                ClientRequestInterceptor interceptor =\n+                    wrapper.getRootInterceptor();\n+                Assert.assertNotNull(interceptor);\n+                LOG.info(\"init client interceptor success for user \" + user);\n+                return interceptor;\n+              }\n+            });\n+      }\n+    }\n+\n+    /*\n+     * We start the first thread. It should not finish initing a chainWrapper\n+     * before the other thread starts. In this way, the second thread can\n+     * init at the same time of the first one. In the end, we validate that\n+     * the 2 threads get the same chainWrapper without going into error.\n+     */\n+    ClientTestThread client1 = new ClientTestThread();\n+    ClientTestThread client2 = new ClientTestThread();\n+    client1.start();\n+    client2.start();\n+    client1.join();\n+    client2.join();\n+\n+    Assert.assertNotNull(client1.interceptor);\n+    Assert.assertNotNull(client2.interceptor);\n+    Assert.assertTrue(client1.interceptor == client2.interceptor);\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/clientrm/TestRouterClientRMService.java",
                "sha": "b03059decfab2555ac88c3aeaf119c0003b6dc3c",
                "status": "modified"
            },
            {
                "additions": 60,
                "blob_url": "https://github.com/apache/hadoop/blob/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/rmadmin/TestRouterRMAdminService.java",
                "changes": 60,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/rmadmin/TestRouterRMAdminService.java?ref=0d9804dcef2eab5ebf84667d9ca49bb035d9a731",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/rmadmin/TestRouterRMAdminService.java",
                "patch": "@@ -19,8 +19,10 @@\n package org.apache.hadoop.yarn.server.router.rmadmin;\n \n import java.io.IOException;\n+import java.security.PrivilegedExceptionAction;\n import java.util.Map;\n \n+import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.yarn.exceptions.YarnException;\n import org.apache.hadoop.yarn.server.api.protocolrecords.AddToClusterNodeLabelsResponse;\n import org.apache.hadoop.yarn.server.api.protocolrecords.CheckForDecommissioningNodesResponse;\n@@ -216,4 +218,62 @@ public void testUsersChainMapWithLRUCache()\n     Assert.assertNull(\"test2 should have been evicted\", chain);\n   }\n \n+  /**\n+   * This test validates if the RMAdminRequestInterceptor chain for the user\n+   * can build and init correctly when a multi-client process begins to\n+   * request RouterRMAdminService for the same user simultaneously.\n+   */\n+  @Test\n+  public void testRMAdminPipelineConcurrent() throws InterruptedException {\n+    final String user = \"test1\";\n+\n+    /*\n+     * ClientTestThread is a thread to simulate a client request to get a\n+     * RMAdminRequestInterceptor for the user.\n+     */\n+    class ClientTestThread extends Thread {\n+      private RMAdminRequestInterceptor interceptor;\n+      @Override public void run() {\n+        try {\n+          interceptor = pipeline();\n+        } catch (IOException | InterruptedException e) {\n+          e.printStackTrace();\n+        }\n+      }\n+      private RMAdminRequestInterceptor pipeline()\n+          throws IOException, InterruptedException {\n+        return UserGroupInformation.createRemoteUser(user).doAs(\n+            new PrivilegedExceptionAction<RMAdminRequestInterceptor>() {\n+              @Override\n+              public RMAdminRequestInterceptor run() throws Exception {\n+                RequestInterceptorChainWrapper wrapper =\n+                    getRouterRMAdminService().getInterceptorChain();\n+                RMAdminRequestInterceptor interceptor =\n+                    wrapper.getRootInterceptor();\n+                Assert.assertNotNull(interceptor);\n+                LOG.info(\"init rm admin interceptor success for user\" + user);\n+                return interceptor;\n+              }\n+            });\n+      }\n+    }\n+\n+    /*\n+     * We start the first thread. It should not finish initing a chainWrapper\n+     * before the other thread starts. In this way, the second thread can\n+     * init at the same time of the first one. In the end, we validate that\n+     * the 2 threads get the same chainWrapper without going into error.\n+     */\n+    ClientTestThread client1 = new ClientTestThread();\n+    ClientTestThread client2 = new ClientTestThread();\n+    client1.start();\n+    client2.start();\n+    client1.join();\n+    client2.join();\n+\n+    Assert.assertNotNull(client1.interceptor);\n+    Assert.assertNotNull(client2.interceptor);\n+    Assert.assertTrue(client1.interceptor == client2.interceptor);\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/rmadmin/TestRouterRMAdminService.java",
                "sha": "07ef73c3cdb857c32840115f55bf3ef660a4fd5b",
                "status": "modified"
            },
            {
                "additions": 65,
                "blob_url": "https://github.com/apache/hadoop/blob/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/webapp/TestRouterWebServices.java",
                "changes": 65,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/webapp/TestRouterWebServices.java?ref=0d9804dcef2eab5ebf84667d9ca49bb035d9a731",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/webapp/TestRouterWebServices.java",
                "patch": "@@ -19,10 +19,12 @@\n package org.apache.hadoop.yarn.server.router.webapp;\n \n import java.io.IOException;\n+import java.security.PrivilegedExceptionAction;\n import java.util.Map;\n \n import javax.ws.rs.core.Response;\n \n+import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.yarn.exceptions.YarnException;\n import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.ActivitiesInfo;\n import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppActivitiesInfo;\n@@ -49,12 +51,17 @@\n import org.apache.hadoop.yarn.server.webapp.dao.ContainersInfo;\n import org.junit.Assert;\n import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n /**\n  * Test class to validate the WebService interceptor model inside the Router.\n  */\n public class TestRouterWebServices extends BaseRouterWebServicesTest {\n \n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(TestRouterWebServices.class);\n+\n   private String user = \"test1\";\n \n   /**\n@@ -266,4 +273,62 @@ public void testUsersChainMapWithLRUCache()\n     Assert.assertNull(\"test2 should have been evicted\", chain);\n   }\n \n+  /**\n+   * This test validates if the RESTRequestInterceptor chain for the user\n+   * can build and init correctly when a multi-client process begins to\n+   * request RouterWebServices for the same user simultaneously.\n+   */\n+  @Test\n+  public void testWebPipelineConcurrent() throws InterruptedException {\n+    final String user = \"test1\";\n+\n+    /*\n+     * ClientTestThread is a thread to simulate a client request to get a\n+     * RESTRequestInterceptor for the user.\n+     */\n+    class ClientTestThread extends Thread {\n+      private RESTRequestInterceptor interceptor;\n+      @Override public void run() {\n+        try {\n+          interceptor = pipeline();\n+        } catch (IOException | InterruptedException e) {\n+          e.printStackTrace();\n+        }\n+      }\n+      private RESTRequestInterceptor pipeline()\n+          throws IOException, InterruptedException {\n+        return UserGroupInformation.createRemoteUser(user).doAs(\n+            new PrivilegedExceptionAction<RESTRequestInterceptor>() {\n+              @Override\n+              public RESTRequestInterceptor run() throws Exception {\n+                RequestInterceptorChainWrapper wrapper =\n+                    getInterceptorChain(user);\n+                RESTRequestInterceptor interceptor =\n+                    wrapper.getRootInterceptor();\n+                Assert.assertNotNull(interceptor);\n+                LOG.info(\"init web interceptor success for user\" + user);\n+                return interceptor;\n+              }\n+            });\n+      }\n+    }\n+\n+    /*\n+     * We start the first thread. It should not finish initing a chainWrapper\n+     * before the other thread starts. In this way, the second thread can\n+     * init at the same time of the first one. In the end, we validate that\n+     * the 2 threads get the same chainWrapper without going into error.\n+     */\n+    ClientTestThread client1 = new ClientTestThread();\n+    ClientTestThread client2 = new ClientTestThread();\n+    client1.start();\n+    client2.start();\n+    client1.join();\n+    client2.join();\n+\n+    Assert.assertNotNull(client1.interceptor);\n+    Assert.assertNotNull(client2.interceptor);\n+    Assert.assertTrue(client1.interceptor == client2.interceptor);\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/0d9804dcef2eab5ebf84667d9ca49bb035d9a731/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-router/src/test/java/org/apache/hadoop/yarn/server/router/webapp/TestRouterWebServices.java",
                "sha": "14652435dac291f4cd09038f493ef4621819d036",
                "status": "modified"
            }
        ],
        "message": "YARN-8435. Fix NPE when the same client simultaneously contact for the first time Yarn Router. Contributed by Rang Jiaheng.",
        "parent": "https://github.com/apache/hadoop/commit/71df8c27c9a0e326232d3baf16414a63b5ea5a4b",
        "repo": "hadoop",
        "unit_tests": [
            "TestRouterClientRMService.java",
            "TestRouterRMAdminService.java",
            "TestRouterWebServices.java"
        ]
    },
    "hadoop_0ffca5d": {
        "bug_id": "hadoop_0ffca5d",
        "commit": "https://github.com/apache/hadoop/commit/0ffca5d347df0acb1979dff7a07ae88ea834adc7",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/0ffca5d347df0acb1979dff7a07ae88ea834adc7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java?ref=0ffca5d347df0acb1979dff7a07ae88ea834adc7",
                "deletions": 12,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "patch": "@@ -275,10 +275,6 @@ protected void addSchedPriorityCommand(List<String> command) {\n     }\n   }\n \n-  protected PrivilegedOperationExecutor getPrivilegedOperationExecutor() {\n-    return PrivilegedOperationExecutor.getInstance(getConf());\n-  }\n-\n   @Override\n   public void init() throws IOException {\n     Configuration conf = super.getConf();\n@@ -289,7 +285,7 @@ public void init() throws IOException {\n       PrivilegedOperation checkSetupOp = new PrivilegedOperation(\n           PrivilegedOperation.OperationType.CHECK_SETUP);\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          getPrivilegedOperationExecutor();\n+          PrivilegedOperationExecutor.getInstance(conf);\n \n       privilegedOperationExecutor.executePrivilegedOperation(checkSetupOp,\n           false);\n@@ -386,7 +382,7 @@ public void startLocalizer(LocalizerStartContext ctx)\n     try {\n       Configuration conf = super.getConf();\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          getPrivilegedOperationExecutor();\n+          PrivilegedOperationExecutor.getInstance(conf);\n \n       privilegedOperationExecutor.executePrivilegedOperation(prefixCommands,\n           initializeContainerOp, null, null, false, true);\n@@ -534,9 +530,8 @@ public int launchContainer(ContainerStartContext ctx)\n         }\n         builder.append(\"Stack trace: \"\n             + StringUtils.stringifyException(e) + \"\\n\");\n-        String output = e.getOutput();\n-        if (output!= null && !e.getOutput().isEmpty()) {\n-          builder.append(\"Shell output: \" + output + \"\\n\");\n+        if (!e.getOutput().isEmpty()) {\n+          builder.append(\"Shell output: \" + e.getOutput() + \"\\n\");\n         }\n         String diagnostics = builder.toString();\n         logOutput(diagnostics);\n@@ -734,7 +729,7 @@ public void deleteAsUser(DeletionAsUserContext ctx) {\n     try {\n       Configuration conf = super.getConf();\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          getPrivilegedOperationExecutor();\n+          PrivilegedOperationExecutor.getInstance(conf);\n \n       privilegedOperationExecutor.executePrivilegedOperation(deleteAsUserOp,\n           false);\n@@ -764,7 +759,7 @@ public void deleteAsUser(DeletionAsUserContext ctx) {\n \n     try {\n       PrivilegedOperationExecutor privOpExecutor =\n-          getPrivilegedOperationExecutor();\n+          PrivilegedOperationExecutor.getInstance(super.getConf());\n \n       String results =\n           privOpExecutor.executePrivilegedOperation(listAsUserOp, true);\n@@ -823,7 +818,7 @@ public void mountCgroups(List<String> cgroupKVs, String hierarchy)\n \n       mountCGroupsOp.appendArgs(cgroupKVs);\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          getPrivilegedOperationExecutor();\n+          PrivilegedOperationExecutor.getInstance(conf);\n \n       privilegedOperationExecutor.executePrivilegedOperation(mountCGroupsOp,\n           false);",
                "raw_url": "https://github.com/apache/hadoop/raw/0ffca5d347df0acb1979dff7a07ae88ea834adc7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "sha": "9a3b2d25cc6b384f27540953fa4e89ecb3fd7c09",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/0ffca5d347df0acb1979dff7a07ae88ea834adc7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java?ref=0ffca5d347df0acb1979dff7a07ae88ea834adc7",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java",
                "patch": "@@ -24,7 +24,7 @@\n \n public class PrivilegedOperationException extends YarnException {\n   private static final long serialVersionUID = 1L;\n-  private int exitCode = -1;\n+  private Integer exitCode;\n   private String output;\n   private String errorOutput;\n \n@@ -36,7 +36,7 @@ public PrivilegedOperationException(String message) {\n     super(message);\n   }\n \n-  public PrivilegedOperationException(String message, int exitCode,\n+  public PrivilegedOperationException(String message, Integer exitCode,\n       String output, String errorOutput) {\n     super(message);\n     this.exitCode = exitCode;\n@@ -48,8 +48,8 @@ public PrivilegedOperationException(Throwable cause) {\n     super(cause);\n   }\n \n-  public PrivilegedOperationException(Throwable cause, int exitCode,\n-      String output, String errorOutput) {\n+  public PrivilegedOperationException(Throwable cause, Integer exitCode, String\n+      output, String errorOutput) {\n     super(cause);\n     this.exitCode = exitCode;\n     this.output = output;\n@@ -59,7 +59,7 @@ public PrivilegedOperationException(String message, Throwable cause) {\n     super(message, cause);\n   }\n \n-  public int getExitCode() {\n+  public Integer getExitCode() {\n     return exitCode;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/0ffca5d347df0acb1979dff7a07ae88ea834adc7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java",
                "sha": "3622489a499f9bea2423e24bba7a8d1b46deacab",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/0ffca5d347df0acb1979dff7a07ae88ea834adc7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java?ref=0ffca5d347df0acb1979dff7a07ae88ea834adc7",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java",
                "patch": "@@ -32,10 +32,10 @@\n @InterfaceStability.Unstable\n public class ContainerExecutionException extends YarnException {\n   private static final long serialVersionUID = 1L;\n-  private static final int EXIT_CODE_UNSET = -1;\n+  private static final Integer EXIT_CODE_UNSET = -1;\n   private static final String OUTPUT_UNSET = \"<unknown>\";\n \n-  private int exitCode;\n+  private Integer exitCode;\n   private String output;\n   private String errorOutput;\n \n@@ -54,23 +54,23 @@ public ContainerExecutionException(Throwable throwable) {\n   }\n \n \n-  public ContainerExecutionException(String message, int exitCode, String\n+  public ContainerExecutionException(String message, Integer exitCode, String\n       output, String errorOutput) {\n     super(message);\n     this.exitCode = exitCode;\n     this.output = output;\n     this.errorOutput = errorOutput;\n   }\n \n-  public ContainerExecutionException(Throwable cause, int exitCode, String\n+  public ContainerExecutionException(Throwable cause, Integer exitCode, String\n       output, String errorOutput) {\n     super(cause);\n     this.exitCode = exitCode;\n     this.output = output;\n     this.errorOutput = errorOutput;\n   }\n \n-  public int getExitCode() {\n+  public Integer getExitCode() {\n     return exitCode;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/0ffca5d347df0acb1979dff7a07ae88ea834adc7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java",
                "sha": "1fbece2205e27daeaa5798732f72177ad66c92cd",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/0ffca5d347df0acb1979dff7a07ae88ea834adc7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "changes": 89,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java?ref=0ffca5d347df0acb1979dff7a07ae88ea834adc7",
                "deletions": 89,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "patch": "@@ -23,9 +23,7 @@\n import static org.junit.Assert.assertNotEquals;\n import static org.junit.Assert.assertTrue;\n import static org.mockito.Matchers.any;\n-import static org.mockito.Matchers.anyBoolean;\n import static org.mockito.Mockito.doAnswer;\n-import static org.mockito.Mockito.doThrow;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.spy;\n import static org.mockito.Mockito.when;\n@@ -42,24 +40,20 @@\n import java.util.HashMap;\n import java.util.LinkedList;\n import java.util.List;\n-import java.util.Map;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.util.StringUtils;\n-import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n-import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.ContainerLaunchContext;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.exceptions.ConfigurationException;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerDiagnosticsUpdateEvent;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation;\n-import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.LinuxContainerRuntime;\n@@ -522,87 +516,4 @@ public void testDeleteAsUser() throws IOException {\n         appSubmitter, cmd, \"\", baseDir0.toString(), baseDir1.toString()),\n         readMockParams());\n   }\n-\n-  @Test\n-  public void testNoExitCodeFromPrivilegedOperation() throws Exception {\n-    Configuration conf = new Configuration();\n-    final PrivilegedOperationExecutor spyPrivilegedExecutor =\n-        spy(PrivilegedOperationExecutor.getInstance(conf));\n-    doThrow(new PrivilegedOperationException(\"interrupted\"))\n-        .when(spyPrivilegedExecutor).executePrivilegedOperation(\n-            any(List.class), any(PrivilegedOperation.class),\n-            any(File.class), any(Map.class), anyBoolean(), anyBoolean());\n-    LinuxContainerRuntime runtime = new DefaultLinuxContainerRuntime(\n-        spyPrivilegedExecutor);\n-    runtime.initialize(conf);\n-    mockExec = new LinuxContainerExecutor(runtime);\n-    mockExec.setConf(conf);\n-    LinuxContainerExecutor lce = new LinuxContainerExecutor(runtime) {\n-      @Override\n-      protected PrivilegedOperationExecutor getPrivilegedOperationExecutor() {\n-        return spyPrivilegedExecutor;\n-      }\n-    };\n-    lce.setConf(conf);\n-    InetSocketAddress address = InetSocketAddress.createUnresolved(\n-        \"localhost\", 8040);\n-    Path nmPrivateCTokensPath= new Path(\"file:///bin/nmPrivateCTokensPath\");\n-    LocalDirsHandlerService dirService = new LocalDirsHandlerService();\n-    dirService.init(conf);\n-\n-    String appSubmitter = \"nobody\";\n-    ApplicationId appId = ApplicationId.newInstance(1, 1);\n-    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(appId, 1);\n-    ContainerId cid = ContainerId.newContainerId(attemptId, 1);\n-    HashMap<String, String> env = new HashMap<>();\n-    Container container = mock(Container.class);\n-    ContainerLaunchContext context = mock(ContainerLaunchContext.class);\n-    when(container.getContainerId()).thenReturn(cid);\n-    when(container.getLaunchContext()).thenReturn(context);\n-    when(context.getEnvironment()).thenReturn(env);\n-    Path workDir = new Path(\"/tmp\");\n-\n-    try {\n-      lce.startLocalizer(new LocalizerStartContext.Builder()\n-          .setNmPrivateContainerTokens(nmPrivateCTokensPath)\n-          .setNmAddr(address)\n-          .setUser(appSubmitter)\n-          .setAppId(appId.toString())\n-          .setLocId(\"12345\")\n-          .setDirsHandler(dirService)\n-          .build());\n-      Assert.fail(\"startLocalizer should have thrown an exception\");\n-    } catch (IOException e) {\n-      assertTrue(\"Unexpected exception \" + e,\n-          e.getMessage().contains(\"exitCode\"));\n-    }\n-\n-    lce.activateContainer(cid, new Path(workDir, \"pid.txt\"));\n-    lce.launchContainer(new ContainerStartContext.Builder()\n-        .setContainer(container)\n-        .setNmPrivateContainerScriptPath(new Path(\"file:///bin/echo\"))\n-        .setNmPrivateTokensPath(new Path(\"file:///dev/null\"))\n-        .setUser(appSubmitter)\n-        .setAppId(appId.toString())\n-        .setContainerWorkDir(workDir)\n-        .setLocalDirs(dirsHandler.getLocalDirs())\n-        .setLogDirs(dirsHandler.getLogDirs())\n-        .setFilecacheDirs(new ArrayList<>())\n-        .setUserLocalDirs(new ArrayList<>())\n-        .setContainerLocalDirs(new ArrayList<>())\n-        .setContainerLogDirs(new ArrayList<>())\n-        .build());\n-    lce.deleteAsUser(new DeletionAsUserContext.Builder()\n-        .setUser(appSubmitter)\n-        .setSubDir(new Path(\"/tmp/testdir\"))\n-        .build());\n-\n-    try {\n-      lce.mountCgroups(new ArrayList<String>(), \"hierarchy\");\n-      Assert.fail(\"mountCgroups should have thrown an exception\");\n-    } catch (IOException e) {\n-      assertTrue(\"Unexpected exception \" + e,\n-          e.getMessage().contains(\"exit code\"));\n-    }\n-  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/0ffca5d347df0acb1979dff7a07ae88ea834adc7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "sha": "07134e8e36dd1ac5111088f2ffbf29d020b14abb",
                "status": "modified"
            }
        ],
        "message": "Revert \"YARN-6805. NPE in LinuxContainerExecutor due to null PrivilegedOperationException exit code. Contributed by Jason Lowe\"\n\nThis reverts commit f76f5c0919cdb0b032edb309d137093952e77268.",
        "parent": "https://github.com/apache/hadoop/commit/f76f5c0919cdb0b032edb309d137093952e77268",
        "repo": "hadoop",
        "unit_tests": [
            "TestLinuxContainerExecutor.java"
        ]
    },
    "hadoop_10db613": {
        "bug_id": "hadoop_10db613",
        "commit": "https://github.com/apache/hadoop/commit/10db613389718d8df519493b958ecd97fca8686d",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/10db613389718d8df519493b958ecd97fca8686d/hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt?ref=10db613389718d8df519493b958ecd97fca8686d",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt",
                "patch": "@@ -51,3 +51,6 @@ fs-encryption (Unreleased)\n   BUG FIXES\n \n     HADOOP-10871. incorrect prototype in OpensslSecureRandom.c (cmccabe)\n+\n+    HADOOP-10886. CryptoCodec#getCodecclasses throws NPE when configurations not \n+    loaded. (umamahesh)",
                "raw_url": "https://github.com/apache/hadoop/raw/10db613389718d8df519493b958ecd97fca8686d/hadoop-common-project/hadoop-common/CHANGES-fs-encryption.txt",
                "sha": "498307e6bc7804db4ecdbc177b6561be579f7195",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hadoop/blob/10db613389718d8df519493b958ecd97fca8686d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoCodec.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoCodec.java?ref=10db613389718d8df519493b958ecd97fca8686d",
                "deletions": 6,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoCodec.java",
                "patch": "@@ -45,14 +45,21 @@\n   \n   /**\n    * Get crypto codec for specified algorithm/mode/padding.\n-   * @param conf the configuration\n-   * @param CipherSuite algorithm/mode/padding\n-   * @return CryptoCodec the codec object\n+   * \n+   * @param conf\n+   *          the configuration\n+   * @param CipherSuite\n+   *          algorithm/mode/padding\n+   * @return CryptoCodec the codec object. Null value will be returned if no\n+   *         crypto codec classes with cipher suite configured.\n    */\n   public static CryptoCodec getInstance(Configuration conf, \n       CipherSuite cipherSuite) {\n     List<Class<? extends CryptoCodec>> klasses = getCodecClasses(\n         conf, cipherSuite);\n+    if (klasses == null) {\n+      return null;\n+    }\n     CryptoCodec codec = null;\n     for (Class<? extends CryptoCodec> klass : klasses) {\n       try {\n@@ -80,10 +87,13 @@ public static CryptoCodec getInstance(Configuration conf,\n   }\n   \n   /**\n-   * Get crypto codec for algorithm/mode/padding in config value \n+   * Get crypto codec for algorithm/mode/padding in config value\n    * hadoop.security.crypto.cipher.suite\n-   * @param conf the configuration\n-   * @return CryptoCodec the codec object\n+   * \n+   * @param conf\n+   *          the configuration\n+   * @return CryptoCodec the codec object Null value will be returned if no\n+   *         crypto codec classes with cipher suite configured.\n    */\n   public static CryptoCodec getInstance(Configuration conf) {\n     String name = conf.get(HADOOP_SECURITY_CRYPTO_CIPHER_SUITE_KEY, \n@@ -97,6 +107,10 @@ public static CryptoCodec getInstance(Configuration conf) {\n     String configName = HADOOP_SECURITY_CRYPTO_CODEC_CLASSES_KEY_PREFIX + \n         cipherSuite.getConfigSuffix();\n     String codecString = conf.get(configName);\n+    if (codecString == null) {\n+      LOG.warn(\"No crypto codec classes with cipher suite configured.\");\n+      return null;\n+    }\n     for (String c : Splitter.on(',').trimResults().omitEmptyStrings().\n         split(codecString)) {\n       try {",
                "raw_url": "https://github.com/apache/hadoop/raw/10db613389718d8df519493b958ecd97fca8686d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/CryptoCodec.java",
                "sha": "9de7f95200f5f44e440619269fbc2ae1135f599d",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/10db613389718d8df519493b958ecd97fca8686d/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsForLocalFS.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsForLocalFS.java?ref=10db613389718d8df519493b958ecd97fca8686d",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsForLocalFS.java",
                "patch": "@@ -25,6 +25,7 @@\n import java.io.OutputStream;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.fs.LocalFileSystem;\n@@ -50,6 +51,11 @@ public static void init() throws Exception {\n     conf = new Configuration(false);\n     conf.set(\"fs.file.impl\", LocalFileSystem.class.getName());\n     fileSys = FileSystem.getLocal(conf);\n+    conf.set(\n+        CommonConfigurationKeysPublic.HADOOP_SECURITY_CRYPTO_CODEC_CLASSES_KEY_PREFIX\n+            + CipherSuite.AES_CTR_NOPADDING.getConfigSuffix(),\n+        OpensslAesCtrCryptoCodec.class.getName() + \",\"\n+            + JceAesCtrCryptoCodec.class.getName());\n     codec = CryptoCodec.getInstance(conf);\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/10db613389718d8df519493b958ecd97fca8686d/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/TestCryptoStreamsForLocalFS.java",
                "sha": "765a364faa6d5cffff9bd1a811caf51bc51b0cc5",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/10db613389718d8df519493b958ecd97fca8686d/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java?ref=10db613389718d8df519493b958ecd97fca8686d",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "patch": "@@ -598,7 +598,9 @@ public DFSClient(URI nameNodeUri, ClientProtocol rpcNamenode,\n         DFSUtil.getRandom().nextInt()  + \"_\" + Thread.currentThread().getId();\n     this.codec = CryptoCodec.getInstance(conf);\n     this.cipherSuites = Lists.newArrayListWithCapacity(1);\n-    cipherSuites.add(codec.getCipherSuite());\n+    if (codec != null) {\n+      cipherSuites.add(codec.getCipherSuite());\n+    }\n     provider = DFSUtil.createKeyProviderCryptoExtension(conf);\n     if (provider == null) {\n       LOG.info(\"No KeyProvider found.\");\n@@ -1333,9 +1335,12 @@ public HdfsDataInputStream createWrappedInputStream(DFSInputStream dfsis)\n     if (feInfo != null) {\n       // File is encrypted, wrap the stream in a crypto stream.\n       KeyVersion decrypted = decryptEncryptedDataEncryptionKey(feInfo);\n+      CryptoCodec codec = CryptoCodec\n+          .getInstance(conf, feInfo.getCipherSuite());\n+      Preconditions.checkNotNull(codec == null,\n+          \"No crypto codec classes with cipher suite configured.\");\n       final CryptoInputStream cryptoIn =\n-          new CryptoInputStream(dfsis, CryptoCodec.getInstance(conf, \n-              feInfo.getCipherSuite()), decrypted.getMaterial(),\n+          new CryptoInputStream(dfsis, codec, decrypted.getMaterial(),\n               feInfo.getIV());\n       return new HdfsDataInputStream(cryptoIn);\n     } else {\n@@ -1361,6 +1366,8 @@ public HdfsDataOutputStream createWrappedOutputStream(DFSOutputStream dfsos,\n       FileSystem.Statistics statistics, long startPos) throws IOException {\n     final FileEncryptionInfo feInfo = dfsos.getFileEncryptionInfo();\n     if (feInfo != null) {\n+      Preconditions.checkNotNull(codec == null,\n+          \"No crypto codec classes with cipher suite configured.\");\n       // File is encrypted, wrap the stream in a crypto stream.\n       KeyVersion decrypted = decryptEncryptedDataEncryptionKey(feInfo);\n       final CryptoOutputStream cryptoOut =",
                "raw_url": "https://github.com/apache/hadoop/raw/10db613389718d8df519493b958ecd97fca8686d/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSClient.java",
                "sha": "d3532607c84b80f9588043f78c1202a625bc1db9",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/10db613389718d8df519493b958ecd97fca8686d/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java?ref=10db613389718d8df519493b958ecd97fca8686d",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "patch": "@@ -38,7 +38,6 @@\n import java.util.EnumSet;\n import java.util.List;\n import java.util.Random;\n-import java.util.concurrent.CancellationException;\n \n import org.apache.commons.lang.ArrayUtils;\n import org.apache.commons.logging.impl.Log4JLogger;",
                "raw_url": "https://github.com/apache/hadoop/raw/10db613389718d8df519493b958ecd97fca8686d/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDistributedFileSystem.java",
                "sha": "b71cc32fb80e93c51153e29045cbaa020c1bf461",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10886. CryptoCodec#getCodecclasses throws NPE when configurations not loaded. Contributed by Uma Maheswara Rao G.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1615523 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/70c99278a9ffb8a22059c20357b435c7b576b3db",
        "repo": "hadoop",
        "unit_tests": [
            "TestCryptoCodec.java"
        ]
    },
    "hadoop_137aa07": {
        "bug_id": "hadoop_137aa07",
        "commit": "https://github.com/apache/hadoop/commit/137aa0763fedd377de7b5b9aeb89a2f1d62b941b",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/137aa0763fedd377de7b5b9aeb89a2f1d62b941b/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=137aa0763fedd377de7b5b9aeb89a2f1d62b941b",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -203,6 +203,10 @@ Release 0.23.2 - UNRELEASED\n     MAPREDUCE-3816. capacity scheduler web ui bar graphs for used capacity wrong\n     (tgraves via bobby)\n \n+    MAPREDUCE-3930. Fixed an NPE while accessing the AM page/webservice for a \n+    task attempt without an assigned container. (Robert Joseph Evans via\n+    sseth)\n+\n Release 0.23.1 - 2012-02-17\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/137aa0763fedd377de7b5b9aeb89a2f1d62b941b/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "d293b52b3f7f2466e11cf730a8d4aa9ef58ad197",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/137aa0763fedd377de7b5b9aeb89a2f1d62b941b/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java?ref=137aa0763fedd377de7b5b9aeb89a2f1d62b941b",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java",
                "patch": "@@ -142,7 +142,7 @@ private static ApplicationId toApplicationId(\n   }\n \n   public static String toString(ContainerId cId) {\n-    return cId.toString();\n+    return cId == null ? null : cId.toString();\n   }\n \n   public static NodeId toNodeId(String nodeIdStr) {",
                "raw_url": "https://github.com/apache/hadoop/raw/137aa0763fedd377de7b5b9aeb89a2f1d62b941b/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/ConverterUtils.java",
                "sha": "21fe2d9874cf0223ea91b4fb1af96c13f306464c",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/137aa0763fedd377de7b5b9aeb89a2f1d62b941b/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestConverterUtils.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestConverterUtils.java?ref=137aa0763fedd377de7b5b9aeb89a2f1d62b941b",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestConverterUtils.java",
                "patch": "@@ -22,6 +22,7 @@\n import java.net.URISyntaxException;\n \n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.URL;\n import org.junit.Test;\n \n@@ -35,4 +36,17 @@ public void testConvertUrlWithNoPort() throws URISyntaxException {\n     assertEquals(expectedPath, actualPath);\n   }\n \n+  @Test\n+  public void testContainerId() throws URISyntaxException {\n+    ContainerId id = BuilderUtils.newContainerId(0, 0, 0, 0);\n+    String cid = ConverterUtils.toString(id);\n+    assertEquals(\"container_0_0000_00_000000\", cid);\n+    ContainerId gen = ConverterUtils.toContainerId(cid);\n+    assertEquals(gen, id);\n+  }\n+\n+  @Test\n+  public void testContainerIdNull() throws URISyntaxException {\n+    assertNull(ConverterUtils.toString((ContainerId)null));\n+  }  \n }",
                "raw_url": "https://github.com/apache/hadoop/raw/137aa0763fedd377de7b5b9aeb89a2f1d62b941b/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestConverterUtils.java",
                "sha": "7924124bf40e7a3d6332a3ee5485a6a595bacfcd",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3930. Fixed an NPE while accessing the AM page/webservice for a task attempt without an assigned container. (Contributed by Robert Joseph Evans)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1294901 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/ffdf980b2056b2a1b31ccb19746f23c31f7d08ef",
        "repo": "hadoop",
        "unit_tests": [
            "TestConverterUtils.java"
        ]
    },
    "hadoop_1415ad3": {
        "bug_id": "hadoop_1415ad3",
        "commit": "https://github.com/apache/hadoop/commit/1415ad3800d117b4fff6ad0ef281acc7051a0bcf",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/1415ad3800d117b4fff6ad0ef281acc7051a0bcf/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java?ref=1415ad3800d117b4fff6ad0ef281acc7051a0bcf",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "patch": "@@ -584,6 +584,7 @@ public boolean hasKerberosCredentials() {\n   @InterfaceAudience.Public\n   @InterfaceStability.Evolving\n   public static UserGroupInformation getCurrentUser() throws IOException {\n+    ensureInitialized();\n     AccessControlContext context = AccessController.getContext();\n     Subject subject = Subject.getSubject(context);\n     if (subject == null || subject.getPrincipals(User.class).isEmpty()) {\n@@ -670,6 +671,7 @@ public static UserGroupInformation getUGIFromSubject(Subject subject)\n   @InterfaceAudience.Public\n   @InterfaceStability.Evolving\n   public static UserGroupInformation getLoginUser() throws IOException {\n+    ensureInitialized();\n     UserGroupInformation loginUser = loginUserRef.get();\n     // a potential race condition exists only for the initial creation of\n     // the login user.  there's no need to penalize all subsequent calls",
                "raw_url": "https://github.com/apache/hadoop/raw/1415ad3800d117b4fff6ad0ef281acc7051a0bcf/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "sha": "884380c43f236f365598139f5bf13a55ead30c42",
                "status": "modified"
            }
        ],
        "message": "HADOOP-16707. NPE in UGI.getCurrentUser in ITestAbfsIdentityTransformer setup.\n\nContributed by Steve Loughran.\r\n\r\nChange-Id: I38fdba2fa70e534d78b15e61de19368912588b0c",
        "parent": "https://github.com/apache/hadoop/commit/dfc61d8ea5971e9742af77fcae2cae28011ae0c9",
        "repo": "hadoop",
        "unit_tests": [
            "TestUserGroupInformation.java"
        ]
    },
    "hadoop_1425c65": {
        "bug_id": "hadoop_1425c65",
        "commit": "https://github.com/apache/hadoop/commit/1425c65b5e2cc7f57bf0ac51e4b6bb546736b601",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/1425c65b5e2cc7f57bf0ac51e4b6bb546736b601/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=1425c65b5e2cc7f57bf0ac51e4b6bb546736b601",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -174,6 +174,7 @@ Release 0.23.2 - UNRELEASED\n     (sharad, todd via todd)\n \n   BUG FIXES\n+    HADOOP-8054 NPE with FilterFileSystem (Daryn Sharp via bobby)\n \n     HADOOP-8042  When copying a file out of HDFS, modifying it, and uploading\n     it back into HDFS, the put fails due to a CRC mismatch",
                "raw_url": "https://github.com/apache/hadoop/raw/1425c65b5e2cc7f57bf0ac51e4b6bb546736b601/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "a199c6e1b88bf677994949ae87c9cf694836c645",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/1425c65b5e2cc7f57bf0ac51e4b6bb546736b601/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java?ref=1425c65b5e2cc7f57bf0ac51e4b6bb546736b601",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java",
                "patch": "@@ -80,6 +80,11 @@ public FileSystem getRawFileSystem() {\n    */\n   public void initialize(URI name, Configuration conf) throws IOException {\n     super.initialize(name, conf);\n+    // this is less than ideal, but existing filesystems sometimes neglect\n+    // to initialize the embedded filesystem\n+    if (fs.getConf() == null) {\n+      fs.initialize(name, conf);\n+    }\n     String scheme = name.getScheme();\n     if (!scheme.equals(fs.getUri().getScheme())) {\n       swapScheme = scheme;",
                "raw_url": "https://github.com/apache/hadoop/raw/1425c65b5e2cc7f57bf0ac51e4b6bb546736b601/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/FilterFileSystem.java",
                "sha": "91ee2ae710566d54b4950413f17e01051ca0f618",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/1425c65b5e2cc7f57bf0ac51e4b6bb546736b601/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalFileSystem.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalFileSystem.java?ref=1425c65b5e2cc7f57bf0ac51e4b6bb546736b601",
                "deletions": 7,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalFileSystem.java",
                "patch": "@@ -48,13 +48,6 @@ public LocalFileSystem(FileSystem rawLocalFileSystem) {\n     super(rawLocalFileSystem);\n   }\n     \n-  @Override\n-  public void initialize(URI uri, Configuration conf) throws IOException {\n-    super.initialize(uri, conf);\n-    // ctor didn't initialize the filtered fs\n-    getRawFileSystem().initialize(uri, conf);\n-  }\n-  \n   /** Convert a path to a File. */\n   public File pathToFile(Path path) {\n     return ((RawLocalFileSystem)fs).pathToFile(path);",
                "raw_url": "https://github.com/apache/hadoop/raw/1425c65b5e2cc7f57bf0ac51e4b6bb546736b601/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalFileSystem.java",
                "sha": "4aff81114b9befe0dc7daf6677c91e65c09356db",
                "status": "modified"
            },
            {
                "additions": 123,
                "blob_url": "https://github.com/apache/hadoop/blob/1425c65b5e2cc7f57bf0ac51e4b6bb546736b601/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java",
                "changes": 125,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java?ref=1425c65b5e2cc7f57bf0ac51e4b6bb546736b601",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java",
                "patch": "@@ -18,24 +18,39 @@\n \n package org.apache.hadoop.fs;\n \n+import static org.junit.Assert.*;\n+import static org.mockito.Matchers.*;\n+import static org.mockito.Mockito.*;\n+\n import java.io.IOException;\n import java.lang.reflect.Method;\n import java.lang.reflect.Modifier;\n+import java.net.URI;\n import java.util.EnumSet;\n import java.util.Iterator;\n \n-import junit.framework.TestCase;\n import org.apache.commons.logging.Log;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.permission.FsPermission;\n import org.apache.hadoop.fs.Options.CreateOpts;\n import org.apache.hadoop.fs.Options.Rename;\n import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.util.Progressable;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n \n-public class TestFilterFileSystem extends TestCase {\n+public class TestFilterFileSystem {\n \n   private static final Log LOG = FileSystem.LOG;\n+  private static final Configuration conf = new Configuration();\n \n+  @BeforeClass\n+  public static void setup() {\n+    conf.set(\"fs.flfs.impl\", FilterLocalFileSystem.class.getName());\n+    conf.setBoolean(\"fs.flfs.impl.disable.cache\", true);\n+    conf.setBoolean(\"fs.file.impl.disable.cache\", true);\n+  }\n+  \n   public static class DontCheck {\n     public BlockLocation[] getFileBlockLocations(Path p, \n         long start, long len) { return null; }\n@@ -153,6 +168,7 @@ public void primitiveMkdir(Path f, FsPermission absolutePermission,\n     \n   }\n   \n+  @Test\n   public void testFilterFileSystem() throws Exception {\n     for (Method m : FileSystem.class.getDeclaredMethods()) {\n       if (Modifier.isStatic(m.getModifiers()))\n@@ -176,4 +192,109 @@ public void testFilterFileSystem() throws Exception {\n     }\n   }\n   \n+  @Test\n+  public void testFilterEmbedInit() throws Exception {\n+    FileSystem mockFs = createMockFs(false); // no conf = need init\n+    checkInit(new FilterFileSystem(mockFs), true);\n+  }\n+\n+  @Test\n+  public void testFilterEmbedNoInit() throws Exception {\n+    FileSystem mockFs = createMockFs(true); // has conf = skip init\n+    checkInit(new FilterFileSystem(mockFs), false);\n+  }\n+\n+  @Test\n+  public void testLocalEmbedInit() throws Exception {\n+    FileSystem mockFs = createMockFs(false); // no conf = need init\n+    checkInit(new LocalFileSystem(mockFs), true);\n+  }  \n+  \n+  @Test\n+  public void testLocalEmbedNoInit() throws Exception {\n+    FileSystem mockFs = createMockFs(true); // has conf = skip init\n+    checkInit(new LocalFileSystem(mockFs), false);\n+  }\n+  \n+  private FileSystem createMockFs(boolean useConf) {\n+    FileSystem mockFs = mock(FileSystem.class);\n+    when(mockFs.getUri()).thenReturn(URI.create(\"mock:/\"));\n+    when(mockFs.getConf()).thenReturn(useConf ? conf : null);\n+    return mockFs;\n+  }\n+\n+  @Test\n+  public void testGetLocalFsSetsConfs() throws Exception {\n+    LocalFileSystem lfs = FileSystem.getLocal(conf);\n+    checkFsConf(lfs, conf, 2);\n+  }\n+\n+  @Test\n+  public void testGetFilterLocalFsSetsConfs() throws Exception {\n+    FilterFileSystem flfs =\n+        (FilterFileSystem) FileSystem.get(URI.create(\"flfs:/\"), conf);\n+    checkFsConf(flfs, conf, 3);\n+  }\n+\n+  @Test\n+  public void testInitLocalFsSetsConfs() throws Exception {\n+    LocalFileSystem lfs = new LocalFileSystem();\n+    checkFsConf(lfs, null, 2);\n+    lfs.initialize(lfs.getUri(), conf);\n+    checkFsConf(lfs, conf, 2);\n+  }\n+\n+  @Test\n+  public void testInitFilterFsSetsEmbedConf() throws Exception {\n+    LocalFileSystem lfs = new LocalFileSystem();\n+    checkFsConf(lfs, null, 2);\n+    FilterFileSystem ffs = new FilterFileSystem(lfs);\n+    assertEquals(lfs, ffs.getRawFileSystem());\n+    checkFsConf(ffs, null, 3);\n+    ffs.initialize(URI.create(\"filter:/\"), conf);\n+    checkFsConf(ffs, conf, 3);\n+  }\n+\n+  @Test\n+  public void testInitFilterLocalFsSetsEmbedConf() throws Exception {\n+    FilterFileSystem flfs = new FilterLocalFileSystem();\n+    assertEquals(LocalFileSystem.class, flfs.getRawFileSystem().getClass());\n+    checkFsConf(flfs, null, 3);\n+    flfs.initialize(URI.create(\"flfs:/\"), conf);\n+    checkFsConf(flfs, conf, 3);\n+  }\n+\n+  private void checkInit(FilterFileSystem fs, boolean expectInit)\n+      throws Exception {\n+    URI uri = URI.create(\"filter:/\");\n+    fs.initialize(uri, conf);\n+    \n+    FileSystem embedFs = fs.getRawFileSystem();\n+    if (expectInit) {\n+      verify(embedFs, times(1)).initialize(eq(uri), eq(conf));\n+    } else {\n+      verify(embedFs, times(0)).initialize(any(URI.class), any(Configuration.class));\n+    }\n+  }\n+\n+  // check the given fs's conf, and all its filtered filesystems\n+  private void checkFsConf(FileSystem fs, Configuration conf, int expectDepth) {\n+    int depth = 0;\n+    while (true) {\n+      depth++; \n+      assertFalse(\"depth \"+depth+\">\"+expectDepth, depth > expectDepth);\n+      assertEquals(conf, fs.getConf());\n+      if (!(fs instanceof FilterFileSystem)) {\n+        break;\n+      }\n+      fs = ((FilterFileSystem) fs).getRawFileSystem();\n+    }\n+    assertEquals(expectDepth, depth);\n+  }\n+  \n+  private static class FilterLocalFileSystem extends FilterFileSystem {\n+    FilterLocalFileSystem() {\n+      super(new LocalFileSystem());\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/1425c65b5e2cc7f57bf0ac51e4b6bb546736b601/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestFilterFileSystem.java",
                "sha": "364f46d2a40652beb7d20a27c3755376ba95d9b8",
                "status": "modified"
            }
        ],
        "message": "HADOOP-8054. NPE with FilterFileSystem (Daryn Sharp via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1245637 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/482f11a5785541ec8d6d812bee42e58c805cf9ce",
        "repo": "hadoop",
        "unit_tests": [
            "TestFilterFileSystem.java",
            "TestLocalFileSystem.java"
        ]
    },
    "hadoop_14384f5": {
        "bug_id": "hadoop_14384f5",
        "commit": "https://github.com/apache/hadoop/commit/14384f5b5142a98a10ce4bffadeb13e89bda9365",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/14384f5b5142a98a10ce4bffadeb13e89bda9365/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=14384f5b5142a98a10ce4bffadeb13e89bda9365",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -482,6 +482,9 @@ Release 2.8.0 - UNRELEASED\n     HDFS-7939. Two fsimage_rollback_* files are created which are not deleted\n     after rollback. (J.Andreina via vinayakumarb)\n \n+    HDFS-8111. NPE thrown when invalid FSImage filename given for\n+    'hdfs oiv_legacy' cmd ( surendra singh lilhore via vinayakumarb )\n+\n Release 2.7.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/14384f5b5142a98a10ce4bffadeb13e89bda9365/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "abbfe6ae4e397ab25521f12df9b00a80a705d4ba",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/14384f5b5142a98a10ce4bffadeb13e89bda9365/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewer.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewer.java?ref=14384f5b5142a98a10ce4bffadeb13e89bda9365",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewer.java",
                "patch": "@@ -137,7 +137,11 @@ public void go() throws IOException  {\n       done = true;\n     } finally {\n       if (!done) {\n-        LOG.error(\"image loading failed at offset \" + tracker.getPos());\n+        if (tracker != null) {\n+          LOG.error(\"image loading failed at offset \" + tracker.getPos());\n+        } else {\n+          LOG.error(\"Failed to load image file.\");\n+        }\n       }\n       IOUtils.cleanup(LOG, in, tracker);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/14384f5b5142a98a10ce4bffadeb13e89bda9365/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/offlineImageViewer/OfflineImageViewer.java",
                "sha": "7f81ba8e67fa6622b5e879eca44ff6bcf4d0417f",
                "status": "modified"
            }
        ],
        "message": "HDFS-8111. NPE thrown when invalid FSImage filename given for 'hdfs oiv_legacy' cmd ( Contributed by surendra singh lilhore )",
        "parent": "https://github.com/apache/hadoop/commit/f8f5887209a7d8e53c0a77abef275cbcaf1f7a5b",
        "repo": "hadoop",
        "unit_tests": [
            "TestOfflineImageViewer.java"
        ]
    },
    "hadoop_14e6f1e": {
        "bug_id": "hadoop_14e6f1e",
        "commit": "https://github.com/apache/hadoop/commit/14e6f1e796bfd77a9505063dfbb36579f124a2e9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/14e6f1e796bfd77a9505063dfbb36579f124a2e9/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=14e6f1e796bfd77a9505063dfbb36579f124a2e9",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -144,6 +144,9 @@ Release 0.23.1 - Unreleased\n     MAPREDUCE-3369. Migrate MR1 tests to run on MR2 using the new interfaces\n     introduced in MAPREDUCE-3169. (Ahmed Radwan via tomwhite)\n \n+    MAPREDUCE-3518. mapred queue -info <queue> -showJobs throws NPE. \n+    (Jonathan Eagles via mahadev)\n+\n   OPTIMIZATIONS\n \n   BUG FIXES",
                "raw_url": "https://github.com/apache/hadoop/raw/14e6f1e796bfd77a9505063dfbb36579f124a2e9/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "f53ffec718a02c41aca8181ac32600b7c013dd30",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/14e6f1e796bfd77a9505063dfbb36579f124a2e9/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java?ref=14e6f1e796bfd77a9505063dfbb36579f124a2e9",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java",
                "patch": "@@ -432,7 +432,6 @@ public String getFailureInfo() throws IOException {\n \n   }\n \n-  Cluster cluster;\n   /**\n    * Ugi of the client. We store this ugi when the client is created and \n    * then make sure that the same ugi is used to run the various protocols.",
                "raw_url": "https://github.com/apache/hadoop/raw/14e6f1e796bfd77a9505063dfbb36579f124a2e9/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/JobClient.java",
                "sha": "fa3d799fe70d60bb3247edc940ff2f9d5f1d953e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/14e6f1e796bfd77a9505063dfbb36579f124a2e9/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java?ref=14e6f1e796bfd77a9505063dfbb36579f124a2e9",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java",
                "patch": "@@ -55,7 +55,7 @@\n @InterfaceStability.Stable\n public class CLI extends Configured implements Tool {\n   private static final Log LOG = LogFactory.getLog(CLI.class);\n-  private Cluster cluster;\n+  protected Cluster cluster;\n \n   public CLI() {\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/14e6f1e796bfd77a9505063dfbb36579f124a2e9/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/tools/CLI.java",
                "sha": "f7ac9c40a6a2074a6602bc8d12548d8c403682c0",
                "status": "modified"
            },
            {
                "additions": 73,
                "blob_url": "https://github.com/apache/hadoop/blob/14e6f1e796bfd77a9505063dfbb36579f124a2e9/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "changes": 81,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java?ref=14e6f1e796bfd77a9505063dfbb36579f124a2e9",
                "deletions": 8,
                "filename": "hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "patch": "@@ -19,21 +19,41 @@\n package org.apache.hadoop.mapred;\n \n import static org.junit.Assert.assertEquals;\n+import static org.mockito.Matchers.isA;\n+import static org.mockito.Mockito.atLeastOnce;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.verify;\n import static org.mockito.Mockito.when;\n \n+import java.io.IOException;\n+import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapreduce.Cluster;\n+import org.apache.hadoop.mapreduce.Job;\n+import org.apache.hadoop.mapreduce.JobPriority;\n+import org.apache.hadoop.mapreduce.JobStatus;\n+import org.apache.hadoop.mapreduce.TaskType;\n+import org.apache.hadoop.mapreduce.TaskReport;\n import org.junit.Test;\n \n public class JobClientUnitTest {\n   \n+  public class TestJobClient extends JobClient {\n+\n+    TestJobClient(JobConf jobConf) throws IOException {\n+      super(jobConf);\n+    }\n+\n+    void setCluster(Cluster cluster) {\n+      this.cluster = cluster;\n+    }\n+  }\n+\n   @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testMapTaskReportsWithNullJob() throws Exception {\n-    JobClient client = new JobClient();\n+    TestJobClient client = new TestJobClient(new JobConf());\n     Cluster mockCluster = mock(Cluster.class);\n-    client.cluster = mockCluster;\n+    client.setCluster(mockCluster);\n     JobID id = new JobID(\"test\",0);\n     \n     when(mockCluster.getJob(id)).thenReturn(null);\n@@ -47,9 +67,9 @@ public void testMapTaskReportsWithNullJob() throws Exception {\n   @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testReduceTaskReportsWithNullJob() throws Exception {\n-    JobClient client = new JobClient();\n+    TestJobClient client = new TestJobClient(new JobConf());\n     Cluster mockCluster = mock(Cluster.class);\n-    client.cluster = mockCluster;\n+    client.setCluster(mockCluster);\n     JobID id = new JobID(\"test\",0);\n     \n     when(mockCluster.getJob(id)).thenReturn(null);\n@@ -63,9 +83,9 @@ public void testReduceTaskReportsWithNullJob() throws Exception {\n   @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testSetupTaskReportsWithNullJob() throws Exception {\n-    JobClient client = new JobClient();\n+    TestJobClient client = new TestJobClient(new JobConf());\n     Cluster mockCluster = mock(Cluster.class);\n-    client.cluster = mockCluster;\n+    client.setCluster(mockCluster);\n     JobID id = new JobID(\"test\",0);\n     \n     when(mockCluster.getJob(id)).thenReturn(null);\n@@ -79,9 +99,9 @@ public void testSetupTaskReportsWithNullJob() throws Exception {\n   @SuppressWarnings(\"deprecation\")\n   @Test\n   public void testCleanupTaskReportsWithNullJob() throws Exception {\n-    JobClient client = new JobClient();\n+    TestJobClient client = new TestJobClient(new JobConf());\n     Cluster mockCluster = mock(Cluster.class);\n-    client.cluster = mockCluster;\n+    client.setCluster(mockCluster);\n     JobID id = new JobID(\"test\",0);\n     \n     when(mockCluster.getJob(id)).thenReturn(null);\n@@ -91,4 +111,49 @@ public void testCleanupTaskReportsWithNullJob() throws Exception {\n     \n     verify(mockCluster).getJob(id);\n   }\n+\n+  @Test\n+  public void testShowJob() throws Exception {\n+    TestJobClient client = new TestJobClient(new JobConf());\n+    JobID jobID = new JobID(\"test\", 0);\n+\n+    JobStatus mockJobStatus = mock(JobStatus.class);\n+    when(mockJobStatus.getJobID()).thenReturn(jobID);\n+    when(mockJobStatus.getState()).thenReturn(JobStatus.State.RUNNING);\n+    when(mockJobStatus.getStartTime()).thenReturn(0L);\n+    when(mockJobStatus.getUsername()).thenReturn(\"mockuser\");\n+    when(mockJobStatus.getQueue()).thenReturn(\"mockqueue\");\n+    when(mockJobStatus.getPriority()).thenReturn(JobPriority.NORMAL);\n+    when(mockJobStatus.getNumUsedSlots()).thenReturn(1);\n+    when(mockJobStatus.getNumReservedSlots()).thenReturn(1);\n+    when(mockJobStatus.getUsedMem()).thenReturn(1024);\n+    when(mockJobStatus.getReservedMem()).thenReturn(512);\n+    when(mockJobStatus.getNeededMem()).thenReturn(2048);\n+    when(mockJobStatus.getSchedulingInfo()).thenReturn(\"NA\");\n+\n+    Job mockJob = mock(Job.class);\n+    when(mockJob.getTaskReports(isA(TaskType.class))).thenReturn(new TaskReport[0]);\n+\n+    Cluster mockCluster = mock(Cluster.class);\n+    when(mockCluster.getJob(jobID)).thenReturn(mockJob);\n+\n+    client.setCluster(mockCluster);\n+    \n+    \n+    client.displayJobList(new JobStatus[] {mockJobStatus});\n+    verify(mockJobStatus, atLeastOnce()).getJobID();\n+    verify(mockJob, atLeastOnce()).getTaskReports(isA(TaskType.class));\n+    verify(mockCluster, atLeastOnce()).getJob(jobID);\n+    verify(mockJobStatus).getState();\n+    verify(mockJobStatus).getStartTime();\n+    verify(mockJobStatus).getUsername();\n+    verify(mockJobStatus).getQueue();\n+    verify(mockJobStatus).getPriority();\n+    verify(mockJobStatus).getNumUsedSlots();\n+    verify(mockJobStatus).getNumReservedSlots();\n+    verify(mockJobStatus).getUsedMem();\n+    verify(mockJobStatus).getReservedMem();\n+    verify(mockJobStatus).getNeededMem();\n+    verify(mockJobStatus).getSchedulingInfo();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/14e6f1e796bfd77a9505063dfbb36579f124a2e9/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/JobClientUnitTest.java",
                "sha": "3f54e09a33d707a0e388239e7e2d8d9a0d274c07",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3518. mapred queue -info <queue> -showJobs throws NPE. (Jonathan Eagles via mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1213464 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/6fc7e2e002c1295a782fd2402d7f4e37194702b0",
        "repo": "hadoop",
        "unit_tests": [
            "TestJobClient.java",
            "TestCLI.java"
        ]
    },
    "hadoop_160b6fd": {
        "bug_id": "hadoop_160b6fd",
        "commit": "https://github.com/apache/hadoop/commit/160b6fd4966f5189f988eaf0f094867fb2155c04",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/160b6fd4966f5189f988eaf0f094867fb2155c04/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/CHANGES.txt?ref=160b6fd4966f5189f988eaf0f094867fb2155c04",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -455,6 +455,8 @@ Release 0.22.0 - Unreleased\n \n     HADOOP-7046. Fix Findbugs warning in Configuration. (Po Cheung via shv)\n \n+    HADOOP-7118. Fix NPE in Configuration.writeXml (todd)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop/raw/160b6fd4966f5189f988eaf0f094867fb2155c04/CHANGES.txt",
                "sha": "18504560f655dd8c1cf667f83c94190fcf02c6b3",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/160b6fd4966f5189f988eaf0f094867fb2155c04/src/java/org/apache/hadoop/conf/Configuration.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/org/apache/hadoop/conf/Configuration.java?ref=160b6fd4966f5189f988eaf0f094867fb2155c04",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/conf/Configuration.java",
                "patch": "@@ -1620,6 +1620,7 @@ private synchronized Document asXmlDocument() throws IOException {\n     Element conf = doc.createElement(\"configuration\");\n     doc.appendChild(conf);\n     conf.appendChild(doc.createTextNode(\"\\n\"));\n+    getProps(); // ensure properties is set\n     for (Enumeration e = properties.keys(); e.hasMoreElements();) {\n       String name = (String)e.nextElement();\n       Object object = properties.get(name);",
                "raw_url": "https://github.com/apache/hadoop/raw/160b6fd4966f5189f988eaf0f094867fb2155c04/src/java/org/apache/hadoop/conf/Configuration.java",
                "sha": "c05acf36208020f5eca5fee3f4573c6e2292ace9",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/160b6fd4966f5189f988eaf0f094867fb2155c04/src/test/core/org/apache/hadoop/conf/TestConfiguration.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/test/core/org/apache/hadoop/conf/TestConfiguration.java?ref=160b6fd4966f5189f988eaf0f094867fb2155c04",
                "deletions": 0,
                "filename": "src/test/core/org/apache/hadoop/conf/TestConfiguration.java",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.conf;\n \n import java.io.BufferedWriter;\n+import java.io.ByteArrayOutputStream;\n import java.io.File;\n import java.io.FileWriter;\n import java.io.IOException;\n@@ -255,6 +256,16 @@ public void testToString() throws IOException {\n     assertEquals(expectedOutput, conf.toString());\n   }\n   \n+  public void testWriteXml() throws IOException {\n+    Configuration conf = new Configuration();\n+    ByteArrayOutputStream baos = new ByteArrayOutputStream(); \n+    conf.writeXml(baos);\n+    String result = baos.toString();\n+    assertTrue(\"Result has proper header\", result.startsWith(\n+        \"<?xml version=\\\"1.0\\\" encoding=\\\"UTF-8\\\" standalone=\\\"no\\\"?><configuration>\"));\n+    assertTrue(\"Result has proper footer\", result.endsWith(\"</configuration>\"));\n+  }\n+  \n   public void testIncludes() throws Exception {\n     tearDown();\n     System.out.println(\"XXX testIncludes\");",
                "raw_url": "https://github.com/apache/hadoop/raw/160b6fd4966f5189f988eaf0f094867fb2155c04/src/test/core/org/apache/hadoop/conf/TestConfiguration.java",
                "sha": "fc9deef34ff7c4cc1cf51427257e6731420180b4",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7118. Fix NPE in Configuration.writeXml. Contributed by Todd Lipcon\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1063613 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/448f8dbb9fd9bf2e0ef72dda7bb235915deca94f",
        "repo": "hadoop",
        "unit_tests": [
            "TestConfiguration.java"
        ]
    },
    "hadoop_1bea785": {
        "bug_id": "hadoop_1bea785",
        "commit": "https://github.com/apache/hadoop/commit/1bea785020a538115b3e08f41ff88167033d2775",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=1bea785020a538115b3e08f41ff88167033d2775",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -839,8 +839,13 @@ private void dumpBlockMeta(Block block, PrintWriter out) {\n       new ArrayList<DatanodeStorageInfo>();\n     \n     NumberReplicas numReplicas = new NumberReplicas();\n+    BlockInfo blockInfo = getStoredBlock(block);\n+    if (blockInfo == null) {\n+      out.println(\"Block \"+ block + \" is Null\");\n+      return;\n+    }\n     // source node returned is not used\n-    chooseSourceDatanodes(getStoredBlock(block), containingNodes,\n+    chooseSourceDatanodes(blockInfo, containingNodes,\n         containingLiveReplicasNodes, numReplicas,\n         new ArrayList<Byte>(), LowRedundancyBlocks.LEVEL);\n     \n@@ -849,7 +854,7 @@ private void dumpBlockMeta(Block block, PrintWriter out) {\n     assert containingLiveReplicasNodes.size() >= numReplicas.liveReplicas();\n     int usableReplicas = numReplicas.liveReplicas() +\n                          numReplicas.decommissionedAndDecommissioning();\n-    \n+\n     if (block instanceof BlockInfo) {\n       BlockCollection bc = getBlockCollection((BlockInfo)block);\n       String fileName = (bc == null) ? \"[orphaned]\" : bc.getName();\n@@ -1765,8 +1770,8 @@ public void setPostponeBlocksFromFuture(boolean postpone) {\n     this.shouldPostponeBlocksFromFuture  = postpone;\n   }\n \n-\n-  private void postponeBlock(Block blk) {\n+  @VisibleForTesting\n+  void postponeBlock(Block blk) {\n     postponedMisreplicatedBlocks.add(blk);\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "6d142f9e64ae60ac287304a712f94fafb898a7c6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=1bea785020a538115b3e08f41ff88167033d2775",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -1768,10 +1768,10 @@ public BlocksWithLocations getBlocks(DatanodeID datanode, long size, long\n   void metaSave(String filename) throws IOException {\n     String operationName = \"metaSave\";\n     checkSuperuserPrivilege(operationName);\n-    checkOperation(OperationCategory.UNCHECKED);\n+    checkOperation(OperationCategory.READ);\n     writeLock();\n     try {\n-      checkOperation(OperationCategory.UNCHECKED);\n+      checkOperation(OperationCategory.READ);\n       File file = new File(System.getProperty(\"hadoop.log.dir\"), filename);\n       PrintWriter out = new PrintWriter(new BufferedWriter(\n           new OutputStreamWriter(new FileOutputStream(file), Charsets.UTF_8)));",
                "raw_url": "https://github.com/apache/hadoop/raw/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "d0fdbac822803eaa2b336c5e992c482d44afe71d",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java?ref=1bea785020a538115b3e08f41ff88167033d2775",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
                "patch": "@@ -94,6 +94,7 @@\n import org.apache.hadoop.ipc.RemoteException;\n import org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB;\n import org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolPB;\n+import org.apache.hadoop.ipc.StandbyException;\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.security.RefreshUserMappingsProtocol;\n import org.apache.hadoop.security.SecurityUtil;\n@@ -1537,11 +1538,20 @@ public int metaSave(String[] argv, int idx) throws IOException {\n           nsId, ClientProtocol.class);\n       List<IOException> exceptions = new ArrayList<>();\n       for (ProxyAndInfo<ClientProtocol> proxy : proxies) {\n-        try{\n+        try {\n           proxy.getProxy().metaSave(pathname);\n           System.out.println(\"Created metasave file \" + pathname\n               + \" in the log directory of namenode \" + proxy.getAddress());\n-        } catch (IOException ioe){\n+        } catch (RemoteException re) {\n+          Exception unwrapped =  re.unwrapRemoteException(\n+              StandbyException.class);\n+          if (unwrapped instanceof StandbyException) {\n+            System.out.println(\"Skip Standby NameNode, since it cannot perform\"\n+                + \" metasave operation\");\n+          } else {\n+            throw re;\n+          }\n+        } catch (IOException ioe) {\n           System.out.println(\"Created metasave file \" + pathname\n               + \" in the log directory of namenode \" + proxy.getAddress()\n               + \" failed\");",
                "raw_url": "https://github.com/apache/hadoop/raw/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSAdmin.java",
                "sha": "d1f83622498f38d5f62a5d2485f1750d87e49aec",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/hadoop/blob/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java?ref=1bea785020a538115b3e08f41ff88167033d2775",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java",
                "patch": "@@ -1478,6 +1478,41 @@ private void verifyPlacementPolicy(final MiniDFSCluster cluster,\n     }\n   }\n \n+  /**\n+   * Unit test to check the race condition for adding a Block to\n+   * postponedMisreplicatedBlocks set which may not present in BlockManager\n+   * thus avoiding NullPointerException.\n+   **/\n+  @Test\n+  public void testMetaSavePostponedMisreplicatedBlocks() throws IOException {\n+    bm.postponeBlock(new Block());\n+\n+    File file = new File(\"test.log\");\n+    PrintWriter out = new PrintWriter(file);\n+\n+    bm.metaSave(out);\n+    out.flush();\n+\n+    FileInputStream fstream = new FileInputStream(file);\n+    DataInputStream in = new DataInputStream(fstream);\n+\n+    BufferedReader reader = new BufferedReader(new InputStreamReader(in));\n+    StringBuffer buffer = new StringBuffer();\n+    String line;\n+    try {\n+      while ((line = reader.readLine()) != null) {\n+        buffer.append(line);\n+      }\n+      String output = buffer.toString();\n+      assertTrue(\"Metasave output should not have null block \",\n+          output.contains(\"Block blk_0_0 is Null\"));\n+\n+    } finally {\n+      reader.close();\n+      file.delete();\n+    }\n+  }\n+\n   @Test\n   public void testMetaSaveMissingReplicas() throws Exception {\n     List<DatanodeStorageInfo> origStorages = getStorages(0, 1);",
                "raw_url": "https://github.com/apache/hadoop/raw/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockManager.java",
                "sha": "de0e1a69cd3e8d8b2b06c295563e39bc574ca3ee",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSAdminWithHA.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSAdminWithHA.java?ref=1bea785020a538115b3e08f41ff88167033d2775",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSAdminWithHA.java",
                "patch": "@@ -414,16 +414,21 @@ public void testSetNegativeBalancerBandwidth() throws Exception {\n   @Test (timeout = 30000)\n   public void testMetaSave() throws Exception {\n     setUpHaCluster(false);\n+    cluster.getDfsCluster().transitionToActive(0);\n     int exitCode = admin.run(new String[] {\"-metasave\", \"dfs.meta\"});\n     assertEquals(err.toString().trim(), 0, exitCode);\n-    String message = \"Created metasave file dfs.meta in the log directory\"\n-        + \" of namenode.*\";\n-    assertOutputMatches(message + newLine + message + newLine);\n+    String messageFromActiveNN = \"Created metasave file dfs.meta \"\n+        + \"in the log directory of namenode.*\";\n+    String messageFromStandbyNN = \"Skip Standby NameNode, since it \"\n+        + \"cannot perform metasave operation\";\n+    assertOutputMatches(messageFromActiveNN + newLine +\n+        messageFromStandbyNN + newLine);\n   }\n \n   @Test (timeout = 30000)\n   public void testMetaSaveNN1UpNN2Down() throws Exception {\n     setUpHaCluster(false);\n+    cluster.getDfsCluster().transitionToActive(0);\n     cluster.getDfsCluster().shutdownNameNode(1);\n     int exitCode = admin.run(new String[] {\"-metasave\", \"dfs.meta\"});\n     assertNotEquals(err.toString().trim(), 0, exitCode);\n@@ -437,6 +442,7 @@ public void testMetaSaveNN1UpNN2Down() throws Exception {\n   @Test (timeout = 30000)\n   public void testMetaSaveNN1DownNN2Up() throws Exception {\n     setUpHaCluster(false);\n+    cluster.getDfsCluster().transitionToActive(1);\n     cluster.getDfsCluster().shutdownNameNode(0);\n     int exitCode = admin.run(new String[] {\"-metasave\", \"dfs.meta\"});\n     assertNotEquals(err.toString().trim(), 0, exitCode);",
                "raw_url": "https://github.com/apache/hadoop/raw/1bea785020a538115b3e08f41ff88167033d2775/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSAdminWithHA.java",
                "sha": "627ea0710df58272a12200e29d562f0201bfef56",
                "status": "modified"
            }
        ],
        "message": "HDFS-14081. hdfs dfsadmin -metasave metasave_test results NPE. Contributed by Shweta Yakkali.\n\nSigned-off-by: Wei-Chiu Chuang <weichiu@apache.org>",
        "parent": "https://github.com/apache/hadoop/commit/f5b4e0f971b138666a1f7015f387ae960f85d589",
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockManager.java",
            "TestFSNamesystem.java",
            "TestDFSAdmin.java"
        ]
    },
    "hadoop_1c05393": {
        "bug_id": "hadoop_1c05393",
        "commit": "https://github.com/apache/hadoop/commit/1c05393b51748033279bff31dbc5c5cae7fc3a86",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/1c05393b51748033279bff31dbc5c5cae7fc3a86/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=1c05393b51748033279bff31dbc5c5cae7fc3a86",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -2415,6 +2415,9 @@ Release 2.8.0 - UNRELEASED\n     HDFS-9467. Fix data race accessing writeLockHeldTimeStamp in FSNamesystem.\n     (Mingliang Liu via jing9)\n \n+    HDFS-9336. deleteSnapshot throws NPE when snapshotname is null.\n+    (Brahma Reddy Battula via aajisaka)\n+\n Release 2.7.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/1c05393b51748033279bff31dbc5c5cae7fc3a86/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "bf77e73fe4bf597caef76013216626ab3a75dd47",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/1c05393b51748033279bff31dbc5c5cae7fc3a86/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java?ref=1c05393b51748033279bff31dbc5c5cae7fc3a86",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "patch": "@@ -1646,6 +1646,9 @@ public String createSnapshot(String snapshotRoot, String snapshotName)\n   public void deleteSnapshot(String snapshotRoot, String snapshotName)\n       throws IOException {\n     checkNNStartup();\n+    if (snapshotName == null || snapshotName.isEmpty()) {\n+      throw new IOException(\"The snapshot name is null or empty.\");\n+    }\n     namesystem.checkOperation(OperationCategory.WRITE);\n     metrics.incrDeleteSnapshotOps();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);",
                "raw_url": "https://github.com/apache/hadoop/raw/1c05393b51748033279bff31dbc5c5cae7fc3a86/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "sha": "490f3e403e9534f33f2dc70db779fbf44752f4f9",
                "status": "modified"
            },
            {
                "additions": 84,
                "blob_url": "https://github.com/apache/hadoop/blob/1c05393b51748033279bff31dbc5c5cae7fc3a86/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRpcServerMethods.java",
                "changes": 84,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRpcServerMethods.java?ref=1c05393b51748033279bff31dbc5c5cae7fc3a86",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRpcServerMethods.java",
                "patch": "@@ -0,0 +1,84 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.hdfs.server.namenode;\n+\n+import java.io.IOException;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.UnresolvedLinkException;\n+import org.apache.hadoop.hdfs.HdfsConfiguration;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols;\n+import org.apache.hadoop.security.AccessControlException;\n+import org.apache.hadoop.test.GenericTestUtils;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+public class TestNameNodeRpcServerMethods {\n+  private static NamenodeProtocols nnRpc;\n+  private static Configuration conf;\n+  private static MiniDFSCluster cluster;\n+\n+  /** Start a cluster */\n+  @Before\n+  public void setup() throws Exception {\n+    conf = new HdfsConfiguration();\n+    cluster = new MiniDFSCluster.Builder(conf).build();\n+    cluster.waitActive();\n+    nnRpc = cluster.getNameNode().getRpcServer();\n+  }\n+\n+  /**\n+   * Cleanup after the test\n+   *\n+   * @throws IOException\n+   * @throws UnresolvedLinkException\n+   * @throws SafeModeException\n+   * @throws AccessControlException\n+   */\n+  @After\n+  public void cleanup() throws IOException {\n+    if (cluster != null)\n+      cluster.shutdown();\n+  }\n+\n+  @Test\n+  public void testDeleteSnapshotWhenSnapshotNameIsEmpty() throws Exception {\n+    String dir = \"/testNamenodeRetryCache/testDelete\";\n+    try {\n+      nnRpc.deleteSnapshot(dir, null);\n+      Assert.fail(\"testdeleteSnapshot is not thrown expected exception \");\n+    } catch (IOException e) {\n+      // expected\n+      GenericTestUtils.assertExceptionContains(\n+          \"The snapshot name is null or empty.\", e);\n+    }\n+    try {\n+      nnRpc.deleteSnapshot(dir, \"\");\n+      Assert.fail(\"testdeleteSnapshot is not thrown expected exception\");\n+    } catch (IOException e) {\n+      // expected\n+      GenericTestUtils.assertExceptionContains(\n+          \"The snapshot name is null or empty.\", e);\n+    }\n+\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/1c05393b51748033279bff31dbc5c5cae7fc3a86/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestNameNodeRpcServerMethods.java",
                "sha": "80d97224d6d3babc3111cc98e0330b1d42def52c",
                "status": "added"
            }
        ],
        "message": "HDFS-9336. deleteSnapshot throws NPE when snapshotname is null. Contributed by Brahma Reddy Battula.",
        "parent": "https://github.com/apache/hadoop/commit/9b8e50b424d060e16c1175b1811e7abc476e2468",
        "repo": "hadoop",
        "unit_tests": [
            "TestNameNodeRpcServer.java"
        ]
    },
    "hadoop_1c93025": {
        "bug_id": "hadoop_1c93025",
        "commit": "https://github.com/apache/hadoop/commit/1c93025a1b370db46e345161dbc15e03f829823f",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/1c93025a1b370db46e345161dbc15e03f829823f/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=1c93025a1b370db46e345161dbc15e03f829823f",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -524,6 +524,9 @@ Release 2.6.0 - UNRELEASED\n     YARN-2624. Resource Localization fails on a cluster due to existing cache\n     directories (Anubhav Dhoot via jlowe)\n \n+    YARN-2527. Fixed the potential NPE in ApplicationACLsManager and added test\n+    cases for it. (Benoy Antony via zjshen)\n+\n Release 2.5.1 - 2014-09-05\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/1c93025a1b370db46e345161dbc15e03f829823f/hadoop-yarn-project/CHANGES.txt",
                "sha": "247167399d4b7679245d33f5ba8c1a86b1c8fd8a",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/1c93025a1b370db46e345161dbc15e03f829823f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/server/security/ApplicationACLsManager.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/server/security/ApplicationACLsManager.java?ref=1c93025a1b370db46e345161dbc15e03f829823f",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/server/security/ApplicationACLsManager.java",
                "patch": "@@ -41,6 +41,8 @@\n   private static final Log LOG = LogFactory\n       .getLog(ApplicationACLsManager.class);\n \n+  private static AccessControlList DEFAULT_YARN_APP_ACL \n+    = new AccessControlList(YarnConfiguration.DEFAULT_YARN_APP_ACL);\n   private final Configuration conf;\n   private final AdminACLsManager adminAclsManager;\n   private final ConcurrentMap<ApplicationId, Map<ApplicationAccessType, AccessControlList>> applicationACLS\n@@ -100,18 +102,26 @@ public boolean checkAccess(UserGroupInformation callerUGI,\n     if (!areACLsEnabled()) {\n       return true;\n     }\n-\n-    AccessControlList applicationACL = this.applicationACLS\n-        .get(applicationId).get(applicationAccessType);\n-    if (applicationACL == null) {\n+    AccessControlList applicationACL = DEFAULT_YARN_APP_ACL;\n+    Map<ApplicationAccessType, AccessControlList> acls = this.applicationACLS\n+        .get(applicationId);\n+    if (acls == null) {\n       if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"ACL not found for application \"\n+            + applicationId + \" owned by \"\n+            + applicationOwner + \". Using default [\"\n+            + YarnConfiguration.DEFAULT_YARN_APP_ACL + \"]\");\n+      }\n+    } else {\n+      AccessControlList applicationACLInMap = acls.get(applicationAccessType);\n+      if (applicationACLInMap != null) {\n+        applicationACL = applicationACLInMap;\n+      } else if (LOG.isDebugEnabled()) {\n         LOG.debug(\"ACL not found for access-type \" + applicationAccessType\n             + \" for application \" + applicationId + \" owned by \"\n             + applicationOwner + \". Using default [\"\n             + YarnConfiguration.DEFAULT_YARN_APP_ACL + \"]\");\n       }\n-      applicationACL =\n-          new AccessControlList(YarnConfiguration.DEFAULT_YARN_APP_ACL);\n     }\n \n     // Allow application-owner for any type of access on the application",
                "raw_url": "https://github.com/apache/hadoop/raw/1c93025a1b370db46e345161dbc15e03f829823f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/server/security/ApplicationACLsManager.java",
                "sha": "e8e3cb56b66f4534335365c47bf86618c9594f6d",
                "status": "modified"
            },
            {
                "additions": 180,
                "blob_url": "https://github.com/apache/hadoop/blob/1c93025a1b370db46e345161dbc15e03f829823f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/server/security/TestApplicationACLsManager.java",
                "changes": 180,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/server/security/TestApplicationACLsManager.java?ref=1c93025a1b370db46e345161dbc15e03f829823f",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/server/security/TestApplicationACLsManager.java",
                "patch": "@@ -0,0 +1,180 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.yarn.server.security;\n+\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.yarn.api.records.ApplicationAccessType;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.conf.YarnConfiguration;\n+import org.junit.Test;\n+\n+public class TestApplicationACLsManager {\n+\n+  private static final String ADMIN_USER = \"adminuser\";\n+  private static final String APP_OWNER = \"appuser\";\n+  private static final String TESTUSER1 = \"testuser1\";\n+  private static final String TESTUSER2 = \"testuser2\";\n+  private static final String TESTUSER3 = \"testuser3\";\n+\n+  @Test\n+  public void testCheckAccess() {\n+    Configuration conf = new Configuration();\n+    conf.setBoolean(YarnConfiguration.YARN_ACL_ENABLE,\n+        true);\n+    conf.set(YarnConfiguration.YARN_ADMIN_ACL,\n+        ADMIN_USER);\n+    ApplicationACLsManager aclManager = new ApplicationACLsManager(conf);\n+    Map<ApplicationAccessType, String> aclMap = \n+        new HashMap<ApplicationAccessType, String>();\n+    aclMap.put(ApplicationAccessType.VIEW_APP, TESTUSER1 + \",\" + TESTUSER3);\n+    aclMap.put(ApplicationAccessType.MODIFY_APP, TESTUSER1);\n+    ApplicationId appId = ApplicationId.newInstance(1, 1);\n+    aclManager.addApplication(appId, aclMap);\n+\n+    //User in ACL, should be allowed access\n+    UserGroupInformation testUser1 = UserGroupInformation\n+        .createRemoteUser(TESTUSER1);\n+    assertTrue(aclManager.checkAccess(testUser1, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertTrue(aclManager.checkAccess(testUser1, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+\n+    //User NOT in ACL, should not be allowed access\n+    UserGroupInformation testUser2 = UserGroupInformation\n+        .createRemoteUser(TESTUSER2);\n+    assertFalse(aclManager.checkAccess(testUser2, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertFalse(aclManager.checkAccess(testUser2, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+\n+    //User has View access, but not modify access\n+    UserGroupInformation testUser3 = UserGroupInformation\n+        .createRemoteUser(TESTUSER3);\n+    assertTrue(aclManager.checkAccess(testUser3, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertFalse(aclManager.checkAccess(testUser3, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+\n+    //Application Owner should have all access\n+    UserGroupInformation appOwner = UserGroupInformation\n+        .createRemoteUser(APP_OWNER);\n+    assertTrue(aclManager.checkAccess(appOwner, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertTrue(aclManager.checkAccess(appOwner, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+\n+    //Admin should have all access\n+    UserGroupInformation adminUser = UserGroupInformation\n+        .createRemoteUser(ADMIN_USER);\n+    assertTrue(aclManager.checkAccess(adminUser, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertTrue(aclManager.checkAccess(adminUser, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+  }\n+\n+  @Test\n+  public void testCheckAccessWithNullACLS() {\n+    Configuration conf = new Configuration();\n+    conf.setBoolean(YarnConfiguration.YARN_ACL_ENABLE,\n+        true);\n+    conf.set(YarnConfiguration.YARN_ADMIN_ACL,\n+        ADMIN_USER);\n+    ApplicationACLsManager aclManager = new ApplicationACLsManager(conf);\n+    UserGroupInformation appOwner = UserGroupInformation\n+        .createRemoteUser(APP_OWNER);\n+    ApplicationId appId = ApplicationId.newInstance(1, 1);\n+    //Application ACL is not added\n+\n+    //Application Owner should have all access even if Application ACL is not added\n+    assertTrue(aclManager.checkAccess(appOwner, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+    assertTrue(aclManager.checkAccess(appOwner, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+\n+    //Admin should have all access\n+    UserGroupInformation adminUser = UserGroupInformation\n+        .createRemoteUser(ADMIN_USER);\n+    assertTrue(aclManager.checkAccess(adminUser, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertTrue(aclManager.checkAccess(adminUser, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+\n+    // A regular user should Not have access\n+    UserGroupInformation testUser1 = UserGroupInformation\n+        .createRemoteUser(TESTUSER1);\n+    assertFalse(aclManager.checkAccess(testUser1, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertFalse(aclManager.checkAccess(testUser1, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+  }\n+  \n+  @Test\n+  public void testCheckAccessWithPartialACLS() {\n+    Configuration conf = new Configuration();\n+    conf.setBoolean(YarnConfiguration.YARN_ACL_ENABLE,\n+        true);\n+    conf.set(YarnConfiguration.YARN_ADMIN_ACL,\n+        ADMIN_USER);\n+    ApplicationACLsManager aclManager = new ApplicationACLsManager(conf);\n+    UserGroupInformation appOwner = UserGroupInformation\n+        .createRemoteUser(APP_OWNER);\n+    // Add only the VIEW ACLS\n+    Map<ApplicationAccessType, String> aclMap = \n+        new HashMap<ApplicationAccessType, String>();\n+    aclMap.put(ApplicationAccessType.VIEW_APP, TESTUSER1 );\n+    ApplicationId appId = ApplicationId.newInstance(1, 1);\n+    aclManager.addApplication(appId, aclMap);\n+\n+    //Application Owner should have all access even if Application ACL is not added\n+    assertTrue(aclManager.checkAccess(appOwner, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+    assertTrue(aclManager.checkAccess(appOwner, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+\n+    //Admin should have all access\n+    UserGroupInformation adminUser = UserGroupInformation\n+        .createRemoteUser(ADMIN_USER);\n+    assertTrue(aclManager.checkAccess(adminUser, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertTrue(aclManager.checkAccess(adminUser, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+\n+    // testuser1 should  have view access only\n+    UserGroupInformation testUser1 = UserGroupInformation\n+        .createRemoteUser(TESTUSER1);\n+    assertTrue(aclManager.checkAccess(testUser1, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertFalse(aclManager.checkAccess(testUser1, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+    \n+    // A testuser2 should Not have access\n+    UserGroupInformation testUser2 = UserGroupInformation\n+        .createRemoteUser(TESTUSER2);\n+    assertFalse(aclManager.checkAccess(testUser2, ApplicationAccessType.VIEW_APP, \n+        APP_OWNER, appId));\n+    assertFalse(aclManager.checkAccess(testUser2, ApplicationAccessType.MODIFY_APP, \n+        APP_OWNER, appId));\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/1c93025a1b370db46e345161dbc15e03f829823f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/server/security/TestApplicationACLsManager.java",
                "sha": "2db1da90416ab4fd4d6f4659cdda38fff90f0ba5",
                "status": "added"
            }
        ],
        "message": "YARN-2527. Fixed the potential NPE in ApplicationACLsManager and added test cases for it. Contributed by Benoy Antony.",
        "parent": "https://github.com/apache/hadoop/commit/f679ca38ce0365c97f1dba79e333a8de18733b8a",
        "repo": "hadoop",
        "unit_tests": [
            "TestApplicationACLsManager.java"
        ]
    },
    "hadoop_1fbb662": {
        "bug_id": "hadoop_1fbb662",
        "commit": "https://github.com/apache/hadoop/commit/1fbb662c7092d08a540acff7e92715693412e486",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/1fbb662c7092d08a540acff7e92715693412e486/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=1fbb662c7092d08a540acff7e92715693412e486",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -4487,8 +4487,12 @@ private void scanAndCompactStorages() throws InterruptedException {\n         for (int i = 0; i < datanodesAndStorages.size(); i += 2) {\n           namesystem.writeLock();\n           try {\n-            DatanodeStorageInfo storage = datanodeManager.\n-                getDatanode(datanodesAndStorages.get(i)).\n+            final DatanodeDescriptor dn = datanodeManager.\n+                getDatanode(datanodesAndStorages.get(i));\n+            if (dn == null) {\n+              continue;\n+            }\n+            final DatanodeStorageInfo storage = dn.\n                 getStorageInfo(datanodesAndStorages.get(i + 1));\n             if (storage != null) {\n               boolean aborted =",
                "raw_url": "https://github.com/apache/hadoop/raw/1fbb662c7092d08a540acff7e92715693412e486/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "e83cbc6ef4a266939307a3b13b46c9b5765d07c9",
                "status": "modified"
            }
        ],
        "message": "HDFS-12363. Possible NPE in BlockManager$StorageInfoDefragmenter#scanAndCompactStorages. Contributed by Xiao Chen",
        "parent": "https://github.com/apache/hadoop/commit/7ecc6dbed62c80397f71949bee41dcd03065755c",
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockManager.java"
        ]
    },
    "hadoop_2044967": {
        "bug_id": "hadoop_2044967",
        "commit": "https://github.com/apache/hadoop/commit/2044967e7581f00c3f6378860426a69078faf694",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/2044967e7581f00c3f6378860426a69078faf694/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/YarnCLI.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/YarnCLI.java?ref=2044967e7581f00c3f6378860426a69078faf694",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/YarnCLI.java",
                "patch": "@@ -72,6 +72,10 @@ public void setClient(YarnClient client) {\n   }\n \n   public void stop() {\n-    this.client.stop();\n+    // this.client may be null when it is called before\n+    // invoking `createAndStartYarnClient`\n+    if (this.client != null) {\n+      this.client.stop();\n+    }\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/2044967e7581f00c3f6378860426a69078faf694/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/cli/YarnCLI.java",
                "sha": "c1e02d5fd1ea0ff54c0cb35de9498a5f92276488",
                "status": "modified"
            }
        ],
        "message": "YARN-9246 NPE when executing a command yarn node -status or -states without additional arguments. Contributed by Masahiro Tanaka",
        "parent": "https://github.com/apache/hadoop/commit/194f0b49fb8ae3b876dde82f04f906f0ab0f5bdf",
        "repo": "hadoop",
        "unit_tests": [
            "TestYarnCLI.java"
        ]
    },
    "hadoop_209b169": {
        "bug_id": "hadoop_209b169",
        "commit": "https://github.com/apache/hadoop/commit/209b1699fcd150676d4cc47e8e817796086c1986",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/209b1699fcd150676d4cc47e8e817796086c1986/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=209b1699fcd150676d4cc47e8e817796086c1986",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -435,6 +435,9 @@ Release 2.6.0 - UNRELEASED\n     MAPREDUCE-5873. Shuffle bandwidth computation includes time spent waiting\n     for maps (Siqi Li via jlowe)\n \n+    MAPREDUCE-5542. Killing a job just as it finishes can generate an NPE in\n+    client (Rohith via jlowe)\n+\n Release 2.5.1 - 2014-09-05\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/209b1699fcd150676d4cc47e8e817796086c1986/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "e152b480bc15917abc91ec151811a34eeb5e03a5",
                "status": "modified"
            },
            {
                "additions": 55,
                "blob_url": "https://github.com/apache/hadoop/blob/209b1699fcd150676d4cc47e8e817796086c1986/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java",
                "changes": 76,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java?ref=209b1699fcd150676d4cc47e8e817796086c1986",
                "deletions": 21,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java",
                "patch": "@@ -594,16 +594,50 @@ public JobStatus getJobStatus(JobID jobID) throws IOException,\n         .getTaskReports(jobID, taskType);\n   }\n \n+  private void killUnFinishedApplication(ApplicationId appId)\n+      throws IOException {\n+    ApplicationReport application = null;\n+    try {\n+      application = resMgrDelegate.getApplicationReport(appId);\n+    } catch (YarnException e) {\n+      throw new IOException(e);\n+    }\n+    if (application.getYarnApplicationState() == YarnApplicationState.FINISHED\n+        || application.getYarnApplicationState() == YarnApplicationState.FAILED\n+        || application.getYarnApplicationState() == YarnApplicationState.KILLED) {\n+      return;\n+    }\n+    killApplication(appId);\n+  }\n+\n+  private void killApplication(ApplicationId appId) throws IOException {\n+    try {\n+      resMgrDelegate.killApplication(appId);\n+    } catch (YarnException e) {\n+      throw new IOException(e);\n+    }\n+  }\n+\n+  private boolean isJobInTerminalState(JobStatus status) {\n+    return status.getState() == JobStatus.State.KILLED\n+        || status.getState() == JobStatus.State.FAILED\n+        || status.getState() == JobStatus.State.SUCCEEDED;\n+  }\n+\n   @Override\n   public void killJob(JobID arg0) throws IOException, InterruptedException {\n     /* check if the status is not running, if not send kill to RM */\n     JobStatus status = clientCache.getClient(arg0).getJobStatus(arg0);\n+    ApplicationId appId = TypeConverter.toYarn(arg0).getAppId();\n+\n+    // get status from RM and return\n+    if (status == null) {\n+      killUnFinishedApplication(appId);\n+      return;\n+    }\n+\n     if (status.getState() != JobStatus.State.RUNNING) {\n-      try {\n-        resMgrDelegate.killApplication(TypeConverter.toYarn(arg0).getAppId());\n-      } catch (YarnException e) {\n-        throw new IOException(e);\n-      }\n+      killApplication(appId);\n       return;\n     }\n \n@@ -612,26 +646,26 @@ public void killJob(JobID arg0) throws IOException, InterruptedException {\n       clientCache.getClient(arg0).killJob(arg0);\n       long currentTimeMillis = System.currentTimeMillis();\n       long timeKillIssued = currentTimeMillis;\n-      while ((currentTimeMillis < timeKillIssued + 10000L) && (status.getState()\n-          != JobStatus.State.KILLED)) {\n-          try {\n-            Thread.sleep(1000L);\n-          } catch(InterruptedException ie) {\n-            /** interrupted, just break */\n-            break;\n-          }\n-          currentTimeMillis = System.currentTimeMillis();\n-          status = clientCache.getClient(arg0).getJobStatus(arg0);\n+      while ((currentTimeMillis < timeKillIssued + 10000L)\n+          && !isJobInTerminalState(status)) {\n+        try {\n+          Thread.sleep(1000L);\n+        } catch (InterruptedException ie) {\n+          /** interrupted, just break */\n+          break;\n+        }\n+        currentTimeMillis = System.currentTimeMillis();\n+        status = clientCache.getClient(arg0).getJobStatus(arg0);\n+        if (status == null) {\n+          killUnFinishedApplication(appId);\n+          return;\n+        }\n       }\n     } catch(IOException io) {\n       LOG.debug(\"Error when checking for application status\", io);\n     }\n-    if (status.getState() != JobStatus.State.KILLED) {\n-      try {\n-        resMgrDelegate.killApplication(TypeConverter.toYarn(arg0).getAppId());\n-      } catch (YarnException e) {\n-        throw new IOException(e);\n-      }\n+    if (status != null && !isJobInTerminalState(status)) {\n+      killApplication(appId);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/209b1699fcd150676d4cc47e8e817796086c1986/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/main/java/org/apache/hadoop/mapred/YARNRunner.java",
                "sha": "40ef9822783ba3e00505a4de90b01509e17c4541",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/209b1699fcd150676d4cc47e8e817796086c1986/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java?ref=209b1699fcd150676d4cc47e8e817796086c1986",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java",
                "patch": "@@ -86,6 +86,7 @@\n import org.apache.hadoop.yarn.api.records.ApplicationReport;\n import org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext;\n import org.apache.hadoop.yarn.api.records.ContainerLaunchContext;\n+import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n import org.apache.hadoop.yarn.api.records.QueueInfo;\n import org.apache.hadoop.yarn.api.records.YarnApplicationState;\n import org.apache.hadoop.yarn.api.records.YarnClusterMetrics;\n@@ -188,6 +189,16 @@ public ClientServiceDelegate answer(InvocationOnMock invocation)\n             State.RUNNING, JobPriority.HIGH, \"tmp\", \"tmp\", \"tmp\", \"tmp\"));\n     yarnRunner.killJob(jobId);\n     verify(clientDelegate).killJob(jobId);\n+\n+    when(clientDelegate.getJobStatus(any(JobID.class))).thenReturn(null);\n+    when(resourceMgrDelegate.getApplicationReport(any(ApplicationId.class)))\n+        .thenReturn(\n+            ApplicationReport.newInstance(appId, null, \"tmp\", \"tmp\", \"tmp\",\n+                \"tmp\", 0, null, YarnApplicationState.FINISHED, \"tmp\", \"tmp\",\n+                0l, 0l, FinalApplicationStatus.SUCCEEDED, null, null, 0f,\n+                \"tmp\", null));\n+    yarnRunner.killJob(jobId);\n+    verify(clientDelegate).killJob(jobId);\n   }\n \n   @Test(timeout=20000)",
                "raw_url": "https://github.com/apache/hadoop/raw/209b1699fcd150676d4cc47e8e817796086c1986/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapred/TestYARNRunner.java",
                "sha": "420a95f9bbfcf114379518d6ed8c77fcabc6a6e6",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-5542. Killing a job just as it finishes can generate an NPE in client. Contributed by Rohith",
        "parent": "https://github.com/apache/hadoop/commit/a6aa6e42cacdbfcc1c2b7c19e7239204fe9ff654",
        "repo": "hadoop",
        "unit_tests": [
            "TestYARNRunner.java"
        ]
    },
    "hadoop_229472c": {
        "bug_id": "hadoop_229472c",
        "commit": "https://github.com/apache/hadoop/commit/229472cea7920194c48f5294bf763a8bee2ade63",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/229472cea7920194c48f5294bf763a8bee2ade63/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=229472cea7920194c48f5294bf763a8bee2ade63",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -439,6 +439,9 @@ Release 2.3.0 - UNRELEASED\n \n     HADOOP-10100. MiniKDC shouldn't use apacheds-all artifact. (rkanter via tucu)\n \n+    HADOOP-10107. Server.getNumOpenConnections may throw NPE. (Kihwal Lee via\n+    jing9)\n+\n Release 2.2.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/229472cea7920194c48f5294bf763a8bee2ade63/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "2b9aeb88a5a759ab34e1eccab28d261d53a65826",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/229472cea7920194c48f5294bf763a8bee2ade63/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java?ref=229472cea7920194c48f5294bf763a8bee2ade63",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "patch": "@@ -2109,6 +2109,7 @@ protected Server(String bindAddress, int port,\n     // Start the listener here and let it bind to the port\n     listener = new Listener();\n     this.port = listener.getAddress().getPort();    \n+    connectionManager = new ConnectionManager();\n     this.rpcMetrics = RpcMetrics.create(this);\n     this.rpcDetailedMetrics = RpcDetailedMetrics.create(this.port);\n     this.tcpNoDelay = conf.getBoolean(\n@@ -2117,7 +2118,6 @@ protected Server(String bindAddress, int port,\n \n     // Create the responder here\n     responder = new Responder();\n-    connectionManager = new ConnectionManager();\n     \n     if (secretManager != null) {\n       SaslRpcServer.init(conf);",
                "raw_url": "https://github.com/apache/hadoop/raw/229472cea7920194c48f5294bf763a8bee2ade63/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "sha": "7fb395cdb056029d85a7940a7e7fbedd941a5a4c",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10107. Server.getNumOpenConnections may throw NPE. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1543335 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/aa002344d0466a62672eae73cdb2bb2ae7c19a72",
        "repo": "hadoop",
        "unit_tests": [
            "TestServer.java"
        ]
    },
    "hadoop_22d7d1f": {
        "bug_id": "hadoop_22d7d1f",
        "commit": "https://github.com/apache/hadoop/commit/22d7d1f8bfe64ee04a7611b004ece8a4d4e81ea4",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/22d7d1f8bfe64ee04a7611b004ece8a4d4e81ea4/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/impl/ZookeeperFederationStateStore.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/impl/ZookeeperFederationStateStore.java?ref=22d7d1f8bfe64ee04a7611b004ece8a4d4e81ea4",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/impl/ZookeeperFederationStateStore.java",
                "patch": "@@ -422,6 +422,10 @@ public GetSubClusterPoliciesConfigurationsResponse getPoliciesConfigurations(\n     try {\n       for (String child : zkManager.getChildren(policiesZNode)) {\n         SubClusterPolicyConfiguration policy = getPolicy(child);\n+        if (policy == null) {\n+          LOG.warn(\"Policy for queue: {} does not exist.\", child);\n+          continue;\n+        }\n         result.add(policy);\n       }\n     } catch (Exception e) {",
                "raw_url": "https://github.com/apache/hadoop/raw/22d7d1f8bfe64ee04a7611b004ece8a4d4e81ea4/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/store/impl/ZookeeperFederationStateStore.java",
                "sha": "39f498c73a21e8bc4b8804952741b83590aeb91a",
                "status": "modified"
            }
        ],
        "message": "YARN-9601.Potential NPE in ZookeeperFederationStateStore#getPoliciesConfigurations (#908) Contributed by hunshenshi.",
        "parent": "https://github.com/apache/hadoop/commit/b0131bc265453051820e54908e70d39433c227ab",
        "repo": "hadoop",
        "unit_tests": [
            "TestZookeeperFederationStateStore.java"
        ]
    },
    "hadoop_23248f6": {
        "bug_id": "hadoop_23248f6",
        "commit": "https://github.com/apache/hadoop/commit/23248f63aab74a19dba38d348f2b231c8360770a",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java?ref=23248f63aab74a19dba38d348f2b231c8360770a",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "patch": "@@ -305,9 +305,7 @@ private boolean checkAccess(UserGroupInformation callerUGI, String owner,\n     return applicationsACLsManager\n         .checkAccess(callerUGI, operationPerformed, owner,\n             application.getApplicationId()) || queueACLsManager\n-        .checkAccess(callerUGI, QueueACL.ADMINISTER_QUEUE,\n-            application.getQueue(), application.getApplicationId(),\n-            application.getName());\n+        .checkAccess(callerUGI, QueueACL.ADMINISTER_QUEUE, application);\n   }\n \n   ApplicationId getNewApplicationId() {",
                "raw_url": "https://github.com/apache/hadoop/raw/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "sha": "c2c24b94cc755fa0cfdd5e34ebebcb4e7f68cf54",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hadoop/blob/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/QueueACLsManager.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/QueueACLsManager.java?ref=23248f63aab74a19dba38d348f2b231c8360770a",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/QueueACLsManager.java",
                "patch": "@@ -18,20 +18,27 @@\n \n package org.apache.hadoop.yarn.server.resourcemanager.security;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.security.UserGroupInformation;\n-import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.QueueACL;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.security.AccessRequest;\n import org.apache.hadoop.yarn.security.YarnAuthorizationProvider;\n+import org.apache.hadoop.yarn.server.resourcemanager.ResourceTrackerService;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;\n \n import com.google.common.annotations.VisibleForTesting;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CSQueue;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler;\n \n public class QueueACLsManager {\n+\n+  private static final Log LOG = LogFactory.getLog(QueueACLsManager.class);\n+\n   private ResourceScheduler scheduler;\n   private boolean isACLsEnable;\n   private YarnAuthorizationProvider authorizer;\n@@ -49,17 +56,28 @@ public QueueACLsManager(ResourceScheduler scheduler, Configuration conf) {\n   }\n \n   public boolean checkAccess(UserGroupInformation callerUGI, QueueACL acl,\n-      String queueName, ApplicationId appId, String appName) {\n+      RMApp app) {\n     if (!isACLsEnable) {\n       return true;\n     }\n+\n     if (scheduler instanceof CapacityScheduler) {\n-      return authorizer.checkPermission(new AccessRequest(\n-          ((CapacityScheduler) scheduler).getQueue(queueName)\n-              .getPrivilegedEntity(), callerUGI,\n-          SchedulerUtils.toAccessType(acl), appId.toString(), appName));\n+      CSQueue queue = ((CapacityScheduler) scheduler).getQueue(app.getQueue());\n+      if (queue == null) {\n+        // Application exists but the associated queue does not exist.\n+        // This may be due to queue is removed after RM restarts. Here, we choose\n+        // to allow users to be able to view the apps for removed queue.\n+        LOG.error(\"Queue \" + app.getQueue() + \" does not exist for \" + app\n+            .getApplicationId());\n+        return true;\n+      }\n+\n+      return authorizer.checkPermission(\n+          new AccessRequest(queue.getPrivilegedEntity(), callerUGI,\n+              SchedulerUtils.toAccessType(acl),\n+              app.getApplicationId().toString(), app.getName()));\n     } else {\n-      return scheduler.checkAccess(callerUGI, acl, queueName);\n+      return scheduler.checkAccess(callerUGI, acl, app.getQueue());\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/QueueACLsManager.java",
                "sha": "15c7052ff004b841f9b87277714fbe9a127b0754",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebServices.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebServices.java?ref=23248f63aab74a19dba38d348f2b231c8360770a",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebServices.java",
                "patch": "@@ -232,8 +232,7 @@ protected Boolean hasAccess(RMApp app, HttpServletRequest hsr) {\n               ApplicationAccessType.VIEW_APP, app.getUser(),\n               app.getApplicationId()) ||\n             this.rm.getQueueACLsManager().checkAccess(callerUGI,\n-                QueueACL.ADMINISTER_QUEUE, app.getQueue(),\n-                app.getApplicationId(), app.getName()))) {\n+                QueueACL.ADMINISTER_QUEUE, app))) {\n       return false;\n     }\n     return true;",
                "raw_url": "https://github.com/apache/hadoop/raw/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebServices.java",
                "sha": "dc8c6ab7cc18d7c7474833a1b7f45b2381333233",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestApplicationACLs.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestApplicationACLs.java?ref=23248f63aab74a19dba38d348f2b231c8360770a",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestApplicationACLs.java",
                "patch": "@@ -21,7 +21,6 @@\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.when;\n import static org.mockito.Matchers.any;\n-import static org.mockito.Matchers.anyString;\n \n import java.io.IOException;\n import java.net.InetSocketAddress;\n@@ -30,6 +29,7 @@\n import java.util.List;\n import java.util.Map;\n \n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.junit.Assert;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -112,8 +112,7 @@ protected QueueACLsManager createQueueACLsManager(\n           Configuration conf) {\n         QueueACLsManager mockQueueACLsManager = mock(QueueACLsManager.class);\n         when(mockQueueACLsManager.checkAccess(any(UserGroupInformation.class),\n-            any(QueueACL.class), anyString(), any(ApplicationId.class),\n-            anyString())).thenAnswer(new Answer() {\n+            any(QueueACL.class), any(RMApp.class))).thenAnswer(new Answer() {\n           public Object answer(InvocationOnMock invocation) {\n             return isQueueUser;\n           }",
                "raw_url": "https://github.com/apache/hadoop/raw/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestApplicationACLs.java",
                "sha": "d6d697ece0375f91ea911bebf7c1fad374328bfe",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMService.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMService.java?ref=23248f63aab74a19dba38d348f2b231c8360770a",
                "deletions": 10,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMService.java",
                "patch": "@@ -473,8 +473,7 @@ public void handle(Event event) {\n     QueueACLsManager mockQueueACLsManager = mock(QueueACLsManager.class);\n     when(\n         mockQueueACLsManager.checkAccess(any(UserGroupInformation.class),\n-            any(QueueACL.class), anyString(), any(ApplicationId.class),\n-            anyString())).thenReturn(true);\n+            any(QueueACL.class), any(RMApp.class))).thenReturn(true);\n     return new ClientRMService(rmContext, yarnScheduler, appManager,\n         mockAclsManager, mockQueueACLsManager, null);\n   }\n@@ -575,8 +574,7 @@ public void testGetQueueInfo() throws Exception {\n     ApplicationACLsManager mockAclsManager = mock(ApplicationACLsManager.class);\n     QueueACLsManager mockQueueACLsManager = mock(QueueACLsManager.class);\n     when(mockQueueACLsManager.checkAccess(any(UserGroupInformation.class),\n-        any(QueueACL.class), anyString(), any(ApplicationId.class),\n-        anyString())).thenReturn(true);\n+        any(QueueACL.class), any(RMApp.class))).thenReturn(true);\n     when(mockAclsManager.checkAccess(any(UserGroupInformation.class),\n         any(ApplicationAccessType.class), anyString(),\n         any(ApplicationId.class))).thenReturn(true);\n@@ -602,8 +600,7 @@ public void testGetQueueInfo() throws Exception {\n     QueueACLsManager mockQueueACLsManager1 =\n         mock(QueueACLsManager.class);\n     when(mockQueueACLsManager1.checkAccess(any(UserGroupInformation.class),\n-        any(QueueACL.class), anyString(), any(ApplicationId.class),\n-        anyString())).thenReturn(false);\n+        any(QueueACL.class), any(RMApp.class))).thenReturn(false);\n     when(mockAclsManager1.checkAccess(any(UserGroupInformation.class),\n         any(ApplicationAccessType.class), anyString(),\n         any(ApplicationId.class))).thenReturn(false);\n@@ -642,8 +639,7 @@ public void handle(Event event) {}\n \n     QueueACLsManager mockQueueACLsManager = mock(QueueACLsManager.class);\n     when(mockQueueACLsManager.checkAccess(any(UserGroupInformation.class),\n-        any(QueueACL.class), anyString(), any(ApplicationId.class),\n-        anyString())).thenReturn(true);\n+        any(QueueACL.class), any(RMApp.class))).thenReturn(true);\n     ClientRMService rmService =\n         new ClientRMService(rmContext, yarnScheduler, appManager,\n             mockAclsManager, mockQueueACLsManager, null);\n@@ -731,8 +727,7 @@ public void handle(Event event) {}\n     ApplicationACLsManager mockAclsManager = mock(ApplicationACLsManager.class);\n     QueueACLsManager mockQueueACLsManager = mock(QueueACLsManager.class);\n     when(mockQueueACLsManager.checkAccess(any(UserGroupInformation.class),\n-        any(QueueACL.class), anyString(), any(ApplicationId.class),\n-        anyString())).thenReturn(true);\n+        any(QueueACL.class), any(RMApp.class))).thenReturn(true);\n     ClientRMService rmService =\n         new ClientRMService(rmContext, yarnScheduler, appManager,\n             mockAclsManager, mockQueueACLsManager, null);",
                "raw_url": "https://github.com/apache/hadoop/raw/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMService.java",
                "sha": "8a6ddae29bcc3379fab95a3420f9348b4efe4482",
                "status": "modified"
            },
            {
                "additions": 54,
                "blob_url": "https://github.com/apache/hadoop/blob/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java",
                "changes": 72,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java?ref=23248f63aab74a19dba38d348f2b231c8360770a",
                "deletions": 18,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java",
                "patch": "@@ -18,23 +18,7 @@\n \n package org.apache.hadoop.yarn.server.resourcemanager;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertNull;\n-import static org.junit.Assert.assertTrue;\n-import static org.mockito.Mockito.mock;\n-import static org.mockito.Mockito.when;\n-\n-import java.io.File;\n-import java.io.IOException;\n-import java.net.UnknownHostException;\n-import java.util.ArrayList;\n-import java.util.Arrays;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.concurrent.TimeUnit;\n-\n+import com.google.common.base.Supplier;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.metrics2.lib.DefaultMetricsSystem;\n@@ -99,7 +83,22 @@\n import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Test;\n-import com.google.common.base.Supplier;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.net.UnknownHostException;\n+import java.security.PrivilegedAction;\n+import java.security.PrivilegedExceptionAction;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.TimeUnit;\n+\n+import static org.junit.Assert.*;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n \n @SuppressWarnings({\"rawtypes\", \"unchecked\"})\n public class TestWorkPreservingRMRestart extends ParameterizedSchedulerTestBase {\n@@ -564,6 +563,43 @@ private void setupQueueConfigurationChildOfB(CapacitySchedulerConfiguration conf\n         .MAXIMUM_APPLICATION_MASTERS_RESOURCE_PERCENT, 0.5f);\n   }\n \n+  // 1. submit an app to default queue and let it finish\n+  // 2. restart rm with no default queue\n+  // 3. getApplicationReport call should succeed (with no NPE)\n+  @Test (timeout = 30000)\n+  public void testRMRestartWithRemovedQueue() throws Exception{\n+    conf.setBoolean(YarnConfiguration.YARN_ACL_ENABLE, true);\n+    conf.set(YarnConfiguration.YARN_ADMIN_ACL, \"\");\n+    MemoryRMStateStore memStore = new MemoryRMStateStore();\n+    memStore.init(conf);\n+    rm1 = new MockRM(conf, memStore);\n+    rm1.start();\n+    MockNM nm1 =\n+        new MockNM(\"127.0.0.1:1234\", 8192, rm1.getResourceTrackerService());\n+    nm1.registerNode();\n+    final RMApp app1 = rm1.submitApp(1024, \"app1\", USER_1, null);\n+    MockAM am1 = MockRM.launchAndRegisterAM(app1,rm1, nm1);\n+    MockRM.finishAMAndVerifyAppState(app1, rm1, nm1, am1);\n+\n+    CapacitySchedulerConfiguration csConf = new CapacitySchedulerConfiguration(conf);\n+    csConf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[]{QUEUE_DOESNT_EXIST});\n+    final String noQueue = CapacitySchedulerConfiguration.ROOT + \".\" + QUEUE_DOESNT_EXIST;\n+    csConf.setCapacity(noQueue, 100);\n+    rm2 = new MockRM(csConf,memStore);\n+\n+    rm2.start();\n+    UserGroupInformation user2 = UserGroupInformation.createRemoteUser(\"user2\");\n+\n+    ApplicationReport report =\n+        user2.doAs(new PrivilegedExceptionAction<ApplicationReport>() {\n+          @Override\n+          public ApplicationReport run() throws Exception {\n+            return rm2.getApplicationReport(app1.getApplicationId());\n+          }\n+    });\n+    Assert.assertNotNull(report);\n+  }\n+\n   // Test CS recovery with multi-level queues and multi-users:\n   // 1. setup 2 NMs each with 8GB memory;\n   // 2. setup 2 level queues: Default -> (QueueA, QueueB)",
                "raw_url": "https://github.com/apache/hadoop/raw/23248f63aab74a19dba38d348f2b231c8360770a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java",
                "sha": "b6d6f691e60685bdff6c0963d83e26ea9f987ce6",
                "status": "modified"
            }
        ],
        "message": "getApplicationReport call may raise NPE for removed queues. (Jian He via wangda)",
        "parent": "https://github.com/apache/hadoop/commit/c9bb96fa81fc925e33ccc0b02c98cc2d929df120",
        "repo": "hadoop",
        "unit_tests": [
            "TestClientRMService.java",
            "TestRMWebServices.java"
        ]
    },
    "hadoop_251f528": {
        "bug_id": "hadoop_251f528",
        "commit": "https://github.com/apache/hadoop/commit/251f528814c4a4647cac0af6effb9a73135db180",
        "file": [
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/251f528814c4a4647cac0af6effb9a73135db180/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java?ref=251f528814c4a4647cac0af6effb9a73135db180",
                "deletions": 14,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "patch": "@@ -44,6 +44,7 @@\n import org.apache.hadoop.yarn.server.api.protocolrecords.NMContainerStatus;\n import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n import org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRunningOnNodeEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent;\n@@ -737,21 +738,22 @@ public void transition(RMContainerImpl container, RMContainerEvent event) {\n \n     private static void updateAttemptMetrics(RMContainerImpl container) {\n       Resource resource = container.getContainer().getResource();\n-      RMAppAttempt rmAttempt = container.rmContext.getRMApps()\n-          .get(container.getApplicationAttemptId().getApplicationId())\n-          .getCurrentAppAttempt();\n-\n-      if (rmAttempt != null) {\n-        long usedMillis = container.finishTime - container.creationTime;\n-        rmAttempt.getRMAppAttemptMetrics()\n-            .updateAggregateAppResourceUsage(resource, usedMillis);\n-        // If this is a preempted container, update preemption metrics\n-        if (ContainerExitStatus.PREEMPTED == container.finishedStatus\n-            .getExitStatus()) {\n+      RMApp app = container.rmContext.getRMApps()\n+          .get(container.getApplicationAttemptId().getApplicationId());\n+      if (app != null) {\n+        RMAppAttempt rmAttempt = app.getCurrentAppAttempt();\n+        if (rmAttempt != null) {\n+          long usedMillis = container.finishTime - container.creationTime;\n           rmAttempt.getRMAppAttemptMetrics()\n-              .updatePreemptionInfo(resource, container);\n-          rmAttempt.getRMAppAttemptMetrics()\n-              .updateAggregatePreemptedAppResourceUsage(resource, usedMillis);\n+              .updateAggregateAppResourceUsage(resource, usedMillis);\n+          // If this is a preempted container, update preemption metrics\n+          if (ContainerExitStatus.PREEMPTED == container.finishedStatus\n+              .getExitStatus()) {\n+            rmAttempt.getRMAppAttemptMetrics()\n+                .updatePreemptionInfo(resource, container);\n+            rmAttempt.getRMAppAttemptMetrics()\n+                .updateAggregatePreemptedAppResourceUsage(resource, usedMillis);\n+          }\n         }\n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/251f528814c4a4647cac0af6effb9a73135db180/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "sha": "b5c8e7cb8e975bd66b587c0c8c776a2b033a0f7c",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/251f528814c4a4647cac0af6effb9a73135db180/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java?ref=251f528814c4a4647cac0af6effb9a73135db180",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java",
                "patch": "@@ -1241,12 +1241,13 @@ public void incNumAllocatedContainers(NodeType containerType,\n       return;\n     }\n \n-    RMAppAttempt attempt =\n-        rmContext.getRMApps().get(attemptId.getApplicationId())\n-          .getCurrentAppAttempt();\n-    if (attempt != null) {\n-      attempt.getRMAppAttemptMetrics().incNumAllocatedContainers(containerType,\n-        requestType);\n+    RMApp app = rmContext.getRMApps().get(attemptId.getApplicationId());\n+    if (app != null) {\n+      RMAppAttempt attempt = app.getCurrentAppAttempt();\n+      if (attempt != null) {\n+        attempt.getRMAppAttemptMetrics()\n+            .incNumAllocatedContainers(containerType, requestType);\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/251f528814c4a4647cac0af6effb9a73135db180/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java",
                "sha": "005569cf2520751528b05cc317aed9585788e3ad",
                "status": "modified"
            }
        ],
        "message": "YARN-8222. Fix potential NPE when gets RMApp from RM context. Contributed by Tao Yang.",
        "parent": "https://github.com/apache/hadoop/commit/3265b55119d39ecbda6d75be04a9a1bf59c631f1",
        "repo": "hadoop",
        "unit_tests": [
            "TestRMContainerImpl.java",
            "TestSchedulerApplicationAttempt.java"
        ]
    },
    "hadoop_2741a21": {
        "bug_id": "hadoop_2741a21",
        "commit": "https://github.com/apache/hadoop/commit/2741a2109b98d0febb463cb318018ecbd3995102",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2741a2109b98d0febb463cb318018ecbd3995102/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=2741a2109b98d0febb463cb318018ecbd3995102",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -2277,6 +2277,9 @@ Release 2.8.0 - UNRELEASED\n     initialization, because HftpFileSystem is missing.\n     (Mingliang Liu via cnauroth)\n \n+    HDFS-9249. NPE is thrown if an IOException is thrown in NameNode constructor.\n+    (Wei-Chiu Chuang via Yongjun Zhang)\n+\n Release 2.7.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/2741a2109b98d0febb463cb318018ecbd3995102/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "533fe34a3c95fd81a0f825fb08c4a48e48b2d157",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/2741a2109b98d0febb463cb318018ecbd3995102/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java?ref=2741a2109b98d0febb463cb318018ecbd3995102",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java",
                "patch": "@@ -217,7 +217,9 @@ void stop(boolean reportError) {\n \n     // Abort current log segment - otherwise the NN shutdown code\n     // will close it gracefully, which is incorrect.\n-    getFSImage().getEditLog().abortCurrentLogSegment();\n+    if (namesystem != null) {\n+      getFSImage().getEditLog().abortCurrentLogSegment();\n+    }\n \n     // Stop name-node threads\n     super.stop();",
                "raw_url": "https://github.com/apache/hadoop/raw/2741a2109b98d0febb463cb318018ecbd3995102/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/BackupNode.java",
                "sha": "36053f71aba0efc933e7fe48ffb73f17f55e8175",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/2741a2109b98d0febb463cb318018ecbd3995102/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=2741a2109b98d0febb463cb318018ecbd3995102",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -889,15 +889,24 @@ protected NameNode(Configuration conf, NamenodeRole role)\n         haContext.writeUnlock();\n       }\n     } catch (IOException e) {\n-      this.stop();\n+      this.stopAtException(e);\n       throw e;\n     } catch (HadoopIllegalArgumentException e) {\n-      this.stop();\n+      this.stopAtException(e);\n       throw e;\n     }\n     this.started.set(true);\n   }\n \n+  private void stopAtException(Exception e){\n+    try {\n+      this.stop();\n+    } catch (Exception ex) {\n+      LOG.warn(\"Encountered exception when handling exception (\"\n+          + e.getMessage() + \"):\", ex);\n+    }\n+  }\n+\n   protected HAState createHAState(StartupOption startOpt) {\n     if (!haEnabled || startOpt == StartupOption.UPGRADE \n         || startOpt == StartupOption.UPGRADEONLY) {",
                "raw_url": "https://github.com/apache/hadoop/raw/2741a2109b98d0febb463cb318018ecbd3995102/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "7371d84f75e1420dfd9d5d11f1d2d45bff86ae3c",
                "status": "modified"
            },
            {
                "additions": 71,
                "blob_url": "https://github.com/apache/hadoop/blob/2741a2109b98d0febb463cb318018ecbd3995102/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBackupNode.java",
                "changes": 71,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBackupNode.java?ref=2741a2109b98d0febb463cb318018ecbd3995102",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBackupNode.java",
                "patch": "@@ -21,6 +21,7 @@\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n \n import java.io.File;\n import java.io.IOException;\n@@ -30,7 +31,9 @@\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n+import org.apache.directory.api.ldap.aci.UserClass;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.fs.Path;\n@@ -46,6 +49,8 @@\n import org.apache.hadoop.hdfs.server.namenode.FileJournalManager.EditLogFile;\n import org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols;\n import org.apache.hadoop.net.NetUtils;\n+import org.apache.hadoop.security.SecurityUtil;\n+import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.test.GenericTestUtils;\n import org.apache.log4j.Level;\n import org.junit.Before;\n@@ -128,6 +133,72 @@ void waitCheckpointDone(MiniDFSCluster cluster, long txid) {\n         Collections.singletonList((int)thisCheckpointTxId));\n   }\n \n+\n+  /**\n+   *  Regression test for HDFS-9249.\n+   *  This test configures the primary name node with SIMPLE authentication,\n+   *  and configures the backup node with Kerberose authentication with\n+   *  invalid keytab settings.\n+   *\n+   *  This configuration causes the backup node to throw a NPE trying to abort\n+   *  the edit log.\n+   *  */\n+  @Test\n+    public void startBackupNodeWithIncorrectAuthentication() throws IOException {\n+    Configuration c = new HdfsConfiguration();\n+    StartupOption startupOpt = StartupOption.CHECKPOINT;\n+    String dirs = getBackupNodeDir(startupOpt, 1);\n+    c.set(DFSConfigKeys.FS_DEFAULT_NAME_KEY, \"hdfs://127.0.0.1:1234\");\n+    c.set(DFSConfigKeys.DFS_NAMENODE_BACKUP_HTTP_ADDRESS_KEY, \"localhost:0\");\n+    c.set(DFSConfigKeys.DFS_BLOCKREPORT_INITIAL_DELAY_KEY, \"0\");\n+    c.setInt(DFSConfigKeys.DFS_DATANODE_SCAN_PERIOD_HOURS_KEY,\n+        -1); // disable block scanner\n+    c.setInt(DFSConfigKeys.DFS_NAMENODE_CHECKPOINT_TXNS_KEY, 1);\n+    c.set(DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY, dirs);\n+    c.set(DFSConfigKeys.DFS_NAMENODE_EDITS_DIR_KEY,\n+        \"${\" + DFSConfigKeys.DFS_NAMENODE_NAME_DIR_KEY + \"}\");\n+    c.set(DFSConfigKeys.DFS_NAMENODE_BACKUP_ADDRESS_KEY,\n+        \"127.0.0.1:0\");\n+    c.set(DFSConfigKeys.DFS_NAMENODE_BACKUP_HTTP_ADDRESS_KEY,\n+        \"127.0.0.1:0\");\n+\n+    NameNode nn;\n+    try {\n+      Configuration nnconf = new HdfsConfiguration(c);\n+      DFSTestUtil.formatNameNode(nnconf);\n+      nn = NameNode.createNameNode(new String[] {}, nnconf);\n+    } catch (IOException e) {\n+      LOG.info(\"IOException is thrown creating name node\");\n+      throw e;\n+    }\n+\n+    c.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,\n+        \"kerberos\");\n+    c.set(DFSConfigKeys.DFS_NAMENODE_KEYTAB_FILE_KEY, \"\");\n+\n+    BackupNode bn = null;\n+    try {\n+      bn = (BackupNode)NameNode.createNameNode(\n+          new String[] {startupOpt.getName()}, c);\n+      assertTrue(\"Namesystem in BackupNode should be null\",\n+          bn.getNamesystem() == null);\n+      fail(\"Incorrect authentication setting should throw IOException\");\n+    } catch (IOException e) {\n+      LOG.info(\"IOException thrown as expected\", e);\n+    } finally {\n+      if (nn != null) {\n+        nn.stop();\n+      }\n+      if (bn != null) {\n+        bn.stop();\n+      }\n+      SecurityUtil.setAuthenticationMethod(\n+          UserGroupInformation.AuthenticationMethod.SIMPLE, c);\n+      // reset security authentication\n+      UserGroupInformation.setConfiguration(c);\n+    }\n+  }\n+\n   @Test\n   public void testCheckpointNode() throws Exception {\n     testCheckpoint(StartupOption.CHECKPOINT);",
                "raw_url": "https://github.com/apache/hadoop/raw/2741a2109b98d0febb463cb318018ecbd3995102/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBackupNode.java",
                "sha": "38d84e35dc54e2298822f75e214d93c2aacd5bc4",
                "status": "modified"
            }
        ],
        "message": "HDFS-9249. NPE is thrown if an IOException is thrown in NameNode constructor. (Wei-Chiu Chuang via Yongjun Zhang)",
        "parent": "https://github.com/apache/hadoop/commit/8fbea531d7f7b665f6f55af54c8ebf330118ff37",
        "repo": "hadoop",
        "unit_tests": [
            "TestBackupNode.java"
        ]
    },
    "hadoop_27e0681": {
        "bug_id": "hadoop_27e0681",
        "commit": "https://github.com/apache/hadoop/commit/27e0681f28ee896ada163bbbc08fd44d113e7d15",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=27e0681f28ee896ada163bbbc08fd44d113e7d15",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -2974,6 +2974,9 @@ Release 2.7.3 - UNRELEASED\n     HDFS-9766. TestDataNodeMetrics#testDataNodeTimeSpend fails intermittently.\n     (Xiao Chen via aajisaka)\n \n+    HDFS-9851. NameNode throws NPE when setPermission is called on a path that\n+    does not exist. (Brahma Reddy Battula via aajisaka)\n+\n Release 2.7.2 - 2016-01-25\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "8be05bff22988fa3824f7e63735895c37d272911",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirXAttrOp.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirXAttrOp.java?ref=27e0681f28ee896ada163bbbc08fd44d113e7d15",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirXAttrOp.java",
                "patch": "@@ -32,6 +32,7 @@\n import org.apache.hadoop.hdfs.protocolPB.PBHelperClient;\n import org.apache.hadoop.security.AccessControlException;\n \n+import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.util.EnumSet;\n import java.util.List;\n@@ -388,7 +389,7 @@ static XAttr unprotectedGetXAttrByPrefixedName(\n   private static void checkXAttrChangeAccess(\n       FSDirectory fsd, INodesInPath iip, XAttr xAttr,\n       FSPermissionChecker pc)\n-      throws AccessControlException {\n+      throws AccessControlException, FileNotFoundException {\n     if (fsd.isPermissionEnabled() && xAttr.getNameSpace() == XAttr.NameSpace\n         .USER) {\n       final INode inode = iip.getLastINode();",
                "raw_url": "https://github.com/apache/hadoop/raw/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirXAttrOp.java",
                "sha": "d27cec5773679ee6ba27bc2ede28f64e9a574a54",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java?ref=27e0681f28ee896ada163bbbc08fd44d113e7d15",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "patch": "@@ -1547,7 +1547,11 @@ FSPermissionChecker getPermissionChecker(String fsOwner, String superGroup,\n   }\n \n   void checkOwner(FSPermissionChecker pc, INodesInPath iip)\n-      throws AccessControlException {\n+      throws AccessControlException, FileNotFoundException {\n+    if (iip.getLastINode() == null) {\n+      throw new FileNotFoundException(\n+          \"Directory/File does not exist \" + iip.getPath());\n+    }\n     checkPermission(pc, iip, true, null, null, null, null);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "sha": "51b7747c1f3c16ca139252779890919c389c1652",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java?ref=27e0681f28ee896ada163bbbc08fd44d113e7d15",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java",
                "patch": "@@ -3344,7 +3344,7 @@ public void testSnapshotReserved() throws IOException {\n       fs.createSnapshot(reserved, \"snap\");\n       fail(\"Can't create snapshot on /.reserved\");\n     } catch (FileNotFoundException e) {\n-      assertTrue(e.getMessage().contains(\"Directory does not exist\"));\n+      assertTrue(e.getMessage().contains(\"Directory/File does not exist\"));\n     }\n     cluster.shutdown();\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSShell.java",
                "sha": "41cd5c0c675975a965b6762e8d9508fb761c98c2",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/security/TestPermission.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/security/TestPermission.java?ref=27e0681f28ee896ada163bbbc08fd44d113e7d15",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/security/TestPermission.java",
                "patch": "@@ -51,6 +51,7 @@\n   final private static Path CHILD_DIR2 = new Path(ROOT_PATH, \"child2\");\n   final private static Path CHILD_FILE1 = new Path(ROOT_PATH, \"file1\");\n   final private static Path CHILD_FILE2 = new Path(ROOT_PATH, \"file2\");\n+  final private static Path CHILD_FILE3 = new Path(ROOT_PATH, \"file3\");\n \n   final private static int FILE_LEN = 100;\n   final private static Random RAN = new Random();\n@@ -270,6 +271,18 @@ public void testFilePermission() throws Exception {\n       final Path RENAME_PATH = new Path(\"/foo/bar\");\n       userfs.mkdirs(RENAME_PATH);\n       assertTrue(canRename(userfs, RENAME_PATH, CHILD_DIR1));\n+      // test permissions on files that do not exist\n+      assertFalse(userfs.exists(CHILD_FILE3));\n+      try {\n+        userfs.setOwner(CHILD_FILE3, \"foo\", \"bar\");\n+        fail(\"setOwner should fail for non-exist file\");\n+      } catch (java.io.FileNotFoundException ignored) {\n+      }\n+      try {\n+        userfs.setPermission(CHILD_FILE3, new FsPermission((short) 0777));\n+        fail(\"setPermission should fail for non-exist file\");\n+      } catch (java.io.FileNotFoundException ignored) {\n+      }\n     } finally {\n       cluster.shutdown();\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/27e0681f28ee896ada163bbbc08fd44d113e7d15/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/security/TestPermission.java",
                "sha": "5a17bbc8efc211662ec60cf91ff04022da7b263e",
                "status": "modified"
            }
        ],
        "message": "HDFS-9851. NameNode throws NPE when setPermission is called on a path that does not exist. Contributed by Brahma Reddy Battula.",
        "parent": "https://github.com/apache/hadoop/commit/e2ddf824694eb4605f3bb04a9c26e4b98529f5bc",
        "repo": "hadoop",
        "unit_tests": [
            "TestFSDirectory.java"
        ]
    },
    "hadoop_296c5de": {
        "bug_id": "hadoop_296c5de",
        "commit": "https://github.com/apache/hadoop/commit/296c5de0cfee88389cf9f90263280b2034e54cd5",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/296c5de0cfee88389cf9f90263280b2034e54cd5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java?ref=296c5de0cfee88389cf9f90263280b2034e54cd5",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "patch": "@@ -247,7 +247,9 @@ public RMContainerImpl(Container container,\n        YarnConfiguration\n                  .DEFAULT_APPLICATION_HISTORY_SAVE_NON_AM_CONTAINER_META_INFO);\n \n-    rmContext.getRMApplicationHistoryWriter().containerStarted(this);\n+    if (container.getId() != null) {\n+      rmContext.getRMApplicationHistoryWriter().containerStarted(this);\n+    }\n \n     // If saveNonAMContainerMetaInfo is true, store system metrics for all\n     // containers. If false, and if this container is marked as the AM, metrics\n@@ -892,6 +894,9 @@ public void setContainerId(ContainerId containerId) {\n     // container creation event to timeline service when id assigned.\n     container.setId(containerId);\n \n+    if (containerId != null) {\n+      rmContext.getRMApplicationHistoryWriter().containerStarted(this);\n+    }\n     // If saveNonAMContainerMetaInfo is true, store system metrics for all\n     // containers. If false, and if this container is marked as the AM, metrics\n     // will still be published for this container, but that calculation happens",
                "raw_url": "https://github.com/apache/hadoop/raw/296c5de0cfee88389cf9f90263280b2034e54cd5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "sha": "f5d8b5b063344eabc7a6fc46b842dd5f223353d6",
                "status": "modified"
            }
        ],
        "message": "YARN-5873. RM crashes with NPE if generic application history is enabled. Contributed by Varun Saxena.",
        "parent": "https://github.com/apache/hadoop/commit/04014c4c739bb4e3bc3fdf9299abc0f47521e8fd",
        "repo": "hadoop",
        "unit_tests": [
            "TestRMContainerImpl.java"
        ]
    },
    "hadoop_2b5ad48": {
        "bug_id": "hadoop_2b5ad48",
        "commit": "https://github.com/apache/hadoop/commit/2b5ad48762587abbcd8bdb50d0ae98f8080d926c",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockUnderConstructionFeature.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockUnderConstructionFeature.java?ref=2b5ad48762587abbcd8bdb50d0ae98f8080d926c",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockUnderConstructionFeature.java",
                "patch": "@@ -223,10 +223,17 @@ void commit() {\n    * Initialize lease recovery for this block.\n    * Find the first alive data-node starting from the previous primary and\n    * make it primary.\n+   * @param blockInfo Block to be recovered\n+   * @param recoveryId Recovery ID (new gen stamp)\n+   * @param startRecovery Issue recovery command to datanode if true.\n    */\n-  public void initializeBlockRecovery(BlockInfo blockInfo, long recoveryId) {\n+  public void initializeBlockRecovery(BlockInfo blockInfo, long recoveryId,\n+      boolean startRecovery) {\n     setBlockUCState(BlockUCState.UNDER_RECOVERY);\n     blockRecoveryId = recoveryId;\n+    if (!startRecovery) {\n+      return;\n+    }\n     if (replicas.length == 0) {\n       NameNode.blockStateChangeLog.warn(\"BLOCK*\" +\n           \" BlockUnderConstructionFeature.initializeBlockRecovery:\" +",
                "raw_url": "https://github.com/apache/hadoop/raw/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockUnderConstructionFeature.java",
                "sha": "61390d9fe835cf26af09a077a4d3acfb9b35aca3",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java?ref=2b5ad48762587abbcd8bdb50d0ae98f8080d926c",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
                "patch": "@@ -642,10 +642,11 @@ public DatanodeDescriptor getDatanode(DatanodeID nodeID)\n       String format, Object... args) throws UnregisteredNodeException {\n     storageIDs = storageIDs == null ? new String[0] : storageIDs;\n     if (datanodeID.length != storageIDs.length) {\n+      // Error for pre-2.0.0-alpha clients.\n       final String err = (storageIDs.length == 0?\n           \"Missing storageIDs: It is likely that the HDFS client,\"\n           + \" who made this call, is running in an older version of Hadoop\"\n-          + \" which does not support storageIDs.\"\n+          + \"(pre-2.0.0-alpha)  which does not support storageIDs.\"\n           : \"Length mismatched: storageIDs.length=\" + storageIDs.length + \" != \"\n           ) + \" datanodeID.length=\" + datanodeID.length;\n       throw new HadoopIllegalArgumentException(",
                "raw_url": "https://github.com/apache/hadoop/raw/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeManager.java",
                "sha": "c303594cda9b38e4c926a048843df00c40627596",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirTruncateOp.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirTruncateOp.java?ref=2b5ad48762587abbcd8bdb50d0ae98f8080d926c",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirTruncateOp.java",
                "patch": "@@ -270,7 +270,7 @@ static Block prepareFileForTruncate(FSNamesystem fsn, INodesInPath iip,\n     }\n     if (shouldRecoverNow) {\n       truncatedBlockUC.getUnderConstructionFeature().initializeBlockRecovery(\n-          truncatedBlockUC, newBlock.getGenerationStamp());\n+          truncatedBlockUC, newBlock.getGenerationStamp(), true);\n     }\n \n     return newBlock;",
                "raw_url": "https://github.com/apache/hadoop/raw/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirTruncateOp.java",
                "sha": "034812058eea93fc4358490fb30f1da58aadef40",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=2b5ad48762587abbcd8bdb50d0ae98f8080d926c",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -3250,7 +3250,7 @@ boolean internalReleaseLease(Lease lease, String src, INodesInPath iip,\n       } else if(truncateRecovery) {\n         recoveryBlock.setGenerationStamp(blockRecoveryId);\n       }\n-      uc.initializeBlockRecovery(lastBlock, blockRecoveryId);\n+      uc.initializeBlockRecovery(lastBlock, blockRecoveryId, true);\n       leaseManager.renewLease(lease);\n       // Cannot close file right now, since the last block requires recovery.\n       // This may potentially cause infinite loop in lease recovery",
                "raw_url": "https://github.com/apache/hadoop/raw/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "11b62d981c66c6a5a4d3bcd9dd6504f6e194fe72",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java?ref=2b5ad48762587abbcd8bdb50d0ae98f8080d926c",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java",
                "patch": "@@ -475,9 +475,16 @@ synchronized boolean checkLeases() {\n           if (!p.startsWith(\"/\")) {\n             throw new IOException(\"Invalid path in the lease \" + p);\n           }\n-          boolean completed = fsnamesystem.internalReleaseLease(\n-              leaseToCheck, p, iip,\n-              HdfsServerConstants.NAMENODE_LEASE_HOLDER);\n+          boolean completed = false;\n+          try {\n+            completed = fsnamesystem.internalReleaseLease(\n+                leaseToCheck, p, iip,\n+                HdfsServerConstants.NAMENODE_LEASE_HOLDER);\n+          } catch (IOException e) {\n+            LOG.warn(\"Cannot release the path \" + p + \" in the lease \"\n+                + leaseToCheck + \". It will be retried.\", e);\n+            continue;\n+          }\n           if (LOG.isDebugEnabled()) {\n             if (completed) {\n               LOG.debug(\"Lease recovery for inode \" + id + \" is complete. \" +\n@@ -491,7 +498,7 @@ synchronized boolean checkLeases() {\n             needSync = true;\n           }\n         } catch (IOException e) {\n-          LOG.error(\"Cannot release the path \" + p + \" in the lease \"\n+          LOG.warn(\"Removing lease with an invalid path: \" + p + \",\"\n               + leaseToCheck, e);\n           removing.add(id);\n         }",
                "raw_url": "https://github.com/apache/hadoop/raw/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/LeaseManager.java",
                "sha": "dcdb958c01b16c80fa3b45dc0f0643fb2742dc18",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockUnderConstructionFeature.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockUnderConstructionFeature.java?ref=2b5ad48762587abbcd8bdb50d0ae98f8080d926c",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockUnderConstructionFeature.java",
                "patch": "@@ -50,23 +50,23 @@ public void testInitializeBlockRecovery() throws Exception {\n     DFSTestUtil.resetLastUpdatesWithOffset(dd1, -3 * 1000);\n     DFSTestUtil.resetLastUpdatesWithOffset(dd2, -1 * 1000);\n     DFSTestUtil.resetLastUpdatesWithOffset(dd3, -2 * 1000);\n-    blockInfo.getUnderConstructionFeature().initializeBlockRecovery(blockInfo, 1);\n+    blockInfo.getUnderConstructionFeature().initializeBlockRecovery(blockInfo, 1, true);\n     BlockInfo[] blockInfoRecovery = dd2.getLeaseRecoveryCommand(1);\n     assertEquals(blockInfoRecovery[0], blockInfo);\n \n     // Recovery attempt #2.\n     DFSTestUtil.resetLastUpdatesWithOffset(dd1, -2 * 1000);\n     DFSTestUtil.resetLastUpdatesWithOffset(dd2, -1 * 1000);\n     DFSTestUtil.resetLastUpdatesWithOffset(dd3, -3 * 1000);\n-    blockInfo.getUnderConstructionFeature().initializeBlockRecovery(blockInfo, 2);\n+    blockInfo.getUnderConstructionFeature().initializeBlockRecovery(blockInfo, 2, true);\n     blockInfoRecovery = dd1.getLeaseRecoveryCommand(1);\n     assertEquals(blockInfoRecovery[0], blockInfo);\n \n     // Recovery attempt #3.\n     DFSTestUtil.resetLastUpdatesWithOffset(dd1, -2 * 1000);\n     DFSTestUtil.resetLastUpdatesWithOffset(dd2, -1 * 1000);\n     DFSTestUtil.resetLastUpdatesWithOffset(dd3, -3 * 1000);\n-    blockInfo.getUnderConstructionFeature().initializeBlockRecovery(blockInfo, 3);\n+    blockInfo.getUnderConstructionFeature().initializeBlockRecovery(blockInfo, 3, true);\n     blockInfoRecovery = dd3.getLeaseRecoveryCommand(1);\n     assertEquals(blockInfoRecovery[0], blockInfo);\n \n@@ -75,7 +75,7 @@ public void testInitializeBlockRecovery() throws Exception {\n     DFSTestUtil.resetLastUpdatesWithOffset(dd1, -2 * 1000);\n     DFSTestUtil.resetLastUpdatesWithOffset(dd2, -1 * 1000);\n     DFSTestUtil.resetLastUpdatesWithOffset(dd3, 0);\n-    blockInfo.getUnderConstructionFeature().initializeBlockRecovery(blockInfo, 3);\n+    blockInfo.getUnderConstructionFeature().initializeBlockRecovery(blockInfo, 3, true);\n     blockInfoRecovery = dd3.getLeaseRecoveryCommand(1);\n     assertEquals(blockInfoRecovery[0], blockInfo);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestBlockUnderConstructionFeature.java",
                "sha": "15502c92fb718a89339ab36d78297642056cca57",
                "status": "modified"
            },
            {
                "additions": 45,
                "blob_url": "https://github.com/apache/hadoop/blob/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockUnderConstruction.java",
                "changes": 45,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockUnderConstruction.java?ref=2b5ad48762587abbcd8bdb50d0ae98f8080d926c",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockUnderConstruction.java",
                "patch": "@@ -38,12 +38,16 @@\n import org.apache.hadoop.hdfs.protocol.LocatedBlocks;\n import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfo;\n import org.apache.hadoop.hdfs.server.blockmanagement.BlockManager;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockUnderConstructionFeature;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.BlockUCState;\n import org.apache.hadoop.hdfs.server.protocol.NamenodeProtocols;\n import org.junit.AfterClass;\n import org.junit.BeforeClass;\n import org.junit.Test;\n \n+import org.apache.hadoop.hdfs.protocol.proto.HdfsProtos.LocatedBlockProto;\n+import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n+\n public class TestBlockUnderConstruction {\n   static final String BASE_DIR = \"/test/TestBlockUnderConstruction\";\n   static final int BLOCK_SIZE = 8192; // same as TestFileCreation.blocksize\n@@ -183,4 +187,45 @@ public void testGetBlockLocations() throws IOException {\n     // close file\n     out.close();\n   }\n+\n+  /**\n+   * A storage ID can be invalid if the storage failed or the node\n+   * reregisters. When the node heart-beats, the storage report in it\n+   * causes storage volumes to be added back. An invalid storage ID\n+   * should not cause an NPE.\n+   */\n+  @Test\n+  public void testEmptyExpectedLocations() throws Exception {\n+    final NamenodeProtocols namenode = cluster.getNameNodeRpc();\n+    final FSNamesystem fsn = cluster.getNamesystem();\n+    final BlockManager bm = fsn.getBlockManager();\n+    final Path p = new Path(BASE_DIR, \"file2.dat\");\n+    final String src = p.toString();\n+    final FSDataOutputStream out = TestFileCreation.createFile(hdfs, p, 1);\n+    writeFile(p, out, 256);\n+    out.hflush();\n+\n+    // make sure the block is readable\n+    LocatedBlocks lbs = namenode.getBlockLocations(src, 0, 256);\n+    LocatedBlock lastLB = lbs.getLocatedBlocks().get(0);\n+    final Block b = lastLB.getBlock().getLocalBlock();\n+\n+    // fake a block recovery\n+    long blockRecoveryId = bm.nextGenerationStamp(false);\n+    BlockUnderConstructionFeature uc = bm.getStoredBlock(b).\n+        getUnderConstructionFeature();\n+    uc.initializeBlockRecovery(null, blockRecoveryId, false);\n+\n+    try {\n+      String[] storages = { \"invalid-storage-id1\" };\n+      fsn.commitBlockSynchronization(lastLB.getBlock(), blockRecoveryId, 256L,\n+          true, false, lastLB.getLocations(), storages);\n+    } catch (java.lang.IllegalStateException ise) {\n+       // Although a failure is expected as of now, future commit policy\n+       // changes may make it not fail. This is not critical to the test.\n+    }\n+\n+    // Invalid storage should not trigger an exception.\n+    lbs = namenode.getBlockLocations(src, 0, 256);\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestBlockUnderConstruction.java",
                "sha": "49e79da674e5d0ec0cd359579aa36f5a6e0aa932",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java?ref=2b5ad48762587abbcd8bdb50d0ae98f8080d926c",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java",
                "patch": "@@ -73,7 +73,7 @@ private FSNamesystem makeNameSystemSpy(Block block, INodeFile file)\n     blockInfo.setBlockCollectionId(file.getId());\n     blockInfo.setGenerationStamp(genStamp);\n     blockInfo.getUnderConstructionFeature().initializeBlockRecovery(blockInfo,\n-        genStamp);\n+        genStamp, true);\n     doReturn(blockInfo).when(file).removeLastBlock(any(Block.class));\n     doReturn(true).when(file).isUnderConstruction();\n     doReturn(new BlockInfoContiguous[1]).when(file).getBlocks();",
                "raw_url": "https://github.com/apache/hadoop/raw/2b5ad48762587abbcd8bdb50d0ae98f8080d926c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java",
                "sha": "071e7171f6ed213293420d7c515c4774a653ca74",
                "status": "modified"
            }
        ],
        "message": "HDFS-11817. A faulty node can cause a lease leak and NPE on accessing data. Contributed by Kihwal Lee.",
        "parent": "https://github.com/apache/hadoop/commit/87590090c887829e874a7132be9cf8de061437d6",
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockUnderConstructionFeature.java",
            "TestDatanodeManager.java",
            "TestFSNamesystem.java",
            "TestLeaseManager.java"
        ]
    },
    "hadoop_2f26475": {
        "bug_id": "hadoop_2f26475",
        "commit": "https://github.com/apache/hadoop/commit/2f26475a39f94e756ef4d15ff8c863b3f692a29e",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/2f26475a39f94e756ef4d15ff8c863b3f692a29e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=2f26475a39f94e756ef4d15ff8c863b3f692a29e",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -105,3 +105,5 @@ HDFS-2766. Test for case where standby partially reads log and then performs che\n HDFS-2738. FSEditLog.selectinputStreams is reading through in-progress streams even when non-in-progress are requested. (atm)\n \n HDFS-2789. TestHAAdmin.testFailover is failing (eli)\n+\n+HDFS-2747. Entering safe mode after starting SBN can NPE. (Uma Maheswara Rao G via todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/2f26475a39f94e756ef4d15ff8c863b3f692a29e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "fc245ed3e690cc629d69542cf53170b90417b462",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop/blob/2f26475a39f94e756ef4d15ff8c863b3f692a29e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=2f26475a39f94e756ef4d15ff8c863b3f692a29e",
                "deletions": 15,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -3774,21 +3774,28 @@ private long getCompleteBlocksTotal() {\n   void enterSafeMode(boolean resourcesLow) throws IOException {\n     writeLock();\n     try {\n-    // Ensure that any concurrent operations have been fully synced\n-    // before entering safe mode. This ensures that the FSImage\n-    // is entirely stable on disk as soon as we're in safe mode.\n-    getEditLog().logSyncAll();\n-    if (!isInSafeMode()) {\n-      safeMode = new SafeModeInfo(resourcesLow);\n-      return;\n-    }\n-    if (resourcesLow) {\n-      safeMode.setResourcesLow();\n-    }\n-    safeMode.setManual();\n-    getEditLog().logSyncAll();\n-    NameNode.stateChangeLog.info(\"STATE* Safe mode is ON. \" \n-                                + safeMode.getTurnOffTip());\n+      // Ensure that any concurrent operations have been fully synced\n+      // before entering safe mode. This ensures that the FSImage\n+      // is entirely stable on disk as soon as we're in safe mode.\n+      boolean isEditlogOpenForWrite = getEditLog().isOpenForWrite();\n+      // Before Editlog is in OpenForWrite mode, editLogStream will be null. So,\n+      // logSyncAll call can be called only when Edlitlog is in OpenForWrite mode\n+      if (isEditlogOpenForWrite) {\n+        getEditLog().logSyncAll();\n+      }\n+      if (!isInSafeMode()) {\n+        safeMode = new SafeModeInfo(resourcesLow);\n+        return;\n+      }\n+      if (resourcesLow) {\n+        safeMode.setResourcesLow();\n+      }\n+      safeMode.setManual();\n+      if (isEditlogOpenForWrite) {\n+        getEditLog().logSyncAll();\n+      }\n+      NameNode.stateChangeLog.info(\"STATE* Safe mode is ON. \"\n+          + safeMode.getTurnOffTip());\n     } finally {\n       writeUnlock();\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/2f26475a39f94e756ef4d15ff8c863b3f692a29e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "f1664f7e62d02065e7fb1231a7238e45d257ac00",
                "status": "modified"
            },
            {
                "additions": 63,
                "blob_url": "https://github.com/apache/hadoop/blob/2f26475a39f94e756ef4d15ff8c863b3f692a29e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java",
                "changes": 63,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java?ref=2f26475a39f94e756ef4d15ff8c863b3f692a29e",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java",
                "patch": "@@ -35,6 +35,7 @@\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.hdfs.MiniDFSNNTopology;\n import org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerTestUtil;\n+import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.hdfs.server.namenode.NameNodeAdapter;\n import org.junit.After;\n@@ -95,6 +96,68 @@ private void restartStandby() throws IOException {\n     nn1.getNamesystem().getEditLogTailer().interrupt();\n   }\n   \n+  /**\n+   * Test case for enter safemode in active namenode, when it is already in startup safemode.\n+   * It is a regression test for HDFS-2747.\n+   */\n+  @Test\n+  public void testEnterSafeModeInANNShouldNotThrowNPE() throws Exception {\n+    banner(\"Restarting active\");\n+    restartActive();\n+    FSNamesystem namesystem = nn0.getNamesystem();\n+    String status = namesystem.getSafemode();\n+    assertTrue(\"Bad safemode status: '\" + status + \"'\", status\n+        .startsWith(\"Safe mode is ON.\"));\n+    NameNodeAdapter.enterSafeMode(nn0, false);\n+    assertTrue(\"Failed to enter into safemode in active\", namesystem\n+        .isInSafeMode());\n+    NameNodeAdapter.enterSafeMode(nn0, false);\n+    assertTrue(\"Failed to enter into safemode in active\", namesystem\n+        .isInSafeMode());\n+  }\n+\n+  /**\n+   * Test case for enter safemode in standby namenode, when it is already in startup safemode.\n+   * It is a regression test for HDFS-2747.\n+   */\n+  @Test\n+  public void testEnterSafeModeInSBNShouldNotThrowNPE() throws Exception {\n+    banner(\"Starting with NN0 active and NN1 standby, creating some blocks\");\n+    DFSTestUtil\n+        .createFile(fs, new Path(\"/test\"), 3 * BLOCK_SIZE, (short) 3, 1L);\n+    // Roll edit log so that, when the SBN restarts, it will load\n+    // the namespace during startup and enter safemode.\n+    nn0.getRpcServer().rollEditLog();\n+    banner(\"Creating some blocks that won't be in the edit log\");\n+    DFSTestUtil.createFile(fs, new Path(\"/test2\"), 5 * BLOCK_SIZE, (short) 3,\n+        1L);\n+    banner(\"Deleting the original blocks\");\n+    fs.delete(new Path(\"/test\"), true);\n+    banner(\"Restarting standby\");\n+    restartStandby();\n+    FSNamesystem namesystem = nn1.getNamesystem();\n+    String status = namesystem.getSafemode();\n+    assertTrue(\"Bad safemode status: '\" + status + \"'\", status\n+        .startsWith(\"Safe mode is ON.\"));\n+    NameNodeAdapter.enterSafeMode(nn1, false);\n+    assertTrue(\"Failed to enter into safemode in standby\", namesystem\n+        .isInSafeMode());\n+    NameNodeAdapter.enterSafeMode(nn1, false);\n+    assertTrue(\"Failed to enter into safemode in standby\", namesystem\n+        .isInSafeMode());\n+  }\n+\n+  private void restartActive() throws IOException {\n+    cluster.shutdownNameNode(0);\n+    // Set the safemode extension to be lengthy, so that the tests\n+    // can check the safemode message after the safemode conditions\n+    // have been achieved, without being racy.\n+    cluster.getConfiguration(0).setInt(\n+        DFSConfigKeys.DFS_NAMENODE_SAFEMODE_EXTENSION_KEY, 30000);\n+    cluster.restartNameNode(0);\n+    nn0 = cluster.getNameNode(0);\n+  }\n+  \n   /**\n    * Tests the case where, while a standby is down, more blocks are\n    * added to the namespace, but not rolled. So, when it starts up,",
                "raw_url": "https://github.com/apache/hadoop/raw/2f26475a39f94e756ef4d15ff8c863b3f692a29e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/ha/TestHASafeMode.java",
                "sha": "af7985e21d37e0577092f1822490922f9fb88eec",
                "status": "modified"
            }
        ],
        "message": "HDFS-2747. Entering safe mode after starting SBN can NPE. Contributed by Uma Maheswara Rao G.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1232176 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/1c24ae0cd82f1cf583a691a6fcd285ed806cc08e",
        "repo": "hadoop",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_303c8dc": {
        "bug_id": "hadoop_303c8dc",
        "commit": "https://github.com/apache/hadoop/commit/303c8dc9b6c853c0939ea9ba14388897cc258071",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/303c8dc9b6c853c0939ea9ba14388897cc258071/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=303c8dc9b6c853c0939ea9ba14388897cc258071",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -3870,9 +3870,13 @@ private void clearCorruptLazyPersistFiles()\n         while (it.hasNext()) {\n           Block b = it.next();\n           BlockInfo blockInfo = blockManager.getStoredBlock(b);\n-          BlockCollection bc = getBlockCollection(blockInfo);\n-          if (bc.getStoragePolicyID() == lpPolicy.getId()) {\n-            filesToDelete.add(bc);\n+          if (blockInfo == null) {\n+            LOG.info(\"Cannot find block info for block \" + b);\n+          } else {\n+            BlockCollection bc = getBlockCollection(blockInfo);\n+            if (bc.getStoragePolicyID() == lpPolicy.getId()) {\n+              filesToDelete.add(bc);\n+            }\n           }\n         }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/303c8dc9b6c853c0939ea9ba14388897cc258071/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "997fd920c9d76c87302ae4da00cd53f4d9ac4a2b",
                "status": "modified"
            }
        ],
        "message": "Fix NPE in LazyPersistFileScrubber. Contributed by Inigo Goiri.",
        "parent": "https://github.com/apache/hadoop/commit/d81372dfad32488e7c46ffcfccdf0aa26bee04a5",
        "repo": "hadoop",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_32d4c14": {
        "bug_id": "hadoop_32d4c14",
        "commit": "https://github.com/apache/hadoop/commit/32d4c148dfd203789386a0587480bd974fbf4c1a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=32d4c148dfd203789386a0587480bd974fbf4c1a",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -488,6 +488,8 @@ Branch-2 ( Unreleased changes )\n     HDFS-3609. libhdfs: don't force the URI to look like hdfs://hostname:port.\n     (Colin Patrick McCabe via eli)\n \n+    HDFS-3654. TestJspHelper#testGetUgi fails with NPE. (eli)\n+\n   BREAKDOWN OF HDFS-3042 SUBTASKS\n \n     HDFS-2185. HDFS portion of ZK-based FailoverController (todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "30d61c92e5b840b8c6a48e79d0ab2a3a58d1075e",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java?ref=32d4c148dfd203789386a0587480bd974fbf4c1a",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "patch": "@@ -538,7 +538,7 @@ public static UserGroupInformation getUGI(ServletContext context,\n     final String usernameFromQuery = getUsernameFromQuery(request, tryUgiParameter);\n     final String doAsUserFromQuery = request.getParameter(DoAsParam.NAME);\n \n-    if(UserGroupInformation.isSecurityEnabled()) {\n+    if (UserGroupInformation.isSecurityEnabled()) {\n       final String remoteUser = request.getRemoteUser();\n       String tokenString = request.getParameter(DELEGATION_PARAMETER_NAME);\n       if (tokenString != null) {\n@@ -556,7 +556,7 @@ public static UserGroupInformation getUGI(ServletContext context,\n         DelegationTokenIdentifier id = new DelegationTokenIdentifier();\n         id.readFields(in);\n         final NameNode nn = NameNodeHttpServer.getNameNodeFromContext(context);\n-        nn.getNamesystem().verifyToken(id, token.getPassword());\n+        nn.verifyToken(id, token.getPassword());\n         ugi = id.getUser();\n         if (ugi.getRealUser() == null) {\n           //non-proxy case",
                "raw_url": "https://github.com/apache/hadoop/raw/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "sha": "39aa8db16d1c8139107767c3529deef7a1900cea",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=32d4c148dfd203789386a0587480bd974fbf4c1a",
                "deletions": 11,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -5460,20 +5460,10 @@ public BlockManager getBlockManager() {\n     return blockManager;\n   }\n   \n-  /**\n-   * Verifies that the given identifier and password are valid and match.\n-   * @param identifier Token identifier.\n-   * @param password Password in the token.\n-   * @throws InvalidToken\n-   */\n-  public synchronized void verifyToken(DelegationTokenIdentifier identifier,\n-      byte[] password) throws InvalidToken {\n-    getDelegationTokenSecretManager().verifyToken(identifier, password);\n-  }\n-  \n   public boolean isGenStampInFuture(long genStamp) {\n     return (genStamp > getGenerationStamp());\n   }\n+\n   @VisibleForTesting\n   public EditLogTailer getEditLogTailer() {\n     return editLogTailer;",
                "raw_url": "https://github.com/apache/hadoop/raw/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "88716a5ecdb890136bec739c7e5fdfc5501c42c3",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=32d4c148dfd203789386a0587480bd974fbf4c1a",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -51,6 +51,7 @@\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.protocol.ClientProtocol;\n import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n+import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.NamenodeRole;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.StartupOption;\n import org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory;\n@@ -78,6 +79,7 @@\n import org.apache.hadoop.security.SecurityUtil;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol;\n+import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n import org.apache.hadoop.tools.GetUserMappingsProtocol;\n import org.apache.hadoop.util.ServicePlugin;\n import org.apache.hadoop.util.StringUtils;\n@@ -1283,7 +1285,18 @@ private synchronized void doImmediateShutdown(Throwable t)\n     }\n     terminate(1, t.getMessage());\n   }\n-  \n+\n+  /**\n+   * Verifies that the given identifier and password are valid and match.\n+   * @param identifier Token identifier.\n+   * @param password Password in the token.\n+   * @throws InvalidToken\n+   */\n+  public synchronized void verifyToken(DelegationTokenIdentifier identifier,\n+      byte[] password) throws InvalidToken {\n+    namesystem.getDelegationTokenSecretManager().verifyToken(identifier, password);\n+  }\n+\n   /**\n    * Class used to expose {@link NameNode} as context to {@link HAState}\n    */",
                "raw_url": "https://github.com/apache/hadoop/raw/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "2d5a90a8adb10e085161687ad7a661a74e29ea38",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java?ref=32d4c148dfd203789386a0587480bd974fbf4c1a",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "patch": "@@ -63,7 +63,7 @@\n   \n   public static final String NAMENODE_ADDRESS_ATTRIBUTE_KEY = \"name.node.address\";\n   public static final String FSIMAGE_ATTRIBUTE_KEY = \"name.system.image\";\n-  protected static final String NAMENODE_ATTRIBUTE_KEY = \"name.node\";\n+  public static final String NAMENODE_ATTRIBUTE_KEY = \"name.node\";\n   \n   public NameNodeHttpServer(\n       Configuration conf,",
                "raw_url": "https://github.com/apache/hadoop/raw/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "sha": "f00bb9c40a12d05de8d69885452b4d02a1604419",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java?ref=32d4c148dfd203789386a0587480bd974fbf4c1a",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "patch": "@@ -30,6 +30,7 @@\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n+import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -69,6 +70,7 @@ public void testGetUgi() throws IOException {\n     conf.set(DFSConfigKeys.FS_DEFAULT_NAME_KEY, \"hdfs://localhost:4321/\");\n     HttpServletRequest request = mock(HttpServletRequest.class);\n     ServletContext context = mock(ServletContext.class);\n+    NameNode nn = mock(NameNode.class);\n     String user = \"TheDoctor\";\n     Text userText = new Text(user);\n     DelegationTokenIdentifier dtId = new DelegationTokenIdentifier(userText,\n@@ -79,6 +81,8 @@ public void testGetUgi() throws IOException {\n     when(request.getParameter(JspHelper.DELEGATION_PARAMETER_NAME)).thenReturn(\n         tokenString);\n     when(request.getRemoteUser()).thenReturn(user);\n+    when(context.getAttribute(\n+        NameNodeHttpServer.NAMENODE_ATTRIBUTE_KEY)).thenReturn(nn);\n \n     //Test attribute in the url to be used as service in the token.\n     when(request.getParameter(JspHelper.NAMENODE_ADDRESS)).thenReturn(",
                "raw_url": "https://github.com/apache/hadoop/raw/32d4c148dfd203789386a0587480bd974fbf4c1a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "sha": "8dad3b33e6fd65d7eefcf83507b93dd5257c86a8",
                "status": "modified"
            }
        ],
        "message": "HDFS-3654. TestJspHelper#testGetUgi fails with NPE. Contributed by Eli Collins\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1361463 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/f5dd4583bb8d3705313534f7677df13dc45d36ab",
        "repo": "hadoop",
        "unit_tests": [
            "TestJspHelper.java",
            "TestFSNamesystem.java",
            "TestNameNodeHttpServer.java"
        ]
    },
    "hadoop_384764c": {
        "bug_id": "hadoop_384764c",
        "commit": "https://github.com/apache/hadoop/commit/384764cdeac6490bc47fa0eb7b936baa4c0d3230",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/384764cdeac6490bc47fa0eb7b936baa4c0d3230/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java?ref=384764cdeac6490bc47fa0eb7b936baa4c0d3230",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java",
                "patch": "@@ -329,9 +329,12 @@ public synchronized boolean parentZNodeExists()\n    * This recursively creates the znode as well as all of its parents.\n    */\n   public synchronized void ensureParentZNode()\n-      throws IOException, InterruptedException {\n+      throws IOException, InterruptedException, KeeperException {\n     Preconditions.checkState(!wantToBeInElection,\n         \"ensureParentZNode() may not be called while in the election\");\n+    if (zkClient == null) {\n+      createConnection();\n+    }\n \n     String pathParts[] = znodeWorkingDir.split(\"/\");\n     Preconditions.checkArgument(pathParts.length >= 1 &&",
                "raw_url": "https://github.com/apache/hadoop/raw/384764cdeac6490bc47fa0eb7b936baa4c0d3230/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ActiveStandbyElector.java",
                "sha": "d099ca71ac7ac5e3143751e3efcaea7d98c6a392",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/384764cdeac6490bc47fa0eb7b936baa4c0d3230/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java?ref=384764cdeac6490bc47fa0eb7b936baa4c0d3230",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java",
                "patch": "@@ -269,7 +269,7 @@ private void printUsage() {\n   }\n \n   private int formatZK(boolean force, boolean interactive)\n-      throws IOException, InterruptedException {\n+      throws IOException, InterruptedException, KeeperException {\n     if (elector.parentZNodeExists()) {\n       if (!force && (!interactive || !confirmFormat())) {\n         return ERR_CODE_FORMAT_DENIED;",
                "raw_url": "https://github.com/apache/hadoop/raw/384764cdeac6490bc47fa0eb7b936baa4c0d3230/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ZKFailoverController.java",
                "sha": "f66e3c97490c664d703861c2f5889078bfdd8be8",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop/blob/384764cdeac6490bc47fa0eb7b936baa4c0d3230/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMEmbeddedElector.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMEmbeddedElector.java?ref=384764cdeac6490bc47fa0eb7b936baa4c0d3230",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMEmbeddedElector.java",
                "patch": "@@ -22,8 +22,10 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.ha.ClientBaseWithFixes;\n import org.apache.hadoop.ha.ServiceFailedException;\n+import org.apache.hadoop.service.ServiceStateException;\n import org.apache.hadoop.test.GenericTestUtils;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n+import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Test;\n \n@@ -304,6 +306,26 @@ private void testCallbackSynchronizationTimingStandby(AdminService as,\n     verify(as, times(1)).transitionToStandby(any());\n   }\n \n+  /**\n+   * Test that active elector service triggers a fatal RM Event when connection\n+   * to ZK fails. YARN-8409\n+   */\n+  @Test\n+  public void testFailureToConnectToZookeeper() throws Exception {\n+    stopServer();\n+    Configuration myConf = new Configuration(conf);\n+    ResourceManager rm = new MockRM(conf);\n+\n+    ActiveStandbyElectorBasedElectorService ees =\n+        new ActiveStandbyElectorBasedElectorService(rm);\n+    try {\n+      ees.init(myConf);\n+      Assert.fail(\"expect failure to connect to Zookeeper\");\n+    } catch (ServiceStateException sse) {\n+      Assert.assertTrue(sse.getMessage().contains(\"ConnectionLoss\"));\n+    }\n+  }\n+\n   private class MockRMWithElector extends MockRM {\n     private long delayMs = 0;\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/384764cdeac6490bc47fa0eb7b936baa4c0d3230/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMEmbeddedElector.java",
                "sha": "8c038618c103a17af053cb617d9cef055645f501",
                "status": "modified"
            }
        ],
        "message": "YARN-8409.  Fixed NPE in ActiveStandbyElectorBasedElectorService.\n            Contributed by Chandni Singh",
        "parent": "https://github.com/apache/hadoop/commit/d3fa83a44b01c85f39bfb4deaf2972912ac61ca3",
        "repo": "hadoop",
        "unit_tests": [
            "TestActiveStandbyElector.java",
            "TestZKFailoverController.java"
        ]
    },
    "hadoop_38d5ca2": {
        "bug_id": "hadoop_38d5ca2",
        "commit": "https://github.com/apache/hadoop/commit/38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -126,6 +126,9 @@ Release 2.4.1 - UNRELEASED\n     YARN-1928. Fixed a race condition in TestAMRMRPCNodeUpdates which caused it\n     to fail occassionally. (Zhijie Shen via vinodkv)\n \n+    YARN-1934. Fixed a potential NPE in ZKRMStateStore caused by handling\n+    Disconnected event from ZK. (Karthik Kambatla via jianhe)\n+\n Release 2.4.0 - 2014-04-07 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344/hadoop-yarn-project/CHANGES.txt",
                "sha": "ed12a7bc8acf7df38decfbfa771b2a7dea07b881",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop/blob/38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java?ref=38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344",
                "deletions": 11,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "patch": "@@ -280,10 +280,9 @@ public String run() throws KeeperException, InterruptedException {\n     }\n   }\n \n-  private void logRootNodeAcls(String prefix) throws KeeperException,\n-      InterruptedException {\n+  private void logRootNodeAcls(String prefix) throws Exception {\n     Stat getStat = new Stat();\n-    List<ACL> getAcls = zkClient.getACL(zkRootNodePath, getStat);\n+    List<ACL> getAcls = getACLWithRetries(zkRootNodePath, getStat);\n \n     StringBuilder builder = new StringBuilder();\n     builder.append(prefix);\n@@ -363,7 +362,7 @@ protected synchronized void storeVersion() throws Exception {\n     String versionNodePath = getNodePath(zkRootNodePath, VERSION_NODE);\n     byte[] data =\n         ((RMStateVersionPBImpl) CURRENT_VERSION_INFO).getProto().toByteArray();\n-    if (zkClient.exists(versionNodePath, true) != null) {\n+    if (existsWithRetries(versionNodePath, true) != null) {\n       setDataWithRetries(versionNodePath, data, -1);\n     } else {\n       createWithRetries(versionNodePath, data, zkAcl, CreateMode.PERSISTENT);\n@@ -374,7 +373,7 @@ protected synchronized void storeVersion() throws Exception {\n   protected synchronized RMStateVersion loadVersion() throws Exception {\n     String versionNodePath = getNodePath(zkRootNodePath, VERSION_NODE);\n \n-    if (zkClient.exists(versionNodePath, true) != null) {\n+    if (existsWithRetries(versionNodePath, true) != null) {\n       byte[] data = getDataWithRetries(versionNodePath, true);\n       RMStateVersion version =\n           new RMStateVersionPBImpl(RMStateVersionProto.parseFrom(data));\n@@ -442,7 +441,8 @@ private void loadRMSequentialNumberState(RMState rmState) throws Exception {\n   }\n \n   private void loadRMDelegationTokenState(RMState rmState) throws Exception {\n-    List<String> childNodes = zkClient.getChildren(delegationTokensRootPath, true);\n+    List<String> childNodes =\n+        getChildrenWithRetries(delegationTokensRootPath, true);\n     for (String childNodeName : childNodes) {\n       String childNodePath =\n           getNodePath(delegationTokensRootPath, childNodeName);\n@@ -567,7 +567,7 @@ public synchronized void updateApplicationStateInternal(ApplicationId appId,\n     }\n     byte[] appStateData = appStateDataPB.getProto().toByteArray();\n \n-    if (zkClient.exists(nodeUpdatePath, true) != null) {\n+    if (existsWithRetries(nodeUpdatePath, true) != null) {\n       setDataWithRetries(nodeUpdatePath, appStateData, -1);\n     } else {\n       createWithRetries(nodeUpdatePath, appStateData, zkAcl,\n@@ -610,7 +610,7 @@ public synchronized void updateApplicationAttemptStateInternal(\n     }\n     byte[] attemptStateData = attemptStateDataPB.getProto().toByteArray();\n \n-    if (zkClient.exists(nodeUpdatePath, true) != null) {\n+    if (existsWithRetries(nodeUpdatePath, true) != null) {\n       setDataWithRetries(nodeUpdatePath, attemptStateData, -1);\n     } else {\n       createWithRetries(nodeUpdatePath, attemptStateData, zkAcl,\n@@ -661,7 +661,7 @@ protected synchronized void removeRMDelegationTokenState(\n       LOG.debug(\"Removing RMDelegationToken_\"\n           + rmDTIdentifier.getSequenceNumber());\n     }\n-    if (zkClient.exists(nodeRemovePath, true) != null) {\n+    if (existsWithRetries(nodeRemovePath, true) != null) {\n       opList.add(Op.delete(nodeRemovePath, -1));\n     } else {\n       LOG.info(\"Attempted to delete a non-existing znode \" + nodeRemovePath);\n@@ -677,7 +677,7 @@ protected void updateRMDelegationTokenAndSequenceNumberInternal(\n     String nodeRemovePath =\n         getNodePath(delegationTokensRootPath, DELEGATION_TOKEN_PREFIX\n             + rmDTIdentifier.getSequenceNumber());\n-    if (zkClient.exists(nodeRemovePath, true) == null) {\n+    if (existsWithRetries(nodeRemovePath, true) == null) {\n       // in case znode doesn't exist\n       addStoreOrUpdateOps(\n           opList, rmDTIdentifier, renewDate, latestSequenceNumber, false);\n@@ -760,7 +760,7 @@ protected synchronized void removeRMDTMasterKeyState(\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Removing RMDelegationKey_\" + delegationKey.getKeyId());\n     }\n-    if (zkClient.exists(nodeRemovePath, true) != null) {\n+    if (existsWithRetries(nodeRemovePath, true) != null) {\n       doMultiWithRetries(Op.delete(nodeRemovePath, -1));\n     } else {\n       LOG.info(\"Attempted to delete a non-existing znode \" + nodeRemovePath);\n@@ -891,6 +891,16 @@ public void setDataWithRetries(final String path, final byte[] data,\n     }.runWithRetries();\n   }\n \n+  private List<ACL> getACLWithRetries(\n+      final String path, final Stat stat) throws Exception {\n+    return new ZKAction<List<ACL>>() {\n+      @Override\n+      public List<ACL> run() throws KeeperException, InterruptedException {\n+        return zkClient.getACL(path, stat);\n+      }\n+    }.runWithRetries();\n+  }\n+\n   private List<String> getChildrenWithRetries(\n       final String path, final boolean watch) throws Exception {\n     return new ZKAction<List<String>>() {\n@@ -901,6 +911,16 @@ public void setDataWithRetries(final String path, final byte[] data,\n     }.runWithRetries();\n   }\n \n+  private Stat existsWithRetries(\n+      final String path, final boolean watch) throws Exception {\n+    return new ZKAction<Stat>() {\n+      @Override\n+      Stat run() throws KeeperException, InterruptedException {\n+        return zkClient.exists(path, watch);\n+      }\n+    }.runWithRetries();\n+  }\n+\n   /**\n    * Helper class that periodically attempts creating a znode to ensure that\n    * this RM continues to be the Active.",
                "raw_url": "https://github.com/apache/hadoop/raw/38d5ca2e47e8ca1b08d77fe2de6e1e83c6440344/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "sha": "9b15bb21e7e62a1dab06a6799dd5a39621c06a4f",
                "status": "modified"
            }
        ],
        "message": "YARN-1934. Fixed a potential NPE in ZKRMStateStore caused by handling Disconnected event from ZK. Contributed by Karthik Kambatla.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1587776 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/9887d7f2502f814200317fa0e516d3ca29ba5ae4",
        "repo": "hadoop",
        "unit_tests": [
            "TestZKRMStateStore.java"
        ]
    },
    "hadoop_39775dc": {
        "bug_id": "hadoop_39775dc",
        "commit": "https://github.com/apache/hadoop/commit/39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 3,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -220,9 +220,6 @@ Release 0.23.3 - UNRELEASED\n     HADOOP-8163. Improve ActiveStandbyElector to provide hooks for\n     fencing old active. (todd)\n \n-    HADOOP-8193. Refactor FailoverController/HAAdmin code to add an abstract\n-    class for \"target\" services. (todd)\n-\n   OPTIMIZATIONS\n \n   BUG FIXES",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "f3fda7668c5ee52db3dacbecaf62f2f9cfa5b5bd",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/BadFencingConfigurationException.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/BadFencingConfigurationException.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 6,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/BadFencingConfigurationException.java",
                "patch": "@@ -19,16 +19,11 @@\n \n import java.io.IOException;\n \n-import org.apache.hadoop.classification.InterfaceAudience;\n-import org.apache.hadoop.classification.InterfaceStability;\n-\n /**\n  * Indicates that the operator has specified an invalid configuration\n  * for fencing methods.\n  */\n-@InterfaceAudience.Public\n-@InterfaceStability.Evolving\n-public class BadFencingConfigurationException extends IOException {\n+class BadFencingConfigurationException extends IOException {\n   private static final long serialVersionUID = 1L;\n \n   public BadFencingConfigurationException(String msg) {",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/BadFencingConfigurationException.java",
                "sha": "3d3b1ba53cca506a3ef8d28ade71a18c25747777",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FailoverController.java",
                "changes": 50,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FailoverController.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 22,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FailoverController.java",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.ha;\n \n import java.io.IOException;\n+import java.net.InetSocketAddress;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -50,21 +51,21 @@\n    * allow it to become active, eg because it triggers a log roll\n    * so the standby can learn about new blocks and leave safemode.\n    *\n-   * @param target service to make active\n+   * @param toSvc service to make active\n+   * @param toSvcName name of service to make active\n    * @param forceActive ignore toSvc if it reports that it is not ready\n    * @throws FailoverFailedException if we should avoid failover\n    */\n-  private static void preFailoverChecks(HAServiceTarget target,\n+  private static void preFailoverChecks(HAServiceProtocol toSvc,\n+                                        InetSocketAddress toSvcAddr,\n                                         boolean forceActive)\n       throws FailoverFailedException {\n     HAServiceStatus toSvcStatus;\n-    HAServiceProtocol toSvc;\n \n     try {\n-      toSvc = target.getProxy();\n       toSvcStatus = toSvc.getServiceStatus();\n     } catch (IOException e) {\n-      String msg = \"Unable to get service state for \" + target;\n+      String msg = \"Unable to get service state for \" + toSvcAddr;\n       LOG.error(msg, e);\n       throw new FailoverFailedException(msg, e);\n     }\n@@ -78,7 +79,7 @@ private static void preFailoverChecks(HAServiceTarget target,\n       String notReadyReason = toSvcStatus.getNotReadyReason();\n       if (!forceActive) {\n         throw new FailoverFailedException(\n-            target + \" is not ready to become active: \" +\n+            toSvcAddr + \" is not ready to become active: \" +\n             notReadyReason);\n       } else {\n         LOG.warn(\"Service is not ready to become active, but forcing: \" +\n@@ -102,72 +103,77 @@ private static void preFailoverChecks(HAServiceTarget target,\n    * then try to failback.\n    *\n    * @param fromSvc currently active service\n+   * @param fromSvcAddr addr of the currently active service\n    * @param toSvc service to make active\n+   * @param toSvcAddr addr of the service to make active\n+   * @param fencer for fencing fromSvc\n    * @param forceFence to fence fromSvc even if not strictly necessary\n    * @param forceActive try to make toSvc active even if it is not ready\n    * @throws FailoverFailedException if the failover fails\n    */\n-  public static void failover(HAServiceTarget fromSvc,\n-                              HAServiceTarget toSvc,\n+  public static void failover(HAServiceProtocol fromSvc,\n+                              InetSocketAddress fromSvcAddr,\n+                              HAServiceProtocol toSvc,\n+                              InetSocketAddress toSvcAddr,\n+                              NodeFencer fencer,\n                               boolean forceFence,\n                               boolean forceActive)\n       throws FailoverFailedException {\n-    Preconditions.checkArgument(fromSvc.getFencer() != null,\n-        \"failover requires a fencer\");\n-    preFailoverChecks(toSvc, forceActive);\n+    Preconditions.checkArgument(fencer != null, \"failover requires a fencer\");\n+    preFailoverChecks(toSvc, toSvcAddr, forceActive);\n \n     // Try to make fromSvc standby\n     boolean tryFence = true;\n     try {\n-      HAServiceProtocolHelper.transitionToStandby(fromSvc.getProxy());\n+      HAServiceProtocolHelper.transitionToStandby(fromSvc);\n       // We should try to fence if we failed or it was forced\n       tryFence = forceFence ? true : false;\n     } catch (ServiceFailedException sfe) {\n-      LOG.warn(\"Unable to make \" + fromSvc + \" standby (\" +\n+      LOG.warn(\"Unable to make \" + fromSvcAddr + \" standby (\" +\n           sfe.getMessage() + \")\");\n     } catch (IOException ioe) {\n-      LOG.warn(\"Unable to make \" + fromSvc +\n+      LOG.warn(\"Unable to make \" + fromSvcAddr +\n           \" standby (unable to connect)\", ioe);\n     }\n \n     // Fence fromSvc if it's required or forced by the user\n     if (tryFence) {\n-      if (!fromSvc.getFencer().fence(fromSvc)) {\n+      if (!fencer.fence(fromSvcAddr)) {\n         throw new FailoverFailedException(\"Unable to fence \" +\n-            fromSvc + \". Fencing failed.\");\n+            fromSvcAddr + \". Fencing failed.\");\n       }\n     }\n \n     // Try to make toSvc active\n     boolean failed = false;\n     Throwable cause = null;\n     try {\n-      HAServiceProtocolHelper.transitionToActive(toSvc.getProxy());\n+      HAServiceProtocolHelper.transitionToActive(toSvc);\n     } catch (ServiceFailedException sfe) {\n-      LOG.error(\"Unable to make \" + toSvc + \" active (\" +\n+      LOG.error(\"Unable to make \" + toSvcAddr + \" active (\" +\n           sfe.getMessage() + \"). Failing back.\");\n       failed = true;\n       cause = sfe;\n     } catch (IOException ioe) {\n-      LOG.error(\"Unable to make \" + toSvc +\n+      LOG.error(\"Unable to make \" + toSvcAddr +\n           \" active (unable to connect). Failing back.\", ioe);\n       failed = true;\n       cause = ioe;\n     }\n \n     // We failed to make toSvc active\n     if (failed) {\n-      String msg = \"Unable to failover to \" + toSvc;\n+      String msg = \"Unable to failover to \" + toSvcAddr;\n       // Only try to failback if we didn't fence fromSvc\n       if (!tryFence) {\n         try {\n           // Unconditionally fence toSvc in case it is still trying to\n           // become active, eg we timed out waiting for its response.\n           // Unconditionally force fromSvc to become active since it\n           // was previously active when we initiated failover.\n-          failover(toSvc, fromSvc, true, true);\n+          failover(toSvc, toSvcAddr, fromSvc, fromSvcAddr, fencer, true, true);\n         } catch (FailoverFailedException ffe) {\n-          msg += \". Failback to \" + fromSvc +\n+          msg += \". Failback to \" + fromSvcAddr +\n             \" failed (\" + ffe.getMessage() + \")\";\n           LOG.fatal(msg);\n         }",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FailoverController.java",
                "sha": "c8878e8b73951f0f7a3290757615241c8f989d08",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FenceMethod.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FenceMethod.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FenceMethod.java",
                "patch": "@@ -17,6 +17,8 @@\n  */\n package org.apache.hadoop.ha;\n \n+import java.net.InetSocketAddress;\n+\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.conf.Configurable;\n@@ -60,6 +62,6 @@\n    * @throws BadFencingConfigurationException if the configuration was\n    *         determined to be invalid only at runtime\n    */\n-  public boolean tryFence(HAServiceTarget target, String args)\n+  public boolean tryFence(InetSocketAddress serviceAddr, String args)\n     throws BadFencingConfigurationException;\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/FenceMethod.java",
                "sha": "d8bda1402fa928ae1001a2a705709aaad03d70fb",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAAdmin.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAAdmin.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 11,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAAdmin.java",
                "patch": "@@ -19,6 +19,7 @@\n \n import java.io.IOException;\n import java.io.PrintStream;\n+import java.net.InetSocketAddress;\n import java.util.Map;\n \n import org.apache.commons.cli.Options;\n@@ -27,8 +28,11 @@\n import org.apache.commons.cli.GnuParser;\n import org.apache.commons.cli.ParseException;\n \n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB;\n+import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.util.Tool;\n import org.apache.hadoop.util.ToolRunner;\n \n@@ -73,8 +77,6 @@\n   protected PrintStream errOut = System.err;\n   PrintStream out = System.out;\n \n-  protected abstract HAServiceTarget resolveTarget(String string);\n-\n   protected String getUsageString() {\n     return \"Usage: HAAdmin\";\n   }\n@@ -107,7 +109,7 @@ private int transitionToActive(final String[] argv)\n       return -1;\n     }\n     \n-    HAServiceProtocol proto = resolveTarget(argv[1]).getProxy();\n+    HAServiceProtocol proto = getProtocol(argv[1]);\n     HAServiceProtocolHelper.transitionToActive(proto);\n     return 0;\n   }\n@@ -120,13 +122,14 @@ private int transitionToStandby(final String[] argv)\n       return -1;\n     }\n     \n-    HAServiceProtocol proto = resolveTarget(argv[1]).getProxy();\n+    HAServiceProtocol proto = getProtocol(argv[1]);\n     HAServiceProtocolHelper.transitionToStandby(proto);\n     return 0;\n   }\n \n   private int failover(final String[] argv)\n       throws IOException, ServiceFailedException {\n+    Configuration conf = getConf();\n     boolean forceFence = false;\n     boolean forceActive = false;\n \n@@ -159,12 +162,29 @@ private int failover(final String[] argv)\n       return -1;\n     }\n \n-    HAServiceTarget fromNode = resolveTarget(args[0]);\n-    HAServiceTarget toNode = resolveTarget(args[1]);\n-    \n+    NodeFencer fencer;\n     try {\n-      FailoverController.failover(fromNode, toNode,\n-          forceFence, forceActive); \n+      fencer = NodeFencer.create(conf);\n+    } catch (BadFencingConfigurationException bfce) {\n+      errOut.println(\"failover: incorrect fencing configuration: \" + \n+          bfce.getLocalizedMessage());\n+      return -1;\n+    }\n+    if (fencer == null) {\n+      errOut.println(\"failover: no fencer configured\");\n+      return -1;\n+    }\n+\n+    InetSocketAddress addr1 = \n+      NetUtils.createSocketAddr(getServiceAddr(args[0]));\n+    InetSocketAddress addr2 = \n+      NetUtils.createSocketAddr(getServiceAddr(args[1]));\n+    HAServiceProtocol proto1 = getProtocol(args[0]);\n+    HAServiceProtocol proto2 = getProtocol(args[1]);\n+\n+    try {\n+      FailoverController.failover(proto1, addr1, proto2, addr2,\n+          fencer, forceFence, forceActive); \n       out.println(\"Failover from \"+args[0]+\" to \"+args[1]+\" successful\");\n     } catch (FailoverFailedException ffe) {\n       errOut.println(\"Failover failed: \" + ffe.getLocalizedMessage());\n@@ -181,7 +201,7 @@ private int checkHealth(final String[] argv)\n       return -1;\n     }\n     \n-    HAServiceProtocol proto = resolveTarget(argv[1]).getProxy();\n+    HAServiceProtocol proto = getProtocol(argv[1]);\n     try {\n       HAServiceProtocolHelper.monitorHealth(proto);\n     } catch (HealthCheckFailedException e) {\n@@ -199,7 +219,7 @@ private int getServiceState(final String[] argv)\n       return -1;\n     }\n \n-    HAServiceProtocol proto = resolveTarget(argv[1]).getProxy();\n+    HAServiceProtocol proto = getProtocol(argv[1]);\n     out.println(proto.getServiceStatus().getState());\n     return 0;\n   }\n@@ -212,6 +232,16 @@ protected String getServiceAddr(String serviceId) {\n     return serviceId;\n   }\n \n+  /**\n+   * Return a proxy to the specified target service.\n+   */\n+  protected HAServiceProtocol getProtocol(String serviceId)\n+      throws IOException {\n+    String serviceAddr = getServiceAddr(serviceId);\n+    InetSocketAddress addr = NetUtils.createSocketAddr(serviceAddr);\n+    return new HAServiceProtocolClientSideTranslatorPB(addr, getConf());\n+  }\n+\n   @Override\n   public int run(String[] argv) throws Exception {\n     try {",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAAdmin.java",
                "sha": "a16ffb4c4004bdf89f821920605e17b78d9fe4a1",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/ea868d3d8b6c5e018eb104a560890c60d30fa269/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceTarget.java",
                "changes": 74,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceTarget.java?ref=ea868d3d8b6c5e018eb104a560890c60d30fa269",
                "deletions": 74,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceTarget.java",
                "patch": "@@ -1,74 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.hadoop.ha;\n-\n-import java.io.IOException;\n-import java.net.InetSocketAddress;\n-\n-import org.apache.hadoop.classification.InterfaceAudience;\n-import org.apache.hadoop.classification.InterfaceStability;\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n-import org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB;\n-\n-/**\n- * Represents a target of the client side HA administration commands.\n- */\n-@InterfaceAudience.Public\n-@InterfaceStability.Evolving\n-public abstract class HAServiceTarget {\n-\n-  /**\n-   * @return the IPC address of the target node.\n-   */\n-  public abstract InetSocketAddress getAddress();\n-\n-  /**\n-   * @return a Fencer implementation configured for this target node\n-   */\n-  public abstract NodeFencer getFencer();\n-  \n-  /**\n-   * @throws BadFencingConfigurationException if the fencing configuration\n-   * appears to be invalid. This is divorced from the above\n-   * {@link #getFencer()} method so that the configuration can be checked\n-   * during the pre-flight phase of failover.\n-   */\n-  public abstract void checkFencingConfigured()\n-      throws BadFencingConfigurationException;\n-  \n-  /**\n-   * @return a proxy to connect to the target HA Service.\n-   */\n-  public HAServiceProtocol getProxy(Configuration conf, int timeoutMs)\n-      throws IOException {\n-    Configuration confCopy = new Configuration(conf);\n-    // Lower the timeout so we quickly fail to connect\n-    confCopy.setInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY, 1);\n-    return new HAServiceProtocolClientSideTranslatorPB(\n-        getAddress(),\n-        confCopy, null, timeoutMs);\n-  }\n-\n-  /**\n-   * @return a proxy to connect to the target HA Service.\n-   */\n-  public final HAServiceProtocol getProxy() throws IOException {\n-    return getProxy(new Configuration(), 0); // default conf, timeout\n-  }\n-}",
                "raw_url": "https://github.com/apache/hadoop/raw/ea868d3d8b6c5e018eb104a560890c60d30fa269/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/HAServiceTarget.java",
                "sha": "78a2f2e4d98ab1b14b3042af340caa3d1c0d751b",
                "status": "removed"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/NodeFencer.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/NodeFencer.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/NodeFencer.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.hadoop.ha;\n \n+import java.net.InetSocketAddress;\n import java.util.List;\n import java.util.Map;\n import java.util.regex.Matcher;\n@@ -90,14 +91,14 @@ public static NodeFencer create(Configuration conf)\n     return new NodeFencer(conf);\n   }\n \n-  public boolean fence(HAServiceTarget fromSvc) {\n+  public boolean fence(InetSocketAddress serviceAddr) {\n     LOG.info(\"====== Beginning Service Fencing Process... ======\");\n     int i = 0;\n     for (FenceMethodWithArg method : methods) {\n       LOG.info(\"Trying method \" + (++i) + \"/\" + methods.size() +\": \" + method);\n       \n       try {\n-        if (method.method.tryFence(fromSvc, method.arg)) {\n+        if (method.method.tryFence(serviceAddr, method.arg)) {\n           LOG.info(\"====== Fencing successful by method \" + method + \" ======\");\n           return true;\n         }",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/NodeFencer.java",
                "sha": "34a2c8b823a3ee2943d18c94431babe22275ef2d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ShellCommandFencer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ShellCommandFencer.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ShellCommandFencer.java",
                "patch": "@@ -75,8 +75,7 @@ public void checkArgs(String args) throws BadFencingConfigurationException {\n   }\n \n   @Override\n-  public boolean tryFence(HAServiceTarget target, String cmd) {\n-    InetSocketAddress serviceAddr = target.getAddress();\n+  public boolean tryFence(InetSocketAddress serviceAddr, String cmd) {\n     List<String> cmdList = Arrays.asList(cmd.split(\"\\\\s+\"));\n \n     // Create arg list with service as the first argument",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/ShellCommandFencer.java",
                "sha": "ca81f23a1878fa7fea3247b8e197e1150209a53c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java",
                "patch": "@@ -79,11 +79,10 @@ public void checkArgs(String argStr) throws BadFencingConfigurationException {\n   }\n \n   @Override\n-  public boolean tryFence(HAServiceTarget target, String argsStr)\n+  public boolean tryFence(InetSocketAddress serviceAddr, String argsStr)\n       throws BadFencingConfigurationException {\n \n     Args args = new Args(argsStr);\n-    InetSocketAddress serviceAddr = target.getAddress();\n     String host = serviceAddr.getHostName();\n     \n     Session session;",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ha/SshFenceByTcpPort.java",
                "sha": "00b9a83a572a3ad8a14cf2f7eab8cd013501a2ee",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/ea868d3d8b6c5e018eb104a560890c60d30fa269/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/DummyHAService.java",
                "changes": 94,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/DummyHAService.java?ref=ea868d3d8b6c5e018eb104a560890c60d30fa269",
                "deletions": 94,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/DummyHAService.java",
                "patch": "@@ -1,94 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.hadoop.ha;\n-\n-import java.io.IOException;\n-import java.net.InetSocketAddress;\n-\n-import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n-import org.apache.hadoop.security.AccessControlException;\n-import org.mockito.Mockito;\n-\n-/**\n- * Test-only implementation of {@link HAServiceTarget}, which returns\n- * a mock implementation.\n- */\n-class DummyHAService extends HAServiceTarget {\n-  HAServiceState state;\n-  HAServiceProtocol proxy;\n-  NodeFencer fencer;\n-  InetSocketAddress address;\n-\n-  DummyHAService(HAServiceState state, InetSocketAddress address) {\n-    this.state = state;\n-    this.proxy = makeMock();\n-    this.fencer = Mockito.mock(NodeFencer.class);\n-    this.address = address;\n-  }\n-  \n-  private HAServiceProtocol makeMock() {\n-    return Mockito.spy(new HAServiceProtocol() {\n-      @Override\n-      public void monitorHealth() throws HealthCheckFailedException,\n-          AccessControlException, IOException {\n-      }\n-\n-      @Override\n-      public void transitionToActive() throws ServiceFailedException,\n-          AccessControlException, IOException {\n-        state = HAServiceState.ACTIVE;\n-      }\n-\n-      @Override\n-      public void transitionToStandby() throws ServiceFailedException,\n-          AccessControlException, IOException {\n-        state = HAServiceState.STANDBY;\n-      }\n-\n-      @Override\n-      public HAServiceStatus getServiceStatus() throws IOException {\n-        HAServiceStatus ret = new HAServiceStatus(state);\n-        if (state == HAServiceState.STANDBY) {\n-          ret.setReadyToBecomeActive();\n-        }\n-        return ret;\n-      }\n-    });\n-  }\n-\n-  @Override\n-  public InetSocketAddress getAddress() {\n-    return address;\n-  }\n-\n-  @Override\n-  public HAServiceProtocol getProxy(Configuration conf, int timeout)\n-      throws IOException {\n-    return proxy;\n-  }\n-\n-  @Override\n-  public NodeFencer getFencer() {\n-    return fencer;\n-  }\n-\n-  @Override\n-  public void checkFencingConfigured() throws BadFencingConfigurationException {\n-  }\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/ea868d3d8b6c5e018eb104a560890c60d30fa269/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/DummyHAService.java",
                "sha": "69c4a6fde41141b8c43c94a4efe41348718f9a32",
                "status": "removed"
            },
            {
                "additions": 214,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestFailoverController.java",
                "changes": 358,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestFailoverController.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 144,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestFailoverController.java",
                "patch": "@@ -24,85 +24,124 @@\n import static org.mockito.Mockito.verify;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n+import org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB;\n import org.apache.hadoop.ha.TestNodeFencer.AlwaysSucceedFencer;\n import org.apache.hadoop.ha.TestNodeFencer.AlwaysFailFencer;\n import static org.apache.hadoop.ha.TestNodeFencer.setupFencer;\n+import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.security.AccessControlException;\n \n import org.junit.Test;\n-import org.mockito.Mockito;\n-import org.mockito.internal.stubbing.answers.ThrowsException;\n-import org.mockito.stubbing.Answer;\n-\n import static org.junit.Assert.*;\n \n public class TestFailoverController {\n+\n   private InetSocketAddress svc1Addr = new InetSocketAddress(\"svc1\", 1234); \n-  private InetSocketAddress svc2Addr = new InetSocketAddress(\"svc2\", 5678);\n+  private InetSocketAddress svc2Addr = new InetSocketAddress(\"svc2\", 5678); \n+\n+  private class DummyService implements HAServiceProtocol {\n+    HAServiceState state;\n+\n+    DummyService(HAServiceState state) {\n+      this.state = state;\n+    }\n+\n+    @Override\n+    public void monitorHealth() throws HealthCheckFailedException, IOException {\n+      // Do nothing\n+    }\n+\n+    @Override\n+    public void transitionToActive() throws ServiceFailedException, IOException {\n+      state = HAServiceState.ACTIVE;\n+    }\n \n-  HAServiceStatus STATE_NOT_READY = new HAServiceStatus(HAServiceState.STANDBY)\n-      .setNotReadyToBecomeActive(\"injected not ready\");\n+    @Override\n+    public void transitionToStandby() throws ServiceFailedException, IOException {\n+      state = HAServiceState.STANDBY;\n+    }\n \n+    @Override\n+    public HAServiceStatus getServiceStatus() throws IOException {\n+      HAServiceStatus ret = new HAServiceStatus(state);\n+      if (state == HAServiceState.STANDBY) {\n+        ret.setReadyToBecomeActive();\n+      }\n+      return ret;\n+    }\n+    \n+    private HAServiceState getServiceState() {\n+      return state;\n+    }\n+  }\n+  \n   @Test\n   public void testFailoverAndFailback() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     AlwaysSucceedFencer.fenceCalled = 0;\n-    FailoverController.failover(svc1, svc2, false, false);\n+    FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n     assertEquals(0, TestNodeFencer.AlwaysSucceedFencer.fenceCalled);\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n \n     AlwaysSucceedFencer.fenceCalled = 0;\n-    FailoverController.failover(svc2, svc1, false, false);\n+    FailoverController.failover(svc2, svc2Addr, svc1, svc1Addr, fencer, false, false);\n     assertEquals(0, TestNodeFencer.AlwaysSucceedFencer.fenceCalled);\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverFromStandbyToStandby() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.STANDBY, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.STANDBY);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n-    FailoverController.failover(svc1, svc2, false, false);\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverFromActiveToActive() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.ACTIVE, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.ACTIVE);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Can't failover to an already active service\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverWithoutPermission() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    Mockito.doThrow(new AccessControlException(\"Access denied\"))\n-        .when(svc1.proxy).getServiceStatus();\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new AccessControlException(\"Access denied\"))\n-        .when(svc2.proxy).getServiceStatus();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE) {\n+      @Override\n+      public HAServiceStatus getServiceStatus() throws IOException {\n+        throw new AccessControlException(\"Access denied\");\n+      }\n+    };\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public HAServiceStatus getServiceStatus() throws IOException {\n+        throw new AccessControlException(\"Access denied\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Can't failover when access is denied\");\n     } catch (FailoverFailedException ffe) {\n       assertTrue(ffe.getCause().getMessage().contains(\"Access denied\"));\n@@ -112,13 +151,19 @@ public void testFailoverWithoutPermission() throws Exception {\n \n   @Test\n   public void testFailoverToUnreadyService() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doReturn(STATE_NOT_READY).when(svc2.proxy).getServiceStatus();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public HAServiceStatus getServiceStatus() throws IOException {\n+        HAServiceStatus ret = new HAServiceStatus(HAServiceState.STANDBY);\n+        ret.setNotReadyToBecomeActive(\"injected not ready\");\n+        return ret;\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Can't failover to a service that's not ready\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n@@ -127,88 +172,95 @@ public void testFailoverToUnreadyService() throws Exception {\n       }\n     }\n \n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n \n     // Forcing it means we ignore readyToBecomeActive\n-    FailoverController.failover(svc1, svc2, false, true);\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, true);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverToUnhealthyServiceFailsAndFailsback() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new HealthCheckFailedException(\"Failed!\"))\n-        .when(svc2.proxy).monitorHealth();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void monitorHealth() throws HealthCheckFailedException {\n+        throw new HealthCheckFailedException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failover to unhealthy service\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverFromFaultyServiceSucceeds() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc1.proxy).transitionToStandby();\n-\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE) {\n+      @Override\n+      public void transitionToStandby() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     AlwaysSucceedFencer.fenceCalled = 0;\n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n     } catch (FailoverFailedException ffe) {\n       fail(\"Faulty active prevented failover\");\n     }\n \n     // svc1 still thinks it's active, that's OK, it was fenced\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(svc1, AlwaysSucceedFencer.fencedSvc);\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    assertEquals(\"svc1:1234\", AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverFromFaultyServiceFencingFailure() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc1.proxy).transitionToStandby();\n-\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysFailFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE) {\n+      @Override\n+      public void transitionToStandby() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysFailFencer.class.getName());\n \n     AlwaysFailFencer.fenceCalled = 0;\n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failed over even though fencing failed\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n     assertEquals(1, AlwaysFailFencer.fenceCalled);\n-    assertSame(svc1, AlwaysFailFencer.fencedSvc);\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(\"svc1:1234\", AlwaysFailFencer.fencedSvc);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFencingFailureDuringFailover() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysFailFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysFailFencer.class.getName());\n \n     AlwaysFailFencer.fenceCalled = 0;\n     try {\n-      FailoverController.failover(svc1, svc2, true, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, true, false);\n       fail(\"Failed over even though fencing requested and failed\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n@@ -217,105 +269,115 @@ public void testFencingFailureDuringFailover() throws Exception {\n     // If fencing was requested and it failed we don't try to make\n     // svc2 active anyway, and we don't failback to svc1.\n     assertEquals(1, AlwaysFailFencer.fenceCalled);\n-    assertSame(svc1, AlwaysFailFencer.fencedSvc);\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(\"svc1:1234\", AlwaysFailFencer.fencedSvc);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n   \n+  private HAServiceProtocol getProtocol(String target)\n+      throws IOException {\n+    InetSocketAddress addr = NetUtils.createSocketAddr(target);\n+    Configuration conf = new Configuration();\n+    // Lower the timeout so we quickly fail to connect\n+    conf.setInt(CommonConfigurationKeysPublic.IPC_CLIENT_CONNECT_MAX_RETRIES_KEY, 1);\n+    return new HAServiceProtocolClientSideTranslatorPB(addr, conf);\n+  }\n+\n   @Test\n   public void testFailoverFromNonExistantServiceWithFencer() throws Exception {\n-    DummyHAService svc1 = spy(new DummyHAService(null, svc1Addr));\n-    // Getting a proxy to a dead server will throw IOException on call,\n-    // not on creation of the proxy.\n-    HAServiceProtocol errorThrowingProxy = Mockito.mock(HAServiceProtocol.class,\n-        new ThrowsException(new IOException(\"Could not connect to host\")));\n-    Mockito.doReturn(errorThrowingProxy).when(svc1).getProxy();\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    HAServiceProtocol svc1 = getProtocol(\"localhost:1234\");\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY);\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n     } catch (FailoverFailedException ffe) {\n       fail(\"Non-existant active prevented failover\");\n     }\n \n     // Don't check svc1 because we can't reach it, but that's OK, it's been fenced.\n-    assertEquals(HAServiceState.ACTIVE, svc2.state);\n+    assertEquals(HAServiceState.ACTIVE, svc2.getServiceState());\n   }\n \n   @Test\n   public void testFailoverToNonExistantServiceFails() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = spy(new DummyHAService(null, svc2Addr));\n-    Mockito.doThrow(new IOException(\"Failed to connect\"))\n-      .when(svc2).getProxy(Mockito.<Configuration>any(),\n-          Mockito.anyInt());\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    HAServiceProtocol svc2 = getProtocol(\"localhost:1234\");\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failed over to a non-existant standby\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n   }\n \n   @Test\n   public void testFailoverToFaultyServiceFailsbackOK() throws Exception {\n-    DummyHAService svc1 = spy(new DummyHAService(HAServiceState.ACTIVE, svc1Addr));\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = spy(new DummyService(HAServiceState.ACTIVE));\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failover to already active service\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n     // svc1 went standby then back to active\n-    verify(svc1.proxy).transitionToStandby();\n-    verify(svc1.proxy).transitionToActive();\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    verify(svc1).transitionToStandby();\n+    verify(svc1).transitionToActive();\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testWeDontFailbackIfActiveWasFenced() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, true, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, true, false);\n       fail(\"Failed over to service that won't transition to active\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n     // We failed to failover and did not failback because we fenced\n     // svc1 (we forced it), therefore svc1 and svc2 should be standby.\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n \n   @Test\n   public void testWeFenceOnFailbackIfTransitionToActiveFails() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException, IOException {\n+        throw new IOException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n     AlwaysSucceedFencer.fenceCalled = 0;\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failed over to service that won't transition to active\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n@@ -324,22 +386,25 @@ public void testWeFenceOnFailbackIfTransitionToActiveFails() throws Exception {\n     // We failed to failover. We did not fence svc1 because it cooperated\n     // and we didn't force it, so we failed back to svc1 and fenced svc2.\n     // Note svc2 still thinks it's active, that's OK, we fenced it.\n-    assertEquals(HAServiceState.ACTIVE, svc1.state);\n+    assertEquals(HAServiceState.ACTIVE, svc1.getServiceState());\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(svc2, AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(\"svc2:5678\", AlwaysSucceedFencer.fencedSvc);\n   }\n \n   @Test\n   public void testFailureToFenceOnFailbackFailsTheFailback() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new IOException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysFailFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE);\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException, IOException {\n+        throw new IOException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysFailFencer.class.getName());\n     AlwaysFailFencer.fenceCalled = 0;\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1,  svc1Addr,  svc2,  svc2Addr, fencer, false, false);\n       fail(\"Failed over to service that won't transition to active\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n@@ -348,30 +413,35 @@ public void testFailureToFenceOnFailbackFailsTheFailback() throws Exception {\n     // We did not fence svc1 because it cooperated and we didn't force it, \n     // we failed to failover so we fenced svc2, we failed to fence svc2\n     // so we did not failback to svc1, ie it's still standby.\n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n     assertEquals(1, AlwaysFailFencer.fenceCalled);\n-    assertSame(svc2, AlwaysFailFencer.fencedSvc);\n+    assertEquals(\"svc2:5678\", AlwaysFailFencer.fencedSvc);\n   }\n \n   @Test\n   public void testFailbackToFaultyServiceFails() throws Exception {\n-    DummyHAService svc1 = new DummyHAService(HAServiceState.ACTIVE, svc1Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc1.proxy).transitionToActive();\n-    DummyHAService svc2 = new DummyHAService(HAServiceState.STANDBY, svc2Addr);\n-    Mockito.doThrow(new ServiceFailedException(\"Failed!\"))\n-        .when(svc2.proxy).transitionToActive();\n-\n-    svc1.fencer = svc2.fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n+    DummyService svc1 = new DummyService(HAServiceState.ACTIVE) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    DummyService svc2 = new DummyService(HAServiceState.STANDBY) {\n+      @Override\n+      public void transitionToActive() throws ServiceFailedException {\n+        throw new ServiceFailedException(\"Failed!\");\n+      }\n+    };\n+    NodeFencer fencer = setupFencer(AlwaysSucceedFencer.class.getName());\n \n     try {\n-      FailoverController.failover(svc1, svc2, false, false);\n+      FailoverController.failover(svc1, svc1Addr, svc2, svc2Addr, fencer, false, false);\n       fail(\"Failover to already active service\");\n     } catch (FailoverFailedException ffe) {\n       // Expected\n     }\n \n-    assertEquals(HAServiceState.STANDBY, svc1.state);\n-    assertEquals(HAServiceState.STANDBY, svc2.state);\n+    assertEquals(HAServiceState.STANDBY, svc1.getServiceState());\n+    assertEquals(HAServiceState.STANDBY, svc2.getServiceState());\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestFailoverController.java",
                "sha": "6dec32c636de5317a679eb3bc609faa8168e0922",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestHAAdmin.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestHAAdmin.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 6,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestHAAdmin.java",
                "patch": "@@ -22,15 +22,14 @@\n import java.io.ByteArrayOutputStream;\n import java.io.IOException;\n import java.io.PrintStream;\n-import java.net.InetSocketAddress;\n \n import org.apache.commons.logging.LogFactory;\n import org.apache.commons.logging.Log;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n \n import org.junit.Before;\n import org.junit.Test;\n+import org.mockito.Mockito;\n \n import com.google.common.base.Charsets;\n import com.google.common.base.Joiner;\n@@ -41,14 +40,15 @@\n   private HAAdmin tool;\n   private ByteArrayOutputStream errOutBytes = new ByteArrayOutputStream();\n   private String errOutput;\n-\n+  private HAServiceProtocol mockProtocol;\n+  \n   @Before\n   public void setup() throws IOException {\n+    mockProtocol = Mockito.mock(HAServiceProtocol.class);\n     tool = new HAAdmin() {\n       @Override\n-      protected HAServiceTarget resolveTarget(String target) {\n-        return new DummyHAService(HAServiceState.STANDBY,\n-            new InetSocketAddress(\"dummy\", 12345));\n+      protected HAServiceProtocol getProtocol(String target) throws IOException {\n+        return mockProtocol;\n       }\n     };\n     tool.setConf(new Configuration());",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestHAAdmin.java",
                "sha": "7f885d8bc2535d2b34eb4fe9e5dd961da12be685",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestNodeFencer.java",
                "changes": 47,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestNodeFencer.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 28,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestNodeFencer.java",
                "patch": "@@ -26,35 +26,26 @@\n import org.apache.hadoop.conf.Configured;\n import org.junit.Before;\n import org.junit.Test;\n-import org.mockito.Mockito;\n \n import com.google.common.collect.Lists;\n \n public class TestNodeFencer {\n \n-  private HAServiceTarget MOCK_TARGET;\n-  \n-\n   @Before\n   public void clearMockState() {\n     AlwaysSucceedFencer.fenceCalled = 0;\n     AlwaysSucceedFencer.callArgs.clear();\n     AlwaysFailFencer.fenceCalled = 0;\n     AlwaysFailFencer.callArgs.clear();\n-    \n-    MOCK_TARGET = Mockito.mock(HAServiceTarget.class);\n-    Mockito.doReturn(\"my mock\").when(MOCK_TARGET).toString();\n-    Mockito.doReturn(new InetSocketAddress(\"host\", 1234))\n-        .when(MOCK_TARGET).getAddress();\n   }\n \n   @Test\n   public void testSingleFencer() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\n         AlwaysSucceedFencer.class.getName() + \"(foo)\");\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(MOCK_TARGET, AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(\"host:1234\", AlwaysSucceedFencer.fencedSvc);\n     assertEquals(\"foo\", AlwaysSucceedFencer.callArgs.get(0));\n   }\n   \n@@ -63,7 +54,7 @@ public void testMultipleFencers() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\n         AlwaysSucceedFencer.class.getName() + \"(foo)\\n\" +\n         AlwaysSucceedFencer.class.getName() + \"(bar)\\n\");\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n     // Only one call, since the first fencer succeeds\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n     assertEquals(\"foo\", AlwaysSucceedFencer.callArgs.get(0));\n@@ -77,12 +68,12 @@ public void testWhitespaceAndCommentsInConfig()\n         \" # the next one will always fail\\n\" +\n         \" \" + AlwaysFailFencer.class.getName() + \"(foo) # <- fails\\n\" +\n         AlwaysSucceedFencer.class.getName() + \"(bar) \\n\");\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n     // One call to each, since top fencer fails\n     assertEquals(1, AlwaysFailFencer.fenceCalled);\n-    assertSame(MOCK_TARGET, AlwaysFailFencer.fencedSvc);\n+    assertEquals(\"host:1234\", AlwaysFailFencer.fencedSvc);\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(MOCK_TARGET, AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(\"host:1234\", AlwaysSucceedFencer.fencedSvc);\n     assertEquals(\"foo\", AlwaysFailFencer.callArgs.get(0));\n     assertEquals(\"bar\", AlwaysSucceedFencer.callArgs.get(0));\n   }\n@@ -91,41 +82,41 @@ public void testWhitespaceAndCommentsInConfig()\n   public void testArglessFencer() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\n         AlwaysSucceedFencer.class.getName());\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n     // One call to each, since top fencer fails\n     assertEquals(1, AlwaysSucceedFencer.fenceCalled);\n-    assertSame(MOCK_TARGET, AlwaysSucceedFencer.fencedSvc);\n+    assertEquals(\"host:1234\", AlwaysSucceedFencer.fencedSvc);\n     assertEquals(null, AlwaysSucceedFencer.callArgs.get(0));\n   }\n \n   @Test\n   public void testShortNameShell() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"shell(true)\");\n-    assertTrue(fencer.fence(MOCK_TARGET));\n+    assertTrue(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   @Test\n   public void testShortNameSsh() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"sshfence\");\n-    assertFalse(fencer.fence(MOCK_TARGET));\n+    assertFalse(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   @Test\n   public void testShortNameSshWithUser() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"sshfence(user)\");\n-    assertFalse(fencer.fence(MOCK_TARGET));\n+    assertFalse(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   @Test\n   public void testShortNameSshWithPort() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"sshfence(:123)\");\n-    assertFalse(fencer.fence(MOCK_TARGET));\n+    assertFalse(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   @Test\n   public void testShortNameSshWithUserPort() throws BadFencingConfigurationException {\n     NodeFencer fencer = setupFencer(\"sshfence(user:123)\");\n-    assertFalse(fencer.fence(MOCK_TARGET));\n+    assertFalse(fencer.fence(new InetSocketAddress(\"host\", 1234)));\n   }\n \n   public static NodeFencer setupFencer(String confStr)\n@@ -142,12 +133,12 @@ public static NodeFencer setupFencer(String confStr)\n   public static class AlwaysSucceedFencer extends Configured\n       implements FenceMethod {\n     static int fenceCalled = 0;\n-    static HAServiceTarget fencedSvc;\n+    static String fencedSvc;\n     static List<String> callArgs = Lists.newArrayList();\n \n     @Override\n-    public boolean tryFence(HAServiceTarget target, String args) {\n-      fencedSvc = target;\n+    public boolean tryFence(InetSocketAddress serviceAddr, String args) {\n+      fencedSvc = serviceAddr.getHostName() + \":\" + serviceAddr.getPort();\n       callArgs.add(args);\n       fenceCalled++;\n       return true;\n@@ -164,12 +155,12 @@ public void checkArgs(String args) {\n   public static class AlwaysFailFencer extends Configured\n       implements FenceMethod {\n     static int fenceCalled = 0;\n-    static HAServiceTarget fencedSvc;\n+    static String fencedSvc;\n     static List<String> callArgs = Lists.newArrayList();\n \n     @Override\n-    public boolean tryFence(HAServiceTarget target, String args) {\n-      fencedSvc = target;\n+    public boolean tryFence(InetSocketAddress serviceAddr, String args) {\n+      fencedSvc = serviceAddr.getHostName() + \":\" + serviceAddr.getPort();\n       callArgs.add(args);\n       fenceCalled++;\n       return false;",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestNodeFencer.java",
                "sha": "5508547c0a52daff57ae2400b14a0f30c946d699",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestShellCommandFencer.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestShellCommandFencer.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 11,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestShellCommandFencer.java",
                "patch": "@@ -22,7 +22,6 @@\n import java.net.InetSocketAddress;\n \n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n import org.apache.hadoop.util.StringUtils;\n import org.junit.Before;\n import org.junit.BeforeClass;\n@@ -33,9 +32,6 @@\n \n public class TestShellCommandFencer {\n   private ShellCommandFencer fencer = createFencer();\n-  private static final HAServiceTarget TEST_TARGET =\n-      new DummyHAService(HAServiceState.ACTIVE,\n-          new InetSocketAddress(\"host\", 1234));\n   \n   @BeforeClass\n   public static void setupLogSpy() {\n@@ -61,10 +57,11 @@ private static ShellCommandFencer createFencer() {\n    */\n   @Test\n   public void testBasicSuccessFailure() {\n-    assertTrue(fencer.tryFence(TEST_TARGET, \"echo\"));\n-    assertFalse(fencer.tryFence(TEST_TARGET, \"exit 1\"));\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    assertTrue(fencer.tryFence(addr, \"echo\"));\n+    assertFalse(fencer.tryFence(addr, \"exit 1\"));\n     // bad path should also fail\n-    assertFalse(fencer.tryFence(TEST_TARGET, \"xxxxxxxxxxxx\"));\n+    assertFalse(fencer.tryFence(addr, \"xxxxxxxxxxxx\"));\n   }\n   \n   @Test\n@@ -101,7 +98,8 @@ public void testCheckParensNoArgs() {\n    */\n   @Test\n   public void testStdoutLogging() {\n-    assertTrue(fencer.tryFence(TEST_TARGET, \"echo hello\"));\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    assertTrue(fencer.tryFence(addr, \"echo hello\"));\n     Mockito.verify(ShellCommandFencer.LOG).info(\n         Mockito.endsWith(\"echo hello: host:1234 hello\"));\n   }\n@@ -112,7 +110,8 @@ public void testStdoutLogging() {\n    */\n   @Test\n   public void testStderrLogging() {\n-    assertTrue(fencer.tryFence(TEST_TARGET, \"echo hello >&2\"));\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    assertTrue(fencer.tryFence(addr, \"echo hello >&2\"));\n     Mockito.verify(ShellCommandFencer.LOG).warn(\n         Mockito.endsWith(\"echo hello >&2: host:1234 hello\"));\n   }\n@@ -123,7 +122,8 @@ public void testStderrLogging() {\n    */\n   @Test\n   public void testConfAsEnvironment() {\n-    fencer.tryFence(TEST_TARGET, \"echo $in_fencing_tests\");\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    fencer.tryFence(addr, \"echo $in_fencing_tests\");\n     Mockito.verify(ShellCommandFencer.LOG).info(\n         Mockito.endsWith(\"echo $in...ing_tests: host:1234 yessir\"));\n   }\n@@ -136,7 +136,8 @@ public void testConfAsEnvironment() {\n    */\n   @Test(timeout=10000)\n   public void testSubprocessInputIsClosed() {\n-    assertFalse(fencer.tryFence(TEST_TARGET, \"read\"));\n+    InetSocketAddress addr = new InetSocketAddress(\"host\", 1234);\n+    assertFalse(fencer.tryFence(addr, \"read\"));\n   }\n   \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestShellCommandFencer.java",
                "sha": "49bae039eccbcc7b7f43a73e64e2084bc66c9a2d",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestSshFenceByTcpPort.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestSshFenceByTcpPort.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 19,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestSshFenceByTcpPort.java",
                "patch": "@@ -23,7 +23,6 @@\n \n import org.apache.commons.logging.impl.Log4JLogger;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n import org.apache.hadoop.ha.SshFenceByTcpPort.Args;\n import org.apache.log4j.Level;\n import org.junit.Assume;\n@@ -35,25 +34,12 @@\n     ((Log4JLogger)SshFenceByTcpPort.LOG).getLogger().setLevel(Level.ALL);\n   }\n   \n-  private static String TEST_FENCING_HOST = System.getProperty(\n+  private String TEST_FENCING_HOST = System.getProperty(\n       \"test.TestSshFenceByTcpPort.host\", \"localhost\");\n-  private static final String TEST_FENCING_PORT = System.getProperty(\n+  private String TEST_FENCING_PORT = System.getProperty(\n       \"test.TestSshFenceByTcpPort.port\", \"8020\");\n-  private static final String TEST_KEYFILE = System.getProperty(\n+  private final String TEST_KEYFILE = System.getProperty(\n       \"test.TestSshFenceByTcpPort.key\");\n-  \n-  private static final InetSocketAddress TEST_ADDR =\n-    new InetSocketAddress(TEST_FENCING_HOST,\n-      Integer.valueOf(TEST_FENCING_PORT));\n-  private static final HAServiceTarget TEST_TARGET =\n-    new DummyHAService(HAServiceState.ACTIVE, TEST_ADDR);\n-  \n-  /**\n-   *  Connect to Google's DNS server - not running ssh!\n-   */\n-  private static final HAServiceTarget UNFENCEABLE_TARGET =\n-    new DummyHAService(HAServiceState.ACTIVE,\n-        new InetSocketAddress(\"8.8.8.8\", 1234));\n \n   @Test(timeout=20000)\n   public void testFence() throws BadFencingConfigurationException {\n@@ -63,7 +49,8 @@ public void testFence() throws BadFencingConfigurationException {\n     SshFenceByTcpPort fence = new SshFenceByTcpPort();\n     fence.setConf(conf);\n     assertTrue(fence.tryFence(\n-        TEST_TARGET,\n+        new InetSocketAddress(TEST_FENCING_HOST,\n+                              Integer.valueOf(TEST_FENCING_PORT)),\n         null));\n   }\n \n@@ -78,7 +65,8 @@ public void testConnectTimeout() throws BadFencingConfigurationException {\n     conf.setInt(SshFenceByTcpPort.CONF_CONNECT_TIMEOUT_KEY, 3000);\n     SshFenceByTcpPort fence = new SshFenceByTcpPort();\n     fence.setConf(conf);\n-    assertFalse(fence.tryFence(UNFENCEABLE_TARGET, \"\"));\n+    // Connect to Google's DNS server - not running ssh!\n+    assertFalse(fence.tryFence(new InetSocketAddress(\"8.8.8.8\", 1234), \"\"));\n   }\n   \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/ha/TestSshFenceByTcpPort.java",
                "sha": "554a7abca5fdd33e027dfbb305c85e2d4a1b9ba3",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSHAAdmin.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSHAAdmin.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSHAAdmin.java",
                "patch": "@@ -25,8 +25,8 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.CommonConfigurationKeys;\n import org.apache.hadoop.ha.HAAdmin;\n-import org.apache.hadoop.ha.HAServiceTarget;\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n+import org.apache.hadoop.hdfs.DFSUtil;\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.util.ToolRunner;\n \n@@ -65,9 +65,15 @@ public void setConf(Configuration conf) {\n    * Try to map the given namenode ID to its service address.\n    */\n   @Override\n-  protected HAServiceTarget resolveTarget(String nnId) {\n+  protected String getServiceAddr(String nnId) {\n     HdfsConfiguration conf = (HdfsConfiguration)getConf();\n-    return new NNHAServiceTarget(conf, nameserviceId, nnId);\n+    String serviceAddr = \n+      DFSUtil.getNamenodeServiceAddr(conf, nameserviceId, nnId);\n+    if (serviceAddr == null) {\n+      throw new IllegalArgumentException(\n+          \"Unable to determine service address for namenode '\" + nnId + \"'\");\n+    }\n+    return serviceAddr;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DFSHAAdmin.java",
                "sha": "13bde2ae53391ee734fd84e669950a2eb1355f4b",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/ea868d3d8b6c5e018eb104a560890c60d30fa269/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java",
                "changes": 84,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java?ref=ea868d3d8b6c5e018eb104a560890c60d30fa269",
                "deletions": 84,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java",
                "patch": "@@ -1,84 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.hadoop.hdfs.tools;\n-\n-import java.net.InetSocketAddress;\n-\n-import org.apache.hadoop.classification.InterfaceAudience;\n-import org.apache.hadoop.ha.BadFencingConfigurationException;\n-import org.apache.hadoop.ha.HAServiceTarget;\n-import org.apache.hadoop.ha.NodeFencer;\n-import org.apache.hadoop.hdfs.DFSUtil;\n-import org.apache.hadoop.hdfs.HdfsConfiguration;\n-import org.apache.hadoop.hdfs.server.namenode.NameNode;\n-import org.apache.hadoop.net.NetUtils;\n-\n-/**\n- * One of the NN NameNodes acting as the target of an administrative command\n- * (e.g. failover).\n- */\n-@InterfaceAudience.Private\n-public class NNHAServiceTarget extends HAServiceTarget {\n-\n-  private final InetSocketAddress addr;\n-  private NodeFencer fencer;\n-  private BadFencingConfigurationException fenceConfigError;\n-\n-  public NNHAServiceTarget(HdfsConfiguration conf,\n-      String nsId, String nnId) {\n-    String serviceAddr = \n-      DFSUtil.getNamenodeServiceAddr(conf, nsId, nnId);\n-    if (serviceAddr == null) {\n-      throw new IllegalArgumentException(\n-          \"Unable to determine service address for namenode '\" + nnId + \"'\");\n-    }\n-    this.addr = NetUtils.createSocketAddr(serviceAddr,\n-        NameNode.DEFAULT_PORT);\n-    try {\n-      this.fencer = NodeFencer.create(conf);\n-    } catch (BadFencingConfigurationException e) {\n-      this.fenceConfigError = e;\n-    }\n-  }\n-\n-  /**\n-   * @return the NN's IPC address.\n-   */\n-  @Override\n-  public InetSocketAddress getAddress() {\n-    return addr;\n-  }\n-\n-  @Override\n-  public void checkFencingConfigured() throws BadFencingConfigurationException {\n-    if (fenceConfigError != null) {\n-      throw fenceConfigError;\n-    }\n-  }\n-  \n-  @Override\n-  public NodeFencer getFencer() {\n-    return fencer;\n-  }\n-  \n-  @Override\n-  public String toString() {\n-    return \"NameNode at \" + addr;\n-  }\n-\n-}",
                "raw_url": "https://github.com/apache/hadoop/raw/ea868d3d8b6c5e018eb104a560890c60d30fa269/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/NNHAServiceTarget.java",
                "sha": "9e8c239e7e8782fffff27109a1c2c2f0f8700028",
                "status": "removed"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java?ref=39775dca68643b37a3b9a5ae7bc8eea1418f60d1",
                "deletions": 12,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java",
                "patch": "@@ -32,7 +32,6 @@\n import org.apache.hadoop.ha.HAServiceProtocol;\n import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n import org.apache.hadoop.ha.HAServiceStatus;\n-import org.apache.hadoop.ha.HAServiceTarget;\n import org.apache.hadoop.ha.HealthCheckFailedException;\n import org.apache.hadoop.ha.NodeFencer;\n \n@@ -80,18 +79,10 @@ private HdfsConfiguration getHAConf() {\n   public void setup() throws IOException {\n     mockProtocol = Mockito.mock(HAServiceProtocol.class);\n     tool = new DFSHAAdmin() {\n-\n       @Override\n-      protected HAServiceTarget resolveTarget(String nnId) {\n-        HAServiceTarget target = super.resolveTarget(nnId);\n-        HAServiceTarget spy = Mockito.spy(target);\n-        // OVerride the target to return our mock protocol\n-        try {\n-          Mockito.doReturn(mockProtocol).when(spy).getProxy();\n-        } catch (IOException e) {\n-          throw new AssertionError(e); // mock setup doesn't really throw\n-        }\n-        return spy;\n+      protected HAServiceProtocol getProtocol(String serviceId) throws IOException {\n+        getServiceAddr(serviceId);\n+        return mockProtocol;\n       }\n     };\n     tool.setConf(getHAConf());",
                "raw_url": "https://github.com/apache/hadoop/raw/39775dca68643b37a3b9a5ae7bc8eea1418f60d1/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDFSHAAdmin.java",
                "sha": "c5ba0eb7e589257272defe6615228876daebf439",
                "status": "modified"
            }
        ],
        "message": "Revert HADOOP-8193 from r1304967. Patch introduced some NPEs in a test case.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1305152 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/ea868d3d8b6c5e018eb104a560890c60d30fa269",
        "repo": "hadoop",
        "unit_tests": [
            "TestFailoverController.java",
            "TestHAAdmin.java",
            "TestNodeFencer.java",
            "TestShellCommandFencer.java",
            "TestSshFenceByTcpPort.java",
            "TestDFSHAAdmin.java"
        ]
    },
    "hadoop_3ab2959": {
        "bug_id": "hadoop_3ab2959",
        "commit": "https://github.com/apache/hadoop/commit/3ab295994a4e7870a1f68d742d26c3ac44546fa5",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/3ab295994a4e7870a1f68d742d26c3ac44546fa5/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=3ab295994a4e7870a1f68d742d26c3ac44546fa5",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -19,3 +19,5 @@ HDFS-2231. Configuration changes for HA namenode. (suresh)\n HDFS-2418. Change ConfiguredFailoverProxyProvider to take advantage of HDFS-2231. (atm)\n \n HDFS-2393. Mark appropriate methods of ClientProtocol with the idempotent annotation. (atm)\n+\n+HDFS-2523. Small NN fixes to include HAServiceProtocol and prevent NPE on shutdown. (todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/3ab295994a4e7870a1f68d742d26c3ac44546fa5/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "37e6e4acac308b051fc5814e9a4d03cc957ddcf4",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/3ab295994a4e7870a1f68d742d26c3ac44546fa5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=3ab295994a4e7870a1f68d742d26c3ac44546fa5",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -572,7 +572,9 @@ public void stop() {\n       stopRequested = true;\n     }\n     try {\n-      state.exitState(haContext);\n+      if (state != null) {\n+        state.exitState(haContext);\n+      }\n     } catch (ServiceFailedException e) {\n       LOG.warn(\"Encountered exception while exiting state \", e);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/3ab295994a4e7870a1f68d742d26c3ac44546fa5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "4eb080105f050c928e37e4d27e2071eafa6753b7",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/3ab295994a4e7870a1f68d742d26c3ac44546fa5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java?ref=3ab295994a4e7870a1f68d742d26c3ac44546fa5",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "patch": "@@ -39,6 +39,7 @@\n import org.apache.hadoop.fs.permission.PermissionStatus;\n import static org.apache.hadoop.hdfs.DFSConfigKeys.*;\n \n+import org.apache.hadoop.ha.HAServiceProtocol;\n import org.apache.hadoop.ha.HealthCheckFailedException;\n import org.apache.hadoop.ha.ServiceFailedException;\n import org.apache.hadoop.hdfs.HDFSPolicyProvider;\n@@ -156,6 +157,7 @@ public NameNodeRpcServer(Configuration conf, NameNode nn)\n     this.server.addProtocol(RefreshAuthorizationPolicyProtocol.class, this);\n     this.server.addProtocol(RefreshUserMappingsProtocol.class, this);\n     this.server.addProtocol(GetUserMappingsProtocol.class, this);\n+    this.server.addProtocol(HAServiceProtocol.class, this);\n     \n \n     // set service-level authorization security policy\n@@ -225,6 +227,8 @@ public long getProtocolVersion(String protocol,\n       return RefreshUserMappingsProtocol.versionID;\n     } else if (protocol.equals(GetUserMappingsProtocol.class.getName())){\n       return GetUserMappingsProtocol.versionID;\n+    } else if (protocol.equals(HAServiceProtocol.class.getName())) {\n+      return HAServiceProtocol.versionID;\n     } else {\n       throw new IOException(\"Unknown protocol to name node: \" + protocol);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/3ab295994a4e7870a1f68d742d26c3ac44546fa5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "sha": "6546b8fe06b07bfe73b85c60f3e143e84e847717",
                "status": "modified"
            }
        ],
        "message": "HDFS-2523. Small NN fixes to include HAServiceProtocol and prevent NPE on shutdown. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1195753 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/b4992f671d36e35fd874958ffbc9e66abc29a725",
        "repo": "hadoop",
        "unit_tests": [
            "TestNameNodeRpcServer.java"
        ]
    },
    "hadoop_3b00eae": {
        "bug_id": "hadoop_3b00eae",
        "commit": "https://github.com/apache/hadoop/commit/3b00eaea256d252be3361a7d9106b88756fcb9ba",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/3b00eaea256d252be3361a7d9106b88756fcb9ba/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=3b00eaea256d252be3361a7d9106b88756fcb9ba",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -1211,6 +1211,9 @@ Release 2.8.0 - UNRELEASED\n     HDFS-8948. Use GenericTestUtils to set log levels in TestPread and\n     TestReplaceDatanodeOnFailure. (Mingliang Liu via wheat9)\n \n+    HDFS-8932. NPE thrown in NameNode when try to get TotalSyncCount metric\n+    before editLogStream initialization. (Surendra Singh Lilhore via xyao)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/3b00eaea256d252be3361a7d9106b88756fcb9ba/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "7aadcc60337a497c271f62f09e74f7c23066b0f5",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/3b00eaea256d252be3361a7d9106b88756fcb9ba/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java?ref=3b00eaea256d252be3361a7d9106b88756fcb9ba",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
                "patch": "@@ -1692,10 +1692,14 @@ private JournalManager createJournal(URI uri) {\n   }\n \n   /**\n-   +   * Return total number of syncs happened on this edit log.\n-   +   * @return long - count\n-   +   */\n+   * Return total number of syncs happened on this edit log.\n+   * @return long - count\n+   */\n   public long getTotalSyncCount() {\n-    return editLogStream.getNumSync();\n+    if (editLogStream != null) {\n+      return editLogStream.getNumSync();\n+    } else {\n+      return 0;\n+    }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/3b00eaea256d252be3361a7d9106b88756fcb9ba/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSEditLog.java",
                "sha": "faaea633448cf6f5b16df7025c7e014629cb9dc5",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/3b00eaea256d252be3361a7d9106b88756fcb9ba/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=3b00eaea256d252be3361a7d9106b88756fcb9ba",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -7295,7 +7295,12 @@ public long getTotalSyncCount() {\n   @Metric({\"TotalSyncTimes\",\n               \"Total time spend in sync operation on various edit logs\"})\n   public String getTotalSyncTimes() {\n-    return fsImage.editLog.getJournalSet().getSyncTimes();\n+    JournalSet journalSet = fsImage.editLog.getJournalSet();\n+    if (journalSet != null) {\n+      return journalSet.getSyncTimes();\n+    } else {\n+      return \"\";\n+    }\n   }\n }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/3b00eaea256d252be3361a7d9106b88756fcb9ba/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "3c3ef0b9797cc703fd7f27b11838af6781133c8d",
                "status": "modified"
            }
        ],
        "message": "HDFS-8932. NPE thrown in NameNode when try to get TotalSyncCount metric before editLogStream initialization. Contributed by Surendra Singh Lilhore",
        "parent": "https://github.com/apache/hadoop/commit/66d0c81d8f4e200a5051c8df87be890c9ad8772e",
        "repo": "hadoop",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_3f622a1": {
        "bug_id": "hadoop_3f622a1",
        "commit": "https://github.com/apache/hadoop/commit/3f622a143c5fb15fee7e5dded99e4a4136f19810",
        "file": [
            {
                "additions": 47,
                "blob_url": "https://github.com/apache/hadoop/blob/3f622a143c5fb15fee7e5dded99e4a4136f19810/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskID.java",
                "changes": 57,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskID.java?ref=3f622a143c5fb15fee7e5dded99e4a4136f19810",
                "deletions": 10,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskID.java",
                "patch": "@@ -63,6 +63,7 @@\n   public static final String TASK_ID_REGEX = TASK + \"_(\\\\d+)_(\\\\d+)_\" +\n       CharTaskTypeMaps.allTaskTypes + \"_(\\\\d+)\";\n   public static final Pattern taskIdPattern = Pattern.compile(TASK_ID_REGEX);\n+\n   static {\n     idFormat.setGroupingUsed(false);\n     idFormat.setMinimumIntegerDigits(6);\n@@ -72,7 +73,8 @@\n   private TaskType type;\n   \n   /**\n-   * Constructs a TaskID object from given {@link JobID}.  \n+   * Constructs a TaskID object from given {@link JobID}.\n+   *\n    * @param jobId JobID that this tip belongs to \n    * @param type the {@link TaskType} of the task \n    * @param id the tip number\n@@ -88,6 +90,7 @@ public TaskID(JobID jobId, TaskType type, int id) {\n   \n   /**\n    * Constructs a TaskInProgressId object from given parts.\n+   *\n    * @param jtIdentifier jobTracker identifier\n    * @param jobId job number \n    * @param type the TaskType \n@@ -99,6 +102,7 @@ public TaskID(String jtIdentifier, int jobId, TaskType type, int id) {\n \n   /**\n    * Constructs a TaskID object from given {@link JobID}.\n+   *\n    * @param jobId JobID that this tip belongs to\n    * @param isMap whether the tip is a map\n    * @param id the tip number\n@@ -110,6 +114,7 @@ public TaskID(JobID jobId, boolean isMap, int id) {\n \n   /**\n    * Constructs a TaskInProgressId object from given parts.\n+   *\n    * @param jtIdentifier jobTracker identifier\n    * @param jobId job number\n    * @param isMap whether the tip is a map\n@@ -120,23 +125,37 @@ public TaskID(String jtIdentifier, int jobId, boolean isMap, int id) {\n     this(new JobID(jtIdentifier, jobId), isMap, id);\n   }\n   \n+  /**\n+   * Default constructor for Writable. Sets the task type to\n+   * {@link TaskType#REDUCE}, the ID to 0, and the job ID to an empty job ID.\n+   */\n   public TaskID() { \n-    jobId = new JobID();\n+    this(new JobID(), TaskType.REDUCE, 0);\n   }\n   \n-  /** Returns the {@link JobID} object that this tip belongs to */\n+  /**\n+   * Returns the {@link JobID} object that this tip belongs to.\n+   *\n+   * @return the JobID object\n+   */\n   public JobID getJobID() {\n     return jobId;\n   }\n   \n-  /**Returns whether this TaskID is a map ID */\n+  /**\n+   * Returns whether this TaskID is a map ID.\n+   *\n+   * @return whether this TaskID is a map ID\n+   */\n   @Deprecated\n   public boolean isMap() {\n     return type == TaskType.MAP;\n   }\n     \n   /**\n-   * Get the type of the task\n+   * Get the type of the task.\n+   *\n+   * @return the type of the task\n    */\n   public TaskType getTaskType() {\n     return type;\n@@ -151,8 +170,14 @@ public boolean equals(Object o) {\n     return this.type == that.type && this.jobId.equals(that.jobId);\n   }\n \n-  /**Compare TaskInProgressIds by first jobIds, then by tip numbers. Reduces are \n-   * defined as greater then maps.*/\n+  /**\n+   * Compare TaskInProgressIds by first jobIds, then by tip numbers.\n+   * Reducers are defined as greater than mappers.\n+   *\n+   * @param o the TaskID against which to compare\n+   * @return 0 if equal, positive if this TaskID is greater, and negative if\n+   * this TaskID is less\n+   */\n   @Override\n   public int compareTo(ID o) {\n     TaskID that = (TaskID)o;\n@@ -174,6 +199,7 @@ public String toString() {\n \n   /**\n    * Add the unique string to the given builder.\n+   *\n    * @param builder the builder to append to\n    * @return the builder that was passed in\n    */\n@@ -204,7 +230,10 @@ public void write(DataOutput out) throws IOException {\n     WritableUtils.writeEnum(out, type);\n   }\n   \n-  /** Construct a TaskID object from given string \n+  /**\n+   * Construct a TaskID object from given string.\n+   *\n+   * @param str the target string\n    * @return constructed TaskID object or null if the given String is null\n    * @throws IllegalArgumentException if the given string is malformed\n    */\n@@ -224,22 +253,30 @@ public static TaskID forName(String str)\n     throw new IllegalArgumentException(exceptionMsg);\n   }\n   /**\n-   * Gets the character representing the {@link TaskType}\n+   * Gets the character representing the {@link TaskType}.\n+   *\n    * @param type the TaskType\n    * @return the character\n    */\n   public static char getRepresentingCharacter(TaskType type) {\n     return CharTaskTypeMaps.getRepresentingCharacter(type);\n   }\n   /**\n-   * Gets the {@link TaskType} corresponding to the character\n+   * Gets the {@link TaskType} corresponding to the character.\n+   *\n    * @param c the character\n    * @return the TaskType\n    */\n   public static TaskType getTaskType(char c) {\n     return CharTaskTypeMaps.getTaskType(c);\n   }\n   \n+  /**\n+   * Returns a string of characters describing all possible {@link TaskType}\n+   * values\n+   *\n+   * @return a string of all task type characters\n+   */\n   public static String getAllTaskTypes() {\n     return CharTaskTypeMaps.allTaskTypes;\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/3f622a143c5fb15fee7e5dded99e4a4136f19810/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/TaskID.java",
                "sha": "3ddfbe97c8437e73e0336450e61355d4d29a0424",
                "status": "modified"
            },
            {
                "additions": 461,
                "blob_url": "https://github.com/apache/hadoop/blob/3f622a143c5fb15fee7e5dded99e4a4136f19810/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestTaskID.java",
                "changes": 461,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestTaskID.java?ref=3f622a143c5fb15fee7e5dded99e4a4136f19810",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestTaskID.java",
                "patch": "@@ -0,0 +1,461 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.mapreduce;\n+\n+import org.apache.hadoop.io.DataInputByteBuffer;\n+import org.apache.hadoop.io.DataOutputByteBuffer;\n+import org.apache.hadoop.io.WritableUtils;\n+import org.junit.Test;\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Test the {@link TaskID} class.\n+ */\n+public class TestTaskID {\n+  /**\n+   * Test of getJobID method, of class TaskID.\n+   */\n+  @Test\n+  public void testGetJobID() {\n+    JobID jobId = new JobID(\"1234\", 0);\n+    TaskID taskId = new TaskID(jobId, TaskType.MAP, 0);\n+\n+    assertSame(\"TaskID did not store the JobID correctly\",\n+        jobId, taskId.getJobID());\n+\n+    taskId = new TaskID();\n+\n+    assertEquals(\"Job ID was set unexpectedly in default contsructor\",\n+        \"\", taskId.getJobID().getJtIdentifier());\n+  }\n+\n+  /**\n+   * Test of isMap method, of class TaskID.\n+   */\n+  @Test\n+  public void testIsMap() {\n+    JobID jobId = new JobID(\"1234\", 0);\n+\n+    for (TaskType type : TaskType.values()) {\n+      TaskID taskId = new TaskID(jobId, type, 0);\n+\n+      if (type == TaskType.MAP) {\n+        assertTrue(\"TaskID for map task did not correctly identify itself \"\n+            + \"as a map task\", taskId.isMap());\n+      } else {\n+        assertFalse(\"TaskID for \" + type + \" task incorrectly identified \"\n+            + \"itself as a map task\", taskId.isMap());\n+      }\n+    }\n+\n+    TaskID taskId = new TaskID();\n+\n+    assertFalse(\"TaskID of default type incorrectly identified itself as a \"\n+        + \"map task\", taskId.isMap());\n+  }\n+\n+  /**\n+   * Test of getTaskType method, of class TaskID.\n+   */\n+  @Test\n+  public void testGetTaskType_0args() {\n+    JobID jobId = new JobID(\"1234\", 0);\n+\n+    for (TaskType type : TaskType.values()) {\n+      TaskID taskId = new TaskID(jobId, type, 0);\n+\n+      assertEquals(\"TaskID incorrectly reported its type\",\n+          type, taskId.getTaskType());\n+    }\n+\n+    TaskID taskId = new TaskID();\n+\n+    assertEquals(\"TaskID of default type incorrectly reported its type\",\n+        TaskType.REDUCE, taskId.getTaskType());\n+  }\n+\n+  /**\n+   * Test of equals method, of class TaskID.\n+   */\n+  @Test\n+  public void testEquals() {\n+    JobID jobId1 = new JobID(\"1234\", 1);\n+    JobID jobId2 = new JobID(\"2345\", 2);\n+    TaskID taskId1 = new TaskID(jobId1, TaskType.MAP, 0);\n+    TaskID taskId2 = new TaskID(jobId1, TaskType.MAP, 0);\n+\n+    assertTrue(\"The equals() method reported two equal task IDs were not equal\",\n+        taskId1.equals(taskId2));\n+\n+    taskId2 = new TaskID(jobId2, TaskType.MAP, 0);\n+\n+    assertFalse(\"The equals() method reported two task IDs with different \"\n+        + \"job IDs were equal\", taskId1.equals(taskId2));\n+\n+    taskId2 = new TaskID(jobId1, TaskType.MAP, 1);\n+\n+    assertFalse(\"The equals() method reported two task IDs with different IDs \"\n+        + \"were equal\", taskId1.equals(taskId2));\n+\n+    TaskType[] types = TaskType.values();\n+\n+    for (int i = 0; i < types.length; i++) {\n+      for (int j = 0; j < types.length; j++) {\n+        taskId1 = new TaskID(jobId1, types[i], 0);\n+        taskId2 = new TaskID(jobId1, types[j], 0);\n+\n+        if (i == j) {\n+          assertTrue(\"The equals() method reported two equal task IDs were not \"\n+              + \"equal\", taskId1.equals(taskId2));\n+        } else {\n+          assertFalse(\"The equals() method reported two task IDs with \"\n+              + \"different types were equal\", taskId1.equals(taskId2));\n+        }\n+      }\n+    }\n+\n+    assertFalse(\"The equals() method matched against a JobID object\",\n+        taskId1.equals(jobId1));\n+\n+    assertFalse(\"The equals() method matched against a null object\",\n+        taskId1.equals(null));\n+  }\n+\n+  /**\n+   * Test of compareTo method, of class TaskID.\n+   */\n+  @Test\n+  public void testCompareTo() {\n+    JobID jobId = new JobID(\"1234\", 1);\n+    TaskID taskId1 = new TaskID(jobId, TaskType.REDUCE, 0);\n+    TaskID taskId2 = new TaskID(jobId, TaskType.REDUCE, 0);\n+\n+    assertEquals(\"The compareTo() method returned non-zero for two equal \"\n+        + \"task IDs\", 0, taskId1.compareTo(taskId2));\n+\n+    taskId2 = new TaskID(jobId, TaskType.MAP, 1);\n+\n+    assertTrue(\"The compareTo() method did not weigh task type more than task \"\n+        + \"ID\", taskId1.compareTo(taskId2) > 0);\n+\n+    TaskType[] types = TaskType.values();\n+\n+    for (int i = 0; i < types.length; i++) {\n+      for (int j = 0; j < types.length; j++) {\n+        taskId1 = new TaskID(jobId, types[i], 0);\n+        taskId2 = new TaskID(jobId, types[j], 0);\n+\n+        if (i == j) {\n+          assertEquals(\"The compareTo() method returned non-zero for two equal \"\n+              + \"task IDs\", 0, taskId1.compareTo(taskId2));\n+        } else if (i < j) {\n+          assertTrue(\"The compareTo() method did not order \" + types[i]\n+              + \" before \" + types[j], taskId1.compareTo(taskId2) < 0);\n+        } else {\n+          assertTrue(\"The compareTo() method did not order \" + types[i]\n+              + \" after \" + types[j], taskId1.compareTo(taskId2) > 0);\n+        }\n+      }\n+    }\n+\n+    try {\n+      taskId1.compareTo(jobId);\n+      fail(\"The compareTo() method allowed comparison to a JobID object\");\n+    } catch (ClassCastException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      taskId1.compareTo(null);\n+      fail(\"The compareTo() method allowed comparison to a null object\");\n+    } catch (NullPointerException ex) {\n+      // Expected\n+    }\n+  }\n+\n+  /**\n+   * Test of toString method, of class TaskID.\n+   */\n+  @Test\n+  public void testToString() {\n+    JobID jobId = new JobID(\"1234\", 1);\n+\n+    for (TaskType type : TaskType.values()) {\n+      TaskID taskId = new TaskID(jobId, type, 0);\n+      String str = String.format(\"task_1234_0001_%c_000000\",\n+          TaskID.getRepresentingCharacter(type));\n+\n+      assertEquals(\"The toString() method returned the wrong value\",\n+          str, taskId.toString());\n+    }\n+  }\n+\n+  /**\n+   * Test of appendTo method, of class TaskID.\n+   */\n+  @Test\n+  public void testAppendTo() {\n+    JobID jobId = new JobID(\"1234\", 1);\n+    StringBuilder builder = new StringBuilder();\n+\n+    for (TaskType type : TaskType.values()) {\n+      builder.setLength(0);\n+      TaskID taskId = new TaskID(jobId, type, 0);\n+      String str = String.format(\"_1234_0001_%c_000000\",\n+          TaskID.getRepresentingCharacter(type));\n+\n+      assertEquals(\"The appendTo() method appended the wrong value\",\n+          str, taskId.appendTo(builder).toString());\n+    }\n+\n+    try {\n+      new TaskID().appendTo(null);\n+      fail(\"The appendTo() method allowed a null builder\");\n+    } catch (NullPointerException ex) {\n+      // Expected\n+    }\n+  }\n+\n+  /**\n+   * Test of hashCode method, of class TaskID.\n+   */\n+  @Test\n+  public void testHashCode() {\n+    TaskType[] types = TaskType.values();\n+\n+    for (int i = 0; i < types.length; i++) {\n+      JobID jobId = new JobID(\"1234\" + i, i);\n+      TaskID taskId1 = new TaskID(jobId, types[i], i);\n+      TaskID taskId2 = new TaskID(jobId, types[i], i);\n+\n+      assertTrue(\"The hashcode() method gave unequal hash codes for two equal \"\n+          + \"task IDs\", taskId1.hashCode() == taskId2.hashCode());\n+    }\n+  }\n+\n+  /**\n+   * Test of readFields method, of class TaskID.\n+   */\n+  @Test\n+  public void testReadFields() throws Exception {\n+    DataOutputByteBuffer out = new DataOutputByteBuffer();\n+\n+    out.writeInt(0);\n+    out.writeInt(1);\n+    WritableUtils.writeVInt(out, 4);\n+    out.write(new byte[] { 0x31, 0x32, 0x33, 0x34});\n+    WritableUtils.writeEnum(out, TaskType.REDUCE);\n+\n+    DataInputByteBuffer in = new DataInputByteBuffer();\n+\n+    in.reset(out.getData());\n+\n+    TaskID instance = new TaskID();\n+\n+    instance.readFields(in);\n+\n+    assertEquals(\"The readFields() method did not produce the expected task ID\",\n+        \"task_1234_0001_r_000000\", instance.toString());\n+  }\n+\n+  /**\n+   * Test of write method, of class TaskID.\n+   */\n+  @Test\n+  public void testWrite() throws Exception {\n+    JobID jobId = new JobID(\"1234\", 1);\n+    TaskID taskId = new TaskID(jobId, TaskType.JOB_SETUP, 0);\n+    DataOutputByteBuffer out = new DataOutputByteBuffer();\n+\n+    taskId.write(out);\n+\n+    DataInputByteBuffer in = new DataInputByteBuffer();\n+    byte[] buffer = new byte[4];\n+\n+    in.reset(out.getData());\n+\n+    assertEquals(\"The write() method did not write the expected task ID\",\n+        0, in.readInt());\n+    assertEquals(\"The write() method did not write the expected job ID\",\n+        1, in.readInt());\n+    assertEquals(\"The write() method did not write the expected job \"\n+        + \"identifier length\", 4, WritableUtils.readVInt(in));\n+    in.readFully(buffer, 0, 4);\n+    assertEquals(\"The write() method did not write the expected job \"\n+        + \"identifier length\", \"1234\", new String(buffer));\n+    assertEquals(\"The write() method did not write the expected task type\",\n+        TaskType.JOB_SETUP, WritableUtils.readEnum(in, TaskType.class));\n+  }\n+\n+  /**\n+   * Test of forName method, of class TaskID.\n+   */\n+  @Test\n+  public void testForName() {\n+    assertEquals(\"The forName() method did not parse the task ID string \"\n+        + \"correctly\", \"task_1_0001_m_000000\",\n+        TaskID.forName(\"task_1_0001_m_000\").toString());\n+    assertEquals(\"The forName() method did not parse the task ID string \"\n+        + \"correctly\", \"task_23_0002_r_000001\",\n+        TaskID.forName(\"task_23_0002_r_0001\").toString());\n+    assertEquals(\"The forName() method did not parse the task ID string \"\n+        + \"correctly\", \"task_345_0003_s_000002\",\n+        TaskID.forName(\"task_345_0003_s_00002\").toString());\n+    assertEquals(\"The forName() method did not parse the task ID string \"\n+        + \"correctly\", \"task_6789_0004_c_000003\",\n+        TaskID.forName(\"task_6789_0004_c_000003\").toString());\n+    assertEquals(\"The forName() method did not parse the task ID string \"\n+        + \"correctly\", \"task_12345_0005_t_4000000\",\n+        TaskID.forName(\"task_12345_0005_t_4000000\").toString());\n+\n+    try {\n+      TaskID.forName(\"tisk_12345_0005_t_4000000\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"tisk_12345_0005_t_4000000\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      TaskID.forName(\"tisk_12345_0005_t_4000000\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"tisk_12345_0005_t_4000000\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      TaskID.forName(\"task_abc_0005_t_4000000\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"task_abc_0005_t_4000000\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      TaskID.forName(\"task_12345_xyz_t_4000000\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"task_12345_xyz_t_4000000\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      TaskID.forName(\"task_12345_0005_x_4000000\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"task_12345_0005_x_4000000\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      TaskID.forName(\"task_12345_0005_t_jkl\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"task_12345_0005_t_jkl\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      TaskID.forName(\"task_12345_0005_t\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"task_12345_0005_t\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      TaskID.forName(\"task_12345_0005_4000000\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"task_12345_0005_4000000\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      TaskID.forName(\"task_12345_t_4000000\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"task_12345_t_4000000\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+\n+    try {\n+      TaskID.forName(\"12345_0005_t_4000000\");\n+      fail(\"The forName() method parsed an invalid job ID: \"\n+          + \"12345_0005_t_4000000\");\n+    } catch (IllegalArgumentException ex) {\n+      // Expected\n+    }\n+  }\n+\n+  /**\n+   * Test of getRepresentingCharacter method, of class TaskID.\n+   */\n+  @Test\n+  public void testGetRepresentingCharacter() {\n+    assertEquals(\"The getRepresentingCharacter() method did not return the \"\n+        + \"expected character\", 'm',\n+        TaskID.getRepresentingCharacter(TaskType.MAP));\n+    assertEquals(\"The getRepresentingCharacter() method did not return the \"\n+        + \"expected character\", 'r',\n+        TaskID.getRepresentingCharacter(TaskType.REDUCE));\n+    assertEquals(\"The getRepresentingCharacter() method did not return the \"\n+        + \"expected character\", 's',\n+        TaskID.getRepresentingCharacter(TaskType.JOB_SETUP));\n+    assertEquals(\"The getRepresentingCharacter() method did not return the \"\n+        + \"expected character\", 'c',\n+        TaskID.getRepresentingCharacter(TaskType.JOB_CLEANUP));\n+    assertEquals(\"The getRepresentingCharacter() method did not return the \"\n+        + \"expected character\", 't',\n+        TaskID.getRepresentingCharacter(TaskType.TASK_CLEANUP));\n+  }\n+\n+  /**\n+   * Test of getTaskType method, of class TaskID.\n+   */\n+  @Test\n+  public void testGetTaskType_char() {\n+    assertEquals(\"The getTaskType() method did not return the expected type\",\n+        TaskType.MAP,\n+        TaskID.getTaskType('m'));\n+    assertEquals(\"The getTaskType() method did not return the expected type\",\n+        TaskType.REDUCE,\n+        TaskID.getTaskType('r'));\n+    assertEquals(\"The getTaskType() method did not return the expected type\",\n+        TaskType.JOB_SETUP,\n+        TaskID.getTaskType('s'));\n+    assertEquals(\"The getTaskType() method did not return the expected type\",\n+        TaskType.JOB_CLEANUP,\n+        TaskID.getTaskType('c'));\n+    assertEquals(\"The getTaskType() method did not return the expected type\",\n+        TaskType.TASK_CLEANUP,\n+        TaskID.getTaskType('t'));\n+    assertNull(\"The getTaskType() method did not return null for an unknown \"\n+        + \"type\", TaskID.getTaskType('x'));\n+  }\n+\n+  /**\n+   * Test of getAllTaskTypes method, of class TaskID.\n+   */\n+  @Test\n+  public void testGetAllTaskTypes() {\n+    assertEquals(\"The getAllTaskTypes method did not return the expected \"\n+        + \"string\", \"(m|r|s|c|t)\", TaskID.getAllTaskTypes());\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/3f622a143c5fb15fee7e5dded99e4a4136f19810/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestTaskID.java",
                "sha": "5531074dce84205e30fd05d7085df8fa63f998c9",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-6535. TaskID default constructor results in NPE on toString(). Contributed by Daniel Templeton",
        "parent": "https://github.com/apache/hadoop/commit/2c268cc9365851f5b02d967d13c8c0cbca850a86",
        "repo": "hadoop",
        "unit_tests": [
            "TestTaskID.java"
        ]
    },
    "hadoop_42a185b": {
        "bug_id": "hadoop_42a185b",
        "commit": "https://github.com/apache/hadoop/commit/42a185b57d3136a1ac108072132aced21d9f5d17",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/42a185b57d3136a1ac108072132aced21d9f5d17/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/CHANGES.txt?ref=42a185b57d3136a1ac108072132aced21d9f5d17",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -12,6 +12,9 @@ Trunk (unreleased changes)\n \n   NEW FEATURES\n \n+    HADOOP-7322. Adding a util method in FileUtil for directory listing,\n+    avoid NPEs on File.listFiles() (Bharath Mundlapudi via mattf)\n+\n     HADOOP-7023. Add listCorruptFileBlocks to Filesysem. (Patrick Kling\n     via hairong)\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/42a185b57d3136a1ac108072132aced21d9f5d17/CHANGES.txt",
                "sha": "b6d57b4104b866a5cfb9af40fea15f9e617741c6",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop/blob/42a185b57d3136a1ac108072132aced21d9f5d17/src/java/org/apache/hadoop/fs/FileUtil.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/org/apache/hadoop/fs/FileUtil.java?ref=42a185b57d3136a1ac108072132aced21d9f5d17",
                "deletions": 3,
                "filename": "src/java/org/apache/hadoop/fs/FileUtil.java",
                "patch": "@@ -324,7 +324,7 @@ public static boolean copy(File src,\n       if (!dstFS.mkdirs(dst)) {\n         return false;\n       }\n-      File contents[] = src.listFiles();\n+      File contents[] = listFiles(src);\n       for (int i = 0; i < contents.length; i++) {\n         copy(contents[i], dstFS, new Path(dst, contents[i].getName()),\n              deleteSource, conf);\n@@ -486,8 +486,10 @@ public static long getDU(File dir) {\n     } else {\n       size = dir.length();\n       File[] allFiles = dir.listFiles();\n-      for (int i = 0; i < allFiles.length; i++) {\n-        size = size + getDU(allFiles[i]);\n+      if(allFiles != null) {\n+         for (int i = 0; i < allFiles.length; i++) {\n+            size = size + getDU(allFiles[i]);\n+         }\n       }\n       return size;\n     }\n@@ -707,4 +709,23 @@ public static void replaceFile(File src, File target) throws IOException {\n       }\n     }\n   }\n+  \n+  /**\n+   * A wrapper for {@link File#listFiles()}. This java.io API returns null \n+   * when a dir is not a directory or for any I/O error. Instead of having\n+   * null check everywhere File#listFiles() is used, we will add utility API\n+   * to get around this problem. For the majority of cases where we prefer \n+   * an IOException to be thrown.\n+   * @param dir directory for which listing should be performed\n+   * @return list of files or empty list\n+   * @exception IOException for invalid directory or for a bad disk.\n+   */\n+  public static File[] listFiles(File dir) throws IOException {\n+    File[] files = dir.listFiles();\n+    if(files == null) {\n+      throw new IOException(\"Invalid directory or I/O error occurred for dir: \"\n+                + dir.toString());\n+    }\n+    return files;\n+  }  \n }",
                "raw_url": "https://github.com/apache/hadoop/raw/42a185b57d3136a1ac108072132aced21d9f5d17/src/java/org/apache/hadoop/fs/FileUtil.java",
                "sha": "537959bd7317e49342f72650c5507729f3b5ea0a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/42a185b57d3136a1ac108072132aced21d9f5d17/src/java/org/apache/hadoop/fs/RawLocalFileSystem.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/org/apache/hadoop/fs/RawLocalFileSystem.java?ref=42a185b57d3136a1ac108072132aced21d9f5d17",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/fs/RawLocalFileSystem.java",
                "patch": "@@ -276,7 +276,7 @@ public boolean delete(Path p, boolean recursive) throws IOException {\n     if (f.isFile()) {\n       return f.delete();\n     } else if ((!recursive) && f.isDirectory() && \n-        (f.listFiles().length != 0)) {\n+        (FileUtil.listFiles(f).length != 0)) {\n       throw new IOException(\"Directory \" + f.toString() + \" is not empty\");\n     }\n     return FileUtil.fullyDelete(f);",
                "raw_url": "https://github.com/apache/hadoop/raw/42a185b57d3136a1ac108072132aced21d9f5d17/src/java/org/apache/hadoop/fs/RawLocalFileSystem.java",
                "sha": "63579980cd8f3ca5d4d3b1b2b6132bf54426cc61",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop/blob/42a185b57d3136a1ac108072132aced21d9f5d17/src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/test/core/org/apache/hadoop/fs/TestFileUtil.java?ref=42a185b57d3136a1ac108072132aced21d9f5d17",
                "deletions": 0,
                "filename": "src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "patch": "@@ -117,6 +117,33 @@ private void createFile(File directory, String name, String contents)\n     }\n   }\n \n+  @Test\n+  public void testListFiles() throws IOException {\n+    setupDirs();\n+    //Test existing files case \n+    File[] files = FileUtil.listFiles(partitioned);\n+    Assert.assertEquals(2, files.length);\n+\n+    //Test existing directory with no files case \n+    File newDir = new File(tmp.getPath(),\"test\");\n+    newDir.mkdir();\n+    Assert.assertTrue(\"Failed to create test dir\", newDir.exists());\n+    files = FileUtil.listFiles(newDir);\n+    Assert.assertEquals(0, files.length);\n+    newDir.delete();\n+    Assert.assertFalse(\"Failed to delete test dir\", newDir.exists());\n+    \n+    //Test non-existing directory case, this throws \n+    //IOException\n+    try {\n+      files = FileUtil.listFiles(newDir);\n+      Assert.fail(\"IOException expected on listFiles() for non-existent dir \"\n+      \t\t+ newDir.toString());\n+    } catch(IOException ioe) {\n+    \t//Expected an IOException\n+    }\n+  }\n+  \n   @After\n   public void tearDown() throws IOException {\n     FileUtil.fullyDelete(del);",
                "raw_url": "https://github.com/apache/hadoop/raw/42a185b57d3136a1ac108072132aced21d9f5d17/src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "sha": "1c193db3e74118e3d496ff68eec991522f008999",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7322. Adding a util method in FileUtil for directory listing, avoid NPEs on File.listFiles(). Contributed by Bharath Mundlapudi.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1127697 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/e79d8afcd05722fe369eba919abe4f4205771a41",
        "repo": "hadoop",
        "unit_tests": [
            "TestFileUtil.java"
        ]
    },
    "hadoop_44809b8": {
        "bug_id": "hadoop_44809b8",
        "commit": "https://github.com/apache/hadoop/commit/44809b80814d5520a73d5609d0f73a13eb2360ac",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/44809b80814d5520a73d5609d0f73a13eb2360ac/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=44809b80814d5520a73d5609d0f73a13eb2360ac",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -469,6 +469,9 @@ Release 2.8.0 - UNRELEASED\n     HADOOP-10027. *Compressor_deflateBytesDirect passes instance instead of\n     jclass to GetStaticObjectField. (Hui Zheng via cnauroth)\n \n+    HADOOP-11724. DistCp throws NPE when the target directory is root.\n+    (Lei Eddy Xu via Yongjun Zhang) \n+\n Release 2.7.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/44809b80814d5520a73d5609d0f73a13eb2360ac/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "2e26b0a48e13b514dd5ee8f9ac6a5e977c9a2e6c",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/44809b80814d5520a73d5609d0f73a13eb2360ac/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java?ref=44809b80814d5520a73d5609d0f73a13eb2360ac",
                "deletions": 0,
                "filename": "hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java",
                "patch": "@@ -133,6 +133,9 @@ private void cleanupTempFiles(JobContext context) {\n   private void deleteAttemptTempFiles(Path targetWorkPath,\n                                       FileSystem targetFS,\n                                       String jobId) throws IOException {\n+    if (targetWorkPath == null) {\n+      return;\n+    }\n \n     FileStatus[] tempFiles = targetFS.globStatus(\n         new Path(targetWorkPath, \".distcp.tmp.\" + jobId.replaceAll(\"job\",\"attempt\") + \"*\"));",
                "raw_url": "https://github.com/apache/hadoop/raw/44809b80814d5520a73d5609d0f73a13eb2360ac/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyCommitter.java",
                "sha": "9ec57f426aa13944452c5fea73082af0b649caea",
                "status": "modified"
            }
        ],
        "message": "HADOOP-11724. DistCp throws NPE when the target directory is root. (Lei Eddy Xu via Yongjun Zhang)",
        "parent": "https://github.com/apache/hadoop/commit/fc1031af749435dc95efea6745b1b2300ce29446",
        "repo": "hadoop",
        "unit_tests": [
            "TestCopyCommitter.java"
        ]
    },
    "hadoop_4530f45": {
        "bug_id": "hadoop_4530f45",
        "commit": "https://github.com/apache/hadoop/commit/4530f4500d308c9cefbcc5990769c04bd061ad87",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/4530f4500d308c9cefbcc5990769c04bd061ad87/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java?ref=4530f4500d308c9cefbcc5990769c04bd061ad87",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "patch": "@@ -526,9 +526,11 @@ protected void serviceStop() throws Exception {\n       DefaultMetricsSystem.shutdown();\n \n       // Cleanup ResourcePluginManager\n-      ResourcePluginManager rpm = context.getResourcePluginManager();\n-      if (rpm != null) {\n-        rpm.cleanup();\n+      if (null != context) {\n+        ResourcePluginManager rpm = context.getResourcePluginManager();\n+        if (rpm != null) {\n+          rpm.cleanup();\n+        }\n       }\n     } finally {\n       // YARN-3641: NM's services stop get failed shouldn't block the",
                "raw_url": "https://github.com/apache/hadoop/raw/4530f4500d308c9cefbcc5990769c04bd061ad87/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "sha": "9eff3a9213e3156aaa56d9c58730ac6ce1c633eb",
                "status": "modified"
            }
        ],
        "message": "YARN-9507. Fix NPE in NodeManager#serviceStop on startup failure. Contributed by Bilwa S T.",
        "parent": "https://github.com/apache/hadoop/commit/21852494815e7314e0873c3963a54457ac2aab28",
        "repo": "hadoop",
        "unit_tests": [
            "TestNodeManager.java"
        ]
    },
    "hadoop_45a8e8c": {
        "bug_id": "hadoop_45a8e8c",
        "commit": "https://github.com/apache/hadoop/commit/45a8e8c5a46535287de97fd6609c0743eef888ee",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -74,3 +74,5 @@ Release 0.23.3 - Unreleased\n     YARN-63. RMNodeImpl is missing valid transitions from the UNHEALTHY state\n     (Jason Lowe via bobby)\n \n+    YARN-60. Fixed a bug in ResourceManager which causes all NMs to get NPEs and\n+    thus causes all containers to be rejected. (vinodkv)",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/CHANGES.txt",
                "sha": "8284215a98abec90778697f810fabb528d00e125",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodeHeartbeatRequest.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodeHeartbeatRequest.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodeHeartbeatRequest.java",
                "patch": "@@ -18,11 +18,14 @@\n \n package org.apache.hadoop.yarn.server.api.protocolrecords;\n \n+import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.api.records.NodeStatus;\n \n-\n public interface NodeHeartbeatRequest {\n-  public abstract NodeStatus getNodeStatus();\n-  \n-  public abstract void setNodeStatus(NodeStatus status);\n+\n+  NodeStatus getNodeStatus();\n+  void setNodeStatus(NodeStatus status);\n+\n+  MasterKey getLastKnownMasterKey();\n+  void setLastKnownMasterKey(MasterKey secretKey);\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/NodeHeartbeatRequest.java",
                "sha": "9e69680d87f6ed2198fd764ac2b9437dc80194fb",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java",
                "patch": "@@ -18,24 +18,25 @@\n \n package org.apache.hadoop.yarn.server.api.protocolrecords.impl.pb;\n \n-\n import org.apache.hadoop.yarn.api.records.ProtoBase;\n+import org.apache.hadoop.yarn.proto.YarnServerCommonProtos.MasterKeyProto;\n import org.apache.hadoop.yarn.proto.YarnServerCommonProtos.NodeStatusProto;\n import org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProto;\n import org.apache.hadoop.yarn.proto.YarnServerCommonServiceProtos.NodeHeartbeatRequestProtoOrBuilder;\n import org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatRequest;\n+import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.api.records.NodeStatus;\n+import org.apache.hadoop.yarn.server.api.records.impl.pb.MasterKeyPBImpl;\n import org.apache.hadoop.yarn.server.api.records.impl.pb.NodeStatusPBImpl;\n \n-\n-    \n-public class NodeHeartbeatRequestPBImpl extends ProtoBase<NodeHeartbeatRequestProto> implements NodeHeartbeatRequest {\n+public class NodeHeartbeatRequestPBImpl extends\n+    ProtoBase<NodeHeartbeatRequestProto> implements NodeHeartbeatRequest {\n   NodeHeartbeatRequestProto proto = NodeHeartbeatRequestProto.getDefaultInstance();\n   NodeHeartbeatRequestProto.Builder builder = null;\n   boolean viaProto = false;\n   \n   private NodeStatus nodeStatus = null;\n-  \n+  private MasterKey lastKnownMasterKey = null;\n   \n   public NodeHeartbeatRequestPBImpl() {\n     builder = NodeHeartbeatRequestProto.newBuilder();\n@@ -57,6 +58,10 @@ private void mergeLocalToBuilder() {\n     if (this.nodeStatus != null) {\n       builder.setNodeStatus(convertToProtoFormat(this.nodeStatus));\n     }\n+    if (this.lastKnownMasterKey != null) {\n+      builder\n+        .setLastKnownMasterKey(convertToProtoFormat(this.lastKnownMasterKey));\n+    }\n   }\n \n   private void mergeLocalToProto() {\n@@ -96,6 +101,27 @@ public void setNodeStatus(NodeStatus nodeStatus) {\n     this.nodeStatus = nodeStatus;\n   }\n \n+  @Override\n+  public MasterKey getLastKnownMasterKey() {\n+    NodeHeartbeatRequestProtoOrBuilder p = viaProto ? proto : builder;\n+    if (this.lastKnownMasterKey != null) {\n+      return this.lastKnownMasterKey;\n+    }\n+    if (!p.hasLastKnownMasterKey()) {\n+      return null;\n+    }\n+    this.lastKnownMasterKey = convertFromProtoFormat(p.getLastKnownMasterKey());\n+    return this.lastKnownMasterKey;\n+  }\n+\n+  @Override\n+  public void setLastKnownMasterKey(MasterKey masterKey) {\n+    maybeInitBuilder();\n+    if (masterKey == null) \n+      builder.clearLastKnownMasterKey();\n+    this.lastKnownMasterKey = masterKey;\n+  }\n+\n   private NodeStatusPBImpl convertFromProtoFormat(NodeStatusProto p) {\n     return new NodeStatusPBImpl(p);\n   }\n@@ -104,6 +130,11 @@ private NodeStatusProto convertToProtoFormat(NodeStatus t) {\n     return ((NodeStatusPBImpl)t).getProto();\n   }\n \n+  private MasterKeyPBImpl convertFromProtoFormat(MasterKeyProto p) {\n+    return new MasterKeyPBImpl(p);\n+  }\n \n-\n+  private MasterKeyProto convertToProtoFormat(MasterKey t) {\n+    return ((MasterKeyPBImpl)t).getProto();\n+  }\n }  ",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/api/protocolrecords/impl/pb/NodeHeartbeatRequestPBImpl.java",
                "sha": "8fcf7f2c147a901117435dfd09262655f2bc75fc",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/proto/yarn_server_common_service_protos.proto",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/proto/yarn_server_common_service_protos.proto?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/proto/yarn_server_common_service_protos.proto",
                "patch": "@@ -35,6 +35,7 @@ message RegisterNodeManagerResponseProto {\n \n message NodeHeartbeatRequestProto {\n   optional NodeStatusProto node_status = 1;\n+  optional MasterKeyProto last_known_master_key = 2;\n }\n \n message NodeHeartbeatResponseProto {",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/proto/yarn_server_common_service_protos.proto",
                "sha": "e4d82c75d61e56c087f51bd90a52da149de00398",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
                "patch": "@@ -31,6 +31,7 @@\n import org.apache.avro.AvroRuntimeException;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.classification.InterfaceAudience.Private;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.yarn.YarnException;\n@@ -111,10 +112,7 @@ public synchronized void init(Configuration conf) {\n     this.totalResource = recordFactory.newRecordInstance(Resource.class);\n     this.totalResource.setMemory(memoryMb);\n     metrics.addResource(totalResource);\n-    this.tokenKeepAliveEnabled =\n-        conf.getBoolean(YarnConfiguration.LOG_AGGREGATION_ENABLED,\n-            YarnConfiguration.DEFAULT_LOG_AGGREGATION_ENABLED)\n-            && isSecurityEnabled();\n+    this.tokenKeepAliveEnabled = isTokenKeepAliveEnabled(conf);\n     this.tokenRemovalDelayMs =\n         conf.getInt(YarnConfiguration.RM_NM_EXPIRY_INTERVAL_MS,\n             YarnConfiguration.DEFAULT_RM_NM_EXPIRY_INTERVAL_MS);\n@@ -163,10 +161,17 @@ synchronized boolean hasToRebootNode() {\n     return this.hasToRebootNode;\n   }\n \n-  protected boolean isSecurityEnabled() {\n+  private boolean isSecurityEnabled() {\n     return UserGroupInformation.isSecurityEnabled();\n   }\n \n+  @Private\n+  protected boolean isTokenKeepAliveEnabled(Configuration conf) {\n+    return conf.getBoolean(YarnConfiguration.LOG_AGGREGATION_ENABLED,\n+        YarnConfiguration.DEFAULT_LOG_AGGREGATION_ENABLED)\n+        && isSecurityEnabled();\n+  }\n+\n   protected ResourceTracker getRMClient() {\n     Configuration conf = getConfig();\n     YarnRPC rpc = YarnRPC.create(conf);\n@@ -321,7 +326,11 @@ public void run() {\n             \n             NodeHeartbeatRequest request = recordFactory\n                 .newRecordInstance(NodeHeartbeatRequest.class);\n-            request.setNodeStatus(nodeStatus);            \n+            request.setNodeStatus(nodeStatus);\n+            if (isSecurityEnabled()) {\n+              request.setLastKnownMasterKey(NodeStatusUpdaterImpl.this.context\n+                .getContainerTokenSecretManager().getCurrentKey());\n+            }\n             HeartbeatResponse response =\n               resourceTracker.nodeHeartbeat(request).getHeartbeatResponse();\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
                "sha": "819e22d2146ae18083ee7ee7609b7678351c8407",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java",
                "patch": "@@ -92,7 +92,6 @@ public synchronized void setMasterKey(MasterKey masterKeyRecord) {\n         containerId.getApplicationAttemptId().getApplicationId();\n \n     MasterKeyData masterKeyToUse = null;\n-\n     if (this.previousMasterKey != null\n         && keyId == this.previousMasterKey.getMasterKey().getKeyId()) {\n       // A container-launch has come in with a token generated off the last",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/security/NMContainerTokenSecretManager.java",
                "sha": "bc70f26a07e93e1bbc53e26d7f8fbd938348a99f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java",
                "patch": "@@ -261,7 +261,7 @@ protected ResourceTracker getRMClient() {\n     }\n     \n     @Override\n-    protected boolean isSecurityEnabled() {\n+    protected boolean isTokenKeepAliveEnabled(Configuration conf) {\n       return true;\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestNodeStatusUpdater.java",
                "sha": "41d171f97c034a2be50743f9d9887121ac3644af",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "patch": "@@ -159,7 +159,7 @@ public synchronized void init(Configuration conf) {\n     DelegationTokenRenewer tokenRenewer = createDelegationTokenRenewer();\n     addService(tokenRenewer);\n \n-    this.containerTokenSecretManager = new RMContainerTokenSecretManager(conf);\n+    this.containerTokenSecretManager = createContainerTokenSecretManager(conf);\n     \n     this.rmContext =\n         new RMContextImpl(this.store, this.rmDispatcher,\n@@ -231,6 +231,11 @@ public synchronized void init(Configuration conf) {\n     super.init(conf);\n   }\n \n+  protected RMContainerTokenSecretManager createContainerTokenSecretManager(\n+      Configuration conf) {\n+    return new RMContainerTokenSecretManager(conf);\n+  }\n+\n   protected EventHandler<SchedulerEvent> createSchedulerEventDispatcher() {\n     return new SchedulerEventDispatcher(this.scheduler);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "sha": "e9e5340b80cbd23cc817c6a9708bf2dc3e82f614",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 9,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "patch": "@@ -169,14 +169,14 @@ public RegisterNodeManagerResponse registerNodeManager(\n       return response;\n     }\n \n-    MasterKey nextMasterKeyForNode = null;\n     if (isSecurityEnabled()) {\n-      nextMasterKeyForNode = this.containerTokenSecretManager.getCurrentKey();\n+      MasterKey nextMasterKeyForNode =\n+          this.containerTokenSecretManager.getCurrentKey();\n       regResponse.setMasterKey(nextMasterKeyForNode);\n     }\n \n     RMNode rmNode = new RMNodeImpl(nodeId, rmContext, host, cmPort, httpPort,\n-        resolve(host), capability, nextMasterKeyForNode);\n+        resolve(host), capability);\n \n     RMNode oldNode = this.rmContext.getRMNodes().putIfAbsent(nodeId, rmNode);\n     if (oldNode == null) {\n@@ -266,17 +266,18 @@ public NodeHeartbeatResponse nodeHeartbeat(NodeHeartbeatRequest request)\n     latestResponse.addAllApplicationsToCleanup(rmNode.getAppsToCleanup());\n     latestResponse.setNodeAction(NodeAction.NORMAL);\n \n-    MasterKey nextMasterKeyForNode = null;\n-\n     // Check if node's masterKey needs to be updated and if the currentKey has\n     // roller over, send it across\n     if (isSecurityEnabled()) {\n+\n       boolean shouldSendMasterKey = false;\n-      MasterKey nodeKnownMasterKey = rmNode.getCurrentMasterKey();\n-      nextMasterKeyForNode = this.containerTokenSecretManager.getNextKey();\n+\n+      MasterKey nextMasterKeyForNode =\n+          this.containerTokenSecretManager.getNextKey();\n       if (nextMasterKeyForNode != null) {\n         // nextMasterKeyForNode can be null if there is no outstanding key that\n         // is in the activation period.\n+        MasterKey nodeKnownMasterKey = request.getLastKnownMasterKey();\n         if (nodeKnownMasterKey.getKeyId() != nextMasterKeyForNode.getKeyId()) {\n           shouldSendMasterKey = true;\n         }\n@@ -290,8 +291,7 @@ public NodeHeartbeatResponse nodeHeartbeat(NodeHeartbeatRequest request)\n     this.rmContext.getDispatcher().getEventHandler().handle(\n         new RMNodeStatusEvent(nodeId, remoteNodeStatus.getNodeHealthStatus(),\n             remoteNodeStatus.getContainersStatuses(), \n-            remoteNodeStatus.getKeepAliveApplications(), latestResponse,\n-            nextMasterKeyForNode));\n+            remoteNodeStatus.getKeepAliveApplications(), latestResponse));\n \n     nodeHeartBeatResponse.setHeartbeatResponse(latestResponse);\n     return nodeHeartBeatResponse;",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "sha": "ed4a021b0db631102cbc075c9d961020362aea4e",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNode.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNode.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNode.java",
                "patch": "@@ -28,7 +28,6 @@\n import org.apache.hadoop.yarn.api.records.NodeId;\n import org.apache.hadoop.yarn.api.records.NodeState;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n-import org.apache.hadoop.yarn.server.api.records.MasterKey;\n \n /**\n  * Node managers information on available resources \n@@ -107,6 +106,4 @@\n   public List<ApplicationId> getAppsToCleanup();\n \n   public HeartbeatResponse getLastHeartBeatResponse();\n-  \n-  public MasterKey getCurrentMasterKey();\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNode.java",
                "sha": "aafa3dbdefeaf585d509d3b0e3cc4dd2d56df23b",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 19,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "patch": "@@ -46,7 +46,6 @@\n import org.apache.hadoop.yarn.factories.RecordFactory;\n import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n-import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.NodesListManagerEventType;\n@@ -105,8 +104,6 @@\n   private HeartbeatResponse latestHeartBeatResponse = recordFactory\n       .newRecordInstance(HeartbeatResponse.class);\n   \n-  private MasterKey currentMasterKey;\n-\n   private static final StateMachineFactory<RMNodeImpl,\n                                            NodeState,\n                                            RMNodeEventType,\n@@ -167,8 +164,7 @@\n                              RMNodeEvent> stateMachine;\n \n   public RMNodeImpl(NodeId nodeId, RMContext context, String hostName,\n-      int cmPort, int httpPort, Node node, Resource capability,\n-      MasterKey masterKey) {\n+      int cmPort, int httpPort, Node node, Resource capability) {\n     this.nodeId = nodeId;\n     this.context = context;\n     this.hostName = hostName;\n@@ -178,7 +174,6 @@ public RMNodeImpl(NodeId nodeId, RMContext context, String hostName,\n     this.nodeAddress = hostName + \":\" + cmPort;\n     this.httpAddress = hostName + \":\" + httpPort;\n     this.node = node;\n-    this.currentMasterKey = masterKey;\n     this.nodeHealthStatus.setIsNodeHealthy(true);\n     this.nodeHealthStatus.setHealthReport(\"Healthy\");\n     this.nodeHealthStatus.setLastHealthReportTime(System.currentTimeMillis());\n@@ -312,17 +307,6 @@ public HeartbeatResponse getLastHeartBeatResponse() {\n       this.readLock.unlock();\n     }\n   }\n-  \n-  @Override\n-  public MasterKey getCurrentMasterKey() {\n-    this.readLock.lock();\n-    try {\n-      return this.currentMasterKey;\n-    } finally {\n-      this.readLock.unlock();\n-    }\n-  }\n-  \n \n   public void handle(RMNodeEvent event) {\n     LOG.debug(\"Processing \" + event.getNodeId() + \" of type \" + event.getType());\n@@ -500,7 +484,6 @@ public NodeState transition(RMNodeImpl rmNode, RMNodeEvent event) {\n \n       // Switch the last heartbeatresponse.\n       rmNode.latestHeartBeatResponse = statusEvent.getLatestResponse();\n-      rmNode.currentMasterKey = statusEvent.getCurrentMasterKey();\n \n       NodeHealthStatus remoteNodeHealthStatus = \n           statusEvent.getNodeHealthStatus();\n@@ -582,7 +565,6 @@ public NodeState transition(RMNodeImpl rmNode, RMNodeEvent event) {\n \n       // Switch the last heartbeatresponse.\n       rmNode.latestHeartBeatResponse = statusEvent.getLatestResponse();\n-      rmNode.currentMasterKey = statusEvent.getCurrentMasterKey();\n       NodeHealthStatus remoteNodeHealthStatus = statusEvent.getNodeHealthStatus();\n       rmNode.setNodeHealthStatus(remoteNodeHealthStatus);\n       if (remoteNodeHealthStatus.getIsNodeHealthy()) {",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeImpl.java",
                "sha": "83833b9bdb3c4ac1e2ea1e9fac616bbfccbc09b0",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeStatusEvent.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeStatusEvent.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 8,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeStatusEvent.java",
                "patch": "@@ -25,25 +25,22 @@\n import org.apache.hadoop.yarn.api.records.NodeHealthStatus;\n import org.apache.hadoop.yarn.api.records.NodeId;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n-import org.apache.hadoop.yarn.server.api.records.MasterKey;\n \n public class RMNodeStatusEvent extends RMNodeEvent {\n \n   private final NodeHealthStatus nodeHealthStatus;\n   private final List<ContainerStatus> containersCollection;\n   private final HeartbeatResponse latestResponse;\n   private final List<ApplicationId> keepAliveAppIds;\n-  private final MasterKey currentMasterKey;\n \n   public RMNodeStatusEvent(NodeId nodeId, NodeHealthStatus nodeHealthStatus,\n       List<ContainerStatus> collection, List<ApplicationId> keepAliveAppIds,\n-      HeartbeatResponse latestResponse, MasterKey currentMasterKey) {\n+      HeartbeatResponse latestResponse) {\n     super(nodeId, RMNodeEventType.STATUS_UPDATE);\n     this.nodeHealthStatus = nodeHealthStatus;\n     this.containersCollection = collection;\n     this.keepAliveAppIds = keepAliveAppIds;\n     this.latestResponse = latestResponse;\n-    this.currentMasterKey = currentMasterKey;\n   }\n \n   public NodeHealthStatus getNodeHealthStatus() {\n@@ -61,8 +58,4 @@ public HeartbeatResponse getLatestResponse() {\n   public List<ApplicationId> getKeepAliveAppIds() {\n     return this.keepAliveAppIds;\n   }\n-  \n-  public MasterKey getCurrentMasterKey() {\n-    return this.currentMasterKey;\n-  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmnode/RMNodeStatusEvent.java",
                "sha": "1285c2bed99d979c34b86cd59238b75bceb41aa8",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/RMContainerTokenSecretManager.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/RMContainerTokenSecretManager.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/RMContainerTokenSecretManager.java",
                "patch": "@@ -89,14 +89,17 @@ public void stop() {\n    * Creates a new master-key and sets it as the primary.\n    */\n   @Private\n-  protected void rollMasterKey() {\n+  public void rollMasterKey() {\n     super.writeLock.lock();\n     try {\n       LOG.info(\"Rolling master-key for container-tokens\");\n       if (this.currentMasterKey == null) { // Setting up for the first time.\n         this.currentMasterKey = createNewMasterKey();\n       } else {\n         this.nextMasterKey = createNewMasterKey();\n+        LOG.info(\"Going to activate master-key with key-id \"\n+            + this.nextMasterKey.getMasterKey().getKeyId() + \" in \"\n+            + this.activationDelay + \"ms\");\n         this.timer.schedule(new NextKeyActivator(), this.activationDelay);\n       }\n     } finally {\n@@ -122,7 +125,7 @@ public MasterKey getNextKey() {\n    * Activate the new master-key\n    */\n   @Private\n-  protected void activateNextMasterKey() {\n+  public void activateNextMasterKey() {\n     super.writeLock.lock();\n     try {\n       LOG.info(\"Activating next master key with id: \"",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/RMContainerTokenSecretManager.java",
                "sha": "cc4ccd7e1ebd863edaae6f6f488c0341e520e4df",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "patch": "@@ -35,7 +35,9 @@\n import org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n+import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.api.records.NodeStatus;\n+import org.apache.hadoop.yarn.server.api.records.RegistrationResponse;\n import org.apache.hadoop.yarn.util.BuilderUtils;\n import org.apache.hadoop.yarn.util.Records;\n \n@@ -46,8 +48,9 @@\n   private final int memory;\n   private final ResourceTrackerService resourceTracker;\n   private final int httpPort = 2;\n+  private MasterKey currentMasterKey;\n \n-  MockNM(String nodeIdStr, int memory, ResourceTrackerService resourceTracker) {\n+  public MockNM(String nodeIdStr, int memory, ResourceTrackerService resourceTracker) {\n     this.memory = memory;\n     this.resourceTracker = resourceTracker;\n     String[] splits = nodeIdStr.split(\":\");\n@@ -72,21 +75,23 @@ public void containerStatus(Container container) throws Exception {\n     nodeHeartbeat(conts, true);\n   }\n \n-  public NodeId registerNode() throws Exception {\n+  public RegistrationResponse registerNode() throws Exception {\n     RegisterNodeManagerRequest req = Records.newRecord(\n         RegisterNodeManagerRequest.class);\n     req.setNodeId(nodeId);\n     req.setHttpPort(httpPort);\n     Resource resource = Records.newRecord(Resource.class);\n     resource.setMemory(memory);\n     req.setResource(resource);\n-    resourceTracker.registerNodeManager(req);\n-    return nodeId;\n+    RegistrationResponse registrationResponse =\n+        resourceTracker.registerNodeManager(req).getRegistrationResponse();\n+    this.currentMasterKey = registrationResponse.getMasterKey();\n+    return registrationResponse;\n   }\n \n-  public HeartbeatResponse nodeHeartbeat(boolean b) throws Exception {\n+  public HeartbeatResponse nodeHeartbeat(boolean isHealthy) throws Exception {\n     return nodeHeartbeat(new HashMap<ApplicationId, List<ContainerStatus>>(),\n-        b, ++responseId);\n+        isHealthy, ++responseId);\n   }\n \n   public HeartbeatResponse nodeHeartbeat(ApplicationAttemptId attemptId,\n@@ -123,7 +128,15 @@ public HeartbeatResponse nodeHeartbeat(Map<ApplicationId,\n     healthStatus.setLastHealthReportTime(1);\n     status.setNodeHealthStatus(healthStatus);\n     req.setNodeStatus(status);\n-    return resourceTracker.nodeHeartbeat(req).getHeartbeatResponse();\n+    req.setLastKnownMasterKey(this.currentMasterKey);\n+    HeartbeatResponse heartbeatResponse =\n+        resourceTracker.nodeHeartbeat(req).getHeartbeatResponse();\n+    MasterKey masterKeyFromRM = heartbeatResponse.getMasterKey();\n+    this.currentMasterKey =\n+        (masterKeyFromRM != null\n+            && masterKeyFromRM.getKeyId() != this.currentMasterKey.getKeyId()\n+            ? masterKeyFromRM : this.currentMasterKey);\n+    return heartbeatResponse;\n   }\n \n }",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "sha": "ba999bfb2e094b837d3cb83cc890548543858186",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNodes.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNodes.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNodes.java",
                "patch": "@@ -25,12 +25,11 @@\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.NodeHealthStatus;\n import org.apache.hadoop.yarn.api.records.NodeId;\n-import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.NodeState;\n+import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.factories.RecordFactory;\n import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\n import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n-import org.apache.hadoop.yarn.server.api.records.MasterKey;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n \n import com.google.common.collect.Lists;\n@@ -188,11 +187,6 @@ public NodeState getState() {\n     public HeartbeatResponse getLastHeartBeatResponse() {\n       return null;\n     }\n-\n-    @Override\n-    public MasterKey getCurrentMasterKey() {\n-      return null;\n-    }\n   };\n \n   private static RMNode buildRMNode(int rack, final Resource perNode, NodeState state, String httpAddr) {",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNodes.java",
                "sha": "0c56a27ada03143e36ffac9c39f469e27807369e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java",
                "patch": "@@ -105,7 +105,7 @@ public Void answer(InvocationOnMock invocation) throws Throwable {\n         new TestSchedulerEventDispatcher());\n     \n     NodeId nodeId = BuilderUtils.newNodeId(\"localhost\", 0);\n-    node = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, null, null);\n+    node = new RMNodeImpl(nodeId, rmContext, null, 0, 0, null, null);\n \n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMNodeTransitions.java",
                "sha": "2b2decccb6bc4d6d2bb0e3f3b6217ef8d23786fd",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesNodes.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesNodes.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesNodes.java",
                "patch": "@@ -54,6 +54,7 @@\n import org.w3c.dom.Element;\n import org.w3c.dom.NodeList;\n import org.xml.sax.InputSource;\n+\n import com.google.inject.Guice;\n import com.google.inject.Injector;\n import com.google.inject.servlet.GuiceServletContextListener;\n@@ -145,7 +146,7 @@ public void testNodesDefaultWithUnHealthyNode() throws JSONException,\n     nodeHealth.setHealthReport(\"test health report\");\n     nodeHealth.setIsNodeHealthy(false);\n     node.handle(new RMNodeStatusEvent(nm3.getNodeId(), nodeHealth,\n-        new ArrayList<ContainerStatus>(), null, null, null));\n+        new ArrayList<ContainerStatus>(), null, null));\n     rm.NMwaitForState(nm3.getNodeId(), NodeState.UNHEALTHY);\n \n     ClientResponse response =\n@@ -360,7 +361,7 @@ public void testNodesQueryHealthyAndState() throws JSONException, Exception {\n     nodeHealth.setHealthReport(\"test health report\");\n     nodeHealth.setIsNodeHealthy(false);\n     node.handle(new RMNodeStatusEvent(nm1.getNodeId(), nodeHealth,\n-        new ArrayList<ContainerStatus>(), null, null, null));\n+        new ArrayList<ContainerStatus>(), null, null));\n     rm.NMwaitForState(nm1.getNodeId(), NodeState.UNHEALTHY);\n \n     ClientResponse response = r.path(\"ws\").path(\"v1\").path(\"cluster\")",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServicesNodes.java",
                "sha": "084dcffe4cbb20c550e21ca1452c6a0a904decd9",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml",
                "patch": "@@ -44,6 +44,12 @@\n       <groupId>org.apache.hadoop</groupId>\n       <artifactId>hadoop-yarn-server-resourcemanager</artifactId>\n     </dependency>\n+    <dependency>\n+      <groupId>org.apache.hadoop</groupId>\n+      <artifactId>hadoop-yarn-server-resourcemanager</artifactId>\n+      <type>test-jar</type>\n+      <scope>test</scope>\n+    </dependency>\n   </dependencies>\n \n   <build>",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/pom.xml",
                "sha": "600c647f9f775e49fb4a4e8e1078435449002a38",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 12,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java",
                "patch": "@@ -46,7 +46,6 @@\n import org.apache.hadoop.io.DataInputBuffer;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.ipc.RPC;\n-import org.apache.hadoop.ipc.RemoteException;\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.security.AccessControlException;\n import org.apache.hadoop.security.SecurityUtil;\n@@ -222,7 +221,7 @@ public void testMaliceUser() throws IOException, InterruptedException {\n     Resource modifiedResource = BuilderUtils.newResource(2048);\n     ContainerTokenIdentifier modifiedIdentifier = new ContainerTokenIdentifier(\n         dummyIdentifier.getContainerID(), dummyIdentifier.getNmHostAddress(),\n-        modifiedResource, Long.MAX_VALUE, 0);\n+        modifiedResource, Long.MAX_VALUE, dummyIdentifier.getMasterKeyId());\n     Token<ContainerTokenIdentifier> modifiedToken = new Token<ContainerTokenIdentifier>(\n         modifiedIdentifier.getBytes(), containerToken.getPassword().array(),\n         new Text(containerToken.getKind()), new Text(containerToken\n@@ -250,19 +249,14 @@ public Void run() {\n               + \"it will indicate RPC success\");\n         } catch (Exception e) {\n           Assert.assertEquals(\n-              java.lang.reflect.UndeclaredThrowableException.class\n-                  .getCanonicalName(), e.getClass().getCanonicalName());\n-          Assert.assertEquals(RemoteException.class.getCanonicalName(), e\n-            .getCause().getClass().getCanonicalName());\n-          Assert.assertEquals(\n-            \"org.apache.hadoop.security.token.SecretManager$InvalidToken\",\n-            ((RemoteException) e.getCause()).getClassName());\n+            java.lang.reflect.UndeclaredThrowableException.class\n+              .getCanonicalName(), e.getClass().getCanonicalName());\n           Assert.assertTrue(e\n             .getCause()\n             .getMessage()\n-            .matches(\n-              \"Given Container container_\\\\d*_\\\\d*_\\\\d\\\\d_\\\\d*\"\n-                  + \" seems to have an illegally generated token.\"));\n+            .contains(\n+              \"DIGEST-MD5: digest response format violation. \"\n+                  + \"Mismatched response.\"));\n         }\n         return null;\n       }",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestContainerManagerSecurity.java",
                "sha": "1c7933ae275e20cc592feac9b26d687c3f2c8724",
                "status": "modified"
            },
            {
                "additions": 120,
                "blob_url": "https://github.com/apache/hadoop/blob/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestRMNMSecretKeys.java",
                "changes": 120,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestRMNMSecretKeys.java?ref=45a8e8c5a46535287de97fd6609c0743eef888ee",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestRMNMSecretKeys.java",
                "patch": "@@ -0,0 +1,120 @@\n+/**\n+* Licensed to the Apache Software Foundation (ASF) under one\n+* or more contributor license agreements.  See the NOTICE file\n+* distributed with this work for additional information\n+* regarding copyright ownership.  The ASF licenses this file\n+* to you under the Apache License, Version 2.0 (the\n+* \"License\"); you may not use this file except in compliance\n+* with the License.  You may obtain a copy of the License at\n+*\n+*     http://www.apache.org/licenses/LICENSE-2.0\n+*\n+* Unless required by applicable law or agreed to in writing, software\n+* distributed under the License is distributed on an \"AS IS\" BASIS,\n+* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+* See the License for the specific language governing permissions and\n+* limitations under the License.\n+*/\n+\n+package org.apache.hadoop.yarn.server;\n+\n+import java.io.IOException;\n+\n+import junit.framework.Assert;\n+\n+import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.yarn.conf.YarnConfiguration;\n+import org.apache.hadoop.yarn.event.Dispatcher;\n+import org.apache.hadoop.yarn.event.DrainDispatcher;\n+import org.apache.hadoop.yarn.server.api.records.HeartbeatResponse;\n+import org.apache.hadoop.yarn.server.api.records.MasterKey;\n+import org.apache.hadoop.yarn.server.api.records.RegistrationResponse;\n+import org.apache.hadoop.yarn.server.resourcemanager.MockNM;\n+import org.apache.hadoop.yarn.server.resourcemanager.ResourceManager;\n+import org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager;\n+import org.junit.Test;\n+\n+public class TestRMNMSecretKeys {\n+\n+  @Test\n+  public void testNMUpdation() throws Exception {\n+    YarnConfiguration conf = new YarnConfiguration();\n+    conf.set(CommonConfigurationKeysPublic.HADOOP_SECURITY_AUTHENTICATION,\n+      \"kerberos\");\n+    UserGroupInformation.setConfiguration(conf);\n+    // Default rolling and activation intervals are large enough, no need to\n+    // intervene\n+\n+    final DrainDispatcher dispatcher = new DrainDispatcher();\n+    ResourceManager rm = new ResourceManager(null) {\n+      @Override\n+      protected void doSecureLogin() throws IOException {\n+        // Do nothing.\n+      }\n+\n+      @Override\n+      protected Dispatcher createDispatcher() {\n+        return dispatcher;\n+      }\n+    };\n+    rm.init(conf);\n+    rm.start();\n+\n+    MockNM nm = new MockNM(\"host:1234\", 3072, rm.getResourceTrackerService());\n+    RegistrationResponse registrationResponse = nm.registerNode();\n+    MasterKey masterKey = registrationResponse.getMasterKey();\n+    Assert.assertNotNull(\"Registration should cause a key-update!\", masterKey);\n+    dispatcher.await();\n+\n+    HeartbeatResponse response = nm.nodeHeartbeat(true);\n+    Assert.assertNull(\n+      \"First heartbeat after registration shouldn't get any key updates!\",\n+      response.getMasterKey());\n+    dispatcher.await();\n+\n+    response = nm.nodeHeartbeat(true);\n+    Assert\n+      .assertNull(\n+        \"Even second heartbeat after registration shouldn't get any key updates!\",\n+        response.getMasterKey());\n+    dispatcher.await();\n+\n+    // Let's force a roll-over\n+    RMContainerTokenSecretManager secretManager =\n+        rm.getRMContainerTokenSecretManager();\n+    secretManager.rollMasterKey();\n+\n+    // Heartbeats after roll-over and before activation should be fine.\n+    response = nm.nodeHeartbeat(true);\n+    Assert.assertNotNull(\n+      \"Heartbeats after roll-over and before activation should not err out.\",\n+      response.getMasterKey());\n+    Assert.assertEquals(\n+      \"Roll-over should have incremented the key-id only by one!\",\n+      masterKey.getKeyId() + 1, response.getMasterKey().getKeyId());\n+    dispatcher.await();\n+\n+    response = nm.nodeHeartbeat(true);\n+    Assert.assertNull(\n+      \"Second heartbeat after roll-over shouldn't get any key updates!\",\n+      response.getMasterKey());\n+    dispatcher.await();\n+\n+    // Let's force activation\n+    secretManager.activateNextMasterKey();\n+\n+    response = nm.nodeHeartbeat(true);\n+    Assert.assertNull(\"Activation shouldn't cause any key updates!\",\n+      response.getMasterKey());\n+    dispatcher.await();\n+\n+    response = nm.nodeHeartbeat(true);\n+    Assert.assertNull(\n+      \"Even second heartbeat after activation shouldn't get any key updates!\",\n+      response.getMasterKey());\n+    dispatcher.await();\n+\n+    rm.stop();\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/45a8e8c5a46535287de97fd6609c0743eef888ee/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-tests/src/test/java/org/apache/hadoop/yarn/server/TestRMNMSecretKeys.java",
                "sha": "9b6024ce3c0621706b5ffce30d59fb58947ed517",
                "status": "added"
            }
        ],
        "message": "YARN-60. Fixed a bug in ResourceManager which causes all NMs to get NPEs and thus causes all containers to be rejected. Contributed by Vinod Kumar Vavilapalli.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1379550 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/38d003a6db26307cd6544e1ca303c5a521299fb4",
        "repo": "hadoop",
        "unit_tests": [
            "TestNMContainerTokenSecretManager.java",
            "TestResourceManager.java",
            "TestResourceTrackerService.java"
        ]
    },
    "hadoop_477ed62": {
        "bug_id": "hadoop_477ed62",
        "commit": "https://github.com/apache/hadoop/commit/477ed62b3fe8db4b07d99479f56a2b997933cb01",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/477ed62b3fe8db4b07d99479f56a2b997933cb01/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=477ed62b3fe8db4b07d99479f56a2b997933cb01",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -443,6 +443,9 @@ Release 2.4.0 - UNRELEASED\n     apps-killed metrics correctly for killed applications. (Varun Vasudev via\n     vinodkv)\n \n+    YARN-1821. NPE on registerNodeManager if the request has containers for \n+    UnmanagedAMs. (kasha)\n+\n Release 2.3.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/477ed62b3fe8db4b07d99479f56a2b997933cb01/hadoop-yarn-project/CHANGES.txt",
                "sha": "a222fed796ef219a5784f065c33fd29eaf1ec68e",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/477ed62b3fe8db4b07d99479f56a2b997933cb01/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java?ref=477ed62b3fe8db4b07d99479f56a2b997933cb01",
                "deletions": 8,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "patch": "@@ -210,14 +210,16 @@ public RegisterNodeManagerResponse registerNodeManager(\n             rmContext.getRMApps().get(appAttemptId.getApplicationId());\n         if (rmApp != null) {\n           RMAppAttempt rmAppAttempt = rmApp.getRMAppAttempt(appAttemptId);\n-          if (rmAppAttempt.getMasterContainer().getId()\n-              .equals(containerStatus.getContainerId())\n-              && containerStatus.getState() == ContainerState.COMPLETE) {\n-            // sending master container finished event.\n-            RMAppAttemptContainerFinishedEvent evt =\n-                new RMAppAttemptContainerFinishedEvent(appAttemptId,\n-                    containerStatus);\n-            rmContext.getDispatcher().getEventHandler().handle(evt);\n+          if (rmAppAttempt != null) {\n+            if (rmAppAttempt.getMasterContainer().getId()\n+                .equals(containerStatus.getContainerId())\n+                && containerStatus.getState() == ContainerState.COMPLETE) {\n+              // sending master container finished event.\n+              RMAppAttemptContainerFinishedEvent evt =\n+                  new RMAppAttemptContainerFinishedEvent(appAttemptId,\n+                      containerStatus);\n+              rmContext.getDispatcher().getEventHandler().handle(evt);\n+            }\n           }\n         } else {\n           LOG.error(\"Received finished container :\"",
                "raw_url": "https://github.com/apache/hadoop/raw/477ed62b3fe8db4b07d99479f56a2b997933cb01/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "sha": "8a2c53958cba746d4fb5e7f0bf30e199235cb4bb",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop/blob/477ed62b3fe8db4b07d99479f56a2b997933cb01/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java?ref=477ed62b3fe8db4b07d99479f56a2b997933cb01",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "patch": "@@ -21,6 +21,8 @@\n import java.io.File;\n import java.io.FileOutputStream;\n import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.List;\n \n@@ -29,7 +31,11 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.net.NetUtils;\n+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.api.records.Container;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.api.records.ContainerState;\n import org.apache.hadoop.yarn.api.records.ContainerStatus;\n import org.apache.hadoop.yarn.api.records.NodeId;\n import org.apache.hadoop.yarn.api.records.NodeState;\n@@ -42,6 +48,7 @@\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse;\n import org.apache.hadoop.yarn.server.api.records.NodeAction;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent;\n import org.apache.hadoop.yarn.server.utils.BuilderUtils;\n@@ -50,6 +57,8 @@\n import org.junit.After;\n import org.junit.Test;\n \n+import static org.junit.Assert.assertEquals;\n+\n public class TestResourceTrackerService {\n \n   private final static File TEMP_DIR = new File(System.getProperty(\n@@ -457,6 +466,28 @@ private void checkUnealthyNMCount(MockRM rm, MockNM nm1, boolean health,\n         ClusterMetrics.getMetrics().getUnhealthyNMs());\n   }\n \n+  @Test\n+  public void testNodeRegistrationWithContainers() throws Exception {\n+    MockRM rm = new MockRM();\n+    rm.init(new YarnConfiguration());\n+    rm.start();\n+    RMApp app = rm.submitApp(1024);\n+\n+    MockNM nm = rm.registerNode(\"host1:1234\", 8192);\n+    nm.nodeHeartbeat(true);\n+\n+    // Register node with some container statuses\n+    ContainerStatus status = ContainerStatus.newInstance(\n+        ContainerId.newInstance(ApplicationAttemptId.newInstance(\n+            app.getApplicationId(), 2), 1),\n+        ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+\n+    // The following shouldn't throw NPE\n+    nm.registerNode(Collections.singletonList(status));\n+    assertEquals(\"Incorrect number of nodes\", 1,\n+        rm.getRMContext().getRMNodes().size());\n+  }\n+\n   @Test\n   public void testReconnectNode() throws Exception {\n     final DrainDispatcher dispatcher = new DrainDispatcher();",
                "raw_url": "https://github.com/apache/hadoop/raw/477ed62b3fe8db4b07d99479f56a2b997933cb01/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "sha": "eed9ecf9fa7e28f66bcb962fc41c2e842d9c7443",
                "status": "modified"
            }
        ],
        "message": "YARN-1821. NPE on registerNodeManager if the request has containers for UnmanagedAMs (kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1576525 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/b027ef8858e6b8ce26635ffbadc2a461c555ff86",
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceTrackerService.java"
        ]
    },
    "hadoop_47c41e7": {
        "bug_id": "hadoop_47c41e7",
        "commit": "https://github.com/apache/hadoop/commit/47c41e7ac7e6b905a58550f8899f629c1cf8b138",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/47c41e7ac7e6b905a58550f8899f629c1cf8b138/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java?ref=47c41e7ac7e6b905a58550f8899f629c1cf8b138",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java",
                "patch": "@@ -539,9 +539,14 @@ public boolean cancel() {\n    */\n   private boolean skipTokenRenewal(Token<?> token)\n       throws IOException {\n+\n     @SuppressWarnings(\"unchecked\")\n-    Text renewer = ((Token<AbstractDelegationTokenIdentifier>)token).\n-        decodeIdentifier().getRenewer();\n+    AbstractDelegationTokenIdentifier identifier =\n+        ((Token<AbstractDelegationTokenIdentifier>) token).decodeIdentifier();\n+    if (identifier == null) {\n+      return false;\n+    }\n+    Text renewer = identifier.getRenewer();\n     return (renewer != null && renewer.toString().equals(\"\"));\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/47c41e7ac7e6b905a58550f8899f629c1cf8b138/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java",
                "sha": "fd12f11c78999a7ba9a95e9e164562c13138a457",
                "status": "modified"
            }
        ],
        "message": "YARN-5048. DelegationTokenRenewer#skipTokenRenewal may throw NPE (Jian He via Yongjun Zhang)",
        "parent": "https://github.com/apache/hadoop/commit/6957e4569996734b1b176e04df5a03d000bed5b7",
        "repo": "hadoop",
        "unit_tests": [
            "TestDelegationTokenRenewer.java"
        ]
    },
    "hadoop_47f711e": {
        "bug_id": "hadoop_47f711e",
        "commit": "https://github.com/apache/hadoop/commit/47f711eebca315804c80012eea5f31275ac25518",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/47f711eebca315804c80012eea5f31275ac25518/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java?ref=47f711eebca315804c80012eea5f31275ac25518",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "patch": "@@ -2825,8 +2825,8 @@ public boolean tryCommit(Resource cluster, ResourceCommitRequest r,\n       // proposal might be outdated if AM failover just finished\n       // and proposal queue was not be consumed in time\n       if (app != null && attemptId.equals(app.getApplicationAttemptId())) {\n-        if (app.accept(cluster, request, updatePending)) {\n-          app.apply(cluster, request, updatePending);\n+        if (app.accept(cluster, request, updatePending)\n+            && app.apply(cluster, request, updatePending)) {\n           LOG.info(\"Allocation proposal accepted\");\n           isSuccess = true;\n         } else{",
                "raw_url": "https://github.com/apache/hadoop/raw/47f711eebca315804c80012eea5f31275ac25518/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "sha": "bf674a83547f365d4a3cac28b94e043bf14246ab",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/47f711eebca315804c80012eea5f31275ac25518/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java?ref=47f711eebca315804c80012eea5f31275ac25518",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "patch": "@@ -489,7 +489,7 @@ public boolean accept(Resource cluster,\n     return accepted;\n   }\n \n-  public void apply(Resource cluster, ResourceCommitRequest<FiCaSchedulerApp,\n+  public boolean apply(Resource cluster, ResourceCommitRequest<FiCaSchedulerApp,\n       FiCaSchedulerNode> request, boolean updatePending) {\n     boolean reReservation = false;\n \n@@ -502,8 +502,16 @@ public void apply(Resource cluster, ResourceCommitRequest<FiCaSchedulerApp,\n             allocation = request.getFirstAllocatedOrReservedContainer();\n         SchedulerContainer<FiCaSchedulerApp, FiCaSchedulerNode>\n             schedulerContainer = allocation.getAllocatedOrReservedContainer();\n-        RMContainer rmContainer = schedulerContainer.getRmContainer();\n \n+        // Required sanity check - AM can call 'allocate' to update resource\n+        // request without locking the scheduler, hence we need to check\n+        if (updatePending &&\n+            getOutstandingAsksCount(schedulerContainer.getSchedulerRequestKey())\n+                <= 0) {\n+          return false;\n+        }\n+\n+        RMContainer rmContainer = schedulerContainer.getRmContainer();\n         reReservation =\n             (!schedulerContainer.isAllocated()) && (rmContainer.getState()\n                 == RMContainerState.RESERVED);\n@@ -545,7 +553,8 @@ public void apply(Resource cluster, ResourceCommitRequest<FiCaSchedulerApp,\n                 containerRequest);\n \n             // If this is from a SchedulingRequest, set allocation tags.\n-            if (containerRequest.getSchedulingRequest() != null) {\n+            if (containerRequest != null\n+                && containerRequest.getSchedulingRequest() != null) {\n               ((RMContainerImpl) rmContainer).setAllocationTags(\n                   containerRequest.getSchedulingRequest().getAllocationTags());\n             }\n@@ -598,6 +607,7 @@ public void apply(Resource cluster, ResourceCommitRequest<FiCaSchedulerApp,\n     if (!reReservation) {\n       getCSLeafQueue().apply(cluster, request);\n     }\n+    return true;\n   }\n \n   public boolean unreserve(SchedulerRequestKey schedulerKey,",
                "raw_url": "https://github.com/apache/hadoop/raw/47f711eebca315804c80012eea5f31275ac25518/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "sha": "32b2cad0ddf89a325f3bc195860f26b9b26ff9d0",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/hadoop/blob/47f711eebca315804c80012eea5f31275ac25518/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java?ref=47f711eebca315804c80012eea5f31275ac25518",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java",
                "patch": "@@ -134,6 +134,7 @@\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNodeReport;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerUtils;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.YarnScheduler;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAddedSchedulerEvent;\n@@ -170,6 +171,8 @@\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Sets;\n+import org.mockito.invocation.InvocationOnMock;\n+import org.mockito.stubbing.Answer;\n \n public class TestCapacityScheduler extends CapacitySchedulerTestBase {\n   private static final Log LOG = LogFactory.getLog(TestCapacityScheduler.class);\n@@ -4857,4 +4860,54 @@ private void waitforNMRegistered(ResourceScheduler scheduler, int nodecount,\n       }\n     }\n   }\n+\n+  @Test (timeout = 60000)\n+  public void testClearRequestsBeforeApplyTheProposal()\n+      throws Exception {\n+    // init RM & NMs & Nodes\n+    final MockRM rm = new MockRM(new CapacitySchedulerConfiguration());\n+    rm.start();\n+    final MockNM nm = rm.registerNode(\"h1:1234\", 200 * GB);\n+\n+    // submit app\n+    final RMApp app = rm.submitApp(200, \"app\", \"user\");\n+    MockRM.launchAndRegisterAM(app, rm, nm);\n+\n+    // spy capacity scheduler to handle CapacityScheduler#apply\n+    final Priority priority = Priority.newInstance(1);\n+    final CapacityScheduler cs = (CapacityScheduler) rm.getResourceScheduler();\n+    final CapacityScheduler spyCs = Mockito.spy(cs);\n+    Mockito.doAnswer(new Answer<Object>() {\n+      public Object answer(InvocationOnMock invocation) throws Exception {\n+        // clear resource request before applying the proposal for container_2\n+        spyCs.allocate(app.getCurrentAppAttempt().getAppAttemptId(),\n+            Arrays.asList(ResourceRequest.newInstance(priority, \"*\",\n+                Resources.createResource(1 * GB), 0)), null,\n+            Collections.<ContainerId>emptyList(), null, null,\n+            NULL_UPDATE_REQUESTS);\n+        // trigger real apply which can raise NPE before YARN-6629\n+        try {\n+          FiCaSchedulerApp schedulerApp = cs.getApplicationAttempt(\n+              app.getCurrentAppAttempt().getAppAttemptId());\n+          schedulerApp.apply((Resource) invocation.getArguments()[0],\n+              (ResourceCommitRequest) invocation.getArguments()[1],\n+              (Boolean) invocation.getArguments()[2]);\n+          // the proposal of removed request should be rejected\n+          Assert.assertEquals(1, schedulerApp.getLiveContainers().size());\n+        } catch (Throwable e) {\n+          Assert.fail();\n+        }\n+        return null;\n+      }\n+    }).when(spyCs).tryCommit(Mockito.any(Resource.class),\n+        Mockito.any(ResourceCommitRequest.class), Mockito.anyBoolean());\n+\n+    // rm allocates container_2 to reproduce the process that can raise NPE\n+    spyCs.allocate(app.getCurrentAppAttempt().getAppAttemptId(),\n+        Arrays.asList(ResourceRequest.newInstance(priority, \"*\",\n+            Resources.createResource(1 * GB), 1)), null,\n+        Collections.<ContainerId>emptyList(), null, null, NULL_UPDATE_REQUESTS);\n+    spyCs.handle(new NodeUpdateSchedulerEvent(\n+        spyCs.getNode(nm.getNodeId()).getRMNode()));\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/47f711eebca315804c80012eea5f31275ac25518/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java",
                "sha": "1d2aadcf2a84bd7bd6cfc61eeec1fd3b0a3c474e",
                "status": "modified"
            }
        ],
        "message": "YARN-6629. NPE occurred when container allocation proposal is applied but its resource requests are removed before. (Tao Yang via wangda)\n\nChange-Id: I805880f90b3f6798ec96ed8e8e75755f390a9ad5",
        "parent": "https://github.com/apache/hadoop/commit/cdee0a4f840868d8b8acac15e62da2ab337618c7",
        "repo": "hadoop",
        "unit_tests": [
            "TestCapacityScheduler.java"
        ]
    },
    "hadoop_4922394": {
        "bug_id": "hadoop_4922394",
        "commit": "https://github.com/apache/hadoop/commit/492239424a3ace9868b6154f44a0f18fa5318235",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/492239424a3ace9868b6154f44a0f18fa5318235/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=492239424a3ace9868b6154f44a0f18fa5318235",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -129,6 +129,9 @@ Release 2.8.0 - UNRELEASED\n \n     YARN-3412. RM tests should use MockRM where possible. (kasha)\n \n+    YARN-3425. NPE from RMNodeLabelsManager.serviceStop when \n+    NodeLabelsManager.serviceInit failed. (Bibin A Chundatt via wangda)\n+\n Release 2.7.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/492239424a3ace9868b6154f44a0f18fa5318235/hadoop-yarn-project/CHANGES.txt",
                "sha": "f5dc39d2c6c163ad1d0fec1574b19c81cdc9815a",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/492239424a3ace9868b6154f44a0f18fa5318235/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/CommonNodeLabelsManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/CommonNodeLabelsManager.java?ref=492239424a3ace9868b6154f44a0f18fa5318235",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/CommonNodeLabelsManager.java",
                "patch": "@@ -258,7 +258,9 @@ protected void serviceStart() throws Exception {\n   // for UT purpose\n   protected void stopDispatcher() {\n     AsyncDispatcher asyncDispatcher = (AsyncDispatcher) dispatcher;\n-    asyncDispatcher.stop();\n+    if (null != asyncDispatcher) {\n+      asyncDispatcher.stop();\n+    }\n   }\n   \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/492239424a3ace9868b6154f44a0f18fa5318235/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/CommonNodeLabelsManager.java",
                "sha": "fe3816474e615342ed560a87f0a9ffdaea3ceb40",
                "status": "modified"
            }
        ],
        "message": "YARN-3425. NPE from RMNodeLabelsManager.serviceStop when NodeLabelsManager.serviceInit failed. (Bibin A Chundatt via wangda)",
        "parent": "https://github.com/apache/hadoop/commit/2e79f1c2125517586c165a84e99d3c4d38ca0938",
        "repo": "hadoop",
        "unit_tests": [
            "TestCommonNodeLabelsManager.java"
        ]
    },
    "hadoop_49c3889": {
        "bug_id": "hadoop_49c3889",
        "commit": "https://github.com/apache/hadoop/commit/49c38898b0be64fc686d039ed2fb2dea1378df02",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/49c38898b0be64fc686d039ed2fb2dea1378df02/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=49c38898b0be64fc686d039ed2fb2dea1378df02",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -90,6 +90,9 @@ Release 2.7.0 - UNRELEASED\n     YARN-2857. ConcurrentModificationException in ContainerLogAppender\n     (Mohammad Kamrul Islam via jlowe)\n \n+    YARN-2816. NM fail to start with NPE during container recovery (Zhihai Xu\n+    via jlowe)\n+\n Release 2.6.0 - 2014-11-18\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/49c38898b0be64fc686d039ed2fb2dea1378df02/hadoop-yarn-project/CHANGES.txt",
                "sha": "1f8af5592a75afbc17cf5c8d918a486795e4fd7f",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hadoop/blob/49c38898b0be64fc686d039ed2fb2dea1378df02/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/recovery/NMLeveldbStateStoreService.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/recovery/NMLeveldbStateStoreService.java?ref=49c38898b0be64fc686d039ed2fb2dea1378df02",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/recovery/NMLeveldbStateStoreService.java",
                "patch": "@@ -146,6 +146,8 @@ public boolean isNewlyCreated() {\n       throws IOException {\n     ArrayList<RecoveredContainerState> containers =\n         new ArrayList<RecoveredContainerState>();\n+    ArrayList<ContainerId> containersToRemove =\n+              new ArrayList<ContainerId>();\n     LeveldbIterator iter = null;\n     try {\n       iter = new LeveldbIterator(db);\n@@ -165,7 +167,14 @@ public boolean isNewlyCreated() {\n         ContainerId containerId = ConverterUtils.toContainerId(\n             key.substring(CONTAINERS_KEY_PREFIX.length(), idEndPos));\n         String keyPrefix = key.substring(0, idEndPos+1);\n-        containers.add(loadContainerState(containerId, iter, keyPrefix));\n+        RecoveredContainerState rcs = loadContainerState(containerId,\n+            iter, keyPrefix);\n+        // Don't load container without StartContainerRequest\n+        if (rcs.startRequest != null) {\n+          containers.add(rcs);\n+        } else {\n+          containersToRemove.add(containerId);\n+        }\n       }\n     } catch (DBException e) {\n       throw new IOException(e);\n@@ -175,6 +184,19 @@ public boolean isNewlyCreated() {\n       }\n     }\n \n+    // remove container without StartContainerRequest\n+    for (ContainerId containerId : containersToRemove) {\n+      LOG.warn(\"Remove container \" + containerId +\n+          \" with incomplete records\");\n+      try {\n+        removeContainer(containerId);\n+        // TODO: kill and cleanup the leaked container\n+      } catch (IOException e) {\n+        LOG.error(\"Unable to remove container \" + containerId +\n+            \" in store\", e);\n+      }\n+    }\n+\n     return containers;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/49c38898b0be64fc686d039ed2fb2dea1378df02/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/recovery/NMLeveldbStateStoreService.java",
                "sha": "9d5468845dac4c5bfad8057db8e8fdc98292cce9",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/49c38898b0be64fc686d039ed2fb2dea1378df02/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/recovery/TestNMLeveldbStateStoreService.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/recovery/TestNMLeveldbStateStoreService.java?ref=49c38898b0be64fc686d039ed2fb2dea1378df02",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/recovery/TestNMLeveldbStateStoreService.java",
                "patch": "@@ -274,6 +274,13 @@ public void testContainerStorage() throws IOException {\n     assertEquals(containerReq, rcs.getStartRequest());\n     assertTrue(rcs.getDiagnostics().isEmpty());\n \n+    // store a new container record without StartContainerRequest\n+    ContainerId containerId1 = ContainerId.newContainerId(appAttemptId, 6);\n+    stateStore.storeContainerLaunched(containerId1);\n+    recoveredContainers = stateStore.loadContainersState();\n+    // check whether the new container record is discarded\n+    assertEquals(1, recoveredContainers.size());\n+\n     // launch the container, add some diagnostics, and verify recovered\n     StringBuilder diags = new StringBuilder();\n     stateStore.storeContainerLaunched(containerId);",
                "raw_url": "https://github.com/apache/hadoop/raw/49c38898b0be64fc686d039ed2fb2dea1378df02/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/recovery/TestNMLeveldbStateStoreService.java",
                "sha": "f7f43cc5aa495ee994920eb336ef1b0087e20de2",
                "status": "modified"
            }
        ],
        "message": "YARN-2816. NM fail to start with NPE during container recovery. Contributed by Zhihai Xu",
        "parent": "https://github.com/apache/hadoop/commit/3baaa42945fd432e10c547d3ead57e220f64cd57",
        "repo": "hadoop",
        "unit_tests": [
            "TestNMLeveldbStateStoreService.java"
        ]
    },
    "hadoop_4a114dd": {
        "bug_id": "hadoop_4a114dd",
        "commit": "https://github.com/apache/hadoop/commit/4a114dd67aae83e5bb2d65470166de954acf36a2",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=4a114dd67aae83e5bb2d65470166de954acf36a2",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -911,6 +911,9 @@ Release 2.6.0 - UNRELEASED\n \n     YARN-2825. Container leak on NM (Jian He via jlowe)\n \n+    YARN-2819. NPE in ATS Timeline Domains when upgrading from 2.4 to 2.6.\n+    (Zhijie Shen via xgong)\n+\n Release 2.5.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/CHANGES.txt",
                "sha": "d4c882719c68977fab8eced897d50d0af87ced4c",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/hadoop/blob/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/LeveldbTimelineStore.java",
                "changes": 46,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/LeveldbTimelineStore.java?ref=4a114dd67aae83e5bb2d65470166de954acf36a2",
                "deletions": 10,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/LeveldbTimelineStore.java",
                "patch": "@@ -792,7 +792,8 @@ private TimelineEntities getEntityByTime(byte[] base,\n    * Put a single entity.  If there is an error, add a TimelinePutError to the\n    * given response.\n    */\n-  private void put(TimelineEntity entity, TimelinePutResponse response) {\n+  private void put(TimelineEntity entity, TimelinePutResponse response,\n+      boolean allowEmptyDomainId) {\n     LockMap.CountingReentrantLock<EntityIdentifier> lock =\n         writeLocks.getLock(new EntityIdentifier(entity.getEntityId(),\n             entity.getEntityType()));\n@@ -867,10 +868,18 @@ private void put(TimelineEntity entity, TimelinePutResponse response) {\n                   new EntityIdentifier(relatedEntityId, relatedEntityType));\n               continue;\n             } else {\n+              // This is the existing entity\n               byte[] domainIdBytes = db.get(createDomainIdKey(\n                   relatedEntityId, relatedEntityType, relatedEntityStartTime));\n-              // This is the existing entity\n-              String domainId = new String(domainIdBytes);\n+              // The timeline data created by the server before 2.6 won't have\n+              // the domain field. We assume this timeline data is in the\n+              // default timeline domain.\n+              String domainId = null;\n+              if (domainIdBytes == null) {\n+                domainId = TimelineDataManager.DEFAULT_DOMAIN_ID;\n+              } else {\n+                domainId = new String(domainIdBytes);\n+              }\n               if (!domainId.equals(entity.getDomainId())) {\n                 // in this case the entity will be put, but the relation will be\n                 // ignored\n@@ -923,12 +932,14 @@ private void put(TimelineEntity entity, TimelinePutResponse response) {\n           entity.getEntityType(), revStartTime);\n       if (entity.getDomainId() == null ||\n           entity.getDomainId().length() == 0) {\n-        TimelinePutError error = new TimelinePutError();\n-        error.setEntityId(entity.getEntityId());\n-        error.setEntityType(entity.getEntityType());\n-        error.setErrorCode(TimelinePutError.NO_DOMAIN);\n-        response.addError(error);\n-        return;\n+        if (!allowEmptyDomainId) {\n+          TimelinePutError error = new TimelinePutError();\n+          error.setEntityId(entity.getEntityId());\n+          error.setEntityType(entity.getEntityType());\n+          error.setErrorCode(TimelinePutError.NO_DOMAIN);\n+          response.addError(error);\n+          return;\n+        }\n       } else {\n         writeBatch.put(key, entity.getDomainId().getBytes());\n         writePrimaryFilterEntries(writeBatch, primaryFilters, key,\n@@ -1011,7 +1022,22 @@ public TimelinePutResponse put(TimelineEntities entities) {\n       deleteLock.readLock().lock();\n       TimelinePutResponse response = new TimelinePutResponse();\n       for (TimelineEntity entity : entities.getEntities()) {\n-        put(entity, response);\n+        put(entity, response, false);\n+      }\n+      return response;\n+    } finally {\n+      deleteLock.readLock().unlock();\n+    }\n+  }\n+\n+  @Private\n+  @VisibleForTesting\n+  public TimelinePutResponse putWithNoDomainId(TimelineEntities entities) {\n+    try {\n+      deleteLock.readLock().lock();\n+      TimelinePutResponse response = new TimelinePutResponse();\n+      for (TimelineEntity entity : entities.getEntities()) {\n+        put(entity, response, true);\n       }\n       return response;\n     } finally {",
                "raw_url": "https://github.com/apache/hadoop/raw/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/LeveldbTimelineStore.java",
                "sha": "c4ea9960ad739fd79391c32702d6b9effcb3e163",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/TimelineDataManager.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/TimelineDataManager.java?ref=4a114dd67aae83e5bb2d65470166de954acf36a2",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/TimelineDataManager.java",
                "patch": "@@ -124,6 +124,7 @@ public TimelineEntities getEntities(\n           entities.getEntities().iterator();\n       while (entitiesItr.hasNext()) {\n         TimelineEntity entity = entitiesItr.next();\n+        addDefaultDomainIdIfAbsent(entity);\n         try {\n           // check ACLs\n           if (!timelineACLsManager.checkAccess(\n@@ -161,6 +162,7 @@ public TimelineEntity getEntity(\n     entity =\n         store.getEntity(entityId, entityType, fields);\n     if (entity != null) {\n+      addDefaultDomainIdIfAbsent(entity);\n       // check ACLs\n       if (!timelineACLsManager.checkAccess(\n           callerUGI, ApplicationAccessType.VIEW_APP, entity)) {\n@@ -203,6 +205,7 @@ public TimelineEvents getEvents(\n               eventsOfOneEntity.getEntityId(),\n               eventsOfOneEntity.getEntityType(),\n               EnumSet.of(Field.PRIMARY_FILTERS));\n+          addDefaultDomainIdIfAbsent(entity);\n           // check ACLs\n           if (!timelineACLsManager.checkAccess(\n               callerUGI, ApplicationAccessType.VIEW_APP, entity)) {\n@@ -254,10 +257,12 @@ public TimelinePutResponse postEntities(\n         existingEntity =\n             store.getEntity(entityID.getId(), entityID.getType(),\n                 EnumSet.of(Field.PRIMARY_FILTERS));\n-        if (existingEntity != null &&\n-            !existingEntity.getDomainId().equals(entity.getDomainId())) {\n-          throw new YarnException(\"The domain of the timeline entity \"\n-            + entityID + \" is not allowed to be changed.\");\n+        if (existingEntity != null) {\n+          addDefaultDomainIdIfAbsent(existingEntity);\n+          if (!existingEntity.getDomainId().equals(entity.getDomainId())) {\n+            throw new YarnException(\"The domain of the timeline entity \"\n+              + entityID + \" is not allowed to be changed.\");\n+          }\n         }\n         if (!timelineACLsManager.checkAccess(\n             callerUGI, ApplicationAccessType.MODIFY_APP, entity)) {\n@@ -355,4 +360,11 @@ public TimelineDomains getDomains(String owner,\n     }\n   }\n \n+  private static void addDefaultDomainIdIfAbsent(TimelineEntity entity) {\n+    // be compatible with the timeline data created before 2.6\n+    if (entity.getDomainId() == null) {\n+      entity.setDomainId(DEFAULT_DOMAIN_ID);\n+    }\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/TimelineDataManager.java",
                "sha": "888c28311579e4300429cfa2eb5e150830c1a424",
                "status": "modified"
            },
            {
                "additions": 74,
                "blob_url": "https://github.com/apache/hadoop/blob/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TestLeveldbTimelineStore.java",
                "changes": 81,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TestLeveldbTimelineStore.java?ref=4a114dd67aae83e5bb2d65470166de954acf36a2",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TestLeveldbTimelineStore.java",
                "patch": "@@ -40,6 +40,7 @@\n import org.apache.hadoop.yarn.api.records.timeline.TimelineEntities;\n import org.apache.hadoop.yarn.api.records.timeline.TimelineEntity;\n import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse;\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse.TimelinePutError;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.server.records.Version;\n import org.apache.hadoop.yarn.server.timeline.LeveldbTimelineStore;\n@@ -160,12 +161,13 @@ private boolean deleteNextEntity(String entityType, byte[] ts)\n   @Test\n   public void testGetEntityTypes() throws IOException {\n     List<String> entityTypes = ((LeveldbTimelineStore)store).getEntityTypes();\n-    assertEquals(5, entityTypes.size());\n-    assertEquals(entityType1, entityTypes.get(0));\n-    assertEquals(entityType2, entityTypes.get(1));\n-    assertEquals(entityType4, entityTypes.get(2));\n-    assertEquals(entityType5, entityTypes.get(3));\n-    assertEquals(entityType7, entityTypes.get(4));\n+    assertEquals(6, entityTypes.size());\n+    assertEquals(\"OLD_ENTITY_TYPE_1\", entityTypes.get(0));\n+    assertEquals(entityType1, entityTypes.get(1));\n+    assertEquals(entityType2, entityTypes.get(2));\n+    assertEquals(entityType4, entityTypes.get(3));\n+    assertEquals(entityType5, entityTypes.get(4));\n+    assertEquals(entityType7, entityTypes.get(5));\n   }\n \n   @Test\n@@ -196,7 +198,7 @@ public void testDeleteEntities() throws IOException, InterruptedException {\n     ((LeveldbTimelineStore)store).discardOldEntities(-123l);\n     assertEquals(2, getEntities(\"type_1\").size());\n     assertEquals(0, getEntities(\"type_2\").size());\n-    assertEquals(4, ((LeveldbTimelineStore)store).getEntityTypes().size());\n+    assertEquals(5, ((LeveldbTimelineStore)store).getEntityTypes().size());\n \n     ((LeveldbTimelineStore)store).discardOldEntities(123l);\n     assertEquals(0, getEntities(\"type_1\").size());\n@@ -327,4 +329,69 @@ public void testGetDomains() throws IOException {\n     super.testGetDomains();\n   }\n \n+  @Test\n+  public void testRelatingToNonExistingEntity() throws IOException {\n+    TimelineEntity entityToStore = new TimelineEntity();\n+    entityToStore.setEntityType(\"TEST_ENTITY_TYPE_1\");\n+    entityToStore.setEntityId(\"TEST_ENTITY_ID_1\");\n+    entityToStore.setDomainId(TimelineDataManager.DEFAULT_DOMAIN_ID);\n+    entityToStore.addRelatedEntity(\"TEST_ENTITY_TYPE_2\", \"TEST_ENTITY_ID_2\");\n+    TimelineEntities entities = new TimelineEntities();\n+    entities.addEntity(entityToStore);\n+    store.put(entities);\n+    TimelineEntity entityToGet =\n+        store.getEntity(\"TEST_ENTITY_ID_2\", \"TEST_ENTITY_TYPE_2\", null);\n+    Assert.assertNotNull(entityToGet);\n+    Assert.assertEquals(\"DEFAULT\", entityToGet.getDomainId());\n+    Assert.assertEquals(\"TEST_ENTITY_TYPE_1\",\n+        entityToGet.getRelatedEntities().keySet().iterator().next());\n+    Assert.assertEquals(\"TEST_ENTITY_ID_1\",\n+        entityToGet.getRelatedEntities().values().iterator().next()\n+            .iterator().next());\n+  }\n+\n+  @Test\n+  public void testRelatingToOldEntityWithoutDomainId() throws IOException {\n+    // New entity is put in the default domain\n+    TimelineEntity entityToStore = new TimelineEntity();\n+    entityToStore.setEntityType(\"NEW_ENTITY_TYPE_1\");\n+    entityToStore.setEntityId(\"NEW_ENTITY_ID_1\");\n+    entityToStore.setDomainId(TimelineDataManager.DEFAULT_DOMAIN_ID);\n+    entityToStore.addRelatedEntity(\"OLD_ENTITY_TYPE_1\", \"OLD_ENTITY_ID_1\");\n+    TimelineEntities entities = new TimelineEntities();\n+    entities.addEntity(entityToStore);\n+    store.put(entities);\n+\n+    TimelineEntity entityToGet =\n+        store.getEntity(\"OLD_ENTITY_ID_1\", \"OLD_ENTITY_TYPE_1\", null);\n+    Assert.assertNotNull(entityToGet);\n+    Assert.assertNull(entityToGet.getDomainId());\n+    Assert.assertEquals(\"NEW_ENTITY_TYPE_1\",\n+        entityToGet.getRelatedEntities().keySet().iterator().next());\n+    Assert.assertEquals(\"NEW_ENTITY_ID_1\",\n+        entityToGet.getRelatedEntities().values().iterator().next()\n+            .iterator().next());\n+\n+    // New entity is not put in the default domain\n+    entityToStore = new TimelineEntity();\n+    entityToStore.setEntityType(\"NEW_ENTITY_TYPE_2\");\n+    entityToStore.setEntityId(\"NEW_ENTITY_ID_2\");\n+    entityToStore.setDomainId(\"NON_DEFAULT\");\n+    entityToStore.addRelatedEntity(\"OLD_ENTITY_TYPE_1\", \"OLD_ENTITY_ID_1\");\n+    entities = new TimelineEntities();\n+    entities.addEntity(entityToStore);\n+    TimelinePutResponse response = store.put(entities);\n+    Assert.assertEquals(1, response.getErrors().size());\n+    Assert.assertEquals(TimelinePutError.FORBIDDEN_RELATION,\n+        response.getErrors().get(0).getErrorCode());\n+    entityToGet =\n+        store.getEntity(\"OLD_ENTITY_ID_1\", \"OLD_ENTITY_TYPE_1\", null);\n+    Assert.assertNotNull(entityToGet);\n+    Assert.assertNull(entityToGet.getDomainId());\n+    // Still have one related entity\n+    Assert.assertEquals(1, entityToGet.getRelatedEntities().keySet().size());\n+    Assert.assertEquals(1, entityToGet.getRelatedEntities().values()\n+        .iterator().next().size());\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TestLeveldbTimelineStore.java",
                "sha": "5ebc96b627b5c8104d78d635a512e2f1257a53a4",
                "status": "modified"
            },
            {
                "additions": 152,
                "blob_url": "https://github.com/apache/hadoop/blob/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TestTimelineDataManager.java",
                "changes": 152,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TestTimelineDataManager.java?ref=4a114dd67aae83e5bb2d65470166de954acf36a2",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TestTimelineDataManager.java",
                "patch": "@@ -0,0 +1,152 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.yarn.server.timeline;\n+\n+import java.io.File;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileContext;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.yarn.api.records.timeline.TimelineEntities;\n+import org.apache.hadoop.yarn.api.records.timeline.TimelineEntity;\n+import org.apache.hadoop.yarn.api.records.timeline.TimelinePutResponse;\n+import org.apache.hadoop.yarn.conf.YarnConfiguration;\n+import org.apache.hadoop.yarn.server.timeline.security.TimelineACLsManager;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+\n+public class TestTimelineDataManager extends TimelineStoreTestUtils {\n+\n+  private FileContext fsContext;\n+  private File fsPath;\n+  private TimelineDataManager dataManaer;\n+\n+  @Before\n+  public void setup() throws Exception {\n+    fsPath = new File(\"target\", this.getClass().getSimpleName() +\n+        \"-tmpDir\").getAbsoluteFile();\n+    fsContext = FileContext.getLocalFSFileContext();\n+    fsContext.delete(new Path(fsPath.getAbsolutePath()), true);\n+    Configuration conf = new YarnConfiguration();\n+    conf.set(YarnConfiguration.TIMELINE_SERVICE_LEVELDB_PATH,\n+        fsPath.getAbsolutePath());\n+    conf.setBoolean(YarnConfiguration.TIMELINE_SERVICE_TTL_ENABLE, false);\n+    store = new LeveldbTimelineStore();\n+    store.init(conf);\n+    store.start();\n+    loadTestEntityData();\n+    loadVerificationEntityData();\n+    loadTestDomainData();\n+\n+    TimelineACLsManager aclsManager = new TimelineACLsManager(conf);\n+    dataManaer = new TimelineDataManager(store, aclsManager);\n+  }\n+\n+  @After\n+  public void tearDown() throws Exception {\n+    if (store != null) {\n+      store.stop();\n+    }\n+    if (fsContext != null) {\n+      fsContext.delete(new Path(fsPath.getAbsolutePath()), true);\n+    }\n+  }\n+\n+  @Test\n+  public void testGetOldEntityWithOutDomainId() throws Exception {\n+    TimelineEntity entity = dataManaer.getEntity(\n+        \"OLD_ENTITY_TYPE_1\", \"OLD_ENTITY_ID_1\", null,\n+        UserGroupInformation.getCurrentUser());\n+    Assert.assertNotNull(entity);\n+    Assert.assertEquals(\"OLD_ENTITY_ID_1\", entity.getEntityId());\n+    Assert.assertEquals(\"OLD_ENTITY_TYPE_1\", entity.getEntityType());\n+    Assert.assertEquals(\n+        TimelineDataManager.DEFAULT_DOMAIN_ID, entity.getDomainId());\n+  }\n+\n+  @Test\n+  public void testGetOldEntitiesWithOutDomainId() throws Exception {\n+    TimelineEntities entities = dataManaer.getEntities(\n+        \"OLD_ENTITY_TYPE_1\", null, null, null, null, null, null, null, null,\n+        UserGroupInformation.getCurrentUser());\n+    Assert.assertEquals(2, entities.getEntities().size());\n+    Assert.assertEquals(\"OLD_ENTITY_ID_2\",\n+        entities.getEntities().get(0).getEntityId());\n+    Assert.assertEquals(\"OLD_ENTITY_TYPE_1\",\n+        entities.getEntities().get(0).getEntityType());\n+    Assert.assertEquals(TimelineDataManager.DEFAULT_DOMAIN_ID,\n+        entities.getEntities().get(0).getDomainId());\n+    Assert.assertEquals(\"OLD_ENTITY_ID_1\",\n+        entities.getEntities().get(1).getEntityId());\n+    Assert.assertEquals(\"OLD_ENTITY_TYPE_1\",\n+        entities.getEntities().get(1).getEntityType());\n+    Assert.assertEquals(TimelineDataManager.DEFAULT_DOMAIN_ID,\n+        entities.getEntities().get(1).getDomainId());\n+  }\n+\n+  @Test\n+  public void testUpdatingOldEntityWithoutDomainId() throws Exception {\n+    // Set the domain to the default domain when updating\n+    TimelineEntity entity = new TimelineEntity();\n+    entity.setEntityType(\"OLD_ENTITY_TYPE_1\");\n+    entity.setEntityId(\"OLD_ENTITY_ID_1\");\n+    entity.setDomainId(TimelineDataManager.DEFAULT_DOMAIN_ID);\n+    entity.addOtherInfo(\"NEW_OTHER_INFO_KEY\", \"NEW_OTHER_INFO_VALUE\");\n+    TimelineEntities entities = new TimelineEntities();\n+    entities.addEntity(entity);\n+    TimelinePutResponse response = dataManaer.postEntities(\n+        entities, UserGroupInformation.getCurrentUser());\n+    Assert.assertEquals(0, response.getErrors().size());\n+    entity = store.getEntity(\"OLD_ENTITY_ID_1\", \"OLD_ENTITY_TYPE_1\", null);\n+    Assert.assertNotNull(entity);\n+    // Even in leveldb, the domain is updated to the default domain Id\n+    Assert.assertEquals(\n+        TimelineDataManager.DEFAULT_DOMAIN_ID, entity.getDomainId());\n+    Assert.assertEquals(1, entity.getOtherInfo().size());\n+    Assert.assertEquals(\"NEW_OTHER_INFO_KEY\",\n+        entity.getOtherInfo().keySet().iterator().next());\n+    Assert.assertEquals(\"NEW_OTHER_INFO_VALUE\",\n+        entity.getOtherInfo().values().iterator().next());\n+    \n+    // Set the domain to the non-default domain when updating\n+    entity = new TimelineEntity();\n+    entity.setEntityType(\"OLD_ENTITY_TYPE_1\");\n+    entity.setEntityId(\"OLD_ENTITY_ID_2\");\n+    entity.setDomainId(\"NON_DEFAULT\");\n+    entity.addOtherInfo(\"NEW_OTHER_INFO_KEY\", \"NEW_OTHER_INFO_VALUE\");\n+    entities = new TimelineEntities();\n+    entities.addEntity(entity);\n+    response = dataManaer.postEntities(\n+        entities, UserGroupInformation.getCurrentUser());\n+    Assert.assertEquals(1, response.getErrors().size());\n+    Assert.assertEquals(TimelinePutResponse.TimelinePutError.ACCESS_DENIED,\n+        response.getErrors().get(0).getErrorCode());\n+    entity = store.getEntity(\"OLD_ENTITY_ID_2\", \"OLD_ENTITY_TYPE_1\", null);\n+    Assert.assertNotNull(entity);\n+    // In leveldb, the domain Id is still null\n+    Assert.assertNull(entity.getDomainId());\n+    // Updating is not executed\n+    Assert.assertEquals(0, entity.getOtherInfo().size());\n+  }\n+  \n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TestTimelineDataManager.java",
                "sha": "f74956735a34b2509ebc81bdf657535c5c1f8bc1",
                "status": "added"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TimelineStoreTestUtils.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TimelineStoreTestUtils.java?ref=4a114dd67aae83e5bb2d65470166de954acf36a2",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TimelineStoreTestUtils.java",
                "patch": "@@ -210,6 +210,18 @@ protected void loadTestEntityData() throws IOException {\n     assertEquals(entityId7, response.getErrors().get(0).getEntityId());\n     assertEquals(TimelinePutError.FORBIDDEN_RELATION,\n         response.getErrors().get(0).getErrorCode());\n+\n+    if (store instanceof LeveldbTimelineStore) {\n+      LeveldbTimelineStore leveldb = (LeveldbTimelineStore) store;\n+      entities.setEntities(Collections.singletonList(createEntity(\n+          \"OLD_ENTITY_ID_1\", \"OLD_ENTITY_TYPE_1\", 63l, null, null, null, null,\n+          null)));\n+      leveldb.putWithNoDomainId(entities);\n+      entities.setEntities(Collections.singletonList(createEntity(\n+          \"OLD_ENTITY_ID_2\", \"OLD_ENTITY_TYPE_1\", 64l, null, null, null, null,\n+          null)));\n+      leveldb.putWithNoDomainId(entities);\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/4a114dd67aae83e5bb2d65470166de954acf36a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TimelineStoreTestUtils.java",
                "sha": "6f15b9245b8d5c0b0643bbe07729b062f87ab150",
                "status": "modified"
            }
        ],
        "message": "YARN-2819. NPE in ATS Timeline Domains when upgrading from 2.4 to 2.6. Contributed by Zhijie Shen",
        "parent": "https://github.com/apache/hadoop/commit/1e215e8ba2e801eb26f16c307daee756d6b2ca66",
        "repo": "hadoop",
        "unit_tests": [
            "TestLeveldbTimelineStore.java",
            "TestTimelineDataManager.java"
        ]
    },
    "hadoop_4a9b3c6": {
        "bug_id": "hadoop_4a9b3c6",
        "commit": "https://github.com/apache/hadoop/commit/4a9b3c693def87579298fb59b7df0b8892a3508e",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/4a9b3c693def87579298fb59b7df0b8892a3508e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-3077.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-3077.txt?ref=4a9b3c693def87579298fb59b7df0b8892a3508e",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-3077.txt",
                "patch": "@@ -18,3 +18,5 @@ HDFS-3773. TestNNWithQJM fails after HDFS-3741. (atm)\n HDFS-3793. Implement genericized format() in QJM (todd)\n \n HDFS-3795. QJM: validate journal dir at startup (todd)\n+\n+HDFS-3798. Avoid throwing NPE when finalizeSegment() is called on invalid segment (todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/4a9b3c693def87579298fb59b7df0b8892a3508e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-3077.txt",
                "sha": "74e443d74d69e12b59b3ac49b5f769f3a7bd7c3b",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/4a9b3c693def87579298fb59b7df0b8892a3508e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java?ref=4a9b3c693def87579298fb59b7df0b8892a3508e",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
                "patch": "@@ -273,6 +273,11 @@ public synchronized void finalizeLogSegment(RequestInfo reqInfo, long startTxId,\n     }\n     \n     FileJournalManager.EditLogFile elf = fjm.getLogFile(startTxId);\n+    if (elf == null) {\n+      throw new IllegalStateException(\"No log file to finalize at \" +\n+          \"transaction ID \" + startTxId);\n+    }\n+\n     if (elf.isInProgress()) {\n       // TODO: this is slow to validate when in non-recovery cases\n       // we already know the length here!\n@@ -281,7 +286,7 @@ public synchronized void finalizeLogSegment(RequestInfo reqInfo, long startTxId,\n       elf.validateLog();\n       \n       Preconditions.checkState(elf.getLastTxId() == endTxId,\n-          \"Trying to finalize log %s-%s, but current state of log\" +\n+          \"Trying to finalize log %s-%s, but current state of log \" +\n           \"is %s\", startTxId, endTxId, elf);\n       fjm.finalizeLogSegment(startTxId, endTxId);\n     } else {",
                "raw_url": "https://github.com/apache/hadoop/raw/4a9b3c693def87579298fb59b7df0b8892a3508e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/qjournal/server/Journal.java",
                "sha": "cf2c11d885db87a04fb4ddbc59f8db25b34bc1fb",
                "status": "modified"
            },
            {
                "additions": 54,
                "blob_url": "https://github.com/apache/hadoop/blob/4a9b3c693def87579298fb59b7df0b8892a3508e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java",
                "changes": 54,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java?ref=4a9b3c693def87579298fb59b7df0b8892a3508e",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java",
                "patch": "@@ -155,6 +155,60 @@ public void testJournalLocking() throws Exception {\n     journal2.newEpoch(FAKE_NSINFO, 2);\n   }\n   \n+  /**\n+   * Test finalizing a segment after some batch of edits were missed.\n+   * This should fail, since we validate the log before finalization.\n+   */\n+  @Test\n+  public void testFinalizeWhenEditsAreMissed() throws Exception {\n+    journal.newEpoch(FAKE_NSINFO, 1);\n+    journal.startLogSegment(makeRI(1), 1);\n+    journal.journal(makeRI(2), 1, 3,\n+        QJMTestUtil.createTxnData(1, 3));\n+    \n+    // Try to finalize up to txn 6, even though we only wrote up to txn 3.\n+    try {\n+      journal.finalizeLogSegment(makeRI(3), 1, 6);\n+      fail(\"did not fail to finalize\");\n+    } catch (IllegalStateException ise) {\n+      GenericTestUtils.assertExceptionContains(\n+          \"but current state of log is\", ise);\n+    }\n+    \n+    // Check that, even if we re-construct the journal by scanning the\n+    // disk, we don't allow finalizing incorrectly.\n+    journal.close();\n+    journal = new Journal(TEST_LOG_DIR, mockErrorReporter);\n+    \n+    try {\n+      journal.finalizeLogSegment(makeRI(4), 1, 6);\n+      fail(\"did not fail to finalize\");\n+    } catch (IllegalStateException ise) {\n+      GenericTestUtils.assertExceptionContains(\n+          \"but current state of log is\", ise);\n+    }\n+  }\n+  \n+  /**\n+   * Ensure that finalizing a segment which doesn't exist throws the\n+   * appropriate exception.\n+   */\n+  @Test\n+  public void testFinalizeMissingSegment() throws Exception {\n+    journal.newEpoch(FAKE_NSINFO, 1);\n+    try {\n+      journal.finalizeLogSegment(makeRI(1), 1000, 1001);\n+      fail(\"did not fail to finalize\");\n+    } catch (IllegalStateException ise) {\n+      GenericTestUtils.assertExceptionContains(\n+          \"No log file to finalize at transaction ID 1000\", ise);\n+    }\n+  }\n+  \n+  private static RequestInfo makeRI(int serial) {\n+    return new RequestInfo(JID, 1, serial);\n+  }\n+  \n   @Test\n   public void testNamespaceVerification() throws Exception {\n     journal.newEpoch(FAKE_NSINFO, 1);",
                "raw_url": "https://github.com/apache/hadoop/raw/4a9b3c693def87579298fb59b7df0b8892a3508e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/qjournal/server/TestJournal.java",
                "sha": "f9539d9bb61c8db495b076a4b32c7523cd71c5a4",
                "status": "modified"
            }
        ],
        "message": "HDFS-3798. Avoid throwing NPE when finalizeSegment() is called on invalid segment. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-3077@1373179 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/160bfcd6c2e2bc4a4adfa397f0f716430a0832bb",
        "repo": "hadoop",
        "unit_tests": [
            "TestJournal.java"
        ]
    },
    "hadoop_4b4e9d7": {
        "bug_id": "hadoop_4b4e9d7",
        "commit": "https://github.com/apache/hadoop/commit/4b4e9d741e3a62e8e38925c5c4033931708a81c0",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/4b4e9d741e3a62e8e38925c5c4033931708a81c0/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/CHANGES.txt?ref=4b4e9d741e3a62e8e38925c5c4033931708a81c0",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -207,6 +207,9 @@ Trunk (unreleased changes)\n     HADOOP-6549. TestDoAsEffectiveUser should use ip address of the host\n      for superuser ip check(jnp via boryas)\n \n+    HADOOP-6570. RPC#stopProxy throws NPE if getProxyEngine(proxy) returns\n+    null. (hairong)\n+\n Release 0.21.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/4b4e9d741e3a62e8e38925c5c4033931708a81c0/CHANGES.txt",
                "sha": "7354c4cec03c5288df758a25a5ba3d2d7bfb07fd",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/4b4e9d741e3a62e8e38925c5c4033931708a81c0/src/java/org/apache/hadoop/ipc/RPC.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/org/apache/hadoop/ipc/RPC.java?ref=4b4e9d741e3a62e8e38925c5c4033931708a81c0",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/ipc/RPC.java",
                "patch": "@@ -244,8 +244,9 @@ public static Object getProxy(Class protocol, long clientVersion,\n    * @param proxy the proxy to be stopped\n    */\n   public static void stopProxy(Object proxy) {\n-    if (proxy!=null) {\n-      getProxyEngine(proxy).stopProxy(proxy);\n+    RpcEngine rpcEngine;\n+    if (proxy!=null && (rpcEngine = getProxyEngine(proxy)) != null) {\n+      rpcEngine.stopProxy(proxy);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/4b4e9d741e3a62e8e38925c5c4033931708a81c0/src/java/org/apache/hadoop/ipc/RPC.java",
                "sha": "36874c511dcca84e6dfa25fd0967d8e5af2a6cab",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/4b4e9d741e3a62e8e38925c5c4033931708a81c0/src/test/core/org/apache/hadoop/ipc/TestRPC.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/test/core/org/apache/hadoop/ipc/TestRPC.java?ref=4b4e9d741e3a62e8e38925c5c4033931708a81c0",
                "deletions": 0,
                "filename": "src/test/core/org/apache/hadoop/ipc/TestRPC.java",
                "patch": "@@ -39,6 +39,8 @@\n import org.apache.hadoop.security.authorize.Service;\n import org.apache.hadoop.security.authorize.ServiceAuthorizationManager;\n \n+import static org.mockito.Mockito.*;\n+\n /** Unit tests for RPC. */\n public class TestRPC extends TestCase {\n   private static final String ADDRESS = \"0.0.0.0\";\n@@ -392,6 +394,14 @@ public void testNoPings() throws Exception {\n     conf.setBoolean(\"ipc.client.ping\", false);\n     new TestRPC(\"testnoPings\").testCalls(conf);\n   }\n+\n+  /**\n+   * Test stopping a non-registered proxy\n+   * @throws Exception\n+   */\n+  public void testStopNonRegisteredProxy() throws Exception {\n+    RPC.stopProxy(mock(TestProtocol.class));\n+  }\n   \n   public static void main(String[] args) throws Exception {\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/4b4e9d741e3a62e8e38925c5c4033931708a81c0/src/test/core/org/apache/hadoop/ipc/TestRPC.java",
                "sha": "0bb3f8dc5c0b3c0820081e6e849e2e1f18e2add8",
                "status": "modified"
            }
        ],
        "message": "HADOOP-6570. RPC#stopProxy throws NPE if getProxyEngine(proxy) returns null. Contributed by Hairong Kuang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@911134 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/3a7841aeb8fe1bd861cc4e959f20fbbcb0172f30",
        "repo": "hadoop",
        "unit_tests": [
            "TestRPC.java"
        ]
    },
    "hadoop_4b9f044": {
        "bug_id": "hadoop_4b9f044",
        "commit": "https://github.com/apache/hadoop/commit/4b9f0443cb0e35747e0c4ec5f416175b42164a60",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/4b9f0443cb0e35747e0c4ec5f416175b42164a60/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=4b9f0443cb0e35747e0c4ec5f416175b42164a60",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -248,6 +248,9 @@ Release 0.23.6 - UNRELEASED\n     YARN-280. RM does not reject app submission with invalid tokens \n     (Daryn Sharp via tgraves)\n \n+    YARN-225. Proxy Link in RM UI thows NPE in Secure mode \n+    (Devaraj K via bobby)\n+\n Release 0.23.5 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/4b9f0443cb0e35747e0c4ec5f416175b42164a60/hadoop-yarn-project/CHANGES.txt",
                "sha": "ed4f895cfacc5776d9999665c87c150dbc80c7a8",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/4b9f0443cb0e35747e0c4ec5f416175b42164a60/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java?ref=4b9f0443cb0e35747e0c4ec5f416175b42164a60",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java",
                "patch": "@@ -254,11 +254,14 @@ protected void doGet(HttpServletRequest req, HttpServletResponse resp)\n       \n       if(securityEnabled) {\n         String cookieName = getCheckCookieName(id); \n-        for(Cookie c: req.getCookies()) {\n-          if(cookieName.equals(c.getName())) {\n-            userWasWarned = true;\n-            userApproved = userApproved || Boolean.valueOf(c.getValue());\n-            break;\n+        Cookie[] cookies = req.getCookies();\n+        if (cookies != null) {\n+          for (Cookie c : cookies) {\n+            if (cookieName.equals(c.getName())) {\n+              userWasWarned = true;\n+              userApproved = userApproved || Boolean.valueOf(c.getValue());\n+              break;\n+            }\n           }\n         }\n       }",
                "raw_url": "https://github.com/apache/hadoop/raw/4b9f0443cb0e35747e0c4ec5f416175b42164a60/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java",
                "sha": "7f6bba14e98e09d7634d9d6cc712868c191d126a",
                "status": "modified"
            }
        ],
        "message": "YARN-225. Proxy Link in RM UI thows NPE in Secure mode (Devaraj K via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1426515 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/0fa9c7a825f444d50c89b986bacea7a547e4ab8b",
        "repo": "hadoop",
        "unit_tests": [
            "TestWebAppProxyServlet.java"
        ]
    },
    "hadoop_4c7a6c6": {
        "bug_id": "hadoop_4c7a6c6",
        "commit": "https://github.com/apache/hadoop/commit/4c7a6c6c3f5879a2f79080753074f078943f8392",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/4c7a6c6c3f5879a2f79080753074f078943f8392/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=4c7a6c6c3f5879a2f79080753074f078943f8392",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -3681,7 +3681,8 @@ public boolean isInStartupSafeMode() {\n \n   @Override\n   public boolean isPopulatingReplQueues() {\n-    if (!haContext.getState().shouldPopulateReplQueues()) {\n+    if (haContext != null && // null during startup!\n+        !haContext.getState().shouldPopulateReplQueues()) {\n       return false;\n     }\n     // safeMode is volatile, and may be set to null at any time",
                "raw_url": "https://github.com/apache/hadoop/raw/4c7a6c6c3f5879a2f79080753074f078943f8392/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "258cb53186ad15632cf9899da3f0304a189f942c",
                "status": "modified"
            }
        ],
        "message": "Amend HDFS-2795. Fix PersistBlocks failure due to an NPE in isPopulatingReplQueues()\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1232510 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/0c1450ca5d922b5bf713bb8bb17459dc11a97330",
        "repo": "hadoop",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_4cf94aa": {
        "bug_id": "hadoop_4cf94aa",
        "commit": "https://github.com/apache/hadoop/commit/4cf94aaf809c77b3b7dc925faa39a72d53e4246e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/4cf94aaf809c77b3b7dc925faa39a72d53e4246e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=4cf94aaf809c77b3b7dc925faa39a72d53e4246e",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -656,6 +656,9 @@ Release 2.5.0 - UNRELEASED\n     HDFS-6552. add DN storage to a BlockInfo will not replace the different\n     storage from same DN. (Amir Langer via Arpit Agarwal)\n \n+    HDFS-6551. Rename with OVERWRITE option may throw NPE when the target\n+    file/directory is a reference INode. (jing9)\n+\n   BREAKDOWN OF HDFS-2006 SUBTASKS AND RELATED JIRAS\n \n     HDFS-6299. Protobuf for XAttr and client-side implementation. (Yi Liu via umamahesh)",
                "raw_url": "https://github.com/apache/hadoop/raw/4cf94aaf809c77b3b7dc925faa39a72d53e4246e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "f0a84bd7d224030b1d5c5518bd0a997e33a2152d",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/4cf94aaf809c77b3b7dc925faa39a72d53e4246e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java?ref=4cf94aaf809c77b3b7dc925faa39a72d53e4246e",
                "deletions": 9,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "patch": "@@ -44,7 +44,6 @@\n import org.apache.hadoop.fs.XAttrSetFlag;\n import org.apache.hadoop.fs.permission.AclEntry;\n import org.apache.hadoop.fs.permission.AclStatus;\n-import org.apache.hadoop.fs.permission.FsAction;\n import org.apache.hadoop.fs.permission.FsPermission;\n import org.apache.hadoop.fs.permission.PermissionStatus;\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n@@ -891,9 +890,10 @@ boolean unprotectedRenameTo(String src, String dst, long timestamp,\n     \n     boolean undoRemoveDst = false;\n     INode removedDst = null;\n+    long removedNum = 0;\n     try {\n       if (dstInode != null) { // dst exists remove it\n-        if (removeLastINode(dstIIP) != -1) {\n+        if ((removedNum = removeLastINode(dstIIP)) != -1) {\n           removedDst = dstIIP.getLastINode();\n           undoRemoveDst = true;\n         }\n@@ -933,13 +933,15 @@ boolean unprotectedRenameTo(String src, String dst, long timestamp,\n         long filesDeleted = -1;\n         if (removedDst != null) {\n           undoRemoveDst = false;\n-          BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();\n-          List<INode> removedINodes = new ChunkedArrayList<INode>();\n-          filesDeleted = removedDst.cleanSubtree(Snapshot.CURRENT_STATE_ID,\n-              dstIIP.getLatestSnapshotId(), collectedBlocks, removedINodes, true)\n-              .get(Quota.NAMESPACE);\n-          getFSNamesystem().removePathAndBlocks(src, collectedBlocks,\n-              removedINodes);\n+          if (removedNum > 0) {\n+            BlocksMapUpdateInfo collectedBlocks = new BlocksMapUpdateInfo();\n+            List<INode> removedINodes = new ChunkedArrayList<INode>();\n+            filesDeleted = removedDst.cleanSubtree(Snapshot.CURRENT_STATE_ID,\n+                dstIIP.getLatestSnapshotId(), collectedBlocks, removedINodes,\n+                true).get(Quota.NAMESPACE);\n+            getFSNamesystem().removePathAndBlocks(src, collectedBlocks,\n+                removedINodes);\n+          }\n         }\n \n         if (snapshottableDirs.size() > 0) {",
                "raw_url": "https://github.com/apache/hadoop/raw/4cf94aaf809c77b3b7dc925faa39a72d53e4246e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "sha": "dc553ab73b96d1c636cdf5fb23a33ebf4ccfec8c",
                "status": "modified"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/hadoop/blob/4cf94aaf809c77b3b7dc925faa39a72d53e4246e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java?ref=4cf94aaf809c77b3b7dc925faa39a72d53e4246e",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java",
                "patch": "@@ -171,8 +171,6 @@ public void testRenameFromSDir2NonSDir() throws Exception {\n   private static boolean existsInDiffReport(List<DiffReportEntry> entries,\n       DiffType type, String relativePath) {\n     for (DiffReportEntry entry : entries) {\n-      System.out.println(\"DiffEntry is:\" + entry.getType() + \"\\\"\"\n-          + new String(entry.getRelativePath()) + \"\\\"\");\n       if ((entry.getType() == type)\n           && ((new String(entry.getRelativePath())).compareTo(relativePath) == 0)) {\n         return true;\n@@ -2374,4 +2372,46 @@ public void testAppendFileAfterRenameInSnapshot() throws Exception {\n     // save namespace and restart\n     restartClusterAndCheckImage(true);\n   }\n+\n+  @Test\n+  public void testRenameWithOverWrite() throws Exception {\n+    final Path root = new Path(\"/\");\n+    final Path foo = new Path(root, \"foo\");\n+    final Path file1InFoo = new Path(foo, \"file1\");\n+    final Path file2InFoo = new Path(foo, \"file2\");\n+    final Path file3InFoo = new Path(foo, \"file3\");\n+    DFSTestUtil.createFile(hdfs, file1InFoo, 1L, REPL, SEED);\n+    DFSTestUtil.createFile(hdfs, file2InFoo, 1L, REPL, SEED);\n+    DFSTestUtil.createFile(hdfs, file3InFoo, 1L, REPL, SEED);\n+    final Path bar = new Path(root, \"bar\");\n+    hdfs.mkdirs(bar);\n+\n+    SnapshotTestHelper.createSnapshot(hdfs, root, \"s0\");\n+    // move file1 from foo to bar\n+    final Path fileInBar = new Path(bar, \"file1\");\n+    hdfs.rename(file1InFoo, fileInBar);\n+    // rename bar to newDir\n+    final Path newDir = new Path(root, \"newDir\");\n+    hdfs.rename(bar, newDir);\n+    // move file2 from foo to newDir\n+    final Path file2InNewDir = new Path(newDir, \"file2\");\n+    hdfs.rename(file2InFoo, file2InNewDir);\n+    // move file3 from foo to newDir and rename it to file1, this will overwrite\n+    // the original file1\n+    final Path file1InNewDir = new Path(newDir, \"file1\");\n+    hdfs.rename(file3InFoo, file1InNewDir, Rename.OVERWRITE);\n+    SnapshotTestHelper.createSnapshot(hdfs, root, \"s1\");\n+\n+    SnapshotDiffReport report = hdfs.getSnapshotDiffReport(root, \"s0\", \"s1\");\n+    LOG.info(\"DiffList is \\n\\\"\" + report.toString() + \"\\\"\");\n+    List<DiffReportEntry> entries = report.getDiffList();\n+    assertEquals(7, entries.size());\n+    assertTrue(existsInDiffReport(entries, DiffType.MODIFY, \"\"));\n+    assertTrue(existsInDiffReport(entries, DiffType.MODIFY, foo.getName()));\n+    assertTrue(existsInDiffReport(entries, DiffType.DELETE, bar.getName()));\n+    assertTrue(existsInDiffReport(entries, DiffType.CREATE, newDir.getName()));\n+    assertTrue(existsInDiffReport(entries, DiffType.DELETE, \"foo/file1\"));\n+    assertTrue(existsInDiffReport(entries, DiffType.DELETE, \"foo/file2\"));\n+    assertTrue(existsInDiffReport(entries, DiffType.DELETE, \"foo/file3\"));\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/4cf94aaf809c77b3b7dc925faa39a72d53e4246e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/snapshot/TestRenameWithSnapshots.java",
                "sha": "c88aaf2410e8b34fd0303b6546aeb9249a2e51b5",
                "status": "modified"
            }
        ],
        "message": "HDFS-6551. Rename with OVERWRITE option may throw NPE when the target file/directory is a reference INode. Contributed by Jing Zhao.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1603612 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/52d18aa217a308e8343ca8b23b5a2dedda77270f",
        "repo": "hadoop",
        "unit_tests": [
            "TestFSDirectory.java"
        ]
    },
    "hadoop_4d779e0": {
        "bug_id": "hadoop_4d779e0",
        "commit": "https://github.com/apache/hadoop/commit/4d779e088a30f958c9788366e0e251476cb18410",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/4d779e088a30f958c9788366e0e251476cb18410/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=4d779e088a30f958c9788366e0e251476cb18410",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -149,3 +149,5 @@ HDFS-2845. SBN should not allow browsing of the file system via web UI. (Bikas S\n HDFS-2742. HA: observed dataloss in replication stress test. (todd via eli)\n \n HDFS-2870. Fix log level for block debug info in processMisReplicatedBlocks (todd)\n+\n+HDFS-2859. LOCAL_ADDRESS_MATCHER.match has NPE when called from DFSUtil.getSuffixIDs when the host is incorrect (Bikas Saha via todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/4d779e088a30f958c9788366e0e251476cb18410/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "7a4ef27f19513e6bb421cf8ad4d79f49e29b0397",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/4d779e088a30f958c9788366e0e251476cb18410/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java?ref=4d779e088a30f958c9788366e0e251476cb18410",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "patch": "@@ -61,6 +61,8 @@\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.net.NodeBase;\n import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n \n import com.google.common.base.Joiner;\n import com.google.common.collect.Lists;\n@@ -69,6 +71,8 @@\n \n @InterfaceAudience.Private\n public class DFSUtil {\n+  private static final Log LOG = LogFactory.getLog(DFSUtil.class.getName());\n+  \n   private DFSUtil() { /* Hidden constructor */ }\n   private static final ThreadLocal<Random> RANDOM = new ThreadLocal<Random>() {\n     @Override\n@@ -935,9 +939,10 @@ private static String getNameServiceId(Configuration conf, String addressKey) {\n         try {\n           s = NetUtils.createSocketAddr(addr);\n         } catch (Exception e) {\n+          LOG.warn(\"Exception in creating socket address\", e);\n           continue;\n         }\n-        if (matcher.match(s)) {\n+        if (!s.isUnresolved() && matcher.match(s)) {\n           nameserviceId = nsId;\n           namenodeId = nnId;\n           found++;",
                "raw_url": "https://github.com/apache/hadoop/raw/4d779e088a30f958c9788366e0e251476cb18410/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "sha": "c9ccf9f38c7cafd26872165b4890710acc96be21",
                "status": "modified"
            }
        ],
        "message": "HDFS-2859. LOCAL_ADDRESS_MATCHER.match has NPE when called from DFSUtil.getSuffixIDs when the host is incorrect. Contributed by Bikas Saha.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1239356 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/048c416beb42ad27cf0e82b144da1d99e50c62b1",
        "repo": "hadoop",
        "unit_tests": [
            "TestDFSUtil.java"
        ]
    },
    "hadoop_50bd067": {
        "bug_id": "hadoop_50bd067",
        "commit": "https://github.com/apache/hadoop/commit/50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416/hadoop-yarn-project/CHANGES.txt",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -1138,6 +1138,9 @@ Release 2.8.0 - UNRELEASED\n     YARN-4440. FSAppAttempt#getAllowedLocalityLevelByTime should init the\n     lastScheduler time. (Lin Yiqun via zxu)\n \n+    YARN-4452. NPE when submit Unmanaged application. (Naganarasimha G R\n+    via junping_du)\n+\n Release 2.7.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES\n@@ -1186,6 +1189,9 @@ Release 2.7.3 - UNRELEASED\n \n     YARN-4439. Clarify NMContainerStatus#toString method. (Jian He via xgong)\n \n+    YARN-4452. NPE when submit Unmanaged application. (Naganarasimha G R via\n+    junping_du)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES\n@@ -2040,6 +2046,9 @@ Release 2.6.4 - UNRELEASED\n   YARN-3535. Scheduler must re-request container resources when RMContainer transitions\n   from ALLOCATED to KILLED (rohithsharma and peng.zhang via asuresh)\n \n+  YARN-4452. NPE when submit Unmanaged application. (Naganarasimha G R\n+  via junping_du)\n+\n Release 2.6.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416/hadoop-yarn-project/CHANGES.txt",
                "sha": "9b2397fdd95293f4847a61fbc7277b660ccf6033",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java?ref=50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416",
                "deletions": 8,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "patch": "@@ -160,14 +160,16 @@ public void appACLsUpdated(RMApp app, String appViewACLs,\n   public void appAttemptRegistered(RMAppAttempt appAttempt,\n       long registeredTime) {\n     if (publishSystemMetrics) {\n+      ContainerId container = (appAttempt.getMasterContainer() == null) ? null\n+          : appAttempt.getMasterContainer().getId();\n       dispatcher.getEventHandler().handle(\n           new AppAttemptRegisteredEvent(\n               appAttempt.getAppAttemptId(),\n               appAttempt.getHost(),\n               appAttempt.getRpcPort(),\n               appAttempt.getTrackingUrl(),\n               appAttempt.getOriginalTrackingUrl(),\n-              appAttempt.getMasterContainer().getId(),\n+              container,\n               registeredTime));\n     }\n   }\n@@ -176,6 +178,8 @@ public void appAttemptRegistered(RMAppAttempt appAttempt,\n   public void appAttemptFinished(RMAppAttempt appAttempt,\n       RMAppAttemptState appAttemtpState, RMApp app, long finishedTime) {\n     if (publishSystemMetrics) {\n+      ContainerId container = (appAttempt.getMasterContainer() == null) ? null\n+          : appAttempt.getMasterContainer().getId();\n       dispatcher.getEventHandler().handle(\n           new AppAttemptFinishedEvent(\n               appAttempt.getAppAttemptId(),\n@@ -187,7 +191,7 @@ public void appAttemptFinished(RMAppAttempt appAttempt,\n               app.getFinalApplicationStatus(),\n               RMServerUtils.createApplicationAttemptState(appAttemtpState),\n               finishedTime,\n-              appAttempt.getMasterContainer().getId()));\n+              container));\n     }\n   }\n \n@@ -390,9 +394,10 @@ private static TimelineEntity createApplicationEntity(\n         event.getHost());\n     eventInfo.put(AppAttemptMetricsConstants.RPC_PORT_EVENT_INFO,\n         event.getRpcPort());\n-    eventInfo.put(\n-        AppAttemptMetricsConstants.MASTER_CONTAINER_EVENT_INFO,\n-        event.getMasterContainerId().toString());\n+    if (event.getMasterContainerId() != null) {\n+      eventInfo.put(AppAttemptMetricsConstants.MASTER_CONTAINER_EVENT_INFO,\n+          event.getMasterContainerId().toString());\n+    }\n     tEvent.setEventInfo(eventInfo);\n     entity.addEvent(tEvent);\n     putEntity(entity);\n@@ -417,9 +422,10 @@ private void publishAppAttemptFinishedEvent(AppAttemptFinishedEvent event) {\n         event.getFinalApplicationStatus().toString());\n     eventInfo.put(AppAttemptMetricsConstants.STATE_EVENT_INFO,\n         event.getYarnApplicationAttemptState().toString());\n-    eventInfo.put(\n-        AppAttemptMetricsConstants.MASTER_CONTAINER_EVENT_INFO,\n-        event.getMasterContainerId().toString());\n+    if (event.getMasterContainerId() != null) {\n+      eventInfo.put(AppAttemptMetricsConstants.MASTER_CONTAINER_EVENT_INFO,\n+          event.getMasterContainerId().toString());\n+    }\n     tEvent.setEventInfo(eventInfo);\n     entity.addEvent(tEvent);\n     putEntity(entity);",
                "raw_url": "https://github.com/apache/hadoop/raw/50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "sha": "f240660da4695bf3cbce6b7fb7cd07fd05f9a67f",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hadoop/blob/50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java?ref=50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "patch": "@@ -256,11 +256,31 @@ public void testPublishApplicationMetrics() throws Exception {\n     }\n   }\n \n+  @Test(timeout = 10000)\n+  public void testPublishAppAttemptMetricsForUnmanagedAM() throws Exception {\n+    ApplicationAttemptId appAttemptId =\n+        ApplicationAttemptId.newInstance(ApplicationId.newInstance(0, 1), 1);\n+    RMAppAttempt appAttempt = createRMAppAttempt(appAttemptId,true);\n+    metricsPublisher.appAttemptRegistered(appAttempt, Integer.MAX_VALUE + 1L);\n+    RMApp app = mock(RMApp.class);\n+    when(app.getFinalApplicationStatus()).thenReturn(FinalApplicationStatus.UNDEFINED);\n+    metricsPublisher.appAttemptFinished(appAttempt, RMAppAttemptState.FINISHED, app,\n+        Integer.MAX_VALUE + 2L);\n+    TimelineEntity entity = null;\n+    do {\n+      entity =\n+          store.getEntity(appAttemptId.toString(),\n+              AppAttemptMetricsConstants.ENTITY_TYPE,\n+              EnumSet.allOf(Field.class));\n+      // ensure two events are both published before leaving the loop\n+    } while (entity == null || entity.getEvents().size() < 2);\n+  }\n+\n   @Test(timeout = 10000)\n   public void testPublishAppAttemptMetrics() throws Exception {\n     ApplicationAttemptId appAttemptId =\n         ApplicationAttemptId.newInstance(ApplicationId.newInstance(0, 1), 1);\n-    RMAppAttempt appAttempt = createRMAppAttempt(appAttemptId);\n+    RMAppAttempt appAttempt = createRMAppAttempt(appAttemptId, false);\n     metricsPublisher.appAttemptRegistered(appAttempt, Integer.MAX_VALUE + 1L);\n     RMApp app = mock(RMApp.class);\n     when(app.getFinalApplicationStatus()).thenReturn(FinalApplicationStatus.UNDEFINED);\n@@ -435,15 +455,17 @@ private static RMApp createRMApp(ApplicationId appId) {\n   }\n \n   private static RMAppAttempt createRMAppAttempt(\n-      ApplicationAttemptId appAttemptId) {\n+      ApplicationAttemptId appAttemptId, boolean unmanagedAMAttempt) {\n     RMAppAttempt appAttempt = mock(RMAppAttempt.class);\n     when(appAttempt.getAppAttemptId()).thenReturn(appAttemptId);\n     when(appAttempt.getHost()).thenReturn(\"test host\");\n     when(appAttempt.getRpcPort()).thenReturn(-100);\n-    Container container = mock(Container.class);\n-    when(container.getId())\n-        .thenReturn(ContainerId.newContainerId(appAttemptId, 1));\n-    when(appAttempt.getMasterContainer()).thenReturn(container);\n+    if (!unmanagedAMAttempt) {\n+      Container container = mock(Container.class);\n+      when(container.getId())\n+          .thenReturn(ContainerId.newContainerId(appAttemptId, 1));\n+      when(appAttempt.getMasterContainer()).thenReturn(container);\n+    }\n     when(appAttempt.getDiagnostics()).thenReturn(\"test diagnostics info\");\n     when(appAttempt.getTrackingUrl()).thenReturn(\"test tracking url\");\n     when(appAttempt.getOriginalTrackingUrl()).thenReturn(",
                "raw_url": "https://github.com/apache/hadoop/raw/50bd067e1d63d4c80dc1e7bf4024bfaf42cf4416/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "sha": "f2b02bcfad78b495f075a27b422e0e64178fec37",
                "status": "modified"
            }
        ],
        "message": "YARN-4452. NPE when submit Unmanaged application. Contributed by Naganarasimha G R.",
        "parent": "https://github.com/apache/hadoop/commit/607473e1d047ccd2a2c3804ae94e04f133af9cc2",
        "repo": "hadoop",
        "unit_tests": [
            "TestSystemMetricsPublisher.java"
        ]
    },
    "hadoop_52c1f27": {
        "bug_id": "hadoop_52c1f27",
        "commit": "https://github.com/apache/hadoop/commit/52c1f272ecb6c29c81898a1ff50d03a1296df1f7",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/52c1f272ecb6c29c81898a1ff50d03a1296df1f7/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=52c1f272ecb6c29c81898a1ff50d03a1296df1f7",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -1084,6 +1084,9 @@ Release 2.8.0 - UNRELEASED\n     HADOOP-12386. RetryPolicies.RETRY_FOREVER should be able to specify a \n     retry interval. (Sunil G via wangda)\n \n+    HADOOP-12252. LocalDirAllocator should not throw NPE with empty string\n+    configuration. (Zhihai Xu)\n+\n   OPTIMIZATIONS\n \n     HADOOP-12051. ProtobufRpcEngine.invoke() should use Exception.toString()",
                "raw_url": "https://github.com/apache/hadoop/raw/52c1f272ecb6c29c81898a1ff50d03a1296df1f7/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "d5ce38b450444de4b3745a3671d05acd591327b7",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/52c1f272ecb6c29c81898a1ff50d03a1296df1f7/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java?ref=52c1f272ecb6c29c81898a1ff50d03a1296df1f7",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java",
                "patch": "@@ -250,9 +250,9 @@ int getCurrentDirectoryIndex() {\n     private int dirNumLastAccessed;\n     private Random dirIndexRandomizer = new Random();\n     private FileSystem localFS;\n-    private DF[] dirDF;\n+    private DF[] dirDF = new DF[0];\n     private String contextCfgItemName;\n-    private String[] localDirs;\n+    private String[] localDirs = new String[0];\n     private String savedLocalDirs = \"\";\n \n     public AllocatorPerContext(String contextCfgItemName) {",
                "raw_url": "https://github.com/apache/hadoop/raw/52c1f272ecb6c29c81898a1ff50d03a1296df1f7/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java",
                "sha": "ccea6e5a2bebfa412f17780735801f2f1b168f01",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hadoop/blob/52c1f272ecb6c29c81898a1ff50d03a1296df1f7/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java?ref=52c1f272ecb6c29c81898a1ff50d03a1296df1f7",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java",
                "patch": "@@ -26,6 +26,7 @@\n import java.util.NoSuchElementException;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.util.DiskChecker.DiskErrorException;\n import org.apache.hadoop.util.Shell;\n \n import org.junit.runner.RunWith;\n@@ -312,7 +313,30 @@ public void testShouldNotthrowNPE() throws Exception {\n     } catch (IOException e) {\n       assertEquals(CONTEXT + \" not configured\", e.getMessage());\n     } catch (NullPointerException e) {\n-      fail(\"Lack of configuration should not have thrown an NPE.\");\n+      fail(\"Lack of configuration should not have thrown a NPE.\");\n+    }\n+\n+    String  NEW_CONTEXT = CONTEXT + \".new\";\n+    conf1.set(NEW_CONTEXT, \"\");\n+    LocalDirAllocator newDirAllocator = new LocalDirAllocator(NEW_CONTEXT);\n+    try {\n+      newDirAllocator.getLocalPathForWrite(\"/test\", conf1);\n+      fail(\"Exception not thrown when \" + NEW_CONTEXT +\n+          \" is set to empty string\");\n+    } catch (IOException e) {\n+      assertTrue(e instanceof DiskErrorException);\n+    } catch (NullPointerException e) {\n+      fail(\"Wrong configuration should not have thrown a NPE.\");\n+    }\n+\n+    try {\n+      newDirAllocator.getLocalPathToRead(\"/test\", conf1);\n+      fail(\"Exception not thrown when \" + NEW_CONTEXT +\n+          \" is set to empty string\");\n+    } catch (IOException e) {\n+      assertTrue(e instanceof DiskErrorException);\n+    } catch (NullPointerException e) {\n+      fail(\"Wrong configuration should not have thrown a NPE.\");\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/52c1f272ecb6c29c81898a1ff50d03a1296df1f7/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java",
                "sha": "2e311744f6a5433c4b3dc12a712d69222effbd14",
                "status": "modified"
            }
        ],
        "message": "HADOOP-12252. LocalDirAllocator should not throw NPE with empty string configuration. Contributed by Zhihai Xu",
        "parent": "https://github.com/apache/hadoop/commit/df31c446bfa628bee9fab88addcfec5a13edda30",
        "repo": "hadoop",
        "unit_tests": [
            "TestLocalDirAllocator.java"
        ]
    },
    "hadoop_5409908": {
        "bug_id": "hadoop_5409908",
        "commit": "https://github.com/apache/hadoop/commit/5409908026dd791cce62a7d71ae56c92a70a9753",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/5409908026dd791cce62a7d71ae56c92a70a9753/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt?ref=5409908026dd791cce62a7d71ae56c92a70a9753",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "patch": "@@ -120,3 +120,6 @@ HDFS-5535 subtasks:\n \n     HDFS-6029. Secondary NN fails to checkpoint after -rollingUpgrade prepare.\n     (jing9)\n+\n+    HDFS-6032. -rollingUpgrade query hits NPE after the NN restarts. (Haohui Mai\n+    via jing9)",
                "raw_url": "https://github.com/apache/hadoop/raw/5409908026dd791cce62a7d71ae56c92a70a9753/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-5535.txt",
                "sha": "002afcd44b67682655424e7e1ddb1daae4b83cab",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/5409908026dd791cce62a7d71ae56c92a70a9753/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java?ref=5409908026dd791cce62a7d71ae56c92a70a9753",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java",
                "patch": "@@ -881,9 +881,12 @@ private void loadFSImage(File imageFile, FSNamesystem target,\n    */\n   private void loadFSImage(File curFile, MD5Hash expectedMd5,\n       FSNamesystem target, MetaRecoveryContext recovery) throws IOException {\n+    // BlockPoolId is required when the FsImageLoader loads the rolling upgrade\n+    // information. Make sure the ID is properly set.\n+    target.setBlockPoolId(this.getBlockPoolID());\n+\n     FSImageFormat.LoaderDelegator loader = FSImageFormat.newLoader(conf, target);\n     loader.load(curFile);\n-    target.setBlockPoolId(this.getBlockPoolID());\n \n     // Check that the image digest we loaded matches up with what\n     // we expected",
                "raw_url": "https://github.com/apache/hadoop/raw/5409908026dd791cce62a7d71ae56c92a70a9753/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSImage.java",
                "sha": "e79b6c9224665352c3ec945cab6ce37ad4b58b39",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop/blob/5409908026dd791cce62a7d71ae56c92a70a9753/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java?ref=5409908026dd791cce62a7d71ae56c92a70a9753",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java",
                "patch": "@@ -432,6 +432,32 @@ public void testQuery() throws Exception {\n     }\n   }\n \n+  @Test (timeout = 300000)\n+  public void testQueryAfterRestart() throws IOException, InterruptedException {\n+    final Configuration conf = new Configuration();\n+    MiniDFSCluster cluster = null;\n+    try {\n+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(0).build();\n+      cluster.waitActive();\n+      DistributedFileSystem dfs = cluster.getFileSystem();\n+\n+      dfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n+      // start rolling upgrade\n+      dfs.rollingUpgrade(RollingUpgradeAction.PREPARE);\n+      queryForPreparation(dfs);\n+      dfs.setSafeMode(SafeModeAction.SAFEMODE_ENTER);\n+      dfs.saveNamespace();\n+      dfs.setSafeMode(SafeModeAction.SAFEMODE_LEAVE);\n+\n+      cluster.restartNameNodes();\n+      dfs.rollingUpgrade(RollingUpgradeAction.QUERY);\n+    } finally {\n+      if (cluster != null) {\n+        cluster.shutdown();\n+      }\n+    }\n+  }\n+\n   @Test(timeout = 300000)\n   public void testCheckpoint() throws IOException, InterruptedException {\n     final Configuration conf = new Configuration();",
                "raw_url": "https://github.com/apache/hadoop/raw/5409908026dd791cce62a7d71ae56c92a70a9753/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestRollingUpgrade.java",
                "sha": "321f8843d0f6f9091f76a43a50ea0d8ff9405171",
                "status": "modified"
            }
        ],
        "message": "HDFS-6032. -rollingUpgrade query hits NPE after the NN restarts. Contributed by Haohui Mai.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-5535@1572801 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/e9a17c8ce0656a4e5d47401ca22a575c5f5f66db",
        "repo": "hadoop",
        "unit_tests": [
            "TestFSImage.java"
        ]
    },
    "hadoop_5459b24": {
        "bug_id": "hadoop_5459b24",
        "commit": "https://github.com/apache/hadoop/commit/5459b241c86cc9a26fecca9a06ceaf524e48fed4",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/5459b241c86cc9a26fecca9a06ceaf524e48fed4/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=5459b241c86cc9a26fecca9a06ceaf524e48fed4",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -520,6 +520,8 @@ Release 2.8.0 - UNRELEASED\n     HDFS-7863. Missing description of some methods and parameters in javadoc of\n     FSDirDeleteOp. (Brahma Reddy Battula via ozawa)\n \n+    HDFS-8043. NPE in MiniDFSCluster teardown. (Brahma Reddy Battula via ozawa)\n+\n Release 2.7.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/5459b241c86cc9a26fecca9a06ceaf524e48fed4/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "30a460fb1e27843d3ce92cb47e4cbc94703bfa3b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/5459b241c86cc9a26fecca9a06ceaf524e48fed4/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java?ref=5459b241c86cc9a26fecca9a06ceaf524e48fed4",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java",
                "patch": "@@ -1744,11 +1744,14 @@ public void shutdown(boolean deleteDfsDir, boolean closeFileSystem) {\n         nameNode = null;\n       }\n     }\n-    if (deleteDfsDir) {\n+    if (base_dir != null) {\n+      if (deleteDfsDir) {\n         base_dir.delete();\n-    } else {\n+      } else {\n         base_dir.deleteOnExit();\n+      }\n     }\n+\n   }\n   \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/5459b241c86cc9a26fecca9a06ceaf524e48fed4/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/MiniDFSCluster.java",
                "sha": "d3eaa6ef6c1831a4a6bb6f172fce9007845329b8",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/5459b241c86cc9a26fecca9a06ceaf524e48fed4/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientFailover.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientFailover.java?ref=5459b241c86cc9a26fecca9a06ceaf524e48fed4",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientFailover.java",
                "patch": "@@ -85,7 +85,9 @@ public void setUpCluster() throws IOException {\n   \n   @After\n   public void tearDownCluster() throws IOException {\n-    cluster.shutdown();\n+    if (cluster != null) {\n+      cluster.shutdown();\n+    }\n   }\n \n   @After",
                "raw_url": "https://github.com/apache/hadoop/raw/5459b241c86cc9a26fecca9a06ceaf524e48fed4/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientFailover.java",
                "sha": "644d66d6ac97d1eb08b1359ed02cad1f7fef6644",
                "status": "modified"
            }
        ],
        "message": "HDFS-8043. NPE in MiniDFSCluster teardown. Contributed by Brahma Reddy Battula.",
        "parent": "https://github.com/apache/hadoop/commit/5112477d9e1f1ebc7d91757924c4bdc6eabc35a9",
        "repo": "hadoop",
        "unit_tests": [
            "TestMiniDFSCluster.java"
        ]
    },
    "hadoop_56996a6": {
        "bug_id": "hadoop_56996a6",
        "commit": "https://github.com/apache/hadoop/commit/56996a685e6201cb186cea866d22418289174574",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/56996a685e6201cb186cea866d22418289174574/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=56996a685e6201cb186cea866d22418289174574",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -752,6 +752,9 @@ Release 2.8.0 - UNRELEASED\n     HADOOP-11927.  Fix \"undefined reference to dlopen\" error when compiling\n     libhadooppipes (Xianyin Xin via Colin P. McCabe)\n \n+    HADOOP-8751. NPE in Token.toString() when Token is constructed using null\n+    identifier. (kanaka kumar avvaru via aajisaka)\n+\n Release 2.7.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/56996a685e6201cb186cea866d22418289174574/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "864865fb40caa53f32c5cf51e33e3d1c62cd7c48",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/56996a685e6201cb186cea866d22418289174574/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java?ref=56996a685e6201cb186cea866d22418289174574",
                "deletions": 4,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java",
                "patch": "@@ -70,10 +70,10 @@ public Token(T id, SecretManager<T> mgr) {\n    * @param service the service for this token\n    */\n   public Token(byte[] identifier, byte[] password, Text kind, Text service) {\n-    this.identifier = identifier;\n-    this.password = password;\n-    this.kind = kind;\n-    this.service = service;\n+    this.identifier = (identifier == null)? new byte[0] : identifier;\n+    this.password = (password == null)? new byte[0] : password;\n+    this.kind = (kind == null)? new Text() : kind;\n+    this.service = (service == null)? new Text() : service;\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/56996a685e6201cb186cea866d22418289174574/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java",
                "sha": "bd254e6d8d09535a7fb5bd0ee504643e85ae4547",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/56996a685e6201cb186cea866d22418289174574/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/delegation/TestDelegationToken.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/delegation/TestDelegationToken.java?ref=56996a685e6201cb186cea866d22418289174574",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/delegation/TestDelegationToken.java",
                "patch": "@@ -44,6 +44,7 @@\n import org.apache.hadoop.security.token.SecretManager;\n import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n+import org.apache.hadoop.security.token.TokenIdentifier;\n import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager.DelegationTokenInformation;\n import org.apache.hadoop.util.Daemon;\n import org.apache.hadoop.util.Time;\n@@ -539,4 +540,18 @@ public void testDelegationKeyEqualAndHash() {\n     Assert.assertEquals(key1, key2);\n     Assert.assertFalse(key2.equals(key3));\n   }\n+\n+  @Test\n+  public void testEmptyToken() throws IOException {\n+    Token<?> token1 = new Token<TokenIdentifier>();\n+\n+    Token<?> token2 = new Token<TokenIdentifier>(new byte[0], new byte[0],\n+        new Text(), new Text());\n+    assertEquals(token1, token2);\n+    assertEquals(token1.encodeToUrlString(), token2.encodeToUrlString());\n+\n+    token2 = new Token<TokenIdentifier>(null, null, null, null);\n+    assertEquals(token1, token2);\n+    assertEquals(token1.encodeToUrlString(), token2.encodeToUrlString());\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/56996a685e6201cb186cea866d22418289174574/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/delegation/TestDelegationToken.java",
                "sha": "b41ff15251050a26a9108b4b59ab4248076da619",
                "status": "modified"
            }
        ],
        "message": "HADOOP-8751. NPE in Token.toString() when Token is constructed using null identifier. Contributed by kanaka kumar avvaru.",
        "parent": "https://github.com/apache/hadoop/commit/39077dba2e877420e7470df253f6154f6ecc64ec",
        "repo": "hadoop",
        "unit_tests": [
            "TestToken.java"
        ]
    },
    "hadoop_585ebd8": {
        "bug_id": "hadoop_585ebd8",
        "commit": "https://github.com/apache/hadoop/commit/585ebd873a55bedd2a364d256837f08ada8ba032",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/585ebd873a55bedd2a364d256837f08ada8ba032/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalResourcesTrackerImpl.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalResourcesTrackerImpl.java?ref=585ebd873a55bedd2a364d256837f08ada8ba032",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalResourcesTrackerImpl.java",
                "patch": "@@ -500,6 +500,11 @@ public Path getPathForLocalization(LocalResourceRequest req,\n \n     Path localPath = new Path(rPath, req.getPath().getName());\n     LocalizedResource rsrc = localrsrc.get(req);\n+    if (rsrc == null) {\n+      LOG.warn(\"Resource \" + req + \" has been removed\"\n+          + \" and will no longer be localized\");\n+      return null;\n+    }\n     rsrc.setLocalPath(localPath);\n     LocalResource lr = LocalResource.newInstance(req.getResource(),\n         req.getType(), req.getVisibility(), req.getSize(),",
                "raw_url": "https://github.com/apache/hadoop/raw/585ebd873a55bedd2a364d256837f08ada8ba032/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalResourcesTrackerImpl.java",
                "sha": "ad24c62828fea4818e0f0f1aec2f306372ca7891",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/585ebd873a55bedd2a364d256837f08ada8ba032/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java?ref=585ebd873a55bedd2a364d256837f08ada8ba032",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "patch": "@@ -885,6 +885,9 @@ public void addResource(LocalizerResourceRequestEvent request) {\n             Path publicDirDestPath =\n                 publicRsrc.getPathForLocalization(key, publicRootPath,\n                     delService);\n+            if (publicDirDestPath == null) {\n+              return;\n+            }\n             if (!publicDirDestPath.getParent().equals(publicRootPath)) {\n               createParentDirs(publicDirDestPath, publicRootPath);\n               if (diskValidator != null) {\n@@ -1175,10 +1178,11 @@ LocalizerHeartbeatResponse processHeartbeat(\n           LocalResourcesTracker tracker = getLocalResourcesTracker(\n               next.getVisibility(), user, applicationId);\n           if (tracker != null) {\n-            ResourceLocalizationSpec resource =\n-                NodeManagerBuilderUtils.newResourceLocalizationSpec(next,\n-                getPathForLocalization(next, tracker));\n-            rsrcs.add(resource);\n+            Path localPath = getPathForLocalization(next, tracker);\n+            if (localPath != null) {\n+              rsrcs.add(NodeManagerBuilderUtils.newResourceLocalizationSpec(\n+                  next, localPath));\n+            }\n           }\n         } catch (IOException e) {\n           LOG.error(\"local path for PRIVATE localization could not be \" +",
                "raw_url": "https://github.com/apache/hadoop/raw/585ebd873a55bedd2a364d256837f08ada8ba032/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "sha": "d9b887f56decbc4dc493b2dced48b6812570ac0c",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/585ebd873a55bedd2a364d256837f08ada8ba032/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java?ref=585ebd873a55bedd2a364d256837f08ada8ba032",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java",
                "patch": "@@ -1717,8 +1717,18 @@ public void testLocalizerHeartbeatWhenAppCleaningUp() throws Exception {\n       assertEquals(\"NM should tell localizer to be LIVE in Heartbeat.\",\n           LocalizerAction.LIVE, response.getLocalizerAction());\n \n-      // Cleanup application.\n+      // Cleanup container.\n       spyService.handle(new ContainerLocalizationCleanupEvent(c, rsrcs));\n+      dispatcher.await();\n+      try {\n+        /*Directly send heartbeat to introduce race as container\n+          is being cleaned up.*/\n+        locRunnerForContainer.processHeartbeat(\n+              Collections.singletonList(rsrcSuccess));\n+      } catch (Exception e) {\n+        fail(\"Exception should not have been thrown on processing heartbeat\");\n+      }\n+      // Cleanup application.\n       spyService.handle(new ApplicationLocalizationEvent(\n           LocalizationEventType.DESTROY_APPLICATION_RESOURCES, app));\n       dispatcher.await();",
                "raw_url": "https://github.com/apache/hadoop/raw/585ebd873a55bedd2a364d256837f08ada8ba032/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java",
                "sha": "21896ca4c01c44179e7e8506515df3756f1f14c5",
                "status": "modified"
            }
        ],
        "message": "YARN-8649. NPE in localizer hearbeat processing if a container is killed while localizing. Contributed by lujie",
        "parent": "https://github.com/apache/hadoop/commit/bed8cb6979e0460141ed77e3b15d4f18db098a8e",
        "repo": "hadoop",
        "unit_tests": [
            "TestLocalResourcesTrackerImpl.java",
            "TestResourceLocalizationService.java"
        ]
    },
    "hadoop_58b97c7": {
        "bug_id": "hadoop_58b97c7",
        "commit": "https://github.com/apache/hadoop/commit/58b97c79e34901938d59acc84ed48c1f9344996a",
        "file": [
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hadoop/blob/58b97c79e34901938d59acc84ed48c1f9344996a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "changes": 46,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java?ref=58b97c79e34901938d59acc84ed48c1f9344996a",
                "deletions": 21,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "patch": "@@ -1065,7 +1065,7 @@ private void addKeytabResourceIfSecure(SliderFileSystem fileSystem,\n       LOG.warn(\"No Kerberos principal name specified for \" + service.getName());\n       return;\n     }\n-    if(StringUtils.isEmpty(service.getKerberosPrincipal().getKeytab())) {\n+    if (StringUtils.isEmpty(service.getKerberosPrincipal().getKeytab())) {\n       LOG.warn(\"No Kerberos keytab specified for \" + service.getName());\n       return;\n     }\n@@ -1077,27 +1077,31 @@ private void addKeytabResourceIfSecure(SliderFileSystem fileSystem,\n       throw new YarnException(e);\n     }\n \n-    switch (keytabURI.getScheme()) {\n-    case \"hdfs\":\n-      Path keytabOnhdfs = new Path(keytabURI);\n-      if (!fileSystem.getFileSystem().exists(keytabOnhdfs)) {\n-        LOG.warn(service.getName() + \"'s keytab (principalName = \" +\n-            principalName + \") doesn't exist at: \" + keytabOnhdfs);\n-        return;\n+    if (keytabURI.getScheme() != null) {\n+      switch (keytabURI.getScheme()) {\n+      case \"hdfs\":\n+        Path keytabOnhdfs = new Path(keytabURI);\n+        if (!fileSystem.getFileSystem().exists(keytabOnhdfs)) {\n+          LOG.warn(service.getName() + \"'s keytab (principalName = \"\n+              + principalName + \") doesn't exist at: \" + keytabOnhdfs);\n+          return;\n+        }\n+        LocalResource keytabRes = fileSystem.createAmResource(keytabOnhdfs,\n+            LocalResourceType.FILE);\n+        localResource.put(String.format(YarnServiceConstants.KEYTAB_LOCATION,\n+            service.getName()), keytabRes);\n+        LOG.info(\"Adding \" + service.getName() + \"'s keytab for \"\n+            + \"localization, uri = \" + keytabOnhdfs);\n+        break;\n+      case \"file\":\n+        LOG.info(\"Using a keytab from localhost: \" + keytabURI);\n+        break;\n+      default:\n+        LOG.warn(\"Unsupported keytab URI scheme \" + keytabURI);\n+        break;\n       }\n-      LocalResource keytabRes =\n-          fileSystem.createAmResource(keytabOnhdfs, LocalResourceType.FILE);\n-      localResource.put(String.format(YarnServiceConstants.KEYTAB_LOCATION,\n-          service.getName()), keytabRes);\n-      LOG.debug(\"Adding \" + service.getName() + \"'s keytab for \" +\n-          \"localization, uri = \" + keytabOnhdfs);\n-      break;\n-    case \"file\":\n-      LOG.debug(\"Using a keytab from localhost: \" + keytabURI);\n-      break;\n-    default:\n-      LOG.warn(\"Unsupported URI scheme \" + keytabURI);\n-      break;\n+    } else {\n+      LOG.warn(\"Unsupported keytab URI scheme \" + keytabURI);\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/58b97c79e34901938d59acc84ed48c1f9344996a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "sha": "364a94ca3f68b273e4d5a75bf0c4417d88d45ff4",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/58b97c79e34901938d59acc84ed48c1f9344996a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/RestApiErrorMessages.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/RestApiErrorMessages.java?ref=58b97c79e34901938d59acc84ed48c1f9344996a",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/RestApiErrorMessages.java",
                "patch": "@@ -103,4 +103,6 @@\n       + \"expression element name %s specified in placement policy of component \"\n       + \"%s. Expression element names should be a valid constraint name or an \"\n       + \"expression name defined for this component only.\";\n+  String ERROR_KEYTAB_URI_SCHEME_INVALID = \"Unsupported keytab URI scheme: %s\";\n+  String ERROR_KEYTAB_URI_INVALID = \"Invalid keytab URI: %s\";\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/58b97c79e34901938d59acc84ed48c1f9344996a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/RestApiErrorMessages.java",
                "sha": "0e42533504d1fbda30a84808a370729b7fc8f3a5",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hadoop/blob/58b97c79e34901938d59acc84ed48c1f9344996a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ServiceApiUtil.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ServiceApiUtil.java?ref=58b97c79e34901938d59acc84ed48c1f9344996a",
                "deletions": 13,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ServiceApiUtil.java",
                "patch": "@@ -29,13 +29,14 @@\n import org.apache.hadoop.registry.client.binding.RegistryUtils;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.yarn.exceptions.YarnException;\n-import org.apache.hadoop.yarn.service.api.records.Container;\n-import org.apache.hadoop.yarn.service.api.records.Service;\n import org.apache.hadoop.yarn.service.api.records.Artifact;\n import org.apache.hadoop.yarn.service.api.records.Component;\n import org.apache.hadoop.yarn.service.api.records.Configuration;\n+import org.apache.hadoop.yarn.service.api.records.Container;\n+import org.apache.hadoop.yarn.service.api.records.KerberosPrincipal;\n import org.apache.hadoop.yarn.service.api.records.PlacementConstraint;\n import org.apache.hadoop.yarn.service.api.records.Resource;\n+import org.apache.hadoop.yarn.service.api.records.Service;\n import org.apache.hadoop.yarn.service.exceptions.SliderException;\n import org.apache.hadoop.yarn.service.conf.RestApiConstants;\n import org.apache.hadoop.yarn.service.exceptions.RestApiErrorMessages;\n@@ -111,14 +112,7 @@ public static void validateAndResolveService(Service service,\n     }\n \n     if (UserGroupInformation.isSecurityEnabled()) {\n-      if (!StringUtils.isEmpty(service.getKerberosPrincipal().getKeytab())) {\n-        try {\n-          // validate URI format\n-          new URI(service.getKerberosPrincipal().getKeytab());\n-        } catch (URISyntaxException e) {\n-          throw new IllegalArgumentException(e);\n-        }\n-      }\n+      validateKerberosPrincipal(service.getKerberosPrincipal());\n     }\n \n     // Validate the Docker client config.\n@@ -145,9 +139,8 @@ public static void validateAndResolveService(Service service,\n         throw new IllegalArgumentException(\"Component name collision: \" +\n             comp.getName());\n       }\n-      // If artifact is of type SERVICE (which cannot be filled from\n-      // global), read external service and add its components to this\n-      // service\n+      // If artifact is of type SERVICE (which cannot be filled from global),\n+      // read external service and add its components to this service\n       if (comp.getArtifact() != null && comp.getArtifact().getType() ==\n           Artifact.TypeEnum.SERVICE) {\n         if (StringUtils.isEmpty(comp.getArtifact().getId())) {\n@@ -226,6 +219,25 @@ public static void validateAndResolveService(Service service,\n     }\n   }\n \n+  public static void validateKerberosPrincipal(\n+      KerberosPrincipal kerberosPrincipal) throws IOException {\n+    if (!StringUtils.isEmpty(kerberosPrincipal.getKeytab())) {\n+      try {\n+        // validate URI format\n+        URI keytabURI = new URI(kerberosPrincipal.getKeytab());\n+        if (keytabURI.getScheme() == null) {\n+          throw new IllegalArgumentException(String.format(\n+              RestApiErrorMessages.ERROR_KEYTAB_URI_SCHEME_INVALID,\n+              kerberosPrincipal.getKeytab()));\n+        }\n+      } catch (URISyntaxException e) {\n+        throw new IllegalArgumentException(\n+            String.format(RestApiErrorMessages.ERROR_KEYTAB_URI_INVALID,\n+                e.getLocalizedMessage()));\n+      }\n+    }\n+  }\n+\n   private static void validateDockerClientConfiguration(Service service,\n       org.apache.hadoop.conf.Configuration conf) throws IOException {\n     String dockerClientConfig = service.getDockerClientConfig();",
                "raw_url": "https://github.com/apache/hadoop/raw/58b97c79e34901938d59acc84ed48c1f9344996a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ServiceApiUtil.java",
                "sha": "6e62c56f23b7bc1498822046755fc33d3b95686c",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hadoop/blob/58b97c79e34901938d59acc84ed48c1f9344996a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceApiUtil.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceApiUtil.java?ref=58b97c79e34901938d59acc84ed48c1f9344996a",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceApiUtil.java",
                "patch": "@@ -22,6 +22,7 @@\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.service.api.records.Artifact;\n import org.apache.hadoop.yarn.service.api.records.Component;\n+import org.apache.hadoop.yarn.service.api.records.KerberosPrincipal;\n import org.apache.hadoop.yarn.service.api.records.PlacementConstraint;\n import org.apache.hadoop.yarn.service.api.records.PlacementPolicy;\n import org.apache.hadoop.yarn.service.api.records.Resource;\n@@ -45,6 +46,7 @@\n import static org.apache.hadoop.yarn.service.exceptions.RestApiErrorMessages.*;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n \n /**\n  * Test for ServiceApiUtil helper methods.\n@@ -525,4 +527,43 @@ public void testPlacementPolicy() throws IOException {\n       Assert.fail(NO_EXCEPTION_PREFIX + e.getMessage());\n     }\n   }\n+\n+  @Test\n+  public void testKerberosPrincipal() throws IOException {\n+    SliderFileSystem sfs = ServiceTestUtils.initMockFs();\n+    Service app = createValidApplication(\"comp-a\");\n+    KerberosPrincipal kp = new KerberosPrincipal();\n+    kp.setKeytab(\"/some/path\");\n+    kp.setPrincipalName(\"user/_HOST@domain.com\");\n+    app.setKerberosPrincipal(kp);\n+\n+    try {\n+      ServiceApiUtil.validateKerberosPrincipal(app.getKerberosPrincipal());\n+      Assert.fail(EXCEPTION_PREFIX + \"service with invalid keytab URI scheme\");\n+    } catch (IllegalArgumentException e) {\n+      assertEquals(\n+          String.format(RestApiErrorMessages.ERROR_KEYTAB_URI_SCHEME_INVALID,\n+              kp.getKeytab()),\n+          e.getMessage());\n+    }\n+\n+    kp.setKeytab(\"/ blank / in / paths\");\n+    try {\n+      ServiceApiUtil.validateKerberosPrincipal(app.getKerberosPrincipal());\n+      Assert.fail(EXCEPTION_PREFIX + \"service with invalid keytab\");\n+    } catch (IllegalArgumentException e) {\n+      // strip out the %s at the end of the RestApiErrorMessages string constant\n+      assertTrue(e.getMessage().contains(\n+          RestApiErrorMessages.ERROR_KEYTAB_URI_INVALID.substring(0,\n+              RestApiErrorMessages.ERROR_KEYTAB_URI_INVALID.length() - 2)));\n+    }\n+\n+    kp.setKeytab(\"file:///tmp/a.keytab\");\n+    // now it should succeed\n+    try {\n+      ServiceApiUtil.validateKerberosPrincipal(app.getKerberosPrincipal());\n+    } catch (IllegalArgumentException e) {\n+      Assert.fail(NO_EXCEPTION_PREFIX + e.getMessage());\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/58b97c79e34901938d59acc84ed48c1f9344996a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceApiUtil.java",
                "sha": "b209bbb3267829c1e829c7f0694b2c284c17adb2",
                "status": "modified"
            }
        ],
        "message": "YARN-8236. Invalid kerberos principal file name cause NPE in native service. Contributed by Gour Saha.",
        "parent": "https://github.com/apache/hadoop/commit/ffb9210dedb79a56075448dc296251896bed49e6",
        "repo": "hadoop",
        "unit_tests": [
            "ServiceClientTest.java",
            "TestServiceClient.java",
            "TestServiceApiUtil.java"
        ]
    },
    "hadoop_592aaa1": {
        "bug_id": "hadoop_592aaa1",
        "commit": "https://github.com/apache/hadoop/commit/592aaa1f060908ed9c8a32d387e0ad8f1fc030b8",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/592aaa1f060908ed9c8a32d387e0ad8f1fc030b8/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=592aaa1f060908ed9c8a32d387e0ad8f1fc030b8",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -573,6 +573,9 @@ Release 0.23.0 - Unreleased\n     HADOOP-7598. Fix smart-apply-patch.sh to handle patching from a sub\n     directory correctly. (Robert Evans via acmurthy) \n \n+    HADOOP-7328. When a serializer class is missing, return null, not throw\n+    an NPE. (Harsh J Chouraria via todd)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/592aaa1f060908ed9c8a32d387e0ad8f1fc030b8/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "23ac34d30664d2f504e3883127cb50b1959ae0b9",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/592aaa1f060908ed9c8a32d387e0ad8f1fc030b8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java?ref=592aaa1f060908ed9c8a32d387e0ad8f1fc030b8",
                "deletions": 10,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "patch": "@@ -27,10 +27,10 @@\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.conf.Configured;\n+import org.apache.hadoop.fs.CommonConfigurationKeys;\n import org.apache.hadoop.io.serializer.avro.AvroReflectSerialization;\n import org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization;\n import org.apache.hadoop.util.ReflectionUtils;\n-import org.apache.hadoop.util.StringUtils;\n \n /**\n  * <p>\n@@ -50,14 +50,15 @@\n    * <p>\n    * Serializations are found by reading the <code>io.serializations</code>\n    * property from <code>conf</code>, which is a comma-delimited list of\n-   * classnames. \n+   * classnames.\n    * </p>\n    */\n   public SerializationFactory(Configuration conf) {\n     super(conf);\n-    for (String serializerName : conf.getStrings(\"io.serializations\", \n-      new String[]{WritableSerialization.class.getName(), \n-        AvroSpecificSerialization.class.getName(), \n+    for (String serializerName : conf.getStrings(\n+      CommonConfigurationKeys.IO_SERIALIZATIONS_KEY,\n+      new String[]{WritableSerialization.class.getName(),\n+        AvroSpecificSerialization.class.getName(),\n         AvroReflectSerialization.class.getName()})) {\n       add(conf, serializerName);\n     }\n@@ -67,27 +68,35 @@ public SerializationFactory(Configuration conf) {\n   private void add(Configuration conf, String serializationName) {\n     try {\n       Class<? extends Serialization> serializionClass =\n-\t(Class<? extends Serialization>) conf.getClassByName(serializationName);\n+        (Class<? extends Serialization>) conf.getClassByName(serializationName);\n       serializations.add((Serialization)\n-\t  ReflectionUtils.newInstance(serializionClass, getConf()));\n+      ReflectionUtils.newInstance(serializionClass, getConf()));\n     } catch (ClassNotFoundException e) {\n       LOG.warn(\"Serialization class not found: \", e);\n     }\n   }\n \n   public <T> Serializer<T> getSerializer(Class<T> c) {\n-    return getSerialization(c).getSerializer(c);\n+    Serialization<T> serializer = getSerialization(c);\n+    if (serializer != null) {\n+      return serializer.getSerializer(c);\n+    }\n+    return null;\n   }\n \n   public <T> Deserializer<T> getDeserializer(Class<T> c) {\n-    return getSerialization(c).getDeserializer(c);\n+    Serialization<T> serializer = getSerialization(c);\n+    if (serializer != null) {\n+      return serializer.getDeserializer(c);\n+    }\n+    return null;\n   }\n \n   @SuppressWarnings(\"unchecked\")\n   public <T> Serialization<T> getSerialization(Class<T> c) {\n     for (Serialization serialization : serializations) {\n       if (serialization.accept(c)) {\n-\treturn (Serialization<T>) serialization;\n+        return (Serialization<T>) serialization;\n       }\n     }\n     return null;",
                "raw_url": "https://github.com/apache/hadoop/raw/592aaa1f060908ed9c8a32d387e0ad8f1fc030b8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/serializer/SerializationFactory.java",
                "sha": "52a0a253bbaa6bb7e7d580f71292e72fd9c28678",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/hadoop/blob/592aaa1f060908ed9c8a32d387e0ad8f1fc030b8/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java?ref=592aaa1f060908ed9c8a32d387e0ad8f1fc030b8",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "patch": "@@ -0,0 +1,44 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.io.serializer;\n+\n+import org.junit.Test;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertNotNull;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.Writable;\n+\n+public class TestSerializationFactory {\n+\n+  @Test\n+  public void testSerializerAvailability() {\n+    Configuration conf = new Configuration();\n+    SerializationFactory factory = new SerializationFactory(conf);\n+    // Test that a valid serializer class is returned when its present\n+    assertNotNull(\"A valid class must be returned for default Writable Serde\",\n+        factory.getSerializer(Writable.class));\n+    assertNotNull(\"A valid class must be returned for default Writable serDe\",\n+        factory.getDeserializer(Writable.class));\n+    // Test that a null is returned when none can be found.\n+    assertNull(\"A null should be returned if there are no serializers found.\",\n+        factory.getSerializer(TestSerializationFactory.class));\n+    assertNull(\"A null should be returned if there are no deserializers found\",\n+        factory.getDeserializer(TestSerializationFactory.class));\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/592aaa1f060908ed9c8a32d387e0ad8f1fc030b8/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/serializer/TestSerializationFactory.java",
                "sha": "18c2637ec5adb644f7a56fb41aef5fe82967e9e2",
                "status": "added"
            }
        ],
        "message": "HADOOP-7328. When a serializer class is missing, return null, not throw an NPE. Contributed by Harsh J Chouraria.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1167363 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/fbf0035ee60aea7209725fdf42710c64bd910982",
        "repo": "hadoop",
        "unit_tests": [
            "TestSerializationFactory.java"
        ]
    },
    "hadoop_5c8d907": {
        "bug_id": "hadoop_5c8d907",
        "commit": "https://github.com/apache/hadoop/commit/5c8d90763c52f6bf5224b59738739bd2d1a4b4b8",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/5c8d90763c52f6bf5224b59738739bd2d1a4b4b8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java?ref=5c8d90763c52f6bf5224b59738739bd2d1a4b4b8",
                "deletions": 4,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java",
                "patch": "@@ -586,12 +586,14 @@ public InetAddress getByName(String host) throws UnknownHostException {\n    *       hadoop.security.token.service.use_ip=false \n    */\n   protected static class QualifiedHostResolver implements HostResolver {\n-    private List<String> searchDomains;\n+    private List<String> searchDomains = new ArrayList<>();\n     {\n       ResolverConfig resolverConfig = ResolverConfig.getCurrentConfig();\n-      searchDomains = new ArrayList<>();\n-      for (Name name : resolverConfig.searchPath()) {\n-        searchDomains.add(name.toString());\n+      Name[] names = resolverConfig.searchPath();\n+      if (names != null) {\n+        for (Name name : names) {\n+          searchDomains.add(name.toString());\n+        }\n       }\n     }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/5c8d90763c52f6bf5224b59738739bd2d1a4b4b8/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SecurityUtil.java",
                "sha": "2313119bfec2eb6a87ed9d259f5b02ae89b374ec",
                "status": "modified"
            }
        ],
        "message": "HADOOP-15764. Addendum patch: Fix NPE in SecurityUtil.",
        "parent": "https://github.com/apache/hadoop/commit/2a5d4315bfe13c12cacc7718537077bf9abb22e2",
        "repo": "hadoop",
        "unit_tests": [
            "TestSecurityUtil.java"
        ]
    },
    "hadoop_5e093f0": {
        "bug_id": "hadoop_5e093f0",
        "commit": "https://github.com/apache/hadoop/commit/5e093f0d400f82f67d9b2d24253c79e4a5abacf9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/5e093f0d400f82f67d9b2d24253c79e4a5abacf9/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=5e093f0d400f82f67d9b2d24253c79e4a5abacf9",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -290,6 +290,9 @@ Release 2.7.1 - UNRELEASED\n     YARN-3522. Fixed DistributedShell to instantiate TimeLineClient as the\n     correct user. (Zhijie Shen via jianhe)\n \n+    YARN-3537. NPE when NodeManager.serviceInit fails and stopRecoveryStore\n+    invoked (Brahma Reddy Battula via jlowe)\n+\n Release 2.7.0 - 2015-04-20\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/5e093f0d400f82f67d9b2d24253c79e4a5abacf9/hadoop-yarn-project/CHANGES.txt",
                "sha": "001396fa80b65a39c4137c42e42bfd58cb14694d",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/5e093f0d400f82f67d9b2d24253c79e4a5abacf9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java?ref=5e093f0d400f82f67d9b2d24253c79e4a5abacf9",
                "deletions": 12,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "patch": "@@ -178,18 +178,20 @@ private void initAndStartRecoveryStore(Configuration conf)\n   }\n \n   private void stopRecoveryStore() throws IOException {\n-    nmStore.stop();\n-    if (null != context) {\n-      if (context.getDecommissioned() && nmStore.canRecover()) {\n-        LOG.info(\"Removing state store due to decommission\");\n-        Configuration conf = getConfig();\n-        Path recoveryRoot =\n-            new Path(conf.get(YarnConfiguration.NM_RECOVERY_DIR));\n-        LOG.info(\"Removing state store at \" + recoveryRoot\n-            + \" due to decommission\");\n-        FileSystem recoveryFs = FileSystem.getLocal(conf);\n-        if (!recoveryFs.delete(recoveryRoot, true)) {\n-          LOG.warn(\"Unable to delete \" + recoveryRoot);\n+    if (null != nmStore) {\n+      nmStore.stop();\n+      if (null != context) {\n+        if (context.getDecommissioned() && nmStore.canRecover()) {\n+          LOG.info(\"Removing state store due to decommission\");\n+          Configuration conf = getConfig();\n+          Path recoveryRoot =\n+              new Path(conf.get(YarnConfiguration.NM_RECOVERY_DIR));\n+          LOG.info(\"Removing state store at \" + recoveryRoot\n+              + \" due to decommission\");\n+          FileSystem recoveryFs = FileSystem.getLocal(conf);\n+          if (!recoveryFs.delete(recoveryRoot, true)) {\n+            LOG.warn(\"Unable to delete \" + recoveryRoot);\n+          }\n         }\n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/5e093f0d400f82f67d9b2d24253c79e4a5abacf9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "sha": "6718b53f7b70943808e1f2b1f2c1f14e639ec3f1",
                "status": "modified"
            }
        ],
        "message": "YARN-3537. NPE when NodeManager.serviceInit fails and stopRecoveryStore invoked. Contributed by Brahma Reddy Battula",
        "parent": "https://github.com/apache/hadoop/commit/5ce3a77f3c00aeabcd791c3373dd3c8c25160ce2",
        "repo": "hadoop",
        "unit_tests": [
            "TestNodeManager.java"
        ]
    },
    "hadoop_60cbcff": {
        "bug_id": "hadoop_60cbcff",
        "commit": "https://github.com/apache/hadoop/commit/60cbcff2f7363e5cc386284981cac67abc965ee7",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/60cbcff2f7363e5cc386284981cac67abc965ee7/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=60cbcff2f7363e5cc386284981cac67abc965ee7",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -270,6 +270,8 @@ Trunk (Unreleased)\n \n     HDFS-7581. HDFS documentation needs updating post-shell rewrite (aw)\n \n+    HDFS-7606. Fix potential NPE in INodeFile.getBlocks(). (Byron Wong via shv)\n+\n Release 2.7.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/60cbcff2f7363e5cc386284981cac67abc965ee7/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "6af1a52324d292c0d3ee474666def12ca8844c75",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/60cbcff2f7363e5cc386284981cac67abc965ee7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java?ref=60cbcff2f7363e5cc386284981cac67abc965ee7",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
                "patch": "@@ -432,7 +432,7 @@ final long getBlockDiskspace() {\n       return snapshotBlocks;\n     // Blocks are not in the current snapshot\n     // Find next snapshot with blocks present or return current file blocks\n-    snapshotBlocks = getDiffs().findLaterSnapshotBlocks(diff.getSnapshotId());\n+    snapshotBlocks = getDiffs().findLaterSnapshotBlocks(snapshot);\n     return (snapshotBlocks == null) ? getBlocks() : snapshotBlocks;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/60cbcff2f7363e5cc386284981cac67abc965ee7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INodeFile.java",
                "sha": "e871bdc59b7f262728f3c670a4e75d0c6f8c5c76",
                "status": "modified"
            }
        ],
        "message": "HDFS-7606. Fix potential NPE in INodeFile.getBlocks(). Contributed by Byron Wong.",
        "parent": "https://github.com/apache/hadoop/commit/ec4389cf7270cff4cc96313b4190422ea7c70ced",
        "repo": "hadoop",
        "unit_tests": [
            "TestINodeFile.java"
        ]
    },
    "hadoop_6124439": {
        "bug_id": "hadoop_6124439",
        "commit": "https://github.com/apache/hadoop/commit/612443951b1a950d463873d8ecff198b6252c25c",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/612443951b1a950d463873d8ecff198b6252c25c/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java?ref=612443951b1a950d463873d8ecff198b6252c25c",
                "deletions": 4,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java",
                "patch": "@@ -202,8 +202,12 @@ public static String uriToString(URI[] uris){\n   }\n   \n   /**\n-   * \n    * @param str\n+   *          The string array to be parsed into an URI array.\n+   * @return <tt>null</tt> if str is <tt>null</tt>, else the URI array\n+   *         equivalent to str.\n+   * @throws IllegalArgumentException\n+   *           If any string in str violates RFC&nbsp;2396.\n    */\n   public static URI[] stringToURI(String[] str){\n     if (str == null) \n@@ -213,9 +217,8 @@ public static String uriToString(URI[] uris){\n       try{\n         uris[i] = new URI(str[i]);\n       }catch(URISyntaxException ur){\n-        System.out.println(\"Exception in specified URI's \" + StringUtils.stringifyException(ur));\n-        //making sure its asssigned to null in case of an error\n-        uris[i] = null;\n+        throw new IllegalArgumentException(\n+            \"Failed to create uri for \" + str[i], ur);\n       }\n     }\n     return uris;",
                "raw_url": "https://github.com/apache/hadoop/raw/612443951b1a950d463873d8ecff198b6252c25c/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java",
                "sha": "67a8f82d9382cfd19d0c803144248c213359a0be",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/612443951b1a950d463873d8ecff198b6252c25c/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java?ref=612443951b1a950d463873d8ecff198b6252c25c",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java",
                "patch": "@@ -269,6 +269,17 @@ public void testCamelize() {\n     assertEquals(\"Yy\", StringUtils.camelize(\"yY\"));\n     assertEquals(\"Zz\", StringUtils.camelize(\"zZ\"));\n   }\n+  \n+  @Test\n+  public void testStringToURI() {\n+    String[] str = new String[] { \"file://\" };\n+    try {\n+      StringUtils.stringToURI(str);\n+      fail(\"Ignoring URISyntaxException while creating URI from string file://\");\n+    } catch (IllegalArgumentException iae) {\n+      assertEquals(\"Failed to create uri for file://\", iae.getMessage());\n+    }\n+  }\n \n   // Benchmark for StringUtils split\n   public static void main(String []args) {",
                "raw_url": "https://github.com/apache/hadoop/raw/612443951b1a950d463873d8ecff198b6252c25c/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java",
                "sha": "fc90984608fb1a2630184419a840177223e9cb12",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/612443951b1a950d463873d8ecff198b6252c25c/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=612443951b1a950d463873d8ecff198b6252c25c",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -236,6 +236,9 @@ Branch-2 ( Unreleased changes )\n     HADOOP-8499. Lower min.user.id to 500 for the tests.\n     (Colin Patrick McCabe via eli)\n \n+    MAPREDUCE-4395. Possible NPE at ClientDistributedCacheManager\n+    #determineTimestamps (Bhallamudi via bobby)\n+\n Release 2.0.0-alpha - 05-23-2012\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/612443951b1a950d463873d8ecff198b6252c25c/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "db352254f14824b21d8b6f84f401ed03722e859e",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4395. Possible NPE at ClientDistributedCacheManager#determineTimestamps (Bhallamudi via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1362052 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/0f122c209d7346e7913907dec86aa8cf221dd8f2",
        "repo": "hadoop",
        "unit_tests": [
            "TestStringUtils.java"
        ]
    },
    "hadoop_614af50": {
        "bug_id": "hadoop_614af50",
        "commit": "https://github.com/apache/hadoop/commit/614af50625a8495812dce8da59db0e1aef40b1c0",
        "file": [
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop/blob/614af50625a8495812dce8da59db0e1aef40b1c0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java?ref=614af50625a8495812dce8da59db0e1aef40b1c0",
                "deletions": 10,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "patch": "@@ -1040,20 +1040,27 @@ public SchedulerNode getNode(NodeId nodeId) {\n     for (Map.Entry<ApplicationId, ContainerStatus> c : updateExistContainers) {\n       SchedulerApplication<T> app = applications.get(c.getKey());\n       ContainerId containerId = c.getValue().getContainerId();\n+      if (app == null || app.getCurrentAppAttempt() == null) {\n+        continue;\n+      }\n+      RMContainer rmContainer\n+          = app.getCurrentAppAttempt().getRMContainer(containerId);\n+      if (rmContainer == null) {\n+        continue;\n+      }\n+      // exposed ports are already set for the container, skip\n+      if (rmContainer.getExposedPorts() != null &&\n+          rmContainer.getExposedPorts().size() > 0) {\n+        continue;\n+      }\n+\n       String strExposedPorts = c.getValue().getExposedPorts();\n-      Map<String, List<Map<String, String>>> exposedPorts = null;\n       if (null != strExposedPorts && !strExposedPorts.isEmpty()) {\n         Gson gson = new Gson();\n-        exposedPorts = gson.fromJson(strExposedPorts,\n+        Map<String, List<Map<String, String>>> exposedPorts =\n+            gson.fromJson(strExposedPorts,\n             new TypeToken<Map<String, List<Map<String, String>>>>()\n-            {}.getType());\n-      }\n-\n-      RMContainer rmContainer\n-          = app.getCurrentAppAttempt().getRMContainer(containerId);\n-      if (null != rmContainer &&\n-          (null == rmContainer.getExposedPorts()\n-              || rmContainer.getExposedPorts().size() == 0)) {\n+                {}.getType());\n         LOG.info(\"update exist container \" + containerId.getContainerId()\n             + \", strExposedPorts = \" + strExposedPorts);\n         rmContainer.setExposedPorts(exposedPorts);",
                "raw_url": "https://github.com/apache/hadoop/raw/614af50625a8495812dce8da59db0e1aef40b1c0/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "sha": "a798b97af5fa4ade6531c71a34c8405973359536",
                "status": "modified"
            }
        ],
        "message": "YARN-9179. Fix NPE in AbstractYarnScheduler#updateNewContainerInfo.",
        "parent": "https://github.com/apache/hadoop/commit/05c84ab01c08d37457bb5aa40df61f45c7ed5fd4",
        "repo": "hadoop",
        "unit_tests": [
            "TestAbstractYarnScheduler.java"
        ]
    },
    "hadoop_6243eab": {
        "bug_id": "hadoop_6243eab",
        "commit": "https://github.com/apache/hadoop/commit/6243eabb48390fffada2418ade5adf9e0766afbe",
        "file": [
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop/blob/6243eabb48390fffada2418ade5adf9e0766afbe/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=6243eabb48390fffada2418ade5adf9e0766afbe",
                "deletions": 6,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -1038,9 +1038,9 @@ private LocatedBlock createLocatedBlock(final BlockInfo blk, final long pos)\n     }\n \n     final int numNodes = blocksMap.numNodes(blk);\n-    final boolean isCorrupt = numCorruptNodes != 0 &&\n-        numCorruptNodes == numNodes;\n-    final int numMachines = isCorrupt ? numNodes: numNodes - numCorruptNodes;\n+    final boolean isCorrupt = numCorruptReplicas != 0 &&\n+        numCorruptReplicas == numNodes;\n+    final int numMachines = isCorrupt ? numNodes: numNodes - numCorruptReplicas;\n     final DatanodeStorageInfo[] machines = new DatanodeStorageInfo[numMachines];\n     final byte[] blockIndices = blk.isStriped() ? new byte[numMachines] : null;\n     int j = 0, i = 0;\n@@ -1366,11 +1366,22 @@ public void findAndMarkBlockAsCorrupt(final ExtendedBlock blk,\n           + \" as corrupt because datanode \" + dn + \" (\" + dn.getDatanodeUuid()\n           + \") does not exist\");\n     }\n-    \n+    DatanodeStorageInfo storage = null;\n+    if (storageID != null) {\n+      storage = node.getStorageInfo(storageID);\n+    }\n+    if (storage == null) {\n+      storage = storedBlock.findStorageInfo(node);\n+    }\n+\n+    if (storage == null) {\n+      blockLog.debug(\"BLOCK* findAndMarkBlockAsCorrupt: {} not found on {}\",\n+          blk, dn);\n+      return;\n+    }\n     markBlockAsCorrupt(new BlockToMarkCorrupt(reportedBlock, storedBlock,\n             blk.getGenerationStamp(), reason, Reason.CORRUPTION_REPORTED),\n-        storageID == null ? null : node.getStorageInfo(storageID),\n-        node);\n+        storage, node);\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/6243eabb48390fffada2418ade5adf9e0766afbe/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "accfc38f0b3c8e85ff3568efd763737fac498321",
                "status": "modified"
            },
            {
                "additions": 86,
                "blob_url": "https://github.com/apache/hadoop/blob/6243eabb48390fffada2418ade5adf9e0766afbe/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCorruption.java",
                "changes": 87,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCorruption.java?ref=6243eabb48390fffada2418ade5adf9e0766afbe",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCorruption.java",
                "patch": "@@ -18,15 +18,22 @@\n \n package org.apache.hadoop.hdfs;\n \n+import org.apache.hadoop.fs.StorageType;\n+import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n \n import java.io.DataInputStream;\n import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.IOException;\n import java.io.FileOutputStream;\n import java.util.ArrayList;\n+import java.util.HashSet;\n import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.ChecksumException;\n@@ -36,6 +43,8 @@\n import org.apache.hadoop.hdfs.protocol.BlockListAsLongs.BlockReportReplica;\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockManager;\n+import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor;\n import org.apache.hadoop.hdfs.server.datanode.DataNode;\n import org.apache.hadoop.hdfs.server.datanode.DataNodeTestUtils;\n import org.apache.hadoop.hdfs.server.namenode.FSNamesystem;\n@@ -167,7 +176,83 @@ public void testArrayOutOfBoundsException() throws Exception {\n       }\n     }\n   }\n-  \n+\n+  @Test\n+  public void testCorruptionWithDiskFailure() throws Exception {\n+    MiniDFSCluster cluster = null;\n+    try {\n+      Configuration conf = new HdfsConfiguration();\n+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(3).build();\n+      cluster.waitActive();\n+      BlockManager bm = cluster.getNamesystem().getBlockManager();\n+      FileSystem fs = cluster.getFileSystem();\n+      final Path FILE_PATH = new Path(\"/tmp.txt\");\n+      final long FILE_LEN = 1L;\n+      DFSTestUtil.createFile(fs, FILE_PATH, FILE_LEN, (short) 3, 1L);\n+\n+      // get the block\n+      final String bpid = cluster.getNamesystem().getBlockPoolId();\n+      File storageDir = cluster.getInstanceStorageDir(0, 0);\n+      File dataDir = MiniDFSCluster.getFinalizedDir(storageDir, bpid);\n+      assertTrue(\"Data directory does not exist\", dataDir.exists());\n+      ExtendedBlock blk = getFirstBlock(cluster.getDataNodes().get(0), bpid);\n+      if (blk == null) {\n+        blk = getFirstBlock(cluster.getDataNodes().get(0), bpid);\n+      }\n+      assertFalse(\"Data directory does not contain any blocks or there was an\" +\n+          \" \" +\n+          \"IO error\", blk == null);\n+      ArrayList<DataNode> datanodes = cluster.getDataNodes();\n+      assertEquals(datanodes.size(), 3);\n+      FSNamesystem ns = cluster.getNamesystem();\n+      //fail the storage on that node which has the block\n+      try {\n+        ns.writeLock();\n+        updateAllStorages(bm);\n+      } finally {\n+        ns.writeUnlock();\n+      }\n+      ns.writeLock();\n+      try {\n+        markAllBlocksAsCorrupt(bm, blk);\n+      } finally {\n+        ns.writeUnlock();\n+      }\n+\n+      // open the file\n+      fs.open(FILE_PATH);\n+\n+      //clean up\n+      fs.delete(FILE_PATH, false);\n+    } finally {\n+      if (cluster != null) { cluster.shutdown(); }\n+    }\n+\n+  }\n+\n+  private void markAllBlocksAsCorrupt(BlockManager bm,\n+                                      ExtendedBlock blk) throws IOException {\n+    for (DatanodeStorageInfo info : bm.getStorages(blk.getLocalBlock())) {\n+      bm.findAndMarkBlockAsCorrupt(\n+          blk, info.getDatanodeDescriptor(), info.getStorageID(), \"STORAGE_ID\");\n+    }\n+  }\n+\n+  private void updateAllStorages(BlockManager bm) {\n+    for (DatanodeDescriptor dd : bm.getDatanodeManager().getDatanodes()) {\n+      Set<DatanodeStorageInfo> setInfos = new HashSet<DatanodeStorageInfo>();\n+      DatanodeStorageInfo[] infos = dd.getStorageInfos();\n+      Random random = new Random();\n+      for (int i = 0; i < infos.length; i++) {\n+        int blkId = random.nextInt(101);\n+        DatanodeStorage storage = new DatanodeStorage(Integer.toString(blkId),\n+            DatanodeStorage.State.FAILED, StorageType.DISK);\n+        infos[i].updateFromStorage(storage);\n+        setInfos.add(infos[i]);\n+      }\n+    }\n+  }\n+\n   private static ExtendedBlock getFirstBlock(DataNode dn, String bpid) {\n     Map<DatanodeStorage, BlockListAsLongs> blockReports =\n         dn.getFSDataset().getBlockReports(bpid);",
                "raw_url": "https://github.com/apache/hadoop/raw/6243eabb48390fffada2418ade5adf9e0766afbe/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestFileCorruption.java",
                "sha": "011baa1ea3ab2ded7fe82c7bb26f08eb3cb2f21b",
                "status": "modified"
            }
        ],
        "message": "HDFS-9958. BlockManager#createLocatedBlocks can throw NPE for corruptBlocks on failed storages. Contributed by Kuhu Shukla",
        "parent": "https://github.com/apache/hadoop/commit/cf2ee45f71f3326c4b5b3367ef534c0277392fc1",
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockManager.java"
        ]
    },
    "hadoop_6285cbb": {
        "bug_id": "hadoop_6285cbb",
        "commit": "https://github.com/apache/hadoop/commit/6285cbb4990736ed25cea739c421fcfe0cd9d591",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/6285cbb4990736ed25cea739c421fcfe0cd9d591/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/CHANGES.txt?ref=6285cbb4990736ed25cea739c421fcfe0cd9d591",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -289,6 +289,9 @@ Trunk (unreleased changes)\n     HADOOP-6991.  Fix SequenceFile::Reader to honor file lengths and call\n     openFile (cdouglas via omalley)\n \n+    HADOOP-7011.  Fix KerberosName.main() to not throw an NPE.\n+    (Aaron T. Myers via tomwhite)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop/raw/6285cbb4990736ed25cea739c421fcfe0cd9d591/CHANGES.txt",
                "sha": "e607a0c1062c4eef025df0df61256d286a37a9af",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/6285cbb4990736ed25cea739c421fcfe0cd9d591/src/java/org/apache/hadoop/security/KerberosName.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/org/apache/hadoop/security/KerberosName.java?ref=6285cbb4990736ed25cea739c421fcfe0cd9d591",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/security/KerberosName.java",
                "patch": "@@ -399,9 +399,10 @@ static void printRules() throws IOException {\n   }\n \n   public static void main(String[] args) throws Exception {\n+    setConfiguration(new Configuration());\n     for(String arg: args) {\n       KerberosName name = new KerberosName(arg);\n       System.out.println(\"Name: \" + name + \" to \" + name.getShortName());\n     }\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/6285cbb4990736ed25cea739c421fcfe0cd9d591/src/java/org/apache/hadoop/security/KerberosName.java",
                "sha": "b533cd22f77d6dbf0aaa0a42edf6338dc6d987c6",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7011.  Fix KerberosName.main() to not throw an NPE.  Contributed by Aaron T. Myers.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1028938 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/0c462b223f151208ff7bd5148cee0e436c23d795",
        "repo": "hadoop",
        "unit_tests": [
            "TestKerberosName.java"
        ]
    },
    "hadoop_63e08ec": {
        "bug_id": "hadoop_63e08ec",
        "commit": "https://github.com/apache/hadoop/commit/63e08ec071852640babea9e39780327a0907712a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/63e08ec071852640babea9e39780327a0907712a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineReaderWebServices.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineReaderWebServices.java?ref=63e08ec071852640babea9e39780327a0907712a",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineReaderWebServices.java",
                "patch": "@@ -3532,7 +3532,8 @@ static boolean validateAuthUserWithEntityUser(\n   static boolean checkAccess(TimelineReaderManager readerManager,\n       UserGroupInformation ugi, String entityUser) {\n     if (isDisplayEntityPerUserFilterEnabled(readerManager.getConfig())) {\n-      if (!validateAuthUserWithEntityUser(readerManager, ugi, entityUser)) {\n+      if (ugi != null && !validateAuthUserWithEntityUser(readerManager, ugi,\n+          entityUser)) {\n         String userName = ugi.getShortUserName();\n         String msg = \"User \" + userName\n             + \" is not allowed to read TimelineService V2 data.\";",
                "raw_url": "https://github.com/apache/hadoop/raw/63e08ec071852640babea9e39780327a0907712a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/main/java/org/apache/hadoop/yarn/server/timelineservice/reader/TimelineReaderWebServices.java",
                "sha": "b10b705bb61f0a0818da6adf586ad70bee11f4c6",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/63e08ec071852640babea9e39780327a0907712a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/reader/TestTimelineReaderWebServicesBasicAcl.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/reader/TestTimelineReaderWebServicesBasicAcl.java?ref=63e08ec071852640babea9e39780327a0907712a",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/reader/TestTimelineReaderWebServicesBasicAcl.java",
                "patch": "@@ -88,6 +88,10 @@\n     Assert.assertFalse(TimelineReaderWebServices\n         .validateAuthUserWithEntityUser(manager, null, user1));\n \n+    // true because ugi is null\n+    Assert.assertTrue(\n+        TimelineReaderWebServices.checkAccess(manager, null, user1));\n+\n     // incoming ugi is admin asking for entity owner user1\n     Assert.assertTrue(\n         TimelineReaderWebServices.checkAccess(manager, adminUgi, user1));",
                "raw_url": "https://github.com/apache/hadoop/raw/63e08ec071852640babea9e39780327a0907712a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timelineservice/src/test/java/org/apache/hadoop/yarn/server/timelineservice/reader/TestTimelineReaderWebServicesBasicAcl.java",
                "sha": "6651457ff7592bc96270845f032521d2c2e6dea6",
                "status": "modified"
            }
        ],
        "message": "YARN-8591. [ATSv2] NPE while checking for entity acl in non-secure cluster. Contributed by Rohith Sharma K S.",
        "parent": "https://github.com/apache/hadoop/commit/0857f116b754d83d3c540cd6f989087af24fef27",
        "repo": "hadoop",
        "unit_tests": [
            "TestTimelineReaderWebServices.java"
        ]
    },
    "hadoop_6468071": {
        "bug_id": "hadoop_6468071",
        "commit": "https://github.com/apache/hadoop/commit/6468071f137e6d918a7b4799ad54558fa26b25ce",
        "file": [
            {
                "additions": 98,
                "blob_url": "https://github.com/apache/hadoop/blob/6468071f137e6d918a7b4799ad54558fa26b25ce/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/SingleConstraintAppPlacementAllocator.java",
                "changes": 187,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/SingleConstraintAppPlacementAllocator.java?ref=6468071f137e6d918a7b4799ad54558fa26b25ce",
                "deletions": 89,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/SingleConstraintAppPlacementAllocator.java",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement;\n \n import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.collect.ImmutableSet;\n import org.apache.commons.collections.IteratorUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -238,110 +239,118 @@ private void validateAndSetSchedulingRequest(SchedulingRequest\n           \"Only GUARANTEED execution type is supported.\");\n     }\n \n-    PlacementConstraint constraint =\n-        newSchedulingRequest.getPlacementConstraint();\n-\n-    // We only accept SingleConstraint\n-    PlacementConstraint.AbstractConstraint ac = constraint.getConstraintExpr();\n-    if (!(ac instanceof PlacementConstraint.SingleConstraint)) {\n-      throwExceptionWithMetaInfo(\n-          \"Only accepts \" + PlacementConstraint.SingleConstraint.class.getName()\n-              + \" as constraint-expression. Rejecting the new added \"\n-              + \"constraint-expression.class=\" + ac.getClass().getName());\n-    }\n-\n-    PlacementConstraint.SingleConstraint singleConstraint =\n-        (PlacementConstraint.SingleConstraint) ac;\n-\n-    // Make sure it is an anti-affinity request (actually this implementation\n-    // should be able to support both affinity / anti-affinity without much\n-    // effort. Considering potential test effort required. Limit to\n-    // anti-affinity to intra-app and scope is node.\n-    if (!singleConstraint.getScope().equals(PlacementConstraints.NODE)) {\n-      throwExceptionWithMetaInfo(\n-          \"Only support scope=\" + PlacementConstraints.NODE\n-              + \"now. PlacementConstraint=\" + singleConstraint);\n-    }\n-\n-    if (singleConstraint.getMinCardinality() != 0\n-        || singleConstraint.getMaxCardinality() != 0) {\n-      throwExceptionWithMetaInfo(\n-          \"Only support anti-affinity, which is: minCardinality=0, \"\n-              + \"maxCardinality=1\");\n-    }\n-\n-    Set<PlacementConstraint.TargetExpression> targetExpressionSet =\n-        singleConstraint.getTargetExpressions();\n-    if (targetExpressionSet == null || targetExpressionSet.isEmpty()) {\n-      throwExceptionWithMetaInfo(\n-          \"TargetExpression should not be null or empty\");\n-    }\n-\n-    // Set node partition\n+    // Node partition\n     String nodePartition = null;\n-\n     // Target allocation tags\n     Set<String> targetAllocationTags = null;\n \n-    for (PlacementConstraint.TargetExpression targetExpression : targetExpressionSet) {\n-      // Handle node partition\n-      if (targetExpression.getTargetType().equals(\n-          PlacementConstraint.TargetExpression.TargetType.NODE_ATTRIBUTE)) {\n-        // For node attribute target, we only support Partition now. And once\n-        // YARN-3409 is merged, we will support node attribute.\n-        if (!targetExpression.getTargetKey().equals(NODE_PARTITION)) {\n-          throwExceptionWithMetaInfo(\"When TargetType=\"\n-              + PlacementConstraint.TargetExpression.TargetType.NODE_ATTRIBUTE\n-              + \" only \" + NODE_PARTITION + \" is accepted as TargetKey.\");\n-        }\n+    PlacementConstraint constraint =\n+        newSchedulingRequest.getPlacementConstraint();\n \n-        if (nodePartition != null) {\n-          // This means we have duplicated node partition entry inside placement\n-          // constraint, which might be set by mistake.\n-          throwExceptionWithMetaInfo(\n-              \"Only one node partition targetExpression is allowed\");\n-        }\n+    if (constraint != null) {\n+      // We only accept SingleConstraint\n+      PlacementConstraint.AbstractConstraint ac = constraint\n+          .getConstraintExpr();\n+      if (!(ac instanceof PlacementConstraint.SingleConstraint)) {\n+        throwExceptionWithMetaInfo(\"Only accepts \"\n+            + PlacementConstraint.SingleConstraint.class.getName()\n+                + \" as constraint-expression. Rejecting the new added \"\n+            + \"constraint-expression.class=\" + ac.getClass().getName());\n+      }\n \n-        Set<String> values = targetExpression.getTargetValues();\n-        if (values == null || values.isEmpty()) {\n-          nodePartition = RMNodeLabelsManager.NO_LABEL;\n-          continue;\n-        }\n+      PlacementConstraint.SingleConstraint singleConstraint =\n+          (PlacementConstraint.SingleConstraint) ac;\n+\n+      // Make sure it is an anti-affinity request (actually this implementation\n+      // should be able to support both affinity / anti-affinity without much\n+      // effort. Considering potential test effort required. Limit to\n+      // anti-affinity to intra-app and scope is node.\n+      if (!singleConstraint.getScope().equals(PlacementConstraints.NODE)) {\n+        throwExceptionWithMetaInfo(\n+            \"Only support scope=\" + PlacementConstraints.NODE\n+                + \"now. PlacementConstraint=\" + singleConstraint);\n+      }\n \n-        if (values.size() > 1) {\n-          throwExceptionWithMetaInfo(\"Inside one targetExpression, we only \"\n-              + \"support affinity to at most one node partition now\");\n-        }\n+      if (singleConstraint.getMinCardinality() != 0\n+          || singleConstraint.getMaxCardinality() != 0) {\n+        throwExceptionWithMetaInfo(\n+            \"Only support anti-affinity, which is: minCardinality=0, \"\n+                + \"maxCardinality=1\");\n+      }\n \n-        nodePartition = values.iterator().next();\n-      } else if (targetExpression.getTargetType().equals(\n-          PlacementConstraint.TargetExpression.TargetType.ALLOCATION_TAG)) {\n-        // Handle allocation tags\n-        if (targetAllocationTags != null) {\n-          // This means we have duplicated AllocationTag expressions entries\n-          // inside placement constraint, which might be set by mistake.\n-          throwExceptionWithMetaInfo(\n-              \"Only one AllocationTag targetExpression is allowed\");\n-        }\n+      Set<PlacementConstraint.TargetExpression> targetExpressionSet =\n+          singleConstraint.getTargetExpressions();\n+      if (targetExpressionSet == null || targetExpressionSet.isEmpty()) {\n+        throwExceptionWithMetaInfo(\n+            \"TargetExpression should not be null or empty\");\n+      }\n \n-        if (targetExpression.getTargetValues() == null || targetExpression\n-            .getTargetValues().isEmpty()) {\n-          throwExceptionWithMetaInfo(\"Failed to find allocation tags from \"\n-              + \"TargetExpressions or couldn't find self-app target.\");\n+      for (PlacementConstraint.TargetExpression targetExpression :\n+          targetExpressionSet) {\n+        // Handle node partition\n+        if (targetExpression.getTargetType().equals(\n+            PlacementConstraint.TargetExpression.TargetType.NODE_ATTRIBUTE)) {\n+          // For node attribute target, we only support Partition now. And once\n+          // YARN-3409 is merged, we will support node attribute.\n+          if (!targetExpression.getTargetKey().equals(NODE_PARTITION)) {\n+            throwExceptionWithMetaInfo(\"When TargetType=\"\n+                + PlacementConstraint.TargetExpression.TargetType.NODE_ATTRIBUTE\n+                + \" only \" + NODE_PARTITION + \" is accepted as TargetKey.\");\n+          }\n+\n+          if (nodePartition != null) {\n+            // This means we have duplicated node partition entry\n+            // inside placement constraint, which might be set by mistake.\n+            throwExceptionWithMetaInfo(\n+                \"Only one node partition targetExpression is allowed\");\n+          }\n+\n+          Set<String> values = targetExpression.getTargetValues();\n+          if (values == null || values.isEmpty()) {\n+            nodePartition = RMNodeLabelsManager.NO_LABEL;\n+            continue;\n+          }\n+\n+          if (values.size() > 1) {\n+            throwExceptionWithMetaInfo(\"Inside one targetExpression, we only \"\n+                + \"support affinity to at most one node partition now\");\n+          }\n+\n+          nodePartition = values.iterator().next();\n+        } else if (targetExpression.getTargetType().equals(\n+            PlacementConstraint.TargetExpression.TargetType.ALLOCATION_TAG)) {\n+          // Handle allocation tags\n+          if (targetAllocationTags != null) {\n+            // This means we have duplicated AllocationTag expressions entries\n+            // inside placement constraint, which might be set by mistake.\n+            throwExceptionWithMetaInfo(\n+                \"Only one AllocationTag targetExpression is allowed\");\n+          }\n+\n+          if (targetExpression.getTargetValues() == null ||\n+              targetExpression.getTargetValues().isEmpty()) {\n+            throwExceptionWithMetaInfo(\"Failed to find allocation tags from \"\n+                + \"TargetExpressions or couldn't find self-app target.\");\n+          }\n+\n+          targetAllocationTags = new HashSet<>(\n+              targetExpression.getTargetValues());\n         }\n+      }\n \n-        targetAllocationTags = new HashSet<>(\n-            targetExpression.getTargetValues());\n+      if (targetAllocationTags == null) {\n+        // That means we don't have ALLOCATION_TAG specified\n+        throwExceptionWithMetaInfo(\n+            \"Couldn't find target expression with type == ALLOCATION_TAG,\"\n+                + \" it is required to include one and only one target\"\n+                + \" expression with type == ALLOCATION_TAG\");\n       }\n     }\n \n+    // If this scheduling request doesn't contain a placement constraint,\n+    // we set allocation tags an empty set.\n     if (targetAllocationTags == null) {\n-      // That means we don't have ALLOCATION_TAG specified\n-      throwExceptionWithMetaInfo(\n-          \"Couldn't find target expression with type == ALLOCATION_TAG, it is \"\n-              + \"required to include one and only one target expression with \"\n-              + \"type == ALLOCATION_TAG\");\n-\n+      targetAllocationTags = ImmutableSet.of();\n     }\n \n     if (nodePartition == null) {",
                "raw_url": "https://github.com/apache/hadoop/raw/6468071f137e6d918a7b4799ad54558fa26b25ce/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/placement/SingleConstraintAppPlacementAllocator.java",
                "sha": "2b610f2fe769ce0e5969ce1f86d29ded199868db",
                "status": "modified"
            },
            {
                "additions": 84,
                "blob_url": "https://github.com/apache/hadoop/blob/6468071f137e6d918a7b4799ad54558fa26b25ce/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestSchedulingRequestContainerAllocation.java",
                "changes": 84,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestSchedulingRequestContainerAllocation.java?ref=6468071f137e6d918a7b4799ad54558fa26b25ce",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestSchedulingRequestContainerAllocation.java",
                "patch": "@@ -18,8 +18,15 @@\n \n package org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity;\n \n+import com.google.common.collect.ImmutableList;\n import com.google.common.collect.ImmutableSet;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.api.protocolrecords.AllocateRequest;\n+import org.apache.hadoop.yarn.api.records.ExecutionType;\n+import org.apache.hadoop.yarn.api.records.ExecutionTypeRequest;\n+import org.apache.hadoop.yarn.api.records.SchedulingRequest;\n+import org.apache.hadoop.yarn.api.resource.PlacementConstraint;\n+import org.apache.hadoop.yarn.api.resource.PlacementConstraints;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.constraint.TargetApplicationsNamespace;\n import org.apache.hadoop.yarn.api.records.Priority;\n import org.apache.hadoop.yarn.api.records.Resource;\n@@ -39,6 +46,8 @@\n import org.junit.Before;\n import org.junit.Test;\n \n+import static org.apache.hadoop.yarn.api.resource.PlacementConstraints.PlacementTargets.*;\n+\n public class TestSchedulingRequestContainerAllocation {\n   private final int GB = 1024;\n \n@@ -393,4 +402,79 @@ public RMNodeLabelsManager createNodeLabelManager() {\n     Assert.assertTrue(caughtException);\n     rm1.close();\n   }\n+\n+  @Test\n+  public void testSchedulingRequestWithNullConstraint() throws Exception {\n+    Configuration csConf = TestUtils.getConfigurationWithMultipleQueues(\n+        new Configuration());\n+    csConf.set(YarnConfiguration.RM_PLACEMENT_CONSTRAINTS_HANDLER,\n+        YarnConfiguration.SCHEDULER_RM_PLACEMENT_CONSTRAINTS_HANDLER);\n+\n+    // inject node label manager\n+    MockRM rm1 = new MockRM(csConf) {\n+      @Override\n+      public RMNodeLabelsManager createNodeLabelManager() {\n+        return mgr;\n+      }\n+    };\n+\n+    rm1.getRMContext().setNodeLabelManager(mgr);\n+    rm1.start();\n+\n+    // 4 NMs.\n+    MockNM[] nms = new MockNM[4];\n+    RMNode[] rmNodes = new RMNode[4];\n+    for (int i = 0; i < 4; i++) {\n+      nms[i] = rm1.registerNode(\"192.168.0.\" + i + \":1234\", 10 * GB);\n+      rmNodes[i] = rm1.getRMContext().getRMNodes().get(nms[i].getNodeId());\n+    }\n+\n+    // app1 -> c\n+    RMApp app1 = rm1.submitApp(1 * GB, \"app\", \"user\", null, \"c\");\n+    MockAM am1 = MockRM.launchAndRegisterAM(app1, rm1, nms[0]);\n+\n+    CapacityScheduler cs = (CapacityScheduler) rm1.getResourceScheduler();\n+\n+    PlacementConstraint constraint = PlacementConstraints\n+        .targetNotIn(\"node\", allocationTag(\"t1\"))\n+        .build();\n+    SchedulingRequest sc = SchedulingRequest\n+        .newInstance(0, Priority.newInstance(1),\n+            ExecutionTypeRequest.newInstance(ExecutionType.GUARANTEED),\n+            ImmutableSet.of(\"t1\"),\n+            ResourceSizing.newInstance(1, Resource.newInstance(1024, 1)),\n+            constraint);\n+    AllocateRequest request = AllocateRequest.newBuilder()\n+        .schedulingRequests(ImmutableList.of(sc)).build();\n+    am1.allocate(request);\n+\n+    for (int i = 0; i < 4; i++) {\n+      cs.handle(new NodeUpdateSchedulerEvent(rmNodes[i]));\n+    }\n+\n+    FiCaSchedulerApp schedApp = cs.getApplicationAttempt(\n+        am1.getApplicationAttemptId());\n+    Assert.assertEquals(2, schedApp.getLiveContainers().size());\n+\n+\n+    // Send another request with null placement constraint,\n+    // ensure there is no NPE while handling this request.\n+    sc = SchedulingRequest\n+        .newInstance(1, Priority.newInstance(1),\n+            ExecutionTypeRequest.newInstance(ExecutionType.GUARANTEED),\n+            ImmutableSet.of(\"t2\"),\n+            ResourceSizing.newInstance(2, Resource.newInstance(1024, 1)),\n+            null);\n+    AllocateRequest request1 = AllocateRequest.newBuilder()\n+        .schedulingRequests(ImmutableList.of(sc)).build();\n+    am1.allocate(request1);\n+\n+    for (int i = 0; i < 4; i++) {\n+      cs.handle(new NodeUpdateSchedulerEvent(rmNodes[i]));\n+    }\n+\n+    Assert.assertEquals(4, schedApp.getLiveContainers().size());\n+\n+    rm1.close();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/6468071f137e6d918a7b4799ad54558fa26b25ce/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestSchedulingRequestContainerAllocation.java",
                "sha": "f23fd8f06f3b2628392252f99b6cf75beba8dd01",
                "status": "modified"
            }
        ],
        "message": "YARN-8367. Fix NPE in SingleConstraintAppPlacementAllocator when placement constraint in SchedulingRequest is null. Contributed by Weiwei Yang.",
        "parent": "https://github.com/apache/hadoop/commit/d1e2b8098078af4af31392ed7f2fa350a7d1c3b2",
        "repo": "hadoop",
        "unit_tests": [
            "TestSingleConstraintAppPlacementAllocator.java"
        ]
    },
    "hadoop_65b308f": {
        "bug_id": "hadoop_65b308f",
        "commit": "https://github.com/apache/hadoop/commit/65b308f7834c0770c7e062def0a67bf9a0e065e8",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/65b308f7834c0770c7e062def0a67bf9a0e065e8/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=65b308f7834c0770c7e062def0a67bf9a0e065e8",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -59,6 +59,9 @@ Release 2.1.0-alpha - Unreleased\n     YARN-79. Implement close on all clients to YARN so that RPC clients don't\n     throw exceptions on shut-down. (Vinod Kumar Vavilapalli)\n \n+    YARN-42. Modify NM's non-aggregating logs' handler to stop properly so that\n+    NMs don't get NPEs on startup errors. (Devaraj K via vinodkv)\n+\n Release 0.23.4 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/65b308f7834c0770c7e062def0a67bf9a0e065e8/hadoop-yarn-project/CHANGES.txt",
                "sha": "8106a37f4a93cbc674e5f820b46b8ae0308f07c7",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/65b308f7834c0770c7e062def0a67bf9a0e065e8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java?ref=65b308f7834c0770c7e062def0a67bf9a0e065e8",
                "deletions": 10,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java",
                "patch": "@@ -79,16 +79,18 @@ public void init(Configuration conf) {\n \n   @Override\n   public void stop() {\n-    sched.shutdown();\n-    boolean isShutdown = false;\n-    try {\n-      isShutdown = sched.awaitTermination(10, TimeUnit.SECONDS);\n-    } catch (InterruptedException e) {\n-      sched.shutdownNow();\n-      isShutdown = true;\n-    }\n-    if (!isShutdown) {\n-      sched.shutdownNow();\n+    if (sched != null) {\n+      sched.shutdown();\n+      boolean isShutdown = false;\n+      try {\n+        isShutdown = sched.awaitTermination(10, TimeUnit.SECONDS);\n+      } catch (InterruptedException e) {\n+        sched.shutdownNow();\n+        isShutdown = true;\n+      }\n+      if (!isShutdown) {\n+        sched.shutdownNow();\n+      }\n     }\n     super.stop();\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/65b308f7834c0770c7e062def0a67bf9a0e065e8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/NonAggregatingLogHandler.java",
                "sha": "7ec634b0ebe59ffd65ce3de334d67a3902be4e6e",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hadoop/blob/65b308f7834c0770c7e062def0a67bf9a0e065e8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/TestNonAggregatingLogHandler.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/TestNonAggregatingLogHandler.java?ref=65b308f7834c0770c7e062def0a67bf9a0e065e8",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/TestNonAggregatingLogHandler.java",
                "patch": "@@ -183,6 +183,24 @@ public void testDelayedDelete() {\n     verify(mockSched).schedule(any(Runnable.class), eq(10800l),\n         eq(TimeUnit.SECONDS));\n   }\n+  \n+  @Test\n+  public void testStop() throws Exception {\n+    NonAggregatingLogHandler aggregatingLogHandler = \n+        new NonAggregatingLogHandler(null, null, null);\n+\n+    // It should not throw NullPointerException\n+    aggregatingLogHandler.stop();\n+\n+    NonAggregatingLogHandlerWithMockExecutor logHandler = \n+        new NonAggregatingLogHandlerWithMockExecutor(null, null, null);\n+    logHandler.init(new Configuration());\n+    logHandler.stop();\n+    verify(logHandler.mockSched).shutdown();\n+    verify(logHandler.mockSched)\n+        .awaitTermination(eq(10l), eq(TimeUnit.SECONDS));\n+    verify(logHandler.mockSched).shutdownNow();\n+  }\n \n   private class NonAggregatingLogHandlerWithMockExecutor extends\n       NonAggregatingLogHandler {",
                "raw_url": "https://github.com/apache/hadoop/raw/65b308f7834c0770c7e062def0a67bf9a0e065e8/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/loghandler/TestNonAggregatingLogHandler.java",
                "sha": "36251e47e129c85380a287a665c8ca475ad71f27",
                "status": "modified"
            }
        ],
        "message": "YARN-42. Modify NM's non-aggregating logs' handler to stop properly so that NMs don't get NPEs on startup errors. Contributed by Devaraj K.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1380954 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/ab74b1addeaf36359f4dd300471e2b185792bcd5",
        "repo": "hadoop",
        "unit_tests": [
            "TestNonAggregatingLogHandler.java"
        ]
    },
    "hadoop_6604131": {
        "bug_id": "hadoop_6604131",
        "commit": "https://github.com/apache/hadoop/commit/660413165aa25815bbba66ac2195b0ae17184844",
        "file": [
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/660413165aa25815bbba66ac2195b0ae17184844/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java?ref=660413165aa25815bbba66ac2195b0ae17184844",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "patch": "@@ -620,13 +620,17 @@ private void setAppCollectorsMapToResponse(\n     Map<ApplicationId, RMApp> rmApps = rmContext.getRMApps();\n     // Set collectors for all running apps on this node.\n     for (ApplicationId appId : runningApps) {\n-      AppCollectorData appCollectorData = rmApps.get(appId).getCollectorData();\n-      if (appCollectorData != null) {\n-        liveAppCollectorsMap.put(appId, appCollectorData);\n-      } else {\n-        if (LOG.isDebugEnabled()) {\n-          LOG.debug(\"Collector for applicaton: \" + appId +\n-              \" hasn't registered yet!\");\n+      RMApp app = rmApps.get(appId);\n+      if (app != null) {\n+        AppCollectorData appCollectorData = rmApps.get(appId)\n+            .getCollectorData();\n+        if (appCollectorData != null) {\n+          liveAppCollectorsMap.put(appId, appCollectorData);\n+        } else {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Collector for applicaton: \" + appId +\n+                \" hasn't registered yet!\");\n+          }\n         }\n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/660413165aa25815bbba66ac2195b0ae17184844/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "sha": "cc47e02cb19d53a1f149c594262db56682d1352b",
                "status": "modified"
            }
        ],
        "message": "YARN-6801. NPE in RM while setting collectors map in NodeHeartbeatResponse. Contributed by Vrushali C.",
        "parent": "https://github.com/apache/hadoop/commit/ac7f52df83d2b4758e7debe9416be7db0ec69d2b",
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceTrackerService.java"
        ]
    },
    "hadoop_687ce1a": {
        "bug_id": "hadoop_687ce1a",
        "commit": "https://github.com/apache/hadoop/commit/687ce1a5fca2d58a781e7382bf0333a16d39839d",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/687ce1a5fca2d58a781e7382bf0333a16d39839d/hadoop-hdfs-project/hadoop-hdfs/CHANGES-fs-encryption.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES-fs-encryption.txt?ref=687ce1a5fca2d58a781e7382bf0333a16d39839d",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES-fs-encryption.txt",
                "patch": "@@ -59,3 +59,6 @@ fs-encryption (Unreleased)\n   OPTIMIZATIONS\n \n   BUG FIXES\n+\n+    HDFS-6733. Creating encryption zone results in NPE when\n+    KeyProvider is null. (clamb)",
                "raw_url": "https://github.com/apache/hadoop/raw/687ce1a5fca2d58a781e7382bf0333a16d39839d/hadoop-hdfs-project/hadoop-hdfs/CHANGES-fs-encryption.txt",
                "sha": "c447d49b52ceab9ba1ad9515a622bb17810bf5ff",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/687ce1a5fca2d58a781e7382bf0333a16d39839d/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=687ce1a5fca2d58a781e7382bf0333a16d39839d",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -8448,6 +8448,11 @@ void createEncryptionZone(final String src, String keyNameArg)\n     String keyName = keyNameArg;\n     boolean success = false;\n     try {\n+      if (provider == null) {\n+        throw new IOException(\n+            \"Can't create an encryption zone for \" + src +\n+            \" since no key provider is available.\");\n+      }\n       if (keyName == null || keyName.isEmpty()) {\n         keyName = UUID.randomUUID().toString();\n         createNewKey(keyName, src);",
                "raw_url": "https://github.com/apache/hadoop/raw/687ce1a5fca2d58a781e7382bf0333a16d39839d/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "9b7cb1995839e75c07298a7dc62e449b1192b541",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop/blob/687ce1a5fca2d58a781e7382bf0333a16d39839d/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java?ref=687ce1a5fca2d58a781e7382bf0333a16d39839d",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java",
                "patch": "@@ -68,6 +68,7 @@\n   private MiniDFSCluster cluster;\n   private HdfsAdmin dfsAdmin;\n   private DistributedFileSystem fs;\n+  private File testRootDir;\n \n   protected FileSystemTestWrapper fsWrapper;\n   protected FileContextTestWrapper fcWrapper;\n@@ -78,14 +79,14 @@ public void setup() throws IOException {\n     fsHelper = new FileSystemTestHelper();\n     // Set up java key store\n     String testRoot = fsHelper.getTestRootDir();\n-    File testRootDir = new File(testRoot).getAbsoluteFile();\n+    testRootDir = new File(testRoot).getAbsoluteFile();\n     conf.set(KeyProviderFactory.KEY_PROVIDER_PATH,\n         JavaKeyStoreProvider.SCHEME_NAME + \"://file\" + testRootDir + \"/test.jks\"\n     );\n     cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1).build();\n     Logger.getLogger(EncryptionZoneManager.class).setLevel(Level.TRACE);\n     fs = cluster.getFileSystem();\n-    fsWrapper = new FileSystemTestWrapper(cluster.getFileSystem());\n+    fsWrapper = new FileSystemTestWrapper(fs);\n     fcWrapper = new FileContextTestWrapper(\n         FileContext.getFileContext(cluster.getURI(), conf));\n     dfsAdmin = new HdfsAdmin(cluster.getURI(), conf);\n@@ -429,4 +430,25 @@ public void testCipherSuiteNegotiation() throws Exception {\n     }\n   }\n \n+  @Test(timeout = 120000)\n+  public void testCreateEZWithNoProvider() throws Exception {\n+\n+    final Configuration clusterConf = cluster.getConfiguration(0);\n+    clusterConf.set(KeyProviderFactory.KEY_PROVIDER_PATH, \"\");\n+    cluster.restartNameNode(true);\n+    /* Test failure of create EZ on a directory that doesn't exist. */\n+    final Path zone1 = new Path(\"/zone1\");\n+    /* Normal creation of an EZ */\n+    fsWrapper.mkdir(zone1, FsPermission.getDirDefault(), true);\n+    try {\n+      dfsAdmin.createEncryptionZone(zone1, null);\n+      fail(\"expected exception\");\n+    } catch (IOException e) {\n+      assertExceptionContains(\"since no key provider is available\", e);\n+    }\n+    clusterConf.set(KeyProviderFactory.KEY_PROVIDER_PATH,\n+        JavaKeyStoreProvider.SCHEME_NAME + \"://file\" + testRootDir + \"/test.jks\"\n+    );\n+    cluster.restartNameNode(true);\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/687ce1a5fca2d58a781e7382bf0333a16d39839d/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestEncryptionZones.java",
                "sha": "421396bdf3873e51886b7e34eaad6a0037162d89",
                "status": "modified"
            }
        ],
        "message": "HDFS-6733. Creating encryption zone results in NPE when KeyProvider is null. (clamb)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/fs-encryption@1612843 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/69b75fca7aec5f5cbf79bc7db3915119cef69e65",
        "repo": "hadoop",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_6981b14": {
        "bug_id": "hadoop_6981b14",
        "commit": "https://github.com/apache/hadoop/commit/6981b14003d3ff99fd719515ac08d748fc5f44bd",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=6981b14003d3ff99fd719515ac08d748fc5f44bd",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -509,8 +509,6 @@ Branch-2 ( Unreleased changes )\n     HDFS-3609. libhdfs: don't force the URI to look like hdfs://hostname:port.\n     (Colin Patrick McCabe via eli)\n \n-    HDFS-3654. TestJspHelper#testGetUgi fails with NPE. (eli)\n-\n   BREAKDOWN OF HDFS-3042 SUBTASKS\n \n     HDFS-2185. HDFS portion of ZK-based FailoverController (todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "4c01ec63631384b08b4b0ab66dbb7a2226b41f61",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java?ref=6981b14003d3ff99fd719515ac08d748fc5f44bd",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "patch": "@@ -540,7 +540,7 @@ public static UserGroupInformation getUGI(ServletContext context,\n     final String usernameFromQuery = getUsernameFromQuery(request, tryUgiParameter);\n     final String doAsUserFromQuery = request.getParameter(DoAsParam.NAME);\n \n-    if (UserGroupInformation.isSecurityEnabled()) {\n+    if(UserGroupInformation.isSecurityEnabled()) {\n       final String remoteUser = request.getRemoteUser();\n       String tokenString = request.getParameter(DELEGATION_PARAMETER_NAME);\n       if (tokenString != null) {\n@@ -558,7 +558,7 @@ public static UserGroupInformation getUGI(ServletContext context,\n         DelegationTokenIdentifier id = new DelegationTokenIdentifier();\n         id.readFields(in);\n         final NameNode nn = NameNodeHttpServer.getNameNodeFromContext(context);\n-        nn.verifyToken(id, token.getPassword());\n+        nn.getNamesystem().verifyToken(id, token.getPassword());\n         ugi = id.getUser();\n         if (ugi.getRealUser() == null) {\n           //non-proxy case",
                "raw_url": "https://github.com/apache/hadoop/raw/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/common/JspHelper.java",
                "sha": "c0da8779fd354d13c394ddf75046f8315df5b240",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=6981b14003d3ff99fd719515ac08d748fc5f44bd",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -5464,11 +5464,21 @@ public BlockManager getBlockManager() {\n     return blockManager;\n   }\n   \n+  /**\n+   * Verifies that the given identifier and password are valid and match.\n+   * @param identifier Token identifier.\n+   * @param password Password in the token.\n+   * @throws InvalidToken\n+   */\n+  public synchronized void verifyToken(DelegationTokenIdentifier identifier,\n+      byte[] password) throws InvalidToken {\n+    getDelegationTokenSecretManager().verifyToken(identifier, password);\n+  }\n+  \n   @Override\n   public boolean isGenStampInFuture(long genStamp) {\n     return (genStamp > getGenerationStamp());\n   }\n-\n   @VisibleForTesting\n   public EditLogTailer getEditLogTailer() {\n     return editLogTailer;",
                "raw_url": "https://github.com/apache/hadoop/raw/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "571bde80b7469124187525f0d6e184c117aea607",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=6981b14003d3ff99fd719515ac08d748fc5f44bd",
                "deletions": 14,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -51,7 +51,6 @@\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.protocol.ClientProtocol;\n import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n-import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.NamenodeRole;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants.StartupOption;\n import org.apache.hadoop.hdfs.server.common.Storage.StorageDirectory;\n@@ -79,7 +78,6 @@\n import org.apache.hadoop.security.SecurityUtil;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol;\n-import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n import org.apache.hadoop.tools.GetUserMappingsProtocol;\n import org.apache.hadoop.util.ServicePlugin;\n import org.apache.hadoop.util.StringUtils;\n@@ -1285,18 +1283,7 @@ private synchronized void doImmediateShutdown(Throwable t)\n     }\n     terminate(1, t);\n   }\n-\n-  /**\n-   * Verifies that the given identifier and password are valid and match.\n-   * @param identifier Token identifier.\n-   * @param password Password in the token.\n-   * @throws InvalidToken\n-   */\n-  public synchronized void verifyToken(DelegationTokenIdentifier identifier,\n-      byte[] password) throws InvalidToken {\n-    namesystem.getDelegationTokenSecretManager().verifyToken(identifier, password);\n-  }\n-\n+  \n   /**\n    * Class used to expose {@link NameNode} as context to {@link HAState}\n    */",
                "raw_url": "https://github.com/apache/hadoop/raw/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "d69328565e623774d335e906df61d5be62cab16e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java?ref=6981b14003d3ff99fd719515ac08d748fc5f44bd",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "patch": "@@ -63,7 +63,7 @@\n   \n   public static final String NAMENODE_ADDRESS_ATTRIBUTE_KEY = \"name.node.address\";\n   public static final String FSIMAGE_ATTRIBUTE_KEY = \"name.system.image\";\n-  public static final String NAMENODE_ATTRIBUTE_KEY = \"name.node\";\n+  protected static final String NAMENODE_ATTRIBUTE_KEY = \"name.node\";\n   \n   public NameNodeHttpServer(\n       Configuration conf,",
                "raw_url": "https://github.com/apache/hadoop/raw/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeHttpServer.java",
                "sha": "44b0437d131646de1483dcf1da4787c6e4c18c38",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java?ref=6981b14003d3ff99fd719515ac08d748fc5f44bd",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "patch": "@@ -30,7 +30,6 @@\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n-import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.hdfs.server.namenode.NameNodeHttpServer;\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -70,7 +69,6 @@ public void testGetUgi() throws IOException {\n     conf.set(DFSConfigKeys.FS_DEFAULT_NAME_KEY, \"hdfs://localhost:4321/\");\n     HttpServletRequest request = mock(HttpServletRequest.class);\n     ServletContext context = mock(ServletContext.class);\n-    NameNode nn = mock(NameNode.class);\n     String user = \"TheDoctor\";\n     Text userText = new Text(user);\n     DelegationTokenIdentifier dtId = new DelegationTokenIdentifier(userText,\n@@ -81,8 +79,6 @@ public void testGetUgi() throws IOException {\n     when(request.getParameter(JspHelper.DELEGATION_PARAMETER_NAME)).thenReturn(\n         tokenString);\n     when(request.getRemoteUser()).thenReturn(user);\n-    when(context.getAttribute(\n-        NameNodeHttpServer.NAMENODE_ATTRIBUTE_KEY)).thenReturn(nn);\n \n     //Test attribute in the url to be used as service in the token.\n     when(request.getParameter(JspHelper.NAMENODE_ADDRESS)).thenReturn(",
                "raw_url": "https://github.com/apache/hadoop/raw/6981b14003d3ff99fd719515ac08d748fc5f44bd/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/common/TestJspHelper.java",
                "sha": "c7fafdb13bc8b2bdf26bf5ed53b3353552975e3c",
                "status": "modified"
            }
        ],
        "message": "Revert HDFS-3654. TestJspHelper#testGetUgi fails with NPE.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1362759 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/e2253b539e5d18b7006e8645a573659ca3e77699",
        "repo": "hadoop",
        "unit_tests": [
            "TestJspHelper.java",
            "TestFSNamesystem.java",
            "TestNameNodeHttpServer.java"
        ]
    },
    "hadoop_69dd284": {
        "bug_id": "hadoop_69dd284",
        "commit": "https://github.com/apache/hadoop/commit/69dd2844527f4d6fba99a13ed25538055e0613dd",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/69dd2844527f4d6fba99a13ed25538055e0613dd/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=69dd2844527f4d6fba99a13ed25538055e0613dd",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -1833,6 +1833,8 @@ Release 0.23.0 - Unreleased\n     MAPREDUCE-3258. Fixed AM & JobHistory web-ui to display counters properly.\n     (Siddharth Seth via acmurthy)\n \n+    MAPREDUCE-3290. Fixed a NPE in ClientRMService. (acmurthy) \n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/69dd2844527f4d6fba99a13ed25538055e0613dd/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "c9d61a6ec79ccd272013d739e90575c699f2c4a3",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/69dd2844527f4d6fba99a13ed25538055e0613dd/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java?ref=69dd2844527f4d6fba99a13ed25538055e0613dd",
                "deletions": 4,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "patch": "@@ -59,6 +59,7 @@\n import org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext;\n import org.apache.hadoop.yarn.api.records.NodeReport;\n import org.apache.hadoop.yarn.api.records.QueueInfo;\n+import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.YarnClusterMetrics;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.exceptions.YarnRemoteException;\n@@ -67,10 +68,12 @@\n import org.apache.hadoop.yarn.ipc.RPCUtil;\n import org.apache.hadoop.yarn.ipc.YarnRPC;\n import org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger.AuditConstants;\n+import org.apache.hadoop.yarn.server.resourcemanager.resource.Resources;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNodeReport;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.YarnScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.security.authorize.RMPolicyProvider;\n import org.apache.hadoop.yarn.server.security.ApplicationACLsManager;\n@@ -396,10 +399,18 @@ private NodeReport createNodeReports(RMNode rmNode) {\n     report.setRackName(rmNode.getRackName());\n     report.setCapability(rmNode.getTotalCapability());\n     report.setNodeHealthStatus(rmNode.getNodeHealthStatus());\n-    org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNodeReport schedulerNodeReport = scheduler\n-        .getNodeReport(rmNode.getNodeID());\n-    report.setUsed(schedulerNodeReport.getUsedResource());\n-    report.setNumContainers(schedulerNodeReport.getNumContainers());\n+    \n+    SchedulerNodeReport schedulerNodeReport = \n+        scheduler.getNodeReport(rmNode.getNodeID());\n+    Resource used = Resources.none();\n+    int numContainers = 0;\n+    if (schedulerNodeReport != null) {\n+      used = schedulerNodeReport.getUsedResource();\n+      numContainers = schedulerNodeReport.getNumContainers();\n+    } \n+    report.setUsed(used);\n+    report.setNumContainers(numContainers);\n+\n     return report;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/69dd2844527f4d6fba99a13ed25538055e0613dd/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "sha": "b19c1c17c5e6656c934d4d92ceb72cd6120b6042",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3290. Fixed a NPE in ClientRMService.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1190162 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/34e6de8f856648b4d74997410d9cc8da4010d7c9",
        "repo": "hadoop",
        "unit_tests": [
            "TestClientRMService.java"
        ]
    },
    "hadoop_6bb741f": {
        "bug_id": "hadoop_6bb741f",
        "commit": "https://github.com/apache/hadoop/commit/6bb741ff0ef208a8628bc64d6537999d4cd67955",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/6bb741ff0ef208a8628bc64d6537999d4cd67955/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NodesListManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NodesListManager.java?ref=6bb741ff0ef208a8628bc64d6537999d4cd67955",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NodesListManager.java",
                "patch": "@@ -44,6 +44,7 @@\n import org.apache.hadoop.util.StringUtils;\n import org.apache.hadoop.yarn.api.records.NodeId;\n import org.apache.hadoop.yarn.api.records.NodeState;\n+import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.event.EventHandler;\n import org.apache.hadoop.yarn.exceptions.YarnException;\n@@ -242,7 +243,8 @@ private void setDecomissionedNMs() {\n     for (final String host : excludeList) {\n       NodeId nodeId = createUnknownNodeId(host);\n       RMNodeImpl rmNode = new RMNodeImpl(nodeId,\n-          rmContext, host, -1, -1, new UnknownNode(host), null, null);\n+          rmContext, host, -1, -1, new UnknownNode(host),\n+          Resource.newInstance(0, 0), \"unknown\");\n       rmContext.getInactiveRMNodes().put(nodeId, rmNode);\n       rmNode.handle(new RMNodeEvent(nodeId, RMNodeEventType.DECOMMISSION));\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/6bb741ff0ef208a8628bc64d6537999d4cd67955/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/NodesListManager.java",
                "sha": "7d69f930cc6562f13d937e24d12a8068789c3ae7",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/6bb741ff0ef208a8628bc64d6537999d4cd67955/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java?ref=6bb741ff0ef208a8628bc64d6537999d4cd67955",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "patch": "@@ -256,4 +256,8 @@ public long getMemory() {\n   public int getvCores() {\n     return vCores;\n   }\n+\n+  public String getVersion() {\n+    return version;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/6bb741ff0ef208a8628bc64d6537999d4cd67955/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockNM.java",
                "sha": "32cdb1b195beeaf53af5aab7ffbe3875ee74fb04",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/6bb741ff0ef208a8628bc64d6537999d4cd67955/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMRestart.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMRestart.java?ref=6bb741ff0ef208a8628bc64d6537999d4cd67955",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMRestart.java",
                "patch": "@@ -40,6 +40,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n+import java.util.concurrent.ConcurrentMap;\n \n import org.apache.commons.io.FileUtils;\n import org.apache.commons.logging.Log;\n@@ -103,6 +104,7 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.TestSchedulerUtils;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.YarnScheduler;\n@@ -1957,6 +1959,9 @@ public void testDecomissionedNMsMetricsOnRMRestart() throws Exception {\n       rm1.start();\n       MockNM nm1 = rm1.registerNode(\"localhost:1234\", 8000);\n       MockNM nm2 = rm1.registerNode(\"host2:1234\", 8000);\n+      Resource expectedCapability =\n+          Resource.newInstance(nm1.getMemory(), nm1.getvCores());\n+      String expectedVersion = nm1.getVersion();\n       Assert\n           .assertEquals(0,\n               ClusterMetrics.getMetrics().getNumDecommisionedNMs());\n@@ -1978,6 +1983,7 @@ public void testDecomissionedNMsMetricsOnRMRestart() throws Exception {\n       Assert\n           .assertEquals(2,\n               ClusterMetrics.getMetrics().getNumDecommisionedNMs());\n+      verifyNodesAfterDecom(rm1, 2, expectedCapability, expectedVersion);\n       rm1.stop();\n       rm1 = null;\n       Assert\n@@ -1991,6 +1997,7 @@ public void testDecomissionedNMsMetricsOnRMRestart() throws Exception {\n       Assert\n           .assertEquals(2,\n               ClusterMetrics.getMetrics().getNumDecommisionedNMs());\n+      verifyNodesAfterDecom(rm2, 2, Resource.newInstance(0, 0), \"unknown\");\n     } finally {\n       if (rm1 != null) {\n         rm1.stop();\n@@ -2001,6 +2008,18 @@ public void testDecomissionedNMsMetricsOnRMRestart() throws Exception {\n     }\n   }\n \n+  private void verifyNodesAfterDecom(MockRM rm, int numNodes,\n+                                     Resource expectedCapability,\n+                                     String expectedVersion) {\n+    ConcurrentMap<NodeId, RMNode> inactiveRMNodes =\n+        rm.getRMContext().getInactiveRMNodes();\n+    Assert.assertEquals(numNodes, inactiveRMNodes.size());\n+    for (RMNode rmNode : inactiveRMNodes.values()) {\n+      Assert.assertEquals(expectedCapability, rmNode.getTotalCapability());\n+      Assert.assertEquals(expectedVersion, rmNode.getNodeManagerVersion());\n+    }\n+  }\n+\n   // Test Delegation token is renewed synchronously so that recover events\n   // can be processed before any other external incoming events, specifically\n   // the ContainerFinished event on NM re-registraton.",
                "raw_url": "https://github.com/apache/hadoop/raw/6bb741ff0ef208a8628bc64d6537999d4cd67955/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMRestart.java",
                "sha": "a98a124057593e55aba42aaeac259159768b53b4",
                "status": "modified"
            }
        ],
        "message": "YARN-5837. NPE when getting node status of a decommissioned node after an RM restart. Contributed by Robert Kanter",
        "parent": "https://github.com/apache/hadoop/commit/0c0ab102ab392ba07ed2aa8d8a67eef4c20cad9b",
        "repo": "hadoop",
        "unit_tests": [
            "TestNodesListManager.java"
        ]
    },
    "hadoop_6d96a28": {
        "bug_id": "hadoop_6d96a28",
        "commit": "https://github.com/apache/hadoop/commit/6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -87,7 +87,10 @@ Trunk (unreleased changes)\n     HDFS-3037. TestMulitipleNNDataBlockScanner#testBlockScannerAfterRestart is\n     racy. (atm)\n \n-    HDFS-2966 TestNameNodeMetrics tests can fail under load. (stevel)\n+    HDFS-2966. TestNameNodeMetrics tests can fail under load. (stevel)\n+\n+    HDFS-3067. NPE in DFSInputStream.readBuffer if read is repeated on\n+    corrupted block. (Henry Robinson via atm)\n \n Release 0.23.3 - UNRELEASED \n ",
                "raw_url": "https://github.com/apache/hadoop/raw/6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "3cc339a596e7986ab09a071236546d5c0b042b5b",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java?ref=6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "patch": "@@ -538,7 +538,9 @@ public synchronized int read(byte buf[], int off, int len) throws IOException {\n       int retries = 2;\n       while (retries > 0) {\n         try {\n-          if (pos > blockEnd) {\n+          // currentNode can be left as null if previous read had a checksum\n+          // error on the same block. See HDFS-3067\n+          if (pos > blockEnd || currentNode == null) {\n             currentNode = blockSeekTo(pos);\n           }\n           int realLen = (int) Math.min(len, (blockEnd - pos + 1L));",
                "raw_url": "https://github.com/apache/hadoop/raw/6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "sha": "71c8a500a83b1fa157204dd18a14c16863585a6d",
                "status": "modified"
            },
            {
                "additions": 49,
                "blob_url": "https://github.com/apache/hadoop/blob/6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientRetries.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientRetries.java?ref=6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientRetries.java",
                "patch": "@@ -48,6 +48,7 @@\n import org.apache.hadoop.fs.FileChecksum;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.UnresolvedLinkException;\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.protocol.DatanodeID;\n import org.apache.hadoop.hdfs.protocol.Block;\n@@ -64,6 +65,7 @@\n import org.apache.hadoop.ipc.Server;\n import org.apache.hadoop.ipc.RpcPayloadHeader.RpcKind;\n import org.apache.hadoop.net.NetUtils;\n+import org.apache.hadoop.test.GenericTestUtils;\n import org.mockito.internal.stubbing.answers.ThrowsException;\n import org.mockito.invocation.InvocationOnMock;\n import org.mockito.stubbing.Answer;\n@@ -649,5 +651,52 @@ public void testClientDNProtocolTimeout() throws IOException {\n       server.stop();\n     }\n   }\n+\n+  /**\n+   * Test that checksum failures are recovered from by the next read on the same\n+   * DFSInputStream. Corruption information is not persisted from read call to\n+   * read call, so the client should expect consecutive calls to behave the same\n+   * way. See HDFS-3067.\n+   */\n+  public void testRetryOnChecksumFailure()\n+      throws UnresolvedLinkException, IOException {\n+    HdfsConfiguration conf = new HdfsConfiguration();\n+    MiniDFSCluster cluster =\n+      new MiniDFSCluster.Builder(conf).numDataNodes(1).build();\n+\n+    try {\n+      final short REPL_FACTOR = 1;\n+      final long FILE_LENGTH = 512L;\n+      cluster.waitActive();\n+      FileSystem fs = cluster.getFileSystem();\n+\n+      Path path = new Path(\"/corrupted\");\n+\n+      DFSTestUtil.createFile(fs, path, FILE_LENGTH, REPL_FACTOR, 12345L);\n+      DFSTestUtil.waitReplication(fs, path, REPL_FACTOR);\n+\n+      ExtendedBlock block = DFSTestUtil.getFirstBlock(fs, path);\n+      int blockFilesCorrupted = cluster.corruptBlockOnDataNodes(block);\n+      assertEquals(\"All replicas not corrupted\", REPL_FACTOR,\n+          blockFilesCorrupted);\n+\n+      InetSocketAddress nnAddr =\n+        new InetSocketAddress(\"localhost\", cluster.getNameNodePort());\n+      DFSClient client = new DFSClient(nnAddr, conf);\n+      DFSInputStream dis = client.open(path.toString());\n+      byte[] arr = new byte[(int)FILE_LENGTH];\n+      for (int i = 0; i < 2; ++i) {\n+        try {\n+          dis.read(arr, 0, (int)FILE_LENGTH);\n+          fail(\"Expected ChecksumException not thrown\");\n+        } catch (Exception ex) {\n+          GenericTestUtils.assertExceptionContains(\n+              \"Checksum error\", ex);\n+        }\n+      }\n+    } finally {\n+      cluster.shutdown();\n+    }\n+  }\n }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/6d96a28a088a7ad465f0fbbb94d5ecd1947f7ffc/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSClientRetries.java",
                "sha": "1e39b9a40d1854b90379763f0ac23c94a4e83d7a",
                "status": "modified"
            }
        ],
        "message": "HDFS-3067. NPE in DFSInputStream.readBuffer if read is repeated on corrupted block. Contributed by Henry Robinson.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1301182 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/1177d4edc29f839b8df1038c4fbf37f56f56a2a0",
        "repo": "hadoop",
        "unit_tests": [
            "TestDFSInputStream.java"
        ]
    },
    "hadoop_710a869": {
        "bug_id": "hadoop_710a869",
        "commit": "https://github.com/apache/hadoop/commit/710a8693e57235d283e704c983722079c8fd6f38",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=710a8693e57235d283e704c983722079c8fd6f38",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -227,6 +227,9 @@ Release 2.5.0 - UNRELEASED\n     YARN-2128. FairScheduler: Incorrect calculation of amResource usage.\n     (Wei Yan via kasha)\n \n+    YARN-2124. Fixed NPE in ProportionalCapacityPreemptionPolicy. (Wangda Tan\n+    via jianhe)\n+\n Release 2.4.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/CHANGES.txt",
                "sha": "401fb102bf47b883c2c57fbf85ee5e61fd4281bb",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java?ref=710a8693e57235d283e704c983722079c8fd6f38",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "patch": "@@ -327,7 +327,7 @@ protected static void validateConfigs(Configuration conf) {\n    * RMActiveServices handles all the Active services in the RM.\n    */\n   @Private\n-  class RMActiveServices extends CompositeService {\n+  public class RMActiveServices extends CompositeService {\n \n     private DelegationTokenRenewer delegationTokenRenewer;\n     private EventHandler<SchedulerEvent> schedulerDispatcher;\n@@ -526,11 +526,9 @@ protected void createPolicyMonitors() {\n                   (PreemptableResourceScheduler) scheduler));\n           for (SchedulingEditPolicy policy : policies) {\n             LOG.info(\"LOADING SchedulingEditPolicy:\" + policy.getPolicyName());\n-            policy.init(conf, rmContext.getDispatcher().getEventHandler(),\n-                (PreemptableResourceScheduler) scheduler);\n             // periodically check whether we need to take action to guarantee\n             // constraints\n-            SchedulingMonitor mon = new SchedulingMonitor(policy);\n+            SchedulingMonitor mon = new SchedulingMonitor(rmContext, policy);\n             addService(mon);\n           }\n         } else {",
                "raw_url": "https://github.com/apache/hadoop/raw/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "sha": "77de2090a00479282f863f110299a592f5c0d77e",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/SchedulingMonitor.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/SchedulingMonitor.java?ref=710a8693e57235d283e704c983722079c8fd6f38",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/SchedulingMonitor.java",
                "patch": "@@ -21,6 +21,8 @@\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.service.AbstractService;\n+import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler;\n \n import com.google.common.annotations.VisibleForTesting;\n \n@@ -34,18 +36,29 @@\n   private Thread checkerThread;\n   private volatile boolean stopped;\n   private long monitorInterval;\n+  private RMContext rmContext;\n \n-  public SchedulingMonitor(SchedulingEditPolicy scheduleEditPolicy) {\n+  public SchedulingMonitor(RMContext rmContext,\n+      SchedulingEditPolicy scheduleEditPolicy) {\n     super(\"SchedulingMonitor (\" + scheduleEditPolicy.getPolicyName() + \")\");\n     this.scheduleEditPolicy = scheduleEditPolicy;\n-    this.monitorInterval = scheduleEditPolicy.getMonitoringInterval();\n+    this.rmContext = rmContext;\n   }\n \n   public long getMonitorInterval() {\n     return monitorInterval;\n   }\n+  \n+  @VisibleForTesting\n+  public synchronized SchedulingEditPolicy getSchedulingEditPolicy() {\n+    return scheduleEditPolicy;\n+  }\n \n+  @SuppressWarnings(\"unchecked\")\n   public void serviceInit(Configuration conf) throws Exception {\n+    scheduleEditPolicy.init(conf, rmContext.getDispatcher().getEventHandler(),\n+        (PreemptableResourceScheduler) rmContext.getScheduler());\n+    this.monitorInterval = scheduleEditPolicy.getMonitoringInterval();\n     super.serviceInit(conf);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/SchedulingMonitor.java",
                "sha": "1682f7d8612602446bc52b157945b5901a6a1a11",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java?ref=710a8693e57235d283e704c983722079c8fd6f38",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
                "patch": "@@ -165,6 +165,11 @@ public void init(Configuration config,\n     observeOnly = config.getBoolean(OBSERVE_ONLY, false);\n     rc = scheduler.getResourceCalculator();\n   }\n+  \n+  @VisibleForTesting\n+  public ResourceCalculator getResourceCalculator() {\n+    return rc;\n+  }\n \n   @Override\n   public void editSchedule(){",
                "raw_url": "https://github.com/apache/hadoop/raw/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/ProportionalCapacityPreemptionPolicy.java",
                "sha": "6d1516158b5aad26990a58d562def6b0ca5df243",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java?ref=710a8693e57235d283e704c983722079c8fd6f38",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java",
                "patch": "@@ -571,4 +571,8 @@ public void clearQueueMetrics(RMApp app) {\n       .getSchedulerApplications().get(app.getApplicationId()).getQueue()\n       .getMetrics().clearQueueMetrics();\n   }\n+  \n+  public RMActiveServices getRMActiveService() {\n+    return activeServices;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java",
                "sha": "67eac76e5f335d4df535ec2c888619ec677e9ae9",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/hadoop/blob/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java",
                "changes": 64,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java?ref=710a8693e57235d283e704c983722079c8fd6f38",
                "deletions": 11,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java",
                "patch": "@@ -17,6 +17,25 @@\n  */\n package org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity;\n \n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.MAX_IGNORED_OVER_CAPACITY;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.MONITORING_INTERVAL;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.NATURAL_TERMINATION_FACTOR;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.OBSERVE_ONLY;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.TOTAL_PREEMPTION_PER_ROUND;\n+import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.WAIT_TIME_BEFORE_KILL;\n+import static org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEventType.KILL_CONTAINER;\n+import static org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEventType.PREEMPT_CONTAINER;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.fail;\n+import static org.mockito.Matchers.argThat;\n+import static org.mockito.Matchers.isA;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.never;\n+import static org.mockito.Mockito.times;\n+import static org.mockito.Mockito.verify;\n+import static org.mockito.Mockito.when;\n+\n import java.util.ArrayList;\n import java.util.Comparator;\n import java.util.Deque;\n@@ -27,12 +46,16 @@\n import java.util.TreeSet;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.service.Service;\n import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.event.EventHandler;\n+import org.apache.hadoop.yarn.server.resourcemanager.MockRM;\n+import org.apache.hadoop.yarn.server.resourcemanager.monitor.SchedulingMonitor;\n import org.apache.hadoop.yarn.server.resourcemanager.resource.Priority;\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEvent;\n@@ -52,17 +75,6 @@\n import org.mockito.ArgumentCaptor;\n import org.mockito.ArgumentMatcher;\n \n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.MAX_IGNORED_OVER_CAPACITY;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.MONITORING_INTERVAL;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.NATURAL_TERMINATION_FACTOR;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.OBSERVE_ONLY;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.TOTAL_PREEMPTION_PER_ROUND;\n-import static org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy.WAIT_TIME_BEFORE_KILL;\n-import static org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEventType.KILL_CONTAINER;\n-import static org.apache.hadoop.yarn.server.resourcemanager.scheduler.ContainerPreemptEventType.PREEMPT_CONTAINER;\n-import static org.junit.Assert.*;\n-import static org.mockito.Mockito.*;\n-\n public class TestProportionalCapacityPreemptionPolicy {\n \n   static final long TS = 3141592653L;\n@@ -424,6 +436,36 @@ public void testContainerOrdering(){\n     assert containers.get(4).equals(rm5);\n \n   }\n+  \n+  @Test\n+  public void testPolicyInitializeAfterSchedulerInitialized() {\n+    Configuration conf = new Configuration();\n+    conf.set(YarnConfiguration.RM_SCHEDULER_MONITOR_POLICIES,\n+        ProportionalCapacityPreemptionPolicy.class.getCanonicalName());\n+    conf.setBoolean(YarnConfiguration.RM_SCHEDULER_ENABLE_MONITORS, true);\n+    \n+    @SuppressWarnings(\"resource\")\n+    MockRM rm = new MockRM(conf);\n+    rm.init(conf);\n+    \n+    // ProportionalCapacityPreemptionPolicy should be initialized after\n+    // CapacityScheduler initialized. We will \n+    // 1) find SchedulingMonitor from RMActiveService's service list, \n+    // 2) check if ResourceCalculator in policy is null or not. \n+    // If it's not null, we can come to a conclusion that policy initialized\n+    // after scheduler got initialized\n+    for (Service service : rm.getRMActiveService().getServices()) {\n+      if (service instanceof SchedulingMonitor) {\n+        ProportionalCapacityPreemptionPolicy policy =\n+            (ProportionalCapacityPreemptionPolicy) ((SchedulingMonitor) service)\n+                .getSchedulingEditPolicy();\n+        assertNotNull(policy.getResourceCalculator());\n+        return;\n+      }\n+    }\n+    \n+    fail(\"Failed to find SchedulingMonitor service, please check what happened\");\n+  }\n \n   static class IsPreemptionRequestFor\n       extends ArgumentMatcher<ContainerPreemptEvent> {",
                "raw_url": "https://github.com/apache/hadoop/raw/710a8693e57235d283e704c983722079c8fd6f38/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/monitor/capacity/TestProportionalCapacityPreemptionPolicy.java",
                "sha": "d0a80eb20bb320d8b1c1ec7009743049bc09eb3f",
                "status": "modified"
            }
        ],
        "message": "YARN-2124. Fixed NPE in ProportionalCapacityPreemptionPolicy. Contributed by Wangda Tan\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1601964 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/e98529858edeed11c4f900b0db30d7e4eb2ab1ec",
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceManager.java",
            "TestSchedulingMonitor.java",
            "TestProportionalCapacityPreemptionPolicy.java"
        ]
    },
    "hadoop_74748ec": {
        "bug_id": "hadoop_74748ec",
        "commit": "https://github.com/apache/hadoop/commit/74748ec62570f92d57dbad3ba4cca47402990db5",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/74748ec62570f92d57dbad3ba4cca47402990db5/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=74748ec62570f92d57dbad3ba4cca47402990db5",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -1695,6 +1695,8 @@ Release 0.23.0 - Unreleased\n     MAPREDUCE-2788. Normalize resource requests in FifoScheduler\n     appropriately. (Ahmed Radwan via acmurthy) \n \n+    MAPREDUCE-2693. Fix NPE in job-blacklisting. (Hitesh Shah via acmurthy) \n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/74748ec62570f92d57dbad3ba4cca47402990db5/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "22917ec8c83d7882471017e315f2646fb1a47dea",
                "status": "modified"
            },
            {
                "additions": 123,
                "blob_url": "https://github.com/apache/hadoop/blob/74748ec62570f92d57dbad3ba4cca47402990db5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
                "changes": 160,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java?ref=74748ec62570f92d57dbad3ba4cca47402990db5",
                "deletions": 37,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
                "patch": "@@ -509,18 +509,6 @@ void addMap(ContainerRequestEvent event) {\n         request = new ContainerRequest(event, PRIORITY_FAST_FAIL_MAP);\n       } else {\n         for (String host : event.getHosts()) {\n-          //host comes from data splitLocations which are hostnames. Containers\n-          // use IP addresses.\n-          //TODO Temporary fix for locality. Use resolvers from h-common. \n-          // Cache to make this more efficient ?\n-          InetAddress addr = null;\n-          try {\n-            addr = InetAddress.getByName(host);\n-          } catch (UnknownHostException e) {\n-            LOG.warn(\"Unable to resolve host to IP for host [: \" + host + \"]\");\n-          }\n-          if (addr != null) //Fallback to host if resolve fails.\n-            host = addr.getHostAddress();\n           LinkedList<TaskAttemptId> list = mapsHostMapping.get(host);\n           if (list == null) {\n             list = new LinkedList<TaskAttemptId>();\n@@ -557,26 +545,101 @@ private void assign(List<Container> allocatedContainers) {\n       while (it.hasNext()) {\n         Container allocated = it.next();\n         LOG.info(\"Assigning container \" + allocated);\n-        ContainerRequest assigned = assign(allocated);\n-          \n-        if (assigned != null) {\n-          // Update resource requests\n-          decContainerReq(assigned);\n+        \n+        // check if allocated container meets memory requirements \n+        // and whether we have any scheduled tasks that need \n+        // a container to be assigned\n+        boolean isAssignable = true;\n+        Priority priority = allocated.getPriority();\n+        if (PRIORITY_FAST_FAIL_MAP.equals(priority) \n+            || PRIORITY_MAP.equals(priority)) {\n+          if (allocated.getResource().getMemory() < mapResourceReqt\n+              || maps.isEmpty()) {\n+            LOG.info(\"Cannot assign container \" + allocated \n+                + \" for a map as either \"\n+                + \" container memory less than required \" + mapResourceReqt\n+                + \" or no pending map tasks - maps.isEmpty=\" \n+                + maps.isEmpty()); \n+            isAssignable = false; \n+          }\n+        } \n+        else if (PRIORITY_REDUCE.equals(priority)) {\n+          if (allocated.getResource().getMemory() < reduceResourceReqt\n+              || reduces.isEmpty()) {\n+            LOG.info(\"Cannot assign container \" + allocated \n+                + \" for a reduce as either \"\n+                + \" container memory less than required \" + reduceResourceReqt\n+                + \" or no pending reduce tasks - reduces.isEmpty=\" \n+                + reduces.isEmpty()); \n+            isAssignable = false;\n+          }\n+        }          \n+        \n+        boolean blackListed = false;         \n+        ContainerRequest assigned = null;\n+        \n+        if (isAssignable) {\n+          // do not assign if allocated container is on a  \n+          // blacklisted host\n+          blackListed = isNodeBlacklisted(allocated.getNodeId().getHost());\n+          if (blackListed) {\n+            // we need to request for a new container \n+            // and release the current one\n+            LOG.info(\"Got allocated container on a blacklisted \"\n+                + \" host. Releasing container \" + allocated);\n+\n+            // find the request matching this allocated container \n+            // and replace it with a new one \n+            ContainerRequest toBeReplacedReq = \n+                getContainerReqToReplace(allocated);\n+            if (toBeReplacedReq != null) {\n+              LOG.info(\"Placing a new container request for task attempt \" \n+                  + toBeReplacedReq.attemptID);\n+              ContainerRequest newReq = \n+                  getFilteredContainerRequest(toBeReplacedReq);\n+              decContainerReq(toBeReplacedReq);\n+              if (toBeReplacedReq.attemptID.getTaskId().getTaskType() ==\n+                  TaskType.MAP) {\n+                maps.put(newReq.attemptID, newReq);\n+              }\n+              else {\n+                reduces.put(newReq.attemptID, newReq);\n+              }\n+              addContainerReq(newReq);\n+            }\n+            else {\n+              LOG.info(\"Could not map allocated container to a valid request.\"\n+                  + \" Releasing allocated container \" + allocated);\n+            }\n+          }\n+          else {\n+            assigned = assign(allocated);\n+            if (assigned != null) {\n+              // Update resource requests\n+              decContainerReq(assigned);\n \n-          // send the container-assigned event to task attempt\n-          eventHandler.handle(new TaskAttemptContainerAssignedEvent(\n-              assigned.attemptID, allocated));\n+              // send the container-assigned event to task attempt\n+              eventHandler.handle(new TaskAttemptContainerAssignedEvent(\n+                  assigned.attemptID, allocated));\n \n-          assignedRequests.add(allocated.getId(), assigned.attemptID);\n-          \n-          LOG.info(\"Assigned container (\" + allocated + \") \" +\n-              \" to task \" + assigned.attemptID +\n-              \" on node \" + allocated.getNodeId().toString());\n-        } else {\n-          //not assigned to any request, release the container\n-          LOG.info(\"Releasing unassigned and invalid container \" + allocated\n-              + \". RM has gone crazy, someone go look!\"\n-              + \" Hey RM, if you are so rich, go donate to non-profits!\");\n+              assignedRequests.add(allocated.getId(), assigned.attemptID);\n+\n+              LOG.info(\"Assigned container (\" + allocated + \") \" +\n+                  \" to task \" + assigned.attemptID +\n+                  \" on node \" + allocated.getNodeId().toString());\n+            }\n+            else {\n+              //not assigned to any request, release the container\n+              LOG.info(\"Releasing unassigned and invalid container \" \n+                  + allocated + \". RM has gone crazy, someone go look!\"\n+                  + \" Hey RM, if you are so rich, go donate to non-profits!\");\n+            }\n+          }\n+        }\n+        \n+        // release container if it was blacklisted \n+        // or if we could not assign it \n+        if (blackListed || assigned == null) {\n           containersReleased++;\n           release(allocated.getId());\n         }\n@@ -604,12 +667,37 @@ private ContainerRequest assign(Container allocated) {\n       return assigned;\n     }\n     \n+    private ContainerRequest getContainerReqToReplace(Container allocated) {\n+      Priority priority = allocated.getPriority();\n+      ContainerRequest toBeReplaced = null;\n+      if (PRIORITY_FAST_FAIL_MAP.equals(priority) \n+          || PRIORITY_MAP.equals(priority)) {\n+        // allocated container was for a map\n+        String host = allocated.getNodeId().getHost();\n+        LinkedList<TaskAttemptId> list = mapsHostMapping.get(host);\n+        if (list != null && list.size() > 0) {\n+          TaskAttemptId tId = list.removeLast();\n+          if (maps.containsKey(tId)) {\n+            toBeReplaced = maps.remove(tId);\n+          }\n+        }\n+        else {\n+          TaskAttemptId tId = maps.keySet().iterator().next();\n+          toBeReplaced = maps.remove(tId);          \n+        }        \n+      }\n+      else if (PRIORITY_REDUCE.equals(priority)) {\n+        TaskAttemptId tId = reduces.keySet().iterator().next();\n+        toBeReplaced = reduces.remove(tId);    \n+      }\n+      return toBeReplaced;\n+    }\n+    \n     \n     private ContainerRequest assignToFailedMap(Container allocated) {\n       //try to assign to earlierFailedMaps if present\n       ContainerRequest assigned = null;\n-      while (assigned == null && earlierFailedMaps.size() > 0 && \n-          allocated.getResource().getMemory() >= mapResourceReqt) {\n+      while (assigned == null && earlierFailedMaps.size() > 0) {\n         TaskAttemptId tId = earlierFailedMaps.removeFirst();\n         if (maps.containsKey(tId)) {\n           assigned = maps.remove(tId);\n@@ -627,8 +715,7 @@ private ContainerRequest assignToFailedMap(Container allocated) {\n     private ContainerRequest assignToReduce(Container allocated) {\n       ContainerRequest assigned = null;\n       //try to assign to reduces if present\n-      if (assigned == null && reduces.size() > 0\n-          && allocated.getResource().getMemory() >= reduceResourceReqt) {\n+      if (assigned == null && reduces.size() > 0) {\n         TaskAttemptId tId = reduces.keySet().iterator().next();\n         assigned = reduces.remove(tId);\n         LOG.info(\"Assigned to reduce\");\n@@ -640,9 +727,8 @@ private ContainerRequest assignToMap(Container allocated) {\n     //try to assign to maps if present \n       //first by host, then by rack, followed by *\n       ContainerRequest assigned = null;\n-      while (assigned == null && maps.size() > 0\n-          && allocated.getResource().getMemory() >= mapResourceReqt) {\n-        String host = getHost(allocated.getNodeId().toString());\n+      while (assigned == null && maps.size() > 0) {\n+        String host = allocated.getNodeId().getHost();\n         LinkedList<TaskAttemptId> list = mapsHostMapping.get(host);\n         while (list != null && list.size() > 0) {\n           LOG.info(\"Host matched to the request list \" + host);",
                "raw_url": "https://github.com/apache/hadoop/raw/74748ec62570f92d57dbad3ba4cca47402990db5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerAllocator.java",
                "sha": "fc45b8f11b285a1bb0fc475c184f61b8e1c65175",
                "status": "modified"
            },
            {
                "additions": 73,
                "blob_url": "https://github.com/apache/hadoop/blob/74748ec62570f92d57dbad3ba4cca47402990db5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java",
                "changes": 84,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java?ref=74748ec62570f92d57dbad3ba4cca47402990db5",
                "deletions": 11,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java",
                "patch": "@@ -18,6 +18,8 @@\n \n package org.apache.hadoop.mapreduce.v2.app.rm;\n \n+import java.net.InetAddress;\n+import java.net.UnknownHostException;\n import java.util.ArrayList;\n import java.util.HashMap;\n import java.util.HashSet;\n@@ -63,7 +65,7 @@\n   //Key->ResourceName (e.g., hostname, rackname, *)\n   //Value->Map\n   //Key->Resource Capability\n-  //Value->ResourceReqeust\n+  //Value->ResourceRequest\n   private final Map<Priority, Map<String, Map<Resource, ResourceRequest>>>\n   remoteRequestsTable =\n       new TreeMap<Priority, Map<String, Map<Resource, ResourceRequest>>>();\n@@ -87,14 +89,22 @@ public RMContainerRequestor(ClientService clientService, AppContext context) {\n     final String[] racks;\n     //final boolean earlierAttemptFailed;\n     final Priority priority;\n+    \n     public ContainerRequest(ContainerRequestEvent event, Priority priority) {\n-      this.attemptID = event.getAttemptID();\n-      this.capability = event.getCapability();\n-      this.hosts = event.getHosts();\n-      this.racks = event.getRacks();\n-      //this.earlierAttemptFailed = event.getEarlierAttemptFailed();\n+      this(event.getAttemptID(), event.getCapability(), event.getHosts(),\n+          event.getRacks(), priority);\n+    }\n+    \n+    public ContainerRequest(TaskAttemptId attemptID,\n+        Resource capability, String[] hosts, String[] racks, \n+        Priority priority) {\n+      this.attemptID = attemptID;\n+      this.capability = capability;\n+      this.hosts = hosts;\n+      this.racks = racks;\n       this.priority = priority;\n     }\n+    \n   }\n \n   @Override\n@@ -149,14 +159,37 @@ protected void containerFailedOnHost(String hostName) {\n       //remove all the requests corresponding to this hostname\n       for (Map<String, Map<Resource, ResourceRequest>> remoteRequests \n           : remoteRequestsTable.values()){\n-        //remove from host\n-        Map<Resource, ResourceRequest> reqMap = remoteRequests.remove(hostName);\n+        //remove from host if no pending allocations\n+        boolean foundAll = true;\n+        Map<Resource, ResourceRequest> reqMap = remoteRequests.get(hostName);\n         if (reqMap != null) {\n           for (ResourceRequest req : reqMap.values()) {\n-            ask.remove(req);\n+            if (!ask.remove(req)) {\n+              foundAll = false;\n+            }\n+            else {\n+              // if ask already sent to RM, we can try and overwrite it if possible.\n+              // send a new ask to RM with numContainers\n+              // specified for the blacklisted host to be 0.\n+              ResourceRequest zeroedRequest = BuilderUtils.newResourceRequest(req);\n+              zeroedRequest.setNumContainers(0);\n+              // to be sent to RM on next heartbeat\n+              ask.add(zeroedRequest);\n+            }\n           }\n+          // if all requests were still in ask queue\n+          // we can remove this request\n+          if (foundAll) {\n+            remoteRequests.remove(hostName);\n+          }     \n         }\n-        //TODO: remove from rack\n+        // TODO handling of rack blacklisting\n+        // Removing from rack should be dependent on no. of failures within the rack \n+        // Blacklisting a rack on the basis of a single node's blacklisting \n+        // may be overly aggressive. \n+        // Node failures could be co-related with other failures on the same rack \n+        // but we probably need a better approach at trying to decide how and when \n+        // to blacklist a rack\n       }\n     } else {\n       nodeFailures.put(hostName, failures);\n@@ -171,7 +204,9 @@ protected void addContainerReq(ContainerRequest req) {\n     // Create resource requests\n     for (String host : req.hosts) {\n       // Data-local\n-      addResourceRequest(req.priority, host, req.capability);\n+      if (!isNodeBlacklisted(host)) {\n+        addResourceRequest(req.priority, host, req.capability);\n+      }      \n     }\n \n     // Nothing Rack-local for now\n@@ -234,6 +269,14 @@ private void decResourceRequest(Priority priority, String resourceName,\n     Map<String, Map<Resource, ResourceRequest>> remoteRequests =\n       this.remoteRequestsTable.get(priority);\n     Map<Resource, ResourceRequest> reqMap = remoteRequests.get(resourceName);\n+    if (reqMap == null) {\n+      // as we modify the resource requests by filtering out blacklisted hosts \n+      // when they are added, this value may be null when being \n+      // decremented\n+      LOG.debug(\"Not decrementing resource as \" + resourceName\n+          + \" is not present in request table\");\n+      return;\n+    }\n     ResourceRequest remoteRequest = reqMap.get(capability);\n \n     LOG.info(\"BEFORE decResourceRequest:\" + \" applicationId=\" + applicationId.getId()\n@@ -267,4 +310,23 @@ protected void release(ContainerId containerId) {\n     release.add(containerId);\n   }\n   \n+  protected boolean isNodeBlacklisted(String hostname) {\n+    if (!nodeBlacklistingEnabled) {\n+      return false;\n+    }\n+    return blacklistedNodes.contains(hostname);\n+  }\n+  \n+  protected ContainerRequest getFilteredContainerRequest(ContainerRequest orig) {\n+    ArrayList<String> newHosts = new ArrayList<String>();\n+    for (String host : orig.hosts) {\n+      if (!isNodeBlacklisted(host)) {\n+        newHosts.add(host);      \n+      }\n+    }\n+    String[] hosts = newHosts.toArray(new String[newHosts.size()]);\n+    ContainerRequest newReq = new ContainerRequest(orig.attemptID, orig.capability,\n+        hosts, orig.racks, orig.priority); \n+    return newReq;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/74748ec62570f92d57dbad3ba4cca47402990db5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/RMContainerRequestor.java",
                "sha": "cfedde2229aadb4355bc2ea93f25d3d13d83e858",
                "status": "modified"
            },
            {
                "additions": 121,
                "blob_url": "https://github.com/apache/hadoop/blob/74748ec62570f92d57dbad3ba4cca47402990db5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRMContainerAllocator.java",
                "changes": 121,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRMContainerAllocator.java?ref=74748ec62570f92d57dbad3ba4cca47402990db5",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRMContainerAllocator.java",
                "patch": "@@ -34,6 +34,7 @@\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.mapreduce.MRJobConfig;\n import org.apache.hadoop.mapreduce.v2.api.records.JobId;\n import org.apache.hadoop.mapreduce.v2.api.records.JobReport;\n import org.apache.hadoop.mapreduce.v2.api.records.JobState;\n@@ -44,6 +45,7 @@\n import org.apache.hadoop.mapreduce.v2.app.job.Job;\n import org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptContainerAssignedEvent;\n import org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl;\n+import org.apache.hadoop.mapreduce.v2.app.rm.ContainerFailedEvent;\n import org.apache.hadoop.mapreduce.v2.app.rm.ContainerRequestEvent;\n import org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator;\n import org.apache.hadoop.mapreduce.v2.util.MRBuilderUtils;\n@@ -478,6 +480,105 @@ public void testReportedAppProgressWithOnlyMaps() throws Exception {\n     Assert.assertEquals(100.0f, app.getProgress(), 0.0);\n   }\n \n+  @Test\n+  public void testBlackListedNodes() throws Exception {\n+    \n+    LOG.info(\"Running testBlackListedNodes\");\n+\n+    Configuration conf = new Configuration();\n+    conf.setBoolean(MRJobConfig.MR_AM_JOB_NODE_BLACKLISTING_ENABLE, true);\n+    conf.setInt(MRJobConfig.MAX_TASK_FAILURES_PER_TRACKER, 1);\n+    \n+    MyResourceManager rm = new MyResourceManager(conf);\n+    rm.start();\n+    DrainDispatcher dispatcher = (DrainDispatcher) rm.getRMContext()\n+        .getDispatcher();\n+\n+    // Submit the application\n+    RMApp app = rm.submitApp(1024);\n+    dispatcher.await();\n+\n+    MockNM amNodeManager = rm.registerNode(\"amNM:1234\", 2048);\n+    amNodeManager.nodeHeartbeat(true);\n+    dispatcher.await();\n+\n+    ApplicationAttemptId appAttemptId = app.getCurrentAppAttempt()\n+        .getAppAttemptId();\n+    rm.sendAMLaunched(appAttemptId);\n+    dispatcher.await();\n+    \n+    JobId jobId = MRBuilderUtils.newJobId(appAttemptId.getApplicationId(), 0);\n+    Job mockJob = mock(Job.class);\n+    when(mockJob.getReport()).thenReturn(\n+        MRBuilderUtils.newJobReport(jobId, \"job\", \"user\", JobState.RUNNING,\n+            0, 0, 0, 0, 0, 0, \"jobfile\"));\n+    MyContainerAllocator allocator = new MyContainerAllocator(rm, conf,\n+        appAttemptId, mockJob);\n+\n+    // add resources to scheduler\n+    MockNM nodeManager1 = rm.registerNode(\"h1:1234\", 10240);\n+    MockNM nodeManager2 = rm.registerNode(\"h2:1234\", 10240);\n+    MockNM nodeManager3 = rm.registerNode(\"h3:1234\", 10240);\n+    dispatcher.await();\n+\n+    // create the container request\n+    ContainerRequestEvent event1 = createReq(jobId, 1, 1024,\n+        new String[] { \"h1\" });\n+    allocator.sendRequest(event1);\n+\n+    // send 1 more request with different resource req\n+    ContainerRequestEvent event2 = createReq(jobId, 2, 1024,\n+        new String[] { \"h2\" });\n+    allocator.sendRequest(event2);\n+\n+    // send another request with different resource and priority\n+    ContainerRequestEvent event3 = createReq(jobId, 3, 1024,\n+        new String[] { \"h3\" });\n+    allocator.sendRequest(event3);\n+\n+    // this tells the scheduler about the requests\n+    // as nodes are not added, no allocations\n+    List<TaskAttemptContainerAssignedEvent> assigned = allocator.schedule();\n+    dispatcher.await();\n+    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());\n+\n+    // Send events to blacklist nodes h1 and h2\n+    ContainerFailedEvent f1 = createFailEvent(jobId, 1, \"h1\", false);            \n+    allocator.sendFailure(f1);\n+    ContainerFailedEvent f2 = createFailEvent(jobId, 1, \"h2\", false);            \n+    allocator.sendFailure(f2);\n+\n+    // update resources in scheduler\n+    nodeManager1.nodeHeartbeat(true); // Node heartbeat\n+    nodeManager2.nodeHeartbeat(true); // Node heartbeat\n+    dispatcher.await();\n+\n+    assigned = allocator.schedule();\n+    dispatcher.await();\n+    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());    \n+\n+    // mark h1/h2 as bad nodes\n+    nodeManager1.nodeHeartbeat(false);\n+    nodeManager2.nodeHeartbeat(false);\n+    dispatcher.await();\n+\n+    assigned = allocator.schedule();\n+    dispatcher.await();\n+    Assert.assertEquals(\"No of assignments must be 0\", 0, assigned.size());    \n+\n+    nodeManager3.nodeHeartbeat(true); // Node heartbeat\n+    assigned = allocator.schedule();    \n+    dispatcher.await();\n+        \n+    Assert.assertTrue(\"No of assignments must be 3\", assigned.size() == 3);\n+    \n+    // validate that all containers are assigned to h3\n+    for (TaskAttemptContainerAssignedEvent assig : assigned) {\n+      Assert.assertTrue(\"Assigned container host not correct\", \"h3\".equals(assig\n+          .getContainer().getNodeId().getHost()));\n+    }\n+  }\n+  \n   private static class MyFifoScheduler extends FifoScheduler {\n \n     public MyFifoScheduler(RMContext rmContext) {\n@@ -534,6 +635,19 @@ private ContainerRequestEvent createReq(JobId jobId, int taskAttemptId,\n         new String[] { NetworkTopology.DEFAULT_RACK });\n   }\n \n+  private ContainerFailedEvent createFailEvent(JobId jobId, int taskAttemptId,\n+      String host, boolean reduce) {\n+    TaskId taskId;\n+    if (reduce) {\n+      taskId = MRBuilderUtils.newTaskId(jobId, 0, TaskType.REDUCE);\n+    } else {\n+      taskId = MRBuilderUtils.newTaskId(jobId, 0, TaskType.MAP);\n+    }\n+    TaskAttemptId attemptId = MRBuilderUtils.newTaskAttemptId(taskId,\n+        taskAttemptId);\n+    return new ContainerFailedEvent(attemptId, host);    \n+  }\n+  \n   private void checkAssignments(ContainerRequestEvent[] requests,\n       List<TaskAttemptContainerAssignedEvent> assignments,\n       boolean checkHostMatch) {\n@@ -653,6 +767,10 @@ public void sendRequests(List<ContainerRequestEvent> reqs) {\n       }\n     }\n \n+    public void sendFailure(ContainerFailedEvent f) {\n+      super.handle(f);\n+    }\n+    \n     // API to be used by tests\n     public List<TaskAttemptContainerAssignedEvent> schedule() {\n       // run the scheduler\n@@ -672,6 +790,7 @@ public void sendRequests(List<ContainerRequestEvent> reqs) {\n     protected void startAllocatorThread() {\n       // override to NOT start thread\n     }\n+        \n   }\n \n   public static void main(String[] args) throws Exception {\n@@ -681,5 +800,7 @@ public static void main(String[] args) throws Exception {\n     t.testMapReduceScheduling();\n     t.testReportedAppProgress();\n     t.testReportedAppProgressWithOnlyMaps();\n+    t.testBlackListedNodes();\n   }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/74748ec62570f92d57dbad3ba4cca47402990db5/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestRMContainerAllocator.java",
                "sha": "dfbae8092c0b8f0e576ea16ce911cf9eb47e4e40",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-2693. Fix NPE in job-blacklisting. Contributed by Hitesh Shah.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1186529 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/ba66ca6856bb4fc8bba786a59051517f19b15d8f",
        "repo": "hadoop",
        "unit_tests": [
            "TestRMContainerAllocator.java"
        ]
    },
    "hadoop_747bafa": {
        "bug_id": "hadoop_747bafa",
        "commit": "https://github.com/apache/hadoop/commit/747bafaf969857b66233a8b4660590bdd712ed7d",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/747bafaf969857b66233a8b4660590bdd712ed7d/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java?ref=747bafaf969857b66233a8b4660590bdd712ed7d",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java",
                "patch": "@@ -323,7 +323,7 @@ synchronized void updateStatus() throws IOException {\n       this.status = ugi.doAs(new PrivilegedExceptionAction<JobStatus>() {\n         @Override\n         public JobStatus run() throws IOException, InterruptedException {\n-          return cluster.getClient().getJobStatus(status.getJobID());\n+          return cluster.getClient().getJobStatus(getJobID());\n         }\n       });\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/747bafaf969857b66233a8b4660590bdd712ed7d/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java",
                "sha": "84d90deff6f15793e121394d271cc3a2e045dbf0",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-6852. Job#updateStatus() failed with NPE due to race condition. Contributed by Junping Du",
        "parent": "https://github.com/apache/hadoop/commit/eeca8b0c4e2804b0fee5b012ea14b58383425ec3",
        "repo": "hadoop",
        "unit_tests": [
            "TestJob.java"
        ]
    },
    "hadoop_7545ce6": {
        "bug_id": "hadoop_7545ce6",
        "commit": "https://github.com/apache/hadoop/commit/7545ce6636066a05763744a817878e03ee87f456",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/7545ce6636066a05763744a817878e03ee87f456/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=7545ce6636066a05763744a817878e03ee87f456",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -780,6 +780,9 @@ Release 2.8.0 - UNRELEASED\n     (Larry McCay via cnauroth)\n \n   IMPROVEMENTS\n+    \n+    HADOOP-12831. LocalFS/FSOutputSummer NPEs in constructor if bytes per checksum \n+    set to 0 (Mingliang Liu via gtcarrera9)\n \n     HADOOP-12458. Retries is typoed to spell Retires in parts of\n     hadoop-yarn and hadoop-common",
                "raw_url": "https://github.com/apache/hadoop/raw/7545ce6636066a05763744a817878e03ee87f456/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "9f952211c09ef471a77b06717bdb2af0d3b14933",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/7545ce6636066a05763744a817878e03ee87f456/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFileSystem.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFileSystem.java?ref=7545ce6636066a05763744a817878e03ee87f456",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFileSystem.java",
                "patch": "@@ -25,6 +25,7 @@\n import java.nio.channels.ClosedChannelException;\n import java.util.Arrays;\n \n+import com.google.common.base.Preconditions;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.conf.Configuration;\n@@ -61,6 +62,9 @@ public void setConf(Configuration conf) {\n     if (conf != null) {\n       bytesPerChecksum = conf.getInt(LocalFileSystemConfigKeys.LOCAL_FS_BYTES_PER_CHECKSUM_KEY,\n \t\t                     LocalFileSystemConfigKeys.LOCAL_FS_BYTES_PER_CHECKSUM_DEFAULT);\n+      Preconditions.checkState(bytesPerChecksum > 0,\n+          \"bytes per checksum should be positive but was %s\",\n+          bytesPerChecksum);\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/7545ce6636066a05763744a817878e03ee87f456/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/ChecksumFileSystem.java",
                "sha": "c19be3d188b6196fd55e644e83f3aaf0ee494c73",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hadoop/blob/7545ce6636066a05763744a817878e03ee87f456/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestChecksumFileSystem.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestChecksumFileSystem.java?ref=7545ce6636066a05763744a817878e03ee87f456",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestChecksumFileSystem.java",
                "patch": "@@ -228,6 +228,29 @@ public void testRenameFileIntoDirFile() throws Exception {\n   }\n \n \n+  @Test\n+  public void testSetConf() {\n+    Configuration conf = new Configuration();\n+\n+    conf.setInt(LocalFileSystemConfigKeys.LOCAL_FS_BYTES_PER_CHECKSUM_KEY, 0);\n+    try {\n+      localFs.setConf(conf);\n+      fail(\"Should have failed because zero bytes per checksum is invalid\");\n+    } catch (IllegalStateException ignored) {\n+    }\n+\n+    conf.setInt(LocalFileSystemConfigKeys.LOCAL_FS_BYTES_PER_CHECKSUM_KEY, -1);\n+    try {\n+      localFs.setConf(conf);\n+      fail(\"Should have failed because negative bytes per checksum is invalid\");\n+    } catch (IllegalStateException ignored) {\n+    }\n+\n+    conf.setInt(LocalFileSystemConfigKeys.LOCAL_FS_BYTES_PER_CHECKSUM_KEY, 512);\n+    localFs.setConf(conf);\n+\n+  }\n+\n   void verifyRename(Path srcPath, Path dstPath, boolean dstIsDir)\n       throws Exception { \n     localFs.delete(srcPath,true);",
                "raw_url": "https://github.com/apache/hadoop/raw/7545ce6636066a05763744a817878e03ee87f456/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestChecksumFileSystem.java",
                "sha": "923d21967c097db341eb7f437b1c62e7e89641cf",
                "status": "modified"
            }
        ],
        "message": "HADOOP-12831. LocalFS/FSOutputSummer NPEs in constructor if bytes per checksum set to 0 (Mingliang Liu via gtcarrera9)",
        "parent": "https://github.com/apache/hadoop/commit/03cfb454fe5a1351e283e4678ad1b432ed231485",
        "repo": "hadoop",
        "unit_tests": [
            "TestChecksumFileSystem.java"
        ]
    },
    "hadoop_76b94c2": {
        "bug_id": "hadoop_76b94c2",
        "commit": "https://github.com/apache/hadoop/commit/76b94c274fe9775efcfd51c676d80c88a4f7fdb9",
        "file": [
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/76b94c274fe9775efcfd51c676d80c88a4f7fdb9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/FairCallQueue.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/FairCallQueue.java?ref=76b94c274fe9775efcfd51c676d80c88a4f7fdb9",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/FairCallQueue.java",
                "patch": "@@ -377,9 +377,21 @@ public void setDelegate(FairCallQueue<? extends Schedulable> obj) {\n       this.revisionNumber++;\n     }\n \n+    /**\n+     * Fetch the current call queue from the weak reference delegate. If there\n+     * is no delegate, or the delegate is empty, this will return null.\n+     */\n+    private FairCallQueue<? extends Schedulable> getCallQueue() {\n+      WeakReference<FairCallQueue<? extends Schedulable>> ref = this.delegate;\n+      if (ref == null) {\n+        return null;\n+      }\n+      return ref.get();\n+    }\n+\n     @Override\n     public int[] getQueueSizes() {\n-      FairCallQueue<? extends Schedulable> obj = this.delegate.get();\n+      FairCallQueue<? extends Schedulable> obj = getCallQueue();\n       if (obj == null) {\n         return new int[]{};\n       }\n@@ -389,7 +401,7 @@ public void setDelegate(FairCallQueue<? extends Schedulable> obj) {\n \n     @Override\n     public long[] getOverflowedCalls() {\n-      FairCallQueue<? extends Schedulable> obj = this.delegate.get();\n+      FairCallQueue<? extends Schedulable> obj = getCallQueue();\n       if (obj == null) {\n         return new long[]{};\n       }",
                "raw_url": "https://github.com/apache/hadoop/raw/76b94c274fe9775efcfd51c676d80c88a4f7fdb9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/FairCallQueue.java",
                "sha": "b4e953948c657b8d443c0af6a31d603a5b7fd051",
                "status": "modified"
            }
        ],
        "message": "HADOOP-16345. Fix a potential NPE when instantiating FairCallQueue metrics. Contributed by Erik Krogen.",
        "parent": "https://github.com/apache/hadoop/commit/4e38dafde4dce8cd8c368783a291e830f06e1def",
        "repo": "hadoop",
        "unit_tests": [
            "TestFairCallQueue.java"
        ]
    },
    "hadoop_77721f3": {
        "bug_id": "hadoop_77721f3",
        "commit": "https://github.com/apache/hadoop/commit/77721f39e26b630352a1f4087524a3fbd21ff06e",
        "file": [
            {
                "additions": 110,
                "blob_url": "https://github.com/apache/hadoop/blob/77721f39e26b630352a1f4087524a3fbd21ff06e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "changes": 179,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java?ref=77721f39e26b630352a1f4087524a3fbd21ff06e",
                "deletions": 69,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "patch": "@@ -40,6 +40,7 @@\n import java.util.Arrays;\n import java.util.Collection;\n import java.util.Collections;\n+import java.util.Date;\n import java.util.EnumMap;\n import java.util.HashMap;\n import java.util.Iterator;\n@@ -851,81 +852,121 @@ void spawnAutoRenewalThreadForUserCreds(boolean force) {\n     }\n \n     //spawn thread only if we have kerb credentials\n-    Thread t = new Thread(new Runnable() {\n+    KerberosTicket tgt = getTGT();\n+    if (tgt == null) {\n+      return;\n+    }\n+    String cmd = conf.get(\"hadoop.kerberos.kinit.command\", \"kinit\");\n+    long nextRefresh = getRefreshTime(tgt);\n+    Thread t =\n+        new Thread(new AutoRenewalForUserCredsRunnable(tgt, cmd, nextRefresh));\n+    t.setDaemon(true);\n+    t.setName(\"TGT Renewer for \" + getUserName());\n+    t.start();\n+  }\n+\n+  @VisibleForTesting\n+  class AutoRenewalForUserCredsRunnable implements Runnable {\n+    private KerberosTicket tgt;\n+    private RetryPolicy rp;\n+    private String kinitCmd;\n+    private long nextRefresh;\n+    private boolean runRenewalLoop = true;\n+\n+    AutoRenewalForUserCredsRunnable(KerberosTicket tgt, String kinitCmd,\n+        long nextRefresh){\n+      this.tgt = tgt;\n+      this.kinitCmd = kinitCmd;\n+      this.nextRefresh = nextRefresh;\n+      this.rp = null;\n+    }\n+\n+    public void setRunRenewalLoop(boolean runRenewalLoop) {\n+      this.runRenewalLoop = runRenewalLoop;\n+    }\n \n-      @Override\n-      public void run() {\n-        String cmd = conf.get(\"hadoop.kerberos.kinit.command\", \"kinit\");\n-        KerberosTicket tgt = getTGT();\n-        if (tgt == null) {\n+    @Override\n+    public void run() {\n+      do {\n+        try {\n+          long now = Time.now();\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Current time is \" + now);\n+            LOG.debug(\"Next refresh is \" + nextRefresh);\n+          }\n+          if (now < nextRefresh) {\n+            Thread.sleep(nextRefresh - now);\n+          }\n+          String output = Shell.execCommand(kinitCmd, \"-R\");\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Renewed ticket. kinit output: {}\", output);\n+          }\n+          reloginFromTicketCache();\n+          tgt = getTGT();\n+          if (tgt == null) {\n+            LOG.warn(\"No TGT after renewal. Aborting renew thread for \" +\n+                getUserName());\n+            return;\n+          }\n+          nextRefresh = Math.max(getRefreshTime(tgt),\n+              now + kerberosMinSecondsBeforeRelogin);\n+          metrics.renewalFailures.set(0);\n+          rp = null;\n+        } catch (InterruptedException ie) {\n+          LOG.warn(\"Terminating renewal thread\");\n           return;\n-        }\n-        long nextRefresh = getRefreshTime(tgt);\n-        RetryPolicy rp = null;\n-        while (true) {\n+        } catch (IOException ie) {\n+          metrics.renewalFailuresTotal.incr();\n+          final long now = Time.now();\n+\n+          if (tgt.isDestroyed()) {\n+            LOG.error(\"TGT is destroyed. Aborting renew thread for {}.\",\n+                getUserName());\n+            return;\n+          }\n+\n+          long tgtEndTime;\n+          // As described in HADOOP-15593 we need to handle the case when\n+          // tgt.getEndTime() throws NPE because of JDK issue JDK-8147772\n+          // NPE is only possible if this issue is not fixed in the JDK\n+          // currently used\n           try {\n-            long now = Time.now();\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Current time is \" + now);\n-              LOG.debug(\"Next refresh is \" + nextRefresh);\n-            }\n-            if (now < nextRefresh) {\n-              Thread.sleep(nextRefresh - now);\n-            }\n-            String output = Shell.execCommand(cmd, \"-R\");\n-            if (LOG.isDebugEnabled()) {\n-              LOG.debug(\"Renewed ticket. kinit output: {}\", output);\n-            }\n-            reloginFromTicketCache();\n-            tgt = getTGT();\n-            if (tgt == null) {\n-              LOG.warn(\"No TGT after renewal. Aborting renew thread for \" +\n-                  getUserName());\n-              return;\n-            }\n-            nextRefresh = Math.max(getRefreshTime(tgt),\n-              now + kerberosMinSecondsBeforeRelogin);\n-            metrics.renewalFailures.set(0);\n-            rp = null;\n-          } catch (InterruptedException ie) {\n-            LOG.warn(\"Terminating renewal thread\");\n+            tgtEndTime = tgt.getEndTime().getTime();\n+          } catch (NullPointerException npe) {\n+            LOG.error(\"NPE thrown while getting KerberosTicket endTime. \"\n+                + \"Aborting renew thread for {}.\", getUserName());\n+            return;\n+          }\n+\n+          LOG.warn(\"Exception encountered while running the renewal \"\n+                  + \"command for {}. (TGT end time:{}, renewalFailures: {},\"\n+                  + \"renewalFailuresTotal: {})\", getUserName(), tgtEndTime,\n+              metrics.renewalFailures.value(),\n+              metrics.renewalFailuresTotal.value(), ie);\n+          if (rp == null) {\n+            // Use a dummy maxRetries to create the policy. The policy will\n+            // only be used to get next retry time with exponential back-off.\n+            // The final retry time will be later limited within the\n+            // tgt endTime in getNextTgtRenewalTime.\n+            rp = RetryPolicies.exponentialBackoffRetry(Long.SIZE - 2,\n+                kerberosMinSecondsBeforeRelogin, TimeUnit.MILLISECONDS);\n+          }\n+          try {\n+            nextRefresh = getNextTgtRenewalTime(tgtEndTime, now, rp);\n+          } catch (Exception e) {\n+            LOG.error(\"Exception when calculating next tgt renewal time\", e);\n+            return;\n+          }\n+          metrics.renewalFailures.incr();\n+          // retry until close enough to tgt endTime.\n+          if (now > nextRefresh) {\n+            LOG.error(\"TGT is expired. Aborting renew thread for {}.\",\n+                getUserName());\n             return;\n-          } catch (IOException ie) {\n-            metrics.renewalFailuresTotal.incr();\n-            final long tgtEndTime = tgt.getEndTime().getTime();\n-            LOG.warn(\"Exception encountered while running the renewal \"\n-                    + \"command for {}. (TGT end time:{}, renewalFailures: {},\"\n-                    + \"renewalFailuresTotal: {})\", getUserName(), tgtEndTime,\n-                metrics.renewalFailures, metrics.renewalFailuresTotal, ie);\n-            final long now = Time.now();\n-            if (rp == null) {\n-              // Use a dummy maxRetries to create the policy. The policy will\n-              // only be used to get next retry time with exponential back-off.\n-              // The final retry time will be later limited within the\n-              // tgt endTime in getNextTgtRenewalTime.\n-              rp = RetryPolicies.exponentialBackoffRetry(Long.SIZE - 2,\n-                  kerberosMinSecondsBeforeRelogin, TimeUnit.MILLISECONDS);\n-            }\n-            try {\n-              nextRefresh = getNextTgtRenewalTime(tgtEndTime, now, rp);\n-            } catch (Exception e) {\n-              LOG.error(\"Exception when calculating next tgt renewal time\", e);\n-              return;\n-            }\n-            metrics.renewalFailures.incr();\n-            // retry until close enough to tgt endTime.\n-            if (now > nextRefresh) {\n-              LOG.error(\"TGT is expired. Aborting renew thread for {}.\",\n-                  getUserName());\n-              return;\n-            }\n           }\n         }\n-      }\n-    });\n-    t.setDaemon(true);\n-    t.setName(\"TGT Renewer for \" + getUserName());\n-    t.start();\n+      } while (runRenewalLoop);\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/77721f39e26b630352a1f4087524a3fbd21ff06e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "sha": "6ce72edb8e20b66882256a98a0a944c4ce67ba79",
                "status": "modified"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/hadoop/blob/77721f39e26b630352a1f4087524a3fbd21ff06e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestUserGroupInformation.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestUserGroupInformation.java?ref=77721f39e26b630352a1f4087524a3fbd21ff06e",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestUserGroupInformation.java",
                "patch": "@@ -47,6 +47,7 @@\n \n import javax.security.auth.Subject;\n import javax.security.auth.kerberos.KerberosPrincipal;\n+import javax.security.auth.kerberos.KerberosTicket;\n import javax.security.auth.kerberos.KeyTab;\n import javax.security.auth.login.AppConfigurationEntry;\n import javax.security.auth.login.LoginContext;\n@@ -61,6 +62,7 @@\n import java.util.Collection;\n import java.util.ConcurrentModificationException;\n import java.util.Date;\n+import java.util.HashSet;\n import java.util.LinkedHashSet;\n import java.util.Set;\n import java.util.concurrent.Callable;\n@@ -88,7 +90,10 @@\n import static org.junit.Assert.assertSame;\n import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n+import static org.mockito.Mockito.atLeastOnce;\n+import static org.mockito.Mockito.doThrow;\n import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.spy;\n import static org.mockito.Mockito.when;\n \n public class TestUserGroupInformation {\n@@ -1211,4 +1216,37 @@ public UserGroupInformation run() throws Exception {\n     barrier.await();\n     assertSame(testUgi1.getSubject(), blockingLookup.get().getSubject());\n   }\n+\n+  @Test\n+  public void testKerberosTicketIsDestroyedChecked() throws Exception {\n+    // Create UserGroupInformation\n+    GenericTestUtils.setLogLevel(UserGroupInformation.LOG, Level.DEBUG);\n+    Set<User> users = new HashSet<>();\n+    users.add(new User(\"Foo\"));\n+    Subject subject =\n+        new Subject(true, users, new HashSet<>(), new HashSet<>());\n+    UserGroupInformation ugi = spy(new UserGroupInformation(subject));\n+\n+    // throw IOException in the middle of the autoRenewalForUserCreds\n+    doThrow(new IOException()).when(ugi).reloginFromTicketCache();\n+\n+    // Create and destroy the KerberosTicket, so endTime will be null\n+    Date d = new Date();\n+    KerberosPrincipal kp = new KerberosPrincipal(\"Foo\");\n+    KerberosTicket tgt = spy(new KerberosTicket(new byte[]{}, kp, kp, new\n+        byte[]{}, 0, null, d, d, d, d, null));\n+    tgt.destroy();\n+\n+    // run AutoRenewalForUserCredsRunnable with this\n+    UserGroupInformation.AutoRenewalForUserCredsRunnable userCredsRunnable =\n+        ugi.new AutoRenewalForUserCredsRunnable(tgt,\n+            Boolean.toString(Boolean.TRUE), 100);\n+\n+    // Set the runnable to not to run in a loop\n+    userCredsRunnable.setRunRenewalLoop(false);\n+    // there should be no exception when calling this\n+    userCredsRunnable.run();\n+    // isDestroyed should be called at least once\n+    Mockito.verify(tgt, atLeastOnce()).isDestroyed();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/77721f39e26b630352a1f4087524a3fbd21ff06e/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestUserGroupInformation.java",
                "sha": "011e930e50c7ea98d76f8f867a2a0cb0c423f0a7",
                "status": "modified"
            }
        ],
        "message": "HADOOP-15593.  Fixed NPE in UGI spawnAutoRenewalThreadForUserCreds.\n               Contributed by Gabor Bota",
        "parent": "https://github.com/apache/hadoop/commit/40fad32824d2f8f960c779d78357e62103453da0",
        "repo": "hadoop",
        "unit_tests": [
            "TestUserGroupInformation.java"
        ]
    },
    "hadoop_778a4a2": {
        "bug_id": "hadoop_778a4a2",
        "commit": "https://github.com/apache/hadoop/commit/778a4a24be176382a5704f709c00bdfcfe6ddc8c",
        "file": [
            {
                "additions": 59,
                "blob_url": "https://github.com/apache/hadoop/blob/778a4a24be176382a5704f709c00bdfcfe6ddc8c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/Component.java",
                "changes": 114,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/Component.java?ref=778a4a24be176382a5704f709c00bdfcfe6ddc8c",
                "deletions": 55,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/Component.java",
                "patch": "@@ -694,62 +694,66 @@ public void requestContainers(long count) {\n       // composite constraints then this AND-ed composite constraint is not\n       // used.\n       PlacementConstraint finalConstraint = null;\n-      for (org.apache.hadoop.yarn.service.api.records.PlacementConstraint\n-          yarnServiceConstraint : placementPolicy.getConstraints()) {\n-        List<TargetExpression> targetExpressions = new ArrayList<>();\n-        // Currently only intra-application allocation tags are supported.\n-        if (!yarnServiceConstraint.getTargetTags().isEmpty()) {\n-          targetExpressions.add(PlacementTargets.allocationTag(\n-              yarnServiceConstraint.getTargetTags().toArray(new String[0])));\n-        }\n-        // Add all node attributes\n-        for (Map.Entry<String, List<String>> attribute : yarnServiceConstraint\n-            .getNodeAttributes().entrySet()) {\n-          targetExpressions.add(PlacementTargets.nodeAttribute(\n-              attribute.getKey(), attribute.getValue().toArray(new String[0])));\n-        }\n-        // Add all node partitions\n-        if (!yarnServiceConstraint.getNodePartitions().isEmpty()) {\n-          targetExpressions\n-              .add(PlacementTargets.nodePartition(yarnServiceConstraint\n-                  .getNodePartitions().toArray(new String[0])));\n-        }\n-        PlacementConstraint constraint = null;\n-        switch (yarnServiceConstraint.getType()) {\n-        case AFFINITY:\n-          constraint = PlacementConstraints\n-              .targetIn(yarnServiceConstraint.getScope().getValue(),\n-                  targetExpressions.toArray(new TargetExpression[0]))\n-              .build();\n-          break;\n-        case ANTI_AFFINITY:\n-          constraint = PlacementConstraints\n-              .targetNotIn(yarnServiceConstraint.getScope().getValue(),\n-                  targetExpressions.toArray(new TargetExpression[0]))\n-              .build();\n-          break;\n-        case AFFINITY_WITH_CARDINALITY:\n-          constraint = PlacementConstraints.targetCardinality(\n-              yarnServiceConstraint.getScope().name().toLowerCase(),\n-              yarnServiceConstraint.getMinCardinality() == null ? 0\n-                  : yarnServiceConstraint.getMinCardinality().intValue(),\n-              yarnServiceConstraint.getMaxCardinality() == null\n-                  ? Integer.MAX_VALUE\n-                  : yarnServiceConstraint.getMaxCardinality().intValue(),\n-              targetExpressions.toArray(new TargetExpression[0])).build();\n-          break;\n-        }\n-        // The default AND-ed final composite constraint\n-        if (finalConstraint != null) {\n-          finalConstraint = PlacementConstraints\n-              .and(constraint.getConstraintExpr(),\n-                  finalConstraint.getConstraintExpr())\n-              .build();\n-        } else {\n-          finalConstraint = constraint;\n+      if (placementPolicy != null) {\n+        for (org.apache.hadoop.yarn.service.api.records.PlacementConstraint\n+            yarnServiceConstraint : placementPolicy.getConstraints()) {\n+          List<TargetExpression> targetExpressions = new ArrayList<>();\n+          // Currently only intra-application allocation tags are supported.\n+          if (!yarnServiceConstraint.getTargetTags().isEmpty()) {\n+            targetExpressions.add(PlacementTargets.allocationTag(\n+                yarnServiceConstraint.getTargetTags().toArray(new String[0])));\n+          }\n+          // Add all node attributes\n+          for (Map.Entry<String, List<String>> attribute : yarnServiceConstraint\n+              .getNodeAttributes().entrySet()) {\n+            targetExpressions\n+                .add(PlacementTargets.nodeAttribute(attribute.getKey(),\n+                    attribute.getValue().toArray(new String[0])));\n+          }\n+          // Add all node partitions\n+          if (!yarnServiceConstraint.getNodePartitions().isEmpty()) {\n+            targetExpressions\n+                .add(PlacementTargets.nodePartition(yarnServiceConstraint\n+                    .getNodePartitions().toArray(new String[0])));\n+          }\n+          PlacementConstraint constraint = null;\n+          switch (yarnServiceConstraint.getType()) {\n+          case AFFINITY:\n+            constraint = PlacementConstraints\n+                .targetIn(yarnServiceConstraint.getScope().getValue(),\n+                    targetExpressions.toArray(new TargetExpression[0]))\n+                .build();\n+            break;\n+          case ANTI_AFFINITY:\n+            constraint = PlacementConstraints\n+                .targetNotIn(yarnServiceConstraint.getScope().getValue(),\n+                    targetExpressions.toArray(new TargetExpression[0]))\n+                .build();\n+            break;\n+          case AFFINITY_WITH_CARDINALITY:\n+            constraint = PlacementConstraints.targetCardinality(\n+                yarnServiceConstraint.getScope().name().toLowerCase(),\n+                yarnServiceConstraint.getMinCardinality() == null ? 0\n+                    : yarnServiceConstraint.getMinCardinality().intValue(),\n+                yarnServiceConstraint.getMaxCardinality() == null\n+                    ? Integer.MAX_VALUE\n+                    : yarnServiceConstraint.getMaxCardinality().intValue(),\n+                targetExpressions.toArray(new TargetExpression[0])).build();\n+            break;\n+          }\n+          // The default AND-ed final composite constraint\n+          if (finalConstraint != null) {\n+            finalConstraint = PlacementConstraints\n+                .and(constraint.getConstraintExpr(),\n+                    finalConstraint.getConstraintExpr())\n+                .build();\n+          } else {\n+            finalConstraint = constraint;\n+          }\n+          LOG.debug(\"[COMPONENT {}] Placement constraint: {}\",\n+              componentSpec.getName(),\n+              constraint.getConstraintExpr().toString());\n         }\n-        LOG.debug(\"[COMPONENT {}] Placement constraint: {}\",\n-            componentSpec.getName(), constraint.getConstraintExpr().toString());\n       }\n       ResourceSizing resourceSizing = ResourceSizing.newInstance((int) count,\n           resource);",
                "raw_url": "https://github.com/apache/hadoop/raw/778a4a24be176382a5704f709c00bdfcfe6ddc8c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/Component.java",
                "sha": "a1ee7964b83ea9bd2efde2d1b685d14797fa6374",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/778a4a24be176382a5704f709c00bdfcfe6ddc8c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/RestApiErrorMessages.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/RestApiErrorMessages.java?ref=778a4a24be176382a5704f709c00bdfcfe6ddc8c",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/RestApiErrorMessages.java",
                "patch": "@@ -91,6 +91,14 @@\n \n   String ERROR_QUICKLINKS_FOR_COMP_INVALID = \"Quicklinks specified at\"\n       + \" component level, needs corresponding values set at service level\";\n+  // Note: %sin is not a typo. Constraint name is optional so the error messages\n+  // below handle that scenario by adding a space if name is specified.\n+  String ERROR_PLACEMENT_POLICY_CONSTRAINT_TYPE_NULL = \"Type not specified \"\n+      + \"for constraint %sin placement policy of component %s.\";\n+  String ERROR_PLACEMENT_POLICY_CONSTRAINT_SCOPE_NULL = \"Scope not specified \"\n+      + \"for constraint %sin placement policy of component %s.\";\n+  String ERROR_PLACEMENT_POLICY_CONSTRAINT_TAGS_NULL = \"Tag(s) not specified \"\n+      + \"for constraint %sin placement policy of component %s.\";\n   String ERROR_PLACEMENT_POLICY_TAG_NAME_NOT_SAME = \"Invalid target tag %s \"\n       + \"specified in placement policy of component %s. For now, target tags \"\n       + \"support self reference only. Specifying anything other than its \"",
                "raw_url": "https://github.com/apache/hadoop/raw/778a4a24be176382a5704f709c00bdfcfe6ddc8c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/exceptions/RestApiErrorMessages.java",
                "sha": "1d2d719d32badd9b1e9e5aa40e4722833cb2c1e4",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop/blob/778a4a24be176382a5704f709c00bdfcfe6ddc8c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ServiceApiUtil.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ServiceApiUtil.java?ref=778a4a24be176382a5704f709c00bdfcfe6ddc8c",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ServiceApiUtil.java",
                "patch": "@@ -40,6 +40,7 @@\n import org.apache.hadoop.yarn.service.api.records.Configuration;\n import org.apache.hadoop.yarn.service.api.records.KerberosPrincipal;\n import org.apache.hadoop.yarn.service.api.records.PlacementConstraint;\n+import org.apache.hadoop.yarn.service.api.records.PlacementPolicy;\n import org.apache.hadoop.yarn.service.api.records.Resource;\n import org.apache.hadoop.yarn.service.exceptions.SliderException;\n import org.apache.hadoop.yarn.service.conf.RestApiConstants;\n@@ -314,9 +315,28 @@ public static void validateNameFormat(String name,\n   private static void validatePlacementPolicy(List<Component> components,\n       Set<String> componentNames) {\n     for (Component comp : components) {\n-      if (comp.getPlacementPolicy() != null) {\n-        for (PlacementConstraint constraint : comp.getPlacementPolicy()\n+      PlacementPolicy placementPolicy = comp.getPlacementPolicy();\n+      if (placementPolicy != null) {\n+        for (PlacementConstraint constraint : placementPolicy\n             .getConstraints()) {\n+          if (constraint.getType() == null) {\n+            throw new IllegalArgumentException(String.format(\n+              RestApiErrorMessages.ERROR_PLACEMENT_POLICY_CONSTRAINT_TYPE_NULL,\n+              constraint.getName() == null ? \"\" : constraint.getName() + \" \",\n+              comp.getName()));\n+          }\n+          if (constraint.getScope() == null) {\n+            throw new IllegalArgumentException(String.format(\n+              RestApiErrorMessages.ERROR_PLACEMENT_POLICY_CONSTRAINT_SCOPE_NULL,\n+              constraint.getName() == null ? \"\" : constraint.getName() + \" \",\n+              comp.getName()));\n+          }\n+          if (constraint.getTargetTags().isEmpty()) {\n+            throw new IllegalArgumentException(String.format(\n+              RestApiErrorMessages.ERROR_PLACEMENT_POLICY_CONSTRAINT_TAGS_NULL,\n+              constraint.getName() == null ? \"\" : constraint.getName() + \" \",\n+              comp.getName()));\n+          }\n           for (String targetTag : constraint.getTargetTags()) {\n             if (!comp.getName().equals(targetTag)) {\n               throw new IllegalArgumentException(String.format(",
                "raw_url": "https://github.com/apache/hadoop/raw/778a4a24be176382a5704f709c00bdfcfe6ddc8c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/utils/ServiceApiUtil.java",
                "sha": "6101bf01363814d65773ba85e5804e456b9b7dfc",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hadoop/blob/778a4a24be176382a5704f709c00bdfcfe6ddc8c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceApiUtil.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceApiUtil.java?ref=778a4a24be176382a5704f709c00bdfcfe6ddc8c",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceApiUtil.java",
                "patch": "@@ -25,6 +25,8 @@\n import org.apache.hadoop.yarn.service.api.records.KerberosPrincipal;\n import org.apache.hadoop.yarn.service.api.records.PlacementConstraint;\n import org.apache.hadoop.yarn.service.api.records.PlacementPolicy;\n+import org.apache.hadoop.yarn.service.api.records.PlacementScope;\n+import org.apache.hadoop.yarn.service.api.records.PlacementType;\n import org.apache.hadoop.yarn.service.api.records.Resource;\n import org.apache.hadoop.yarn.service.api.records.Service;\n import org.apache.hadoop.yarn.service.exceptions.RestApiErrorMessages;\n@@ -503,13 +505,48 @@ public void testPlacementPolicy() throws IOException {\n     PlacementPolicy pp = new PlacementPolicy();\n     PlacementConstraint pc = new PlacementConstraint();\n     pc.setName(\"CA1\");\n-    pc.setTargetTags(Collections.singletonList(\"comp-invalid\"));\n     pp.setConstraints(Collections.singletonList(pc));\n     comp.setPlacementPolicy(pp);\n \n     try {\n       ServiceApiUtil.validateAndResolveService(app, sfs, CONF_DNS_ENABLED);\n-      Assert.fail(EXCEPTION_PREFIX + \"service with empty placement\");\n+      Assert.fail(EXCEPTION_PREFIX + \"constraint with no type\");\n+    } catch (IllegalArgumentException e) {\n+      assertEquals(String.format(\n+          RestApiErrorMessages.ERROR_PLACEMENT_POLICY_CONSTRAINT_TYPE_NULL,\n+          \"CA1 \", \"comp-a\"), e.getMessage());\n+    }\n+\n+    // Set the type\n+    pc.setType(PlacementType.ANTI_AFFINITY);\n+\n+    try {\n+      ServiceApiUtil.validateAndResolveService(app, sfs, CONF_DNS_ENABLED);\n+      Assert.fail(EXCEPTION_PREFIX + \"constraint with no scope\");\n+    } catch (IllegalArgumentException e) {\n+      assertEquals(String.format(\n+          RestApiErrorMessages.ERROR_PLACEMENT_POLICY_CONSTRAINT_SCOPE_NULL,\n+          \"CA1 \", \"comp-a\"), e.getMessage());\n+    }\n+\n+    // Set the scope\n+    pc.setScope(PlacementScope.NODE);\n+\n+    try {\n+      ServiceApiUtil.validateAndResolveService(app, sfs, CONF_DNS_ENABLED);\n+      Assert.fail(EXCEPTION_PREFIX + \"constraint with no tag(s)\");\n+    } catch (IllegalArgumentException e) {\n+      assertEquals(String.format(\n+          RestApiErrorMessages.ERROR_PLACEMENT_POLICY_CONSTRAINT_TAGS_NULL,\n+          \"CA1 \", \"comp-a\"), e.getMessage());\n+    }\n+\n+    // Set a target tag - but an invalid one\n+    pc.setTargetTags(Collections.singletonList(\"comp-invalid\"));\n+\n+    try {\n+      ServiceApiUtil.validateAndResolveService(app, sfs, CONF_DNS_ENABLED);\n+      Assert.fail(EXCEPTION_PREFIX + \"constraint with invalid tag name\");\n     } catch (IllegalArgumentException e) {\n       assertEquals(\n           String.format(\n@@ -518,9 +555,10 @@ public void testPlacementPolicy() throws IOException {\n           e.getMessage());\n     }\n \n+    // Set valid target tags now\n     pc.setTargetTags(Collections.singletonList(\"comp-a\"));\n \n-    // now it should succeed\n+    // Finally it should succeed\n     try {\n       ServiceApiUtil.validateAndResolveService(app, sfs, CONF_DNS_ENABLED);\n     } catch (IllegalArgumentException e) {",
                "raw_url": "https://github.com/apache/hadoop/raw/778a4a24be176382a5704f709c00bdfcfe6ddc8c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceApiUtil.java",
                "sha": "243c6b3a618145458a5eb37c7d3aa52d1e65c995",
                "status": "modified"
            }
        ],
        "message": "YARN-8350. NPE in service AM related to placement policy. Contributed by Gour Saha",
        "parent": "https://github.com/apache/hadoop/commit/96eefcc84aacc4cc82ad7e3e72c5bdad56f4a7b7",
        "repo": "hadoop",
        "unit_tests": [
            "TestComponent.java",
            "TestServiceApiUtil.java"
        ]
    },
    "hadoop_78b05fd": {
        "bug_id": "hadoop_78b05fd",
        "commit": "https://github.com/apache/hadoop/commit/78b05fde6c41f7a6b2dc2d99b435d1d83242590c",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/78b05fde6c41f7a6b2dc2d99b435d1d83242590c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java?ref=78b05fde6c41f7a6b2dc2d99b435d1d83242590c",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
                "patch": "@@ -533,6 +533,9 @@ private static Object toJsonMap(\n \n   public static String toJsonString(\n       SnapshottableDirectoryStatus[] snapshottableDirectoryList) {\n+    if (snapshottableDirectoryList == null) {\n+      return toJsonString(\"SnapshottableDirectoryList\", null);\n+    }\n     Object[] a = new Object[snapshottableDirectoryList.length];\n     for (int i = 0; i < snapshottableDirectoryList.length; i++) {\n       a[i] = toJsonMap(snapshottableDirectoryList[i]);",
                "raw_url": "https://github.com/apache/hadoop/raw/78b05fde6c41f7a6b2dc2d99b435d1d83242590c/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/JsonUtil.java",
                "sha": "43a252b1c3ed5c78875c32e6d02978372f8c932f",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/78b05fde6c41f7a6b2dc2d99b435d1d83242590c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java?ref=78b05fde6c41f7a6b2dc2d99b435d1d83242590c",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java",
                "patch": "@@ -755,14 +755,16 @@ public void testWebHdfsSnapshottableDirectoryList() throws Exception {\n       final Path bar = new Path(\"/bar\");\n       dfs.mkdirs(foo);\n       dfs.mkdirs(bar);\n+      SnapshottableDirectoryStatus[] statuses =\n+          webHdfs.getSnapshottableDirectoryList();\n+      Assert.assertNull(statuses);\n       dfs.allowSnapshot(foo);\n       dfs.allowSnapshot(bar);\n       Path file0 = new Path(foo, \"file0\");\n       DFSTestUtil.createFile(dfs, file0, 100, (short) 1, 0);\n       Path file1 = new Path(bar, \"file1\");\n       DFSTestUtil.createFile(dfs, file1, 100, (short) 1, 0);\n-      SnapshottableDirectoryStatus[] statuses =\n-          webHdfs.getSnapshottableDirectoryList();\n+      statuses = webHdfs.getSnapshottableDirectoryList();\n       SnapshottableDirectoryStatus[] dfsStatuses =\n           dfs.getSnapshottableDirListing();\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/78b05fde6c41f7a6b2dc2d99b435d1d83242590c/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/web/TestWebHDFS.java",
                "sha": "224735546b9d0b5b772b1b0ad4398a2fb5832060",
                "status": "modified"
            }
        ],
        "message": "HDFS-13280. WebHDFS: Fix NPE in get snasphottable directory list call. Contributed by Lokesh Jain.",
        "parent": "https://github.com/apache/hadoop/commit/e71bc00a471422ddb26dd54e706f09f0fe09925c",
        "repo": "hadoop",
        "unit_tests": [
            "TestJsonUtil.java"
        ]
    },
    "hadoop_7ba5bba": {
        "bug_id": "hadoop_7ba5bba",
        "commit": "https://github.com/apache/hadoop/commit/7ba5bbac02b688f68a8d23671a1e869234b4cebe",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/7ba5bbac02b688f68a8d23671a1e869234b4cebe/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=7ba5bbac02b688f68a8d23671a1e869234b4cebe",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -338,6 +338,9 @@ Trunk (Unreleased)\n \n     HDFS-8412. Fix the test failures in HTTPFS. (umamahesh)\n \n+    HDFS-8627. NPE thrown if unable to fetch token from Namenode\n+    (J.Andreina via vinayakumarb)\n+\n Release 2.8.0 - UNRELEASED\n \n   NEW FEATURES",
                "raw_url": "https://github.com/apache/hadoop/raw/7ba5bbac02b688f68a8d23671a1e869234b4cebe/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "b065f98e73256032a5a524bf67bcb749cce1733f",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/7ba5bbac02b688f68a8d23671a1e869234b4cebe/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DelegationTokenFetcher.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DelegationTokenFetcher.java?ref=7ba5bbac02b688f68a8d23671a1e869234b4cebe",
                "deletions": 8,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DelegationTokenFetcher.java",
                "patch": "@@ -176,14 +176,17 @@ static void saveDelegationToken(Configuration conf, FileSystem fs,\n                                   final String renewer, final Path tokenFile)\n           throws IOException {\n     Token<?> token = fs.getDelegationToken(renewer);\n-\n-    Credentials cred = new Credentials();\n-    cred.addToken(token.getKind(), token);\n-    cred.writeTokenStorageFile(tokenFile, conf);\n-\n-    if (LOG.isDebugEnabled()) {\n-      LOG.debug(\"Fetched token \" + fs.getUri() + \" for \" + token.getService()\n-              + \" into \" + tokenFile);\n+    if (null != token) {\n+      Credentials cred = new Credentials();\n+      cred.addToken(token.getKind(), token);\n+      cred.writeTokenStorageFile(tokenFile, conf);\n+\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Fetched token \" + fs.getUri() + \" for \" +\n+            token.getService() + \" into \" + tokenFile);\n+      }\n+    } else {\n+      System.err.println(\"ERROR: Failed to fetch token from \" + fs.getUri());\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/7ba5bbac02b688f68a8d23671a1e869234b4cebe/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/tools/DelegationTokenFetcher.java",
                "sha": "803402dc942b777a33cbd71ba3faa6cad6592472",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/7ba5bbac02b688f68a8d23671a1e869234b4cebe/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDelegationTokenFetcher.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDelegationTokenFetcher.java?ref=7ba5bbac02b688f68a8d23671a1e869234b4cebe",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDelegationTokenFetcher.java",
                "patch": "@@ -90,4 +90,19 @@ public void expectedTokenIsRetrievedFromHttp() throws Exception {\n     DelegationTokenFetcher.cancelTokens(conf, p);\n     Assert.assertEquals(testToken, FakeRenewer.getLastCanceled());\n   }\n+\n+  /**\n+   * If token returned is null, saveDelegationToken should not\n+   * throw nullPointerException\n+   */\n+  @Test\n+  public void testReturnedTokenIsNull() throws Exception {\n+    WebHdfsFileSystem fs = mock(WebHdfsFileSystem.class);\n+    doReturn(null).when(fs).getDelegationToken(anyString());\n+    Path p = new Path(f.getRoot().getAbsolutePath(), tokenFile);\n+    DelegationTokenFetcher.saveDelegationToken(conf, fs, null, p);\n+    // When Token returned is null, TokenFile should not exist\n+    Assert.assertFalse(p.getFileSystem(conf).exists(p));\n+\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/7ba5bbac02b688f68a8d23671a1e869234b4cebe/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/tools/TestDelegationTokenFetcher.java",
                "sha": "ab3933bb6865d3c36ddbd7b32df6b2f158738937",
                "status": "modified"
            }
        ],
        "message": "HDFS-8627. NPE thrown if unable to fetch token from Namenode (Contributed by J.Andreina)",
        "parent": "https://github.com/apache/hadoop/commit/6d99017f38f5a158b5cb65c74688b4c833e4e35f",
        "repo": "hadoop",
        "unit_tests": [
            "TestDelegationTokenFetcher.java"
        ]
    },
    "hadoop_7ca7fda": {
        "bug_id": "hadoop_7ca7fda",
        "commit": "https://github.com/apache/hadoop/commit/7ca7fdadc491290abb0949dbb56cba8e66de9862",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/7ca7fdadc491290abb0949dbb56cba8e66de9862/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=7ca7fdadc491290abb0949dbb56cba8e66de9862",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -1697,6 +1697,9 @@ Release 0.23.0 - Unreleased\n \n     MAPREDUCE-2693. Fix NPE in job-blacklisting. (Hitesh Shah via acmurthy) \n \n+    MAPREDUCE-3208. Fix NPE task/container log appenders. (liangzhwa via\n+    acmurthy) \n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/7ca7fdadc491290abb0949dbb56cba8e66de9862/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "cf1c7d1696a3da0f1ddf516536a82303b8f196ac",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/7ca7fdadc491290abb0949dbb56cba8e66de9862/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLogAppender.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLogAppender.java?ref=7ca7fdadc491290abb0949dbb56cba8e66de9862",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLogAppender.java",
                "patch": "@@ -93,7 +93,9 @@ public void append(LoggingEvent event) {\n   }\n   \n   public void flush() {\n-    qw.flush();\n+    if (qw != null) {\n+      qw.flush();\n+    }\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/7ca7fdadc491290abb0949dbb56cba8e66de9862/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/TaskLogAppender.java",
                "sha": "0b79837f62d57a93f88a1b45a3ef9795a2ee0f0a",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/7ca7fdadc491290abb0949dbb56cba8e66de9862/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/ContainerLogAppender.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/ContainerLogAppender.java?ref=7ca7fdadc491290abb0949dbb56cba8e66de9862",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/ContainerLogAppender.java",
                "patch": "@@ -65,7 +65,9 @@ public void append(LoggingEvent event) {\n   }\n   \n   public void flush() {\n-    qw.flush();\n+    if (qw != null) {\n+      qw.flush();\n+    }\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/7ca7fdadc491290abb0949dbb56cba8e66de9862/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/ContainerLogAppender.java",
                "sha": "7f09c175b6b234a8624d6a04433ef23345393fb9",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3208. Fix NPE task/container log appenders. Contributed by liangzhwa.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1186542 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/74748ec62570f92d57dbad3ba4cca47402990db5",
        "repo": "hadoop",
        "unit_tests": [
            "TestTaskLogAppender.java",
            "TestContainerLogAppender.java"
        ]
    },
    "hadoop_7d06806": {
        "bug_id": "hadoop_7d06806",
        "commit": "https://github.com/apache/hadoop/commit/7d06806dfdeb3252ac0defe23e8c468eabfa8b5e",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/7d06806dfdeb3252ac0defe23e8c468eabfa8b5e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java?ref=7d06806dfdeb3252ac0defe23e8c468eabfa8b5e",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "patch": "@@ -1273,8 +1273,6 @@ synchronized void transitionToStandby(boolean initialize)\n   protected void serviceStart() throws Exception {\n     if (this.rmContext.isHAEnabled()) {\n       transitionToStandby(false);\n-    } else {\n-      transitionToActive();\n     }\n \n     startWepApp();\n@@ -1284,6 +1282,11 @@ protected void serviceStart() throws Exception {\n       WebAppUtils.setRMWebAppPort(conf, port);\n     }\n     super.serviceStart();\n+\n+    // Non HA case, start after RM services are started.\n+    if (!this.rmContext.isHAEnabled()) {\n+      transitionToActive();\n+    }\n   }\n   \n   protected void doSecureLogin() throws IOException {",
                "raw_url": "https://github.com/apache/hadoop/raw/7d06806dfdeb3252ac0defe23e8c468eabfa8b5e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "sha": "05745ec272e1f72d924376688782c1bc4a71495b",
                "status": "modified"
            }
        ],
        "message": "YARN-6827. [ATS1/1.5] NPE exception while publishing recovering applications into ATS during RM restart. Contributed by Rohith Sharma K S.",
        "parent": "https://github.com/apache/hadoop/commit/c6d7d3eb059c7539db7d00586e181ec44da13557",
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceManager.java"
        ]
    },
    "hadoop_7ffb994": {
        "bug_id": "hadoop_7ffb994",
        "commit": "https://github.com/apache/hadoop/commit/7ffb9943b8838a3bb56684e0722db40d800743a2",
        "file": [
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hadoop/blob/7ffb9943b8838a3bb56684e0722db40d800743a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java?ref=7ffb9943b8838a3bb56684e0722db40d800743a2",
                "deletions": 19,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "patch": "@@ -1036,7 +1036,6 @@ LocalizerHeartbeatResponse processHeartbeat(\n         List<LocalResourceStatus> remoteResourceStatuses) {\n       LocalizerHeartbeatResponse response =\n         recordFactory.newRecordInstance(LocalizerHeartbeatResponse.class);\n-\n       String user = context.getUser();\n       ApplicationId applicationId =\n           context.getContainerId().getApplicationAttemptId().getApplicationId();\n@@ -1059,14 +1058,19 @@ LocalizerHeartbeatResponse processHeartbeat(\n           LOG.error(\"Unknown resource reported: \" + req);\n           continue;\n         }\n+        LocalResourcesTracker tracker =\n+            getLocalResourcesTracker(req.getVisibility(), user, applicationId);\n+        if (tracker == null) {\n+          // This is likely due to a race between heartbeat and\n+          // app cleaning up.\n+          continue;\n+        }\n         switch (stat.getStatus()) {\n           case FETCH_SUCCESS:\n             // notify resource\n             try {\n-            getLocalResourcesTracker(req.getVisibility(), user, applicationId)\n-              .handle(\n-                new ResourceLocalizedEvent(req, stat.getLocalPath().toPath(),\n-                    stat.getLocalSize()));\n+              tracker.handle(new ResourceLocalizedEvent(req,\n+                  stat.getLocalPath().toPath(), stat.getLocalSize()));\n             } catch (URISyntaxException e) { }\n \n             // unlocking the resource and removing it from scheduled resource\n@@ -1080,9 +1084,8 @@ LocalizerHeartbeatResponse processHeartbeat(\n             final String diagnostics = stat.getException().toString();\n             LOG.warn(req + \" failed: \" + diagnostics);\n             fetchFailed = true;\n-            getLocalResourcesTracker(req.getVisibility(), user, applicationId)\n-              .handle(new ResourceFailedLocalizationEvent(\n-                  req, diagnostics));\n+            tracker.handle(new ResourceFailedLocalizationEvent(req,\n+                diagnostics));\n \n             // unlocking the resource and removing it from scheduled resource\n             // list\n@@ -1092,9 +1095,8 @@ LocalizerHeartbeatResponse processHeartbeat(\n           default:\n             LOG.info(\"Unknown status: \" + stat.getStatus());\n             fetchFailed = true;\n-            getLocalResourcesTracker(req.getVisibility(), user, applicationId)\n-              .handle(new ResourceFailedLocalizationEvent(\n-                  req, stat.getException().getMessage()));\n+            tracker.handle(new ResourceFailedLocalizationEvent(req,\n+                stat.getException().getMessage()));\n             break;\n         }\n       }\n@@ -1114,10 +1116,14 @@ LocalizerHeartbeatResponse processHeartbeat(\n       LocalResource next = findNextResource();\n       if (next != null) {\n         try {\n-          ResourceLocalizationSpec resource =\n-              NodeManagerBuilderUtils.newResourceLocalizationSpec(next,\n-                getPathForLocalization(next));\n-          rsrcs.add(resource);\n+          LocalResourcesTracker tracker = getLocalResourcesTracker(\n+              next.getVisibility(), user, applicationId);\n+          if (tracker != null) {\n+            ResourceLocalizationSpec resource =\n+                NodeManagerBuilderUtils.newResourceLocalizationSpec(next,\n+                getPathForLocalization(next, tracker));\n+            rsrcs.add(resource);\n+          }\n         } catch (IOException e) {\n           LOG.error(\"local path for PRIVATE localization could not be \" +\n             \"found. Disks might have failed.\", e);\n@@ -1136,14 +1142,12 @@ LocalizerHeartbeatResponse processHeartbeat(\n       return response;\n     }\n \n-    private Path getPathForLocalization(LocalResource rsrc) throws IOException,\n-        URISyntaxException {\n+    private Path getPathForLocalization(LocalResource rsrc,\n+        LocalResourcesTracker tracker) throws IOException, URISyntaxException {\n       String user = context.getUser();\n       ApplicationId appId =\n           context.getContainerId().getApplicationAttemptId().getApplicationId();\n       LocalResourceVisibility vis = rsrc.getVisibility();\n-      LocalResourcesTracker tracker =\n-          getLocalResourcesTracker(vis, user, appId);\n       String cacheDirectory = null;\n       if (vis == LocalResourceVisibility.PRIVATE) {// PRIVATE Only\n         cacheDirectory = getUserFileCachePath(user);",
                "raw_url": "https://github.com/apache/hadoop/raw/7ffb9943b8838a3bb56684e0722db40d800743a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "sha": "71971c757deb8f7f0db4f51fba24d5ad9a4d2e4b",
                "status": "modified"
            },
            {
                "additions": 109,
                "blob_url": "https://github.com/apache/hadoop/blob/7ffb9943b8838a3bb56684e0722db40d800743a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java",
                "changes": 110,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java?ref=7ffb9943b8838a3bb56684e0722db40d800743a2",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java",
                "patch": "@@ -22,6 +22,7 @@\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n import static org.mockito.Matchers.anyBoolean;\n import static org.mockito.Matchers.anyInt;\n import static org.mockito.Matchers.anyLong;\n@@ -147,7 +148,6 @@\n import org.apache.hadoop.yarn.server.nodemanager.security.NMTokenSecretManagerInNM;\n import org.apache.hadoop.yarn.server.security.ApplicationACLsManager;\n import org.apache.hadoop.yarn.server.utils.BuilderUtils;\n-import org.apache.hadoop.yarn.util.ConverterUtils;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.BeforeClass;\n@@ -1482,6 +1482,114 @@ public void testPublicResourceInitializesLocalDir() throws Exception {\n     }\n   }\n \n+  @Test(timeout = 20000)\n+  @SuppressWarnings(\"unchecked\")\n+  public void testLocalizerHeartbeatWhenAppCleaningUp() throws Exception {\n+    conf.set(YarnConfiguration.NM_LOCAL_DIRS,\n+        lfs.makeQualified(new Path(basedir, 0 + \"\")).toString());\n+    // Start dispatcher.\n+    DrainDispatcher dispatcher = new DrainDispatcher();\n+    dispatcher.init(conf);\n+    dispatcher.start();\n+    dispatcher.register(ApplicationEventType.class, mock(EventHandler.class));\n+    dispatcher.register(ContainerEventType.class, mock(EventHandler.class));\n+\n+    DummyExecutor exec = new DummyExecutor();\n+    LocalDirsHandlerService dirsHandler = new LocalDirsHandlerService();\n+    dirsHandler.init(conf);\n+    // Start resource localization service.\n+    ResourceLocalizationService rawService = new ResourceLocalizationService(\n+        dispatcher, exec, mock(DeletionService.class), dirsHandler, nmContext);\n+    ResourceLocalizationService spyService = spy(rawService);\n+    doReturn(mockServer).when(spyService).createServer();\n+    doReturn(lfs).when(spyService).\n+        getLocalFileContext(isA(Configuration.class));\n+    try {\n+      spyService.init(conf);\n+      spyService.start();\n+\n+      // Init application resources.\n+      final Application app = mock(Application.class);\n+      final ApplicationId appId = BuilderUtils.newApplicationId(1234567890L, 3);\n+      when(app.getUser()).thenReturn(\"user0\");\n+      when(app.getAppId()).thenReturn(appId);\n+      when(app.toString()).thenReturn(appId.toString());\n+      spyService.handle(new ApplicationLocalizationEvent(\n+          LocalizationEventType.INIT_APPLICATION_RESOURCES, app));\n+      dispatcher.await();\n+\n+      // Initialize localizer.\n+      Random r = new Random();\n+      long seed = r.nextLong();\n+      System.out.println(\"SEED: \" + seed);\n+      r.setSeed(seed);\n+      final Container c = getMockContainer(appId, 46, \"user0\");\n+      FSDataOutputStream out =\n+          new FSDataOutputStream(new DataOutputBuffer(), null);\n+      doReturn(out).when(spylfs).createInternal(isA(Path.class),\n+          isA(EnumSet.class), isA(FsPermission.class), anyInt(), anyShort(),\n+          anyLong(), isA(Progressable.class), isA(ChecksumOpt.class),\n+          anyBoolean());\n+      final LocalResource resource1 = getAppMockedResource(r);\n+      final LocalResource resource2 = getAppMockedResource(r);\n+\n+      // Send localization requests for container.\n+      // 2 resources generated with APPLICATION visibility.\n+      final LocalResourceRequest req1 = new LocalResourceRequest(resource1);\n+      final LocalResourceRequest req2 = new LocalResourceRequest(resource2);\n+      Map<LocalResourceVisibility, Collection<LocalResourceRequest>> rsrcs =\n+          new HashMap<LocalResourceVisibility,\n+              Collection<LocalResourceRequest>>();\n+      List<LocalResourceRequest> appResourceList = Arrays.asList(req1, req2);\n+      rsrcs.put(LocalResourceVisibility.APPLICATION, appResourceList);\n+      spyService.handle(new ContainerLocalizationRequestEvent(c, rsrcs));\n+      dispatcher.await();\n+\n+      // Wait for localization to begin.\n+      exec.waitForLocalizers(1);\n+      final String containerIdStr = c.getContainerId().toString();\n+      LocalizerRunner locRunnerForContainer =\n+          spyService.getLocalizerRunner(containerIdStr);\n+      // Heartbeats from container localizer\n+      LocalResourceStatus rsrcSuccess = mock(LocalResourceStatus.class);\n+      LocalizerStatus stat = mock(LocalizerStatus.class);\n+      when(stat.getLocalizerId()).thenReturn(containerIdStr);\n+      when(rsrcSuccess.getResource()).thenReturn(resource1);\n+      when(rsrcSuccess.getLocalSize()).thenReturn(4344L);\n+      when(rsrcSuccess.getLocalPath()).thenReturn(getPath(\"/some/path\"));\n+      when(rsrcSuccess.getStatus()).\n+          thenReturn(ResourceStatusType.FETCH_SUCCESS);\n+      when(stat.getResources()).\n+          thenReturn(Collections.<LocalResourceStatus>emptyList());\n+\n+      // First heartbeat which schedules first resource.\n+      LocalizerHeartbeatResponse response = spyService.heartbeat(stat);\n+      assertEquals(\"NM should tell localizer to be LIVE in Heartbeat.\",\n+          LocalizerAction.LIVE, response.getLocalizerAction());\n+\n+      // Cleanup application.\n+      spyService.handle(new ContainerLocalizationCleanupEvent(c, rsrcs));\n+      spyService.handle(new ApplicationLocalizationEvent(\n+          LocalizationEventType.DESTROY_APPLICATION_RESOURCES, app));\n+      dispatcher.await();\n+      try {\n+        // Directly send heartbeat to introduce race as app is being cleaned up.\n+        locRunnerForContainer.processHeartbeat(\n+            Collections.singletonList(rsrcSuccess));\n+      } catch (Exception e) {\n+        fail(\"Exception should not have been thrown on processing heartbeat\");\n+      }\n+      // Send another heartbeat.\n+      response = spyService.heartbeat(stat);\n+      assertEquals(\"NM should tell localizer to DIE in Heartbeat.\",\n+          LocalizerAction.DIE, response.getLocalizerAction());\n+      exec.setStopLocalization();\n+    } finally {\n+      spyService.stop();\n+      dispatcher.stop();\n+    }\n+  }\n+\n   @Test(timeout=20000)\n   @SuppressWarnings(\"unchecked\") // mocked generics\n   public void testFailedPublicResource() throws Exception {",
                "raw_url": "https://github.com/apache/hadoop/raw/7ffb9943b8838a3bb56684e0722db40d800743a2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java",
                "sha": "13ba2c12c37113519887db0a98769504bc59f00c",
                "status": "modified"
            }
        ],
        "message": "YARN-4355. NPE while processing localizer heartbeat. Contributed by Varun Saxena & Jonathan Hung.",
        "parent": "https://github.com/apache/hadoop/commit/43aef303bf9b71293b00c7ed6e8807d15274ca95",
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceLocalizationService.java"
        ]
    },
    "hadoop_8201ed8": {
        "bug_id": "hadoop_8201ed8",
        "commit": "https://github.com/apache/hadoop/commit/8201ed8009e5f04c49568a8133635d47fcde3989",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/8201ed8009e5f04c49568a8133635d47fcde3989/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/AMRMClient.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/AMRMClient.java?ref=8201ed8009e5f04c49568a8133635d47fcde3989",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/AMRMClient.java",
                "patch": "@@ -122,9 +122,10 @@ protected void serviceInit(Configuration conf) throws Exception {\n     private List<String> racks;\n     private Priority priority;\n     private long allocationRequestId;\n-    private boolean relaxLocality;\n+    private boolean relaxLocality = true;\n     private String nodeLabelsExpression;\n-    private ExecutionTypeRequest executionTypeRequest;\n+    private ExecutionTypeRequest executionTypeRequest =\n+        ExecutionTypeRequest.newInstance();\n     \n     /**\n      * Instantiates a {@link ContainerRequest} with the given constraints and",
                "raw_url": "https://github.com/apache/hadoop/raw/8201ed8009e5f04c49568a8133635d47fcde3989/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/main/java/org/apache/hadoop/yarn/client/api/AMRMClient.java",
                "sha": "60e305f4ff49cfb2fb0665195a44dab0be41a893",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/8201ed8009e5f04c49568a8133635d47fcde3989/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClientContainerRequest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClientContainerRequest.java?ref=8201ed8009e5f04c49568a8133635d47fcde3989",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClientContainerRequest.java",
                "patch": "@@ -51,8 +51,10 @@ public void testOpportunisticAndGuaranteedRequests() {\n \n     Resource capability = Resource.newInstance(1024, 1);\n     ContainerRequest request =\n-        new ContainerRequest(capability, new String[] {\"host1\", \"host2\"},\n-            new String[] {\"/rack2\"}, Priority.newInstance(1));\n+        ContainerRequest.newBuilder().capability(capability)\n+            .nodes(new String[] { \"host1\", \"host2\" })\n+            .racks(new String[] { \"/rack2\" }).priority(Priority.newInstance(1))\n+            .build();\n     client.addContainerRequest(request);\n     verifyResourceRequest(client, request, \"host1\", true);\n     verifyResourceRequest(client, request, \"host2\", true);",
                "raw_url": "https://github.com/apache/hadoop/raw/8201ed8009e5f04c49568a8133635d47fcde3989/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-client/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestAMRMClientContainerRequest.java",
                "sha": "96035394ec7c2b225f123e73b528b08dd2715c36",
                "status": "modified"
            }
        ],
        "message": "YARN-6756. ContainerRequest#executionTypeRequest causes NPE. Contributed by Jian He",
        "parent": "https://github.com/apache/hadoop/commit/cf93d60d3f032000e5b78a08d320793d78799f3d",
        "repo": "hadoop",
        "unit_tests": [
            "TestAMRMClient.java"
        ]
    },
    "hadoop_83798f1": {
        "bug_id": "hadoop_83798f1",
        "commit": "https://github.com/apache/hadoop/commit/83798f15f8602ef580a7885876de114b2425da89",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/83798f15f8602ef580a7885876de114b2425da89/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceSet.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceSet.java?ref=83798f15f8602ef580a7885876de114b2425da89",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceSet.java",
                "patch": "@@ -136,6 +136,10 @@\n   }\n \n   public void resourceLocalizationFailed(LocalResourceRequest request) {\n+    // Skip null request when localization failed for running container\n+    if (request == null) {\n+      return;\n+    }\n     pendingResources.remove(request);\n     resourcesFailedToBeLocalized.add(request);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/83798f15f8602ef580a7885876de114b2425da89/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceSet.java",
                "sha": "745f8a88fb935d700a70b334553e8626026d3f30",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop/blob/83798f15f8602ef580a7885876de114b2425da89/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/TestContainer.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/TestContainer.java?ref=83798f15f8602ef580a7885876de114b2425da89",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/TestContainer.java",
                "patch": "@@ -524,6 +524,27 @@ public void testDockerContainerLocalizationFailureAtDone() throws Exception {\n     }\n   }\n   \n+  @Test\n+  @SuppressWarnings(\"unchecked\")\n+  public void testLocalizationFailureWhileRunning()\n+      throws Exception {\n+    WrappedContainer wc = null;\n+    try {\n+      wc = new WrappedContainer(6, 314159265358979L, 4344, \"yak\");\n+      wc.initContainer();\n+      wc.localizeResources();\n+      wc.launchContainer();\n+      reset(wc.localizerBus);\n+      assertEquals(ContainerState.RUNNING, wc.c.getContainerState());\n+      // Now in RUNNING, handle ContainerResourceFailedEvent, cause NPE before\n+      wc.handleContainerResourceFailedEvent();\n+    } finally {\n+      if (wc != null) {\n+        wc.finished();\n+      }\n+    }\n+  }\n+\n   @Test\n   @SuppressWarnings(\"unchecked\") // mocked generic\n   public void testCleanupOnKillRequest() throws Exception {\n@@ -1400,6 +1421,11 @@ public void resourceFailedContainer() {\n       drainDispatcherEvents();\n     }\n \n+    public void handleContainerResourceFailedEvent() {\n+      c.handle(new ContainerResourceFailedEvent(cId, null, null));\n+      drainDispatcherEvents();\n+    }\n+\n     // Localize resources \n     // Skip some resources so as to consider them failed\n     public Map<Path, List<String>> doLocalizeResources(",
                "raw_url": "https://github.com/apache/hadoop/raw/83798f15f8602ef580a7885876de114b2425da89/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/container/TestContainer.java",
                "sha": "c32ff1a66be6f721b537578546b6af1a34f12ac7",
                "status": "modified"
            }
        ],
        "message": "YARN-7511. NPE in ContainerLocalizer when localization failed for running container. Contributed by Tao Yang",
        "parent": "https://github.com/apache/hadoop/commit/55669515f626eb5b1f3ba25095f3e306c243d899",
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceSet.java"
        ]
    },
    "hadoop_84c35ac": {
        "bug_id": "hadoop_84c35ac",
        "commit": "https://github.com/apache/hadoop/commit/84c35ac6c4a76c31d9bf9c87b87ed29394564611",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/dev-support/findbugs-exclude.xml",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/dev-support/findbugs-exclude.xml?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/dev-support/findbugs-exclude.xml",
                "patch": "@@ -117,8 +117,15 @@\n \n   <!-- Object cast is based on the event type -->\n   <Match>\n-    <Class name=\"org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher$ApplicationEventHandler\" />\n-     <Bug pattern=\"BC_UNCONFIRMED_CAST\" />\n+    <Class name=\"org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher\" />\n+    <Method name=\"publishApplicationEvent\" />\n+    <Bug pattern=\"BC_UNCONFIRMED_CAST\" />\n+  </Match>\n+\n+  <Match>\n+    <Class name=\"org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher\" />\n+    <Method name=\"publishLocalizationEvent\" />\n+    <Bug pattern=\"BC_UNCONFIRMED_CAST\" />\n   </Match>\n \n   <Match>",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/dev-support/findbugs-exclude.xml",
                "sha": "08c6ba28c766401de336d98b5c306f6103ff461a",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/TimelineEntity.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/TimelineEntity.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 10,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/TimelineEntity.java",
                "patch": "@@ -17,22 +17,23 @@\n  */\n package org.apache.hadoop.yarn.api.records.timelineservice;\n \n-import org.apache.hadoop.classification.InterfaceAudience;\n-import org.apache.hadoop.classification.InterfaceStability;\n-import org.apache.hadoop.yarn.util.TimelineServiceHelper;\n-import org.codehaus.jackson.annotate.JsonSetter;\n-\n-import javax.xml.bind.annotation.XmlAccessType;\n-import javax.xml.bind.annotation.XmlAccessorType;\n-import javax.xml.bind.annotation.XmlElement;\n-import javax.xml.bind.annotation.XmlRootElement;\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.Map;\n import java.util.NavigableSet;\n import java.util.Set;\n import java.util.TreeSet;\n \n+import javax.xml.bind.annotation.XmlAccessType;\n+import javax.xml.bind.annotation.XmlAccessorType;\n+import javax.xml.bind.annotation.XmlElement;\n+import javax.xml.bind.annotation.XmlRootElement;\n+\n+import org.apache.hadoop.classification.InterfaceAudience;\n+import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.yarn.util.TimelineServiceHelper;\n+import org.codehaus.jackson.annotate.JsonSetter;\n+\n /**\n  * The basic timeline entity data structure for timeline service v2. Timeline\n  * entity objects are not thread safe and should not be accessed concurrently.\n@@ -564,6 +565,10 @@ protected TimelineEntity getReal() {\n   }\n \n   public String toString() {\n-    return identifier.toString();\n+    if (real == null) {\n+      return identifier.toString();\n+    } else {\n+      return real.toString();\n+    }\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/records/timelineservice/TimelineEntity.java",
                "sha": "7ce8279e5f496f4858713282347008c11f833a14",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineClientImpl.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineClientImpl.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 18,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineClientImpl.java",
                "patch": "@@ -429,9 +429,8 @@ protected void putObjects(String path, MultivaluedMap<String, String> params,\n         URI uri = constructResURI(getConfig(), timelineServiceAddress, true);\n         putObjects(uri, path, params, obj);\n         needRetry = false;\n-      } catch (Exception e) {\n-        // TODO only handle exception for timelineServiceAddress being updated.\n-        // skip retry for other exceptions.\n+      } catch (IOException e) {\n+        // handle exception for timelineServiceAddress being updated.\n         checkRetryWithSleep(retries, e);\n         retries--;\n       }\n@@ -458,29 +457,27 @@ private int verifyRestEndPointAvailable() throws YarnException {\n    * @param retries\n    * @param e\n    */\n-  private void checkRetryWithSleep(int retries, Exception e) throws\n-      YarnException, IOException {\n+  private void checkRetryWithSleep(int retries, IOException e)\n+      throws YarnException, IOException {\n     if (retries > 0) {\n       try {\n         Thread.sleep(this.serviceRetryInterval);\n       } catch (InterruptedException ex) {\n         Thread.currentThread().interrupt();\n+        throw new YarnException(\"Interrupted while retrying to connect to ATS\");\n       }\n     } else {\n-      LOG.error(\"TimelineClient has reached to max retry times :\" +\n-          this.maxServiceRetries + \" for service address: \" +\n-          timelineServiceAddress);\n-      if (e instanceof YarnException) {\n-        throw (YarnException)e;\n-      } else if (e instanceof IOException) {\n-        throw (IOException)e;\n-      } else {\n-        throw new YarnException(e);\n-      }\n+      StringBuilder msg =\n+          new StringBuilder(\"TimelineClient has reached to max retry times : \");\n+      msg.append(this.maxServiceRetries);\n+      msg.append(\" for service address: \");\n+      msg.append(timelineServiceAddress);\n+      LOG.error(msg.toString());\n+      throw new IOException(msg.toString(), e);\n     }\n   }\n \n-  private void putObjects(\n+  protected void putObjects(\n       URI base, String path, MultivaluedMap<String, String> params, Object obj)\n           throws IOException, YarnException {\n     ClientResponse resp;\n@@ -636,17 +633,19 @@ private Object operateDelegationToken(\n \n   /**\n    * Poll TimelineServiceAddress for maximum of retries times if it is null.\n+   *\n    * @param retries\n    * @return the left retry times\n+   * @throws IOException\n    */\n-  private int pollTimelineServiceAddress(int retries) {\n+  private int pollTimelineServiceAddress(int retries) throws YarnException {\n     while (timelineServiceAddress == null && retries > 0) {\n       try {\n         Thread.sleep(this.serviceRetryInterval);\n       } catch (InterruptedException e) {\n         Thread.currentThread().interrupt();\n+        throw new YarnException(\"Interrupted while trying to connect ATS\");\n       }\n-      // timelineServiceAddress = getTimelineServiceAddress();\n       retries--;\n     }\n     return retries;",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/client/api/impl/TimelineClientImpl.java",
                "sha": "ef8838e1a1fe349afda5d4349a881fbb0439f2ab",
                "status": "modified"
            },
            {
                "additions": 81,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineClientV2Impl.java",
                "changes": 91,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineClientV2Impl.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 10,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineClientV2Impl.java",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.yarn.client.api.impl;\n \n import java.io.IOException;\n+import java.net.URI;\n import java.util.ArrayList;\n import java.util.List;\n \n@@ -34,23 +35,33 @@\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n+import org.junit.Rule;\n import org.junit.Test;\n+import org.junit.rules.TestName;\n \n public class TestTimelineClientV2Impl {\n   private static final Log LOG =\n       LogFactory.getLog(TestTimelineClientV2Impl.class);\n   private TestV2TimelineClient client;\n   private static long TIME_TO_SLEEP = 150;\n+  private static final String EXCEPTION_MSG = \"Exception in the content\";\n \n   @Before\n   public void setup() {\n-    YarnConfiguration conf = new YarnConfiguration();\n+    conf = new YarnConfiguration();\n     conf.setBoolean(YarnConfiguration.TIMELINE_SERVICE_ENABLED, true);\n     conf.setFloat(YarnConfiguration.TIMELINE_SERVICE_VERSION, 1.0f);\n     conf.setInt(YarnConfiguration.NUMBER_OF_ASYNC_ENTITIES_TO_MERGE, 3);\n-    client = createTimelineClient(conf);\n+    if (!currTestName.getMethodName()\n+        .contains(\"testRetryOnConnectionFailure\")) {\n+      client = createTimelineClient(conf);\n+    }\n   }\n \n+  @Rule\n+  public TestName currTestName = new TestName();\n+  private YarnConfiguration conf;\n+\n   private TestV2TimelineClient createTimelineClient(YarnConfiguration conf) {\n     ApplicationId id = ApplicationId.newInstance(0, 0);\n     TestV2TimelineClient client = new TestV2TimelineClient(id);\n@@ -59,9 +70,34 @@ private TestV2TimelineClient createTimelineClient(YarnConfiguration conf) {\n     return client;\n   }\n \n-  private class TestV2TimelineClient extends TimelineClientImpl {\n+  private class TestV2TimelineClientForExceptionHandling\n+      extends TimelineClientImpl {\n+    public TestV2TimelineClientForExceptionHandling(ApplicationId id) {\n+      super(id);\n+    }\n+\n+    protected boolean throwYarnException;\n+\n+    public void setThrowYarnException(boolean throwYarnException) {\n+      this.throwYarnException = throwYarnException;\n+    }\n+\n+    @Override\n+    protected void putObjects(URI base, String path,\n+        MultivaluedMap<String, String> params, Object obj)\n+            throws IOException, YarnException {\n+      if (throwYarnException) {\n+        throw new YarnException(EXCEPTION_MSG);\n+      } else {\n+        throw new IOException(\n+            \"Failed to get the response from the timeline server.\");\n+      }\n+    }\n+  }\n+\n+  private class TestV2TimelineClient\n+      extends TestV2TimelineClientForExceptionHandling {\n     private boolean sleepBeforeReturn;\n-    private boolean throwException;\n \n     private List<TimelineEntities> publishedEntities;\n \n@@ -75,10 +111,6 @@ public void setSleepBeforeReturn(boolean sleepBeforeReturn) {\n       this.sleepBeforeReturn = sleepBeforeReturn;\n     }\n \n-    public void setThrowException(boolean throwException) {\n-      this.throwException = throwException;\n-    }\n-\n     public int getNumOfTimelineEntitiesPublished() {\n       return publishedEntities.size();\n     }\n@@ -91,7 +123,7 @@ public TestV2TimelineClient(ApplicationId id) {\n     protected void putObjects(String path,\n         MultivaluedMap<String, String> params, Object obj)\n             throws IOException, YarnException {\n-      if (throwException) {\n+      if (throwYarnException) {\n         throw new YarnException(\"ActualException\");\n       }\n       publishedEntities.add((TimelineEntities) obj);\n@@ -105,6 +137,45 @@ protected void putObjects(String path,\n     }\n   }\n \n+  @Test\n+  public void testExceptionMultipleRetry() {\n+    TestV2TimelineClientForExceptionHandling client =\n+        new TestV2TimelineClientForExceptionHandling(\n+            ApplicationId.newInstance(0, 0));\n+    int maxRetries = 2;\n+    conf.setInt(YarnConfiguration.TIMELINE_SERVICE_CLIENT_MAX_RETRIES,\n+        maxRetries);\n+    client.init(conf);\n+    client.start();\n+    client.setTimelineServiceAddress(\"localhost:12345\");\n+    try {\n+      client.putEntities(new TimelineEntity());\n+    } catch (IOException e) {\n+      Assert.fail(\"YARN exception is expected\");\n+    } catch (YarnException e) {\n+      Throwable cause = e.getCause();\n+      Assert.assertTrue(\"IOException is expected\",\n+          cause instanceof IOException);\n+      Assert.assertTrue(\"YARN exception is expected\",\n+          cause.getMessage().contains(\n+              \"TimelineClient has reached to max retry times : \" + maxRetries));\n+    }\n+\n+    client.setThrowYarnException(true);\n+    try {\n+      client.putEntities(new TimelineEntity());\n+    } catch (IOException e) {\n+      Assert.fail(\"YARN exception is expected\");\n+    } catch (YarnException e) {\n+      Throwable cause = e.getCause();\n+      Assert.assertTrue(\"YARN exception is expected\",\n+          cause instanceof YarnException);\n+      Assert.assertTrue(\"YARN exception is expected\",\n+          cause.getMessage().contains(EXCEPTION_MSG));\n+    }\n+    client.stop();\n+  }\n+\n   @Test\n   public void testPostEntities() throws Exception {\n     try {\n@@ -189,7 +260,7 @@ public void testSyncCall() throws Exception {\n \n   @Test\n   public void testExceptionCalls() throws Exception {\n-    client.setThrowException(true);\n+    client.setThrowYarnException(true);\n     try {\n       client.putEntitiesAsync(generateEntity(\"1\"));\n     } catch (YarnException e) {",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/client/api/impl/TestTimelineClientV2Impl.java",
                "sha": "71dafdc8461cd58cd88df07c7b6f4926010bc852",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/metrics/ContainerMetricsConstants.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/metrics/ContainerMetricsConstants.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/metrics/ContainerMetricsConstants.java",
                "patch": "@@ -69,4 +69,12 @@\n \n   public static final String ALLOCATED_HOST_HTTP_ADDRESS_ENTITY_INFO =\n       \"YARN_CONTAINER_ALLOCATED_HOST_HTTP_ADDRESS\";\n+\n+  // Event of this type will be emitted by NM.\n+  public static final String LOCALIZATION_START_EVENT_TYPE =\n+      \"YARN_NM_CONTAINER_LOCALIZATION_STARTED\";\n+\n+  // Event of this type will be emitted by NM.\n+  public static final String LOCALIZATION_FINISHED_EVENT_TYPE =\n+      \"YARN_NM_CONTAINER_LOCALIZATION_FINISHED\";\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/metrics/ContainerMetricsConstants.java",
                "sha": "eadb5b792d928c3f33c3b7d7726618660ca3aa19",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
                "patch": "@@ -55,7 +55,6 @@\n import org.apache.hadoop.yarn.api.records.NodeLabel;\n import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.ResourceUtilization;\n-import org.apache.hadoop.yarn.client.api.TimelineClient;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.event.Dispatcher;\n import org.apache.hadoop.yarn.exceptions.YarnException;\n@@ -89,6 +88,7 @@\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitor;\n import org.apache.hadoop.yarn.server.nodemanager.metrics.NodeManagerMetrics;\n import org.apache.hadoop.yarn.server.nodemanager.nodelabels.NodeLabelsProvider;\n+import org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher;\n import org.apache.hadoop.yarn.server.nodemanager.util.NodeManagerHardwareUtils;\n import org.apache.hadoop.yarn.server.utils.BuilderUtils;\n import org.apache.hadoop.yarn.util.resource.Resources;\n@@ -983,9 +983,11 @@ private void updateTimelineClientsAddress(\n                 LOG.debug(\"Sync a new collector address: \" + collectorAddr +\n                     \" for application: \" + appId + \" from RM.\");\n               }\n-              TimelineClient client = application.getTimelineClient();\n-              if (client != null) {\n-                client.setTimelineServiceAddress(collectorAddr);\n+              NMTimelinePublisher nmTimelinePublisher =\n+                  context.getNMTimelinePublisher();\n+              if (nmTimelinePublisher != null) {\n+                nmTimelinePublisher.setTimelineServiceAddress(\n+                    application.getAppId(), collectorAddr);\n               }\n             }\n           }",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeStatusUpdaterImpl.java",
                "sha": "6e0e7601a01c550d3f78ac28da23af8580e78e25",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/collectormanager/NMCollectorService.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/collectormanager/NMCollectorService.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/collectormanager/NMCollectorService.java",
                "patch": "@@ -29,7 +29,6 @@\n import org.apache.hadoop.ipc.Server;\n import org.apache.hadoop.service.CompositeService;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n-import org.apache.hadoop.yarn.client.api.TimelineClient;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.exceptions.YarnException;\n import org.apache.hadoop.yarn.ipc.YarnRPC;\n@@ -42,6 +41,7 @@\n import org.apache.hadoop.yarn.server.nodemanager.Context;\n import org.apache.hadoop.yarn.server.nodemanager.NodeManager;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application;\n+import org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher;\n \n /**\n  * Service that handles collector information. It is used only if the timeline\n@@ -116,10 +116,10 @@ public ReportNewCollectorInfoResponse reportNewCollectorInfo(\n         String collectorAddr = collector.getCollectorAddr();\n         newCollectorsMap.put(appId, collectorAddr);\n         // set registered collector address to TimelineClient.\n-        TimelineClient client =\n-            context.getApplications().get(appId).getTimelineClient();\n-        if (client != null) {\n-          client.setTimelineServiceAddress(collectorAddr);\n+        NMTimelinePublisher nmTimelinePublisher =\n+            context.getNMTimelinePublisher();\n+        if (nmTimelinePublisher != null) {\n+          nmTimelinePublisher.setTimelineServiceAddress(appId, collectorAddr);\n         }\n       }\n       ((NodeManager.NMContext)context).addRegisteredCollectors(",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/collectormanager/NMCollectorService.java",
                "sha": "d667c0ee2463a0cc1d47aa899462cc70d2c4ce47",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/Application.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/Application.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/Application.java",
                "patch": "@@ -22,7 +22,6 @@\n \n import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n-import org.apache.hadoop.yarn.client.api.TimelineClient;\n import org.apache.hadoop.yarn.event.EventHandler;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;\n \n@@ -41,7 +40,4 @@\n   String getFlowVersion();\n \n   long getFlowRunId();\n-  \n-  TimelineClient getTimelineClient();\n-\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/Application.java",
                "sha": "aee0862ae813fbcb96158dd7b84d4ad5fe51b2b8",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 17,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java",
                "patch": "@@ -58,6 +58,7 @@\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerAppFinishedEvent;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerAppStartedEvent;\n import org.apache.hadoop.yarn.server.nodemanager.recovery.NMStateStoreService;\n+import org.apache.hadoop.yarn.server.nodemanager.timelineservice.NMTimelinePublisher;\n import org.apache.hadoop.yarn.server.security.ApplicationACLsManager;\n import org.apache.hadoop.yarn.state.InvalidStateTransitionException;\n import org.apache.hadoop.yarn.state.MultipleArcTransition;\n@@ -83,7 +84,6 @@\n   private final ReadLock readLock;\n   private final WriteLock writeLock;\n   private final Context context;\n-  private TimelineClient timelineClient;\n \n   private static final Log LOG = LogFactory.getLog(ApplicationImpl.class);\n \n@@ -143,7 +143,7 @@ public ApplicationImpl(Dispatcher dispatcher, String user,\n       }\n       this.flowContext = flowContext;\n       if (YarnConfiguration.systemMetricsPublisherEnabled(conf)) {\n-        createAndStartTimelineClient(conf);\n+        context.getNMTimelinePublisher().createTimelineClient(appId);\n       }\n     }\n   }\n@@ -175,13 +175,6 @@ public long getFlowRunId() {\n     }\n   }\n \n-  private void createAndStartTimelineClient(Configuration conf) {\n-    // create and start timeline client\n-    this.timelineClient = TimelineClient.createTimelineClient(appId);\n-    timelineClient.init(conf);\n-    timelineClient.start();\n-  }\n-\n   @Override\n   public String getUser() {\n     return user.toString();\n@@ -192,11 +185,6 @@ public ApplicationId getAppId() {\n     return appId;\n   }\n   \n-  @Override\n-  public TimelineClient getTimelineClient() {\n-    return timelineClient;\n-  }\n-\n   @Override\n   public ApplicationState getApplicationState() {\n     this.readLock.lock();\n@@ -575,9 +563,10 @@ public void transition(ApplicationImpl app, ApplicationEvent event) {\n         registeredCollectors.remove(app.getAppId());\n       }\n       // stop timelineClient when application get finished.\n-      TimelineClient timelineClient = app.getTimelineClient();\n-      if (timelineClient != null) {\n-        timelineClient.stop();\n+      NMTimelinePublisher nmTimelinePublisher =\n+          app.context.getNMTimelinePublisher();\n+      if (nmTimelinePublisher != null) {\n+        nmTimelinePublisher.stopTimelineClient(app.getAppId());\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/application/ApplicationImpl.java",
                "sha": "22779bb4b3869350f70315fbde4d4e0027ae59cd",
                "status": "modified"
            },
            {
                "additions": 119,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/timelineservice/NMTimelinePublisher.java",
                "changes": 210,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/timelineservice/NMTimelinePublisher.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 91,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/timelineservice/NMTimelinePublisher.java",
                "patch": "@@ -18,8 +18,10 @@\n \n package org.apache.hadoop.yarn.server.nodemanager.timelineservice;\n \n+import java.io.IOException;\n import java.util.HashMap;\n import java.util.Map;\n+import java.util.concurrent.ConcurrentHashMap;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -29,7 +31,6 @@\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.ContainerStatus;\n import org.apache.hadoop.yarn.api.records.NodeId;\n-import org.apache.hadoop.yarn.api.records.Priority;\n import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.timelineservice.ContainerEntity;\n import org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity;\n@@ -41,16 +42,15 @@\n import org.apache.hadoop.yarn.event.AsyncDispatcher;\n import org.apache.hadoop.yarn.event.Dispatcher;\n import org.apache.hadoop.yarn.event.EventHandler;\n+import org.apache.hadoop.yarn.exceptions.YarnException;\n import org.apache.hadoop.yarn.server.metrics.ContainerMetricsConstants;\n import org.apache.hadoop.yarn.server.nodemanager.Context;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationContainerFinishedEvent;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent;\n-import org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEvent;\n-import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerEventType;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.ContainerLocalizationEvent;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEvent;\n-import org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.event.LocalizationEventType;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.monitor.ContainersMonitorImpl.ContainerMetric;\n import org.apache.hadoop.yarn.util.ResourceCalculatorProcessTree;\n import org.apache.hadoop.yarn.util.timeline.TimelineUtils;\n@@ -72,22 +72,19 @@\n \n   private String httpAddress;\n \n+  protected final Map<ApplicationId, TimelineClient> appToClientMap;\n+\n   public NMTimelinePublisher(Context context) {\n     super(NMTimelinePublisher.class.getName());\n     this.context = context;\n+    appToClientMap = new ConcurrentHashMap<>();\n   }\n \n   @Override\n   protected void serviceInit(Configuration conf) throws Exception {\n     dispatcher = new AsyncDispatcher();\n     dispatcher.register(NMTimelineEventType.class,\n         new ForwardingEventHandler());\n-    dispatcher\n-        .register(ContainerEventType.class, new ContainerEventHandler());\n-    dispatcher.register(ApplicationEventType.class,\n-        new ApplicationEventHandler());\n-    dispatcher.register(LocalizationEventType.class,\n-        new LocalizationEventDispatcher());\n     addIfService(dispatcher);\n     super.serviceInit(conf);\n   }\n@@ -112,7 +109,6 @@ protected void handleNMTimelineEvent(NMTimelineEvent event) {\n     }\n   }\n \n-  @SuppressWarnings(\"unchecked\")\n   public void reportContainerResourceUsage(Container container, Long pmemUsage,\n       Float cpuUsagePercentPerCore) {\n     if (pmemUsage != ResourceCalculatorProcessTree.UNAVAILABLE ||\n@@ -133,15 +129,32 @@ public void reportContainerResourceUsage(Container container, Long pmemUsage,\n             Math.round(cpuUsagePercentPerCore));\n         entity.addMetric(cpuMetric);\n       }\n-      dispatcher.getEventHandler()\n-          .handle(new TimelinePublishEvent(entity, container.getContainerId()\n-              .getApplicationAttemptId().getApplicationId()));\n+      ApplicationId appId = container.getContainerId().getApplicationAttemptId()\n+          .getApplicationId();\n+      try {\n+        // no need to put it as part of publisher as timeline client already has\n+        // Queuing concept\n+        TimelineClient timelineClient = getTimelineClient(appId);\n+        if (timelineClient != null) {\n+          timelineClient.putEntitiesAsync(entity);\n+        } else {\n+          LOG.error(\"Seems like client has been removed before the container\"\n+              + \" metric could be published for \" + container.getContainerId());\n+        }\n+      } catch (IOException | YarnException e) {\n+        LOG.error(\"Failed to publish Container metrics for container \"\n+            + container.getContainerId(), e);\n+      }\n     }\n   }\n \n-  private void publishContainerCreatedEvent(ContainerEntity entity,\n-      ContainerId containerId, Resource resource, Priority priority,\n-      long timestamp) {\n+  @SuppressWarnings(\"unchecked\")\n+  private void publishContainerCreatedEvent(ContainerEvent event) {\n+    ContainerId containerId = event.getContainerID();\n+    ContainerEntity entity = createContainerEntity(containerId);\n+    Container container = context.getContainers().get(containerId);\n+    Resource resource = container.getResource();\n+\n     Map<String, Object> entityInfo = new HashMap<String, Object>();\n     entityInfo.put(ContainerMetricsConstants.ALLOCATED_MEMORY_ENTITY_INFO,\n         resource.getMemory());\n@@ -152,21 +165,23 @@ private void publishContainerCreatedEvent(ContainerEntity entity,\n     entityInfo.put(ContainerMetricsConstants.ALLOCATED_PORT_ENTITY_INFO,\n         nodeId.getPort());\n     entityInfo.put(ContainerMetricsConstants.ALLOCATED_PRIORITY_ENTITY_INFO,\n-        priority.toString());\n+        container.getPriority().toString());\n     entityInfo.put(\n         ContainerMetricsConstants.ALLOCATED_HOST_HTTP_ADDRESS_ENTITY_INFO,\n         httpAddress);\n     entity.setInfo(entityInfo);\n \n     TimelineEvent tEvent = new TimelineEvent();\n     tEvent.setId(ContainerMetricsConstants.CREATED_EVENT_TYPE);\n-    tEvent.setTimestamp(timestamp);\n+    tEvent.setTimestamp(event.getTimestamp());\n \n     entity.addEvent(tEvent);\n-    entity.setCreatedTime(timestamp);\n-    putEntity(entity, containerId.getApplicationAttemptId().getApplicationId());\n+    entity.setCreatedTime(event.getTimestamp());\n+    dispatcher.getEventHandler().handle(new TimelinePublishEvent(entity,\n+        containerId.getApplicationAttemptId().getApplicationId()));\n   }\n \n+  @SuppressWarnings(\"unchecked\")\n   private void publishContainerFinishedEvent(ContainerStatus containerStatus,\n       long timeStamp) {\n     ContainerId containerId = containerStatus.getContainerId();\n@@ -186,7 +201,38 @@ private void publishContainerFinishedEvent(ContainerStatus containerStatus,\n     tEvent.setInfo(eventInfo);\n \n     entity.addEvent(tEvent);\n-    putEntity(entity, containerId.getApplicationAttemptId().getApplicationId());\n+\n+    dispatcher.getEventHandler().handle(new TimelinePublishEvent(entity,\n+        containerId.getApplicationAttemptId().getApplicationId()));\n+  }\n+\n+  private void publishContainerLocalizationEvent(\n+      ContainerLocalizationEvent event, String eventType) {\n+    Container container = event.getContainer();\n+    ContainerId containerId = container.getContainerId();\n+    TimelineEntity entity = createContainerEntity(containerId);\n+\n+    TimelineEvent tEvent = new TimelineEvent();\n+    tEvent.setId(eventType);\n+    tEvent.setTimestamp(event.getTimestamp());\n+    entity.addEvent(tEvent);\n+\n+    ApplicationId appId =\n+        container.getContainerId().getApplicationAttemptId().getApplicationId();\n+    try {\n+      // no need to put it as part of publisher as timeline client already has\n+      // Queuing concept\n+      TimelineClient timelineClient = getTimelineClient(appId);\n+      if (timelineClient != null) {\n+        timelineClient.putEntitiesAsync(entity);\n+      } else {\n+        LOG.error(\"Seems like client has been removed before the event could be\"\n+            + \" published for \" + container.getContainerId());\n+      }\n+    } catch (IOException | YarnException e) {\n+      LOG.error(\"Failed to publish Container metrics for container \"\n+          + container.getContainerId(), e);\n+    }\n   }\n \n   private static ContainerEntity createContainerEntity(\n@@ -207,23 +253,33 @@ private void putEntity(TimelineEntity entity, ApplicationId appId) {\n         LOG.debug(\"Publishing the entity \" + entity + \", JSON-style content: \"\n             + TimelineUtils.dumpTimelineRecordtoJSON(entity));\n       }\n-      TimelineClient timelineClient =\n-          context.getApplications().get(appId).getTimelineClient();\n-      timelineClient.putEntities(entity);\n+      TimelineClient timelineClient = getTimelineClient(appId);\n+      if (timelineClient != null) {\n+        timelineClient.putEntities(entity);\n+      } else {\n+        LOG.error(\"Seems like client has been removed before the entity \"\n+            + \"could be published for \" + entity);\n+      }\n     } catch (Exception e) {\n       LOG.error(\"Error when publishing entity \" + entity, e);\n     }\n   }\n \n-  @SuppressWarnings(\"unchecked\")\n   public void publishApplicationEvent(ApplicationEvent event) {\n     // publish only when the desired event is received\n     switch (event.getType()) {\n     case INIT_APPLICATION:\n     case FINISH_APPLICATION:\n-    case APPLICATION_CONTAINER_FINISHED:\n     case APPLICATION_LOG_HANDLING_FAILED:\n-      dispatcher.getEventHandler().handle(event);\n+      // TODO need to be handled in future,\n+      // not sure to publish under which entity\n+      break;\n+    case APPLICATION_CONTAINER_FINISHED:\n+      // this is actually used to publish the container Event\n+      ApplicationContainerFinishedEvent evnt =\n+          (ApplicationContainerFinishedEvent) event;\n+      publishContainerFinishedEvent(evnt.getContainerStatus(),\n+          event.getTimestamp());\n       break;\n \n     default:\n@@ -235,12 +291,11 @@ public void publishApplicationEvent(ApplicationEvent event) {\n     }\n   }\n \n-  @SuppressWarnings(\"unchecked\")\n   public void publishContainerEvent(ContainerEvent event) {\n     // publish only when the desired event is received\n     switch (event.getType()) {\n     case INIT_CONTAINER:\n-      dispatcher.getEventHandler().handle(event);\n+      publishContainerCreatedEvent(event);\n       break;\n \n     default:\n@@ -253,15 +308,17 @@ public void publishContainerEvent(ContainerEvent event) {\n     }\n   }\n \n-  @SuppressWarnings(\"unchecked\")\n   public void publishLocalizationEvent(LocalizationEvent event) {\n     // publish only when the desired event is received\n     switch (event.getType()) {\n     case CONTAINER_RESOURCES_LOCALIZED:\n+      publishContainerLocalizationEvent((ContainerLocalizationEvent) event,\n+          ContainerMetricsConstants.LOCALIZATION_FINISHED_EVENT_TYPE);\n+      break;\n     case INIT_CONTAINER_RESOURCES:\n-      dispatcher.getEventHandler().handle(event);\n+      publishContainerLocalizationEvent((ContainerLocalizationEvent) event,\n+          ContainerMetricsConstants.LOCALIZATION_START_EVENT_TYPE);\n       break;\n-\n     default:\n       if (LOG.isDebugEnabled()) {\n         LOG.debug(event.getType()\n@@ -272,64 +329,6 @@ public void publishLocalizationEvent(LocalizationEvent event) {\n     }\n   }\n \n-  private class ApplicationEventHandler implements\n-      EventHandler<ApplicationEvent> {\n-    @Override\n-    public void handle(ApplicationEvent event) {\n-      switch (event.getType()) {\n-      case APPLICATION_CONTAINER_FINISHED:\n-        // this is actually used to publish the container Event\n-        ApplicationContainerFinishedEvent evnt =\n-            (ApplicationContainerFinishedEvent) event;\n-        publishContainerFinishedEvent(evnt.getContainerStatus(),\n-            event.getTimestamp());\n-        break;\n-      default:\n-        LOG.error(\"Seems like event type is captured only in \"\n-            + \"publishApplicationEvent method and not handled here\");\n-        break;\n-      }\n-    }\n-  }\n-\n-  private class ContainerEventHandler implements EventHandler<ContainerEvent> {\n-    @Override\n-    public void handle(ContainerEvent event) {\n-      ContainerId containerId = event.getContainerID();\n-      Container container = context.getContainers().get(containerId);\n-      long timestamp = event.getTimestamp();\n-      ContainerEntity entity = createContainerEntity(containerId);\n-\n-      switch (event.getType()) {\n-      case INIT_CONTAINER:\n-        publishContainerCreatedEvent(entity, containerId,\n-            container.getResource(), container.getPriority(), timestamp);\n-        break;\n-      default:\n-        LOG.error(\"Seems like event type is captured only in \"\n-            + \"publishContainerEvent method and not handled here\");\n-        break;\n-      }\n-    }\n-  }\n-\n-  private static final class LocalizationEventDispatcher implements\n-      EventHandler<LocalizationEvent> {\n-    @Override\n-    public void handle(LocalizationEvent event) {\n-      switch (event.getType()) {\n-      case INIT_CONTAINER_RESOURCES:\n-      case CONTAINER_RESOURCES_LOCALIZED:\n-        // TODO after priority based flush jira is finished\n-        break;\n-      default:\n-        LOG.error(\"Seems like event type is captured only in \"\n-            + \"publishLocalizationEvent method and not handled here\");\n-        break;\n-      }\n-    }\n-  }\n-\n   /**\n    * EventHandler implementation which forward events to NMMetricsPublisher.\n    * Making use of it, NMMetricsPublisher can avoid to have a public handle\n@@ -363,4 +362,33 @@ public TimelineEntity getTimelineEntityToPublish() {\n       return entityToPublish;\n     }\n   }\n+\n+  public void createTimelineClient(ApplicationId appId) {\n+    if (!appToClientMap.containsKey(appId)) {\n+      TimelineClient timelineClient =\n+          TimelineClient.createTimelineClient(appId);\n+      timelineClient.init(getConfig());\n+      timelineClient.start();\n+      appToClientMap.put(appId, timelineClient);\n+    }\n+  }\n+\n+  public void stopTimelineClient(ApplicationId appId) {\n+    TimelineClient client = appToClientMap.remove(appId);\n+    if (client != null) {\n+      client.stop();\n+    }\n+  }\n+\n+  public void setTimelineServiceAddress(ApplicationId appId,\n+      String collectorAddr) {\n+    TimelineClient client = appToClientMap.get(appId);\n+    if (client != null) {\n+      client.setTimelineServiceAddress(collectorAddr);\n+    }\n+  }\n+\n+  private TimelineClient getTimelineClient(ApplicationId appId) {\n+    return appToClientMap.get(appId);\n+  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/timelineservice/NMTimelinePublisher.java",
                "sha": "4d3dafdc06889bbd05c4373b7198d638d2cb605c",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/timelineservice/TestNMTimelinePublisher.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/timelineservice/TestNMTimelinePublisher.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 12,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/timelineservice/TestNMTimelinePublisher.java",
                "patch": "@@ -20,14 +20,12 @@\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNotNull;\n-import static org.mockito.Matchers.any;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.when;\n \n import java.io.IOException;\n import java.util.Iterator;\n import java.util.Map.Entry;\n-import java.util.concurrent.ConcurrentMap;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n@@ -39,7 +37,6 @@\n import org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl;\n import org.apache.hadoop.yarn.exceptions.YarnException;\n import org.apache.hadoop.yarn.server.nodemanager.Context;\n-import org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;\n import org.apache.hadoop.yarn.util.ResourceCalculatorProcessTree;\n import org.junit.Assert;\n@@ -53,20 +50,23 @@\n   public void testContainerResourceUsage() {\n     Context context = mock(Context.class);\n     @SuppressWarnings(\"unchecked\")\n-    ConcurrentMap<ApplicationId, Application> map = mock(ConcurrentMap.class);\n-    Application aApp = mock(Application.class);\n-    when(map.get(any(ApplicationId.class))).thenReturn(aApp);\n-    DummyTimelineClient timelineClient = new DummyTimelineClient();\n-    when(aApp.getTimelineClient()).thenReturn(timelineClient);\n-    when(context.getApplications()).thenReturn(map);\n+    final DummyTimelineClient timelineClient = new DummyTimelineClient();\n     when(context.getNodeId()).thenReturn(NodeId.newInstance(\"localhost\", 0));\n     when(context.getHttpPort()).thenReturn(0);\n-    NMTimelinePublisher publisher = new NMTimelinePublisher(context);\n+    NMTimelinePublisher publisher = new NMTimelinePublisher(context) {\n+      public void createTimelineClient(ApplicationId appId) {\n+        if (!appToClientMap.containsKey(appId)) {\n+          appToClientMap.put(appId, timelineClient);\n+        }\n+      }\n+    };\n     publisher.init(new Configuration());\n     publisher.start();\n+    ApplicationId appId = ApplicationId.newInstance(0, 1);\n+    publisher.createTimelineClient(appId);\n     Container aContainer = mock(Container.class);\n     when(aContainer.getContainerId()).thenReturn(ContainerId.newContainerId(\n-        ApplicationAttemptId.newInstance(ApplicationId.newInstance(0, 1), 1),\n+        ApplicationAttemptId.newInstance(appId, 1),\n         0L));\n     publisher.reportContainerResourceUsage(aContainer, 1024L, 8F);\n     verifyPublishedResourceUsageMetrics(timelineClient, 1024L, 8);\n@@ -141,7 +141,7 @@ private void verifyPublishedResourceUsageMetrics(\n     private TimelineEntity[] lastPublishedEntities;\n \n     @Override\n-    public void putEntities(TimelineEntity... entities)\n+    public void putEntitiesAsync(TimelineEntity... entities)\n         throws IOException, YarnException {\n       this.lastPublishedEntities = entities;\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/timelineservice/TestNMTimelinePublisher.java",
                "sha": "4aa28d2dbe7e96fd1e9fba92239de581bd8bcce8",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/webapp/MockApp.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/webapp/MockApp.java?ref=84c35ac6c4a76c31d9bf9c87b87ed29394564611",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/webapp/MockApp.java",
                "patch": "@@ -101,9 +101,4 @@ public String getFlowVersion() {\n   public long getFlowRunId() {\n     return flowRunId;\n   }\n-  \n-  @Override\n-  public TimelineClient getTimelineClient() {\n-    return timelineClient;\n-  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/84c35ac6c4a76c31d9bf9c87b87ed29394564611/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/webapp/MockApp.java",
                "sha": "c98304001a17c2af13f1958189c3cca4e5f7fbbd",
                "status": "modified"
            }
        ],
        "message": "YARN-4711. NM is going down with NPE's due to single thread processing of events by Timeline client (Naganarasimha G R via sjlee)",
        "parent": "https://github.com/apache/hadoop/commit/6f6cc647d6e77f6cc4c66e0534f8c73bc1612a1b",
        "repo": "hadoop",
        "unit_tests": [
            "TestApplication.java",
            "TestNMTimelinePublisher.java"
        ]
    },
    "hadoop_84dfae2": {
        "bug_id": "hadoop_84dfae2",
        "commit": "https://github.com/apache/hadoop/commit/84dfae2f8a1c787380c65bf8de59bfd2d65901e3",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/84dfae2f8a1c787380c65bf8de59bfd2d65901e3/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=84dfae2f8a1c787380c65bf8de59bfd2d65901e3",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -222,6 +222,9 @@ Release 2.4.1 - UNRELEASED\n     YARN-1957. Consider the max capacity of the queue when computing the ideal\n     capacity for preemption. (Carlo Curino via cdouglas)\n \n+    YARN-1986. In Fifo Scheduler, node heartbeat in between creating app and\n+    attempt causes NPE (Hong Zhiguo via Sandy Ryza)\n+\n Release 2.4.0 - 2014-04-07 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/84dfae2f8a1c787380c65bf8de59bfd2d65901e3/hadoop-yarn-project/CHANGES.txt",
                "sha": "d82cd482a1fb2a3384b3b6a8bfc544f343ee50ae",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/84dfae2f8a1c787380c65bf8de59bfd2d65901e3/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java?ref=84dfae2f8a1c787380c65bf8de59bfd2d65901e3",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java",
                "patch": "@@ -360,7 +360,8 @@ private FiCaSchedulerNode getNode(NodeId nodeId) {\n     return nodes.get(nodeId);\n   }\n \n-  private synchronized void addApplication(ApplicationId applicationId,\n+  @VisibleForTesting\n+  public synchronized void addApplication(ApplicationId applicationId,\n       String queue, String user) {\n     SchedulerApplication application =\n         new SchedulerApplication(DEFAULT_QUEUE, user);\n@@ -372,7 +373,8 @@ private synchronized void addApplication(ApplicationId applicationId,\n         .handle(new RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED));\n   }\n \n-  private synchronized void\n+  @VisibleForTesting\n+  public synchronized void\n       addApplicationAttempt(ApplicationAttemptId appAttemptId,\n           boolean transferStateFromPreviousAttempt) {\n     SchedulerApplication application =\n@@ -458,6 +460,9 @@ private void assignContainers(FiCaSchedulerNode node) {\n         .entrySet()) {\n       FiCaSchedulerApp application =\n           (FiCaSchedulerApp) e.getValue().getCurrentAppAttempt();\n+      if (application == null) {\n+        continue;\n+      }\n       LOG.debug(\"pre-assignContainers\");\n       application.showRequests();\n       synchronized (application) {\n@@ -497,6 +502,9 @@ private void assignContainers(FiCaSchedulerNode node) {\n     for (SchedulerApplication application : applications.values()) {\n       FiCaSchedulerApp attempt =\n           (FiCaSchedulerApp) application.getCurrentAppAttempt();\n+      if (attempt == null) {\n+        continue;\n+      }\n       attempt.setHeadroom(Resources.subtract(clusterResource, usedResource));\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/84dfae2f8a1c787380c65bf8de59bfd2d65901e3/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fifo/FifoScheduler.java",
                "sha": "21fcdecf4f9ef9fa2ee555996ba746252012e656",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hadoop/blob/84dfae2f8a1c787380c65bf8de59bfd2d65901e3/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestFifoScheduler.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestFifoScheduler.java?ref=84dfae2f8a1c787380c65bf8de59bfd2d65901e3",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestFifoScheduler.java",
                "patch": "@@ -52,6 +52,7 @@\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler;\n import org.apache.hadoop.yarn.server.utils.BuilderUtils;\n+import org.apache.hadoop.yarn.util.resource.Resources;\n import org.apache.log4j.Level;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.Logger;\n@@ -66,7 +67,7 @@\n   \n   private final int GB = 1024;\n   private static YarnConfiguration conf;\n-  \n+\n   @BeforeClass\n   public static void setup() {\n     conf = new YarnConfiguration();\n@@ -213,6 +214,32 @@ public void test() throws Exception {\n     rm.stop();\n   }\n \n+  @Test\n+  public void testNodeUpdateBeforeAppAttemptInit() throws Exception {\n+    FifoScheduler scheduler = new FifoScheduler();\n+    MockRM rm = new MockRM(conf);\n+    scheduler.reinitialize(conf, rm.getRMContext());\n+\n+    RMNode node = MockNodes.newNodeInfo(1,\n+            Resources.createResource(1024, 4), 1, \"127.0.0.1\");\n+    scheduler.handle(new NodeAddedSchedulerEvent(node));\n+\n+    ApplicationId appId = ApplicationId.newInstance(0, 1);\n+    scheduler.addApplication(appId, \"queue1\", \"user1\");\n+\n+    NodeUpdateSchedulerEvent updateEvent = new NodeUpdateSchedulerEvent(node);\n+    try {\n+      scheduler.handle(updateEvent);\n+    } catch (NullPointerException e) {\n+        Assert.fail();\n+    }\n+\n+    ApplicationAttemptId attId = ApplicationAttemptId.newInstance(appId, 1);\n+    scheduler.addApplicationAttempt(attId, false);\n+\n+    rm.stop();\n+  }\n+\n   private void testMinimumAllocation(YarnConfiguration conf, int testAlloc)\n       throws Exception {\n     MockRM rm = new MockRM(conf);",
                "raw_url": "https://github.com/apache/hadoop/raw/84dfae2f8a1c787380c65bf8de59bfd2d65901e3/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestFifoScheduler.java",
                "sha": "fcd5041e4257d4cc1ce64ae90278aba09576f14d",
                "status": "modified"
            }
        ],
        "message": "YARN-1986. In Fifo Scheduler, node heartbeat in between creating app and attempt causes NPE (Hong Zhiguo via Sandy Ryza)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1594476 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/cf7dddb603b762e0808d77b7ddefa73274c09156",
        "repo": "hadoop",
        "unit_tests": [
            "TestFifoScheduler.java"
        ]
    },
    "hadoop_855d529": {
        "bug_id": "hadoop_855d529",
        "commit": "https://github.com/apache/hadoop/commit/855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -1181,6 +1181,9 @@ Release 2.7.2 - UNRELEASED\n     YARN-4320. TestJobHistoryEventHandler fails as AHS in MiniYarnCluster no longer\n     binds to default port 8188. (Varun Saxena via ozawa)\n \n+    YARN-4354. Public resource localization fails with NPE. (Jason Lowe via \n+    junping_du)\n+\n Release 2.7.1 - 2015-07-06\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb/hadoop-yarn-project/CHANGES.txt",
                "sha": "747c7290128fa6e806e74810daaf885e9b86c06c",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalResourcesTrackerImpl.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalResourcesTrackerImpl.java?ref=855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalResourcesTrackerImpl.java",
                "patch": "@@ -185,14 +185,22 @@ public synchronized void handle(ResourceEvent event) {\n       break;\n     }\n \n+    if (rsrc == null) {\n+      LOG.warn(\"Received \" + event.getType() + \" event for request \" + req\n+          + \" but localized resource is missing\");\n+      return;\n+    }\n     rsrc.handle(event);\n \n     // Remove the resource if its downloading and its reference count has\n     // become 0 after RELEASE. This maybe because a container was killed while\n     // localizing and no other container is referring to the resource.\n+    // NOTE: This should NOT be done for public resources since the\n+    //       download is not associated with a container-specific localizer.\n     if (event.getType() == ResourceEventType.RELEASE) {\n       if (rsrc.getState() == ResourceState.DOWNLOADING &&\n-          rsrc.getRefCount() <= 0) {\n+          rsrc.getRefCount() <= 0 &&\n+          rsrc.getRequest().getVisibility() != LocalResourceVisibility.PUBLIC) {\n         removeResource(req);\n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/LocalResourcesTrackerImpl.java",
                "sha": "38fffe6c6cf1df17f1d11fbf278aba155b39d803",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/hadoop/blob/855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestLocalResourcesTrackerImpl.java",
                "changes": 56,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestLocalResourcesTrackerImpl.java?ref=855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestLocalResourcesTrackerImpl.java",
                "patch": "@@ -141,12 +141,12 @@ public void test() {\n       tracker.handle(rel21Event);\n \n       dispatcher.await();\n-      verifyTrackedResourceCount(tracker, 1);\n+      verifyTrackedResourceCount(tracker, 2);\n \n       // Verify resource with non zero ref count is not removed.\n       Assert.assertEquals(2, lr1.getRefCount());\n       Assert.assertFalse(tracker.remove(lr1, mockDelService));\n-      verifyTrackedResourceCount(tracker, 1);\n+      verifyTrackedResourceCount(tracker, 2);\n \n       // Localize resource1\n       ResourceLocalizedEvent rle =\n@@ -161,7 +161,7 @@ public void test() {\n \n       // Verify resources in state LOCALIZED with ref-count=0 is removed.\n       Assert.assertTrue(tracker.remove(lr1, mockDelService));\n-      verifyTrackedResourceCount(tracker, 0);\n+      verifyTrackedResourceCount(tracker, 1);\n     } finally {\n       if (dispatcher != null) {\n         dispatcher.stop();\n@@ -899,6 +899,56 @@ public void testResourcePresentInGoodDir() throws IOException {\n     }\n   }\n \n+  @Test\n+  @SuppressWarnings(\"unchecked\")\n+  public void testReleaseWhileDownloading() throws Exception {\n+    String user = \"testuser\";\n+    DrainDispatcher dispatcher = null;\n+    try {\n+      Configuration conf = new Configuration();\n+      dispatcher = createDispatcher(conf);\n+      EventHandler<LocalizerEvent> localizerEventHandler =\n+          mock(EventHandler.class);\n+      EventHandler<LocalizerEvent> containerEventHandler =\n+          mock(EventHandler.class);\n+      dispatcher.register(LocalizerEventType.class, localizerEventHandler);\n+      dispatcher.register(ContainerEventType.class, containerEventHandler);\n+\n+      ContainerId cId = BuilderUtils.newContainerId(1, 1, 1, 1);\n+      LocalizerContext lc = new LocalizerContext(user, cId, null);\n+\n+      LocalResourceRequest req =\n+          createLocalResourceRequest(user, 1, 1, LocalResourceVisibility.PUBLIC);\n+      LocalizedResource lr = createLocalizedResource(req, dispatcher);\n+      ConcurrentMap<LocalResourceRequest, LocalizedResource> localrsrc =\n+          new ConcurrentHashMap<LocalResourceRequest, LocalizedResource>();\n+      localrsrc.put(req, lr);\n+      LocalResourcesTracker tracker =\n+          new LocalResourcesTrackerImpl(user, null, dispatcher, localrsrc,\n+              false, conf, new NMNullStateStoreService(), null);\n+\n+      // request the resource\n+      ResourceEvent reqEvent =\n+          new ResourceRequestEvent(req, LocalResourceVisibility.PUBLIC, lc);\n+      tracker.handle(reqEvent);\n+\n+      // release the resource\n+      ResourceEvent relEvent = new ResourceReleaseEvent(req, cId);\n+      tracker.handle(relEvent);\n+\n+      // download completing after release\n+      ResourceLocalizedEvent rle =\n+          new ResourceLocalizedEvent(req, new Path(\"file:///tmp/r1\"), 1);\n+      tracker.handle(rle);\n+\n+      dispatcher.await();\n+    } finally {\n+      if (dispatcher != null) {\n+        dispatcher.stop();\n+      }\n+    }\n+  }\n+\n   private boolean createdummylocalizefile(Path path) {\n     boolean ret = false;\n     File file = new File(path.toUri().getRawPath().toString());",
                "raw_url": "https://github.com/apache/hadoop/raw/855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestLocalResourcesTrackerImpl.java",
                "sha": "21ceaa4f9810820f89ff19803eb0f46606b25166",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java?ref=855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java",
                "patch": "@@ -487,8 +487,8 @@ public void testResourceRelease() throws Exception {\n         Assert.assertEquals(\"Incorrect reference count\", 0, lr.getRefCount());\n         pubRsrcs.remove(lr.getRequest());\n       }\n-      Assert.assertEquals(2, pubRsrcs.size());\n-      Assert.assertEquals(0, pubRsrcCount);\n+      Assert.assertEquals(0, pubRsrcs.size());\n+      Assert.assertEquals(2, pubRsrcCount);\n \n       appRsrcCount = 0;\n       for (LocalizedResource lr : appTracker) {",
                "raw_url": "https://github.com/apache/hadoop/raw/855d52927b6115e2cfbd97a94d6c1a3ddf0e94bb/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/TestResourceLocalizationService.java",
                "sha": "c14ec7f45deadc7cee1ddd43f049fa4d3e5f0525",
                "status": "modified"
            }
        ],
        "message": "YARN-4354. Public resource localization fails with NPE. Contributed by Jason Lowe.",
        "parent": "https://github.com/apache/hadoop/commit/c753617a48bffed491b9ca7a5ca6b3d2df5721bf",
        "repo": "hadoop",
        "unit_tests": [
            "TestLocalResourcesTrackerImpl.java"
        ]
    },
    "hadoop_883f682": {
        "bug_id": "hadoop_883f682",
        "commit": "https://github.com/apache/hadoop/commit/883f68222a9cfd06f79a8fcd75ec9fef00abc035",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperation.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperation.java?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperation.java",
                "patch": "@@ -54,7 +54,9 @@\n     GPU(\"--module-gpu\"),\n     FPGA(\"--module-fpga\"),\n     LIST_AS_USER(\"\"), // no CLI switch supported yet.\n-    ADD_NUMA_PARAMS(\"\"); // no CLI switch supported yet.\n+    ADD_NUMA_PARAMS(\"\"), // no CLI switch supported yet.\n+    REMOVE_DOCKER_CONTAINER(\"--remove-docker-container\"),\n+    INSPECT_DOCKER_CONTAINER(\"--inspect-docker-container\");\n \n     private final String option;\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperation.java",
                "sha": "92a82e8fbcdab790b80b82d5b5b585fcb508f3d6",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DockerLinuxContainerRuntime.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DockerLinuxContainerRuntime.java?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 9,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DockerLinuxContainerRuntime.java",
                "patch": "@@ -22,6 +22,7 @@\n \n import com.google.common.annotations.VisibleForTesting;\n import org.apache.hadoop.security.Credentials;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.server.nodemanager.Context;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommand;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker.DockerCommandExecutor;\n@@ -384,7 +385,7 @@ private String runDockerVolumeCommand(DockerVolumeCommand dockerVolumeCommand,\n       Container container) throws ContainerExecutionException {\n     try {\n       String commandFile = dockerClient.writeCommandToTempFile(\n-          dockerVolumeCommand, container, nmContext);\n+          dockerVolumeCommand, container.getContainerId(), nmContext);\n       PrivilegedOperation privOp = new PrivilegedOperation(\n           PrivilegedOperation.OperationType.RUN_DOCKER_CMD);\n       privOp.appendArgs(commandFile);\n@@ -734,6 +735,7 @@ private String getUserIdInfo(String userName)\n   public void launchContainer(ContainerRuntimeContext ctx)\n       throws ContainerExecutionException {\n     Container container = ctx.getContainer();\n+    ContainerId containerId = container.getContainerId();\n     Map<String, String> environment = container.getLaunchContext()\n         .getEnvironment();\n     String imageName = environment.get(ENV_DOCKER_CONTAINER_IMAGE);\n@@ -750,7 +752,7 @@ public void launchContainer(ContainerRuntimeContext ctx)\n \n     validateImageName(imageName);\n \n-    String containerIdStr = container.getContainerId().toString();\n+    String containerIdStr = containerId.toString();\n     String runAsUser = ctx.getExecutionAttribute(RUN_AS_USER);\n     String dockerRunAsUser = runAsUser;\n     Path containerWorkDir = ctx.getExecutionAttribute(CONTAINER_WORK_DIR);\n@@ -908,7 +910,7 @@ public void launchContainer(ContainerRuntimeContext ctx)\n     }\n \n     String commandFile = dockerClient.writeCommandToTempFile(runCommand,\n-        container, nmContext);\n+        containerId, nmContext);\n     PrivilegedOperation launchOp = buildLaunchOp(ctx,\n         commandFile, runCommand);\n \n@@ -927,8 +929,8 @@ public void launchContainer(ContainerRuntimeContext ctx)\n   @Override\n   public void relaunchContainer(ContainerRuntimeContext ctx)\n       throws ContainerExecutionException {\n-    Container container = ctx.getContainer();\n-    String containerIdStr = container.getContainerId().toString();\n+    ContainerId containerId = ctx.getContainer().getContainerId();\n+    String containerIdStr = containerId.toString();\n     // Check to see if the container already exists for relaunch\n     DockerCommandExecutor.DockerContainerStatus containerStatus =\n         DockerCommandExecutor.getContainerStatus(containerIdStr, conf,\n@@ -937,7 +939,7 @@ public void relaunchContainer(ContainerRuntimeContext ctx)\n         DockerCommandExecutor.isStartable(containerStatus)) {\n       DockerStartCommand startCommand = new DockerStartCommand(containerIdStr);\n       String commandFile = dockerClient.writeCommandToTempFile(startCommand,\n-          container, nmContext);\n+          containerId, nmContext);\n       PrivilegedOperation launchOp = buildLaunchOp(ctx, commandFile,\n           startCommand);\n \n@@ -1042,12 +1044,13 @@ public void reapContainer(ContainerRuntimeContext ctx)\n   // ipAndHost[1] contains the hostname.\n   @Override\n   public String[] getIpAndHost(Container container) {\n-    String containerId = container.getContainerId().toString();\n+    ContainerId containerId = container.getContainerId();\n+    String containerIdStr = containerId.toString();\n     DockerInspectCommand inspectCommand =\n-        new DockerInspectCommand(containerId).getIpAndHost();\n+        new DockerInspectCommand(containerIdStr).getIpAndHost();\n     try {\n       String commandFile = dockerClient.writeCommandToTempFile(inspectCommand,\n-          container, nmContext);\n+          containerId, nmContext);\n       PrivilegedOperation privOp = new PrivilegedOperation(\n           PrivilegedOperation.OperationType.RUN_DOCKER_CMD);\n       privOp.appendArgs(commandFile);",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DockerLinuxContainerRuntime.java",
                "sha": "ec1d055668de701b39fcae1ff8209edce3382c19",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerClient.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerClient.java?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerClient.java",
                "patch": "@@ -28,7 +28,6 @@\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.server.nodemanager.Context;\n-import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.runtime.ContainerExecutionException;\n import org.slf4j.Logger;\n@@ -104,9 +103,9 @@ public String writeCommandToTempFile(DockerCommand cmd, String filePrefix)\n     }\n   }\n \n-  public String writeCommandToTempFile(DockerCommand cmd, Container container,\n-      Context nmContext) throws ContainerExecutionException {\n-    ContainerId containerId = container.getContainerId();\n+  public String writeCommandToTempFile(DockerCommand cmd,\n+      ContainerId containerId, Context nmContext)\n+      throws ContainerExecutionException {\n     String filePrefix = containerId.toString();\n     ApplicationId appId = containerId.getApplicationAttemptId()\n         .getApplicationId();",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerClient.java",
                "sha": "7bd4546fc8d1f4bf39e3fc5e5cb8118de0beaf50",
                "status": "modified"
            },
            {
                "additions": 32,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerCommand.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerCommand.java?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerCommand.java",
                "patch": "@@ -22,7 +22,12 @@\n \n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n+import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.util.StringUtils;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.server.nodemanager.Context;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.runtime.ContainerExecutionException;\n \n import java.util.ArrayList;\n import java.util.Collections;\n@@ -104,4 +109,31 @@ public void setClientConfigDir(String clientConfigDir) {\n       addCommandArguments(\"docker-config\", clientConfigDir);\n     }\n   }\n+\n+  /**\n+   * Prepare the privileged operation object that will be used to invoke\n+   * the container-executor.\n+   *\n+   * @param dockerCommand Specific command to be run by docker.\n+   * @param containerName\n+   * @param env\n+   * @param conf\n+   * @param nmContext\n+   * @return Returns the PrivilegedOperation object to be used.\n+   * @throws ContainerExecutionException\n+   */\n+  public PrivilegedOperation preparePrivilegedOperation(\n+      DockerCommand dockerCommand, String containerName, Map<String,\n+      String> env, Configuration conf, Context nmContext)\n+      throws ContainerExecutionException {\n+    DockerClient dockerClient = new DockerClient(conf);\n+    String commandFile =\n+        dockerClient.writeCommandToTempFile(dockerCommand,\n+        ContainerId.fromString(containerName),\n+        nmContext);\n+    PrivilegedOperation dockerOp = new PrivilegedOperation(\n+        PrivilegedOperation.OperationType.RUN_DOCKER_CMD);\n+    dockerOp.appendArgs(commandFile);\n+    return dockerOp;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerCommand.java",
                "sha": "366457d11e4376e6be5166a9502046d99163efb2",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerCommandExecutor.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerCommandExecutor.java?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 9,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerCommandExecutor.java",
                "patch": "@@ -17,7 +17,6 @@\n package org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker;\n \n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.server.nodemanager.Context;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException;\n@@ -80,14 +79,9 @@ public static String executeDockerCommand(DockerCommand dockerCommand,\n       PrivilegedOperationExecutor privilegedOperationExecutor,\n       boolean disableFailureLogging, Context nmContext)\n       throws ContainerExecutionException {\n-    DockerClient dockerClient = new DockerClient(conf);\n-    String commandFile =\n-        dockerClient.writeCommandToTempFile(dockerCommand,\n-        nmContext.getContainers().get(ContainerId.fromString(containerId)),\n-        nmContext);\n-    PrivilegedOperation dockerOp = new PrivilegedOperation(\n-        PrivilegedOperation.OperationType.RUN_DOCKER_CMD);\n-    dockerOp.appendArgs(commandFile);\n+    PrivilegedOperation dockerOp = dockerCommand.preparePrivilegedOperation(\n+        dockerCommand, containerId, env, conf, nmContext);\n+\n     if (disableFailureLogging) {\n       dockerOp.disableFailureLogging();\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerCommandExecutor.java",
                "sha": "8a4888cb2cd72a503f8e8877bbe2b1674cece5aa",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerInspectCommand.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerInspectCommand.java?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerInspectCommand.java",
                "patch": "@@ -20,12 +20,19 @@\n \n package org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker;\n \n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.server.nodemanager.Context;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation;\n+\n+import java.util.Map;\n+\n /**\n  * Encapsulates the docker inspect command and its command\n  * line arguments.\n  */\n public class DockerInspectCommand extends DockerCommand {\n   private static final String INSPECT_COMMAND = \"inspect\";\n+  private String commandArguments;\n \n   public DockerInspectCommand(String containerName) {\n     super(INSPECT_COMMAND);\n@@ -34,6 +41,7 @@ public DockerInspectCommand(String containerName) {\n \n   public DockerInspectCommand getContainerStatus() {\n     super.addCommandArguments(\"format\", \"{{.State.Status}}\");\n+    this.commandArguments = \"--format={{.State.Status}}\";\n     return this;\n   }\n \n@@ -43,6 +51,17 @@ public DockerInspectCommand getIpAndHost() {\n     // cannot parse the arguments correctly.\n     super.addCommandArguments(\"format\", \"{{range(.NetworkSettings.Networks)}}\"\n         + \"{{.IPAddress}},{{end}}{{.Config.Hostname}}\");\n+    this.commandArguments = \"--format={{range(.NetworkSettings.Networks)}}\"\n+        + \"{{.IPAddress}},{{end}}{{.Config.Hostname}}\";\n     return this;\n   }\n+  @Override\n+  public PrivilegedOperation preparePrivilegedOperation(\n+      DockerCommand dockerCommand, String containerName, Map<String,\n+      String> env, Configuration conf, Context nmContext) {\n+    PrivilegedOperation dockerOp = new PrivilegedOperation(\n+        PrivilegedOperation.OperationType.INSPECT_DOCKER_CONTAINER);\n+    dockerOp.appendArgs(commandArguments, containerName);\n+    return dockerOp;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerInspectCommand.java",
                "sha": "3ed9c185243aac9f06ace74d423de29f84f6ec90",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerRmCommand.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerRmCommand.java?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerRmCommand.java",
                "patch": "@@ -16,6 +16,12 @@\n  */\n package org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.docker;\n \n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.server.nodemanager.Context;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation;\n+\n+import java.util.Map;\n+\n /**\n  * Encapsulates the docker rm command and its command\n  * line arguments.\n@@ -27,4 +33,14 @@ public DockerRmCommand(String containerName) {\n     super(RM_COMMAND);\n     super.addCommandArguments(\"name\", containerName);\n   }\n+\n+  @Override\n+  public PrivilegedOperation preparePrivilegedOperation(\n+      DockerCommand dockerCommand, String containerName, Map<String,\n+      String> env, Configuration conf, Context nmContext) {\n+    PrivilegedOperation dockerOp = new PrivilegedOperation(\n+        PrivilegedOperation.OperationType.REMOVE_DOCKER_CONTAINER);\n+    dockerOp.appendArgs(containerName);\n+    return dockerOp;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/DockerRmCommand.java",
                "sha": "3a02982d0d02b6192cad121d6a0dded84e90726f",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c",
                "patch": "@@ -1332,6 +1332,34 @@ int run_docker(const char *command_file) {\n   return exit_code;\n }\n \n+int exec_docker_command(char *docker_command, char **argv,\n+    int argc, int optind) {\n+  int i;\n+  char* docker_binary = get_docker_binary(&CFG);\n+  size_t command_size = argc - optind + 2;\n+\n+  char **args = alloc_and_clear_memory(command_size + 1, sizeof(char));\n+  args[0] = docker_binary;\n+  args[1] = docker_command;\n+  for(i = 2; i < command_size; i++) {\n+    args[i] = (char *) argv[i];\n+  }\n+  args[i] = NULL;\n+\n+  execvp(docker_binary, args);\n+\n+  // will only get here if execvp fails\n+  fprintf(ERRORFILE, \"Couldn't execute the container launch with args %s - %s\\n\",\n+      docker_binary, strerror(errno));\n+  fflush(LOGFILE);\n+  fflush(ERRORFILE);\n+\n+  free(docker_binary);\n+  free(args);\n+\n+  return DOCKER_RUN_FAILED;\n+}\n+\n int create_script_paths(const char *work_dir,\n   const char *script_name, const char *cred_file,\n   char** script_file_dest, char** cred_file_dest,",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c",
                "sha": "6b4ec0c8c02fa7e40b29a1716fd69b46a09da655",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h",
                "patch": "@@ -47,7 +47,9 @@ enum operations {\n   RUN_AS_USER_DELETE = 9,\n   RUN_AS_USER_LAUNCH_DOCKER_CONTAINER = 10,\n   RUN_DOCKER = 11,\n-  RUN_AS_USER_LIST = 12\n+  RUN_AS_USER_LIST = 12,\n+  REMOVE_DOCKER_CONTAINER = 13,\n+  INSPECT_DOCKER_CONTAINER = 14\n };\n \n #define NM_GROUP_KEY \"yarn.nodemanager.linux-container-executor.group\"\n@@ -263,6 +265,12 @@ int is_docker_support_enabled();\n  */\n int run_docker(const char *command_file);\n \n+/**\n+ * Run a docker command without a command file\n+ */\n+int exec_docker_command(char *docker_command, char **argv,\n+    int argc, int optind);\n+\n /*\n  * Compile the regex_str and determine if the input string matches.\n  * Return 0 on match, 1 of non-match.",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h",
                "sha": "47c422191e2787a43ae9c2f9c3f40cdfdbf98e8a",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c",
                "changes": 47,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c",
                "patch": "@@ -36,7 +36,7 @@ static void display_usage(FILE *stream) {\n   fprintf(stream,\n     \"Usage: container-executor --checksetup\\n\"\n     \"       container-executor --mount-cgroups <hierarchy> \"\n-    \"<controller=path>...\\n\" );\n+    \"<controller=path>\\n\" );\n \n   if(is_tc_support_enabled()) {\n     fprintf(stream,\n@@ -52,10 +52,15 @@ static void display_usage(FILE *stream) {\n \n   if(is_docker_support_enabled()) {\n     fprintf(stream,\n-      \"       container-executor --run-docker <command-file>\\n\");\n+      \"       container-executor --run-docker <command-file>\\n\"\n+      \"       container-executor --remove-docker-container <container_id>\\n\"\n+      \"       container-executor --inspect-docker-container <container_id>\\n\");\n   } else {\n     fprintf(stream,\n-      \"[DISABLED] container-executor --run-docker <command-file>\\n\");\n+      \"[DISABLED] container-executor --run-docker <command-file>\\n\"\n+      \"[DISABLED] container-executor --remove-docker-container <container_id>\\n\"\n+      \"[DISABLED] container-executor --inspect-docker-container \"\n+      \"<format> ... <container_id>\\n\");\n   }\n \n   fprintf(stream,\n@@ -331,6 +336,36 @@ static int validate_arguments(int argc, char **argv , int *operation) {\n     }\n   }\n \n+  if (strcmp(\"--remove-docker-container\", argv[1]) == 0) {\n+    if(is_docker_support_enabled()) {\n+      if (argc != 3) {\n+        display_usage(stdout);\n+        return INVALID_ARGUMENT_NUMBER;\n+      }\n+      optind++;\n+      *operation = REMOVE_DOCKER_CONTAINER;\n+      return 0;\n+    } else {\n+        display_feature_disabled_message(\"docker\");\n+        return FEATURE_DISABLED;\n+    }\n+  }\n+\n+  if (strcmp(\"--inspect-docker-container\", argv[1]) == 0) {\n+    if(is_docker_support_enabled()) {\n+      if (argc != 4) {\n+        display_usage(stdout);\n+        return INVALID_ARGUMENT_NUMBER;\n+      }\n+      optind++;\n+      *operation = INSPECT_DOCKER_CONTAINER;\n+      return 0;\n+    } else {\n+        display_feature_disabled_message(\"docker\");\n+        return FEATURE_DISABLED;\n+    }\n+  }\n+\n   /* Now we have to validate 'run as user' operations that don't use\n     a 'long option' - we should fix this at some point. The validation/argument\n     parsing here is extensive enough that it done in a separate function */\n@@ -561,6 +596,12 @@ int main(int argc, char **argv) {\n   case RUN_DOCKER:\n     exit_code = run_docker(cmd_input.docker_command_file);\n     break;\n+  case REMOVE_DOCKER_CONTAINER:\n+    exit_code = exec_docker_command(\"rm\", argv, argc, optind);\n+    break;\n+  case INSPECT_DOCKER_CONTAINER:\n+    exit_code = exec_docker_command(\"inspect\", argv, argc, optind);\n+    break;\n   case RUN_AS_USER_INITIALIZE_CONTAINER:\n     exit_code = set_user(cmd_input.run_as_user_name);\n     if (exit_code != 0) {",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c",
                "sha": "c54fd3ea900f5e7a397c948c2bb90b08eee5b49d",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/TestDockerCommandExecutor.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/TestDockerCommandExecutor.java?ref=883f68222a9cfd06f79a8fcd75ec9fef00abc035",
                "deletions": 16,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/TestDockerCommandExecutor.java",
                "patch": "@@ -153,14 +153,14 @@ public void testExecuteDockerRm() throws Exception {\n         env, configuration, mockExecutor, false, nmContext);\n     List<PrivilegedOperation> ops = MockPrivilegedOperationCaptor\n         .capturePrivilegedOperations(mockExecutor, 1, true);\n-    List<String> dockerCommands = getValidatedDockerCommands(ops);\n+    PrivilegedOperation privOp = ops.get(0);\n+    List<String> args = privOp.getArguments();\n     assertEquals(1, ops.size());\n-    assertEquals(PrivilegedOperation.OperationType.RUN_DOCKER_CMD.name(),\n-        ops.get(0).getOperationType().name());\n-    assertEquals(3, dockerCommands.size());\n-    assertEquals(\"[docker-command-execution]\", dockerCommands.get(0));\n-    assertEquals(\"  docker-command=rm\", dockerCommands.get(1));\n-    assertEquals(\"  name=\" + MOCK_CONTAINER_ID, dockerCommands.get(2));\n+    assertEquals(PrivilegedOperation.OperationType.\n+        REMOVE_DOCKER_CONTAINER.name(),\n+        privOp.getOperationType().name());\n+    assertEquals(1, args.size());\n+    assertEquals(MOCK_CONTAINER_ID, args.get(0));\n   }\n \n   @Test\n@@ -188,16 +188,15 @@ public void testExecuteDockerInspectStatus() throws Exception {\n         env, configuration, mockExecutor, false, nmContext);\n     List<PrivilegedOperation> ops = MockPrivilegedOperationCaptor\n         .capturePrivilegedOperations(mockExecutor, 1, true);\n-    List<String> dockerCommands = getValidatedDockerCommands(ops);\n+    PrivilegedOperation privOp = ops.get(0);\n+    List<String> args = privOp.getArguments();\n     assertEquals(1, ops.size());\n-    assertEquals(PrivilegedOperation.OperationType.RUN_DOCKER_CMD.name(),\n-        ops.get(0).getOperationType().name());\n-    assertEquals(4, dockerCommands.size());\n-    assertEquals(\"[docker-command-execution]\", dockerCommands.get(0));\n-    assertEquals(\"  docker-command=inspect\", dockerCommands.get(1));\n-    assertEquals(\"  format={{.State.Status}}\", dockerCommands.get(2));\n-    assertEquals(\"  name=\" + MOCK_CONTAINER_ID, dockerCommands.get(3));\n-\n+    assertEquals(PrivilegedOperation.OperationType.\n+        INSPECT_DOCKER_CONTAINER.name(),\n+        privOp.getOperationType().name());\n+    assertEquals(2, args.size());\n+    assertEquals(\"--format={{.State.Status}}\", args.get(0));\n+    assertEquals(MOCK_CONTAINER_ID, args.get(1));\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/883f68222a9cfd06f79a8fcd75ec9fef00abc035/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/docker/TestDockerCommandExecutor.java",
                "sha": "50d00bb518c107ca3ff21148e8059313f87141cb",
                "status": "modified"
            }
        ],
        "message": "YARN-8209.  Fixed NPE in Yarn Service deletion.\n            Contributed by Eric Badger",
        "parent": "https://github.com/apache/hadoop/commit/19ae588fde9930c042cdb2848b8a1a0ff514b575",
        "repo": "hadoop",
        "unit_tests": [
            "TestDockerClient.java",
            "TestDockerCommandExecutor.java",
            "TestDockerInspectCommand.java",
            "TestDockerRmCommand.java"
        ]
    },
    "hadoop_8854c21": {
        "bug_id": "hadoop_8854c21",
        "commit": "https://github.com/apache/hadoop/commit/8854c210bdf8aba763b2a0f0327729f315a066d5",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/8854c210bdf8aba763b2a0f0327729f315a066d5/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=8854c210bdf8aba763b2a0f0327729f315a066d5",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -3114,6 +3114,9 @@ Release 0.23.9 - UNRELEASED\n \n   BUG FIXES\n \n+    HDFS-4867. metaSave NPEs when there are invalid blocks in repl queue.\n+    (Plamen Jeliazkov and Ravi Prakash via shv)\n+\n Release 0.23.8 - 2013-06-05\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/8854c210bdf8aba763b2a0f0327729f315a066d5/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "9bc76f3d5078b72e219dfc0f9dcc468276a052a6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/8854c210bdf8aba763b2a0f0327729f315a066d5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=8854c210bdf8aba763b2a0f0327729f315a066d5",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -458,7 +458,8 @@ private void dumpBlockMeta(Block block, PrintWriter out) {\n                          numReplicas.decommissionedReplicas();\n     \n     if (block instanceof BlockInfo) {\n-      String fileName = ((BlockInfo)block).getBlockCollection().getName();\n+      BlockCollection bc = ((BlockInfo) block).getBlockCollection();\n+      String fileName = (bc == null) ? \"[orphaned]\" : bc.getName();\n       out.print(fileName + \": \");\n     }\n     // l: == live:, d: == decommissioned c: == corrupt e: == excess",
                "raw_url": "https://github.com/apache/hadoop/raw/8854c210bdf8aba763b2a0f0327729f315a066d5/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "966c59656742edeb7431b031687a716c42c8f8b7",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/8854c210bdf8aba763b2a0f0327729f315a066d5/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java?ref=8854c210bdf8aba763b2a0f0327729f315a066d5",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java",
                "patch": "@@ -24,11 +24,8 @@\n import java.io.FileInputStream;\n import java.io.IOException;\n import java.io.InputStreamReader;\n-import java.util.Random;\n \n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.CommonConfigurationKeys;\n-import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hdfs.DFSConfigKeys;",
                "raw_url": "https://github.com/apache/hadoop/raw/8854c210bdf8aba763b2a0f0327729f315a066d5/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestMetaSave.java",
                "sha": "c0775a62504d43d5526e674c082c2017edb8f8b0",
                "status": "modified"
            }
        ],
        "message": "HDFS-4867. metaSave NPEs when there are invalid blocks in repl queue. Contributed by Plamen Jeliazkov and Ravi Prakash.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1490433 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/d46e1608626c64400d2b6c7693a4c035783c55b4",
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockManager.java"
        ]
    },
    "hadoop_8a6e354": {
        "bug_id": "hadoop_8a6e354",
        "commit": "https://github.com/apache/hadoop/commit/8a6e3541226fb1b6798cedecc56f1f160012becf",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/8a6e3541226fb1b6798cedecc56f1f160012becf/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java?ref=8a6e3541226fb1b6798cedecc56f1f160012becf",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java",
                "patch": "@@ -104,6 +104,7 @@ public DiskBalancer(String dataNodeUUID,\n     scheduler = Executors.newSingleThreadExecutor();\n     lock = new ReentrantLock();\n     workMap = new ConcurrentHashMap<>();\n+    this.planID = \"\";  // to keep protobuf happy.\n     this.isDiskBalancerEnabled = conf.getBoolean(\n         DFSConfigKeys.DFS_DISK_BALANCER_ENABLED,\n         DFSConfigKeys.DFS_DISK_BALANCER_ENABLED_DEFAULT);\n@@ -223,7 +224,9 @@ public void cancelPlan(String planID) throws DiskBalancerException {\n     lock.lock();\n     try {\n       checkDiskBalancerEnabled();\n-      if ((this.planID == null) || (!this.planID.equals(planID))) {\n+      if (this.planID == null ||\n+          !this.planID.equals(planID) ||\n+          this.planID.isEmpty()) {\n         LOG.error(\"Disk Balancer - No such plan. Cancel plan failed. PlanID: \" +\n             planID);\n         throw new DiskBalancerException(\"No such plan.\",",
                "raw_url": "https://github.com/apache/hadoop/raw/8a6e3541226fb1b6798cedecc56f1f160012becf/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DiskBalancer.java",
                "sha": "5a1fb9ec40877bec539bf1ae2f2260bc3db03e73",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/8a6e3541226fb1b6798cedecc56f1f160012becf/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/QueryCommand.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/QueryCommand.java?ref=8a6e3541226fb1b6798cedecc56f1f160012becf",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/QueryCommand.java",
                "patch": "@@ -63,7 +63,7 @@ public void execute(CommandLine cmd) throws Exception {\n     String nodeAddress = nodeName;\n \n     // if the string is not name:port format use the default port.\n-    if (!nodeName.matches(\"^.*:\\\\d$\")) {\n+    if (!nodeName.matches(\"[^\\\\:]+:[0-9]{2,5}\")) {\n       int defaultIPC = NetUtils.createSocketAddr(\n           getConf().getTrimmed(DFSConfigKeys.DFS_DATANODE_IPC_ADDRESS_KEY,\n               DFSConfigKeys.DFS_DATANODE_IPC_ADDRESS_DEFAULT)).getPort();",
                "raw_url": "https://github.com/apache/hadoop/raw/8a6e3541226fb1b6798cedecc56f1f160012becf/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/diskbalancer/command/QueryCommand.java",
                "sha": "fac1e51bfa9886ea8d4dbc1f94a941c82cd6be23",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hadoop/blob/8a6e3541226fb1b6798cedecc56f1f160012becf/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/command/TestDiskBalancerCommand.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/command/TestDiskBalancerCommand.java?ref=8a6e3541226fb1b6798cedecc56f1f160012becf",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/command/TestDiskBalancerCommand.java",
                "patch": "@@ -35,6 +35,7 @@\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.HdfsConfiguration;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.apache.hadoop.hdfs.server.datanode.DataNode;\n import org.apache.hadoop.hdfs.server.diskbalancer.connectors.ClusterConnector;\n import org.apache.hadoop.hdfs.server.diskbalancer.connectors.ConnectorFactory;\n import org.apache.hadoop.hdfs.server.diskbalancer.datamodel.DiskBalancerCluster;\n@@ -351,4 +352,28 @@ public void testHelpCommand() throws Exception {\n     }\n     return outputs;\n   }\n+\n+  /**\n+   * Making sure that we can query the node without having done a submit.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDiskBalancerQueryWithoutSubmit() throws Exception {\n+    Configuration conf = new HdfsConfiguration();\n+    conf.setBoolean(DFSConfigKeys.DFS_DISK_BALANCER_ENABLED, true);\n+    final int numDatanodes = 2;\n+    MiniDFSCluster miniDFSCluster = new MiniDFSCluster.Builder(conf)\n+        .numDataNodes(numDatanodes).build();\n+    try {\n+      miniDFSCluster.waitActive();\n+      DataNode dataNode = miniDFSCluster.getDataNodes().get(0);\n+      final String queryArg = String.format(\"-query localhost:%d\", dataNode\n+          .getIpcPort());\n+      final String cmdLine = String.format(\"hdfs diskbalancer %s\",\n+          queryArg);\n+      runCommand(cmdLine);\n+    } finally {\n+      miniDFSCluster.shutdown();\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/8a6e3541226fb1b6798cedecc56f1f160012becf/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/diskbalancer/command/TestDiskBalancerCommand.java",
                "sha": "b0821e2e8d22bc3d26deeb7e045bd9be42155cab",
                "status": "modified"
            }
        ],
        "message": "HDFS-10552. DiskBalancer \"-query\" results in NPE if no plan for the node. Contributed by Anu Engineer.",
        "parent": "https://github.com/apache/hadoop/commit/e8de28181a3ed0053d5cd5f196434739880ee978",
        "repo": "hadoop",
        "unit_tests": [
            "TestDiskBalancer.java"
        ]
    },
    "hadoop_8c7f6b2": {
        "bug_id": "hadoop_8c7f6b2",
        "commit": "https://github.com/apache/hadoop/commit/8c7f6b2d4df2e5ca7b766db68213b778d28f198b",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/8c7f6b2d4df2e5ca7b766db68213b778d28f198b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java?ref=8c7f6b2d4df2e5ca7b766db68213b778d28f198b",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "patch": "@@ -350,8 +350,10 @@ static void handleComponentInstanceRelaunch(ComponentInstance compInstance,\n         // record in ATS\n         LOG.info(\"Publishing component instance status {} {} \",\n             event.getContainerId(), containerState);\n+        int exitStatus = failureBeforeLaunch || event.getStatus() == null ?\n+            ContainerExitStatus.INVALID : event.getStatus().getExitStatus();\n         compInstance.serviceTimelinePublisher.componentInstanceFinished(\n-            event.getContainerId(), event.getStatus().getExitStatus(),\n+            event.getContainerId(), exitStatus,\n             containerState, containerDiag);\n       }\n \n@@ -366,8 +368,10 @@ static void handleComponentInstanceRelaunch(ComponentInstance compInstance,\n \n       if (compInstance.timelineServiceEnabled) {\n         // record in ATS\n+        int exitStatus = failureBeforeLaunch || event.getStatus() == null ?\n+            ContainerExitStatus.INVALID : event.getStatus().getExitStatus();\n         compInstance.serviceTimelinePublisher.componentInstanceFinished(\n-            event.getContainerId(), event.getStatus().getExitStatus(),\n+            event.getContainerId(), exitStatus,\n             containerState, containerDiag);\n       }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/8c7f6b2d4df2e5ca7b766db68213b778d28f198b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "sha": "66c298d337ca9a8da98370edb2de3c878a1455db",
                "status": "modified"
            }
        ],
        "message": "YARN-9197.  Add safe guard against NPE for component instance failure.\n            Contributed by kyungwan nam",
        "parent": "https://github.com/apache/hadoop/commit/dacc1a759e3ba3eca000cbacc6145b231253b174",
        "repo": "hadoop",
        "unit_tests": [
            "TestComponentInstance.java"
        ]
    },
    "hadoop_8ed0d4b": {
        "bug_id": "hadoop_8ed0d4b",
        "commit": "https://github.com/apache/hadoop/commit/8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -896,6 +896,9 @@ Release 2.8.0 - UNRELEASED\n \n     YARN-4171. Fix findbugs warnings in YARN-1197 branch. (Wangda Tan via jianhe)\n \n+    YARN-4152. NodeManager crash with NPE when LogAggregationService#stopContainer called for \n+    absent container. (Bibin A Chundatt via rohithsharmaks)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6/hadoop-yarn-project/CHANGES.txt",
                "sha": "0a0a65c69b537072a5f4dd06113f116ae00333ad",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java?ref=8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java",
                "patch": "@@ -56,6 +56,7 @@\n import org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEvent;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationEventType;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.LogHandler;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerAppFinishedEvent;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.loghandler.event.LogHandlerAppStartedEvent;\n@@ -423,8 +424,14 @@ private void stopContainer(ContainerId containerId, int exitCode) {\n           + \", did it fail to start?\");\n       return;\n     }\n-    ContainerType containerType = context.getContainers().get(\n-        containerId).getContainerTokenIdentifier().getContainerType();\n+    Container container = context.getContainers().get(containerId);\n+    if (null == container) {\n+      LOG.warn(\"Log aggregation cannot be started for \" + containerId\n+          + \", as its an absent container\");\n+      return;\n+    }\n+    ContainerType containerType =\n+        container.getContainerTokenIdentifier().getContainerType();\n     aggregator.startContainerLogAggregation(\n         new ContainerLogContext(containerId, containerType, exitCode));\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/LogAggregationService.java",
                "sha": "f64685da543a93bc2464f533cdc678e5257f7bcd",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java?ref=8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java",
                "patch": "@@ -1509,6 +1509,25 @@ public void testFailedOrKilledContainerPolicy() throws Exception {\n     verifyLogAggFinishEvent(appId);\n   }\n \n+  @Test(timeout = 50000)\n+  public void testLogAggregationAbsentContainer() throws Exception {\n+    ApplicationId appId = createApplication();\n+    LogAggregationService logAggregationService =\n+        createLogAggregationService(appId,\n+            FailedOrKilledContainerLogAggregationPolicy.class, null);\n+    ApplicationAttemptId appAttemptId1 =\n+        BuilderUtils.newApplicationAttemptId(appId, 1);\n+    ContainerId containerId = BuilderUtils.newContainerId(appAttemptId1, 2l);\n+    try {\n+      logAggregationService.handle(new LogHandlerContainerFinishedEvent(\n+          containerId, 100));\n+      assertTrue(\"Should skip when null containerID\", true);\n+    } catch (Exception e) {\n+      Assert.assertFalse(\"Exception not expected should skip null containerid\",\n+          true);\n+    }\n+  }\n+\n   @Test (timeout = 50000)\n   @SuppressWarnings(\"unchecked\")\n   public void testAMOnlyContainerPolicy() throws Exception {",
                "raw_url": "https://github.com/apache/hadoop/raw/8ed0d4b744e5321c9f0f7f19a6c9737bb2da2ef6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java",
                "sha": "0b33634450bcf952c878009ba7ec2329947c3bb8",
                "status": "modified"
            }
        ],
        "message": "YARN-4152. NodeManager crash with NPE when LogAggregationService#stopContainer called for absent container. (Bibin A Chundatt via rohithsharmaks)",
        "parent": "https://github.com/apache/hadoop/commit/06d1c9033effcd2b1ea54e87229d5478d85732ca",
        "repo": "hadoop",
        "unit_tests": [
            "TestLogAggregationService.java"
        ]
    },
    "hadoop_8f9661d": {
        "bug_id": "hadoop_8f9661d",
        "commit": "https://github.com/apache/hadoop/commit/8f9661da4823bfbb243e430252ec1bb5780ecbfc",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=8f9661da4823bfbb243e430252ec1bb5780ecbfc",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -509,6 +509,10 @@ Release 0.23.0 - Unreleased\n     HADOOP-7360. Preserve relative paths that do not contain globs in FsShell.\n     (Daryn Sharp and Kihwal Lee via szetszwo)\n \n+    HADOOP-7771. FsShell -copyToLocal, -get, etc. commands throw NPE if the\n+    destination directory does not exist.  (John George and Daryn Sharp\n+    via szetszwo)\n+\n   OPTIMIZATIONS\n   \n     HADOOP-7333. Performance improvement in PureJavaCrc32. (Eric Caspole",
                "raw_url": "https://github.com/apache/hadoop/raw/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "0b0e1beb76248ba3b27554efd8cb2bd9b239c27d",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java?ref=8f9661da4823bfbb243e430252ec1bb5780ecbfc",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java",
                "patch": "@@ -55,6 +55,7 @@\n   protected int exitCode = 0;\n   protected int numErrors = 0;\n   protected boolean recursive = false;\n+  private int depth = 0;\n   protected ArrayList<Exception> exceptions = new ArrayList<Exception>();\n \n   private static final Log LOG = LogFactory.getLog(Command.class);\n@@ -86,6 +87,10 @@ protected boolean isRecursive() {\n     return recursive;\n   }\n \n+  protected int getDepth() {\n+    return depth;\n+  }\n+  \n   /** \n    * Execute the command on the input path\n    * \n@@ -269,6 +274,7 @@ protected void processArgument(PathData item) throws IOException {\n   protected void processPathArgument(PathData item) throws IOException {\n     // null indicates that the call is not via recursion, ie. there is\n     // no parent directory that was expanded\n+    depth = 0;\n     processPaths(null, item);\n   }\n   \n@@ -326,7 +332,12 @@ protected void processPath(PathData item) throws IOException {\n    *  @throws IOException if anything goes wrong...\n    */\n   protected void recursePath(PathData item) throws IOException {\n-    processPaths(item, item.getDirectoryContents());\n+    try {\n+      depth++;\n+      processPaths(item, item.getDirectoryContents());\n+    } finally {\n+      depth--;\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java",
                "sha": "b24d47e02b1027b5decd17b36d9ab8a71b589875",
                "status": "modified"
            },
            {
                "additions": 116,
                "blob_url": "https://github.com/apache/hadoop/blob/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java",
                "changes": 142,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java?ref=8f9661da4823bfbb243e430252ec1bb5780ecbfc",
                "deletions": 26,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java",
                "patch": "@@ -20,13 +20,18 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.io.InputStream;\n import java.util.LinkedList;\n \n+import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.shell.PathExceptions.PathExistsException;\n import org.apache.hadoop.fs.shell.PathExceptions.PathIOException;\n+import org.apache.hadoop.fs.shell.PathExceptions.PathIsDirectoryException;\n import org.apache.hadoop.fs.shell.PathExceptions.PathIsNotDirectoryException;\n import org.apache.hadoop.fs.shell.PathExceptions.PathNotFoundException;\n+import org.apache.hadoop.fs.shell.PathExceptions.PathOperationException;\n+import org.apache.hadoop.io.IOUtils;\n \n /**\n  * Provides: argument processing to ensure the destination is valid\n@@ -106,51 +111,136 @@ protected void processArguments(LinkedList<PathData> args)\n   }\n \n   @Override\n-  protected void processPaths(PathData parent, PathData ... items)\n+  protected void processPathArgument(PathData src)\n   throws IOException {\n+    if (src.stat.isDirectory() && src.fs.equals(dst.fs)) {\n+      PathData target = getTargetPath(src);\n+      String srcPath = src.fs.makeQualified(src.path).toString();\n+      String dstPath = dst.fs.makeQualified(target.path).toString();\n+      if (dstPath.equals(srcPath)) {\n+        PathIOException e = new PathIOException(src.toString(),\n+            \"are identical\");\n+        e.setTargetPath(dstPath.toString());\n+        throw e;\n+      }\n+      if (dstPath.startsWith(srcPath+Path.SEPARATOR)) {\n+        PathIOException e = new PathIOException(src.toString(),\n+            \"is a subdirectory of itself\");\n+        e.setTargetPath(target.toString());\n+        throw e;\n+      }\n+    }\n+    super.processPathArgument(src);\n+  }\n+\n+  @Override\n+  protected void processPath(PathData src) throws IOException {\n+    processPath(src, getTargetPath(src));\n+  }\n+  \n+  /**\n+   * Called with a source and target destination pair\n+   * @param src for the operation\n+   * @param target for the operation\n+   * @throws IOException if anything goes wrong\n+   */\n+  protected void processPath(PathData src, PathData dst) throws IOException {\n+    if (src.stat.isSymlink()) {\n+      // TODO: remove when FileContext is supported, this needs to either\n+      // copy the symlink or deref the symlink\n+      throw new PathOperationException(src.toString());        \n+    } else if (src.stat.isFile()) {\n+      copyFileToTarget(src, dst);\n+    } else if (src.stat.isDirectory() && !isRecursive()) {\n+      throw new PathIsDirectoryException(src.toString());\n+    }\n+  }\n+\n+  @Override\n+  protected void recursePath(PathData src) throws IOException {\n     PathData savedDst = dst;\n     try {\n       // modify dst as we descend to append the basename of the\n       // current directory being processed\n-      if (parent != null) dst = dst.getPathDataForChild(parent);\n-      super.processPaths(parent, items);\n+      dst = getTargetPath(src);\n+      if (dst.exists) {\n+        if (!dst.stat.isDirectory()) {\n+          throw new PathIsNotDirectoryException(dst.toString());\n+        }\n+      } else {\n+        if (!dst.fs.mkdirs(dst.path)) {\n+          // too bad we have no clue what failed\n+          PathIOException e = new PathIOException(dst.toString());\n+          e.setOperation(\"mkdir\");\n+          throw e;\n+        }    \n+        dst.refreshStatus(); // need to update stat to know it exists now\n+      }      \n+      super.recursePath(src);\n     } finally {\n       dst = savedDst;\n     }\n   }\n   \n-  @Override\n-  protected void processPath(PathData src) throws IOException {\n+  protected PathData getTargetPath(PathData src) throws IOException {\n     PathData target;\n-    // if the destination is a directory, make target a child path,\n-    // else use the destination as-is\n-    if (dst.exists && dst.stat.isDirectory()) {\n+    // on the first loop, the dst may be directory or a file, so only create\n+    // a child path if dst is a dir; after recursion, it's always a dir\n+    if ((getDepth() > 0) || (dst.exists && dst.stat.isDirectory())) {\n       target = dst.getPathDataForChild(src);\n     } else {\n       target = dst;\n     }\n-    if (target.exists && !overwrite) {\n+    return target;\n+  }\n+  \n+  /**\n+   * Copies the source file to the target.\n+   * @param src item to copy\n+   * @param target where to copy the item\n+   * @throws IOException if copy fails\n+   */ \n+  protected void copyFileToTarget(PathData src, PathData target) throws IOException {\n+    copyStreamToTarget(src.fs.open(src.path), target);\n+  }\n+  \n+  /**\n+   * Copies the stream contents to a temporary file.  If the copy is\n+   * successful, the temporary file will be renamed to the real path,\n+   * else the temporary file will be deleted.\n+   * @param in the input stream for the copy\n+   * @param target where to store the contents of the stream\n+   * @throws IOException if copy fails\n+   */ \n+  protected void copyStreamToTarget(InputStream in, PathData target)\n+  throws IOException {\n+    if (target.exists && (target.stat.isDirectory() || !overwrite)) {\n       throw new PathExistsException(target.toString());\n     }\n-\n-    try { \n-      // invoke processPath with both a source and resolved target\n-      processPath(src, target);\n-    } catch (PathIOException e) {\n-      // add the target unless it already has one\n-      if (e.getTargetPath() == null) {\n+    PathData tempFile = null;\n+    try {\n+      tempFile = target.createTempFile(target+\"._COPYING_\");\n+      FSDataOutputStream out = target.fs.create(tempFile.path, true);\n+      IOUtils.copyBytes(in, out, getConf(), true);\n+      // the rename method with an option to delete the target is deprecated\n+      if (target.exists && !target.fs.delete(target.path, false)) {\n+        // too bad we don't know why it failed\n+        PathIOException e = new PathIOException(target.toString());\n+        e.setOperation(\"delete\");\n+        throw e;\n+      }\n+      if (!tempFile.fs.rename(tempFile.path, target.path)) {\n+        // too bad we don't know why it failed\n+        PathIOException e = new PathIOException(tempFile.toString());\n+        e.setOperation(\"rename\");\n         e.setTargetPath(target.toString());\n+        throw e;\n+      }\n+      tempFile = null;\n+    } finally {\n+      if (tempFile != null) {\n+        tempFile.fs.delete(tempFile.path, false);\n       }\n-      throw e;\n     }\n   }\n-\n-  /**\n-   * Called with a source and target destination pair\n-   * @param src for the operation\n-   * @param target for the operation\n-   * @throws IOException if anything goes wrong\n-   */\n-  protected abstract void processPath(PathData src, PathData target)\n-  throws IOException;\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java",
                "sha": "6b3b40389f9f72528cdc248ee23af4af559df101",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java?ref=8f9661da4823bfbb243e430252ec1bb5780ecbfc",
                "deletions": 73,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java",
                "patch": "@@ -26,13 +26,7 @@\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.fs.ChecksumFileSystem;\n-import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.FileUtil;\n-import org.apache.hadoop.fs.LocalFileSystem;\n-import org.apache.hadoop.fs.shell.PathExceptions.PathExistsException;\n-import org.apache.hadoop.fs.shell.PathExceptions.PathIOException;\n-import org.apache.hadoop.fs.shell.PathExceptions.PathOperationException;\n-import org.apache.hadoop.io.IOUtils;\n \n /** Various commands for copy files */\n @InterfaceAudience.Private\n@@ -95,18 +89,10 @@ protected void processOptions(LinkedList<String> args) throws IOException {\n       CommandFormat cf = new CommandFormat(2, Integer.MAX_VALUE, \"f\");\n       cf.parse(args);\n       setOverwrite(cf.getOpt(\"f\"));\n+      // should have a -r option\n+      setRecursive(true);\n       getRemoteDestination(args);\n     }\n-\n-    @Override\n-    protected void processPath(PathData src, PathData target)\n-    throws IOException {\n-      if (!FileUtil.copy(src.fs, src.path, target.fs, target.path, false, overwrite, getConf())) {\n-        // we have no idea what the error is...  FileUtils masks it and in\n-        // some cases won't even report an error\n-        throw new PathIOException(src.toString());\n-      }\n-    }\n   }\n   \n   /** \n@@ -126,7 +112,6 @@ protected void processPath(PathData src, PathData target)\n      * It must be at least three characters long, required by\n      * {@link java.io.File#createTempFile(String, String, File)}.\n      */\n-    private static final String COPYTOLOCAL_PREFIX = \"_copyToLocal_\";\n     private boolean copyCrc;\n     private boolean verifyChecksum;\n \n@@ -144,7 +129,7 @@ protected void processOptions(LinkedList<String> args)\n     }\n \n     @Override\n-    protected void processPath(PathData src, PathData target)\n+    protected void copyFileToTarget(PathData src, PathData target)\n     throws IOException {\n       src.fs.setVerifyChecksum(verifyChecksum);\n \n@@ -153,41 +138,10 @@ protected void processPath(PathData src, PathData target)\n         copyCrc = false;\n       }      \n \n-      if (src.stat.isFile()) {\n-        // copy the file and maybe its crc\n-        copyFileToLocal(src, target);\n-        if (copyCrc) {\n-          copyFileToLocal(src.getChecksumFile(), target.getChecksumFile());\n-        }\n-      } else if (src.stat.isDirectory()) {\n-        // create the remote directory structure locally\n-        if (!target.toFile().mkdirs()) {\n-          throw new PathIOException(target.toString());\n-        }\n-      } else {\n-        throw new PathOperationException(src.toString());\n-      }\n-    }\n-\n-    private void copyFileToLocal(PathData src, PathData target)\n-    throws IOException {\n-      File targetFile = target.toFile();\n-      File tmpFile = FileUtil.createLocalTempFile(\n-          targetFile, COPYTOLOCAL_PREFIX, true);\n-      // too bad we can't tell exactly why it failed...\n-      if (!FileUtil.copy(src.fs, src.path, tmpFile, false, getConf())) {\n-        PathIOException e = new PathIOException(src.toString());\n-        e.setOperation(\"copy\");\n-        e.setTargetPath(tmpFile.toString());\n-        throw e;\n-      }\n-\n-      // too bad we can't tell exactly why it failed...\n-      if (!tmpFile.renameTo(targetFile)) {\n-        PathIOException e = new PathIOException(tmpFile.toString());\n-        e.setOperation(\"rename\");\n-        e.setTargetPath(targetFile.toString());\n-        throw e;\n+      super.copyFileToTarget(src, target);\n+      if (copyCrc) {\n+        // should we delete real file if crc copy fails?\n+        super.copyFileToTarget(src.getChecksumFile(), target.getChecksumFile());\n       }\n     }\n   }\n@@ -208,6 +162,8 @@ protected void processOptions(LinkedList<String> args) throws IOException {\n       cf.parse(args);\n       setOverwrite(cf.getOpt(\"f\"));\n       getRemoteDestination(args);\n+      // should have a -r option\n+      setRecursive(true);\n     }\n \n     // commands operating on local paths have no need for glob expansion\n@@ -223,30 +179,11 @@ protected void processArguments(LinkedList<PathData> args)\n     throws IOException {\n       // NOTE: this logic should be better, mimics previous implementation\n       if (args.size() == 1 && args.get(0).toString().equals(\"-\")) {\n-        if (dst.exists && !overwrite) {\n-          throw new PathExistsException(dst.toString());\n-        }\n-        copyFromStdin();\n+        copyStreamToTarget(System.in, getTargetPath(args.get(0)));\n         return;\n       }\n       super.processArguments(args);\n     }\n-\n-    @Override\n-    protected void processPath(PathData src, PathData target)\n-    throws IOException {\n-      target.fs.copyFromLocalFile(false, overwrite, src.path, target.path);\n-    }\n-\n-    /** Copies from stdin to the destination file. */\n-    protected void copyFromStdin() throws IOException {\n-      FSDataOutputStream out = dst.fs.create(dst.path); \n-      try {\n-        IOUtils.copyBytes(System.in, out, getConf(), false);\n-      } finally {\n-        out.close();\n-      }\n-    }\n   }\n \n   public static class CopyFromLocal extends Put {",
                "raw_url": "https://github.com/apache/hadoop/raw/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java",
                "sha": "066e5fdb899df7f517fb508451caf3a86f7caf68",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java?ref=8f9661da4823bfbb243e430252ec1bb5780ecbfc",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java",
                "patch": "@@ -182,6 +182,19 @@ public PathData getChecksumFile() throws IOException {\n     return new PathData(srcFs.getRawFileSystem(), srcPath.toString());\n   }\n \n+  /**\n+   * Returns a temporary file for this PathData with the given extension.\n+   * The file will be deleted on exit.\n+   * @param extension for the temporary file\n+   * @return PathData\n+   * @throws IOException shouldn't happen\n+   */\n+  public PathData createTempFile(String extension) throws IOException {\n+    PathData tmpFile = new PathData(fs, uri+\"._COPYING_\");\n+    fs.deleteOnExit(tmpFile.path);\n+    return tmpFile;\n+  }\n+\n   /**\n    * Returns a list of PathData objects of the items contained in the given\n    * directory.",
                "raw_url": "https://github.com/apache/hadoop/raw/8f9661da4823bfbb243e430252ec1bb5780ecbfc/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java",
                "sha": "a3c88f1f2af2736442e7db320919298356c681b4",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7771. FsShell -copyToLocal, -get, etc. commands throw NPE if the destination directory does not exist.  Contributed by John George and Daryn Sharp\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1195760 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/bb8fd6a2670d00d562673f32a55d3d0dd7aaa69c",
        "repo": "hadoop",
        "unit_tests": [
            "TestPathData.java"
        ]
    },
    "hadoop_902c6ea": {
        "bug_id": "hadoop_902c6ea",
        "commit": "https://github.com/apache/hadoop/commit/902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -409,6 +409,9 @@ Release 2.7.0 - UNRELEASED\n     YARN-3082. Non thread safe access to systemCredentials in NodeHeartbeatResponse\n     processing. (Anubhav Dhoot via ozawa)\n \n+    YARN-3088. LinuxContainerExecutor.deleteAsUser can throw NPE if native\n+    executor returns an error (Eric Payne via jlowe)\n+\n Release 2.6.0 - 2014-11-18\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89/hadoop-yarn-project/CHANGES.txt",
                "sha": "872f16e75c2d75120ed5d57f1afdadf3a8a656df",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java?ref=902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "patch": "@@ -392,18 +392,23 @@ public void deleteAsUser(String user, Path dir, Path... baseDirs) {\n     verifyUsernamePattern(user);\n     String runAsUser = getRunAsUser(user);\n \n+    String dirString = dir == null ? \"\" : dir.toUri().getPath();\n+\n     List<String> command = new ArrayList<String>(\n         Arrays.asList(containerExecutorExe,\n                     runAsUser,\n                     user,\n                     Integer.toString(Commands.DELETE_AS_USER.getValue()),\n-                    dir == null ? \"\" : dir.toUri().getPath()));\n+                    dirString));\n+    List<String> pathsToDelete = new ArrayList<String>();\n     if (baseDirs == null || baseDirs.length == 0) {\n       LOG.info(\"Deleting absolute path : \" + dir);\n+      pathsToDelete.add(dirString);\n     } else {\n       for (Path baseDir : baseDirs) {\n         Path del = dir == null ? baseDir : new Path(baseDir, dir);\n         LOG.info(\"Deleting path : \" + del);\n+        pathsToDelete.add(del.toString());\n         command.add(baseDir.toUri().getPath());\n       }\n     }\n@@ -419,7 +424,7 @@ public void deleteAsUser(String user, Path dir, Path... baseDirs) {\n       }\n     } catch (IOException e) {\n       int exitCode = shExec.getExitCode();\n-      LOG.error(\"DeleteAsUser for \" + dir.toUri().getPath()\n+      LOG.error(\"DeleteAsUser for \" + StringUtils.join(\" \", pathsToDelete)\n           + \" returned with exit code: \" + exitCode, e);\n       LOG.error(\"Output from LinuxContainerExecutor's deleteAsUser follows:\");\n       logOutput(shExec.getOutput());",
                "raw_url": "https://github.com/apache/hadoop/raw/902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "sha": "d6e6894974d6d6fe8dc304d3e4a8ebd96852e318",
                "status": "modified"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/hadoop/blob/902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java?ref=902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "patch": "@@ -319,10 +319,57 @@ public void testDeleteAsUser() throws IOException {\n     String cmd = String.valueOf(\n         LinuxContainerExecutor.Commands.DELETE_AS_USER.getValue());\n     Path dir = new Path(\"/tmp/testdir\");\n-    \n+    Path testFile = new Path(\"testfile\");\n+    Path baseDir0 = new Path(\"/grid/0/BaseDir\");\n+    Path baseDir1 = new Path(\"/grid/1/BaseDir\");\n+\n+    mockExec.deleteAsUser(appSubmitter, dir);\n+    assertEquals(Arrays.asList(YarnConfiguration.DEFAULT_NM_NONSECURE_MODE_LOCAL_USER,\n+        appSubmitter, cmd, \"/tmp/testdir\"),\n+        readMockParams());\n+\n+    mockExec.deleteAsUser(appSubmitter, null);\n+    assertEquals(Arrays.asList(YarnConfiguration.DEFAULT_NM_NONSECURE_MODE_LOCAL_USER,\n+        appSubmitter, cmd, \"\"),\n+        readMockParams());\n+\n+    mockExec.deleteAsUser(appSubmitter, testFile, baseDir0, baseDir1);\n+    assertEquals(Arrays.asList(YarnConfiguration.DEFAULT_NM_NONSECURE_MODE_LOCAL_USER,\n+        appSubmitter, cmd, testFile.toString(), baseDir0.toString(), baseDir1.toString()),\n+        readMockParams());\n+\n+    mockExec.deleteAsUser(appSubmitter, null, baseDir0, baseDir1);\n+    assertEquals(Arrays.asList(YarnConfiguration.DEFAULT_NM_NONSECURE_MODE_LOCAL_USER,\n+        appSubmitter, cmd, \"\", baseDir0.toString(), baseDir1.toString()),\n+        readMockParams());\n+\n+    File f = new File(\"./src/test/resources/mock-container-executer-with-error\");\n+    if (!FileUtil.canExecute(f)) {\n+      FileUtil.setExecutable(f, true);\n+    }\n+    String executorPath = f.getAbsolutePath();\n+    Configuration conf = new Configuration();\n+    conf.set(YarnConfiguration.NM_LINUX_CONTAINER_EXECUTOR_PATH, executorPath);\n+    mockExec.setConf(conf);\n+\n     mockExec.deleteAsUser(appSubmitter, dir);\n     assertEquals(Arrays.asList(YarnConfiguration.DEFAULT_NM_NONSECURE_MODE_LOCAL_USER,\n         appSubmitter, cmd, \"/tmp/testdir\"),\n         readMockParams());\n+\n+    mockExec.deleteAsUser(appSubmitter, null);\n+    assertEquals(Arrays.asList(YarnConfiguration.DEFAULT_NM_NONSECURE_MODE_LOCAL_USER,\n+        appSubmitter, cmd, \"\"),\n+        readMockParams());\n+\n+    mockExec.deleteAsUser(appSubmitter, testFile, baseDir0, baseDir1);\n+    assertEquals(Arrays.asList(YarnConfiguration.DEFAULT_NM_NONSECURE_MODE_LOCAL_USER,\n+        appSubmitter, cmd, testFile.toString(), baseDir0.toString(), baseDir1.toString()),\n+        readMockParams());\n+\n+    mockExec.deleteAsUser(appSubmitter, null, baseDir0, baseDir1);\n+    assertEquals(Arrays.asList(YarnConfiguration.DEFAULT_NM_NONSECURE_MODE_LOCAL_USER,\n+        appSubmitter, cmd, \"\", baseDir0.toString(), baseDir1.toString()),\n+        readMockParams());\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/902c6ea7e4d3b49e49d9ce51ae9d12694ecfcf89/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "sha": "98ab8e0631add31b032ed0db78c1a7b8bb212e01",
                "status": "modified"
            }
        ],
        "message": "YARN-3088. LinuxContainerExecutor.deleteAsUser can throw NPE if native executor returns an error. Contributed by Eric Payne",
        "parent": "https://github.com/apache/hadoop/commit/2b0fa20f69417326a92beac10ffa072db2616e73",
        "repo": "hadoop",
        "unit_tests": [
            "TestLinuxContainerExecutor.java"
        ]
    },
    "hadoop_9478484": {
        "bug_id": "hadoop_9478484",
        "commit": "https://github.com/apache/hadoop/commit/94784848456a92a6502f3a3c0074e44fba4b19c9",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/94784848456a92a6502f3a3c0074e44fba4b19c9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcScheduler.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcScheduler.java?ref=94784848456a92a6502f3a3c0074e44fba4b19c9",
                "deletions": 7,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcScheduler.java",
                "patch": "@@ -206,11 +206,6 @@ public DecayRpcScheduler(int numLevels, String ns, Configuration conf) {\n     this.backOffResponseTimeThresholds =\n         parseBackOffResponseTimeThreshold(ns, conf, numLevels);\n \n-    // Setup delay timer\n-    Timer timer = new Timer();\n-    DecayTask task = new DecayTask(this, timer);\n-    timer.scheduleAtFixedRate(task, decayPeriodMillis, decayPeriodMillis);\n-\n     // Setup response time metrics\n     responseTimeTotalInCurrWindow = new AtomicLongArray(numLevels);\n     responseTimeCountInCurrWindow = new AtomicLongArray(numLevels);\n@@ -223,6 +218,11 @@ public DecayRpcScheduler(int numLevels, String ns, Configuration conf) {\n     Preconditions.checkArgument(topUsersCount > 0,\n         \"the number of top users for scheduler metrics must be at least 1\");\n \n+    // Setup delay timer\n+    Timer timer = new Timer();\n+    DecayTask task = new DecayTask(this, timer);\n+    timer.scheduleAtFixedRate(task, decayPeriodMillis, decayPeriodMillis);\n+\n     MetricsProxy prox = MetricsProxy.getInstance(ns, numLevels);\n     prox.setDelegate(this);\n     prox.registerMetrics2Source(ns);\n@@ -821,9 +821,10 @@ private void addTopNCallerSummary(MetricsRecordBuilder rb) {\n     final int topCallerCount = 10;\n     TopN topNCallers = getTopCallers(topCallerCount);\n     Map<Object, Integer> decisions = scheduleCacheRef.get();\n-    for (int i=0; i < topNCallers.size(); i++) {\n+    final int actualCallerCount = topNCallers.size();\n+    for (int i = 0; i < actualCallerCount; i++) {\n       NameValuePair entry =  topNCallers.poll();\n-      String topCaller = \"Top.\" + (topCallerCount - i) + \".\" +\n+      String topCaller = \"Top.\" + (actualCallerCount - i) + \".\" +\n           \"Caller(\" + entry.getName() + \")\";\n       String topCallerVolume = topCaller + \".Volume\";\n       String topCallerPriority = topCaller + \".Priority\";",
                "raw_url": "https://github.com/apache/hadoop/raw/94784848456a92a6502f3a3c0074e44fba4b19c9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/DecayRpcScheduler.java",
                "sha": "3443d0394ad75a44e8506398762fcb622e727f05",
                "status": "modified"
            }
        ],
        "message": "HADOOP-13159. Fix potential NPE in Metrics2 source for DecayRpcScheduler. Contributed by Xiaoyu Yao.",
        "parent": "https://github.com/apache/hadoop/commit/0c6726e20d9503589b21123f30757ddfbd405dde",
        "repo": "hadoop",
        "unit_tests": [
            "TestDecayRpcScheduler.java"
        ]
    },
    "hadoop_951c98f": {
        "bug_id": "hadoop_951c98f",
        "commit": "https://github.com/apache/hadoop/commit/951c98f89059d64fda8456366f680eff4a7a6785",
        "file": [
            {
                "additions": 58,
                "blob_url": "https://github.com/apache/hadoop/blob/951c98f89059d64fda8456366f680eff4a7a6785/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "changes": 86,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java?ref=951c98f89059d64fda8456366f680eff4a7a6785",
                "deletions": 28,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "patch": "@@ -2771,18 +2771,28 @@ public ResourceUsage getClusterResourceUsage() {\n         .getContainersToKill().isEmpty()) {\n       list = new ArrayList<>();\n       for (RMContainer rmContainer : csAssignment.getContainersToKill()) {\n-        list.add(getSchedulerContainer(rmContainer, false));\n+        SchedulerContainer schedulerContainer =\n+            getSchedulerContainer(rmContainer, false);\n+        if (schedulerContainer != null) {\n+          list.add(schedulerContainer);\n+        }\n       }\n     }\n \n     if (csAssignment.getExcessReservation() != null) {\n       if (null == list) {\n         list = new ArrayList<>();\n       }\n-      list.add(\n-          getSchedulerContainer(csAssignment.getExcessReservation(), false));\n+      SchedulerContainer schedulerContainer =\n+          getSchedulerContainer(csAssignment.getExcessReservation(), false);\n+      if (schedulerContainer != null) {\n+        list.add(schedulerContainer);\n+      }\n     }\n \n+    if (list != null && list.isEmpty()) {\n+      list = null;\n+    }\n     return list;\n   }\n \n@@ -2867,11 +2877,15 @@ public boolean attemptAllocationOnNode(SchedulerApplicationAttempt appAttempt,\n       ((RMContainerImpl)rmContainer).setAllocationTags(\n           new HashSet<>(schedulingRequest.getAllocationTags()));\n \n-      allocated = new ContainerAllocationProposal<>(\n-          getSchedulerContainer(rmContainer, true),\n-          null, null, NodeType.NODE_LOCAL, NodeType.NODE_LOCAL,\n-          SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY,\n-          resource);\n+      SchedulerContainer<FiCaSchedulerApp, FiCaSchedulerNode>\n+          schedulerContainer = getSchedulerContainer(rmContainer, true);\n+      if (schedulerContainer == null) {\n+        allocated = null;\n+      } else {\n+        allocated = new ContainerAllocationProposal<>(schedulerContainer,\n+            null, null, NodeType.NODE_LOCAL, NodeType.NODE_LOCAL,\n+            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY, resource);\n+      }\n     }\n \n     if (null != allocated) {\n@@ -2901,33 +2915,49 @@ public boolean attemptAllocationOnNode(SchedulerApplicationAttempt appAttempt,\n           csAssignment.getAssignmentInformation().getAllocationDetails();\n       if (!allocations.isEmpty()) {\n         RMContainer rmContainer = allocations.get(0).rmContainer;\n-        allocated = new ContainerAllocationProposal<>(\n-            getSchedulerContainer(rmContainer, true),\n-            getSchedulerContainersToRelease(csAssignment),\n-            getSchedulerContainer(csAssignment.getFulfilledReservedContainer(),\n-                false), csAssignment.getType(),\n-            csAssignment.getRequestLocalityType(),\n-            csAssignment.getSchedulingMode() != null ?\n-                csAssignment.getSchedulingMode() :\n-                SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY,\n-            csAssignment.getResource());\n+        SchedulerContainer<FiCaSchedulerApp, FiCaSchedulerNode>\n+            schedulerContainer = getSchedulerContainer(rmContainer, true);\n+        if (schedulerContainer == null) {\n+          allocated = null;\n+          // Decrease unconfirmed resource if app is alive\n+          FiCaSchedulerApp app = getApplicationAttempt(\n+              rmContainer.getApplicationAttemptId());\n+          if (app != null) {\n+            app.decUnconfirmedRes(rmContainer.getAllocatedResource());\n+          }\n+        } else {\n+          allocated = new ContainerAllocationProposal<>(schedulerContainer,\n+              getSchedulerContainersToRelease(csAssignment),\n+              getSchedulerContainer(\n+                  csAssignment.getFulfilledReservedContainer(), false),\n+              csAssignment.getType(), csAssignment.getRequestLocalityType(),\n+              csAssignment.getSchedulingMode() != null ?\n+                  csAssignment.getSchedulingMode() :\n+                  SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY,\n+              csAssignment.getResource());\n+        }\n       }\n \n       // Reserved something\n       List<AssignmentInformation.AssignmentDetails> reservation =\n           csAssignment.getAssignmentInformation().getReservationDetails();\n       if (!reservation.isEmpty()) {\n         RMContainer rmContainer = reservation.get(0).rmContainer;\n-        reserved = new ContainerAllocationProposal<>(\n-            getSchedulerContainer(rmContainer, false),\n-            getSchedulerContainersToRelease(csAssignment),\n-            getSchedulerContainer(csAssignment.getFulfilledReservedContainer(),\n-                false), csAssignment.getType(),\n-            csAssignment.getRequestLocalityType(),\n-            csAssignment.getSchedulingMode() != null ?\n-                csAssignment.getSchedulingMode() :\n-                SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY,\n-            csAssignment.getResource());\n+        SchedulerContainer<FiCaSchedulerApp, FiCaSchedulerNode>\n+            schedulerContainer = getSchedulerContainer(rmContainer, false);\n+        if (schedulerContainer == null) {\n+          reserved = null;\n+        } else {\n+          reserved = new ContainerAllocationProposal<>(schedulerContainer,\n+              getSchedulerContainersToRelease(csAssignment),\n+              getSchedulerContainer(\n+                  csAssignment.getFulfilledReservedContainer(), false),\n+              csAssignment.getType(), csAssignment.getRequestLocalityType(),\n+              csAssignment.getSchedulingMode() != null ?\n+                  csAssignment.getSchedulingMode() :\n+                  SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY,\n+              csAssignment.getResource());\n+        }\n       }\n     }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/951c98f89059d64fda8456366f680eff4a7a6785/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "sha": "e604b81e7f9495e4513eaa9670188f81803ee055",
                "status": "modified"
            },
            {
                "additions": 83,
                "blob_url": "https://github.com/apache/hadoop/blob/951c98f89059d64fda8456366f680eff4a7a6785/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacitySchedulerAsyncScheduling.java",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacitySchedulerAsyncScheduling.java?ref=951c98f89059d64fda8456366f680eff4a7a6785",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacitySchedulerAsyncScheduling.java",
                "patch": "@@ -56,8 +56,11 @@\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.ResourceCommitRequest;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.SchedulerContainer;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerApp;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.common.fica.FiCaSchedulerNode;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAttemptRemovedSchedulerEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeUpdateSchedulerEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.CandidateNodeSet;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.placement.SimpleCandidateNodeSet;\n import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;\n import org.apache.hadoop.yarn.util.resource.Resources;\n import org.junit.Assert;\n@@ -843,6 +846,86 @@ private ResourceCommitRequest createAllocateFromReservedProposal(\n     return new ResourceCommitRequest(allocateProposals, null, null);\n   }\n \n+  @Test(timeout = 30000)\n+  public void testReturnNullWhenGetSchedulerContainer() throws Exception {\n+    // disable async-scheduling for simulating complex scenario\n+    Configuration disableAsyncConf = new Configuration(conf);\n+    disableAsyncConf.setBoolean(\n+        CapacitySchedulerConfiguration.SCHEDULE_ASYNCHRONOUSLY_ENABLE, false);\n+\n+    // init RM & NMs\n+    final MockRM rm = new MockRM(disableAsyncConf);\n+    rm.start();\n+    final MockNM nm1 = rm.registerNode(\"192.168.0.1:1234\", 8 * GB);\n+    final MockNM nm2 = rm.registerNode(\"192.168.0.2:2234\", 8 * GB);\n+    rm.drainEvents();\n+    CapacityScheduler cs = (CapacityScheduler) rm.getRMContext().getScheduler();\n+    SchedulerNode sn1 = cs.getSchedulerNode(nm1.getNodeId());\n+    RMNode rmNode1 = cs.getNode(nm1.getNodeId()).getRMNode();\n+    SchedulerNode sn2 = cs.getSchedulerNode(nm2.getNodeId());\n+\n+    // launch app1-am on nm1\n+    RMApp app1 = rm.submitApp(1 * GB, \"app1\", \"user\", null, false, \"default\",\n+        YarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS, null, null, true, true);\n+    MockAM am1 = MockRM.launchAndRegisterAM(app1, rm, nm1);\n+\n+    // app2 asks 1 * 1G container\n+    am1.allocate(ImmutableList.of(ResourceRequest\n+        .newInstance(Priority.newInstance(0), \"*\",\n+            Resources.createResource(1 * GB), 1)), null);\n+    RMContainer amContainer = cs.getRMContainer(\n+        ContainerId.newContainerId(am1.getApplicationAttemptId(), 1));\n+\n+    // spy CapacityScheduler\n+    final CapacityScheduler spyCs = Mockito.spy(cs);\n+    // hook CapacityScheduler#submitResourceCommitRequest\n+    List<CSAssignment> assignmentSnapshots = new ArrayList<>();\n+    Mockito.doAnswer(new Answer<Object>() {\n+      public Boolean answer(InvocationOnMock invocation) throws Exception {\n+        CSAssignment assignment = (CSAssignment) invocation.getArguments()[1];\n+        if (cs.getNode(nm1.getNodeId()) != null) {\n+          // decommission nm1 for first allocation on nm1\n+          cs.getRMContext().getDispatcher().getEventHandler().handle(\n+              new RMNodeEvent(nm1.getNodeId(), RMNodeEventType.DECOMMISSION));\n+          rm.drainEvents();\n+          Assert.assertEquals(NodeState.DECOMMISSIONED, rmNode1.getState());\n+          Assert.assertNull(cs.getNode(nm1.getNodeId()));\n+          assignmentSnapshots.add(assignment);\n+        } else {\n+          // add am container on nm1 to containersToKill\n+          // for second allocation on nm2\n+          assignment.setContainersToKill(ImmutableList.of(amContainer));\n+        }\n+        // check no NPE in actual submit, before YARN-8233 will throw NPE\n+        cs.submitResourceCommitRequest((Resource) invocation.getArguments()[0],\n+            assignment);\n+        return false;\n+      }\n+    }).when(spyCs).submitResourceCommitRequest(Mockito.any(Resource.class),\n+        Mockito.any(CSAssignment.class));\n+\n+    // allocation on nm1, test return null when get scheduler container\n+    CandidateNodeSet<FiCaSchedulerNode> candidateNodeSet =\n+        new SimpleCandidateNodeSet(sn1);\n+    spyCs.allocateContainersToNode(candidateNodeSet, false);\n+    // make sure unconfirmed resource is decreased correctly\n+    Assert.assertTrue(spyCs.getApplicationAttempt(am1.getApplicationAttemptId())\n+        .hasPendingResourceRequest(RMNodeLabelsManager.NO_LABEL,\n+            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY));\n+\n+    // allocation on nm2,\n+    // test return null when get scheduler container to release\n+    candidateNodeSet =\n+        new SimpleCandidateNodeSet(sn2);\n+    spyCs.allocateContainersToNode(candidateNodeSet, false);\n+    // make sure unconfirmed resource is decreased correctly\n+    Assert.assertTrue(spyCs.getApplicationAttempt(am1.getApplicationAttemptId())\n+        .hasPendingResourceRequest(RMNodeLabelsManager.NO_LABEL,\n+            SchedulingMode.RESPECT_PARTITION_EXCLUSIVITY));\n+\n+    rm.stop();\n+  }\n+\n   private void keepNMHeartbeat(List<MockNM> mockNMs, int interval) {\n     if (nmHeartbeatThread != null) {\n       nmHeartbeatThread.setShouldStop();",
                "raw_url": "https://github.com/apache/hadoop/raw/951c98f89059d64fda8456366f680eff4a7a6785/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacitySchedulerAsyncScheduling.java",
                "sha": "67c504d3df102c7b58ae8b520cd8b42bd627770b",
                "status": "modified"
            }
        ],
        "message": "YARN-8233. NPE in CapacityScheduler#tryCommit when handling allocate/reserve proposal whose allocatedOrReservedContainer is null. Contributed by Tao Yang.",
        "parent": "https://github.com/apache/hadoop/commit/ba1f9d66d94ed0b85084d7c40c09a87478b3a05a",
        "repo": "hadoop",
        "unit_tests": [
            "TestCapacityScheduler.java"
        ]
    },
    "hadoop_95a87ca": {
        "bug_id": "hadoop_95a87ca",
        "commit": "https://github.com/apache/hadoop/commit/95a87caed0d2b68d196bf5ead835fd81dc95c2e3",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/95a87caed0d2b68d196bf5ead835fd81dc95c2e3/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=95a87caed0d2b68d196bf5ead835fd81dc95c2e3",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -434,6 +434,9 @@ Release 2.3.0 - UNRELEASED\n     HADOOP-10093. hadoop-env.cmd sets HADOOP_CLIENT_OPTS with a max heap size\n     that is too small. (Shanyu Zhao via cnauroth)\n \n+    HADOOP-10094. NPE in GenericOptionsParser#preProcessForWindows().\n+    (Enis Soztutar via cnauroth)\n+\n Release 2.2.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/95a87caed0d2b68d196bf5ead835fd81dc95c2e3/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "4fd41a876c51b5258033fc4e71d5f2656c402207",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/95a87caed0d2b68d196bf5ead835fd81dc95c2e3/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java?ref=95a87caed0d2b68d196bf5ead835fd81dc95c2e3",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java",
                "patch": "@@ -431,6 +431,9 @@ private String validateFiles(String files, Configuration conf)\n     if (!Shell.WINDOWS) {\n       return args;\n     }\n+    if (args == null) {\n+      return null;\n+    }\n     List<String> newArgs = new ArrayList<String>(args.length);\n     for (int i=0; i < args.length; i++) {\n       String prop = null;",
                "raw_url": "https://github.com/apache/hadoop/raw/95a87caed0d2b68d196bf5ead835fd81dc95c2e3/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/GenericOptionsParser.java",
                "sha": "678185553939200900c73b1ac6952a05c7c07816",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/95a87caed0d2b68d196bf5ead835fd81dc95c2e3/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java?ref=95a87caed0d2b68d196bf5ead835fd81dc95c2e3",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java",
                "patch": "@@ -282,4 +282,12 @@ private void assertDOptionParsing(String[] args,\n       Arrays.toString(remainingArgs) + Arrays.toString(expectedRemainingArgs),\n       expectedRemainingArgs, remainingArgs);\n   }\n+\n+  /** Test passing null as args. Some classes still call\n+   * Tool interface from java passing null.\n+   */\n+  public void testNullArgs() throws IOException {\n+    GenericOptionsParser parser = new GenericOptionsParser(conf, null);\n+    parser.getRemainingArgs();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/95a87caed0d2b68d196bf5ead835fd81dc95c2e3/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestGenericOptionsParser.java",
                "sha": "48a419b3a52f7b72889e8820fb2561630d73531d",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10094. NPE in GenericOptionsParser#preProcessForWindows(). Contributed by Enis Soztutar.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1541991 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/fe67e30bc2794e7ff073cf938ee80eba805d1e69",
        "repo": "hadoop",
        "unit_tests": [
            "TestGenericOptionsParser.java"
        ]
    },
    "hadoop_960940e": {
        "bug_id": "hadoop_960940e",
        "commit": "https://github.com/apache/hadoop/commit/960940e0e08f7839775f2d8a352b444d104d36b4",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/960940e0e08f7839775f2d8a352b444d104d36b4/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java?ref=960940e0e08f7839775f2d8a352b444d104d36b4",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "patch": "@@ -790,13 +790,24 @@ protected synchronized int readWithStrategy(ReaderStrategy strategy)\n           // Check if need to report block replicas corruption either read\n           // was successful or ChecksumException occurred.\n           reportCheckSumFailure(corruptedBlocks,\n-              currentLocatedBlock.getLocations().length, false);\n+              getCurrentBlockLocationsLength(), false);\n         }\n       }\n     }\n     return -1;\n   }\n \n+  protected int getCurrentBlockLocationsLength() {\n+    int len = 0;\n+    if (currentLocatedBlock == null) {\n+      DFSClient.LOG.info(\"Found null currentLocatedBlock. pos={}, \"\n+          + \"blockEnd={}, fileLength={}\", pos, blockEnd, getFileLength());\n+    } else {\n+      len = currentLocatedBlock.getLocations().length;\n+    }\n+    return len;\n+  }\n+\n   /**\n    * Read the entire buffer.\n    */",
                "raw_url": "https://github.com/apache/hadoop/raw/960940e0e08f7839775f2d8a352b444d104d36b4/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "sha": "b38e6299030abeb41c6e45780a4e5c97ce52e0aa",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/960940e0e08f7839775f2d8a352b444d104d36b4/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java?ref=960940e0e08f7839775f2d8a352b444d104d36b4",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.hadoop.hdfs;\n \n+import com.google.common.annotations.VisibleForTesting;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.fs.ReadOption;\n import org.apache.hadoop.hdfs.protocol.BlockType;\n@@ -160,7 +161,8 @@ protected ThreadPoolExecutor getStripedReadsThreadPool(){\n    * When seeking into a new block group, create blockReader for each internal\n    * block in the group.\n    */\n-  private synchronized void blockSeekTo(long target) throws IOException {\n+  @VisibleForTesting\n+  synchronized void blockSeekTo(long target) throws IOException {\n     if (target >= getFileLength()) {\n       throw new IOException(\"Attempted to read past end of file\");\n     }\n@@ -400,8 +402,8 @@ protected synchronized int readWithStrategy(ReaderStrategy strategy)\n       } finally {\n         // Check if need to report block replicas corruption either read\n         // was successful or ChecksumException occurred.\n-        reportCheckSumFailure(corruptedBlocks,\n-            currentLocatedBlock.getLocations().length, true);\n+        reportCheckSumFailure(corruptedBlocks, getCurrentBlockLocationsLength(),\n+            true);\n       }\n     }\n     return -1;",
                "raw_url": "https://github.com/apache/hadoop/raw/960940e0e08f7839775f2d8a352b444d104d36b4/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSStripedInputStream.java",
                "sha": "f3b16e09812eb21fed5e148b112421d95ac70ff1",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/hadoop/blob/960940e0e08f7839775f2d8a352b444d104d36b4/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedInputStream.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedInputStream.java?ref=960940e0e08f7839775f2d8a352b444d104d36b4",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedInputStream.java",
                "patch": "@@ -35,6 +35,7 @@\n import org.apache.hadoop.io.erasurecode.ErasureCoderOptions;\n import org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory;\n import org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder;\n+import org.apache.hadoop.test.GenericTestUtils;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n@@ -51,7 +52,12 @@\n import static org.apache.hadoop.fs.CommonConfigurationKeysPublic.IO_FILE_BUFFER_SIZE_KEY;\n import static org.junit.Assert.assertArrayEquals;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+import static org.mockito.Matchers.anyLong;\n+import static org.mockito.Mockito.doThrow;\n+import static org.mockito.Mockito.spy;\n \n public class TestDFSStripedInputStream {\n \n@@ -504,4 +510,23 @@ public void testIdempotentClose() throws Exception {\n       in.close();\n     }\n   }\n+\n+  @Test\n+  public void testReadFailToGetCurrentBlock() throws Exception {\n+    DFSTestUtil.writeFile(cluster.getFileSystem(), filePath, \"test\");\n+    try (DFSStripedInputStream in = (DFSStripedInputStream) fs.getClient()\n+        .open(filePath.toString())) {\n+      final DFSStripedInputStream spy = spy(in);\n+      final String msg = \"Injected exception for testReadNPE\";\n+      doThrow(new IOException(msg)).when(spy).blockSeekTo(anyLong());\n+      assertNull(in.getCurrentBlock());\n+      try {\n+        spy.read();\n+        fail(\"read should have failed\");\n+      } catch (IOException expected) {\n+        LOG.info(\"Exception caught\", expected);\n+        GenericTestUtils.assertExceptionContains(msg, expected);\n+      }\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/960940e0e08f7839775f2d8a352b444d104d36b4/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestDFSStripedInputStream.java",
                "sha": "cdebee0dc8da76b4eec2a8826ea536bde584833a",
                "status": "modified"
            }
        ],
        "message": "HDFS-13539. DFSStripedInputStream NPE when reportCheckSumFailure.",
        "parent": "https://github.com/apache/hadoop/commit/fc5d49c202354c6f39b33ea3f80f38e85794c6b3",
        "repo": "hadoop",
        "unit_tests": [
            "TestDFSInputStream.java",
            "TestDFSStripedInputStream.java"
        ]
    },
    "hadoop_9678020": {
        "bug_id": "hadoop_9678020",
        "commit": "https://github.com/apache/hadoop/commit/9678020e59cf073f74cce70ac57d1f6869349a36",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/9678020e59cf073f74cce70ac57d1f6869349a36/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=9678020e59cf073f74cce70ac57d1f6869349a36",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -813,6 +813,8 @@ Release 2.3.0 - UNRELEASED\n     HDFS-5074. Allow starting up from an fsimage checkpoint in the middle of a\n     segment. (Todd Lipcon via atm)\n \n+    HDFS-4201. NPE in BPServiceActor#sendHeartBeat. (jxiang via cmccabe)\n+\n Release 2.2.0 - 2013-10-13\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/9678020e59cf073f74cce70ac57d1f6869349a36/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "ee9b964724cead83d407f35b7c57096567db2885",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/9678020e59cf073f74cce70ac57d1f6869349a36/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java?ref=9678020e59cf073f74cce70ac57d1f6869349a36",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "patch": "@@ -274,12 +274,22 @@ DataNode getDataNode() {\n   synchronized void verifyAndSetNamespaceInfo(NamespaceInfo nsInfo) throws IOException {\n     if (this.bpNSInfo == null) {\n       this.bpNSInfo = nsInfo;\n-      \n+      boolean success = false;\n+\n       // Now that we know the namespace ID, etc, we can pass this to the DN.\n       // The DN can now initialize its local storage if we are the\n       // first BP to handshake, etc.\n-      dn.initBlockPool(this);\n-      return;\n+      try {\n+        dn.initBlockPool(this);\n+        success = true;\n+      } finally {\n+        if (!success) {\n+          // The datanode failed to initialize the BP. We need to reset\n+          // the namespace info so that other BPService actors still have\n+          // a chance to set it, and re-initialize the datanode.\n+          this.bpNSInfo = null;\n+        }\n+      }\n     } else {\n       checkNSEquality(bpNSInfo.getBlockPoolID(), nsInfo.getBlockPoolID(),\n           \"Blockpool ID\");",
                "raw_url": "https://github.com/apache/hadoop/raw/9678020e59cf073f74cce70ac57d1f6869349a36/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "sha": "e646be9a650bd13d88caf1a6c51cbeda26d47173",
                "status": "modified"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/hadoop/blob/9678020e59cf073f74cce70ac57d1f6869349a36/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java?ref=9678020e59cf073f74cce70ac57d1f6869349a36",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java",
                "patch": "@@ -25,7 +25,9 @@\n import java.io.File;\n import java.io.IOException;\n import java.net.InetSocketAddress;\n+import java.util.List;\n import java.util.Map;\n+import java.util.concurrent.atomic.AtomicInteger;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n@@ -294,6 +296,47 @@ public void testPickActiveNameNode() throws Exception {\n     }\n   }\n \n+  /**\n+   * Test datanode block pool initialization error handling.\n+   * Failure in initializing a block pool should not cause NPE.\n+   */\n+  @Test\n+  public void testBPInitErrorHandling() throws Exception {\n+    final DataNode mockDn = Mockito.mock(DataNode.class);\n+    Mockito.doReturn(true).when(mockDn).shouldRun();\n+    Configuration conf = new Configuration();\n+    File dnDataDir = new File(\n+      new File(TEST_BUILD_DATA, \"testBPInitErrorHandling\"), \"data\");\n+    conf.set(DFS_DATANODE_DATA_DIR_KEY, dnDataDir.toURI().toString());\n+    Mockito.doReturn(conf).when(mockDn).getConf();\n+    Mockito.doReturn(new DNConf(conf)).when(mockDn).getDnConf();\n+    Mockito.doReturn(DataNodeMetrics.create(conf, \"fake dn\")).\n+      when(mockDn).getMetrics();\n+    final AtomicInteger count = new AtomicInteger();\n+    Mockito.doAnswer(new Answer<Void>() {\n+      @Override\n+      public Void answer(InvocationOnMock invocation) throws Throwable {\n+        if (count.getAndIncrement() == 0) {\n+          throw new IOException(\"faked initBlockPool exception\");\n+        }\n+        // The initBlockPool is called again. Now mock init is done.\n+        Mockito.doReturn(mockFSDataset).when(mockDn).getFSDataset();\n+        return null;\n+      }\n+    }).when(mockDn).initBlockPool(Mockito.any(BPOfferService.class));\n+    BPOfferService bpos = setupBPOSForNNs(mockDn, mockNN1, mockNN2);\n+    bpos.start();\n+    try {\n+      waitForInitialization(bpos);\n+      List<BPServiceActor> actors = bpos.getBPServiceActors();\n+      assertEquals(1, actors.size());\n+      BPServiceActor actor = actors.get(0);\n+      waitForBlockReport(actor.getNameNodeProxy());\n+    } finally {\n+      bpos.stop();\n+    }\n+  }\n+\n   private void waitForOneToFail(final BPOfferService bpos)\n       throws Exception {\n     GenericTestUtils.waitFor(new Supplier<Boolean>() {\n@@ -311,6 +354,11 @@ public Boolean get() {\n    */\n   private BPOfferService setupBPOSForNNs(\n       DatanodeProtocolClientSideTranslatorPB ... nns) throws IOException {\n+    return setupBPOSForNNs(mockDn, nns);\n+  }\n+\n+  private BPOfferService setupBPOSForNNs(DataNode mockDn,\n+      DatanodeProtocolClientSideTranslatorPB ... nns) throws IOException {\n     // Set up some fake InetAddresses, then override the connectToNN\n     // function to return the corresponding proxies.\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/9678020e59cf073f74cce70ac57d1f6869349a36/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBPOfferService.java",
                "sha": "c5cb6c77f129e8e7a61507403342d8fab2b89505",
                "status": "modified"
            }
        ],
        "message": "HDFS-4201. NPE in BPServiceActor#sendHeartBeat (jxiang via cmccabe)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1550269 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/ed4d318d681eaa59bf4e9048c13175260da5a719",
        "repo": "hadoop",
        "unit_tests": [
            "TestBPOfferService.java"
        ]
    },
    "hadoop_99c8c58": {
        "bug_id": "hadoop_99c8c58",
        "commit": "https://github.com/apache/hadoop/commit/99c8c5839b65666e6099116e4d7024e0eb4682b9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/99c8c5839b65666e6099116e4d7024e0eb4682b9/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=99c8c5839b65666e6099116e4d7024e0eb4682b9",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -933,6 +933,9 @@ Release 2.8.0 - UNRELEASED\n     HADOOP-12186. ActiveStandbyElector shouldn't call monitorLockNodeAsync\n     multiple times (zhihai xu via vinayakumarb)\n \n+    HADOOP-12117. Potential NPE from Configuration#loadProperty with\n+    allowNullValueProperties set. (zhihai xu via vinayakumarb)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/99c8c5839b65666e6099116e4d7024e0eb4682b9/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "5d11db9c82b0658666a999c1ed020029e03896d9",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/99c8c5839b65666e6099116e4d7024e0eb4682b9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java?ref=99c8c5839b65666e6099116e4d7024e0eb4682b9",
                "deletions": 4,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java",
                "patch": "@@ -2735,14 +2735,14 @@ private void overlay(Properties to, Properties from) {\n       to.put(entry.getKey(), entry.getValue());\n     }\n   }\n-  \n+\n   private void loadProperty(Properties properties, String name, String attr,\n       String value, boolean finalParameter, String[] source) {\n     if (value != null || allowNullValueProperties) {\n+      if (value == null) {\n+        value = DEFAULT_STRING_CHECK;\n+      }\n       if (!finalParameters.contains(attr)) {\n-        if (value==null && allowNullValueProperties) {\n-          value = DEFAULT_STRING_CHECK;\n-        }\n         properties.setProperty(attr, value);\n         if(source != null) {\n           updatingResource.put(attr, source);",
                "raw_url": "https://github.com/apache/hadoop/raw/99c8c5839b65666e6099116e4d7024e0eb4682b9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/conf/Configuration.java",
                "sha": "0b4542966c478799f0fa0097550bcf2c3b470276",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/99c8c5839b65666e6099116e4d7024e0eb4682b9/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java?ref=99c8c5839b65666e6099116e4d7024e0eb4682b9",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java",
                "patch": "@@ -42,13 +42,15 @@\n \n import junit.framework.TestCase;\n import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.fail;\n \n import org.apache.commons.lang.StringUtils;\n import org.apache.hadoop.conf.Configuration.IntegerRanges;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.net.NetUtils;\n import static org.apache.hadoop.util.PlatformName.IBM_JAVA;\n+\n import org.codehaus.jackson.map.ObjectMapper;\n import org.mockito.Mockito;\n \n@@ -1511,6 +1513,19 @@ public void run() {\n     // it's expected behaviour.\n   }\n \n+  public void testNullValueProperties() throws Exception {\n+    Configuration conf = new Configuration();\n+    conf.setAllowNullValueProperties(true);\n+    out = new BufferedWriter(new FileWriter(CONFIG));\n+    startConfig();\n+    appendProperty(\"attr\", \"value\", true);\n+    appendProperty(\"attr\", \"\", true);\n+    endConfig();\n+    Path fileResource = new Path(CONFIG);\n+    conf.addResource(fileResource);\n+    assertEquals(\"value\", conf.get(\"attr\"));\n+  }\n+\n   public static void main(String[] argv) throws Exception {\n     junit.textui.TestRunner.main(new String[]{\n       TestConfiguration.class.getName()",
                "raw_url": "https://github.com/apache/hadoop/raw/99c8c5839b65666e6099116e4d7024e0eb4682b9/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/conf/TestConfiguration.java",
                "sha": "a0397414ce1eaa0cb91bf98230f63a7f5d21e69e",
                "status": "modified"
            }
        ],
        "message": "HADOOP-12117. Potential NPE from Configuration#loadProperty with allowNullValueProperties set. (Contributed by zhihai xu)",
        "parent": "https://github.com/apache/hadoop/commit/af63427c6d7d2fc251eafb1f152b7a90c5bd07e5",
        "repo": "hadoop",
        "unit_tests": [
            "TestConfiguration.java"
        ]
    },
    "hadoop_9a10b4e": {
        "bug_id": "hadoop_9a10b4e",
        "commit": "https://github.com/apache/hadoop/commit/9a10b4e773ac937b59b458343457bbbd686d7f1e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/9a10b4e773ac937b59b458343457bbbd686d7f1e/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=9a10b4e773ac937b59b458343457bbbd686d7f1e",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -335,6 +335,9 @@ Release 0.23.3 - UNRELEASED\n     MAPREDUCE-4128. AM Recovery expects all attempts of a completed task to\n     also be completed. (Bikas Saha via bobby)\n \n+    MAPREDUCE-4144. Fix a NPE in the ResourceManager when handling node\n+    updates. (Jason Lowe via sseth)\n+\n Release 0.23.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/9a10b4e773ac937b59b458343457bbbd686d7f1e/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "d0eaa60a7342be1d2e8012c739814d903a2851de",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/9a10b4e773ac937b59b458343457bbbd686d7f1e/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java?ref=9a10b4e773ac937b59b458343457bbbd686d7f1e",
                "deletions": 5,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "patch": "@@ -1118,13 +1118,12 @@ private Resource assignOffSwitchContainers(Resource clusterResource, SchedulerNo\n   boolean canAssign(SchedulerApp application, Priority priority, \n       SchedulerNode node, NodeType type, RMContainer reservedContainer) {\n \n-    // Reserved... \n-    if (reservedContainer != null) {\n-      return true;\n-    }\n-    \n     // Clearly we need containers for this application...\n     if (type == NodeType.OFF_SWITCH) {\n+      if (reservedContainer != null) {\n+        return true;\n+      }\n+\n       // 'Delay' off-switch\n       ResourceRequest offSwitchRequest = \n           application.getResourceRequest(priority, RMNode.ANY);",
                "raw_url": "https://github.com/apache/hadoop/raw/9a10b4e773ac937b59b458343457bbbd686d7f1e/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "sha": "2256799f9b55e4212bc184560789dcf12bcfb178",
                "status": "modified"
            },
            {
                "additions": 97,
                "blob_url": "https://github.com/apache/hadoop/blob/9a10b4e773ac937b59b458343457bbbd686d7f1e/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java",
                "changes": 97,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java?ref=9a10b4e773ac937b59b458343457bbbd686d7f1e",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java",
                "patch": "@@ -926,6 +926,103 @@ public void testReservation() throws Exception {\n     assertEquals(4*GB, a.getMetrics().getAllocatedMB());\n   }\n   \n+  @Test\n+  public void testStolenReservedContainer() throws Exception {\n+    // Manipulate queue 'a'\n+    LeafQueue a = stubLeafQueue((LeafQueue)queues.get(A));\n+    //unset maxCapacity\n+    a.setMaxCapacity(1.0f);\n+\n+    // Users\n+    final String user_0 = \"user_0\";\n+    final String user_1 = \"user_1\";\n+\n+    // Submit applications\n+    final ApplicationAttemptId appAttemptId_0 =\n+        TestUtils.getMockApplicationAttemptId(0, 0);\n+    SchedulerApp app_0 =\n+        new SchedulerApp(appAttemptId_0, user_0, a,\n+            mock(ActiveUsersManager.class), rmContext, null);\n+    a.submitApplication(app_0, user_0, A);\n+\n+    final ApplicationAttemptId appAttemptId_1 =\n+        TestUtils.getMockApplicationAttemptId(1, 0);\n+    SchedulerApp app_1 =\n+        new SchedulerApp(appAttemptId_1, user_1, a,\n+            mock(ActiveUsersManager.class), rmContext, null);\n+    a.submitApplication(app_1, user_1, A);\n+\n+    // Setup some nodes\n+    String host_0 = \"host_0\";\n+    SchedulerNode node_0 = TestUtils.getMockNode(host_0, DEFAULT_RACK, 0, 4*GB);\n+    String host_1 = \"host_1\";\n+    SchedulerNode node_1 = TestUtils.getMockNode(host_1, DEFAULT_RACK, 0, 4*GB);\n+\n+    final int numNodes = 3;\n+    Resource clusterResource = Resources.createResource(numNodes * (4*GB));\n+    when(csContext.getNumClusterNodes()).thenReturn(numNodes);\n+\n+    // Setup resource-requests\n+    Priority priority = TestUtils.createMockPriority(1);\n+    app_0.updateResourceRequests(Collections.singletonList(\n+            TestUtils.createResourceRequest(RMNodeImpl.ANY, 2*GB, 1, priority,\n+                recordFactory)));\n+\n+    // Setup app_1 to request a 4GB container on host_0 and\n+    // another 4GB container anywhere.\n+    ArrayList<ResourceRequest> appRequests_1 =\n+        new ArrayList<ResourceRequest>(4);\n+    appRequests_1.add(TestUtils.createResourceRequest(host_0, 4*GB, 1,\n+        priority, recordFactory));\n+    appRequests_1.add(TestUtils.createResourceRequest(DEFAULT_RACK, 4*GB, 1,\n+        priority, recordFactory));\n+    appRequests_1.add(TestUtils.createResourceRequest(RMNodeImpl.ANY, 4*GB, 2,\n+        priority, recordFactory));\n+    app_1.updateResourceRequests(appRequests_1);\n+\n+    // Start testing...\n+\n+    a.assignContainers(clusterResource, node_0);\n+    assertEquals(2*GB, a.getUsedResources().getMemory());\n+    assertEquals(2*GB, app_0.getCurrentConsumption().getMemory());\n+    assertEquals(0*GB, app_1.getCurrentConsumption().getMemory());\n+    assertEquals(0*GB, a.getMetrics().getReservedMB());\n+    assertEquals(2*GB, a.getMetrics().getAllocatedMB());\n+    assertEquals(0*GB, a.getMetrics().getAvailableMB());\n+\n+    // Now, reservation should kick in for app_1\n+    a.assignContainers(clusterResource, node_0);\n+    assertEquals(6*GB, a.getUsedResources().getMemory());\n+    assertEquals(2*GB, app_0.getCurrentConsumption().getMemory());\n+    assertEquals(0*GB, app_1.getCurrentConsumption().getMemory());\n+    assertEquals(4*GB, app_1.getCurrentReservation().getMemory());\n+    assertEquals(2*GB, node_0.getUsedResource().getMemory());\n+    assertEquals(4*GB, a.getMetrics().getReservedMB());\n+    assertEquals(2*GB, a.getMetrics().getAllocatedMB());\n+\n+    // node_1 heartbeats in and gets the DEFAULT_RACK request for app_1\n+    a.assignContainers(clusterResource, node_1);\n+    assertEquals(10*GB, a.getUsedResources().getMemory());\n+    assertEquals(2*GB, app_0.getCurrentConsumption().getMemory());\n+    assertEquals(4*GB, app_1.getCurrentConsumption().getMemory());\n+    assertEquals(4*GB, app_1.getCurrentReservation().getMemory());\n+    assertEquals(4*GB, node_1.getUsedResource().getMemory());\n+    assertEquals(4*GB, a.getMetrics().getReservedMB());\n+    assertEquals(6*GB, a.getMetrics().getAllocatedMB());\n+\n+    // Now free 1 container from app_0 and try to assign to node_0\n+    a.completedContainer(clusterResource, app_0, node_0,\n+        app_0.getLiveContainers().iterator().next(), null, RMContainerEventType.KILL);\n+    a.assignContainers(clusterResource, node_0);\n+    assertEquals(8*GB, a.getUsedResources().getMemory());\n+    assertEquals(0*GB, app_0.getCurrentConsumption().getMemory());\n+    assertEquals(8*GB, app_1.getCurrentConsumption().getMemory());\n+    assertEquals(0*GB, app_1.getCurrentReservation().getMemory());\n+    assertEquals(4*GB, node_0.getUsedResource().getMemory());\n+    assertEquals(0*GB, a.getMetrics().getReservedMB());\n+    assertEquals(8*GB, a.getMetrics().getAllocatedMB());\n+  }\n+\n   @Test\n   public void testReservationExchange() throws Exception {\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/9a10b4e773ac937b59b458343457bbbd686d7f1e/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestLeafQueue.java",
                "sha": "8be9b20193e1b19eb3e99808b94dc5b15b8325b6",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4144. Fix a NPE in the ResourceManager when handling node updates. (Contributed by Jason Lowe)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1325991 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/5a20d446cf2a947b37fd5856a7e1fe6c21547557",
        "repo": "hadoop",
        "unit_tests": [
            "TestLeafQueue.java"
        ]
    },
    "hadoop_9e35571": {
        "bug_id": "hadoop_9e35571",
        "commit": "https://github.com/apache/hadoop/commit/9e355719653c5e7b48b601090634882e4f29a743",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/9e355719653c5e7b48b601090634882e4f29a743/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=9e355719653c5e7b48b601090634882e4f29a743",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -782,6 +782,8 @@ Release 2.6.0 - UNRELEASED\n     HDFS-7096. Fix TestRpcProgramNfs3 to use DFS_ENCRYPTION_KEY_PROVIDER_URI\n     (clamb via cmccabe)\n \n+    HDFS-7046. HA NN can NPE upon transition to active. (kihwal)\n+\n     BREAKDOWN OF HDFS-6134 AND HADOOP-10150 SUBTASKS AND RELATED JIRAS\n   \n       HDFS-6387. HDFS CLI admin tool for creating & deleting an",
                "raw_url": "https://github.com/apache/hadoop/raw/9e355719653c5e7b48b601090634882e4f29a743/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "8c45d699c08a6fd5bc2abdea5733dd93667440bc",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/9e355719653c5e7b48b601090634882e4f29a743/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=9e355719653c5e7b48b601090634882e4f29a743",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -1156,8 +1156,9 @@ void startActiveServices() throws IOException {\n       cacheManager.startMonitorThread();\n       blockManager.getDatanodeManager().setShouldSendCachingCommands(true);\n     } finally {\n-      writeUnlock();\n       startingActiveService = false;\n+      checkSafeMode();\n+      writeUnlock();\n     }\n   }\n \n@@ -5570,6 +5571,9 @@ private void checkMode() {\n       // Have to have write-lock since leaving safemode initializes\n       // repl queues, which requires write lock\n       assert hasWriteLock();\n+      if (inTransitionToActive()) {\n+        return;\n+      }\n       // if smmthread is already running, the block threshold must have been \n       // reached before, there is no need to enter the safe mode again\n       if (smmthread == null && needEnter()) {",
                "raw_url": "https://github.com/apache/hadoop/raw/9e355719653c5e7b48b601090634882e4f29a743/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "4dc278667d1261e5128d1cc6029d6ab4dc7c05ff",
                "status": "modified"
            }
        ],
        "message": "HDFS-7046. HA NN can NPE upon transition to active. Contributed by\nKihwal Lee.",
        "parent": "https://github.com/apache/hadoop/commit/adf0b67a7104bd457b20c95ff78dd48753dcd699",
        "repo": "hadoop",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_9ee891a": {
        "bug_id": "hadoop_9ee891a",
        "commit": "https://github.com/apache/hadoop/commit/9ee891aa90333bf18cba412400daa5834f15c41d",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/9ee891aa90333bf18cba412400daa5834f15c41d/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=9ee891aa90333bf18cba412400daa5834f15c41d",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -777,6 +777,8 @@ Release 2.6.0 - UNRELEASED\n     HADOOP-10925. Compilation fails in native link0 function on Windows.\n     (cnauroth)\n \n+    HADOOP-11077. NPE if hosts not specified in ProxyUsers. (gchanan via tucu)\n+\n Release 2.5.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/9ee891aa90333bf18cba412400daa5834f15c41d/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "b0150873cd83fb2408295ae0fdae1dc07f74ce99",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/9ee891aa90333bf18cba412400daa5834f15c41d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java?ref=9ee891aa90333bf18cba412400daa5834f15c41d",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java",
                "patch": "@@ -123,7 +123,7 @@ public void authorize(UserGroupInformation user,\n     MachineList MachineList = proxyHosts.get(\n         getProxySuperuserIpConfKey(realUser.getShortUserName()));\n \n-    if(!MachineList.includes(remoteAddress)) {\n+    if(MachineList == null || !MachineList.includes(remoteAddress)) {\n       throw new AuthorizationException(\"Unauthorized connection for super-user: \"\n           + realUser.getUserName() + \" from IP \" + remoteAddress);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/9ee891aa90333bf18cba412400daa5834f15c41d/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java",
                "sha": "b36ac80717ec567e4579006b5aae08f4ee0b7e2e",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop/blob/9ee891aa90333bf18cba412400daa5834f15c41d/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/authorize/TestProxyUsers.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/authorize/TestProxyUsers.java?ref=9ee891aa90333bf18cba412400daa5834f15c41d",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/authorize/TestProxyUsers.java",
                "patch": "@@ -478,6 +478,21 @@ public void testProxyUsersWithCustomPrefix() throws Exception {\n     assertNotAuthorized(proxyUserUgi, \"1.2.3.5\");\n   }\n \n+  @Test\n+  public void testNoHostsForUsers() throws Exception {\n+    Configuration conf = new Configuration(false);\n+    conf.set(\"y.\" + REAL_USER_NAME + \".users\",\n+      StringUtils.join(\",\", Arrays.asList(AUTHORIZED_PROXY_USER_NAME)));\n+    ProxyUsers.refreshSuperUserGroupsConfiguration(conf, \"y\");\n+\n+    UserGroupInformation realUserUgi = UserGroupInformation\n+      .createRemoteUser(REAL_USER_NAME);\n+    UserGroupInformation proxyUserUgi = UserGroupInformation.createProxyUserForTesting(\n+      AUTHORIZED_PROXY_USER_NAME, realUserUgi, GROUP_NAMES);\n+\n+    // IP doesn't matter\n+    assertNotAuthorized(proxyUserUgi, \"1.2.3.4\");\n+  }\n \n   private void assertNotAuthorized(UserGroupInformation proxyUgi, String host) {\n     try {",
                "raw_url": "https://github.com/apache/hadoop/raw/9ee891aa90333bf18cba412400daa5834f15c41d/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/authorize/TestProxyUsers.java",
                "sha": "8ff4bfb10884566dad9e426f3abce3f8d1949a49",
                "status": "modified"
            }
        ],
        "message": "HADOOP-11077. NPE if hosts not specified in ProxyUsers. (gchanan via tucu)",
        "parent": "https://github.com/apache/hadoop/commit/bbff44cb03d0150f990acc3b77170893241cc282",
        "repo": "hadoop",
        "unit_tests": [
            "TestDefaultImpersonationProvider.java"
        ]
    },
    "hadoop_9f2b77a": {
        "bug_id": "hadoop_9f2b77a",
        "commit": "https://github.com/apache/hadoop/commit/9f2b77aee496b0636aabafa61f13903f28bd86fe",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/9f2b77aee496b0636aabafa61f13903f28bd86fe/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=9f2b77aee496b0636aabafa61f13903f28bd86fe",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -715,6 +715,9 @@ Release 0.23.1 - Unreleased\n \n     MAPREDUCE-3813. Added a cache for resolved racks. (vinodkv via acmurthy)   \n \n+    MAPREDUCE-3808. Fixed an NPE in FileOutputCommitter for jobs with maps\n+    but no reduces. (Robert Joseph Evans via vinodkv)\n+\n Release 0.23.0 - 2011-11-01 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/9f2b77aee496b0636aabafa61f13903f28bd86fe/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "050af54fcebc18df5a1efc456ce94a44380010c8",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/9f2b77aee496b0636aabafa61f13903f28bd86fe/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileOutputCommitter.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileOutputCommitter.java?ref=9f2b77aee496b0636aabafa61f13903f28bd86fe",
                "deletions": 7,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileOutputCommitter.java",
                "patch": "@@ -85,18 +85,21 @@ private static Path getOutputPath(TaskAttemptContext context) {\n    */\n   @Private\n   Path getJobAttemptPath(JobContext context) {\n-    return org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n-        .getJobAttemptPath(context, getOutputPath(context));\n+    Path out = getOutputPath(context);\n+    return out == null ? null : \n+      org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n+        .getJobAttemptPath(context, out);\n   }\n \n   @Private\n   Path getTaskAttemptPath(TaskAttemptContext context) throws IOException {\n-    return getTaskAttemptPath(context, getOutputPath(context));\n+    Path out = getOutputPath(context);\n+    return out == null ? null : getTaskAttemptPath(context, out);\n   }\n \n   private Path getTaskAttemptPath(TaskAttemptContext context, Path out) throws IOException {\n     Path workPath = FileOutputFormat.getWorkOutputPath(context.getJobConf());\n-    if(workPath == null) {\n+    if(workPath == null && out != null) {\n       return org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n       .getTaskAttemptPath(context, out);\n     }\n@@ -110,14 +113,17 @@ private Path getTaskAttemptPath(TaskAttemptContext context, Path out) throws IOE\n    * @return the path where the output of a committed task is stored until\n    * the entire job is committed.\n    */\n+  @Private\n   Path getCommittedTaskPath(TaskAttemptContext context) {\n-    return org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n-        .getCommittedTaskPath(context, getOutputPath(context));\n+    Path out = getOutputPath(context);\n+    return out == null ? null : \n+      org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\n+        .getCommittedTaskPath(context, out);\n   }\n \n   public Path getWorkPath(TaskAttemptContext context, Path outputPath) \n   throws IOException {\n-    return getTaskAttemptPath(context, outputPath);\n+    return outputPath == null ? null : getTaskAttemptPath(context, outputPath);\n   }\n   \n   @Override\n@@ -156,6 +162,7 @@ public void abortJob(JobContext context, int runState)\n     getWrapped(context).abortJob(context, state);\n   }\n   \n+  @Override\n   public void setupTask(TaskAttemptContext context) throws IOException {\n     getWrapped(context).setupTask(context);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/9f2b77aee496b0636aabafa61f13903f28bd86fe/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapred/FileOutputCommitter.java",
                "sha": "a6190d2060d4c3ee0fc1934f86235ac51366e7ca",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop/blob/9f2b77aee496b0636aabafa61f13903f28bd86fe/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java?ref=9f2b77aee496b0636aabafa61f13903f28bd86fe",
                "deletions": 27,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
                "patch": "@@ -495,36 +495,40 @@ public boolean isRecoverySupported() {\n   @Override\n   public void recoverTask(TaskAttemptContext context)\n       throws IOException {\n-    context.progress();\n-    TaskAttemptID attemptId = context.getTaskAttemptID();\n-    int previousAttempt = getAppAttemptId(context) - 1;\n-    if (previousAttempt < 0) {\n-      throw new IOException (\"Cannot recover task output for first attempt...\");\n-    }\n-    \n-    Path committedTaskPath = getCommittedTaskPath(context);\n-    Path previousCommittedTaskPath = getCommittedTaskPath(\n-        previousAttempt, context);\n-    FileSystem fs = committedTaskPath.getFileSystem(context.getConfiguration());\n-    \n-    LOG.debug(\"Trying to recover task from \" + previousCommittedTaskPath \n-        + \" into \" + committedTaskPath);\n-    if (fs.exists(previousCommittedTaskPath)) {\n-      if(fs.exists(committedTaskPath)) {\n-        if(!fs.delete(committedTaskPath, true)) {\n-          throw new IOException(\"Could not delete \"+committedTaskPath);\n-        }\n+    if(hasOutputPath()) {\n+      context.progress();\n+      TaskAttemptID attemptId = context.getTaskAttemptID();\n+      int previousAttempt = getAppAttemptId(context) - 1;\n+      if (previousAttempt < 0) {\n+        throw new IOException (\"Cannot recover task output for first attempt...\");\n       }\n-      //Rename can fail if the parent directory does not yet exist.\n-      Path committedParent = committedTaskPath.getParent();\n-      fs.mkdirs(committedParent);\n-      if(!fs.rename(previousCommittedTaskPath, committedTaskPath)) {\n-        throw new IOException(\"Could not rename \" + previousCommittedTaskPath +\n-            \" to \" + committedTaskPath);\n+\n+      Path committedTaskPath = getCommittedTaskPath(context);\n+      Path previousCommittedTaskPath = getCommittedTaskPath(\n+          previousAttempt, context);\n+      FileSystem fs = committedTaskPath.getFileSystem(context.getConfiguration());\n+\n+      LOG.debug(\"Trying to recover task from \" + previousCommittedTaskPath \n+          + \" into \" + committedTaskPath);\n+      if (fs.exists(previousCommittedTaskPath)) {\n+        if(fs.exists(committedTaskPath)) {\n+          if(!fs.delete(committedTaskPath, true)) {\n+            throw new IOException(\"Could not delete \"+committedTaskPath);\n+          }\n+        }\n+        //Rename can fail if the parent directory does not yet exist.\n+        Path committedParent = committedTaskPath.getParent();\n+        fs.mkdirs(committedParent);\n+        if(!fs.rename(previousCommittedTaskPath, committedTaskPath)) {\n+          throw new IOException(\"Could not rename \" + previousCommittedTaskPath +\n+              \" to \" + committedTaskPath);\n+        }\n+        LOG.info(\"Saved output of \" + attemptId + \" to \" + committedTaskPath);\n+      } else {\n+        LOG.warn(attemptId+\" had no output to recover.\");\n       }\n-      LOG.info(\"Saved output of \" + attemptId + \" to \" + committedTaskPath);\n     } else {\n-      LOG.warn(attemptId+\" had no output to recover.\");\n+      LOG.warn(\"Output Path is null in recoverTask()\");\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/9f2b77aee496b0636aabafa61f13903f28bd86fe/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/output/FileOutputCommitter.java",
                "sha": "7bad09f303977f66b49e7aad180c0e1d4268703e",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop/blob/9f2b77aee496b0636aabafa61f13903f28bd86fe/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestFileOutputCommitter.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestFileOutputCommitter.java?ref=9f2b77aee496b0636aabafa61f13903f28bd86fe",
                "deletions": 3,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestFileOutputCommitter.java",
                "patch": "@@ -104,7 +104,9 @@ public void testRecovery() throws Exception {\n     writeOutput(theRecordWriter, tContext);\n \n     // do commit\n-    committer.commitTask(tContext);\n+    if(committer.needsTaskCommit(tContext)) {\n+      committer.commitTask(tContext);\n+    }\n     Path jobTempDir1 = committer.getCommittedTaskPath(tContext);\n     File jtd1 = new File(jobTempDir1.toUri().getPath());\n     assertTrue(jtd1.exists());\n@@ -188,7 +190,9 @@ public void testCommitter() throws Exception {\n     writeOutput(theRecordWriter, tContext);\n \n     // do commit\n-    committer.commitTask(tContext);\n+    if(committer.needsTaskCommit(tContext)) {\n+      committer.commitTask(tContext);\n+    }\n     committer.commitJob(jContext);\n \n     // validate output\n@@ -214,14 +218,38 @@ public void testMapFileOutputCommitter() throws Exception {\n     writeMapFileOutput(theRecordWriter, tContext);\n \n     // do commit\n-    committer.commitTask(tContext);\n+    if(committer.needsTaskCommit(tContext)) {\n+      committer.commitTask(tContext);\n+    }\n     committer.commitJob(jContext);\n \n     // validate output\n     validateMapFileOutputContent(FileSystem.get(conf), outDir);\n     FileUtil.fullyDelete(new File(outDir.toString()));\n   }\n   \n+  public void testMapOnlyNoOutput() throws Exception {\n+    JobConf conf = new JobConf();\n+    //This is not set on purpose. FileOutputFormat.setOutputPath(conf, outDir);\n+    conf.set(JobContext.TASK_ATTEMPT_ID, attempt);\n+    JobContext jContext = new JobContextImpl(conf, taskID.getJobID());\n+    TaskAttemptContext tContext = new TaskAttemptContextImpl(conf, taskID);\n+    FileOutputCommitter committer = new FileOutputCommitter();    \n+    \n+    // setup\n+    committer.setupJob(jContext);\n+    committer.setupTask(tContext);\n+    \n+    if(committer.needsTaskCommit(tContext)) {\n+      // do commit\n+      committer.commitTask(tContext);\n+    }\n+    committer.commitJob(jContext);\n+\n+    // validate output\n+    FileUtil.fullyDelete(new File(outDir.toString()));\n+  }\n+  \n   public void testAbort() throws IOException, InterruptedException {\n     JobConf conf = new JobConf();\n     FileOutputFormat.setOutputPath(conf, outDir);",
                "raw_url": "https://github.com/apache/hadoop/raw/9f2b77aee496b0636aabafa61f13903f28bd86fe/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapred/TestFileOutputCommitter.java",
                "sha": "0859571d1f2bde79064403fd1d5023d368f1826d",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3808. Fixed an NPE in FileOutputCommitter for jobs with maps but no reduces. Contributed by Robert Joseph Evans.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1241217 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/c6282df3e59eb1e5481158184c344034872d2a89",
        "repo": "hadoop",
        "unit_tests": [
            "TestFileOutputCommitter.java",
            "TestFileOutputCommitter.java"
        ]
    },
    "hadoop_a196ee9": {
        "bug_id": "hadoop_a196ee9",
        "commit": "https://github.com/apache/hadoop/commit/a196ee9362a1b35e5de20ee519f7c544ab1588e1",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/a196ee9362a1b35e5de20ee519f7c544ab1588e1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerRelaunch.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerRelaunch.java?ref=a196ee9362a1b35e5de20ee519f7c544ab1588e1",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerRelaunch.java",
                "patch": "@@ -35,7 +35,6 @@\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ContainerLocalizer;\n import org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext;\n import org.apache.hadoop.yarn.server.nodemanager.executor.DeletionAsUserContext;\n-import org.apache.hadoop.yarn.util.ConverterUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -97,6 +96,8 @@ public Integer call() {\n       List<String> logDirs = dirsHandler.getLogDirs();\n       List<String> containerLocalDirs = getContainerLocalDirs(localDirs);\n       List<String> containerLogDirs = getContainerLogDirs(logDirs);\n+      List<String> filecacheDirs = getNMFilecacheDirs(localDirs);\n+      List<String> userLocalDirs = getUserLocalDirs(localDirs);\n \n       if (!dirsHandler.areDisksHealthy()) {\n         ret = ContainerExitStatus.DISKS_FAILED;\n@@ -114,6 +115,8 @@ public Integer call() {\n           .setContainerWorkDir(containerWorkDir)\n           .setLocalDirs(localDirs)\n           .setLogDirs(logDirs)\n+          .setFilecacheDirs(filecacheDirs)\n+          .setUserLocalDirs(userLocalDirs)\n           .setContainerLocalDirs(containerLocalDirs)\n           .setContainerLogDirs(containerLogDirs)\n           .build());",
                "raw_url": "https://github.com/apache/hadoop/raw/a196ee9362a1b35e5de20ee519f7c544ab1588e1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerRelaunch.java",
                "sha": "6a0761a5d24245b694fd0e3c1f682e4c7cbe0cc4",
                "status": "modified"
            },
            {
                "additions": 97,
                "blob_url": "https://github.com/apache/hadoop/blob/a196ee9362a1b35e5de20ee519f7c544ab1588e1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/TestContainerRelaunch.java",
                "changes": 97,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/TestContainerRelaunch.java?ref=a196ee9362a1b35e5de20ee519f7c544ab1588e1",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/TestContainerRelaunch.java",
                "patch": "@@ -0,0 +1,97 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n+import org.apache.hadoop.yarn.event.Dispatcher;\n+import org.apache.hadoop.yarn.event.InlineDispatcher;\n+import org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor;\n+import org.apache.hadoop.yarn.server.nodemanager.Context;\n+import org.apache.hadoop.yarn.server.nodemanager.LocalDirsHandlerService;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;\n+import org.apache.hadoop.yarn.server.nodemanager.executor.ContainerStartContext;\n+import org.apache.hadoop.yarn.server.nodemanager.recovery.NMNullStateStoreService;\n+import org.junit.Test;\n+import org.mockito.ArgumentCaptor;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.mockito.Matchers.any;\n+import static org.mockito.Matchers.anyString;\n+import static org.mockito.Mockito.doReturn;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+\n+/** Unit tests for relaunching containers. */\n+public class TestContainerRelaunch {\n+\n+  @Test\n+  public void testRelaunchContext() throws Exception {\n+    Configuration conf = new Configuration();\n+\n+    Context mockContext = mock(Context.class);\n+    doReturn(new NMNullStateStoreService()).when(mockContext).getNMStateStore();\n+    ApplicationId appId = ApplicationId.newInstance(1, 1);\n+    ApplicationAttemptId appAttemptId =\n+        ApplicationAttemptId.newInstance(appId, 1);\n+    ContainerId cid = ContainerId.newContainerId(appAttemptId, 1);\n+    Application mockApp = mock(Application.class);\n+    doReturn(appId).when(mockApp).getAppId();\n+    Container mockContainer = mock(Container.class);\n+    doReturn(\"somebody\").when(mockContainer).getUser();\n+    doReturn(cid).when(mockContainer).getContainerId();\n+    doReturn(\"/foo\").when(mockContainer).getWorkDir();\n+    doReturn(\"/bar\").when(mockContainer).getLogDir();\n+    LocalDirsHandlerService mockDirsHandler =\n+        mock(LocalDirsHandlerService.class);\n+    doReturn(true).when(mockDirsHandler).isGoodLocalDir(any(String.class));\n+    doReturn(true).when(mockDirsHandler).isGoodLogDir(anyString());\n+    doReturn(true).when(mockDirsHandler).areDisksHealthy();\n+    doReturn(new Path(\"/some/file\")).when(mockDirsHandler)\n+        .getLocalPathForRead(anyString());\n+    Dispatcher dispatcher = new InlineDispatcher();\n+    ContainerExecutor mockExecutor = mock(ContainerExecutor.class);\n+    ContainerRelaunch cr = new ContainerRelaunch(mockContext, conf, dispatcher,\n+        mockExecutor, mockApp, mockContainer, mockDirsHandler, null);\n+    int result = cr.call();\n+    assertEquals(\"relaunch failed\", 0, result);\n+    ArgumentCaptor<ContainerStartContext> captor =\n+        ArgumentCaptor.forClass(ContainerStartContext.class);\n+    verify(mockExecutor).launchContainer(captor.capture());\n+    ContainerStartContext csc = captor.getValue();\n+    assertNotNull(\"app ID null\", csc.getAppId());\n+    assertNotNull(\"container null\", csc.getContainer());\n+    assertNotNull(\"container local dirs null\", csc.getContainerLocalDirs());\n+    assertNotNull(\"container log dirs null\", csc.getContainerLogDirs());\n+    assertNotNull(\"work dir null\", csc.getContainerWorkDir());\n+    assertNotNull(\"filecache dirs null\", csc.getFilecacheDirs());\n+    assertNotNull(\"local dirs null\", csc.getLocalDirs());\n+    assertNotNull(\"localized resources null\", csc.getLocalizedResources());\n+    assertNotNull(\"log dirs null\", csc.getLogDirs());\n+    assertNotNull(\"script path null\", csc.getNmPrivateContainerScriptPath());\n+    assertNotNull(\"tokens path null\", csc.getNmPrivateTokensPath());\n+    assertNotNull(\"user null\", csc.getUser());\n+    assertNotNull(\"user local dirs null\", csc.getUserLocalDirs());\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/a196ee9362a1b35e5de20ee519f7c544ab1588e1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/TestContainerRelaunch.java",
                "sha": "95f706c3e079e46e23ae72977367f83a78ab650e",
                "status": "added"
            }
        ],
        "message": "YARN-7890. NPE during container relaunch. Contributed by Jason Lowe",
        "parent": "https://github.com/apache/hadoop/commit/60656bcecadd80e28c81bc943b44abf13d20abae",
        "repo": "hadoop",
        "unit_tests": [
            "TestContainerRelaunch.java"
        ]
    },
    "hadoop_a446ad2": {
        "bug_id": "hadoop_a446ad2",
        "commit": "https://github.com/apache/hadoop/commit/a446ad2c26359beb2b5367195de4257fbae648c6",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/a446ad2c26359beb2b5367195de4257fbae648c6/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=a446ad2c26359beb2b5367195de4257fbae648c6",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -481,6 +481,8 @@ Release 0.23.3 - UNRELEASED\n     MAPREDUCE-4237. TestNodeStatusUpdater can fail if localhost has a domain\n     associated with it (bobby)\n \n+    MAPREDUCE-4233. NPE can happen in RMNMNodeInfo. (bobby)\n+\n Release 0.23.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/a446ad2c26359beb2b5367195de4257fbae648c6/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "ff4664675d44bb97f2aff87288a3d2c7127d3307",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hadoop/blob/a446ad2c26359beb2b5367195de4257fbae648c6/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java?ref=a446ad2c26359beb2b5367195de4257fbae648c6",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java",
                "patch": "@@ -21,22 +21,28 @@\n import java.io.File;\n import java.io.IOException;\n import java.util.Iterator;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.yarn.api.records.NodeId;\n+import org.apache.hadoop.yarn.server.resourcemanager.MockNodes;\n import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n import org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;\n import org.codehaus.jackson.JsonNode;\n import org.codehaus.jackson.map.ObjectMapper;\n import org.junit.AfterClass;\n import org.junit.Assert;\n import org.junit.BeforeClass;\n import org.junit.Test;\n+import static org.mockito.Mockito.*;\n \n public class TestRMNMInfo {\n   private static final Log LOG = LogFactory.getLog(TestRMNMInfo.class);\n@@ -116,14 +122,47 @@ public void testRMNMInfo() throws Exception {\n               n.get(\"HealthStatus\").getValueAsText().contains(\"Healthy\"));\n       Assert.assertNotNull(n.get(\"LastHealthUpdate\"));\n       Assert.assertNotNull(n.get(\"HealthReport\"));\n-      Assert.assertNotNull(n.get(\"NumContainersMB\"));\n+      Assert.assertNotNull(n.get(\"NumContainers\"));\n       Assert.assertEquals(\n               n.get(\"NodeId\") + \": Unexpected number of used containers\",\n-              0, n.get(\"NumContainersMB\").getValueAsInt());\n+              0, n.get(\"NumContainers\").getValueAsInt());\n       Assert.assertEquals(\n               n.get(\"NodeId\") + \": Unexpected amount of used memory\",\n               0, n.get(\"UsedMemoryMB\").getValueAsInt());\n       Assert.assertNotNull(n.get(\"AvailableMemoryMB\"));\n     }\n   }\n+  \n+  @Test\n+  public void testRMNMInfoMissmatch() throws Exception {\n+    RMContext rmc = mock(RMContext.class);\n+    ResourceScheduler rms = mock(ResourceScheduler.class);\n+    ConcurrentMap<NodeId, RMNode> map = new ConcurrentHashMap<NodeId, RMNode>();\n+    RMNode node = MockNodes.newNodeInfo(1, MockNodes.newResource(4 * 1024));\n+    map.put(node.getNodeID(), node);\n+    when(rmc.getRMNodes()).thenReturn(map);\n+    \n+    RMNMInfo rmInfo = new RMNMInfo(rmc,rms);\n+    String liveNMs = rmInfo.getLiveNodeManagers();\n+    ObjectMapper mapper = new ObjectMapper();\n+    JsonNode jn = mapper.readTree(liveNMs);\n+    Assert.assertEquals(\"Unexpected number of live nodes:\",\n+                                               1, jn.size());\n+    Iterator<JsonNode> it = jn.iterator();\n+    while (it.hasNext()) {\n+      JsonNode n = it.next();\n+      Assert.assertNotNull(n.get(\"HostName\"));\n+      Assert.assertNotNull(n.get(\"Rack\"));\n+      Assert.assertTrue(\"Node \" + n.get(\"NodeId\") + \" should be RUNNING\",\n+              n.get(\"State\").getValueAsText().contains(\"RUNNING\"));\n+      Assert.assertNotNull(n.get(\"NodeHTTPAddress\"));\n+      Assert.assertTrue(\"Node \" + n.get(\"NodeId\") + \" should be Healthy\",\n+              n.get(\"HealthStatus\").getValueAsText().contains(\"Healthy\"));\n+      Assert.assertNotNull(n.get(\"LastHealthUpdate\"));\n+      Assert.assertNotNull(n.get(\"HealthReport\"));\n+      Assert.assertNull(n.get(\"NumContainers\"));\n+      Assert.assertNull(n.get(\"UsedMemoryMB\"));\n+      Assert.assertNull(n.get(\"AvailableMemoryMB\"));\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/a446ad2c26359beb2b5367195de4257fbae648c6/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java",
                "sha": "4ee485644d91ad5b1fcffde9846301c36d237353",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/a446ad2c26359beb2b5367195de4257fbae648c6/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java?ref=a446ad2c26359beb2b5367195de4257fbae648c6",
                "deletions": 4,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java",
                "patch": "@@ -93,10 +93,12 @@ public String getLiveNodeManagers() {\n                         ni.getNodeHealthStatus().getLastHealthReportTime());\n         info.put(\"HealthReport\",\n                         ni.getNodeHealthStatus().getHealthReport());\n-        info.put(\"NumContainersMB\", report.getNumContainers());\n-        info.put(\"UsedMemoryMB\", report.getUsedResource().getMemory());\n-        info.put(\"AvailableMemoryMB\",\n-                                report.getAvailableResource().getMemory());\n+        if(report != null) {\n+          info.put(\"NumContainers\", report.getNumContainers());\n+          info.put(\"UsedMemoryMB\", report.getUsedResource().getMemory());\n+          info.put(\"AvailableMemoryMB\",\n+              report.getAvailableResource().getMemory());\n+        }\n \n         nodesInfo.add(info);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/a446ad2c26359beb2b5367195de4257fbae648c6/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java",
                "sha": "0db42e40ec0e08d72413048956baf0393062157d",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4233. NPE can happen in RMNMNodeInfo. (bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1337363 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/f092e9fc8a2b58c755ecfc6828cc3e2af624b90b",
        "repo": "hadoop",
        "unit_tests": [
            "TestRMNMInfo.java"
        ]
    },
    "hadoop_a4bae51": {
        "bug_id": "hadoop_a4bae51",
        "commit": "https://github.com/apache/hadoop/commit/a4bae51b7dac4301942ed28d0128fc9ef6a0d13a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/a4bae51b7dac4301942ed28d0128fc9ef6a0d13a/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=a4bae51b7dac4301942ed28d0128fc9ef6a0d13a",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -273,6 +273,8 @@ Trunk (Unreleased)\n     HDFS-4785. Concat operation does not remove concatenated files from\n     InodeMap. (suresh)\n \n+    HDFS-4784. NPE in FSDirectory.resolvePath(). (Brandon Li via suresh)\n+\n   BREAKDOWN OF HADOOP-8562 and HDFS-3602 SUBTASKS AND RELATED JIRAS\n \n     HDFS-4145. Merge hdfs cmd line scripts from branch-1-win. (David Lao,",
                "raw_url": "https://github.com/apache/hadoop/raw/a4bae51b7dac4301942ed28d0128fc9ef6a0d13a/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "4329d145cb8f54eb93bcf59ea28f76aa48002736",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/a4bae51b7dac4301942ed28d0128fc9ef6a0d13a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java?ref=a4bae51b7dac4301942ed28d0128fc9ef6a0d13a",
                "deletions": 6,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "patch": "@@ -1090,7 +1090,7 @@ int unprotectedDelete(String src, BlocksMapUpdateInfo collectedBlocks,\n       NameNode.stateChangeLog.debug(\"DIR* FSDirectory.unprotectedDelete: \"\n           +src+\" is removed\");\n     }\n-    remvoedAllFromInodesFromMap(targetNode);\n+    removeAllFromInodesFromMap(targetNode);\n     return filesRemoved;\n   }\n   \n@@ -1783,14 +1783,14 @@ private final void removeFromInodeMap(INode inode) {\n   }\n   \n   /** Remove all the inodes under given inode from the map */\n-  private void remvoedAllFromInodesFromMap(INode inode) {\n+  private void removeAllFromInodesFromMap(INode inode) {\n     removeFromInodeMap(inode);\n     if (!inode.isDirectory()) {\n       return;\n     }\n     INodeDirectory dir = (INodeDirectory) inode;\n     for (INode child : dir.getChildrenList()) {\n-      remvoedAllFromInodesFromMap(child);\n+      removeAllFromInodesFromMap(child);\n     }\n     dir.clearChildren();\n   }\n@@ -2258,14 +2258,18 @@ static String resolvePath(String src, byte[][] pathComponents, FSDirectory fsd)\n     try {\n       id = Long.valueOf(inodeId);\n     } catch (NumberFormatException e) {\n-      throw new FileNotFoundException(\n-          \"File for given inode path does not exist: \" + src);\n+      throw new FileNotFoundException(\"Invalid inode path: \" + src);\n     }\n     if (id == INodeId.ROOT_INODE_ID && pathComponents.length == 4) {\n       return Path.SEPARATOR;\n     }\n+    INode inode = fsd.getInode(id);\n+    if (inode == null) {\n+      throw new FileNotFoundException(\n+          \"File for given inode path does not exist: \" + src);\n+    }\n     StringBuilder path = id == INodeId.ROOT_INODE_ID ? new StringBuilder()\n-        : new StringBuilder(fsd.getInode(id).getFullPathName());\n+        : new StringBuilder(inode.getFullPathName());\n     for (int i = 4; i < pathComponents.length; i++) {\n       path.append(Path.SEPARATOR).append(DFSUtil.bytes2String(pathComponents[i]));\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/a4bae51b7dac4301942ed28d0128fc9ef6a0d13a/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "sha": "a3150a8b20eff2ab7bb2beb9b0699e8f923348fd",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/a4bae51b7dac4301942ed28d0128fc9ef6a0d13a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java?ref=a4bae51b7dac4301942ed28d0128fc9ef6a0d13a",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java",
                "patch": "@@ -909,6 +909,17 @@ public void testInodePath() throws FileNotFoundException {\n     components = INode.getPathComponents(testPath);\n     resolvedPath = FSDirectory.resolvePath(testPath, components, fsd);\n     assertEquals(testPath, resolvedPath);\n+    \n+    // Test path with nonexistent(deleted or wrong id) inode\n+    Mockito.doReturn(null).when(fsd).getInode(Mockito.anyLong());\n+    testPath = \"/.reserved/.inodes/1234\";\n+    components = INode.getPathComponents(testPath);\n+    try {\n+      String realPath = FSDirectory.resolvePath(testPath, components, fsd);\n+      fail(\"Path should not be resolved:\" + realPath);\n+    } catch (IOException e) {\n+      assertTrue(e instanceof FileNotFoundException);\n+    }\n   }\n   \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/a4bae51b7dac4301942ed28d0128fc9ef6a0d13a/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeFile.java",
                "sha": "3b6491f6e5b4a08d5f5f0f430cb73caff9b9b0c7",
                "status": "modified"
            }
        ],
        "message": "HDFS-4784. NPE in FSDirectory.resolvePath(). Contributed by Brandon Li.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1478276 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/03ba436d42418226a5edb754f5119fe69039c8b8",
        "repo": "hadoop",
        "unit_tests": [
            "TestFSDirectory.java"
        ]
    },
    "hadoop_a4f62a2": {
        "bug_id": "hadoop_a4f62a2",
        "commit": "https://github.com/apache/hadoop/commit/a4f62a2d58d4e00cda3632411c6abda6eaa12a0e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/a4f62a2d58d4e00cda3632411c6abda6eaa12a0e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=a4f62a2d58d4e00cda3632411c6abda6eaa12a0e",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -2308,6 +2308,9 @@ Release 2.8.0 - UNRELEASED\n     HDFS-9387. Fix namenodeUri parameter parsing in NNThroughputBenchmark.\n     (Mingliang Liu via xyao)\n \n+    HDFS-9421. NNThroughputBenchmark replication test NPE with -namenode option.\n+    (Mingliang Liu via xyao)\n+\n Release 2.7.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/a4f62a2d58d4e00cda3632411c6abda6eaa12a0e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "d76253a908a73012071f3b8a5eb38ff395cd2cb3",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop/blob/a4f62a2d58d4e00cda3632411c6abda6eaa12a0e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java?ref=a4f62a2d58d4e00cda3632411c6abda6eaa12a0e",
                "deletions": 9,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java",
                "patch": "@@ -106,9 +106,9 @@\n  * By default the refresh is never called.</li>\n  * <li>-keepResults do not clean up the name-space after execution.</li>\n  * <li>-useExisting do not recreate the name-space, use existing data.</li>\n- * <li>-namenode will run the test against a namenode in another\n- * process or on another host. If you use this option, the namenode\n- * must have dfs.namenode.fs-limits.min-block-size set to 16.</li>\n+ * <li>-namenode will run the test (except {@link ReplicationStats}) against a\n+ * namenode in another process or on another host. If you use this option,\n+ * the namenode must have dfs.namenode.fs-limits.min-block-size set to 16.</li>\n  * </ol>\n  * \n  * The benchmark first generates inputs for each thread so that the\n@@ -126,8 +126,9 @@\n   private static final String GENERAL_OPTIONS_USAGE = \n     \"     [-keepResults] | [-logLevel L] | [-UGCacheRefreshCount G] |\" +\n     \" [-namenode <namenode URI>]\\n\" +\n-    \"     If using -namenode, set the namenode's\" +\n-    \"         dfs.namenode.fs-limits.min-block-size to 16.\";\n+    \"     If using -namenode, set the namenode's \" +\n+    \"dfs.namenode.fs-limits.min-block-size to 16. Replication test does not \" +\n+        \"support -namenode.\";\n \n   static Configuration config;\n   static NameNode nameNode;\n@@ -1471,13 +1472,22 @@ public int run(String[] aArgs) throws Exception {\n         ops.add(opStat);\n       }\n       if(runAll || ReplicationStats.OP_REPLICATION_NAME.equals(type)) {\n-        opStat = new ReplicationStats(args);\n-        ops.add(opStat);\n+        if (namenodeUri != null || args.contains(\"-namenode\")) {\n+          LOG.warn(\"The replication test is ignored as it does not support \" +\n+              \"standalone namenode in another process or on another host. \" +\n+              \"Please run replication test without -namenode argument.\");\n+        } else {\n+          opStat = new ReplicationStats(args);\n+          ops.add(opStat);\n+        }\n       }\n       if(runAll || CleanAllStats.OP_CLEAN_NAME.equals(type)) {\n         opStat = new CleanAllStats(args);\n         ops.add(opStat);\n       }\n+      if (ops.isEmpty()) {\n+        printUsage();\n+      }\n \n       if (namenodeUri == null) {\n         nameNode = NameNode.createNameNode(argv, config);\n@@ -1501,8 +1511,6 @@ public int run(String[] aArgs) throws Exception {\n             DFSTestUtil.getRefreshUserMappingsProtocolProxy(config, nnUri);\n         getBlockPoolId(dfs);\n       }\n-      if(ops.size() == 0)\n-        printUsage();\n       // run each benchmark\n       for(OperationStatsBase op : ops) {\n         LOG.info(\"Starting benchmark: \" + op.getOpName());",
                "raw_url": "https://github.com/apache/hadoop/raw/a4f62a2d58d4e00cda3632411c6abda6eaa12a0e/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/NNThroughputBenchmark.java",
                "sha": "affbe2f9af54d24dadaa4e31b5f018aee1efc084",
                "status": "modified"
            }
        ],
        "message": "HDFS-9421. NNThroughputBenchmark replication test NPE with -namenode option. Contributed by Mingliang Liu.",
        "parent": "https://github.com/apache/hadoop/commit/2701f2d2558f3ade879539f3f7bedf749709f2f1",
        "repo": "hadoop",
        "unit_tests": [
            "TestNNThroughputBenchmark.java"
        ]
    },
    "hadoop_a583a40": {
        "bug_id": "hadoop_a583a40",
        "commit": "https://github.com/apache/hadoop/commit/a583a40693f5c56c40b39fd12cfa0bb7174fc526",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/a583a40693f5c56c40b39fd12cfa0bb7174fc526/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=a583a40693f5c56c40b39fd12cfa0bb7174fc526",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -316,6 +316,8 @@ Release 2.8.0 - UNRELEASED\n     YARN-3343. Increased TestCapacitySchedulerNodeLabelUpdate#testNodeUpdate\n     timeout. (Rohith Sharmaks via jianhe)\n \n+    YARN-3582. NPE in WebAppProxyServlet. (jian he via xgong)\n+\n Release 2.7.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/a583a40693f5c56c40b39fd12cfa0bb7174fc526/hadoop-yarn-project/CHANGES.txt",
                "sha": "97b7ee4df5db98fcf21db191ca504bf1ac5221af",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/a583a40693f5c56c40b39fd12cfa0bb7174fc526/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java?ref=a583a40693f5c56c40b39fd12cfa0bb7174fc526",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java",
                "patch": "@@ -248,8 +248,11 @@ protected void doGet(HttpServletRequest req, HttpServletResponse resp)\n       final String remoteUser = req.getRemoteUser();\n       final String pathInfo = req.getPathInfo();\n \n-      String[] parts = pathInfo.split(\"/\", 3);\n-      if(parts.length < 2) {\n+      String[] parts = null;\n+      if (pathInfo != null) {\n+        parts = pathInfo.split(\"/\", 3);\n+      }\n+      if(parts == null || parts.length < 2) {\n         LOG.warn(\"{} gave an invalid proxy path {}\", remoteUser,  pathInfo);\n         notFound(resp, \"Your path appears to be formatted incorrectly.\");\n         return;",
                "raw_url": "https://github.com/apache/hadoop/raw/a583a40693f5c56c40b39fd12cfa0bb7174fc526/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java",
                "sha": "d45beb68d777b59ce4dfdc0917073d788679956e",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/a583a40693f5c56c40b39fd12cfa0bb7174fc526/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/test/java/org/apache/hadoop/yarn/server/webproxy/TestWebAppProxyServlet.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/test/java/org/apache/hadoop/yarn/server/webproxy/TestWebAppProxyServlet.java?ref=a583a40693f5c56c40b39fd12cfa0bb7174fc526",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/test/java/org/apache/hadoop/yarn/server/webproxy/TestWebAppProxyServlet.java",
                "patch": "@@ -131,6 +131,13 @@ public void testWebAppProxyServlet() throws Exception {\n \n     // wrong url\n     try {\n+      // wrong url without app ID\n+      URL emptyUrl = new URL(\"http://localhost:\" + proxyPort + \"/proxy\");\n+      HttpURLConnection emptyProxyConn = (HttpURLConnection) emptyUrl\n+          .openConnection();\n+      emptyProxyConn.connect();;\n+      assertEquals(HttpURLConnection.HTTP_NOT_FOUND, emptyProxyConn.getResponseCode());\n+\n       // wrong url. Set wrong app ID\n       URL wrongUrl = new URL(\"http://localhost:\" + proxyPort + \"/proxy/app\");\n       HttpURLConnection proxyConn = (HttpURLConnection) wrongUrl",
                "raw_url": "https://github.com/apache/hadoop/raw/a583a40693f5c56c40b39fd12cfa0bb7174fc526/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/test/java/org/apache/hadoop/yarn/server/webproxy/TestWebAppProxyServlet.java",
                "sha": "2a2ca2ca3d2f57b3d8d0540c8590fb8dcc401ba8",
                "status": "modified"
            }
        ],
        "message": "YARN-3582. NPE in WebAppProxyServlet. Contributed by Jian He",
        "parent": "https://github.com/apache/hadoop/commit/90b384564875bb353224630e501772b46d4ca9c5",
        "repo": "hadoop",
        "unit_tests": [
            "TestWebAppProxyServlet.java"
        ]
    },
    "hadoop_a7e450c": {
        "bug_id": "hadoop_a7e450c",
        "commit": "https://github.com/apache/hadoop/commit/a7e450c7cc0adef3ff832368a2a041bb51396945",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/a7e450c7cc0adef3ff832368a2a041bb51396945/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=a7e450c7cc0adef3ff832368a2a041bb51396945",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -234,6 +234,10 @@ Trunk (Unreleased)\n     HADOOP-8815. RandomDatum needs to override hashCode().\n     (Brandon Li via suresh)\n \n+    HADOOP-8436. NPE In getLocalPathForWrite ( path, conf ) when the\n+    required context item is not configured\n+    (Brahma Reddy Battula via harsh)\n+\n   OPTIMIZATIONS\n \n     HADOOP-7761. Improve the performance of raw comparisons. (todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/a7e450c7cc0adef3ff832368a2a041bb51396945/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "5b4066b80f22163dc57d54001f70fd088847f806",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/a7e450c7cc0adef3ff832368a2a041bb51396945/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java?ref=a7e450c7cc0adef3ff832368a2a041bb51396945",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java",
                "patch": "@@ -265,6 +265,9 @@ public AllocatorPerContext(String contextCfgItemName) {\n     private synchronized void confChanged(Configuration conf) \n         throws IOException {\n       String newLocalDirs = conf.get(contextCfgItemName);\n+      if (null == newLocalDirs) {\n+        throw new IOException(contextCfgItemName + \" not configured\");\n+      }\n       if (!newLocalDirs.equals(savedLocalDirs)) {\n         localDirs = StringUtils.getTrimmedStrings(newLocalDirs);\n         localFS = FileSystem.getLocal(conf);",
                "raw_url": "https://github.com/apache/hadoop/raw/a7e450c7cc0adef3ff832368a2a041bb51396945/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/LocalDirAllocator.java",
                "sha": "16a1c99b5c0ca566a6bce500f9ecdc44063a5bff",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop/blob/a7e450c7cc0adef3ff832368a2a041bb51396945/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java?ref=a7e450c7cc0adef3ff832368a2a041bb51396945",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java",
                "patch": "@@ -293,6 +293,23 @@ public void testLocalPathForWriteDirCreation() throws IOException {\n     }\n   }\n \n+  /*\n+   * Test when mapred.local.dir not configured and called\n+   * getLocalPathForWrite\n+   */\n+  @Test\n+  public void testShouldNotthrowNPE() throws Exception {\n+    Configuration conf1 = new Configuration();\n+    try {\n+      dirAllocator.getLocalPathForWrite(\"/test\", conf1);\n+      fail(\"Exception not thrown when \" + CONTEXT + \" is not set\");\n+    } catch (IOException e) {\n+      assertEquals(CONTEXT + \" not configured\", e.getMessage());\n+    } catch (NullPointerException e) {\n+      fail(\"Lack of configuration should not have thrown an NPE.\");\n+    }\n+  }\n+\n   /** Test no side effect files are left over. After creating a temp\n    * temp file, remove both the temp file and its parent. Verify that\n    * no files or directories are left over as can happen when File objects",
                "raw_url": "https://github.com/apache/hadoop/raw/a7e450c7cc0adef3ff832368a2a041bb51396945/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/fs/TestLocalDirAllocator.java",
                "sha": "7a4618e650b26f9419ce26022863a3a8c45027ae",
                "status": "modified"
            }
        ],
        "message": "HADOOP-8436. NPE In getLocalPathForWrite ( path, conf ) when the required context item is not configured. Contributed by Brahma Reddy Battula. (harsh)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1389799 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/d50e06d9e0232dccf5681ae7dda5996046e7eb1c",
        "repo": "hadoop",
        "unit_tests": [
            "TestLocalDirAllocator.java"
        ]
    },
    "hadoop_a8d60f4": {
        "bug_id": "hadoop_a8d60f4",
        "commit": "https://github.com/apache/hadoop/commit/a8d60f4190a3a5f7a88c04f30bf61052c53f2b44",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/a8d60f4190a3a5f7a88c04f30bf61052c53f2b44/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=a8d60f4190a3a5f7a88c04f30bf61052c53f2b44",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -148,6 +148,8 @@ Trunk (Unreleased)\n \n   BUG FIXES\n \n+    HADOOP-8419. Fixed GzipCode NPE reset for IBM JDK. (Yu Li via eyang)\n+\n     HADOOP-9041. FsUrlStreamHandlerFactory could cause an infinite loop in\n     FileSystem initialization. (Yanbo Liang and Radim Kolar via llu)\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/a8d60f4190a3a5f7a88c04f30bf61052c53f2b44/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "1d9febccf2eccfb1751aa062dded300ba7728be2",
                "status": "modified"
            },
            {
                "additions": 62,
                "blob_url": "https://github.com/apache/hadoop/blob/a8d60f4190a3a5f7a88c04f30bf61052c53f2b44/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/GzipCodec.java",
                "changes": 64,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/GzipCodec.java?ref=a8d60f4190a3a5f7a88c04f30bf61052c53f2b44",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/GzipCodec.java",
                "patch": "@@ -40,14 +40,74 @@\n   protected static class GzipOutputStream extends CompressorStream {\n \n     private static class ResetableGZIPOutputStream extends GZIPOutputStream {\n-      \n+      private static final int TRAILER_SIZE = 8;\n+      public static final String JVMVendor= System.getProperty(\"java.vendor\");\n+      public static final String JVMVersion= System.getProperty(\"java.version\");\n+      private static final boolean HAS_BROKEN_FINISH =\n+          (JVMVendor.contains(\"IBM\") && JVMVersion.contains(\"1.6.0\"));\n+\n       public ResetableGZIPOutputStream(OutputStream out) throws IOException {\n         super(out);\n       }\n-      \n+\n       public void resetState() throws IOException {\n         def.reset();\n       }\n+\n+      /**\n+       * Override this method for HADOOP-8419.\n+       * Override because IBM implementation calls def.end() which\n+       * causes problem when reseting the stream for reuse.\n+       *\n+       */\n+      @Override\n+      public void finish() throws IOException {\n+        if (HAS_BROKEN_FINISH) {\n+          if (!def.finished()) {\n+            def.finish();\n+            while (!def.finished()) {\n+              int i = def.deflate(this.buf, 0, this.buf.length);\n+              if ((def.finished()) && (i <= this.buf.length - TRAILER_SIZE)) {\n+                writeTrailer(this.buf, i);\n+                i += TRAILER_SIZE;\n+                out.write(this.buf, 0, i);\n+\n+                return;\n+              }\n+              if (i > 0) {\n+                out.write(this.buf, 0, i);\n+              }\n+            }\n+\n+            byte[] arrayOfByte = new byte[TRAILER_SIZE];\n+            writeTrailer(arrayOfByte, 0);\n+            out.write(arrayOfByte);\n+          }\n+        } else {\n+          super.finish();\n+        }\n+      }\n+\n+      /** re-implement for HADOOP-8419 because the relative method in jdk is invisible */\n+      private void writeTrailer(byte[] paramArrayOfByte, int paramInt)\n+        throws IOException {\n+        writeInt((int)this.crc.getValue(), paramArrayOfByte, paramInt);\n+        writeInt(this.def.getTotalIn(), paramArrayOfByte, paramInt + 4);\n+      }\n+\n+      /** re-implement for HADOOP-8419 because the relative method in jdk is invisible */\n+      private void writeInt(int paramInt1, byte[] paramArrayOfByte, int paramInt2)\n+        throws IOException {\n+        writeShort(paramInt1 & 0xFFFF, paramArrayOfByte, paramInt2);\n+        writeShort(paramInt1 >> 16 & 0xFFFF, paramArrayOfByte, paramInt2 + 2);\n+      }\n+\n+      /** re-implement for HADOOP-8419 because the relative method in jdk is invisible */\n+      private void writeShort(int paramInt1, byte[] paramArrayOfByte, int paramInt2)\n+        throws IOException {\n+        paramArrayOfByte[paramInt2] = (byte)(paramInt1 & 0xFF);\n+        paramArrayOfByte[(paramInt2 + 1)] = (byte)(paramInt1 >> 8 & 0xFF);\n+      }\n     }\n \n     public GzipOutputStream(OutputStream out) throws IOException {",
                "raw_url": "https://github.com/apache/hadoop/raw/a8d60f4190a3a5f7a88c04f30bf61052c53f2b44/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/GzipCodec.java",
                "sha": "6ac692c14e7e97cc97beaeeb54a1e9b239e3b914",
                "status": "modified"
            }
        ],
        "message": "HADOOP-8419. Fixed GzipCode NPE reset for IBM JDK. (Yu Li via eyang)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1431739 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/be5509c53743a0beddda3f5798e72b919e797bd0",
        "repo": "hadoop",
        "unit_tests": [
            "TestGzipCodec.java"
        ]
    },
    "hadoop_a94b6a0": {
        "bug_id": "hadoop_a94b6a0",
        "commit": "https://github.com/apache/hadoop/commit/a94b6a0529a87577142aa16a53cf54d4dd4596ba",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/a94b6a0529a87577142aa16a53cf54d4dd4596ba/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/CHANGES.txt?ref=a94b6a0529a87577142aa16a53cf54d4dd4596ba",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -12,6 +12,9 @@ Trunk (unreleased changes)\n \n   NEW FEATURES\n \n+    HADOOP-7342. Add an utility API in FileUtil for JDK File.list\n+    avoid NPEs on File.list() (Bharath Mundlapudi via mattf)\n+\n     HADOOP-7322. Adding a util method in FileUtil for directory listing,\n     avoid NPEs on File.listFiles() (Bharath Mundlapudi via mattf)\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/a94b6a0529a87577142aa16a53cf54d4dd4596ba/CHANGES.txt",
                "sha": "16152a6c9f688f892ae747b874e7f6f7105ac83b",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/a94b6a0529a87577142aa16a53cf54d4dd4596ba/src/java/org/apache/hadoop/fs/FileUtil.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/org/apache/hadoop/fs/FileUtil.java?ref=a94b6a0529a87577142aa16a53cf54d4dd4596ba",
                "deletions": 0,
                "filename": "src/java/org/apache/hadoop/fs/FileUtil.java",
                "patch": "@@ -728,4 +728,23 @@ public static void replaceFile(File src, File target) throws IOException {\n     }\n     return files;\n   }  \n+  \n+  /**\n+   * A wrapper for {@link File#list()}. This java.io API returns null \n+   * when a dir is not a directory or for any I/O error. Instead of having\n+   * null check everywhere File#list() is used, we will add utility API\n+   * to get around this problem. For the majority of cases where we prefer \n+   * an IOException to be thrown.\n+   * @param dir directory for which listing should be performed\n+   * @return list of file names or empty string list\n+   * @exception IOException for invalid directory or for a bad disk.\n+   */\n+  public static String[] list(File dir) throws IOException {\n+    String[] fileNames = dir.list();\n+    if(fileNames == null) {\n+      throw new IOException(\"Invalid directory or I/O error occurred for dir: \"\n+                + dir.toString());\n+    }\n+    return fileNames;\n+  }  \n }",
                "raw_url": "https://github.com/apache/hadoop/raw/a94b6a0529a87577142aa16a53cf54d4dd4596ba/src/java/org/apache/hadoop/fs/FileUtil.java",
                "sha": "86956e368c079cfa9e221c8317550404fe53a29e",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop/blob/a94b6a0529a87577142aa16a53cf54d4dd4596ba/src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/test/core/org/apache/hadoop/fs/TestFileUtil.java?ref=a94b6a0529a87577142aa16a53cf54d4dd4596ba",
                "deletions": 0,
                "filename": "src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "patch": "@@ -143,6 +143,33 @@ public void testListFiles() throws IOException {\n     \t//Expected an IOException\n     }\n   }\n+\n+  @Test\n+  public void testListAPI() throws IOException {\n+    setupDirs();\n+    //Test existing files case \n+    String[] files = FileUtil.list(partitioned);\n+    Assert.assertEquals(\"Unexpected number of pre-existing files\", 2, files.length);\n+\n+    //Test existing directory with no files case \n+    File newDir = new File(tmp.getPath(),\"test\");\n+    newDir.mkdir();\n+    Assert.assertTrue(\"Failed to create test dir\", newDir.exists());\n+    files = FileUtil.list(newDir);\n+    Assert.assertEquals(\"New directory unexpectedly contains files\", 0, files.length);\n+    newDir.delete();\n+    Assert.assertFalse(\"Failed to delete test dir\", newDir.exists());\n+    \n+    //Test non-existing directory case, this throws \n+    //IOException\n+    try {\n+      files = FileUtil.list(newDir);\n+      Assert.fail(\"IOException expected on list() for non-existent dir \"\n+          + newDir.toString());\n+    } catch(IOException ioe) {\n+      //Expected an IOException\n+    }\n+  }\n   \n   @After\n   public void tearDown() throws IOException {",
                "raw_url": "https://github.com/apache/hadoop/raw/a94b6a0529a87577142aa16a53cf54d4dd4596ba/src/test/core/org/apache/hadoop/fs/TestFileUtil.java",
                "sha": "65c43435b75b9fb79d8be4e247a5f8175da3bf22",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7342. Add an utility API in FileUtil for JDK File.list avoid NPEs on File.list().  Contributed by Bharath Mundlapudi.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1131330 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/babd19de331c875a1dffee908617c07c3e1eb31b",
        "repo": "hadoop",
        "unit_tests": [
            "TestFileUtil.java"
        ]
    },
    "hadoop_a9dc5cd": {
        "bug_id": "hadoop_a9dc5cd",
        "commit": "https://github.com/apache/hadoop/commit/a9dc5cd7069f721e8c55794b877026ba02537167",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/a9dc5cd7069f721e8c55794b877026ba02537167/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=a9dc5cd7069f721e8c55794b877026ba02537167",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -890,6 +890,9 @@ Release 2.7.0 - UNRELEASED\n     HDFS-7756. Restore method signature for LocatedBlock#getLocations(). (Ted\n     Yu via yliu)\n \n+    HDFS-7744. Fix potential NPE in DFSInputStream after setDropBehind or\n+    setReadahead is called (cmccabe)\n+\n Release 2.6.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/a9dc5cd7069f721e8c55794b877026ba02537167/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "446c6a3a50d827872020128d58d3cca5f0105284",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/a9dc5cd7069f721e8c55794b877026ba02537167/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java?ref=a9dc5cd7069f721e8c55794b877026ba02537167",
                "deletions": 8,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "patch": "@@ -580,10 +580,7 @@ private synchronized DatanodeInfo blockSeekTo(long target) throws IOException {\n     }\n \n     // Will be getting a new BlockReader.\n-    if (blockReader != null) {\n-      blockReader.close();\n-      blockReader = null;\n-    }\n+    closeCurrentBlockReader();\n \n     //\n     // Connect to best DataNode for desired Block, with potential offset\n@@ -686,10 +683,7 @@ public void accept(ByteBuffer k, Object v) {\n           \"unreleased ByteBuffers allocated by read().  \" +\n           \"Please release \" + builder.toString() + \".\");\n     }\n-    if (blockReader != null) {\n-      blockReader.close();\n-      blockReader = null;\n-    }\n+    closeCurrentBlockReader();\n     super.close();\n   }\n \n@@ -1649,6 +1643,7 @@ private void closeCurrentBlockReader() {\n       DFSClient.LOG.error(\"error closing blockReader\", e);\n     }\n     blockReader = null;\n+    blockEnd = -1;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/a9dc5cd7069f721e8c55794b877026ba02537167/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "sha": "618f0407dfe968a1c73a10094d36e3b13af07761",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hadoop/blob/a9dc5cd7069f721e8c55794b877026ba02537167/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestCachingStrategy.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestCachingStrategy.java?ref=a9dc5cd7069f721e8c55794b877026ba02537167",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestCachingStrategy.java",
                "patch": "@@ -369,4 +369,34 @@ public void testNoFadviseAfterWriteThenRead() throws Exception {\n       }\n     }\n   }\n+\n+  @Test(timeout=120000)\n+  public void testSeekAfterSetDropBehind() throws Exception {\n+    // start a cluster\n+    LOG.info(\"testSeekAfterSetDropBehind\");\n+    Configuration conf = new HdfsConfiguration();\n+    MiniDFSCluster cluster = null;\n+    String TEST_PATH = \"/test\";\n+    int TEST_PATH_LEN = MAX_TEST_FILE_LEN;\n+    try {\n+      cluster = new MiniDFSCluster.Builder(conf).numDataNodes(1)\n+          .build();\n+      cluster.waitActive();\n+      FileSystem fs = cluster.getFileSystem();\n+      createHdfsFile(fs, new Path(TEST_PATH), TEST_PATH_LEN, false);\n+      // verify that we can seek after setDropBehind\n+      FSDataInputStream fis = fs.open(new Path(TEST_PATH));\n+      try {\n+        Assert.assertTrue(fis.read() != -1); // create BlockReader\n+        fis.setDropBehind(false); // clear BlockReader\n+        fis.seek(2); // seek\n+      } finally {\n+        fis.close();\n+      }\n+    } finally {\n+      if (cluster != null) {\n+        cluster.shutdown();\n+      }\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/a9dc5cd7069f721e8c55794b877026ba02537167/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestCachingStrategy.java",
                "sha": "709554a0ea42f1c241c5773d0efff58e73b98fc1",
                "status": "modified"
            }
        ],
        "message": "HDFS-7744. Fix potential NPE in DFSInputStream after setDropBehind or setReadahead is called (cmccabe)",
        "parent": "https://github.com/apache/hadoop/commit/260b5e32c427d54c8c74b9f84432700317d1f282",
        "repo": "hadoop",
        "unit_tests": [
            "TestDFSInputStream.java"
        ]
    },
    "hadoop_aac5472": {
        "bug_id": "hadoop_aac5472",
        "commit": "https://github.com/apache/hadoop/commit/aac547249134ec4f9d5229d28e4fdeef0f6f0dbb",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/aac547249134ec4f9d5229d28e4fdeef0f6f0dbb/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/CHANGES.txt?ref=aac547249134ec4f9d5229d28e4fdeef0f6f0dbb",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -100,6 +100,9 @@ Trunk (unreleased changes)\n     HADOOP-7131. Exceptions thrown by Text methods should include the causing\n     exception. (Uma Maheswara Rao G via todd)\n \n+    HADOOP-6912. Guard against NPE when calling UGI.isLoginKeytabBased().\n+    (Kan Zhang via jitendra)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/aac547249134ec4f9d5229d28e4fdeef0f6f0dbb/CHANGES.txt",
                "sha": "ead2d3f88a9ba1f5b25e5526666fa9d823086f02",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/aac547249134ec4f9d5229d28e4fdeef0f6f0dbb/src/java/org/apache/hadoop/security/UserGroupInformation.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/src/java/org/apache/hadoop/security/UserGroupInformation.java?ref=aac547249134ec4f9d5229d28e4fdeef0f6f0dbb",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/security/UserGroupInformation.java",
                "patch": "@@ -811,8 +811,8 @@ private boolean hasSufficientTimeElapsed(long now) {\n    * Did the login happen via keytab\n    * @return true or false\n    */\n-  public synchronized static boolean isLoginKeytabBased() {\n-    return loginUser.isKeytab;\n+  public synchronized static boolean isLoginKeytabBased() throws IOException {\n+    return getLoginUser().isKeytab;\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/aac547249134ec4f9d5229d28e4fdeef0f6f0dbb/src/java/org/apache/hadoop/security/UserGroupInformation.java",
                "sha": "085ce61719eefec5cd06738b846867a1335f08cd",
                "status": "modified"
            }
        ],
        "message": "HADOOP-6912. Guard against NPE when calling UGI.isLoginKeytabBased(). Contributed by Kan Zhang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1079068 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/69fe37a007045c3a3cf3b2b410e1ad14717fdb76",
        "repo": "hadoop",
        "unit_tests": [
            "TestUserGroupInformation.java"
        ]
    },
    "hadoop_ac5ae00": {
        "bug_id": "hadoop_ac5ae00",
        "commit": "https://github.com/apache/hadoop/commit/ac5ae0065a127ac150a887fa6c6f3cffd86ef733",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/ac5ae0065a127ac150a887fa6c6f3cffd86ef733/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java?ref=ac5ae0065a127ac150a887fa6c6f3cffd86ef733",
                "deletions": 6,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "patch": "@@ -2282,12 +2282,14 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,\n       if (memBlockInfo.getGenerationStamp() != diskGS) {\n         File memMetaFile = FsDatasetUtil.getMetaFile(diskFile, \n             memBlockInfo.getGenerationStamp());\n-        if (memMetaFile.exists()) {\n-          if (memMetaFile.compareTo(diskMetaFile) != 0) {\n-            LOG.warn(\"Metadata file in memory \"\n-                + memMetaFile.getAbsolutePath()\n-                + \" does not match file found by scan \"\n-                + (diskMetaFile == null? null: diskMetaFile.getAbsolutePath()));\n+        if (fileIoProvider.exists(vol, memMetaFile)) {\n+          String warningPrefix = \"Metadata file in memory \"\n+              + memMetaFile.getAbsolutePath()\n+              + \" does not match file found by scan \";\n+          if (!diskMetaFileExists) {\n+            LOG.warn(warningPrefix + \"null\");\n+          } else if (memMetaFile.compareTo(diskMetaFile) != 0) {\n+            LOG.warn(warningPrefix + diskMetaFile.getAbsolutePath());\n           }\n         } else {\n           // Metadata file corresponding to block in memory is missing",
                "raw_url": "https://github.com/apache/hadoop/raw/ac5ae0065a127ac150a887fa6c6f3cffd86ef733/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "sha": "aff19ce97e0c008722ad6a33bd6a38ae91220d76",
                "status": "modified"
            }
        ],
        "message": "HDFS-11476. Fix NPE in FsDatasetImpl#checkAndUpdate. Contributed by Xiaobing Zhou.",
        "parent": "https://github.com/apache/hadoop/commit/2148b83993fd8ce73bcbc7677c57ee5028a59cd4",
        "repo": "hadoop",
        "unit_tests": [
            "TestFsDatasetImpl.java"
        ]
    },
    "hadoop_adca1a7": {
        "bug_id": "hadoop_adca1a7",
        "commit": "https://github.com/apache/hadoop/commit/adca1a72e4eca2ea634551e9fb8e9b878c36cb5c",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/adca1a72e4eca2ea634551e9fb8e9b878c36cb5c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java?ref=adca1a72e4eca2ea634551e9fb8e9b878c36cb5c",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "patch": "@@ -1301,6 +1301,10 @@ private CSAssignment allocateContainerOnSingleNode(\n     if (reservedContainer != null) {\n       FiCaSchedulerApp reservedApplication = getCurrentAttemptForContainer(\n           reservedContainer.getContainerId());\n+      if (reservedApplication == null) {\n+        LOG.error(\"Trying to schedule for a finished app, please double check.\");\n+        return null;\n+      }\n \n       // Try to fulfill the reservation\n       LOG.info(",
                "raw_url": "https://github.com/apache/hadoop/raw/adca1a72e4eca2ea634551e9fb8e9b878c36cb5c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "sha": "8de363140fb8e943f5b5590765cf76154d2c58bd",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/hadoop/blob/adca1a72e4eca2ea634551e9fb8e9b878c36cb5c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java?ref=adca1a72e4eca2ea634551e9fb8e9b878c36cb5c",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "patch": "@@ -1201,7 +1201,14 @@ public boolean accept(Resource cluster,\n             allocation.getSchedulingMode(), null);\n \n         // Deduct resources that we can release\n-        Resource usedResource = Resources.clone(getUser(username).getUsed(p));\n+        User user = getUser(username);\n+        if (user == null) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"User \" + username + \" has been removed!\");\n+          }\n+          return false;\n+        }\n+        Resource usedResource = Resources.clone(user.getUsed(p));\n         Resources.subtractFrom(usedResource,\n             request.getTotalReleasedResource());\n \n@@ -1406,6 +1413,12 @@ Resource computeUserLimitAndSetHeadroom(FiCaSchedulerApp application,\n       SchedulingMode schedulingMode, Resource userLimit) {\n     String user = application.getUser();\n     User queueUser = getUser(user);\n+    if (queueUser == null) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"User \" + user + \" has been removed!\");\n+      }\n+      return Resources.none();\n+    }\n \n     // Compute user limit respect requested labels,\n     // TODO, need consider headroom respect labels also\n@@ -1500,6 +1513,12 @@ protected boolean canAssignToUser(Resource clusterResource,\n     try {\n       readLock.lock();\n       User user = getUser(userName);\n+      if (user == null) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"User \" + userName + \" has been removed!\");\n+        }\n+        return false;\n+      }\n \n       currentResourceLimits.setAmountNeededUnreserve(Resources.none());\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/adca1a72e4eca2ea634551e9fb8e9b878c36cb5c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "sha": "ac1a26ccef2581dbba85444aa927b65a873d2aba",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/adca1a72e4eca2ea634551e9fb8e9b878c36cb5c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java?ref=adca1a72e4eca2ea634551e9fb8e9b878c36cb5c",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "patch": "@@ -322,6 +322,11 @@ private boolean commonCheckContainerAllocation(\n     RMContainer reservedContainerOnNode =\n         schedulerContainer.getSchedulerNode().getReservedContainer();\n     if (reservedContainerOnNode != null) {\n+      // adding NP check as this proposal could not be allocated from reserved\n+      // container in async-scheduling mode\n+      if (allocation.getAllocateFromReservedContainer() == null) {\n+        return false;\n+      }\n       RMContainer fromReservedContainer =\n           allocation.getAllocateFromReservedContainer().getRmContainer();\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/adca1a72e4eca2ea634551e9fb8e9b878c36cb5c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "sha": "776a7e98af299a70cf9adfae8e6a813b9271c8e3",
                "status": "modified"
            }
        ],
        "message": "YARN-7591. NPE in async-scheduling mode of CapacityScheduler. (Tao Yang via wangda)\n\nChange-Id: I46689e530550ee0a6ac7a29786aab2cc1bdf314f",
        "parent": "https://github.com/apache/hadoop/commit/a8316df8c05a7b3d1a5577174b838711a49ef971",
        "repo": "hadoop",
        "unit_tests": [
            "TestCapacityScheduler.java",
            "TestLeafQueue.java"
        ]
    },
    "hadoop_aed836e": {
        "bug_id": "hadoop_aed836e",
        "commit": "https://github.com/apache/hadoop/commit/aed836efbff775d95899d05ff947f1048df8cf19",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/aed836efbff775d95899d05ff947f1048df8cf19/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/router/LoadBasedRouterPolicy.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/router/LoadBasedRouterPolicy.java?ref=aed836efbff775d95899d05ff947f1048df8cf19",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/router/LoadBasedRouterPolicy.java",
                "patch": "@@ -26,6 +26,7 @@\n import org.apache.hadoop.yarn.server.federation.policies.FederationPolicyInitializationContext;\n import org.apache.hadoop.yarn.server.federation.policies.FederationPolicyUtils;\n import org.apache.hadoop.yarn.server.federation.policies.dao.WeightedPolicyInfo;\n+import org.apache.hadoop.yarn.server.federation.policies.exceptions.FederationPolicyException;\n import org.apache.hadoop.yarn.server.federation.policies.exceptions.FederationPolicyInitializationException;\n import org.apache.hadoop.yarn.server.federation.store.records.SubClusterId;\n import org.apache.hadoop.yarn.server.federation.store.records.SubClusterIdInfo;\n@@ -95,7 +96,10 @@ public SubClusterId getHomeSubcluster(\n         }\n       }\n     }\n-\n+    if (chosen == null) {\n+      throw new FederationPolicyException(\n+          \"Zero Active Subcluster with weight 1.\");\n+    }\n     return chosen.toId();\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/aed836efbff775d95899d05ff947f1048df8cf19/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/main/java/org/apache/hadoop/yarn/server/federation/policies/router/LoadBasedRouterPolicy.java",
                "sha": "fa5eb4be2cfd5f5f4cc625d0548b28d66caab925",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop/blob/aed836efbff775d95899d05ff947f1048df8cf19/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/router/TestLoadBasedRouterPolicy.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/router/TestLoadBasedRouterPolicy.java?ref=aed836efbff775d95899d05ff947f1048df8cf19",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/router/TestLoadBasedRouterPolicy.java",
                "patch": "@@ -17,6 +17,8 @@\n \n package org.apache.hadoop.yarn.server.federation.policies.router;\n \n+import static org.junit.Assert.fail;\n+\n import java.util.HashMap;\n import java.util.Map;\n \n@@ -103,4 +105,33 @@ public void testLoadIsRespected() throws YarnException {\n     Assert.assertEquals(\"sc05\", chosen.getId());\n   }\n \n+  @Test\n+  public void testIfNoSubclustersWithWeightOne() {\n+    setPolicy(new LoadBasedRouterPolicy());\n+    setPolicyInfo(new WeightedPolicyInfo());\n+    Map<SubClusterIdInfo, Float> routerWeights = new HashMap<>();\n+    Map<SubClusterIdInfo, Float> amrmWeights = new HashMap<>();\n+    // update subcluster with weight 0\n+    SubClusterIdInfo sc = new SubClusterIdInfo(String.format(\"sc%02d\", 0));\n+    SubClusterInfo federationSubClusterInfo = SubClusterInfo.newInstance(\n+        sc.toId(), null, null, null, null, -1, SubClusterState.SC_RUNNING, -1,\n+        generateClusterMetricsInfo(0));\n+    getActiveSubclusters().clear();\n+    getActiveSubclusters().put(sc.toId(), federationSubClusterInfo);\n+    routerWeights.put(sc, 0.0f);\n+    amrmWeights.put(sc, 0.0f);\n+    getPolicyInfo().setRouterPolicyWeights(routerWeights);\n+    getPolicyInfo().setAMRMPolicyWeights(amrmWeights);\n+\n+    try {\n+      FederationPoliciesTestUtil.initializePolicyContext(getPolicy(),\n+          getPolicyInfo(), getActiveSubclusters());\n+      ((FederationRouterPolicy) getPolicy())\n+          .getHomeSubcluster(getApplicationSubmissionContext(), null);\n+      fail();\n+    } catch (YarnException ex) {\n+      Assert.assertTrue(\n+          ex.getMessage().contains(\"Zero Active Subcluster with weight 1\"));\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/aed836efbff775d95899d05ff947f1048df8cf19/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/server/federation/policies/router/TestLoadBasedRouterPolicy.java",
                "sha": "58f1b9947bd81377978b0b00620c385c6e7cfa43",
                "status": "modified"
            }
        ],
        "message": "YARN-8897. LoadBasedRouterPolicy throws NPE in case of sub cluster unavailability. Contributed by Bilwa S T.",
        "parent": "https://github.com/apache/hadoop/commit/babc946d4017e9c385d19a8e6f7f1ecd5080d619",
        "repo": "hadoop",
        "unit_tests": [
            "TestLoadBasedRouterPolicy.java"
        ]
    },
    "hadoop_af61f4a": {
        "bug_id": "hadoop_af61f4a",
        "commit": "https://github.com/apache/hadoop/commit/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=af61f4ae15adf3bf6c863945f8c8e3ea7b12320c",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -1384,6 +1384,9 @@ Release 0.23.0 - Unreleased\n     MAPREDUCE-3023. Fixed clients to display queue state correctly. (Ravi\n     Prakash via acmurthy) \n \n+    MAPREDUCE-2970. Fixed NPEs in corner cases with different configurations\n+    for mapreduce.framework.name. (Venu Gopala Rao via vinodkv)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "36a5f24c037c8d3caf8b79c48d22df7ff361a02d",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop/blob/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java?ref=af61f4ae15adf3bf6c863945f8c8e3ea7b12320c",
                "deletions": 13,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java",
                "patch": "@@ -41,8 +41,8 @@\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.security.AccessControlException;\n import org.apache.hadoop.security.UserGroupInformation;\n-import org.apache.hadoop.security.token.Token;\n import org.apache.hadoop.security.token.SecretManager.InvalidToken;\n+import org.apache.hadoop.security.token.Token;\n \n /**\n  * Provides a way to access information about the map/reduce cluster.\n@@ -68,30 +68,41 @@\n   }\n   \n   public Cluster(Configuration conf) throws IOException {\n-    this.conf = conf;\n-    this.ugi = UserGroupInformation.getCurrentUser();\n-    for (ClientProtocolProvider provider : ServiceLoader.load(ClientProtocolProvider.class)) {\n-      ClientProtocol clientProtocol = provider.create(conf);\n-      if (clientProtocol != null) {\n-        clientProtocolProvider = provider;\n-        client = clientProtocol;\n-        break;\n-      }\n-    }\n+    this(null, conf);\n   }\n \n   public Cluster(InetSocketAddress jobTrackAddr, Configuration conf) \n       throws IOException {\n     this.conf = conf;\n     this.ugi = UserGroupInformation.getCurrentUser();\n-    for (ClientProtocolProvider provider : ServiceLoader.load(ClientProtocolProvider.class)) {\n-      ClientProtocol clientProtocol = provider.create(jobTrackAddr, conf);\n+    initialize(jobTrackAddr, conf);\n+  }\n+  \n+  private void initialize(InetSocketAddress jobTrackAddr, Configuration conf)\n+      throws IOException {\n+\n+    for (ClientProtocolProvider provider : ServiceLoader\n+        .load(ClientProtocolProvider.class)) {\n+      ClientProtocol clientProtocol = null;\n+      if (jobTrackAddr == null) {\n+        clientProtocol = provider.create(conf);\n+      } else {\n+        clientProtocol = provider.create(jobTrackAddr, conf);\n+      }\n+\n       if (clientProtocol != null) {\n         clientProtocolProvider = provider;\n         client = clientProtocol;\n         break;\n       }\n     }\n+\n+    if (null == clientProtocolProvider || null == client) {\n+      throw new IOException(\n+          \"Cannot initialize Cluster. Please check your configuration for \"\n+              + MRConfig.FRAMEWORK_NAME\n+              + \" and the correspond server addresses.\");\n+    }\n   }\n \n   ClientProtocol getClient() {",
                "raw_url": "https://github.com/apache/hadoop/raw/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Cluster.java",
                "sha": "33d5f81b4fc3b5120fc28035823f38a43da848d3",
                "status": "modified"
            },
            {
                "additions": 59,
                "blob_url": "https://github.com/apache/hadoop/blob/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestYarnClientProtocolProvider.java",
                "changes": 59,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestYarnClientProtocolProvider.java?ref=af61f4ae15adf3bf6c863945f8c8e3ea7b12320c",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestYarnClientProtocolProvider.java",
                "patch": "@@ -0,0 +1,59 @@\n+/**\r\n+ * Licensed to the Apache Software Foundation (ASF) under one\r\n+ * or more contributor license agreements.  See the NOTICE file\r\n+ * distributed with this work for additional information\r\n+ * regarding copyright ownership.  The ASF licenses this file\r\n+ * to you under the Apache License, Version 2.0 (the\r\n+ * \"License\"); you may not use this file except in compliance\r\n+ * with the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *     http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.hadoop.mapreduce;\r\n+\r\n+import java.io.IOException;\r\n+\r\n+import junit.framework.TestCase;\r\n+\r\n+import org.apache.hadoop.conf.Configuration;\r\n+import org.apache.hadoop.mapred.YARNRunner;\r\n+import org.apache.hadoop.mapreduce.protocol.ClientProtocol;\r\n+import org.junit.Test;\r\n+\r\n+public class TestYarnClientProtocolProvider extends TestCase {\r\n+\r\n+  @Test\r\n+  public void testClusterWithYarnClientProtocolProvider() throws Exception {\r\n+\r\n+    Configuration conf = new Configuration(false);\r\n+    Cluster cluster = null;\r\n+\r\n+    try {\r\n+      cluster = new Cluster(conf);\r\n+      fail(\"Cluster should not be initialized with out any framework name\");\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+\r\n+    try {\r\n+      conf = new Configuration();\r\n+      conf.set(MRConfig.FRAMEWORK_NAME, MRConfig.YARN_FRAMEWORK_NAME);\r\n+      cluster = new Cluster(conf);\r\n+      ClientProtocol client = cluster.getClient();\r\n+      assertTrue(client instanceof YARNRunner);\r\n+    } catch (IOException e) {\r\n+\r\n+    } finally {\r\n+      if (cluster != null) {\r\n+        cluster.close();\r\n+      }\r\n+    }\r\n+  }\r\n+}\r",
                "raw_url": "https://github.com/apache/hadoop/raw/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/TestYarnClientProtocolProvider.java",
                "sha": "2bc9030bf85ea491055dbced5260c7f3b728459f",
                "status": "added"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTrackerClientProtocolProvider.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTrackerClientProtocolProvider.java?ref=af61f4ae15adf3bf6c863945f8c8e3ea7b12320c",
                "deletions": 5,
                "filename": "hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTrackerClientProtocolProvider.java",
                "patch": "@@ -43,20 +43,24 @@ public ClientProtocol create(Configuration conf) throws IOException {\n     String tracker = conf.get(JTConfig.JT_IPC_ADDRESS, \"local\");\n     if (!\"local\".equals(tracker)) {\n       return createRPCProxy(JobTracker.getAddress(conf), conf);\n+    } else {\n+      throw new IOException(\"Invalid \\\"\" + JTConfig.JT_IPC_ADDRESS\n+          + \"\\\" configuration value for JobTracker: \\\"\"\n+          + tracker + \"\\\"\");\n     }\n-    return null;\n   }\n \n   @Override\n-  public ClientProtocol create(InetSocketAddress addr, Configuration conf) throws IOException {\n+  public ClientProtocol create(InetSocketAddress addr, Configuration conf)\n+      throws IOException {\n     return createRPCProxy(addr, conf);\n   }\n-  \n+\n   private ClientProtocol createRPCProxy(InetSocketAddress addr,\n       Configuration conf) throws IOException {\n     return (ClientProtocol) RPC.getProxy(ClientProtocol.class,\n-      ClientProtocol.versionID, addr, UserGroupInformation.getCurrentUser(),\n-      conf, NetUtils.getSocketFactory(conf, ClientProtocol.class));\n+        ClientProtocol.versionID, addr, UserGroupInformation.getCurrentUser(),\n+        conf, NetUtils.getSocketFactory(conf, ClientProtocol.class));\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobTrackerClientProtocolProvider.java",
                "sha": "d12132c68d24c5b0cc793c05818cb7aa016f95b5",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalClientProtocolProvider.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalClientProtocolProvider.java?ref=af61f4ae15adf3bf6c863945f8c8e3ea7b12320c",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalClientProtocolProvider.java",
                "patch": "@@ -37,11 +37,16 @@ public ClientProtocol create(Configuration conf) throws IOException {\n     if (framework != null && !framework.equals(\"local\")) {\n       return null;\n     }\n-    if (\"local\".equals(conf.get(JTConfig.JT_IPC_ADDRESS, \"local\"))) {\n+    String tracker = conf.get(JTConfig.JT_IPC_ADDRESS, \"local\");\n+    if (\"local\".equals(tracker)) {\n       conf.setInt(\"mapreduce.job.maps\", 1);\n       return new LocalJobRunner(conf);\n+    } else {\n+\n+      throw new IOException(\"Invalid \\\"\" + JTConfig.JT_IPC_ADDRESS\n+          + \"\\\" configuration value for LocalJobRunner : \\\"\"\n+          + tracker + \"\\\"\");\n     }\n-    return null;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/LocalClientProtocolProvider.java",
                "sha": "d09b222ee9b2ed9a6972d139e0da0fa66792c420",
                "status": "modified"
            },
            {
                "additions": 99,
                "blob_url": "https://github.com/apache/hadoop/blob/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapreduce/TestClientProtocolProviderImpls.java",
                "changes": 99,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapreduce/TestClientProtocolProviderImpls.java?ref=af61f4ae15adf3bf6c863945f8c8e3ea7b12320c",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapreduce/TestClientProtocolProviderImpls.java",
                "patch": "@@ -0,0 +1,99 @@\n+/**\r\n+ * Licensed to the Apache Software Foundation (ASF) under one\r\n+ * or more contributor license agreements.  See the NOTICE file\r\n+ * distributed with this work for additional information\r\n+ * regarding copyright ownership.  The ASF licenses this file\r\n+ * to you under the Apache License, Version 2.0 (the\r\n+ * \"License\"); you may not use this file except in compliance\r\n+ * with the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *     http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+\r\n+package org.apache.hadoop.mapreduce;\r\n+\r\n+import java.io.IOException;\r\n+\r\n+import junit.framework.TestCase;\r\n+\r\n+import org.apache.hadoop.conf.Configuration;\r\n+import org.apache.hadoop.mapred.LocalJobRunner;\r\n+import org.apache.hadoop.mapreduce.server.jobtracker.JTConfig;\r\n+import org.junit.Test;\r\n+\r\n+public class TestClientProtocolProviderImpls extends TestCase {\r\n+\r\n+  @Test\r\n+  public void testClusterWithLocalClientProvider() throws Exception {\r\n+\r\n+    Configuration conf = new Configuration();\r\n+\r\n+    try {\r\n+      conf.set(MRConfig.FRAMEWORK_NAME, \"incorrect\");\r\n+      new Cluster(conf);\r\n+      fail(\"Cluster should not be initialized with incorrect framework name\");\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+\r\n+    try {\r\n+      conf.set(MRConfig.FRAMEWORK_NAME, \"local\");\r\n+      conf.set(JTConfig.JT_IPC_ADDRESS, \"127.0.0.1:0\");\r\n+\r\n+      new Cluster(conf);\r\n+      fail(\"Cluster with Local Framework name should use local JT address\");\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+\r\n+    try {\r\n+      conf.set(JTConfig.JT_IPC_ADDRESS, \"local\");\r\n+      Cluster cluster = new Cluster(conf);\r\n+      assertTrue(cluster.getClient() instanceof LocalJobRunner);\r\n+      cluster.close();\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+  }\r\n+\r\n+  @Test\r\n+  public void testClusterWithJTClientProvider() throws Exception {\r\n+\r\n+    Configuration conf = new Configuration();\r\n+    try {\r\n+      conf.set(MRConfig.FRAMEWORK_NAME, \"incorrect\");\r\n+      new Cluster(conf);\r\n+      fail(\"Cluster should not be initialized with incorrect framework name\");\r\n+\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+\r\n+    try {\r\n+      conf.set(MRConfig.FRAMEWORK_NAME, \"classic\");\r\n+      conf.set(JTConfig.JT_IPC_ADDRESS, \"local\");\r\n+      new Cluster(conf);\r\n+      fail(\"Cluster with classic Framework name shouldnot use local JT address\");\r\n+\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+\r\n+    try {\r\n+      conf = new Configuration();\r\n+      conf.set(MRConfig.FRAMEWORK_NAME, \"classic\");\r\n+      conf.set(JTConfig.JT_IPC_ADDRESS, \"127.0.0.1:0\");\r\n+      Cluster cluster = new Cluster(conf);\r\n+      cluster.close();\r\n+    } catch (IOException e) {\r\n+\r\n+    }\r\n+  }\r\n+\r\n+}\r",
                "raw_url": "https://github.com/apache/hadoop/raw/af61f4ae15adf3bf6c863945f8c8e3ea7b12320c/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapreduce/TestClientProtocolProviderImpls.java",
                "sha": "a9044e24308133a4d0902e600a3a34fbaa9e93b5",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-2970. Fixed NPEs in corner cases with different configurations for mapreduce.framework.name. Contributed by Venu Gopala Rao.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1173534 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/01fbb0fb4502dfa6bd8f76a4dfe7dfd0033e7d62",
        "repo": "hadoop",
        "unit_tests": [
            "TestCluster.java"
        ]
    },
    "hadoop_b061215": {
        "bug_id": "hadoop_b061215",
        "commit": "https://github.com/apache/hadoop/commit/b061215ecfebe476bf58f70788113d1af816f553",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ClientContext.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ClientContext.java?ref=b061215ecfebe476bf58f70788113d1af816f553",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ClientContext.java",
                "patch": "@@ -20,6 +20,7 @@\n import static org.apache.hadoop.fs.CommonConfigurationKeys.FS_CLIENT_TOPOLOGY_RESOLUTION_ENABLED;\n import static org.apache.hadoop.fs.CommonConfigurationKeys.FS_CLIENT_TOPOLOGY_RESOLUTION_ENABLED_DEFAULT;\n \n+import java.io.IOException;\n import java.util.ArrayList;\n import java.util.HashMap;\n import java.util.List;\n@@ -238,7 +239,7 @@ public ByteArrayManager getByteArrayManager() {\n     return byteArrayManager;\n   }\n \n-  public int getNetworkDistance(DatanodeInfo datanodeInfo) {\n+  public int getNetworkDistance(DatanodeInfo datanodeInfo) throws IOException {\n     // If applications disable the feature or the client machine can't\n     // resolve its network location, clientNode will be set to null.\n     if (clientNode == null) {",
                "raw_url": "https://github.com/apache/hadoop/raw/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/ClientContext.java",
                "sha": "ad1b3595cbd5b8efdc29f11b4a88bcfe394cc9d3",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSUtilClient.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSUtilClient.java?ref=b061215ecfebe476bf58f70788113d1af816f553",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSUtilClient.java",
                "patch": "@@ -550,7 +550,11 @@ public static String dateToIso8601String(Date date) {\n   private static final Map<String, Boolean> localAddrMap = Collections\n       .synchronizedMap(new HashMap<String, Boolean>());\n \n-  public static boolean isLocalAddress(InetSocketAddress targetAddr) {\n+  public static boolean isLocalAddress(InetSocketAddress targetAddr)\n+      throws IOException {\n+    if (targetAddr.isUnresolved()) {\n+      throw new IOException(\"Unresolved host: \" + targetAddr);\n+    }\n     InetAddress addr = targetAddr.getAddress();\n     Boolean cached = localAddrMap.get(addr.getHostAddress());\n     if (cached != null) {",
                "raw_url": "https://github.com/apache/hadoop/raw/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/DFSUtilClient.java",
                "sha": "2edd7554e838ae738401b97b620d70d1f2de9e24",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop/blob/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java?ref=b061215ecfebe476bf58f70788113d1af816f553",
                "deletions": 18,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java",
                "patch": "@@ -357,28 +357,32 @@ public BlockReader build() throws IOException {\n       return reader;\n     }\n     final ShortCircuitConf scConf = conf.getShortCircuitConf();\n-    if (scConf.isShortCircuitLocalReads() && allowShortCircuitLocalReads) {\n-      if (clientContext.getUseLegacyBlockReaderLocal()) {\n-        reader = getLegacyBlockReaderLocal();\n-        if (reader != null) {\n-          LOG.trace(\"{}: returning new legacy block reader local.\", this);\n-          return reader;\n+    try {\n+      if (scConf.isShortCircuitLocalReads() && allowShortCircuitLocalReads) {\n+        if (clientContext.getUseLegacyBlockReaderLocal()) {\n+          reader = getLegacyBlockReaderLocal();\n+          if (reader != null) {\n+            LOG.trace(\"{}: returning new legacy block reader local.\", this);\n+            return reader;\n+          }\n+        } else {\n+          reader = getBlockReaderLocal();\n+          if (reader != null) {\n+            LOG.trace(\"{}: returning new block reader local.\", this);\n+            return reader;\n+          }\n         }\n-      } else {\n-        reader = getBlockReaderLocal();\n+      }\n+      if (scConf.isDomainSocketDataTraffic()) {\n+        reader = getRemoteBlockReaderFromDomain();\n         if (reader != null) {\n-          LOG.trace(\"{}: returning new block reader local.\", this);\n+          LOG.trace(\"{}: returning new remote block reader using UNIX domain \"\n+              + \"socket on {}\", this, pathInfo.getPath());\n           return reader;\n         }\n       }\n-    }\n-    if (scConf.isDomainSocketDataTraffic()) {\n-      reader = getRemoteBlockReaderFromDomain();\n-      if (reader != null) {\n-        LOG.trace(\"{}: returning new remote block reader using UNIX domain \"\n-            + \"socket on {}\", this, pathInfo.getPath());\n-        return reader;\n-      }\n+    } catch (IOException e) {\n+      LOG.debug(\"Block read failed. Getting remote block reader using TCP\", e);\n     }\n     Preconditions.checkState(!DFSInputStream.tcpReadsDisabledForTesting,\n         \"TCP reads were disabled for testing, but we failed to \" +\n@@ -469,7 +473,7 @@ private BlockReader getLegacyBlockReaderLocal() throws IOException {\n     return null;\n   }\n \n-  private BlockReader getBlockReaderLocal() throws InvalidToken {\n+  private BlockReader getBlockReaderLocal() throws IOException {\n     LOG.trace(\"{}: trying to construct a BlockReaderLocal for short-circuit \"\n         + \" reads.\", this);\n     if (pathInfo == null) {",
                "raw_url": "https://github.com/apache/hadoop/raw/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/client/impl/BlockReaderFactory.java",
                "sha": "e83c8ae92b2747bd1b451c83c2356af325b46bc6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DomainSocketFactory.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DomainSocketFactory.java?ref=b061215ecfebe476bf58f70788113d1af816f553",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DomainSocketFactory.java",
                "patch": "@@ -133,7 +133,8 @@ public DomainSocketFactory(ShortCircuitConf conf) {\n    *\n    * @return             Information about the socket path.\n    */\n-  public PathInfo getPathInfo(InetSocketAddress addr, ShortCircuitConf conf) {\n+  public PathInfo getPathInfo(InetSocketAddress addr, ShortCircuitConf conf)\n+      throws IOException {\n     // If there is no domain socket path configured, we can't use domain\n     // sockets.\n     if (conf.getDomainSocketPath().isEmpty()) return PathInfo.NOT_CONFIGURED;",
                "raw_url": "https://github.com/apache/hadoop/raw/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs-client/src/main/java/org/apache/hadoop/hdfs/shortcircuit/DomainSocketFactory.java",
                "sha": "760e920c232b5241f0c8c5d92ef86331846f12de",
                "status": "modified"
            },
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/hadoop/blob/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/client/impl/TestBlockReaderFactory.java",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/client/impl/TestBlockReaderFactory.java?ref=b061215ecfebe476bf58f70788113d1af816f553",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/client/impl/TestBlockReaderFactory.java",
                "patch": "@@ -28,6 +28,7 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.net.InetSocketAddress;\n import java.nio.channels.ClosedByInterruptException;\n import java.util.Arrays;\n import java.util.HashMap;\n@@ -53,6 +54,7 @@\n import org.apache.hadoop.hdfs.protocol.LocatedBlock;\n import org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager.PerDatanodeVisitorInfo;\n import org.apache.hadoop.hdfs.shortcircuit.DfsClientShmManager.Visitor;\n+import org.apache.hadoop.hdfs.shortcircuit.DomainSocketFactory;\n import org.apache.hadoop.hdfs.shortcircuit.ShortCircuitCache;\n import org.apache.hadoop.hdfs.shortcircuit.ShortCircuitReplicaInfo;\n import org.apache.hadoop.io.IOUtils;\n@@ -68,6 +70,7 @@\n import org.junit.Test;\n \n import com.google.common.util.concurrent.Uninterruptibles;\n+import org.junit.rules.ExpectedException;\n import org.junit.rules.Timeout;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -79,6 +82,9 @@\n   @Rule\n   public final Timeout globalTimeout = new Timeout(180000);\n \n+  @Rule\n+  public ExpectedException thrown = ExpectedException.none();\n+\n   @Before\n   public void init() {\n     DomainSocket.disableBindPathValidation();\n@@ -144,6 +150,33 @@ public void testFallbackFromShortCircuitToUnixDomainTraffic()\n     sockDir.close();\n   }\n \n+  /**\n+   * Test the case where address passed to DomainSocketFactory#getPathInfo is\n+   * unresolved. In such a case an exception should be thrown.\n+   */\n+  @Test(timeout=60000)\n+  public void testGetPathInfoWithUnresolvedHost() throws Exception {\n+    TemporarySocketDirectory sockDir = new TemporarySocketDirectory();\n+\n+    Configuration conf =\n+        createShortCircuitConf(\"testGetPathInfoWithUnresolvedHost\", sockDir);\n+    conf.set(DFS_CLIENT_CONTEXT,\n+        \"testGetPathInfoWithUnresolvedHost_Context\");\n+    conf.setBoolean(DFS_CLIENT_DOMAIN_SOCKET_DATA_TRAFFIC, true);\n+\n+    DfsClientConf.ShortCircuitConf shortCircuitConf =\n+        new DfsClientConf.ShortCircuitConf(conf);\n+    DomainSocketFactory domainSocketFactory =\n+        new DomainSocketFactory(shortCircuitConf);\n+    InetSocketAddress targetAddr =\n+        InetSocketAddress.createUnresolved(\"random\", 32456);\n+\n+    thrown.expect(IOException.class);\n+    thrown.expectMessage(\"Unresolved host: \" + targetAddr);\n+    domainSocketFactory.getPathInfo(targetAddr, shortCircuitConf);\n+    sockDir.close();\n+  }\n+\n   /**\n    * Test the case where we have multiple threads waiting on the\n    * ShortCircuitCache delivering a certain ShortCircuitReplica.",
                "raw_url": "https://github.com/apache/hadoop/raw/b061215ecfebe476bf58f70788113d1af816f553/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/client/impl/TestBlockReaderFactory.java",
                "sha": "6b04b14f49a77e0333a871d5a63de540741f8255",
                "status": "modified"
            }
        ],
        "message": "HDFS-11701. NPE from Unresolved Host causes permanent DFSInputStream failures. Contributed by Lokesh Jain.",
        "parent": "https://github.com/apache/hadoop/commit/456705a07c8b80658950acc99f23086244c6b20f",
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockReaderFactory.java"
        ]
    },
    "hadoop_b06cc16": {
        "bug_id": "hadoop_b06cc16",
        "commit": "https://github.com/apache/hadoop/commit/b06cc16f7df63766531721a55280061949cab9b4",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/b06cc16f7df63766531721a55280061949cab9b4/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=b06cc16f7df63766531721a55280061949cab9b4",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -413,6 +413,8 @@ Release 2.4.0 - UNRELEASED\n     YARN-1768. Fixed error message being too verbose when killing a non-existent\n     application\n     \n+    YARN-1774. FS: Submitting to non-leaf queue throws NPE. (Anubhav Dhoot and\n+    Karthik Kambatla via kasha)\n \n Release 2.3.1 - UNRELEASED\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/b06cc16f7df63766531721a55280061949cab9b4/hadoop-yarn-project/CHANGES.txt",
                "sha": "9286a7ead8254cf637f41654d9c50c6958e7e261",
                "status": "modified"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/hadoop/blob/b06cc16f7df63766531721a55280061949cab9b4/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java?ref=b06cc16f7df63766531721a55280061949cab9b4",
                "deletions": 11,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "patch": "@@ -611,9 +611,6 @@ protected synchronized void addApplication(ApplicationId applicationId,\n     RMApp rmApp = rmContext.getRMApps().get(applicationId);\n     FSLeafQueue queue = assignToQueue(rmApp, queueName, user);\n     if (queue == null) {\n-      rmContext.getDispatcher().getEventHandler().handle(\n-          new RMAppRejectedEvent(applicationId,\n-              \"Application rejected by queue placement policy\"));\n       return;\n     }\n \n@@ -679,27 +676,43 @@ protected synchronized void addApplicationAttempt(\n         new RMAppAttemptEvent(applicationAttemptId,\n             RMAppAttemptEventType.ATTEMPT_ADDED));\n   }\n-  \n+\n+  /**\n+   * Helper method that attempts to assign the app to a queue. The method is\n+   * responsible to call the appropriate event-handler if the app is rejected.\n+   */\n   @VisibleForTesting\n   FSLeafQueue assignToQueue(RMApp rmApp, String queueName, String user) {\n     FSLeafQueue queue = null;\n+    String appRejectMsg = null;\n+\n     try {\n       QueuePlacementPolicy placementPolicy = allocConf.getPlacementPolicy();\n       queueName = placementPolicy.assignAppToQueue(queueName, user);\n       if (queueName == null) {\n-        return null;\n+        appRejectMsg = \"Application rejected by queue placement policy\";\n+      } else {\n+        queue = queueMgr.getLeafQueue(queueName, true);\n+        if (queue == null) {\n+          appRejectMsg = queueName + \" is not a leaf queue\";\n+        }\n       }\n-      queue = queueMgr.getLeafQueue(queueName, true);\n-    } catch (IOException ex) {\n-      LOG.error(\"Error assigning app to queue, rejecting\", ex);\n+    } catch (IOException ioe) {\n+      appRejectMsg = \"Error assigning app to queue \" + queueName;\n     }\n-    \n+\n+    if (appRejectMsg != null && rmApp != null) {\n+      LOG.error(appRejectMsg);\n+      rmContext.getDispatcher().getEventHandler().handle(\n+          new RMAppRejectedEvent(rmApp.getApplicationId(), appRejectMsg));\n+      return null;\n+    }\n+\n     if (rmApp != null) {\n       rmApp.setQueue(queue.getName());\n     } else {\n-      LOG.warn(\"Couldn't find RM app to set queue name on\");\n+      LOG.error(\"Couldn't find RM app to set queue name on\");\n     }\n-    \n     return queue;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/b06cc16f7df63766531721a55280061949cab9b4/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "sha": "3cdff7f405563bc14e3c7ac7b5522ca977a25a54",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/b06cc16f7df63766531721a55280061949cab9b4/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java?ref=b06cc16f7df63766531721a55280061949cab9b4",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "patch": "@@ -704,6 +704,22 @@ public void testAssignToQueue() throws Exception {\n     assertEquals(rmApp2.getQueue(), queue2.getName());\n     assertEquals(\"root.notdefault\", rmApp2.getQueue());\n   }\n+\n+  @Test\n+  public void testAssignToNonLeafQueueReturnsNull() throws Exception {\n+    conf.set(FairSchedulerConfiguration.USER_AS_DEFAULT_QUEUE, \"true\");\n+    scheduler.reinitialize(conf, resourceManager.getRMContext());\n+\n+    scheduler.getQueueManager().getLeafQueue(\"root.child1.granchild\", true);\n+    scheduler.getQueueManager().getLeafQueue(\"root.child2\", true);\n+\n+    RMApp rmApp1 = new MockRMApp(0, 0, RMAppState.NEW);\n+    RMApp rmApp2 = new MockRMApp(1, 1, RMAppState.NEW);\n+\n+    // Trying to assign to non leaf queue would return null\n+    assertNull(scheduler.assignToQueue(rmApp1, \"root.child1\", \"tintin\"));\n+    assertNotNull(scheduler.assignToQueue(rmApp2, \"root.child2\", \"snowy\"));\n+  }\n   \n   @Test\n   public void testQueuePlacementWithPolicy() throws Exception {",
                "raw_url": "https://github.com/apache/hadoop/raw/b06cc16f7df63766531721a55280061949cab9b4/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "sha": "d1052bb165e6ae3798697208b56814da897157ba",
                "status": "modified"
            }
        ],
        "message": "YARN-1774. FS: Submitting to non-leaf queue throws NPE. (Anubhav Dhoot and Karthik Kambatla via kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1575415 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/b1f87bbabd38c90f55c833db70f82d89eae15de6",
        "repo": "hadoop",
        "unit_tests": [
            "TestFairScheduler.java"
        ]
    },
    "hadoop_b2ed6ae": {
        "bug_id": "hadoop_b2ed6ae",
        "commit": "https://github.com/apache/hadoop/commit/b2ed6ae73197990a950ce71ece80c0f23221c384",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/b2ed6ae73197990a950ce71ece80c0f23221c384/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java?ref=b2ed6ae73197990a950ce71ece80c0f23221c384",
                "deletions": 9,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java",
                "patch": "@@ -587,19 +587,22 @@ private static ContainerReport convertToContainerReport(\n         }\n       }\n     }\n-    NodeId allocatedNode = NodeId.newInstance(allocatedHost, allocatedPort);\n     ContainerId containerId =\n         ConverterUtils.toContainerId(entity.getEntityId());\n-    String logUrl = WebAppUtils.getAggregatedLogURL(\n-        serverHttpAddress,\n-        allocatedNode.toString(),\n-        containerId.toString(),\n-        containerId.toString(),\n-        user);\n+    String logUrl = null;\n+    NodeId allocatedNode = null;\n+    if (allocatedHost != null) {\n+      allocatedNode = NodeId.newInstance(allocatedHost, allocatedPort);\n+      logUrl = WebAppUtils.getAggregatedLogURL(\n+          serverHttpAddress,\n+          allocatedNode.toString(),\n+          containerId.toString(),\n+          containerId.toString(),\n+          user);\n+    }\n     return ContainerReport.newInstance(\n         ConverterUtils.toContainerId(entity.getEntityId()),\n-        Resource.newInstance(allocatedMem, allocatedVcore),\n-        NodeId.newInstance(allocatedHost, allocatedPort),\n+        Resource.newInstance(allocatedMem, allocatedVcore), allocatedNode,\n         Priority.newInstance(allocatedPriority),\n         createdTime, finishedTime, diagnosticsInfo, logUrl, exitStatus, state,\n         nodeHttpAddress);",
                "raw_url": "https://github.com/apache/hadoop/raw/b2ed6ae73197990a950ce71ece80c0f23221c384/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java",
                "sha": "19afb253e0f48fd26bed27f5be2e01996d2e0582",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/b2ed6ae73197990a950ce71ece80c0f23221c384/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/ContainerFinishedEvent.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/ContainerFinishedEvent.java?ref=b2ed6ae73197990a950ce71ece80c0f23221c384",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/ContainerFinishedEvent.java",
                "patch": "@@ -20,24 +20,28 @@\n \n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.ContainerState;\n+import org.apache.hadoop.yarn.api.records.NodeId;\n \n public class ContainerFinishedEvent extends SystemMetricsEvent {\n \n   private ContainerId containerId;\n   private String diagnosticsInfo;\n   private int containerExitStatus;\n   private ContainerState state;\n+  private NodeId allocatedNode;\n \n   public ContainerFinishedEvent(\n       ContainerId containerId,\n       String diagnosticsInfo,\n       int containerExitStatus,\n       ContainerState state,\n-      long finishedTime) {\n+      long finishedTime,\n+      NodeId allocatedNode) {\n     super(SystemMetricsEventType.CONTAINER_FINISHED, finishedTime);\n     this.containerId = containerId;\n     this.diagnosticsInfo = diagnosticsInfo;\n     this.containerExitStatus = containerExitStatus;\n+    this.allocatedNode = allocatedNode;\n     this.state = state;\n   }\n \n@@ -62,4 +66,7 @@ public ContainerState getContainerState() {\n     return state;\n   }\n \n+  public NodeId getAllocatedNode() {\n+    return allocatedNode;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/b2ed6ae73197990a950ce71ece80c0f23221c384/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/ContainerFinishedEvent.java",
                "sha": "ca4d3117aa52f4973bd5c53dde74bf07d526d42e",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/b2ed6ae73197990a950ce71ece80c0f23221c384/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java?ref=b2ed6ae73197990a950ce71ece80c0f23221c384",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "patch": "@@ -218,7 +218,7 @@ public void containerFinished(RMContainer container, long finishedTime) {\n               container.getDiagnosticsInfo(),\n               container.getContainerExitStatus(),\n               container.getContainerState(),\n-              finishedTime));\n+              finishedTime, container.getAllocatedNode()));\n     }\n   }\n \n@@ -479,6 +479,12 @@ private void publishContainerFinishedEvent(ContainerFinishedEvent event) {\n         event.getContainerExitStatus());\n     eventInfo.put(ContainerMetricsConstants.STATE_EVENT_INFO,\n         event.getContainerState().toString());\n+    Map<String, Object> entityInfo = new HashMap<String, Object>();\n+    entityInfo.put(ContainerMetricsConstants.ALLOCATED_HOST_ENTITY_INFO,\n+        event.getAllocatedNode().getHost());\n+    entityInfo.put(ContainerMetricsConstants.ALLOCATED_PORT_ENTITY_INFO,\n+        event.getAllocatedNode().getPort());\n+    entity.setOtherInfo(entityInfo);\n     tEvent.setEventInfo(eventInfo);\n     entity.addEvent(tEvent);\n     putEntity(entity);",
                "raw_url": "https://github.com/apache/hadoop/raw/b2ed6ae73197990a950ce71ece80c0f23221c384/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "sha": "cba87903fb3332c93121e40a46ada5cf5e2ff1c6",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hadoop/blob/b2ed6ae73197990a950ce71ece80c0f23221c384/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java?ref=b2ed6ae73197990a950ce71ece80c0f23221c384",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "patch": "@@ -344,6 +344,36 @@ public void testPublishAppAttemptMetrics() throws Exception {\n     Assert.assertTrue(hasRegisteredEvent && hasFinishedEvent);\n   }\n \n+  @Test(timeout = 10000)\n+  public void testPublishHostPortInfoOnContainerFinished() throws Exception {\n+    ContainerId containerId =\n+        ContainerId.newContainerId(ApplicationAttemptId.newInstance(\n+            ApplicationId.newInstance(0, 1), 1), 1);\n+    RMContainer container = createRMContainer(containerId);\n+    metricsPublisher.containerFinished(container, container.getFinishTime());\n+    TimelineEntity entity = null;\n+    do {\n+      entity =\n+          store.getEntity(containerId.toString(),\n+              ContainerMetricsConstants.ENTITY_TYPE,\n+              EnumSet.allOf(Field.class));\n+    } while (entity == null || entity.getEvents().size() < 1);\n+    Assert.assertNotNull(entity.getOtherInfo());\n+    Assert.assertEquals(2, entity.getOtherInfo().size());\n+    Assert.assertNotNull(entity.getOtherInfo().get(\n+        ContainerMetricsConstants.ALLOCATED_HOST_ENTITY_INFO));\n+    Assert.assertNotNull(entity.getOtherInfo().get(\n+        ContainerMetricsConstants.ALLOCATED_PORT_ENTITY_INFO));\n+    Assert.assertEquals(\n+        container.getAllocatedNode().getHost(),\n+        entity.getOtherInfo().get(\n+            ContainerMetricsConstants.ALLOCATED_HOST_ENTITY_INFO));\n+    Assert.assertEquals(\n+        container.getAllocatedNode().getPort(),\n+        entity.getOtherInfo().get(\n+            ContainerMetricsConstants.ALLOCATED_PORT_ENTITY_INFO));\n+  }\n+\n   @Test(timeout = 10000)\n   public void testPublishContainerMetrics() throws Exception {\n     ContainerId containerId =",
                "raw_url": "https://github.com/apache/hadoop/raw/b2ed6ae73197990a950ce71ece80c0f23221c384/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "sha": "0738a2b77ced683a9cc82264a28767072cb74294",
                "status": "modified"
            }
        ],
        "message": "YARN-4747. AHS error 500 due to NPE when container start event is missing. Contributed by Varun Saxena",
        "parent": "https://github.com/apache/hadoop/commit/23248f63aab74a19dba38d348f2b231c8360770a",
        "repo": "hadoop",
        "unit_tests": [
            "TestApplicationHistoryManagerOnTimelineStore.java",
            "TestSystemMetricsPublisher.java"
        ]
    },
    "hadoop_b406d8e": {
        "bug_id": "hadoop_b406d8e",
        "commit": "https://github.com/apache/hadoop/commit/b406d8e3755d24ce72c443fd893a5672fd56babc",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/b406d8e3755d24ce72c443fd893a5672fd56babc/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.java?ref=b406d8e3755d24ce72c443fd893a5672fd56babc",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.java",
                "patch": "@@ -275,8 +275,16 @@ private INodeAttributes getINodeAttrs(byte[][] pathByNameArr, int pathIdx,\n     INodeAttributes inodeAttrs = inode.getSnapshotINode(snapshotId);\n     if (getAttributesProvider() != null) {\n       String[] elements = new String[pathIdx + 1];\n-      for (int i = 0; i < elements.length; i++) {\n-        elements[i] = DFSUtil.bytes2String(pathByNameArr[i]);\n+      /**\n+       * {@link INode#getPathComponents(String)} returns a null component\n+       * for the root only path \"/\". Assign an empty string if so.\n+       */\n+      if (pathByNameArr.length == 1 && pathByNameArr[0] == null) {\n+        elements[0] = \"\";\n+      } else {\n+        for (int i = 0; i < elements.length; i++) {\n+          elements[i] = DFSUtil.bytes2String(pathByNameArr[i]);\n+        }\n       }\n       inodeAttrs = getAttributesProvider().getAttributes(elements, inodeAttrs);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/b406d8e3755d24ce72c443fd893a5672fd56babc/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSPermissionChecker.java",
                "sha": "c854b49fb4f0687cb6c7ee86af0efe16b73e1543",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/hadoop/blob/b406d8e3755d24ce72c443fd893a5672fd56babc/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeAttributeProvider.java",
                "changes": 60,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeAttributeProvider.java?ref=b406d8e3755d24ce72c443fd893a5672fd56babc",
                "deletions": 16,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeAttributeProvider.java",
                "patch": "@@ -313,31 +313,59 @@ public void testAuthzBypassingProvider() throws Exception {\n     testBypassProviderHelper(users, HDFS_PERMISSION, true);\n   }\n \n-  @Test\n-  public void testCustomProvider() throws Exception {\n+  private void verifyFileStatus(UserGroupInformation ugi) throws IOException {\n     FileSystem fs = FileSystem.get(miniDFS.getConfiguration(0));\n-    fs.mkdirs(new Path(\"/user/xxx\"));\n-    FileStatus status = fs.getFileStatus(new Path(\"/user/xxx\"));\n-    Assert.assertEquals(System.getProperty(\"user.name\"), status.getOwner());\n+\n+    FileStatus status = fs.getFileStatus(new Path(\"/\"));\n+    LOG.info(\"Path '/' is owned by: \"\n+        + status.getOwner() + \":\" + status.getGroup());\n+\n+    Path userDir = new Path(\"/user/\" + ugi.getShortUserName());\n+    fs.mkdirs(userDir);\n+    status = fs.getFileStatus(userDir);\n+    Assert.assertEquals(ugi.getShortUserName(), status.getOwner());\n     Assert.assertEquals(\"supergroup\", status.getGroup());\n     Assert.assertEquals(new FsPermission((short) 0755), status.getPermission());\n-    fs.mkdirs(new Path(\"/user/authz\"));\n-    Path p = new Path(\"/user/authz\");\n-    status = fs.getFileStatus(p);\n+\n+    Path authzDir = new Path(\"/user/authz\");\n+    fs.mkdirs(authzDir);\n+    status = fs.getFileStatus(authzDir);\n     Assert.assertEquals(\"foo\", status.getOwner());\n     Assert.assertEquals(\"bar\", status.getGroup());\n     Assert.assertEquals(new FsPermission((short) 0770), status.getPermission());\n-    AclStatus aclStatus = fs.getAclStatus(p);\n+\n+    AclStatus aclStatus = fs.getAclStatus(authzDir);\n     Assert.assertEquals(1, aclStatus.getEntries().size());\n-    Assert.assertEquals(AclEntryType.GROUP, aclStatus.getEntries().get(0)\n-            .getType());\n-    Assert.assertEquals(\"xxx\", aclStatus.getEntries().get(0)\n-            .getName());\n-    Assert.assertEquals(FsAction.ALL, aclStatus.getEntries().get(0)\n-            .getPermission());\n-    Map<String, byte[]> xAttrs = fs.getXAttrs(p);\n+    Assert.assertEquals(AclEntryType.GROUP,\n+        aclStatus.getEntries().get(0).getType());\n+    Assert.assertEquals(\"xxx\",\n+        aclStatus.getEntries().get(0).getName());\n+    Assert.assertEquals(FsAction.ALL,\n+        aclStatus.getEntries().get(0).getPermission());\n+    Map<String, byte[]> xAttrs = fs.getXAttrs(authzDir);\n     Assert.assertTrue(xAttrs.containsKey(\"user.test\"));\n     Assert.assertEquals(2, xAttrs.get(\"user.test\").length);\n   }\n \n+  /**\n+   * With the custom provider configured, verify file status attributes.\n+   * A superuser can bypass permission check while resolving paths. So,\n+   * verify file status for both superuser and non-superuser.\n+   */\n+  @Test\n+  public void testCustomProvider() throws Exception {\n+    final UserGroupInformation[] users = new UserGroupInformation[]{\n+        UserGroupInformation.createUserForTesting(\n+            System.getProperty(\"user.name\"), new String[]{\"supergroup\"}),\n+        UserGroupInformation.createUserForTesting(\n+            \"normaluser\", new String[]{\"normalusergroup\"}),\n+    };\n+\n+    for (final UserGroupInformation user : users) {\n+      user.doAs((PrivilegedExceptionAction<Object>) () -> {\n+        verifyFileStatus(user);\n+        return null;\n+      });\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/b406d8e3755d24ce72c443fd893a5672fd56babc/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestINodeAttributeProvider.java",
                "sha": "9c7dcd335232e18859841e3bcb523e8f118e496a",
                "status": "modified"
            }
        ],
        "message": "HDFS-12614. FSPermissionChecker#getINodeAttrs() throws NPE when INodeAttributesProvider configured.",
        "parent": "https://github.com/apache/hadoop/commit/e906108fc98a011630d12a43e557b81d7ef7ea5d",
        "repo": "hadoop",
        "unit_tests": [
            "TestFSPermissionChecker.java"
        ]
    },
    "hadoop_b4097b9": {
        "bug_id": "hadoop_b4097b9",
        "commit": "https://github.com/apache/hadoop/commit/b4097b96a39bad6214b01989e7f2fb37dad70793",
        "file": [
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/hadoop/blob/b4097b96a39bad6214b01989e7f2fb37dad70793/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/RollingLevelDBTimelineStore.java",
                "changes": 55,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/RollingLevelDBTimelineStore.java?ref=b4097b96a39bad6214b01989e7f2fb37dad70793",
                "deletions": 26,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/RollingLevelDBTimelineStore.java",
                "patch": "@@ -793,39 +793,42 @@ private TimelineEntities getEntityByTime(byte[] base, String entityType,\n             entity = getEntity(entityId, entityType, startTime, queryFields,\n                 iterator, key, kp.getOffset());\n           }\n-          // determine if the retrieved entity matches the provided secondary\n-          // filters, and if so add it to the list of entities to return\n-          boolean filterPassed = true;\n-          if (secondaryFilters != null) {\n-            for (NameValuePair filter : secondaryFilters) {\n-              Object v = entity.getOtherInfo().get(filter.getName());\n-              if (v == null) {\n-                Set<Object> vs = entity.getPrimaryFilters()\n-                    .get(filter.getName());\n-                if (vs == null || !vs.contains(filter.getValue())) {\n+\n+          if (entity != null) {\n+            // determine if the retrieved entity matches the provided secondary\n+            // filters, and if so add it to the list of entities to return\n+            boolean filterPassed = true;\n+            if (secondaryFilters != null) {\n+              for (NameValuePair filter : secondaryFilters) {\n+                Object v = entity.getOtherInfo().get(filter.getName());\n+                if (v == null) {\n+                  Set<Object> vs = entity.getPrimaryFilters()\n+                          .get(filter.getName());\n+                  if (vs == null || !vs.contains(filter.getValue())) {\n+                    filterPassed = false;\n+                    break;\n+                  }\n+                } else if (!v.equals(filter.getValue())) {\n                   filterPassed = false;\n                   break;\n                 }\n-              } else if (!v.equals(filter.getValue())) {\n-                filterPassed = false;\n-                break;\n               }\n             }\n-          }\n-          if (filterPassed) {\n-            if (entity.getDomainId() == null) {\n-              entity.setDomainId(DEFAULT_DOMAIN_ID);\n-            }\n-            if (checkAcl == null || checkAcl.check(entity)) {\n-              // Remove primary filter and other info if they are added for\n-              // matching secondary filters\n-              if (addPrimaryFilters) {\n-                entity.setPrimaryFilters(null);\n+            if (filterPassed) {\n+              if (entity.getDomainId() == null) {\n+                entity.setDomainId(DEFAULT_DOMAIN_ID);\n               }\n-              if (addOtherInfo) {\n-                entity.setOtherInfo(null);\n+              if (checkAcl == null || checkAcl.check(entity)) {\n+                // Remove primary filter and other info if they are added for\n+                // matching secondary filters\n+                if (addPrimaryFilters) {\n+                  entity.setPrimaryFilters(null);\n+                }\n+                if (addOtherInfo) {\n+                  entity.setOtherInfo(null);\n+                }\n+                entities.addEntity(entity);\n               }\n-              entities.addEntity(entity);\n             }\n           }\n         }",
                "raw_url": "https://github.com/apache/hadoop/raw/b4097b96a39bad6214b01989e7f2fb37dad70793/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/RollingLevelDBTimelineStore.java",
                "sha": "e85505f73e877128a52bd24b0668469b55a5e097",
                "status": "modified"
            }
        ],
        "message": "YARN-9744. RollingLevelDBTimelineStore.getEntityByTime fails with NPE. Contributed by Prabhu Joseph.",
        "parent": "https://github.com/apache/hadoop/commit/0b507d2ddf132985b43b4e2d3ad11d7fd2d90cd3",
        "repo": "hadoop",
        "unit_tests": [
            "TestRollingLevelDBTimelineStore.java"
        ]
    },
    "hadoop_b5cdf78": {
        "bug_id": "hadoop_b5cdf78",
        "commit": "https://github.com/apache/hadoop/commit/b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -598,6 +598,9 @@ Release 2.7.2 - UNRELEASED\n \n   BUG FIXES\n \n+    YARN-3793. Several NPEs when deleting local files on NM recovery (Varun\n+    Saxena via jlowe)\n+\n Release 2.7.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6/hadoop-yarn-project/CHANGES.txt",
                "sha": "3620a718a74ea02eaea1e12686321100483320ab",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java?ref=b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
                "patch": "@@ -276,10 +276,10 @@ private void uploadLogsForContainers(boolean appFinished) {\n             aggregator.doContainerLogAggregation(writer, appFinished);\n         if (uploadedFilePathsInThisCycle.size() > 0) {\n           uploadedLogsInThisCycle = true;\n+          this.delService.delete(this.userUgi.getShortUserName(), null,\n+              uploadedFilePathsInThisCycle\n+                  .toArray(new Path[uploadedFilePathsInThisCycle.size()]));\n         }\n-        this.delService.delete(this.userUgi.getShortUserName(), null,\n-          uploadedFilePathsInThisCycle\n-            .toArray(new Path[uploadedFilePathsInThisCycle.size()]));\n \n         // This container is finished, and all its logs have been uploaded,\n         // remove it from containerLogAggregators.",
                "raw_url": "https://github.com/apache/hadoop/raw/b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/AppLogAggregatorImpl.java",
                "sha": "654eb0b1a52b6db6c7a4123f407e79f2460793c5",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/hadoop/blob/b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java?ref=b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java",
                "patch": "@@ -31,6 +31,7 @@\n import static org.mockito.Mockito.never;\n import static org.mockito.Mockito.reset;\n import static org.mockito.Mockito.spy;\n+import static org.mockito.Mockito.times;\n import static org.mockito.Mockito.verify;\n import static org.mockito.Mockito.when;\n \n@@ -280,6 +281,41 @@ public void testLocalFileDeletionOnDiskFull() throws Exception {\n     verifyLocalFileDeletion(logAggregationService);\n   }\n \n+  /* Test to verify fix for YARN-3793 */\n+  @Test\n+  public void testNoLogsUploadedOnAppFinish() throws Exception {\n+    this.delSrvc = new DeletionService(createContainerExecutor());\n+    delSrvc = spy(delSrvc);\n+    this.delSrvc.init(conf);\n+    this.conf.set(YarnConfiguration.NM_LOG_DIRS, localLogDir.getAbsolutePath());\n+    this.conf.set(YarnConfiguration.NM_REMOTE_APP_LOG_DIR,\n+        this.remoteRootLogDir.getAbsolutePath());\n+\n+    LogAggregationService logAggregationService = new LogAggregationService(\n+        dispatcher, this.context, this.delSrvc, super.dirsHandler);\n+    logAggregationService.init(this.conf);\n+    logAggregationService.start();\n+\n+    ApplicationId app = BuilderUtils.newApplicationId(1234, 1);\n+    File appLogDir = new File(localLogDir, ConverterUtils.toString(app));\n+    appLogDir.mkdir();\n+    LogAggregationContext context =\n+        LogAggregationContext.newInstance(\"HOST*\", \"sys*\");\n+    logAggregationService.handle(new LogHandlerAppStartedEvent(app, this.user,\n+        null, ContainerLogsRetentionPolicy.ALL_CONTAINERS, this.acls, context));\n+\n+    ApplicationAttemptId appAttemptId =\n+        BuilderUtils.newApplicationAttemptId(app, 1);\n+    ContainerId cont = BuilderUtils.newContainerId(appAttemptId, 1);\n+    writeContainerLogs(appLogDir, cont, new String[] { \"stdout\",\n+        \"stderr\", \"syslog\" });\n+    logAggregationService.handle(new LogHandlerContainerFinishedEvent(cont, 0));\n+    logAggregationService.handle(new LogHandlerAppFinishedEvent(app));\n+    logAggregationService.stop();\n+    delSrvc.stop();\n+    // Aggregated logs should not be deleted if not uploaded.\n+    verify(delSrvc, times(0)).delete(user, null);\n+  }\n \n   @Test\n   public void testNoContainerOnNode() throws Exception {",
                "raw_url": "https://github.com/apache/hadoop/raw/b5cdf78e8e6cd6c5c1fb7286207dac72be32c0d6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/logaggregation/TestLogAggregationService.java",
                "sha": "6a3d270eac01f910ce0980be26e3e3397951e638",
                "status": "modified"
            }
        ],
        "message": "YARN-3793. Several NPEs when deleting local files on NM recovery. Contributed by Varun Saxena",
        "parent": "https://github.com/apache/hadoop/commit/eac1d1894354e90d314087af8e7fb168ddef9a3d",
        "repo": "hadoop",
        "unit_tests": [
            "TestAppLogAggregatorImpl.java"
        ]
    },
    "hadoop_b7070f3": {
        "bug_id": "hadoop_b7070f3",
        "commit": "https://github.com/apache/hadoop/commit/b7070f3308fc4c6a8a9a25021562169cae87d223",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java?ref=b7070f3308fc4c6a8a9a25021562169cae87d223",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "patch": "@@ -1442,8 +1442,10 @@ public static InetSocketAddress getBindAddress(Configuration conf) {\n    * @param conf\n    * @throws Exception\n    */\n-  private static void deleteRMStateStore(Configuration conf) throws Exception {\n+  @VisibleForTesting\n+  static void deleteRMStateStore(Configuration conf) throws Exception {\n     RMStateStore rmStore = RMStateStoreFactory.getStore(conf);\n+    rmStore.setResourceManager(new ResourceManager());\n     rmStore.init(conf);\n     rmStore.start();\n     try {\n@@ -1455,9 +1457,11 @@ private static void deleteRMStateStore(Configuration conf) throws Exception {\n     }\n   }\n \n-  private static void removeApplication(Configuration conf, String applicationId)\n+  @VisibleForTesting\n+  static void removeApplication(Configuration conf, String applicationId)\n       throws Exception {\n     RMStateStore rmStore = RMStateStoreFactory.getStore(conf);\n+    rmStore.setResourceManager(new ResourceManager());\n     rmStore.init(conf);\n     rmStore.start();\n     try {",
                "raw_url": "https://github.com/apache/hadoop/raw/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "sha": "8ddbc20569e881c42641ccc22a2f301f6af37abf",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStore.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStore.java?ref=b7070f3308fc4c6a8a9a25021562169cae87d223",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStore.java",
                "patch": "@@ -86,7 +86,8 @@\n public abstract class RMStateStore extends AbstractService {\n \n   // constants for RM App state and RMDTSecretManagerState.\n-  protected static final String RM_APP_ROOT = \"RMAppRoot\";\n+  @VisibleForTesting\n+  public static final String RM_APP_ROOT = \"RMAppRoot\";\n   protected static final String RM_DT_SECRET_MANAGER_ROOT = \"RMDTSecretManagerRoot\";\n   protected static final String DELEGATION_KEY_PREFIX = \"DelegationKey_\";\n   protected static final String DELEGATION_TOKEN_PREFIX = \"RMDelegationToken_\";",
                "raw_url": "https://github.com/apache/hadoop/raw/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/RMStateStore.java",
                "sha": "a6527d8a4023b11ffaff99ec5b719ebc59c4ef28",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java?ref=b7070f3308fc4c6a8a9a25021562169cae87d223",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "patch": "@@ -127,7 +127,8 @@\n       \"RMDTSequentialNumber\";\n   private static final String RM_DT_MASTER_KEYS_ROOT_ZNODE_NAME =\n       \"RMDTMasterKeysRoot\";\n-  protected static final String ROOT_ZNODE_NAME = \"ZKRMStateRoot\";\n+  @VisibleForTesting\n+  public static final String ROOT_ZNODE_NAME = \"ZKRMStateRoot\";\n   protected static final Version CURRENT_VERSION_INFO =\n       Version.newInstance(1, 3);\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "sha": "cf6380f55ed09d730272bba04519f44743d48d7b",
                "status": "modified"
            },
            {
                "additions": 103,
                "blob_url": "https://github.com/apache/hadoop/blob/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMStoreCommands.java",
                "changes": 103,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMStoreCommands.java?ref=b7070f3308fc4c6a8a9a25021562169cae87d223",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMStoreCommands.java",
                "patch": "@@ -0,0 +1,103 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.yarn.server.resourcemanager;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.test.TestingServer;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.ha.HAServiceProtocol;\n+import org.apache.hadoop.ha.HAServiceProtocol.StateChangeRequestInfo;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.conf.YarnConfiguration;\n+import org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore;\n+import org.apache.hadoop.yarn.server.resourcemanager.recovery.TestZKRMStateStore;\n+import org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore;\n+import org.junit.Test;\n+\n+public class TestRMStoreCommands {\n+\n+  @Test\n+  public void testFormatStateStoreCmdForZK() throws Exception {\n+    StateChangeRequestInfo req = new StateChangeRequestInfo(\n+        HAServiceProtocol.RequestSource.REQUEST_BY_USER);\n+    try (TestingServer curatorTestingServer =\n+        TestZKRMStateStore.setupCuratorServer();\n+        CuratorFramework curatorFramework = TestZKRMStateStore.\n+            setupCuratorFramework(curatorTestingServer)) {\n+      Configuration conf = TestZKRMStateStore.createHARMConf(\"rm1,rm2\", \"rm1\",\n+          1234, false, curatorTestingServer);\n+      ResourceManager rm = new MockRM(conf);\n+      rm.start();\n+      rm.getRMContext().getRMAdminService().transitionToActive(req);\n+      String zkStateRoot = ZKRMStateStore.ROOT_ZNODE_NAME;\n+      assertEquals(\"RM State store parent path should have a child node \" +\n+          zkStateRoot, zkStateRoot, curatorFramework.getChildren().forPath(\n+              YarnConfiguration.DEFAULT_ZK_RM_STATE_STORE_PARENT_PATH).get(0));\n+      rm.close();\n+      try {\n+        ResourceManager.deleteRMStateStore(conf);\n+      } catch (Exception e) {\n+        fail(\"Exception should not be thrown during format rm state store\" +\n+            \" operation.\");\n+      }\n+      assertTrue(\"After store format parent path should have no child nodes\",\n+          curatorFramework.getChildren().forPath(\n+          YarnConfiguration.DEFAULT_ZK_RM_STATE_STORE_PARENT_PATH).isEmpty());\n+    }\n+  }\n+\n+  @Test\n+  public void testRemoveApplicationFromStateStoreCmdForZK() throws Exception {\n+    StateChangeRequestInfo req = new StateChangeRequestInfo(\n+        HAServiceProtocol.RequestSource.REQUEST_BY_USER);\n+    try (TestingServer curatorTestingServer =\n+        TestZKRMStateStore.setupCuratorServer();\n+        CuratorFramework curatorFramework = TestZKRMStateStore.\n+            setupCuratorFramework(curatorTestingServer)) {\n+      Configuration conf = TestZKRMStateStore.createHARMConf(\"rm1,rm2\", \"rm1\",\n+          1234, false, curatorTestingServer);\n+      ResourceManager rm = new MockRM(conf);\n+      rm.start();\n+      rm.getRMContext().getRMAdminService().transitionToActive(req);\n+      rm.close();\n+      String appId = ApplicationId.newInstance(\n+          System.currentTimeMillis(), 1).toString();\n+      String appRootPath = YarnConfiguration.\n+          DEFAULT_ZK_RM_STATE_STORE_PARENT_PATH + \"/\"+\n+          ZKRMStateStore.ROOT_ZNODE_NAME + \"/\" + RMStateStore.RM_APP_ROOT;\n+      String appIdPath = appRootPath + \"/\" + appId;\n+      curatorFramework.create().forPath(appIdPath);\n+      assertEquals(\"Application node for \" + appId + \"should exist\",\n+          appId, curatorFramework.getChildren().forPath(appRootPath).get(0));\n+      try {\n+        ResourceManager.removeApplication(conf, appId);\n+      } catch (Exception e) {\n+        fail(\"Exception should not be thrown while removing app from \" +\n+            \"rm state store.\");\n+      }\n+      assertTrue(\"After remove app from store there should be no child nodes\" +\n+          \" in app root path\",\n+          curatorFramework.getChildren().forPath(appRootPath).isEmpty());\n+    }\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestRMStoreCommands.java",
                "sha": "6c74616379e735a08cc29adf2fa66eb1fd114ff3",
                "status": "added"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop/blob/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/TestZKRMStateStore.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/TestZKRMStateStore.java?ref=b7070f3308fc4c6a8a9a25021562169cae87d223",
                "deletions": 15,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/TestZKRMStateStore.java",
                "patch": "@@ -53,7 +53,6 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState;\n import org.apache.hadoop.yarn.server.resourcemanager.security.ClientToAMTokenSecretManagerInRM;\n-import org.apache.hadoop.yarn.util.ConverterUtils;\n import org.apache.zookeeper.KeeperException;\n import org.apache.zookeeper.ZooDefs.Perms;\n import org.apache.zookeeper.data.ACL;\n@@ -79,15 +78,26 @@\n   private TestingServer curatorTestingServer;\n   private CuratorFramework curatorFramework;\n \n-  @Before\n-  public void setupCuratorServer() throws Exception {\n-    curatorTestingServer = new TestingServer();\n+  public static TestingServer setupCuratorServer() throws Exception {\n+    TestingServer curatorTestingServer = new TestingServer();\n     curatorTestingServer.start();\n-    curatorFramework = CuratorFrameworkFactory.builder()\n+    return curatorTestingServer;\n+  }\n+\n+  public static CuratorFramework setupCuratorFramework(\n+      TestingServer curatorTestingServer) throws Exception {\n+    CuratorFramework curatorFramework = CuratorFrameworkFactory.builder()\n         .connectString(curatorTestingServer.getConnectString())\n         .retryPolicy(new RetryNTimes(100, 100))\n         .build();\n     curatorFramework.start();\n+    return curatorFramework;\n+  }\n+\n+  @Before\n+  public void setupCurator() throws Exception {\n+    curatorTestingServer = setupCuratorServer();\n+    curatorFramework = setupCuratorFramework(curatorTestingServer);\n   }\n \n   @After\n@@ -243,19 +253,21 @@ protected synchronized void storeVersion() throws Exception {\n     Assert.assertEquals(defaultVersion, store.loadVersion());\n   }\n \n-  private Configuration createHARMConf(\n-      String rmIds, String rmId, int adminPort) {\n+  public static Configuration createHARMConf(String rmIds, String rmId,\n+      int adminPort, boolean autoFailoverEnabled,\n+      TestingServer curatorTestServer) {\n     Configuration conf = new YarnConfiguration();\n     conf.setBoolean(YarnConfiguration.RM_HA_ENABLED, true);\n     conf.set(YarnConfiguration.RM_HA_IDS, rmIds);\n     conf.setBoolean(YarnConfiguration.RECOVERY_ENABLED, true);\n     conf.set(YarnConfiguration.RM_STORE, ZKRMStateStore.class.getName());\n     conf.set(YarnConfiguration.RM_ZK_ADDRESS,\n-        curatorTestingServer.getConnectString());\n+        curatorTestServer.getConnectString());\n     conf.setInt(YarnConfiguration.RM_ZK_TIMEOUT_MS, ZK_TIMEOUT_MS);\n     conf.set(YarnConfiguration.RM_HA_ID, rmId);\n     conf.set(YarnConfiguration.RM_WEBAPP_ADDRESS, \"localhost:0\");\n-\n+    conf.setBoolean(\n+        YarnConfiguration.AUTO_FAILOVER_ENABLED, autoFailoverEnabled);\n     for (String rpcAddress : YarnConfiguration.getServiceAddressConfKeys(conf)) {\n       for (String id : HAUtil.getRMHAIds(conf)) {\n         conf.set(HAUtil.addSuffix(rpcAddress, id), \"localhost:0\");\n@@ -293,8 +305,8 @@ public void testZKRootPathAcls() throws Exception {\n             ZKRMStateStore.ROOT_ZNODE_NAME;\n \n     // Start RM with HA enabled\n-    Configuration conf = createHARMConf(\"rm1,rm2\", \"rm1\", 1234);\n-    conf.setBoolean(YarnConfiguration.AUTO_FAILOVER_ENABLED, false);\n+    Configuration conf =\n+        createHARMConf(\"rm1,rm2\", \"rm1\", 1234, false, curatorTestingServer);\n     ResourceManager rm = new MockRM(conf);\n     rm.start();\n     rm.getRMContext().getRMAdminService().transitionToActive(req);\n@@ -336,8 +348,8 @@ public void testFencing() throws Exception {\n     StateChangeRequestInfo req = new StateChangeRequestInfo(\n         HAServiceProtocol.RequestSource.REQUEST_BY_USER);\n \n-    Configuration conf1 = createHARMConf(\"rm1,rm2\", \"rm1\", 1234);\n-    conf1.setBoolean(YarnConfiguration.AUTO_FAILOVER_ENABLED, false);\n+    Configuration conf1 =\n+        createHARMConf(\"rm1,rm2\", \"rm1\", 1234, false, curatorTestingServer);\n     ResourceManager rm1 = new MockRM(conf1);\n     rm1.start();\n     rm1.getRMContext().getRMAdminService().transitionToActive(req);\n@@ -347,8 +359,8 @@ public void testFencing() throws Exception {\n         HAServiceProtocol.HAServiceState.ACTIVE,\n         rm1.getRMContext().getRMAdminService().getServiceStatus().getState());\n \n-    Configuration conf2 = createHARMConf(\"rm1,rm2\", \"rm2\", 5678);\n-    conf2.setBoolean(YarnConfiguration.AUTO_FAILOVER_ENABLED, false);\n+    Configuration conf2 =\n+        createHARMConf(\"rm1,rm2\", \"rm2\", 5678, false, curatorTestingServer);\n     ResourceManager rm2 = new MockRM(conf2);\n     rm2.start();\n     rm2.getRMContext().getRMAdminService().transitionToActive(req);",
                "raw_url": "https://github.com/apache/hadoop/raw/b7070f3308fc4c6a8a9a25021562169cae87d223/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/TestZKRMStateStore.java",
                "sha": "f71cf25568d9c5bcc9672fd12e3ea18bc3ed2b1a",
                "status": "modified"
            }
        ],
        "message": "YARN-5874. RM -format-state-store and -remove-application-from-state-store commands fail with NPE. Contributed by Varun Saxena.",
        "parent": "https://github.com/apache/hadoop/commit/296c5de0cfee88389cf9f90263280b2034e54cd5",
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceManager.java",
            "TestZKRMStateStore.java"
        ]
    },
    "hadoop_b897d6c": {
        "bug_id": "hadoop_b897d6c",
        "commit": "https://github.com/apache/hadoop/commit/b897d6c35a0036ab8b6a73f8dc76064f351b612d",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/b897d6c35a0036ab8b6a73f8dc76064f351b612d/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=b897d6c35a0036ab8b6a73f8dc76064f351b612d",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -558,6 +558,9 @@ Release 0.23.4 - UNRELEASED\n     MAPREDUCE-4691. Historyserver can report \"Unknown job\" after RM says job\n     has completed (Robert Joseph Evans via jlowe)\n \n+    MAPREDUCE-4689. JobClient.getMapTaskReports on failed job results in NPE\n+    (jlowe via bobby)\n+\n Release 0.23.3 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/b897d6c35a0036ab8b6a73f8dc76064f351b612d/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "9e98ac96e1d250ab9b6265c43322bbd37c6f80f3",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/b897d6c35a0036ab8b6a73f8dc76064f351b612d/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedTask.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedTask.java?ref=b897d6c35a0036ab8b6a73f8dc76064f351b612d",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedTask.java",
                "patch": "@@ -42,6 +42,8 @@\n \n public class CompletedTask implements Task {\n \n+  private static final Counters EMPTY_COUNTERS = new Counters();\n+\n   private final TaskId taskId;\n   private final TaskInfo taskInfo;\n   private TaskReport report;\n@@ -124,7 +126,11 @@ private void constructTaskReport() {\n     report.setFinishTime(taskInfo.getFinishTime());\n     report.setTaskState(getState());\n     report.setProgress(getProgress());\n-    report.setCounters(TypeConverter.toYarn(getCounters()));\n+    Counters counters = getCounters();\n+    if (counters == null) {\n+      counters = EMPTY_COUNTERS;\n+    }\n+    report.setCounters(TypeConverter.toYarn(counters));\n     if (successfulAttempt != null) {\n       report.setSuccessfulAttempt(successfulAttempt);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/b897d6c35a0036ab8b6a73f8dc76064f351b612d/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/main/java/org/apache/hadoop/mapreduce/v2/hs/CompletedTask.java",
                "sha": "830b64f1ad364a1d9982e3dad1b0bd37f2175732",
                "status": "modified"
            },
            {
                "additions": 78,
                "blob_url": "https://github.com/apache/hadoop/blob/b897d6c35a0036ab8b6a73f8dc76064f351b612d/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java",
                "changes": 78,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java?ref=b897d6c35a0036ab8b6a73f8dc76064f351b612d",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java",
                "patch": "@@ -49,6 +49,7 @@\n import org.apache.hadoop.mapreduce.v2.api.records.JobId;\n import org.apache.hadoop.mapreduce.v2.api.records.JobState;\n import org.apache.hadoop.mapreduce.v2.api.records.TaskAttemptId;\n+import org.apache.hadoop.mapreduce.v2.api.records.TaskId;\n import org.apache.hadoop.mapreduce.v2.api.records.TaskState;\n import org.apache.hadoop.mapreduce.v2.app.MRApp;\n import org.apache.hadoop.mapreduce.v2.app.job.Job;\n@@ -402,6 +403,63 @@ public void testHistoryParsingForFailedAttempts() throws Exception {\n     }\n   }\n   \n+  @Test\n+  public void testCountersForFailedTask() throws Exception {\n+    LOG.info(\"STARTING testCountersForFailedTask\");\n+    try {\n+    Configuration conf = new Configuration();\n+    conf\n+        .setClass(\n+            CommonConfigurationKeysPublic.NET_TOPOLOGY_NODE_SWITCH_MAPPING_IMPL_KEY,\n+            MyResolver.class, DNSToSwitchMapping.class);\n+    RackResolver.init(conf);\n+    MRApp app = new MRAppWithHistoryWithFailedTask(2, 1, true,\n+        this.getClass().getName(), true);\n+    app.submit(conf);\n+    Job job = app.getContext().getAllJobs().values().iterator().next();\n+    JobId jobId = job.getID();\n+    app.waitForState(job, JobState.FAILED);\n+\n+    // make sure all events are flushed\n+    app.waitForState(Service.STATE.STOPPED);\n+\n+    String jobhistoryDir = JobHistoryUtils\n+        .getHistoryIntermediateDoneDirForUser(conf);\n+    JobHistory jobHistory = new JobHistory();\n+    jobHistory.init(conf);\n+\n+    JobIndexInfo jobIndexInfo = jobHistory.getJobFileInfo(jobId)\n+        .getJobIndexInfo();\n+    String jobhistoryFileName = FileNameIndexUtils\n+        .getDoneFileName(jobIndexInfo);\n+\n+    Path historyFilePath = new Path(jobhistoryDir, jobhistoryFileName);\n+    FSDataInputStream in = null;\n+    FileContext fc = null;\n+    try {\n+      fc = FileContext.getFileContext(conf);\n+      in = fc.open(fc.makeQualified(historyFilePath));\n+    } catch (IOException ioe) {\n+      LOG.info(\"Can not open history file: \" + historyFilePath, ioe);\n+      throw (new Exception(\"Can not open History File\"));\n+    }\n+\n+    JobHistoryParser parser = new JobHistoryParser(in);\n+    JobInfo jobInfo = parser.parse();\n+    Exception parseException = parser.getParseException();\n+    Assert.assertNull(\"Caught an expected exception \" + parseException,\n+        parseException);\n+    for (Map.Entry<TaskID,TaskInfo> entry : jobInfo.getAllTasks().entrySet()) {\n+      TaskId yarnTaskID = TypeConverter.toYarn(entry.getKey());\n+      CompletedTask ct = new CompletedTask(yarnTaskID, entry.getValue());\n+      Assert.assertNotNull(\"completed task report has null counters\",\n+          ct.getReport().getCounters());\n+    }\n+    } finally {\n+      LOG.info(\"FINISHED testCountersForFailedTask\");\n+    }\n+  }\n+\n   static class MRAppWithHistoryWithFailedAttempt extends MRAppWithHistory {\n \n     public MRAppWithHistoryWithFailedAttempt(int maps, int reduces, boolean autoComplete,\n@@ -422,6 +480,26 @@ protected void attemptLaunched(TaskAttemptId attemptID) {\n     }\n   }\n \n+  static class MRAppWithHistoryWithFailedTask extends MRAppWithHistory {\n+\n+    public MRAppWithHistoryWithFailedTask(int maps, int reduces, boolean autoComplete,\n+        String testName, boolean cleanOnStart) {\n+      super(maps, reduces, autoComplete, testName, cleanOnStart);\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    @Override\n+    protected void attemptLaunched(TaskAttemptId attemptID) {\n+      if (attemptID.getTaskId().getId() == 0) {\n+        getContext().getEventHandler().handle(\n+            new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_FAILMSG));\n+      } else {\n+        getContext().getEventHandler().handle(\n+            new TaskAttemptEvent(attemptID, TaskAttemptEventType.TA_DONE));\n+      }\n+    }\n+  }\n+\n   public static void main(String[] args) throws Exception {\n     TestJobHistoryParsing t = new TestJobHistoryParsing();\n     t.testHistoryParsing();",
                "raw_url": "https://github.com/apache/hadoop/raw/b897d6c35a0036ab8b6a73f8dc76064f351b612d/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-hs/src/test/java/org/apache/hadoop/mapreduce/v2/hs/TestJobHistoryParsing.java",
                "sha": "f9acb1a38212081cd4aa43beb959dc58d3cd310d",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4689. JobClient.getMapTaskReports on failed job results in NPE (jlowe via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1391679 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/03b7ad04fadeb1a98271ac1775f900999989eafb",
        "repo": "hadoop",
        "unit_tests": [
            "TestCompletedTask.java"
        ]
    },
    "hadoop_b908c9e": {
        "bug_id": "hadoop_b908c9e",
        "commit": "https://github.com/apache/hadoop/commit/b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853/common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/common/CHANGES.txt?ref=b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853",
                "deletions": 0,
                "filename": "common/CHANGES.txt",
                "patch": "@@ -346,6 +346,9 @@ Trunk (unreleased changes)\n     HADOOP-7090. Fix resource leaks in s3.INode, BloomMapFile, WritableUtils\n     and CBZip2OutputStream.  (Uma Maheswara Rao G via szetszwo)\n \n+    HADOOP-7440. HttpServer.getParameterValues throws NPE for missing\n+    parameters. (Uma Maheswara Rao G and todd via todd)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853/common/CHANGES.txt",
                "sha": "c84dc559c8557a4434498e93f6e7e12e1e42daef",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853/common/src/java/org/apache/hadoop/http/HttpServer.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/common/src/java/org/apache/hadoop/http/HttpServer.java?ref=b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853",
                "deletions": 0,
                "filename": "common/src/java/org/apache/hadoop/http/HttpServer.java",
                "patch": "@@ -800,6 +800,9 @@ public String getParameter(String name) {\n       public String[] getParameterValues(String name) {\n         String unquoteName = HtmlQuoting.unquoteHtmlChars(name);\n         String[] unquoteValue = rawRequest.getParameterValues(unquoteName);\n+        if (unquoteValue == null) {\n+          return null;\n+        }\n         String[] result = new String[unquoteValue.length];\n         for(int i=0; i < result.length; ++i) {\n           result[i] = HtmlQuoting.quoteHtmlChars(unquoteValue[i]);",
                "raw_url": "https://github.com/apache/hadoop/raw/b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853/common/src/java/org/apache/hadoop/http/HttpServer.java",
                "sha": "6d6864c63dcb62429fd757215868e1cae03c94fd",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/hadoop/blob/b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853/common/src/test/core/org/apache/hadoop/http/TestHtmlQuoting.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/common/src/test/core/org/apache/hadoop/http/TestHtmlQuoting.java?ref=b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853",
                "deletions": 3,
                "filename": "common/src/test/core/org/apache/hadoop/http/TestHtmlQuoting.java",
                "patch": "@@ -17,11 +17,12 @@\n  */\n package org.apache.hadoop.http;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.*;\n+\n+import javax.servlet.http.HttpServletRequest;\n \n import org.junit.Test;\n+import org.mockito.Mockito;\n \n public class TestHtmlQuoting {\n \n@@ -62,4 +63,28 @@ private void runRoundTrip(String str) throws Exception {\n     }\n     runRoundTrip(buffer.toString());\n   }\n+  \n+\n+  @Test\n+  public void testRequestQuoting() throws Exception {\n+    HttpServletRequest mockReq = Mockito.mock(HttpServletRequest.class);\n+    HttpServer.QuotingInputFilter.RequestQuoter quoter =\n+      new HttpServer.QuotingInputFilter.RequestQuoter(mockReq);\n+    \n+    Mockito.doReturn(\"a<b\").when(mockReq).getParameter(\"x\");\n+    assertEquals(\"Test simple param quoting\",\n+        \"a&lt;b\", quoter.getParameter(\"x\"));\n+    \n+    Mockito.doReturn(null).when(mockReq).getParameter(\"x\");\n+    assertEquals(\"Test that missing parameters dont cause NPE\",\n+        null, quoter.getParameter(\"x\"));\n+\n+    Mockito.doReturn(new String[]{\"a<b\", \"b\"}).when(mockReq).getParameterValues(\"x\");\n+    assertArrayEquals(\"Test escaping of an array\",\n+        new String[]{\"a&lt;b\", \"b\"}, quoter.getParameterValues(\"x\"));\n+\n+    Mockito.doReturn(null).when(mockReq).getParameterValues(\"x\");\n+    assertArrayEquals(\"Test that missing parameters dont cause NPE for array\",\n+        null, quoter.getParameterValues(\"x\"));\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853/common/src/test/core/org/apache/hadoop/http/TestHtmlQuoting.java",
                "sha": "9fc53a3b6fb9d50beafba1b66721310239b18b8b",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop/blob/b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853/common/src/test/core/org/apache/hadoop/http/TestHttpServer.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/common/src/test/core/org/apache/hadoop/http/TestHttpServer.java?ref=b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853",
                "deletions": 0,
                "filename": "common/src/test/core/org/apache/hadoop/http/TestHttpServer.java",
                "patch": "@@ -45,16 +45,20 @@\n import javax.servlet.http.HttpServletRequestWrapper;\n import javax.servlet.http.HttpServletResponse;\n \n+import junit.framework.Assert;\n+\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.CommonConfigurationKeys;\n+import org.apache.hadoop.http.HttpServer.QuotingInputFilter.RequestQuoter;\n import org.apache.hadoop.security.Groups;\n import org.apache.hadoop.security.ShellBasedUnixGroupsMapping;\n import org.apache.hadoop.security.authorize.AccessControlList;\n import org.junit.AfterClass;\n import org.junit.BeforeClass;\n import org.junit.Test;\n+import org.mockito.Mockito;\n \n public class TestHttpServer extends HttpServerFunctionalTest {\n   private static HttpServer server;\n@@ -379,4 +383,26 @@ public void testAuthorizationOfDefaultServlets() throws Exception {\n     }\n     myServer.stop();\n   }\n+  \n+  @Test\n+  public void testRequestQuoterWithNull() throws Exception {\n+    HttpServletRequest request = Mockito.mock(HttpServletRequest.class);\n+    Mockito.doReturn(null).when(request).getParameterValues(\"dummy\");\n+    RequestQuoter requestQuoter = new RequestQuoter(request);\n+    String[] parameterValues = requestQuoter.getParameterValues(\"dummy\");\n+    Assert.assertEquals(\"It should return null \"\n+        + \"when there are no values for the parameter\", null, parameterValues);\n+  }\n+\n+  @Test\n+  public void testRequestQuoterWithNotNull() throws Exception {\n+    HttpServletRequest request = Mockito.mock(HttpServletRequest.class);\n+    String[] values = new String[] { \"abc\", \"def\" };\n+    Mockito.doReturn(values).when(request).getParameterValues(\"dummy\");\n+    RequestQuoter requestQuoter = new RequestQuoter(request);\n+    String[] parameterValues = requestQuoter.getParameterValues(\"dummy\");\n+    Assert.assertTrue(\"It should return Parameter Values\", Arrays.equals(\n+        values, parameterValues));\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/b908c9eb0e010ed62d6fd1c7bd1ec4ca5bdc1853/common/src/test/core/org/apache/hadoop/http/TestHttpServer.java",
                "sha": "0a25236fc8381e3ece6424ab16a94c890afc2f0d",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7440. HttpServer.getParameterValues throws NPE for missing parameters. Contributed by Uma Maheswara Rao G and Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1143212 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/d7f712cd4262f51ea2972596ce0a48cde623ecf9",
        "repo": "hadoop",
        "unit_tests": [
            "TestHttpServer.java"
        ]
    },
    "hadoop_b9d561c": {
        "bug_id": "hadoop_b9d561c",
        "commit": "https://github.com/apache/hadoop/commit/b9d561c548c26d0db4994e6c13c7ebf43705d794",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "patch": "@@ -49,3 +49,5 @@ IMPROVEMENTS:\n     HDFS-5401. Fix NPE in Directory Scanner. (Arpit Agarwal)\n \n     HDFS-5417. Fix storage IDs in PBHelper and UpgradeUtilities.  (szetszwo)\n+\n+    HDFS-5214. Fix NPEs in BlockManager and DirectoryScanner. (Arpit Agarwal)",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "sha": "d878b66b3032b4dcaa4bfc1dbe688bbd82fcf1d9",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -1833,7 +1833,10 @@ private void reportDiff(DatanodeDescriptor dn, DatanodeStorage storage,\n       ReplicaState iState = itBR.getCurrentReplicaState();\n       BlockInfo storedBlock = processReportedBlock(dn, storage.getStorageID(),\n           iblk, iState, toAdd, toInvalidate, toCorrupt, toUC);\n-      toRemove.remove(storedBlock);\n+\n+      if (storedBlock != null) {\n+        toRemove.remove(storedBlock);\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "6d5c604ba7dbc631855003ffb675575f3d842d5d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java",
                "patch": "@@ -187,7 +187,7 @@ public LinkedElement getNext() {\n         + hours + \" hours for block pool \" + bpid);\n \n     // get the list of blocks and arrange them in random order\n-    List<Block> arr = dataset.getFinalizedBlocks(blockPoolId);\n+    List<FinalizedReplica> arr = dataset.getFinalizedBlocks(blockPoolId);\n     Collections.shuffle(arr);\n     \n     long scanTime = -1;",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BlockPoolSliceScanner.java",
                "sha": "13a83bce5fdc7a701bba62f192f37f588df07ead",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 10,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "patch": "@@ -230,10 +230,6 @@ private static String getSuffix(File f, String prefix) {\n       throw new RuntimeException(prefix + \" is not a prefix of \" + fullPath);\n     }\n \n-    ScanInfo(long blockId) {\n-      this(blockId, null, null, null);\n-    }\n-\n     ScanInfo(long blockId, File blockFile, File metaFile, FsVolumeSpi vol) {\n       this.blockId = blockId;\n       String condensedVolPath = vol == null ? null :\n@@ -439,8 +435,8 @@ void scan() {\n         diffs.put(bpid, diffRecord);\n         \n         statsRecord.totalBlocks = blockpoolReport.length;\n-        List<Block> bl = dataset.getFinalizedBlocks(bpid);\n-        Block[] memReport = bl.toArray(new Block[bl.size()]);\n+        List<FinalizedReplica> bl = dataset.getFinalizedBlocks(bpid);\n+        FinalizedReplica[] memReport = bl.toArray(new FinalizedReplica[bl.size()]);\n         Arrays.sort(memReport); // Sort based on blockId\n   \n         int d = 0; // index for blockpoolReport\n@@ -458,7 +454,8 @@ void scan() {\n           }\n           if (info.getBlockId() > memBlock.getBlockId()) {\n             // Block is missing on the disk\n-            addDifference(diffRecord, statsRecord, memBlock.getBlockId());\n+            addDifference(diffRecord, statsRecord,\n+                          memBlock.getBlockId(), info.getVolume());\n             m++;\n             continue;\n           }\n@@ -478,7 +475,9 @@ void scan() {\n           m++;\n         }\n         while (m < memReport.length) {\n-          addDifference(diffRecord, statsRecord, memReport[m++].getBlockId());\n+          FinalizedReplica current = memReport[m++];\n+          addDifference(diffRecord, statsRecord,\n+                        current.getBlockId(), current.getVolume());\n         }\n         while (d < blockpoolReport.length) {\n           statsRecord.missingMemoryBlocks++;\n@@ -502,10 +501,11 @@ private void addDifference(LinkedList<ScanInfo> diffRecord,\n \n   /** Block is not found on the disk */\n   private void addDifference(LinkedList<ScanInfo> diffRecord,\n-                             Stats statsRecord, long blockId) {\n+                             Stats statsRecord, long blockId,\n+                             FsVolumeSpi vol) {\n     statsRecord.missingBlockFile++;\n     statsRecord.missingMetaFile++;\n-    diffRecord.add(new ScanInfo(blockId));\n+    diffRecord.add(new ScanInfo(blockId, null, null, vol));\n   }\n \n   /** Is the given volume still valid in the dataset? */",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DirectoryScanner.java",
                "sha": "17ec35d6fb670e13c585e456bbdfafc917a8d991",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java",
                "patch": "@@ -61,6 +61,10 @@ public FinalizedReplica(FinalizedReplica from) {\n     this.unlinked = from.isUnlinked();\n   }\n \n+  public FinalizedReplica(ReplicaInfo replicaInfo) {\n+    super(replicaInfo);\n+  }\n+\n   @Override  // ReplicaInfo\n   public ReplicaState getState() {\n     return ReplicaState.FINALIZED;",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/FinalizedReplica.java",
                "sha": "1a852c346689ca36638e027e966dcc8ae1b080c2",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java",
                "patch": "@@ -34,6 +34,7 @@\n import org.apache.hadoop.hdfs.protocol.HdfsBlocksMetadata;\n import org.apache.hadoop.hdfs.server.datanode.DataNode;\n import org.apache.hadoop.hdfs.server.datanode.DataStorage;\n+import org.apache.hadoop.hdfs.server.datanode.FinalizedReplica;\n import org.apache.hadoop.hdfs.server.datanode.Replica;\n import org.apache.hadoop.hdfs.server.datanode.ReplicaInPipelineInterface;\n import org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.FsDatasetFactory;\n@@ -98,7 +99,7 @@ public RollingLogs createRollingLogs(String bpid, String prefix\n   public Map<String, Object> getVolumeInfoMap();\n \n   /** @return a list of finalized blocks for the given block pool. */\n-  public List<Block> getFinalizedBlocks(String bpid);\n+  public List<FinalizedReplica> getFinalizedBlocks(String bpid);\n \n   /**\n    * Check whether the in-memory block record matches the block on the disk,",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/FsDatasetSpi.java",
                "sha": "90edd5104ffbf33263006a7c4a17a07433bb74f4",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "patch": "@@ -1079,11 +1079,12 @@ public BlockListAsLongs getBlockReport(String bpid) {\n    * Get the list of finalized blocks from in-memory blockmap for a block pool.\n    */\n   @Override\n-  public synchronized List<Block> getFinalizedBlocks(String bpid) {\n-    ArrayList<Block> finalized = new ArrayList<Block>(volumeMap.size(bpid));\n+  public synchronized List<FinalizedReplica> getFinalizedBlocks(String bpid) {\n+    ArrayList<FinalizedReplica> finalized =\n+        new ArrayList<FinalizedReplica>(volumeMap.size(bpid));\n     for (ReplicaInfo b : volumeMap.replicas(bpid)) {\n       if(b.getState() == ReplicaState.FINALIZED) {\n-        finalized.add(new Block(b));\n+        finalized.add(new FinalizedReplica(b));\n       }\n     }\n     return finalized;",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "sha": "8677131d4abbe7ffcae4aa85738d8991ce486dcc",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "patch": "@@ -1006,7 +1006,7 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,\n   }\n \n   @Override\n-  public List<Block> getFinalizedBlocks(String bpid) {\n+  public List<FinalizedReplica> getFinalizedBlocks(String bpid) {\n     throw new UnsupportedOperationException();\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/SimulatedFSDataset.java",
                "sha": "6f3bed9fda04b728c1665a972ff34b09696110fd",
                "status": "modified"
            },
            {
                "additions": 115,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockReport.java",
                "changes": 163,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockReport.java?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 48,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockReport.java",
                "patch": "@@ -17,14 +17,17 @@\n  */\n package org.apache.hadoop.hdfs.server.datanode;\n \n+import static org.hamcrest.core.Is.is;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertThat;\n import static org.junit.Assert.assertTrue;\n \n import java.io.File;\n import java.io.FilenameFilter;\n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.List;\n+import java.util.Map;\n import java.util.Random;\n import java.util.concurrent.CountDownLatch;\n import java.util.concurrent.TimeoutException;\n@@ -89,7 +92,7 @@\n   private MiniDFSCluster cluster;\n   private DistributedFileSystem fs;\n \n-  Random rand = new Random(RAND_LIMIT);\n+  private static Random rand = new Random(RAND_LIMIT);\n \n   private static Configuration conf;\n \n@@ -113,6 +116,57 @@ public void shutDownCluster() throws IOException {\n     cluster.shutdown();\n   }\n \n+  private static StorageBlockReport[] getBlockReports(DataNode dn, String bpid) {\n+    Map<String, BlockListAsLongs> perVolumeBlockLists =\n+        dn.getFSDataset().getBlockReports(bpid);\n+\n+    // Send block report\n+    StorageBlockReport[] reports =\n+        new StorageBlockReport[perVolumeBlockLists.size()];\n+\n+    int i = 0;\n+    for(Map.Entry<String, BlockListAsLongs> kvPair : perVolumeBlockLists.entrySet()) {\n+      String storageID = kvPair.getKey();\n+      long[] blockList = kvPair.getValue().getBlockListAsLongs();\n+\n+      // Dummy DatanodeStorage object just for sending the block report.\n+      DatanodeStorage dnStorage = new DatanodeStorage(storageID);\n+      reports[i++] = new StorageBlockReport(dnStorage, blockList);\n+    }\n+\n+    return reports;\n+  }\n+\n+  // Get block reports but modify the GS of one of the blocks.\n+  private static StorageBlockReport[] getBlockReportsCorruptSingleBlockGS(\n+      DataNode dn, String bpid) {\n+    Map<String, BlockListAsLongs> perVolumeBlockLists =\n+        dn.getFSDataset().getBlockReports(bpid);\n+\n+    // Send block report\n+    StorageBlockReport[] reports =\n+        new StorageBlockReport[perVolumeBlockLists.size()];\n+\n+    boolean corruptedBlock = false;\n+\n+    int i = 0;\n+    for(Map.Entry<String, BlockListAsLongs> kvPair : perVolumeBlockLists.entrySet()) {\n+      String storageID = kvPair.getKey();\n+      long[] blockList = kvPair.getValue().getBlockListAsLongs();\n+\n+      if (!corruptedBlock) {\n+        blockList[4] = rand.nextInt();      // Bad GS.\n+        corruptedBlock = true;\n+      }\n+\n+      // Dummy DatanodeStorage object just for sending the block report.\n+      DatanodeStorage dnStorage = new DatanodeStorage(storageID);\n+      reports[i++] = new StorageBlockReport(dnStorage, blockList);\n+    }\n+\n+    return reports;\n+  }\n+\n   /**\n    * Test write a file, verifies and closes it. Then the length of the blocks\n    * are messed up and BlockReport is forced.\n@@ -153,10 +207,8 @@ public void blockReport_01() throws IOException {\n     DataNode dn = cluster.getDataNodes().get(DN_N0);\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n-    cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n+    StorageBlockReport[] reports = getBlockReports(dn, poolId);\n+    cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n \n     List<LocatedBlock> blocksAfterReport =\n       DFSTestUtil.getAllBlocks(fs.open(filePath));\n@@ -211,7 +263,6 @@ public void blockReport_02() throws IOException {\n     for (Integer aRemovedIndex : removedIndex) {\n       blocks2Remove.add(lBlocks.get(aRemovedIndex).getBlock());\n     }\n-    ArrayList<Block> blocks = locatedToBlocks(lBlocks, removedIndex);\n \n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Number of blocks allocated \" + lBlocks.size());\n@@ -225,8 +276,11 @@ public void blockReport_02() throws IOException {\n       for (File f : findAllFiles(dataDir,\n         new MyFileFilter(b.getBlockName(), true))) {\n         DataNodeTestUtils.getFSDataset(dn0).unfinalizeBlock(b);\n-        if (!f.delete())\n+        if (!f.delete()) {\n           LOG.warn(\"Couldn't delete \" + b.getBlockName());\n+        } else {\n+          LOG.debug(\"Deleted file \" + f.toString());\n+        }\n       }\n     }\n \n@@ -235,10 +289,8 @@ public void blockReport_02() throws IOException {\n     // all blocks belong to the same file, hence same BP\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn0.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n-    cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n+    StorageBlockReport[] reports = getBlockReports(dn0, poolId);\n+    cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n \n     BlockManagerTestUtil.getComputedDatanodeWork(cluster.getNamesystem()\n         .getBlockManager());\n@@ -253,9 +305,8 @@ public void blockReport_02() throws IOException {\n \n \n   /**\n-   * Test writes a file and closes it. Then test finds a block\n-   * and changes its GS to be < of original one.\n-   * New empty block is added to the list of blocks.\n+   * Test writes a file and closes it.\n+   * Block reported is generated with a bad GS for a single block.\n    * Block report is forced and the check for # of corrupted blocks is performed.\n    *\n    * @throws IOException in case of an error\n@@ -264,41 +315,65 @@ public void blockReport_02() throws IOException {\n   public void blockReport_03() throws IOException {\n     final String METHOD_NAME = GenericTestUtils.getMethodName();\n     Path filePath = new Path(\"/\" + METHOD_NAME + \".dat\");\n-\n-    ArrayList<Block> blocks =\n-      prepareForRide(filePath, METHOD_NAME, FILE_SIZE);\n-\n-    // The block with modified GS won't be found. Has to be deleted\n-    blocks.get(0).setGenerationStamp(rand.nextLong());\n-    // This new block is unknown to NN and will be mark for deletion.\n-    blocks.add(new Block());\n+    DFSTestUtil.createFile(fs, filePath,\n+                           FILE_SIZE, REPL_FACTOR, rand.nextLong());\n     \n     // all blocks belong to the same file, hence same BP\n     DataNode dn = cluster.getDataNodes().get(DN_N0);\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n+    StorageBlockReport[] reports = getBlockReportsCorruptSingleBlockGS(dn, poolId);\n     DatanodeCommand dnCmd =\n-      cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n+      cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n     if(LOG.isDebugEnabled()) {\n       LOG.debug(\"Got the command: \" + dnCmd);\n     }\n     printStats();\n \n-    assertEquals(\"Wrong number of CorruptedReplica+PendingDeletion \" +\n-      \"blocks is found\", 2,\n-        cluster.getNamesystem().getCorruptReplicaBlocks() +\n-        cluster.getNamesystem().getPendingDeletionBlocks());\n+    assertThat(\"Wrong number of corrupt blocks\",\n+               cluster.getNamesystem().getCorruptReplicaBlocks(), is(1L));\n+    assertThat(\"Wrong number of PendingDeletion blocks\",\n+               cluster.getNamesystem().getPendingDeletionBlocks(), is(0L));\n   }\n \n   /**\n-   * This test isn't a representative case for BlockReport\n-   * The empty method is going to be left here to keep the naming\n-   * of the test plan in synch with the actual implementation\n+   * Test writes a file and closes it.\n+   * Block reported is generated with an extra block.\n+   * Block report is forced and the check for # of pendingdeletion\n+   * blocks is performed.\n+   *\n+   * @throws IOException in case of an error\n    */\n-  public void blockReport_04() {\n+  @Test\n+  public void blockReport_04() throws IOException {\n+    final String METHOD_NAME = GenericTestUtils.getMethodName();\n+    Path filePath = new Path(\"/\" + METHOD_NAME + \".dat\");\n+    DFSTestUtil.createFile(fs, filePath,\n+                           FILE_SIZE, REPL_FACTOR, rand.nextLong());\n+\n+\n+    DataNode dn = cluster.getDataNodes().get(DN_N0);\n+    // all blocks belong to the same file, hence same BP\n+    String poolId = cluster.getNamesystem().getBlockPoolId();\n+\n+    // Create a bogus new block which will not be present on the namenode.\n+    ExtendedBlock b = new ExtendedBlock(\n+        poolId, rand.nextLong(), 1024L, rand.nextLong());\n+    dn.getFSDataset().createRbw(b);\n+\n+    DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n+    StorageBlockReport[] reports = getBlockReports(dn, poolId);\n+    DatanodeCommand dnCmd =\n+        cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n+    if(LOG.isDebugEnabled()) {\n+      LOG.debug(\"Got the command: \" + dnCmd);\n+    }\n+    printStats();\n+\n+    assertThat(\"Wrong number of corrupt blocks\",\n+               cluster.getNamesystem().getCorruptReplicaBlocks(), is(0L));\n+    assertThat(\"Wrong number of PendingDeletion blocks\",\n+               cluster.getNamesystem().getPendingDeletionBlocks(), is(1L));\n   }\n \n   // Client requests new block from NN. The test corrupts this very block\n@@ -331,10 +406,8 @@ public void blockReport_06() throws Exception {\n     DataNode dn = cluster.getDataNodes().get(DN_N1);\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n-    cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n+    StorageBlockReport[] reports = getBlockReports(dn, poolId);\n+    cluster.getNameNodeRpc().blockReport(dnR, poolId, reports);\n     printStats();\n     assertEquals(\"Wrong number of PendingReplication Blocks\",\n       0, cluster.getNamesystem().getUnderReplicatedBlocks());\n@@ -382,9 +455,7 @@ public void blockReport_07() throws Exception {\n     DataNode dn = cluster.getDataNodes().get(DN_N1);\n     String poolId = cluster.getNamesystem().getBlockPoolId();\n     DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-    StorageBlockReport[] report = { new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n-        new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n+    StorageBlockReport[] report = getBlockReports(dn, poolId);\n     cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n     printStats();\n     assertEquals(\"Wrong number of Corrupted blocks\",\n@@ -407,7 +478,7 @@ public void blockReport_07() throws Exception {\n     }\n     \n     report[0] = new StorageBlockReport(\n-        new DatanodeStorage(dnR.getDatanodeUuid()),\n+        report[0].getStorage(),\n         new BlockListAsLongs(blocks, null).getBlockListAsLongs());\n     cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n     printStats();\n@@ -458,9 +529,7 @@ public void blockReport_08() throws IOException {\n       DataNode dn = cluster.getDataNodes().get(DN_N1);\n       String poolId = cluster.getNamesystem().getBlockPoolId();\n       DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-      StorageBlockReport[] report = { new StorageBlockReport(\n-          new DatanodeStorage(dnR.getDatanodeUuid()),\n-          new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n+      StorageBlockReport[] report = getBlockReports(dn, poolId);\n       cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n       printStats();\n       assertEquals(\"Wrong number of PendingReplication blocks\",\n@@ -506,9 +575,7 @@ public void blockReport_09() throws IOException {\n       DataNode dn = cluster.getDataNodes().get(DN_N1);\n       String poolId = cluster.getNamesystem().getBlockPoolId();\n       DatanodeRegistration dnR = dn.getDNRegistrationForBP(poolId);\n-      StorageBlockReport[] report = { new StorageBlockReport(\n-          new DatanodeStorage(dnR.getDatanodeUuid()),\n-          new BlockListAsLongs(blocks, null).getBlockListAsLongs()) };\n+      StorageBlockReport[] report = getBlockReports(dn, poolId);\n       cluster.getNameNodeRpc().blockReport(dnR, poolId, report);\n       printStats();\n       assertEquals(\"Wrong number of PendingReplication blocks\",",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestBlockReport.java",
                "sha": "21d0339888a6729d0c75a8febe0d7c9413df252c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java?ref=b9d561c548c26d0db4994e6c13c7ebf43705d794",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java",
                "patch": "@@ -447,7 +447,7 @@ void testScanInfoObject(long blockId, File blockFile, File metaFile)\n   \n   void testScanInfoObject(long blockId) throws Exception {\n     DirectoryScanner.ScanInfo scanInfo =\n-        new DirectoryScanner.ScanInfo(blockId);\n+        new DirectoryScanner.ScanInfo(blockId, null, null, null);\n     assertEquals(blockId, scanInfo.getBlockId());\n     assertNull(scanInfo.getBlockFile());\n     assertNull(scanInfo.getMetaFile());",
                "raw_url": "https://github.com/apache/hadoop/raw/b9d561c548c26d0db4994e6c13c7ebf43705d794/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDirectoryScanner.java",
                "sha": "f5b535d394363b27012b10a4b62bf1d7de7481d1",
                "status": "modified"
            }
        ],
        "message": "HDFS-5214. Fix NPEs in BlockManager and DirectoryScanner.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1536179 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/9043a92219149390d81f7443180ff62efc9b4c29",
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockManager.java",
            "TestDirectoryScanner.java",
            "TestFsDatasetImpl.java",
            "TestSimulatedFSDataset.java"
        ]
    },
    "hadoop_b9e74da": {
        "bug_id": "hadoop_b9e74da",
        "commit": "https://github.com/apache/hadoop/commit/b9e74da41b750ff93f2524da09f06ded1a7bd6e2",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/b9e74da41b750ff93f2524da09f06ded1a7bd6e2/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=b9e74da41b750ff93f2524da09f06ded1a7bd6e2",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -180,3 +180,5 @@ HDFS-2733. Document HA configuration and CLI. (atm)\n HDFS-2794. Active NN may purge edit log files before standby NN has a chance to read them (todd)\n \n HDFS-2901. Improvements for SBN web UI - not show under-replicated/missing blocks. (Brandon Li via jitendra)\n+\n+HDFS-2905. HA: Standby NN NPE when shared edits dir is deleted. (Bikas Saha via jitendra)",
                "raw_url": "https://github.com/apache/hadoop/raw/b9e74da41b750ff93f2524da09f06ded1a7bd6e2/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "36c162482b0eb0bf27c12881b58dd11c50b67e73",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop/blob/b9e74da41b750ff93f2524da09f06ded1a7bd6e2/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java?ref=b9e74da41b750ff93f2524da09f06ded1a7bd6e2",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java",
                "patch": "@@ -135,8 +135,7 @@ public void purgeLogsOlderThan(long minTxIdToKeep)\n    */\n   List<RemoteEditLog> getRemoteEditLogs(long firstTxId) throws IOException {\n     File currentDir = sd.getCurrentDir();\n-    List<EditLogFile> allLogFiles = matchEditLogs(\n-        FileUtil.listFiles(currentDir));\n+    List<EditLogFile> allLogFiles = matchEditLogs(currentDir);\n     List<RemoteEditLog> ret = Lists.newArrayListWithCapacity(\n         allLogFiles.size());\n \n@@ -155,6 +154,20 @@ public void purgeLogsOlderThan(long minTxIdToKeep)\n     return ret;\n   }\n \n+  /**\n+   * returns matching edit logs via the log directory. Simple helper function\n+   * that lists the files in the logDir and calls matchEditLogs(File[])\n+   * \n+   * @param logDir\n+   *          directory to match edit logs in\n+   * @return matched edit logs\n+   * @throws IOException\n+   *           IOException thrown for invalid logDir\n+   */\n+  static List<EditLogFile> matchEditLogs(File logDir) throws IOException {\n+    return matchEditLogs(FileUtil.listFiles(logDir));\n+  }\n+  \n   static List<EditLogFile> matchEditLogs(File[] filesInStorage) {\n     List<EditLogFile> ret = Lists.newArrayList();\n     for (File f : filesInStorage) {\n@@ -278,7 +291,7 @@ public long getNumberOfTransactions(long fromTxId, boolean inProgressOk)\n   synchronized public void recoverUnfinalizedSegments() throws IOException {\n     File currentDir = sd.getCurrentDir();\n     LOG.info(\"Recovering unfinalized segments in \" + currentDir);\n-    List<EditLogFile> allLogFiles = matchEditLogs(currentDir.listFiles());\n+    List<EditLogFile> allLogFiles = matchEditLogs(currentDir);\n \n     for (EditLogFile elf : allLogFiles) {\n       if (elf.getFile().equals(currentInProgress)) {\n@@ -318,7 +331,7 @@ synchronized public void recoverUnfinalizedSegments() throws IOException {\n \n   private List<EditLogFile> getLogFiles(long fromTxId) throws IOException {\n     File currentDir = sd.getCurrentDir();\n-    List<EditLogFile> allLogFiles = matchEditLogs(currentDir.listFiles());\n+    List<EditLogFile> allLogFiles = matchEditLogs(currentDir);\n     List<EditLogFile> logFiles = Lists.newArrayList();\n     \n     for (EditLogFile elf : allLogFiles) {",
                "raw_url": "https://github.com/apache/hadoop/raw/b9e74da41b750ff93f2524da09f06ded1a7bd6e2/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FileJournalManager.java",
                "sha": "1eca2797b44c09be1830b59be2c639d7f061435b",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/b9e74da41b750ff93f2524da09f06ded1a7bd6e2/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java?ref=b9e74da41b750ff93f2524da09f06ded1a7bd6e2",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java",
                "patch": "@@ -440,7 +440,7 @@ public static EditLogFile findLatestEditsLog(StorageDirectory sd)\n   throws IOException {\n     File currentDir = sd.getCurrentDir();\n     List<EditLogFile> foundEditLogs \n-      = Lists.newArrayList(FileJournalManager.matchEditLogs(currentDir.listFiles()));\n+      = Lists.newArrayList(FileJournalManager.matchEditLogs(currentDir));\n     return Collections.max(foundEditLogs, EditLogFile.COMPARE_BY_START_TXID);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/b9e74da41b750ff93f2524da09f06ded1a7bd6e2/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/FSImageTestUtil.java",
                "sha": "665e088cb800bd95af4a5a4771d5f5b280e188b6",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/b9e74da41b750ff93f2524da09f06ded1a7bd6e2/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileJournalManager.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileJournalManager.java?ref=b9e74da41b750ff93f2524da09f06ded1a7bd6e2",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileJournalManager.java",
                "patch": "@@ -315,6 +315,15 @@ public void testGetRemoteEditLog() throws IOException {\n         \"\", getLogsAsString(fjm, 9999));\n   }\n \n+  /**\n+   * tests that passing an invalid dir to matchEditLogs throws IOException \n+   */\n+  @Test(expected = IOException.class)\n+  public void testMatchEditLogInvalidDirThrowsIOException() throws IOException {\n+    File badDir = new File(\"does not exist\");\n+    FileJournalManager.matchEditLogs(badDir);\n+  }\n+  \n   /**\n    * Make sure that we starting reading the correct op when we request a stream\n    * with a txid in the middle of an edit log file.",
                "raw_url": "https://github.com/apache/hadoop/raw/b9e74da41b750ff93f2524da09f06ded1a7bd6e2/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestFileJournalManager.java",
                "sha": "def293657768e434d587492e65dea58631153e45",
                "status": "modified"
            }
        ],
        "message": "HDFS-2905. HA: Standby NN NPE when shared edits dir is deleted. Contributed by Bikas Saha.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1241757 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/acacde55e6a4488cd749eba630ff2e68c4dc5c63",
        "repo": "hadoop",
        "unit_tests": [
            "TestFileJournalManager.java"
        ]
    },
    "hadoop_bbe8591": {
        "bug_id": "hadoop_bbe8591",
        "commit": "https://github.com/apache/hadoop/commit/bbe859177d67fcdfd5377b1abff4a637fbbd4587",
        "file": [
            {
                "additions": 129,
                "blob_url": "https://github.com/apache/hadoop/blob/bbe859177d67fcdfd5377b1abff4a637fbbd4587/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java",
                "changes": 149,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java?ref=bbe859177d67fcdfd5377b1abff4a637fbbd4587",
                "deletions": 20,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java",
                "patch": "@@ -168,8 +168,12 @@ public void close() {\n     }\n   }\n \n-  private FederationMetrics getFederationMetrics() {\n-    return this.router.getMetrics();\n+  private FederationMetrics getFederationMetrics() throws IOException {\n+    FederationMetrics metrics = getRouter().getMetrics();\n+    if (metrics == null) {\n+      throw new IOException(\"Federated metrics is not initialized\");\n+    }\n+    return metrics;\n   }\n \n   /////////////////////////////////////////////////////////\n@@ -188,22 +192,42 @@ public String getSoftwareVersion() {\n \n   @Override\n   public long getUsed() {\n-    return getFederationMetrics().getUsedCapacity();\n+    try {\n+      return getFederationMetrics().getUsedCapacity();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get the used capacity\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public long getFree() {\n-    return getFederationMetrics().getRemainingCapacity();\n+    try {\n+      return getFederationMetrics().getRemainingCapacity();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get remaining capacity\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public long getTotal() {\n-    return getFederationMetrics().getTotalCapacity();\n+    try {\n+      return getFederationMetrics().getTotalCapacity();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to Get total capacity\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public long getProvidedCapacity() {\n-    return getFederationMetrics().getProvidedSpace();\n+    try {\n+      return getFederationMetrics().getProvidedSpace();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get provided capacity\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n@@ -261,39 +285,79 @@ public float getPercentBlockPoolUsed() {\n \n   @Override\n   public long getTotalBlocks() {\n-    return getFederationMetrics().getNumBlocks();\n+    try {\n+      return getFederationMetrics().getNumBlocks();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of blocks\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public long getNumberOfMissingBlocks() {\n-    return getFederationMetrics().getNumOfMissingBlocks();\n+    try {\n+      return getFederationMetrics().getNumOfMissingBlocks();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of missing blocks\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   @Deprecated\n   public long getPendingReplicationBlocks() {\n-    return getFederationMetrics().getNumOfBlocksPendingReplication();\n+    try {\n+      return getFederationMetrics().getNumOfBlocksPendingReplication();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of blocks pending replica\",\n+          e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public long getPendingReconstructionBlocks() {\n-    return getFederationMetrics().getNumOfBlocksPendingReplication();\n+    try {\n+      return getFederationMetrics().getNumOfBlocksPendingReplication();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of blocks pending replica\",\n+          e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   @Deprecated\n   public long getUnderReplicatedBlocks() {\n-    return getFederationMetrics().getNumOfBlocksUnderReplicated();\n+    try {\n+      return getFederationMetrics().getNumOfBlocksUnderReplicated();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of blocks under replicated\",\n+          e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public long getLowRedundancyBlocks() {\n-    return getFederationMetrics().getNumOfBlocksUnderReplicated();\n+    try {\n+      return getFederationMetrics().getNumOfBlocksUnderReplicated();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of blocks under replicated\",\n+          e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public long getPendingDeletionBlocks() {\n-    return getFederationMetrics().getNumOfBlocksPendingDeletion();\n+    try {\n+      return getFederationMetrics().getNumOfBlocksPendingDeletion();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of blocks pending deletion\",\n+          e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n@@ -471,7 +535,12 @@ public String getJournalTransactionInfo() {\n \n   @Override\n   public long getNNStartedTimeInMillis() {\n-    return this.router.getStartTime();\n+    try {\n+      return getRouter().getStartTime();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get the router startup time\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n@@ -527,7 +596,12 @@ public long getProvidedCapacityTotal() {\n \n   @Override\n   public long getFilesTotal() {\n-    return getFederationMetrics().getNumFiles();\n+    try {\n+      return getFederationMetrics().getNumFiles();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of files\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n@@ -537,12 +611,22 @@ public int getTotalLoad() {\n \n   @Override\n   public int getNumLiveDataNodes() {\n-    return this.router.getMetrics().getNumLiveNodes();\n+    try {\n+      return getFederationMetrics().getNumLiveNodes();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of live nodes\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public int getNumDeadDataNodes() {\n-    return this.router.getMetrics().getNumDeadNodes();\n+    try {\n+      return getFederationMetrics().getNumDeadNodes();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of dead nodes\", e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n@@ -552,17 +636,35 @@ public int getNumStaleDataNodes() {\n \n   @Override\n   public int getNumDecomLiveDataNodes() {\n-    return this.router.getMetrics().getNumDecomLiveNodes();\n+    try {\n+      return getFederationMetrics().getNumDecomLiveNodes();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get the number of live decommissioned datanodes\",\n+          e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public int getNumDecomDeadDataNodes() {\n-    return this.router.getMetrics().getNumDecomDeadNodes();\n+    try {\n+      return getFederationMetrics().getNumDecomDeadNodes();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get the number of dead decommissioned datanodes\",\n+          e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n   public int getNumDecommissioningDataNodes() {\n-    return this.router.getMetrics().getNumDecommissioningNodes();\n+    try {\n+      return getFederationMetrics().getNumDecommissioningNodes();\n+    } catch (IOException e) {\n+      LOG.debug(\"Failed to get number of decommissioning nodes\",\n+          e.getMessage());\n+    }\n+    return 0;\n   }\n \n   @Override\n@@ -702,4 +804,11 @@ public int getNumEncryptionZones() {\n   public String getVerifyECWithTopologyResult() {\n     return null;\n   }\n+\n+  private Router getRouter() throws IOException {\n+    if (this.router == null) {\n+      throw new IOException(\"Router is not initialized\");\n+    }\n+    return this.router;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/bbe859177d67fcdfd5377b1abff4a637fbbd4587/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/metrics/NamenodeBeanMetrics.java",
                "sha": "a05fdc144989dec0f4e71e5cee9747f797f6da72",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/bbe859177d67fcdfd5377b1abff4a637fbbd4587/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/Router.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/Router.java?ref=bbe859177d67fcdfd5377b1abff4a637fbbd4587",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/Router.java",
                "patch": "@@ -586,11 +586,11 @@ public FederationMetrics getMetrics() {\n    *\n    * @return Namenode metrics.\n    */\n-  public NamenodeBeanMetrics getNamenodeMetrics() {\n-    if (this.metrics != null) {\n-      return this.metrics.getNamenodeMetrics();\n+  public NamenodeBeanMetrics getNamenodeMetrics() throws IOException {\n+    if (this.metrics == null) {\n+      throw new IOException(\"Namenode metrics is not initialized\");\n     }\n-    return null;\n+    return this.metrics.getNamenodeMetrics();\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/bbe859177d67fcdfd5377b1abff4a637fbbd4587/hadoop-hdfs-project/hadoop-hdfs-rbf/src/main/java/org/apache/hadoop/hdfs/server/federation/router/Router.java",
                "sha": "3182e27bcc93d46f1c29995a89914715c8ffa503",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/bbe859177d67fcdfd5377b1abff4a637fbbd4587/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouter.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouter.java?ref=bbe859177d67fcdfd5377b1abff4a637fbbd4587",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouter.java",
                "patch": "@@ -203,4 +203,18 @@ public void testRouterIDInRouterRpcClient() throws Exception {\n     router.stop();\n     router.close();\n   }\n+\n+  @Test\n+  public void testRouterMetricsWhenDisabled() throws Exception {\n+\n+    Router router = new Router();\n+    router.init(new RouterConfigBuilder(conf).rpc().build());\n+    router.start();\n+\n+    intercept(IOException.class, \"Namenode metrics is not initialized\",\n+        () -> router.getNamenodeMetrics().getCacheCapacity());\n+\n+    router.stop();\n+    router.close();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/bbe859177d67fcdfd5377b1abff4a637fbbd4587/hadoop-hdfs-project/hadoop-hdfs-rbf/src/test/java/org/apache/hadoop/hdfs/server/federation/router/TestRouter.java",
                "sha": "f83cfda6015eaacafc36087dbcbfc3160db99e2b",
                "status": "modified"
            }
        ],
        "message": "HDFS-13869. RBF: Handle NPE for NamenodeBeanMetrics#getFederationMetrics. Contributed by Ranith Sardar.",
        "parent": "https://github.com/apache/hadoop/commit/01b4126b4e8124edfde20ba4733c6300bb994251",
        "repo": "hadoop",
        "unit_tests": [
            "TestRouter.java"
        ]
    },
    "hadoop_bbff96b": {
        "bug_id": "hadoop_bbff96b",
        "commit": "https://github.com/apache/hadoop/commit/bbff96be48119774688981d04baf444639135977",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=bbff96be48119774688981d04baf444639135977",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -491,6 +491,9 @@ Release 2.6.0 - UNRELEASED\n     YARN-2594. Potential deadlock in RM when querying \n     ApplicationResourceUsageReport. (Wangda Tan via kasha)\n \n+    YARN-2602. Fixed possible NPE in ApplicationHistoryManagerOnTimelineStore.\n+    (Zhijie Shen via jianhe)\n+\n Release 2.5.1 - 2014-09-05\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/CHANGES.txt",
                "sha": "bfaaa90d0841ebb2fc41544fbe2443e232def9ee",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java?ref=bbff96be48119774688981d04baf444639135977",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java",
                "patch": "@@ -227,7 +227,9 @@ private static ApplicationReportExt convertToApplicationReport(\n       if (entityInfo.containsKey(ApplicationMetricsConstants.APP_VIEW_ACLS_ENTITY_INFO)) {\n         String appViewACLsStr = entityInfo.get(\n             ApplicationMetricsConstants.APP_VIEW_ACLS_ENTITY_INFO).toString();\n-        appViewACLs.put(ApplicationAccessType.VIEW_APP, appViewACLsStr);\n+        if (appViewACLsStr.length() > 0) {\n+          appViewACLs.put(ApplicationAccessType.VIEW_APP, appViewACLsStr);\n+        }\n       }\n       if (field == ApplicationReportField.USER_AND_ACLS) {\n         return new ApplicationReportExt(ApplicationReport.newInstance(",
                "raw_url": "https://github.com/apache/hadoop/raw/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java",
                "sha": "5381bd6cb216566887102013e7237de3a5f044ce",
                "status": "modified"
            },
            {
                "additions": 63,
                "blob_url": "https://github.com/apache/hadoop/blob/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java",
                "changes": 110,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java?ref=bbff96be48119774688981d04baf444639135977",
                "deletions": 47,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java",
                "patch": "@@ -122,7 +122,11 @@ private static void prepareTimelineStore(TimelineStore store)\n     for (int i = 1; i <= SCALE; ++i) {\n       TimelineEntities entities = new TimelineEntities();\n       ApplicationId appId = ApplicationId.newInstance(0, i);\n-      entities.addEntity(createApplicationTimelineEntity(appId));\n+      if (i == 2) {\n+        entities.addEntity(createApplicationTimelineEntity(appId, true));\n+      } else {\n+        entities.addEntity(createApplicationTimelineEntity(appId, false));\n+      }\n       store.put(entities);\n       for (int j = 1; j <= SCALE; ++j) {\n         entities = new TimelineEntities();\n@@ -142,50 +146,58 @@ private static void prepareTimelineStore(TimelineStore store)\n \n   @Test\n   public void testGetApplicationReport() throws Exception {\n-    final ApplicationId appId = ApplicationId.newInstance(0, 1);\n-    ApplicationReport app;\n-    if (callerUGI == null) {\n-      app = historyManager.getApplication(appId);\n-    } else {\n-      app =\n-          callerUGI.doAs(new PrivilegedExceptionAction<ApplicationReport> () {\n-        @Override\n-        public ApplicationReport run() throws Exception {\n-          return historyManager.getApplication(appId);\n-        }\n-      });\n-    }\n-    Assert.assertNotNull(app);\n-    Assert.assertEquals(appId, app.getApplicationId());\n-    Assert.assertEquals(\"test app\", app.getName());\n-    Assert.assertEquals(\"test app type\", app.getApplicationType());\n-    Assert.assertEquals(\"user1\", app.getUser());\n-    Assert.assertEquals(\"test queue\", app.getQueue());\n-    Assert.assertEquals(Integer.MAX_VALUE + 2L, app.getStartTime());\n-    Assert.assertEquals(Integer.MAX_VALUE + 3L, app.getFinishTime());\n-    Assert.assertTrue(Math.abs(app.getProgress() - 1.0F) < 0.0001);\n-    if (callerUGI != null && callerUGI.getShortUserName().equals(\"user3\")) {\n-      Assert.assertEquals(ApplicationAttemptId.newInstance(appId, -1),\n-          app.getCurrentApplicationAttemptId());\n-      Assert.assertEquals(null, app.getHost());\n-      Assert.assertEquals(-1, app.getRpcPort());\n-      Assert.assertEquals(null, app.getTrackingUrl());\n-      Assert.assertEquals(null, app.getOriginalTrackingUrl());\n-      Assert.assertEquals(null, app.getDiagnostics());\n-    } else {\n-      Assert.assertEquals(ApplicationAttemptId.newInstance(appId, 1),\n-          app.getCurrentApplicationAttemptId());\n-      Assert.assertEquals(\"test host\", app.getHost());\n-      Assert.assertEquals(-100, app.getRpcPort());\n-      Assert.assertEquals(\"test tracking url\", app.getTrackingUrl());\n-      Assert.assertEquals(\"test original tracking url\",\n-          app.getOriginalTrackingUrl());\n-      Assert.assertEquals(\"test diagnostics info\", app.getDiagnostics());\n+    for (int i = 1; i <= 2; ++i) {\n+      final ApplicationId appId = ApplicationId.newInstance(0, i);\n+      ApplicationReport app;\n+      if (callerUGI == null) {\n+        app = historyManager.getApplication(appId);\n+      } else {\n+        app =\n+            callerUGI.doAs(new PrivilegedExceptionAction<ApplicationReport> () {\n+          @Override\n+          public ApplicationReport run() throws Exception {\n+            return historyManager.getApplication(appId);\n+          }\n+        });\n+      }\n+      Assert.assertNotNull(app);\n+      Assert.assertEquals(appId, app.getApplicationId());\n+      Assert.assertEquals(\"test app\", app.getName());\n+      Assert.assertEquals(\"test app type\", app.getApplicationType());\n+      Assert.assertEquals(\"user1\", app.getUser());\n+      Assert.assertEquals(\"test queue\", app.getQueue());\n+      Assert.assertEquals(Integer.MAX_VALUE + 2L, app.getStartTime());\n+      Assert.assertEquals(Integer.MAX_VALUE + 3L, app.getFinishTime());\n+      Assert.assertTrue(Math.abs(app.getProgress() - 1.0F) < 0.0001);\n+      // App 2 doesn't have the ACLs, such that the default ACLs \" \" will be used.\n+      // Nobody except admin and owner has access to the details of the app.\n+      if ((i ==  1 && callerUGI != null &&\n+          callerUGI.getShortUserName().equals(\"user3\")) ||\n+          (i ==  2 && callerUGI != null &&\n+          (callerUGI.getShortUserName().equals(\"user2\") ||\n+              callerUGI.getShortUserName().equals(\"user3\")))) {\n+        Assert.assertEquals(ApplicationAttemptId.newInstance(appId, -1),\n+            app.getCurrentApplicationAttemptId());\n+        Assert.assertEquals(null, app.getHost());\n+        Assert.assertEquals(-1, app.getRpcPort());\n+        Assert.assertEquals(null, app.getTrackingUrl());\n+        Assert.assertEquals(null, app.getOriginalTrackingUrl());\n+        Assert.assertEquals(null, app.getDiagnostics());\n+      } else {\n+        Assert.assertEquals(ApplicationAttemptId.newInstance(appId, 1),\n+            app.getCurrentApplicationAttemptId());\n+        Assert.assertEquals(\"test host\", app.getHost());\n+        Assert.assertEquals(-100, app.getRpcPort());\n+        Assert.assertEquals(\"test tracking url\", app.getTrackingUrl());\n+        Assert.assertEquals(\"test original tracking url\",\n+            app.getOriginalTrackingUrl());\n+        Assert.assertEquals(\"test diagnostics info\", app.getDiagnostics());\n+      }\n+      Assert.assertEquals(FinalApplicationStatus.UNDEFINED,\n+          app.getFinalApplicationStatus());\n+      Assert.assertEquals(YarnApplicationState.FINISHED,\n+          app.getYarnApplicationState());\n     }\n-    Assert.assertEquals(FinalApplicationStatus.UNDEFINED,\n-        app.getFinalApplicationStatus());\n-    Assert.assertEquals(YarnApplicationState.FINISHED,\n-        app.getYarnApplicationState());\n   }\n \n   @Test\n@@ -396,7 +408,7 @@ public ContainerReport run() throws Exception {\n   }\n \n   private static TimelineEntity createApplicationTimelineEntity(\n-      ApplicationId appId) {\n+      ApplicationId appId, boolean emptyACLs) {\n     TimelineEntity entity = new TimelineEntity();\n     entity.setEntityType(ApplicationMetricsConstants.ENTITY_TYPE);\n     entity.setEntityId(appId.toString());\n@@ -410,8 +422,12 @@ private static TimelineEntity createApplicationTimelineEntity(\n     entityInfo.put(ApplicationMetricsConstants.QUEUE_ENTITY_INFO, \"test queue\");\n     entityInfo.put(ApplicationMetricsConstants.SUBMITTED_TIME_ENTITY_INFO,\n         Integer.MAX_VALUE + 1L);\n-    entityInfo.put(ApplicationMetricsConstants.APP_VIEW_ACLS_ENTITY_INFO,\n-        \"user2\");\n+    if (emptyACLs) {\n+      entityInfo.put(ApplicationMetricsConstants.APP_VIEW_ACLS_ENTITY_INFO, \"\");\n+    } else {\n+      entityInfo.put(ApplicationMetricsConstants.APP_VIEW_ACLS_ENTITY_INFO,\n+          \"user2\");\n+    }\n     entity.setOtherInfo(entityInfo);\n     TimelineEvent tEvent = new TimelineEvent();\n     tEvent.setEventType(ApplicationMetricsConstants.CREATED_EVENT_TYPE);",
                "raw_url": "https://github.com/apache/hadoop/raw/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java",
                "sha": "49386c5b3cfac1caf36aef63f1c517354664709d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java?ref=bbff96be48119774688981d04baf444639135977",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "patch": "@@ -137,7 +137,7 @@ public void appACLsUpdated(RMApp app, String appViewACLs,\n       dispatcher.getEventHandler().handle(\n           new ApplicationACLsUpdatedEvent(\n               app.getApplicationId(),\n-              appViewACLs,\n+              appViewACLs == null ? \"\" : appViewACLs,\n               updatedTime));\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "sha": "e2ecf9a83df9c2f2807b6c47b0476bcf1bc1b486",
                "status": "modified"
            },
            {
                "additions": 79,
                "blob_url": "https://github.com/apache/hadoop/blob/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "changes": 146,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java?ref=bbff96be48119774688981d04baf444639135977",
                "deletions": 67,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "patch": "@@ -95,77 +95,89 @@ public static void tearDown() throws Exception {\n \n   @Test(timeout = 10000)\n   public void testPublishApplicationMetrics() throws Exception {\n-    ApplicationId appId = ApplicationId.newInstance(0, 1);\n-    RMApp app = createRMApp(appId);\n-    metricsPublisher.appCreated(app, app.getStartTime());\n-    metricsPublisher.appFinished(app, RMAppState.FINISHED, app.getFinishTime());\n-    metricsPublisher.appACLsUpdated(app, \"uers1,user2\", 4L);\n-    TimelineEntity entity = null;\n-    do {\n-      entity =\n-          store.getEntity(appId.toString(),\n-              ApplicationMetricsConstants.ENTITY_TYPE,\n-              EnumSet.allOf(Field.class));\n-      // ensure three events are both published before leaving the loop\n-    } while (entity == null || entity.getEvents().size() < 3);\n-    // verify all the fields\n-    Assert.assertEquals(ApplicationMetricsConstants.ENTITY_TYPE,\n-        entity.getEntityType());\n-    Assert\n-        .assertEquals(app.getApplicationId().toString(), entity.getEntityId());\n-    Assert\n-        .assertEquals(\n-            app.getName(),\n-            entity.getOtherInfo().get(\n-                ApplicationMetricsConstants.NAME_ENTITY_INFO));\n-    Assert.assertEquals(app.getQueue(),\n-        entity.getOtherInfo()\n-            .get(ApplicationMetricsConstants.QUEUE_ENTITY_INFO));\n-    Assert\n-        .assertEquals(\n-            app.getUser(),\n-            entity.getOtherInfo().get(\n-                ApplicationMetricsConstants.USER_ENTITY_INFO));\n-    Assert\n-        .assertEquals(\n-            app.getApplicationType(),\n+    for (int i = 1; i <= 2; ++i) {\n+      ApplicationId appId = ApplicationId.newInstance(0, i);\n+      RMApp app = createRMApp(appId);\n+      metricsPublisher.appCreated(app, app.getStartTime());\n+      metricsPublisher.appFinished(app, RMAppState.FINISHED, app.getFinishTime());\n+      if (i == 1) {\n+        metricsPublisher.appACLsUpdated(app, \"uers1,user2\", 4L);\n+      } else {\n+        // in case user doesn't specify the ACLs\n+        metricsPublisher.appACLsUpdated(app, null, 4L);\n+      }\n+      TimelineEntity entity = null;\n+      do {\n+        entity =\n+            store.getEntity(appId.toString(),\n+                ApplicationMetricsConstants.ENTITY_TYPE,\n+                EnumSet.allOf(Field.class));\n+        // ensure three events are both published before leaving the loop\n+      } while (entity == null || entity.getEvents().size() < 3);\n+      // verify all the fields\n+      Assert.assertEquals(ApplicationMetricsConstants.ENTITY_TYPE,\n+          entity.getEntityType());\n+      Assert\n+          .assertEquals(app.getApplicationId().toString(), entity.getEntityId());\n+      Assert\n+          .assertEquals(\n+              app.getName(),\n+              entity.getOtherInfo().get(\n+                  ApplicationMetricsConstants.NAME_ENTITY_INFO));\n+      Assert.assertEquals(app.getQueue(),\n+          entity.getOtherInfo()\n+              .get(ApplicationMetricsConstants.QUEUE_ENTITY_INFO));\n+      Assert\n+          .assertEquals(\n+              app.getUser(),\n+              entity.getOtherInfo().get(\n+                  ApplicationMetricsConstants.USER_ENTITY_INFO));\n+      Assert\n+          .assertEquals(\n+              app.getApplicationType(),\n+              entity.getOtherInfo().get(\n+                  ApplicationMetricsConstants.TYPE_ENTITY_INFO));\n+      Assert.assertEquals(app.getSubmitTime(),\n+          entity.getOtherInfo().get(\n+              ApplicationMetricsConstants.SUBMITTED_TIME_ENTITY_INFO));\n+      if (i == 1) {\n+        Assert.assertEquals(\"uers1,user2\",\n             entity.getOtherInfo().get(\n-                ApplicationMetricsConstants.TYPE_ENTITY_INFO));\n-    Assert.assertEquals(app.getSubmitTime(),\n-        entity.getOtherInfo().get(\n-            ApplicationMetricsConstants.SUBMITTED_TIME_ENTITY_INFO));\n-    Assert.assertEquals(\"uers1,user2\",\n-        entity.getOtherInfo().get(\n+                ApplicationMetricsConstants.APP_VIEW_ACLS_ENTITY_INFO));\n+      } else {\n+        Assert.assertEquals(\"\", entity.getOtherInfo().get(\n             ApplicationMetricsConstants.APP_VIEW_ACLS_ENTITY_INFO));\n-    boolean hasCreatedEvent = false;\n-    boolean hasFinishedEvent = false;\n-    boolean hasACLsUpdatedEvent = false;\n-    for (TimelineEvent event : entity.getEvents()) {\n-      if (event.getEventType().equals(\n-          ApplicationMetricsConstants.CREATED_EVENT_TYPE)) {\n-        hasCreatedEvent = true;\n-        Assert.assertEquals(app.getStartTime(), event.getTimestamp());\n-      } else if (event.getEventType().equals(\n-          ApplicationMetricsConstants.FINISHED_EVENT_TYPE)) {\n-        hasFinishedEvent = true;\n-        Assert.assertEquals(app.getFinishTime(), event.getTimestamp());\n-        Assert.assertEquals(\n-            app.getDiagnostics().toString(),\n-            event.getEventInfo().get(\n-                ApplicationMetricsConstants.DIAGNOSTICS_INFO_EVENT_INFO));\n-        Assert.assertEquals(\n-            app.getFinalApplicationStatus().toString(),\n-            event.getEventInfo().get(\n-                ApplicationMetricsConstants.FINAL_STATUS_EVENT_INFO));\n-        Assert.assertEquals(YarnApplicationState.FINISHED.toString(), event\n-            .getEventInfo().get(ApplicationMetricsConstants.STATE_EVENT_INFO));\n-      } else if (event.getEventType().equals(\n-          ApplicationMetricsConstants.ACLS_UPDATED_EVENT_TYPE)) {\n-        hasACLsUpdatedEvent = true;\n-        Assert.assertEquals(4L, event.getTimestamp());\n       }\n+      boolean hasCreatedEvent = false;\n+      boolean hasFinishedEvent = false;\n+      boolean hasACLsUpdatedEvent = false;\n+      for (TimelineEvent event : entity.getEvents()) {\n+        if (event.getEventType().equals(\n+            ApplicationMetricsConstants.CREATED_EVENT_TYPE)) {\n+          hasCreatedEvent = true;\n+          Assert.assertEquals(app.getStartTime(), event.getTimestamp());\n+        } else if (event.getEventType().equals(\n+            ApplicationMetricsConstants.FINISHED_EVENT_TYPE)) {\n+          hasFinishedEvent = true;\n+          Assert.assertEquals(app.getFinishTime(), event.getTimestamp());\n+          Assert.assertEquals(\n+              app.getDiagnostics().toString(),\n+              event.getEventInfo().get(\n+                  ApplicationMetricsConstants.DIAGNOSTICS_INFO_EVENT_INFO));\n+          Assert.assertEquals(\n+              app.getFinalApplicationStatus().toString(),\n+              event.getEventInfo().get(\n+                  ApplicationMetricsConstants.FINAL_STATUS_EVENT_INFO));\n+          Assert.assertEquals(YarnApplicationState.FINISHED.toString(), event\n+              .getEventInfo().get(ApplicationMetricsConstants.STATE_EVENT_INFO));\n+        } else if (event.getEventType().equals(\n+            ApplicationMetricsConstants.ACLS_UPDATED_EVENT_TYPE)) {\n+          hasACLsUpdatedEvent = true;\n+          Assert.assertEquals(4L, event.getTimestamp());\n+        }\n+      }\n+      Assert.assertTrue(hasCreatedEvent && hasFinishedEvent && hasACLsUpdatedEvent);\n     }\n-    Assert.assertTrue(hasCreatedEvent && hasFinishedEvent && hasACLsUpdatedEvent);\n   }\n \n   @Test(timeout = 10000)",
                "raw_url": "https://github.com/apache/hadoop/raw/bbff96be48119774688981d04baf444639135977/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "sha": "52faf12f506e46ec0199a7913386421736d04118",
                "status": "modified"
            }
        ],
        "message": "YARN-2602. Fixed possible NPE in ApplicationHistoryManagerOnTimelineStore. Contributed by Zhijie Shen",
        "parent": "https://github.com/apache/hadoop/commit/d7075ada5d3019a8c520d34bfddb0cd73a449343",
        "repo": "hadoop",
        "unit_tests": [
            "TestApplicationHistoryManagerOnTimelineStore.java",
            "TestSystemMetricsPublisher.java"
        ]
    },
    "hadoop_beb65c9": {
        "bug_id": "hadoop_beb65c9",
        "commit": "https://github.com/apache/hadoop/commit/beb65c9465806114237aa271b07b31ff3c1f4404",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/beb65c9465806114237aa271b07b31ff3c1f4404/hadoop-yarn-project/CHANGES.txt",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=beb65c9465806114237aa271b07b31ff3c1f4404",
                "deletions": 2,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -413,12 +413,15 @@ Release 2.8.0 - UNRELEASED\n     YARN-4026. Refactored ContainerAllocator to accept a list of priorites\n     rather than a single priority. (Wangda Tan via jianhe)\n \n-   YARN-4031. Add JvmPauseMonitor to ApplicationHistoryServer and\n-   WebAppProxyServer (djp via rkanter)\n+    YARN-4031. Add JvmPauseMonitor to ApplicationHistoryServer and\n+    WebAppProxyServer (djp via rkanter)\n \n     YARN-4057. If ContainersMonitor is not enabled, only print\n     related log info one time. (Jun Gong via zxu)\n \n+    YARN-1556. NPE getting application report with a null appId. (Weiwei Yang via \n+    junping_du)\n+\n   OPTIMIZATIONS\n \n     YARN-3339. TestDockerContainerExecutor should pull a single image and not",
                "raw_url": "https://github.com/apache/hadoop/raw/beb65c9465806114237aa271b07b31ff3c1f4404/hadoop-yarn-project/CHANGES.txt",
                "sha": "0b733a4a40c3a19aad03dead146859f97b7bc815",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/beb65c9465806114237aa271b07b31ff3c1f4404/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java?ref=beb65c9465806114237aa271b07b31ff3c1f4404",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "patch": "@@ -315,6 +315,9 @@ public GetNewApplicationResponse getNewApplication(\n   public GetApplicationReportResponse getApplicationReport(\n       GetApplicationReportRequest request) throws YarnException {\n     ApplicationId applicationId = request.getApplicationId();\n+    if (applicationId == null) {\n+      throw new ApplicationNotFoundException(\"Invalid application id: null\");\n+    }\n \n     UserGroupInformation callerUGI;\n     try {",
                "raw_url": "https://github.com/apache/hadoop/raw/beb65c9465806114237aa271b07b31ff3c1f4404/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "sha": "cce0fe57aa75b24cc02920f6c9a1e0859e3a1290",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/beb65c9465806114237aa271b07b31ff3c1f4404/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMService.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMService.java?ref=beb65c9465806114237aa271b07b31ff3c1f4404",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMService.java",
                "patch": "@@ -333,6 +333,18 @@ public void testGetApplicationReport() throws Exception {\n           report.getApplicationResourceUsageReport();\n       Assert.assertEquals(10, usageReport.getMemorySeconds());\n       Assert.assertEquals(3, usageReport.getVcoreSeconds());\n+\n+      // if application id is null\n+      GetApplicationReportRequest invalidRequest = recordFactory\n+          .newRecordInstance(GetApplicationReportRequest.class);\n+      invalidRequest.setApplicationId(null);\n+      try {\n+        rmService.getApplicationReport(invalidRequest);\n+      } catch (YarnException e) {\n+        // rmService should return a ApplicationNotFoundException\n+        // when a null application id is provided\n+        Assert.assertTrue(e instanceof ApplicationNotFoundException);\n+      }\n     } finally {\n       rmService.close();\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/beb65c9465806114237aa271b07b31ff3c1f4404/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestClientRMService.java",
                "sha": "6a0b99c7461ca0f490ed90feb85b5efb16c0250f",
                "status": "modified"
            }
        ],
        "message": "YARN-1556. NPE getting application report with a null appId. Contributed by Weiwei Yang.",
        "parent": "https://github.com/apache/hadoop/commit/e166c038c0aaa57b245f985a1c0fadd5fe33c384",
        "repo": "hadoop",
        "unit_tests": [
            "TestClientRMService.java"
        ]
    },
    "hadoop_bf44d16": {
        "bug_id": "hadoop_bf44d16",
        "commit": "https://github.com/apache/hadoop/commit/bf44d16ef467405a13fa95a6805a4a3ca1e2d898",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/bf44d16ef467405a13fa95a6805a4a3ca1e2d898/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=bf44d16ef467405a13fa95a6805a4a3ca1e2d898",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -324,6 +324,9 @@ Release 2.0.4-beta - UNRELEASED\n     but not in dfs.namenode.edits.dir are silently ignored.  (Arpit Agarwal\n     via szetszwo)\n \n+    HDFS-4482. ReplicationMonitor thread can exit with NPE due to the race \n+    between delete and replication of same file. (umamahesh)\n+\n Release 2.0.3-alpha - 2013-02-06\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/bf44d16ef467405a13fa95a6805a4a3ca1e2d898/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "3b1a1e36bceb5c82791b0db68a2ddae8471b5c0a",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/bf44d16ef467405a13fa95a6805a4a3ca1e2d898/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java?ref=bf44d16ef467405a13fa95a6805a4a3ca1e2d898",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "patch": "@@ -1343,6 +1343,11 @@ static String getFullPathName(INode inode) {\n \n     // fill up the inodes in the path from this inode to root\n     for (int i = 0; i < depth; i++) {\n+      if (inode == null) {\n+        NameNode.stateChangeLog.warn(\"Could not get full path.\"\n+            + \" Corresponding file might have deleted already.\");\n+        return null;\n+      }\n       inodes[depth-i-1] = inode;\n       inode = inode.parent;\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/bf44d16ef467405a13fa95a6805a4a3ca1e2d898/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "sha": "b11059a4bb47c865d14e439b1bbabfa418b90e2b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/bf44d16ef467405a13fa95a6805a4a3ca1e2d898/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java?ref=bf44d16ef467405a13fa95a6805a4a3ca1e2d898",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "patch": "@@ -282,7 +282,11 @@ String getLocalName() {\n \n   String getLocalParentDir() {\n     INode inode = isRoot() ? this : getParent();\n-    return (inode != null) ? inode.getFullPathName() : \"\";\n+    String parentDir = \"\";\n+    if (inode != null) {\n+      parentDir = inode.getFullPathName();\n+    }\n+    return (parentDir != null) ? parentDir : \"\";\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/bf44d16ef467405a13fa95a6805a4a3ca1e2d898/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "sha": "b407a62da97b1808bbbc8d7169ecd48a9b05292a",
                "status": "modified"
            }
        ],
        "message": "HDFS-4482. ReplicationMonitor thread can exit with NPE due to the race between delete and replication of same file. Contributed by Uma Maheswara Rao G.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1448708 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/0b73dde6ce865ff94b483558ff0701de9932e211",
        "repo": "hadoop",
        "unit_tests": [
            "TestFSDirectory.java"
        ]
    },
    "hadoop_c020b62": {
        "bug_id": "hadoop_c020b62",
        "commit": "https://github.com/apache/hadoop/commit/c020b62cf8de1f3baadc9d2f3410640ef7880543",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c020b62cf8de1f3baadc9d2f3410640ef7880543/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=c020b62cf8de1f3baadc9d2f3410640ef7880543",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -707,6 +707,9 @@ Release 2.8.0 - UNRELEASED\n     YARN-3982. container-executor parsing of container-executor.cfg broken in\n     trunk and branch-2. (Varun Vasudev via xgong)\n \n+    YARN-3919. NPEs' while stopping service after exception during\n+    CommonNodeLabelsManager#start. (varun saxane via rohithsharmaks)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/c020b62cf8de1f3baadc9d2f3410640ef7880543/hadoop-yarn-project/CHANGES.txt",
                "sha": "8e8a76b7c87b8eb9a2d2bd9eb6d27dddacc621c6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c020b62cf8de1f3baadc9d2f3410640ef7880543/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java?ref=c020b62cf8de1f3baadc9d2f3410640ef7880543",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java",
                "patch": "@@ -139,7 +139,8 @@ protected void serviceStop() throws Exception {\n       blockNewEvents = true;\n       LOG.info(\"AsyncDispatcher is draining to stop, igonring any new events.\");\n       synchronized (waitForDrained) {\n-        while (!drained && eventHandlingThread.isAlive()) {\n+        while (!drained && eventHandlingThread != null\n+            && eventHandlingThread.isAlive()) {\n           waitForDrained.wait(1000);\n           LOG.info(\"Waiting for AsyncDispatcher to drain. Thread state is :\" +\n               eventHandlingThread.getState());",
                "raw_url": "https://github.com/apache/hadoop/raw/c020b62cf8de1f3baadc9d2f3410640ef7880543/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/event/AsyncDispatcher.java",
                "sha": "f6701128ac625fed1dd8ddbcccb6a7ea206a40f3",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c020b62cf8de1f3baadc9d2f3410640ef7880543/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/FileSystemNodeLabelsStore.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/FileSystemNodeLabelsStore.java?ref=c020b62cf8de1f3baadc9d2f3410640ef7880543",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/FileSystemNodeLabelsStore.java",
                "patch": "@@ -33,6 +33,7 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.LocalFileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.security.UserGroupInformation;\n import org.apache.hadoop.yarn.api.records.NodeId;\n import org.apache.hadoop.yarn.api.records.NodeLabel;\n@@ -92,12 +93,7 @@ public void init(Configuration conf) throws Exception {\n \n   @Override\n   public void close() throws IOException {\n-    try {\n-      fs.close();\n-      editlogOs.close();\n-    } catch (IOException e) {\n-      LOG.warn(\"Exception happened whiling shutting down,\", e);\n-    }\n+    IOUtils.cleanup(LOG, fs, editlogOs);\n   }\n \n   private void setFileSystem(Configuration conf) throws IOException {",
                "raw_url": "https://github.com/apache/hadoop/raw/c020b62cf8de1f3baadc9d2f3410640ef7880543/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/nodelabels/FileSystemNodeLabelsStore.java",
                "sha": "20dc67c10e4a4068eb122076a32a441a98f3eda1",
                "status": "modified"
            }
        ],
        "message": "YARN-3919. NPEs' while stopping service after exception during CommonNodeLabelsManager#start. (varun saxena via rohithsharmaks)",
        "parent": "https://github.com/apache/hadoop/commit/5205a330b387d2e133ee790b9fe7d5af3cd8bccc",
        "repo": "hadoop",
        "unit_tests": [
            "TestAsyncDispatcher.java",
            "TestFileSystemNodeLabelsStore.java"
        ]
    },
    "hadoop_c0a903d": {
        "bug_id": "hadoop_c0a903d",
        "commit": "https://github.com/apache/hadoop/commit/c0a903da22c65294b232c7530a6a684ee93daba4",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c0a903da22c65294b232c7530a6a684ee93daba4/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=c0a903da22c65294b232c7530a6a684ee93daba4",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -525,6 +525,9 @@ Release 2.4.0 - UNRELEASED\n     HDFS-6040. fix DFSClient issue without libhadoop.so and some other\n     ShortCircuitShm cleanups (cmccabe)\n \n+    HDFS-6047 TestPread NPE inside in DFSInputStream hedgedFetchBlockByteRange\n+    (stack)\n+\n   BREAKDOWN OF HDFS-5698 SUBTASKS AND RELATED JIRAS\n \n     HDFS-5717. Save FSImage header in protobuf. (Haohui Mai via jing9)",
                "raw_url": "https://github.com/apache/hadoop/raw/c0a903da22c65294b232c7530a6a684ee93daba4/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "acd5f15ae89902dd1eb04f86547b0e58e1cb793b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/c0a903da22c65294b232c7530a6a684ee93daba4/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java?ref=c0a903da22c65294b232c7530a6a684ee93daba4",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "patch": "@@ -1177,8 +1177,11 @@ private void hedgedFetchBlockByteRange(LocatedBlock block, long start,\n           // exception already handled in the call method. getFirstToComplete\n           // will remove the failing future from the list. nothing more to do.\n         }\n-        // We got here if exception.  Ignore this node on next go around.\n-        ignored.add(chosenNode.info);\n+        // We got here if exception.  Ignore this node on next go around IFF\n+        // we found a chosenNode to hedge read against.\n+        if (chosenNode != null && chosenNode.info != null) {\n+          ignored.add(chosenNode.info);\n+        }\n       }\n       // executed if we get an error from a data node\n       block = getBlockAt(block.getStartOffset(), false);",
                "raw_url": "https://github.com/apache/hadoop/raw/c0a903da22c65294b232c7530a6a684ee93daba4/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSInputStream.java",
                "sha": "3705a2fd4f353b3074cc01d750440d26ce8da993",
                "status": "modified"
            }
        ],
        "message": "HDFS-6047 TestPread NPE inside in DFSInputStream hedgedFetchBlockByteRange (stack)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1574205 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/88245b6a41171f939b22186c533ea2bc7994f9b3",
        "repo": "hadoop",
        "unit_tests": [
            "TestDFSInputStream.java"
        ]
    },
    "hadoop_c2460da": {
        "bug_id": "hadoop_c2460da",
        "commit": "https://github.com/apache/hadoop/commit/c2460dad642feee1086442d33c30c24ec77236b9",
        "file": [
            {
                "additions": 96,
                "blob_url": "https://github.com/apache/hadoop/blob/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java",
                "changes": 143,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java?ref=c2460dad642feee1086442d33c30c24ec77236b9",
                "deletions": 47,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java",
                "patch": "@@ -132,6 +132,9 @@\n   private static final FastDateFormat DATE_FORMAT =\n       FastDateFormat.getInstance(\"yyyyMMddHH\", TimeZone.getTimeZone(\"GMT\"));\n   private final Object lock = new Object();\n+  private boolean initialized = false;\n+  private SubsetConfiguration properties;\n+  private Configuration conf;\n   private String source;\n   private boolean ignoreError;\n   private boolean allowAppend;\n@@ -163,63 +166,102 @@\n   protected static FileSystem suppliedFilesystem = null;\n \n   @Override\n-  public void init(SubsetConfiguration conf) {\n-    basePath = new Path(conf.getString(BASEPATH_KEY, BASEPATH_DEFAULT));\n-    source = conf.getString(SOURCE_KEY, SOURCE_DEFAULT);\n-    ignoreError = conf.getBoolean(IGNORE_ERROR_KEY, false);\n-    allowAppend = conf.getBoolean(ALLOW_APPEND_KEY, false);\n+  public void init(SubsetConfiguration metrics2Properties) {\n+    properties = metrics2Properties;\n+    basePath = new Path(properties.getString(BASEPATH_KEY, BASEPATH_DEFAULT));\n+    source = properties.getString(SOURCE_KEY, SOURCE_DEFAULT);\n+    ignoreError = properties.getBoolean(IGNORE_ERROR_KEY, false);\n+    allowAppend = properties.getBoolean(ALLOW_APPEND_KEY, false);\n \n-    Configuration configuration = loadConf();\n-\n-    UserGroupInformation.setConfiguration(configuration);\n+    conf = loadConf();\n+    UserGroupInformation.setConfiguration(conf);\n \n     // Don't do secure setup if it's not needed.\n     if (UserGroupInformation.isSecurityEnabled()) {\n       // Validate config so that we don't get an NPE\n-      checkForProperty(conf, KEYTAB_PROPERTY_KEY);\n-      checkForProperty(conf, USERNAME_PROPERTY_KEY);\n+      checkForProperty(properties, KEYTAB_PROPERTY_KEY);\n+      checkForProperty(properties, USERNAME_PROPERTY_KEY);\n \n \n       try {\n         // Login as whoever we're supposed to be and let the hostname be pulled\n         // from localhost. If security isn't enabled, this does nothing.\n-        SecurityUtil.login(configuration, conf.getString(KEYTAB_PROPERTY_KEY),\n-            conf.getString(USERNAME_PROPERTY_KEY));\n+        SecurityUtil.login(conf, properties.getString(KEYTAB_PROPERTY_KEY),\n+            properties.getString(USERNAME_PROPERTY_KEY));\n       } catch (IOException ex) {\n         throw new MetricsException(\"Error logging in securely: [\"\n             + ex.toString() + \"]\", ex);\n       }\n     }\n+  }\n+\n+  /**\n+   * Initialize the connection to HDFS and create the base directory. Also\n+   * launch the flush thread.\n+   */\n+  private boolean initFs() {\n+    boolean success = false;\n \n-    fileSystem = getFileSystem(configuration);\n+    fileSystem = getFileSystem();\n \n     // This step isn't strictly necessary, but it makes debugging issues much\n     // easier. We try to create the base directory eagerly and fail with\n     // copious debug info if it fails.\n     try {\n       fileSystem.mkdirs(basePath);\n+      success = true;\n     } catch (Exception ex) {\n-      throw new MetricsException(\"Failed to create \" + basePath + \"[\"\n-          + SOURCE_KEY + \"=\" + source + \", \"\n-          + IGNORE_ERROR_KEY + \"=\" + ignoreError + \", \"\n-          + ALLOW_APPEND_KEY + \"=\" + allowAppend + \", \"\n-          + KEYTAB_PROPERTY_KEY + \"=\"\n-          + conf.getString(KEYTAB_PROPERTY_KEY) + \", \"\n-          + conf.getString(KEYTAB_PROPERTY_KEY) + \"=\"\n-          + configuration.get(conf.getString(KEYTAB_PROPERTY_KEY)) + \", \"\n-          + USERNAME_PROPERTY_KEY + \"=\"\n-          + conf.getString(USERNAME_PROPERTY_KEY) + \", \"\n-          + conf.getString(USERNAME_PROPERTY_KEY) + \"=\"\n-          + configuration.get(conf.getString(USERNAME_PROPERTY_KEY))\n-          + \"] -- \" + ex.toString(), ex);\n+      if (!ignoreError) {\n+        throw new MetricsException(\"Failed to create \" + basePath + \"[\"\n+            + SOURCE_KEY + \"=\" + source + \", \"\n+            + ALLOW_APPEND_KEY + \"=\" + allowAppend + \", \"\n+            + stringifySecurityProperty(KEYTAB_PROPERTY_KEY) + \", \"\n+            + stringifySecurityProperty(USERNAME_PROPERTY_KEY)\n+            + \"] -- \" + ex.toString(), ex);\n+      }\n     }\n \n-    // If we're permitted to append, check if we actually can\n-    if (allowAppend) {\n-      allowAppend = checkAppend(fileSystem);\n+    if (success) {\n+      // If we're permitted to append, check if we actually can\n+      if (allowAppend) {\n+        allowAppend = checkAppend(fileSystem);\n+      }\n+\n+      flushTimer = new Timer(\"RollingFileSystemSink Flusher\", true);\n     }\n \n-    flushTimer = new Timer(\"RollingFileSystemSink Flusher\", true);\n+    return success;\n+  }\n+\n+  /**\n+   * Turn a security property into a nicely formatted set of <i>name=value</i>\n+   * strings, allowing for either the property or the configuration not to be\n+   * set.\n+   *\n+   * @param properties the sink properties\n+   * @param conf the conf\n+   * @param property the property to stringify\n+   * @return the stringified property\n+   */\n+  private String stringifySecurityProperty(String property) {\n+    String securityProperty;\n+\n+    if (properties.containsKey(property)) {\n+      String propertyValue = properties.getString(property);\n+      String confValue = conf.get(properties.getString(property));\n+\n+      if (confValue != null) {\n+        securityProperty = property + \"=\" + propertyValue\n+            + \", \" + properties.getString(property) + \"=\" + confValue;\n+      } else {\n+        securityProperty = property + \"=\" + propertyValue\n+            + \", \" + properties.getString(property) + \"=<NOT SET>\";\n+      }\n+    } else {\n+      securityProperty = property + \"=<NOT SET>\";\n+    }\n+\n+    return securityProperty;\n   }\n \n   /**\n@@ -242,17 +284,17 @@ private static void checkForProperty(SubsetConfiguration conf, String key) {\n    * @return the configuration to use\n    */\n   private Configuration loadConf() {\n-    Configuration conf;\n+    Configuration c;\n \n     if (suppliedConf != null) {\n-      conf = suppliedConf;\n+      c = suppliedConf;\n     } else {\n       // The config we're handed in init() isn't the one we want here, so we\n       // create a new one to pick up the full settings.\n-      conf = new Configuration();\n+      c = new Configuration();\n     }\n \n-    return conf;\n+    return c;\n   }\n \n   /**\n@@ -263,7 +305,7 @@ private Configuration loadConf() {\n    * @return the file system to use\n    * @throws MetricsException thrown if the file system could not be retrieved\n    */\n-  private FileSystem getFileSystem(Configuration conf) throws MetricsException {\n+  private FileSystem getFileSystem() throws MetricsException {\n     FileSystem fs = null;\n \n     if (suppliedFilesystem != null) {\n@@ -317,22 +359,29 @@ private void rollLogDirIfNeeded() throws MetricsException {\n     // because if currentDirPath is null, then currentOutStream is null, but\n     // currentOutStream can be null for other reasons.\n     if ((currentOutStream == null) || !path.equals(currentDirPath)) {\n-      // Close the stream. This step could have been handled already by the\n-      // flusher thread, but if it has, the PrintStream will just swallow the\n-      // exception, which is fine.\n-      if (currentOutStream != null) {\n-        currentOutStream.close();\n+      // If we're not yet connected to HDFS, create the connection\n+      if (!initialized) {\n+        initialized = initFs();\n       }\n \n-      currentDirPath = path;\n+      if (initialized) {\n+        // Close the stream. This step could have been handled already by the\n+        // flusher thread, but if it has, the PrintStream will just swallow the\n+        // exception, which is fine.\n+        if (currentOutStream != null) {\n+          currentOutStream.close();\n+        }\n \n-      try {\n-        rollLogDir();\n-      } catch (IOException ex) {\n-        throwMetricsException(\"Failed to create new log file\", ex);\n-      }\n+        currentDirPath = path;\n \n-      scheduleFlush(now);\n+        try {\n+          rollLogDir();\n+        } catch (IOException ex) {\n+          throwMetricsException(\"Failed to create new log file\", ex);\n+        }\n+\n+        scheduleFlush(now);\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java",
                "sha": "9a439013e6e19306eb168e414d9d43883ab61c6d",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSinkTestBase.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSinkTestBase.java?ref=c2460dad642feee1086442d33c30c24ec77236b9",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSinkTestBase.java",
                "patch": "@@ -175,7 +175,7 @@ protected MetricsSystem initMetricsSystem(String path, boolean ignoreErrors,\n     String prefix = methodName.getMethodName().toLowerCase();\n \n     ConfigBuilder builder = new ConfigBuilder().add(\"*.period\", 10000)\n-        .add(prefix + \".sink.mysink0.class\", ErrorSink.class.getName())\n+        .add(prefix + \".sink.mysink0.class\", MockSink.class.getName())\n         .add(prefix + \".sink.mysink0.basepath\", path)\n         .add(prefix + \".sink.mysink0.source\", \"testsrc\")\n         .add(prefix + \".sink.mysink0.context\", \"test1\")\n@@ -503,8 +503,9 @@ public void assertFileCount(FileSystem fs, Path dir, int expected)\n    * This class is a {@link RollingFileSystemSink} wrapper that tracks whether\n    * an exception has been thrown during operations.\n    */\n-  public static class ErrorSink extends RollingFileSystemSink {\n+  public static class MockSink extends RollingFileSystemSink {\n     public static volatile boolean errored = false;\n+    public static volatile boolean initialized = false;\n \n     @Override\n     public void init(SubsetConfiguration conf) {\n@@ -515,6 +516,8 @@ public void init(SubsetConfiguration conf) {\n \n         throw new MetricsException(ex);\n       }\n+\n+      initialized = true;\n     }\n \n     @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSinkTestBase.java",
                "sha": "9914c5e30f3565334390e97ce2fbe5dbda1aaca5",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSink.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSink.java?ref=c2460dad642feee1086442d33c30c24ec77236b9",
                "deletions": 4,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSink.java",
                "patch": "@@ -23,6 +23,8 @@\n import org.junit.Test;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertTrue;\n \n /**\n  * Test the {@link RollingFileSystemSink} class in the context of the local file\n@@ -106,15 +108,15 @@ public void testFailedWrite() {\n     new MyMetrics1().registerWith(ms);\n \n     methodDir.setWritable(false);\n-    ErrorSink.errored = false;\n+    MockSink.errored = false;\n \n     try {\n       // publish the metrics\n       ms.publishMetricsNow();\n \n       assertTrue(\"No exception was generated while writing metrics \"\n           + \"even though the target directory was not writable\",\n-          ErrorSink.errored);\n+          MockSink.errored);\n \n       ms.stop();\n       ms.shutdown();\n@@ -135,7 +137,7 @@ public void testSilentFailedWrite() {\n     new MyMetrics1().registerWith(ms);\n \n     methodDir.setWritable(false);\n-    ErrorSink.errored = false;\n+    MockSink.errored = false;\n \n     try {\n       // publish the metrics\n@@ -144,7 +146,7 @@ public void testSilentFailedWrite() {\n       assertFalse(\"An exception was generated while writing metrics \"\n           + \"when the target directory was not writable, even though the \"\n           + \"sink is set to ignore errors\",\n-          ErrorSink.errored);\n+          MockSink.errored);\n \n       ms.stop();\n       ms.shutdown();",
                "raw_url": "https://github.com/apache/hadoop/raw/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSink.java",
                "sha": "3c6cd27b59b396afdeec123e78d519da6e534bdf",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=c2460dad642feee1086442d33c30c24ec77236b9",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -1047,6 +1047,9 @@ Release 2.9.0 - UNRELEASED\n     HDFS-9608. Disk IO imbalance in HDFS with heterogeneous storages.\n     (Wei Zhou via wang)\n \n+    HDFS-9858. RollingFileSystemSink can throw an NPE on non-secure clusters. \n+    (Daniel Templeton via kasha)\n+\n Release 2.8.0 - UNRELEASED\n \n   NEW FEATURES",
                "raw_url": "https://github.com/apache/hadoop/raw/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "104b46ddc3add9e8485717093c3ad9be462e9753",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop/blob/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithHdfs.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithHdfs.java?ref=c2460dad642feee1086442d33c30c24ec77236b9",
                "deletions": 8,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithHdfs.java",
                "patch": "@@ -151,12 +151,12 @@ public void testFailedWrite() throws IOException {\n     new MyMetrics1().registerWith(ms);\n \n     shutdownHdfs();\n-    ErrorSink.errored = false;\n+    MockSink.errored = false;\n \n     ms.publishMetricsNow(); // publish the metrics\n \n     assertTrue(\"No exception was generated while writing metrics \"\n-        + \"even though HDFS was unavailable\", ErrorSink.errored);\n+        + \"even though HDFS was unavailable\", MockSink.errored);\n \n     ms.stop();\n     ms.shutdown();\n@@ -178,12 +178,12 @@ public void testFailedClose() throws IOException {\n     ms.publishMetricsNow(); // publish the metrics\n \n     shutdownHdfs();\n-    ErrorSink.errored = false;\n+    MockSink.errored = false;\n \n     ms.stop();\n \n     assertTrue(\"No exception was generated while stopping sink \"\n-        + \"even though HDFS was unavailable\", ErrorSink.errored);\n+        + \"even though HDFS was unavailable\", MockSink.errored);\n \n     ms.shutdown();\n   }\n@@ -203,13 +203,13 @@ public void testSilentFailedWrite() throws IOException, InterruptedException {\n     new MyMetrics1().registerWith(ms);\n \n     shutdownHdfs();\n-    ErrorSink.errored = false;\n+    MockSink.errored = false;\n \n     ms.publishMetricsNow(); // publish the metrics\n \n     assertFalse(\"An exception was generated writing metrics \"\n         + \"while HDFS was unavailable, even though the sink is set to \"\n-        + \"ignore errors\", ErrorSink.errored);\n+        + \"ignore errors\", MockSink.errored);\n \n     ms.stop();\n     ms.shutdown();\n@@ -231,13 +231,13 @@ public void testSilentFailedClose() throws IOException {\n     ms.publishMetricsNow(); // publish the metrics\n \n     shutdownHdfs();\n-    ErrorSink.errored = false;\n+    MockSink.errored = false;\n \n     ms.stop();\n \n     assertFalse(\"An exception was generated stopping sink \"\n         + \"while HDFS was unavailable, even though the sink is set to \"\n-        + \"ignore errors\", ErrorSink.errored);\n+        + \"ignore errors\", MockSink.errored);\n \n     ms.shutdown();\n   }\n@@ -283,4 +283,22 @@ public void testFlushThread() throws Exception {\n \n     ms.stop();\n   }\n+\n+  /**\n+   * Test that a failure to connect to HDFS does not cause the init() method\n+   * to fail.\n+   */\n+  @Test\n+  public void testInitWithNoHDFS() {\n+    String path = \"hdfs://\" + cluster.getNameNode().getHostAndPort() + \"/tmp\";\n+\n+    shutdownHdfs();\n+    MockSink.errored = false;\n+    initMetricsSystem(path, true, false);\n+\n+    assertTrue(\"The sink was not initialized as expected\",\n+        MockSink.initialized);\n+    assertFalse(\"The sink threw an unexpected error on initialization\",\n+        MockSink.errored);\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithHdfs.java",
                "sha": "9984b34fbff33518d81137c2a29bc0e158316900",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithSecureHdfs.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithSecureHdfs.java?ref=c2460dad642feee1086442d33c30c24ec77236b9",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithSecureHdfs.java",
                "patch": "@@ -51,6 +51,7 @@\n import org.apache.hadoop.security.ssl.KeyStoreTestUtil;\n import org.junit.Test;\n import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.assertTrue;\n \n /**\n  * Test the {@link RollingFileSystemSink} class in the context of HDFS with\n@@ -147,7 +148,7 @@ public void testMissingPropertiesWithSecureHDFS() throws Exception {\n \n       assertTrue(\"No exception was generated initializing the sink against a \"\n           + \"secure cluster even though the principal and keytab properties \"\n-          + \"were missing\", ErrorSink.errored);\n+          + \"were missing\", MockSink.errored);\n     } finally {\n       if (cluster != null) {\n         cluster.shutdown();",
                "raw_url": "https://github.com/apache/hadoop/raw/c2460dad642feee1086442d33c30c24ec77236b9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/metrics2/sink/TestRollingFileSystemSinkWithSecureHdfs.java",
                "sha": "dce4fdcf2294bec9cd9b83f21950ecf2af92cdda",
                "status": "modified"
            }
        ],
        "message": "HDFS-9858. RollingFileSystemSink can throw an NPE on non-secure clusters. (Daniel Templeton via kasha)",
        "parent": "https://github.com/apache/hadoop/commit/b2951f9fbccee8aeab04c1f5ee3fc6db1ef6b2da",
        "repo": "hadoop",
        "unit_tests": [
            "TestRollingFileSystemSink.java"
        ]
    },
    "hadoop_c30e495": {
        "bug_id": "hadoop_c30e495",
        "commit": "https://github.com/apache/hadoop/commit/c30e495557359b23681a61edbc90cfafafdb7dfe",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c30e495557359b23681a61edbc90cfafafdb7dfe/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSNetworkTopology.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSNetworkTopology.java?ref=c30e495557359b23681a61edbc90cfafafdb7dfe",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSNetworkTopology.java",
                "patch": "@@ -226,6 +226,9 @@ Node chooseRandomWithStorageType(final String scope,\n           String nodeLocation = excludedNode.getNetworkLocation()\n               + \"/\" + excludedNode.getName();\n           DatanodeDescriptor dn = (DatanodeDescriptor)getNode(nodeLocation);\n+          if (dn == null) {\n+            continue;\n+          }\n           availableCount -= dn.hasStorageType(type)? 1 : 0;\n         } else {\n           LOG.error(\"Unexpected node type: {}.\", excludedNode.getClass());",
                "raw_url": "https://github.com/apache/hadoop/raw/c30e495557359b23681a61edbc90cfafafdb7dfe/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/net/DFSNetworkTopology.java",
                "sha": "0884fc002c3784e951604e0647fa0066c033ac60",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/hadoop/blob/c30e495557359b23681a61edbc90cfafafdb7dfe/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java?ref=c30e495557359b23681a61edbc90cfafafdb7dfe",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java",
                "patch": "@@ -23,6 +23,8 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.StorageType;\n import org.apache.hadoop.hdfs.DFSTestUtil;\n+import org.apache.hadoop.hdfs.protocol.DatanodeID;\n+import org.apache.hadoop.hdfs.protocol.DatanodeInfo.DatanodeInfoBuilder;\n import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor;\n import org.apache.hadoop.hdfs.server.blockmanagement.DatanodeStorageInfo;\n import org.apache.hadoop.net.Node;\n@@ -37,9 +39,11 @@\n import java.util.Set;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n \n+\n /**\n  * This class tests the correctness of storage type info stored in\n  * DFSNetworkTopology.\n@@ -368,6 +372,18 @@ public void testChooseRandomWithStorageTypeWithExcluded() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testChooseRandomWithStorageTypeWithExcludedforNullCheck()\n+      throws Exception {\n+    HashSet<Node> excluded = new HashSet<>();\n+\n+    excluded.add(new DatanodeInfoBuilder()\n+        .setNodeID(DatanodeID.EMPTY_DATANODE_ID).build());\n+    Node node = CLUSTER.chooseRandomWithStorageType(\"/\", \"/l1/d1/r1\", excluded,\n+        StorageType.ARCHIVE);\n+\n+    assertNotNull(node);\n+  }\n \n   /**\n    * This test tests the wrapper method. The wrapper method only takes one scope",
                "raw_url": "https://github.com/apache/hadoop/raw/c30e495557359b23681a61edbc90cfafafdb7dfe/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/net/TestDFSNetworkTopology.java",
                "sha": "3360d68f2bd4131416d5cd10ed8165b27ca20c6c",
                "status": "modified"
            }
        ],
        "message": "HDFS-14853. NPE in DFSNetworkTopology#chooseRandomWithStorageType() when the excludedNode is not present. Contributed by Ranith Sardar.",
        "parent": "https://github.com/apache/hadoop/commit/2b5fc95851552599e33674d9a23e7e9af74a304e",
        "repo": "hadoop",
        "unit_tests": [
            "TestDFSNetworkTopology.java"
        ]
    },
    "hadoop_c36d69a": {
        "bug_id": "hadoop_c36d69a",
        "commit": "https://github.com/apache/hadoop/commit/c36d69a7b30927eaea16335e06cfcc247accde35",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/c36d69a7b30927eaea16335e06cfcc247accde35/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java?ref=c36d69a7b30927eaea16335e06cfcc247accde35",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java",
                "patch": "@@ -52,7 +52,7 @@\n   /**\n    * Block collection ID.\n    */\n-  private long bcId;\n+  private volatile long bcId;\n \n   /** For implementing {@link LightWeightGSet.LinkedElement} interface. */\n   private LightWeightGSet.LinkedElement nextLinkedElement;",
                "raw_url": "https://github.com/apache/hadoop/raw/c36d69a7b30927eaea16335e06cfcc247accde35/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockInfo.java",
                "sha": "d160f61fc8f54c13fb12ff4a30778cd681c843c7",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/c36d69a7b30927eaea16335e06cfcc247accde35/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=c36d69a7b30927eaea16335e06cfcc247accde35",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -4171,6 +4171,10 @@ void processExtraRedundancyBlocksOnInService(\n     int numExtraRedundancy = 0;\n     while(it.hasNext()) {\n       final BlockInfo block = it.next();\n+      if (block.isDeleted()) {\n+        //Orphan block, will be handled eventually, skip\n+        continue;\n+      }\n       int expectedReplication = this.getExpectedRedundancyNum(block);\n       NumberReplicas num = countNodes(block);\n       if (shouldProcessExtraRedundancy(num, expectedReplication)) {",
                "raw_url": "https://github.com/apache/hadoop/raw/c36d69a7b30927eaea16335e06cfcc247accde35/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "675221a1ec52d4ec859306ae56d581bf94483e48",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/c36d69a7b30927eaea16335e06cfcc247accde35/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=c36d69a7b30927eaea16335e06cfcc247accde35",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -4128,7 +4128,7 @@ private void clearCorruptLazyPersistFiles()\n         while (it.hasNext()) {\n           Block b = it.next();\n           BlockInfo blockInfo = blockManager.getStoredBlock(b);\n-          if (blockInfo == null) {\n+          if (blockInfo == null || blockInfo.isDeleted()) {\n             LOG.info(\"Cannot find block info for block \" + b);\n           } else {\n             BlockCollection bc = getBlockCollection(blockInfo);",
                "raw_url": "https://github.com/apache/hadoop/raw/c36d69a7b30927eaea16335e06cfcc247accde35/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "74c9f10482609ad6709292084e47309769576151",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/c36d69a7b30927eaea16335e06cfcc247accde35/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java?ref=c36d69a7b30927eaea16335e06cfcc247accde35",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
                "patch": "@@ -264,12 +264,13 @@ public void blockIdCK(String blockId) {\n       return;\n     }\n \n+    namenode.getNamesystem().readLock();\n     try {\n       //get blockInfo\n       Block block = new Block(Block.getBlockId(blockId));\n       //find which file this block belongs to\n       BlockInfo blockInfo = blockManager.getStoredBlock(block);\n-      if(blockInfo == null) {\n+      if (blockInfo == null || blockInfo.isDeleted()) {\n         out.println(\"Block \"+ blockId +\" \" + NONEXISTENT_STATUS);\n         LOG.warn(\"Block \"+ blockId + \" \" + NONEXISTENT_STATUS);\n         return;\n@@ -329,6 +330,8 @@ public void blockIdCK(String blockId) {\n       out.println(e.getMessage());\n       out.print(\"\\n\\n\" + errMsg);\n       LOG.warn(\"Error in looking up block\", e);\n+    } finally {\n+      namenode.getNamesystem().readUnlock(\"fsck\");\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/c36d69a7b30927eaea16335e06cfcc247accde35/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NamenodeFsck.java",
                "sha": "0201ca116105de5577eb027d0ab8d0f22c5dd2ff",
                "status": "modified"
            }
        ],
        "message": "HDFS-13027. Handle possible NPEs due to deleted blocks in race condition. Contributed by Vinayakumar B.\n\n(cherry picked from commit 65977e5d8124be2bc208af25beed934933f170b3)",
        "parent": "https://github.com/apache/hadoop/commit/f2c2a68ec208f640e778fc41f95f0284fcc44729",
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockInfo.java",
            "TestBlockManager.java",
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_c416284": {
        "bug_id": "hadoop_c416284",
        "commit": "https://github.com/apache/hadoop/commit/c416284bb7581747beef36d7899d8680fe33abbd",
        "file": [
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop/blob/c416284bb7581747beef36d7899d8680fe33abbd/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/GpuResourcePlugin.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/GpuResourcePlugin.java?ref=c416284bb7581747beef36d7899d8680fe33abbd",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/GpuResourcePlugin.java",
                "patch": "@@ -18,6 +18,7 @@\n \n package org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu;\n \n+import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.exceptions.YarnException;\n import org.apache.hadoop.yarn.server.nodemanager.Context;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor;\n@@ -33,8 +34,14 @@\n import org.apache.hadoop.yarn.server.nodemanager.webapp.dao.gpu.NMGpuResourceInfo;\n \n import java.util.List;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n public class GpuResourcePlugin implements ResourcePlugin {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(GpuResourcePlugin.class);\n+\n   private final GpuNodeResourceUpdateHandler resourceDiscoverHandler;\n   private final GpuDiscoverer gpuDiscoverer;\n   private GpuResourceHandlerImpl gpuResourceHandler = null;\n@@ -84,6 +91,10 @@ public DockerCommandPlugin getDockerCommandPluginInstance() {\n   public synchronized NMResourceInfo getNMResourceInfo() throws YarnException {\n     GpuDeviceInformation gpuDeviceInformation =\n         gpuDiscoverer.getGpuDeviceInformation();\n+\n+    //At this point the gpu plugin is already enabled\n+    checkGpuResourceHandler();\n+\n     GpuResourceAllocator gpuResourceAllocator =\n         gpuResourceHandler.getGpuAllocator();\n     List<GpuDevice> totalGpus = gpuResourceAllocator.getAllowedGpusCopy();\n@@ -94,6 +105,17 @@ public synchronized NMResourceInfo getNMResourceInfo() throws YarnException {\n         assignedGpuDevices);\n   }\n \n+  private void checkGpuResourceHandler() throws YarnException {\n+    if(gpuResourceHandler == null) {\n+      String errorMsg =\n+          \"Linux Container Executor is not configured for the NodeManager. \"\n+              + \"To fully enable GPU feature on the node also set \"\n+              + YarnConfiguration.NM_CONTAINER_EXECUTOR + \" properly.\";\n+      LOG.warn(errorMsg);\n+      throw new YarnException(errorMsg);\n+    }\n+  }\n+\n   @Override\n   public String toString() {\n     return GpuResourcePlugin.class.getName();",
                "raw_url": "https://github.com/apache/hadoop/raw/c416284bb7581747beef36d7899d8680fe33abbd/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/GpuResourcePlugin.java",
                "sha": "1ac6f83846633574a248bd1f2bb35d227570672f",
                "status": "modified"
            },
            {
                "additions": 54,
                "blob_url": "https://github.com/apache/hadoop/blob/c416284bb7581747beef36d7899d8680fe33abbd/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/TestGpuResourcePlugin.java",
                "changes": 54,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/TestGpuResourcePlugin.java?ref=c416284bb7581747beef36d7899d8680fe33abbd",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/TestGpuResourcePlugin.java",
                "patch": "@@ -0,0 +1,54 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.yarn.server.nodemanager.containermanager.resourceplugin.gpu;\n+\n+import static org.mockito.Mockito.mock;\n+\n+import org.apache.hadoop.yarn.exceptions.YarnException;\n+import org.junit.Test;\n+\n+public class TestGpuResourcePlugin {\n+\n+  @Test(expected = YarnException.class)\n+  public void testResourceHandlerNotInitialized() throws YarnException {\n+    GpuDiscoverer gpuDiscoverer = mock(GpuDiscoverer.class);\n+    GpuNodeResourceUpdateHandler gpuNodeResourceUpdateHandler =\n+        mock(GpuNodeResourceUpdateHandler.class);\n+\n+    GpuResourcePlugin target =\n+        new GpuResourcePlugin(gpuNodeResourceUpdateHandler, gpuDiscoverer);\n+\n+    target.getNMResourceInfo();\n+  }\n+\n+  @Test\n+  public void testResourceHandlerIsInitialized() throws YarnException {\n+    GpuDiscoverer gpuDiscoverer = mock(GpuDiscoverer.class);\n+    GpuNodeResourceUpdateHandler gpuNodeResourceUpdateHandler =\n+        mock(GpuNodeResourceUpdateHandler.class);\n+\n+    GpuResourcePlugin target =\n+        new GpuResourcePlugin(gpuNodeResourceUpdateHandler, gpuDiscoverer);\n+\n+    target.createResourceHandler(null, null, null);\n+\n+    //Not throwing any exception\n+    target.getNMResourceInfo();\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/c416284bb7581747beef36d7899d8680fe33abbd/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/resourceplugin/gpu/TestGpuResourcePlugin.java",
                "sha": "888f8999d5d755976256a2a499775029f645979f",
                "status": "added"
            }
        ],
        "message": "YARN-9235. If linux container executor is not set for a GPU cluster GpuResourceHandlerImpl is not initialized and NPE is thrown. Contributed by Antal Balint Steinbach, Adam Antal",
        "parent": "https://github.com/apache/hadoop/commit/190e4349d77e7ae0601ff81a70c7569c72833ee3",
        "repo": "hadoop",
        "unit_tests": [
            "TestGpuResourcePlugin.java"
        ]
    },
    "hadoop_c4382e7": {
        "bug_id": "hadoop_c4382e7",
        "commit": "https://github.com/apache/hadoop/commit/c4382e7565447277e716c22dd20053113e0732cb",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c4382e7565447277e716c22dd20053113e0732cb/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=c4382e7565447277e716c22dd20053113e0732cb",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -561,6 +561,8 @@ Release 2.1.0-beta - UNRELEASED\n \n     HDFS-4382. Fix typo MAX_NOT_CHANGED_INTERATIONS. (Ted Yu via suresh)\n \n+    HDFS-4840. ReplicationMonitor gets NPE during shutdown. (kihwal)\n+\n   BREAKDOWN OF HDFS-347 SUBTASKS AND RELATED JIRAS\n \n     HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes.",
                "raw_url": "https://github.com/apache/hadoop/raw/c4382e7565447277e716c22dd20053113e0732cb/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "a85341ec5480ca225f9dfb62af0e0a0ffabd4601",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/c4382e7565447277e716c22dd20053113e0732cb/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=c4382e7565447277e716c22dd20053113e0732cb",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -3094,10 +3094,15 @@ public void run() {\n           computeDatanodeWork();\n           processPendingReplications();\n           Thread.sleep(replicationRecheckInterval);\n-        } catch (InterruptedException ie) {\n-          LOG.warn(\"ReplicationMonitor thread received InterruptedException.\", ie);\n-          break;\n         } catch (Throwable t) {\n+          if (!namesystem.isRunning()) {\n+            LOG.info(\"Stopping ReplicationMonitor.\");\n+            if (!(t instanceof InterruptedException)) {\n+              LOG.info(\"ReplicationMonitor received an exception\"\n+                  + \" while shutting down.\", t);\n+            }\n+            break;\n+          }\n           LOG.fatal(\"ReplicationMonitor thread received Runtime exception. \", t);\n           terminate(1, t);\n         }",
                "raw_url": "https://github.com/apache/hadoop/raw/c4382e7565447277e716c22dd20053113e0732cb/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "46809da5cb1dca4eaeaef0696b662f86e6bdd197",
                "status": "modified"
            }
        ],
        "message": "HDFS-4840. ReplicationMonitor gets NPE during shutdown. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1489634 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/af65d6f80ee095f8a7652244511d02ce5584a160",
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockManager.java"
        ]
    },
    "hadoop_c48f297": {
        "bug_id": "hadoop_c48f297",
        "commit": "https://github.com/apache/hadoop/commit/c48f2976a3de60b95c4a5ada4f0131c4cdde177a",
        "file": [
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop/blob/c48f2976a3de60b95c4a5ada4f0131c4cdde177a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java?ref=c48f2976a3de60b95c4a5ada4f0131c4cdde177a",
                "deletions": 13,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java",
                "patch": "@@ -330,20 +330,19 @@ private static ApplicationReportExt convertToApplicationReport(\n       }\n \n       if (entityInfo.containsKey(ApplicationMetricsConstants.APP_CPU_METRICS)) {\n-        long vcoreSeconds=Long.parseLong(entityInfo.get(\n-                ApplicationMetricsConstants.APP_CPU_METRICS).toString());\n-        long memorySeconds=Long.parseLong(entityInfo.get(\n-                ApplicationMetricsConstants.APP_MEM_METRICS).toString());\n-        long preemptedMemorySeconds = Long.parseLong(entityInfo.get(\n-            ApplicationMetricsConstants\n-                .APP_MEM_PREEMPT_METRICS).toString());\n-        long preemptedVcoreSeconds = Long.parseLong(entityInfo.get(\n-            ApplicationMetricsConstants\n-                .APP_CPU_PREEMPT_METRICS).toString());\n-        appResources = ApplicationResourceUsageReport\n-            .newInstance(0, 0, null, null, null, memorySeconds, vcoreSeconds, 0,\n-                0, preemptedMemorySeconds, preemptedVcoreSeconds);\n+        long vcoreSeconds = parseLong(entityInfo,\n+            ApplicationMetricsConstants.APP_CPU_METRICS);\n+        long memorySeconds = parseLong(entityInfo,\n+            ApplicationMetricsConstants.APP_MEM_METRICS);\n+        long preemptedMemorySeconds = parseLong(entityInfo,\n+            ApplicationMetricsConstants.APP_MEM_PREEMPT_METRICS);\n+        long preemptedVcoreSeconds = parseLong(entityInfo,\n+            ApplicationMetricsConstants.APP_CPU_PREEMPT_METRICS);\n+        appResources = ApplicationResourceUsageReport.newInstance(0, 0, null,\n+            null, null, memorySeconds, vcoreSeconds, 0, 0,\n+            preemptedMemorySeconds, preemptedVcoreSeconds);\n       }\n+\n       if (entityInfo.containsKey(ApplicationMetricsConstants.APP_TAGS_INFO)) {\n         appTags = new HashSet<String>();\n         Object obj = entityInfo.get(ApplicationMetricsConstants.APP_TAGS_INFO);\n@@ -445,6 +444,16 @@ private static ApplicationReportExt convertToApplicationReport(\n         amNodeLabelExpression), appViewACLs);\n   }\n \n+  private static long parseLong(Map<String, Object> entityInfo,\n+      String infoKey) {\n+    long result = 0;\n+    Object infoValue = entityInfo.get(infoKey);\n+    if (infoValue != null) {\n+      result = Long.parseLong(infoValue.toString());\n+    }\n+    return result;\n+  }\n+\n   private static boolean isFinalState(YarnApplicationState state) {\n     return state == YarnApplicationState.FINISHED\n         || state == YarnApplicationState.FAILED",
                "raw_url": "https://github.com/apache/hadoop/raw/c48f2976a3de60b95c4a5ada4f0131c4cdde177a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/ApplicationHistoryManagerOnTimelineStore.java",
                "sha": "d18f3dc2257feaad6c842afca273a9e79900ba0d",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop/blob/c48f2976a3de60b95c4a5ada4f0131c4cdde177a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java?ref=c48f2976a3de60b95c4a5ada4f0131c4cdde177a",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java",
                "patch": "@@ -143,6 +143,10 @@ private static void prepareTimelineStore(TimelineStore store, int scale)\n       if (i == 2) {\n         entities.addEntity(createApplicationTimelineEntity(\n             appId, true, false, false, true, YarnApplicationState.FINISHED));\n+      } else if (i == 3) {\n+        entities.addEntity(createApplicationTimelineEntity(\n+            appId, false, false, false, false, YarnApplicationState.FINISHED,\n+            true));\n       } else {\n         entities.addEntity(createApplicationTimelineEntity(\n             appId, false, false, false, false, YarnApplicationState.FINISHED));\n@@ -176,7 +180,7 @@ private static void prepareTimelineStore(TimelineStore store, int scale)\n \n   @Test\n   public void testGetApplicationReport() throws Exception {\n-    for (int i = 1; i <= 2; ++i) {\n+    for (int i = 1; i <= 3; ++i) {\n       final ApplicationId appId = ApplicationId.newInstance(0, i);\n       ApplicationReport app;\n       if (callerUGI == null) {\n@@ -214,7 +218,7 @@ public ApplicationReport run() throws Exception {\n       Assert.assertTrue(app.getApplicationTags().contains(\"Test_APP_TAGS_2\"));\n       // App 2 doesn't have the ACLs, such that the default ACLs \" \" will be used.\n       // Nobody except admin and owner has access to the details of the app.\n-      if ((i ==  1 && callerUGI != null &&\n+      if ((i != 2 && callerUGI != null &&\n           callerUGI.getShortUserName().equals(\"user3\")) ||\n           (i ==  2 && callerUGI != null &&\n           (callerUGI.getShortUserName().equals(\"user2\") ||\n@@ -245,10 +249,16 @@ public ApplicationReport run() throws Exception {\n           applicationResourceUsageReport.getMemorySeconds());\n       Assert\n           .assertEquals(345, applicationResourceUsageReport.getVcoreSeconds());\n-      Assert.assertEquals(456,\n+      long expectedPreemptMemSecs = 456;\n+      long expectedPreemptVcoreSecs = 789;\n+      if (i == 3) {\n+        expectedPreemptMemSecs = 0;\n+        expectedPreemptVcoreSecs = 0;\n+      }\n+      Assert.assertEquals(expectedPreemptMemSecs,\n           applicationResourceUsageReport.getPreemptedMemorySeconds());\n       Assert\n-          .assertEquals(789, applicationResourceUsageReport\n+          .assertEquals(expectedPreemptVcoreSecs, applicationResourceUsageReport\n               .getPreemptedVcoreSeconds());\n       Assert.assertEquals(FinalApplicationStatus.UNDEFINED,\n           app.getFinalApplicationStatus());\n@@ -486,6 +496,14 @@ private static TimelineEntity createApplicationTimelineEntity(\n       ApplicationId appId, boolean emptyACLs, boolean noAttemptId,\n       boolean wrongAppId, boolean enableUpdateEvent,\n       YarnApplicationState state) {\n+    return createApplicationTimelineEntity(appId, emptyACLs, noAttemptId,\n+        wrongAppId, enableUpdateEvent, state, false);\n+  }\n+\n+  private static TimelineEntity createApplicationTimelineEntity(\n+      ApplicationId appId, boolean emptyACLs, boolean noAttemptId,\n+      boolean wrongAppId, boolean enableUpdateEvent,\n+      YarnApplicationState state, boolean missingPreemptMetrics) {\n     TimelineEntity entity = new TimelineEntity();\n     entity.setEntityType(ApplicationMetricsConstants.ENTITY_TYPE);\n     if (wrongAppId) {\n@@ -510,8 +528,10 @@ private static TimelineEntity createApplicationTimelineEntity(\n         Integer.MAX_VALUE + 1L);\n     entityInfo.put(ApplicationMetricsConstants.APP_MEM_METRICS, 123);\n     entityInfo.put(ApplicationMetricsConstants.APP_CPU_METRICS, 345);\n-    entityInfo.put(ApplicationMetricsConstants.APP_MEM_PREEMPT_METRICS,456);\n-    entityInfo.put(ApplicationMetricsConstants.APP_CPU_PREEMPT_METRICS,789);\n+    if (!missingPreemptMetrics) {\n+      entityInfo.put(ApplicationMetricsConstants.APP_MEM_PREEMPT_METRICS, 456);\n+      entityInfo.put(ApplicationMetricsConstants.APP_CPU_PREEMPT_METRICS, 789);\n+    }\n     if (emptyACLs) {\n       entityInfo.put(ApplicationMetricsConstants.APP_VIEW_ACLS_ENTITY_INFO, \"\");\n     } else {",
                "raw_url": "https://github.com/apache/hadoop/raw/c48f2976a3de60b95c4a5ada4f0131c4cdde177a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/applicationhistoryservice/TestApplicationHistoryManagerOnTimelineStore.java",
                "sha": "96002516fbafd587b7b667f3d7a3262041430f5e",
                "status": "modified"
            }
        ],
        "message": "YARN-6598. History server getApplicationReport NPE when fetching report for pre-2.8 job (Jason Lowe via jeagles)",
        "parent": "https://github.com/apache/hadoop/commit/6600abbb5c23a83e3a9ef48a945bc8fe19c8178a",
        "repo": "hadoop",
        "unit_tests": [
            "TestApplicationHistoryManagerOnTimelineStore.java"
        ]
    },
    "hadoop_c54a4bb": {
        "bug_id": "hadoop_c54a4bb",
        "commit": "https://github.com/apache/hadoop/commit/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -326,6 +326,8 @@ Trunk (Unreleased)\n \n     HADOOP-10431. Change visibility of KeyStore.Options getter methods to public. (tucu)\n \n+    HADOOP-10583. bin/hadoop key throws NPE with no args and assorted other fixups. (clamb via tucu)\n+\n   OPTIMIZATIONS\n \n     HADOOP-7761. Improve the performance of raw comparisons. (todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "36fe52b7b5d307bfa4020b5af7d58dc7c841997c",
                "status": "modified"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/hadoop/blob/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
                "changes": 71,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java?ref=c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337",
                "deletions": 33,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
                "patch": "@@ -27,9 +27,7 @@\n import java.security.NoSuchAlgorithmException;\n import java.text.MessageFormat;\n import java.util.Date;\n-import java.util.LinkedHashMap;\n import java.util.List;\n-import java.util.Map;\n \n import com.google.gson.stream.JsonReader;\n import com.google.gson.stream.JsonWriter;\n@@ -176,22 +174,26 @@ protected int addVersion() {\n     protected byte[] serialize() throws IOException {\n       ByteArrayOutputStream buffer = new ByteArrayOutputStream();\n       JsonWriter writer = new JsonWriter(new OutputStreamWriter(buffer));\n-      writer.beginObject();\n-      if (cipher != null) {\n-        writer.name(CIPHER_FIELD).value(cipher);\n-      }\n-      if (bitLength != 0) {\n-        writer.name(BIT_LENGTH_FIELD).value(bitLength);\n-      }\n-      if (created != null) {\n-        writer.name(CREATED_FIELD).value(created.getTime());\n-      }\n-      if (description != null) {\n-        writer.name(DESCRIPTION_FIELD).value(description);\n+      try {\n+        writer.beginObject();\n+        if (cipher != null) {\n+          writer.name(CIPHER_FIELD).value(cipher);\n+        }\n+        if (bitLength != 0) {\n+          writer.name(BIT_LENGTH_FIELD).value(bitLength);\n+        }\n+        if (created != null) {\n+          writer.name(CREATED_FIELD).value(created.getTime());\n+        }\n+        if (description != null) {\n+          writer.name(DESCRIPTION_FIELD).value(description);\n+        }\n+        writer.name(VERSIONS_FIELD).value(versions);\n+        writer.endObject();\n+        writer.flush();\n+      } finally {\n+        writer.close();\n       }\n-      writer.name(VERSIONS_FIELD).value(versions);\n-      writer.endObject();\n-      writer.flush();\n       return buffer.toByteArray();\n     }\n \n@@ -207,23 +209,27 @@ protected Metadata(byte[] bytes) throws IOException {\n       int versions = 0;\n       String description = null;\n       JsonReader reader = new JsonReader(new InputStreamReader\n-          (new ByteArrayInputStream(bytes)));\n-      reader.beginObject();\n-      while (reader.hasNext()) {\n-        String field = reader.nextName();\n-        if (CIPHER_FIELD.equals(field)) {\n-          cipher = reader.nextString();\n-        } else if (BIT_LENGTH_FIELD.equals(field)) {\n-          bitLength = reader.nextInt();\n-        } else if (CREATED_FIELD.equals(field)) {\n-          created = new Date(reader.nextLong());\n-        } else if (VERSIONS_FIELD.equals(field)) {\n-          versions = reader.nextInt();\n-        } else if (DESCRIPTION_FIELD.equals(field)) {\n-          description = reader.nextString();\n+        (new ByteArrayInputStream(bytes)));\n+      try {\n+        reader.beginObject();\n+        while (reader.hasNext()) {\n+          String field = reader.nextName();\n+          if (CIPHER_FIELD.equals(field)) {\n+            cipher = reader.nextString();\n+          } else if (BIT_LENGTH_FIELD.equals(field)) {\n+            bitLength = reader.nextInt();\n+          } else if (CREATED_FIELD.equals(field)) {\n+            created = new Date(reader.nextLong());\n+          } else if (VERSIONS_FIELD.equals(field)) {\n+            versions = reader.nextInt();\n+          } else if (DESCRIPTION_FIELD.equals(field)) {\n+            description = reader.nextString();\n+          }\n         }\n+        reader.endObject();\n+      } finally {\n+        reader.close();\n       }\n-      reader.endObject();\n       this.cipher = cipher;\n       this.bitLength = bitLength;\n       this.created = created;\n@@ -310,7 +316,6 @@ public abstract KeyVersion getKeyVersion(String versionName\n    */\n   public abstract List<String> getKeys() throws IOException;\n \n-\n   /**\n    * Get key metadata in bulk.\n    * @param names the names of the keys to get",
                "raw_url": "https://github.com/apache/hadoop/raw/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
                "sha": "0b031c0493b8f6cc2961f2ecd54b3d74ddc54ef8",
                "status": "modified"
            },
            {
                "additions": 88,
                "blob_url": "https://github.com/apache/hadoop/blob/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java",
                "changes": 168,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java?ref=c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337",
                "deletions": 80,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java",
                "patch": "@@ -23,9 +23,6 @@\n import java.security.InvalidParameterException;\n import java.security.NoSuchAlgorithmException;\n import java.util.List;\n-import java.util.Map;\n-\n-import javax.crypto.KeyGenerator;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.conf.Configured;\n@@ -93,41 +90,54 @@ public int run(String[] args) throws Exception {\n    */\n   private int init(String[] args) throws IOException {\n     for (int i = 0; i < args.length; i++) { // parse command line\n+      boolean moreTokens = (i < args.length - 1);\n       if (args[i].equals(\"create\")) {\n-        String keyName = args[++i];\n+        String keyName = \"--help\";\n+        if (moreTokens) {\n+          keyName = args[++i];\n+        }\n+\n         command = new CreateCommand(keyName);\n-        if (keyName.equals(\"--help\")) {\n+        if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n           return -1;\n         }\n       } else if (args[i].equals(\"delete\")) {\n-        String keyName = args[++i];\n+        String keyName = \"--help\";\n+        if (moreTokens) {\n+          keyName = args[++i];\n+        }\n+\n         command = new DeleteCommand(keyName);\n-        if (keyName.equals(\"--help\")) {\n+        if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n           return -1;\n         }\n       } else if (args[i].equals(\"roll\")) {\n-        String keyName = args[++i];\n+        String keyName = \"--help\";\n+        if (moreTokens) {\n+          keyName = args[++i];\n+        }\n+\n         command = new RollCommand(keyName);\n-        if (keyName.equals(\"--help\")) {\n+        if (\"--help\".equals(keyName)) {\n           printKeyShellUsage();\n           return -1;\n         }\n-      } else if (args[i].equals(\"list\")) {\n+      } else if (\"list\".equals(args[i])) {\n         command = new ListCommand();\n-      } else if (args[i].equals(\"--size\")) {\n+      } else if (\"--size\".equals(args[i]) && moreTokens) {\n         getConf().set(KeyProvider.DEFAULT_BITLENGTH_NAME, args[++i]);\n-      } else if (args[i].equals(\"--cipher\")) {\n+      } else if (\"--cipher\".equals(args[i]) && moreTokens) {\n         getConf().set(KeyProvider.DEFAULT_CIPHER_NAME, args[++i]);\n-      } else if (args[i].equals(\"--provider\")) {\n+      } else if (\"--provider\".equals(args[i]) && moreTokens) {\n         userSuppliedProvider = true;\n         getConf().set(KeyProviderFactory.KEY_PROVIDER_PATH, args[++i]);\n-      } else if (args[i].equals(\"--metadata\")) {\n+      } else if (\"--metadata\".equals(args[i])) {\n         getConf().setBoolean(LIST_METADATA, true);\n-      } else if (args[i].equals(\"-i\") || (args[i].equals(\"--interactive\"))) {\n+      } else if (\"-i\".equals(args[i]) || (\"--interactive\".equals(args[i]))) {\n         interactive = true;\n-      } else if (args[i].equals(\"--help\")) {\n+      } else if (\"--help\".equals(args[i])) {\n         printKeyShellUsage();\n         return -1;\n       } else {\n@@ -136,15 +146,20 @@ private int init(String[] args) throws IOException {\n         return -1;\n       }\n     }\n+\n+    if (command == null) {\n+      printKeyShellUsage();\n+      return -1;\n+    }\n+\n     return 0;\n   }\n \n   private void printKeyShellUsage() {\n     out.println(USAGE_PREFIX + COMMANDS);\n     if (command != null) {\n       out.println(command.getUsage());\n-    }\n-    else {\n+    } else {\n       out.println(\"=========================================================\" +\n       \t\t\"======\");\n       out.println(CreateCommand.USAGE + \":\\n\\n\" + CreateCommand.DESC);\n@@ -174,8 +189,7 @@ protected KeyProvider getKeyProvider() {\n         providers = KeyProviderFactory.getProviders(getConf());\n         if (userSuppliedProvider) {\n           provider = providers.get(0);\n-        }\n-        else {\n+        } else {\n           for (KeyProvider p : providers) {\n             if (!p.isTransient()) {\n               provider = p;\n@@ -190,7 +204,7 @@ protected KeyProvider getKeyProvider() {\n     }\n \n     protected void printProviderWritten() {\n-        out.println(provider.getClass().getName() + \" has been updated.\");\n+        out.println(provider + \" has been updated.\");\n     }\n \n     protected void warnIfTransientProvider() {\n@@ -206,12 +220,12 @@ protected void warnIfTransientProvider() {\n \n   private class ListCommand extends Command {\n     public static final String USAGE =\n-        \"list [--provider] [--metadata] [--help]\";\n+        \"list [--provider <provider>] [--metadata] [--help]\";\n     public static final String DESC =\n-        \"The list subcommand displays the keynames contained within \\n\" +\n-        \"a particular provider - as configured in core-site.xml or \" +\n-        \"indicated\\nthrough the --provider argument.\\n\" +\n-        \"If the --metadata option is used, the keys metadata will be printed\";\n+        \"The list subcommand displays the keynames contained within\\n\" +\n+        \"a particular provider as configured in core-site.xml or\\n\" +\n+        \"specified with the --provider argument. --metadata displays\\n\" +\n+        \"the metadata.\";\n \n     private boolean metadata = false;\n \n@@ -220,9 +234,9 @@ public boolean validate() {\n       provider = getKeyProvider();\n       if (provider == null) {\n         out.println(\"There are no non-transient KeyProviders configured.\\n\"\n-            + \"Consider using the --provider option to indicate the provider\\n\"\n-            + \"to use. If you want to list a transient provider then you\\n\"\n-            + \"you MUST use the --provider argument.\");\n+          + \"Use the --provider option to specify a provider. If you\\n\"\n+          + \"want to list a transient provider then you must use the\\n\"\n+          + \"--provider argument.\");\n         rc = false;\n       }\n       metadata = getConf().getBoolean(LIST_METADATA, false);\n@@ -231,12 +245,12 @@ public boolean validate() {\n \n     public void execute() throws IOException {\n       try {\n-        List<String> keys = provider.getKeys();\n-        out.println(\"Listing keys for KeyProvider: \" + provider.toString());\n+        final List<String> keys = provider.getKeys();\n+        out.println(\"Listing keys for KeyProvider: \" + provider);\n         if (metadata) {\n-          Metadata[] meta =\n+          final Metadata[] meta =\n             provider.getKeysMetadata(keys.toArray(new String[keys.size()]));\n-          for(int i=0; i < meta.length; ++i) {\n+          for (int i = 0; i < meta.length; ++i) {\n             out.println(keys.get(i) + \" : \" + meta[i]);\n           }\n         } else {\n@@ -245,7 +259,7 @@ public void execute() throws IOException {\n           }\n         }\n       } catch (IOException e) {\n-        out.println(\"Cannot list keys for KeyProvider: \" + provider.toString()\n+        out.println(\"Cannot list keys for KeyProvider: \" + provider\n             + \": \" + e.getMessage());\n         throw e;\n       }\n@@ -258,11 +272,10 @@ public String getUsage() {\n   }\n \n   private class RollCommand extends Command {\n-    public static final String USAGE = \"roll <keyname> [--provider] [--help]\";\n+    public static final String USAGE = \"roll <keyname> [--provider <provider>] [--help]\";\n     public static final String DESC =\n-        \"The roll subcommand creates a new version of the key specified\\n\" +\n-        \"through the <keyname> argument within the provider indicated using\\n\" +\n-        \"the --provider argument\";\n+      \"The roll subcommand creates a new version for the specified key\\n\" +\n+      \"within the provider indicated using the --provider argument\\n\";\n \n     String keyName = null;\n \n@@ -274,39 +287,37 @@ public boolean validate() {\n       boolean rc = true;\n       provider = getKeyProvider();\n       if (provider == null) {\n-        out.println(\"There are no valid KeyProviders configured.\\n\"\n-            + \"Key will not be rolled.\\n\"\n-            + \"Consider using the --provider option to indicate the provider\"\n-            + \" to use.\");\n+        out.println(\"There are no valid KeyProviders configured. The key\\n\" +\n+          \"has not been rolled. Use the --provider option to specify\\n\" +\n+          \"a provider.\");\n         rc = false;\n       }\n       if (keyName == null) {\n-        out.println(\"There is no keyName specified. Please provide the\" +\n-            \"mandatory <keyname>. See the usage description with --help.\");\n+        out.println(\"Please provide a <keyname>.\\n\" +\n+          \"See the usage description by using --help.\");\n         rc = false;\n       }\n       return rc;\n     }\n \n     public void execute() throws NoSuchAlgorithmException, IOException {\n       try {\n-        Metadata md = provider.getMetadata(keyName);\n         warnIfTransientProvider();\n         out.println(\"Rolling key version from KeyProvider: \"\n-            + provider.toString() + \" for key name: \" + keyName);\n+            + provider + \"\\n  for key name: \" + keyName);\n         try {\n           provider.rollNewVersion(keyName);\n           out.println(keyName + \" has been successfully rolled.\");\n           provider.flush();\n           printProviderWritten();\n         } catch (NoSuchAlgorithmException e) {\n           out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-              + provider.toString());\n+              + provider);\n           throw e;\n         }\n       } catch (IOException e1) {\n         out.println(\"Cannot roll key: \" + keyName + \" within KeyProvider: \"\n-            + provider.toString());\n+            + provider);\n         throw e1;\n       }\n     }\n@@ -318,11 +329,11 @@ public String getUsage() {\n   }\n \n   private class DeleteCommand extends Command {\n-    public static final String USAGE = \"delete <keyname> [--provider] [--help]\";\n+    public static final String USAGE = \"delete <keyname> [--provider <provider>] [--help]\";\n     public static final String DESC =\n-        \"The delete subcommand deletes all of the versions of the key\\n\" +\n-        \"specified as the <keyname> argument from within the provider\\n\" +\n-        \"indicated through the --provider argument\";\n+        \"The delete subcommand deletes all versions of the key\\n\" +\n+        \"specified by the <keyname> argument from within the\\n\" +\n+        \"provider specified --provider.\";\n \n     String keyName = null;\n     boolean cont = true;\n@@ -335,23 +346,21 @@ public DeleteCommand(String keyName) {\n     public boolean validate() {\n       provider = getKeyProvider();\n       if (provider == null) {\n-        out.println(\"There are no valid KeyProviders configured.\\n\"\n-            + \"Nothing will be deleted.\\n\"\n-            + \"Consider using the --provider option to indicate the provider\"\n-            + \" to use.\");\n+        out.println(\"There are no valid KeyProviders configured. Nothing\\n\"\n+          + \"was deleted. Use the --provider option to specify a provider.\");\n         return false;\n       }\n       if (keyName == null) {\n-        out.println(\"There is no keyName specified. Please provide the\" +\n-            \"mandatory <keyname>. See the usage description with --help.\");\n+        out.println(\"There is no keyName specified. Please specify a \" +\n+            \"<keyname>. See the usage description with --help.\");\n         return false;\n       }\n       if (interactive) {\n         try {\n           cont = ToolRunner\n               .confirmPrompt(\"You are about to DELETE all versions of \"\n-                  + \"the key: \" + keyName + \" from KeyProvider \"\n-                  + provider.toString() + \". Continue?:\");\n+                  + \" key: \" + keyName + \" from KeyProvider \"\n+                  + provider + \". Continue?:\");\n           if (!cont) {\n             out.println(\"Nothing has been be deleted.\");\n           }\n@@ -367,15 +376,15 @@ public boolean validate() {\n     public void execute() throws IOException {\n       warnIfTransientProvider();\n       out.println(\"Deleting key: \" + keyName + \" from KeyProvider: \"\n-          + provider.toString());\n+          + provider);\n       if (cont) {\n         try {\n           provider.deleteKey(keyName);\n           out.println(keyName + \" has been successfully deleted.\");\n           provider.flush();\n           printProviderWritten();\n         } catch (IOException e) {\n-          out.println(keyName + \"has NOT been deleted.\");\n+          out.println(keyName + \" has not been deleted.\");\n           throw e;\n         }\n       }\n@@ -388,16 +397,16 @@ public String getUsage() {\n   }\n \n   private class CreateCommand extends Command {\n-    public static final String USAGE = \"create <keyname> [--cipher] \" +\n-    \t\t\"[--size] [--provider] [--help]\";\n+    public static final String USAGE =\n+      \"create <keyname> [--cipher <cipher>] [--size <size>]\\n\" +\n+      \"                     [--provider <provider>] [--help]\";\n     public static final String DESC =\n-        \"The create subcommand creates a new key for the name specified\\n\" +\n-        \"as the <keyname> argument within the provider indicated through\\n\" +\n-        \"the --provider argument. You may also indicate the specific\\n\" +\n-        \"cipher through the --cipher argument. The default for cipher is\\n\" +\n-        \"currently \\\"AES/CTR/NoPadding\\\". The default keysize is \\\"256\\\".\\n\" +\n-        \"You may also indicate the requested key length through the --size\\n\" +\n-        \"argument.\";\n+      \"The create subcommand creates a new key for the name specified\\n\" +\n+      \"by the <keyname> argument within the provider specified by the\\n\" +\n+      \"--provider argument. You may specify a cipher with the --cipher\\n\" +\n+      \"argument. The default cipher is currently \\\"AES/CTR/NoPadding\\\".\\n\" +\n+      \"The default keysize is 256. You may specify the requested key\\n\" +\n+      \"length using the --size argument.\\n\";\n \n     String keyName = null;\n \n@@ -409,15 +418,14 @@ public boolean validate() {\n       boolean rc = true;\n       provider = getKeyProvider();\n       if (provider == null) {\n-        out.println(\"There are no valid KeyProviders configured.\\nKey\" +\n-        \t\t\" will not be created.\\n\"\n-            + \"Consider using the --provider option to indicate the provider\" +\n-            \" to use.\");\n+        out.println(\"There are no valid KeyProviders configured. No key\\n\" +\n+          \" was created. You can use the --provider option to specify\\n\" +\n+          \" a provider to use.\");\n         rc = false;\n       }\n       if (keyName == null) {\n-        out.println(\"There is no keyName specified. Please provide the\" +\n-        \t\t\"mandatory <keyname>. See the usage description with --help.\");\n+        out.println(\"Please provide a <keyname>. See the usage description\" +\n+          \" with --help.\");\n         rc = false;\n       }\n       return rc;\n@@ -432,13 +440,13 @@ public void execute() throws IOException, NoSuchAlgorithmException {\n         provider.flush();\n         printProviderWritten();\n       } catch (InvalidParameterException e) {\n-        out.println(keyName + \" has NOT been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.getMessage());\n         throw e;\n       } catch (IOException e) {\n-        out.println(keyName + \" has NOT been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.getMessage());\n         throw e;\n       } catch (NoSuchAlgorithmException e) {\n-        out.println(keyName + \" has NOT been created. \" + e.getMessage());\n+        out.println(keyName + \" has not been created. \" + e.getMessage());\n         throw e;\n       }\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyShell.java",
                "sha": "3d56640e11e4f674a7c09c65b47b7208373fd599",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java?ref=c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java",
                "patch": "@@ -126,7 +126,6 @@ public KeyProvider createProvider(URI providerName, Configuration conf)\n     return o;\n   }\n \n-\n   public static String checkNotEmpty(String s, String name)\n       throws IllegalArgumentException {\n     checkNotNull(s, name);\n@@ -140,6 +139,13 @@ public static String checkNotEmpty(String s, String name)\n   private String kmsUrl;\n   private SSLFactory sslFactory;\n \n+  @Override\n+  public String toString() {\n+    final StringBuilder sb = new StringBuilder(\"KMSClientProvider[\");\n+    sb.append(kmsUrl).append(\"]\");\n+    return sb.toString();\n+  }\n+\n   public KMSClientProvider(URI uri, Configuration conf) throws IOException {\n     Path path = unnestUri(uri);\n     URL url = path.toUri().toURL();\n@@ -515,5 +521,4 @@ public void flush() throws IOException {\n   public static String buildVersionName(String name, int version) {\n     return KeyProvider.buildVersionName(name, version);\n   }\n-\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java",
                "sha": "ff30f86de377f4a763aec5ccd62d532101d99311",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java?ref=c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337",
                "deletions": 2,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java",
                "patch": "@@ -121,7 +121,7 @@ public void testInvalidKeySize() throws Exception {\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n     assertEquals(-1, rc);\n-    assertTrue(outContent.toString().contains(\"key1 has NOT been created.\"));\n+    assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n   @Test\n@@ -134,7 +134,7 @@ public void testInvalidCipher() throws Exception {\n     ks.setConf(new Configuration());\n     rc = ks.run(args1);\n     assertEquals(-1, rc);\n-    assertTrue(outContent.toString().contains(\"key1 has NOT been created.\"));\n+    assertTrue(outContent.toString().contains(\"key1 has not been created.\"));\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/c54a4bb666cdeef41a71ed8eeb5ddbe7c5ccc337/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyShell.java",
                "sha": "ae6938eb68ce2fe2f963d70438e8c0959a4edd1d",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10583. bin/hadoop key throws NPE with no args and assorted other fixups. (clamb via tucu)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1594320 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/4bc3371824d6f0495a5a6a2892c0683b4d66a5a8",
        "repo": "hadoop",
        "unit_tests": [
            "TestKeyProvider.java",
            "TestKeyShell.java",
            "TestKMSClientProvider.java"
        ]
    },
    "hadoop_c5a241f": {
        "bug_id": "hadoop_c5a241f",
        "commit": "https://github.com/apache/hadoop/commit/c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -280,6 +280,8 @@ Release 2.4.0 - UNRELEASED\n     MAPREDUCE-5724. JobHistoryServer does not start if HDFS is not running. \n     (tucu)\n \n+    MAPREDUCE-5729. mapred job -list throws NPE (kasha)\n+\n Release 2.3.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "88b0ff8ec1ae84287873f6f5ba86abadc8e0a6ed",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/TypeConverter.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/TypeConverter.java?ref=c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975",
                "deletions": 5,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/TypeConverter.java",
                "patch": "@@ -43,6 +43,7 @@\n import org.apache.hadoop.mapreduce.v2.util.MRApps;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.ApplicationReport;\n+import org.apache.hadoop.yarn.api.records.ApplicationResourceUsageReport;\n import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n import org.apache.hadoop.yarn.api.records.NodeReport;\n import org.apache.hadoop.yarn.api.records.QueueACL;\n@@ -445,11 +446,18 @@ public static JobStatus fromYarn(ApplicationReport application,\n     jobStatus.setStartTime(application.getStartTime());\n     jobStatus.setFinishTime(application.getFinishTime());\n     jobStatus.setFailureInfo(application.getDiagnostics());\n-    jobStatus.setNeededMem(application.getApplicationResourceUsageReport().getNeededResources().getMemory());\n-    jobStatus.setNumReservedSlots(application.getApplicationResourceUsageReport().getNumReservedContainers());\n-    jobStatus.setNumUsedSlots(application.getApplicationResourceUsageReport().getNumUsedContainers());\n-    jobStatus.setReservedMem(application.getApplicationResourceUsageReport().getReservedResources().getMemory());\n-    jobStatus.setUsedMem(application.getApplicationResourceUsageReport().getUsedResources().getMemory());\n+    ApplicationResourceUsageReport resourceUsageReport =\n+        application.getApplicationResourceUsageReport();\n+    if (resourceUsageReport != null) {\n+      jobStatus.setNeededMem(\n+          resourceUsageReport.getNeededResources().getMemory());\n+      jobStatus.setNumReservedSlots(\n+          resourceUsageReport.getNumReservedContainers());\n+      jobStatus.setNumUsedSlots(resourceUsageReport.getNumUsedContainers());\n+      jobStatus.setReservedMem(\n+          resourceUsageReport.getReservedResources().getMemory());\n+      jobStatus.setUsedMem(resourceUsageReport.getUsedResources().getMemory());\n+    }\n     return jobStatus;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/main/java/org/apache/hadoop/mapreduce/TypeConverter.java",
                "sha": "6b4aa4ed1e40a57ce9558d146e02b40b60bc35ea",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/TestTypeConverter.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/TestTypeConverter.java?ref=c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/TestTypeConverter.java",
                "patch": "@@ -23,8 +23,6 @@\n import java.util.ArrayList;\n import java.util.List;\n \n-import junit.framework.Assert;\n-\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.mapreduce.JobStatus.State;\n import org.apache.hadoop.mapreduce.v2.api.records.JobId;\n@@ -40,6 +38,7 @@\n import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.YarnApplicationState;\n import org.apache.hadoop.yarn.util.Records;\n+import org.junit.Assert;\n import org.junit.Test;\n import org.mockito.Mockito;\n \n@@ -112,6 +111,14 @@ public void testFromYarnApplicationReport() {\n     when(mockReport.getUser()).thenReturn(\"dummy-user\");\n     when(mockReport.getQueue()).thenReturn(\"dummy-queue\");\n     String jobFile = \"dummy-path/job.xml\";\n+\n+    try {\n+      JobStatus status = TypeConverter.fromYarn(mockReport, jobFile);\n+    } catch (NullPointerException npe) {\n+      Assert.fail(\"Type converstion from YARN fails for jobs without \" +\n+          \"ApplicationUsageReport\");\n+    }\n+\n     ApplicationResourceUsageReport appUsageRpt = Records\n         .newRecord(ApplicationResourceUsageReport.class);\n     Resource r = Records.newRecord(Resource.class);",
                "raw_url": "https://github.com/apache/hadoop/raw/c5a241f1ddc3f7bca516c6fb1d7be29f2a7a9975/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/src/test/java/org/apache/hadoop/mapreduce/TestTypeConverter.java",
                "sha": "cc42b9c220f4b7704379054d7d90025117aed5d5",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-5729. mapred job -list throws NPE (kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1559811 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/602f71a8daa0dc98d0183028935e3b10460e28a5",
        "repo": "hadoop",
        "unit_tests": [
            "TestTypeConverter.java"
        ]
    },
    "hadoop_c684f2b": {
        "bug_id": "hadoop_c684f2b",
        "commit": "https://github.com/apache/hadoop/commit/c684f2b007a4808dafbe1c1d3ce01758e281d329",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c684f2b007a4808dafbe1c1d3ce01758e281d329/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=c684f2b007a4808dafbe1c1d3ce01758e281d329",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -227,6 +227,9 @@ Release 2.9.0 - UNRELEASED\n     YARN-4651. Document movetoqueue option in 'YARN Commands'\n     (Takashi Ohnishi via rohithsharmaks)\n \n+    YARN-4729. SchedulerApplicationAttempt#getTotalRequiredResources can throw \n+    an NPE. (kasha)\n+\n Release 2.8.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/c684f2b007a4808dafbe1c1d3ce01758e281d329/hadoop-yarn-project/CHANGES.txt",
                "sha": "7f26f8e63cda9d19ac4d07eec0a910e412a5d630",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c684f2b007a4808dafbe1c1d3ce01758e281d329/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java?ref=c684f2b007a4808dafbe1c1d3ce01758e281d329",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java",
                "patch": "@@ -244,7 +244,8 @@ public synchronized ResourceRequest getResourceRequest(Priority priority,\n   }\n \n   public synchronized int getTotalRequiredResources(Priority priority) {\n-    return getResourceRequest(priority, ResourceRequest.ANY).getNumContainers();\n+    ResourceRequest request = getResourceRequest(priority, ResourceRequest.ANY);\n+    return request == null ? 0 : request.getNumContainers();\n   }\n \n   public synchronized Resource getResource(Priority priority) {",
                "raw_url": "https://github.com/apache/hadoop/raw/c684f2b007a4808dafbe1c1d3ce01758e281d329/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerApplicationAttempt.java",
                "sha": "254200972cbb71aff59569c4d2c50df28390de0d",
                "status": "modified"
            }
        ],
        "message": "YARN-4729. SchedulerApplicationAttempt#getTotalRequiredResources can throw an NPE. (kasha)",
        "parent": "https://github.com/apache/hadoop/commit/dbbfc58c33fd1d2f7abae1784c2d78b7438825e2",
        "repo": "hadoop",
        "unit_tests": [
            "TestSchedulerApplicationAttempt.java"
        ]
    },
    "hadoop_c797103": {
        "bug_id": "hadoop_c797103",
        "commit": "https://github.com/apache/hadoop/commit/c79710302ee51e1a9ee17dadb161c69bb3aba5c9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c79710302ee51e1a9ee17dadb161c69bb3aba5c9/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=c79710302ee51e1a9ee17dadb161c69bb3aba5c9",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -1107,6 +1107,9 @@ Release 2.7.0 - UNRELEASED\n     HDFS-7885. Datanode should not trust the generation stamp provided by\n     client. (Tsz Wo Nicholas Sze via jing9)\n \n+    HDFS-7818. OffsetParam should return the default value instead of throwing\n+    NPE when the value is unspecified. (Eric Payne via wheat9)\n+\n     BREAKDOWN OF HDFS-7584 SUBTASKS AND RELATED JIRAS\n \n       HDFS-7720. Quota by Storage Type API, tools and ClientNameNode",
                "raw_url": "https://github.com/apache/hadoop/raw/c79710302ee51e1a9ee17dadb161c69bb3aba5c9/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "b443902f3c7b4fd825f4e4bc599d0ffdb6e61dc4",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/c79710302ee51e1a9ee17dadb161c69bb3aba5c9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ParameterParser.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ParameterParser.java?ref=c79710302ee51e1a9ee17dadb161c69bb3aba5c9",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ParameterParser.java",
                "patch": "@@ -62,7 +62,7 @@ String op() {\n   }\n \n   long offset() {\n-    return new OffsetParam(param(OffsetParam.NAME)).getValue();\n+    return new OffsetParam(param(OffsetParam.NAME)).getOffset();\n   }\n \n   String namenodeId() {",
                "raw_url": "https://github.com/apache/hadoop/raw/c79710302ee51e1a9ee17dadb161c69bb3aba5c9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/ParameterParser.java",
                "sha": "2baafe8f29c4c195c31ac5dfb10681034e86cefb",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/c79710302ee51e1a9ee17dadb161c69bb3aba5c9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/OffsetParam.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/OffsetParam.java?ref=c79710302ee51e1a9ee17dadb161c69bb3aba5c9",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/OffsetParam.java",
                "patch": "@@ -46,4 +46,9 @@ public OffsetParam(final String str) {\n   public String getName() {\n     return NAME;\n   }\n+\n+  public Long getOffset() {\n+    Long offset = getValue();\n+    return (offset == null) ? Long.valueOf(0) : offset;\n+  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/c79710302ee51e1a9ee17dadb161c69bb3aba5c9/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/web/resources/OffsetParam.java",
                "sha": "6d88703da0abc27dce2baeccbadf2b7b2c26c8b5",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/c79710302ee51e1a9ee17dadb161c69bb3aba5c9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/TestParameterParser.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/TestParameterParser.java?ref=c79710302ee51e1a9ee17dadb161c69bb3aba5c9",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/TestParameterParser.java",
                "patch": "@@ -23,6 +23,7 @@\n import org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenIdentifier;\n import org.apache.hadoop.hdfs.web.resources.DelegationParam;\n import org.apache.hadoop.hdfs.web.resources.NamenodeAddressParam;\n+import org.apache.hadoop.hdfs.web.resources.OffsetParam;\n import org.apache.hadoop.security.token.Token;\n import org.junit.Assert;\n import org.junit.Test;\n@@ -65,4 +66,22 @@ public void testDecodePath() {\n     ParameterParser testParser = new ParameterParser(decoder, conf);\n     Assert.assertEquals(EXPECTED_PATH, testParser.path());\n   }\n+\n+  @Test\n+  public void testOffset() throws IOException {\n+    final long X = 42;\n+\n+    long offset = new OffsetParam(Long.toString(X)).getOffset();\n+    Assert.assertEquals(\"OffsetParam: \", X, offset);\n+\n+    offset = new OffsetParam((String) null).getOffset();\n+    Assert.assertEquals(\"OffsetParam with null should have defaulted to 0\", 0, offset);\n+\n+    try {\n+      offset = new OffsetParam(\"abc\").getValue();\n+      Assert.fail(\"OffsetParam with nondigit value should have thrown IllegalArgumentException\");\n+    } catch (IllegalArgumentException iae) {\n+      // Ignore\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/c79710302ee51e1a9ee17dadb161c69bb3aba5c9/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/web/webhdfs/TestParameterParser.java",
                "sha": "8aee1d8565c1caa83b51cd9d3a77357319432f67",
                "status": "modified"
            }
        ],
        "message": "HDFS-7818. OffsetParam should return the default value instead of throwing NPE when the value is unspecified. Contributed by Eric Payne.",
        "parent": "https://github.com/apache/hadoop/commit/21101c01f242439ec8ec40fb3a9ab1991ae0adc7",
        "repo": "hadoop",
        "unit_tests": [
            "TestParameterParser.java"
        ]
    },
    "hadoop_c9cb6a5": {
        "bug_id": "hadoop_c9cb6a5",
        "commit": "https://github.com/apache/hadoop/commit/c9cb6a5960ad335a3ee93a6ee219eae5aad372f9",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c9cb6a5960ad335a3ee93a6ee219eae5aad372f9/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=c9cb6a5960ad335a3ee93a6ee219eae5aad372f9",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -863,6 +863,8 @@ Release 2.8.0 - UNRELEASED\n     YARN-4135. Improve the assertion message in MockRM while failing after waiting for the state.\n     (Nijel S F via rohithsharmaks)\n \n+    YARN-4167. NPE on RMActiveServices#serviceStop when store is null. (Bibin A Chundatt via rohithsharmaks)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/c9cb6a5960ad335a3ee93a6ee219eae5aad372f9/hadoop-yarn-project/CHANGES.txt",
                "sha": "a3dfb85445ab887596e48830f8ba686ce32cc482",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c9cb6a5960ad335a3ee93a6ee219eae5aad372f9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java?ref=c9cb6a5960ad335a3ee93a6ee219eae5aad372f9",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "patch": "@@ -605,7 +605,9 @@ protected void serviceStop() throws Exception {\n       if (rmContext != null) {\n         RMStateStore store = rmContext.getStateStore();\n         try {\n-          store.close();\n+          if (null != store) {\n+            store.close();\n+          }\n         } catch (Exception e) {\n           LOG.error(\"Error closing store.\", e);\n         }",
                "raw_url": "https://github.com/apache/hadoop/raw/c9cb6a5960ad335a3ee93a6ee219eae5aad372f9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceManager.java",
                "sha": "d1f339a1a848082dd2dcef8b13c34a5e2da2620a",
                "status": "modified"
            }
        ],
        "message": "YARN-4167. NPE on RMActiveServices#serviceStop when store is null. (Bibin A Chundatt via rohithsharmaks)",
        "parent": "https://github.com/apache/hadoop/commit/3a9c7076e81c1cc47c0ecf30c60abd9a65d8a501",
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceManager.java"
        ]
    },
    "hadoop_c9dd2ca": {
        "bug_id": "hadoop_c9dd2ca",
        "commit": "https://github.com/apache/hadoop/commit/c9dd2cada055c0beffd04bad0ded8324f66ad1b7",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/c9dd2cada055c0beffd04bad0ded8324f66ad1b7/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=c9dd2cada055c0beffd04bad0ded8324f66ad1b7",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -598,6 +598,9 @@ Release 2.8.0 - UNRELEASED\n     YARN-2194. Fix bug causing CGroups functionality to fail on RHEL7.\n     (Wei Yan via vvasudev)\n \n+    YARN-3892. Fixed NPE on RMStateStore#serviceStop when\n+    CapacityScheduler#serviceInit fails. (Bibin A Chundatt via jianhe)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/c9dd2cada055c0beffd04bad0ded8324f66ad1b7/hadoop-yarn-project/CHANGES.txt",
                "sha": "8b5cc0cd9ba6f7ace5feb0c8dfd27eb6cfc2bff1",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/c9dd2cada055c0beffd04bad0ded8324f66ad1b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java?ref=c9dd2cada055c0beffd04bad0ded8324f66ad1b7",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "patch": "@@ -40,6 +40,7 @@\n import org.apache.hadoop.classification.InterfaceAudience.Private;\n import org.apache.hadoop.classification.InterfaceStability.Unstable;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.security.token.delegation.DelegationKey;\n import org.apache.hadoop.util.ZKUtil;\n import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n@@ -312,7 +313,7 @@ protected synchronized void closeInternal() throws Exception {\n       verifyActiveStatusThread.interrupt();\n       verifyActiveStatusThread.join(1000);\n     }\n-    curatorFramework.close();\n+    IOUtils.closeStream(curatorFramework);\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/c9dd2cada055c0beffd04bad0ded8324f66ad1b7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "sha": "8f096d882aaf14adf29a5235f481904bbade6376",
                "status": "modified"
            }
        ],
        "message": "YARN-3892. Fixed NPE on RMStateStore#serviceStop when CapacityScheduler#serviceInit fails. Contributed by Bibin A Chundatt",
        "parent": "https://github.com/apache/hadoop/commit/c0b8e4e5b5083631ed22d8d36c8992df7d34303c",
        "repo": "hadoop",
        "unit_tests": [
            "TestZKRMStateStore.java"
        ]
    },
    "hadoop_cde3a00": {
        "bug_id": "hadoop_cde3a00",
        "commit": "https://github.com/apache/hadoop/commit/cde3a00526c562a500308232e2b93498d22c90d7",
        "file": [
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/ContainerExecutor.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/ContainerExecutor.java?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 15,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/ContainerExecutor.java",
                "patch": "@@ -67,8 +67,8 @@\n  * underlying OS.  All executor implementations must extend ContainerExecutor.\n  */\n public abstract class ContainerExecutor implements Configurable {\n-  private static final String WILDCARD = \"*\";\n   private static final Log LOG = LogFactory.getLog(ContainerExecutor.class);\n+  protected static final String WILDCARD = \"*\";\n \n   /**\n    * The permissions to use when creating the launch script.\n@@ -274,15 +274,16 @@ public int reacquireContainer(ContainerReacquisitionContext ctx)\n    * @param environment the environment variables and their values\n    * @param resources the resources which have been localized for this\n    * container. Symlinks will be created to these localized resources\n-   * @param command the command that will be run.\n-   * @param logDir the log dir to copy debugging information to\n+   * @param command the command that will be run\n+   * @param logDir the log dir to which to copy debugging information\n+   * @param user the username of the job owner\n    * @throws IOException if any errors happened writing to the OutputStream,\n    * while creating symlinks\n    */\n   public void writeLaunchEnv(OutputStream out, Map<String, String> environment,\n-      Map<Path, List<String>> resources, List<String> command, Path logDir)\n-      throws IOException {\n-    this.writeLaunchEnv(out, environment, resources, command, logDir,\n+      Map<Path, List<String>> resources, List<String> command, Path logDir,\n+      String user) throws IOException {\n+    this.writeLaunchEnv(out, environment, resources, command, logDir, user,\n         ContainerLaunch.CONTAINER_SCRIPT);\n   }\n \n@@ -295,17 +296,17 @@ public void writeLaunchEnv(OutputStream out, Map<String, String> environment,\n    * @param environment the environment variables and their values\n    * @param resources the resources which have been localized for this\n    * container. Symlinks will be created to these localized resources\n-   * @param command the command that will be run.\n-   * @param logDir the log dir to copy debugging information to\n+   * @param command the command that will be run\n+   * @param logDir the log dir to which to copy debugging information\n+   * @param user the username of the job owner\n    * @param outFilename the path to which to write the launch environment\n    * @throws IOException if any errors happened writing to the OutputStream,\n    * while creating symlinks\n    */\n   @VisibleForTesting\n-  public void writeLaunchEnv(OutputStream out,\n-      Map<String, String> environment, Map<Path, List<String>> resources,\n-      List<String> command, Path logDir, String outFilename)\n-      throws IOException {\n+  public void writeLaunchEnv(OutputStream out, Map<String, String> environment,\n+      Map<Path, List<String>> resources, List<String> command, Path logDir,\n+      String user, String outFilename) throws IOException {\n     ContainerLaunch.ShellScriptBuilder sb =\n         ContainerLaunch.ShellScriptBuilder.create();\n     Set<String> whitelist = new HashSet<>();\n@@ -334,9 +335,7 @@ public void writeLaunchEnv(OutputStream out,\n           if (new Path(linkName).getName().equals(WILDCARD)) {\n             // If this is a wildcarded path, link to everything in the\n             // directory from the working directory\n-            File directory = new File(resourceEntry.getKey().toString());\n-\n-            for (File wildLink : directory.listFiles()) {\n+            for (File wildLink : readDirAsUser(user, resourceEntry.getKey())) {\n               sb.symlink(new Path(wildLink.toString()),\n                   new Path(wildLink.getName()));\n             }\n@@ -370,6 +369,19 @@ public void writeLaunchEnv(OutputStream out,\n     }\n   }\n \n+  /**\n+   * Return the files in the target directory. If retrieving the list of files\n+   * requires specific access rights, that access will happen as the\n+   * specified user. The list will not include entries for \".\" or \"..\".\n+   *\n+   * @param user the user as whom to access the target directory\n+   * @param dir the target directory\n+   * @return a list of files in the target directory\n+   */\n+  protected File[] readDirAsUser(String user, Path dir) {\n+    return new File(dir.toString()).listFiles();\n+  }\n+\n   /**\n    * The container exit code.\n    */",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/ContainerExecutor.java",
                "sha": "818b0ea4557e4e897a90930aad58a360a0c273e8",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DockerContainerExecutor.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DockerContainerExecutor.java?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DockerContainerExecutor.java",
                "patch": "@@ -330,8 +330,8 @@ public int launchContainer(ContainerStartContext ctx) throws IOException {\n    * the docker image and write them out to an OutputStream.\n    */\n   public void writeLaunchEnv(OutputStream out, Map<String, String> environment,\n-    Map<Path, List<String>> resources, List<String> command, Path logDir)\n-    throws IOException {\n+      Map<Path, List<String>> resources, List<String> command, Path logDir,\n+      String user) throws IOException {\n     ContainerLaunch.ShellScriptBuilder sb =\n       ContainerLaunch.ShellScriptBuilder.create();\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/DockerContainerExecutor.java",
                "sha": "ebf9566fb3ddcafefa075826ce1e6a5b7c2782f3",
                "status": "modified"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "patch": "@@ -54,7 +54,6 @@\n import org.apache.hadoop.yarn.server.nodemanager.util.CgroupsLCEResourcesHandler;\n import org.apache.hadoop.yarn.server.nodemanager.util.DefaultLCEResourcesHandler;\n import org.apache.hadoop.yarn.server.nodemanager.util.LCEResourcesHandler;\n-\n import java.io.File;\n import java.io.IOException;\n import java.net.InetSocketAddress;\n@@ -644,6 +643,45 @@ public void deleteAsUser(DeletionAsUserContext ctx) {\n     }\n   }\n \n+  @Override\n+  protected File[] readDirAsUser(String user, Path dir) {\n+    List<File> files = new ArrayList<>();\n+    PrivilegedOperation listAsUserOp = new PrivilegedOperation(\n+        PrivilegedOperation.OperationType.LIST_AS_USER, (String)null);\n+    String runAsUser = getRunAsUser(user);\n+    String dirString = \"\";\n+\n+    if (dir != null) {\n+      dirString = dir.toUri().getPath();\n+    }\n+\n+    listAsUserOp.appendArgs(runAsUser, user,\n+        Integer.toString(\n+            PrivilegedOperation.RunAsUserCommand.LIST_AS_USER.getValue()),\n+        dirString);\n+\n+    try {\n+      PrivilegedOperationExecutor privOpExecutor =\n+          PrivilegedOperationExecutor.getInstance(super.getConf());\n+\n+      String results =\n+          privOpExecutor.executePrivilegedOperation(listAsUserOp, true);\n+\n+      for (String file: results.split(\"\\n\")) {\n+        // The container-executor always dumps its log output to stdout, which\n+        // includes 3 lines that start with \"main : \"\n+        if (!file.startsWith(\"main :\")) {\n+          files.add(new File(new File(dirString), file));\n+        }\n+      }\n+    } catch (PrivilegedOperationException e) {\n+      LOG.error(\"ListAsUser for \" + dir + \" returned with exit code: \"\n+          + e.getExitCode(), e);\n+    }\n+\n+    return files.toArray(new File[files.size()]);\n+  }\n+\n   @Override\n   public boolean isContainerAlive(ContainerLivenessContext ctx)\n       throws IOException {",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "sha": "6890b256e52b262a87737de319a2866a7be3c43d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java",
                "patch": "@@ -267,7 +267,7 @@ public Integer call() {\n         // Write out the environment\n         exec.writeLaunchEnv(containerScriptOutStream, environment,\n           localResources, launchContext.getCommands(),\n-            new Path(containerLogDirs.get(0)));\n+            new Path(containerLogDirs.get(0)), user);\n \n         // /////////// End of writing out container-script\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/ContainerLaunch.java",
                "sha": "14190fc98dc2b02cb0e3a36b64d269e2de775eeb",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperation.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperation.java?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperation.java",
                "patch": "@@ -50,7 +50,8 @@\n     TC_READ_STATE(\"--tc-read-state\"),\n     TC_READ_STATS(\"--tc-read-stats\"),\n     ADD_PID_TO_CGROUP(\"\"), //no CLI switch supported yet.\n-    RUN_DOCKER_CMD(\"--run-docker\");\n+    RUN_DOCKER_CMD(\"--run-docker\"),\n+    LIST_AS_USER(\"\"); //no CLI switch supported yet.\n \n     private final String option;\n \n@@ -146,7 +147,8 @@ public int hashCode() {\n     LAUNCH_CONTAINER(1),\n     SIGNAL_CONTAINER(2),\n     DELETE_AS_USER(3),\n-    LAUNCH_DOCKER_CONTAINER(4);\n+    LAUNCH_DOCKER_CONTAINER(4),\n+    LIST_AS_USER(5);\n \n     private int value;\n     RunAsUserCommand(int value) {",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperation.java",
                "sha": "8402a16339dbb0f992fd56228bd096c7e8048591",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c",
                "changes": 54,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c",
                "patch": "@@ -439,7 +439,7 @@ char *concatenate(char *concat_pattern, char *return_path_name,\n   for (j = 0; j < numArgs; j++) {\n     arg = va_arg(ap, char*);\n     if (arg == NULL) {\n-      fprintf(LOGFILE, \"One of the arguments passed for %s in null.\\n\",\n+      fprintf(LOGFILE, \"One of the arguments passed for %s is null.\\n\",\n           return_path_name);\n       return NULL;\n     }\n@@ -1929,6 +1929,58 @@ int delete_as_user(const char *user,\n   return ret;\n }\n \n+/**\n+ * List the files in the given directory as the user.\n+ * user: the user listing the files\n+ * target_dir: the directory from which to list files\n+ */\n+int list_as_user(const char *target_dir) {\n+  int ret = 0;\n+  struct stat sb;\n+\n+  if (stat(target_dir, &sb) != 0) {\n+    // If directory doesn't exist or can't be accessed, error out\n+    fprintf(LOGFILE, \"Could not stat %s - %s\\n\", target_dir,\n+        strerror(errno));\n+    ret = -1;\n+  } else if (!S_ISDIR(sb.st_mode)) {\n+    // If it's not a directory, list it as the only file\n+    printf(\"%s\\n\", target_dir);\n+  } else {\n+    DIR *dir = opendir(target_dir);\n+\n+    if (dir != NULL) {\n+      struct dirent *file;\n+\n+      errno = 0;\n+\n+      do {\n+        file = readdir(dir);\n+\n+        // Ignore the . and .. entries\n+        if ((file != NULL) &&\n+            (strcmp(\".\", file->d_name) != 0) &&\n+            (strcmp(\"..\", file->d_name) != 0)) {\n+          printf(\"%s\\n\", file->d_name);\n+        }\n+      } while (file != NULL);\n+\n+      // If we ended the directory read early on an error, then error out\n+      if (errno != 0) {\n+        fprintf(LOGFILE, \"Could not read directory %s - %s\\n\", target_dir,\n+            strerror(errno));\n+        ret = -1;\n+      }\n+    } else {\n+      fprintf(LOGFILE, \"Could not open directory %s - %s\\n\", target_dir,\n+          strerror(errno));\n+      ret = -1;\n+    }\n+  }\n+\n+  return ret;\n+}\n+\n void chown_dir_contents(const char *dir_path, uid_t uid, gid_t gid) {\n   DIR *dp;\n   struct dirent *ep;",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.c",
                "sha": "ca3847edde59c3cb6177be55582fa7d68b0cb94c",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h",
                "patch": "@@ -31,7 +31,8 @@ enum command {\n   LAUNCH_CONTAINER = 1,\n   SIGNAL_CONTAINER = 2,\n   DELETE_AS_USER = 3,\n-  LAUNCH_DOCKER_CONTAINER = 4\n+  LAUNCH_DOCKER_CONTAINER = 4,\n+  LIST_AS_USER = 5\n };\n \n enum errorcodes {\n@@ -79,7 +80,8 @@ enum operations {\n   RUN_AS_USER_SIGNAL_CONTAINER = 8,\n   RUN_AS_USER_DELETE = 9,\n   RUN_AS_USER_LAUNCH_DOCKER_CONTAINER = 10,\n-  RUN_DOCKER = 11\n+  RUN_DOCKER = 11,\n+  RUN_AS_USER_LIST = 12\n };\n \n #define NM_GROUP_KEY \"yarn.nodemanager.linux-container-executor.group\"\n@@ -189,6 +191,10 @@ int delete_as_user(const char *user,\n                    const char *dir_to_be_deleted,\n                    char* const* baseDirs);\n \n+// List the files in the given directory on stdout. The target_dir is always\n+// assumed to be an absolute path.\n+int list_as_user(const char *target_dir);\n+\n // set the uid and gid of the node manager.  This is used when doing some\n // priviledged operations for setting the effective uid and gid.\n void set_nm_uid(uid_t user, gid_t group);",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/container-executor.h",
                "sha": "18585550ed2a88e59e959b4a1da33aa582209fa6",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 13,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c",
                "patch": "@@ -58,11 +58,12 @@ static void display_usage(FILE *stream) {\n       \"            launch docker container:      %2d appid containerid workdir container-script \" \\\n                               \"tokens pidfile nm-local-dirs nm-log-dirs docker-command-file resources optional-tc-command-file\\n\" \\\n       \"            signal container:      %2d container-pid signal\\n\" \\\n-      \"            delete as user:        %2d relative-path\\n\" ;\n+      \"            delete as user:        %2d relative-path\\n\" \\\n+      \"            list as user:          %2d relative-path\\n\" ;\n \n \n   fprintf(stream, usage_template, INITIALIZE_CONTAINER, LAUNCH_CONTAINER, LAUNCH_DOCKER_CONTAINER,\n-          SIGNAL_CONTAINER, DELETE_AS_USER);\n+          SIGNAL_CONTAINER, DELETE_AS_USER, LIST_AS_USER);\n }\n \n /* Sets up log files for normal/error logging */\n@@ -169,20 +170,20 @@ static void assert_valid_setup(char *argv0) {\n static struct {\n   char *cgroups_hierarchy;\n   char *traffic_control_command_file;\n-  const char * run_as_user_name;\n-  const char * yarn_user_name;\n+  const char *run_as_user_name;\n+  const char *yarn_user_name;\n   char *local_dirs;\n   char *log_dirs;\n   char *resources_key;\n   char *resources_value;\n   char **resources_values;\n-  const char * app_id;\n-  const char * container_id;\n-  const char * cred_file;\n-  const char * script_file;\n-  const char * current_dir;\n-  const char * pid_file;\n-  const char *dir_to_be_deleted;\n+  const char *app_id;\n+  const char *container_id;\n+  const char *cred_file;\n+  const char *script_file;\n+  const char *current_dir;\n+  const char *pid_file;\n+  const char *target_dir;\n   int container_pid;\n   int signal;\n   const char *docker_command_file;\n@@ -417,9 +418,13 @@ static int validate_run_as_user_commands(int argc, char **argv, int *operation)\n     return 0;\n \n   case DELETE_AS_USER:\n-    cmd_input.dir_to_be_deleted = argv[optind++];\n+    cmd_input.target_dir = argv[optind++];\n     *operation = RUN_AS_USER_DELETE;\n     return 0;\n+  case LIST_AS_USER:\n+    cmd_input.target_dir = argv[optind++];\n+    *operation = RUN_AS_USER_LIST;\n+    return 0;\n   default:\n     fprintf(ERRORFILE, \"Invalid command %d not supported.\",command);\n     fflush(ERRORFILE);\n@@ -554,9 +559,18 @@ int main(int argc, char **argv) {\n     }\n \n     exit_code = delete_as_user(cmd_input.yarn_user_name,\n-                        cmd_input.dir_to_be_deleted,\n+                        cmd_input.target_dir,\n                         argv + optind);\n     break;\n+  case RUN_AS_USER_LIST:\n+    exit_code = set_user(cmd_input.run_as_user_name);\n+\n+    if (exit_code != 0) {\n+      break;\n+    }\n+\n+    exit_code = list_as_user(cmd_input.target_dir);\n+    break;\n   }\n \n   flush_and_close_log_files();",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/impl/main.c",
                "sha": "56215ca4c79531acf18016c0a0d9b282c2d368ed",
                "status": "modified"
            },
            {
                "additions": 157,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/test-container-executor.c",
                "changes": 157,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/test-container-executor.c?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/test-container-executor.c",
                "patch": "@@ -454,6 +454,163 @@ void test_delete_user() {\n   free(app_dir);\n }\n \n+/**\n+ * Read a file and tokenize it on newlines.  Place up to max lines into lines.\n+ * The max+1st element of lines will be set to NULL.\n+ *\n+ * @param file the name of the file to open\n+ * @param lines the pointer array into which to place the lines\n+ * @param max the max number of lines to add to lines\n+ */\n+void read_lines(const char* file, char **lines, size_t max) {\n+  char buf[4096];\n+  size_t nread;\n+\n+  int fd = open(file, O_RDONLY);\n+\n+  if (fd < 0) {\n+    printf(\"FAIL: failed to open directory listing file: %s\\n\", file);\n+    exit(1);\n+  } else {\n+    char *cur = buf;\n+    size_t count = sizeof buf;\n+\n+    while ((nread = read(fd, cur, count)) > 0) {\n+      cur += nread;\n+      count -= nread;\n+    }\n+\n+    if (nread < 0) {\n+      printf(\"FAIL: failed to read directory listing file: %s\\n\", file);\n+      exit(1);\n+    }\n+\n+    close(fd);\n+  }\n+\n+  char* entity = strtok(buf, \"\\n\");\n+  int i;\n+\n+  for (i = 0; i < max; i++) {\n+    if (entity == NULL) {\n+      break;\n+    }\n+\n+    lines[i] = (char *)malloc(sizeof(char) * (strlen(entity) + 1));\n+    strcpy(lines[i], entity);\n+    entity = strtok(NULL, \"\\n\");\n+  }\n+\n+  lines[i] = NULL;\n+}\n+\n+void test_list_as_user() {\n+  printf(\"\\nTesting list_as_user\\n\");\n+  char buffer[4096];\n+\n+  char *app_dir =\n+      get_app_directory(TEST_ROOT \"/local-1\", \"yarn\", \"app_4\");\n+\n+  if (mkdirs(app_dir, 0700) != 0) {\n+    printf(\"FAIL: unble to create application directory: %s\\n\", app_dir);\n+    exit(1);\n+  }\n+\n+  // Test with empty dir string\n+  sprintf(buffer, \"\");\n+  int ret = list_as_user(buffer);\n+\n+  if (ret == 0) {\n+    printf(\"FAIL: did not fail on empty directory string\\n\");\n+    exit(1);\n+  }\n+\n+  // Test with a non-existent directory\n+  sprintf(buffer, \"%s/output\", app_dir);\n+\n+  ret = list_as_user(buffer);\n+\n+  if (ret == 0) {\n+    printf(\"FAIL: did not fail on non-existent directory\\n\");\n+    exit(1);\n+  }\n+\n+  // Write a couple files to list\n+  sprintf(buffer, \"%s/file1\", app_dir);\n+\n+  if (write_config_file(buffer, 1) != 0) {\n+    exit(1);\n+  }\n+\n+  sprintf(buffer, \"%s/.file2\", app_dir);\n+\n+  if (write_config_file(buffer, 1) != 0) {\n+    exit(1);\n+  }\n+\n+  // Also create a directory\n+  sprintf(buffer, \"%s/output\", app_dir);\n+\n+  if (mkdirs(buffer, 0700) != 0) {\n+    exit(1);\n+  }\n+\n+  // Test the regular case\n+  // Store a copy of stdout, then redirect it to a file\n+  sprintf(buffer, \"%s/output/files\", app_dir);\n+\n+  int oldout = dup(STDOUT_FILENO);\n+  int fd = open(buffer, O_WRONLY | O_CREAT, S_IRUSR | S_IWUSR);\n+\n+  dup2(fd, STDOUT_FILENO);\n+\n+  // Now list the files\n+  ret = list_as_user(app_dir);\n+\n+  if (ret != 0) {\n+    printf(\"FAIL: unable to list files in regular case\\n\");\n+    exit(1);\n+  }\n+\n+  // Restore stdout\n+  close(fd);\n+  dup2(oldout, STDOUT_FILENO);\n+\n+  // Check the output -- shouldn't be more than a couple lines\n+  char *lines[16];\n+\n+  read_lines(buffer, lines, 15);\n+\n+  int got_file1 = 0;\n+  int got_file2 = 0;\n+  int got_output = 0;\n+  int i;\n+\n+  for (i = 0; i < sizeof lines; i++) {\n+    if (lines[i] == NULL) {\n+      break;\n+    } else if (strcmp(\"file1\", lines[i]) == 0) {\n+      got_file1 = 1;\n+    } else if (strcmp(\".file2\", lines[i]) == 0) {\n+      got_file2 = 1;\n+    } else if (strcmp(\"output\", lines[i]) == 0) {\n+      got_output = 1;\n+    } else {\n+      printf(\"FAIL: listed extraneous file: %s\\n\", lines[i]);\n+      exit(1);\n+    }\n+\n+    free(lines[i]);\n+  }\n+\n+  if (!got_file1 || !got_file2 || !got_output) {\n+    printf(\"FAIL: missing files in listing\\n\");\n+    exit(1);\n+  }\n+\n+  free(app_dir);\n+}\n+\n void run_test_in_child(const char* test_name, void (*func)()) {\n   printf(\"\\nRunning test %s in child process\\n\", test_name);\n   fflush(stdout);",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/native/container-executor/test/test-container-executor.c",
                "sha": "f174a9ff1c7479f2e18fe637077d8f37b7b94471",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/TestContainerLaunch.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/TestContainerLaunch.java?ref=cde3a00526c562a500308232e2b93498d22c90d7",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/TestContainerLaunch.java",
                "patch": "@@ -174,7 +174,8 @@ public void testSpecialCharSymlinks() throws IOException  {\n \n       new DefaultContainerExecutor()\n           .writeLaunchEnv(fos, env, resources, commands,\n-              new Path(localLogDir.getAbsolutePath()), tempFile.getName());\n+              new Path(localLogDir.getAbsolutePath()), \"user\",\n+              tempFile.getName());\n       fos.flush();\n       fos.close();\n       FileUtil.setExecutable(tempFile, true);\n@@ -243,7 +244,7 @@ public void testInvalidSymlinkDiagnostics() throws IOException  {\n       }\n       new DefaultContainerExecutor()\n           .writeLaunchEnv(fos, env, resources, commands,\n-              new Path(localLogDir.getAbsolutePath()));\n+              new Path(localLogDir.getAbsolutePath()), \"user\");\n       fos.flush();\n       fos.close();\n       FileUtil.setExecutable(tempFile, true);\n@@ -298,7 +299,7 @@ public void testInvalidEnvSyntaxDiagnostics() throws IOException  {\n       List<String> commands = new ArrayList<String>();\n       new DefaultContainerExecutor()\n           .writeLaunchEnv(fos, env, resources, commands,\n-              new Path(localLogDir.getAbsolutePath()));\n+              new Path(localLogDir.getAbsolutePath()), \"user\");\n       fos.flush();\n       fos.close();\n \n@@ -377,7 +378,7 @@ public void testContainerLaunchStdoutAndStderrDiagnostics() throws IOException {\n       commands.add(command);\n       ContainerExecutor exec = new DefaultContainerExecutor();\n       exec.writeLaunchEnv(fos, env, resources, commands,\n-          new Path(localLogDir.getAbsolutePath()));\n+          new Path(localLogDir.getAbsolutePath()), \"user\");\n       fos.flush();\n       fos.close();\n \n@@ -1400,7 +1401,8 @@ public void testDebuggingInformation() throws IOException {\n         ContainerExecutor exec = new DefaultContainerExecutor();\n         exec.setConf(conf);\n         exec.writeLaunchEnv(fos, env, resources, commands,\n-          new Path(localLogDir.getAbsolutePath()), tempFile.getName());\n+            new Path(localLogDir.getAbsolutePath()), \"user\",\n+            tempFile.getName());\n         fos.flush();\n         fos.close();\n         FileUtil.setExecutable(tempFile, true);",
                "raw_url": "https://github.com/apache/hadoop/raw/cde3a00526c562a500308232e2b93498d22c90d7/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/launcher/TestContainerLaunch.java",
                "sha": "be6eadba0a34e51b4d3265ffef23292c7cd9f75f",
                "status": "modified"
            }
        ],
        "message": "YARN-5373. NPE listing wildcard directory in containerLaunch. (Daniel Templeton via kasha)",
        "parent": "https://github.com/apache/hadoop/commit/9ef632f3b0b0e0808116cd1c7482a205b7ebef7d",
        "repo": "hadoop",
        "unit_tests": [
            "TestContainerExecutor.java",
            "TestLinuxContainerExecutor.java",
            "TestContainerLaunch.java"
        ]
    },
    "hadoop_cdf1af0": {
        "bug_id": "hadoop_cdf1af0",
        "commit": "https://github.com/apache/hadoop/commit/cdf1af0e5a21361924f3f7c6cea5170767d2b6bc",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/cdf1af0e5a21361924f3f7c6cea5170767d2b6bc/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=cdf1af0e5a21361924f3f7c6cea5170767d2b6bc",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -772,6 +772,11 @@ Release 2.6.0 - UNRELEASED\n     HDFS-7157. Using Time.now() for recording start/end time of reconfiguration\n     tasks (Lei Xu via cmccabe)\n \n+    HDFS-6664. HDFS permissions guide documentation states incorrect default \n+    group mapping class. (Ray Chiang via aw)\n+\n+    HDFS-4227. Document dfs.namenode.resource.*  (Daisuke Kobayashi via aw)\n+\n     BREAKDOWN OF HDFS-6134 AND HADOOP-10150 SUBTASKS AND RELATED JIRAS\n   \n       HDFS-6387. HDFS CLI admin tool for creating & deleting an\n@@ -1003,10 +1008,7 @@ Release 2.6.0 - UNRELEASED\n     HDFS-7140. Add a tool to list all the existing block storage policies.\n     (jing9)\n \n-    HDFS-6664. HDFS permissions guide documentation states incorrect default \n-    group mapping class. (Ray Chiang via aw)\n-\n-    HDFS-4227. Document dfs.namenode.resource.*  (Daisuke Kobayashi via aw)\n+    HDFS-7167. NPE while running Mover if the given path is for a file. (jing9)\n \n Release 2.5.1 - 2014-09-05\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/cdf1af0e5a21361924f3f7c6cea5170767d2b6bc/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "40891bfc5193b609999b8ebcceb4955ae8aecac5",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/cdf1af0e5a21361924f3f7c6cea5170767d2b6bc/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java?ref=cdf1af0e5a21361924f3f7c6cea5170767d2b6bc",
                "deletions": 14,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
                "patch": "@@ -252,14 +252,9 @@ private boolean isSnapshotPathInCurrent(String path) throws IOException {\n      */\n     private boolean processNamespace() {\n       getSnapshottableDirs();\n-      boolean hasRemaining = true;\n-      try {\n-        for (Path target : targetPaths) {\n-          hasRemaining = processDirRecursively(\"\", dfs.getFileInfo(target\n-              .toUri().getPath()));\n-        }\n-      } catch (IOException e) {\n-        LOG.warn(\"Failed to get root directory status. Ignore and continue.\", e);\n+      boolean hasRemaining = false;\n+      for (Path target : targetPaths) {\n+        hasRemaining |= processPath(target.toUri().getPath());\n       }\n       // wait for pending move to finish and retry the failed migration\n       hasRemaining |= Dispatcher.waitForMoveCompletion(storages.targets.values());\n@@ -270,7 +265,7 @@ private boolean processNamespace() {\n      * @return whether there is still remaing migration work for the next\n      *         round\n      */\n-    private boolean processChildrenList(String fullPath) {\n+    private boolean processPath(String fullPath) {\n       boolean hasRemaining = false;\n       for (byte[] lastReturnedName = HdfsFileStatus.EMPTY_NAME;;) {\n         final DirectoryListing children;\n@@ -285,7 +280,7 @@ private boolean processChildrenList(String fullPath) {\n           return hasRemaining;\n         }\n         for (HdfsFileStatus child : children.getPartialListing()) {\n-          hasRemaining |= processDirRecursively(fullPath, child);\n+          hasRemaining |= processRecursively(fullPath, child);\n         }\n         if (children.hasMore()) {\n           lastReturnedName = children.getLastName();\n@@ -296,20 +291,19 @@ private boolean processChildrenList(String fullPath) {\n     }\n \n     /** @return whether the migration requires next round */\n-    private boolean processDirRecursively(String parent,\n-                                          HdfsFileStatus status) {\n+    private boolean processRecursively(String parent, HdfsFileStatus status) {\n       String fullPath = status.getFullName(parent);\n       boolean hasRemaining = false;\n       if (status.isDir()) {\n         if (!fullPath.endsWith(Path.SEPARATOR)) {\n           fullPath = fullPath + Path.SEPARATOR;\n         }\n \n-        hasRemaining = processChildrenList(fullPath);\n+        hasRemaining = processPath(fullPath);\n         // process snapshots if this is a snapshottable directory\n         if (snapshottableDirs.contains(fullPath)) {\n           final String dirSnapshot = fullPath + HdfsConstants.DOT_SNAPSHOT_DIR;\n-          hasRemaining |= processChildrenList(dirSnapshot);\n+          hasRemaining |= processPath(dirSnapshot);\n         }\n       } else if (!status.isSymlink()) { // file\n         try {",
                "raw_url": "https://github.com/apache/hadoop/raw/cdf1af0e5a21361924f3f7c6cea5170767d2b6bc/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/mover/Mover.java",
                "sha": "04133bd23cc5694fb6a2ee47cf1c80415eeedbd5",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/hadoop/blob/cdf1af0e5a21361924f3f7c6cea5170767d2b6bc/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/mover/TestStorageMover.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/mover/TestStorageMover.java?ref=cdf1af0e5a21361924f3f7c6cea5170767d2b6bc",
                "deletions": 4,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/mover/TestStorageMover.java",
                "patch": "@@ -17,7 +17,6 @@\n  */\n package org.apache.hadoop.hdfs.server.mover;\n \n-import java.io.File;\n import java.io.IOException;\n import java.net.URI;\n import java.util.ArrayList;\n@@ -27,12 +26,10 @@\n import java.util.List;\n import java.util.Map;\n \n-import com.google.common.base.Joiner;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.commons.logging.impl.Log4JLogger;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.conf.ReconfigurationException;\n import org.apache.hadoop.fs.FSDataInputStream;\n import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.Path;\n@@ -46,7 +43,6 @@\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.hdfs.StorageType;\n import org.apache.hadoop.hdfs.protocol.DirectoryListing;\n-import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n import org.apache.hadoop.hdfs.protocol.HdfsFileStatus;\n import org.apache.hadoop.hdfs.protocol.HdfsLocatedFileStatus;\n import org.apache.hadoop.hdfs.protocol.LocatedBlock;\n@@ -514,6 +510,42 @@ static void banner(String string) {\n         \"==================================================\\n\\n\");\n   }\n \n+  /**\n+   * Run Mover with arguments specifying files and directories\n+   */\n+  @Test\n+  public void testMoveSpecificPaths() throws Exception {\n+    LOG.info(\"testMoveSpecificPaths\");\n+    final Path foo = new Path(\"/foo\");\n+    final Path barFile = new Path(foo, \"bar\");\n+    final Path foo2 = new Path(\"/foo2\");\n+    final Path bar2File = new Path(foo2, \"bar2\");\n+    Map<Path, BlockStoragePolicy> policyMap = Maps.newHashMap();\n+    policyMap.put(foo, COLD);\n+    policyMap.put(foo2, WARM);\n+    NamespaceScheme nsScheme = new NamespaceScheme(Arrays.asList(foo, foo2),\n+        Arrays.asList(barFile, bar2File), BLOCK_SIZE, null, policyMap);\n+    ClusterScheme clusterScheme = new ClusterScheme(DEFAULT_CONF,\n+        NUM_DATANODES, REPL, genStorageTypes(NUM_DATANODES), null);\n+    MigrationTest test = new MigrationTest(clusterScheme, nsScheme);\n+    test.setupCluster();\n+\n+    try {\n+      test.prepareNamespace();\n+      test.setStoragePolicy();\n+\n+      Map<URI, List<Path>> map = Mover.Cli.getNameNodePathsToMove(test.conf,\n+          \"-p\", \"/foo/bar\", \"/foo2\");\n+      int result = Mover.run(map, test.conf);\n+      Assert.assertEquals(ExitStatus.SUCCESS.getExitCode(), result);\n+\n+      Thread.sleep(5000);\n+      test.verify(true);\n+    } finally {\n+      test.shutdownCluster();\n+    }\n+  }\n+\n   /**\n    * Move an open file into archival storage\n    */",
                "raw_url": "https://github.com/apache/hadoop/raw/cdf1af0e5a21361924f3f7c6cea5170767d2b6bc/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/mover/TestStorageMover.java",
                "sha": "a6edd8090e8e9cb1beb636da6667e370826e98a4",
                "status": "modified"
            }
        ],
        "message": "HDFS-7167. NPE while running Mover if the given path is for a file. Contributed by Jing Zhao.",
        "parent": "https://github.com/apache/hadoop/commit/ea32a66f7d37d4d6a121e34c95d93ae8992be571",
        "repo": "hadoop",
        "unit_tests": [
            "TestMover.java"
        ]
    },
    "hadoop_ce74e64": {
        "bug_id": "hadoop_ce74e64",
        "commit": "https://github.com/apache/hadoop/commit/ce74e64363abc64561263be70a923ab3e67f043f",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ServiceScheduler.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ServiceScheduler.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ServiceScheduler.java",
                "patch": "@@ -344,11 +344,7 @@ private void registerServiceInstance(ApplicationAttemptId attemptId,\n         attemptId.getApplicationId().toString());\n     serviceRecord.set(YarnRegistryAttributes.YARN_PERSISTENCE,\n         PersistencePolicies.APPLICATION);\n-    serviceRecord.description = \"Yarn Service Master\";\n-\n-    serviceRecord.addExternalEndpoint(RegistryTypeUtils\n-        .ipcEndpoint(\"classpath:org.apache.hadoop.yarn.service.appmaster.ipc\",\n-            context.clientAMService.getBindAddress()));\n+    serviceRecord.description = \"YarnServiceMaster\";\n \n     // set any provided attributes\n     setUserProvidedServiceRecordAttributes(service.getConfiguration(),",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ServiceScheduler.java",
                "sha": "ec5f3ed1519d5e0fd639dfc25023c4359e8f1de9",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "patch": "@@ -170,6 +170,9 @@ private Service loadAppJsonFromLocalFS(\n     if (!StringUtils.isEmpty(args.getServiceName())) {\n       service.setName(args.getServiceName());\n     }\n+    if (!StringUtils.isEmpty(args.queue)) {\n+      service.setQueue(args.queue);\n+    }\n     return service;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "sha": "a3a9fd0ae5fc10ff78724a0305c613148515599d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/params/AbstractClusterBuildingActionArgs.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/params/AbstractClusterBuildingActionArgs.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/params/AbstractClusterBuildingActionArgs.java",
                "patch": "@@ -37,7 +37,7 @@ public File getFile() {\n   }\n \n   @Parameter(names = {\n-      ARG_QUEUE }, description = \"Queue to submit the service\")\n+      ARG_QUEUE, ARG_SHORT_QUEUE}, description = \"Queue to submit the service\")\n   public String queue;\n \n   @Parameter(names = {",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/params/AbstractClusterBuildingActionArgs.java",
                "sha": "4ecbe9c64230afa14836aa91e4febc144de1c19f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/params/Arguments.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/params/Arguments.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/params/Arguments.java",
                "patch": "@@ -77,6 +77,7 @@\n   String ARG_PATH = \"--path\";\n   String ARG_PRINCIPAL = \"--principal\";\n   String ARG_QUEUE = \"--queue\";\n+  String ARG_SHORT_QUEUE = \"-q\";\n   String ARG_LIFETIME = \"--lifetime\";\n   String ARG_RESOURCE = \"--resource\";\n   String ARG_RESOURCE_MANAGER = \"--rm\";",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/params/Arguments.java",
                "sha": "67571e2b45ad81231e15b01d30f24daa9c186cba",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "patch": "@@ -59,6 +59,7 @@\n import java.util.concurrent.locks.ReentrantReadWriteLock.ReadLock;\n import java.util.concurrent.locks.ReentrantReadWriteLock.WriteLock;\n \n+import static org.apache.hadoop.registry.client.types.yarn.YarnRegistryAttributes.*;\n import static org.apache.hadoop.yarn.api.records.ContainerExitStatus.KILLED_BY_APPMASTER;\n import static org.apache.hadoop.yarn.api.records.ContainerState.COMPLETE;\n import static org.apache.hadoop.yarn.service.component.instance.ComponentInstanceEventType.*;\n@@ -356,12 +357,11 @@ private  void updateServiceRecord(\n       YarnRegistryViewForProviders yarnRegistry, ContainerStatus status) {\n     ServiceRecord record = new ServiceRecord();\n     String containerId = status.getContainerId().toString();\n-    record.set(YarnRegistryAttributes.YARN_ID, containerId);\n+    record.set(YARN_ID, containerId);\n     record.description = getCompInstanceName();\n-    record.set(YarnRegistryAttributes.YARN_PERSISTENCE,\n-        PersistencePolicies.CONTAINER);\n-    record.set(\"yarn:ip\", status.getIPs());\n-    record.set(\"yarn:hostname\", status.getHost());\n+    record.set(YARN_PERSISTENCE, PersistencePolicies.CONTAINER);\n+    record.set(YARN_IP, status.getIPs().get(0));\n+    record.set(YARN_HOSTNAME, status.getHost());\n     try {\n       yarnRegistry\n           .putComponent(RegistryPathUtils.encodeYarnID(containerId), record);",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "sha": "3c1e48ff1ab63bd580879b45ba4170f8bddc0b0a",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/AbstractProviderService.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/AbstractProviderService.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/AbstractProviderService.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.hadoop.yarn.service.provider;\n \n+import org.apache.commons.lang.StringUtils;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.yarn.api.ApplicationConstants;\n import org.apache.hadoop.yarn.service.api.records.Service;\n@@ -91,13 +92,16 @@ public void buildContainerLaunchContext(AbstractLauncher launcher,\n         component, tokensForSubstitution, instance, context);\n \n     // substitute launch command\n-    String launchCommand = ProviderUtils\n-        .substituteStrWithTokens(component.getLaunchCommand(),\n-            tokensForSubstitution);\n-    CommandLineBuilder operation = new CommandLineBuilder();\n-    operation.add(launchCommand);\n-    operation.addOutAndErrFiles(OUT_FILE, ERR_FILE);\n-    launcher.addCommand(operation.build());\n+    String launchCommand = component.getLaunchCommand();\n+    // docker container may have empty commands\n+    if (!StringUtils.isEmpty(launchCommand)) {\n+      launchCommand = ProviderUtils\n+          .substituteStrWithTokens(launchCommand, tokensForSubstitution);\n+      CommandLineBuilder operation = new CommandLineBuilder();\n+      operation.add(launchCommand);\n+      operation.addOutAndErrFiles(OUT_FILE, ERR_FILE);\n+      launcher.addCommand(operation.build());\n+    }\n \n     // By default retry forever every 30 seconds\n     launcher.setRetryContext(YarnServiceConf",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/AbstractProviderService.java",
                "sha": "6ffb84defb83a5e4768b827025048b403526a1ae",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/client/binding/RegistryTypeUtils.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/client/binding/RegistryTypeUtils.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/client/binding/RegistryTypeUtils.java",
                "patch": "@@ -22,7 +22,6 @@\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.registry.client.exceptions.InvalidRecordException;\n-import static org.apache.hadoop.registry.client.types.AddressTypes.*;\n import org.apache.hadoop.registry.client.types.Endpoint;\n import org.apache.hadoop.registry.client.types.ProtocolTypes;\n import org.apache.hadoop.registry.client.types.ServiceRecord;\n@@ -36,6 +35,8 @@\n import java.util.List;\n import java.util.Map;\n \n+import static org.apache.hadoop.registry.client.types.AddressTypes.*;\n+\n /**\n  * Static methods to work with registry types \u2014primarily endpoints and the\n  * list representation of addresses.",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/client/binding/RegistryTypeUtils.java",
                "sha": "05df3255e3a76d2c78230e0660ff5d9b07ba0f8b",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/server/dns/ApplicationServiceRecordProcessor.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/server/dns/ApplicationServiceRecordProcessor.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/server/dns/ApplicationServiceRecordProcessor.java",
                "patch": "@@ -18,6 +18,8 @@\n \n import org.apache.hadoop.registry.client.types.Endpoint;\n import org.apache.hadoop.registry.client.types.ServiceRecord;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n import org.xbill.DNS.Name;\n import org.xbill.DNS.Type;\n \n@@ -32,7 +34,8 @@\n  */\n public class ApplicationServiceRecordProcessor extends\n     BaseServiceRecordProcessor {\n-\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(ApplicationServiceRecordProcessor.class);\n   /**\n    * Create an application service record processor.\n    *\n@@ -57,6 +60,10 @@ public ApplicationServiceRecordProcessor(\n    */\n   @Override public void initTypeToInfoMapping(ServiceRecord serviceRecord)\n       throws Exception {\n+    if (serviceRecord.external.isEmpty()) {\n+      LOG.info(serviceRecord.description + \": No external endpoints defined.\");\n+      return;\n+    }\n     for (int type : getRecordTypes()) {\n       switch (type) {\n       case Type.A:\n@@ -309,6 +316,9 @@ public AApplicationRecordDescriptor(String path,\n         throws Exception {\n       this.setNames(new Name[] {getServiceName()});\n       List<Endpoint> endpoints = serviceRecord.external;\n+      if (endpoints.isEmpty()) {\n+        return;\n+      }\n       // TODO:  do we need a \"hostname\" attribute for an application record or\n       // can we rely on the first endpoint record.\n       this.setTarget(InetAddress.getByName(\n@@ -342,6 +352,9 @@ public AAAAApplicationRecordDescriptor(String path,\n     @Override protected void init(ServiceRecord serviceRecord)\n         throws Exception {\n       super.init(serviceRecord);\n+      if (getTarget() == null) {\n+        return;\n+      }\n       try {\n         this.setTarget(getIpv6Address(getTarget()));\n       } catch (UnknownHostException e) {",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/server/dns/ApplicationServiceRecordProcessor.java",
                "sha": "0b5f724f8691cebefbce0090ec8678567b3108dd",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/server/dns/BaseServiceRecordProcessor.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/server/dns/BaseServiceRecordProcessor.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/server/dns/BaseServiceRecordProcessor.java",
                "patch": "@@ -52,8 +52,8 @@\n   private String domain;\n \n   private static final Pattern USER_NAME = Pattern.compile(\"/users/(\\\\w*)/?\");\n-  private static final String SLIDER_API_PREFIX =\n-      \"classpath:org.apache.slider.\";\n+  private static final String YARN_SERVICE_API_PREFIX =\n+      \"classpath:org.apache.hadoop.yarn.service.\";\n   private static final String HTTP_API_TYPE = \"http://\";\n \n   /**\n@@ -425,8 +425,8 @@ protected int getPort(Endpoint endpoint) {\n      */\n     protected String getDNSApiFragment(String api) {\n       String dnsApi = null;\n-      if (api.startsWith(SLIDER_API_PREFIX)) {\n-        dnsApi = api.substring(SLIDER_API_PREFIX.length());\n+      if (api.startsWith(YARN_SERVICE_API_PREFIX)) {\n+        dnsApi = api.substring(YARN_SERVICE_API_PREFIX.length());\n       } else if (api.startsWith(HTTP_API_TYPE)) {\n         dnsApi = \"http\";\n       }",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/main/java/org/apache/hadoop/registry/server/dns/BaseServiceRecordProcessor.java",
                "sha": "fd5c74f64900971d8648f28471a23b9890f35b65",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/test/java/org/apache/hadoop/registry/server/dns/TestRegistryDNS.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/test/java/org/apache/hadoop/registry/server/dns/TestRegistryDNS.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/test/java/org/apache/hadoop/registry/server/dns/TestRegistryDNS.java",
                "patch": "@@ -69,7 +69,8 @@\n       + \"  \\\"type\\\" : \\\"JSONServiceRecord\\\",\\n\"\n       + \"  \\\"description\\\" : \\\"Slider Application Master\\\",\\n\"\n       + \"  \\\"external\\\" : [ {\\n\"\n-      + \"    \\\"api\\\" : \\\"classpath:org.apache.slider.appmaster.ipc\\\",\\n\"\n+      + \"    \\\"api\\\" : \\\"classpath:org.apache.hadoop.yarn.service.appmaster.ipc\"\n+      + \"\\\",\\n\"\n       + \"    \\\"addressType\\\" : \\\"host/port\\\",\\n\"\n       + \"    \\\"protocolType\\\" : \\\"hadoop/IPC\\\",\\n\"\n       + \"    \\\"addresses\\\" : [ {\\n\"\n@@ -84,22 +85,25 @@\n       + \"      \\\"uri\\\" : \\\"http://192.168.1.5:1027\\\"\\n\"\n       + \"    } ]\\n\"\n       + \"  }, {\\n\"\n-      + \"    \\\"api\\\" : \\\"classpath:org.apache.slider.management\\\",\\n\"\n+      + \"    \\\"api\\\" : \\\"classpath:org.apache.hadoop.yarn.service.management\\\"\"\n+      + \",\\n\"\n       + \"    \\\"addressType\\\" : \\\"uri\\\",\\n\"\n       + \"    \\\"protocolType\\\" : \\\"REST\\\",\\n\"\n       + \"    \\\"addresses\\\" : [ {\\n\"\n       + \"      \\\"uri\\\" : \\\"http://192.168.1.5:1027/ws/v1/slider/mgmt\\\"\\n\"\n       + \"    } ]\\n\"\n       + \"  } ],\\n\"\n       + \"  \\\"internal\\\" : [ {\\n\"\n-      + \"    \\\"api\\\" : \\\"classpath:org.apache.slider.agents.secure\\\",\\n\"\n+      + \"    \\\"api\\\" : \\\"classpath:org.apache.hadoop.yarn.service.agents.secure\"\n+      + \"\\\",\\n\"\n       + \"    \\\"addressType\\\" : \\\"uri\\\",\\n\"\n       + \"    \\\"protocolType\\\" : \\\"REST\\\",\\n\"\n       + \"    \\\"addresses\\\" : [ {\\n\"\n       + \"      \\\"uri\\\" : \\\"https://192.168.1.5:47700/ws/v1/slider/agents\\\"\\n\"\n       + \"    } ]\\n\"\n       + \"  }, {\\n\"\n-      + \"    \\\"api\\\" : \\\"classpath:org.apache.slider.agents.oneway\\\",\\n\"\n+      + \"    \\\"api\\\" : \\\"classpath:org.apache.hadoop.yarn.service.agents.oneway\"\n+      + \"\\\",\\n\"\n       + \"    \\\"addressType\\\" : \\\"uri\\\",\\n\"\n       + \"    \\\"protocolType\\\" : \\\"REST\\\",\\n\"\n       + \"    \\\"addresses\\\" : [ {\\n\"",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-registry/src/test/java/org/apache/hadoop/registry/server/dns/TestRegistryDNS.java",
                "sha": "ac8d9392d5b7323a550a76f94a585410d2c40463",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DockerLinuxContainerRuntime.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DockerLinuxContainerRuntime.java?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DockerLinuxContainerRuntime.java",
                "patch": "@@ -812,13 +812,13 @@ public void reapContainer(ContainerRuntimeContext ctx)\n           .executePrivilegedOperation(null, privOp, null,\n               null, true, false);\n       LOG.info(\"Docker inspect output for \" + containerId + \": \" + output);\n+      // strip off quotes if any\n+      output = output.replaceAll(\"['\\\"]\", \"\");\n       int index = output.lastIndexOf(',');\n       if (index == -1) {\n         LOG.error(\"Incorrect format for ip and host\");\n         return null;\n       }\n-      // strip off quotes if any\n-      output = output.replaceAll(\"['\\\"]\", \"\");\n       String ips = output.substring(0, index).trim();\n       String host = output.substring(index+1).trim();\n       String[] ipAndHost = new String[2];",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/runtime/DockerLinuxContainerRuntime.java",
                "sha": "75a28e648b36fbf5d77bf28951553f0db15caf2c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/YarnCommands.md",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/YarnCommands.md?ref=ce74e64363abc64561263be70a923ab3e67f043f",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/YarnCommands.md",
                "patch": "@@ -92,6 +92,7 @@ Usage `yarn service [sub-command] [service-name] [options]`\n \n    Options:\n     --file,-f       The local path to the service definition file.\n+    --queue,-q      The queue to which the service is submitted.\n     --example,-e    The name of the example service such as:\n                     Sleeper      A simple service that launches a few non-docker sleep containers on YARN.\n    ```",
                "raw_url": "https://github.com/apache/hadoop/raw/ce74e64363abc64561263be70a923ab3e67f043f/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-site/src/site/markdown/YarnCommands.md",
                "sha": "51e27cc1cf07d8cf18eaf81a1e1f949f25a2eabc",
                "status": "modified"
            }
        ],
        "message": "YARN-7210. Some NPE fixes in Registry DNS. Contributed by Jian He",
        "parent": "https://github.com/apache/hadoop/commit/37c9b7327d188ccad7fd36b7466a65f68ad0c899",
        "repo": "hadoop",
        "unit_tests": [
            "ServiceClientTest.java",
            "TestServiceClient.java",
            "TestComponentInstance.java",
            "TestAbstractProviderService.java"
        ]
    },
    "hadoop_cf23f2c": {
        "bug_id": "hadoop_cf23f2c",
        "commit": "https://github.com/apache/hadoop/commit/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -945,6 +945,9 @@ Release 2.8.0 - UNRELEASED\n \n     YARN-4250. NPE in AppSchedulingInfo#isRequestLabelChanged. (Brahma Reddy Battula via rohithsharmaks)\n \n+    YARN-4000. RM crashes with NPE if leaf queue becomes parent queue during restart. \n+    (Varun Saxena via jianhe)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/CHANGES.txt",
                "sha": "824a4b8863aa6b043a8f3143aff67a52a6bbd9d5",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 8,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "patch": "@@ -141,7 +141,8 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppMoveEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptFailedEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType;\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeSignalContainerEvent;\n@@ -676,11 +677,8 @@ public FailApplicationAttemptResponse failApplicationAttempt(\n       }\n     }\n \n-    this.rmContext\n-        .getDispatcher()\n-        .getEventHandler()\n-        .handle(\n-            new RMAppAttemptFailedEvent(attemptId,\n+    this.rmContext.getDispatcher().getEventHandler().handle(\n+        new RMAppAttemptEvent(attemptId, RMAppAttemptEventType.FAIL,\n         \"Attempt failed by user.\"));\n \n     RMAuditLogger.logSuccess(callerUGI.getShortUserName(),\n@@ -735,8 +733,9 @@ public KillApplicationResponse forceKillApplication(\n       return KillApplicationResponse.newInstance(true);\n     }\n \n-    this.rmContext.getDispatcher().getEventHandler()\n-        .handle(new RMAppEvent(applicationId, RMAppEventType.KILL));\n+    this.rmContext.getDispatcher().getEventHandler().handle(\n+        new RMAppEvent(applicationId, RMAppEventType.KILL,\n+        \"Application killed by user.\"));\n \n     // For UnmanagedAMs, return true so they don't retry\n     return KillApplicationResponse.newInstance(",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "sha": "4a02580b51017579d7a412400d9d06094e27a8fc",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java",
                "patch": "@@ -50,7 +50,6 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRecoverEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRejectedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl;\n@@ -304,7 +303,8 @@ protected void submitApplication(\n       // scheduler about the existence of the application\n       assert application.getState() == RMAppState.NEW;\n       this.rmContext.getDispatcher().getEventHandler()\n-          .handle(new RMAppRejectedEvent(applicationId, e.getMessage()));\n+          .handle(new RMAppEvent(applicationId,\n+              RMAppEventType.APP_REJECTED, e.getMessage()));\n       throw RPCUtil.getRemoteException(e);\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMAppManager.java",
                "sha": "0b7083c45f9068db95280681b6287d193b9efb62",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/amlauncher/AMLauncher.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/amlauncher/AMLauncher.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/amlauncher/AMLauncher.java",
                "patch": "@@ -60,7 +60,6 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptLaunchFailedEvent;\n import org.apache.hadoop.yarn.util.ConverterUtils;\n \n import com.google.common.annotations.VisibleForTesting;\n@@ -257,8 +256,8 @@ public void run() {\n         String message = \"Error launching \" + application.getAppAttemptId()\n             + \". Got exception: \" + StringUtils.stringifyException(ie);\n         LOG.info(message);\n-        handler.handle(new RMAppAttemptLaunchFailedEvent(application\n-            .getAppAttemptId(), message));\n+        handler.handle(new RMAppAttemptEvent(application\n+            .getAppAttemptId(), RMAppAttemptEventType.LAUNCH_FAILED, message));\n       }\n       break;\n     case CLEANUP:",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/amlauncher/AMLauncher.java",
                "sha": "b927bb425726b47875ba9c94e03ba7feb5c252d9",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppEvent.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppEvent.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppEvent.java",
                "patch": "@@ -24,13 +24,24 @@\n public class RMAppEvent extends AbstractEvent<RMAppEventType>{\n \n   private final ApplicationId appId;\n+  private final String diagnosticMsg;\n \n   public RMAppEvent(ApplicationId appId, RMAppEventType type) {\n+    this(appId, type, \"\");\n+  }\n+\n+  public RMAppEvent(ApplicationId appId, RMAppEventType type,\n+      String diagnostic) {\n     super(type);\n     this.appId = appId;\n+    this.diagnosticMsg = diagnostic;\n   }\n \n   public ApplicationId getApplicationId() {\n     return this.appId;\n   }\n+\n+  public String getDiagnosticMsg() {\n+    return this.diagnosticMsg;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppEvent.java",
                "sha": "649640207211522532ab2e934f826ff07262b1c3",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppFailedAttemptEvent.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppFailedAttemptEvent.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppFailedAttemptEvent.java",
                "patch": "@@ -22,20 +22,14 @@\n \n public class RMAppFailedAttemptEvent extends RMAppEvent {\n \n-  private final String diagnostics;\n   private final boolean transferStateFromPreviousAttempt;\n \n   public RMAppFailedAttemptEvent(ApplicationId appId, RMAppEventType event, \n       String diagnostics, boolean transferStateFromPreviousAttempt) {\n-    super(appId, event);\n-    this.diagnostics = diagnostics;\n+    super(appId, event, diagnostics);\n     this.transferStateFromPreviousAttempt = transferStateFromPreviousAttempt;\n   }\n \n-  public String getDiagnostics() {\n-    return this.diagnostics;\n-  }\n-\n   public boolean getTransferStateFromPreviousAttempt() {\n     return transferStateFromPreviousAttempt;\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppFailedAttemptEvent.java",
                "sha": "835322a37e61022d9eeb6181ea77aa0f77f5c667",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppFinishedAttemptEvent.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppFinishedAttemptEvent.java?ref=a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc",
                "deletions": 35,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppFinishedAttemptEvent.java",
                "patch": "@@ -1,35 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hadoop.yarn.server.resourcemanager.rmapp;\n-\n-import org.apache.hadoop.yarn.api.records.ApplicationId;\n-\n-public class RMAppFinishedAttemptEvent extends RMAppEvent {\n-\n-  private final String diagnostics;\n-\n-  public RMAppFinishedAttemptEvent(ApplicationId appId, String diagnostics) {\n-    super(appId, RMAppEventType.ATTEMPT_FINISHED);\n-    this.diagnostics = diagnostics;\n-  }\n-\n-  public String getDiagnostics() {\n-    return this.diagnostics;\n-  }\n-}",
                "raw_url": "https://github.com/apache/hadoop/raw/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppFinishedAttemptEvent.java",
                "sha": "f1a6340ba8ef06784949b9a573937b97da25acec",
                "status": "removed"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 23,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java",
                "patch": "@@ -1046,7 +1046,7 @@ private String getAppAttemptFailedDiagnostics(RMAppEvent event) {\n     if (this.submissionContext.getUnmanagedAM()) {\n       // RM does not manage the AM. Do not retry\n       msg = \"Unmanaged application \" + this.getApplicationId()\n-              + \" failed due to \" + failedEvent.getDiagnostics()\n+              + \" failed due to \" + failedEvent.getDiagnosticMsg()\n               + \". Failing the application.\";\n     } else if (this.isNumAttemptsBeyondThreshold) {\n       int globalLimit = conf.getInt(YarnConfiguration.RM_AM_MAX_ATTEMPTS,\n@@ -1061,7 +1061,7 @@ private String getAppAttemptFailedDiagnostics(RMAppEvent event) {\n           (globalLimit == maxAppAttempts) ? \"\"\n               : (\" (global limit =\" + globalLimit\n                  + \"; local limit is =\" + maxAppAttempts + \")\"),\n-          failedEvent.getDiagnostics());\n+          failedEvent.getDiagnosticMsg());\n     }\n     return msg;\n   }\n@@ -1102,21 +1102,14 @@ private void rememberTargetTransitionsAndStoreState(RMAppEvent event,\n     String diags = null;\n     switch (event.getType()) {\n     case APP_REJECTED:\n-      RMAppRejectedEvent rejectedEvent = (RMAppRejectedEvent) event;\n-      diags = rejectedEvent.getMessage();\n-      break;\n     case ATTEMPT_FINISHED:\n-      RMAppFinishedAttemptEvent finishedEvent =\n-          (RMAppFinishedAttemptEvent) event;\n-      diags = finishedEvent.getDiagnostics();\n+    case ATTEMPT_KILLED:\n+      diags = event.getDiagnosticMsg();\n       break;\n     case ATTEMPT_FAILED:\n       RMAppFailedAttemptEvent failedEvent = (RMAppFailedAttemptEvent) event;\n       diags = getAppAttemptFailedDiagnostics(failedEvent);\n       break;\n-    case ATTEMPT_KILLED:\n-      diags = getAppKilledDiagnostics();\n-      break;\n     default:\n       break;\n     }\n@@ -1164,9 +1157,7 @@ public AppFinishedTransition() {\n     }\n \n     public void transition(RMAppImpl app, RMAppEvent event) {\n-      RMAppFinishedAttemptEvent finishedEvent =\n-          (RMAppFinishedAttemptEvent)event;\n-      app.diagnostics.append(finishedEvent.getDiagnostics());\n+      app.diagnostics.append(event.getDiagnosticMsg());\n       super.transition(app, event);\n     };\n   }\n@@ -1212,21 +1203,21 @@ public AppKilledTransition() {\n \n     @Override\n     public void transition(RMAppImpl app, RMAppEvent event) {\n-      app.diagnostics.append(getAppKilledDiagnostics());\n+      app.diagnostics.append(event.getDiagnosticMsg());\n       super.transition(app, event);\n     };\n   }\n \n-  private static String getAppKilledDiagnostics() {\n-    return \"Application killed by user.\";\n-  }\n-\n   private static class KillAttemptTransition extends RMAppTransition {\n     @Override\n     public void transition(RMAppImpl app, RMAppEvent event) {\n       app.stateBeforeKilling = app.getState();\n-      app.handler.handle(new RMAppAttemptEvent(app.currentAttempt\n-        .getAppAttemptId(), RMAppAttemptEventType.KILL));\n+      // Forward app kill diagnostics in the event to kill app attempt.\n+      // These diagnostics will be returned back in ATTEMPT_KILLED event sent by\n+      // RMAppAttemptImpl.\n+      app.handler.handle(\n+          new RMAppAttemptEvent(app.currentAttempt.getAppAttemptId(),\n+              RMAppAttemptEventType.KILL, event.getDiagnosticMsg()));\n     }\n   }\n \n@@ -1237,8 +1228,7 @@ public AppRejectedTransition() {\n     }\n \n     public void transition(RMAppImpl app, RMAppEvent event) {\n-      RMAppRejectedEvent rejectedEvent = (RMAppRejectedEvent)event;\n-      app.diagnostics.append(rejectedEvent.getMessage());\n+      app.diagnostics.append(event.getDiagnosticMsg());\n       super.transition(app, event);\n     };\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppImpl.java",
                "sha": "43a3a51a467940f91fc41b66b8c164d44fd8f52f",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppRejectedEvent.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppRejectedEvent.java?ref=a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc",
                "deletions": 35,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppRejectedEvent.java",
                "patch": "@@ -1,35 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hadoop.yarn.server.resourcemanager.rmapp;\n-\n-import org.apache.hadoop.yarn.api.records.ApplicationId;\n-\n-public class RMAppRejectedEvent extends RMAppEvent {\n-\n-  private final String message;\n-\n-  public RMAppRejectedEvent(ApplicationId appId, String message) {\n-    super(appId, RMAppEventType.APP_REJECTED);\n-    this.message = message;\n-  }\n-\n-  public String getMessage() {\n-    return this.message;\n-  }\n-}",
                "raw_url": "https://github.com/apache/hadoop/raw/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/RMAppRejectedEvent.java",
                "sha": "baaef238ca5070d887be04844e7a0703cbec3d50",
                "status": "removed"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptEvent.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptEvent.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptEvent.java",
                "patch": "@@ -24,14 +24,25 @@\n public class RMAppAttemptEvent extends AbstractEvent<RMAppAttemptEventType> {\n \n   private final ApplicationAttemptId appAttemptId;\n+  private final String diagnosticMsg;\n \n   public RMAppAttemptEvent(ApplicationAttemptId appAttemptId,\n       RMAppAttemptEventType type) {\n+    this(appAttemptId, type, \"\");\n+  }\n+\n+  public RMAppAttemptEvent(ApplicationAttemptId appAttemptId,\n+      RMAppAttemptEventType type, String diagnostics) {\n     super(type);\n     this.appAttemptId = appAttemptId;\n+    this.diagnosticMsg = diagnostics;\n   }\n \n   public ApplicationAttemptId getApplicationAttemptId() {\n     return this.appAttemptId;\n   }\n+\n+  public String getDiagnosticMsg() {\n+    return diagnosticMsg;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptEvent.java",
                "sha": "6df6b19f97843691a4ad5194c36ef24572d9953a",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 22,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "patch": "@@ -82,12 +82,8 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppFailedAttemptEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppFinishedAttemptEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptContainerAllocatedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptContainerFinishedEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptFailedEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptLaunchFailedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptRegistrationEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptStatusupdateEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptUnregistrationEvent;\n@@ -1085,8 +1081,9 @@ public void run() {\n           LOG.warn(\"Interrupted while waiting to resend the\"\n               + \" ContainerAllocated Event.\");\n         }\n-        appAttempt.eventHandler.handle(new RMAppAttemptContainerAllocatedEvent(\n-          appAttempt.applicationAttemptId));\n+        appAttempt.eventHandler.handle(\n+            new RMAppAttemptEvent(appAttempt.applicationAttemptId,\n+                RMAppAttemptEventType.CONTAINER_ALLOCATED));\n       }\n     }.start();\n   }\n@@ -1195,17 +1192,15 @@ private void rememberTargetTransitionsAndStoreState(RMAppAttemptEvent event,\n     int exitStatus = ContainerExitStatus.INVALID;\n     switch (event.getType()) {\n     case LAUNCH_FAILED:\n-      RMAppAttemptLaunchFailedEvent launchFaileEvent =\n-          (RMAppAttemptLaunchFailedEvent) event;\n-      diags = launchFaileEvent.getMessage();\n+      diags = event.getDiagnosticMsg();\n       break;\n     case REGISTERED:\n       diags = getUnexpectedAMRegisteredDiagnostics();\n       break;\n     case UNREGISTERED:\n       RMAppAttemptUnregistrationEvent unregisterEvent =\n           (RMAppAttemptUnregistrationEvent) event;\n-      diags = unregisterEvent.getDiagnostics();\n+      diags = unregisterEvent.getDiagnosticMsg();\n       // reset finalTrackingUrl to url sent by am\n       finalTrackingUrl = sanitizeTrackingUrl(unregisterEvent.getFinalTrackingUrl());\n       finalStatus = unregisterEvent.getFinalApplicationStatus();\n@@ -1219,9 +1214,7 @@ private void rememberTargetTransitionsAndStoreState(RMAppAttemptEvent event,\n     case KILL:\n       break;\n     case FAIL:\n-      RMAppAttemptFailedEvent failEvent =\n-          (RMAppAttemptFailedEvent) event;\n-      diags = failEvent.getDiagnostics();\n+      diags = event.getDiagnosticMsg();\n       break;\n     case EXPIRE:\n       diags = getAMExpiredDiagnostics(event);\n@@ -1309,17 +1302,19 @@ public void transition(RMAppAttemptImpl appAttempt,\n       switch (finalAttemptState) {\n         case FINISHED:\n         {\n-          appEvent = new RMAppFinishedAttemptEvent(applicationId,\n+          appEvent =\n+              new RMAppEvent(applicationId, RMAppEventType.ATTEMPT_FINISHED,\n               appAttempt.getDiagnostics());\n         }\n         break;\n         case KILLED:\n         {\n           appAttempt.invalidateAMHostAndPort();\n+          // Forward diagnostics received in attempt kill event.\n           appEvent =\n               new RMAppFailedAttemptEvent(applicationId,\n                   RMAppEventType.ATTEMPT_KILLED,\n-                  \"Application killed by user.\", false);\n+                  event.getDiagnosticMsg(), false);\n         }\n         break;\n         case FAILED:\n@@ -1377,9 +1372,8 @@ public AttemptFailedTransition() {\n \n     @Override\n     public void transition(RMAppAttemptImpl appAttempt, RMAppAttemptEvent event) {\n-      RMAppAttemptFailedEvent failedEvent = (RMAppAttemptFailedEvent) event;\n-      if (failedEvent.getDiagnostics() != null) {\n-        appAttempt.diagnostics.append(failedEvent.getDiagnostics());\n+      if (event.getDiagnosticMsg() != null) {\n+        appAttempt.diagnostics.append(event.getDiagnosticMsg());\n       }\n       super.transition(appAttempt, event);\n     }\n@@ -1451,9 +1445,7 @@ public void transition(RMAppAttemptImpl appAttempt,\n         RMAppAttemptEvent event) {\n \n       // Use diagnostic from launcher\n-      RMAppAttemptLaunchFailedEvent launchFaileEvent\n-        = (RMAppAttemptLaunchFailedEvent) event;\n-      appAttempt.diagnostics.append(launchFaileEvent.getMessage());\n+      appAttempt.diagnostics.append(event.getDiagnosticMsg());\n \n       // Tell the app, scheduler\n       super.transition(appAttempt, event);\n@@ -1708,7 +1700,7 @@ private void updateInfoOnAMUnregister(RMAppAttemptEvent event) {\n     progress = 1.0f;\n     RMAppAttemptUnregistrationEvent unregisterEvent =\n         (RMAppAttemptUnregistrationEvent) event;\n-    diagnostics.append(unregisterEvent.getDiagnostics());\n+    diagnostics.append(unregisterEvent.getDiagnosticMsg());\n     originalTrackingUrl = sanitizeTrackingUrl(unregisterEvent.getFinalTrackingUrl());\n     finalStatus = unregisterEvent.getFinalApplicationStatus();\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "sha": "36fb9fc73879fce8d89c64cda5c54397ca66a97e",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptContainerAllocatedEvent.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptContainerAllocatedEvent.java?ref=a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc",
                "deletions": 31,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptContainerAllocatedEvent.java",
                "patch": "@@ -1,31 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event;\n-\n-import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n-import org.apache.hadoop.yarn.api.records.Container;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType;\n-\n-public class RMAppAttemptContainerAllocatedEvent extends RMAppAttemptEvent {\n-\n-  public RMAppAttemptContainerAllocatedEvent(ApplicationAttemptId appAttemptId) {\n-    super(appAttemptId, RMAppAttemptEventType.CONTAINER_ALLOCATED);\n-  }\n-}",
                "raw_url": "https://github.com/apache/hadoop/raw/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptContainerAllocatedEvent.java",
                "sha": "681f38c2c2dbe00906e1123bc5bad855a2dcce69",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptFailedEvent.java",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptFailedEvent.java?ref=a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc",
                "deletions": 39,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptFailedEvent.java",
                "patch": "@@ -1,39 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event;\n-\n-import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType;\n-\n-public class RMAppAttemptFailedEvent extends RMAppAttemptEvent {\n-\n-  private final String diagnostics;\n-\n-  public RMAppAttemptFailedEvent(ApplicationAttemptId appAttemptId,\n-      String diagnostics) {\n-    super(appAttemptId, RMAppAttemptEventType.FAIL);\n-    this.diagnostics = diagnostics;\n-  }\n-\n-  public String getDiagnostics() {\n-    return this.diagnostics;\n-  }\n-\n-}",
                "raw_url": "https://github.com/apache/hadoop/raw/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptFailedEvent.java",
                "sha": "c698e7d872417a3e199623f6110a7c9595385c4d",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop/blob/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptLaunchFailedEvent.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptLaunchFailedEvent.java?ref=a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc",
                "deletions": 38,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptLaunchFailedEvent.java",
                "patch": "@@ -1,38 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-\n-package org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event;\n-\n-import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType;\n-\n-public class RMAppAttemptLaunchFailedEvent extends RMAppAttemptEvent {\n-\n-  private final String message;\n-\n-  public RMAppAttemptLaunchFailedEvent(ApplicationAttemptId appAttemptId,\n-      String message) {\n-    super(appAttemptId, RMAppAttemptEventType.LAUNCH_FAILED);\n-    this.message = message;\n-  }\n-\n-  public String getMessage() {\n-    return this.message;\n-  }\n-}",
                "raw_url": "https://github.com/apache/hadoop/raw/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptLaunchFailedEvent.java",
                "sha": "d0b49b2d160adcf6fd3fb90a42c2123441a8b2d1",
                "status": "removed"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptUnregistrationEvent.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptUnregistrationEvent.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 8,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptUnregistrationEvent.java",
                "patch": "@@ -27,14 +27,13 @@\n \n   private final String finalTrackingUrl;\n   private final FinalApplicationStatus finalStatus;\n-  private final String diagnostics;\n \n   public RMAppAttemptUnregistrationEvent(ApplicationAttemptId appAttemptId,\n-      String trackingUrl, FinalApplicationStatus finalStatus, String diagnostics) {\n-    super(appAttemptId, RMAppAttemptEventType.UNREGISTERED);\n+      String trackingUrl, FinalApplicationStatus finalStatus,\n+      String diagnostics) {\n+    super(appAttemptId, RMAppAttemptEventType.UNREGISTERED, diagnostics);\n     this.finalTrackingUrl = trackingUrl;\n     this.finalStatus = finalStatus;\n-    this.diagnostics = diagnostics;\n   }\n \n   public String getFinalTrackingUrl() {\n@@ -45,8 +44,4 @@ public FinalApplicationStatus getFinalApplicationStatus() {\n     return this.finalStatus;\n   }\n \n-  public String getDiagnostics() {\n-    return this.diagnostics;\n-  }\n-\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/event/RMAppAttemptUnregistrationEvent.java",
                "sha": "1ce51507ee515adbe677289fc912c87c6875900e",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "patch": "@@ -45,7 +45,8 @@\n import org.apache.hadoop.yarn.server.resourcemanager.nodelabels.RMNodeLabelsManager;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRunningOnNodeEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptContainerAllocatedEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptContainerFinishedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeCleanContainerEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.ContainerRescheduledEvent;\n@@ -511,8 +512,8 @@ public void transition(RMContainerImpl container, RMContainerEvent event) {\n \n     @Override\n     public void transition(RMContainerImpl container, RMContainerEvent event) {\n-      container.eventHandler.handle(new RMAppAttemptContainerAllocatedEvent(\n-          container.appAttemptId));\n+      container.eventHandler.handle(new RMAppAttemptEvent(\n+          container.appAttemptId, RMAppAttemptEventType.CONTAINER_ALLOCATED));\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmcontainer/RMContainerImpl.java",
                "sha": "96c4f2772d6d504c28e051f4bc34e0f61f787281",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "patch": "@@ -651,7 +651,9 @@ public synchronized void killAllAppsInQueue(String queueName)\n       this.rmContext\n           .getDispatcher()\n           .getEventHandler()\n-          .handle(new RMAppEvent(app.getApplicationId(), RMAppEventType.KILL));\n+          .handle(new RMAppEvent(app.getApplicationId(), RMAppEventType.KILL,\n+          \"Application killed due to expiry of reservation queue \" +\n+          queueName + \".\"));\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "sha": "abd72bfa8baca48883b2055e7416bcce9298d356",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueInvalidException.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueInvalidException.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueInvalidException.java",
                "patch": "@@ -22,11 +22,11 @@\n import org.apache.hadoop.yarn.exceptions.YarnRuntimeException;\n \n @Private\n-public class QueueNotFoundException extends YarnRuntimeException {\n+public class QueueInvalidException extends YarnRuntimeException {\n \n   private static final long serialVersionUID = 187239430L;\n \n-  public QueueNotFoundException(String message) {\n+  public QueueInvalidException(String message) {\n     super(message);\n   }\n }",
                "previous_filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueNotFoundException.java",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/QueueInvalidException.java",
                "sha": "7c6be4f0ab5ff73f356895db6b53f4f10364ab12",
                "status": "renamed"
            },
            {
                "additions": 88,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "changes": 124,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 36,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "patch": "@@ -80,7 +80,6 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRejectedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType;\n@@ -97,8 +96,8 @@\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.NodeType;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.PreemptableResourceScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.Queue;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueInvalidException;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics;\n-import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueNotFoundException;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceLimits;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedContainerChangeRequest;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplication;\n@@ -666,47 +665,97 @@ public CSQueue getQueue(String queueName) {\n     return queues.get(queueName);\n   }\n \n-  private synchronized void addApplication(ApplicationId applicationId,\n-      String queueName, String user, boolean isAppRecovering, Priority priority) {\n-    // sanity checks.\n+  private synchronized void addApplicationOnRecovery(\n+      ApplicationId applicationId, String queueName, String user,\n+      Priority priority) {\n     CSQueue queue = getQueue(queueName);\n     if (queue == null) {\n       //During a restart, this indicates a queue was removed, which is\n       //not presently supported\n-      if (isAppRecovering) {\n+      if (!YarnConfiguration.shouldRMFailFast(getConfig())) {\n+        this.rmContext.getDispatcher().getEventHandler().handle(\n+            new RMAppEvent(applicationId, RMAppEventType.KILL,\n+            \"Application killed on recovery as it was submitted to queue \" +\n+            queueName + \" which no longer exists after restart.\"));\n+        return;\n+      } else {\n         String queueErrorMsg = \"Queue named \" + queueName\n-           + \" missing during application recovery.\"\n-           + \" Queue removal during recovery is not presently supported by the\"\n-           + \" capacity scheduler, please restart with all queues configured\"\n-           + \" which were present before shutdown/restart.\";\n+            + \" missing during application recovery.\"\n+            + \" Queue removal during recovery is not presently supported by the\"\n+            + \" capacity scheduler, please restart with all queues configured\"\n+            + \" which were present before shutdown/restart.\";\n         LOG.fatal(queueErrorMsg);\n-        throw new QueueNotFoundException(queueErrorMsg);\n+        throw new QueueInvalidException(queueErrorMsg);\n       }\n-      String message = \"Application \" + applicationId + \n+    }\n+    if (!(queue instanceof LeafQueue)) {\n+      // During RM restart, this means leaf queue was converted to a parent\n+      // queue, which is not supported for running apps.\n+      if (!YarnConfiguration.shouldRMFailFast(getConfig())) {\n+        this.rmContext.getDispatcher().getEventHandler().handle(\n+            new RMAppEvent(applicationId, RMAppEventType.KILL,\n+            \"Application killed on recovery as it was submitted to queue \" +\n+            queueName + \" which is no longer a leaf queue after restart.\"));\n+        return;\n+      } else {\n+        String queueErrorMsg = \"Queue named \" + queueName\n+            + \" is no longer a leaf queue during application recovery.\"\n+            + \" Changing a leaf queue to a parent queue during recovery is\"\n+            + \" not presently supported by the capacity scheduler. Please\"\n+            + \" restart with leaf queues before shutdown/restart continuing\"\n+            + \" as leaf queues.\";\n+        LOG.fatal(queueErrorMsg);\n+        throw new QueueInvalidException(queueErrorMsg);\n+      }\n+    }\n+    // Submit to the queue\n+    try {\n+      queue.submitApplication(applicationId, user, queueName);\n+    } catch (AccessControlException ace) {\n+      // Ignore the exception for recovered app as the app was previously\n+      // accepted.\n+    }\n+    queue.getMetrics().submitApp(user);\n+    SchedulerApplication<FiCaSchedulerApp> application =\n+        new SchedulerApplication<FiCaSchedulerApp>(queue, user, priority);\n+    applications.put(applicationId, application);\n+    LOG.info(\"Accepted application \" + applicationId + \" from user: \" + user\n+        + \", in queue: \" + queueName);\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(applicationId + \" is recovering. Skip notifying APP_ACCEPTED\");\n+    }\n+  }\n+\n+  private synchronized void addApplication(ApplicationId applicationId,\n+      String queueName, String user, Priority priority) {\n+    // Sanity checks.\n+    CSQueue queue = getQueue(queueName);\n+    if (queue == null) {\n+      String message = \"Application \" + applicationId +\n       \" submitted by user \" + user + \" to unknown queue: \" + queueName;\n       this.rmContext.getDispatcher().getEventHandler()\n-          .handle(new RMAppRejectedEvent(applicationId, message));\n+          .handle(new RMAppEvent(applicationId,\n+              RMAppEventType.APP_REJECTED, message));\n       return;\n     }\n     if (!(queue instanceof LeafQueue)) {\n       String message = \"Application \" + applicationId + \n           \" submitted by user \" + user + \" to non-leaf queue: \" + queueName;\n       this.rmContext.getDispatcher().getEventHandler()\n-          .handle(new RMAppRejectedEvent(applicationId, message));\n+          .handle(new RMAppEvent(applicationId,\n+              RMAppEventType.APP_REJECTED, message));\n       return;\n     }\n     // Submit to the queue\n     try {\n       queue.submitApplication(applicationId, user, queueName);\n     } catch (AccessControlException ace) {\n-      // Ignore the exception for recovered app as the app was previously accepted\n-      if (!isAppRecovering) {\n-        LOG.info(\"Failed to submit application \" + applicationId + \" to queue \"\n-            + queueName + \" from user \" + user, ace);\n-        this.rmContext.getDispatcher().getEventHandler()\n-            .handle(new RMAppRejectedEvent(applicationId, ace.toString()));\n-        return;\n-      }\n+      LOG.info(\"Failed to submit application \" + applicationId + \" to queue \"\n+          + queueName + \" from user \" + user, ace);\n+      this.rmContext.getDispatcher().getEventHandler()\n+          .handle(new RMAppEvent(applicationId,\n+              RMAppEventType.APP_REJECTED, ace.toString()));\n+      return;\n     }\n     // update the metrics\n     queue.getMetrics().submitApp(user);\n@@ -715,14 +764,8 @@ private synchronized void addApplication(ApplicationId applicationId,\n     applications.put(applicationId, application);\n     LOG.info(\"Accepted application \" + applicationId + \" from user: \" + user\n         + \", in queue: \" + queueName);\n-    if (isAppRecovering) {\n-      if (LOG.isDebugEnabled()) {\n-        LOG.debug(applicationId + \" is recovering. Skip notifying APP_ACCEPTED\");\n-      }\n-    } else {\n-      rmContext.getDispatcher().getEventHandler()\n+    rmContext.getDispatcher().getEventHandler()\n         .handle(new RMAppEvent(applicationId, RMAppEventType.APP_ACCEPTED));\n-    }\n   }\n \n   private synchronized void addApplicationAttempt(\n@@ -731,6 +774,11 @@ private synchronized void addApplicationAttempt(\n       boolean isAttemptRecovering) {\n     SchedulerApplication<FiCaSchedulerApp> application =\n         applications.get(applicationAttemptId.getApplicationId());\n+    if (application == null) {\n+      LOG.warn(\"Application \" + applicationAttemptId.getApplicationId() +\n+          \" cannot be found in scheduler.\");\n+      return;\n+    }\n     CSQueue queue = (CSQueue) application.getQueue();\n \n     FiCaSchedulerApp attempt = new FiCaSchedulerApp(applicationAttemptId,\n@@ -1277,11 +1325,13 @@ public void handle(SchedulerEvent event) {\n               appAddedEvent.getApplicationId(),\n               appAddedEvent.getReservationID());\n       if (queueName != null) {\n-        addApplication(appAddedEvent.getApplicationId(),\n-            queueName,\n-            appAddedEvent.getUser(),\n-            appAddedEvent.getIsAppRecovering(),\n-            appAddedEvent.getApplicatonPriority());\n+        if (!appAddedEvent.getIsAppRecovering()) {\n+          addApplication(appAddedEvent.getApplicationId(), queueName,\n+              appAddedEvent.getUser(), appAddedEvent.getApplicatonPriority());\n+        } else {\n+          addApplicationOnRecovery(appAddedEvent.getApplicationId(), queueName,\n+              appAddedEvent.getUser(), appAddedEvent.getApplicatonPriority());\n+        }\n       }\n     }\n     break;\n@@ -1631,7 +1681,8 @@ private synchronized String resolveReservationQueueName(String queueName,\n                 + \" submitted to a reservation which is not yet currently active: \"\n                 + resQName;\n         this.rmContext.getDispatcher().getEventHandler()\n-            .handle(new RMAppRejectedEvent(applicationId, message));\n+            .handle(new RMAppEvent(applicationId,\n+                RMAppEventType.APP_REJECTED, message));\n         return null;\n       }\n       if (!queue.getParent().getQueueName().equals(queueName)) {\n@@ -1640,7 +1691,8 @@ private synchronized String resolveReservationQueueName(String queueName,\n                 + resQName + \" which does not belong to the specified queue: \"\n                 + queueName;\n         this.rmContext.getDispatcher().getEventHandler()\n-            .handle(new RMAppRejectedEvent(applicationId, message));\n+            .handle(new RMAppEvent(applicationId,\n+                RMAppEventType.APP_REJECTED, message));\n         return null;\n       }\n       // use the reservation queue to run the app",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "sha": "6e356b5f16ec93f7c095ca533bedfc0656abd337",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "patch": "@@ -61,7 +61,6 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRejectedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType;\n@@ -614,7 +613,8 @@ protected synchronized void addApplication(ApplicationId applicationId,\n               \" submitted by user \" + user + \" with an empty queue name.\";\n       LOG.info(message);\n       rmContext.getDispatcher().getEventHandler()\n-          .handle(new RMAppRejectedEvent(applicationId, message));\n+          .handle(new RMAppEvent(applicationId,\n+              RMAppEventType.APP_REJECTED, message));\n       return;\n     }\n \n@@ -625,7 +625,8 @@ protected synchronized void addApplication(ApplicationId applicationId,\n           + \"The queue name cannot start/end with period.\";\n       LOG.info(message);\n       rmContext.getDispatcher().getEventHandler()\n-          .handle(new RMAppRejectedEvent(applicationId, message));\n+          .handle(new RMAppEvent(applicationId,\n+              RMAppEventType.APP_REJECTED, message));\n       return;\n     }\n \n@@ -644,7 +645,8 @@ protected synchronized void addApplication(ApplicationId applicationId,\n               \" cannot submit applications to queue \" + queue.getName();\n       LOG.info(msg);\n       rmContext.getDispatcher().getEventHandler()\n-          .handle(new RMAppRejectedEvent(applicationId, msg));\n+          .handle(new RMAppEvent(applicationId,\n+              RMAppEventType.APP_REJECTED, msg));\n       return;\n     }\n   \n@@ -742,7 +744,8 @@ FSLeafQueue assignToQueue(RMApp rmApp, String queueName, String user) {\n     if (appRejectMsg != null && rmApp != null) {\n       LOG.error(appRejectMsg);\n       rmContext.getDispatcher().getEventHandler().handle(\n-          new RMAppRejectedEvent(rmApp.getApplicationId(), appRejectMsg));\n+          new RMAppEvent(rmApp.getApplicationId(),\n+              RMAppEventType.APP_REJECTED, appRejectMsg));\n       return null;\n     }\n \n@@ -1302,7 +1305,8 @@ private synchronized String resolveReservationQueueName(String queueName,\n                 + \" submitted to a reservation which is not yet currently active: \"\n                 + resQName;\n         this.rmContext.getDispatcher().getEventHandler()\n-            .handle(new RMAppRejectedEvent(applicationId, message));\n+            .handle(new RMAppEvent(applicationId,\n+                RMAppEventType.APP_REJECTED, message));\n         return null;\n       }\n       if (!queue.getParent().getQueueName().equals(queueName)) {\n@@ -1311,7 +1315,8 @@ private synchronized String resolveReservationQueueName(String queueName,\n                 + resQName + \" which does not belong to the specified queue: \"\n                 + queueName;\n         this.rmContext.getDispatcher().getEventHandler()\n-            .handle(new RMAppRejectedEvent(applicationId, message));\n+            .handle(new RMAppEvent(applicationId,\n+                RMAppEventType.APP_REJECTED, message));\n         return null;\n       }\n       // use the reservation queue to run the app",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "sha": "33d01fc1d9ba4d4ae7810013cdd57808786536cf",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java",
                "patch": "@@ -66,7 +66,6 @@\n import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRejectedEvent;\n \n import com.google.common.annotations.VisibleForTesting;\n import com.google.common.util.concurrent.ThreadFactoryBuilder;\n@@ -872,7 +871,8 @@ private void handleDTRenewerAppSubmitEvent(\n         // RMApp is in NEW state and thus we havne't yet informed the\n         // Scheduler about the existence of the application\n         rmContext.getDispatcher().getEventHandler().handle(\n-            new RMAppRejectedEvent(event.getApplicationId(), t.getMessage()));\n+            new RMAppEvent(event.getApplicationId(),\n+                RMAppEventType.APP_REJECTED, t.getMessage()));\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/security/DelegationTokenRenewer.java",
                "sha": "426e460ec6b68e0851a519dc3b0a69c5ceac1b5d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java",
                "patch": "@@ -76,7 +76,6 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptEventType;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptLaunchFailedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n@@ -622,7 +621,8 @@ public void sendAMLaunchFailed(ApplicationAttemptId appAttemptId)\n     MockAM am = new MockAM(getRMContext(), masterService, appAttemptId);\n     am.waitForState(RMAppAttemptState.ALLOCATED);\n     getRMContext().getDispatcher().getEventHandler()\n-        .handle(new RMAppAttemptLaunchFailedEvent(appAttemptId, \"Failed\"));\n+        .handle(new RMAppAttemptEvent(appAttemptId,\n+            RMAppAttemptEventType.LAUNCH_FAILED, \"Failed\"));\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/MockRM.java",
                "sha": "a0619cf06006d63f0801e1b798d6e8d0c151e0d6",
                "status": "modified"
            },
            {
                "additions": 137,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java",
                "changes": 151,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 14,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java",
                "patch": "@@ -22,6 +22,8 @@\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n \n import java.io.IOException;\n import java.net.UnknownHostException;\n@@ -39,25 +41,31 @@\n import org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse;\n import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.api.records.ApplicationReport;\n+import org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext;\n import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.ContainerState;\n import org.apache.hadoop.yarn.api.records.ContainerStatus;\n+import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.ResourceRequest;\n+import org.apache.hadoop.yarn.api.records.YarnApplicationState;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.server.api.protocolrecords.NMContainerStatus;\n import org.apache.hadoop.yarn.server.resourcemanager.TestRMRestart.TestSecurityMockRM;\n import org.apache.hadoop.yarn.server.resourcemanager.recovery.MemoryRMStateStore;\n+import org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.RMState;\n+import org.apache.hadoop.yarn.server.resourcemanager.recovery.records.ApplicationStateData;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttempt;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.AbstractYarnScheduler;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueInvalidException;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics;\n-import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueNotFoundException;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplication;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNode;\n@@ -86,6 +94,7 @@\n import org.junit.Test;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n+import org.mortbay.log.Log;\n \n import com.google.common.base.Supplier;\n \n@@ -361,6 +370,8 @@ private void checkFSQueue(ResourceManager rm,\n   private static final String R = \"Default\";\n   private static final String A = \"QueueA\";\n   private static final String B = \"QueueB\";\n+  private static final String B1 = \"QueueB1\";\n+  private static final String B2 = \"QueueB2\";\n   //don't ever create the below queue ;-)\n   private static final String QUEUE_DOESNT_EXIST = \"NoSuchQueue\";\n   private static final String USER_1 = \"user1\";\n@@ -391,6 +402,24 @@ private void setupQueueConfigurationOnlyA(\n       .MAXIMUM_APPLICATION_MASTERS_RESOURCE_PERCENT, 1.0f);\n   }\n \n+  private void setupQueueConfigurationChildOfB(CapacitySchedulerConfiguration conf) {\n+    conf.setQueues(CapacitySchedulerConfiguration.ROOT, new String[] { R });\n+    final String Q_R = CapacitySchedulerConfiguration.ROOT + \".\" + R;\n+    conf.setCapacity(Q_R, 100);\n+    final String Q_A = Q_R + \".\" + A;\n+    final String Q_B = Q_R + \".\" + B;\n+    final String Q_B1 = Q_B + \".\" + B1;\n+    final String Q_B2 = Q_B + \".\" + B2;\n+    conf.setQueues(Q_R, new String[] {A, B});\n+    conf.setCapacity(Q_A, 50);\n+    conf.setCapacity(Q_B, 50);\n+    conf.setQueues(Q_B, new String[] {B1, B2});\n+    conf.setCapacity(Q_B1, 50);\n+    conf.setCapacity(Q_B2, 50);\n+    conf.setDouble(CapacitySchedulerConfiguration\n+        .MAXIMUM_APPLICATION_MASTERS_RESOURCE_PERCENT, 0.5f);\n+  }\n+\n   // Test CS recovery with multi-level queues and multi-users:\n   // 1. setup 2 NMs each with 8GB memory;\n   // 2. setup 2 level queues: Default -> (QueueA, QueueB)\n@@ -513,18 +542,106 @@ public void testCapacitySchedulerRecovery() throws Exception {\n         totalAvailableResource.getVirtualCores(), totalUsedResource.getMemory(),\n         totalUsedResource.getVirtualCores());\n   }\n-  \n-  //Test that we receive a meaningful exit-causing exception if a queue\n-  //is removed during recovery\n+\n+  private void verifyAppRecoveryWithWrongQueueConfig(\n+      CapacitySchedulerConfiguration csConf, RMApp app, String diagnostics,\n+      MemoryRMStateStore memStore, RMState state) throws Exception {\n+    // Restart RM with fail-fast as false. App should be killed.\n+    csConf.setBoolean(YarnConfiguration.RM_FAIL_FAST, false);\n+    rm2 = new MockRM(csConf, memStore);\n+    rm2.start();\n+    // Wait for app to be killed.\n+    rm2.waitForState(app.getApplicationId(), RMAppState.KILLED);\n+    ApplicationReport report = rm2.getApplicationReport(app.getApplicationId());\n+    assertEquals(report.getFinalApplicationStatus(),\n+        FinalApplicationStatus.KILLED);\n+    assertEquals(report.getYarnApplicationState(), YarnApplicationState.KILLED);\n+    assertEquals(report.getDiagnostics(), diagnostics);\n+\n+    // Remove updated app info(app being KILLED) from state store and reinstate\n+    // state store to previous state i.e. which indicates app is RUNNING.\n+    // This is to simulate app recovery with fail fast config as true.\n+    for(Map.Entry<ApplicationId, ApplicationStateData> entry :\n+        state.getApplicationState().entrySet()) {\n+      ApplicationStateData appState = mock(ApplicationStateData.class);\n+      ApplicationSubmissionContext ctxt =\n+          mock(ApplicationSubmissionContext.class);\n+      when(appState.getApplicationSubmissionContext()).thenReturn(ctxt);\n+      when(ctxt.getApplicationId()).thenReturn(entry.getKey());\n+      memStore.removeApplicationStateInternal(appState);\n+      memStore.storeApplicationStateInternal(\n+          entry.getKey(), entry.getValue());\n+    }\n+\n+    // Now restart RM with fail-fast as true. QueueException should be thrown.\n+    csConf.setBoolean(YarnConfiguration.RM_FAIL_FAST, true);\n+    MockRM rm = new MockRM(csConf, memStore);\n+    try {\n+      rm.start();\n+      Assert.fail(\"QueueException must have been thrown\");\n+    } catch (QueueInvalidException e) {\n+    } finally {\n+      rm.close();\n+    }\n+  }\n+\n+  //Test behavior of an app if queue is changed from leaf to parent during\n+  //recovery. Test case does following:\n+  //1. Add an app to QueueB and start the attempt.\n+  //2. Add 2 subqueues(QueueB1 and QueueB2) to QueueB, restart the RM, once with\n+  //   fail fast config as false and once with fail fast as true.\n+  //3. Verify that app was killed if fail fast is false.\n+  //4. Verify that QueueException was thrown if fail fast is true.\n+  @Test (timeout = 30000)\n+  public void testCapacityLeafQueueBecomesParentOnRecovery() throws Exception {\n+    if (getSchedulerType() != SchedulerType.CAPACITY) {\n+      return;\n+    }\n+    conf.setBoolean(CapacitySchedulerConfiguration.ENABLE_USER_METRICS, true);\n+    conf.set(CapacitySchedulerConfiguration.RESOURCE_CALCULATOR_CLASS,\n+        DominantResourceCalculator.class.getName());\n+    CapacitySchedulerConfiguration csConf =\n+        new CapacitySchedulerConfiguration(conf);\n+    setupQueueConfiguration(csConf);\n+    MemoryRMStateStore memStore = new MemoryRMStateStore();\n+    memStore.init(csConf);\n+    rm1 = new MockRM(csConf, memStore);\n+    rm1.start();\n+    MockNM nm =\n+        new MockNM(\"127.1.1.1:4321\", 8192, rm1.getResourceTrackerService());\n+    nm.registerNode();\n+\n+    // Submit an app to QueueB.\n+    RMApp app = rm1.submitApp(1024, \"app\", USER_2, null, B);\n+    MockRM.launchAndRegisterAM(app, rm1, nm);\n+    assertEquals(rm1.getApplicationReport(app.getApplicationId()).\n+        getYarnApplicationState(), YarnApplicationState.RUNNING);\n+\n+    // Take a copy of state store so that it can be reset to this state.\n+    RMState state = memStore.loadState();\n+\n+    // Change scheduler config with child queues added to QueueB.\n+    csConf = new CapacitySchedulerConfiguration(conf);\n+    setupQueueConfigurationChildOfB(csConf);\n+\n+    String diags = \"Application killed on recovery as it was submitted to \" +\n+        \"queue QueueB which is no longer a leaf queue after restart.\";\n+    verifyAppRecoveryWithWrongQueueConfig(csConf, app, diags, memStore, state);\n+  }\n+\n+  //Test behavior of an app if queue is removed during recovery. Test case does\n+  //following:\n   //1. Add some apps to two queues, attempt to add an app to a non-existant\n   //   queue to verify that the new logic is not in effect during normal app\n   //   submission\n-  //2. Remove one of the queues, restart the RM\n-  //3. Verify that the expected exception was thrown\n-  @Test (timeout = 30000, expected = QueueNotFoundException.class)\n+  //2. Remove one of the queues, restart the RM, once with fail fast config as\n+  //   false and once with fail fast as true.\n+  //3. Verify that app was killed if fail fast is false.\n+  //4. Verify that QueueException was thrown if fail fast is true.\n+  @Test (timeout = 30000)\n   public void testCapacitySchedulerQueueRemovedRecovery() throws Exception {\n     if (getSchedulerType() != SchedulerType.CAPACITY) {\n-      throw new QueueNotFoundException(\"Dummy\");\n+      return;\n     }\n     conf.setBoolean(CapacitySchedulerConfiguration.ENABLE_USER_METRICS, true);\n     conf.set(CapacitySchedulerConfiguration.RESOURCE_CALCULATOR_CLASS,\n@@ -549,7 +666,9 @@ public void testCapacitySchedulerQueueRemovedRecovery() throws Exception {\n \n     RMApp app2 = rm1.submitApp(1024, \"app2\", USER_2, null, B);\n     MockAM am2 = MockRM.launchAndRegisterAM(app2, rm1, nm2);\n-    \n+    assertEquals(rm1.getApplicationReport(app2.getApplicationId()).\n+        getYarnApplicationState(), YarnApplicationState.RUNNING);\n+\n     //Submit an app with a non existant queue to make sure it does not\n     //cause a fatal failure in the non-recovery case\n     RMApp appNA = rm1.submitApp(1024, \"app1_2\", USER_1, null,\n@@ -560,12 +679,16 @@ public void testCapacitySchedulerQueueRemovedRecovery() throws Exception {\n     rm1.clearQueueMetrics(app1_2);\n     rm1.clearQueueMetrics(app2);\n \n-    // Re-start RM\n-    csConf =\n-        new CapacitySchedulerConfiguration(conf);\n+    // Take a copy of state store so that it can be reset to this state.\n+    RMState state = memStore.loadState();\n+\n+    // Set new configuration with QueueB removed.\n+    csConf = new CapacitySchedulerConfiguration(conf);\n     setupQueueConfigurationOnlyA(csConf);\n-    rm2 = new MockRM(csConf, memStore);\n-    rm2.start();\n+\n+    String diags = \"Application killed on recovery as it was submitted to \" +\n+        \"queue QueueB which no longer exists after restart.\";\n+    verifyAppRecoveryWithWrongQueueConfig(csConf, app2, diags, memStore, state);\n   }\n \n   private void checkParentQueue(ParentQueue parentQueue, int numContainers,",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestWorkPreservingRMRestart.java",
                "sha": "479ee9344c5b6e62f84bdf8f68d9f4b3ec712f4f",
                "status": "modified"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 35,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java",
                "patch": "@@ -473,8 +473,8 @@ protected RMApp testCreateAppFinished(\n       application = testCreateAppFinishing(submissionContext);\n     }\n     // RUNNING/FINISHING => FINISHED event RMAppEventType.ATTEMPT_FINISHED\n-    RMAppEvent finishedEvent = new RMAppFinishedAttemptEvent(\n-        application.getApplicationId(), diagnostics);\n+    RMAppEvent finishedEvent = new RMAppEvent(application.getApplicationId(),\n+        RMAppEventType.ATTEMPT_FINISHED, diagnostics);\n     application.handle(finishedEvent);\n     assertAppState(RMAppState.FINISHED, application);\n     assertTimesAtFinish(application);\n@@ -548,8 +548,9 @@ public void testAppNewKill() throws IOException {\n \n     RMApp application = createNewTestApp(null);\n     // NEW => KILLED event RMAppEventType.KILL\n-    RMAppEvent event = \n-        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL);\n+    RMAppEvent event =\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n     sendAppUpdateSavedEvent(application);\n@@ -566,8 +567,8 @@ public void testAppNewReject() throws IOException {\n     RMApp application = createNewTestApp(null);\n     // NEW => FAILED event RMAppEventType.APP_REJECTED\n     String rejectedText = \"Test Application Rejected\";\n-    RMAppEvent event = \n-        new RMAppRejectedEvent(application.getApplicationId(), rejectedText);\n+    RMAppEvent event = new RMAppEvent(application.getApplicationId(),\n+        RMAppEventType.APP_REJECTED, rejectedText);\n     application.handle(event);\n     rmDispatcher.await();\n     sendAppUpdateSavedEvent(application);\n@@ -583,8 +584,8 @@ public void testAppNewRejectAddToStore() throws IOException {\n     RMApp application = createNewTestApp(null);\n     // NEW => FAILED event RMAppEventType.APP_REJECTED\n     String rejectedText = \"Test Application Rejected\";\n-    RMAppEvent event =\n-        new RMAppRejectedEvent(application.getApplicationId(), rejectedText);\n+    RMAppEvent event = new RMAppEvent(application.getApplicationId(),\n+        RMAppEventType.APP_REJECTED, rejectedText);\n     application.handle(event);\n     rmDispatcher.await();\n     sendAppUpdateSavedEvent(application);\n@@ -601,7 +602,8 @@ public void testAppNewSavingKill() throws IOException {\n     RMApp application = testCreateAppNewSaving(null);\n     // NEW_SAVING => KILLED event RMAppEventType.KILL\n     RMAppEvent event =\n-        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL);\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n     sendAppUpdateSavedEvent(application);\n@@ -617,8 +619,8 @@ public void testAppNewSavingReject() throws IOException {\n     RMApp application = testCreateAppNewSaving(null);\n     // NEW_SAVING => FAILED event RMAppEventType.APP_REJECTED\n     String rejectedText = \"Test Application Rejected\";\n-    RMAppEvent event =\n-        new RMAppRejectedEvent(application.getApplicationId(), rejectedText);\n+    RMAppEvent event = new RMAppEvent(application.getApplicationId(),\n+        RMAppEventType.APP_REJECTED, rejectedText);\n     application.handle(event);\n     rmDispatcher.await();\n     sendAppUpdateSavedEvent(application);\n@@ -634,8 +636,8 @@ public void testAppSubmittedRejected() throws IOException {\n     RMApp application = testCreateAppSubmittedNoRecovery(null);\n     // SUBMITTED => FAILED event RMAppEventType.APP_REJECTED\n     String rejectedText = \"app rejected\";\n-    RMAppEvent event = \n-        new RMAppRejectedEvent(application.getApplicationId(), rejectedText);\n+    RMAppEvent event = new RMAppEvent(application.getApplicationId(),\n+        RMAppEventType.APP_REJECTED, rejectedText);\n     application.handle(event);\n     rmDispatcher.await();\n     sendAppUpdateSavedEvent(application);\n@@ -649,8 +651,9 @@ public void testAppSubmittedKill() throws IOException, InterruptedException {\n     LOG.info(\"--- START: testAppSubmittedKill---\");\n     RMApp application = testCreateAppSubmittedNoRecovery(null);\n     // SUBMITTED => KILLED event RMAppEventType.KILL\n-    RMAppEvent event = new RMAppEvent(application.getApplicationId(),\n-        RMAppEventType.KILL);\n+    RMAppEvent event =\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n     sendAppUpdateSavedEvent(application);\n@@ -700,15 +703,16 @@ public void testAppAcceptedKill() throws IOException, InterruptedException {\n     LOG.info(\"--- START: testAppAcceptedKill ---\");\n     RMApp application = testCreateAppAccepted(null);\n     // ACCEPTED => KILLED event RMAppEventType.KILL\n-    RMAppEvent event = new RMAppEvent(application.getApplicationId(),\n-        RMAppEventType.KILL);\n+    RMAppEvent event =\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n \n     assertAppState(RMAppState.KILLING, application);\n     RMAppEvent appAttemptKilled =\n         new RMAppEvent(application.getApplicationId(),\n-          RMAppEventType.ATTEMPT_KILLED);\n+          RMAppEventType.ATTEMPT_KILLED, \"Application killed by user.\");\n     application.handle(appAttemptKilled);\n     assertAppState(RMAppState.FINAL_SAVING, application);\n     sendAppUpdateSavedEvent(application);\n@@ -729,7 +733,7 @@ public void testAppAcceptedAttemptKilled() throws IOException,\n     // RUNNING.\n     RMAppEvent event =\n         new RMAppEvent(application.getApplicationId(),\n-            RMAppEventType.ATTEMPT_KILLED);\n+            RMAppEventType.ATTEMPT_KILLED, \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n \n@@ -747,8 +751,9 @@ public void testAppRunningKill() throws IOException {\n \n     RMApp application = testCreateAppRunning(null);\n     // RUNNING => KILLED event RMAppEventType.KILL\n-    RMAppEvent event = \n-        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL);\n+    RMAppEvent event =\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n \n@@ -806,7 +811,9 @@ public void testAppRunningFailed() throws IOException {\n     assertAppFinalStateSaved(application);\n \n     // FAILED => FAILED event RMAppEventType.KILL\n-    event = new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL);\n+    event =\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n     assertFailed(application, \".*Failing the application.*\");\n@@ -821,7 +828,8 @@ public void testAppAtFinishingIgnoreKill() throws IOException {\n     RMApp application = testCreateAppFinishing(null);\n     // FINISHING => FINISHED event RMAppEventType.KILL\n     RMAppEvent event =\n-        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL);\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n     assertAppState(RMAppState.FINISHING, application);\n@@ -838,8 +846,8 @@ public void testAppFinalSavingToFinished() throws IOException {\n     RMApp application = testCreateAppFinalSaving(null);\n     final String diagMsg = \"some diagnostics\";\n     // attempt_finished event comes before attempt_saved event\n-    RMAppEvent event =\n-        new RMAppFinishedAttemptEvent(application.getApplicationId(), diagMsg);\n+    RMAppEvent event = new RMAppEvent(application.getApplicationId(),\n+        RMAppEventType.ATTEMPT_FINISHED, diagMsg);\n     application.handle(event);\n     assertAppState(RMAppState.FINAL_SAVING, application);\n     RMAppEvent appUpdated =\n@@ -860,8 +868,9 @@ public void testAppFinishedFinished() throws IOException {\n \n     RMApp application = testCreateAppFinished(null, \"\");\n     // FINISHED => FINISHED event RMAppEventType.KILL\n-    RMAppEvent event = \n-        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL);\n+    RMAppEvent event =\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n     assertTimesAtFinish(application);\n@@ -879,8 +888,8 @@ public void testAppFailedFailed() throws IOException {\n     RMApp application = testCreateAppNewSaving(null);\n \n     // NEW_SAVING => FAILED event RMAppEventType.APP_REJECTED\n-    RMAppEvent event =\n-        new RMAppRejectedEvent(application.getApplicationId(), \"\");\n+    RMAppEvent event = new RMAppEvent(application.getApplicationId(),\n+        RMAppEventType.APP_REJECTED, \"\");\n     application.handle(event);\n     rmDispatcher.await();\n     sendAppUpdateSavedEvent(application);\n@@ -889,7 +898,8 @@ public void testAppFailedFailed() throws IOException {\n \n     // FAILED => FAILED event RMAppEventType.KILL\n     event =\n-        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL);\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n     assertTimesAtFinish(application);\n@@ -907,8 +917,9 @@ public void testAppKilledKilled() throws IOException {\n     RMApp application = testCreateAppRunning(null);\n \n     // RUNNING => KILLED event RMAppEventType.KILL\n-    RMAppEvent event = \n-        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL);\n+    RMAppEvent event =\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n     sendAttemptUpdateSavedEvent(application);\n@@ -917,8 +928,8 @@ public void testAppKilledKilled() throws IOException {\n     assertAppState(RMAppState.KILLED, application);\n \n     // KILLED => KILLED event RMAppEventType.ATTEMPT_FINISHED\n-    event = new RMAppFinishedAttemptEvent(\n-        application.getApplicationId(), \"\");\n+    event = new RMAppEvent(application.getApplicationId(),\n+        RMAppEventType.ATTEMPT_FINISHED, \"\");\n     application.handle(event);\n     rmDispatcher.await();\n     assertTimesAtFinish(application);\n@@ -935,7 +946,9 @@ public void testAppKilledKilled() throws IOException {\n \n \n     // KILLED => KILLED event RMAppEventType.KILL\n-    event = new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL);\n+    event =\n+        new RMAppEvent(application.getApplicationId(), RMAppEventType.KILL,\n+        \"Application killed by user.\");\n     application.handle(event);\n     rmDispatcher.await();\n     assertTimesAtFinish(application);",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/TestRMAppTransitions.java",
                "sha": "91388dbc8096c407e7bad0e60afbaece5c7b3e76",
                "status": "modified"
            },
            {
                "additions": 32,
                "blob_url": "https://github.com/apache/hadoop/blob/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java",
                "changes": 59,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java?ref=cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5",
                "deletions": 27,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java",
                "patch": "@@ -26,6 +26,7 @@\n import static org.junit.Assume.assumeTrue;\n import static org.mockito.Matchers.any;\n import static org.mockito.Matchers.anyLong;\n+import static org.mockito.Matchers.argThat;\n import static org.mockito.Matchers.eq;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.never;\n@@ -86,12 +87,8 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppFailedAttemptEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRejectedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppRunningOnNodeEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptContainerAllocatedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptContainerFinishedEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptFailedEvent;\n-import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptLaunchFailedEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptRegistrationEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.event.RMAppAttemptUnregistrationEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.ContainerAllocationExpirer;\n@@ -124,6 +121,7 @@\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n import org.mockito.ArgumentCaptor;\n+import org.mockito.ArgumentMatcher;\n import org.mockito.Matchers;\n import org.mockito.Mockito;\n import org.mockito.invocation.InvocationOnMock;\n@@ -416,10 +414,16 @@ private void testAppAttemptSubmittedToFailedState(String diagnostics) {\n     // Check events\n     verify(masterService).\n         unregisterAttempt(applicationAttempt.getAppAttemptId());\n-    \n-    // this works for unmanaged and managed AM's because this is actually doing\n-    // verify(application).handle(anyObject());\n-    verify(application).handle(any(RMAppRejectedEvent.class));\n+    // ATTEMPT_FAILED should be notified to app if app attempt is submitted to\n+    // failed state.\n+    ArgumentMatcher<RMAppEvent> matcher = new ArgumentMatcher<RMAppEvent>() {\n+      @Override\n+      public boolean matches(Object o) {\n+        RMAppEvent event = (RMAppEvent) o;\n+        return event.getType() == RMAppEventType.ATTEMPT_FAILED;\n+      }\n+    };\n+    verify(application).handle(argThat(matcher));\n     verifyTokenCount(applicationAttempt.getAppAttemptId(), 1);\n     verifyApplicationAttemptFinished(RMAppAttemptState.FAILED);\n   }\n@@ -649,8 +653,8 @@ private Container allocateApplicationAttempt() {\n         thenReturn(rmContainer);\n     \n     applicationAttempt.handle(\n-        new RMAppAttemptContainerAllocatedEvent(\n-            applicationAttempt.getAppAttemptId()));\n+        new RMAppAttemptEvent(applicationAttempt.getAppAttemptId(),\n+            RMAppAttemptEventType.CONTAINER_ALLOCATED));\n     \n     assertEquals(RMAppAttemptState.ALLOCATED_SAVING, \n         applicationAttempt.getAppAttemptState());\n@@ -906,9 +910,8 @@ public void testAllocatedToFailed() {\n     Container amContainer = allocateApplicationAttempt();\n     String diagnostics = \"Launch Failed\";\n     applicationAttempt.handle(\n-        new RMAppAttemptLaunchFailedEvent(\n-            applicationAttempt.getAppAttemptId(), \n-            diagnostics));\n+        new RMAppAttemptEvent(applicationAttempt.getAppAttemptId(),\n+            RMAppAttemptEventType.LAUNCH_FAILED, diagnostics));\n     assertEquals(YarnApplicationAttemptState.ALLOCATED,\n         applicationAttempt.createApplicationAttemptState());\n     testAppAttemptFailedState(amContainer, diagnostics);\n@@ -927,8 +930,9 @@ public void testLaunchedAtFinalSaving() {\n     // verify for both launched and launch_failed transitions in final_saving\n     applicationAttempt.handle(new RMAppAttemptEvent(applicationAttempt\n         .getAppAttemptId(), RMAppAttemptEventType.LAUNCHED));\n-    applicationAttempt.handle(new RMAppAttemptLaunchFailedEvent(\n-        applicationAttempt.getAppAttemptId(), \"Launch Failed\"));\n+    applicationAttempt.handle(\n+        new RMAppAttemptEvent(applicationAttempt.getAppAttemptId(),\n+            RMAppAttemptEventType.LAUNCH_FAILED, \"Launch Failed\"));\n \n     assertEquals(RMAppAttemptState.FINAL_SAVING,\n         applicationAttempt.getAppAttemptState());\n@@ -938,8 +942,9 @@ public void testLaunchedAtFinalSaving() {\n     // verify for both launched and launch_failed transitions in killed\n     applicationAttempt.handle(new RMAppAttemptEvent(applicationAttempt\n         .getAppAttemptId(), RMAppAttemptEventType.LAUNCHED));\n-    applicationAttempt.handle(new RMAppAttemptLaunchFailedEvent(\n-        applicationAttempt.getAppAttemptId(), \"Launch Failed\"));\n+    applicationAttempt.handle(new RMAppAttemptEvent(\n+        applicationAttempt.getAppAttemptId(),\n+            RMAppAttemptEventType.LAUNCH_FAILED, \"Launch Failed\"));\n     assertEquals(RMAppAttemptState.KILLED,\n         applicationAttempt.getAppAttemptState());\n   }\n@@ -1546,8 +1551,8 @@ public Allocation answer(InvocationOnMock invocation)\n \n   @Test(timeout = 30000)\n   public void testNewToFailed() {\n-    applicationAttempt.handle(new RMAppAttemptFailedEvent(applicationAttempt\n-        .getAppAttemptId(), FAILED_DIAGNOSTICS));\n+    applicationAttempt.handle(new RMAppAttemptEvent(applicationAttempt\n+        .getAppAttemptId(), RMAppAttemptEventType.FAIL, FAILED_DIAGNOSTICS));\n     assertEquals(YarnApplicationAttemptState.NEW,\n         applicationAttempt.createApplicationAttemptState());\n     testAppAttemptFailedState(null, FAILED_DIAGNOSTICS);\n@@ -1557,8 +1562,8 @@ public void testNewToFailed() {\n   @Test(timeout = 30000)\n   public void testSubmittedToFailed() {\n     submitApplicationAttempt();\n-    applicationAttempt.handle(new RMAppAttemptFailedEvent(applicationAttempt\n-        .getAppAttemptId(), FAILED_DIAGNOSTICS));\n+    applicationAttempt.handle(new RMAppAttemptEvent(applicationAttempt\n+        .getAppAttemptId(), RMAppAttemptEventType.FAIL, FAILED_DIAGNOSTICS));\n     assertEquals(YarnApplicationAttemptState.SUBMITTED,\n         applicationAttempt.createApplicationAttemptState());\n     testAppAttemptFailedState(null, FAILED_DIAGNOSTICS);\n@@ -1567,8 +1572,8 @@ public void testSubmittedToFailed() {\n   @Test(timeout = 30000)\n   public void testScheduledToFailed() {\n     scheduleApplicationAttempt();\n-    applicationAttempt.handle(new RMAppAttemptFailedEvent(applicationAttempt\n-        .getAppAttemptId(), FAILED_DIAGNOSTICS));\n+    applicationAttempt.handle(new RMAppAttemptEvent(applicationAttempt\n+        .getAppAttemptId(), RMAppAttemptEventType.FAIL, FAILED_DIAGNOSTICS));\n     assertEquals(YarnApplicationAttemptState.SCHEDULED,\n         applicationAttempt.createApplicationAttemptState());\n     testAppAttemptFailedState(null, FAILED_DIAGNOSTICS);\n@@ -1579,8 +1584,8 @@ public void testAllocatedToFailedUserTriggeredFailEvent() {\n     Container amContainer = allocateApplicationAttempt();\n     assertEquals(YarnApplicationAttemptState.ALLOCATED,\n         applicationAttempt.createApplicationAttemptState());\n-    applicationAttempt.handle(new RMAppAttemptFailedEvent(applicationAttempt\n-        .getAppAttemptId(), FAILED_DIAGNOSTICS));\n+    applicationAttempt.handle(new RMAppAttemptEvent(applicationAttempt\n+        .getAppAttemptId(), RMAppAttemptEventType.FAIL, FAILED_DIAGNOSTICS));\n     testAppAttemptFailedState(amContainer, FAILED_DIAGNOSTICS);\n   }\n \n@@ -1589,8 +1594,8 @@ public void testRunningToFailedUserTriggeredFailEvent() {\n     Container amContainer = allocateApplicationAttempt();\n     launchApplicationAttempt(amContainer);\n     runApplicationAttempt(amContainer, \"host\", 8042, \"oldtrackingurl\", false);\n-    applicationAttempt.handle(new RMAppAttemptFailedEvent(applicationAttempt\n-        .getAppAttemptId(), FAILED_DIAGNOSTICS));\n+    applicationAttempt.handle(new RMAppAttemptEvent(applicationAttempt\n+        .getAppAttemptId(), RMAppAttemptEventType.FAIL, FAILED_DIAGNOSTICS));\n     assertEquals(RMAppAttemptState.FINAL_SAVING,\n         applicationAttempt.getAppAttemptState());\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/cf23f2c2b5b4eb9e51de1a66b7aa57dee7ff30b5/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java",
                "sha": "7f9610faa5313a9f8dc24aed49562f7231874e9f",
                "status": "modified"
            }
        ],
        "message": "YARN-4000. RM crashes with NPE if leaf queue becomes parent queue during restart. Contributed by Varun Saxena",
        "parent": "https://github.com/apache/hadoop/commit/a121fa1d39b2eb129bcc0e786d0d24c9ec0cdefc",
        "repo": "hadoop",
        "unit_tests": [
            "TestClientRMService.java",
            "TestRMContainerImpl.java",
            "TestAbstractYarnScheduler.java",
            "TestCapacityScheduler.java",
            "TestFairScheduler.java",
            "TestDelegationTokenRenewer.java"
        ]
    },
    "hadoop_cfe89e6": {
        "bug_id": "hadoop_cfe89e6",
        "commit": "https://github.com/apache/hadoop/commit/cfe89e6f963ba25b5fff1ce48cad36d74b3c789c",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/cfe89e6f963ba25b5fff1ce48cad36d74b3c789c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/OpportunisticContainerAllocatorAMService.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/OpportunisticContainerAllocatorAMService.java?ref=cfe89e6f963ba25b5fff1ce48cad36d74b3c789c",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/OpportunisticContainerAllocatorAMService.java",
                "patch": "@@ -348,9 +348,11 @@ private void handleNewContainers(List<Container> allocContainers,\n       RMContainer rmContainer =\n           SchedulerUtils.createOpportunisticRmContainer(\n               rmContext, container, isRemotelyAllocated);\n-      rmContainer.handle(\n-          new RMContainerEvent(container.getId(),\n-              RMContainerEventType.ACQUIRED));\n+      if (rmContainer!=null) {\n+        rmContainer.handle(\n+            new RMContainerEvent(container.getId(),\n+                RMContainerEventType.ACQUIRED));\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/cfe89e6f963ba25b5fff1ce48cad36d74b3c789c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/OpportunisticContainerAllocatorAMService.java",
                "sha": "69a704ee96ddb9f0b7be526e172bb18687d4411b",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/cfe89e6f963ba25b5fff1ce48cad36d74b3c789c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java?ref=cfe89e6f963ba25b5fff1ce48cad36d74b3c789c",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "patch": "@@ -693,8 +693,10 @@ public void completedContainer(RMContainer rmContainer,\n         LOG.debug(\"Completed container: \" + rmContainer.getContainerId() +\n             \" in state: \" + rmContainer.getState() + \" event:\" + event);\n       }\n-      getSchedulerNode(rmContainer.getNodeId()).releaseContainer(\n-          rmContainer.getContainerId(), false);\n+      SchedulerNode node = getSchedulerNode(rmContainer.getNodeId());\n+      if (node != null) {\n+        node.releaseContainer(rmContainer.getContainerId(), false);\n+      }\n     }\n \n     // If the container is getting killed in ACQUIRED state, the requester (AM\n@@ -1300,8 +1302,10 @@ private void handleDecreaseRequests(SchedulerApplicationAttempt appAttempt,\n               uReq.getContainerUpdateType()) {\n             RMContainer demotedRMContainer =\n                 createDemotedRMContainer(appAttempt, oppCntxt, rmContainer);\n-            appAttempt.addToNewlyDemotedContainers(\n-                uReq.getContainerId(), demotedRMContainer);\n+            if (demotedRMContainer != null) {\n+              appAttempt.addToNewlyDemotedContainers(\n+                      uReq.getContainerId(), demotedRMContainer);\n+            }\n           } else {\n             RMContainer demotedRMContainer = createDecreasedRMContainer(\n                 appAttempt, uReq, rmContainer);",
                "raw_url": "https://github.com/apache/hadoop/raw/cfe89e6f963ba25b5fff1ce48cad36d74b3c789c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AbstractYarnScheduler.java",
                "sha": "0ad47bb8f909f9e39ef783a7ba6a6460d56c3946",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/cfe89e6f963ba25b5fff1ce48cad36d74b3c789c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java?ref=cfe89e6f963ba25b5fff1ce48cad36d74b3c789c",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java",
                "patch": "@@ -564,6 +564,11 @@ public static boolean hasPendingResourceRequest(ResourceCalculator rc,\n \n   public static RMContainer createOpportunisticRmContainer(RMContext rmContext,\n       Container container, boolean isRemotelyAllocated) {\n+    SchedulerNode node = ((AbstractYarnScheduler) rmContext.getScheduler())\n+        .getNode(container.getNodeId());\n+    if (node == null) {\n+      return null;\n+    }\n     SchedulerApplicationAttempt appAttempt =\n         ((AbstractYarnScheduler) rmContext.getScheduler())\n             .getCurrentAttemptForContainer(container.getId());\n@@ -572,8 +577,7 @@ public static RMContainer createOpportunisticRmContainer(RMContext rmContext,\n         appAttempt.getApplicationAttemptId(), container.getNodeId(),\n         appAttempt.getUser(), rmContext, isRemotelyAllocated);\n     appAttempt.addRMContainer(container.getId(), rmContainer);\n-    ((AbstractYarnScheduler) rmContext.getScheduler()).getNode(\n-        container.getNodeId()).allocateContainer(rmContainer);\n+    node.allocateContainer(rmContainer);\n     return rmContainer;\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/cfe89e6f963ba25b5fff1ce48cad36d74b3c789c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/SchedulerUtils.java",
                "sha": "a048dacb688afa39d96a937ff139eaea414f3ee5",
                "status": "modified"
            },
            {
                "additions": 78,
                "blob_url": "https://github.com/apache/hadoop/blob/cfe89e6f963ba25b5fff1ce48cad36d74b3c789c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestOpportunisticContainerAllocatorAMService.java",
                "changes": 78,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestOpportunisticContainerAllocatorAMService.java?ref=cfe89e6f963ba25b5fff1ce48cad36d74b3c789c",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestOpportunisticContainerAllocatorAMService.java",
                "patch": "@@ -23,6 +23,7 @@\n import org.apache.hadoop.ipc.RPC;\n import org.apache.hadoop.ipc.Server;\n import org.apache.hadoop.net.NetUtils;\n+import org.apache.hadoop.test.GenericTestUtils;\n import org.apache.hadoop.yarn.api.ApplicationMasterProtocolPB;\n import org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateRequestPBImpl;\n import org.apache.hadoop.yarn.api.protocolrecords.impl.pb.AllocateResponsePBImpl;\n@@ -72,14 +73,19 @@\n import org.apache.hadoop.yarn.server.api.records.OpportunisticContainersStatus;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.AMLivelinessMonitor;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainer;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerState;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNodeImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApplicationAttempt;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacitySchedulerConfiguration;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAttemptRemovedSchedulerEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeAddedSchedulerEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeRemovedSchedulerEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.NodeUpdateSchedulerEvent;\n@@ -88,19 +94,25 @@\n     .FifoScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.security.RMContainerTokenSecretManager;\n import org.apache.hadoop.yarn.server.scheduler.OpportunisticContainerContext;\n+import org.apache.hadoop.yarn.server.scheduler.SchedulerRequestKey;\n import org.apache.hadoop.yarn.util.resource.Resources;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Test;\n import org.mockito.Mockito;\n \n+import com.google.common.base.Supplier;\n+\n+import static org.junit.Assert.fail;\n+\n import java.io.IOException;\n import java.net.InetSocketAddress;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.HashMap;\n import java.util.List;\n+import java.util.concurrent.TimeoutException;\n \n /**\n  * Test cases for {@link OpportunisticContainerAllocatorAMService}.\n@@ -798,6 +810,72 @@ public void testNodeRemovalDuringAllocate() throws Exception {\n     Assert.assertEquals(1, ctxt.getNodeMap().size());\n   }\n \n+  @Test(timeout = 60000)\n+  public void testAppAttemptRemovalAfterNodeRemoval() throws Exception {\n+    MockNM nm = new MockNM(\"h:1234\", 4096, rm.getResourceTrackerService());\n+    nm.registerNode();\n+    OpportunisticContainerAllocatorAMService amservice =\n+        (OpportunisticContainerAllocatorAMService) rm\n+            .getApplicationMasterService();\n+    RMApp app = rm.submitApp(1 * GB, \"app\", \"user\", null, \"default\");\n+    ApplicationAttemptId attemptId =\n+        app.getCurrentAppAttempt().getAppAttemptId();\n+    MockAM am = MockRM.launchAndRegisterAM(app, rm, nm);\n+    ResourceScheduler scheduler = rm.getResourceScheduler();\n+    SchedulerApplicationAttempt schedulerAttempt =\n+        ((CapacityScheduler)scheduler).getApplicationAttempt(attemptId);\n+    RMNode rmNode1 = rm.getRMContext().getRMNodes().get(nm.getNodeId());\n+    nm.nodeHeartbeat(true);\n+    ((RMNodeImpl) rmNode1)\n+        .setOpportunisticContainersStatus(getOppurtunisticStatus(-1, 100));\n+    // Send add and update node events to AM Service.\n+    amservice.handle(new NodeAddedSchedulerEvent(rmNode1));\n+    amservice.handle(new NodeUpdateSchedulerEvent(rmNode1));\n+    try {\n+      GenericTestUtils.waitFor(new Supplier<Boolean>() {\n+        @Override public Boolean get() {\n+          return scheduler.getNumClusterNodes() == 1;\n+        }\n+      }, 10, 200 * 100);\n+    }catch (TimeoutException e) {\n+      fail(\"timed out while waiting for NM to add.\");\n+    }\n+    AllocateResponse allocateResponse = am.allocate(\n+            Arrays.asList(ResourceRequest.newInstance(Priority.newInstance(1),\n+                \"*\", Resources.createResource(1 * GB), 2, true, null,\n+                ExecutionTypeRequest.newInstance(\n+                    ExecutionType.OPPORTUNISTIC, true))),\n+                null);\n+    List<Container> allocatedContainers = allocateResponse\n+        .getAllocatedContainers();\n+    Container container = allocatedContainers.get(0);\n+    scheduler.handle(new NodeRemovedSchedulerEvent(rmNode1));\n+    try {\n+      GenericTestUtils.waitFor(new Supplier<Boolean>() {\n+        @Override public Boolean get() {\n+          return scheduler.getNumClusterNodes() == 0;\n+        }\n+      }, 10, 200 * 100);\n+    }catch (TimeoutException e) {\n+      fail(\"timed out while waiting for NM to remove.\");\n+    }\n+    //test YARN-9165\n+    RMContainer rmContainer = null;\n+    rmContainer = SchedulerUtils.createOpportunisticRmContainer(\n+                    rm.getRMContext(), container, true);\n+    if (rmContainer == null) {\n+      rmContainer = new RMContainerImpl(container,\n+        SchedulerRequestKey.extractFrom(container),\n+        schedulerAttempt.getApplicationAttemptId(), container.getNodeId(),\n+        schedulerAttempt.getUser(), rm.getRMContext(), true);\n+    }\n+    assert(rmContainer!=null);\n+    //test YARN-9164\n+    schedulerAttempt.addRMContainer(container.getId(), rmContainer);\n+    scheduler.handle(new AppAttemptRemovedSchedulerEvent(attemptId,\n+        RMAppAttemptState.FAILED, false));\n+  }\n+\n   private OpportunisticContainersStatus getOppurtunisticStatus(int waitTime,\n       int queueLength) {\n     OpportunisticContainersStatus status1 =",
                "raw_url": "https://github.com/apache/hadoop/raw/cfe89e6f963ba25b5fff1ce48cad36d74b3c789c/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestOpportunisticContainerAllocatorAMService.java",
                "sha": "b2be1e732ba8081583febd5f70c1ee3457caec60",
                "status": "modified"
            }
        ],
        "message": "YARN-9164. Shutdown NM may cause NPE when opportunistic container scheduling is enabled. Contributed by lujie.",
        "parent": "https://github.com/apache/hadoop/commit/040a202b202a37f3b922cd321eb0a8ded457d88b",
        "repo": "hadoop",
        "unit_tests": [
            "TestOpportunisticContainerAllocatorAMService.java",
            "TestAbstractYarnScheduler.java",
            "TestSchedulerUtils.java"
        ]
    },
    "hadoop_d19d187": {
        "bug_id": "hadoop_d19d187",
        "commit": "https://github.com/apache/hadoop/commit/d19d18775368f5aaa254881165acc1299837072b",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/d19d18775368f5aaa254881165acc1299837072b/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=d19d18775368f5aaa254881165acc1299837072b",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -672,6 +672,9 @@ Release 2.8.0 - UNRELEASED\n     YARN-3845. Scheduler page does not render RGBA color combinations in IE11. \n     (Contributed by Mohammad Shahid Khan)\n \n+    YARN-3957. FairScheduler NPE In FairSchedulerQueueInfo causing scheduler page to \n+    return 500. (Anubhav Dhoot via kasha)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/d19d18775368f5aaa254881165acc1299837072b/hadoop-yarn-project/CHANGES.txt",
                "sha": "44e55100eb90c135d07ec95872f796cab8f56a69",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/d19d18775368f5aaa254881165acc1299837072b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/FairSchedulerQueueInfo.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/FairSchedulerQueueInfo.java?ref=d19d18775368f5aaa254881165acc1299837072b",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/FairSchedulerQueueInfo.java",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.yarn.server.resourcemanager.webapp.dao;\n \n \n+import java.util.ArrayList;\n import java.util.Collection;\n \n import javax.xml.bind.annotation.XmlAccessType;\n@@ -204,6 +205,7 @@ public String getSchedulingPolicy() {\n   }\n \n   public Collection<FairSchedulerQueueInfo> getChildQueues() {\n-    return childQueues.getQueueInfoList();\n+    return childQueues != null ? childQueues.getQueueInfoList() :\n+        new ArrayList<FairSchedulerQueueInfo>();\n   }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/d19d18775368f5aaa254881165acc1299837072b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/FairSchedulerQueueInfo.java",
                "sha": "7ba0988be80e91e5ac9205e474d5eb7cbbaaeb07",
                "status": "modified"
            },
            {
                "additions": 59,
                "blob_url": "https://github.com/apache/hadoop/blob/d19d18775368f5aaa254881165acc1299837072b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/TestFairSchedulerQueueInfo.java",
                "changes": 59,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/TestFairSchedulerQueueInfo.java?ref=d19d18775368f5aaa254881165acc1299837072b",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/TestFairSchedulerQueueInfo.java",
                "patch": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.yarn.server.resourcemanager.webapp.dao;\n+\n+import org.apache.hadoop.yarn.api.records.Resource;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.AllocationConfiguration;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FSQueue;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSchedulerConfiguration;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.QueueManager;\n+import org.apache.hadoop.yarn.util.SystemClock;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.util.Collection;\n+\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+public class TestFairSchedulerQueueInfo {\n+\n+  @Test\n+  public void testEmptyChildQueues() throws Exception {\n+    FairSchedulerConfiguration conf = new FairSchedulerConfiguration();\n+    FairScheduler scheduler = mock(FairScheduler.class);\n+    AllocationConfiguration allocConf = new AllocationConfiguration(conf);\n+    when(scheduler.getAllocationConfiguration()).thenReturn(allocConf);\n+    when(scheduler.getConf()).thenReturn(conf);\n+    when(scheduler.getClusterResource()).thenReturn(Resource.newInstance(1, 1));\n+    SystemClock clock = new SystemClock();\n+    when(scheduler.getClock()).thenReturn(clock);\n+    QueueManager queueManager = new QueueManager(scheduler);\n+    queueManager.initialize(conf);\n+\n+    FSQueue testQueue = queueManager.getLeafQueue(\"test\", true);\n+    FairSchedulerQueueInfo queueInfo =\n+        new FairSchedulerQueueInfo(testQueue, scheduler);\n+    Collection<FairSchedulerQueueInfo> childQueues =\n+        queueInfo.getChildQueues();\n+    Assert.assertNotNull(childQueues);\n+    Assert.assertEquals(\"Child QueueInfo was not empty\", 0, childQueues.size());\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/d19d18775368f5aaa254881165acc1299837072b/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/dao/TestFairSchedulerQueueInfo.java",
                "sha": "973afcfb99f7a37e0cc41bf9f601b28eb2015f4f",
                "status": "added"
            }
        ],
        "message": "YARN-3957. FairScheduler NPE In FairSchedulerQueueInfo causing scheduler page to return 500. (Anubhav Dhoot via kasha)",
        "parent": "https://github.com/apache/hadoop/commit/f8f60918230dd466ae8dda1fbc28878e19273232",
        "repo": "hadoop",
        "unit_tests": [
            "TestFairSchedulerQueueInfo.java"
        ]
    },
    "hadoop_d215357": {
        "bug_id": "hadoop_d215357",
        "commit": "https://github.com/apache/hadoop/commit/d2153577181f900ee6d8bf67d254e408bbaad243",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/d2153577181f900ee6d8bf67d254e408bbaad243/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java?ref=d2153577181f900ee6d8bf67d254e408bbaad243",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java",
                "patch": "@@ -23,6 +23,7 @@\n import com.google.common.primitives.Bytes;\n \n import org.apache.commons.codec.binary.Base64;\n+import org.apache.hadoop.HadoopIllegalArgumentException;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.conf.Configuration;\n@@ -358,6 +359,10 @@ private static String encodeWritable(Writable obj) throws IOException {\n    */\n   private static void decodeWritable(Writable obj,\n                                      String newValue) throws IOException {\n+    if (newValue == null) {\n+      throw new HadoopIllegalArgumentException(\n+              \"Invalid argument, newValue is null\");\n+    }\n     Base64 decoder = new Base64(0, null, true);\n     DataInputBuffer buf = new DataInputBuffer();\n     byte[] decoded = decoder.decode(newValue);",
                "raw_url": "https://github.com/apache/hadoop/raw/d2153577181f900ee6d8bf67d254e408bbaad243/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/token/Token.java",
                "sha": "25aac8853ef8f1231f73703e457160c2accff2bb",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hadoop/blob/d2153577181f900ee6d8bf67d254e408bbaad243/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/TestToken.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/TestToken.java?ref=d2153577181f900ee6d8bf67d254e408bbaad243",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/TestToken.java",
                "patch": "@@ -21,6 +21,7 @@\n import java.io.*;\n import java.util.Arrays;\n \n+import org.apache.hadoop.HadoopIllegalArgumentException;\n import org.apache.hadoop.io.*;\n import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier;\n import org.apache.hadoop.security.token.delegation.TestDelegationToken.TestDelegationTokenIdentifier;\n@@ -100,6 +101,23 @@ public void testEncodeWritable() throws Exception {\n     }\n   }\n \n+  /*\n+   * Test decodeWritable() with null newValue string argument,\n+   * should throw HadoopIllegalArgumentException.\n+   */\n+  @Test\n+  public void testDecodeWritableArgSanityCheck() throws Exception {\n+    Token<AbstractDelegationTokenIdentifier> token =\n+            new Token<AbstractDelegationTokenIdentifier>();\n+    try {\n+      token.decodeFromUrlString(null);\n+      fail(\"Should have thrown HadoopIllegalArgumentException\");\n+    }\n+    catch (HadoopIllegalArgumentException e) {\n+      Token.LOG.info(\"Test decodeWritable() sanity check success.\");\n+    }\n+  }\n+\n   @Test\n   public void testDecodeIdentifier() throws IOException {\n     TestDelegationTokenSecretManager secretManager =",
                "raw_url": "https://github.com/apache/hadoop/raw/d2153577181f900ee6d8bf67d254e408bbaad243/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/token/TestToken.java",
                "sha": "3a3567ce2a4314efb774fdbf7c0a56e71f583463",
                "status": "modified"
            }
        ],
        "message": "HDFS-13485. DataNode WebHDFS endpoint throws NPE. Contributed by Siyao Meng.",
        "parent": "https://github.com/apache/hadoop/commit/121865c3f96166e2190ed54b433ebcf8d053b91c",
        "repo": "hadoop",
        "unit_tests": [
            "TestToken.java"
        ]
    },
    "hadoop_d31c9d8": {
        "bug_id": "hadoop_d31c9d8",
        "commit": "https://github.com/apache/hadoop/commit/d31c9d8c495794a803fb20729b5ed6b374e23eb4",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/d31c9d8c495794a803fb20729b5ed6b374e23eb4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java?ref=d31c9d8c495794a803fb20729b5ed6b374e23eb4",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "patch": "@@ -1253,7 +1253,10 @@ void fixKerberosTicketOrder() {\n         Object cred = iter.next();\n         if (cred instanceof KerberosTicket) {\n           KerberosTicket ticket = (KerberosTicket) cred;\n-          if (!ticket.getServer().getName().startsWith(\"krbtgt\")) {\n+          if (ticket.isDestroyed() || ticket.getServer() == null) {\n+            LOG.warn(\"Ticket is already destroyed, remove it.\");\n+            iter.remove();\n+          } else if (!ticket.getServer().getName().startsWith(\"krbtgt\")) {\n             LOG.warn(\n                 \"The first kerberos ticket is not TGT\"\n                     + \"(the server principal is {}), remove and destroy it.\",",
                "raw_url": "https://github.com/apache/hadoop/raw/d31c9d8c495794a803fb20729b5ed6b374e23eb4/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/UserGroupInformation.java",
                "sha": "726e81111d1d9ef2d44a0aa2848f711f43012a19",
                "status": "modified"
            },
            {
                "additions": 77,
                "blob_url": "https://github.com/apache/hadoop/blob/d31c9d8c495794a803fb20729b5ed6b374e23eb4/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestFixKerberosTicketOrder.java",
                "changes": 77,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestFixKerberosTicketOrder.java?ref=d31c9d8c495794a803fb20729b5ed6b374e23eb4",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestFixKerberosTicketOrder.java",
                "patch": "@@ -155,4 +155,81 @@ public Void run() throws Exception {\n             .filter(t -> t.getServer().getName().startsWith(server2Protocol))\n             .findAny().isPresent());\n   }\n+\n+  @Test\n+  public void testWithDestroyedTGT() throws Exception {\n+    UserGroupInformation ugi =\n+        UserGroupInformation.loginUserFromKeytabAndReturnUGI(clientPrincipal,\n+            keytabFile.getCanonicalPath());\n+    ugi.doAs(new PrivilegedExceptionAction<Void>() {\n+\n+      @Override\n+      public Void run() throws Exception {\n+        SaslClient client = Sasl.createSaslClient(\n+            new String[] {AuthMethod.KERBEROS.getMechanismName()},\n+            clientPrincipal, server1Protocol, host, props, null);\n+        client.evaluateChallenge(new byte[0]);\n+        client.dispose();\n+        return null;\n+      }\n+    });\n+\n+    Subject subject = ugi.getSubject();\n+\n+    // mark the ticket as destroyed\n+    for (KerberosTicket ticket : subject\n+        .getPrivateCredentials(KerberosTicket.class)) {\n+      if (ticket.getServer().getName().startsWith(\"krbtgt\")) {\n+        ticket.destroy();\n+        break;\n+      }\n+    }\n+\n+    ugi.fixKerberosTicketOrder();\n+\n+    // verify that after fixing, the tgt ticket should be removed\n+    assertFalse(\"The first ticket is not tgt\",\n+        subject.getPrivateCredentials().stream()\n+            .filter(c -> c instanceof KerberosTicket)\n+            .map(c -> ((KerberosTicket) c).getServer().getName()).findFirst()\n+            .isPresent());\n+\n+\n+    // should fail as we send a service ticket instead of tgt to KDC.\n+    intercept(SaslException.class,\n+        () -> ugi.doAs(new PrivilegedExceptionAction<Void>() {\n+\n+          @Override\n+          public Void run() throws Exception {\n+            SaslClient client = Sasl.createSaslClient(\n+                new String[] {AuthMethod.KERBEROS.getMechanismName()},\n+                clientPrincipal, server2Protocol, host, props, null);\n+            client.evaluateChallenge(new byte[0]);\n+            client.dispose();\n+            return null;\n+          }\n+        }));\n+\n+    // relogin to get a new ticket\n+    ugi.reloginFromKeytab();\n+\n+    // make sure we can get new service ticket after the relogin.\n+    ugi.doAs(new PrivilegedExceptionAction<Void>() {\n+\n+      @Override\n+      public Void run() throws Exception {\n+        SaslClient client = Sasl.createSaslClient(\n+            new String[] {AuthMethod.KERBEROS.getMechanismName()},\n+            clientPrincipal, server2Protocol, host, props, null);\n+        client.evaluateChallenge(new byte[0]);\n+        client.dispose();\n+        return null;\n+      }\n+    });\n+\n+    assertTrue(\"No service ticket for \" + server2Protocol + \" found\",\n+        subject.getPrivateCredentials(KerberosTicket.class).stream()\n+            .filter(t -> t.getServer().getName().startsWith(server2Protocol))\n+            .findAny().isPresent());\n+  }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop/raw/d31c9d8c495794a803fb20729b5ed6b374e23eb4/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/security/TestFixKerberosTicketOrder.java",
                "sha": "cbea393d931644f906519dc0013a1b5e3a01d6df",
                "status": "modified"
            }
        ],
        "message": "HADOOP-15143. NPE due to Invalid KerberosTicket in UGI. Contributed by Mukul Kumar Singh.",
        "parent": "https://github.com/apache/hadoop/commit/52babbb4a0e3c89f2025bf6e9a1b51a96e8f8fb0",
        "repo": "hadoop",
        "unit_tests": [
            "TestUserGroupInformation.java"
        ]
    },
    "hadoop_d37b45d": {
        "bug_id": "hadoop_d37b45d",
        "commit": "https://github.com/apache/hadoop/commit/d37b45d613b768950d1cbe342961cd71776816ae",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/d37b45d613b768950d1cbe342961cd71776816ae/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java?ref=d37b45d613b768950d1cbe342961cd71776816ae",
                "deletions": 1,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java",
                "patch": "@@ -422,7 +422,7 @@ public JobPriority getPriority() throws IOException, InterruptedException {\n    * The user-specified job name.\n    */\n   public String getJobName() {\n-    if (state == JobState.DEFINE) {\n+    if (state == JobState.DEFINE || status == null) {\n       return super.getJobName();\n     }\n     ensureState(JobState.RUNNING);",
                "raw_url": "https://github.com/apache/hadoop/raw/d37b45d613b768950d1cbe342961cd71776816ae/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/Job.java",
                "sha": "45c065da5e48461b847901b607b16215f606b60d",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/hadoop/blob/d37b45d613b768950d1cbe342961cd71776816ae/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJob.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJob.java?ref=d37b45d613b768950d1cbe342961cd71776816ae",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJob.java",
                "patch": "@@ -26,6 +26,7 @@\n import org.apache.hadoop.io.Text;\n import org.apache.hadoop.mapred.JobConf;\n import org.apache.hadoop.mapreduce.JobStatus.State;\n+import org.apache.hadoop.mapreduce.lib.jobcontrol.ControlledJob;\n import org.apache.hadoop.mapreduce.protocol.ClientProtocol;\n import org.apache.hadoop.security.Credentials;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -54,6 +55,41 @@ public void testJobToString() throws IOException, InterruptedException {\n     Assert.assertNotNull(job.toString());\n   }\n \n+  @Test\n+  public void testUnexpectedJobStatus() throws Exception {\n+    Cluster cluster = mock(Cluster.class);\n+    JobID jobid = new JobID(\"1014873536921\", 6);\n+    ClientProtocol clientProtocol = mock(ClientProtocol.class);\n+    when(cluster.getClient()).thenReturn(clientProtocol);\n+    JobStatus status = new JobStatus(jobid, 0f, 0f, 0f, 0f,\n+        State.RUNNING, JobPriority.DEFAULT, \"root\",\n+        \"testUnexpectedJobStatus\", \"job file\", \"tracking URL\");\n+    when(clientProtocol.getJobStatus(jobid)).thenReturn(status);\n+    Job job = Job.getInstance(cluster, status, new JobConf());\n+\n+    // ensurer job status is RUNNING\n+    Assert.assertNotNull(job.getStatus());\n+    Assert.assertTrue(job.getStatus().getState() == State.RUNNING);\n+\n+    // when updating job status, job client could not retrieve\n+    // job status, and status reset to null\n+    when(clientProtocol.getJobStatus(jobid)).thenReturn(null);\n+\n+    try {\n+      job.updateStatus();\n+    } catch (IOException e) {\n+      Assert.assertTrue(e != null\n+          && e.getMessage().contains(\"Job status not available\"));\n+    }\n+\n+    try {\n+      ControlledJob cj = new ControlledJob(job, null);\n+      Assert.assertNotNull(cj.toString());\n+    } catch (NullPointerException e) {\n+      Assert.fail(\"job API fails with NPE\");\n+    }\n+  }\n+\n   @Test\n   public void testUGICredentialsPropogation() throws Exception {\n     Credentials creds = new Credentials();",
                "raw_url": "https://github.com/apache/hadoop/raw/d37b45d613b768950d1cbe342961cd71776816ae/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/src/test/java/org/apache/hadoop/mapreduce/TestJob.java",
                "sha": "60f390f44650d7559715b9f2f50a8185800642b2",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-6762. ControlledJob#toString failed with NPE when job status is not successfully updated (Weiwei Yang via Varun Saxena)",
        "parent": "https://github.com/apache/hadoop/commit/0faee62a0c8c1b8fd83227babfd00fbc2b26bddf",
        "repo": "hadoop",
        "unit_tests": [
            "TestJob.java"
        ]
    },
    "hadoop_d55f378": {
        "bug_id": "hadoop_d55f378",
        "commit": "https://github.com/apache/hadoop/commit/d55f3780fbf9308554ef3362c2be89651db43f46",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/d55f3780fbf9308554ef3362c2be89651db43f46/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=d55f3780fbf9308554ef3362c2be89651db43f46",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -179,6 +179,8 @@ Release 2.1.2 - UNRELEASED\n     YARN-1273. Fixed Distributed-shell to account for containers that failed\n     to start. (Hitesh Shah via vinodkv)\n \n+    YARN-1032. Fixed NPE in RackResolver. (Lohit Vijayarenu via acmurthy)\n+\n Release 2.1.1-beta - 2013-09-23\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/d55f3780fbf9308554ef3362c2be89651db43f46/hadoop-yarn-project/CHANGES.txt",
                "sha": "0c3a0307fbf19277a23271d1b9cc3981adc9398f",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/d55f3780fbf9308554ef3362c2be89651db43f46/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java?ref=d55f3780fbf9308554ef3362c2be89651db43f46",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java",
                "patch": "@@ -29,6 +29,7 @@\n import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.net.CachedDNSToSwitchMapping;\n import org.apache.hadoop.net.DNSToSwitchMapping;\n+import org.apache.hadoop.net.NetworkTopology;\n import org.apache.hadoop.net.Node;\n import org.apache.hadoop.net.NodeBase;\n import org.apache.hadoop.net.ScriptBasedMapping;\n@@ -98,8 +99,15 @@ private static Node coreResolve(String hostName) {\n     List <String> tmpList = new ArrayList<String>(1);\n     tmpList.add(hostName);\n     List <String> rNameList = dnsToSwitchMapping.resolve(tmpList);\n-    String rName = rNameList.get(0);\n-    LOG.info(\"Resolved \" + hostName + \" to \" + rName);\n+    String rName = null;\n+    if (rNameList == null || rNameList.get(0) == null) {\n+      rName = NetworkTopology.DEFAULT_RACK;\n+      LOG.info(\"Couldn't resolve \" + hostName + \". Falling back to \"\n+          + NetworkTopology.DEFAULT_RACK);\n+    } else {\n+      rName = rNameList.get(0);\n+      LOG.info(\"Resolved \" + hostName + \" to \" + rName);\n+    }\n     return new NodeBase(hostName, rName);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/d55f3780fbf9308554ef3362c2be89651db43f46/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/RackResolver.java",
                "sha": "cc2a56c3be6819cfaa5a05b6ccc38a0b54079607",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/d55f3780fbf9308554ef3362c2be89651db43f46/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java?ref=d55f3780fbf9308554ef3362c2be89651db43f46",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java",
                "patch": "@@ -28,13 +28,16 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.CommonConfigurationKeysPublic;\n import org.apache.hadoop.net.DNSToSwitchMapping;\n+import org.apache.hadoop.net.NetworkTopology;\n import org.apache.hadoop.net.Node;\n import org.junit.Assert;\n import org.junit.Test;\n \n public class TestRackResolver {\n \n   private static Log LOG = LogFactory.getLog(TestRackResolver.class);\n+  private static final String invalidHost = \"invalidHost\";\n+\n \n   public static final class MyResolver implements DNSToSwitchMapping {\n \n@@ -50,6 +53,11 @@\n       if (hostList.isEmpty()) {\n         return returnList;\n       }\n+      if (hostList.get(0).equals(invalidHost)) {\n+        // Simulate condition where resolving host returns null\n+        return null; \n+      }\n+        \n       LOG.info(\"Received resolve request for \"\n           + hostList.get(0));\n       if (hostList.get(0).equals(\"host1\")\n@@ -90,6 +98,8 @@ public void testCaching() {\n     Assert.assertEquals(\"/rack1\", node.getNetworkLocation());\n     node = RackResolver.resolve(\"host1\");\n     Assert.assertEquals(\"/rack1\", node.getNetworkLocation());\n+    node = RackResolver.resolve(invalidHost);\n+    Assert.assertEquals(NetworkTopology.DEFAULT_RACK, node.getNetworkLocation());\n   }\n \n }",
                "raw_url": "https://github.com/apache/hadoop/raw/d55f3780fbf9308554ef3362c2be89651db43f46/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/test/java/org/apache/hadoop/yarn/util/TestRackResolver.java",
                "sha": "70ca23c3a2e0b97fd7100ce0519947a806b43a79",
                "status": "modified"
            }
        ],
        "message": "YARN-1032. Fixed NPE in RackResolver. Contributed by Lohit Vijayarenu.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1529534 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/25361d56cf824ae2e68f45a6962146ba7bd54e01",
        "repo": "hadoop",
        "unit_tests": [
            "TestRackResolver.java"
        ]
    },
    "hadoop_d62e121": {
        "bug_id": "hadoop_d62e121",
        "commit": "https://github.com/apache/hadoop/commit/d62e121ffc0239e7feccc1e23ece92c5fac685f6",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop/blob/d62e121ffc0239e7feccc1e23ece92c5fac685f6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java?ref=d62e121ffc0239e7feccc1e23ece92c5fac685f6",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "patch": "@@ -1209,11 +1209,18 @@ private void updateSchedulerHealth(long now, FiCaSchedulerNode node,\n  }\n \n   @VisibleForTesting\n-  protected synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n+  public synchronized void allocateContainersToNode(FiCaSchedulerNode node) {\n     if (rmContext.isWorkPreservingRecoveryEnabled()\n         && !rmContext.isSchedulerReadyForAllocatingContainers()) {\n       return;\n     }\n+\n+    if (!nodeTracker.exists(node.getNodeID())) {\n+      LOG.info(\"Skipping scheduling as the node \" + node.getNodeID() +\n+          \" has been removed\");\n+      return;\n+    }\n+\n     // reset allocation and reservation stats before we start doing any work\n     updateSchedulerHealth(lastNodeUpdateTime, node,\n       new CSAssignment(Resources.none(), NodeType.NODE_LOCAL));",
                "raw_url": "https://github.com/apache/hadoop/raw/d62e121ffc0239e7feccc1e23ece92c5fac685f6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/CapacityScheduler.java",
                "sha": "bedf45570c4dc5e8d468460a74211f13a3bd4028",
                "status": "modified"
            },
            {
                "additions": 40,
                "blob_url": "https://github.com/apache/hadoop/blob/d62e121ffc0239e7feccc1e23ece92c5fac685f6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java?ref=d62e121ffc0239e7feccc1e23ece92c5fac685f6",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java",
                "patch": "@@ -3375,4 +3375,44 @@ public void handle(Event event) {\n     Assert.assertEquals(availableResource.getMemorySize(), 0);\n     Assert.assertEquals(availableResource.getVirtualCores(), 0);\n   }\n+\n+  @Test\n+  public void testSchedulingOnRemovedNode() throws Exception {\n+    Configuration conf = new YarnConfiguration();\n+    conf.setClass(YarnConfiguration.RM_SCHEDULER, CapacityScheduler.class,\n+        ResourceScheduler.class);\n+    conf.setBoolean(\n+        CapacitySchedulerConfiguration.SCHEDULE_ASYNCHRONOUSLY_ENABLE,\n+            false);\n+\n+    MockRM rm = new MockRM(conf);\n+    rm.start();\n+    RMApp app = rm.submitApp(100);\n+    rm.drainEvents();\n+\n+    MockNM nm1 = rm.registerNode(\"127.0.0.1:1234\", 10240, 10);\n+    MockAM am = MockRM.launchAndRegisterAM(app, rm, nm1);\n+\n+    //remove nm2 to keep am alive\n+    MockNM nm2 = rm.registerNode(\"127.0.0.1:1235\", 10240, 10);\n+\n+    am.allocate(ResourceRequest.ANY, 2048, 1, null);\n+\n+    CapacityScheduler scheduler =\n+        (CapacityScheduler) rm.getRMContext().getScheduler();\n+    FiCaSchedulerNode node =\n+        (FiCaSchedulerNode)\n+            scheduler.getNodeTracker().getNode(nm2.getNodeId());\n+    scheduler.handle(new NodeRemovedSchedulerEvent(\n+        rm.getRMContext().getRMNodes().get(nm2.getNodeId())));\n+    // schedulerNode is removed, try allocate a container\n+    scheduler.allocateContainersToNode(node);\n+\n+    AppAttemptRemovedSchedulerEvent appRemovedEvent1 =\n+        new AppAttemptRemovedSchedulerEvent(\n+            am.getApplicationAttemptId(),\n+            RMAppAttemptState.FINISHED, false);\n+    scheduler.handle(appRemovedEvent1);\n+    rm.stop();\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/d62e121ffc0239e7feccc1e23ece92c5fac685f6/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/TestCapacityScheduler.java",
                "sha": "d3567f5110c07b1d4ca14164e7e7e8a761e17871",
                "status": "modified"
            }
        ],
        "message": "YARN-5195. RM intermittently crashed with NPE while handling APP_ATTEMPT_REMOVED event when async-scheduling enabled in CapacityScheduler. (sandflee via wangda)",
        "parent": "https://github.com/apache/hadoop/commit/2d8d183b1992b82c4d8dd3d6b41a1964685d909e",
        "repo": "hadoop",
        "unit_tests": [
            "TestCapacityScheduler.java"
        ]
    },
    "hadoop_d6c8bad": {
        "bug_id": "hadoop_d6c8bad",
        "commit": "https://github.com/apache/hadoop/commit/d6c8bad86964dbad3cc810914f786c7c4722227a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/d6c8bad86964dbad3cc810914f786c7c4722227a/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=d6c8bad86964dbad3cc810914f786c7c4722227a",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -940,6 +940,8 @@ Release 2.8.0 - UNRELEASED\n     YARN-4255. container-executor does not clean up docker operation command files.\n     (Sidharta Seethana via vvasudev)\n \n+    YARN-4250. NPE in AppSchedulingInfo#isRequestLabelChanged. (Brahma Reddy Battula via rohithsharmaks)\n+\n Release 2.7.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/d6c8bad86964dbad3cc810914f786c7c4722227a/hadoop-yarn-project/CHANGES.txt",
                "sha": "de5fbe3c53b88d10931a9243a1aeb78a7c7c7b25",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/d6c8bad86964dbad3cc810914f786c7c4722227a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java?ref=d6c8bad86964dbad3cc810914f786c7c4722227a",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java",
                "patch": "@@ -417,7 +417,15 @@ private boolean isRequestLabelChanged(ResourceRequest requestOne,\n       ResourceRequest requestTwo) {\n     String requestOneLabelExp = requestOne.getNodeLabelExpression();\n     String requestTwoLabelExp = requestTwo.getNodeLabelExpression();\n-    return (!(requestOneLabelExp.equals(requestTwoLabelExp)));\n+    // First request label expression can be null and second request\n+    // is not null then we have to consider it as changed.\n+    if ((null == requestOneLabelExp) && (null != requestTwoLabelExp)) {\n+      return true;\n+    }\n+    // If the label is not matching between both request when\n+    // requestOneLabelExp is not null.\n+    return ((null != requestOneLabelExp) && !(requestOneLabelExp\n+        .equals(requestTwoLabelExp)));\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop/raw/d6c8bad86964dbad3cc810914f786c7c4722227a/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/AppSchedulingInfo.java",
                "sha": "31560867a6266c0716b86e30649bedd391d6f261",
                "status": "modified"
            }
        ],
        "message": "YARN-4250. NPE in AppSchedulingInfo#isRequestLabelChanged. (Brahma Reddy Battula via rohithsharmaks)",
        "parent": "https://github.com/apache/hadoop/commit/da1ee078f9d3c2c25c51d0b392b0925821c42ad3",
        "repo": "hadoop",
        "unit_tests": [
            "TestAppSchedulingInfo.java"
        ]
    },
    "hadoop_d9852eb": {
        "bug_id": "hadoop_d9852eb",
        "commit": "https://github.com/apache/hadoop/commit/d9852eb5897a25323ab0302c2c0decb61d310e5e",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/d9852eb5897a25323ab0302c2c0decb61d310e5e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java?ref=d9852eb5897a25323ab0302c2c0decb61d310e5e",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "patch": "@@ -1198,6 +1198,7 @@ public Service getStatus(String serviceName)\n     ServiceApiUtil.validateNameFormat(serviceName, getConfig());\n     Service appSpec = new Service();\n     appSpec.setName(serviceName);\n+    appSpec.setState(ServiceState.STOPPED);\n     ApplicationId currentAppId = getAppId(serviceName);\n     if (currentAppId == null) {\n       LOG.info(\"Service {} does not have an application ID\", serviceName);",
                "raw_url": "https://github.com/apache/hadoop/raw/d9852eb5897a25323ab0302c2c0decb61d310e5e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/client/ServiceClient.java",
                "sha": "0ab332280f2447a16347dfd1b81b2ba2fa399140",
                "status": "modified"
            }
        ],
        "message": "YARN-8357.  Fixed NPE when YARN service is saved and not deployed.\n            Contributed by Chandni Singh",
        "parent": "https://github.com/apache/hadoop/commit/7ff5a40218241ad2380595175a493794129a7402",
        "repo": "hadoop",
        "unit_tests": [
            "ServiceClientTest.java",
            "TestServiceClient.java"
        ]
    },
    "hadoop_da2fb2b": {
        "bug_id": "hadoop_da2fb2b",
        "commit": "https://github.com/apache/hadoop/commit/da2fb2bc46bddf42d79c6d7664cbf0311973709e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/da2fb2bc46bddf42d79c6d7664cbf0311973709e/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=da2fb2bc46bddf42d79c6d7664cbf0311973709e",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -510,6 +510,9 @@ Release 2.7.0 - UNRELEASED\n     YARN-3089. LinuxContainerExecutor does not handle file arguments to\n     deleteAsUser (Eric Payne via jlowe)\n \n+    YARN-3143. RM Apps REST API can return NPE or entries missing id and other\n+    fields (jlowe)\n+\n Release 2.6.0 - 2014-11-18\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/da2fb2bc46bddf42d79c6d7664cbf0311973709e/hadoop-yarn-project/CHANGES.txt",
                "sha": "7951eef27ad91b2871baf875595884b1d041fff8",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/da2fb2bc46bddf42d79c6d7664cbf0311973709e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebServices.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebServices.java?ref=da2fb2bc46bddf42d79c6d7664cbf0311973709e",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebServices.java",
                "patch": "@@ -24,7 +24,6 @@\n import java.nio.ByteBuffer;\n import java.security.Principal;\n import java.security.PrivilegedExceptionAction;\n-import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collection;\n import java.util.EnumSet;\n@@ -169,6 +168,12 @@ public RMWebServices(final ResourceManager rm, Configuration conf) {\n     this.conf = conf;\n   }\n \n+  RMWebServices(ResourceManager rm, Configuration conf,\n+      HttpServletResponse response) {\n+    this(rm, conf);\n+    this.response = response;\n+  }\n+\n   protected Boolean hasAccess(RMApp app, HttpServletRequest hsr) {\n     // Check for the authorization.\n     UserGroupInformation callerUGI = getCallerUserGroupInformation(hsr, true);\n@@ -459,6 +464,9 @@ public AppsInfo getApps(@Context HttpServletRequest hsr,\n     AppsInfo allApps = new AppsInfo();\n     for (ApplicationReport report : appReports) {\n       RMApp rmapp = apps.get(report.getApplicationId());\n+      if (rmapp == null) {\n+        continue;\n+      }\n \n       if (finalStatusQuery != null && !finalStatusQuery.isEmpty()) {\n         FinalApplicationStatus.valueOf(finalStatusQuery);",
                "raw_url": "https://github.com/apache/hadoop/raw/da2fb2bc46bddf42d79c6d7664cbf0311973709e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/RMWebServices.java",
                "sha": "1834b6a1a7dcaf975c40eb47546bbe0e8ef6cc4d",
                "status": "modified"
            },
            {
                "additions": 55,
                "blob_url": "https://github.com/apache/hadoop/blob/da2fb2bc46bddf42d79c6d7664cbf0311973709e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServices.java",
                "changes": 55,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServices.java?ref=da2fb2bc46bddf42d79c6d7664cbf0311973709e",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServices.java",
                "patch": "@@ -21,24 +21,40 @@\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n+import static org.mockito.Matchers.anyBoolean;\n+import static org.mockito.Matchers.isA;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n \n import java.io.StringReader;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.Set;\n \n+import javax.servlet.http.HttpServletRequest;\n+import javax.servlet.http.HttpServletResponse;\n import javax.ws.rs.core.MediaType;\n import javax.xml.parsers.DocumentBuilder;\n import javax.xml.parsers.DocumentBuilderFactory;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.service.Service.STATE;\n import org.apache.hadoop.util.VersionInfo;\n+import org.apache.hadoop.yarn.api.protocolrecords.GetApplicationsRequest;\n+import org.apache.hadoop.yarn.api.protocolrecords.GetApplicationsResponse;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.api.records.ApplicationReport;\n import org.apache.hadoop.yarn.api.records.QueueState;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n+import org.apache.hadoop.yarn.server.resourcemanager.ClientRMService;\n import org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.MockRM;\n+import org.apache.hadoop.yarn.server.resourcemanager.RMContextImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.ResourceManager;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.fifo.FifoScheduler;\n+import org.apache.hadoop.yarn.server.resourcemanager.webapp.dao.AppsInfo;\n import org.apache.hadoop.yarn.util.YarnVersionInfo;\n import org.apache.hadoop.yarn.webapp.GenericExceptionHandler;\n import org.apache.hadoop.yarn.webapp.JerseyTestBase;\n@@ -586,4 +602,43 @@ public void verifyClusterSchedulerFifoGeneric(String type, String state,\n \n   }\n \n+  // Test the scenario where the RM removes an app just as we try to\n+  // look at it in the apps list\n+  @Test\n+  public void testAppsRace() throws Exception {\n+    // mock up an RM that returns app reports for apps that don't exist\n+    // in the RMApps list\n+    ApplicationId appId = ApplicationId.newInstance(1, 1);\n+    ApplicationReport mockReport = mock(ApplicationReport.class);\n+    when(mockReport.getApplicationId()).thenReturn(appId);\n+    GetApplicationsResponse mockAppsResponse =\n+        mock(GetApplicationsResponse.class);\n+    when(mockAppsResponse.getApplicationList())\n+      .thenReturn(Arrays.asList(new ApplicationReport[] { mockReport }));\n+    ClientRMService mockClientSvc = mock(ClientRMService.class);\n+    when(mockClientSvc.getApplications(isA(GetApplicationsRequest.class),\n+        anyBoolean())).thenReturn(mockAppsResponse);\n+    ResourceManager mockRM = mock(ResourceManager.class);\n+    RMContextImpl rmContext = new RMContextImpl(null, null, null, null, null,\n+        null, null, null, null, null);\n+    when(mockRM.getRMContext()).thenReturn(rmContext);\n+    when(mockRM.getClientRMService()).thenReturn(mockClientSvc);\n+\n+    RMWebServices webSvc = new RMWebServices(mockRM, new Configuration(),\n+        mock(HttpServletResponse.class));\n+\n+    final Set<String> emptySet =\n+        Collections.unmodifiableSet(Collections.<String>emptySet());\n+\n+    // verify we don't get any apps when querying\n+    HttpServletRequest mockHsr = mock(HttpServletRequest.class);\n+    AppsInfo appsInfo = webSvc.getApps(mockHsr, null, emptySet, null,\n+        null, null, null, null, null, null, null, emptySet, emptySet);\n+    assertTrue(appsInfo.getApps().isEmpty());\n+\n+    // verify we don't get an NPE when specifying a final status query\n+    appsInfo = webSvc.getApps(mockHsr, null, emptySet, \"FAILED\",\n+        null, null, null, null, null, null, null, emptySet, emptySet);\n+    assertTrue(appsInfo.getApps().isEmpty());\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/da2fb2bc46bddf42d79c6d7664cbf0311973709e/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/webapp/TestRMWebServices.java",
                "sha": "298246ca301e2da4a545ce5ab36136bddef6e8c8",
                "status": "modified"
            }
        ],
        "message": "YARN-3143. RM Apps REST API can return NPE or entries missing id and other fields. Contributed by Jason Lowe",
        "parent": "https://github.com/apache/hadoop/commit/5c79439568ff0c73062cf09d87f1e739703c7dc0",
        "repo": "hadoop",
        "unit_tests": [
            "TestRMWebServices.java"
        ]
    },
    "hadoop_da6f1b8": {
        "bug_id": "hadoop_da6f1b8",
        "commit": "https://github.com/apache/hadoop/commit/da6f1b88dd47e22b24d44f6fc8bbee73e85746f7",
        "file": [
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop/blob/da6f1b88dd47e22b24d44f6fc8bbee73e85746f7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java?ref=da6f1b88dd47e22b24d44f6fc8bbee73e85746f7",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "patch": "@@ -1169,8 +1169,25 @@ public void notifyNamenodeDeletedBlock(ExtendedBlock block, String storageUuid)\n    * Report a bad block which is hosted on the local DN.\n    */\n   public void reportBadBlocks(ExtendedBlock block) throws IOException{\n-    BPOfferService bpos = getBPOSForBlock(block);\n     FsVolumeSpi volume = getFSDataset().getVolume(block);\n+    if (volume == null) {\n+      LOG.warn(\"Cannot find FsVolumeSpi to report bad block: \" + block);\n+      return;\n+    }\n+    reportBadBlocks(block, volume);\n+  }\n+\n+  /**\n+   * Report a bad block which is hosted on the local DN.\n+   *\n+   * @param block the bad block which is hosted on the local DN\n+   * @param volume the volume that block is stored in and the volume\n+   *        must not be null\n+   * @throws IOException\n+   */\n+  public void reportBadBlocks(ExtendedBlock block, FsVolumeSpi volume)\n+      throws IOException {\n+    BPOfferService bpos = getBPOSForBlock(block);\n     bpos.reportBadBlocks(\n         block, volume.getStorageID(), volume.getStorageType());\n   }\n@@ -2101,6 +2118,10 @@ public void decrementXmitsInProgress() {\n   private void reportBadBlock(final BPOfferService bpos,\n       final ExtendedBlock block, final String msg) {\n     FsVolumeSpi volume = getFSDataset().getVolume(block);\n+    if (volume == null) {\n+      LOG.warn(\"Cannot find FsVolumeSpi to report bad block: \" + block);\n+      return;\n+    }\n     bpos.reportBadBlocks(\n         block, volume.getStorageID(), volume.getStorageType());\n     LOG.warn(msg);",
                "raw_url": "https://github.com/apache/hadoop/raw/da6f1b88dd47e22b24d44f6fc8bbee73e85746f7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "sha": "1cd2dee669a13f62969de803d646a99ebbc14ea1",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/da6f1b88dd47e22b24d44f6fc8bbee73e85746f7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java?ref=da6f1b88dd47e22b24d44f6fc8bbee73e85746f7",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java",
                "patch": "@@ -283,7 +283,7 @@ public void handle(ExtendedBlock block, IOException e) {\n       }\n       LOG.warn(\"Reporting bad {} on {}\", block, volume.getBasePath());\n       try {\n-        scanner.datanode.reportBadBlocks(block);\n+        scanner.datanode.reportBadBlocks(block, volume);\n       } catch (IOException ie) {\n         // This is bad, but not bad enough to shut down the scanner.\n         LOG.warn(\"Cannot report bad \" + block.getBlockId(), e);",
                "raw_url": "https://github.com/apache/hadoop/raw/da6f1b88dd47e22b24d44f6fc8bbee73e85746f7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/VolumeScanner.java",
                "sha": "7a9ecf2aaa7a40542da26089064dc22953391532",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/da6f1b88dd47e22b24d44f6fc8bbee73e85746f7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java?ref=da6f1b88dd47e22b24d44f6fc8bbee73e85746f7",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "patch": "@@ -2417,7 +2417,8 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,\n       LOG.warn(\"Reporting the block \" + corruptBlock\n           + \" as corrupt due to length mismatch\");\n       try {\n-        datanode.reportBadBlocks(new ExtendedBlock(bpid, corruptBlock));  \n+        datanode.reportBadBlocks(new ExtendedBlock(bpid, corruptBlock),\n+            memBlockInfo.getVolume());\n       } catch (IOException e) {\n         LOG.warn(\"Failed to repot bad block \" + corruptBlock, e);\n       }",
                "raw_url": "https://github.com/apache/hadoop/raw/da6f1b88dd47e22b24d44f6fc8bbee73e85746f7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "sha": "c0f2fbdb7b14f50d5cbf7e417c65532f7f681f1d",
                "status": "modified"
            },
            {
                "additions": 43,
                "blob_url": "https://github.com/apache/hadoop/blob/da6f1b88dd47e22b24d44f6fc8bbee73e85746f7/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java?ref=da6f1b88dd47e22b24d44f6fc8bbee73e85746f7",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java",
                "patch": "@@ -35,6 +35,7 @@\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n import org.apache.hadoop.hdfs.protocol.LocatedBlock;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerTestUtil;\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants;\n import org.apache.hadoop.hdfs.server.common.Storage;\n import org.apache.hadoop.hdfs.server.common.StorageInfo;\n@@ -98,6 +99,7 @@\n import static org.mockito.Mockito.spy;\n import static org.mockito.Mockito.verify;\n import static org.mockito.Mockito.when;\n+\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -691,4 +693,45 @@ public void testCleanShutdownOfVolume() throws Exception {\n     cluster.shutdown();\n     }\n   }\n+\n+  @Test(timeout = 30000)\n+  public void testReportBadBlocks() throws Exception {\n+    boolean threwException = false;\n+    MiniDFSCluster cluster = null;\n+    try {\n+      Configuration config = new HdfsConfiguration();\n+      cluster = new MiniDFSCluster.Builder(config).numDataNodes(1).build();\n+      cluster.waitActive();\n+\n+      Assert.assertEquals(0, cluster.getNamesystem().getCorruptReplicaBlocks());\n+      DataNode dataNode = cluster.getDataNodes().get(0);\n+      ExtendedBlock block =\n+          new ExtendedBlock(cluster.getNamesystem().getBlockPoolId(), 0);\n+      try {\n+        // Test the reportBadBlocks when the volume is null\n+        dataNode.reportBadBlocks(block);\n+      } catch (NullPointerException npe) {\n+        threwException = true;\n+      }\n+      Thread.sleep(3000);\n+      Assert.assertFalse(threwException);\n+      Assert.assertEquals(0, cluster.getNamesystem().getCorruptReplicaBlocks());\n+\n+      FileSystem fs = cluster.getFileSystem();\n+      Path filePath = new Path(\"testData\");\n+      DFSTestUtil.createFile(fs, filePath, 1, (short) 1, 0);\n+\n+      block = DFSTestUtil.getFirstBlock(fs, filePath);\n+      // Test for the overloaded method reportBadBlocks\n+      dataNode.reportBadBlocks(block, dataNode.getFSDataset()\n+          .getFsVolumeReferences().get(0));\n+      Thread.sleep(3000);\n+      BlockManagerTestUtil.updateState(cluster.getNamesystem()\n+          .getBlockManager());\n+      // Verify the bad block has been reported to namenode\n+      Assert.assertEquals(1, cluster.getNamesystem().getCorruptReplicaBlocks());\n+    } finally {\n+      cluster.shutdown();\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/da6f1b88dd47e22b24d44f6fc8bbee73e85746f7/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/TestFsDatasetImpl.java",
                "sha": "e73a6127df4bcb5ede7162dbd897957d20e25a6a",
                "status": "modified"
            }
        ],
        "message": "HDFS-10512. VolumeScanner may terminate due to NPE in DataNode.reportBadBlocks. Contributed by Wei-Chiu Chuang and Yiqun Lin.",
        "parent": "https://github.com/apache/hadoop/commit/932aed64d77edcc8483a95c1ce31a4c9ae679446",
        "repo": "hadoop",
        "unit_tests": [
            "TestFsDatasetImpl.java"
        ]
    },
    "hadoop_db334bb": {
        "bug_id": "hadoop_db334bb",
        "commit": "https://github.com/apache/hadoop/commit/db334bb8625da97c7e518cbcf477530c7ba7001e",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/db334bb8625da97c7e518cbcf477530c7ba7001e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=db334bb8625da97c7e518cbcf477530c7ba7001e",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -783,6 +783,9 @@ Release 2.6.1 - UNRELEASED\n     HDFS-7443. Datanode upgrade to BLOCKID_BASED_LAYOUT fails if duplicate\n     block files are present in the same volume (cmccabe)\n \n+    HDFS-3443. Fix NPE when namenode transition to active during startup by\n+    adding checkNNStartup() in NameNodeRpcServer.  (Vinayakumar B via szetszwo)\n+\n Release 2.6.0 - 2014-11-18\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/db334bb8625da97c7e518cbcf477530c7ba7001e/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "0a301f8373328bff0268b07ae38e79dfb3eaeb92",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/db334bb8625da97c7e518cbcf477530c7ba7001e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=db334bb8625da97c7e518cbcf477530c7ba7001e",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -79,6 +79,7 @@\n import java.util.Arrays;\n import java.util.Collection;\n import java.util.List;\n+import java.util.concurrent.atomic.AtomicBoolean;\n \n import static org.apache.hadoop.fs.CommonConfigurationKeysPublic.FS_DEFAULT_NAME_KEY;\n import static org.apache.hadoop.fs.CommonConfigurationKeysPublic.FS_TRASH_INTERVAL_DEFAULT;\n@@ -265,6 +266,7 @@ public long getProtocolVersion(String protocol,\n   private final boolean haEnabled;\n   private final HAContext haContext;\n   protected final boolean allowStaleStandbyReads;\n+  private AtomicBoolean started = new AtomicBoolean(false); \n \n   \n   /** httpServer */\n@@ -775,6 +777,7 @@ protected NameNode(Configuration conf, NamenodeRole role)\n       this.stop();\n       throw e;\n     }\n+    this.started.set(true);\n   }\n \n   protected HAState createHAState(StartupOption startOpt) {\n@@ -1743,7 +1746,14 @@ public boolean isStandbyState() {\n   public boolean isActiveState() {\n     return (state.equals(ACTIVE_STATE));\n   }\n-  \n+\n+  /**\n+   * Returns whether the NameNode is completely started\n+   */\n+  boolean isStarted() {\n+    return this.started.get();\n+  }\n+\n   /**\n    * Check that a request to change this node's HA state is valid.\n    * In particular, verifies that, if auto failover is enabled, non-forced",
                "raw_url": "https://github.com/apache/hadoop/raw/db334bb8625da97c7e518cbcf477530c7ba7001e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "fea7c62be4c2d4888ea3b63b53056c9814944557",
                "status": "modified"
            },
            {
                "additions": 151,
                "blob_url": "https://github.com/apache/hadoop/blob/db334bb8625da97c7e518cbcf477530c7ba7001e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "changes": 187,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java?ref=db334bb8625da97c7e518cbcf477530c7ba7001e",
                "deletions": 36,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "patch": "@@ -69,7 +69,6 @@\n import org.apache.hadoop.hdfs.DFSConfigKeys;\n import org.apache.hadoop.hdfs.DFSUtil;\n import org.apache.hadoop.hdfs.HDFSPolicyProvider;\n-import org.apache.hadoop.hdfs.inotify.Event;\n import org.apache.hadoop.hdfs.inotify.EventBatch;\n import org.apache.hadoop.hdfs.inotify.EventBatchList;\n import org.apache.hadoop.hdfs.protocol.AclException;\n@@ -479,12 +478,14 @@ public BlocksWithLocations getBlocks(DatanodeInfo datanode, long size)\n       throw new IllegalArgumentException(\n         \"Unexpected not positive size: \"+size);\n     }\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.getBlockManager().getBlocks(datanode, size); \n   }\n \n   @Override // NamenodeProtocol\n   public ExportedBlockKeys getBlockKeys() throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.getBlockManager().getBlockKeys();\n   }\n@@ -493,6 +494,7 @@ public ExportedBlockKeys getBlockKeys() throws IOException {\n   public void errorReport(NamenodeRegistration registration,\n                           int errorCode, \n                           String msg) throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.UNCHECKED);\n     namesystem.checkSuperuserPrivilege();\n     verifyRequest(registration);\n@@ -505,6 +507,7 @@ public void errorReport(NamenodeRegistration registration,\n   @Override // NamenodeProtocol\n   public NamenodeRegistration registerSubordinateNamenode(\n       NamenodeRegistration registration) throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     verifyLayoutVersion(registration.getVersion());\n     NamenodeRegistration myRegistration = nn.setRegistration();\n@@ -514,7 +517,8 @@ public NamenodeRegistration registerSubordinateNamenode(\n \n   @Override // NamenodeProtocol\n   public NamenodeCommand startCheckpoint(NamenodeRegistration registration)\n-  throws IOException {\n+      throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     verifyRequest(registration);\n     if(!nn.isRole(NamenodeRole.NAMENODE))\n@@ -537,6 +541,7 @@ public NamenodeCommand startCheckpoint(NamenodeRegistration registration)\n   @Override // NamenodeProtocol\n   public void endCheckpoint(NamenodeRegistration registration,\n                             CheckpointSignature sig) throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n@@ -554,18 +559,21 @@ public void endCheckpoint(NamenodeRegistration registration,\n   @Override // ClientProtocol\n   public Token<DelegationTokenIdentifier> getDelegationToken(Text renewer)\n       throws IOException {\n+    checkNNStartup();\n     return namesystem.getDelegationToken(renewer);\n   }\n \n   @Override // ClientProtocol\n   public long renewDelegationToken(Token<DelegationTokenIdentifier> token)\n       throws InvalidToken, IOException {\n+    checkNNStartup();\n     return namesystem.renewDelegationToken(token);\n   }\n \n   @Override // ClientProtocol\n   public void cancelDelegationToken(Token<DelegationTokenIdentifier> token)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.cancelDelegationToken(token);\n   }\n   \n@@ -574,13 +582,15 @@ public LocatedBlocks getBlockLocations(String src,\n                                           long offset, \n                                           long length) \n       throws IOException {\n+    checkNNStartup();\n     metrics.incrGetBlockLocations();\n     return namesystem.getBlockLocations(getClientMachine(), \n                                         src, offset, length);\n   }\n   \n   @Override // ClientProtocol\n   public FsServerDefaults getServerDefaults() throws IOException {\n+    checkNNStartup();\n     return namesystem.getServerDefaults();\n   }\n \n@@ -590,6 +600,7 @@ public HdfsFileStatus create(String src, FsPermission masked,\n       boolean createParent, short replication, long blockSize, \n       CryptoProtocolVersion[] supportedVersions)\n       throws IOException {\n+    checkNNStartup();\n     String clientMachine = getClientMachine();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.create: file \"\n@@ -624,6 +635,7 @@ public HdfsFileStatus create(String src, FsPermission masked,\n   @Override // ClientProtocol\n   public LastBlockWithStatus append(String src, String clientName) \n       throws IOException {\n+    checkNNStartup();\n     String clientMachine = getClientMachine();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.append: file \"\n@@ -649,36 +661,42 @@ public LastBlockWithStatus append(String src, String clientName)\n \n   @Override // ClientProtocol\n   public boolean recoverLease(String src, String clientName) throws IOException {\n+    checkNNStartup();\n     String clientMachine = getClientMachine();\n     return namesystem.recoverLease(src, clientName, clientMachine);\n   }\n \n   @Override // ClientProtocol\n   public boolean setReplication(String src, short replication) \n-    throws IOException {  \n+    throws IOException {\n+    checkNNStartup();\n     return namesystem.setReplication(src, replication);\n   }\n \n   @Override\n   public void setStoragePolicy(String src, String policyName)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.setStoragePolicy(src, policyName);\n   }\n \n   @Override\n   public BlockStoragePolicy[] getStoragePolicies() throws IOException {\n+    checkNNStartup();\n     return namesystem.getStoragePolicies();\n   }\n \n   @Override // ClientProtocol\n   public void setPermission(String src, FsPermission permissions)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.setPermission(src, permissions);\n   }\n \n   @Override // ClientProtocol\n   public void setOwner(String src, String username, String groupname)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.setOwner(src, username, groupname);\n   }\n   \n@@ -687,6 +705,7 @@ public LocatedBlock addBlock(String src, String clientName,\n       ExtendedBlock previous, DatanodeInfo[] excludedNodes, long fileId,\n       String[] favoredNodes)\n       throws IOException {\n+    checkNNStartup();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*BLOCK* NameNode.addBlock: file \" + src\n           + \" fileId=\" + fileId + \" for \" + clientName);\n@@ -714,6 +733,7 @@ public LocatedBlock getAdditionalDatanode(final String src,\n       final DatanodeInfo[] excludes,\n       final int numAdditionalNodes, final String clientName\n       ) throws IOException {\n+    checkNNStartup();\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"getAdditionalDatanode: src=\" + src\n           + \", fileId=\" + fileId\n@@ -742,6 +762,7 @@ public LocatedBlock getAdditionalDatanode(final String src,\n   @Override // ClientProtocol\n   public void abandonBlock(ExtendedBlock b, long fileId, String src,\n         String holder) throws IOException {\n+    checkNNStartup();\n     if(stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*BLOCK* NameNode.abandonBlock: \"\n           +b+\" of file \"+src);\n@@ -755,6 +776,7 @@ public void abandonBlock(ExtendedBlock b, long fileId, String src,\n   public boolean complete(String src, String clientName,\n                           ExtendedBlock last,  long fileId)\n       throws IOException {\n+    checkNNStartup();\n     if(stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.complete: \"\n           + src + \" fileId=\" + fileId +\" for \" + clientName);\n@@ -770,12 +792,14 @@ public boolean complete(String src, String clientName,\n    */\n   @Override // ClientProtocol, DatanodeProtocol\n   public void reportBadBlocks(LocatedBlock[] blocks) throws IOException {\n+    checkNNStartup();\n     namesystem.reportBadBlocks(blocks);\n   }\n \n   @Override // ClientProtocol\n   public LocatedBlock updateBlockForPipeline(ExtendedBlock block, String clientName)\n       throws IOException {\n+    checkNNStartup();\n     return namesystem.updateBlockForPipeline(block, clientName);\n   }\n \n@@ -784,6 +808,7 @@ public LocatedBlock updateBlockForPipeline(ExtendedBlock block, String clientNam\n   public void updatePipeline(String clientName, ExtendedBlock oldBlock,\n       ExtendedBlock newBlock, DatanodeID[] newNodes, String[] newStorageIDs)\n       throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -805,19 +830,22 @@ public void commitBlockSynchronization(ExtendedBlock block,\n       boolean closeFile, boolean deleteblock, DatanodeID[] newtargets,\n       String[] newtargetstorages)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.commitBlockSynchronization(block, newgenerationstamp,\n         newlength, closeFile, deleteblock, newtargets, newtargetstorages);\n   }\n   \n   @Override // ClientProtocol\n   public long getPreferredBlockSize(String filename) \n       throws IOException {\n+    checkNNStartup();\n     return namesystem.getPreferredBlockSize(filename);\n   }\n     \n   @Deprecated\n   @Override // ClientProtocol\n   public boolean rename(String src, String dst) throws IOException {\n+    checkNNStartup();\n     if(stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.rename: \" + src + \" to \" + dst);\n     }\n@@ -845,6 +873,7 @@ public boolean rename(String src, String dst) throws IOException {\n   \n   @Override // ClientProtocol\n   public void concat(String trg, String[] src) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -862,6 +891,7 @@ public void concat(String trg, String[] src) throws IOException {\n   @Override // ClientProtocol\n   public void rename2(String src, String dst, Options.Rename... options)\n       throws IOException {\n+    checkNNStartup();\n     if(stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.rename: \" + src + \" to \" + dst);\n     }\n@@ -886,6 +916,7 @@ public void rename2(String src, String dst, Options.Rename... options)\n   @Override // ClientProtocol\n   public boolean truncate(String src, long newLength, String clientName)\n       throws IOException {\n+    checkNNStartup();\n     if(stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.truncate: \" + src + \" to \" +\n           newLength);\n@@ -901,6 +932,7 @@ public boolean truncate(String src, long newLength, String clientName)\n \n   @Override // ClientProtocol\n   public boolean delete(String src, boolean recursive) throws IOException {\n+    checkNNStartup();\n     if (stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* Namenode.delete: src=\" + src\n           + \", recursive=\" + recursive);\n@@ -935,6 +967,7 @@ private boolean checkPathLength(String src) {\n   @Override // ClientProtocol\n   public boolean mkdirs(String src, FsPermission masked, boolean createParent)\n       throws IOException {\n+    checkNNStartup();\n     if(stateChangeLog.isDebugEnabled()) {\n       stateChangeLog.debug(\"*DIR* NameNode.mkdirs: \" + src);\n     }\n@@ -949,12 +982,14 @@ public boolean mkdirs(String src, FsPermission masked, boolean createParent)\n \n   @Override // ClientProtocol\n   public void renewLease(String clientName) throws IOException {\n+    checkNNStartup();\n     namesystem.renewLease(clientName);        \n   }\n \n   @Override // ClientProtocol\n   public DirectoryListing getListing(String src, byte[] startAfter,\n       boolean needLocation) throws IOException {\n+    checkNNStartup();\n     DirectoryListing files = namesystem.getListing(\n         src, startAfter, needLocation);\n     if (files != null) {\n@@ -966,44 +1001,51 @@ public DirectoryListing getListing(String src, byte[] startAfter,\n \n   @Override // ClientProtocol\n   public HdfsFileStatus getFileInfo(String src)  throws IOException {\n+    checkNNStartup();\n     metrics.incrFileInfoOps();\n     return namesystem.getFileInfo(src, true);\n   }\n   \n   @Override // ClientProtocol\n   public boolean isFileClosed(String src) throws IOException{\n+    checkNNStartup();\n     return namesystem.isFileClosed(src);\n   }\n   \n   @Override // ClientProtocol\n-  public HdfsFileStatus getFileLinkInfo(String src) throws IOException { \n+  public HdfsFileStatus getFileLinkInfo(String src) throws IOException {\n+    checkNNStartup();\n     metrics.incrFileInfoOps();\n     return namesystem.getFileInfo(src, false);\n   }\n   \n   @Override // ClientProtocol\n   public long[] getStats() throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.READ);\n     return namesystem.getStats();\n   }\n \n   @Override // ClientProtocol\n   public DatanodeInfo[] getDatanodeReport(DatanodeReportType type)\n   throws IOException {\n+    checkNNStartup();\n     DatanodeInfo results[] = namesystem.datanodeReport(type);\n     return results;\n   }\n     \n   @Override // ClientProtocol\n   public DatanodeStorageReport[] getDatanodeStorageReport(\n       DatanodeReportType type) throws IOException {\n+    checkNNStartup();\n     final DatanodeStorageReport[] reports = namesystem.getDatanodeStorageReport(type);\n     return reports;\n   }\n \n   @Override // ClientProtocol\n   public boolean setSafeMode(SafeModeAction action, boolean isChecked)\n       throws IOException {\n+    checkNNStartup();\n     OperationCategory opCategory = OperationCategory.UNCHECKED;\n     if (isChecked) {\n       if (action == SafeModeAction.SAFEMODE_GET) {\n@@ -1018,11 +1060,13 @@ public boolean setSafeMode(SafeModeAction action, boolean isChecked)\n \n   @Override // ClientProtocol\n   public boolean restoreFailedStorage(String arg) throws IOException { \n+    checkNNStartup();\n     return namesystem.restoreFailedStorage(arg);\n   }\n \n   @Override // ClientProtocol\n   public void saveNamespace() throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -1038,50 +1082,58 @@ public void saveNamespace() throws IOException {\n   \n   @Override // ClientProtocol\n   public long rollEdits() throws AccessControlException, IOException {\n+    checkNNStartup();\n     CheckpointSignature sig = namesystem.rollEditLog();\n     return sig.getCurSegmentTxId();\n   }\n \n   @Override // ClientProtocol\n   public void refreshNodes() throws IOException {\n+    checkNNStartup();\n     namesystem.refreshNodes();\n   }\n \n   @Override // NamenodeProtocol\n   public long getTransactionID() throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.UNCHECKED);\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.getFSImage().getLastAppliedOrWrittenTxId();\n   }\n   \n   @Override // NamenodeProtocol\n   public long getMostRecentCheckpointTxId() throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.UNCHECKED);\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.getFSImage().getMostRecentCheckpointTxId();\n   }\n   \n   @Override // NamenodeProtocol\n   public CheckpointSignature rollEditLog() throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.rollEditLog();\n   }\n   \n   @Override // NamenodeProtocol\n   public RemoteEditLogManifest getEditLogManifest(long sinceTxId)\n-  throws IOException {\n+      throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.READ);\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.getEditLog().getEditLogManifest(sinceTxId);\n   }\n     \n   @Override // ClientProtocol\n   public void finalizeUpgrade() throws IOException {\n+    checkNNStartup();\n     namesystem.finalizeUpgrade();\n   }\n \n   @Override // ClientProtocol\n   public RollingUpgradeInfo rollingUpgrade(RollingUpgradeAction action) throws IOException {\n+    checkNNStartup();\n     LOG.info(\"rollingUpgrade \" + action);\n     switch(action) {\n     case QUERY:\n@@ -1098,12 +1150,14 @@ public RollingUpgradeInfo rollingUpgrade(RollingUpgradeAction action) throws IOE\n \n   @Override // ClientProtocol\n   public void metaSave(String filename) throws IOException {\n+    checkNNStartup();\n     namesystem.metaSave(filename);\n   }\n \n   @Override // ClientProtocol\n   public CorruptFileBlocks listCorruptFileBlocks(String path, String cookie)\n       throws IOException {\n+    checkNNStartup();\n     String[] cookieTab = new String[] { cookie };\n     Collection<FSNamesystem.CorruptFileBlockInfo> fbs =\n       namesystem.listCorruptFileBlocks(path, cookieTab);\n@@ -1124,36 +1178,42 @@ public CorruptFileBlocks listCorruptFileBlocks(String path, String cookie)\n    */\n   @Override // ClientProtocol\n   public void setBalancerBandwidth(long bandwidth) throws IOException {\n+    checkNNStartup();\n     namesystem.setBalancerBandwidth(bandwidth);\n   }\n   \n   @Override // ClientProtocol\n   public ContentSummary getContentSummary(String path) throws IOException {\n+    checkNNStartup();\n     return namesystem.getContentSummary(path);\n   }\n \n   @Override // ClientProtocol\n   public void setQuota(String path, long namespaceQuota, long diskspaceQuota) \n       throws IOException {\n+    checkNNStartup();\n     namesystem.setQuota(path, namespaceQuota, diskspaceQuota);\n   }\n   \n   @Override // ClientProtocol\n   public void fsync(String src, long fileId, String clientName,\n                     long lastBlockLength)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.fsync(src, fileId, clientName, lastBlockLength);\n   }\n \n   @Override // ClientProtocol\n   public void setTimes(String src, long mtime, long atime) \n       throws IOException {\n+    checkNNStartup();\n     namesystem.setTimes(src, mtime, atime);\n   }\n \n   @Override // ClientProtocol\n   public void createSymlink(String target, String link, FsPermission dirPerms,\n       boolean createParent) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -1184,6 +1244,7 @@ public void createSymlink(String target, String link, FsPermission dirPerms,\n \n   @Override // ClientProtocol\n   public String getLinkTarget(String path) throws IOException {\n+    checkNNStartup();\n     metrics.incrGetLinkTargetOps();\n     HdfsFileStatus stat = null;\n     try {\n@@ -1206,6 +1267,7 @@ public String getLinkTarget(String path) throws IOException {\n   @Override // DatanodeProtocol\n   public DatanodeRegistration registerDatanode(DatanodeRegistration nodeReg)\n       throws IOException {\n+    checkNNStartup();\n     verifySoftwareVersion(nodeReg);\n     namesystem.registerDatanode(nodeReg);\n     return nodeReg;\n@@ -1216,6 +1278,7 @@ public HeartbeatResponse sendHeartbeat(DatanodeRegistration nodeReg,\n       StorageReport[] report, long dnCacheCapacity, long dnCacheUsed,\n       int xmitsInProgress, int xceiverCount,\n       int failedVolumes) throws IOException {\n+    checkNNStartup();\n     verifyRequest(nodeReg);\n     return namesystem.handleHeartbeat(nodeReg, report,\n         dnCacheCapacity, dnCacheUsed, xceiverCount, xmitsInProgress,\n@@ -1225,6 +1288,7 @@ public HeartbeatResponse sendHeartbeat(DatanodeRegistration nodeReg,\n   @Override // DatanodeProtocol\n   public DatanodeCommand blockReport(DatanodeRegistration nodeReg,\n       String poolId, StorageBlockReport[] reports) throws IOException {\n+    checkNNStartup();\n     verifyRequest(nodeReg);\n     if(blockStateChangeLog.isDebugEnabled()) {\n       blockStateChangeLog.debug(\"*BLOCK* NameNode.blockReport: \"\n@@ -1256,6 +1320,7 @@ public DatanodeCommand blockReport(DatanodeRegistration nodeReg,\n   @Override\n   public DatanodeCommand cacheReport(DatanodeRegistration nodeReg,\n       String poolId, List<Long> blockIds) throws IOException {\n+    checkNNStartup();\n     verifyRequest(nodeReg);\n     if (blockStateChangeLog.isDebugEnabled()) {\n       blockStateChangeLog.debug(\"*BLOCK* NameNode.cacheReport: \"\n@@ -1268,6 +1333,7 @@ public DatanodeCommand cacheReport(DatanodeRegistration nodeReg,\n   @Override // DatanodeProtocol\n   public void blockReceivedAndDeleted(DatanodeRegistration nodeReg, String poolId,\n       StorageReceivedDeletedBlocks[] receivedAndDeletedBlocks) throws IOException {\n+    checkNNStartup();\n     verifyRequest(nodeReg);\n     metrics.incrBlockReceivedAndDeletedOps();\n     if(blockStateChangeLog.isDebugEnabled()) {\n@@ -1283,6 +1349,7 @@ public void blockReceivedAndDeleted(DatanodeRegistration nodeReg, String poolId,\n   @Override // DatanodeProtocol\n   public void errorReport(DatanodeRegistration nodeReg,\n                           int errorCode, String msg) throws IOException { \n+    checkNNStartup();\n     String dnName = \n        (nodeReg == null) ? \"Unknown DataNode\" : nodeReg.toString();\n \n@@ -1304,6 +1371,7 @@ public void errorReport(DatanodeRegistration nodeReg,\n     \n   @Override // DatanodeProtocol, NamenodeProtocol\n   public NamespaceInfo versionRequest() throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     return namesystem.getNamespaceInfo();\n   }\n@@ -1328,6 +1396,7 @@ private void verifyRequest(NodeRegistration nodeReg) throws IOException {\n \n   @Override // RefreshAuthorizationPolicyProtocol\n   public void refreshServiceAcl() throws IOException {\n+    checkNNStartup();\n     if (!serviceAuthEnabled) {\n       throw new AuthorizationException(\"Service Level Authorization not enabled!\");\n     }\n@@ -1378,28 +1447,32 @@ public void refreshCallQueue() {\n   }\n \n   @Override // HAServiceProtocol\n-  public synchronized void monitorHealth() \n-      throws HealthCheckFailedException, AccessControlException {\n+  public synchronized void monitorHealth() throws HealthCheckFailedException,\n+      AccessControlException, IOException {\n+    checkNNStartup();\n     nn.monitorHealth();\n   }\n   \n   @Override // HAServiceProtocol\n   public synchronized void transitionToActive(StateChangeRequestInfo req) \n-      throws ServiceFailedException, AccessControlException {\n+      throws ServiceFailedException, AccessControlException, IOException {\n+    checkNNStartup();\n     nn.checkHaStateChange(req);\n     nn.transitionToActive();\n   }\n   \n   @Override // HAServiceProtocol\n   public synchronized void transitionToStandby(StateChangeRequestInfo req) \n-      throws ServiceFailedException, AccessControlException {\n+      throws ServiceFailedException, AccessControlException, IOException {\n+    checkNNStartup();\n     nn.checkHaStateChange(req);\n     nn.transitionToStandby();\n   }\n \n   @Override // HAServiceProtocol\n   public synchronized HAServiceStatus getServiceStatus() \n-      throws AccessControlException, ServiceFailedException {\n+      throws AccessControlException, ServiceFailedException, IOException {\n+    checkNNStartup();\n     return nn.getServiceStatus();\n   }\n \n@@ -1456,12 +1529,14 @@ private static String getClientMachine() {\n \n   @Override\n   public DataEncryptionKey getDataEncryptionKey() throws IOException {\n+    checkNNStartup();\n     return namesystem.getBlockManager().generateDataEncryptionKey();\n   }\n \n   @Override\n   public String createSnapshot(String snapshotRoot, String snapshotName)\n       throws IOException {\n+    checkNNStartup();\n     if (!checkPathLength(snapshotRoot)) {\n       throw new IOException(\"createSnapshot: Pathname too long.  Limit \"\n           + MAX_PATH_LENGTH + \" characters, \" + MAX_PATH_DEPTH + \" levels.\");\n@@ -1486,6 +1561,7 @@ public String createSnapshot(String snapshotRoot, String snapshotName)\n   @Override\n   public void deleteSnapshot(String snapshotRoot, String snapshotName)\n       throws IOException {\n+    checkNNStartup();\n     metrics.incrDeleteSnapshotOps();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n@@ -1503,20 +1579,24 @@ public void deleteSnapshot(String snapshotRoot, String snapshotName)\n   @Override\n   // Client Protocol\n   public void allowSnapshot(String snapshotRoot) throws IOException {\n+    checkNNStartup();\n     metrics.incrAllowSnapshotOps();\n     namesystem.allowSnapshot(snapshotRoot);\n   }\n \n   @Override\n   // Client Protocol\n   public void disallowSnapshot(String snapshot) throws IOException {\n+    checkNNStartup();\n     metrics.incrDisAllowSnapshotOps();\n     namesystem.disallowSnapshot(snapshot);\n   }\n \n   @Override\n+  // ClientProtocol\n   public void renameSnapshot(String snapshotRoot, String snapshotOldName,\n       String snapshotNewName) throws IOException {\n+    checkNNStartup();\n     if (snapshotNewName == null || snapshotNewName.isEmpty()) {\n       throw new IOException(\"The new snapshot name is null or empty.\");\n     }\n@@ -1538,24 +1618,27 @@ public void renameSnapshot(String snapshotRoot, String snapshotOldName,\n   @Override // Client Protocol\n   public SnapshottableDirectoryStatus[] getSnapshottableDirListing()\n       throws IOException {\n+    checkNNStartup();\n     SnapshottableDirectoryStatus[] status = namesystem\n         .getSnapshottableDirListing();\n     metrics.incrListSnapshottableDirOps();\n     return status;\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public SnapshotDiffReport getSnapshotDiffReport(String snapshotRoot,\n       String earlierSnapshotName, String laterSnapshotName) throws IOException {\n+    checkNNStartup();\n     SnapshotDiffReport report = namesystem.getSnapshotDiffReport(snapshotRoot,\n         earlierSnapshotName, laterSnapshotName);\n     metrics.incrSnapshotDiffReportOps();\n     return report;\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public long addCacheDirective(\n       CacheDirectiveInfo path, EnumSet<CacheFlag> flags) throws IOException {\n+    checkNNStartup();\n     CacheEntryWithPayload cacheEntry = RetryCache.waitForCompletion\n       (retryCache, null);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n@@ -1573,9 +1656,10 @@ public long addCacheDirective(\n     return ret;\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void modifyCacheDirective(\n       CacheDirectiveInfo directive, EnumSet<CacheFlag> flags) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return;\n@@ -1590,8 +1674,9 @@ public void modifyCacheDirective(\n     }\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void removeCacheDirective(long id) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return;\n@@ -1605,17 +1690,19 @@ public void removeCacheDirective(long id) throws IOException {\n     }\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public BatchedEntries<CacheDirectiveEntry> listCacheDirectives(long prevId,\n       CacheDirectiveInfo filter) throws IOException {\n+    checkNNStartup();\n     if (filter == null) {\n       filter = new CacheDirectiveInfo.Builder().build();\n     }\n     return namesystem.listCacheDirectives(prevId, filter);\n   }\n \n-  @Override\n+  @Override //ClientProtocol\n   public void addCachePool(CachePoolInfo info) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -1629,8 +1716,9 @@ public void addCachePool(CachePoolInfo info) throws IOException {\n     }\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void modifyCachePool(CachePoolInfo info) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -1644,8 +1732,9 @@ public void modifyCachePool(CachePoolInfo info) throws IOException {\n     }\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void removeCachePool(String cachePoolName) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return;\n@@ -1659,47 +1748,55 @@ public void removeCachePool(String cachePoolName) throws IOException {\n     }\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public BatchedEntries<CachePoolEntry> listCachePools(String prevKey)\n       throws IOException {\n+    checkNNStartup();\n     return namesystem.listCachePools(prevKey != null ? prevKey : \"\");\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void modifyAclEntries(String src, List<AclEntry> aclSpec)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.modifyAclEntries(src, aclSpec);\n   }\n \n-  @Override\n+  @Override // ClienProtocol\n   public void removeAclEntries(String src, List<AclEntry> aclSpec)\n       throws IOException {\n+    checkNNStartup();\n     namesystem.removeAclEntries(src, aclSpec);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void removeDefaultAcl(String src) throws IOException {\n+    checkNNStartup();\n     namesystem.removeDefaultAcl(src);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void removeAcl(String src) throws IOException {\n+    checkNNStartup();\n     namesystem.removeAcl(src);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void setAcl(String src, List<AclEntry> aclSpec) throws IOException {\n+    checkNNStartup();\n     namesystem.setAcl(src, aclSpec);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public AclStatus getAclStatus(String src) throws IOException {\n+    checkNNStartup();\n     return namesystem.getAclStatus(src);\n   }\n   \n-  @Override\n+  @Override // ClientProtocol\n   public void createEncryptionZone(String src, String keyName)\n     throws IOException {\n+    checkNNStartup();\n     final CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return;\n@@ -1713,21 +1810,24 @@ public void createEncryptionZone(String src, String keyName)\n     }\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public EncryptionZone getEZForPath(String src)\n     throws IOException {\n+    checkNNStartup();\n     return namesystem.getEZForPath(src);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public BatchedEntries<EncryptionZone> listEncryptionZones(\n       long prevId) throws IOException {\n+    checkNNStartup();\n     return namesystem.listEncryptionZones(prevId);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public void setXAttr(String src, XAttr xAttr, EnumSet<XAttrSetFlag> flag)\n       throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -1741,19 +1841,22 @@ public void setXAttr(String src, XAttr xAttr, EnumSet<XAttrSetFlag> flag)\n     }\n   }\n   \n-  @Override\n+  @Override // ClientProtocol\n   public List<XAttr> getXAttrs(String src, List<XAttr> xAttrs) \n       throws IOException {\n+    checkNNStartup();\n     return namesystem.getXAttrs(src, xAttrs);\n   }\n \n-  @Override\n+  @Override // ClientProtocol\n   public List<XAttr> listXAttrs(String src) throws IOException {\n+    checkNNStartup();\n     return namesystem.listXAttrs(src);\n   }\n   \n-  @Override\n+  @Override // ClientProtocol\n   public void removeXAttr(String src, XAttr xAttr) throws IOException {\n+    checkNNStartup();\n     CacheEntry cacheEntry = RetryCache.waitForCompletion(retryCache);\n     if (cacheEntry != null && cacheEntry.isSuccess()) {\n       return; // Return previous response\n@@ -1767,13 +1870,21 @@ public void removeXAttr(String src, XAttr xAttr) throws IOException {\n     }\n   }\n \n-  @Override\n+  private void checkNNStartup() throws IOException {\n+    if (!this.nn.isStarted()) {\n+      throw new IOException(this.nn.getRole() + \" still not started\");\n+    }\n+  }\n+\n+  @Override // ClientProtocol\n   public void checkAccess(String path, FsAction mode) throws IOException {\n+    checkNNStartup();\n     namesystem.checkAccess(path, mode);\n   }\n \n   @Override // ClientProtocol\n   public long getCurrentEditLogTxid() throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.READ); // only active\n     namesystem.checkSuperuserPrivilege();\n     // if it's not yet open for write, we may be in the process of transitioning\n@@ -1802,6 +1913,7 @@ private static FSEditLogOp readOp(EditLogInputStream elis)\n \n   @Override // ClientProtocol\n   public EventBatchList getEditsFromTxid(long txid) throws IOException {\n+    checkNNStartup();\n     namesystem.checkOperation(OperationCategory.READ); // only active\n     namesystem.checkSuperuserPrivilege();\n     int maxEventsPerRPC = nn.conf.getInt(\n@@ -1885,20 +1997,23 @@ public EventBatchList getEditsFromTxid(long txid) throws IOException {\n     return new EventBatchList(batches, firstSeenTxid, maxSeenTxid, syncTxid);\n   }\n \n-  @Override\n+  @Override // TraceAdminProtocol\n   public SpanReceiverInfo[] listSpanReceivers() throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     return nn.spanReceiverHost.listSpanReceivers();\n   }\n \n-  @Override\n+  @Override // TraceAdminProtocol\n   public long addSpanReceiver(SpanReceiverInfo info) throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     return nn.spanReceiverHost.addSpanReceiver(info);\n   }\n \n-  @Override\n+  @Override // TraceAdminProtocol\n   public void removeSpanReceiver(long id) throws IOException {\n+    checkNNStartup();\n     namesystem.checkSuperuserPrivilege();\n     nn.spanReceiverHost.removeSpanReceiver(id);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/db334bb8625da97c7e518cbcf477530c7ba7001e/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "sha": "a3ac455ee3ad5349298f4798020baeb3885df38f",
                "status": "modified"
            }
        ],
        "message": "HDFS-3443. Fix NPE when namenode transition to active during startup by adding checkNNStartup() in NameNodeRpcServer.  Contributed by Vinayakumar B",
        "parent": "https://github.com/apache/hadoop/commit/39c1bcf7d9c9331d25ca0aded85a293df04e0b52",
        "repo": "hadoop",
        "unit_tests": [
            "TestNameNodeRpcServer.java"
        ]
    },
    "hadoop_dd57c20": {
        "bug_id": "hadoop_dd57c20",
        "commit": "https://github.com/apache/hadoop/commit/dd57c2047bfd21910acc38c98153eedf1db75169",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=dd57c2047bfd21910acc38c98153eedf1db75169",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -319,6 +319,9 @@ Release 2.7.0 - UNRELEASED\n     YARN-2958. Made RMStateStore not update the last sequence number when updating the\n     delegation token. (Varun Saxena via zjshen)\n \n+    YARN-2978. Fixed potential NPE while getting queue info. (Varun Saxena via\n+    jianhe)\n+\n Release 2.6.0 - 2014-11-18\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/CHANGES.txt",
                "sha": "2f7b07cfacb52efe641f2194699b1db83dd9b352",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/AbstractCSQueue.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/AbstractCSQueue.java?ref=dd57c2047bfd21910acc38c98153eedf1db75169",
                "deletions": 11,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/AbstractCSQueue.java",
                "patch": "@@ -65,7 +65,6 @@\n   RMNodeLabelsManager labelManager;\n   String defaultLabelExpression;\n   Resource usedResources = Resources.createResource(0, 0);\n-  QueueInfo queueInfo;\n   Map<String, Float> absoluteCapacityByNodeLabels;\n   Map<String, Float> capacitiyByNodeLabels;\n   Map<String, Resource> usedResourcesByNodeLabels = new HashMap<String, Resource>();\n@@ -87,7 +86,6 @@ public AbstractCSQueue(CapacitySchedulerContext cs,\n     this.parent = parent;\n     this.queueName = queueName;\n     this.resourceCalculator = cs.getResourceCalculator();\n-    this.queueInfo = recordFactory.newRecordInstance(QueueInfo.class);\n     \n     // must be called after parent and queueName is set\n     this.metrics = old != null ? old.getMetrics() :\n@@ -99,9 +97,7 @@ public AbstractCSQueue(CapacitySchedulerContext cs,\n     this.accessibleLabels = cs.getConfiguration().getAccessibleNodeLabels(getQueuePath());\n     this.defaultLabelExpression = cs.getConfiguration()\n         .getDefaultNodeLabelExpression(getQueuePath());\n-    \n-    this.queueInfo.setQueueName(queueName);\n-    \n+\n     // inherit from parent if labels not set\n     if (this.accessibleLabels == null && parent != null) {\n       this.accessibleLabels = parent.getAccessibleNodeLabels();\n@@ -280,12 +276,6 @@ synchronized void setupQueueConfigs(Resource clusterResource, float capacity,\n     this.capacitiyByNodeLabels = new HashMap<String, Float>(nodeLabelCapacities);\n     this.maxCapacityByNodeLabels =\n         new HashMap<String, Float>(maximumNodeLabelCapacities);\n-    \n-    this.queueInfo.setAccessibleNodeLabels(this.accessibleLabels);\n-    this.queueInfo.setCapacity(this.capacity);\n-    this.queueInfo.setMaximumCapacity(this.maximumCapacity);\n-    this.queueInfo.setQueueState(this.state);\n-    this.queueInfo.setDefaultNodeLabelExpression(this.defaultLabelExpression);\n \n     // Update metrics\n     CSQueueUtils.updateQueueStatistics(\n@@ -330,6 +320,18 @@ synchronized void setupQueueConfigs(Resource clusterResource, float capacity,\n     this.reservationsContinueLooking = reservationContinueLooking;\n   }\n   \n+  protected QueueInfo getQueueInfo() {\n+    QueueInfo queueInfo = recordFactory.newRecordInstance(QueueInfo.class);\n+    queueInfo.setQueueName(queueName);\n+    queueInfo.setAccessibleNodeLabels(accessibleLabels);\n+    queueInfo.setCapacity(capacity);\n+    queueInfo.setMaximumCapacity(maximumCapacity);\n+    queueInfo.setQueueState(state);\n+    queueInfo.setDefaultNodeLabelExpression(defaultLabelExpression);\n+    queueInfo.setCurrentCapacity(getUsedCapacity());\n+    return queueInfo;\n+  }\n+  \n   @Private\n   public Resource getMaximumAllocation() {\n     return maximumAllocation;",
                "raw_url": "https://github.com/apache/hadoop/raw/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/AbstractCSQueue.java",
                "sha": "fec3a567744612905cb323d27e1ac9184b60ac07",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java?ref=dd57c2047bfd21910acc38c98153eedf1db75169",
                "deletions": 6,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "patch": "@@ -163,8 +163,6 @@ public LeafQueue(CapacitySchedulerContext cs,\n         CSQueueUtils.computeMaxActiveApplicationsPerUser(\n             maxActiveAppsUsingAbsCap, userLimit, userLimitFactor);\n \n-    this.queueInfo.setChildQueues(new ArrayList<QueueInfo>());\n-\n     QueueState state = cs.getConfiguration().getState(getQueuePath());\n \n     Map<QueueACL, AccessControlList> acls = \n@@ -235,14 +233,14 @@ protected synchronized void setupQueueConfigs(\n         this.defaultLabelExpression)) {\n       throw new IOException(\"Invalid default label expression of \"\n           + \" queue=\"\n-          + queueInfo.getQueueName()\n+          + getQueueName()\n           + \" doesn't have permission to access all labels \"\n           + \"in default label expression. labelExpression of resource request=\"\n           + (this.defaultLabelExpression == null ? \"\"\n               : this.defaultLabelExpression)\n           + \". Queue labels=\"\n-          + (queueInfo.getAccessibleNodeLabels() == null ? \"\" : StringUtils.join(queueInfo\n-              .getAccessibleNodeLabels().iterator(), ',')));\n+          + (getAccessibleNodeLabels() == null ? \"\" : StringUtils.join(\n+              getAccessibleNodeLabels().iterator(), ',')));\n     }\n     \n     this.nodeLocalityDelay = nodeLocalityDelay;\n@@ -433,7 +431,7 @@ public synchronized float getUserLimitFactor() {\n   @Override\n   public synchronized QueueInfo getQueueInfo(\n       boolean includeChildQueues, boolean recursive) {\n-    queueInfo.setCurrentCapacity(usedCapacity);\n+    QueueInfo queueInfo = getQueueInfo();\n     return queueInfo;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "sha": "dd710695f186a387c37e3f7e237f9add53372966",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java?ref=dd57c2047bfd21910acc38c98153eedf1db75169",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java",
                "patch": "@@ -110,8 +110,6 @@ public ParentQueue(CapacitySchedulerContext cs,\n     Map<QueueACL, AccessControlList> acls = \n       cs.getConfiguration().getAcls(getQueuePath());\n \n-    this.queueInfo.setChildQueues(new ArrayList<QueueInfo>());\n-\n     setupQueueConfigs(cs.getClusterResource(), capacity, absoluteCapacity,\n         maximumCapacity, absoluteMaxCapacity, state, acls, accessibleLabels,\n         defaultLabelExpression, capacitiyByNodeLabels, maxCapacityByNodeLabels, \n@@ -206,7 +204,7 @@ public String getQueuePath() {\n   @Override\n   public synchronized QueueInfo getQueueInfo( \n       boolean includeChildQueues, boolean recursive) {\n-    queueInfo.setCurrentCapacity(usedCapacity);\n+    QueueInfo queueInfo = getQueueInfo();\n \n     List<QueueInfo> childQueuesInfo = new ArrayList<QueueInfo>();\n     if (includeChildQueues) {",
                "raw_url": "https://github.com/apache/hadoop/raw/dd57c2047bfd21910acc38c98153eedf1db75169/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/ParentQueue.java",
                "sha": "f820ccab929a905c78ccd1cf8422fb78f58d82e4",
                "status": "modified"
            }
        ],
        "message": "YARN-2978. Fixed potential NPE while getting queue info. Contributed by Varun Saxena",
        "parent": "https://github.com/apache/hadoop/commit/d02fb53750bc592c23ba470ae82eb6f47d9a00ec",
        "repo": "hadoop",
        "unit_tests": [
            "TestLeafQueue.java",
            "TestParentQueue.java"
        ]
    },
    "hadoop_dd852f5": {
        "bug_id": "hadoop_dd852f5",
        "commit": "https://github.com/apache/hadoop/commit/dd852f5b8c8fe9e52d15987605f36b5b60f02701",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/dd852f5b8c8fe9e52d15987605f36b5b60f02701/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=dd852f5b8c8fe9e52d15987605f36b5b60f02701",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -153,6 +153,9 @@ Release 2.8.0 - UNRELEASED\n \n     YARN-3110. Few issues in ApplicationHistory web ui. (Naganarasimha G R via xgong)\n \n+    YARN-3457. NPE when NodeManager.serviceInit fails and stopRecoveryStore called.\n+    (Bibin A Chundatt via ozawa)\n+\n Release 2.7.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/dd852f5b8c8fe9e52d15987605f36b5b60f02701/hadoop-yarn-project/CHANGES.txt",
                "sha": "d5f6ce0c4e4a12079e5ca90e6d8a1ca6e42f7885",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/dd852f5b8c8fe9e52d15987605f36b5b60f02701/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java?ref=dd852f5b8c8fe9e52d15987605f36b5b60f02701",
                "deletions": 10,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "patch": "@@ -177,16 +177,18 @@ private void initAndStartRecoveryStore(Configuration conf)\n \n   private void stopRecoveryStore() throws IOException {\n     nmStore.stop();\n-    if (context.getDecommissioned() && nmStore.canRecover()) {\n-      LOG.info(\"Removing state store due to decommission\");\n-      Configuration conf = getConfig();\n-      Path recoveryRoot = new Path(\n-          conf.get(YarnConfiguration.NM_RECOVERY_DIR));\n-      LOG.info(\"Removing state store at \" + recoveryRoot\n-          + \" due to decommission\");\n-      FileSystem recoveryFs = FileSystem.getLocal(conf);\n-      if (!recoveryFs.delete(recoveryRoot, true)) {\n-        LOG.warn(\"Unable to delete \" + recoveryRoot);\n+    if (null != context) {\n+      if (context.getDecommissioned() && nmStore.canRecover()) {\n+        LOG.info(\"Removing state store due to decommission\");\n+        Configuration conf = getConfig();\n+        Path recoveryRoot =\n+            new Path(conf.get(YarnConfiguration.NM_RECOVERY_DIR));\n+        LOG.info(\"Removing state store at \" + recoveryRoot\n+            + \" due to decommission\");\n+        FileSystem recoveryFs = FileSystem.getLocal(conf);\n+        if (!recoveryFs.delete(recoveryRoot, true)) {\n+          LOG.warn(\"Unable to delete \" + recoveryRoot);\n+        }\n       }\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/dd852f5b8c8fe9e52d15987605f36b5b60f02701/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/NodeManager.java",
                "sha": "9831fc42c46a9ece18a7987e91c69f0fbfc12aa4",
                "status": "modified"
            }
        ],
        "message": "YARN-3457. NPE when NodeManager.serviceInit fails and stopRecoveryStore called. Contributed by Bibin A Chundatt.",
        "parent": "https://github.com/apache/hadoop/commit/ab04ff9efe632b4eca6faca7407ac35e00e6a379",
        "repo": "hadoop",
        "unit_tests": [
            "TestNodeManager.java"
        ]
    },
    "hadoop_e4b4901": {
        "bug_id": "hadoop_e4b4901",
        "commit": "https://github.com/apache/hadoop/commit/e4b4901d36875faa98ec8628e22e75499e0741ab",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/e4b4901d36875faa98ec8628e22e75499e0741ab/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=e4b4901d36875faa98ec8628e22e75499e0741ab",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -872,6 +872,8 @@ Release 2.6.0 - UNRELEASED\n     FatalEventDispatcher try to transition RM to StandBy at the same time.\n     (Rohith Sharmaks via jianhe)\n \n+    YARN-2813. Fixed NPE from MemoryTimelineStore.getDomains. (Zhijie Shen via xgong)\n+\n Release 2.5.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/e4b4901d36875faa98ec8628e22e75499e0741ab/hadoop-yarn-project/CHANGES.txt",
                "sha": "d65860cdb25f2de7163fe95161d55a6abe0d59e2",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/e4b4901d36875faa98ec8628e22e75499e0741ab/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/MemoryTimelineStore.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/MemoryTimelineStore.java?ref=e4b4901d36875faa98ec8628e22e75499e0741ab",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/MemoryTimelineStore.java",
                "patch": "@@ -241,6 +241,10 @@ public TimelineDomain getDomain(String domainId)\n   public TimelineDomains getDomains(String owner)\n       throws IOException {\n     List<TimelineDomain> domains = new ArrayList<TimelineDomain>();\n+    Set<TimelineDomain> domainsOfOneOwner = domainsByOwner.get(owner);\n+    if (domainsOfOneOwner == null) {\n+      return new TimelineDomains();\n+    }\n     for (TimelineDomain domain : domainsByOwner.get(owner)) {\n       TimelineDomain domainToReturn = createTimelineDomain(\n           domain.getId(),",
                "raw_url": "https://github.com/apache/hadoop/raw/e4b4901d36875faa98ec8628e22e75499e0741ab/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/main/java/org/apache/hadoop/yarn/server/timeline/MemoryTimelineStore.java",
                "sha": "af714b17b800f55c8c2c1cf07af7287df747fcaa",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/e4b4901d36875faa98ec8628e22e75499e0741ab/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TimelineStoreTestUtils.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TimelineStoreTestUtils.java?ref=e4b4901d36875faa98ec8628e22e75499e0741ab",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TimelineStoreTestUtils.java",
                "patch": "@@ -946,6 +946,10 @@ public void testGetDomains() throws IOException {\n     assertEquals(2, actualDomains.getDomains().size());\n     verifyDomainInfo(domain3, actualDomains.getDomains().get(0));\n     verifyDomainInfo(domain1, actualDomains.getDomains().get(1));\n+\n+    // owner without any domain\n+    actualDomains = store.getDomains(\"owner_4\");\n+    assertEquals(0, actualDomains.getDomains().size());\n   }\n \n   private static void verifyDomainInfo(",
                "raw_url": "https://github.com/apache/hadoop/raw/e4b4901d36875faa98ec8628e22e75499e0741ab/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-applicationhistoryservice/src/test/java/org/apache/hadoop/yarn/server/timeline/TimelineStoreTestUtils.java",
                "sha": "242478cafa98322e0479119af52b519a5fcfeaf2",
                "status": "modified"
            }
        ],
        "message": "YARN-2813. Fixed NPE from MemoryTimelineStore.getDomains. Contributed by Zhijie Shen",
        "parent": "https://github.com/apache/hadoop/commit/ef5af4f8de91fbe7891ae3471eb03397e74e1811",
        "repo": "hadoop",
        "unit_tests": [
            "TestMemoryTimelineStore.java"
        ]
    },
    "hadoop_e71f61e": {
        "bug_id": "hadoop_e71f61e",
        "commit": "https://github.com/apache/hadoop/commit/e71f61ecb87e04727a5a76e578a75714c9db6706",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/e71f61ecb87e04727a5a76e578a75714c9db6706/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java?ref=e71f61ecb87e04727a5a76e578a75714c9db6706",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java",
                "patch": "@@ -743,7 +743,7 @@ public static String createStartupShutdownMessage(String classname,\n     return toStartupShutdownString(\"STARTUP_MSG: \", new String[] {\n         \"Starting \" + classname,\n         \"  host = \" + hostname,\n-        \"  args = \" + Arrays.asList(args),\n+        \"  args = \" + (args != null ? Arrays.asList(args) : new ArrayList<>()),\n         \"  version = \" + VersionInfo.getVersion(),\n         \"  classpath = \" + System.getProperty(\"java.class.path\"),\n         \"  build = \" + VersionInfo.getUrl() + \" -r \"",
                "raw_url": "https://github.com/apache/hadoop/raw/e71f61ecb87e04727a5a76e578a75714c9db6706/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/StringUtils.java",
                "sha": "f49698ca5ac1b09b37f584998b248ac2eac83dd1",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/e71f61ecb87e04727a5a76e578a75714c9db6706/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java?ref=e71f61ecb87e04727a5a76e578a75714c9db6706",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java",
                "patch": "@@ -503,6 +503,15 @@ public void testEscapeHTML() {\n         escapedStr, StringUtils.escapeHTML(htmlStr));\n   }\n \n+  @Test\n+  public void testCreateStartupShutdownMessage() {\n+    //pass null args and method must still return a string beginning with\n+    // \"STARTUP_MSG\"\n+    String msg = StringUtils.createStartupShutdownMessage(\n+        this.getClass().getName(), \"test.host\", null);\n+    assertTrue(msg.startsWith(\"STARTUP_MSG:\"));\n+  }\n+\n   // Benchmark for StringUtils split\n   public static void main(String []args) {\n     final String TO_SPLIT = \"foo,bar,baz,blah,blah\";",
                "raw_url": "https://github.com/apache/hadoop/raw/e71f61ecb87e04727a5a76e578a75714c9db6706/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestStringUtils.java",
                "sha": "f05b5895676060d84087b13f89751fc3893b29db",
                "status": "modified"
            }
        ],
        "message": "HADOOP-15755. StringUtils#createStartupShutdownMessage throws NPE when args is null. Contributed by Lokesh Jain and Dinesh Chitlangia",
        "parent": "https://github.com/apache/hadoop/commit/589637276105b0f9b9d5b7f6207f6ad0892f0b28",
        "repo": "hadoop",
        "unit_tests": [
            "TestStringUtils.java"
        ]
    },
    "hadoop_e83be44": {
        "bug_id": "hadoop_e83be44",
        "commit": "https://github.com/apache/hadoop/commit/e83be44af530d57d9c49cd989d030052548a068b",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/e83be44af530d57d9c49cd989d030052548a068b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java?ref=e83be44af530d57d9c49cd989d030052548a068b",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
                "patch": "@@ -557,6 +557,10 @@ public void close() throws IOException {\n   public KeyVersion rollNewVersion(String name) throws NoSuchAlgorithmException,\n                                                        IOException {\n     Metadata meta = getMetadata(name);\n+    if (meta == null) {\n+      throw new IOException(\"Can't find Metadata for key \" + name);\n+    }\n+\n     byte[] material = generateKey(meta.getBitLength(), meta.getCipher());\n     return rollNewVersion(name, material);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/e83be44af530d57d9c49cd989d030052548a068b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/crypto/key/KeyProvider.java",
                "sha": "c99a7bf08ceee613156f085aab9432e5c462a8e4",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/hadoop/blob/e83be44af530d57d9c49cd989d030052548a068b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyProvider.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyProvider.java?ref=e83be44af530d57d9c49cd989d030052548a068b",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyProvider.java",
                "patch": "@@ -22,6 +22,7 @@\n \n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.security.ProviderUtils;\n+import org.apache.hadoop.test.GenericTestUtils;\n import org.junit.Test;\n \n import java.io.IOException;\n@@ -38,6 +39,7 @@\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.fail;\n \n public class TestKeyProvider {\n \n@@ -182,7 +184,10 @@ public KeyVersion getKeyVersion(String versionName)\n \n     @Override\n     public Metadata getMetadata(String name) throws IOException {\n-      return new Metadata(CIPHER, 128, \"description\", null, new Date(), 0);\n+      if (!\"unknown\".equals(name)) {\n+        return new Metadata(CIPHER, 128, \"description\", null, new Date(), 0);\n+      }\n+      return null;\n     }\n \n     @Override\n@@ -236,6 +241,27 @@ public void testMaterialGeneration() throws Exception {\n     Assert.assertNotNull(kp.material);\n   }\n \n+  @Test\n+  public void testRolloverUnknownKey() throws Exception {\n+    MyKeyProvider kp = new MyKeyProvider(new Configuration());\n+    KeyProvider.Options options = new KeyProvider.Options(new Configuration());\n+    options.setCipher(CIPHER);\n+    options.setBitLength(128);\n+    kp.createKey(\"hello\", options);\n+    Assert.assertEquals(128, kp.size);\n+    Assert.assertEquals(CIPHER, kp.algorithm);\n+    Assert.assertNotNull(kp.material);\n+\n+    kp = new MyKeyProvider(new Configuration());\n+    try {\n+      kp.rollNewVersion(\"unknown\");\n+      fail(\"should have thrown\");\n+    } catch (IOException e) {\n+      String expectedError = \"Can't find Metadata for key\";\n+      GenericTestUtils.assertExceptionContains(expectedError, e);\n+    }\n+  }\n+\n   @Test\n   public void testConfiguration() throws Exception {\n     Configuration conf = new Configuration(false);",
                "raw_url": "https://github.com/apache/hadoop/raw/e83be44af530d57d9c49cd989d030052548a068b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/crypto/key/TestKeyProvider.java",
                "sha": "9c01175c43dd9559cac11cd2b77d7a12a2b43490",
                "status": "modified"
            }
        ],
        "message": "HADOOP-13461. NPE in KeyProvider.rollNewVersion. Contributed by Colm O hEigeartaigh.",
        "parent": "https://github.com/apache/hadoop/commit/ec289bbeceff064ad24e189db20a3e0a296822c1",
        "repo": "hadoop",
        "unit_tests": [
            "TestKeyProvider.java"
        ]
    },
    "hadoop_eb484bb": {
        "bug_id": "hadoop_eb484bb",
        "commit": "https://github.com/apache/hadoop/commit/eb484bb5629e57c97192b6794f30c1fbb290b6ee",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/eb484bb5629e57c97192b6794f30c1fbb290b6ee/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=eb484bb5629e57c97192b6794f30c1fbb290b6ee",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -401,6 +401,9 @@ Release 2.1.1-beta - UNRELEASED\n     HDFS-5132. Deadlock in NameNode between SafeModeMonitor#run and \n     DatanodeManager#handleHeartbeat. (kihwal)\n \n+    HDFS-5077. NPE in FSNamesystem.commitBlockSynchronization().\n+    (Plamen Jeliazkov via shv)\n+\n Release 2.1.0-beta - 2013-08-22\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/eb484bb5629e57c97192b6794f30c1fbb290b6ee/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "84100e18c4dd693927880cc330ce7cad9758642e",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/hadoop/blob/eb484bb5629e57c97192b6794f30c1fbb290b6ee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=eb484bb5629e57c97192b6794f30c1fbb290b6ee",
                "deletions": 10,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -174,7 +174,6 @@\n import org.apache.hadoop.hdfs.server.namenode.INode.BlocksMapUpdateInfo;\n import org.apache.hadoop.hdfs.server.namenode.JournalSet.JournalAndStream;\n import org.apache.hadoop.hdfs.server.namenode.LeaseManager.Lease;\n-import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.hdfs.server.namenode.NameNode.OperationCategory;\n import org.apache.hadoop.hdfs.server.namenode.startupprogress.Phase;\n import org.apache.hadoop.hdfs.server.namenode.startupprogress.StartupProgress;\n@@ -3772,24 +3771,32 @@ void commitBlockSynchronization(ExtendedBlock lastblock,\n         // find the DatanodeDescriptor objects\n         // There should be no locations in the blockManager till now because the\n         // file is underConstruction\n-        DatanodeDescriptor[] descriptors = null;\n+        List<DatanodeDescriptor> targetList =\n+            new ArrayList<DatanodeDescriptor>(newtargets.length);\n         if (newtargets.length > 0) {\n-          descriptors = new DatanodeDescriptor[newtargets.length];\n-          for(int i = 0; i < newtargets.length; i++) {\n-            descriptors[i] = blockManager.getDatanodeManager().getDatanode(\n-                newtargets[i]);\n+          for (DatanodeID newtarget : newtargets) {\n+            // try to get targetNode\n+            DatanodeDescriptor targetNode =\n+                blockManager.getDatanodeManager().getDatanode(newtarget);\n+            if (targetNode != null)\n+              targetList.add(targetNode);\n+            else if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"DatanodeDescriptor (=\" + newtarget + \") not found\");\n+            }\n           }\n         }\n-        if ((closeFile) && (descriptors != null)) {\n+        if ((closeFile) && !targetList.isEmpty()) {\n           // the file is getting closed. Insert block locations into blockManager.\n           // Otherwise fsck will report these blocks as MISSING, especially if the\n           // blocksReceived from Datanodes take a long time to arrive.\n-          for (int i = 0; i < descriptors.length; i++) {\n-            descriptors[i].addBlock(storedBlock);\n+          for (DatanodeDescriptor targetNode : targetList) {\n+            targetNode.addBlock(storedBlock);\n           }\n         }\n         // add pipeline locations into the INodeUnderConstruction\n-        pendingFile.setLastBlock(storedBlock, descriptors);\n+        DatanodeDescriptor[] targetArray =\n+            new DatanodeDescriptor[targetList.size()];\n+        pendingFile.setLastBlock(storedBlock, targetList.toArray(targetArray));\n       }\n \n       if (closeFile) {",
                "raw_url": "https://github.com/apache/hadoop/raw/eb484bb5629e57c97192b6794f30c1fbb290b6ee/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "a397ce94fd1e4727903d2b56e5d84a487b8c0502",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/eb484bb5629e57c97192b6794f30c1fbb290b6ee/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java?ref=eb484bb5629e57c97192b6794f30c1fbb290b6ee",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java",
                "patch": "@@ -169,4 +169,23 @@ public void testCommitBlockSynchronizationWithClose() throws IOException {\n     namesystemSpy.commitBlockSynchronization(\n         lastBlock, genStamp, length, true, false, newTargets, null);\n   }\n+\n+  @Test\n+  public void testCommitBlockSynchronizationWithCloseAndNonExistantTarget()\n+      throws IOException {\n+    INodeFileUnderConstruction file = mock(INodeFileUnderConstruction.class);\n+    Block block = new Block(blockId, length, genStamp);\n+    FSNamesystem namesystemSpy = makeNameSystemSpy(block, file);\n+    DatanodeID[] newTargets = new DatanodeID[]{\n+        new DatanodeID(\"0.0.0.0\", \"nonexistantHost\", \"1\", 0, 0, 0)};\n+\n+    ExtendedBlock lastBlock = new ExtendedBlock();\n+    namesystemSpy.commitBlockSynchronization(\n+        lastBlock, genStamp, length, true,\n+        false, newTargets, null);\n+\n+    // Repeat the call to make sure it returns true\n+    namesystemSpy.commitBlockSynchronization(\n+        lastBlock, genStamp, length, true, false, newTargets, null);\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/eb484bb5629e57c97192b6794f30c1fbb290b6ee/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestCommitBlockSynchronization.java",
                "sha": "f40b799d1a824c27dfc948f51d85052a8fb04392",
                "status": "modified"
            }
        ],
        "message": "HDFS-5077. NPE in FSNamesystem.commitBlockSynchronization(). Contributed by Plamen Jeliazkov.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1518851 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/eef32121d1d81076fd7e49ae65af03d1a6837dca",
        "repo": "hadoop",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_ebc048c": {
        "bug_id": "hadoop_ebc048c",
        "commit": "https://github.com/apache/hadoop/commit/ebc048cc055d0f7d1b85bc0b6f56cd15673e837d",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/ebc048cc055d0f7d1b85bc0b6f56cd15673e837d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java?ref=ebc048cc055d0f7d1b85bc0b6f56cd15673e837d",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "patch": "@@ -275,6 +275,10 @@ protected void addSchedPriorityCommand(List<String> command) {\n     }\n   }\n \n+  protected PrivilegedOperationExecutor getPrivilegedOperationExecutor() {\n+    return PrivilegedOperationExecutor.getInstance(getConf());\n+  }\n+\n   @Override\n   public void init() throws IOException {\n     Configuration conf = super.getConf();\n@@ -285,7 +289,7 @@ public void init() throws IOException {\n       PrivilegedOperation checkSetupOp = new PrivilegedOperation(\n           PrivilegedOperation.OperationType.CHECK_SETUP);\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          PrivilegedOperationExecutor.getInstance(conf);\n+          getPrivilegedOperationExecutor();\n \n       privilegedOperationExecutor.executePrivilegedOperation(checkSetupOp,\n           false);\n@@ -382,7 +386,7 @@ public void startLocalizer(LocalizerStartContext ctx)\n     try {\n       Configuration conf = super.getConf();\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          PrivilegedOperationExecutor.getInstance(conf);\n+          getPrivilegedOperationExecutor();\n \n       privilegedOperationExecutor.executePrivilegedOperation(prefixCommands,\n           initializeContainerOp, null, null, false, true);\n@@ -530,8 +534,9 @@ public int launchContainer(ContainerStartContext ctx)\n         }\n         builder.append(\"Stack trace: \"\n             + StringUtils.stringifyException(e) + \"\\n\");\n-        if (!e.getOutput().isEmpty()) {\n-          builder.append(\"Shell output: \" + e.getOutput() + \"\\n\");\n+        String output = e.getOutput();\n+        if (output != null && !e.getOutput().isEmpty()) {\n+          builder.append(\"Shell output: \" + output + \"\\n\");\n         }\n         String diagnostics = builder.toString();\n         logOutput(diagnostics);\n@@ -729,7 +734,7 @@ public void deleteAsUser(DeletionAsUserContext ctx) {\n     try {\n       Configuration conf = super.getConf();\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          PrivilegedOperationExecutor.getInstance(conf);\n+          getPrivilegedOperationExecutor();\n \n       privilegedOperationExecutor.executePrivilegedOperation(deleteAsUserOp,\n           false);\n@@ -759,7 +764,7 @@ public void deleteAsUser(DeletionAsUserContext ctx) {\n \n     try {\n       PrivilegedOperationExecutor privOpExecutor =\n-          PrivilegedOperationExecutor.getInstance(super.getConf());\n+          getPrivilegedOperationExecutor();\n \n       String results =\n           privOpExecutor.executePrivilegedOperation(listAsUserOp, true);\n@@ -818,7 +823,7 @@ public void mountCgroups(List<String> cgroupKVs, String hierarchy)\n \n       mountCGroupsOp.appendArgs(cgroupKVs);\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          PrivilegedOperationExecutor.getInstance(conf);\n+          getPrivilegedOperationExecutor();\n \n       privilegedOperationExecutor.executePrivilegedOperation(mountCGroupsOp,\n           false);",
                "raw_url": "https://github.com/apache/hadoop/raw/ebc048cc055d0f7d1b85bc0b6f56cd15673e837d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "sha": "2aaa8359e7b9abc6ed29c39b11dfede9e3ef376f",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/ebc048cc055d0f7d1b85bc0b6f56cd15673e837d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java?ref=ebc048cc055d0f7d1b85bc0b6f56cd15673e837d",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java",
                "patch": "@@ -24,7 +24,7 @@\n \n public class PrivilegedOperationException extends YarnException {\n   private static final long serialVersionUID = 1L;\n-  private Integer exitCode;\n+  private int exitCode = -1;\n   private String output;\n   private String errorOutput;\n \n@@ -36,7 +36,7 @@ public PrivilegedOperationException(String message) {\n     super(message);\n   }\n \n-  public PrivilegedOperationException(String message, Integer exitCode,\n+  public PrivilegedOperationException(String message, int exitCode,\n       String output, String errorOutput) {\n     super(message);\n     this.exitCode = exitCode;\n@@ -48,8 +48,8 @@ public PrivilegedOperationException(Throwable cause) {\n     super(cause);\n   }\n \n-  public PrivilegedOperationException(Throwable cause, Integer exitCode, String\n-      output, String errorOutput) {\n+  public PrivilegedOperationException(Throwable cause, int exitCode,\n+      String output, String errorOutput) {\n     super(cause);\n     this.exitCode = exitCode;\n     this.output = output;\n@@ -59,7 +59,7 @@ public PrivilegedOperationException(String message, Throwable cause) {\n     super(message, cause);\n   }\n \n-  public Integer getExitCode() {\n+  public int getExitCode() {\n     return exitCode;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/ebc048cc055d0f7d1b85bc0b6f56cd15673e837d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java",
                "sha": "9a11194f143e0e832d9371bddfd1229aeaeb0bd4",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/ebc048cc055d0f7d1b85bc0b6f56cd15673e837d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java?ref=ebc048cc055d0f7d1b85bc0b6f56cd15673e837d",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java",
                "patch": "@@ -32,10 +32,10 @@\n @InterfaceStability.Unstable\n public class ContainerExecutionException extends YarnException {\n   private static final long serialVersionUID = 1L;\n-  private static final Integer EXIT_CODE_UNSET = -1;\n+  private static final int EXIT_CODE_UNSET = -1;\n   private static final String OUTPUT_UNSET = \"<unknown>\";\n \n-  private Integer exitCode;\n+  private int exitCode;\n   private String output;\n   private String errorOutput;\n \n@@ -54,23 +54,23 @@ public ContainerExecutionException(Throwable throwable) {\n   }\n \n \n-  public ContainerExecutionException(String message, Integer exitCode, String\n+  public ContainerExecutionException(String message, int exitCode, String\n       output, String errorOutput) {\n     super(message);\n     this.exitCode = exitCode;\n     this.output = output;\n     this.errorOutput = errorOutput;\n   }\n \n-  public ContainerExecutionException(Throwable cause, Integer exitCode, String\n+  public ContainerExecutionException(Throwable cause, int exitCode, String\n       output, String errorOutput) {\n     super(cause);\n     this.exitCode = exitCode;\n     this.output = output;\n     this.errorOutput = errorOutput;\n   }\n \n-  public Integer getExitCode() {\n+  public int getExitCode() {\n     return exitCode;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/ebc048cc055d0f7d1b85bc0b6f56cd15673e837d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java",
                "sha": "3147277704265b1cb5bc2aa2f21d8a417e31c381",
                "status": "modified"
            },
            {
                "additions": 89,
                "blob_url": "https://github.com/apache/hadoop/blob/ebc048cc055d0f7d1b85bc0b6f56cd15673e837d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "changes": 89,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java?ref=ebc048cc055d0f7d1b85bc0b6f56cd15673e837d",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "patch": "@@ -23,7 +23,9 @@\n import static org.junit.Assert.assertNotEquals;\n import static org.junit.Assert.assertTrue;\n import static org.mockito.Matchers.any;\n+import static org.mockito.Matchers.anyBoolean;\n import static org.mockito.Mockito.doAnswer;\n+import static org.mockito.Mockito.doThrow;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.spy;\n import static org.mockito.Mockito.when;\n@@ -40,20 +42,24 @@\n import java.util.HashMap;\n import java.util.LinkedList;\n import java.util.List;\n+import java.util.Map;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.util.StringUtils;\n+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.ContainerLaunchContext;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.exceptions.ConfigurationException;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerDiagnosticsUpdateEvent;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.LinuxContainerRuntime;\n@@ -516,4 +522,87 @@ public void testDeleteAsUser() throws IOException {\n         appSubmitter, cmd, \"\", baseDir0.toString(), baseDir1.toString()),\n         readMockParams());\n   }\n+\n+  @Test\n+  public void testNoExitCodeFromPrivilegedOperation() throws Exception {\n+    Configuration conf = new Configuration();\n+    final PrivilegedOperationExecutor spyPrivilegedExecutor =\n+        spy(PrivilegedOperationExecutor.getInstance(conf));\n+    doThrow(new PrivilegedOperationException(\"interrupted\"))\n+        .when(spyPrivilegedExecutor).executePrivilegedOperation(\n+            any(List.class), any(PrivilegedOperation.class),\n+            any(File.class), any(Map.class), anyBoolean(), anyBoolean());\n+    LinuxContainerRuntime runtime = new DefaultLinuxContainerRuntime(\n+        spyPrivilegedExecutor);\n+    runtime.initialize(conf);\n+    mockExec = new LinuxContainerExecutor(runtime);\n+    mockExec.setConf(conf);\n+    LinuxContainerExecutor lce = new LinuxContainerExecutor(runtime) {\n+      @Override\n+      protected PrivilegedOperationExecutor getPrivilegedOperationExecutor() {\n+        return spyPrivilegedExecutor;\n+      }\n+    };\n+    lce.setConf(conf);\n+    InetSocketAddress address = InetSocketAddress.createUnresolved(\n+        \"localhost\", 8040);\n+    Path nmPrivateCTokensPath= new Path(\"file:///bin/nmPrivateCTokensPath\");\n+    LocalDirsHandlerService dirService = new LocalDirsHandlerService();\n+    dirService.init(conf);\n+\n+    String appSubmitter = \"nobody\";\n+    ApplicationId appId = ApplicationId.newInstance(1, 1);\n+    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(appId, 1);\n+    ContainerId cid = ContainerId.newContainerId(attemptId, 1);\n+    HashMap<String, String> env = new HashMap<>();\n+    Container container = mock(Container.class);\n+    ContainerLaunchContext context = mock(ContainerLaunchContext.class);\n+    when(container.getContainerId()).thenReturn(cid);\n+    when(container.getLaunchContext()).thenReturn(context);\n+    when(context.getEnvironment()).thenReturn(env);\n+    Path workDir = new Path(\"/tmp\");\n+\n+    try {\n+      lce.startLocalizer(new LocalizerStartContext.Builder()\n+          .setNmPrivateContainerTokens(nmPrivateCTokensPath)\n+          .setNmAddr(address)\n+          .setUser(appSubmitter)\n+          .setAppId(appId.toString())\n+          .setLocId(\"12345\")\n+          .setDirsHandler(dirService)\n+          .build());\n+      Assert.fail(\"startLocalizer should have thrown an exception\");\n+    } catch (IOException e) {\n+      assertTrue(\"Unexpected exception \" + e,\n+          e.getMessage().contains(\"exitCode\"));\n+    }\n+\n+    lce.activateContainer(cid, new Path(workDir, \"pid.txt\"));\n+    lce.launchContainer(new ContainerStartContext.Builder()\n+        .setContainer(container)\n+        .setNmPrivateContainerScriptPath(new Path(\"file:///bin/echo\"))\n+        .setNmPrivateTokensPath(new Path(\"file:///dev/null\"))\n+        .setUser(appSubmitter)\n+        .setAppId(appId.toString())\n+        .setContainerWorkDir(workDir)\n+        .setLocalDirs(dirsHandler.getLocalDirs())\n+        .setLogDirs(dirsHandler.getLogDirs())\n+        .setFilecacheDirs(new ArrayList<>())\n+        .setUserLocalDirs(new ArrayList<>())\n+        .setContainerLocalDirs(new ArrayList<>())\n+        .setContainerLogDirs(new ArrayList<>())\n+        .build());\n+    lce.deleteAsUser(new DeletionAsUserContext.Builder()\n+        .setUser(appSubmitter)\n+        .setSubDir(new Path(\"/tmp/testdir\"))\n+        .build());\n+\n+    try {\n+      lce.mountCgroups(new ArrayList<String>(), \"hierarchy\");\n+      Assert.fail(\"mountCgroups should have thrown an exception\");\n+    } catch (IOException e) {\n+      assertTrue(\"Unexpected exception \" + e,\n+          e.getMessage().contains(\"exit code\"));\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/ebc048cc055d0f7d1b85bc0b6f56cd15673e837d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "sha": "cfd0e364a2d4ad5735590d69d90cdce9f81b0012",
                "status": "modified"
            }
        ],
        "message": "YARN-6805. NPE in LinuxContainerExecutor due to null PrivilegedOperationException exit code. Contributed by Jason Lowe",
        "parent": "https://github.com/apache/hadoop/commit/0ffca5d347df0acb1979dff7a07ae88ea834adc7",
        "repo": "hadoop",
        "unit_tests": [
            "TestLinuxContainerExecutor.java"
        ]
    },
    "hadoop_ee21b13": {
        "bug_id": "hadoop_ee21b13",
        "commit": "https://github.com/apache/hadoop/commit/ee21b13cbd4654d7181306404174329f12193613",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=ee21b13cbd4654d7181306404174329f12193613",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -380,6 +380,9 @@ Release 2.6.0 - UNRELEASED\n     YARN-2558. Updated ContainerTokenIdentifier#read/write to use\n     ContainerId#getContainerId. (Tsuyoshi OZAWA via jianhe)\n \n+    YARN-2559. Fixed NPE in SystemMetricsPublisher when retrieving\n+    FinalApplicationStatus. (Zhijie Shen via jianhe)\n+\n Release 2.5.1 - 2014-09-05\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/CHANGES.txt",
                "sha": "5a238140c89771e308fd744dd64e43fbfe14519e",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java?ref=ee21b13cbd4654d7181306404174329f12193613",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "patch": "@@ -160,16 +160,18 @@ public void appAttemptRegistered(RMAppAttempt appAttempt,\n \n   @SuppressWarnings(\"unchecked\")\n   public void appAttemptFinished(RMAppAttempt appAttempt,\n-      RMAppAttemptState state, long finishedTime) {\n+      RMAppAttemptState appAttemtpState, RMApp app, long finishedTime) {\n     if (publishSystemMetrics) {\n       dispatcher.getEventHandler().handle(\n           new AppAttemptFinishedEvent(\n               appAttempt.getAppAttemptId(),\n               appAttempt.getTrackingUrl(),\n               appAttempt.getOriginalTrackingUrl(),\n               appAttempt.getDiagnostics(),\n-              appAttempt.getFinalApplicationStatus(),\n-              RMServerUtils.createApplicationAttemptState(state),\n+              // app will get the final status from app attempt, or create one\n+              // based on app state if it doesn't exist\n+              app.getFinalApplicationStatus(),\n+              RMServerUtils.createApplicationAttemptState(appAttemtpState),\n               finishedTime));\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/SystemMetricsPublisher.java",
                "sha": "5da006c0095561e299c719ce07017868a679a60d",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java?ref=ee21b13cbd4654d7181306404174329f12193613",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "patch": "@@ -1159,8 +1159,10 @@ public void transition(RMAppAttemptImpl appAttempt,\n       appAttempt.rmContext.getRMApplicationHistoryWriter()\n           .applicationAttemptFinished(appAttempt, finalAttemptState);\n       appAttempt.rmContext.getSystemMetricsPublisher()\n-          .appAttemptFinished(\n-              appAttempt, finalAttemptState, System.currentTimeMillis());\n+          .appAttemptFinished(appAttempt, finalAttemptState,\n+              appAttempt.rmContext.getRMApps().get(\n+                  appAttempt.applicationAttemptId.getApplicationId()),\n+              System.currentTimeMillis());\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "sha": "7ca57ee018a188ad14c12feff5992930773cecfe",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java?ref=ee21b13cbd4654d7181306404174329f12193613",
                "deletions": 4,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "patch": "@@ -174,7 +174,9 @@ public void testPublishAppAttemptMetrics() throws Exception {\n         ApplicationAttemptId.newInstance(ApplicationId.newInstance(0, 1), 1);\n     RMAppAttempt appAttempt = createRMAppAttempt(appAttemptId);\n     metricsPublisher.appAttemptRegistered(appAttempt, Integer.MAX_VALUE + 1L);\n-    metricsPublisher.appAttemptFinished(appAttempt, RMAppAttemptState.FINISHED,\n+    RMApp app = mock(RMApp.class);\n+    when(app.getFinalApplicationStatus()).thenReturn(FinalApplicationStatus.UNDEFINED);\n+    metricsPublisher.appAttemptFinished(appAttempt, RMAppAttemptState.FINISHED, app,\n         Integer.MAX_VALUE + 2L);\n     TimelineEntity entity = null;\n     do {\n@@ -222,7 +224,7 @@ public void testPublishAppAttemptMetrics() throws Exception {\n             event.getEventInfo().get(\n                 AppAttemptMetricsConstants.ORIGINAL_TRACKING_URL_EVENT_INFO));\n         Assert.assertEquals(\n-            appAttempt.getFinalApplicationStatus().toString(),\n+            FinalApplicationStatus.UNDEFINED.toString(),\n             event.getEventInfo().get(\n                 AppAttemptMetricsConstants.FINAL_STATUS_EVENT_INFO));\n         Assert.assertEquals(\n@@ -340,8 +342,6 @@ private static RMAppAttempt createRMAppAttempt(\n     when(appAttempt.getTrackingUrl()).thenReturn(\"test tracking url\");\n     when(appAttempt.getOriginalTrackingUrl()).thenReturn(\n         \"test original tracking url\");\n-    when(appAttempt.getFinalApplicationStatus()).thenReturn(\n-        FinalApplicationStatus.UNDEFINED);\n     return appAttempt;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/metrics/TestSystemMetricsPublisher.java",
                "sha": "63343e9521d49df756693f9307ba9731b90d430b",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java?ref=ee21b13cbd4654d7181306404174329f12193613",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java",
                "patch": "@@ -76,6 +76,7 @@\n import org.apache.hadoop.yarn.server.resourcemanager.metrics.SystemMetricsPublisher;\n import org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore;\n import org.apache.hadoop.yarn.server.resourcemanager.recovery.RMStateStore.ApplicationAttemptState;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppFailedAttemptEvent;\n@@ -92,7 +93,6 @@\n import org.apache.hadoop.yarn.server.resourcemanager.rmcontainer.RMContainerImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.Allocation;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;\n-import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerAppReport;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerUtils;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.YarnScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.AppAttemptAddedSchedulerEvent;\n@@ -289,7 +289,6 @@ public void setUp() throws Exception {\n     Mockito.doReturn(resourceScheduler).when(spyRMContext).getScheduler();\n \n \n-    final String user = MockApps.newUserName();\n     final String queue = MockApps.newQueue();\n     submissionContext = mock(ApplicationSubmissionContext.class);\n     when(submissionContext.getQueue()).thenReturn(queue);\n@@ -1385,7 +1384,7 @@ private void verifyApplicationAttemptFinished(RMAppAttemptState state) {\n     finalState =\n         ArgumentCaptor.forClass(RMAppAttemptState.class);\n     verify(publisher).appAttemptFinished(any(RMAppAttempt.class), finalState.capture(),\n-        anyLong());\n+        any(RMApp.class), anyLong());\n     Assert.assertEquals(state, finalState.getValue());\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/ee21b13cbd4654d7181306404174329f12193613/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/TestRMAppAttemptTransitions.java",
                "sha": "b8e6f434e7c94b8b267109436fd2e8e3bc617327",
                "status": "modified"
            }
        ],
        "message": "YARN-2559. Fixed NPE in SystemMetricsPublisher when retrieving FinalApplicationStatus. Contributed by Zhijie Shen",
        "parent": "https://github.com/apache/hadoop/commit/0ecefe60179968984b1892a14411566b7a0c8df3",
        "repo": "hadoop",
        "unit_tests": [
            "TestSystemMetricsPublisher.java"
        ]
    },
    "hadoop_ef43257": {
        "bug_id": "hadoop_ef43257",
        "commit": "https://github.com/apache/hadoop/commit/ef432579a7763cc0e482fe049027c6e5325eb034",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/ef432579a7763cc0e482fe049027c6e5325eb034/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceBlockPlacementPolicy.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceBlockPlacementPolicy.java?ref=ef432579a7763cc0e482fe049027c6e5325eb034",
                "deletions": 6,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceBlockPlacementPolicy.java",
                "patch": "@@ -76,13 +76,17 @@ protected DatanodeDescriptor chooseDataNode(final String scope,\n         (DatanodeDescriptor) clusterMap.chooseRandom(scope, excludedNode);\n     DatanodeDescriptor b =\n         (DatanodeDescriptor) clusterMap.chooseRandom(scope, excludedNode);\n-    int ret = compareDataNode(a, b);\n-    if (ret == 0) {\n-      return a;\n-    } else if (ret < 0) {\n-      return (RAND.nextInt(100) < balancedPreference) ? a : b;\n+    if (a != null && b != null){\n+      int ret = compareDataNode(a, b);\n+      if (ret == 0) {\n+        return a;\n+      } else if (ret < 0) {\n+        return (RAND.nextInt(100) < balancedPreference) ? a : b;\n+      } else {\n+        return (RAND.nextInt(100) < balancedPreference) ? b : a;\n+      }\n     } else {\n-      return (RAND.nextInt(100) < balancedPreference) ? b : a;\n+      return a == null ? b : a;\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/ef432579a7763cc0e482fe049027c6e5325eb034/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/AvailableSpaceBlockPlacementPolicy.java",
                "sha": "706768c8ade547c147f8902774afe3cc11a97adb",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hadoop/blob/ef432579a7763cc0e482fe049027c6e5325eb034/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestAvailableSpaceBlockPlacementPolicy.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestAvailableSpaceBlockPlacementPolicy.java?ref=ef432579a7763cc0e482fe049027c6e5325eb034",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestAvailableSpaceBlockPlacementPolicy.java",
                "patch": "@@ -20,6 +20,8 @@\n \n import java.io.File;\n import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n@@ -30,6 +32,7 @@\n import org.apache.hadoop.hdfs.server.common.HdfsServerConstants;\n import org.apache.hadoop.hdfs.server.namenode.NameNode;\n import org.apache.hadoop.net.NetworkTopology;\n+import org.apache.hadoop.net.Node;\n import org.apache.hadoop.test.PathUtils;\n import org.junit.AfterClass;\n import org.junit.Assert;\n@@ -158,6 +161,21 @@ public void testChooseTarget() {\n     Assert.assertTrue(possibility < 0.55);\n   }\n \n+  @Test\n+  public void testChooseDataNode() {\n+    try {\n+      Collection<Node> allNodes = new ArrayList<>(dataNodes.length);\n+      Collections.addAll(allNodes, dataNodes);\n+      if (placementPolicy instanceof AvailableSpaceBlockPlacementPolicy){\n+        // exclude all datanodes when chooseDataNode, no NPE should be thrown\n+        ((AvailableSpaceBlockPlacementPolicy)placementPolicy)\n+                .chooseDataNode(\"~\", allNodes);\n+      }\n+    }catch (NullPointerException npe){\n+      Assert.fail(\"NPE should not be thrown\");\n+    }\n+  }\n+\n   @AfterClass\n   public static void teardownCluster() {\n     if (namenode != null) {",
                "raw_url": "https://github.com/apache/hadoop/raw/ef432579a7763cc0e482fe049027c6e5325eb034/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/blockmanagement/TestAvailableSpaceBlockPlacementPolicy.java",
                "sha": "5b8ad1c4fd85bb4d2268d6e195ef0c287e69afd3",
                "status": "modified"
            }
        ],
        "message": "HDFS-10715. NPE when applying AvailableSpaceBlockPlacementPolicy. Contributed by Guangbin Zhu.",
        "parent": "https://github.com/apache/hadoop/commit/18d9e6ec0bdb4bce316f8af5d3f13902dd899325",
        "repo": "hadoop",
        "unit_tests": [
            "TestAvailableSpaceBlockPlacementPolicy.java"
        ]
    },
    "hadoop_eff5d9b": {
        "bug_id": "hadoop_eff5d9b",
        "commit": "https://github.com/apache/hadoop/commit/eff5d9b17e0853e82968a695b498b4be37148a05",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/eff5d9b17e0853e82968a695b498b4be37148a05/hadoop-yarn-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=eff5d9b17e0853e82968a695b498b4be37148a05",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -663,6 +663,8 @@ Release 2.1.0-beta - 2013-07-02\n     mechanisms are enabled and thus fix YARN/MR test failures after HADOOP-9421.\n     (Daryn Sharp and Vinod Kumar Vavilapalli via vinodkv)\n \n+    YARN-845. RM crash with NPE on NODE_UPDATE (Mayank Bansal via bikas)\n+\n   BREAKDOWN OF HADOOP-8562 SUBTASKS AND RELATED JIRAS\n \n     YARN-158. Yarn creating package-info.java must not depend on sh.",
                "raw_url": "https://github.com/apache/hadoop/raw/eff5d9b17e0853e82968a695b498b4be37148a05/hadoop-yarn-project/CHANGES.txt",
                "sha": "a0e3a9b75e4a6f22677ddb2bea8c2e4a0fc80283",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/eff5d9b17e0853e82968a695b498b4be37148a05/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java?ref=eff5d9b17e0853e82968a695b498b4be37148a05",
                "deletions": 3,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "patch": "@@ -801,9 +801,10 @@ private synchronized FiCaSchedulerApp getApplication(\n     if (reservedContainer != null) {\n       FiCaSchedulerApp application = \n           getApplication(reservedContainer.getApplicationAttemptId());\n-      return \n-          assignReservedContainer(application, node, reservedContainer, \n-              clusterResource); \n+      synchronized (application) {\n+        return assignReservedContainer(application, node, reservedContainer,\n+          clusterResource);\n+      }\n     }\n     \n     // Try to assign containers to applications in order",
                "raw_url": "https://github.com/apache/hadoop/raw/eff5d9b17e0853e82968a695b498b4be37148a05/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/capacity/LeafQueue.java",
                "sha": "dbfa7444183198dd512254b8ff5daaa6b7208180",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/eff5d9b17e0853e82968a695b498b4be37148a05/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java?ref=eff5d9b17e0853e82968a695b498b4be37148a05",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "patch": "@@ -38,6 +38,7 @@\n import org.apache.hadoop.yarn.api.records.Priority;\n import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.ResourceRequest;\n+import org.apache.hadoop.yarn.exceptions.YarnRuntimeException;\n import org.apache.hadoop.yarn.factories.RecordFactory;\n import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\n import org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger;\n@@ -426,6 +427,16 @@ public synchronized void unreserve(FiCaSchedulerNode node, Priority priority) {\n       this.reservedContainers.remove(priority);\n     }\n     \n+    // reservedContainer should not be null here\n+    if (reservedContainer == null) {\n+      String errorMesssage =\n+          \"Application \" + getApplicationId() + \" is trying to unreserve \"\n+              + \" on node \" + node + \", currently has \"\n+              + reservedContainers.size() + \" at priority \" + priority\n+              + \"; currentReservation \" + currentReservation;\n+      LOG.warn(errorMesssage);\n+      throw new YarnRuntimeException(errorMesssage);\n+    }\n     // Reset the re-reservation count\n     resetReReservations(priority);\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/eff5d9b17e0853e82968a695b498b4be37148a05/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/common/fica/FiCaSchedulerApp.java",
                "sha": "8e2020abc79259fa89228678556aead46ddddbc6",
                "status": "modified"
            }
        ],
        "message": "YARN-845. RM crash with NPE on NODE_UPDATE (Mayank Bansal via bikas)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1499886 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/5428bfbf53d264cc3c67b39b94c30a93cc4578c3",
        "repo": "hadoop",
        "unit_tests": [
            "TestLeafQueue.java"
        ]
    },
    "hadoop_f1552f6": {
        "bug_id": "hadoop_f1552f6",
        "commit": "https://github.com/apache/hadoop/commit/f1552f6edb8fe152003fd71944851b2b46a6677d",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/f1552f6edb8fe152003fd71944851b2b46a6677d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/main/java/org/apache/hadoop/yarn/server/timeline/EntityGroupFSTimelineStore.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/main/java/org/apache/hadoop/yarn/server/timeline/EntityGroupFSTimelineStore.java?ref=f1552f6edb8fe152003fd71944851b2b46a6677d",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/main/java/org/apache/hadoop/yarn/server/timeline/EntityGroupFSTimelineStore.java",
                "patch": "@@ -1115,6 +1115,11 @@ public TimelineEvents getEntityTimelines(String entityType,\n     LOG.debug(\"getEntityTimelines type={} ids={}\", entityType, entityIds);\n     TimelineEvents returnEvents = new TimelineEvents();\n     List<EntityCacheItem> relatedCacheItems = new ArrayList<>();\n+\n+    if (entityIds == null || entityIds.isEmpty()) {\n+      return returnEvents;\n+    }\n+\n     for (String entityId : entityIds) {\n       LOG.debug(\"getEntityTimeline type={} id={}\", entityType, entityId);\n       List<TimelineStore> stores",
                "raw_url": "https://github.com/apache/hadoop/raw/f1552f6edb8fe152003fd71944851b2b46a6677d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/main/java/org/apache/hadoop/yarn/server/timeline/EntityGroupFSTimelineStore.java",
                "sha": "a5e5b419d50d6a357df1a3085f0aff363c0b2561",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/f1552f6edb8fe152003fd71944851b2b46a6677d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/test/java/org/apache/hadoop/yarn/server/timeline/TestEntityGroupFSTimelineStore.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/test/java/org/apache/hadoop/yarn/server/timeline/TestEntityGroupFSTimelineStore.java?ref=f1552f6edb8fe152003fd71944851b2b46a6677d",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/test/java/org/apache/hadoop/yarn/server/timeline/TestEntityGroupFSTimelineStore.java",
                "patch": "@@ -41,6 +41,7 @@\n import org.apache.hadoop.yarn.server.timeline.TimelineReader.Field;\n import org.apache.hadoop.yarn.util.ConverterUtils;\n import org.junit.After;\n+import org.junit.Assert;\n import org.junit.AfterClass;\n import org.junit.Before;\n import org.junit.BeforeClass;\n@@ -379,6 +380,16 @@ public void testCleanBuckets() throws Exception {\n     assertFalse(fs.exists(clusterTimeStampDir1));\n   }\n \n+  @Test\n+  public void testNullCheckGetEntityTimelines() throws Exception {\n+    try {\n+      store.getEntityTimelines(\"YARN_APPLICATION\", null, null, null, null,\n+          null);\n+    } catch (NullPointerException e) {\n+      Assert.fail(\"NPE when getEntityTimelines called with Null EntityIds\");\n+    }\n+  }\n+\n   @Test\n   public void testPluginRead() throws Exception {\n     // Verify precondition",
                "raw_url": "https://github.com/apache/hadoop/raw/f1552f6edb8fe152003fd71944851b2b46a6677d/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-timeline-pluginstorage/src/test/java/org/apache/hadoop/yarn/server/timeline/TestEntityGroupFSTimelineStore.java",
                "sha": "8fcc696aad42b580e93406ac998928544f3dd953",
                "status": "modified"
            }
        ],
        "message": "YARN-9553. Fix NPE in EntityGroupFSTimelineStore#getEntityTimelines. Contributed by Prabhu Joseph.",
        "parent": "https://github.com/apache/hadoop/commit/30c6dd92e1d4075d143adc891dc8ec536dddc0d9",
        "repo": "hadoop",
        "unit_tests": [
            "TestEntityGroupFSTimelineStore.java"
        ]
    },
    "hadoop_f39f8c5": {
        "bug_id": "hadoop_f39f8c5",
        "commit": "https://github.com/apache/hadoop/commit/f39f8c57344ede533ca4363c98230f3a0c401a76",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt?ref=f39f8c57344ede533ca4363c98230f3a0c401a76",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "patch": "@@ -45,3 +45,5 @@ IMPROVEMENTS:\n \n     HDFS-5390. Send one incremental block report per storage directory.\n     (Arpit Agarwal)\n+\n+    HDFS-5401. Fix NPE in Directory Scanner. (Arpit Agarwal)",
                "raw_url": "https://github.com/apache/hadoop/raw/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/CHANGES_HDFS-2832.txt",
                "sha": "cd139d4845e203e0125d72371d510352118fa6bc",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java?ref=f39f8c57344ede533ca4363c98230f3a0c401a76",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.commons.logging.Log;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.ha.HAServiceProtocol.HAServiceState;\n+import org.apache.hadoop.hdfs.StorageType;\n import org.apache.hadoop.hdfs.protocol.Block;\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n@@ -180,10 +181,11 @@ public String toString() {\n     }\n   }\n   \n-  void reportBadBlocks(ExtendedBlock block) {\n+  void reportBadBlocks(ExtendedBlock block,\n+                       String storageUuid, StorageType storageType) {\n     checkBlock(block);\n     for (BPServiceActor actor : bpServices) {\n-      actor.reportBadBlocks(block);\n+      actor.reportBadBlocks(block, storageUuid, storageType);\n     }\n   }\n   ",
                "raw_url": "https://github.com/apache/hadoop/raw/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPOfferService.java",
                "sha": "5d584616df335258b426878141001405130adb92",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/hadoop/blob/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java?ref=f39f8c57344ede533ca4363c98230f3a0c401a76",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
                "patch": "@@ -28,6 +28,7 @@\n import org.apache.commons.logging.Log;\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.hdfs.DFSUtil;\n+import org.apache.hadoop.hdfs.StorageType;\n import org.apache.hadoop.hdfs.protocol.BlockListAsLongs;\n import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.ExtendedBlock;\n@@ -237,12 +238,18 @@ void scheduleBlockReport(long delay) {\n     resetBlockReportTime = true; // reset future BRs for randomness\n   }\n \n-  void reportBadBlocks(ExtendedBlock block) {\n+  void reportBadBlocks(ExtendedBlock block,\n+      String storageUuid, StorageType storageType) {\n     if (bpRegistration == null) {\n       return;\n     }\n     DatanodeInfo[] dnArr = { new DatanodeInfo(bpRegistration) };\n-    LocatedBlock[] blocks = { new LocatedBlock(block, dnArr) }; \n+    String[] uuids = { storageUuid };\n+    StorageType[] types = { storageType };\n+    // TODO: Corrupt flag is set to false for compatibility. We can probably\n+    // set it to true here.\n+    LocatedBlock[] blocks = {\n+        new LocatedBlock(block, dnArr, uuids, types, -1, false) };\n     \n     try {\n       bpNamenode.reportBadBlocks(blocks);  ",
                "raw_url": "https://github.com/apache/hadoop/raw/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/BPServiceActor.java",
                "sha": "172fb0fc30ec6cd3d5fdbe7dc2459e8c7d25cf48",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop/blob/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java?ref=f39f8c57344ede533ca4363c98230f3a0c401a76",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "patch": "@@ -559,7 +559,9 @@ public void notifyNamenodeDeletedBlock(ExtendedBlock block, String storageUuid)\n    */\n   public void reportBadBlocks(ExtendedBlock block) throws IOException{\n     BPOfferService bpos = getBPOSForBlock(block);\n-    bpos.reportBadBlocks(block);\n+    FsVolumeSpi volume = getFSDataset().getVolume(block);\n+    bpos.reportBadBlocks(\n+        block, volume.getStorageID(), volume.getStorageType());\n   }\n \n   /**\n@@ -1265,8 +1267,10 @@ private void transferBlock(ExtendedBlock block, DatanodeInfo xferTargets[])\n     // Check if NN recorded length matches on-disk length \n     long onDiskLength = data.getLength(block);\n     if (block.getNumBytes() > onDiskLength) {\n+      FsVolumeSpi volume = getFSDataset().getVolume(block);\n       // Shorter on-disk len indicates corruption so report NN the corrupt block\n-      bpos.reportBadBlocks(block);\n+      bpos.reportBadBlocks(\n+          block, volume.getStorageID(), volume.getStorageType());\n       LOG.warn(\"Can't replicate block \" + block\n           + \" because on-disk length \" + onDiskLength \n           + \" is shorter than NameNode recorded length \" + block.getNumBytes());",
                "raw_url": "https://github.com/apache/hadoop/raw/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/DataNode.java",
                "sha": "318d2f3705a05cad964d1b9b68ed52b7f4f3237a",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java?ref=f39f8c57344ede533ca4363c98230f3a0c401a76",
                "deletions": 16,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "patch": "@@ -198,7 +198,9 @@ public LengthInputStream getMetaDataInputStream(ExtendedBlock b)\n   //                 two maps. This might require some refactoring\n   //                 rewrite of FsDatasetImpl.\n   final ReplicaMap volumeMap;\n-  final Map<FsVolumeImpl, ReplicaMap> perVolumeReplicaMap;\n+\n+  // Map from StorageID to ReplicaMap.\n+  final Map<String, ReplicaMap> perVolumeReplicaMap;\n \n \n   // Used for synchronizing access to usage stats\n@@ -249,7 +251,7 @@ public LengthInputStream getMetaDataInputStream(ExtendedBlock b)\n       LOG.info(\"Added volume - \" + dir + \", StorageType: \" + storageType);\n     }\n     volumeMap = new ReplicaMap(this);\n-    perVolumeReplicaMap = new HashMap<FsVolumeImpl, ReplicaMap>();\n+    perVolumeReplicaMap = new HashMap<String, ReplicaMap>();\n \n     @SuppressWarnings(\"unchecked\")\n     final VolumeChoosingPolicy<FsVolumeImpl> blockChooserImpl =\n@@ -628,7 +630,7 @@ private synchronized ReplicaBeingWritten append(String bpid,\n     \n     // Replace finalized replica by a RBW replica in replicas map\n     volumeMap.add(bpid, newReplicaInfo);\n-    perVolumeReplicaMap.get(v).add(bpid, newReplicaInfo);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(bpid, newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n@@ -759,7 +761,7 @@ public synchronized ReplicaInPipeline createRbw(ExtendedBlock b)\n     ReplicaBeingWritten newReplicaInfo = new ReplicaBeingWritten(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile());\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n-    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), newReplicaInfo);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(b.getBlockPoolId(), newReplicaInfo);\n     return newReplicaInfo;\n   }\n   \n@@ -878,7 +880,7 @@ public synchronized ReplicaInPipeline convertTemporaryToRbw(\n     rbw.setBytesAcked(visible);\n     // overwrite the RBW in the volume map\n     volumeMap.add(b.getBlockPoolId(), rbw);\n-    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), rbw);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(b.getBlockPoolId(), rbw);\n     return rbw;\n   }\n \n@@ -898,7 +900,7 @@ public synchronized ReplicaInPipeline createTemporary(ExtendedBlock b)\n     ReplicaInPipeline newReplicaInfo = new ReplicaInPipeline(b.getBlockId(), \n         b.getGenerationStamp(), v, f.getParentFile());\n     volumeMap.add(b.getBlockPoolId(), newReplicaInfo);\n-    perVolumeReplicaMap.get(v).add(b.getBlockPoolId(), newReplicaInfo);\n+    perVolumeReplicaMap.get(v.getStorageID()).add(b.getBlockPoolId(), newReplicaInfo);\n     \n     return newReplicaInfo;\n   }\n@@ -967,7 +969,8 @@ private synchronized FinalizedReplica finalizeReplica(String bpid,\n       newReplicaInfo = new FinalizedReplica(replicaInfo, v, dest.getParentFile());\n     }\n     volumeMap.add(bpid, newReplicaInfo);\n-    perVolumeReplicaMap.get(newReplicaInfo.getVolume()).add(bpid, newReplicaInfo);\n+    perVolumeReplicaMap.get(newReplicaInfo.getVolume().getStorageID())\n+        .add(bpid, newReplicaInfo);\n     return newReplicaInfo;\n   }\n \n@@ -981,7 +984,7 @@ public synchronized void unfinalizeBlock(ExtendedBlock b) throws IOException {\n     if (replicaInfo != null && replicaInfo.getState() == ReplicaState.TEMPORARY) {\n       // remove from volumeMap\n       volumeMap.remove(b.getBlockPoolId(), b.getLocalBlock());\n-      perVolumeReplicaMap.get((FsVolumeImpl) replicaInfo.getVolume())\n+      perVolumeReplicaMap.get(replicaInfo.getVolume().getStorageID())\n           .remove(b.getBlockPoolId(), b.getLocalBlock());\n       \n       // delete the on-disk temp file\n@@ -1064,7 +1067,7 @@ public BlockListAsLongs getBlockReport(String bpid) {\n         new HashMap<String, BlockListAsLongs>();\n \n     for (FsVolumeImpl v : getVolumes()) {\n-      ReplicaMap rMap = perVolumeReplicaMap.get(v);\n+      ReplicaMap rMap = perVolumeReplicaMap.get(v.getStorageID());\n       BlockListAsLongs blockList = getBlockReportWithReplicaMap(bpid, rMap);\n       blockReportMap.put(v.getStorageID(), blockList);\n     }\n@@ -1212,7 +1215,7 @@ public void invalidate(String bpid, Block invalidBlks[]) throws IOException {\n           v.clearPath(bpid, parent);\n         }\n         volumeMap.remove(bpid, invalidBlks[i]);\n-        perVolumeReplicaMap.get(v).remove(bpid, invalidBlks[i]);\n+        perVolumeReplicaMap.get(v.getStorageID()).remove(bpid, invalidBlks[i]);\n       }\n \n       // Delete the block asynchronously to make sure we can do it fast enough\n@@ -1274,7 +1277,8 @@ public void checkDataDir() throws DiskErrorException {\n               LOG.warn(\"Removing replica \" + bpid + \":\" + b.getBlockId()\n                   + \" on failed volume \" + fv.getCurrentDir().getAbsolutePath());\n               ib.remove();\n-              perVolumeReplicaMap.get(fv).remove(bpid, b.getBlockId());\n+              perVolumeReplicaMap.get(fv.getStorageID())\n+                  .remove(bpid, b.getBlockId());\n               removedBlocks++;\n             }\n           }\n@@ -1391,8 +1395,7 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,\n           // Block is in memory and not on the disk\n           // Remove the block from volumeMap\n           volumeMap.remove(bpid, blockId);\n-          perVolumeReplicaMap.get((FsVolumeImpl) memBlockInfo.getVolume())\n-              .remove(bpid, blockId);\n+          perVolumeReplicaMap.get(vol.getStorageID()).remove(bpid, blockId);\n           final DataBlockScanner blockScanner = datanode.getBlockScanner();\n           if (blockScanner != null) {\n             blockScanner.deleteBlock(bpid, new Block(blockId));\n@@ -1416,8 +1419,8 @@ public void checkAndUpdate(String bpid, long blockId, File diskFile,\n         ReplicaInfo diskBlockInfo = new FinalizedReplica(blockId, \n             diskFile.length(), diskGS, vol, diskFile.getParentFile());\n         volumeMap.add(bpid, diskBlockInfo);\n-        perVolumeReplicaMap.get((FsVolumeImpl) memBlockInfo.getVolume()).\n-            remove(bpid, diskBlockInfo);\n+        perVolumeReplicaMap.get(vol.getStorageID())\n+            .remove(bpid, diskBlockInfo);\n         final DataBlockScanner blockScanner = datanode.getBlockScanner();\n         if (blockScanner != null) {\n           blockScanner.addBlock(new ExtendedBlock(bpid, diskBlockInfo));\n@@ -1695,7 +1698,7 @@ public synchronized void addBlockPool(String bpid, Configuration conf)\n \n     // TODO: Avoid the double scan.\n     for (FsVolumeImpl v : getVolumes()) {\n-      ReplicaMap rMap = perVolumeReplicaMap.get(v);\n+      ReplicaMap rMap = perVolumeReplicaMap.get(v.getStorageID());\n       rMap.initBlockPool(bpid);\n       volumes.getVolumeMap(bpid, v, rMap);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsDatasetImpl.java",
                "sha": "9077c40a8367a9f4bdddc10e439a1e95920ef481",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java?ref=f39f8c57344ede533ca4363c98230f3a0c401a76",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java",
                "patch": "@@ -90,13 +90,13 @@ long getRemaining() throws IOException {\n     return remaining;\n   }\n     \n-  void initializeReplicaMaps(Map<FsVolumeImpl, ReplicaMap> perVolumeReplicaMap,\n+  void initializeReplicaMaps(Map<String, ReplicaMap> perVolumeReplicaMap,\n                              ReplicaMap globalReplicaMap,\n                              Object mutex) throws IOException {\n     for (FsVolumeImpl v : volumes) {\n       ReplicaMap rMap = new ReplicaMap(mutex);\n       v.getVolumeMap(rMap);\n-      perVolumeReplicaMap.put(v, rMap);\n+      perVolumeReplicaMap.put(v.getStorageID(), rMap);\n       globalReplicaMap.addAll(rMap);\n     }\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/f39f8c57344ede533ca4363c98230f3a0c401a76/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/datanode/fsdataset/impl/FsVolumeList.java",
                "sha": "671996718be2eea6db9b076cf8a5aed4de36705e",
                "status": "modified"
            }
        ],
        "message": "HDFS-5401. Fix NPE in Directory Scanner.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-2832@1535158 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/b442fe92fbaeb6bd891b94c50d6086a46d4af4ac",
        "repo": "hadoop",
        "unit_tests": [
            "TestBPOfferService.java",
            "TestFsDatasetImpl.java",
            "TestFsVolumeList.java"
        ]
    },
    "hadoop_f3f5e7a": {
        "bug_id": "hadoop_f3f5e7a",
        "commit": "https://github.com/apache/hadoop/commit/f3f5e7ad005a88afad6fa09602073eaa450e21ed",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/f3f5e7ad005a88afad6fa09602073eaa450e21ed/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=f3f5e7ad005a88afad6fa09602073eaa450e21ed",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -2447,7 +2447,7 @@ public long getProvidedCapacity() {\n     return providedStorageMap.getCapacity();\n   }\n \n-  public void updateHeartbeat(DatanodeDescriptor node, StorageReport[] reports,\n+  void updateHeartbeat(DatanodeDescriptor node, StorageReport[] reports,\n       long cacheCapacity, long cacheUsed, int xceiverCount, int failedVolumes,\n       VolumeFailureSummary volumeFailureSummary) {\n \n@@ -2458,6 +2458,17 @@ public void updateHeartbeat(DatanodeDescriptor node, StorageReport[] reports,\n         failedVolumes, volumeFailureSummary);\n   }\n \n+  void updateHeartbeatState(DatanodeDescriptor node,\n+      StorageReport[] reports, long cacheCapacity, long cacheUsed,\n+      int xceiverCount, int failedVolumes,\n+      VolumeFailureSummary volumeFailureSummary) {\n+    for (StorageReport report: reports) {\n+      providedStorageMap.updateStorage(node, report.getStorage());\n+    }\n+    node.updateHeartbeatState(reports, cacheCapacity, cacheUsed, xceiverCount,\n+        failedVolumes, volumeFailureSummary);\n+  }\n+\n   /**\n    * StatefulBlockInfo is used to build the \"toUC\" list, which is a list of\n    * updates to the information about under-construction blocks.",
                "raw_url": "https://github.com/apache/hadoop/raw/f3f5e7ad005a88afad6fa09602073eaa450e21ed/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "a5fb0b17303cef37f436f4148e499a5fc3831aec",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/f3f5e7ad005a88afad6fa09602073eaa450e21ed/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java?ref=f3f5e7ad005a88afad6fa09602073eaa450e21ed",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
                "patch": "@@ -373,7 +373,7 @@ public int numBlocks() {\n   /**\n    * Updates stats from datanode heartbeat.\n    */\n-  public void updateHeartbeat(StorageReport[] reports, long cacheCapacity,\n+  void updateHeartbeat(StorageReport[] reports, long cacheCapacity,\n       long cacheUsed, int xceiverCount, int volFailures,\n       VolumeFailureSummary volumeFailureSummary) {\n     updateHeartbeatState(reports, cacheCapacity, cacheUsed, xceiverCount,\n@@ -384,7 +384,7 @@ public void updateHeartbeat(StorageReport[] reports, long cacheCapacity,\n   /**\n    * process datanode heartbeat or stats initialization.\n    */\n-  public void updateHeartbeatState(StorageReport[] reports, long cacheCapacity,\n+  void updateHeartbeatState(StorageReport[] reports, long cacheCapacity,\n       long cacheUsed, int xceiverCount, int volFailures,\n       VolumeFailureSummary volumeFailureSummary) {\n     updateStorageStats(reports, cacheCapacity, cacheUsed, xceiverCount,",
                "raw_url": "https://github.com/apache/hadoop/raw/f3f5e7ad005a88afad6fa09602073eaa450e21ed/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/DatanodeDescriptor.java",
                "sha": "6aa23765c766f260c18fbb70deafd00141d2a8ff",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/f3f5e7ad005a88afad6fa09602073eaa450e21ed/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java?ref=f3f5e7ad005a88afad6fa09602073eaa450e21ed",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java",
                "patch": "@@ -251,7 +251,7 @@ synchronized void updateLifeline(final DatanodeDescriptor node,\n     // updateHeartbeat, because we don't want to modify the\n     // heartbeatedSinceRegistration flag.  Arrival of a lifeline message does\n     // not count as arrival of the first heartbeat.\n-    node.updateHeartbeatState(reports, cacheCapacity, cacheUsed,\n+    blockManager.updateHeartbeatState(node, reports, cacheCapacity, cacheUsed,\n         xceiverCount, failedVolumes, volumeFailureSummary);\n     stats.add(node);\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/f3f5e7ad005a88afad6fa09602073eaa450e21ed/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/HeartbeatManager.java",
                "sha": "d2c279f8b4a3b484039b2a7cd77b06fbe68c772f",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/f3f5e7ad005a88afad6fa09602073eaa450e21ed/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeLifeline.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeLifeline.java?ref=f3f5e7ad005a88afad6fa09602073eaa450e21ed",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeLifeline.java",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.hadoop.hdfs.server.datanode;\n \n import static java.util.concurrent.TimeUnit.SECONDS;\n+import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_DATANODE_DATA_DIR_KEY;\n import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_DATANODE_LIFELINE_INTERVAL_SECONDS_KEY;\n import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_HEARTBEAT_INTERVAL_KEY;\n import static org.apache.hadoop.hdfs.DFSConfigKeys.DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL_KEY;\n@@ -196,6 +197,10 @@ public void testSendLifelineIfHeartbeatBlocked() throws Exception {\n           namesystem.getNumDeadDataNodes());\n       assertEquals(\"Expect DataNode not marked stale due to lifeline.\", 0,\n           namesystem.getNumStaleDataNodes());\n+      // add a new volume on the next heartbeat\n+      cluster.getDataNodes().get(0).reconfigurePropertyImpl(\n+          DFS_DATANODE_DATA_DIR_KEY,\n+          cluster.getDataDirectory().concat(\"/data-new\"));\n     }\n \n     // Verify that we did in fact call the lifeline RPC.",
                "raw_url": "https://github.com/apache/hadoop/raw/f3f5e7ad005a88afad6fa09602073eaa450e21ed/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/datanode/TestDataNodeLifeline.java",
                "sha": "14134e697dbd35729448467eee1520de938e6cfc",
                "status": "modified"
            }
        ],
        "message": "HDFS-14042. Fix NPE when PROVIDED storage is missing. Contributed by Virajith Jalaparti.",
        "parent": "https://github.com/apache/hadoop/commit/50f40e0536f38517aa33e8859f299bcf19f2f319",
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockManager.java",
            "TestDatanodeDescriptor.java"
        ]
    },
    "hadoop_f4d5d20": {
        "bug_id": "hadoop_f4d5d20",
        "commit": "https://github.com/apache/hadoop/commit/f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
        "file": [
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ServiceScheduler.java",
                "changes": 50,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ServiceScheduler.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 27,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ServiceScheduler.java",
                "patch": "@@ -132,7 +132,6 @@\n   private AMRMClientAsync<AMRMClient.ContainerRequest> amRMClient;\n   private NMClientAsync nmClient;\n   private AsyncDispatcher dispatcher;\n-  AsyncDispatcher compInstanceDispatcher;\n   private YarnRegistryViewForProviders yarnRegistryOperations;\n   private ServiceContext context;\n   private ContainerLaunchService containerLaunchService;\n@@ -152,7 +151,7 @@ public void buildInstance(ServiceContext context, Configuration configuration)\n     yarnRegistryOperations =\n         createYarnRegistryOperations(context, registryClient);\n \n-    // register metrics\n+    // register metrics,\n     serviceMetrics = ServiceMetrics\n         .register(app.getName(), \"Metrics for service\");\n     serviceMetrics.tag(\"type\", \"Metrics type [component or service]\", \"service\");\n@@ -167,14 +166,11 @@ public void buildInstance(ServiceContext context, Configuration configuration)\n     dispatcher = new AsyncDispatcher(\"Component  dispatcher\");\n     dispatcher.register(ComponentEventType.class,\n         new ComponentEventHandler());\n+    dispatcher.register(ComponentInstanceEventType.class,\n+        new ComponentInstanceEventHandler());\n     dispatcher.setDrainEventsOnStop();\n     addIfService(dispatcher);\n \n-    compInstanceDispatcher =\n-        new AsyncDispatcher(\"CompInstance dispatcher\");\n-    compInstanceDispatcher.register(ComponentInstanceEventType.class,\n-        new ComponentInstanceEventHandler());\n-    addIfService(compInstanceDispatcher);\n     containerLaunchService = new ContainerLaunchService(context.fs);\n     addService(containerLaunchService);\n \n@@ -277,10 +273,10 @@ public void serviceStart() throws Exception {\n   }\n \n   private void recoverComponents(RegisterApplicationMasterResponse response) {\n-    List<Container> recoveredContainers = response\n+    List<Container> containersFromPrevAttempt = response\n         .getContainersFromPreviousAttempts();\n     LOG.info(\"Received {} containers from previous attempt.\",\n-        recoveredContainers.size());\n+        containersFromPrevAttempt.size());\n     Map<String, ServiceRecord> existingRecords = new HashMap<>();\n     List<String> existingComps = null;\n     try {\n@@ -302,9 +298,8 @@ private void recoverComponents(RegisterApplicationMasterResponse response) {\n         }\n       }\n     }\n-    for (Container container : recoveredContainers) {\n-      LOG.info(\"Handling container {} from previous attempt\",\n-          container.getId());\n+    for (Container container : containersFromPrevAttempt) {\n+      LOG.info(\"Handling {} from previous attempt\", container.getId());\n       ServiceRecord record = existingRecords.get(RegistryPathUtils\n           .encodeYarnID(container.getId().toString()));\n       if (record != null) {\n@@ -487,16 +482,21 @@ public void onContainersAllocated(List<Container> containers) {\n             new ComponentEvent(comp.getName(), CONTAINER_ALLOCATED)\n                 .setContainer(container);\n         dispatcher.getEventHandler().handle(event);\n-        Collection<AMRMClient.ContainerRequest> requests = amRMClient\n-            .getMatchingRequests(container.getAllocationRequestId());\n-        LOG.info(\"[COMPONENT {}]: {} outstanding container requests.\",\n-            comp.getName(), requests.size());\n-        // remove the corresponding request\n-        if (requests.iterator().hasNext()) {\n-          LOG.info(\"[COMPONENT {}]: removing one container request.\", comp\n-              .getName());\n-          AMRMClient.ContainerRequest request = requests.iterator().next();\n-          amRMClient.removeContainerRequest(request);\n+        try {\n+          Collection<AMRMClient.ContainerRequest> requests = amRMClient\n+              .getMatchingRequests(container.getAllocationRequestId());\n+          LOG.info(\"[COMPONENT {}]: remove {} outstanding container requests \" +\n+                  \"for allocateId \" + container.getAllocationRequestId(),\n+              comp.getName(), requests.size());\n+          // remove the corresponding request\n+          if (requests.iterator().hasNext()) {\n+            AMRMClient.ContainerRequest request = requests.iterator().next();\n+            amRMClient.removeContainerRequest(request);\n+          }\n+        } catch(Exception e) {\n+          //TODO Due to YARN-7490, exception may be thrown, catch and ignore for\n+          //now.\n+          LOG.error(\"Exception when removing the matching requests. \", e);\n         }\n       }\n     }\n@@ -569,7 +569,7 @@ public void onContainersUpdated(List<UpdatedContainer> containers) {\n       }\n       ComponentEvent event =\n           new ComponentEvent(instance.getCompName(), CONTAINER_STARTED)\n-              .setInstance(instance);\n+              .setInstance(instance).setContainerId(containerId);\n       dispatcher.getEventHandler().handle(event);\n     }\n \n@@ -649,10 +649,6 @@ public void removeLiveCompInstance(ContainerId containerId) {\n     liveInstances.remove(containerId);\n   }\n \n-  public AsyncDispatcher getCompInstanceDispatcher() {\n-    return compInstanceDispatcher;\n-  }\n-\n   public YarnRegistryViewForProviders getYarnRegistryOperations() {\n     return yarnRegistryOperations;\n   }",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/ServiceScheduler.java",
                "sha": "6bc567328fe0f91693e239efbdcc1783160a428d",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/Component.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/Component.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 36,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/Component.java",
                "patch": "@@ -82,7 +82,8 @@\n   private Map<String, ComponentInstance> compInstances =\n       new ConcurrentHashMap<>();\n   // component instances to be assigned with a container\n-  private List<ComponentInstance> pendingInstances = new LinkedList<>();\n+  private List<ComponentInstance> pendingInstances =\n+      Collections.synchronizedList(new LinkedList<>());\n   private ContainerFailureTracker failureTracker;\n   private Probe probe;\n   private final ReentrantReadWriteLock.ReadLock readLock;\n@@ -94,7 +95,7 @@\n \n   private StateMachine<ComponentState, ComponentEventType, ComponentEvent>\n       stateMachine;\n-  private AsyncDispatcher compInstanceDispatcher;\n+  private AsyncDispatcher dispatcher;\n   private static final StateMachineFactory<Component, ComponentState, ComponentEventType, ComponentEvent>\n       stateMachineFactory =\n       new StateMachineFactory<Component, ComponentState, ComponentEventType, ComponentEvent>(\n@@ -149,7 +150,7 @@ public Component(\n     this.readLock = lock.readLock();\n     this.writeLock = lock.writeLock();\n     this.stateMachine = stateMachineFactory.make(this);\n-    compInstanceDispatcher = scheduler.getCompInstanceDispatcher();\n+    dispatcher = scheduler.getDispatcher();\n     failureTracker =\n         new ContainerFailureTracker(context, this);\n     probe = MonitorUtils.getProbe(componentSpec.getReadinessCheck());\n@@ -256,30 +257,18 @@ public void transition(Component component, ComponentEvent event) {\n         component.releaseContainer(container);\n         return;\n       }\n-      if (instance.hasContainer()) {\n-        LOG.info(\n-            \"[COMPONENT {}]: Instance {} already has container, release \" +\n-                \"surplus container {}\",\n-            instance.getCompName(), instance.getCompInstanceId(), container\n-                .getId());\n-        component.releaseContainer(container);\n-        return;\n-      }\n+\n       component.pendingInstances.remove(instance);\n-      LOG.info(\"[COMPONENT {}]: Recovered {} for component instance {} on \" +\n-              \"host {}, num pending component instances reduced to {} \",\n-          component.getName(), container.getId(), instance\n-              .getCompInstanceName(), container.getNodeId(), component\n-              .pendingInstances.size());\n       instance.setContainer(container);\n       ProviderUtils.initCompInstanceDir(component.getContext().fs, instance);\n       component.getScheduler().addLiveCompInstance(container.getId(), instance);\n-      LOG.info(\"[COMPONENT {}]: Marking {} as started for component \" +\n-          \"instance {}\", component.getName(), event.getContainer().getId(),\n-          instance.getCompInstanceId());\n-      component.compInstanceDispatcher.getEventHandler().handle(\n-          new ComponentInstanceEvent(instance.getContainerId(),\n-              START));\n+      LOG.info(\"[COMPONENT {}]: Recovered {} for component instance {} on \" +\n+              \"host {}, num pending component instances reduced to {} \",\n+          component.getName(), container.getId(),\n+          instance.getCompInstanceName(), container.getNodeId(),\n+          component.pendingInstances.size());\n+      component.dispatcher.getEventHandler().handle(\n+          new ComponentInstanceEvent(container.getId(), START));\n     }\n   }\n \n@@ -288,9 +277,8 @@ public void transition(Component component, ComponentEvent event) {\n \n     @Override public ComponentState transition(Component component,\n         ComponentEvent event) {\n-      component.compInstanceDispatcher.getEventHandler().handle(\n-          new ComponentInstanceEvent(event.getInstance().getContainerId(),\n-              START));\n+      component.dispatcher.getEventHandler().handle(\n+          new ComponentInstanceEvent(event.getContainerId(), START));\n       return checkIfStable(component);\n     }\n   }\n@@ -313,23 +301,16 @@ private static ComponentState checkIfStable(Component component) {\n     @Override\n     public void transition(Component component, ComponentEvent event) {\n       component.updateMetrics(event.getStatus());\n-\n-      // add back to pending list\n-      component.pendingInstances.add(event.getInstance());\n-      LOG.info(\n-          \"[COMPONENT {}]: {} completed, num pending comp instances increased to {}.\",\n-          component.getName(), event.getStatus().getContainerId(),\n-          component.pendingInstances.size());\n-      component.compInstanceDispatcher.getEventHandler().handle(\n+      component.dispatcher.getEventHandler().handle(\n           new ComponentInstanceEvent(event.getStatus().getContainerId(),\n               STOP).setStatus(event.getStatus()));\n       component.componentSpec.setState(\n           org.apache.hadoop.yarn.service.api.records.ComponentState.FLEXING);\n     }\n   }\n \n-  public ServiceMetrics getCompMetrics () {\n-    return componentMetrics;\n+  public void reInsertPendingInstance(ComponentInstance instance) {\n+    pendingInstances.add(instance);\n   }\n \n   private void releaseContainer(Container container) {\n@@ -581,4 +562,9 @@ public void handle(ComponentEvent event) {\n   public ServiceContext getContext() {\n     return context;\n   }\n+\n+  // Only for testing\n+  public List<ComponentInstance> getPendingInstances() {\n+    return pendingInstances;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/Component.java",
                "sha": "88f47635e275a3d4e85b2954fab4db0dc4ad75bf",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/ComponentEvent.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/ComponentEvent.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/ComponentEvent.java",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.yarn.service.component;\n \n import org.apache.hadoop.yarn.api.records.Container;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.ContainerStatus;\n import org.apache.hadoop.yarn.event.AbstractEvent;\n import org.apache.hadoop.yarn.service.component.instance.ComponentInstance;\n@@ -30,6 +31,16 @@\n   private Container container;\n   private ComponentInstance instance;\n   private ContainerStatus status;\n+  private ContainerId containerId;\n+\n+  public ContainerId getContainerId() {\n+    return containerId;\n+  }\n+\n+  public ComponentEvent setContainerId(ContainerId containerId) {\n+    this.containerId = containerId;\n+    return this;\n+  }\n \n   public ComponentEvent(String name, ComponentEventType type) {\n     super(type);",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/ComponentEvent.java",
                "sha": "447b436fc9d9853a06901c7548ba584c5f2ae357",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 39,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "patch": "@@ -146,7 +146,7 @@ public ComponentInstance(Component component,\n       compInstance.containerStatusFuture =\n           compInstance.scheduler.executorService.scheduleAtFixedRate(\n               new ContainerStatusRetriever(compInstance.scheduler,\n-                  compInstance.getContainerId(), compInstance), 0, 1,\n+                  event.getContainerId(), compInstance), 0, 1,\n               TimeUnit.SECONDS);\n       compInstance.component.incRunningContainers();\n       long containerStartTime = System.currentTimeMillis();\n@@ -160,10 +160,10 @@ public ComponentInstance(Component component,\n       }\n       org.apache.hadoop.yarn.service.api.records.Container container =\n           new org.apache.hadoop.yarn.service.api.records.Container();\n-      container.setId(compInstance.getContainerId().toString());\n+      container.setId(event.getContainerId().toString());\n       container.setLaunchTime(new Date(containerStartTime));\n       container.setState(ContainerState.RUNNING_BUT_UNREADY);\n-      container.setBareHost(compInstance.container.getNodeId().getHost());\n+      container.setBareHost(compInstance.getNodeId().getHost());\n       container.setComponentInstanceName(compInstance.getCompInstanceName());\n       if (compInstance.containerSpec != null) {\n         // remove the previous container.\n@@ -219,15 +219,11 @@ public void transition(ComponentInstance compInstance,\n       // re-ask the failed container.\n       Component comp = compInstance.component;\n       comp.requestContainers(1);\n-      LOG.info(compInstance.getCompInstanceId()\n-              + \": Container completed. Requested a new container.\" + System\n-              .lineSeparator() + \" exitStatus={}, diagnostics={}.\",\n-          event.getStatus().getExitStatus(),\n-          event.getStatus().getDiagnostics());\n       String containerDiag =\n           compInstance.getCompInstanceId() + \": \" + event.getStatus()\n               .getDiagnostics();\n       compInstance.diagnostics.append(containerDiag + System.lineSeparator());\n+      compInstance.cancelContainerStatusRetriever();\n \n       if (compInstance.getState().equals(READY)) {\n         compInstance.component.decContainersReady();\n@@ -255,18 +251,28 @@ public void transition(ComponentInstance compInstance,\n         // hdfs dir content will be overwritten when a new container gets started,\n         // so no need remove.\n         compInstance.scheduler.executorService\n-            .submit(compInstance::cleanupRegistry);\n+            .submit(() -> compInstance.cleanupRegistry(event.getContainerId()));\n+\n         if (compInstance.timelineServiceEnabled) {\n           // record in ATS\n-          compInstance.serviceTimelinePublisher.componentInstanceFinished\n-              (compInstance, event.getStatus().getExitStatus(), containerDiag);\n+          compInstance.serviceTimelinePublisher\n+              .componentInstanceFinished(event.getContainerId(),\n+                  event.getStatus().getExitStatus(), containerDiag);\n         }\n         compInstance.containerSpec.setState(ContainerState.STOPPED);\n       }\n \n       // remove the failed ContainerId -> CompInstance mapping\n       comp.getScheduler().removeLiveCompInstance(event.getContainerId());\n \n+      comp.reInsertPendingInstance(compInstance);\n+\n+      LOG.info(compInstance.getCompInstanceId()\n+              + \": {} completed. Reinsert back to pending list and requested \" +\n+              \"a new container.\" + System.lineSeparator() +\n+              \" exitStatus={}, diagnostics={}.\",\n+          event.getContainerId(), event.getStatus().getExitStatus(),\n+          event.getStatus().getDiagnostics());\n       if (shouldExit) {\n         // Sleep for 5 seconds in hope that the state can be recorded in ATS.\n         // in case there's a client polling the comp state, it can be notified.\n@@ -277,8 +283,6 @@ public void transition(ComponentInstance compInstance,\n         }\n         ExitUtil.terminate(-1);\n       }\n-\n-      compInstance.removeContainer();\n     }\n   }\n \n@@ -312,15 +316,6 @@ public void handle(ComponentInstanceEvent event) {\n     }\n   }\n \n-  public boolean hasContainer() {\n-    return this.container != null;\n-  }\n-\n-  public void removeContainer() {\n-    this.container = null;\n-    this.compInstanceId.setContainerId(null);\n-  }\n-\n   public void setContainer(Container container) {\n     this.container = container;\n     this.compInstanceId.setContainerId(container.getId());\n@@ -337,7 +332,7 @@ public ContainerStatus getContainerStatus() {\n   public void updateContainerStatus(ContainerStatus status) {\n     this.status = status;\n     org.apache.hadoop.yarn.service.api.records.Container container =\n-        getCompSpec().getContainer(getContainerId().toString());\n+        getCompSpec().getContainer(status.getContainerId().toString());\n     if (container != null) {\n       container.setIp(StringUtils.join(\",\", status.getIPs()));\n       container.setHostname(status.getHost());\n@@ -348,10 +343,6 @@ public void updateContainerStatus(ContainerStatus status) {\n     updateServiceRecord(yarnRegistryOperations, status);\n   }\n \n-  public ContainerId getContainerId() {\n-    return container.getId();\n-  }\n-\n   public String getCompName() {\n     return compInstanceId.getCompName();\n   }\n@@ -423,12 +414,7 @@ private  void updateServiceRecord(\n   public void destroy() {\n     LOG.info(getCompInstanceId() + \": Flexed down by user, destroying.\");\n     diagnostics.append(getCompInstanceId() + \": Flexed down by user\");\n-    if (container != null) {\n-      scheduler.removeLiveCompInstance(container.getId());\n-      component.getScheduler().getAmRMClient()\n-          .releaseAssignedContainer(container.getId());\n-      getCompSpec().removeContainer(containerSpec);\n-    }\n+\n     // update metrics\n     if (getState() == STARTED) {\n       component.decRunningContainers();\n@@ -437,16 +423,29 @@ public void destroy() {\n       component.decContainersReady();\n       component.decRunningContainers();\n     }\n+    getCompSpec().removeContainer(containerSpec);\n+\n+    if (container == null) {\n+      LOG.info(getCompInstanceId() + \" no container is assigned when \" +\n+          \"destroying\");\n+      return;\n+    }\n+\n+    ContainerId containerId = container.getId();\n+    scheduler.removeLiveCompInstance(containerId);\n+    component.getScheduler().getAmRMClient()\n+        .releaseAssignedContainer(containerId);\n \n     if (timelineServiceEnabled) {\n-      serviceTimelinePublisher.componentInstanceFinished(this,\n+      serviceTimelinePublisher.componentInstanceFinished(containerId,\n           KILLED_BY_APPMASTER, diagnostics.toString());\n     }\n-    scheduler.executorService.submit(this::cleanupRegistryAndCompHdfsDir);\n+    cancelContainerStatusRetriever();\n+    scheduler.executorService.submit(() ->\n+        cleanupRegistryAndCompHdfsDir(containerId));\n   }\n \n-  private void cleanupRegistry() {\n-    ContainerId containerId = getContainerId();\n+  private void cleanupRegistry(ContainerId containerId) {\n     String cid = RegistryPathUtils.encodeYarnID(containerId.toString());\n     try {\n        yarnRegistryOperations.deleteComponent(getCompInstanceId(), cid);\n@@ -456,8 +455,8 @@ private void cleanupRegistry() {\n   }\n \n   //TODO Maybe have a dedicated cleanup service.\n-  public void cleanupRegistryAndCompHdfsDir() {\n-    cleanupRegistry();\n+  public void cleanupRegistryAndCompHdfsDir(ContainerId containerId) {\n+    cleanupRegistry(containerId);\n     try {\n       if (compInstanceDir != null && fs.exists(compInstanceDir)) {\n         boolean deleted = fs.delete(compInstanceDir, true);\n@@ -515,6 +514,12 @@ public void cleanupRegistryAndCompHdfsDir() {\n     }\n   }\n \n+  private void cancelContainerStatusRetriever() {\n+    if (containerStatusFuture != null && !containerStatusFuture.isDone()) {\n+      containerStatusFuture.cancel(true);\n+    }\n+  }\n+\n   @Override\n   public int compareTo(ComponentInstance to) {\n     long delta = containerStartedTime - to.containerStartedTime;",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/component/instance/ComponentInstance.java",
                "sha": "509f6675006d4a3ebfa9da02234bbb821e739c8a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/containerlaunch/ContainerLaunchService.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/containerlaunch/ContainerLaunchService.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/containerlaunch/ContainerLaunchService.java",
                "patch": "@@ -87,7 +87,7 @@ public ContainerLauncher(\n       AbstractLauncher launcher = new AbstractLauncher(fs, null);\n       try {\n         provider.buildContainerLaunchContext(launcher, service,\n-            instance, fs, getConfig());\n+            instance, fs, getConfig(), container);\n         instance.getComponent().getScheduler().getNmClient()\n             .startContainerAsync(container,\n                 launcher.completeContainerLaunch());",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/containerlaunch/ContainerLaunchService.java",
                "sha": "b9f3a245c69b27859cc393ef4b0105bf29659259",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/AbstractProviderService.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/AbstractProviderService.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/AbstractProviderService.java",
                "patch": "@@ -20,6 +20,7 @@\n import org.apache.commons.lang.StringUtils;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.yarn.api.ApplicationConstants;\n+import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.service.api.records.Service;\n import org.apache.hadoop.yarn.service.conf.YarnServiceConf;\n import org.apache.hadoop.yarn.service.api.records.Component;\n@@ -55,7 +56,7 @@ public abstract void processArtifact(AbstractLauncher launcher,\n \n   public void buildContainerLaunchContext(AbstractLauncher launcher,\n       Service service, ComponentInstance instance,\n-      SliderFileSystem fileSystem, Configuration yarnConf)\n+      SliderFileSystem fileSystem, Configuration yarnConf, Container container)\n       throws IOException, SliderException {\n     Component component = instance.getComponent().getComponentSpec();;\n     processArtifact(launcher, instance, fileSystem, service);\n@@ -67,7 +68,7 @@ public void buildContainerLaunchContext(AbstractLauncher launcher,\n     Map<String, String> globalTokens =\n         instance.getComponent().getScheduler().globalTokens;\n     Map<String, String> tokensForSubstitution = ProviderUtils\n-        .initCompTokensForSubstitute(instance);\n+        .initCompTokensForSubstitute(instance, container);\n     tokensForSubstitution.putAll(globalTokens);\n     // Set the environment variables in launcher\n     launcher.putEnv(ServiceUtils",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/AbstractProviderService.java",
                "sha": "70155915ea627ab730acfc4a0aa36ad1c24c83c6",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/ProviderService.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/ProviderService.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/ProviderService.java",
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.hadoop.yarn.service.provider;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.service.api.records.Service;\n import org.apache.hadoop.yarn.service.utils.SliderFileSystem;\n import org.apache.hadoop.yarn.service.exceptions.SliderException;\n@@ -34,6 +35,6 @@\n    */\n   void buildContainerLaunchContext(AbstractLauncher containerLauncher,\n       Service service, ComponentInstance instance,\n-      SliderFileSystem sliderFileSystem, Configuration yarnConf)\n-      throws IOException, SliderException;\n+      SliderFileSystem sliderFileSystem, Configuration yarnConf, Container\n+      container) throws IOException, SliderException;\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/ProviderService.java",
                "sha": "11015ea17504455bfdc21a02b17316b1a346352a",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/ProviderUtils.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/ProviderUtils.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/ProviderUtils.java",
                "patch": "@@ -24,6 +24,7 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.permission.FsAction;\n import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.api.records.LocalResource;\n import org.apache.hadoop.yarn.api.records.LocalResourceType;\n import org.apache.hadoop.yarn.service.ServiceContext;\n@@ -393,13 +394,13 @@ private static void resolvePlainTemplateAndSaveOnHdfs(FileSystem fs,\n    * @return tokens to replace\n    */\n   public static Map<String, String> initCompTokensForSubstitute(\n-      ComponentInstance instance) {\n+      ComponentInstance instance, Container container) {\n     Map<String, String> tokens = new HashMap<>();\n     tokens.put(COMPONENT_NAME, instance.getCompSpec().getName());\n     tokens\n         .put(COMPONENT_NAME_LC, instance.getCompSpec().getName().toLowerCase());\n     tokens.put(COMPONENT_INSTANCE_NAME, instance.getCompInstanceName());\n-    tokens.put(CONTAINER_ID, instance.getContainer().getId().toString());\n+    tokens.put(CONTAINER_ID, container.getId().toString());\n     tokens.put(COMPONENT_ID,\n         String.valueOf(instance.getCompInstanceId().getId()));\n     tokens.putAll(instance.getComponent().getDependencyHostIpTokens());",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/provider/ProviderUtils.java",
                "sha": "c0c44c3db23b3247b905c9d61304f5ff8a82a875",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/timelineservice/ServiceTimelinePublisher.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/timelineservice/ServiceTimelinePublisher.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 2,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/timelineservice/ServiceTimelinePublisher.java",
                "patch": "@@ -20,6 +20,7 @@\n \n import org.apache.hadoop.metrics2.AbstractMetric;\n import org.apache.hadoop.service.CompositeService;\n+import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n import org.apache.hadoop.yarn.api.records.timelineservice.TimelineEntity;\n import org.apache.hadoop.yarn.api.records.timelineservice.TimelineEvent;\n@@ -178,10 +179,10 @@ public void componentInstanceStarted(Container container,\n     putEntity(entity);\n   }\n \n-  public void componentInstanceFinished(ComponentInstance instance,\n+  public void componentInstanceFinished(ContainerId containerId,\n       int exitCode, String diagnostics) {\n     TimelineEntity entity = createComponentInstanceEntity(\n-        instance.getContainer().getId().toString());\n+        containerId.toString());\n \n     // create info keys\n     Map<String, Object> entityInfos = new HashMap<String, Object>();",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/main/java/org/apache/hadoop/yarn/service/timelineservice/ServiceTimelinePublisher.java",
                "sha": "949ce19c8dc1880107dc1e7fdcaff7a4ddc6fc60",
                "status": "modified"
            },
            {
                "additions": 52,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/MockServiceAM.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/MockServiceAM.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 14,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/MockServiceAM.java",
                "patch": "@@ -24,14 +24,8 @@\n import org.apache.hadoop.test.GenericTestUtils;\n import org.apache.hadoop.yarn.api.protocolrecords.AllocateResponse;\n import org.apache.hadoop.yarn.api.protocolrecords.RegisterApplicationMasterResponse;\n-import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n-import org.apache.hadoop.yarn.api.records.ApplicationId;\n-import org.apache.hadoop.yarn.api.records.Container;\n-import org.apache.hadoop.yarn.api.records.ContainerId;\n-import org.apache.hadoop.yarn.api.records.FinalApplicationStatus;\n-import org.apache.hadoop.yarn.api.records.NodeId;\n-import org.apache.hadoop.yarn.api.records.Priority;\n-import org.apache.hadoop.yarn.api.records.Resource;\n+\n+import org.apache.hadoop.yarn.api.records.*;\n import org.apache.hadoop.yarn.client.api.AMRMClient;\n import org.apache.hadoop.yarn.client.api.NMClient;\n import org.apache.hadoop.yarn.client.api.async.AMRMClientAsync;\n@@ -42,15 +36,15 @@\n import org.apache.hadoop.yarn.service.api.records.Service;\n import org.apache.hadoop.yarn.service.component.Component;\n import org.apache.hadoop.yarn.service.component.ComponentState;\n+import org.apache.hadoop.yarn.service.component.instance.ComponentInstance;\n+import org.apache.hadoop.yarn.service.component.instance.ComponentInstanceState;\n import org.apache.hadoop.yarn.service.exceptions.BadClusterStateException;\n import org.apache.hadoop.yarn.service.registry.YarnRegistryViewForProviders;\n import org.apache.hadoop.yarn.service.utils.SliderFileSystem;\n+import org.apache.hadoop.yarn.util.Records;\n \n import java.io.IOException;\n-import java.util.Collections;\n-import java.util.Iterator;\n-import java.util.LinkedList;\n-import java.util.List;\n+import java.util.*;\n import java.util.concurrent.TimeoutException;\n \n import static org.mockito.Mockito.mock;\n@@ -63,6 +57,8 @@\n   final List<Container> feedContainers =\n       Collections.synchronizedList(new LinkedList<>());\n \n+  final List<ContainerStatus> failedContainers =\n+      Collections.synchronizedList(new LinkedList<>());\n   public MockServiceAM(Service service) {\n     super(service.getName());\n     this.service = service;\n@@ -102,10 +98,10 @@ protected YarnRegistryViewForProviders createYarnRegistryOperations(\n \n             AllocateResponse.AllocateResponseBuilder builder =\n                 AllocateResponse.newBuilder();\n+            // add new containers if any\n             synchronized (feedContainers) {\n               if (feedContainers.isEmpty()) {\n                 System.out.println(\"Allocating........ no containers\");\n-                return builder.build();\n               } else {\n                 // The AMRMClient will return containers for compoenent that are\n                 // at FLEXING state\n@@ -121,9 +117,20 @@ protected YarnRegistryViewForProviders createYarnRegistryOperations(\n                     itor.remove();\n                   }\n                 }\n-                return builder.allocatedContainers(allocatedContainers).build();\n+                builder.allocatedContainers(allocatedContainers);\n+              }\n+            }\n+\n+            // add failed containers if any\n+            synchronized (failedContainers) {\n+              if (!failedContainers.isEmpty()) {\n+                List<ContainerStatus> failed =\n+                    new LinkedList<>(failedContainers);\n+                failedContainers.clear();\n+                builder.completedContainersStatuses(failed);\n               }\n             }\n+            return builder.build();\n           }\n \n           @Override\n@@ -184,6 +191,19 @@ public Container feedContainerToComp(Service service, int id,\n     return container;\n   }\n \n+  public void feedFailedContainerToComp(Service service, int id, String\n+      compName) {\n+    ApplicationId applicationId = ApplicationId.fromString(service.getId());\n+    ContainerId containerId = ContainerId\n+        .newContainerId(ApplicationAttemptId.newInstance(applicationId, 1), id);\n+    ContainerStatus containerStatus = Records.newRecord(ContainerStatus.class);\n+    containerStatus.setContainerId(containerId);\n+    synchronized (failedContainers) {\n+      failedContainers.add(containerStatus);\n+    }\n+  }\n+\n+\n   public void flexComponent(String compName, long numberOfContainers)\n       throws IOException {\n     ClientAMProtocol.ComponentCountProto componentCountProto =\n@@ -218,4 +238,22 @@ public void waitForNumDesiredContainers(String compName,\n       }\n     }, 1000, 20000);\n   }\n+\n+\n+  public ComponentInstance getCompInstance(String compName, String\n+      instanceName) {\n+    return context.scheduler.getAllComponents().get(compName)\n+        .getComponentInstance(instanceName);\n+  }\n+\n+  public void waitForCompInstanceState(ComponentInstance instance,\n+      ComponentInstanceState state)\n+      throws TimeoutException, InterruptedException {\n+    GenericTestUtils.waitFor(new Supplier<Boolean>() {\n+      @Override\n+      public Boolean get() {\n+        return instance.getState().equals(state);\n+      }\n+    }, 1000, 20000);\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/MockServiceAM.java",
                "sha": "429816137c5f74eb220ea0c15cab54196acbd01f",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/ServiceTestUtils.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/ServiceTestUtils.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/ServiceTestUtils.java",
                "patch": "@@ -65,6 +65,7 @@\n \n   private MiniYARNCluster yarnCluster = null;\n   private MiniDFSCluster hdfsCluster = null;\n+  TestingCluster zkCluster;\n   private FileSystem fs = null;\n   private Configuration conf = null;\n   public static final int NUM_NMS = 1;\n@@ -165,7 +166,6 @@ protected void setupInternal(int numNodeManager)\n     conf.setBoolean(NM_VMEM_CHECK_ENABLED, false);\n     conf.setBoolean(NM_PMEM_CHECK_ENABLED, false);\n     // setup zk cluster\n-    TestingCluster zkCluster;\n     zkCluster = new TestingCluster(1);\n     zkCluster.start();\n     conf.set(YarnConfiguration.RM_ZK_ADDRESS, zkCluster.getConnectString());\n@@ -239,6 +239,9 @@ public void shutdown() throws IOException {\n         hdfsCluster = null;\n       }\n     }\n+    if (zkCluster != null) {\n+      zkCluster.stop();\n+    }\n     if (basedir != null) {\n       FileUtils.deleteDirectory(basedir);\n     }",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/ServiceTestUtils.java",
                "sha": "a70a0c280d377232fdc5a4bb301e7bb0b8e090b4",
                "status": "modified"
            },
            {
                "additions": 109,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceAM.java",
                "changes": 109,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceAM.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceAM.java",
                "patch": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.hadoop.yarn.service;\n+\n+import org.apache.commons.io.FileUtils;\n+import org.apache.curator.test.TestingCluster;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n+import org.apache.hadoop.yarn.conf.YarnConfiguration;\n+import org.apache.hadoop.yarn.service.api.records.Service;\n+import org.apache.hadoop.yarn.service.component.instance.ComponentInstance;\n+import org.apache.hadoop.yarn.service.component.instance.ComponentInstanceState;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.concurrent.TimeoutException;\n+\n+import static org.apache.hadoop.registry.client.api.RegistryConstants\n+    .KEY_REGISTRY_ZK_QUORUM;\n+\n+public class TestServiceAM extends ServiceTestUtils{\n+\n+  private File basedir;\n+  YarnConfiguration conf = new YarnConfiguration();\n+  TestingCluster zkCluster;\n+\n+  @Before\n+  public void setup() throws Exception {\n+    basedir = new File(\"target\", \"apps\");\n+    if (basedir.exists()) {\n+      FileUtils.deleteDirectory(basedir);\n+    } else {\n+      basedir.mkdirs();\n+    }\n+    zkCluster = new TestingCluster(1);\n+    zkCluster.start();\n+    conf.set(KEY_REGISTRY_ZK_QUORUM, zkCluster.getConnectString());\n+    System.out.println(\"ZK cluster: \" +  zkCluster.getConnectString());\n+  }\n+\n+  @After\n+  public void tearDown() throws IOException {\n+    if (basedir != null) {\n+      FileUtils.deleteDirectory(basedir);\n+    }\n+    if (zkCluster != null) {\n+      zkCluster.stop();\n+    }\n+  }\n+\n+  // Race condition YARN-7486\n+  // 1. Allocate 1 container to compa and wait it to be started\n+  // 2. Fail this container, and in the meanwhile allocate the 2nd container.\n+  // 3. The 2nd container should not be assigned to compa-0 instance, because\n+  //   the compa-0 instance is not stopped yet.\n+  // 4. check compa still has the instance in the pending list.\n+  @Test\n+  public void testContainerCompleted() throws TimeoutException,\n+      InterruptedException {\n+    ApplicationId applicationId = ApplicationId.newInstance(123456, 1);\n+    Service exampleApp = new Service();\n+    exampleApp.setId(applicationId.toString());\n+    exampleApp.setName(\"testContainerCompleted\");\n+    exampleApp.addComponent(createComponent(\"compa\", 1, \"pwd\"));\n+\n+    MockServiceAM am = new MockServiceAM(exampleApp);\n+    am.init(conf);\n+    am.start();\n+\n+    ComponentInstance compa0 = am.getCompInstance(\"compa\", \"compa-0\");\n+    // allocate a container\n+    am.feedContainerToComp(exampleApp, 1, \"compa\");\n+    am.waitForCompInstanceState(compa0, ComponentInstanceState.STARTED);\n+\n+    System.out.println(\"Fail the container 1\");\n+    // fail the container\n+    am.feedFailedContainerToComp(exampleApp, 1, \"compa\");\n+\n+    // allocate the second container immediately, this container will not be\n+    // assigned to comp instance\n+    // because the instance is not yet added to the pending list.\n+    am.feedContainerToComp(exampleApp, 2, \"compa\");\n+\n+    am.waitForCompInstanceState(compa0, ComponentInstanceState.INIT);\n+    // still 1 pending instance\n+    Assert.assertEquals(1,\n+        am.getComponent(\"compa\").getPendingInstances().size());\n+    am.stop();\n+  }\n+}",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/TestServiceAM.java",
                "sha": "fb4de0d57147e7e087c17670ebbb615d3e63e52e",
                "status": "added"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/monitor/TestServiceMonitor.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/monitor/TestServiceMonitor.java?ref=f4d5d20286eb05449f6fd7cd6ff0554228205fe2",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/monitor/TestServiceMonitor.java",
                "patch": "@@ -20,6 +20,7 @@\n package org.apache.hadoop.yarn.service.monitor;\n \n import org.apache.commons.io.FileUtils;\n+import org.apache.curator.test.TestingCluster;\n import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.service.MockServiceAM;\n@@ -37,10 +38,14 @@\n import java.io.IOException;\n import java.util.Collections;\n \n+import static org.apache.hadoop.registry.client.api.RegistryConstants\n+    .KEY_REGISTRY_ZK_QUORUM;\n+\n public class TestServiceMonitor extends ServiceTestUtils {\n \n   private File basedir;\n   YarnConfiguration conf = new YarnConfiguration();\n+  TestingCluster zkCluster;\n \n   @Before\n   public void setup() throws Exception {\n@@ -51,13 +56,20 @@ public void setup() throws Exception {\n       basedir.mkdirs();\n     }\n     conf.setLong(YarnServiceConf.READINESS_CHECK_INTERVAL, 2);\n+    zkCluster = new TestingCluster(1);\n+    zkCluster.start();\n+    conf.set(KEY_REGISTRY_ZK_QUORUM, zkCluster.getConnectString());\n+    System.out.println(\"ZK cluster: \" +  zkCluster.getConnectString());\n   }\n \n   @After\n   public void tearDown() throws IOException {\n     if (basedir != null) {\n       FileUtils.deleteDirectory(basedir);\n     }\n+    if (zkCluster != null) {\n+      zkCluster.stop();\n+    }\n   }\n \n   // Create compa with 1 container",
                "raw_url": "https://github.com/apache/hadoop/raw/f4d5d20286eb05449f6fd7cd6ff0554228205fe2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-services/hadoop-yarn-services-core/src/test/java/org/apache/hadoop/yarn/service/monitor/TestServiceMonitor.java",
                "sha": "e25d38dd491e85acb07bf163ffd5682581c2ee4f",
                "status": "modified"
            }
        ],
        "message": "YARN-7486. Race condition in service AM that can cause NPE. Contributed by Jian He",
        "parent": "https://github.com/apache/hadoop/commit/462e25a3b264e1148d0cbca00db7f10d43a0555f",
        "repo": "hadoop",
        "unit_tests": [
            "TestComponent.java",
            "TestComponentInstance.java",
            "TestAbstractProviderService.java",
            "TestProviderUtils.java",
            "TestServiceTimelinePublisher.java"
        ]
    },
    "hadoop_f5756a2": {
        "bug_id": "hadoop_f5756a2",
        "commit": "https://github.com/apache/hadoop/commit/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -543,6 +543,8 @@ Trunk (Unreleased)\n     HADOOP-12638. UnsatisfiedLinkError while checking ISA-L in checknative\n     command. (Kai Sasaki via Colin P. McCabe)\n \n+    HADOOP-12615. Fix NPE in MiniKMS.start(). (Wei-Chiu Chuang via zhz)\n+\n   OPTIMIZATIONS\n \n     HADOOP-7761. Improve the performance of raw comparisons. (todd)",
                "raw_url": "https://github.com/apache/hadoop/raw/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "3c98eadad054bd20bdfcbd7d9636e9e9f4c98113",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/hadoop/blob/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ThreadUtil.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ThreadUtil.java?ref=f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ThreadUtil.java",
                "patch": "@@ -22,6 +22,9 @@\n \n import org.apache.hadoop.classification.InterfaceStability;\n \n+import java.io.IOException;\n+import java.io.InputStream;\n+\n @InterfaceStability.Evolving\n public class ThreadUtil {\n   \n@@ -46,4 +49,31 @@ public static void sleepAtLeastIgnoreInterrupts(long millis) {\n       }\n     }\n   }\n+\n+  /**\n+   * Convenience method that returns a resource as inputstream from the\n+   * classpath.\n+   * <p>\n+   * It first attempts to use the Thread's context classloader and if not\n+   * set it uses the class' classloader.\n+   *\n+   * @param resourceName resource to retrieve.\n+   *\n+   * @throws IOException thrown if resource cannot be loaded\n+   * @return inputstream with the resource.\n+   */\n+  public static InputStream getResourceAsStream(String resourceName)\n+      throws IOException {\n+    ClassLoader cl = Thread.currentThread().getContextClassLoader();\n+    if (cl == null) {\n+      throw new IOException(\"Can not read resource file '\" + resourceName +\n+          \"' because class loader of the current thread is null\");\n+    }\n+    InputStream is = cl.getResourceAsStream(resourceName);\n+    if (is == null) {\n+      throw new IOException(\"Can not read resource file '\" +\n+          resourceName + \"'\");\n+    }\n+    return is;\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/ThreadUtil.java",
                "sha": "ab7b5fdeddbd56e94998c0effb1ab86a085ea70e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/VersionInfo.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/VersionInfo.java?ref=f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
                "deletions": 5,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/VersionInfo.java",
                "patch": "@@ -43,11 +43,7 @@ protected VersionInfo(String component) {\n     String versionInfoFile = component + \"-version-info.properties\";\n     InputStream is = null;\n     try {\n-      is = Thread.currentThread().getContextClassLoader()\n-        .getResourceAsStream(versionInfoFile);\n-      if (is == null) {\n-        throw new IOException(\"Resource not found\");\n-      }\n+      is = ThreadUtil.getResourceAsStream(versionInfoFile);\n       info.load(is);\n     } catch (IOException ex) {\n       LogFactory.getLog(getClass()).warn(\"Could not read '\" +",
                "raw_url": "https://github.com/apache/hadoop/raw/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/VersionInfo.java",
                "sha": "dc8d36959a0867cc2cbcab46688d749c3466fdb1",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/hadoop/blob/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/MiniKMS.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/MiniKMS.java?ref=f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
                "deletions": 10,
                "filename": "hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/MiniKMS.java",
                "patch": "@@ -23,6 +23,7 @@\n import org.apache.hadoop.crypto.key.kms.KMSRESTConstants;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.security.ssl.SslSocketConnectorSecure;\n+import org.apache.hadoop.util.ThreadUtil;\n import org.mortbay.jetty.Connector;\n import org.mortbay.jetty.Server;\n import org.mortbay.jetty.security.SslSocketConnector;\n@@ -34,6 +35,7 @@\n import java.io.InputStream;\n import java.io.OutputStream;\n import java.io.Writer;\n+import java.io.IOException;\n import java.net.InetAddress;\n import java.net.MalformedURLException;\n import java.net.ServerSocket;\n@@ -149,16 +151,26 @@ public MiniKMS(String kmsConfDir, String log4ConfFile, String keyStore,\n     this.inPort = inPort;\n   }\n \n+  private void copyResource(String inputResourceName, File outputFile) throws\n+      IOException {\n+    InputStream is = null;\n+    OutputStream os = null;\n+    try {\n+      is = ThreadUtil.getResourceAsStream(inputResourceName);\n+      os = new FileOutputStream(outputFile);\n+      IOUtils.copy(is, os);\n+    } finally {\n+      IOUtils.closeQuietly(is);\n+      IOUtils.closeQuietly(os);\n+    }\n+  }\n+\n   public void start() throws Exception {\n     ClassLoader cl = Thread.currentThread().getContextClassLoader();\n     System.setProperty(KMSConfiguration.KMS_CONFIG_DIR, kmsConfDir);\n     File aclsFile = new File(kmsConfDir, \"kms-acls.xml\");\n     if (!aclsFile.exists()) {\n-      InputStream is = cl.getResourceAsStream(\"mini-kms-acls-default.xml\");\n-      OutputStream os = new FileOutputStream(aclsFile);\n-      IOUtils.copy(is, os);\n-      is.close();\n-      os.close();\n+      copyResource(\"mini-kms-acls-default.xml\", aclsFile);\n     }\n     File coreFile = new File(kmsConfDir, \"core-site.xml\");\n     if (!coreFile.exists()) {\n@@ -195,11 +207,7 @@ public void start() throws Exception {\n           \"/kms-webapp/WEB-INF\");\n       webInf.mkdirs();\n       new File(webInf, \"web.xml\").delete();\n-      InputStream is = cl.getResourceAsStream(\"kms-webapp/WEB-INF/web.xml\");\n-      OutputStream os = new FileOutputStream(new File(webInf, \"web.xml\"));\n-      IOUtils.copy(is, os);\n-      is.close();\n-      os.close();\n+      copyResource(\"kms-webapp/WEB-INF/web.xml\", new File(webInf, \"web.xml\"));\n       webappPath = webInf.getParentFile().getAbsolutePath();\n     } else {\n       webappPath = cl.getResource(\"kms-webapp\").getPath();",
                "raw_url": "https://github.com/apache/hadoop/raw/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/MiniKMS.java",
                "sha": "f520edfa51d390cae77068a50d0e7d2be4746af5",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop/blob/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMSAudit.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMSAudit.java?ref=f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
                "deletions": 4,
                "filename": "hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMSAudit.java",
                "patch": "@@ -19,11 +19,15 @@\n \n import java.io.ByteArrayOutputStream;\n import java.io.FilterOutputStream;\n+import java.io.InputStream;\n+import java.io.IOException;\n import java.io.OutputStream;\n import java.io.PrintStream;\n \n import org.apache.hadoop.crypto.key.kms.server.KMS.KMSOp;\n+import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.hadoop.util.ThreadUtil;\n import org.apache.log4j.LogManager;\n import org.apache.log4j.PropertyConfigurator;\n import org.junit.After;\n@@ -52,15 +56,17 @@ public void setOutputStream(OutputStream out) {\n   }\n \n   @Before\n-  public void setUp() {\n+  public void setUp() throws IOException {\n     originalOut = System.err;\n     memOut = new ByteArrayOutputStream();\n     filterOut = new FilterOut(memOut);\n     capturedOut = new PrintStream(filterOut);\n     System.setErr(capturedOut);\n-    PropertyConfigurator.configure(Thread.currentThread().\n-        getContextClassLoader()\n-        .getResourceAsStream(\"log4j-kmsaudit.properties\"));\n+    InputStream is =\n+        ThreadUtil.getResourceAsStream(\"log4j-kmsaudit.properties\");\n+    PropertyConfigurator.configure(is);\n+    IOUtils.closeStream(is);\n+\n     this.kmsAudit = new KMSAudit(1000);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-kms/src/test/java/org/apache/hadoop/crypto/key/kms/server/TestKMSAudit.java",
                "sha": "7e1c0d7b08ec209fded079985ca929c95f797ed7",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/hadoop/blob/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-minikdc/src/main/java/org/apache/hadoop/minikdc/MiniKdc.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-common-project/hadoop-minikdc/src/main/java/org/apache/hadoop/minikdc/MiniKdc.java?ref=f5756a2038cdacc6faf590dcab0aa62d56f5bcaf",
                "deletions": 3,
                "filename": "hadoop-common-project/hadoop-minikdc/src/main/java/org/apache/hadoop/minikdc/MiniKdc.java",
                "patch": "@@ -61,6 +61,7 @@\n import java.io.FileInputStream;\n import java.io.InputStream;\n import java.io.InputStreamReader;\n+import java.io.IOException;\n import java.io.StringReader;\n import java.lang.reflect.Method;\n import java.net.InetAddress;\n@@ -389,6 +390,32 @@ private void initDirectoryService() throws Exception {\n     ds.getAdminSession().add(entry);\n   }\n \n+  /**\n+   * Convenience method that returns a resource as inputstream from the\n+   * classpath.\n+   * <p>\n+   * It first attempts to use the Thread's context classloader and if not\n+   * set it uses the class' classloader.\n+   *\n+   * @param resourceName resource to retrieve.\n+   *\n+   * @throws IOException thrown if resource cannot be loaded\n+   * @return inputstream with the resource.\n+   */\n+  public static InputStream getResourceAsStream(String resourceName)\n+      throws IOException {\n+    ClassLoader cl = Thread.currentThread().getContextClassLoader();\n+    if (cl == null) {\n+      cl = MiniKdc.class.getClassLoader();\n+    }\n+    InputStream is = cl.getResourceAsStream(resourceName);\n+    if (is == null) {\n+      throw new IOException(\"Can not read resource file '\" +\n+          resourceName + \"'\");\n+    }\n+    return is;\n+  }\n+\n   private void initKDCServer() throws Exception {\n     String orgName= conf.getProperty(ORG_NAME);\n     String orgDomain = conf.getProperty(ORG_DOMAIN);\n@@ -400,8 +427,7 @@ private void initKDCServer() throws Exception {\n     map.put(\"3\", orgDomain.toUpperCase(Locale.ENGLISH));\n     map.put(\"4\", bindAddress);\n \n-    ClassLoader cl = Thread.currentThread().getContextClassLoader();\n-    InputStream is1 = cl.getResourceAsStream(\"minikdc.ldiff\");\n+    InputStream is1 = getResourceAsStream(\"minikdc.ldiff\");\n \n     SchemaManager schemaManager = ds.getSchemaManager();\n     LdifReader reader = null;\n@@ -443,7 +469,7 @@ private void initKDCServer() throws Exception {\n     kdc.start();\n \n     StringBuilder sb = new StringBuilder();\n-    InputStream is2 = cl.getResourceAsStream(\"minikdc-krb5.conf\");\n+    InputStream is2 = getResourceAsStream(\"minikdc-krb5.conf\");\n \n     BufferedReader r = null;\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/f5756a2038cdacc6faf590dcab0aa62d56f5bcaf/hadoop-common-project/hadoop-minikdc/src/main/java/org/apache/hadoop/minikdc/MiniKdc.java",
                "sha": "a5253c4521715cc911a29003685f196bce456ad6",
                "status": "modified"
            }
        ],
        "message": "HADOOP-12615. Fix NPE in MiniKMS.start(). Contributed by Wei-Chiu Chuang.\n\nChange-Id: Ie3e148bd1401618b1737a577957298bf622891f4",
        "parent": "https://github.com/apache/hadoop/commit/5104077e1f431ad3675d0b1c5c3cf53936902d8e",
        "repo": "hadoop",
        "unit_tests": [
            "TestMiniKdc.java"
        ]
    },
    "hadoop_f672188": {
        "bug_id": "hadoop_f672188",
        "commit": "https://github.com/apache/hadoop/commit/f67218809c50b194e463af6e6196db298353c8c1",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/f67218809c50b194e463af6e6196db298353c8c1/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=f67218809c50b194e463af6e6196db298353c8c1",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -539,6 +539,9 @@ Release 2.4.0 - UNRELEASED\n     YARN-1670. Fixed a bug in log-aggregation that can cause the writer to write\n     more log-data than the log-length that it records. (Mit Desai via vinodk)\n \n+    YARN-1849. Fixed NPE in ResourceTrackerService#registerNodeManager for UAM\n+    (Karthik Kambatla via jianhe )\n+\n Release 2.3.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/f67218809c50b194e463af6e6196db298353c8c1/hadoop-yarn-project/CHANGES.txt",
                "sha": "cfb0052a201002073a5a8de6ed7ca0adea4dea93",
                "status": "modified"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/hadoop/blob/f67218809c50b194e463af6e6196db298353c8c1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java?ref=f67218809c50b194e463af6e6196db298353c8c1",
                "deletions": 24,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "patch": "@@ -31,6 +31,7 @@\n import org.apache.hadoop.service.AbstractService;\n import org.apache.hadoop.util.VersionUtil;\n import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n+import org.apache.hadoop.yarn.api.records.Container;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.ContainerState;\n import org.apache.hadoop.yarn.api.records.ContainerStatus;\n@@ -187,12 +188,51 @@ protected void serviceStop() throws Exception {\n     super.serviceStop();\n   }\n \n+  /**\n+   * Helper method to handle received ContainerStatus. If this corresponds to\n+   * the completion of a master-container of a managed AM,\n+   * we call the handler for RMAppAttemptContainerFinishedEvent.\n+   */\n+  @SuppressWarnings(\"unchecked\")\n+  @VisibleForTesting\n+  void handleContainerStatus(ContainerStatus containerStatus) {\n+    ApplicationAttemptId appAttemptId =\n+        containerStatus.getContainerId().getApplicationAttemptId();\n+    RMApp rmApp =\n+        rmContext.getRMApps().get(appAttemptId.getApplicationId());\n+    if (rmApp == null) {\n+      LOG.error(\"Received finished container : \"\n+          + containerStatus.getContainerId()\n+          + \"for unknown application \" + appAttemptId.getApplicationId()\n+          + \" Skipping.\");\n+      return;\n+    }\n+\n+    if (rmApp.getApplicationSubmissionContext().getUnmanagedAM()) {\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Ignoring container completion status for unmanaged AM\"\n+            + rmApp.getApplicationId());\n+      }\n+      return;\n+    }\n+\n+    RMAppAttempt rmAppAttempt = rmApp.getRMAppAttempt(appAttemptId);\n+    Container masterContainer = rmAppAttempt.getMasterContainer();\n+    if (masterContainer.getId().equals(containerStatus.getContainerId())\n+        && containerStatus.getState() == ContainerState.COMPLETE) {\n+      // sending master container finished event.\n+      RMAppAttemptContainerFinishedEvent evt =\n+          new RMAppAttemptContainerFinishedEvent(appAttemptId,\n+              containerStatus);\n+      rmContext.getDispatcher().getEventHandler().handle(evt);\n+    }\n+  }\n+\n   @SuppressWarnings(\"unchecked\")\n   @Override\n   public RegisterNodeManagerResponse registerNodeManager(\n       RegisterNodeManagerRequest request) throws YarnException,\n       IOException {\n-\n     NodeId nodeId = request.getNodeId();\n     String host = nodeId.getHost();\n     int cmPort = nodeId.getPort();\n@@ -204,29 +244,7 @@ public RegisterNodeManagerResponse registerNodeManager(\n       LOG.info(\"received container statuses on node manager register :\"\n           + request.getContainerStatuses());\n       for (ContainerStatus containerStatus : request.getContainerStatuses()) {\n-        ApplicationAttemptId appAttemptId =\n-            containerStatus.getContainerId().getApplicationAttemptId();\n-        RMApp rmApp =\n-            rmContext.getRMApps().get(appAttemptId.getApplicationId());\n-        if (rmApp != null) {\n-          RMAppAttempt rmAppAttempt = rmApp.getRMAppAttempt(appAttemptId);\n-          if (rmAppAttempt != null) {\n-            if (rmAppAttempt.getMasterContainer().getId()\n-                .equals(containerStatus.getContainerId())\n-                && containerStatus.getState() == ContainerState.COMPLETE) {\n-              // sending master container finished event.\n-              RMAppAttemptContainerFinishedEvent evt =\n-                  new RMAppAttemptContainerFinishedEvent(appAttemptId,\n-                      containerStatus);\n-              rmContext.getDispatcher().getEventHandler().handle(evt);\n-            }\n-          }\n-        } else {\n-          LOG.error(\"Received finished container :\"\n-              + containerStatus.getContainerId()\n-              + \" for non existing application :\"\n-              + appAttemptId.getApplicationId());\n-        }\n+        handleContainerStatus(containerStatus);\n       }\n     }\n     RegisterNodeManagerResponse response = recordFactory",
                "raw_url": "https://github.com/apache/hadoop/raw/f67218809c50b194e463af6e6196db298353c8c1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ResourceTrackerService.java",
                "sha": "1d4032048e468a9652093acc18cf0c0b82007a70",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/f67218809c50b194e463af6e6196db298353c8c1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java?ref=f67218809c50b194e463af6e6196db298353c8c1",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "patch": "@@ -35,9 +35,11 @@\n \n import javax.crypto.SecretKey;\n \n+import com.google.common.annotations.VisibleForTesting;\n import org.apache.commons.lang.StringUtils;\n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n+import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.security.Credentials;\n import org.apache.hadoop.security.UserGroupInformation;\n@@ -629,7 +631,9 @@ public Container getMasterContainer() {\n     }\n   }\n \n-  private void setMasterContainer(Container container) {\n+  @InterfaceAudience.Private\n+  @VisibleForTesting\n+  public void setMasterContainer(Container container) {\n     masterContainer = container;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/f67218809c50b194e463af6e6196db298353c8c1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/rmapp/attempt/RMAppAttemptImpl.java",
                "sha": "3e90ec8ec1d5aaec6619f357ceaadecbb7bca194",
                "status": "modified"
            },
            {
                "additions": 57,
                "blob_url": "https://github.com/apache/hadoop/blob/f67218809c50b194e463af6e6196db298353c8c1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "changes": 70,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java?ref=f67218809c50b194e463af6e6196db298353c8c1",
                "deletions": 13,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "patch": "@@ -26,8 +26,6 @@\n import java.util.HashMap;\n import java.util.List;\n \n-import org.junit.Assert;\n-\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.io.IOUtils;\n import org.apache.hadoop.metrics2.MetricsSystem;\n@@ -45,21 +43,29 @@\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.event.Dispatcher;\n import org.apache.hadoop.yarn.event.DrainDispatcher;\n+import org.apache.hadoop.yarn.event.Event;\n import org.apache.hadoop.yarn.event.EventHandler;\n import org.apache.hadoop.yarn.server.api.protocolrecords.NodeHeartbeatResponse;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerRequest;\n import org.apache.hadoop.yarn.server.api.protocolrecords.RegisterNodeManagerResponse;\n import org.apache.hadoop.yarn.server.api.records.NodeAction;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.QueueMetrics;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.event.SchedulerEvent;\n import org.apache.hadoop.yarn.server.utils.BuilderUtils;\n import org.apache.hadoop.yarn.util.Records;\n import org.apache.hadoop.yarn.util.YarnVersionInfo;\n+\n import org.junit.After;\n+import org.junit.Assert;\n import org.junit.Test;\n \n import static org.junit.Assert.assertEquals;\n+import static org.mockito.Matchers.any;\n+import static org.mockito.Mockito.never;\n+import static org.mockito.Mockito.spy;\n+import static org.mockito.Mockito.verify;\n \n public class TestResourceTrackerService {\n \n@@ -468,26 +474,64 @@ private void checkUnealthyNMCount(MockRM rm, MockNM nm1, boolean health,\n         ClusterMetrics.getMetrics().getUnhealthyNMs());\n   }\n \n+  @SuppressWarnings(\"unchecked\")\n   @Test\n-  public void testNodeRegistrationWithContainers() throws Exception {\n-    rm = new MockRM();\n-    rm.init(new YarnConfiguration());\n+  public void testHandleContainerStatusInvalidCompletions() throws Exception {\n+    rm = new MockRM(new YarnConfiguration());\n     rm.start();\n-    RMApp app = rm.submitApp(1024);\n \n-    MockNM nm = rm.registerNode(\"host1:1234\", 8192);\n-    nm.nodeHeartbeat(true);\n+    EventHandler handler =\n+        spy(rm.getRMContext().getDispatcher().getEventHandler());\n \n-    // Register node with some container statuses\n+    // Case 1: Unmanaged AM\n+    RMApp app = rm.submitApp(1024, true);\n+\n+    // Case 1.1: AppAttemptId is null\n     ContainerStatus status = ContainerStatus.newInstance(\n         ContainerId.newInstance(ApplicationAttemptId.newInstance(\n             app.getApplicationId(), 2), 1),\n         ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+    rm.getResourceTrackerService().handleContainerStatus(status);\n+    verify(handler, never()).handle((Event) any());\n+\n+    // Case 1.2: Master container is null\n+    RMAppAttemptImpl currentAttempt =\n+        (RMAppAttemptImpl) app.getCurrentAppAttempt();\n+    currentAttempt.setMasterContainer(null);\n+    status = ContainerStatus.newInstance(\n+        ContainerId.newInstance(currentAttempt.getAppAttemptId(), 0),\n+        ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+    rm.getResourceTrackerService().handleContainerStatus(status);\n+    verify(handler, never()).handle((Event)any());\n \n-    // The following shouldn't throw NPE\n-    nm.registerNode(Collections.singletonList(status));\n-    assertEquals(\"Incorrect number of nodes\", 1,\n-        rm.getRMContext().getRMNodes().size());\n+    // Case 2: Managed AM\n+    app = rm.submitApp(1024);\n+\n+    // Case 2.1: AppAttemptId is null\n+    status = ContainerStatus.newInstance(\n+        ContainerId.newInstance(ApplicationAttemptId.newInstance(\n+            app.getApplicationId(), 2), 1),\n+        ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+    try {\n+      rm.getResourceTrackerService().handleContainerStatus(status);\n+    } catch (Exception e) {\n+      // expected - ignore\n+    }\n+    verify(handler, never()).handle((Event)any());\n+\n+    // Case 2.2: Master container is null\n+    currentAttempt =\n+        (RMAppAttemptImpl) app.getCurrentAppAttempt();\n+    currentAttempt.setMasterContainer(null);\n+    status = ContainerStatus.newInstance(\n+        ContainerId.newInstance(currentAttempt.getAppAttemptId(), 0),\n+        ContainerState.COMPLETE, \"Dummy Completed\", 0);\n+    try {\n+      rm.getResourceTrackerService().handleContainerStatus(status);\n+    } catch (Exception e) {\n+      // expected - ignore\n+    }\n+    verify(handler, never()).handle((Event)any());\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/hadoop/raw/f67218809c50b194e463af6e6196db298353c8c1/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/TestResourceTrackerService.java",
                "sha": "2f16b85699d3705be3743e4d54b069b42fab2e3d",
                "status": "modified"
            }
        ],
        "message": "YARN-1849. Fixed NPE in ResourceTrackerService#registerNodeManager for UAM. Contributed by Karthik Kambatla\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1580077 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/a5c08eed16e797d2ba9f98f7bc6a8e1bf09aaddd",
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceTrackerService.java"
        ]
    },
    "hadoop_f74e446": {
        "bug_id": "hadoop_f74e446",
        "commit": "https://github.com/apache/hadoop/commit/f74e44635596276f35b7127f99bc5ab96ab534ed",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/f74e44635596276f35b7127f99bc5ab96ab534ed/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/CHANGES.txt?ref=f74e44635596276f35b7127f99bc5ab96ab534ed",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -139,6 +139,9 @@ Trunk (Unreleased)\n \n     MAPREDUCE-5717. Task pings are interpreted as task progress (jlowe)\n \n+    MAPREDUCE-5867. Fix NPE in KillAMPreemptionPolicy related to \n+    ProportionalCapacityPreemptionPolicy (Sunil G via devaraj)\n+\n Release 2.5.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/f74e44635596276f35b7127f99bc5ab96ab534ed/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "c2a4f14514d0bd184f92e05ba595fadc2966b1b3",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop/blob/f74e44635596276f35b7127f99bc5ab96ab534ed/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/KillAMPreemptionPolicy.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/KillAMPreemptionPolicy.java?ref=f74e44635596276f35b7127f99bc5ab96ab534ed",
                "deletions": 6,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/KillAMPreemptionPolicy.java",
                "patch": "@@ -29,7 +29,9 @@\n import org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEventType;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.PreemptionContainer;\n+import org.apache.hadoop.yarn.api.records.PreemptionContract;\n import org.apache.hadoop.yarn.api.records.PreemptionMessage;\n+import org.apache.hadoop.yarn.api.records.StrictPreemptionContract;\n import org.apache.hadoop.yarn.event.EventHandler;\n \n /**\n@@ -52,13 +54,18 @@ public void init(AppContext context) {\n   public void preempt(Context ctxt, PreemptionMessage preemptionRequests) {\n     // for both strict and negotiable preemption requests kill the\n     // container\n-    for (PreemptionContainer c :\n-        preemptionRequests.getStrictContract().getContainers()) {\n-      killContainer(ctxt, c);\n+    StrictPreemptionContract strictContract = preemptionRequests\n+        .getStrictContract();\n+    if (strictContract != null) {\n+      for (PreemptionContainer c : strictContract.getContainers()) {\n+        killContainer(ctxt, c);\n+      }\n     }\n-    for (PreemptionContainer c :\n-         preemptionRequests.getContract().getContainers()) {\n-       killContainer(ctxt, c);\n+    PreemptionContract contract = preemptionRequests.getContract();\n+    if (contract != null) {\n+      for (PreemptionContainer c : contract.getContainers()) {\n+        killContainer(ctxt, c);\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/f74e44635596276f35b7127f99bc5ab96ab534ed/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapreduce/v2/app/rm/preemption/KillAMPreemptionPolicy.java",
                "sha": "09237aaa297f82b2a60ecb2f5ba9f0d620130094",
                "status": "modified"
            },
            {
                "additions": 144,
                "blob_url": "https://github.com/apache/hadoop/blob/f74e44635596276f35b7127f99bc5ab96ab534ed/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKillAMPreemptionPolicy.java",
                "changes": 144,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKillAMPreemptionPolicy.java?ref=f74e44635596276f35b7127f99bc5ab96ab534ed",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKillAMPreemptionPolicy.java",
                "patch": "@@ -0,0 +1,144 @@\n+/**\r\n+ * Licensed to the Apache Software Foundation (ASF) under one\r\n+ * or more contributor license agreements.  See the NOTICE file\r\n+ * distributed with this work for additional information\r\n+ * regarding copyright ownership.  The ASF licenses this file\r\n+ * to you under the Apache License, Version 2.0 (the\r\n+ * \"License\"); you may not use this file except in compliance\r\n+ * with the License.  You may obtain a copy of the License at\r\n+ *\r\n+ *     http://www.apache.org/licenses/LICENSE-2.0\r\n+ *\r\n+ * Unless required by applicable law or agreed to in writing, software\r\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n+ * See the License for the specific language governing permissions and\r\n+ * limitations under the License.\r\n+ */\r\n+package org.apache.hadoop.mapreduce.v2.app;\r\n+\r\n+import static org.mockito.Matchers.any;\r\n+import static org.mockito.Mockito.mock;\r\n+import static org.mockito.Mockito.times;\r\n+import static org.mockito.Mockito.verify;\r\n+import static org.mockito.Mockito.when;\r\n+\r\n+import java.util.ArrayList;\r\n+import java.util.HashSet;\r\n+import java.util.List;\r\n+import java.util.Set;\r\n+\r\n+import org.apache.hadoop.mapreduce.v2.api.records.TaskType;\r\n+import org.apache.hadoop.mapreduce.v2.app.MRAppMaster.RunningAppContext;\r\n+import org.apache.hadoop.mapreduce.v2.app.job.event.JobCounterUpdateEvent;\r\n+import org.apache.hadoop.mapreduce.v2.app.job.event.TaskAttemptEvent;\r\n+import org.apache.hadoop.mapreduce.v2.app.rm.preemption.AMPreemptionPolicy;\r\n+import org.apache.hadoop.mapreduce.v2.app.rm.preemption.KillAMPreemptionPolicy;\r\n+import org.apache.hadoop.mapreduce.v2.util.MRBuilderUtils;\r\n+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\r\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\r\n+import org.apache.hadoop.yarn.api.records.Container;\r\n+import org.apache.hadoop.yarn.api.records.ContainerId;\r\n+import org.apache.hadoop.yarn.api.records.PreemptionContainer;\r\n+import org.apache.hadoop.yarn.api.records.PreemptionContract;\r\n+import org.apache.hadoop.yarn.api.records.PreemptionMessage;\r\n+import org.apache.hadoop.yarn.api.records.StrictPreemptionContract;\r\n+import org.apache.hadoop.yarn.event.EventHandler;\r\n+import org.apache.hadoop.yarn.factories.RecordFactory;\r\n+import org.apache.hadoop.yarn.factory.providers.RecordFactoryProvider;\r\n+import org.junit.Test;\r\n+\r\n+public class TestKillAMPreemptionPolicy {\r\n+  private final RecordFactory recordFactory = RecordFactoryProvider\r\n+      .getRecordFactory(null);\r\n+\r\n+  @SuppressWarnings(\"unchecked\")\r\n+  @Test\r\n+  public void testKillAMPreemptPolicy() {\r\n+\r\n+    ApplicationId appId = ApplicationId.newInstance(123456789, 1);\r\n+    ContainerId container = ContainerId.newInstance(\r\n+        ApplicationAttemptId.newInstance(appId, 1), 1);\r\n+    AMPreemptionPolicy.Context mPctxt = mock(AMPreemptionPolicy.Context.class);\r\n+    when(mPctxt.getTaskAttempt(any(ContainerId.class))).thenReturn(\r\n+        MRBuilderUtils.newTaskAttemptId(MRBuilderUtils.newTaskId(\r\n+            MRBuilderUtils.newJobId(appId, 1), 1, TaskType.MAP), 0));\r\n+    List<Container> p = new ArrayList<Container>();\r\n+    p.add(Container.newInstance(container, null, null, null, null, null));\r\n+    when(mPctxt.getContainers(any(TaskType.class))).thenReturn(p);\r\n+\r\n+    KillAMPreemptionPolicy policy = new KillAMPreemptionPolicy();\r\n+\r\n+    // strictContract is null & contract is null\r\n+    RunningAppContext mActxt = getRunningAppContext();\r\n+    policy.init(mActxt);\r\n+    PreemptionMessage pM = getPreemptionMessage(false, false, container);\r\n+    policy.preempt(mPctxt, pM);\r\n+    verify(mActxt.getEventHandler(), times(0)).handle(\r\n+        any(TaskAttemptEvent.class));\r\n+    verify(mActxt.getEventHandler(), times(0)).handle(\r\n+        any(JobCounterUpdateEvent.class));\r\n+\r\n+    // strictContract is not null & contract is null\r\n+    mActxt = getRunningAppContext();\r\n+    policy.init(mActxt);\r\n+    pM = getPreemptionMessage(true, false, container);\r\n+    policy.preempt(mPctxt, pM);\r\n+    verify(mActxt.getEventHandler(), times(2)).handle(\r\n+        any(TaskAttemptEvent.class));\r\n+    verify(mActxt.getEventHandler(), times(2)).handle(\r\n+        any(JobCounterUpdateEvent.class));\r\n+\r\n+    // strictContract is null & contract is not null\r\n+    mActxt = getRunningAppContext();\r\n+    policy.init(mActxt);\r\n+    pM = getPreemptionMessage(false, true, container);\r\n+    policy.preempt(mPctxt, pM);\r\n+    verify(mActxt.getEventHandler(), times(2)).handle(\r\n+        any(TaskAttemptEvent.class));\r\n+    verify(mActxt.getEventHandler(), times(2)).handle(\r\n+        any(JobCounterUpdateEvent.class));\r\n+\r\n+    // strictContract is not null & contract is not null\r\n+    mActxt = getRunningAppContext();\r\n+    policy.init(mActxt);\r\n+    pM = getPreemptionMessage(true, true, container);\r\n+    policy.preempt(mPctxt, pM);\r\n+    verify(mActxt.getEventHandler(), times(4)).handle(\r\n+        any(TaskAttemptEvent.class));\r\n+    verify(mActxt.getEventHandler(), times(4)).handle(\r\n+        any(JobCounterUpdateEvent.class));\r\n+  }\r\n+\r\n+  private RunningAppContext getRunningAppContext() {\r\n+    RunningAppContext mActxt = mock(RunningAppContext.class);\r\n+    EventHandler<?> eventHandler = mock(EventHandler.class);\r\n+    when(mActxt.getEventHandler()).thenReturn(eventHandler);\r\n+    return mActxt;\r\n+  }\r\n+\r\n+  private PreemptionMessage getPreemptionMessage(boolean strictContract,\r\n+      boolean contract, final ContainerId container) {\r\n+    PreemptionMessage preemptionMessage = recordFactory\r\n+        .newRecordInstance(PreemptionMessage.class);\r\n+    Set<PreemptionContainer> cntrs = new HashSet<PreemptionContainer>();\r\n+    PreemptionContainer preemptContainer = recordFactory\r\n+        .newRecordInstance(PreemptionContainer.class);\r\n+    preemptContainer.setId(container);\r\n+    cntrs.add(preemptContainer);\r\n+    if (strictContract) {\r\n+      StrictPreemptionContract set = recordFactory\r\n+          .newRecordInstance(StrictPreemptionContract.class);\r\n+      set.setContainers(cntrs);\r\n+      preemptionMessage.setStrictContract(set);\r\n+    }\r\n+    if (contract) {\r\n+      PreemptionContract preemptContract = recordFactory\r\n+          .newRecordInstance(PreemptionContract.class);\r\n+      preemptContract.setContainers(cntrs);\r\n+      preemptionMessage.setContract(preemptContract);\r\n+    }\r\n+    return preemptionMessage;\r\n+  }\r\n+\r\n+}\r",
                "raw_url": "https://github.com/apache/hadoop/raw/f74e44635596276f35b7127f99bc5ab96ab534ed/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/test/java/org/apache/hadoop/mapreduce/v2/app/TestKillAMPreemptionPolicy.java",
                "sha": "fa930ae1262be2ed4687cc09456a5d80496c7b0d",
                "status": "added"
            }
        ],
        "message": "MAPREDUCE-5867. Fix NPE in KillAMPreemptionPolicy related to ProportionalCapacityPreemptionPolicy. Contributed by Sunil G.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1595754 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/9a0ed1c4afd95827c6ff27490f33d0b86851e551",
        "repo": "hadoop",
        "unit_tests": [
            "TestKillAMPreemptionPolicy.java"
        ]
    },
    "hadoop_f76f5c0": {
        "bug_id": "hadoop_f76f5c0",
        "commit": "https://github.com/apache/hadoop/commit/f76f5c0919cdb0b032edb309d137093952e77268",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop/blob/f76f5c0919cdb0b032edb309d137093952e77268/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java?ref=f76f5c0919cdb0b032edb309d137093952e77268",
                "deletions": 7,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "patch": "@@ -275,6 +275,10 @@ protected void addSchedPriorityCommand(List<String> command) {\n     }\n   }\n \n+  protected PrivilegedOperationExecutor getPrivilegedOperationExecutor() {\n+    return PrivilegedOperationExecutor.getInstance(getConf());\n+  }\n+\n   @Override\n   public void init() throws IOException {\n     Configuration conf = super.getConf();\n@@ -285,7 +289,7 @@ public void init() throws IOException {\n       PrivilegedOperation checkSetupOp = new PrivilegedOperation(\n           PrivilegedOperation.OperationType.CHECK_SETUP);\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          PrivilegedOperationExecutor.getInstance(conf);\n+          getPrivilegedOperationExecutor();\n \n       privilegedOperationExecutor.executePrivilegedOperation(checkSetupOp,\n           false);\n@@ -382,7 +386,7 @@ public void startLocalizer(LocalizerStartContext ctx)\n     try {\n       Configuration conf = super.getConf();\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          PrivilegedOperationExecutor.getInstance(conf);\n+          getPrivilegedOperationExecutor();\n \n       privilegedOperationExecutor.executePrivilegedOperation(prefixCommands,\n           initializeContainerOp, null, null, false, true);\n@@ -530,8 +534,9 @@ public int launchContainer(ContainerStartContext ctx)\n         }\n         builder.append(\"Stack trace: \"\n             + StringUtils.stringifyException(e) + \"\\n\");\n-        if (!e.getOutput().isEmpty()) {\n-          builder.append(\"Shell output: \" + e.getOutput() + \"\\n\");\n+        String output = e.getOutput();\n+        if (output!= null && !e.getOutput().isEmpty()) {\n+          builder.append(\"Shell output: \" + output + \"\\n\");\n         }\n         String diagnostics = builder.toString();\n         logOutput(diagnostics);\n@@ -729,7 +734,7 @@ public void deleteAsUser(DeletionAsUserContext ctx) {\n     try {\n       Configuration conf = super.getConf();\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          PrivilegedOperationExecutor.getInstance(conf);\n+          getPrivilegedOperationExecutor();\n \n       privilegedOperationExecutor.executePrivilegedOperation(deleteAsUserOp,\n           false);\n@@ -759,7 +764,7 @@ public void deleteAsUser(DeletionAsUserContext ctx) {\n \n     try {\n       PrivilegedOperationExecutor privOpExecutor =\n-          PrivilegedOperationExecutor.getInstance(super.getConf());\n+          getPrivilegedOperationExecutor();\n \n       String results =\n           privOpExecutor.executePrivilegedOperation(listAsUserOp, true);\n@@ -818,7 +823,7 @@ public void mountCgroups(List<String> cgroupKVs, String hierarchy)\n \n       mountCGroupsOp.appendArgs(cgroupKVs);\n       PrivilegedOperationExecutor privilegedOperationExecutor =\n-          PrivilegedOperationExecutor.getInstance(conf);\n+          getPrivilegedOperationExecutor();\n \n       privilegedOperationExecutor.executePrivilegedOperation(mountCGroupsOp,\n           false);",
                "raw_url": "https://github.com/apache/hadoop/raw/f76f5c0919cdb0b032edb309d137093952e77268/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/LinuxContainerExecutor.java",
                "sha": "47b99c25bb6d9571fe3d027101791344468c05c8",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/f76f5c0919cdb0b032edb309d137093952e77268/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java?ref=f76f5c0919cdb0b032edb309d137093952e77268",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java",
                "patch": "@@ -24,7 +24,7 @@\n \n public class PrivilegedOperationException extends YarnException {\n   private static final long serialVersionUID = 1L;\n-  private Integer exitCode;\n+  private int exitCode = -1;\n   private String output;\n   private String errorOutput;\n \n@@ -36,7 +36,7 @@ public PrivilegedOperationException(String message) {\n     super(message);\n   }\n \n-  public PrivilegedOperationException(String message, Integer exitCode,\n+  public PrivilegedOperationException(String message, int exitCode,\n       String output, String errorOutput) {\n     super(message);\n     this.exitCode = exitCode;\n@@ -48,8 +48,8 @@ public PrivilegedOperationException(Throwable cause) {\n     super(cause);\n   }\n \n-  public PrivilegedOperationException(Throwable cause, Integer exitCode, String\n-      output, String errorOutput) {\n+  public PrivilegedOperationException(Throwable cause, int exitCode,\n+      String output, String errorOutput) {\n     super(cause);\n     this.exitCode = exitCode;\n     this.output = output;\n@@ -59,7 +59,7 @@ public PrivilegedOperationException(String message, Throwable cause) {\n     super(message, cause);\n   }\n \n-  public Integer getExitCode() {\n+  public int getExitCode() {\n     return exitCode;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/f76f5c0919cdb0b032edb309d137093952e77268/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/linux/privileged/PrivilegedOperationException.java",
                "sha": "9a11194f143e0e832d9371bddfd1229aeaeb0bd4",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop/blob/f76f5c0919cdb0b032edb309d137093952e77268/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java?ref=f76f5c0919cdb0b032edb309d137093952e77268",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java",
                "patch": "@@ -32,10 +32,10 @@\n @InterfaceStability.Unstable\n public class ContainerExecutionException extends YarnException {\n   private static final long serialVersionUID = 1L;\n-  private static final Integer EXIT_CODE_UNSET = -1;\n+  private static final int EXIT_CODE_UNSET = -1;\n   private static final String OUTPUT_UNSET = \"<unknown>\";\n \n-  private Integer exitCode;\n+  private int exitCode;\n   private String output;\n   private String errorOutput;\n \n@@ -54,23 +54,23 @@ public ContainerExecutionException(Throwable throwable) {\n   }\n \n \n-  public ContainerExecutionException(String message, Integer exitCode, String\n+  public ContainerExecutionException(String message, int exitCode, String\n       output, String errorOutput) {\n     super(message);\n     this.exitCode = exitCode;\n     this.output = output;\n     this.errorOutput = errorOutput;\n   }\n \n-  public ContainerExecutionException(Throwable cause, Integer exitCode, String\n+  public ContainerExecutionException(Throwable cause, int exitCode, String\n       output, String errorOutput) {\n     super(cause);\n     this.exitCode = exitCode;\n     this.output = output;\n     this.errorOutput = errorOutput;\n   }\n \n-  public Integer getExitCode() {\n+  public int getExitCode() {\n     return exitCode;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop/raw/f76f5c0919cdb0b032edb309d137093952e77268/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/runtime/ContainerExecutionException.java",
                "sha": "3147277704265b1cb5bc2aa2f21d8a417e31c381",
                "status": "modified"
            },
            {
                "additions": 89,
                "blob_url": "https://github.com/apache/hadoop/blob/f76f5c0919cdb0b032edb309d137093952e77268/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "changes": 89,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java?ref=f76f5c0919cdb0b032edb309d137093952e77268",
                "deletions": 0,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "patch": "@@ -23,7 +23,9 @@\n import static org.junit.Assert.assertNotEquals;\n import static org.junit.Assert.assertTrue;\n import static org.mockito.Matchers.any;\n+import static org.mockito.Matchers.anyBoolean;\n import static org.mockito.Mockito.doAnswer;\n+import static org.mockito.Mockito.doThrow;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.spy;\n import static org.mockito.Mockito.when;\n@@ -40,20 +42,24 @@\n import java.util.HashMap;\n import java.util.LinkedList;\n import java.util.List;\n+import java.util.Map;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.util.StringUtils;\n+import org.apache.hadoop.yarn.api.records.ApplicationAttemptId;\n+import org.apache.hadoop.yarn.api.records.ApplicationId;\n import org.apache.hadoop.yarn.api.records.ContainerId;\n import org.apache.hadoop.yarn.api.records.ContainerLaunchContext;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.exceptions.ConfigurationException;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.container.ContainerDiagnosticsUpdateEvent;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperation;\n+import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationException;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.privileged.PrivilegedOperationExecutor;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.DefaultLinuxContainerRuntime;\n import org.apache.hadoop.yarn.server.nodemanager.containermanager.linux.runtime.LinuxContainerRuntime;\n@@ -516,4 +522,87 @@ public void testDeleteAsUser() throws IOException {\n         appSubmitter, cmd, \"\", baseDir0.toString(), baseDir1.toString()),\n         readMockParams());\n   }\n+\n+  @Test\n+  public void testNoExitCodeFromPrivilegedOperation() throws Exception {\n+    Configuration conf = new Configuration();\n+    final PrivilegedOperationExecutor spyPrivilegedExecutor =\n+        spy(PrivilegedOperationExecutor.getInstance(conf));\n+    doThrow(new PrivilegedOperationException(\"interrupted\"))\n+        .when(spyPrivilegedExecutor).executePrivilegedOperation(\n+            any(List.class), any(PrivilegedOperation.class),\n+            any(File.class), any(Map.class), anyBoolean(), anyBoolean());\n+    LinuxContainerRuntime runtime = new DefaultLinuxContainerRuntime(\n+        spyPrivilegedExecutor);\n+    runtime.initialize(conf);\n+    mockExec = new LinuxContainerExecutor(runtime);\n+    mockExec.setConf(conf);\n+    LinuxContainerExecutor lce = new LinuxContainerExecutor(runtime) {\n+      @Override\n+      protected PrivilegedOperationExecutor getPrivilegedOperationExecutor() {\n+        return spyPrivilegedExecutor;\n+      }\n+    };\n+    lce.setConf(conf);\n+    InetSocketAddress address = InetSocketAddress.createUnresolved(\n+        \"localhost\", 8040);\n+    Path nmPrivateCTokensPath= new Path(\"file:///bin/nmPrivateCTokensPath\");\n+    LocalDirsHandlerService dirService = new LocalDirsHandlerService();\n+    dirService.init(conf);\n+\n+    String appSubmitter = \"nobody\";\n+    ApplicationId appId = ApplicationId.newInstance(1, 1);\n+    ApplicationAttemptId attemptId = ApplicationAttemptId.newInstance(appId, 1);\n+    ContainerId cid = ContainerId.newContainerId(attemptId, 1);\n+    HashMap<String, String> env = new HashMap<>();\n+    Container container = mock(Container.class);\n+    ContainerLaunchContext context = mock(ContainerLaunchContext.class);\n+    when(container.getContainerId()).thenReturn(cid);\n+    when(container.getLaunchContext()).thenReturn(context);\n+    when(context.getEnvironment()).thenReturn(env);\n+    Path workDir = new Path(\"/tmp\");\n+\n+    try {\n+      lce.startLocalizer(new LocalizerStartContext.Builder()\n+          .setNmPrivateContainerTokens(nmPrivateCTokensPath)\n+          .setNmAddr(address)\n+          .setUser(appSubmitter)\n+          .setAppId(appId.toString())\n+          .setLocId(\"12345\")\n+          .setDirsHandler(dirService)\n+          .build());\n+      Assert.fail(\"startLocalizer should have thrown an exception\");\n+    } catch (IOException e) {\n+      assertTrue(\"Unexpected exception \" + e,\n+          e.getMessage().contains(\"exitCode\"));\n+    }\n+\n+    lce.activateContainer(cid, new Path(workDir, \"pid.txt\"));\n+    lce.launchContainer(new ContainerStartContext.Builder()\n+        .setContainer(container)\n+        .setNmPrivateContainerScriptPath(new Path(\"file:///bin/echo\"))\n+        .setNmPrivateTokensPath(new Path(\"file:///dev/null\"))\n+        .setUser(appSubmitter)\n+        .setAppId(appId.toString())\n+        .setContainerWorkDir(workDir)\n+        .setLocalDirs(dirsHandler.getLocalDirs())\n+        .setLogDirs(dirsHandler.getLogDirs())\n+        .setFilecacheDirs(new ArrayList<>())\n+        .setUserLocalDirs(new ArrayList<>())\n+        .setContainerLocalDirs(new ArrayList<>())\n+        .setContainerLogDirs(new ArrayList<>())\n+        .build());\n+    lce.deleteAsUser(new DeletionAsUserContext.Builder()\n+        .setUser(appSubmitter)\n+        .setSubDir(new Path(\"/tmp/testdir\"))\n+        .build());\n+\n+    try {\n+      lce.mountCgroups(new ArrayList<String>(), \"hierarchy\");\n+      Assert.fail(\"mountCgroups should have thrown an exception\");\n+    } catch (IOException e) {\n+      assertTrue(\"Unexpected exception \" + e,\n+          e.getMessage().contains(\"exit code\"));\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/f76f5c0919cdb0b032edb309d137093952e77268/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/java/org/apache/hadoop/yarn/server/nodemanager/TestLinuxContainerExecutorWithMocks.java",
                "sha": "cfd0e364a2d4ad5735590d69d90cdce9f81b0012",
                "status": "modified"
            }
        ],
        "message": "YARN-6805. NPE in LinuxContainerExecutor due to null PrivilegedOperationException exit code. Contributed by Jason Lowe",
        "parent": "https://github.com/apache/hadoop/commit/5f1ee72b0ebf0330417b7c0115083bc851923be4",
        "repo": "hadoop",
        "unit_tests": [
            "TestLinuxContainerExecutor.java"
        ]
    },
    "hadoop_f9e36de": {
        "bug_id": "hadoop_f9e36de",
        "commit": "https://github.com/apache/hadoop/commit/f9e36dea96f592d09f159e521379e426e7f07ec9",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/f9e36dea96f592d09f159e521379e426e7f07ec9/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=f9e36dea96f592d09f159e521379e426e7f07ec9",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -99,6 +99,9 @@ Release 2.9.0 - UNRELEASED\n     YARN-2934. Improve handling of container's stderr.\n     (Naganarasimha G R via gera)\n \n+    YARN-4530. LocalizedResource trigger a NPE Cause the NodeManager exit\n+    (tangshangwen via rohithsharmaks)\n+\n Release 2.8.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/f9e36dea96f592d09f159e521379e426e7f07ec9/hadoop-yarn-project/CHANGES.txt",
                "sha": "80a3ed1dd270719218e99ba5966206fdc132ba33",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/f9e36dea96f592d09f159e521379e426e7f07ec9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java?ref=f9e36dea96f592d09f159e521379e426e7f07ec9",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "patch": "@@ -878,12 +878,12 @@ public void run() {\n             Future<Path> completed = queue.take();\n             LocalizerResourceRequestEvent assoc = pending.remove(completed);\n             try {\n-              Path local = completed.get();\n               if (null == assoc) {\n                 LOG.error(\"Localized unknown resource to \" + completed);\n                 // TODO delete\n                 return;\n               }\n+              Path local = completed.get();\n               LocalResourceRequest key = assoc.getResource().getRequest();\n               publicRsrc.handle(new ResourceLocalizedEvent(key, local, FileUtil\n                 .getDU(new File(local.toUri()))));",
                "raw_url": "https://github.com/apache/hadoop/raw/f9e36dea96f592d09f159e521379e426e7f07ec9/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/localizer/ResourceLocalizationService.java",
                "sha": "c0c2e8e4fbcc69cc1b20aa6ba95f2bbff9dfe939",
                "status": "modified"
            }
        ],
        "message": "YARN-4530. LocalizedResource trigger a NPE Cause the NodeManager exit. (tangshangwen via rohithsharmaks)",
        "parent": "https://github.com/apache/hadoop/commit/4e4b3a8465a8433e78e015cb1ce7e0dc1ebeb523",
        "repo": "hadoop",
        "unit_tests": [
            "TestResourceLocalizationService.java"
        ]
    },
    "hadoop_fbf7e81": {
        "bug_id": "hadoop_fbf7e81",
        "commit": "https://github.com/apache/hadoop/commit/fbf7e81ca007e009b492e3b99060bbfb74394f46",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/fbf7e81ca007e009b492e3b99060bbfb74394f46/hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt?ref=fbf7e81ca007e009b492e3b99060bbfb74394f46",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt",
                "patch": "@@ -391,3 +391,6 @@\n \n     HDFS-8857. Erasure Coding: Fix ArrayIndexOutOfBoundsException in\n     TestWriteStripedFileWithFailure. (Li Bo)\n+\n+    HDFS-8827. Erasure Coding: Fix NPE when NameNode processes over-replicated\n+    striped blocks. (Walter Su and Takuya Fukudome via jing9)",
                "raw_url": "https://github.com/apache/hadoop/raw/fbf7e81ca007e009b492e3b99060bbfb74394f46/hadoop-hdfs-project/hadoop-hdfs/CHANGES-HDFS-EC-7285.txt",
                "sha": "45afd2cea912ae83159c1dab50a01d6ac577dab7",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/hadoop/blob/fbf7e81ca007e009b492e3b99060bbfb74394f46/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=fbf7e81ca007e009b492e3b99060bbfb74394f46",
                "deletions": 8,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -3135,14 +3135,13 @@ private void chooseExcessReplicates(\n     assert namesystem.hasWriteLock();\n     // first form a rack to datanodes map and\n     BlockCollection bc = getBlockCollection(storedBlock);\n-    final BlockStoragePolicy storagePolicy = storagePolicySuite.getPolicy(\n-        bc.getStoragePolicyID());\n-    final List<StorageType> excessTypes = storagePolicy.chooseExcess(\n-        replication, DatanodeStorageInfo.toStorageTypes(nonExcess));\n     if (storedBlock.isStriped()) {\n-      chooseExcessReplicasStriped(bc, nonExcess, storedBlock, delNodeHint,\n-          excessTypes);\n+      chooseExcessReplicasStriped(bc, nonExcess, storedBlock, delNodeHint);\n     } else {\n+      final BlockStoragePolicy storagePolicy = storagePolicySuite.getPolicy(\n+          bc.getStoragePolicyID());\n+      final List<StorageType> excessTypes = storagePolicy.chooseExcess(\n+          replication, DatanodeStorageInfo.toStorageTypes(nonExcess));\n       chooseExcessReplicasContiguous(bc, nonExcess, storedBlock,\n           replication, addedNode, delNodeHint, excessTypes);\n     }\n@@ -3216,8 +3215,7 @@ private void chooseExcessReplicasContiguous(BlockCollection bc,\n   private void chooseExcessReplicasStriped(BlockCollection bc,\n       final Collection<DatanodeStorageInfo> nonExcess,\n       BlockInfo storedBlock,\n-      DatanodeDescriptor delNodeHint,\n-      List<StorageType> excessTypes) {\n+      DatanodeDescriptor delNodeHint) {\n     assert storedBlock instanceof BlockInfoStriped;\n     BlockInfoStriped sblk = (BlockInfoStriped) storedBlock;\n     short groupSize = sblk.getTotalBlockNum();\n@@ -3237,6 +3235,14 @@ private void chooseExcessReplicasStriped(BlockCollection bc,\n       found.set(index);\n       storage2index.put(storage, index);\n     }\n+    // the number of target left replicas equals to the of number of the found\n+    // indices.\n+    int numOfTarget = found.cardinality();\n+\n+    final BlockStoragePolicy storagePolicy = storagePolicySuite.getPolicy(\n+        bc.getStoragePolicyID());\n+    final List<StorageType> excessTypes = storagePolicy.chooseExcess(\n+        (short)numOfTarget, DatanodeStorageInfo.toStorageTypes(nonExcess));\n \n     // use delHint only if delHint is duplicated\n     final DatanodeStorageInfo delStorageHint =",
                "raw_url": "https://github.com/apache/hadoop/raw/fbf7e81ca007e009b492e3b99060bbfb74394f46/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "0ceb3921fb90c25921a589f5454c2c9d930d7888",
                "status": "modified"
            },
            {
                "additions": 151,
                "blob_url": "https://github.com/apache/hadoop/blob/fbf7e81ca007e009b492e3b99060bbfb74394f46/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddOverReplicatedStripedBlocks.java",
                "changes": 152,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddOverReplicatedStripedBlocks.java?ref=fbf7e81ca007e009b492e3b99060bbfb74394f46",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddOverReplicatedStripedBlocks.java",
                "patch": "@@ -24,9 +24,14 @@\n import org.apache.hadoop.hdfs.DistributedFileSystem;\n import org.apache.hadoop.hdfs.MiniDFSCluster;\n import org.apache.hadoop.hdfs.protocol.Block;\n+import org.apache.hadoop.hdfs.protocol.DatanodeInfo;\n import org.apache.hadoop.hdfs.protocol.HdfsConstants;\n import org.apache.hadoop.hdfs.protocol.LocatedBlocks;\n import org.apache.hadoop.hdfs.protocol.LocatedStripedBlock;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockInfoStriped;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockManager;\n+import org.apache.hadoop.hdfs.server.blockmanagement.BlockManagerTestUtil;\n+import org.apache.hadoop.hdfs.server.datanode.DataNode;\n import org.apache.hadoop.hdfs.server.datanode.SimulatedFSDataset;\n import org.junit.After;\n import org.junit.Before;\n@@ -35,6 +40,7 @@\n import java.io.IOException;\n import java.util.Arrays;\n import java.util.HashSet;\n+import java.util.List;\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertTrue;\n@@ -49,14 +55,16 @@\n   private final short PARITY_BLK_NUM = HdfsConstants.NUM_PARITY_BLOCKS;\n   private final short GROUP_SIZE = DATA_BLK_NUM + PARITY_BLK_NUM;\n   private final int CELLSIZE = HdfsConstants.BLOCK_STRIPED_CELL_SIZE;\n-  private final int NUM_STRIPE_PER_BLOCK = 1;\n+  private final int NUM_STRIPE_PER_BLOCK = 4;\n   private final int BLOCK_SIZE = NUM_STRIPE_PER_BLOCK * CELLSIZE;\n   private final int numDNs = GROUP_SIZE + 3;\n \n   @Before\n   public void setup() throws IOException {\n     Configuration conf = new Configuration();\n     conf.setLong(DFSConfigKeys.DFS_BLOCK_SIZE_KEY, BLOCK_SIZE);\n+    // disable block recovery\n+    conf.setInt(DFSConfigKeys.DFS_NAMENODE_REPLICATION_MAX_STREAMS_KEY, 0);\n     SimulatedFSDataset.setFactory(conf);\n     cluster = new MiniDFSCluster.Builder(conf).numDataNodes(numDNs).build();\n     cluster.waitActive();\n@@ -113,4 +121,146 @@ public void testProcessOverReplicatedStripedBlock() throws Exception {\n         filePath.toString(), 0, fileLen);\n     DFSTestUtil.verifyLocatedStripedBlocks(lbs, GROUP_SIZE);\n   }\n+\n+  @Test\n+  public void testProcessOverReplicatedSBSmallerThanFullBlocks()\n+      throws Exception {\n+    // Create a EC file which doesn't fill full internal blocks.\n+    int fileLen = CELLSIZE * (DATA_BLK_NUM - 1);\n+    byte[] content = new byte[fileLen];\n+    DFSTestUtil.writeFile(fs, filePath, new String(content));\n+    LocatedBlocks lbs = cluster.getNameNodeRpc().getBlockLocations(\n+        filePath.toString(), 0, fileLen);\n+    LocatedStripedBlock bg = (LocatedStripedBlock) (lbs.get(0));\n+    long gs = bg.getBlock().getGenerationStamp();\n+    String bpid = bg.getBlock().getBlockPoolId();\n+    long groupId = bg.getBlock().getBlockId();\n+    Block blk = new Block(groupId, BLOCK_SIZE, gs);\n+    cluster.triggerBlockReports();\n+    List<DatanodeInfo> infos = Arrays.asList(bg.getLocations());\n+\n+    // let a internal block be over replicated with 2 redundant blocks.\n+    // Therefor number of internal blocks is over GROUP_SIZE. (5 data blocks +\n+    // 3 parity blocks  + 2 redundant blocks > GROUP_SIZE)\n+    blk.setBlockId(groupId + 2);\n+    List<DataNode> dataNodeList = cluster.getDataNodes();\n+    for (int i = 0; i < numDNs; i++) {\n+      if (!infos.contains(dataNodeList.get(i).getDatanodeId())) {\n+        cluster.injectBlocks(i, Arrays.asList(blk), bpid);\n+        System.out.println(\"XXX: inject block into datanode \" + i);\n+      }\n+    }\n+\n+    // update blocksMap\n+    cluster.triggerBlockReports();\n+    // add to invalidates\n+    cluster.triggerHeartbeats();\n+    // datanode delete block\n+    cluster.triggerHeartbeats();\n+    // update blocksMap\n+    cluster.triggerBlockReports();\n+\n+    // verify that all internal blocks exists\n+    lbs = cluster.getNameNodeRpc().getBlockLocations(\n+        filePath.toString(), 0, fileLen);\n+    DFSTestUtil.verifyLocatedStripedBlocks(lbs, GROUP_SIZE - 1);\n+  }\n+\n+  @Test\n+  public void testProcessOverReplicatedAndCorruptStripedBlock()\n+      throws Exception {\n+    long fileLen = DATA_BLK_NUM * BLOCK_SIZE;\n+    DFSTestUtil.createStripedFile(cluster, filePath, null, 1,\n+        NUM_STRIPE_PER_BLOCK, false);\n+    LocatedBlocks lbs = cluster.getNameNodeRpc().getBlockLocations(\n+        filePath.toString(), 0, fileLen);\n+    LocatedStripedBlock bg = (LocatedStripedBlock) (lbs.get(0));\n+    long gs = bg.getBlock().getGenerationStamp();\n+    String bpid = bg.getBlock().getBlockPoolId();\n+    long groupId = bg.getBlock().getBlockId();\n+    Block blk = new Block(groupId, BLOCK_SIZE, gs);\n+    BlockInfoStriped blockInfo = new BlockInfoStriped(blk,\n+        ErasureCodingSchemaManager.getSystemDefaultSchema(), CELLSIZE);\n+    for (int i = 0; i < GROUP_SIZE; i++) {\n+      blk.setBlockId(groupId + i);\n+      cluster.injectBlocks(i, Arrays.asList(blk), bpid);\n+    }\n+    cluster.triggerBlockReports();\n+\n+    // let a internal block be corrupt\n+    BlockManager bm = cluster.getNamesystem().getBlockManager();\n+    List<DatanodeInfo> infos = Arrays.asList(bg.getLocations());\n+    List<String> storages = Arrays.asList(bg.getStorageIDs());\n+    cluster.getNamesystem().writeLock();\n+    try {\n+      bm.findAndMarkBlockAsCorrupt(lbs.getLastLocatedBlock().getBlock(),\n+          infos.get(0), storages.get(0), \"TEST\");\n+    } finally {\n+      cluster.getNamesystem().writeUnlock();\n+    }\n+    assertEquals(1, bm.countNodes(blockInfo).corruptReplicas());\n+\n+    // let a internal block be over replicated with 2 redundant block.\n+    blk.setBlockId(groupId + 2);\n+    cluster.injectBlocks(numDNs - 3, Arrays.asList(blk), bpid);\n+    cluster.injectBlocks(numDNs - 2, Arrays.asList(blk), bpid);\n+\n+    // update blocksMap\n+    cluster.triggerBlockReports();\n+    // add to invalidates\n+    cluster.triggerHeartbeats();\n+    // datanode delete block\n+    cluster.triggerHeartbeats();\n+    // update blocksMap\n+    cluster.triggerBlockReports();\n+\n+    // verify that all internal blocks exists\n+    lbs = cluster.getNameNodeRpc().getBlockLocations(\n+        filePath.toString(), 0, fileLen);\n+    DFSTestUtil.verifyLocatedStripedBlocks(lbs, GROUP_SIZE);\n+  }\n+\n+  @Test\n+  public void testProcessOverReplicatedAndMissingStripedBlock()\n+      throws Exception {\n+    long fileLen = CELLSIZE * DATA_BLK_NUM;\n+    DFSTestUtil.createStripedFile(cluster, filePath, null, 1,\n+        NUM_STRIPE_PER_BLOCK, false);\n+    LocatedBlocks lbs = cluster.getNameNodeRpc().getBlockLocations(\n+        filePath.toString(), 0, fileLen);\n+    LocatedStripedBlock bg = (LocatedStripedBlock) (lbs.get(0));\n+    long gs = bg.getBlock().getGenerationStamp();\n+    String bpid = bg.getBlock().getBlockPoolId();\n+    long groupId = bg.getBlock().getBlockId();\n+    Block blk = new Block(groupId, BLOCK_SIZE, gs);\n+    // only inject GROUP_SIZE - 1 blocks, so there is one block missing\n+    for (int i = 0; i < GROUP_SIZE - 1; i++) {\n+      blk.setBlockId(groupId + i);\n+      cluster.injectBlocks(i, Arrays.asList(blk), bpid);\n+    }\n+    cluster.triggerBlockReports();\n+\n+    // let a internal block be over replicated with 2 redundant blocks.\n+    // Therefor number of internal blocks is over GROUP_SIZE. (5 data blocks +\n+    // 3 parity blocks  + 2 redundant blocks > GROUP_SIZE)\n+    blk.setBlockId(groupId + 2);\n+    cluster.injectBlocks(numDNs - 3, Arrays.asList(blk), bpid);\n+    cluster.injectBlocks(numDNs - 2, Arrays.asList(blk), bpid);\n+\n+    // update blocksMap\n+    cluster.triggerBlockReports();\n+    // add to invalidates\n+    cluster.triggerHeartbeats();\n+    // datanode delete block\n+    cluster.triggerHeartbeats();\n+    // update blocksMap\n+    cluster.triggerBlockReports();\n+\n+    // Since one block is missing, when over-replicated blocks got deleted,\n+    // we are left GROUP_SIZE - 1 blocks.\n+    lbs = cluster.getNameNodeRpc().getBlockLocations(\n+        filePath.toString(), 0, fileLen);\n+    DFSTestUtil.verifyLocatedStripedBlocks(lbs, GROUP_SIZE - 1);\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/fbf7e81ca007e009b492e3b99060bbfb74394f46/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/server/namenode/TestAddOverReplicatedStripedBlocks.java",
                "sha": "337911d0cae11ce5e0338e4d8ffa2659b2cc1db4",
                "status": "modified"
            }
        ],
        "message": "HDFS-8827. Erasure Coding: Fix NPE when NameNode processes over-replicated striped blocks. Contributed by Walter Su and Takuya Fukudome.",
        "parent": "https://github.com/apache/hadoop/commit/8799363db1c0e0ce0abd4ab68b780092e7dc5263",
        "repo": "hadoop",
        "unit_tests": [
            "TestBlockManager.java"
        ]
    },
    "hadoop_fdf7b18": {
        "bug_id": "hadoop_fdf7b18",
        "commit": "https://github.com/apache/hadoop/commit/fdf7b182475050aaf67765eb53aaf342ebaebe8b",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop/blob/fdf7b182475050aaf67765eb53aaf342ebaebe8b/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=fdf7b182475050aaf67765eb53aaf342ebaebe8b",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -131,3 +131,5 @@ HDFS-2804. Should not mark blocks under-replicated when exiting safemode (todd)\n HDFS-2807. Service level authorizartion for HAServiceProtocol. (jitendra)\n \n HDFS-2809. Add test to verify that delegation tokens are honored after failover. (jitendra and atm)\n+\n+HDFS-2838. NPE in FSNamesystem when in safe mode. (Gregory Chanan via eli)",
                "raw_url": "https://github.com/apache/hadoop/raw/fdf7b182475050aaf67765eb53aaf342ebaebe8b/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "c8a760336a9c9c95f075c4e2d894747b31d6207e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop/blob/fdf7b182475050aaf67765eb53aaf342ebaebe8b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=fdf7b182475050aaf67765eb53aaf342ebaebe8b",
                "deletions": 2,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -3623,11 +3623,10 @@ private void doConsistencyCheck() {\n       assert assertsOn = true; // set to true if asserts are on\n       if (!assertsOn) return;\n       \n-      \n-      int activeBlocks = blockManager.getActiveBlockCount();\n       if (blockTotal == -1 && blockSafe == -1) {\n         return; // manual safe mode\n       }\n+      int activeBlocks = blockManager.getActiveBlockCount();\n       if ((blockTotal != activeBlocks) &&\n           !(blockSafe >= 0 && blockSafe <= blockTotal)) {\n         throw new AssertionError(",
                "raw_url": "https://github.com/apache/hadoop/raw/fdf7b182475050aaf67765eb53aaf342ebaebe8b/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "b3b3dbdaf31bcb28f1c91d0cbdd4949a93c8f0fa",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/hadoop/blob/fdf7b182475050aaf67765eb53aaf342ebaebe8b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMiniDFSCluster.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMiniDFSCluster.java?ref=fdf7b182475050aaf67765eb53aaf342ebaebe8b",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMiniDFSCluster.java",
                "patch": "@@ -20,6 +20,7 @@\n \n import junit.framework.Assert;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.protocol.FSConstants;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n@@ -37,6 +38,7 @@\n   private static final String CLUSTER_1 = \"cluster1\";\n   private static final String CLUSTER_2 = \"cluster2\";\n   private static final String CLUSTER_3 = \"cluster3\";\n+  private static final String CLUSTER_4 = \"cluster4\";\n   protected String testDataPath;\n   protected File testDataDir;\n   @Before\n@@ -104,5 +106,21 @@ public void testDualClusters() throws Throwable {\n     }\n   }\n \n-\n+  @Test(timeout=100000)\n+  public void testIsClusterUpAfterShutdown() throws Throwable {\n+    Configuration conf = new HdfsConfiguration();\n+    File testDataCluster4 = new File(testDataPath, CLUSTER_4);\n+    String c4Path = testDataCluster4.getAbsolutePath();\n+    conf.set(MiniDFSCluster.HDFS_MINIDFS_BASEDIR, c4Path);\n+    MiniDFSCluster cluster4 = new MiniDFSCluster.Builder(conf).build();\n+    try {\n+      DistributedFileSystem dfs = (DistributedFileSystem) cluster4.getFileSystem();\n+      dfs.setSafeMode(FSConstants.SafeModeAction.SAFEMODE_ENTER);\n+      cluster4.shutdown();\n+    } finally {\n+      while(cluster4.isClusterUp()){\n+        Thread.sleep(1000);\n+      }  \n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop/raw/fdf7b182475050aaf67765eb53aaf342ebaebe8b/hadoop-hdfs-project/hadoop-hdfs/src/test/java/org/apache/hadoop/hdfs/TestMiniDFSCluster.java",
                "sha": "0eec0d187746184179ed9f814d59dbd059c390e0",
                "status": "modified"
            }
        ],
        "message": "HDFS-2838. NPE in FSNamesystem when in safe mode. Contributed by Gregory Chanan\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1236450 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/1c95060a720680aa1dfb14e08603aa5bb405ba65",
        "repo": "hadoop",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop_fe35103": {
        "bug_id": "hadoop_fe35103",
        "commit": "https://github.com/apache/hadoop/commit/fe35103591ece0209f8345aba5544313e45a073c",
        "file": [
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hadoop/blob/fe35103591ece0209f8345aba5544313e45a073c/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java?ref=fe35103591ece0209f8345aba5544313e45a073c",
                "deletions": 18,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
                "patch": "@@ -585,33 +585,38 @@ public void setCheckpointID(TaskID taskId, TaskCheckpointID cid) {\n   private void coalesceStatusUpdate(TaskAttemptId yarnAttemptID,\n       TaskAttemptStatus taskAttemptStatus,\n       AtomicReference<TaskAttemptStatus> lastStatusRef) {\n-    boolean asyncUpdatedNeeded = false;\n-    TaskAttemptStatus lastStatus = lastStatusRef.get();\n-\n-    if (lastStatus == null) {\n-      lastStatusRef.set(taskAttemptStatus);\n-      asyncUpdatedNeeded = true;\n-    } else {\n-      List<TaskAttemptId> oldFetchFailedMaps =\n-          taskAttemptStatus.fetchFailedMaps;\n-\n-      // merge fetchFailedMaps from the previous update\n-      if (lastStatus.fetchFailedMaps != null) {\n+    List<TaskAttemptId> fetchFailedMaps = taskAttemptStatus.fetchFailedMaps;\n+    TaskAttemptStatus lastStatus = null;\n+    boolean done = false;\n+    while (!done) {\n+      lastStatus = lastStatusRef.get();\n+      if (lastStatus != null && lastStatus.fetchFailedMaps != null) {\n+        // merge fetchFailedMaps from the previous update\n         if (taskAttemptStatus.fetchFailedMaps == null) {\n           taskAttemptStatus.fetchFailedMaps = lastStatus.fetchFailedMaps;\n         } else {\n-          taskAttemptStatus.fetchFailedMaps.addAll(lastStatus.fetchFailedMaps);\n+          taskAttemptStatus.fetchFailedMaps =\n+              new ArrayList<>(lastStatus.fetchFailedMaps.size() +\n+                  fetchFailedMaps.size());\n+          taskAttemptStatus.fetchFailedMaps.addAll(\n+              lastStatus.fetchFailedMaps);\n+          taskAttemptStatus.fetchFailedMaps.addAll(\n+              fetchFailedMaps);\n         }\n       }\n \n-      if (!lastStatusRef.compareAndSet(lastStatus, taskAttemptStatus)) {\n-        // update failed - async dispatcher has processed it in the meantime\n-        taskAttemptStatus.fetchFailedMaps = oldFetchFailedMaps;\n-        lastStatusRef.set(taskAttemptStatus);\n-        asyncUpdatedNeeded = true;\n+      // lastStatusRef may be changed by either the AsyncDispatcher when\n+      // it processes the update, or by another IPC server handler\n+      done = lastStatusRef.compareAndSet(lastStatus, taskAttemptStatus);\n+      if (!done) {\n+        LOG.info(\"TaskAttempt \" + yarnAttemptID +\n+            \": lastStatusRef changed by another thread, retrying...\");\n+        // let's revert taskAttemptStatus.fetchFailedMaps\n+        taskAttemptStatus.fetchFailedMaps = fetchFailedMaps;\n       }\n     }\n \n+    boolean asyncUpdatedNeeded = (lastStatus == null);\n     if (asyncUpdatedNeeded) {\n       context.getEventHandler().handle(\n           new TaskAttemptStatusUpdateEvent(taskAttemptStatus.id,",
                "raw_url": "https://github.com/apache/hadoop/raw/fe35103591ece0209f8345aba5544313e45a073c/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-app/src/main/java/org/apache/hadoop/mapred/TaskAttemptListenerImpl.java",
                "sha": "556c90c4412173dbb9f9975be9ed53c29ef6dd77",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-7028. Concurrent task progress updates causing NPE in Application Master. Contributed by Gergo Repas",
        "parent": "https://github.com/apache/hadoop/commit/c9bf813c9a6c018d14f2bef49ba086ec0e60c761",
        "repo": "hadoop",
        "unit_tests": [
            "TestTaskAttemptListenerImpl.java"
        ]
    },
    "hadoop_ff77582": {
        "bug_id": "hadoop_ff77582",
        "commit": "https://github.com/apache/hadoop/commit/ff7758299151e3b69c27314010b4ef3a9fda3b41",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop/blob/ff7758299151e3b69c27314010b4ef3a9fda3b41/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/CHANGES.txt?ref=ff7758299151e3b69c27314010b4ef3a9fda3b41",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -88,6 +88,9 @@ Release 2.6.0 - UNRELEASED\n     YARN-2321. NodeManager web UI can incorrectly report Pmem enforcement\n     (Leitao Guo via jlowe)\n \n+    YARN-2273. NPE in ContinuousScheduling thread when we lose a node. \n+    (Wei Yan via kasha)\n+\n Release 2.5.0 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop/raw/ff7758299151e3b69c27314010b4ef3a9fda3b41/hadoop-yarn-project/CHANGES.txt",
                "sha": "2e69a756c6415598c7f26f304b969407987ee68e",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/hadoop/blob/ff7758299151e3b69c27314010b4ef3a9fda3b41/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "changes": 65,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java?ref=ff7758299151e3b69c27314010b4ef3a9fda3b41",
                "deletions": 30,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "patch": "@@ -970,37 +970,27 @@ private synchronized void nodeUpdate(RMNode nm) {\n     }\n   }\n \n-  private void continuousScheduling() {\n-    while (true) {\n-      List<NodeId> nodeIdList = new ArrayList<NodeId>(nodes.keySet());\n-      // Sort the nodes by space available on them, so that we offer\n-      // containers on emptier nodes first, facilitating an even spread. This\n-      // requires holding the scheduler lock, so that the space available on a\n-      // node doesn't change during the sort.\n-      synchronized (this) {\n-        Collections.sort(nodeIdList, nodeAvailableResourceComparator);\n-      }\n+  void continuousSchedulingAttempt() {\n+    List<NodeId> nodeIdList = new ArrayList<NodeId>(nodes.keySet());\n+    // Sort the nodes by space available on them, so that we offer\n+    // containers on emptier nodes first, facilitating an even spread. This\n+    // requires holding the scheduler lock, so that the space available on a\n+    // node doesn't change during the sort.\n+    synchronized (this) {\n+      Collections.sort(nodeIdList, nodeAvailableResourceComparator);\n+    }\n \n-      // iterate all nodes\n-      for (NodeId nodeId : nodeIdList) {\n-        if (nodes.containsKey(nodeId)) {\n-          FSSchedulerNode node = getFSSchedulerNode(nodeId);\n-          try {\n-            if (Resources.fitsIn(minimumAllocation,\n-                    node.getAvailableResource())) {\n-              attemptScheduling(node);\n-            }\n-          } catch (Throwable ex) {\n-            LOG.warn(\"Error while attempting scheduling for node \" + node +\n-                    \": \" + ex.toString(), ex);\n-          }\n-        }\n-      }\n+    // iterate all nodes\n+    for (NodeId nodeId : nodeIdList) {\n+      FSSchedulerNode node = getFSSchedulerNode(nodeId);\n       try {\n-        Thread.sleep(getContinuousSchedulingSleepMs());\n-      } catch (InterruptedException e) {\n-        LOG.warn(\"Error while doing sleep in continuous scheduling: \" +\n-                e.toString(), e);\n+        if (node != null && Resources.fitsIn(minimumAllocation,\n+            node.getAvailableResource())) {\n+          attemptScheduling(node);\n+        }\n+      } catch (Throwable ex) {\n+        LOG.error(\"Error while attempting scheduling for node \" + node +\n+            \": \" + ex.toString(), ex);\n       }\n     }\n   }\n@@ -1010,6 +1000,12 @@ private void continuousScheduling() {\n \n     @Override\n     public int compare(NodeId n1, NodeId n2) {\n+      if (!nodes.containsKey(n1)) {\n+        return 1;\n+      }\n+      if (!nodes.containsKey(n2)) {\n+        return -1;\n+      }\n       return RESOURCE_CALCULATOR.compare(clusterResource,\n               nodes.get(n2).getAvailableResource(),\n               nodes.get(n1).getAvailableResource());\n@@ -1234,7 +1230,16 @@ private synchronized void initScheduler(Configuration conf)\n           new Runnable() {\n             @Override\n             public void run() {\n-              continuousScheduling();\n+              while (!Thread.currentThread().isInterrupted()) {\n+                try {\n+                  continuousSchedulingAttempt();\n+                  Thread.sleep(getContinuousSchedulingSleepMs());\n+                } catch (InterruptedException e) {\n+                  LOG.error(\"Continuous scheduling thread interrupted. Exiting. \",\n+                      e);\n+                  return;\n+                }\n+              }\n             }\n           }\n       );",
                "raw_url": "https://github.com/apache/hadoop/raw/ff7758299151e3b69c27314010b4ef3a9fda3b41/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/FairScheduler.java",
                "sha": "c0687bcbc249c470465c73ebeda16c64c623b9c6",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/hadoop/blob/ff7758299151e3b69c27314010b4ef3a9fda3b41/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hadoop/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java?ref=ff7758299151e3b69c27314010b4ef3a9fda3b41",
                "deletions": 1,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "patch": "@@ -2763,7 +2763,43 @@ public void testContinuousScheduling() throws Exception {\n     Assert.assertEquals(2, nodes.size());\n   }\n \n-  \n+  @Test\n+  public void testContinuousSchedulingWithNodeRemoved() throws Exception {\n+    // Disable continuous scheduling, will invoke continuous scheduling once manually\n+    scheduler.init(conf);\n+    scheduler.start();\n+    Assert.assertTrue(\"Continuous scheduling should be disabled.\",\n+        !scheduler.isContinuousSchedulingEnabled());\n+\n+    // Add two nodes\n+    RMNode node1 =\n+        MockNodes.newNodeInfo(1, Resources.createResource(8 * 1024, 8), 1,\n+            \"127.0.0.1\");\n+    NodeAddedSchedulerEvent nodeEvent1 = new NodeAddedSchedulerEvent(node1);\n+    scheduler.handle(nodeEvent1);\n+    RMNode node2 =\n+        MockNodes.newNodeInfo(1, Resources.createResource(8 * 1024, 8), 2,\n+            \"127.0.0.2\");\n+    NodeAddedSchedulerEvent nodeEvent2 = new NodeAddedSchedulerEvent(node2);\n+    scheduler.handle(nodeEvent2);\n+    Assert.assertEquals(\"We should have two alive nodes.\",\n+        2, scheduler.getNumClusterNodes());\n+\n+    // Remove one node\n+    NodeRemovedSchedulerEvent removeNode1 = new NodeRemovedSchedulerEvent(node1);\n+    scheduler.handle(removeNode1);\n+    Assert.assertEquals(\"We should only have one alive node.\",\n+        1, scheduler.getNumClusterNodes());\n+\n+    // Invoke the continuous scheduling once\n+    try {\n+      scheduler.continuousSchedulingAttempt();\n+    } catch (Exception e) {\n+      fail(\"Exception happened when doing continuous scheduling. \" +\n+        e.toString());\n+    }\n+  }\n+\n   @Test\n   public void testDontAllowUndeclaredPools() throws Exception{\n     conf.setBoolean(FairSchedulerConfiguration.ALLOW_UNDECLARED_POOLS, false);",
                "raw_url": "https://github.com/apache/hadoop/raw/ff7758299151e3b69c27314010b4ef3a9fda3b41/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/test/java/org/apache/hadoop/yarn/server/resourcemanager/scheduler/fair/TestFairScheduler.java",
                "sha": "df157e75001fd51b7687096383deb70b4eb25d4b",
                "status": "modified"
            }
        ],
        "message": "YARN-2273. NPE in ContinuousScheduling thread when we lose a node. (Wei Yan via kasha)\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1612720 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop/commit/853ed29f2df417878f896a30b68f7412baaa6bf8",
        "repo": "hadoop",
        "unit_tests": [
            "TestFairScheduler.java"
        ]
    }
}