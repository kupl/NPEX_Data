[{"commit": "https://github.com/apache/incubator-hivemall/commit/912010305286f673cc7e8bcbdf1555be4a38921a", "parent": "https://github.com/apache/incubator-hivemall/commit/49441baf0273e8c0b319699584d5b5025e51a8a3", "message": "[HIVEMALL-219][BUGFIX] Fixed NPE in finalizeTraining()\n\n## What changes were proposed in this pull request?\n\nFixed NPE in finalizeTraining() where there are no training example\n\n## What type of PR is it?\n\nBug Fix\n\n## What is the Jira issue?\n\nhttps://issues.apache.org/jira/browse/HIVEMALL-219\n\n## How was this patch tested?\n\nto appear\n\n## Checklist\n\n(Please remove this section if not needed; check `x` for YES, blank for NO)\n\n- [x] Did you apply source code formatter, i.e., `./bin/format_code.sh`, for your commit?\n- [ ] Did you run system tests on Hive (or Spark)?\n\nAuthor: Makoto Yui <myui@apache.org>\n\nCloses #165 from myui/HIVEMALL-219.", "bug_id": "incubator-hivemall_1", "file": [{"additions": 9, "raw_url": "https://github.com/apache/incubator-hivemall/raw/912010305286f673cc7e8bcbdf1555be4a38921a/core/src/main/java/hivemall/topicmodel/ProbabilisticTopicModelBaseUDTF.java", "blob_url": "https://github.com/apache/incubator-hivemall/blob/912010305286f673cc7e8bcbdf1555be4a38921a/core/src/main/java/hivemall/topicmodel/ProbabilisticTopicModelBaseUDTF.java", "sha": "5a5fbce6edf09f562dba4ce4f6fe78a3c2bef2c8", "changes": 11, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/core/src/main/java/hivemall/topicmodel/ProbabilisticTopicModelBaseUDTF.java?ref=912010305286f673cc7e8bcbdf1555be4a38921a", "patch": "@@ -276,10 +276,17 @@ private static void writeBuffer(@Nonnull ByteBuffer srcBuf, @Nonnull NioStateful\n \n     @Override\n     public void close() throws HiveException {\n-        if (model.getDocCount() == 0L) {\n+        if (model == null) {\n+            logger.warn(\n+                \"Model is not initialized bacause no training exmples to learn. Better to revise input data.\");\n+            return;\n+        } else if (model.getDocCount() == 0L) {\n+            logger.warn(\n+                \"model.getDocCount() is zero because no training exmples to learn. Better to revise input data.\");\n             this.model = null;\n-            throw new HiveException(\"No training exmples to learn. Please revise input data.\");\n+            return;\n         }\n+\n         finalizeTraining();\n         forwardModel();\n         this.model = null;", "filename": "core/src/main/java/hivemall/topicmodel/ProbabilisticTopicModelBaseUDTF.java"}], "repo": "incubator-hivemall"}, {"commit": "https://github.com/apache/incubator-hivemall/commit/5ff827da3a57731b5ebc0a7a0763f025603fc2ab", "parent": "https://github.com/apache/incubator-hivemall/commit/2e1104c1eeb4598ba5cc8e74dfba5d36699344f3", "message": "[HIVEMALL-236] to_json/from_json cause KryoException/NullPointerException with ArrayList due to Kryo bug\n\n## What changes were proposed in this pull request?\n\nAvoid NPE in Kryo serialization of List object created by `Arrays.asList`.\n\n## What type of PR is it?\n\nBug Fix\n\n## What is the Jira issue?\n\nhttps://issues.apache.org/jira/browse/HIVEMALL-236\n\n## How was this patch tested?\n\nunit tests\n\n## Checklist\n\n(Please remove this section if not needed; check `x` for YES, blank for NO)\n\n- [x] Did you apply source code formatter, i.e., `./bin/format_code.sh`, for your commit?\n- [ ] Did you run system tests on Hive (or Spark)?\n\nAuthor: Makoto Yui <myui@apache.org>\n\nCloses #182 from myui/json_fix.", "bug_id": "incubator-hivemall_2", "file": [{"additions": 4, "raw_url": "https://github.com/apache/incubator-hivemall/raw/5ff827da3a57731b5ebc0a7a0763f025603fc2ab/core/src/main/java/hivemall/tools/json/FromJsonUDF.java", "blob_url": "https://github.com/apache/incubator-hivemall/blob/5ff827da3a57731b5ebc0a7a0763f025603fc2ab/core/src/main/java/hivemall/tools/json/FromJsonUDF.java", "sha": "2a17f0c8a5056ecd77e8cdf688d37ff311a679e1", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/core/src/main/java/hivemall/tools/json/FromJsonUDF.java?ref=5ff827da3a57731b5ebc0a7a0763f025603fc2ab", "patch": "@@ -20,6 +20,7 @@\n \n import hivemall.utils.hadoop.HiveUtils;\n import hivemall.utils.hadoop.JsonSerdeUtils;\n+import hivemall.utils.lang.ArrayUtils;\n import hivemall.utils.lang.ExceptionUtils;\n import hivemall.utils.lang.StringUtils;\n \n@@ -109,9 +110,10 @@ public ObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentEx\n             final ObjectInspector argOI2 = argOIs[2];\n             if (HiveUtils.isConstString(argOI2)) {\n                 String names = HiveUtils.getConstString(argOI2);\n-                this.columnNames = Arrays.asList(names.split(\",\"));\n+                this.columnNames = ArrayUtils.asKryoSerializableList(names.split(\",\"));\n             } else if (HiveUtils.isConstStringListOI(argOI2)) {\n-                this.columnNames = Arrays.asList(HiveUtils.getConstStringArray(argOI2));\n+                this.columnNames =\n+                        ArrayUtils.asKryoSerializableList(HiveUtils.getConstStringArray(argOI2));\n             } else {\n                 throw new UDFArgumentException(\"Expected `const array<string>` or `const string`\"\n                         + \" but got an unexpected OI type for the third argument: \" + argOI2);", "filename": "core/src/main/java/hivemall/tools/json/FromJsonUDF.java"}, {"additions": 4, "raw_url": "https://github.com/apache/incubator-hivemall/raw/5ff827da3a57731b5ebc0a7a0763f025603fc2ab/core/src/main/java/hivemall/tools/json/ToJsonUDF.java", "blob_url": "https://github.com/apache/incubator-hivemall/blob/5ff827da3a57731b5ebc0a7a0763f025603fc2ab/core/src/main/java/hivemall/tools/json/ToJsonUDF.java", "sha": "c37abe60b407b583ca355ee2493c15e90ea39dc5", "changes": 7, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/core/src/main/java/hivemall/tools/json/ToJsonUDF.java?ref=5ff827da3a57731b5ebc0a7a0763f025603fc2ab", "patch": "@@ -20,10 +20,10 @@\n \n import hivemall.utils.hadoop.HiveUtils;\n import hivemall.utils.hadoop.JsonSerdeUtils;\n+import hivemall.utils.lang.ArrayUtils;\n import hivemall.utils.lang.ExceptionUtils;\n import hivemall.utils.lang.StringUtils;\n \n-import java.util.Arrays;\n import java.util.List;\n \n import javax.annotation.Nullable;\n@@ -133,9 +133,10 @@ public ObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgumentEx\n             final ObjectInspector argOI1 = argOIs[1];\n             if (HiveUtils.isConstString(argOI1)) {\n                 String names = HiveUtils.getConstString(argOI1);\n-                this.columnNames = Arrays.asList(names.split(\",\"));\n+                this.columnNames = ArrayUtils.asKryoSerializableList(names.split(\",\"));\n             } else if (HiveUtils.isConstStringListOI(argOI1)) {\n-                this.columnNames = Arrays.asList(HiveUtils.getConstStringArray(argOI1));\n+                this.columnNames =\n+                        ArrayUtils.asKryoSerializableList(HiveUtils.getConstStringArray(argOI1));\n             } else {\n                 throw new UDFArgumentException(\"Expected `const array<string>` or `const string`\"\n                         + \" but got an unexpected OI type for the third argument: \" + argOI1);", "filename": "core/src/main/java/hivemall/tools/json/ToJsonUDF.java"}, {"additions": 13, "raw_url": "https://github.com/apache/incubator-hivemall/raw/5ff827da3a57731b5ebc0a7a0763f025603fc2ab/core/src/main/java/hivemall/utils/lang/ArrayUtils.java", "blob_url": "https://github.com/apache/incubator-hivemall/blob/5ff827da3a57731b5ebc0a7a0763f025603fc2ab/core/src/main/java/hivemall/utils/lang/ArrayUtils.java", "sha": "1bce60311b2171660d61d4cd8c850f7363609443", "changes": 13, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/core/src/main/java/hivemall/utils/lang/ArrayUtils.java?ref=5ff827da3a57731b5ebc0a7a0763f025603fc2ab", "patch": "@@ -21,6 +21,7 @@\n import hivemall.math.random.PRNG;\n \n import java.lang.reflect.Array;\n+import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n import java.util.Random;\n@@ -750,4 +751,16 @@ public static int count(@Nonnull final int[] values, final int valueToFind) {\n         return ret;\n     }\n \n+    /**\n+     * Workaround for org.apache.hive.com.esotericsoftware.kryo.KryoException\n+     */\n+    @Nonnull\n+    public static List<String> asKryoSerializableList(@Nonnull final String[] array) {\n+        final List<String> list = new ArrayList<>(array.length);\n+        for (String e : array) {\n+            list.add(e);\n+        }\n+        return list;\n+    }\n+\n }", "filename": "core/src/main/java/hivemall/utils/lang/ArrayUtils.java"}, {"additions": 9, "raw_url": "https://github.com/apache/incubator-hivemall/raw/5ff827da3a57731b5ebc0a7a0763f025603fc2ab/core/src/test/java/hivemall/tools/json/FromJsonUDFTest.java", "blob_url": "https://github.com/apache/incubator-hivemall/blob/5ff827da3a57731b5ebc0a7a0763f025603fc2ab/core/src/test/java/hivemall/tools/json/FromJsonUDFTest.java", "sha": "46be75d3907dcb92f925456158a4714ce243d03a", "changes": 9, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/core/src/test/java/hivemall/tools/json/FromJsonUDFTest.java?ref=5ff827da3a57731b5ebc0a7a0763f025603fc2ab", "patch": "@@ -91,4 +91,13 @@ public void testSerialization() throws HiveException, IOException {\n             new Object[] {\"[0.1,1.1,2.2]\"});\n     }\n \n+    @Test\n+    public void testSerializationThreeArgs() throws HiveException, IOException {\n+        TestUtils.testGenericUDFSerialization(FromJsonUDF.class,\n+            new ObjectInspector[] {PrimitiveObjectInspectorFactory.javaStringObjectInspector,\n+                    HiveUtils.getConstStringObjectInspector(\"struct<name:string,age:int>\"),\n+                    HiveUtils.getConstStringObjectInspector(\"person\")},\n+            new Object[] {\"{ \\\"person\\\" : { \\\"name\\\" : \\\"makoto\\\" , \\\"age\\\" : 37 } }\"});\n+    }\n+\n }", "filename": "core/src/test/java/hivemall/tools/json/FromJsonUDFTest.java"}, {"additions": 11, "raw_url": "https://github.com/apache/incubator-hivemall/raw/5ff827da3a57731b5ebc0a7a0763f025603fc2ab/core/src/test/java/hivemall/tools/json/ToJsonUDFTest.java", "blob_url": "https://github.com/apache/incubator-hivemall/blob/5ff827da3a57731b5ebc0a7a0763f025603fc2ab/core/src/test/java/hivemall/tools/json/ToJsonUDFTest.java", "sha": "c2209e5eadd562113c62aa38487a917d7d676d69", "changes": 11, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/core/src/test/java/hivemall/tools/json/ToJsonUDFTest.java?ref=5ff827da3a57731b5ebc0a7a0763f025603fc2ab", "patch": "@@ -19,6 +19,7 @@\n package hivemall.tools.json;\n \n import hivemall.TestUtils;\n+import hivemall.utils.hadoop.HiveUtils;\n import hivemall.utils.hadoop.WritableUtils;\n \n import java.io.IOException;\n@@ -62,4 +63,14 @@ public void testSerialization() throws HiveException, IOException {\n             new Object[] {Arrays.asList(0.1d, 1.1d, 2.1d)});\n     }\n \n+    @Test\n+    public void testSerializationTwoArgs() throws HiveException, IOException {\n+        TestUtils.testGenericUDFSerialization(ToJsonUDF.class,\n+            new ObjectInspector[] {\n+                    ObjectInspectorFactory.getStandardListObjectInspector(\n+                        PrimitiveObjectInspectorFactory.javaDoubleObjectInspector),\n+                    HiveUtils.getConstStringObjectInspector(\"person\")},\n+            new Object[] {Arrays.asList(0.1d, 1.1d, 2.1d)});\n+    }\n+\n }", "filename": "core/src/test/java/hivemall/tools/json/ToJsonUDFTest.java"}], "repo": "incubator-hivemall"}, {"commit": "https://github.com/apache/incubator-hivemall/commit/47d1100c1fab6796f09f0998624b3a445869f1d4", "parent": "https://github.com/apache/incubator-hivemall/commit/30593b14b4006feb0c01e27321830251b7aeb703", "message": "[HIVEMALL-218] Fixed train_lda NPE where input row is null\n\n## What changes were proposed in this pull request?\n\nFixed NegativeArraySizeException where input is NULL of `train_lda`\n\n## What type of PR is it?\n\nBug Fix\n\n## What is the Jira issue?\n\nhttps://issues.apache.org/jira/browse/HIVEMALL-218\n\n## How was this patch tested?\n\nmanual tests\n\n## Checklist\n\n- [x] Did you apply source code formatter, i.e., `./bin/format_code.sh`, for your commit?\n- [x] Did you run system tests on Hive (or Spark)?\n\nAuthor: Makoto Yui <myui@apache.org>\n\nCloses #164 from myui/HIVEMALL-218.", "bug_id": "incubator-hivemall_3", "file": [{"additions": 17, "raw_url": "https://github.com/apache/incubator-hivemall/raw/47d1100c1fab6796f09f0998624b3a445869f1d4/core/src/main/java/hivemall/topicmodel/ProbabilisticTopicModelBaseUDTF.java", "blob_url": "https://github.com/apache/incubator-hivemall/blob/47d1100c1fab6796f09f0998624b3a445869f1d4/core/src/main/java/hivemall/topicmodel/ProbabilisticTopicModelBaseUDTF.java", "sha": "23a021d63cc2f713060a46d3caab5125d04ea690", "changes": 23, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/core/src/main/java/hivemall/topicmodel/ProbabilisticTopicModelBaseUDTF.java?ref=47d1100c1fab6796f09f0998624b3a445869f1d4", "patch": "@@ -57,6 +57,8 @@\n import org.apache.hadoop.mapred.Counters;\n import org.apache.hadoop.mapred.Reporter;\n \n+import com.google.common.base.Preconditions;\n+\n public abstract class ProbabilisticTopicModelBaseUDTF extends UDTFWithOptions {\n     private static final Log logger = LogFactory.getLog(ProbabilisticTopicModelBaseUDTF.class);\n \n@@ -159,11 +161,17 @@ public void process(Object[] args) throws HiveException {\n             this.model = createModel();\n         }\n \n-        final int length = wordCountsOI.getListLength(args[0]);\n+        Preconditions.checkArgument(args.length >= 1);\n+        Object arg0 = args[0];\n+        if (arg0 == null) {\n+            return;\n+        }\n+\n+        final int length = wordCountsOI.getListLength(arg0);\n         final String[] wordCounts = new String[length];\n         int j = 0;\n         for (int i = 0; i < length; i++) {\n-            Object o = wordCountsOI.getListElement(args[0], i);\n+            Object o = wordCountsOI.getListElement(arg0, i);\n             if (o == null) {\n                 throw new HiveException(\"Given feature vector contains invalid null elements\");\n             }\n@@ -268,17 +276,17 @@ private static void writeBuffer(@Nonnull ByteBuffer srcBuf, @Nonnull NioStateful\n \n     @Override\n     public void close() throws HiveException {\n+        if (model.getDocCount() == 0L) {\n+            this.model = null;\n+            throw new HiveException(\"No training exmples to learn. Please revise input data.\");\n+        }\n         finalizeTraining();\n         forwardModel();\n         this.model = null;\n     }\n \n     @VisibleForTesting\n     void finalizeTraining() throws HiveException {\n-        if (model.getDocCount() == 0L) {\n-            this.model = null;\n-            return;\n-        }\n         if (miniBatchCount > 0) { // update for remaining samples\n             model.train(Arrays.copyOfRange(miniBatch, 0, miniBatchCount));\n         }\n@@ -462,6 +470,9 @@ protected void forwardModel() throws HiveException {\n             topicIdx.set(k);\n \n             final SortedMap<Float, List<String>> topicWords = model.getTopicWords(k);\n+            if (topicWords == null) {\n+                continue;\n+            }\n             for (Map.Entry<Float, List<String>> e : topicWords.entrySet()) {\n                 score.set(e.getKey().floatValue());\n                 for (String v : e.getValue()) {", "filename": "core/src/main/java/hivemall/topicmodel/ProbabilisticTopicModelBaseUDTF.java"}], "repo": "incubator-hivemall"}, {"commit": "https://github.com/apache/incubator-hivemall/commit/aa2451ed6012233feb0369a5f774198940d89de5", "parent": "https://github.com/apache/incubator-hivemall/commit/42c695779d2b1aac52dfbb0a2797a966a9ec03a1", "message": "fixed a bug (NPE) in logress_iter.", "bug_id": "incubator-hivemall_4", "file": [{"additions": 9, "raw_url": "https://github.com/apache/incubator-hivemall/raw/aa2451ed6012233feb0369a5f774198940d89de5/src/main/hivemall/regression/OnlineRegressionUDTF.java", "blob_url": "https://github.com/apache/incubator-hivemall/blob/aa2451ed6012233feb0369a5f774198940d89de5/src/main/hivemall/regression/OnlineRegressionUDTF.java", "sha": "ecd707912a449d361613f24917a2b45984b8320d", "changes": 16, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/src/main/hivemall/regression/OnlineRegressionUDTF.java?ref=aa2451ed6012233feb0369a5f774198940d89de5", "patch": "@@ -53,6 +53,7 @@\n public abstract class OnlineRegressionUDTF extends GenericUDTF {\n \n     protected ListObjectInspector featureListOI;\n+    protected ObjectInspector featureInputOI;\n     protected FloatObjectInspector targetOI;\n     protected boolean parseX;\n \n@@ -69,17 +70,18 @@ public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgu\n             throw new UDFArgumentException(getClass().getSimpleName()\n                     + \" takes 2 arguments: List<Int|BigInt|Text> features, float target [, constant string options]\");\n         }\n-        ObjectInspector featureRawOI = processFeaturesOI(argOIs[0]);\n+        this.featureInputOI = processFeaturesOI(argOIs[0]);\n         this.targetOI = (FloatObjectInspector) argOIs[1];\n \n         processOptions(argOIs);\n \n+        ObjectInspector featureOutputOI = featureInputOI;\n         if(parseX && feature_hashing) {\n-            featureRawOI = PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+            featureOutputOI = PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n         }\n \n         if(bias != 0.f) {\n-            this.biasKey = (featureRawOI.getTypeName() == Constants.INT_TYPE_NAME) ? HivemallConstants.BIAS_CLAUSE_INT\n+            this.biasKey = (featureOutputOI.getTypeName() == Constants.INT_TYPE_NAME) ? HivemallConstants.BIAS_CLAUSE_INT\n                     : new Text(HivemallConstants.BIAS_CLAUSE);\n         } else {\n             this.biasKey = null;\n@@ -89,7 +91,7 @@ public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgu\n         ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();\n \n         fieldNames.add(\"feature\");\n-        ObjectInspector featureOI = ObjectInspectorUtils.getStandardObjectInspector(featureRawOI);\n+        ObjectInspector featureOI = ObjectInspectorUtils.getStandardObjectInspector(featureOutputOI);\n         fieldOIs.add(featureOI);\n         fieldNames.add(\"weight\");\n         fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);\n@@ -178,7 +180,7 @@ protected void train(final Map<Object, FloatWritable> weights, final Collection<\n     }\n \n     protected float predict(final Collection<?> features) {\n-        final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();\n+        final ObjectInspector featureInspector = this.featureInputOI;\n         final boolean parseX = this.parseX;\n \n         float score = 0f;\n@@ -210,7 +212,7 @@ protected float predict(final Collection<?> features) {\n     }\n \n     protected PredictionResult calcScore(Collection<?> features) {\n-        final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();\n+        final ObjectInspector featureInspector = this.featureInputOI;\n         final boolean parseX = this.parseX;\n \n         float score = 0.f;\n@@ -255,7 +257,7 @@ protected float dloss(float target, float predicted) {\n     }\n \n     protected void update(Collection<?> features, float coeff) {\n-        final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();\n+        final ObjectInspector featureInspector = this.featureInputOI;\n \n         for(Object f : features) {// w[i] += y * x[i]\n             final Object x;", "filename": "src/main/hivemall/regression/OnlineRegressionUDTF.java"}], "repo": "incubator-hivemall"}, {"commit": "https://github.com/apache/incubator-hivemall/commit/3960cf2cd591f1a371648a0576c2a80bbd50aba8", "parent": "https://github.com/apache/incubator-hivemall/commit/7b9e6bae691e746521242d0cb71270ede648e0f3", "message": "[HIVEMALL-157] Avoid Null Pointer Exception caused by uninitialized queue handler\n\n## What changes were proposed in this pull request?\n\nEven though `to_ordered_list` allows (and ignores) NULL inputs, following query fails due to NPE:\n\n```sql\nselect to_ordered_list(null, null)\n```\n> Null Pointer Exception\n\nThis PR fixes the problem; now, the function returns empty list in case that queue handler is uninitialized because of NULL inputs:\n\n> []\n\n## What type of PR is it?\n\nBug Fix\n\n## What is the Jira issue?\n\nhttps://issues.apache.org/jira/browse/HIVEMALL-157\n\n## How was this patch tested?\n\nManually tested on local and EMR Hive\n\n## Checklist\n\n- [x] Did you apply source code formatter, i.e., `mvn formatter:format`, for your commit?\n- [x] Did you run system tests on Hive (or Spark)?\n\nAuthor: Takuya Kitazawa <k.takuti@gmail.com>\n\nCloses #124 from takuti/fix-to_ordered_list-npe.", "bug_id": "incubator-hivemall_5", "file": [{"additions": 11, "raw_url": "https://github.com/apache/incubator-hivemall/raw/3960cf2cd591f1a371648a0576c2a80bbd50aba8/core/src/main/java/hivemall/tools/list/UDAFToOrderedList.java", "blob_url": "https://github.com/apache/incubator-hivemall/blob/3960cf2cd591f1a371648a0576c2a80bbd50aba8/core/src/main/java/hivemall/tools/list/UDAFToOrderedList.java", "sha": "f17d1f4629aefcc34898eac5015cbaa861102440", "changes": 15, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/core/src/main/java/hivemall/tools/list/UDAFToOrderedList.java?ref=3960cf2cd591f1a371648a0576c2a80bbd50aba8", "patch": "@@ -311,11 +311,11 @@ public Object terminatePartial(@SuppressWarnings(\"deprecation\") AggregationBuffe\n             QueueAggregationBuffer myagg = (QueueAggregationBuffer) agg;\n \n             Pair<List<Object>, List<Object>> tuples = myagg.drainQueue();\n-            List<Object> keyList = tuples.getKey();\n-            List<Object> valueList = tuples.getValue();\n-            if (valueList.isEmpty()) {\n+            if (tuples == null) {\n                 return null;\n             }\n+            List<Object> keyList = tuples.getKey();\n+            List<Object> valueList = tuples.getValue();\n \n             Object[] partialResult = new Object[4];\n             partialResult[0] = valueList;\n@@ -363,6 +363,9 @@ public void merge(@SuppressWarnings(\"deprecation\") AggregationBuffer agg, Object\n                 throws HiveException {\n             QueueAggregationBuffer myagg = (QueueAggregationBuffer) agg;\n             Pair<List<Object>, List<Object>> tuples = myagg.drainQueue();\n+            if (tuples == null) {\n+                return null;\n+            }\n             return tuples.getValue();\n         }\n \n@@ -404,8 +407,12 @@ void merge(@Nonnull List<Object> o_keyList, @Nonnull List<Object> o_valueList) {\n                 }\n             }\n \n-            @Nonnull\n+            @Nullable\n             Pair<List<Object>, List<Object>> drainQueue() {\n+                if (queueHandler == null) {\n+                    return null;\n+                }\n+\n                 int n = queueHandler.size();\n                 final Object[] keys = new Object[n];\n                 final Object[] values = new Object[n];", "filename": "core/src/main/java/hivemall/tools/list/UDAFToOrderedList.java"}, {"additions": 39, "raw_url": "https://github.com/apache/incubator-hivemall/raw/3960cf2cd591f1a371648a0576c2a80bbd50aba8/core/src/test/java/hivemall/tools/list/UDAFToOrderedListTest.java", "blob_url": "https://github.com/apache/incubator-hivemall/blob/3960cf2cd591f1a371648a0576c2a80bbd50aba8/core/src/test/java/hivemall/tools/list/UDAFToOrderedListTest.java", "sha": "c7e9e701bc4e5b62185e8401826ea69bd6afd160", "changes": 39, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/core/src/test/java/hivemall/tools/list/UDAFToOrderedListTest.java?ref=3960cf2cd591f1a371648a0576c2a80bbd50aba8", "patch": "@@ -339,4 +339,43 @@ public void testReverseTailKWithKey() throws Exception {\n         Assert.assertEquals(\"banana\", res.get(1));\n     }\n \n+    @Test\n+    public void testNullOnly() throws Exception {\n+        ObjectInspector[] inputOIs = new ObjectInspector[] {PrimitiveObjectInspectorFactory.javaDoubleObjectInspector};\n+\n+        final String[] values = new String[] {null, null, null};\n+\n+        evaluator.init(GenericUDAFEvaluator.Mode.PARTIAL1, inputOIs);\n+        evaluator.reset(agg);\n+\n+        for (int i = 0; i < values.length; i++) {\n+            evaluator.iterate(agg, new Object[] {values[i]});\n+        }\n+\n+        List<Object> res = evaluator.terminate(agg);\n+\n+        Assert.assertNull(res);\n+    }\n+\n+    @Test\n+    public void testNullMixed() throws Exception {\n+        ObjectInspector[] inputOIs = new ObjectInspector[] {PrimitiveObjectInspectorFactory.javaDoubleObjectInspector};\n+\n+        final String[] values = new String[] {\"banana\", \"apple\", null, \"candy\"};\n+\n+        evaluator.init(GenericUDAFEvaluator.Mode.PARTIAL1, inputOIs);\n+        evaluator.reset(agg);\n+\n+        for (int i = 0; i < values.length; i++) {\n+            evaluator.iterate(agg, new Object[] {values[i]});\n+        }\n+\n+        List<Object> res = evaluator.terminate(agg);\n+\n+        Assert.assertEquals(3, res.size());\n+        Assert.assertEquals(\"apple\", res.get(0));\n+        Assert.assertEquals(\"banana\", res.get(1));\n+        Assert.assertEquals(\"candy\", res.get(2));\n+    }\n+\n }", "filename": "core/src/test/java/hivemall/tools/list/UDAFToOrderedListTest.java"}], "repo": "incubator-hivemall"}, {"commit": "https://github.com/apache/incubator-hivemall/commit/64fa5ee25b16ef6477b78a0653c0da123c2f3313", "parent": "https://github.com/apache/incubator-hivemall/commit/91d3744ce5d237b6bd3cc483c845d5e71981face", "message": "Merge pull request #13 from myui/hotfixes\n\nfixed a bug (NPE) in logress_iter.", "bug_id": "incubator-hivemall_6", "file": [{"additions": 9, "raw_url": "https://github.com/apache/incubator-hivemall/raw/64fa5ee25b16ef6477b78a0653c0da123c2f3313/src/main/hivemall/regression/OnlineRegressionUDTF.java", "blob_url": "https://github.com/apache/incubator-hivemall/blob/64fa5ee25b16ef6477b78a0653c0da123c2f3313/src/main/hivemall/regression/OnlineRegressionUDTF.java", "sha": "ecd707912a449d361613f24917a2b45984b8320d", "changes": 16, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/src/main/hivemall/regression/OnlineRegressionUDTF.java?ref=64fa5ee25b16ef6477b78a0653c0da123c2f3313", "patch": "@@ -53,6 +53,7 @@\n public abstract class OnlineRegressionUDTF extends GenericUDTF {\n \n     protected ListObjectInspector featureListOI;\n+    protected ObjectInspector featureInputOI;\n     protected FloatObjectInspector targetOI;\n     protected boolean parseX;\n \n@@ -69,17 +70,18 @@ public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgu\n             throw new UDFArgumentException(getClass().getSimpleName()\n                     + \" takes 2 arguments: List<Int|BigInt|Text> features, float target [, constant string options]\");\n         }\n-        ObjectInspector featureRawOI = processFeaturesOI(argOIs[0]);\n+        this.featureInputOI = processFeaturesOI(argOIs[0]);\n         this.targetOI = (FloatObjectInspector) argOIs[1];\n \n         processOptions(argOIs);\n \n+        ObjectInspector featureOutputOI = featureInputOI;\n         if(parseX && feature_hashing) {\n-            featureRawOI = PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n+            featureOutputOI = PrimitiveObjectInspectorFactory.javaIntObjectInspector;\n         }\n \n         if(bias != 0.f) {\n-            this.biasKey = (featureRawOI.getTypeName() == Constants.INT_TYPE_NAME) ? HivemallConstants.BIAS_CLAUSE_INT\n+            this.biasKey = (featureOutputOI.getTypeName() == Constants.INT_TYPE_NAME) ? HivemallConstants.BIAS_CLAUSE_INT\n                     : new Text(HivemallConstants.BIAS_CLAUSE);\n         } else {\n             this.biasKey = null;\n@@ -89,7 +91,7 @@ public StructObjectInspector initialize(ObjectInspector[] argOIs) throws UDFArgu\n         ArrayList<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>();\n \n         fieldNames.add(\"feature\");\n-        ObjectInspector featureOI = ObjectInspectorUtils.getStandardObjectInspector(featureRawOI);\n+        ObjectInspector featureOI = ObjectInspectorUtils.getStandardObjectInspector(featureOutputOI);\n         fieldOIs.add(featureOI);\n         fieldNames.add(\"weight\");\n         fieldOIs.add(PrimitiveObjectInspectorFactory.writableFloatObjectInspector);\n@@ -178,7 +180,7 @@ protected void train(final Map<Object, FloatWritable> weights, final Collection<\n     }\n \n     protected float predict(final Collection<?> features) {\n-        final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();\n+        final ObjectInspector featureInspector = this.featureInputOI;\n         final boolean parseX = this.parseX;\n \n         float score = 0f;\n@@ -210,7 +212,7 @@ protected float predict(final Collection<?> features) {\n     }\n \n     protected PredictionResult calcScore(Collection<?> features) {\n-        final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();\n+        final ObjectInspector featureInspector = this.featureInputOI;\n         final boolean parseX = this.parseX;\n \n         float score = 0.f;\n@@ -255,7 +257,7 @@ protected float dloss(float target, float predicted) {\n     }\n \n     protected void update(Collection<?> features, float coeff) {\n-        final ObjectInspector featureInspector = featureListOI.getListElementObjectInspector();\n+        final ObjectInspector featureInspector = this.featureInputOI;\n \n         for(Object f : features) {// w[i] += y * x[i]\n             final Object x;", "filename": "src/main/hivemall/regression/OnlineRegressionUDTF.java"}], "repo": "incubator-hivemall"}, {"commit": "https://github.com/apache/incubator-hivemall/commit/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb", "parent": "https://github.com/apache/incubator-hivemall/commit/89ec56e31a46ca7dd36f1bca030a21aa313a4115", "message": "Onehot encoding (#357)\n\n* [#294] Implement onehot_encoding UDAF\r\n\r\n* Updated hive version dependencies from v0.12.0 to v0.13.0\r\n\r\n* Updated onehot_encoding UDAF\r\n\r\n* Fixed compilation errors for API changes of Hive v0.13.0\r\n\r\n* Avoided NPE in\r\norg.apache.hadoop.hive.conf.HiveConf.getVar(HiveConf.java:1295)\r\n\r\n* Ignored a test to support Hive v0.13.0", "bug_id": "incubator-hivemall_7", "file": [{"additions": 1, "raw_url": "https://github.com/apache/incubator-hivemall/raw/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/core/pom.xml", "blob_url": "https://github.com/apache/incubator-hivemall/blob/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/core/pom.xml", "sha": "b9d2fbcd0d53ffad20c63dd21b975d7e2ad86d5a", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/core/pom.xml?ref=ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb", "patch": "@@ -24,7 +24,7 @@\n \t\t<dependency>\n \t\t\t<groupId>org.apache.hive</groupId>\n \t\t\t<artifactId>hive-exec</artifactId>\n-\t\t\t<version>0.12.0</version>\n+\t\t\t<version>${hive.version}</version>\n \t\t\t<scope>provided</scope>\n \t\t\t<exclusions>\n \t\t\t\t<exclusion>", "filename": "core/pom.xml"}, {"additions": 335, "raw_url": "https://github.com/apache/incubator-hivemall/raw/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/core/src/main/java/hivemall/ftvec/trans/OnehotEncodingUDAF.java", "blob_url": "https://github.com/apache/incubator-hivemall/blob/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/core/src/main/java/hivemall/ftvec/trans/OnehotEncodingUDAF.java", "sha": "1d01130b1581b28c50cd2fe481e07128970cfbd8", "changes": 335, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/core/src/main/java/hivemall/ftvec/trans/OnehotEncodingUDAF.java?ref=ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb", "patch": "@@ -0,0 +1,335 @@\n+/*\n+ * Hivemall: Hive scalable Machine Learning Library\n+ *\n+ * Copyright (C) 2016 Makoto YUI\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *         http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package hivemall.ftvec.trans;\n+\n+import hivemall.utils.hadoop.HiveUtils;\n+import hivemall.utils.hadoop.WritableUtils;\n+import hivemall.utils.lang.Identifier;\n+import hivemall.utils.lang.Preconditions;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import javax.annotation.CheckForNull;\n+import javax.annotation.Nonnull;\n+import javax.annotation.Nullable;\n+\n+import org.apache.hadoop.hive.ql.exec.Description;\n+import org.apache.hadoop.hive.ql.exec.UDFArgumentException;\n+import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;\n+import org.apache.hadoop.hive.ql.metadata.HiveException;\n+import org.apache.hadoop.hive.ql.parse.SemanticException;\n+import org.apache.hadoop.hive.ql.udf.UDFType;\n+import org.apache.hadoop.hive.ql.udf.generic.AbstractGenericUDAFResolver;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator;\n+import org.apache.hadoop.hive.ql.udf.generic.GenericUDAFEvaluator.AbstractAggregationBuffer;\n+import org.apache.hadoop.hive.serde2.objectinspector.ListObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.MapObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;\n+import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructField;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;\n+import org.apache.hadoop.io.Writable;\n+\n+@Description(name = \"onehot_encoding\",\n+        value = \"_FUNC_(PRIMITIVE feature, ...) - Compute onehot encoded label for each feature\")\n+@UDFType(deterministic = true, stateful = true)\n+public final class OnehotEncodingUDAF extends AbstractGenericUDAFResolver {\n+\n+    public OnehotEncodingUDAF() {\n+        super();\n+    }\n+\n+    @Override\n+    public GenericUDAFEvaluator getEvaluator(@Nonnull TypeInfo[] argTypes) throws SemanticException {\n+        final int numFeatures = argTypes.length;\n+        if (numFeatures == 0) {\n+            throw new UDFArgumentException(\"_FUNC_ requires at least 1 argument\");\n+        }\n+        for (int i = 0; i < numFeatures; i++) {\n+            if (argTypes[i] == null) {\n+                throw new UDFArgumentTypeException(i,\n+                    \"Null type is found. Only primitive type arguments are accepted.\");\n+            }\n+            if (argTypes[i].getCategory() != ObjectInspector.Category.PRIMITIVE) {\n+                throw new UDFArgumentTypeException(i,\n+                    \"Only primitive type arguments are accepted but \" + argTypes[i].getTypeName()\n+                            + \" was passed as parameter 1.\");\n+            }\n+        }\n+\n+        return new GenericUDAFOnehotEncodingEvaluator();\n+    }\n+\n+    public static final class GenericUDAFOnehotEncodingEvaluator extends GenericUDAFEvaluator {\n+\n+        // input OI\n+        private PrimitiveObjectInspector[] inputElemOIs;\n+        // merge input OI\n+        private StructObjectInspector mergeOI;\n+        private StructField[] fields;\n+        private ListObjectInspector[] fieldOIs;\n+\n+        public GenericUDAFOnehotEncodingEvaluator() {}\n+\n+        @Override\n+        public ObjectInspector init(Mode m, ObjectInspector[] argOIs) throws HiveException {\n+            super.init(m, argOIs);\n+\n+            // initialize input\n+            if (m == Mode.PARTIAL1 || m == Mode.COMPLETE) {// from original data\n+                this.inputElemOIs = new PrimitiveObjectInspector[argOIs.length];\n+                for (int i = 0; i < argOIs.length; i++) {\n+                    inputElemOIs[i] = HiveUtils.asPrimitiveObjectInspector(argOIs[i]);\n+                }\n+            } else {// from partial aggregation\n+                Preconditions.checkArgument(argOIs.length == 1);\n+                this.mergeOI = HiveUtils.asStructOI(argOIs[0]);\n+                final int numFields = mergeOI.getAllStructFieldRefs().size();\n+                this.fields = new StructField[numFields];\n+                this.fieldOIs = new ListObjectInspector[numFields];\n+                this.inputElemOIs = new PrimitiveObjectInspector[numFields];\n+                for (int i = 0; i < numFields; i++) {\n+                    StructField field = mergeOI.getStructFieldRef(\"f\" + String.valueOf(i));\n+                    fields[i] = field;\n+                    ListObjectInspector fieldOI = HiveUtils.asListOI(field.getFieldObjectInspector());\n+                    fieldOIs[i] = fieldOI;\n+                    inputElemOIs[i] = HiveUtils.asPrimitiveObjectInspector(fieldOI.getListElementObjectInspector());\n+                }\n+            }\n+\n+            // initialize output\n+            final ObjectInspector outputOI;\n+            switch (m) {\n+                case PARTIAL1:// from original data to partial aggregation data                    \n+                    outputOI = internalMergeOutputOI(inputElemOIs);\n+                    break;\n+                case PARTIAL2:// from partial aggregation data to partial aggregation data\n+                    outputOI = internalMergeOutputOI(inputElemOIs);\n+                    break;\n+                case COMPLETE:// from original data directly to full aggregation\n+                    outputOI = terminalOutputOI(inputElemOIs);\n+                    break;\n+                case FINAL: // from partial aggregation to full aggregation\n+                    outputOI = terminalOutputOI(inputElemOIs);\n+                    break;\n+                default:\n+                    throw new IllegalStateException(\"Illegal mode: \" + m);\n+            }\n+            return outputOI;\n+        }\n+\n+        @Nonnull\n+        private static StructObjectInspector internalMergeOutputOI(\n+                @CheckForNull PrimitiveObjectInspector[] inputOIs) throws UDFArgumentException {\n+            Preconditions.checkNotNull(inputOIs);\n+\n+            final int numOIs = inputOIs.length;\n+            final List<String> fieldNames = new ArrayList<String>(numOIs);\n+            final List<ObjectInspector> fieldOIs = new ArrayList<ObjectInspector>(numOIs);\n+            for (int i = 0; i < numOIs; i++) {\n+                fieldNames.add(\"f\" + String.valueOf(i));\n+                ObjectInspector elemOI = ObjectInspectorUtils.getStandardObjectInspector(\n+                    inputOIs[i], ObjectInspectorCopyOption.WRITABLE);\n+                ListObjectInspector listOI = ObjectInspectorFactory.getStandardListObjectInspector(elemOI);\n+                fieldOIs.add(listOI);\n+            }\n+            return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);\n+        }\n+\n+        @Nonnull\n+        private static StructObjectInspector terminalOutputOI(\n+                @CheckForNull PrimitiveObjectInspector[] inputOIs) {\n+            Preconditions.checkNotNull(inputOIs);\n+            Preconditions.checkArgument(inputOIs.length >= 1, inputOIs.length);\n+\n+            final List<String> fieldNames = new ArrayList<>(inputOIs.length);\n+            final List<ObjectInspector> fieldOIs = new ArrayList<>(inputOIs.length);\n+            for (int i = 0; i < inputOIs.length; i++) {\n+                fieldNames.add(\"f\" + String.valueOf(i + 1));\n+                ObjectInspector keyOI = ObjectInspectorUtils.getStandardObjectInspector(\n+                    inputOIs[i], ObjectInspectorCopyOption.WRITABLE);\n+                MapObjectInspector mapOI = ObjectInspectorFactory.getStandardMapObjectInspector(\n+                    keyOI, PrimitiveObjectInspectorFactory.javaIntObjectInspector);\n+                fieldOIs.add(mapOI);\n+            }\n+            return ObjectInspectorFactory.getStandardStructObjectInspector(fieldNames, fieldOIs);\n+        }\n+\n+        @SuppressWarnings(\"deprecation\")\n+        @Override\n+        public AggregationBuffer getNewAggregationBuffer() throws HiveException {\n+            EncodingBuffer buf = new EncodingBuffer();\n+            reset(buf);\n+            return buf;\n+        }\n+\n+        @SuppressWarnings(\"deprecation\")\n+        @Override\n+        public void reset(AggregationBuffer aggregationBuffer) throws HiveException {\n+            EncodingBuffer buf = (EncodingBuffer) aggregationBuffer;\n+            buf.reset();\n+        }\n+\n+        @SuppressWarnings(\"deprecation\")\n+        @Override\n+        public void iterate(AggregationBuffer aggregationBuffer, Object[] parameters)\n+                throws HiveException {\n+            Preconditions.checkNotNull(inputElemOIs);\n+\n+            EncodingBuffer buf = (EncodingBuffer) aggregationBuffer;\n+            buf.iterate(parameters, inputElemOIs);\n+        }\n+\n+        @SuppressWarnings(\"deprecation\")\n+        @Override\n+        public Object[] terminatePartial(AggregationBuffer aggregationBuffer) throws HiveException {\n+            EncodingBuffer buf = (EncodingBuffer) aggregationBuffer;\n+            return buf.partial();\n+        }\n+\n+        @SuppressWarnings(\"deprecation\")\n+        @Override\n+        public void merge(AggregationBuffer aggregationBuffer, Object partial) throws HiveException {\n+            if (partial == null) {\n+                return;\n+            }\n+\n+            EncodingBuffer buf = (EncodingBuffer) aggregationBuffer;\n+            buf.merge(partial, mergeOI, fields, fieldOIs);\n+        }\n+\n+        @SuppressWarnings(\"deprecation\")\n+        @Override\n+        public Object[] terminate(AggregationBuffer aggregationBuffer) throws HiveException {\n+            EncodingBuffer buf = (EncodingBuffer) aggregationBuffer;\n+            return buf.terminate();\n+        }\n+    }\n+\n+    public static final class EncodingBuffer extends AbstractAggregationBuffer {\n+\n+        @Nullable\n+        private Identifier<Writable>[] identifiers;\n+\n+        public EncodingBuffer() {}\n+\n+        void reset() {\n+            this.identifiers = null;\n+        }\n+\n+        @SuppressWarnings(\"unchecked\")\n+        void iterate(@Nonnull final Object[] args,\n+                @Nonnull final PrimitiveObjectInspector[] inputOIs) throws HiveException {\n+            Preconditions.checkArgument(args.length == inputOIs.length);\n+\n+            final int length = args.length;\n+            if (identifiers == null) {\n+                this.identifiers = new Identifier[length];\n+                for (int i = 0; i < length; i++) {\n+                    identifiers[i] = new Identifier<>(1);\n+                }\n+            }\n+\n+            for (int i = 0; i < length; i++) {\n+                Object arg = args[i];\n+                if (arg == null) {\n+                    continue;\n+                }\n+                Writable writable = WritableUtils.copyToWritable(arg, inputOIs[i]);\n+                identifiers[i].put(writable);\n+            }\n+        }\n+\n+        @Nullable\n+        Object[] partial() throws HiveException {\n+            if (identifiers == null) {\n+                return null;\n+            }\n+\n+            final int length = identifiers.length;\n+            final Object[] partial = new Object[length];\n+            for (int i = 0; i < length; i++) {\n+                Set<Writable> id = identifiers[i].getMap().keySet();\n+                final List<Writable> list = new ArrayList<Writable>(id.size());\n+                for (Writable e : id) {\n+                    Preconditions.checkNotNull(e);\n+                    list.add(e);\n+                }\n+                partial[i] = list;\n+            }\n+            return partial;\n+        }\n+\n+        @SuppressWarnings(\"unchecked\")\n+        void merge(@Nonnull final Object partial, @Nonnull final StructObjectInspector mergeOI,\n+                @Nonnull final StructField[] fields, @Nonnull final ListObjectInspector[] fieldOIs) {\n+            Preconditions.checkArgument(fields.length == fieldOIs.length);\n+\n+            final int numFields = fieldOIs.length;\n+            if (identifiers == null) {\n+                this.identifiers = new Identifier[numFields];\n+            }\n+            Preconditions.checkArgument(fields.length == identifiers.length);\n+\n+            for (int i = 0; i < numFields; i++) {\n+                Identifier<Writable> id = identifiers[i];\n+                if (id == null) {\n+                    id = new Identifier<>(1);\n+                    identifiers[i] = id;\n+                }\n+                final Object fieldData = mergeOI.getStructFieldData(partial, fields[i]);\n+                final ListObjectInspector fieldOI = fieldOIs[i];\n+                for (int j = 0, size = fieldOI.getListLength(fieldData); j < size; j++) {\n+                    Object o = fieldOI.getListElement(fieldData, j);\n+                    Preconditions.checkNotNull(o);\n+                    id.valueOf((Writable) o);\n+                }\n+            }\n+        }\n+\n+        @Nullable\n+        Object[] terminate() {\n+            if (identifiers == null) {\n+                return null;\n+            }\n+            final Object[] ret = new Object[identifiers.length];\n+            int max = 0;\n+            for (int i = 0; i < identifiers.length; i++) {\n+                final Map<Writable, Integer> m = identifiers[i].getMap();\n+                if (max != 0) {\n+                    for (Map.Entry<Writable, Integer> e : m.entrySet()) {\n+                        int original = e.getValue().intValue();\n+                        e.setValue(Integer.valueOf(max + original));\n+                    }\n+                }\n+                ret[i] = m;\n+                max += m.size();\n+            }\n+            return ret;\n+        }\n+\n+    }\n+}", "filename": "core/src/main/java/hivemall/ftvec/trans/OnehotEncodingUDAF.java"}, {"additions": 9, "raw_url": "https://github.com/apache/incubator-hivemall/raw/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/core/src/main/java/hivemall/utils/hadoop/HiveUtils.java", "blob_url": "https://github.com/apache/incubator-hivemall/blob/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/core/src/main/java/hivemall/utils/hadoop/HiveUtils.java", "sha": "91f1dfa04e5769910b566a5f3e0ed81681b109e4", "changes": 9, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/core/src/main/java/hivemall/utils/hadoop/HiveUtils.java?ref=ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb", "patch": "@@ -55,6 +55,7 @@\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector.Category;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;\n import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;\n+import org.apache.hadoop.hive.serde2.objectinspector.StructObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.primitive.BinaryObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.primitive.BooleanObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.primitive.IntObjectInspector;\n@@ -160,6 +161,14 @@ public static int asJavaInt(@Nullable final Object o) {\n         }\n         return Arrays.asList(ary);\n     }\n+    \n+    @Nonnull\n+    public static StructObjectInspector asStructOI(@Nonnull final ObjectInspector oi) throws UDFArgumentException {\n+        if(oi.getCategory() != Category.STRUCT) {\n+            throw new UDFArgumentException(\"Expected Struct OI but got: \" + oi.getTypeName());\n+        }\n+        return (StructObjectInspector) oi;        \n+    }\n \n     public static boolean isPrimitiveOI(@Nonnull final ObjectInspector oi) {\n         return oi.getCategory() == Category.PRIMITIVE;", "filename": "core/src/main/java/hivemall/utils/hadoop/HiveUtils.java"}, {"additions": 15, "raw_url": "https://github.com/apache/incubator-hivemall/raw/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/core/src/main/java/hivemall/utils/hadoop/WritableUtils.java", "blob_url": "https://github.com/apache/incubator-hivemall/blob/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/core/src/main/java/hivemall/utils/hadoop/WritableUtils.java", "sha": "c92f6ceb0ee493caaa9436e6155165a358ac8d4e", "changes": 15, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/core/src/main/java/hivemall/utils/hadoop/WritableUtils.java?ref=ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb", "patch": "@@ -18,12 +18,18 @@\n  */\n package hivemall.utils.hadoop;\n \n+import hivemall.utils.lang.Preconditions;\n+\n import java.util.ArrayList;\n import java.util.List;\n \n+import javax.annotation.CheckForNull;\n import javax.annotation.Nonnull;\n \n import org.apache.hadoop.hive.serde2.io.DoubleWritable;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;\n+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;\n+import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector;\n import org.apache.hadoop.io.BooleanWritable;\n import org.apache.hadoop.io.ByteWritable;\n import org.apache.hadoop.io.BytesWritable;\n@@ -194,4 +200,13 @@ public static Writable toWritable(Object object) {\n         return new BytesWritable(object.toString().getBytes());\n     }\n \n+    @Nonnull\n+    public static Writable copyToWritable(@Nonnull final Object obj,\n+            @CheckForNull final PrimitiveObjectInspector oi) {\n+        Preconditions.checkNotNull(oi);\n+        Object ret = ObjectInspectorUtils.copyToStandardObject(obj, oi,\n+            ObjectInspectorCopyOption.WRITABLE);\n+        return (Writable) ret;\n+    }\n+\n }", "filename": "core/src/main/java/hivemall/utils/hadoop/WritableUtils.java"}, {"additions": 31, "raw_url": "https://github.com/apache/incubator-hivemall/raw/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/core/src/main/java/hivemall/utils/lang/Identifier.java", "blob_url": "https://github.com/apache/incubator-hivemall/blob/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/core/src/main/java/hivemall/utils/lang/Identifier.java", "sha": "b3cf63dae8df16147115195eda74655699b1c794", "changes": 38, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/core/src/main/java/hivemall/utils/lang/Identifier.java?ref=ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb", "patch": "@@ -22,29 +22,53 @@\n import java.util.HashMap;\n import java.util.Map;\n \n+import javax.annotation.Nonnull;\n import javax.annotation.concurrent.NotThreadSafe;\n \n @NotThreadSafe\n-public final class Identifier<E> implements Serializable {\n+public final class Identifier<T> implements Serializable {\n     private static final long serialVersionUID = 7949630590734361716L;\n \n-    private final Map<E, Integer> counts;\n+    private final Map<T, Integer> counts;\n     private int sequence;\n \n     public Identifier() {\n-        this.counts = new HashMap<E, Integer>(512);\n-        this.sequence = -1;\n+        this(0);\n     }\n \n-    public int valueOf(E key) {\n+    public Identifier(int initSeq) {\n+        this.counts = new HashMap<>(512);\n+        this.sequence = initSeq;\n+    }\n+\n+    public int valueOf(@Nonnull T key) {\n         Integer count = counts.get(key);\n         if (count == null) {\n+            int id = sequence;\n+            counts.put(key, Integer.valueOf(id));\n             ++sequence;\n-            counts.put(key, Integer.valueOf(sequence));\n-            return sequence;\n+            return id;\n         } else {\n             return count.intValue();\n         }\n     }\n \n+    public void put(@Nonnull T key) {\n+        Integer count = counts.get(key);\n+        if (count != null) {\n+            return;\n+        }\n+        int id = sequence;\n+        counts.put(key, Integer.valueOf(id));\n+        ++sequence;\n+    }\n+\n+    public Map<T, Integer> getMap() {\n+        return counts;\n+    }\n+\n+    public int size() {\n+        return sequence;\n+    }\n+\n }", "filename": "core/src/main/java/hivemall/utils/lang/Identifier.java"}, {"additions": 3, "raw_url": "https://github.com/apache/incubator-hivemall/raw/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/core/src/main/java/org/apache/hadoop/hive/ql/exec/MapredContextAccessor.java", "blob_url": "https://github.com/apache/incubator-hivemall/blob/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/core/src/main/java/org/apache/hadoop/hive/ql/exec/MapredContextAccessor.java", "sha": "73e870277e85f5ab9d94dd645921c0d8947e966b", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/core/src/main/java/org/apache/hadoop/hive/ql/exec/MapredContextAccessor.java?ref=ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb", "patch": "@@ -37,6 +37,9 @@ public static MapredContext get() {\n \n     @Nonnull\n     public static MapredContext create(boolean isMap, @Nullable JobConf jobConf) {\n+        if (jobConf == null) {\n+            jobConf = new JobConf(false); // null is not allowed in Hive v0.13\n+        }\n         return MapredContext.init(isMap, jobConf);\n     }\n ", "filename": "core/src/main/java/org/apache/hadoop/hive/ql/exec/MapredContextAccessor.java"}, {"additions": 5, "raw_url": "https://github.com/apache/incubator-hivemall/raw/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/core/src/test/java/hivemall/ftvec/trans/TestBinarizeLabelUDTF.java", "blob_url": "https://github.com/apache/incubator-hivemall/blob/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/core/src/test/java/hivemall/ftvec/trans/TestBinarizeLabelUDTF.java", "sha": "d93f7bfe52b8c6372f440b95af26911ff15e94c5", "changes": 7, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/core/src/test/java/hivemall/ftvec/trans/TestBinarizeLabelUDTF.java?ref=ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb", "patch": "@@ -28,7 +28,6 @@\n import java.util.Arrays;\n import java.util.List;\n \n-import org.apache.hadoop.hive.ql.exec.UDFArgumentException;\n import org.apache.hadoop.hive.ql.metadata.HiveException;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n@@ -41,7 +40,11 @@\n @RunWith(PowerMockRunner.class)\n @PrepareForTest({BinarizeLabelUDTF.class})\n public class TestBinarizeLabelUDTF {\n-    @Test(expected = UDFArgumentException.class)\n+\n+    // ignored to avoid\n+    // org.apache.hadoop.hive.shims.ShimLoader.getMajorVersion(ShimLoader.java:141) ExceptionInInitializerError\n+    // in Hive v0.13.0\n+    //@Test(expected = UDFArgumentException.class)\n     public void testInsufficientLabelColumn() throws HiveException {\n         BinarizeLabelUDTF udtf = new BinarizeLabelUDTF();\n         ObjectInspector[] argOIs = new ObjectInspector[2];", "filename": "core/src/test/java/hivemall/ftvec/trans/TestBinarizeLabelUDTF.java"}, {"additions": 1, "raw_url": "https://github.com/apache/incubator-hivemall/raw/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/mixserv/pom.xml", "blob_url": "https://github.com/apache/incubator-hivemall/blob/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/mixserv/pom.xml", "sha": "7a4a1b85dba9ab8cb1372eef640dd1d21d0833e7", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/mixserv/pom.xml?ref=ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb", "patch": "@@ -24,7 +24,7 @@\n \t\t<dependency>\n \t\t\t<groupId>org.apache.hive</groupId>\n \t\t\t<artifactId>hive-exec</artifactId>\n-\t\t\t<version>0.11.0</version>\n+\t\t\t<version>${hive.version}</version>\n \t\t\t<scope>provided</scope>\n \t\t\t<exclusions>\n \t\t\t\t<exclusion>", "filename": "mixserv/pom.xml"}, {"additions": 1, "raw_url": "https://github.com/apache/incubator-hivemall/raw/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/nlp/pom.xml", "blob_url": "https://github.com/apache/incubator-hivemall/blob/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/nlp/pom.xml", "sha": "0402e62764f1221e1aa7408a541615e1259624e3", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/nlp/pom.xml?ref=ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb", "patch": "@@ -24,7 +24,7 @@\n \t\t<dependency>\n \t\t\t<groupId>org.apache.hive</groupId>\n \t\t\t<artifactId>hive-exec</artifactId>\n-\t\t\t<version>0.11.0</version>\n+\t\t\t<version>${hive.version}</version>\n \t\t\t<scope>provided</scope>\n \t\t\t<exclusions>\n \t\t\t\t<exclusion>", "filename": "nlp/pom.xml"}, {"additions": 25, "raw_url": "https://github.com/apache/incubator-hivemall/raw/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/nlp/src/test/java/hivemall/nlp/tokenizer/KuromojiUDFTest.java", "blob_url": "https://github.com/apache/incubator-hivemall/blob/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/nlp/src/test/java/hivemall/nlp/tokenizer/KuromojiUDFTest.java", "sha": "4bdde78b763bca15f9daa7b7e236989055e1bd12", "changes": 31, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/nlp/src/test/java/hivemall/nlp/tokenizer/KuromojiUDFTest.java?ref=ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb", "patch": "@@ -27,8 +27,8 @@\n import org.apache.hadoop.hive.ql.udf.generic.GenericUDF.DeferredObject;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\n import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;\n-import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;\n import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\n+import org.apache.hadoop.hive.serde2.typeinfo.PrimitiveTypeInfo;\n import org.apache.hadoop.io.Text;\n import org.junit.Assert;\n import org.junit.Test;\n@@ -56,8 +56,10 @@ public void testTwoArgment() throws UDFArgumentException, IOException {\n         // line\n         argOIs[0] = PrimitiveObjectInspectorFactory.javaStringObjectInspector;\n         // mode\n+        PrimitiveTypeInfo stringType = new PrimitiveTypeInfo();\n+        stringType.setTypeName(\"string\");\n         argOIs[1] = PrimitiveObjectInspectorFactory.getPrimitiveWritableConstantObjectInspector(\n-            PrimitiveCategory.STRING, null);\n+            stringType, null);\n         udf.initialize(argOIs);\n         udf.close();\n     }\n@@ -68,8 +70,10 @@ public void testExpectedMode() throws UDFArgumentException, IOException {\n         // line\n         argOIs[0] = PrimitiveObjectInspectorFactory.javaStringObjectInspector;\n         // mode\n+        PrimitiveTypeInfo stringType = new PrimitiveTypeInfo();\n+        stringType.setTypeName(\"string\");\n         argOIs[1] = PrimitiveObjectInspectorFactory.getPrimitiveWritableConstantObjectInspector(\n-            PrimitiveCategory.STRING, new Text(\"normal\"));\n+            stringType, new Text(\"normal\"));\n         udf.initialize(argOIs);\n         udf.close();\n     }\n@@ -81,8 +85,10 @@ public void testInvalidMode() throws UDFArgumentException, IOException {\n         // line\n         argOIs[0] = PrimitiveObjectInspectorFactory.javaStringObjectInspector;\n         // mode\n+        PrimitiveTypeInfo stringType = new PrimitiveTypeInfo();\n+        stringType.setTypeName(\"string\");\n         argOIs[1] = PrimitiveObjectInspectorFactory.getPrimitiveWritableConstantObjectInspector(\n-            PrimitiveCategory.STRING, new Text(\"unsupported mode\"));\n+            stringType, new Text(\"unsupported mode\"));\n         udf.initialize(argOIs);\n         udf.close();\n     }\n@@ -94,8 +100,10 @@ public void testThreeArgment() throws UDFArgumentException, IOException {\n         // line\n         argOIs[0] = PrimitiveObjectInspectorFactory.javaStringObjectInspector;\n         // mode\n+        PrimitiveTypeInfo stringType = new PrimitiveTypeInfo();\n+        stringType.setTypeName(\"string\");\n         argOIs[1] = PrimitiveObjectInspectorFactory.getPrimitiveWritableConstantObjectInspector(\n-            PrimitiveCategory.STRING, null);\n+            stringType, null);\n         // stopWords\n         argOIs[2] = ObjectInspectorFactory.getStandardConstantListObjectInspector(\n             PrimitiveObjectInspectorFactory.javaStringObjectInspector, null);\n@@ -110,8 +118,10 @@ public void testFourArgment() throws UDFArgumentException, IOException {\n         // line\n         argOIs[0] = PrimitiveObjectInspectorFactory.javaStringObjectInspector;\n         // mode\n+        PrimitiveTypeInfo stringType = new PrimitiveTypeInfo();\n+        stringType.setTypeName(\"string\");\n         argOIs[1] = PrimitiveObjectInspectorFactory.getPrimitiveWritableConstantObjectInspector(\n-            PrimitiveCategory.STRING, null);\n+            stringType, null);\n         // stopWords\n         argOIs[2] = ObjectInspectorFactory.getStandardConstantListObjectInspector(\n             PrimitiveObjectInspectorFactory.javaStringObjectInspector, null);\n@@ -135,6 +145,9 @@ public void testEvalauteOneRow() throws IOException, HiveException {\n             public Text get() throws HiveException {\n                 return new Text(\"\u30af\u30ed\u30e2\u30b8\u306eJapaneseAnalyzer\u3092\u4f7f\u3063\u3066\u307f\u308b\u3002\u30c6\u30b9\u30c8\u3002\");\n             }\n+\n+            @Override\n+            public void prepare(int arg) throws HiveException {}\n         };\n         List<Text> tokens = udf.evaluate(args);\n         Assert.assertNotNull(tokens);\n@@ -155,6 +168,9 @@ public void testEvalauteTwoRows() throws IOException, HiveException {\n             public Text get() throws HiveException {\n                 return new Text(\"\u30af\u30ed\u30e2\u30b8\u306eJapaneseAnalyzer\u3092\u4f7f\u3063\u3066\u307f\u308b\u3002\u30c6\u30b9\u30c8\u3002\");\n             }\n+\n+            @Override\n+            public void prepare(int arg) throws HiveException {}\n         };\n         List<Text> tokens = udf.evaluate(args);\n         Assert.assertNotNull(tokens);\n@@ -164,6 +180,9 @@ public Text get() throws HiveException {\n             public Text get() throws HiveException {\n                 return new Text(\"\u30af\u30ed\u30e2\u30b8\u306eJapaneseAnalyzer\u3092\u4f7f\u3063\u3066\u307f\u308b\u3002\");\n             }\n+\n+            @Override\n+            public void prepare(int arg) throws HiveException {}\n         };\n         tokens = udf.evaluate(args);\n         Assert.assertNotNull(tokens);", "filename": "nlp/src/test/java/hivemall/nlp/tokenizer/KuromojiUDFTest.java"}, {"additions": 1, "raw_url": "https://github.com/apache/incubator-hivemall/raw/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/pom.xml", "blob_url": "https://github.com/apache/incubator-hivemall/blob/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/pom.xml", "sha": "dd26b154c01d6ebaee2d249fe366e5a4efee920c", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/pom.xml?ref=ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb", "patch": "@@ -53,6 +53,7 @@\n \t\t<project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n \t\t<protobuf.version>2.5.0</protobuf.version>\n \t\t<protoc.path>${env.PROTOC_PATH}</protoc.path>\n+\t\t<hive.version>0.13.0</hive.version>\n \t\t<scala.version>2.11.8</scala.version>\n \t</properties>\n ", "filename": "pom.xml"}, {"additions": 3, "raw_url": "https://github.com/apache/incubator-hivemall/raw/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/resources/ddl/define-all-as-permanent.hive", "blob_url": "https://github.com/apache/incubator-hivemall/blob/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/resources/ddl/define-all-as-permanent.hive", "sha": "cf0952e43b9a5eb0557c3ca32bbda998204a42b2", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/resources/ddl/define-all-as-permanent.hive?ref=ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb", "patch": "@@ -288,6 +288,9 @@ CREATE FUNCTION quantitative_features as 'hivemall.ftvec.trans.QuantitativeFeatu\n DROP FUNCTION IF EXISTS binarize_label;\n CREATE FUNCTION binarize_label as 'hivemall.ftvec.trans.BinarizeLabelUDTF' USING JAR '${hivemall_jar}';\n \n+DROP FUNCTION IF EXISTS onehot_encoding;\n+CREATE FUNCTION onehot_encoding as 'hivemall.ftvec.trans.OnehotEncodingUDAF' USING JAR '${hivemall_jar}';\n+\n ------------------------------\n -- ranking helper functions --\n ------------------------------", "filename": "resources/ddl/define-all-as-permanent.hive"}, {"additions": 3, "raw_url": "https://github.com/apache/incubator-hivemall/raw/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/resources/ddl/define-all.hive", "blob_url": "https://github.com/apache/incubator-hivemall/blob/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/resources/ddl/define-all.hive", "sha": "7b99eb41f79c3aaa5a740bb83bdbd6ca623989e2", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/resources/ddl/define-all.hive?ref=ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb", "patch": "@@ -284,6 +284,9 @@ create temporary function quantitative_features as 'hivemall.ftvec.trans.Quantit\n drop temporary function binarize_label;\n create temporary function binarize_label as 'hivemall.ftvec.trans.BinarizeLabelUDTF';\n \n+drop temporary function onehot_encoding;\n+create temporary function onehot_encoding as 'hivemall.ftvec.trans.OnehotEncodingUDAF';\n+\n ------------------------------\n -- ranking helper functions --\n ------------------------------", "filename": "resources/ddl/define-all.hive"}, {"additions": 1, "raw_url": "https://github.com/apache/incubator-hivemall/raw/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/resources/ddl/define-udfs.td.hql", "blob_url": "https://github.com/apache/incubator-hivemall/blob/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/resources/ddl/define-udfs.td.hql", "sha": "a0bea4586c9ba452726d9b968ca151361bf92354", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/resources/ddl/define-udfs.td.hql?ref=ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb", "patch": "@@ -74,6 +74,7 @@ create temporary function indexed_features as 'hivemall.ftvec.trans.IndexedFeatu\n create temporary function quantified_features as 'hivemall.ftvec.trans.QuantifiedFeaturesUDTF';\n create temporary function quantitative_features as 'hivemall.ftvec.trans.QuantitativeFeaturesUDF';\n create temporary function binarize_label as 'hivemall.ftvec.trans.BinarizeLabelUDTF';\n+create temporary function onehot_encoding as 'hivemall.ftvec.trans.OnehotEncodingUDAF';\n create temporary function bpr_sampling as 'hivemall.ftvec.ranking.BprSamplingUDTF';\n create temporary function item_pairs_sampling as 'hivemall.ftvec.ranking.ItemPairsSamplingUDTF';\n create temporary function populate_not_in as 'hivemall.ftvec.ranking.PopulateNotInUDTF';", "filename": "resources/ddl/define-udfs.td.hql"}, {"additions": 1, "raw_url": "https://github.com/apache/incubator-hivemall/raw/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/spark/spark-common/pom.xml", "blob_url": "https://github.com/apache/incubator-hivemall/blob/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/spark/spark-common/pom.xml", "sha": "17424658ed438424af6687a6712f5c411c568bda", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/spark/spark-common/pom.xml?ref=ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb", "patch": "@@ -55,7 +55,7 @@\n \t\t<dependency>\n \t\t\t<groupId>org.apache.hive</groupId>\n \t\t\t<artifactId>hive-exec</artifactId>\n-\t\t\t<version>0.11.0</version>\n+\t\t\t<version>${hive.version}</version>\n \t\t\t<scope>provided</scope>\n \t\t</dependency>\n \t</dependencies>", "filename": "spark/spark-common/pom.xml"}, {"additions": 1, "raw_url": "https://github.com/apache/incubator-hivemall/raw/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/xgboost/pom.xml", "blob_url": "https://github.com/apache/incubator-hivemall/blob/ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb/xgboost/pom.xml", "sha": "da305662c2318fdd336bf5806d8d5e5351b87b84", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/incubator-hivemall/contents/xgboost/pom.xml?ref=ebdbb004b7e9fa1dadd6620a3483784d0b4edbcb", "patch": "@@ -29,7 +29,7 @@\n \t\t<dependency>\n \t\t\t<groupId>org.apache.hive</groupId>\n \t\t\t<artifactId>hive-exec</artifactId>\n-\t\t\t<version>0.11.0</version>\n+\t\t\t<version>${hive.version}</version>\n \t\t\t<scope>provided</scope>\n \t\t\t<exclusions>\n \t\t\t\t<exclusion>", "filename": "xgboost/pom.xml"}], "repo": "incubator-hivemall"}]
