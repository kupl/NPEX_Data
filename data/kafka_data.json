[{"commit": "https://github.com/apache/kafka/commit/5b6de9f2d022cf25df73067b9de77ec267b4af8c", "parent": "https://github.com/apache/kafka/commit/95581f33f3f0a3dfb8769a2aa782cc11ffe1dccd", "message": "KAFKA-8933; Fix NPE in DefaultMetadataUpdater after authentication failure (#7682)\n\nThis patch fixes an NPE in `DefaultMetadataUpdater` due to an inconsistency in event expectations. Whenever there is an authentication failure, we were treating it as a failed update even if was from a separate connection from an inflight metadata request. This patch fixes the problem by making the `MetadataUpdater` api clearer in terms of the events that are handled.\r\n\r\nReviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Rajini Sivaram <rajinisivaram@googlemail.com>", "bug_id": "kafka_1", "file": [{"additions": 7, "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/ManualMetadataUpdater.java", "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/ManualMetadataUpdater.java", "sha": "c1c1fba4a56d058ee47a84301bfcb09cf8dda1f5", "changes": 23, "status": "modified", "deletions": 16, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/ManualMetadataUpdater.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c", "patch": "@@ -18,13 +18,13 @@\n \n import org.apache.kafka.common.KafkaException;\n import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.errors.AuthenticationException;\n import org.apache.kafka.common.requests.MetadataResponse;\n import org.apache.kafka.common.requests.RequestHeader;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n \n import java.util.ArrayList;\n import java.util.List;\n+import java.util.Optional;\n \n /**\n  * A simple implementation of `MetadataUpdater` that returns the cluster nodes set via the constructor or via\n@@ -36,9 +36,6 @@\n  * This class is not thread-safe!\n  */\n public class ManualMetadataUpdater implements MetadataUpdater {\n-\n-    private static final Logger log = LoggerFactory.getLogger(ManualMetadataUpdater.class);\n-\n     private List<Node> nodes;\n \n     public ManualMetadataUpdater() {\n@@ -69,24 +66,18 @@ public long maybeUpdate(long now) {\n     }\n \n     @Override\n-    public void handleDisconnection(String destination) {\n-        // Do nothing\n-    }\n-\n-    @Override\n-    public void handleFatalException(KafkaException exception) {\n-        // We don't fail the broker on failures, but there should be sufficient information in the logs indicating the reason\n-        // for failure.\n-        log.debug(\"An error occurred in broker-to-broker communication.\", exception);\n+    public void handleServerDisconnect(long now, String nodeId, Optional<AuthenticationException> maybeAuthException) {\n+        // We don't fail the broker on failures. There should be sufficient information from\n+        // the NetworkClient logs to indicate the reason for the failure.\n     }\n \n     @Override\n-    public void handleCompletedMetadataResponse(RequestHeader requestHeader, long now, MetadataResponse response) {\n+    public void handleFailedRequest(long now, Optional<KafkaException> maybeFatalException) {\n         // Do nothing\n     }\n \n     @Override\n-    public void requestUpdate() {\n+    public void handleSuccessfulResponse(RequestHeader requestHeader, long now, MetadataResponse response) {\n         // Do nothing\n     }\n ", "filename": "clients/src/main/java/org/apache/kafka/clients/ManualMetadataUpdater.java"}, {"additions": 12, "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/Metadata.java", "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/Metadata.java", "sha": "a9e68b861fedfcad76d627f4c8cf425318beb000", "changes": 17, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/Metadata.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c", "patch": "@@ -209,10 +209,8 @@ public synchronized boolean updateRequested() {\n         }\n     }\n \n-    public synchronized void bootstrap(List<InetSocketAddress> addresses, long now) {\n+    public synchronized void bootstrap(List<InetSocketAddress> addresses) {\n         this.needUpdate = true;\n-        this.lastRefreshMs = now;\n-        this.lastSuccessfulRefreshMs = now;\n         this.updateVersion += 1;\n         this.cache = MetadataCache.bootstrap(addresses);\n     }\n@@ -419,9 +417,18 @@ private void clearRecoverableErrors() {\n      * Record an attempt to update the metadata that failed. We need to keep track of this\n      * to avoid retrying immediately.\n      */\n-    public synchronized void failedUpdate(long now, KafkaException fatalException) {\n+    public synchronized void failedUpdate(long now) {\n         this.lastRefreshMs = now;\n-        this.fatalException = fatalException;\n+    }\n+\n+    /**\n+     * Propagate a fatal error which affects the ability to fetch metadata for the cluster.\n+     * Two examples are authentication and unsupported version exceptions.\n+     *\n+     * @param exception The fatal exception\n+     */\n+    public synchronized void fatalError(KafkaException exception) {\n+        this.fatalException = exception;\n     }\n \n     /**", "filename": "clients/src/main/java/org/apache/kafka/clients/Metadata.java"}, {"additions": 15, "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/MetadataUpdater.java", "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/MetadataUpdater.java", "sha": "77f3efadce3a0f50270f7c41536b46cee6da1d18", "changes": 29, "status": "modified", "deletions": 14, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/MetadataUpdater.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c", "patch": "@@ -18,11 +18,14 @@\n \n import org.apache.kafka.common.KafkaException;\n import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.errors.AuthenticationException;\n+import org.apache.kafka.common.errors.UnsupportedVersionException;\n import org.apache.kafka.common.requests.MetadataResponse;\n import org.apache.kafka.common.requests.RequestHeader;\n \n import java.io.Closeable;\n import java.util.List;\n+import java.util.Optional;\n \n /**\n  * The interface used by `NetworkClient` to request cluster metadata info to be updated and to retrieve the cluster nodes\n@@ -46,7 +49,7 @@\n      * Starts a cluster metadata update if needed and possible. Returns the time until the metadata update (which would\n      * be 0 if an update has been started as a result of this call).\n      *\n-     * If the implementation relies on `NetworkClient` to send requests, `handleCompletedMetadataResponse` will be\n+     * If the implementation relies on `NetworkClient` to send requests, `handleSuccessfulResponse` will be\n      * invoked after the metadata response is received.\n      *\n      * The semantics of `needed` and `possible` are implementation-dependent and may take into account a number of\n@@ -55,34 +58,32 @@\n     long maybeUpdate(long now);\n \n     /**\n-     * Handle disconnections for metadata requests.\n+     * Handle a server disconnect.\n      *\n      * This provides a mechanism for the `MetadataUpdater` implementation to use the NetworkClient instance for its own\n      * requests with special handling for disconnections of such requests.\n-     * @param destination\n+     *\n+     * @param now Current time in milliseconds\n+     * @param nodeId The id of the node that disconnected\n+     * @param maybeAuthException Optional authentication error\n      */\n-    void handleDisconnection(String destination);\n+    void handleServerDisconnect(long now, String nodeId, Optional<AuthenticationException> maybeAuthException);\n \n     /**\n-     * Handle failure. Propagate the exception if awaiting metadata.\n+     * Handle a metadata request failure.\n      *\n-     * @param fatalException exception corresponding to the failure\n+     * @param now Current time in milliseconds\n+     * @param maybeFatalException Optional fatal error (e.g. {@link UnsupportedVersionException})\n      */\n-    void handleFatalException(KafkaException fatalException);\n+    void handleFailedRequest(long now, Optional<KafkaException> maybeFatalException);\n \n     /**\n      * Handle responses for metadata requests.\n      *\n      * This provides a mechanism for the `MetadataUpdater` implementation to use the NetworkClient instance for its own\n      * requests with special handling for completed receives of such requests.\n      */\n-    void handleCompletedMetadataResponse(RequestHeader requestHeader, long now, MetadataResponse metadataResponse);\n-\n-    /**\n-     * Schedules an update of the current cluster metadata info. A subsequent call to `maybeUpdate` would trigger the\n-     * start of the update if possible (see `maybeUpdate` for more information).\n-     */\n-    void requestUpdate();\n+    void handleSuccessfulResponse(RequestHeader requestHeader, long now, MetadataResponse metadataResponse);\n \n     /**\n      * Close this updater.", "filename": "clients/src/main/java/org/apache/kafka/clients/MetadataUpdater.java"}, {"additions": 54, "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/NetworkClient.java", "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/NetworkClient.java", "sha": "3431b83c9dacd8d97031ddb3c0ef3221443e883d", "changes": 112, "status": "modified", "deletions": 58, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/NetworkClient.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c", "patch": "@@ -30,8 +30,8 @@\n import org.apache.kafka.common.network.Selectable;\n import org.apache.kafka.common.network.Send;\n import org.apache.kafka.common.protocol.ApiKeys;\n-import org.apache.kafka.common.protocol.Errors;\n import org.apache.kafka.common.protocol.CommonFields;\n+import org.apache.kafka.common.protocol.Errors;\n import org.apache.kafka.common.protocol.types.Struct;\n import org.apache.kafka.common.requests.AbstractRequest;\n import org.apache.kafka.common.requests.AbstractResponse;\n@@ -51,11 +51,13 @@\n import java.net.InetSocketAddress;\n import java.nio.ByteBuffer;\n import java.util.ArrayList;\n+import java.util.Collection;\n import java.util.HashMap;\n import java.util.Iterator;\n import java.util.LinkedList;\n import java.util.List;\n import java.util.Map;\n+import java.util.Optional;\n import java.util.Random;\n import java.util.concurrent.atomic.AtomicReference;\n import java.util.stream.Collectors;\n@@ -308,24 +310,29 @@ public void disconnect(String nodeId) {\n             return;\n \n         selector.close(nodeId);\n-        List<ApiKeys> requestTypes = new ArrayList<>();\n         long now = time.milliseconds();\n-        for (InFlightRequest request : inFlightRequests.clearAll(nodeId)) {\n-            if (request.isInternalRequest) {\n-                if (request.header.apiKey() == ApiKeys.METADATA) {\n-                    metadataUpdater.handleDisconnection(request.destination);\n-                }\n-            } else {\n-                requestTypes.add(request.header.apiKey());\n-                abortedSends.add(new ClientResponse(request.header,\n-                        request.callback, request.destination, request.createdTimeMs, now,\n-                        true, null, null, null));\n-            }\n-        }\n+\n+        cancelInFlightRequests(nodeId, now, abortedSends);\n+\n         connectionStates.disconnected(nodeId, now);\n-        if (log.isDebugEnabled()) {\n-            log.debug(\"Manually disconnected from {}. Removed requests: {}.\", nodeId,\n-                Utils.join(requestTypes, \", \"));\n+\n+        if (log.isTraceEnabled()) {\n+            log.trace(\"Manually disconnected from {}. Aborted in-flight requests: {}.\", nodeId, inFlightRequests);\n+        }\n+    }\n+\n+    private void cancelInFlightRequests(String nodeId, long now, Collection<ClientResponse> responses) {\n+        Iterable<InFlightRequest> inFlightRequests = this.inFlightRequests.clearAll(nodeId);\n+        for (InFlightRequest request : inFlightRequests) {\n+            log.trace(\"Cancelled request {} {} with correlation id {} due to node {} being disconnected\",\n+                    request.header.apiKey(), request.request, request.header.correlationId(), nodeId);\n+\n+            if (!request.isInternalRequest) {\n+                if (responses != null)\n+                    responses.add(request.disconnected(now, null));\n+            } else if (request.header.apiKey() == ApiKeys.METADATA) {\n+                metadataUpdater.handleFailedRequest(now, Optional.empty());\n+            }\n         }\n     }\n \n@@ -339,9 +346,8 @@ public void disconnect(String nodeId) {\n     @Override\n     public void close(String nodeId) {\n         selector.close(nodeId);\n-        for (InFlightRequest request : inFlightRequests.clearAll(nodeId))\n-            if (request.isInternalRequest && request.header.apiKey() == ApiKeys.METADATA)\n-                metadataUpdater.handleDisconnection(request.destination);\n+        long now = time.milliseconds();\n+        cancelInFlightRequests(nodeId, now, null);\n         connectionStates.remove(nodeId);\n     }\n \n@@ -481,10 +487,11 @@ private void doSend(ClientRequest clientRequest, boolean isInternalRequest, long\n             ClientResponse clientResponse = new ClientResponse(clientRequest.makeHeader(builder.latestAllowedVersion()),\n                     clientRequest.callback(), clientRequest.destination(), now, now,\n                     false, unsupportedVersionException, null, null);\n-            abortedSends.add(clientResponse);\n \n-            if (isInternalRequest && clientRequest.apiKey() == ApiKeys.METADATA)\n-                metadataUpdater.handleFatalException(unsupportedVersionException);\n+            if (!isInternalRequest)\n+                abortedSends.add(clientResponse);\n+            else if (clientRequest.apiKey() == ApiKeys.METADATA)\n+                metadataUpdater.handleFailedRequest(now, Optional.of(unsupportedVersionException));\n         }\n     }\n \n@@ -735,7 +742,6 @@ private void processDisconnection(List<ClientResponse> responses,\n             case AUTHENTICATION_FAILED:\n                 AuthenticationException exception = disconnectState.exception();\n                 connectionStates.authenticationFailed(nodeId, now, exception);\n-                metadataUpdater.handleFatalException(exception);\n                 log.error(\"Connection to node {} ({}) failed authentication due to: {}\", nodeId,\n                     disconnectState.remoteAddress(), exception.getMessage());\n                 break;\n@@ -752,14 +758,9 @@ private void processDisconnection(List<ClientResponse> responses,\n             default:\n                 break; // Disconnections in other states are logged at debug level in Selector\n         }\n-        for (InFlightRequest request : this.inFlightRequests.clearAll(nodeId)) {\n-            log.trace(\"Cancelled request {} {} with correlation id {} due to node {} being disconnected\",\n-                    request.header.apiKey(), request.request, request.header.correlationId(), nodeId);\n-            if (!request.isInternalRequest)\n-                responses.add(request.disconnected(now, disconnectState.exception()));\n-            else if (request.header.apiKey() == ApiKeys.METADATA)\n-                metadataUpdater.handleDisconnection(request.destination);\n-        }\n+\n+        cancelInFlightRequests(nodeId, now, responses);\n+        metadataUpdater.handleServerDisconnect(now, nodeId, Optional.ofNullable(disconnectState.exception()));\n     }\n \n     /**\n@@ -777,10 +778,6 @@ private void handleTimedOutRequests(List<ClientResponse> responses, long now) {\n             log.debug(\"Disconnecting from node {} due to request timeout.\", nodeId);\n             processDisconnection(responses, nodeId, now, ChannelState.LOCAL_CLOSE);\n         }\n-\n-        // we disconnected, so we should probably refresh our metadata\n-        if (!nodeIds.isEmpty())\n-            metadataUpdater.requestUpdate();\n     }\n \n     private void handleAbortedSends(List<ClientResponse> responses) {\n@@ -844,7 +841,7 @@ private void handleCompletedReceives(List<ClientResponse> responses, long now) {\n                     parseResponse(req.header.apiKey(), responseStruct, req.header.apiVersion());\n             maybeThrottle(body, req.header.apiVersion(), req.destination, now);\n             if (req.isInternalRequest && body instanceof MetadataResponse)\n-                metadataUpdater.handleCompletedMetadataResponse(req.header, now, (MetadataResponse) body);\n+                metadataUpdater.handleSuccessfulResponse(req.header, now, (MetadataResponse) body);\n             else if (req.isInternalRequest && body instanceof ApiVersionsResponse)\n                 handleApiVersionsResponse(responses, req, now, (ApiVersionsResponse) body);\n             else\n@@ -894,9 +891,6 @@ private void handleDisconnections(List<ClientResponse> responses, long now) {\n             log.debug(\"Node {} disconnected.\", node);\n             processDisconnection(responses, node, now, entry.getValue());\n         }\n-        // we got a disconnect so we should probably refresh our metadata and see if that broker is dead\n-        if (this.selector.disconnected().size() > 0)\n-            metadataUpdater.requestUpdate();\n     }\n \n     /**\n@@ -960,10 +954,10 @@ private void initiateConnect(Node node, long now) {\n                     this.socketReceiveBuffer);\n         } catch (IOException e) {\n             log.warn(\"Error connecting to node {}\", node, e);\n-            /* attempt failed, we'll try again after the backoff */\n+            // Attempt failed, we'll try again after the backoff\n             connectionStates.disconnected(nodeConnectionId, now);\n-            /* maybe the problem is our metadata, update it */\n-            metadataUpdater.requestUpdate();\n+            // Notify metadata updater of the connection failure\n+            metadataUpdater.handleServerDisconnect(now, nodeConnectionId, Optional.empty());\n         }\n     }\n \n@@ -1001,7 +995,6 @@ public long maybeUpdate(long now) {\n             long waitForMetadataFetch = hasFetchInProgress() ? defaultRequestTimeoutMs : 0;\n \n             long metadataTimeout = Math.max(timeToNextMetadataUpdate, waitForMetadataFetch);\n-\n             if (metadataTimeout > 0) {\n                 return metadataTimeout;\n             }\n@@ -1018,31 +1011,39 @@ public long maybeUpdate(long now) {\n         }\n \n         @Override\n-        public void handleDisconnection(String destination) {\n+        public void handleServerDisconnect(long now, String destinationId, Optional<AuthenticationException> maybeFatalException) {\n             Cluster cluster = metadata.fetch();\n             // 'processDisconnection' generates warnings for misconfigured bootstrap server configuration\n             // resulting in 'Connection Refused' and misconfigured security resulting in authentication failures.\n             // The warning below handles the case where a connection to a broker was established, but was disconnected\n             // before metadata could be obtained.\n             if (cluster.isBootstrapConfigured()) {\n-                int nodeId = Integer.parseInt(destination);\n+                int nodeId = Integer.parseInt(destinationId);\n                 Node node = cluster.nodeById(nodeId);\n                 if (node != null)\n                     log.warn(\"Bootstrap broker {} disconnected\", node);\n             }\n \n-            inProgressRequestVersion = null;\n+            // If we have a disconnect while an update is due, we treat it as a failed update\n+            // so that we can backoff properly\n+            if (isUpdateDue(now))\n+                handleFailedRequest(now, Optional.empty());\n+\n+            maybeFatalException.ifPresent(metadata::fatalError);\n+\n+            // The disconnect may be the result of stale metadata, so request an update\n+            metadata.requestUpdate();\n         }\n \n         @Override\n-        public void handleFatalException(KafkaException fatalException) {\n-            if (metadata.updateRequested())\n-                metadata.failedUpdate(time.milliseconds(), fatalException);\n+        public void handleFailedRequest(long now, Optional<KafkaException> maybeFatalException) {\n+            maybeFatalException.ifPresent(metadata::fatalError);\n+            metadata.failedUpdate(now);\n             inProgressRequestVersion = null;\n         }\n \n         @Override\n-        public void handleCompletedMetadataResponse(RequestHeader requestHeader, long now, MetadataResponse response) {\n+        public void handleSuccessfulResponse(RequestHeader requestHeader, long now, MetadataResponse response) {\n             // If any partition has leader with missing listeners, log up to ten of these partitions\n             // for diagnosing broker configuration issues.\n             // This could be a transient issue if listeners were added dynamically to brokers.\n@@ -1066,19 +1067,14 @@ public void handleCompletedMetadataResponse(RequestHeader requestHeader, long no\n             // created which means we will get errors and no nodes until it exists\n             if (response.brokers().isEmpty()) {\n                 log.trace(\"Ignoring empty metadata response with correlation id {}.\", requestHeader.correlationId());\n-                this.metadata.failedUpdate(now, null);\n+                this.metadata.failedUpdate(now);\n             } else {\n                 this.metadata.update(inProgressRequestVersion, response, now);\n             }\n \n             inProgressRequestVersion = null;\n         }\n \n-        @Override\n-        public void requestUpdate() {\n-            this.metadata.requestUpdate();\n-        }\n-\n         @Override\n         public void close() {\n             this.metadata.close();\n@@ -1104,10 +1100,10 @@ private long maybeUpdate(long now, Node node) {\n \n             if (canSendRequest(nodeConnectionId, now)) {\n                 Metadata.MetadataRequestAndVersion requestAndVersion = metadata.newMetadataRequestAndVersion();\n-                this.inProgressRequestVersion = requestAndVersion.requestVersion;\n                 MetadataRequest.Builder metadataRequest = requestAndVersion.requestBuilder;\n                 log.debug(\"Sending metadata request {} to node {}\", metadataRequest, node);\n                 sendInternalMetadataRequest(metadataRequest, nodeConnectionId, now);\n+                this.inProgressRequestVersion = requestAndVersion.requestVersion;\n                 return defaultRequestTimeoutMs;\n             }\n ", "filename": "clients/src/main/java/org/apache/kafka/clients/NetworkClient.java"}, {"additions": 7, "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminMetadataManager.java", "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminMetadataManager.java", "sha": "6e834520c46f7b0dab82d8fe0a055f03dbd4db14", "changes": 17, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminMetadataManager.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c", "patch": "@@ -29,6 +29,7 @@\n \n import java.util.Collections;\n import java.util.List;\n+import java.util.Optional;\n \n /**\n  * Manages the metadata for KafkaAdminClient.\n@@ -100,23 +101,19 @@ public long maybeUpdate(long now) {\n         }\n \n         @Override\n-        public void handleDisconnection(String destination) {\n-            // Do nothing\n-        }\n-\n-        @Override\n-        public void handleFatalException(KafkaException e) {\n-            updateFailed(e);\n+        public void handleServerDisconnect(long now, String destinationId, Optional<AuthenticationException> maybeFatalException) {\n+            maybeFatalException.ifPresent(AdminMetadataManager.this::updateFailed);\n+            AdminMetadataManager.this.requestUpdate();\n         }\n \n         @Override\n-        public void handleCompletedMetadataResponse(RequestHeader requestHeader, long now, MetadataResponse metadataResponse) {\n+        public void handleFailedRequest(long now, Optional<KafkaException> maybeFatalException) {\n             // Do nothing\n         }\n \n         @Override\n-        public void requestUpdate() {\n-            AdminMetadataManager.this.requestUpdate();\n+        public void handleSuccessfulResponse(RequestHeader requestHeader, long now, MetadataResponse metadataResponse) {\n+            // Do nothing\n         }\n \n         @Override", "filename": "clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminMetadataManager.java"}, {"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java", "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java", "sha": "b5a1047f0efd3e3caaf1d673c4684a3bd56ebf81", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c", "patch": "@@ -734,7 +734,7 @@ else if (enableAutoCommit)\n                     subscriptions, logContext, clusterResourceListeners);\n             List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(\n                     config.getList(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG), config.getString(ConsumerConfig.CLIENT_DNS_LOOKUP_CONFIG));\n-            this.metadata.bootstrap(addresses, time.milliseconds());\n+            this.metadata.bootstrap(addresses);\n             String metricGrpPrefix = \"consumer\";\n \n             FetcherMetricsRegistry metricsRegistry = new FetcherMetricsRegistry(Collections.singleton(CLIENT_ID_METRIC_TAG), metricGrpPrefix);", "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java"}, {"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java", "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java", "sha": "dd3a94acbe40f0e13a901e0acc83600387deccce", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c", "patch": "@@ -415,7 +415,7 @@ public KafkaProducer(Properties properties, Serializer<K> keySerializer, Seriali\n                         logContext,\n                         clusterResourceListeners,\n                         Time.SYSTEM);\n-                this.metadata.bootstrap(addresses, time.milliseconds());\n+                this.metadata.bootstrap(addresses);\n             }\n             this.errors = this.metrics.sensor(\"errors\");\n             this.sender = newSender(logContext, kafkaClient, this.metadata);", "filename": "clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java"}, {"additions": 3, "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerMetadata.java", "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerMetadata.java", "sha": "fea086a3a93023049eaae855cf1d12631fe11b0f", "changes": 7, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerMetadata.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c", "patch": "@@ -107,10 +107,9 @@ public synchronized void update(int requestVersion, MetadataResponse response, l\n     }\n \n     @Override\n-    public synchronized void failedUpdate(long now, KafkaException fatalException) {\n-        super.failedUpdate(now, fatalException);\n-        if (fatalException != null)\n-            notifyAll();\n+    public synchronized void fatalError(KafkaException fatalException) {\n+        super.fatalError(fatalException);\n+        notifyAll();\n     }\n \n     /**", "filename": "clients/src/main/java/org/apache/kafka/clients/producer/internals/ProducerMetadata.java"}, {"additions": 15, "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/MetadataTest.java", "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/MetadataTest.java", "sha": "4517f76664274f13c6600b343aad1969be308273", "changes": 19, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/MetadataTest.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c", "patch": "@@ -105,6 +105,18 @@ private static void checkTimeToNextUpdate(long refreshBackoffMs, long metadataEx\n         assertEquals(0, metadata.timeToNextUpdate(now + 1));\n     }\n \n+    @Test\n+    public void testUpdateMetadataAllowedImmediatelyAfterBootstrap() {\n+        MockTime time = new MockTime();\n+\n+        Metadata metadata = new Metadata(refreshBackoffMs, metadataExpireMs, new LogContext(),\n+                new ClusterResourceListeners());\n+        metadata.bootstrap(Collections.singletonList(new InetSocketAddress(\"localhost\", 9002)));\n+\n+        assertEquals(0, metadata.timeToAllowUpdate(time.milliseconds()));\n+        assertEquals(0, metadata.timeToNextUpdate(time.milliseconds()));\n+    }\n+\n     @Test\n     public void testTimeToNextUpdate() {\n         checkTimeToNextUpdate(100, 1000);\n@@ -119,7 +131,7 @@ public void testTimeToNextUpdate_RetryBackoff() {\n         long now = 10000;\n \n         // lastRefreshMs updated to now.\n-        metadata.failedUpdate(now, null);\n+        metadata.failedUpdate(now);\n \n         // Backing off. Remaining time until next try should be returned.\n         assertEquals(refreshBackoffMs, metadata.timeToNextUpdate(now));\n@@ -141,7 +153,7 @@ public void testFailedUpdate() {\n         metadata.update(emptyMetadataResponse(), time);\n \n         assertEquals(100, metadata.timeToNextUpdate(1000));\n-        metadata.failedUpdate(1100, null);\n+        metadata.failedUpdate(1100);\n \n         assertEquals(100, metadata.timeToNextUpdate(1100));\n         assertEquals(100, metadata.lastSuccessfulUpdate());\n@@ -152,14 +164,13 @@ public void testFailedUpdate() {\n \n     @Test\n     public void testClusterListenerGetsNotifiedOfUpdate() {\n-        long time = 0;\n         MockClusterResourceListener mockClusterListener = new MockClusterResourceListener();\n         ClusterResourceListeners listeners = new ClusterResourceListeners();\n         listeners.maybeAdd(mockClusterListener);\n         metadata = new Metadata(refreshBackoffMs, metadataExpireMs, new LogContext(), listeners);\n \n         String hostName = \"www.example.com\";\n-        metadata.bootstrap(Collections.singletonList(new InetSocketAddress(hostName, 9002)), time);\n+        metadata.bootstrap(Collections.singletonList(new InetSocketAddress(hostName, 9002)));\n         assertFalse(\"ClusterResourceListener should not called when metadata is updated with bootstrap Cluster\",\n                 MockClusterResourceListener.IS_ON_UPDATE_CALLED.get());\n ", "filename": "clients/src/test/java/org/apache/kafka/clients/MetadataTest.java"}, {"additions": 72, "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java", "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java", "sha": "a4145d1bc0c15a6ae22cfcf0ca99650a82fb8f4b", "changes": 77, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c", "patch": "@@ -16,10 +16,13 @@\n  */\n package org.apache.kafka.clients;\n \n+import org.apache.kafka.common.Cluster;\n import org.apache.kafka.common.KafkaException;\n import org.apache.kafka.common.Node;\n import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.errors.AuthenticationException;\n import org.apache.kafka.common.errors.UnsupportedVersionException;\n+import org.apache.kafka.common.internals.ClusterResourceListeners;\n import org.apache.kafka.common.message.ApiVersionsResponseData;\n import org.apache.kafka.common.message.ApiVersionsResponseData.ApiVersionsResponseKey;\n import org.apache.kafka.common.message.ApiVersionsResponseData.ApiVersionsResponseKeyCollection;\n@@ -31,6 +34,7 @@\n import org.apache.kafka.common.record.MemoryRecords;\n import org.apache.kafka.common.requests.ApiVersionsResponse;\n import org.apache.kafka.common.requests.MetadataRequest;\n+import org.apache.kafka.common.requests.MetadataResponse;\n import org.apache.kafka.common.requests.ProduceRequest;\n import org.apache.kafka.common.requests.RequestHeader;\n import org.apache.kafka.common.requests.ResponseHeader;\n@@ -48,11 +52,13 @@\n import java.util.Arrays;\n import java.util.Collections;\n import java.util.List;\n+import java.util.Optional;\n \n import static org.apache.kafka.common.protocol.ApiKeys.PRODUCE;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n \n@@ -83,6 +89,12 @@ private NetworkClient createNetworkClientWithStaticNodes() {\n                 ClientDnsLookup.DEFAULT, time, true, new ApiVersions(), new LogContext());\n     }\n \n+    private NetworkClient createNetworkClientWithNoVersionDiscovery(Metadata metadata) {\n+        return new NetworkClient(selector, metadata, \"mock\", Integer.MAX_VALUE,\n+                reconnectBackoffMsTest, 0, 64 * 1024, 64 * 1024,\n+                defaultRequestTimeoutMs, ClientDnsLookup.DEFAULT, time, false, new ApiVersions(), new LogContext());\n+    }\n+\n     private NetworkClient createNetworkClientWithNoVersionDiscovery() {\n         return new NetworkClient(selector, metadataUpdater, \"mock\", Integer.MAX_VALUE,\n                 reconnectBackoffMsTest, reconnectBackoffMaxMsTest,\n@@ -532,16 +544,19 @@ public void testThrottlingNotEnabledForConnectionToOlderBroker() {\n     }\n \n     private int sendEmptyProduceRequest() {\n+        return sendEmptyProduceRequest(node.idString());\n+    }\n+\n+    private int sendEmptyProduceRequest(String nodeId) {\n         ProduceRequest.Builder builder = ProduceRequest.Builder.forCurrentMagic((short) 1, 1000,\n                 Collections.emptyMap());\n         TestCallbackHandler handler = new TestCallbackHandler();\n-        ClientRequest request = client.newClientRequest(node.idString(), builder, time.milliseconds(), true,\n+        ClientRequest request = client.newClientRequest(nodeId, builder, time.milliseconds(), true,\n                 defaultRequestTimeoutMs, handler);\n         client.send(request, time.milliseconds());\n         return request.correlationId();\n     }\n \n-\n     private void sendResponse(ResponseHeader respHeader, Struct response) {\n         Struct responseHeaderStruct = respHeader.toStruct();\n         int size = responseHeaderStruct.sizeOf() + response.sizeOf();\n@@ -587,6 +602,49 @@ public void testLeastLoadedNode() {\n         assertNull(\"There should be NO leastloadednode\", leastNode);\n     }\n \n+    @Test\n+    public void testAuthenticationFailureWithInFlightMetadataRequest() {\n+        int refreshBackoffMs = 50;\n+\n+        MetadataResponse metadataResponse = TestUtils.metadataUpdateWith(2, Collections.emptyMap());\n+        Metadata metadata = new Metadata(refreshBackoffMs, 5000, new LogContext(), new ClusterResourceListeners());\n+        metadata.update(metadataResponse, time.milliseconds());\n+\n+        Cluster cluster = metadata.fetch();\n+        Node node1 = cluster.nodes().get(0);\n+        Node node2 = cluster.nodes().get(1);\n+\n+        NetworkClient client = createNetworkClientWithNoVersionDiscovery(metadata);\n+\n+        awaitReady(client, node1);\n+\n+        metadata.requestUpdate();\n+        time.sleep(refreshBackoffMs);\n+\n+        client.poll(0, time.milliseconds());\n+\n+        Optional<Node> nodeWithPendingMetadataOpt = cluster.nodes().stream()\n+                .filter(node -> client.hasInFlightRequests(node.idString()))\n+                .findFirst();\n+        assertEquals(Optional.of(node1), nodeWithPendingMetadataOpt);\n+\n+        assertFalse(client.ready(node2, time.milliseconds()));\n+        selector.serverAuthenticationFailed(node2.idString());\n+        client.poll(0, time.milliseconds());\n+        assertNotNull(client.authenticationException(node2));\n+\n+        ByteBuffer requestBuffer = selector.completedSendBuffers().get(0).buffer();\n+        RequestHeader header = parseHeader(requestBuffer);\n+        assertEquals(ApiKeys.METADATA, header.apiKey());\n+\n+        ByteBuffer responseBuffer = metadataResponse.serialize(ApiKeys.METADATA, header.apiVersion(), header.correlationId());\n+        selector.delayedReceive(new DelayedReceive(node1.idString(), new NetworkReceive(node1.idString(), responseBuffer)));\n+\n+        int initialUpdateVersion = metadata.updateVersion();\n+        client.poll(0, time.milliseconds());\n+        assertEquals(initialUpdateVersion + 1, metadata.updateVersion());\n+    }\n+\n     @Test\n     public void testLeastLoadedNodeConsidersThrottledConnections() {\n         client.ready(node, time.milliseconds());\n@@ -840,9 +898,18 @@ public TestMetadataUpdater(List<Node> nodes) {\n         }\n \n         @Override\n-        public void handleFatalException(KafkaException exception) {\n-            failure = exception;\n-            super.handleFatalException(exception);\n+        public void handleServerDisconnect(long now, String destinationId, Optional<AuthenticationException> maybeAuthException) {\n+            maybeAuthException.ifPresent(exception -> {\n+                failure = exception;\n+            });\n+            super.handleServerDisconnect(now, destinationId, maybeAuthException);\n+        }\n+\n+        @Override\n+        public void handleFailedRequest(long now, Optional<KafkaException> maybeFatalException) {\n+            maybeFatalException.ifPresent(exception -> {\n+                failure = exception;\n+            });\n         }\n \n         public KafkaException getAndClearFailure() {", "filename": "clients/src/test/java/org/apache/kafka/clients/NetworkClientTest.java"}, {"additions": 2, "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClientTest.java", "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClientTest.java", "sha": "c50bce45dcfe0f1d8ee7c79f807fe66dfdf4e446", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClientTest.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c", "patch": "@@ -235,7 +235,7 @@ public void run() {\n \n     @Test\n     public void testAuthenticationExceptionPropagatedFromMetadata() {\n-        metadata.failedUpdate(time.milliseconds(), new AuthenticationException(\"Authentication failed\"));\n+        metadata.fatalError(new AuthenticationException(\"Authentication failed\"));\n         try {\n             consumerClient.poll(time.timer(Duration.ZERO));\n             fail(\"Expected authentication error thrown\");\n@@ -264,7 +264,7 @@ public void testTopicAuthorizationExceptionPropagatedFromMetadata() {\n     @Test\n     public void testMetadataFailurePropagated() {\n         KafkaException metadataException = new KafkaException();\n-        metadata.failedUpdate(time.milliseconds(), metadataException);\n+        metadata.fatalError(metadataException);\n         try {\n             consumerClient.poll(time.timer(Duration.ZERO));\n             fail(\"Expected poll to throw exception\");", "filename": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClientTest.java"}, {"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "sha": "6440c42db53c203f7769cba2b85e9f9377a2b8b4", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c", "patch": "@@ -3338,7 +3338,7 @@ private void testGetOffsetsForTimesWithError(Errors errorForP0,\n         TopicPartition t2p0 = new TopicPartition(topicName2, 0);\n         // Expect a metadata refresh.\n         metadata.bootstrap(ClientUtils.parseAndValidateAddresses(Collections.singletonList(\"1.1.1.1:1111\"),\n-                ClientDnsLookup.DEFAULT), time.milliseconds());\n+                ClientDnsLookup.DEFAULT));\n \n         Map<String, Integer> partitionNumByTopic = new HashMap<>();\n         partitionNumByTopic.put(topicName, 2);", "filename": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java"}, {"additions": 3, "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerMetadataTest.java", "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerMetadataTest.java", "sha": "51168474fa4af81f7c9789aba3815f10d33a1d0e", "changes": 8, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerMetadataTest.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c", "patch": "@@ -22,7 +22,6 @@\n import org.apache.kafka.common.internals.ClusterResourceListeners;\n import org.apache.kafka.common.requests.MetadataResponse;\n import org.apache.kafka.common.utils.LogContext;\n-import org.apache.kafka.common.utils.MockTime;\n import org.apache.kafka.common.utils.Time;\n import org.apache.kafka.test.TestUtils;\n import org.junit.After;\n@@ -37,9 +36,9 @@\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertThrows;\n import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n-import static org.junit.Assert.assertThrows;\n \n public class ProducerMetadataTest {\n \n@@ -178,9 +177,8 @@ public void testTopicExpiry() {\n     }\n \n     @Test\n-    public void testMetadataWaitAbortedOnFatalException() throws Exception {\n-        Time time = new MockTime();\n-        metadata.failedUpdate(time.milliseconds(), new AuthenticationException(\"Fatal exception from test\"));\n+    public void testMetadataWaitAbortedOnFatalException() {\n+        metadata.fatalError(new AuthenticationException(\"Fatal exception from test\"));\n         assertThrows(AuthenticationException.class, () -> metadata.awaitUpdate(0, 1000));\n     }\n ", "filename": "clients/src/test/java/org/apache/kafka/clients/producer/internals/ProducerMetadataTest.java"}, {"additions": 15, "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/common/security/authenticator/ClientAuthenticationFailureTest.java", "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/common/security/authenticator/ClientAuthenticationFailureTest.java", "sha": "0a93c6afebc9fe9a6a5c31a85ee2acbe18b27d23", "changes": 40, "status": "modified", "deletions": 25, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/security/authenticator/ClientAuthenticationFailureTest.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c", "patch": "@@ -18,12 +18,14 @@\n \n import org.apache.kafka.clients.CommonClientConfigs;\n import org.apache.kafka.clients.admin.Admin;\n-import org.apache.kafka.clients.admin.DescribeTopicsResult;\n+import org.apache.kafka.clients.admin.TopicDescription;\n import org.apache.kafka.clients.consumer.ConsumerConfig;\n import org.apache.kafka.clients.consumer.KafkaConsumer;\n import org.apache.kafka.clients.producer.KafkaProducer;\n import org.apache.kafka.clients.producer.ProducerConfig;\n import org.apache.kafka.clients.producer.ProducerRecord;\n+import org.apache.kafka.clients.producer.RecordMetadata;\n+import org.apache.kafka.common.KafkaFuture;\n import org.apache.kafka.common.config.SaslConfigs;\n import org.apache.kafka.common.config.internals.BrokerSecurityConfigs;\n import org.apache.kafka.common.errors.SaslAuthenticationException;\n@@ -35,6 +37,7 @@\n import org.apache.kafka.common.serialization.StringDeserializer;\n import org.apache.kafka.common.serialization.StringSerializer;\n import org.apache.kafka.common.utils.MockTime;\n+import org.apache.kafka.test.TestUtils;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n@@ -44,9 +47,9 @@\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.Map;\n+import java.util.concurrent.Future;\n \n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.fail;\n+import static org.junit.Assert.assertThrows;\n \n public class ClientAuthenticationFailureTest {\n     private static MockTime time = new MockTime(50);\n@@ -88,13 +91,10 @@ public void testConsumerWithInvalidCredentials() {\n         StringDeserializer deserializer = new StringDeserializer();\n \n         try (KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props, deserializer, deserializer)) {\n-            consumer.subscribe(Arrays.asList(topic));\n-            consumer.poll(Duration.ofSeconds(10));\n-            fail(\"Expected an authentication error!\");\n-        } catch (SaslAuthenticationException e) {\n-            // OK\n-        } catch (Exception e) {\n-            throw new AssertionError(\"Expected only an authentication error, but another error occurred.\", e);\n+            assertThrows(SaslAuthenticationException.class, () -> {\n+                consumer.subscribe(Collections.singleton(topic));\n+                consumer.poll(Duration.ofSeconds(10));\n+            });\n         }\n     }\n \n@@ -106,11 +106,8 @@ public void testProducerWithInvalidCredentials() {\n \n         try (KafkaProducer<String, String> producer = new KafkaProducer<>(props, serializer, serializer)) {\n             ProducerRecord<String, String> record = new ProducerRecord<>(topic, \"message\");\n-            producer.send(record).get();\n-            fail(\"Expected an authentication error!\");\n-        } catch (Exception e) {\n-            assertTrue(\"Expected SaslAuthenticationException, got \" + e.getCause().getClass(),\n-                    e.getCause() instanceof SaslAuthenticationException);\n+            Future<RecordMetadata> future = producer.send(record);\n+            TestUtils.assertFutureThrows(future, SaslAuthenticationException.class);\n         }\n     }\n \n@@ -119,12 +116,8 @@ public void testAdminClientWithInvalidCredentials() {\n         Map<String, Object> props = new HashMap<>(saslClientConfigs);\n         props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:\" + server.port());\n         try (Admin client = Admin.create(props)) {\n-            DescribeTopicsResult result = client.describeTopics(Collections.singleton(\"test\"));\n-            result.all().get();\n-            fail(\"Expected an authentication error!\");\n-        } catch (Exception e) {\n-            assertTrue(\"Expected SaslAuthenticationException, got \" + e.getCause().getClass(),\n-                    e.getCause() instanceof SaslAuthenticationException);\n+            KafkaFuture<Map<String, TopicDescription>> future = client.describeTopics(Collections.singleton(\"test\")).all();\n+            TestUtils.assertFutureThrows(future, SaslAuthenticationException.class);\n         }\n     }\n \n@@ -137,10 +130,7 @@ public void testTransactionalProducerWithInvalidCredentials() {\n         StringSerializer serializer = new StringSerializer();\n \n         try (KafkaProducer<String, String> producer = new KafkaProducer<>(props, serializer, serializer)) {\n-            producer.initTransactions();\n-            fail(\"Expected an authentication error!\");\n-        } catch (SaslAuthenticationException e) {\n-            // expected exception\n+            assertThrows(SaslAuthenticationException.class, producer::initTransactions);\n         }\n     }\n ", "filename": "clients/src/test/java/org/apache/kafka/common/security/authenticator/ClientAuthenticationFailureTest.java"}, {"additions": 9, "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/test/MockSelector.java", "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/clients/src/test/java/org/apache/kafka/test/MockSelector.java", "sha": "90a83b0adc7cb656afe18982de58dd8bcbd4f18d", "changes": 20, "status": "modified", "deletions": 11, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/test/MockSelector.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c", "patch": "@@ -16,9 +16,9 @@\n  */\n package org.apache.kafka.test;\n \n+import org.apache.kafka.common.errors.AuthenticationException;\n import org.apache.kafka.common.network.ChannelState;\n import org.apache.kafka.common.network.NetworkReceive;\n-import org.apache.kafka.common.network.NetworkSend;\n import org.apache.kafka.common.network.Selectable;\n import org.apache.kafka.common.network.Send;\n import org.apache.kafka.common.requests.ByteBufferChannel;\n@@ -88,13 +88,15 @@ public void serverDisconnect(String id) {\n         close(id);\n     }\n \n+    public void serverAuthenticationFailed(String id) {\n+        ChannelState authFailed = new ChannelState(ChannelState.State.AUTHENTICATION_FAILED,\n+                new AuthenticationException(\"Authentication failed\"), null);\n+        this.disconnected.put(id, authFailed);\n+        close(id);\n+    }\n+\n     private void removeSendsForNode(String id, Collection<Send> sends) {\n-        Iterator<Send> iter = sends.iterator();\n-        while (iter.hasNext()) {\n-            Send send = iter.next();\n-            if (id.equals(send.destination()))\n-                iter.remove();\n-        }\n+        sends.removeIf(send -> id.equals(send.destination()));\n     }\n \n     public void clear() {\n@@ -153,10 +155,6 @@ private void completeDelayedReceives() {\n         return completedSends;\n     }\n \n-    public void completeSend(NetworkSend send) {\n-        this.completedSends.add(send);\n-    }\n-\n     public List<ByteBufferChannel> completedSendBuffers() {\n         return completedSendBuffers;\n     }", "filename": "clients/src/test/java/org/apache/kafka/test/MockSelector.java"}, {"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerGroupMember.java", "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerGroupMember.java", "sha": "dfe54867fa9ac77e6e7f3caef00c9bd671156d31", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerGroupMember.java?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c", "patch": "@@ -96,7 +96,7 @@ public WorkerGroupMember(DistributedConfig config,\n             List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(\n                     config.getList(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG),\n                     config.getString(CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG));\n-            this.metadata.bootstrap(addresses, time.milliseconds());\n+            this.metadata.bootstrap(addresses);\n             String metricGrpPrefix = \"connect\";\n             ChannelBuilder channelBuilder = ClientUtils.createChannelBuilder(config, time);\n             NetworkClient netClient = new NetworkClient(", "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/WorkerGroupMember.java"}, {"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/5b6de9f2d022cf25df73067b9de77ec267b4af8c/core/src/main/scala/kafka/admin/BrokerApiVersionsCommand.scala", "blob_url": "https://github.com/apache/kafka/blob/5b6de9f2d022cf25df73067b9de77ec267b4af8c/core/src/main/scala/kafka/admin/BrokerApiVersionsCommand.scala", "sha": "92cdb9ee023966bb6f98b7dcafc476aa4571b019", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/admin/BrokerApiVersionsCommand.scala?ref=5b6de9f2d022cf25df73067b9de77ec267b4af8c", "patch": "@@ -278,7 +278,7 @@ object BrokerApiVersionsCommand {\n       val brokerUrls = config.getList(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG)\n       val clientDnsLookup = config.getString(CommonClientConfigs.CLIENT_DNS_LOOKUP_CONFIG)\n       val brokerAddresses = ClientUtils.parseAndValidateAddresses(brokerUrls, clientDnsLookup)\n-      metadata.bootstrap(brokerAddresses, time.milliseconds())\n+      metadata.bootstrap(brokerAddresses)\n \n       val selector = new Selector(\n         DefaultConnectionMaxIdleMs,", "filename": "core/src/main/scala/kafka/admin/BrokerApiVersionsCommand.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/45c800ff0189bb8343f276aa1077fd01d0483860", "parent": "https://github.com/apache/kafka/commit/9aa660786e46c1efbf5605a6a69136a1dac6edb9", "message": "KAFKA-8911: Using proper WindowSerdes constructors in their implicit definitions (#7352)\n\nDetailed info is available in the ticket: https://issues.apache.org/jira/browse/KAFKA-8911\r\n\r\nBriefly, implicit defs are calling empty constructors, which exists only for reflection object creation.\r\nTherefore, while using the implicit definitons, a NPE occurs when Serde is called.\r\n\r\nReviewers: John Roesler <john@confluent.io>, Bill Bejeck <bbejeck@gmail.com>", "bug_id": "kafka_2", "file": [{"additions": 5, "raw_url": "https://github.com/apache/kafka/raw/45c800ff0189bb8343f276aa1077fd01d0483860/streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/Serdes.scala", "blob_url": "https://github.com/apache/kafka/blob/45c800ff0189bb8343f276aa1077fd01d0483860/streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/Serdes.scala", "sha": "92ff01ee953fe5b8ae48a83673455409c7f73119", "changes": 8, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/Serdes.scala?ref=45c800ff0189bb8343f276aa1077fd01d0483860", "patch": "@@ -37,9 +37,11 @@ object Serdes {\n   implicit def Integer: Serde[Int] = JSerdes.Integer().asInstanceOf[Serde[Int]]\n   implicit def JavaInteger: Serde[java.lang.Integer] = JSerdes.Integer()\n \n-  implicit def timeWindowedSerde[T]: WindowedSerdes.TimeWindowedSerde[T] = new WindowedSerdes.TimeWindowedSerde[T]()\n-  implicit def sessionWindowedSerde[T]: WindowedSerdes.SessionWindowedSerde[T] =\n-    new WindowedSerdes.SessionWindowedSerde[T]()\n+  implicit def timeWindowedSerde[T](implicit tSerde: Serde[T]): WindowedSerdes.TimeWindowedSerde[T] =\n+    new WindowedSerdes.TimeWindowedSerde[T](tSerde)\n+\n+  implicit def sessionWindowedSerde[T](implicit tSerde: Serde[T]): WindowedSerdes.SessionWindowedSerde[T] =\n+    new WindowedSerdes.SessionWindowedSerde[T](tSerde)\n \n   def fromFn[T >: Null](serializer: T => Array[Byte], deserializer: Array[Byte] => Option[T]): Serde[T] =\n     JSerdes.serdeFrom(", "filename": "streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/Serdes.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/87d493f07219a215816783557a5981a74f192f06", "parent": "https://github.com/apache/kafka/commit/529a5a502e5fc4d05536d23df0bd75973affff01", "message": "KAFKA-8446: Kafka Streams restoration crashes with NPE when the record value is null (#6842)\n\nWhen the restored record value is null, we are in danger of NPE during restoration phase.\r\n\r\nReviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>, Matthias J. Sax <matthias@confluent.io>", "bug_id": "kafka_3", "file": [{"additions": 6, "raw_url": "https://github.com/apache/kafka/raw/87d493f07219a215816783557a5981a74f192f06/streams/src/main/java/org/apache/kafka/streams/state/internals/RecordConverters.java", "blob_url": "https://github.com/apache/kafka/blob/87d493f07219a215816783557a5981a74f192f06/streams/src/main/java/org/apache/kafka/streams/state/internals/RecordConverters.java", "sha": "8305d5216afc386d5d4f16854c23fee5250c4831", "changes": 11, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/RecordConverters.java?ref=87d493f07219a215816783557a5981a74f192f06", "patch": "@@ -27,6 +27,11 @@\n     private static final RecordConverter RAW_TO_TIMESTAMED_INSTANCE = record -> {\n         final byte[] rawValue = record.value();\n         final long timestamp = record.timestamp();\n+        final byte[] recordValue = rawValue == null ? null :\n+            ByteBuffer.allocate(8 + rawValue.length)\n+                .putLong(timestamp)\n+                .put(rawValue)\n+                .array();\n         return new ConsumerRecord<>(\n             record.topic(),\n             record.partition(),\n@@ -37,11 +42,7 @@\n             record.serializedKeySize(),\n             record.serializedValueSize(),\n             record.key(),\n-            ByteBuffer\n-                .allocate(8 + rawValue.length)\n-                .putLong(timestamp)\n-                .put(rawValue)\n-                .array(),\n+            recordValue,\n             record.headers(),\n             record.leaderEpoch()\n         );", "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/RecordConverters.java"}, {"additions": 122, "raw_url": "https://github.com/apache/kafka/raw/87d493f07219a215816783557a5981a74f192f06/streams/src/test/java/org/apache/kafka/streams/integration/StateRestorationIntegrationTest.java", "blob_url": "https://github.com/apache/kafka/blob/87d493f07219a215816783557a5981a74f192f06/streams/src/test/java/org/apache/kafka/streams/integration/StateRestorationIntegrationTest.java", "sha": "e22ff4f76597d1f410ab75a312f92f7c315877d6", "changes": 122, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/integration/StateRestorationIntegrationTest.java?ref=87d493f07219a215816783557a5981a74f192f06", "patch": "@@ -0,0 +1,122 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.integration;\n+\n+import kafka.utils.MockTime;\n+import org.apache.kafka.common.serialization.BytesDeserializer;\n+import org.apache.kafka.common.serialization.BytesSerializer;\n+import org.apache.kafka.common.serialization.IntegerDeserializer;\n+import org.apache.kafka.common.serialization.IntegerSerializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.utils.Bytes;\n+import org.apache.kafka.streams.KafkaStreams;\n+import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.StreamsBuilder;\n+import org.apache.kafka.streams.integration.utils.EmbeddedKafkaCluster;\n+import org.apache.kafka.streams.integration.utils.IntegrationTestUtils;\n+import org.apache.kafka.streams.kstream.Materialized;\n+import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.test.IntegrationTest;\n+import org.apache.kafka.test.StreamsTestUtils;\n+import org.apache.kafka.test.TestUtils;\n+import org.junit.Before;\n+import org.junit.ClassRule;\n+import org.junit.Test;\n+import org.junit.experimental.categories.Category;\n+\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Properties;\n+import java.util.concurrent.ExecutionException;\n+\n+@Category({IntegrationTest.class})\n+public class StateRestorationIntegrationTest {\n+    private StreamsBuilder builder = new StreamsBuilder();\n+\n+    private static final String APPLICATION_ID = \"restoration-test-app\";\n+    private static final String STATE_STORE_NAME = \"stateStore\";\n+    private static final String INPUT_TOPIC = \"input\";\n+    private static final String OUTPUT_TOPIC = \"output\";\n+\n+    private Properties streamsConfiguration;\n+\n+    @ClassRule\n+    public static final EmbeddedKafkaCluster CLUSTER = new EmbeddedKafkaCluster(1);\n+    private final MockTime mockTime = CLUSTER.time;\n+\n+    @Before\n+    public void setUp() throws Exception {\n+        final Properties props = new Properties();\n+\n+        streamsConfiguration = StreamsTestUtils.getStreamsConfig(\n+                APPLICATION_ID,\n+                CLUSTER.bootstrapServers(),\n+                Serdes.Integer().getClass().getName(),\n+                Serdes.ByteArray().getClass().getName(),\n+                props);\n+\n+        CLUSTER.createTopics(INPUT_TOPIC);\n+        CLUSTER.createTopics(OUTPUT_TOPIC);\n+\n+        IntegrationTestUtils.purgeLocalStreamsState(streamsConfiguration);\n+    }\n+\n+    @Test\n+    public void shouldRestoreNullRecord() throws InterruptedException, ExecutionException {\n+        builder.table(INPUT_TOPIC, Materialized.<Integer, Bytes>as(\n+                Stores.persistentTimestampedKeyValueStore(STATE_STORE_NAME))\n+                .withKeySerde(Serdes.Integer())\n+                .withValueSerde(Serdes.Bytes())\n+                .withCachingDisabled()).toStream().to(OUTPUT_TOPIC);\n+\n+        final Properties producerConfig = TestUtils.producerConfig(\n+                CLUSTER.bootstrapServers(), IntegerSerializer.class, BytesSerializer.class);\n+\n+        final List<KeyValue<Integer, Bytes>> initialKeyValues = Arrays.asList(\n+                KeyValue.pair(3, new Bytes(new byte[]{3})),\n+                KeyValue.pair(3, null),\n+                KeyValue.pair(1, new Bytes(new byte[]{1})));\n+\n+        IntegrationTestUtils.produceKeyValuesSynchronously(\n+                INPUT_TOPIC, initialKeyValues, producerConfig, mockTime);\n+\n+        KafkaStreams streams = new KafkaStreams(builder.build(streamsConfiguration), streamsConfiguration);\n+        streams.start();\n+\n+        final Properties consumerConfig = TestUtils.consumerConfig(\n+                CLUSTER.bootstrapServers(), IntegerDeserializer.class, BytesDeserializer.class);\n+\n+        IntegrationTestUtils.waitUntilFinalKeyValueRecordsReceived(\n+                consumerConfig, OUTPUT_TOPIC, initialKeyValues);\n+\n+        // wipe out state store to trigger restore process on restart\n+        streams.close();\n+        streams.cleanUp();\n+\n+        // Restart the stream instance. There should not be exception handling the null value within changelog topic.\n+        final List<KeyValue<Integer, Bytes>> newKeyValues =\n+                Collections.singletonList(KeyValue.pair(2, new Bytes(new byte[3])));\n+        IntegrationTestUtils.produceKeyValuesSynchronously(\n+                INPUT_TOPIC, newKeyValues, producerConfig, mockTime);\n+        streams = new KafkaStreams(builder.build(streamsConfiguration), streamsConfiguration);\n+        streams.start();\n+        IntegrationTestUtils.waitUntilFinalKeyValueRecordsReceived(\n+                consumerConfig, OUTPUT_TOPIC, newKeyValues);\n+        streams.close();\n+    }\n+}", "filename": "streams/src/test/java/org/apache/kafka/streams/integration/StateRestorationIntegrationTest.java"}, {"additions": 49, "raw_url": "https://github.com/apache/kafka/raw/87d493f07219a215816783557a5981a74f192f06/streams/src/test/java/org/apache/kafka/streams/state/internals/RecordConvertersTest.java", "blob_url": "https://github.com/apache/kafka/blob/87d493f07219a215816783557a5981a74f192f06/streams/src/test/java/org/apache/kafka/streams/state/internals/RecordConvertersTest.java", "sha": "bacbacd2fed01a49fd2f3b9f140872121edecc95", "changes": 49, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/RecordConvertersTest.java?ref=87d493f07219a215816783557a5981a74f192f06", "patch": "@@ -0,0 +1,49 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.state.internals;\n+\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.record.TimestampType;\n+import org.junit.Test;\n+\n+import java.nio.ByteBuffer;\n+\n+import static org.apache.kafka.streams.state.internals.RecordConverters.rawValueToTimestampedValue;\n+import static org.junit.Assert.assertArrayEquals;\n+import static org.junit.Assert.assertNull;\n+\n+public class RecordConvertersTest {\n+\n+    private final RecordConverter timestampedValueConverter = rawValueToTimestampedValue();\n+\n+    @Test\n+    public void shouldPreserveNullValueOnConversion() {\n+        final ConsumerRecord<byte[], byte[]> nullValueRecord = new ConsumerRecord<>(\"\", 0, 0L, new byte[0], null);\n+        assertNull(timestampedValueConverter.convert(nullValueRecord).value());\n+    }\n+\n+    @Test\n+    public void shouldAddTimestampToValueOnConversionWhenValueIsNotNull() {\n+        final long timestamp = 10L;\n+        final byte[] value = new byte[1];\n+        final ConsumerRecord<byte[], byte[]> inputRecord = new ConsumerRecord<>(\n+                \"topic\", 1, 0, timestamp, TimestampType.CREATE_TIME, 0L, 0, 0, new byte[0], value);\n+        final byte[] expectedValue = ByteBuffer.allocate(9).putLong(timestamp).put(value).array();\n+        final byte[] actualValue = timestampedValueConverter.convert(inputRecord).value();\n+        assertArrayEquals(expectedValue, actualValue);\n+    }\n+}", "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/RecordConvertersTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/40432e31f7c5b0e0992a51621dc8b1c0e3d3103f", "parent": "https://github.com/apache/kafka/commit/d18d6b033e09515adff19225f8ec6845ca34c23b", "message": "MONIR: Check for NULL in case of version probing (#7275)\n\nIn case of version probing we would skip the logic for setting cluster / assigned tasks; since these values are initialized as null they are vulnerable to NPE when code changes.\r\n\r\nReviewers: A. Sophie Blee-Goldman <sophie@confluent.io>, Bill Bejeck <bill@confluent.io>", "bug_id": "kafka_4", "file": [{"additions": 2, "raw_url": "https://github.com/apache/kafka/raw/40432e31f7c5b0e0992a51621dc8b1c0e3d3103f/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "blob_url": "https://github.com/apache/kafka/blob/40432e31f7c5b0e0992a51621dc8b1c0e3d3103f/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java", "sha": "a7fa1fa96706c2486777dc1f8f0463b80d643b8b", "changes": 5, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java?ref=40432e31f7c5b0e0992a51621dc8b1c0e3d3103f", "patch": "@@ -112,7 +112,7 @@ void createTasks(final Collection<TopicPartition> assignment) {\n     }\n \n     private void addStreamTasks(final Collection<TopicPartition> assignment) {\n-        if (assignedActiveTasks.isEmpty()) {\n+        if (assignedActiveTasks == null || assignedActiveTasks.isEmpty()) {\n             return;\n         }\n         final Map<TaskId, Set<TopicPartition>> newTasks = new HashMap<>();\n@@ -151,8 +151,7 @@ private void addStreamTasks(final Collection<TopicPartition> assignment) {\n     }\n \n     private void addStandbyTasks() {\n-        final Map<TaskId, Set<TopicPartition>> assignedStandbyTasks = this.assignedStandbyTasks;\n-        if (assignedStandbyTasks.isEmpty()) {\n+        if (assignedStandbyTasks == null || assignedStandbyTasks.isEmpty()) {\n             return;\n         }\n         log.debug(\"Adding assigned standby tasks {}\", assignedStandbyTasks);", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/TaskManager.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/3e48bdbc333602a042b6b0fb7fb9e14625ab4ece", "parent": "https://github.com/apache/kafka/commit/528e5c0f57aa5014cd13baef2b683a1a328f459a", "message": "KAFKA-8620: fix NPE due to race condition during shutdown while rebalancing (#7021)\n\nReviewers: Matthias J. Sax <matthias@confluent.io>, Bruno Cadonna <bruno@confluent.io>, John Roesler <john@confluent.io>", "bug_id": "kafka_5", "file": [{"additions": 27, "raw_url": "https://github.com/apache/kafka/raw/3e48bdbc333602a042b6b0fb7fb9e14625ab4ece/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "blob_url": "https://github.com/apache/kafka/blob/3e48bdbc333602a042b6b0fb7fb9e14625ab4ece/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "sha": "ea1e29d0bcba6bf84e5717ef769a64e621516b8b", "changes": 34, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java?ref=3e48bdbc333602a042b6b0fb7fb9e14625ab4ece", "patch": "@@ -190,14 +190,20 @@ State setState(final State newState) {\n             oldState = state;\n \n             if (state == State.PENDING_SHUTDOWN && newState != State.DEAD) {\n+                log.debug(\"Ignoring request to transit from PENDING_SHUTDOWN to {}: \" +\n+                              \"only DEAD state is a valid next state\", newState);\n                 // when the state is already in PENDING_SHUTDOWN, all other transitions will be\n                 // refused but we do not throw exception here\n                 return null;\n             } else if (state == State.DEAD) {\n+                log.debug(\"Ignoring request to transit from DEAD to {}: \" +\n+                              \"no valid next state after DEAD\", newState);\n                 // when the state is already in NOT_RUNNING, all its transitions\n                 // will be refused but we do not throw exception here\n                 return null;\n             } else if (state == State.PARTITIONS_REVOKED && newState == State.PARTITIONS_REVOKED) {\n+                log.debug(\"Ignoring request to transit from PARTITIONS_REVOKED to PARTITIONS_REVOKED: \" +\n+                              \"self transition is not allowed\");\n                 // when the state is already in PARTITIONS_REVOKED, its transition to itself will be\n                 // refused but we do not throw exception here\n                 return null;\n@@ -268,17 +274,23 @@ public void onPartitionsAssigned(final Collection<TopicPartition> assignment) {\n             final long start = time.milliseconds();\n             try {\n                 if (streamThread.setState(State.PARTITIONS_ASSIGNED) == null) {\n-                    return;\n-                }\n-                if (streamThread.assignmentErrorCode.get() == StreamsPartitionAssignor.Error.NONE.code()) {\n+                    log.debug(\n+                        \"Skipping task creation in rebalance because we are already in {} state.\",\n+                        streamThread.state()\n+                    );\n+                } else if (streamThread.assignmentErrorCode.get() != StreamsPartitionAssignor.Error.NONE.code()) {\n+                    log.debug(\n+                        \"Encountered assignment error during partition assignment: {}. Skipping task initialization\",\n+                        streamThread.assignmentErrorCode\n+                    );\n+                } else {\n+                    log.debug(\"Creating tasks based on assignment.\");\n                     taskManager.createTasks(assignment);\n                 }\n             } catch (final Throwable t) {\n                 log.error(\n                     \"Error caught during partition assignment, \" +\n-                        \"will abort the current process and re-throw at the end of rebalance: {}\",\n-                    t\n-                );\n+                        \"will abort the current process and re-throw at the end of rebalance\", t);\n                 streamThread.setRebalanceException(t);\n             } finally {\n                 log.info(\"partition assignment took {} ms.\\n\" +\n@@ -809,7 +821,6 @@ private void enforceRebalance() {\n     // Visible for testing\n     void runOnce() {\n         final ConsumerRecords<byte[], byte[]> records;\n-\n         now = time.milliseconds();\n \n         if (state == State.PARTITIONS_ASSIGNED) {\n@@ -830,6 +841,15 @@ void runOnce() {\n             throw new StreamsException(logPrefix + \"Unexpected state \" + state + \" during normal iteration\");\n         }\n \n+        // Shutdown hook could potentially be triggered and transit the thread state to PENDING_SHUTDOWN during #pollRequests().\n+        // The task manager internal states could be uninitialized if the state transition happens during #onPartitionsAssigned().\n+        // Should only proceed when the thread is still running after #pollRequests(), because no external state mutation\n+        // could affect the task manager state beyond this point within #runOnce().\n+        if (!isRunning()) {\n+            log.debug(\"State already transits to {}, skipping the run once call after poll request\", state);\n+            return;\n+        }\n+\n         final long pollLatency = advanceNowAndComputeLatency();\n \n         if (records != null && !records.isEmpty()) {", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java"}, {"additions": 81, "raw_url": "https://github.com/apache/kafka/raw/3e48bdbc333602a042b6b0fb7fb9e14625ab4ece/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java", "blob_url": "https://github.com/apache/kafka/blob/3e48bdbc333602a042b6b0fb7fb9e14625ab4ece/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java", "sha": "aff5d6c74861c16a04eec1c7c190bc4cb4f71c1f", "changes": 81, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java?ref=3e48bdbc333602a042b6b0fb7fb9e14625ab4ece", "patch": "@@ -20,6 +20,7 @@\n import org.apache.kafka.clients.consumer.Consumer;\n import org.apache.kafka.clients.consumer.ConsumerRebalanceListener;\n import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n import org.apache.kafka.clients.consumer.InvalidOffsetException;\n import org.apache.kafka.clients.consumer.MockConsumer;\n import org.apache.kafka.clients.consumer.OffsetResetStrategy;\n@@ -78,6 +79,7 @@\n import java.io.File;\n import java.time.Duration;\n import java.util.ArrayList;\n+import java.util.Collection;\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n@@ -701,6 +703,85 @@ public void shouldShutdownTaskManagerOnCloseWithoutStart() {\n         EasyMock.verify(taskManager);\n     }\n \n+    @Test\n+    public void shouldNotThrowWhenPendingShutdownInRunOnce() {\n+        mockRunOnce(true);\n+    }\n+\n+    @Test\n+    public void shouldNotThrowWithoutPendingShutdownInRunOnce() {\n+        // A reference test to verify that without intermediate shutdown the runOnce should pass\n+        // without any exception.\n+        mockRunOnce(false);\n+    }\n+\n+    private void mockRunOnce(final boolean shutdownOnPoll) {\n+        final Collection<TopicPartition> assignedPartitions = Collections.singletonList(t1p1);\n+        class MockStreamThreadConsumer<K, V> extends MockConsumer<K, V> {\n+\n+            private StreamThread streamThread;\n+\n+            private MockStreamThreadConsumer(final OffsetResetStrategy offsetResetStrategy) {\n+                super(offsetResetStrategy);\n+            }\n+\n+            @Override\n+            public synchronized ConsumerRecords<K, V> poll(final Duration timeout) {\n+                assertNotNull(streamThread);\n+                if (shutdownOnPoll) {\n+                    streamThread.shutdown();\n+                }\n+                streamThread.rebalanceListener.onPartitionsAssigned(assignedPartitions);\n+                return super.poll(timeout);\n+            }\n+\n+            private void setStreamThread(final StreamThread streamThread) {\n+                this.streamThread = streamThread;\n+            }\n+        }\n+\n+        final MockStreamThreadConsumer<byte[], byte[]> mockStreamThreadConsumer =\n+            new MockStreamThreadConsumer<>(OffsetResetStrategy.EARLIEST);\n+\n+        final TaskManager taskManager = new TaskManager(new MockChangelogReader(),\n+                                                        processId,\n+                                                        \"log-prefix\",\n+                                                        mockStreamThreadConsumer,\n+                                                        streamsMetadataState,\n+                                                        null,\n+                                                        null,\n+                                                        null,\n+                                                        new AssignedStreamsTasks(new LogContext()),\n+                                                        new AssignedStandbyTasks(new LogContext()));\n+        taskManager.setConsumer(mockStreamThreadConsumer);\n+        taskManager.setAssignmentMetadata(Collections.emptyMap(), Collections.emptyMap());\n+\n+        final StreamsMetricsImpl streamsMetrics = new StreamsMetricsImpl(metrics, clientId);\n+        final StreamThread thread = new StreamThread(\n+            mockTime,\n+            config,\n+            null,\n+            mockStreamThreadConsumer,\n+            mockStreamThreadConsumer,\n+            null,\n+            taskManager,\n+            streamsMetrics,\n+            internalTopologyBuilder,\n+            clientId,\n+            new LogContext(\"\"),\n+            new AtomicInteger()\n+        ).updateThreadMetadata(getSharedAdminClientId(clientId));\n+\n+        mockStreamThreadConsumer.setStreamThread(thread);\n+        mockStreamThreadConsumer.assign(assignedPartitions);\n+        mockStreamThreadConsumer.updateBeginningOffsets(Collections.singletonMap(t1p1, 0L));\n+\n+        addRecord(mockStreamThreadConsumer, 1L, 0L);\n+        thread.setState(StreamThread.State.STARTING);\n+        thread.setState(StreamThread.State.PARTITIONS_REVOKED);\n+        thread.runOnce();\n+    }\n+\n     @Test\n     public void shouldOnlyShutdownOnce() {\n         final Consumer<byte[], byte[]> consumer = EasyMock.createNiceMock(Consumer.class);", "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamThreadTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/635c21311f6bf0405e743dcf5004ef6ea36737e0", "parent": "https://github.com/apache/kafka/commit/a7e771c6da72bb7f3c5c5cbab3dc9c4fd403f866", "message": "KAFKA-8564; Fix NPE on deleted partition dir when no segments remain (#6968)\n\nKafka should not NPE while loading a deleted partition dir with no log segments. This patch ensures that there will always be at least one segment after initialization.\r\n\r\nCo-authored-by: Edoardo Comar <ecomar@uk.ibm.com>\r\nCo-authored-by: Mickael Maison <mickael.maison@gmail.com>\r\n\r\nReviewers: Ismael Juma <ismael@juma.me.uk>, Jason Gustafson <jason@confluent.io>", "bug_id": "kafka_6", "file": [{"additions": 9, "raw_url": "https://github.com/apache/kafka/raw/635c21311f6bf0405e743dcf5004ef6ea36737e0/core/src/main/scala/kafka/log/Log.scala", "blob_url": "https://github.com/apache/kafka/blob/635c21311f6bf0405e743dcf5004ef6ea36737e0/core/src/main/scala/kafka/log/Log.scala", "sha": "7ad43b4c92c6ba25b564180d6f184264ba563acd", "changes": 9, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/log/Log.scala?ref=635c21311f6bf0405e743dcf5004ef6ea36737e0", "patch": "@@ -633,6 +633,15 @@ class Log(@volatile var dir: File,\n       activeSegment.resizeIndexes(config.maxIndexSize)\n       nextOffset\n     } else {\n+       if (logSegments.isEmpty) {\n+          addSegment(LogSegment.open(dir = dir,\n+            baseOffset = 0,\n+            config,\n+            time = time,\n+            fileAlreadyExists = false,\n+            initFileSize = this.initFileSize,\n+            preallocate = false))\n+       }\n       0\n     }\n   }", "filename": "core/src/main/scala/kafka/log/Log.scala"}, {"additions": 11, "raw_url": "https://github.com/apache/kafka/raw/635c21311f6bf0405e743dcf5004ef6ea36737e0/core/src/test/scala/unit/kafka/log/LogTest.scala", "blob_url": "https://github.com/apache/kafka/blob/635c21311f6bf0405e743dcf5004ef6ea36737e0/core/src/test/scala/unit/kafka/log/LogTest.scala", "sha": "9cd53e529ba93db479d38d2fd8b03242166f677c", "changes": 12, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/log/LogTest.scala?ref=635c21311f6bf0405e743dcf5004ef6ea36737e0", "patch": "@@ -3741,7 +3741,17 @@ class LogTest {\n     assertEquals(new AbortedTransaction(pid, 0), fetchDataInfo.abortedTransactions.get.head)\n   }\n \n- private def allAbortedTransactions(log: Log) = log.logSegments.flatMap(_.txnIndex.allAbortedTxns)\n+  @Test\n+  def testLoadPartitionDirWithNoSegmentsShouldNotThrow() {\n+    val dirName = Log.logDeleteDirName(new TopicPartition(\"foo\", 3))\n+    val logDir = new File(tmpDir, dirName)\n+    logDir.mkdirs()\n+    val logConfig = LogTest.createLogConfig()\n+    val log = createLog(logDir, logConfig)\n+    assertEquals(1, log.numberOfSegments)\n+  }\n+\n+  private def allAbortedTransactions(log: Log) = log.logSegments.flatMap(_.txnIndex.allAbortedTxns)\n \n   private def appendTransactionalAsLeader(log: Log, producerId: Long, producerEpoch: Short): Int => Unit = {\n     var sequence = 0", "filename": "core/src/test/scala/unit/kafka/log/LogTest.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/7334222a71d522f2e99ee84622c672a4c5636e1f", "parent": "https://github.com/apache/kafka/commit/d7f8ec8628ee18326d22f6a58c787629a8faf307", "message": "KAFKA-8412: Fix nullpointer exception thrown on flushing before closing producers (#7207)\n\nPrior to this change an NPE is raised when calling AssignedTasks.close\r\nunder the following conditions:\r\n\r\n1. EOS is enabled\r\n2. The task was in a suspended state\r\n\r\nThe cause for the NPE is that when a clean close is requested for a\r\nStreamTask the StreamTask tries to commit. However, in the suspended\r\nstate there is no producer so ultimately an NPE is thrown for the\r\ncontained RecordCollector in flush.\r\n\r\nThe fix put forth in this commit is to have AssignedTasks call\r\ncloseSuspended when it knows the underlying StreamTask is suspended.\r\n\r\nNote also that this test is quite involved. I could have just tested\r\nthat AssignedTasks calls closeSuspended when appropriate, but that is\r\ntesting, IMO, a detail of the implementation and doesn't actually verify\r\nwe reproduced the original problem as it was described. I feel much more\r\nconfident that we are reproducing the behavior - and we can test exactly\r\nthe conditions that lead to it - when testing across AssignedTasks and\r\nStreamTask. I believe this is an additional support for the argument of\r\neventually consolidating the state split across classes.\r\n\r\nReviewers: Matthias J. Sax <matthias@confluent.io>, Guozhang Wang <wangguoz@gmail.com>", "bug_id": "kafka_7", "file": [{"additions": 2, "raw_url": "https://github.com/apache/kafka/raw/7334222a71d522f2e99ee84622c672a4c5636e1f/checkstyle/import-control.xml", "blob_url": "https://github.com/apache/kafka/blob/7334222a71d522f2e99ee84622c672a4c5636e1f/checkstyle/import-control.xml", "sha": "063c1ba089dc78ea8a12a6b1402eb8794daada5b", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/checkstyle/import-control.xml?ref=7334222a71d522f2e99ee84622c672a4c5636e1f", "patch": "@@ -265,8 +265,10 @@\n     <subpackage name=\"processor\">\n       <subpackage name=\"internals\">\n         <allow pkg=\"com.fasterxml.jackson\" />\n+        <allow pkg=\"kafka.utils\" />\n         <allow pkg=\"org.apache.zookeeper\" />\n         <allow pkg=\"org.apache.zookeeper\" />\n+        <allow pkg=\"org.apache.log4j\" />\n         <subpackage name=\"testutil\">\n           <allow pkg=\"org.apache.log4j\" />\n         </subpackage>", "filename": "checkstyle/import-control.xml"}, {"additions": 11, "raw_url": "https://github.com/apache/kafka/raw/7334222a71d522f2e99ee84622c672a4c5636e1f/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java", "blob_url": "https://github.com/apache/kafka/blob/7334222a71d522f2e99ee84622c672a4c5636e1f/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java", "sha": "85d8307e7beed28d77c8430a313d75989405e9e9", "changes": 17, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java?ref=7334222a71d522f2e99ee84622c672a4c5636e1f", "patch": "@@ -331,18 +331,23 @@ void closeNonAssignedSuspendedTasks(final Map<TaskId, Set<TopicPartition>> newAs\n \n     void close(final boolean clean) {\n         final AtomicReference<RuntimeException> firstException = new AtomicReference<>(null);\n-        for (final T task : allTasks()) {\n+\n+        for (final T task: allTasks()) {\n             try {\n-                task.close(clean, false);\n+                if (suspended.containsKey(task.id())) {\n+                    task.closeSuspended(clean, false, null);\n+                } else {\n+                    task.close(clean, false);\n+                }\n             } catch (final TaskMigratedException e) {\n                 log.info(\"Failed to close {} {} since it got migrated to another thread already. \" +\n-                        \"Closing it as zombie and move on.\", taskTypeName, task.id());\n+                    \"Closing it as zombie and move on.\", taskTypeName, task.id());\n                 firstException.compareAndSet(null, closeZombieTask(task));\n             } catch (final RuntimeException t) {\n                 log.error(\"Failed while closing {} {} due to the following error:\",\n-                          task.getClass().getSimpleName(),\n-                          task.id(),\n-                          t);\n+                    task.getClass().getSimpleName(),\n+                    task.id(),\n+                    t);\n                 if (clean) {\n                     if (!closeUnclean(task)) {\n                         firstException.compareAndSet(null, t);", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/AssignedTasks.java"}, {"additions": 117, "raw_url": "https://github.com/apache/kafka/raw/7334222a71d522f2e99ee84622c672a4c5636e1f/streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasksTest.java", "blob_url": "https://github.com/apache/kafka/blob/7334222a71d522f2e99ee84622c672a4c5636e1f/streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasksTest.java", "sha": "ca51a3b1b17c79f4c413d50a8c8fc26fc89f612a", "changes": 129, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasksTest.java?ref=7334222a71d522f2e99ee84622c672a4c5636e1f", "patch": "@@ -17,27 +17,42 @@\n \n package org.apache.kafka.streams.processor.internals;\n \n+import static org.hamcrest.CoreMatchers.not;\n+import static org.hamcrest.CoreMatchers.nullValue;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.hamcrest.core.IsEqual.equalTo;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertSame;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Set;\n+import kafka.utils.LogCaptureAppender;\n+import org.apache.kafka.clients.consumer.MockConsumer;\n+import org.apache.kafka.clients.consumer.OffsetResetStrategy;\n+import org.apache.kafka.clients.producer.MockProducer;\n import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.metrics.MetricConfig;\n+import org.apache.kafka.common.metrics.Metrics;\n+import org.apache.kafka.common.metrics.Sensor.RecordingLevel;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.common.serialization.Serializer;\n import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.MockTime;\n import org.apache.kafka.common.utils.Utils;\n import org.apache.kafka.streams.errors.TaskMigratedException;\n import org.apache.kafka.streams.processor.TaskId;\n+import org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl;\n+import org.apache.kafka.test.MockSourceNode;\n+import org.apache.log4j.Level;\n+import org.apache.log4j.spi.LoggingEvent;\n import org.easymock.EasyMock;\n import org.junit.Before;\n import org.junit.Test;\n \n-import java.util.Collection;\n-import java.util.Collections;\n-import java.util.Set;\n-\n-import static org.hamcrest.CoreMatchers.not;\n-import static org.hamcrest.CoreMatchers.nullValue;\n-import static org.hamcrest.MatcherAssert.assertThat;\n-import static org.hamcrest.core.IsEqual.equalTo;\n-import static org.junit.Assert.assertSame;\n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.fail;\n-\n public class AssignedStreamsTasksTest {\n \n     private final StreamTask t1 = EasyMock.createMock(StreamTask.class);\n@@ -451,6 +466,96 @@ public void shouldReturnNumberOfPunctuations() {\n         EasyMock.verify(t1);\n     }\n \n+    @Test\n+    public void shouldCloseCleanlyWithSuspendedTaskAndEOS() {\n+        final String topic = \"topic\";\n+\n+        final Deserializer<byte[]> deserializer = Serdes.ByteArray().deserializer();\n+        final Serializer<byte[]> serializer = Serdes.ByteArray().serializer();\n+\n+        final MockConsumer<byte[], byte[]> consumer =\n+            new MockConsumer<>(OffsetResetStrategy.EARLIEST);\n+        final MockProducer<byte[], byte[]> producer =\n+            new MockProducer<>(false, serializer, serializer);\n+\n+        final MockSourceNode<byte[], byte[]> source = new MockSourceNode<>(\n+            new String[] {\"topic\"},\n+            deserializer,\n+            deserializer);\n+\n+        final ChangelogReader changelogReader = new MockChangelogReader();\n+\n+        final ProcessorTopology topology = new ProcessorTopology(\n+            Collections.singletonList(source),\n+            Collections.singletonMap(topic, source),\n+            Collections.emptyMap(),\n+            Collections.emptyList(),\n+            Collections.emptyList(),\n+            Collections.emptyMap(),\n+            Collections.emptySet());\n+\n+        final Set<TopicPartition> partitions = Collections.singleton(\n+            new TopicPartition(topic, 1));\n+\n+        final Metrics metrics = new Metrics(new MetricConfig().recordLevel(RecordingLevel.DEBUG));\n+\n+        final StreamsMetricsImpl streamsMetrics = new MockStreamsMetrics(metrics);\n+\n+        final MockTime time = new MockTime();\n+\n+        final StateDirectory stateDirectory = new StateDirectory(\n+            StreamTaskTest.createConfig(true),\n+            time,\n+            true);\n+\n+        final StreamTask task = new StreamTask(\n+            new TaskId(0, 0),\n+            partitions,\n+            topology,\n+            consumer,\n+            changelogReader,\n+            StreamTaskTest.createConfig(true),\n+            streamsMetrics,\n+            stateDirectory,\n+            null,\n+            time,\n+            () -> producer);\n+\n+        assignedTasks.addNewTask(task);\n+        assignedTasks.initializeNewTasks();\n+        assertNull(assignedTasks.suspend());\n+\n+        // We have to test for close failure by looking at the logs because the current close\n+        // logic suppresses the raised exception in AssignedTasks.close. It's not clear if this\n+        // is the intended behavior.\n+        //\n+        // Also note that capturing the failure through this side effect is very brittle.\n+        final LogCaptureAppender appender = LogCaptureAppender.createAndRegister();\n+        final Level previousLevel =\n+            LogCaptureAppender.setClassLoggerLevel(AssignedStreamsTasks.class, Level.ERROR);\n+        try {\n+            assignedTasks.close(true);\n+        } finally {\n+            LogCaptureAppender.setClassLoggerLevel(AssignedStreamsTasks.class, previousLevel);\n+            LogCaptureAppender.unregister(appender);\n+        }\n+        if (!appender.getMessages().isEmpty()) {\n+            final LoggingEvent firstError = appender.getMessages().head();\n+            final String firstErrorCause =\n+                firstError.getThrowableStrRep() != null\n+                    ? String.join(\"\\n\", firstError.getThrowableStrRep())\n+                    : \"N/A\";\n+\n+            final String failMsg =\n+                String.format(\"Expected no ERROR message while closing assignedTasks, but got %d. \" +\n+                    \"First error: %s. Cause: %s\",\n+                    appender.getMessages().size(),\n+                    firstError.getMessage(),\n+                    firstErrorCause);\n+            fail(failMsg);\n+        }\n+    }\n+\n     private void addAndInitTask() {\n         assignedTasks.addNewTask(t1);\n         assignedTasks.initializeNewTasks();", "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/AssignedStreamsTasksTest.java"}, {"additions": 6, "raw_url": "https://github.com/apache/kafka/raw/7334222a71d522f2e99ee84622c672a4c5636e1f/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java", "blob_url": "https://github.com/apache/kafka/blob/7334222a71d522f2e99ee84622c672a4c5636e1f/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java", "sha": "fb196fdcda31960f6b3c244c161eaa85f676199b", "changes": 10, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java?ref=7334222a71d522f2e99ee84622c672a4c5636e1f", "patch": "@@ -89,6 +89,8 @@\n \n public class StreamTaskTest {\n \n+    private static final File BASE_DIR = TestUtils.tempDirectory();\n+\n     private final Serializer<Integer> intSerializer = Serdes.Integer().serializer();\n     private final Serializer<byte[]> bytesSerializer = Serdes.ByteArray().serializer();\n     private final Deserializer<Integer> intDeserializer = Serdes.Integer().deserializer();\n@@ -140,7 +142,6 @@ public void close() {\n     private final StreamsMetricsImpl streamsMetrics = new MockStreamsMetrics(metrics);\n     private final TaskId taskId00 = new TaskId(0, 0);\n     private final MockTime time = new MockTime();\n-    private final File baseDir = TestUtils.tempDirectory();\n     private StateDirectory stateDirectory;\n     private StreamTask task;\n     private long punctuatedAt;\n@@ -175,10 +176,11 @@ static ProcessorTopology withSources(final List<ProcessorNode> processorNodes,\n                                      Collections.emptySet());\n     }\n \n-    private StreamsConfig createConfig(final boolean enableEoS) {\n+    // Exposed to make it easier to create StreamTask config from other tests.\n+    static StreamsConfig createConfig(final boolean enableEoS) {\n         final String canonicalPath;\n         try {\n-            canonicalPath = baseDir.getCanonicalPath();\n+            canonicalPath = BASE_DIR.getCanonicalPath();\n         } catch (final IOException e) {\n             throw new RuntimeException(e);\n         }\n@@ -210,7 +212,7 @@ public void cleanup() throws IOException {\n                 }\n             }\n         } finally {\n-            Utils.delete(baseDir);\n+            Utils.delete(BASE_DIR);\n         }\n     }\n ", "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/5ecac84cd8b5cb7be701e0cbcc53b37f41b50d60", "parent": "https://github.com/apache/kafka/commit/46a02f3231cd6d340c622636159b9f59b4b3cb6e", "message": "MINOR: Fix `toString` NPE in tableProcessorNode (#6807)\n\nThis gets hit when debug logging is enabled.\r\n\r\nReviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <wangguoz@gmail.com>", "bug_id": "kafka_8", "file": [{"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/5ecac84cd8b5cb7be701e0cbcc53b37f41b50d60/streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNode.java", "blob_url": "https://github.com/apache/kafka/blob/5ecac84cd8b5cb7be701e0cbcc53b37f41b50d60/streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNode.java", "sha": "6fc5e25862b93acdc548971187d4c485762c742a", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNode.java?ref=5ecac84cd8b5cb7be701e0cbcc53b37f41b50d60", "patch": "@@ -49,7 +49,7 @@ public TableProcessorNode(final String nodeName,\n     public String toString() {\n         return \"TableProcessorNode{\" +\n             \", processorParameters=\" + processorParameters +\n-            \", storeBuilder=\" + storeBuilder.name() +\n+            \", storeBuilder=\" + (storeBuilder == null ? \"null\" : storeBuilder.name()) +\n             \", storeNames=\" + Arrays.toString(storeNames) +\n             \"} \" + super.toString();\n     }", "filename": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNode.java"}, {"additions": 60, "raw_url": "https://github.com/apache/kafka/raw/5ecac84cd8b5cb7be701e0cbcc53b37f41b50d60/streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNodeTest.java", "blob_url": "https://github.com/apache/kafka/blob/5ecac84cd8b5cb7be701e0cbcc53b37f41b50d60/streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNodeTest.java", "sha": "a2c4938336376e4be2bc3eb7a464dd136cd93745", "changes": 60, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNodeTest.java?ref=5ecac84cd8b5cb7be701e0cbcc53b37f41b50d60", "patch": "@@ -0,0 +1,60 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.kafka.streams.kstream.internals.graph;\n+\n+import org.apache.kafka.streams.processor.AbstractProcessor;\n+import org.apache.kafka.streams.processor.ProcessorContext;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+public class TableProcessorNodeTest {\n+    private static class TestProcessor extends AbstractProcessor<String, String> {\n+        @Override\n+        public void init(final ProcessorContext context) {\n+        }\n+\n+        @Override\n+        public void process(final String key, final String value) {\n+        }\n+\n+        @Override\n+        public void close() {\n+        }\n+    }\n+\n+    @Test\n+    public void shouldConvertToStringWithNullStoreBuilder() {\n+        final TableProcessorNode<String, String> node = new TableProcessorNode<>(\n+            \"name\",\n+            new ProcessorParameters<>(TestProcessor::new, \"processor\"),\n+            null,\n+            new String[]{\"store1\", \"store2\"}\n+        );\n+\n+        final String asString = node.toString();\n+        final String expected = \"storeBuilder=null\";\n+        assertTrue(\n+            String.format(\n+                \"Expected toString to return string with \\\"%s\\\", received: %s\",\n+                expected,\n+                asString),\n+            asString.contains(expected)\n+        );\n+    }\n+}\n\\ No newline at end of file", "filename": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/graph/TableProcessorNodeTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/6d3ff132b57835fc879d678e9addc5e7c3804205", "parent": "https://github.com/apache/kafka/commit/3ba4686d4d650f0f9155b2e22dddb192a5a56a6c", "message": "KAFKA-8240: Fix NPE in Source.equals() (#6589)\n\nReviewers: John Roesler <john@confluent.io>, Bruno Cadonna <bruno@confluent.io>, Bill Bejeck <bbejeck@gmail.com>", "bug_id": "kafka_9", "file": [{"additions": 22, "raw_url": "https://github.com/apache/kafka/raw/6d3ff132b57835fc879d678e9addc5e7c3804205/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java", "blob_url": "https://github.com/apache/kafka/blob/6d3ff132b57835fc879d678e9addc5e7c3804205/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java", "sha": "2d527e51f6b650be09ee8ac78598c57b1555c4e0", "changes": 25, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java?ref=6d3ff132b57835fc879d678e9addc5e7c3804205", "patch": "@@ -282,7 +282,7 @@ private boolean isMatch(final String topic) {\n \n         @Override\n         Source describe() {\n-            return new Source(name, new HashSet<>(topics), pattern);\n+            return new Source(name, topics.size() == 0 ? null : new HashSet<>(topics), pattern);\n         }\n     }\n \n@@ -1281,6 +1281,9 @@ private boolean nodeGroupContainsGlobalSourceNode(final Set<String> allNodesOfGr\n         @Override\n         public int compare(final TopologyDescription.Node node1,\n                            final TopologyDescription.Node node2) {\n+            if (node1.equals(node2)) {\n+                return 0;\n+            }\n             final int size1 = ((AbstractNode) node1).size;\n             final int size2 = ((AbstractNode) node2).size;\n \n@@ -1399,6 +1402,7 @@ public int hashCode() {\n         int size;\n \n         AbstractNode(final String name) {\n+            Objects.requireNonNull(name, \"name cannot be null\");\n             this.name = name;\n             this.size = 1;\n         }\n@@ -1435,6 +1439,13 @@ public Source(final String name,\n                       final Set<String> topics,\n                       final Pattern pattern) {\n             super(name);\n+            if (topics == null && pattern == null) {\n+                throw new IllegalArgumentException(\"Either topics or pattern must be not-null, but both are null.\");\n+            }\n+            if (topics != null && pattern != null) {\n+                throw new IllegalArgumentException(\"Either topics or pattern must be null, but both are not null.\");\n+            }\n+\n             this.topics = topics;\n             this.topicPattern = pattern;\n         }\n@@ -1479,8 +1490,10 @@ public boolean equals(final Object o) {\n             final Source source = (Source) o;\n             // omit successor to avoid infinite loops\n             return name.equals(source.name)\n-                && topics.equals(source.topics)\n-                && topicPattern.equals(source.topicPattern);\n+                && (topics == null && source.topics == null\n+                    || topics != null && topics.equals(source.topics))\n+                && (topicPattern == null && source.topicPattern == null\n+                    || topicPattern != null && topicPattern.pattern().equals(source.topicPattern.pattern()));\n         }\n \n         @Override\n@@ -1709,6 +1722,9 @@ public String toString() {\n         @Override\n         public int compare(final TopologyDescription.GlobalStore globalStore1,\n                            final TopologyDescription.GlobalStore globalStore2) {\n+            if (globalStore1.equals(globalStore2)) {\n+                return 0;\n+            }\n             return globalStore1.id() - globalStore2.id();\n         }\n     }\n@@ -1719,6 +1735,9 @@ public int compare(final TopologyDescription.GlobalStore globalStore1,\n         @Override\n         public int compare(final TopologyDescription.Subtopology subtopology1,\n                            final TopologyDescription.Subtopology subtopology2) {\n+            if (subtopology1.equals(subtopology2)) {\n+                return 0;\n+            }\n             return subtopology1.id() - subtopology2.id();\n         }\n     }", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java"}, {"additions": 99, "raw_url": "https://github.com/apache/kafka/raw/6d3ff132b57835fc879d678e9addc5e7c3804205/streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilderTest.java", "blob_url": "https://github.com/apache/kafka/blob/6d3ff132b57835fc879d678e9addc5e7c3804205/streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilderTest.java", "sha": "b86211fbcd238ef9858438295f1628b3a6edb26f", "changes": 133, "status": "modified", "deletions": 34, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilderTest.java?ref=6d3ff132b57835fc879d678e9addc5e7c3804205", "patch": "@@ -23,15 +23,13 @@\n import org.apache.kafka.streams.Topology;\n import org.apache.kafka.streams.TopologyDescription;\n import org.apache.kafka.streams.errors.TopologyException;\n-import org.apache.kafka.streams.processor.Processor;\n-import org.apache.kafka.streams.processor.ProcessorSupplier;\n import org.apache.kafka.streams.processor.StateStore;\n import org.apache.kafka.streams.processor.TopicNameExtractor;\n import org.apache.kafka.streams.state.KeyValueStore;\n import org.apache.kafka.streams.state.StoreBuilder;\n import org.apache.kafka.streams.state.Stores;\n-import org.apache.kafka.test.MockProcessorSupplier;\n import org.apache.kafka.test.MockKeyValueStoreBuilder;\n+import org.apache.kafka.test.MockProcessorSupplier;\n import org.apache.kafka.test.MockTimestampExtractor;\n import org.apache.kafka.test.StreamsTestUtils;\n import org.junit.Test;\n@@ -50,12 +48,14 @@\n import static java.time.Duration.ofSeconds;\n import static java.util.Arrays.asList;\n import static org.apache.kafka.common.utils.Utils.mkSet;\n-import static org.hamcrest.core.IsInstanceOf.instanceOf;\n+import static org.hamcrest.CoreMatchers.equalTo;\n import static org.hamcrest.MatcherAssert.assertThat;\n-\n+import static org.hamcrest.Matchers.not;\n+import static org.hamcrest.core.IsInstanceOf.instanceOf;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNotEquals;\n+import static org.junit.Assert.assertThrows;\n import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n \n@@ -353,9 +353,9 @@ public void testTopicGroups() {\n         final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = builder.topicGroups();\n \n         final Map<Integer, InternalTopologyBuilder.TopicsInfo> expectedTopicGroups = new HashMap<>();\n-        expectedTopicGroups.put(0, new InternalTopologyBuilder.TopicsInfo(Collections.<String>emptySet(), mkSet(\"topic-1\", \"X-topic-1x\", \"topic-2\"), Collections.<String, InternalTopicConfig>emptyMap(), Collections.<String, InternalTopicConfig>emptyMap()));\n-        expectedTopicGroups.put(1, new InternalTopologyBuilder.TopicsInfo(Collections.<String>emptySet(), mkSet(\"topic-3\", \"topic-4\"), Collections.<String, InternalTopicConfig>emptyMap(), Collections.<String, InternalTopicConfig>emptyMap()));\n-        expectedTopicGroups.put(2, new InternalTopologyBuilder.TopicsInfo(Collections.<String>emptySet(), mkSet(\"topic-5\"), Collections.<String, InternalTopicConfig>emptyMap(), Collections.<String, InternalTopicConfig>emptyMap()));\n+        expectedTopicGroups.put(0, new InternalTopologyBuilder.TopicsInfo(Collections.emptySet(), mkSet(\"topic-1\", \"X-topic-1x\", \"topic-2\"), Collections.emptyMap(), Collections.emptyMap()));\n+        expectedTopicGroups.put(1, new InternalTopologyBuilder.TopicsInfo(Collections.emptySet(), mkSet(\"topic-3\", \"topic-4\"), Collections.emptyMap(), Collections.emptyMap()));\n+        expectedTopicGroups.put(2, new InternalTopologyBuilder.TopicsInfo(Collections.emptySet(), mkSet(\"topic-5\"), Collections.emptyMap(), Collections.emptyMap()));\n \n         assertEquals(3, topicGroups.size());\n         assertEquals(expectedTopicGroups, topicGroups);\n@@ -393,17 +393,17 @@ public void testTopicGroupsByStateStore() {\n         final String store2 = ProcessorStateManager.storeChangelogTopic(\"X\", \"store-2\");\n         final String store3 = ProcessorStateManager.storeChangelogTopic(\"X\", \"store-3\");\n         expectedTopicGroups.put(0, new InternalTopologyBuilder.TopicsInfo(\n-            Collections.<String>emptySet(), mkSet(\"topic-1\", \"topic-1x\", \"topic-2\"),\n-            Collections.<String, InternalTopicConfig>emptyMap(),\n-            Collections.singletonMap(store1, (InternalTopicConfig) new UnwindowedChangelogTopicConfig(store1, Collections.<String, String>emptyMap()))));\n+            Collections.emptySet(), mkSet(\"topic-1\", \"topic-1x\", \"topic-2\"),\n+            Collections.emptyMap(),\n+            Collections.singletonMap(store1, new UnwindowedChangelogTopicConfig(store1, Collections.emptyMap()))));\n         expectedTopicGroups.put(1, new InternalTopologyBuilder.TopicsInfo(\n-            Collections.<String>emptySet(), mkSet(\"topic-3\", \"topic-4\"),\n-            Collections.<String, InternalTopicConfig>emptyMap(),\n-            Collections.singletonMap(store2, (InternalTopicConfig) new UnwindowedChangelogTopicConfig(store2, Collections.<String, String>emptyMap()))));\n+            Collections.emptySet(), mkSet(\"topic-3\", \"topic-4\"),\n+            Collections.emptyMap(),\n+            Collections.singletonMap(store2, new UnwindowedChangelogTopicConfig(store2, Collections.emptyMap()))));\n         expectedTopicGroups.put(2, new InternalTopologyBuilder.TopicsInfo(\n-            Collections.<String>emptySet(), mkSet(\"topic-5\"),\n-            Collections.<String, InternalTopicConfig>emptyMap(),\n-            Collections.singletonMap(store3, (InternalTopicConfig) new UnwindowedChangelogTopicConfig(store3, Collections.<String, String>emptyMap()))));\n+            Collections.emptySet(), mkSet(\"topic-5\"),\n+            Collections.emptyMap(),\n+            Collections.singletonMap(store3, new UnwindowedChangelogTopicConfig(store3, Collections.emptyMap()))));\n \n         assertEquals(3, topicGroups.size());\n         assertEquals(expectedTopicGroups, topicGroups);\n@@ -499,12 +499,7 @@ public void shouldNotAllowNullTopicChooserWhenAddingSink() {\n \n     @Test(expected = NullPointerException.class)\n     public void shouldNotAllowNullNameWhenAddingProcessor() {\n-        builder.addProcessor(null, new ProcessorSupplier() {\n-            @Override\n-            public Processor get() {\n-                return null;\n-            }\n-        });\n+        builder.addProcessor(null, () -> null);\n     }\n \n     @Test(expected = NullPointerException.class)\n@@ -604,14 +599,14 @@ public void shouldAddInternalTopicConfigForWindowStores() {\n         final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = builder.topicGroups();\n         final InternalTopologyBuilder.TopicsInfo topicsInfo = topicGroups.values().iterator().next();\n         final InternalTopicConfig topicConfig1 = topicsInfo.stateChangelogTopics.get(\"appId-store1-changelog\");\n-        final Map<String, String> properties1 = topicConfig1.getProperties(Collections.<String, String>emptyMap(), 10000);\n+        final Map<String, String> properties1 = topicConfig1.getProperties(Collections.emptyMap(), 10000);\n         assertEquals(2, properties1.size());\n         assertEquals(TopicConfig.CLEANUP_POLICY_COMPACT + \",\" + TopicConfig.CLEANUP_POLICY_DELETE, properties1.get(TopicConfig.CLEANUP_POLICY_CONFIG));\n         assertEquals(\"40000\", properties1.get(TopicConfig.RETENTION_MS_CONFIG));\n         assertEquals(\"appId-store1-changelog\", topicConfig1.name());\n         assertTrue(topicConfig1 instanceof WindowedChangelogTopicConfig);\n         final InternalTopicConfig topicConfig2 = topicsInfo.stateChangelogTopics.get(\"appId-store2-changelog\");\n-        final Map<String, String> properties2 = topicConfig2.getProperties(Collections.<String, String>emptyMap(), 10000);\n+        final Map<String, String> properties2 = topicConfig2.getProperties(Collections.emptyMap(), 10000);\n         assertEquals(2, properties2.size());\n         assertEquals(TopicConfig.CLEANUP_POLICY_COMPACT + \",\" + TopicConfig.CLEANUP_POLICY_DELETE, properties2.get(TopicConfig.CLEANUP_POLICY_CONFIG));\n         assertEquals(\"40000\", properties2.get(TopicConfig.RETENTION_MS_CONFIG));\n@@ -628,7 +623,7 @@ public void shouldAddInternalTopicConfigForNonWindowStores() {\n         final Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups = builder.topicGroups();\n         final InternalTopologyBuilder.TopicsInfo topicsInfo = topicGroups.values().iterator().next();\n         final InternalTopicConfig topicConfig = topicsInfo.stateChangelogTopics.get(\"appId-store-changelog\");\n-        final Map<String, String> properties = topicConfig.getProperties(Collections.<String, String>emptyMap(), 10000);\n+        final Map<String, String> properties = topicConfig.getProperties(Collections.emptyMap(), 10000);\n         assertEquals(1, properties.size());\n         assertEquals(TopicConfig.CLEANUP_POLICY_COMPACT, properties.get(TopicConfig.CLEANUP_POLICY_CONFIG));\n         assertEquals(\"appId-store-changelog\", topicConfig.name());\n@@ -642,7 +637,7 @@ public void shouldAddInternalTopicConfigForRepartitionTopics() {\n         builder.addSource(null, \"source\", null, null, null, \"foo\");\n         final InternalTopologyBuilder.TopicsInfo topicsInfo = builder.topicGroups().values().iterator().next();\n         final InternalTopicConfig topicConfig = topicsInfo.repartitionSourceTopics.get(\"appId-foo\");\n-        final Map<String, String> properties = topicConfig.getProperties(Collections.<String, String>emptyMap(), 10000);\n+        final Map<String, String> properties = topicConfig.getProperties(Collections.emptyMap(), 10000);\n         assertEquals(3, properties.size());\n         assertEquals(String.valueOf(-1), properties.get(TopicConfig.RETENTION_MS_CONFIG));\n         assertEquals(TopicConfig.CLEANUP_POLICY_DELETE, properties.get(TopicConfig.CLEANUP_POLICY_CONFIG));\n@@ -708,32 +703,32 @@ public void shouldSortProcessorNodesCorrectly() {\n \n         assertTrue(iterator.hasNext());\n         InternalTopologyBuilder.AbstractNode node = (InternalTopologyBuilder.AbstractNode) iterator.next();\n-        assertTrue(node.name.equals(\"source1\"));\n+        assertEquals(\"source1\", node.name);\n         assertEquals(6, node.size);\n \n         assertTrue(iterator.hasNext());\n         node = (InternalTopologyBuilder.AbstractNode) iterator.next();\n-        assertTrue(node.name.equals(\"source2\"));\n+        assertEquals(\"source2\", node.name);\n         assertEquals(4, node.size);\n \n         assertTrue(iterator.hasNext());\n         node = (InternalTopologyBuilder.AbstractNode) iterator.next();\n-        assertTrue(node.name.equals(\"processor2\"));\n+        assertEquals(\"processor2\", node.name);\n         assertEquals(3, node.size);\n \n         assertTrue(iterator.hasNext());\n         node = (InternalTopologyBuilder.AbstractNode) iterator.next();\n-        assertTrue(node.name.equals(\"processor1\"));\n+        assertEquals(\"processor1\", node.name);\n         assertEquals(2, node.size);\n \n         assertTrue(iterator.hasNext());\n         node = (InternalTopologyBuilder.AbstractNode) iterator.next();\n-        assertTrue(node.name.equals(\"processor3\"));\n+        assertEquals(\"processor3\", node.name);\n         assertEquals(2, node.size);\n \n         assertTrue(iterator.hasNext());\n         node = (InternalTopologyBuilder.AbstractNode) iterator.next();\n-        assertTrue(node.name.equals(\"sink1\"));\n+        assertEquals(\"sink1\", node.name);\n         assertEquals(1, node.size);\n     }\n \n@@ -760,7 +755,7 @@ public void shouldConnectRegexMatchedTopicsToStateStore() throws Exception {\n         final Map<String, List<String>> stateStoreAndTopics = builder.stateStoreNameToSourceTopics();\n         final List<String> topics = stateStoreAndTopics.get(storeBuilder.name());\n \n-        assertTrue(\"Expected to contain two topics\", topics.size() == 2);\n+        assertEquals(\"Expected to contain two topics\", 2, topics.size());\n \n         assertTrue(topics.contains(\"topic-2\"));\n         assertTrue(topics.contains(\"topic-3\"));\n@@ -781,4 +776,74 @@ public void shouldNotAllowToAddGlobalStoreWithSourceNameEqualsProcessorName() {\n             sameNameForSourceAndProcessor,\n             new MockProcessorSupplier());\n     }\n+\n+    @Test\n+    public void shouldThrowIfNameIsNull() {\n+        final Exception e = assertThrows(NullPointerException.class, () -> new InternalTopologyBuilder.Source(null, Collections.emptySet(), null));\n+        assertEquals(\"name cannot be null\", e.getMessage());\n+    }\n+\n+    @Test\n+    public void shouldThrowIfTopicAndPatternAreNull() {\n+        final Exception e = assertThrows(IllegalArgumentException.class, () -> new InternalTopologyBuilder.Source(\"name\", null, null));\n+        assertEquals(\"Either topics or pattern must be not-null, but both are null.\", e.getMessage());\n+    }\n+\n+    @Test\n+    public void shouldThrowIfBothTopicAndPatternAreNotNull() {\n+        final Exception e = assertThrows(IllegalArgumentException.class, () -> new InternalTopologyBuilder.Source(\"name\", Collections.emptySet(), Pattern.compile(\"\")));\n+        assertEquals(\"Either topics or pattern must be null, but both are not null.\", e.getMessage());\n+    }\n+\n+    @Test\n+    public void sourceShouldBeEqualIfNameAndTopicListAreTheSame() {\n+        final InternalTopologyBuilder.Source base = new InternalTopologyBuilder.Source(\"name\", Collections.singleton(\"topic\"), null);\n+        final InternalTopologyBuilder.Source sameAsBase = new InternalTopologyBuilder.Source(\"name\", Collections.singleton(\"topic\"), null);\n+\n+        assertThat(base, equalTo(sameAsBase));\n+    }\n+\n+    @Test\n+    public void sourceShouldBeEqualIfNameAndPatternAreTheSame() {\n+        final InternalTopologyBuilder.Source base = new InternalTopologyBuilder.Source(\"name\", null, Pattern.compile(\"topic\"));\n+        final InternalTopologyBuilder.Source sameAsBase = new InternalTopologyBuilder.Source(\"name\", null, Pattern.compile(\"topic\"));\n+\n+        assertThat(base, equalTo(sameAsBase));\n+    }\n+\n+    @Test\n+    public void sourceShouldNotBeEqualForDifferentNamesWithSameTopicList() {\n+        final InternalTopologyBuilder.Source base = new InternalTopologyBuilder.Source(\"name\", Collections.singleton(\"topic\"), null);\n+        final InternalTopologyBuilder.Source differentName = new InternalTopologyBuilder.Source(\"name2\", Collections.singleton(\"topic\"), null);\n+\n+        assertThat(base, not(equalTo(differentName)));\n+    }\n+\n+    @Test\n+    public void sourceShouldNotBeEqualForDifferentNamesWithSamePattern() {\n+        final InternalTopologyBuilder.Source base = new InternalTopologyBuilder.Source(\"name\", null, Pattern.compile(\"topic\"));\n+        final InternalTopologyBuilder.Source differentName = new InternalTopologyBuilder.Source(\"name2\", null, Pattern.compile(\"topic\"));\n+\n+        assertThat(base, not(equalTo(differentName)));\n+    }\n+\n+    @Test\n+    public void sourceShouldNotBeEqualForDifferentTopicList() {\n+        final InternalTopologyBuilder.Source base = new InternalTopologyBuilder.Source(\"name\", Collections.singleton(\"topic\"), null);\n+        final InternalTopologyBuilder.Source differentTopicList = new InternalTopologyBuilder.Source(\"name\", Collections.emptySet(), null);\n+        final InternalTopologyBuilder.Source differentTopic = new InternalTopologyBuilder.Source(\"name\", Collections.singleton(\"topic2\"), null);\n+\n+        assertThat(base, not(equalTo(differentTopicList)));\n+        assertThat(base, not(equalTo(differentTopic)));\n+    }\n+\n+    @Test\n+    public void sourceShouldNotBeEqualForDifferentPattern() {\n+        final InternalTopologyBuilder.Source base = new InternalTopologyBuilder.Source(\"name\", null, Pattern.compile(\"topic\"));\n+        final InternalTopologyBuilder.Source differentPattern = new InternalTopologyBuilder.Source(\"name\", null, Pattern.compile(\"topic2\"));\n+        final InternalTopologyBuilder.Source overlappingPattern = new InternalTopologyBuilder.Source(\"name\", null, Pattern.compile(\"top*\"));\n+\n+        assertThat(base, not(equalTo(differentPattern)));\n+        assertThat(base, not(equalTo(overlappingPattern)));\n+    }\n }", "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilderTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/1f692bdf53af4a80b7fd256de4e94ff1d17fc861", "parent": "https://github.com/apache/kafka/commit/3124b070666d3b8cdc0e96067e69b8e7a6245742", "message": "KAFKA-8142: Fix NPE for nulls in Headers (#6484)\n\nReviewers: Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>", "bug_id": "kafka_10", "file": [{"additions": 7, "raw_url": "https://github.com/apache/kafka/raw/1f692bdf53af4a80b7fd256de4e94ff1d17fc861/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeaders.java", "blob_url": "https://github.com/apache/kafka/blob/1f692bdf53af4a80b7fd256de4e94ff1d17fc861/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeaders.java", "sha": "5801bed99cd8c712b33c94031a2ed55ce4353791", "changes": 12, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeaders.java?ref=1f692bdf53af4a80b7fd256de4e94ff1d17fc861", "patch": "@@ -16,16 +16,17 @@\n  */\n package org.apache.kafka.common.header.internals;\n \n+import org.apache.kafka.common.header.Header;\n+import org.apache.kafka.common.header.Headers;\n+import org.apache.kafka.common.record.Record;\n+import org.apache.kafka.common.utils.AbstractIterator;\n+\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collection;\n import java.util.Iterator;\n import java.util.List;\n-\n-import org.apache.kafka.common.header.Header;\n-import org.apache.kafka.common.header.Headers;\n-import org.apache.kafka.common.record.Record;\n-import org.apache.kafka.common.utils.AbstractIterator;\n+import java.util.Objects;\n \n public class RecordHeaders implements Headers {\n \n@@ -61,6 +62,7 @@ public RecordHeaders(Iterable<Header> headers) {\n \n     @Override\n     public Headers add(Header header) throws IllegalStateException {\n+        Objects.requireNonNull(header, \"Header cannot be null.\");\n         canWrite();\n         headers.add(header);\n         return this;", "filename": "clients/src/main/java/org/apache/kafka/common/header/internals/RecordHeaders.java"}, {"additions": 13, "raw_url": "https://github.com/apache/kafka/raw/1f692bdf53af4a80b7fd256de4e94ff1d17fc861/clients/src/test/java/org/apache/kafka/common/header/internals/RecordHeadersTest.java", "blob_url": "https://github.com/apache/kafka/blob/1f692bdf53af4a80b7fd256de4e94ff1d17fc861/clients/src/test/java/org/apache/kafka/common/header/internals/RecordHeadersTest.java", "sha": "5b9f95ea91f182a75551f96f9c074ced41aee3c0", "changes": 21, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/header/internals/RecordHeadersTest.java?ref=1f692bdf53af4a80b7fd256de4e94ff1d17fc861", "patch": "@@ -16,19 +16,19 @@\n  */\n package org.apache.kafka.common.header.internals;\n \n-import static org.junit.Assert.assertEquals;\n-import static org.junit.Assert.assertFalse;\n-import static org.junit.Assert.assertNull;\n-import static org.junit.Assert.assertTrue;\n-import static org.junit.Assert.fail;\n+import org.apache.kafka.common.header.Header;\n+import org.apache.kafka.common.header.Headers;\n+import org.junit.Test;\n \n import java.io.IOException;\n import java.util.Arrays;\n import java.util.Iterator;\n \n-import org.apache.kafka.common.header.Header;\n-import org.apache.kafka.common.header.Headers;\n-import org.junit.Test;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n \n public class RecordHeadersTest {\n \n@@ -206,6 +206,11 @@ public void testNew() throws IOException {\n         assertEquals(2, getCount(newHeaders));\n     }\n \n+    @Test(expected = NullPointerException.class)\n+    public void shouldThrowNpeWhenAddingNullHeader() {\n+        new RecordHeaders().add(null);\n+    }\n+\n     private int getCount(Headers headers) {\n         int count = 0;\n         Iterator<Header> headerIterator = headers.iterator();", "filename": "clients/src/test/java/org/apache/kafka/common/header/internals/RecordHeadersTest.java"}, {"additions": 4, "raw_url": "https://github.com/apache/kafka/raw/1f692bdf53af4a80b7fd256de4e94ff1d17fc861/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContext.java", "blob_url": "https://github.com/apache/kafka/blob/1f692bdf53af4a80b7fd256de4e94ff1d17fc861/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContext.java", "sha": "00012746bd02dddcc5c07929912d3a3192dc147b", "changes": 5, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContext.java?ref=1f692bdf53af4a80b7fd256de4e94ff1d17fc861", "patch": "@@ -90,7 +90,10 @@ public long sizeBytes() {\n         if (headers != null) {\n             for (final Header header : headers) {\n                 size += header.key().toCharArray().length;\n-                size += header.value().length;\n+                final byte[] value = header.value();\n+                if (value != null) {\n+                    size += value.length;\n+                }\n             }\n         }\n         return size;", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContext.java"}, {"additions": 98, "raw_url": "https://github.com/apache/kafka/raw/1f692bdf53af4a80b7fd256de4e94ff1d17fc861/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContextTest.java", "blob_url": "https://github.com/apache/kafka/blob/1f692bdf53af4a80b7fd256de4e94ff1d17fc861/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContextTest.java", "sha": "1ea646fce2f3e8860d844c6a3939a4b6958f5b21", "changes": 98, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContextTest.java?ref=1f692bdf53af4a80b7fd256de4e94ff1d17fc861", "patch": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.kafka.streams.processor.internals;\n+\n+import org.apache.kafka.common.header.Headers;\n+import org.apache.kafka.common.header.internals.RecordHeaders;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class ProcessorRecordContextTest {\n+    // timestamp + offset + partition: 8 + 8 + 4\n+    private final static long MIN_SIZE = 20L;\n+\n+    @Test\n+    public void shouldEstimateNullTopicAndNullHeadersAsZeroLength() {\n+        final Headers headers = new RecordHeaders();\n+        final ProcessorRecordContext context = new ProcessorRecordContext(\n+            42L,\n+            73L,\n+            0,\n+            null,\n+            null\n+        );\n+\n+        assertEquals(MIN_SIZE, context.sizeBytes());\n+    }\n+\n+    @Test\n+    public void shouldEstimateEmptyHeaderAsZeroLength() {\n+        final ProcessorRecordContext context = new ProcessorRecordContext(\n+            42L,\n+            73L,\n+            0,\n+            null,\n+            new RecordHeaders()\n+        );\n+\n+        assertEquals(MIN_SIZE, context.sizeBytes());\n+    }\n+\n+    @Test\n+    public void shouldEstimateTopicLength() {\n+        final ProcessorRecordContext context = new ProcessorRecordContext(\n+            42L,\n+            73L,\n+            0,\n+            \"topic\",\n+            null\n+        );\n+\n+        assertEquals(MIN_SIZE + 5L, context.sizeBytes());\n+    }\n+\n+    @Test\n+    public void shouldEstimateHeadersLength() {\n+        final Headers headers = new RecordHeaders();\n+        headers.add(\"header-key\", \"header-value\".getBytes());\n+        final ProcessorRecordContext context = new ProcessorRecordContext(\n+            42L,\n+            73L,\n+            0,\n+            null,\n+            headers\n+        );\n+\n+        assertEquals(MIN_SIZE + 10L + 12L, context.sizeBytes());\n+    }\n+\n+    @Test\n+    public void shouldEstimateNullValueInHeaderAsZero() {\n+        final Headers headers = new RecordHeaders();\n+        headers.add(\"header-key\", null);\n+        final ProcessorRecordContext context = new ProcessorRecordContext(\n+            42L,\n+            73L,\n+            0,\n+            null,\n+            headers\n+        );\n+\n+        assertEquals(MIN_SIZE + 10L, context.sizeBytes());\n+    }\n+}", "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorRecordContextTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/fc4fea6761986749f0ac640868d9b4d2a552eb62", "parent": "https://github.com/apache/kafka/commit/8cabd44e1d35d3cf3f865d8a4eee8c04228e0d75", "message": "KAFKA-6605: Fix NPE in Flatten when optional Struct is null (#5705)\n\nCorrect the Flatten SMT to properly handle null key or value `Struct` instances.\r\n\r\nAuthor: Michal Borowiecki <michal.borowiecki@openbet.com>\r\nReviewers: Arjun Satish <arjun@confluent.io>, Robert Yokota <rayokota@gmail.com>, Randall Hauch <rhauch@gmail.com>", "bug_id": "kafka_11", "file": [{"additions": 18, "raw_url": "https://github.com/apache/kafka/raw/fc4fea6761986749f0ac640868d9b4d2a552eb62/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Flatten.java", "blob_url": "https://github.com/apache/kafka/blob/fc4fea6761986749f0ac640868d9b4d2a552eb62/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Flatten.java", "sha": "d7d21445d73d818b3ebd56c8d3b7e2691b5a55e6", "changes": 29, "status": "modified", "deletions": 11, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Flatten.java?ref=fc4fea6761986749f0ac640868d9b4d2a552eb62", "patch": "@@ -35,7 +35,7 @@\n import java.util.Map;\n \n import static org.apache.kafka.connect.transforms.util.Requirements.requireMap;\n-import static org.apache.kafka.connect.transforms.util.Requirements.requireStruct;\n+import static org.apache.kafka.connect.transforms.util.Requirements.requireStructOrNull;\n \n public abstract class Flatten<R extends ConnectRecord<R>> implements Transformation<R> {\n \n@@ -136,20 +136,24 @@ private void applySchemaless(Map<String, Object> originalRecord, String fieldNam\n     }\n \n     private R applyWithSchema(R record) {\n-        final Struct value = requireStruct(operatingValue(record), PURPOSE);\n+        final Struct value = requireStructOrNull(operatingValue(record), PURPOSE);\n \n-        Schema updatedSchema = schemaUpdateCache.get(value.schema());\n+        Schema schema = operatingSchema(record);\n+        Schema updatedSchema = schemaUpdateCache.get(schema);\n         if (updatedSchema == null) {\n-            final SchemaBuilder builder = SchemaUtil.copySchemaBasics(value.schema(), SchemaBuilder.struct());\n-            Struct defaultValue = (Struct) value.schema().defaultValue();\n-            buildUpdatedSchema(value.schema(), \"\", builder, value.schema().isOptional(), defaultValue);\n+            final SchemaBuilder builder = SchemaUtil.copySchemaBasics(schema, SchemaBuilder.struct());\n+            Struct defaultValue = (Struct) schema.defaultValue();\n+            buildUpdatedSchema(schema, \"\", builder, schema.isOptional(), defaultValue);\n             updatedSchema = builder.build();\n-            schemaUpdateCache.put(value.schema(), updatedSchema);\n+            schemaUpdateCache.put(schema, updatedSchema);\n+        }\n+        if (value == null) {\n+            return newRecord(record, updatedSchema, null);\n+        } else {\n+            final Struct updatedValue = new Struct(updatedSchema);\n+            buildWithSchema(value, \"\", updatedValue);\n+            return newRecord(record, updatedSchema, updatedValue);\n         }\n-\n-        final Struct updatedValue = new Struct(updatedSchema);\n-        buildWithSchema(value, \"\", updatedValue);\n-        return newRecord(record, updatedSchema, updatedValue);\n     }\n \n     /**\n@@ -216,6 +220,9 @@ private Schema convertFieldSchema(Schema orig, boolean optional, Object defaultF\n     }\n \n     private void buildWithSchema(Struct record, String fieldNamePrefix, Struct newRecord) {\n+        if (record == null) {\n+            return;\n+        }\n         for (Field field : record.schema().fields()) {\n             final String fieldName = fieldName(fieldNamePrefix, field.name());\n             switch (field.schema().type()) {", "filename": "connect/transforms/src/main/java/org/apache/kafka/connect/transforms/Flatten.java"}, {"additions": 40, "raw_url": "https://github.com/apache/kafka/raw/fc4fea6761986749f0ac640868d9b4d2a552eb62/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/FlattenTest.java", "blob_url": "https://github.com/apache/kafka/blob/fc4fea6761986749f0ac640868d9b4d2a552eb62/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/FlattenTest.java", "sha": "2e7be9562cb26111e5344ce4ede8bfc68a244a9c", "changes": 40, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/transforms/src/test/java/org/apache/kafka/connect/transforms/FlattenTest.java?ref=fc4fea6761986749f0ac640868d9b4d2a552eb62", "patch": "@@ -182,6 +182,46 @@ public void testOptionalFieldStruct() {\n         assertNull(transformedStruct.get(\"B.opt_int32\"));\n     }\n \n+    @Test\n+    public void testOptionalStruct() {\n+        xformValue.configure(Collections.<String, String>emptyMap());\n+\n+        SchemaBuilder builder = SchemaBuilder.struct().optional();\n+        builder.field(\"opt_int32\", Schema.OPTIONAL_INT32_SCHEMA);\n+        Schema schema = builder.build();\n+\n+        SourceRecord transformed = xformValue.apply(new SourceRecord(null, null,\n+            \"topic\", 0,\n+            schema, null));\n+\n+        assertEquals(Schema.Type.STRUCT, transformed.valueSchema().type());\n+        assertNull(transformed.value());\n+    }\n+\n+    @Test\n+    public void testOptionalNestedStruct() {\n+        xformValue.configure(Collections.<String, String>emptyMap());\n+\n+        SchemaBuilder builder = SchemaBuilder.struct().optional();\n+        builder.field(\"opt_int32\", Schema.OPTIONAL_INT32_SCHEMA);\n+        Schema supportedTypesSchema = builder.build();\n+\n+        builder = SchemaBuilder.struct();\n+        builder.field(\"B\", supportedTypesSchema);\n+        Schema oneLevelNestedSchema = builder.build();\n+\n+        Struct oneLevelNestedStruct = new Struct(oneLevelNestedSchema);\n+        oneLevelNestedStruct.put(\"B\", null);\n+\n+        SourceRecord transformed = xformValue.apply(new SourceRecord(null, null,\n+            \"topic\", 0,\n+            oneLevelNestedSchema, oneLevelNestedStruct));\n+\n+        assertEquals(Schema.Type.STRUCT, transformed.valueSchema().type());\n+        Struct transformedStruct = (Struct) transformed.value();\n+        assertNull(transformedStruct.get(\"B.opt_int32\"));\n+    }\n+\n     @Test\n     public void testOptionalFieldMap() {\n         xformValue.configure(Collections.<String, String>emptyMap());", "filename": "connect/transforms/src/test/java/org/apache/kafka/connect/transforms/FlattenTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/9f954ac614cd5dd7efbcabe34799207128f16e63", "parent": "https://github.com/apache/kafka/commit/b616f913c8a57609476e5cf99b1802ef1dfe1c98", "message": "MINOR: Safe string conversion to avoid NPEs\n\nShould be ported back to 2.0\n\nAuthor: Cyrus Vafadari <cyrus@confluent.io>\n\nReviewers: Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #6004 from cyrusv/cyrus-npe", "bug_id": "kafka_12", "file": [{"additions": 2, "raw_url": "https://github.com/apache/kafka/raw/9f954ac614cd5dd7efbcabe34799207128f16e63/connect/api/src/main/java/org/apache/kafka/connect/connector/ConnectRecord.java", "blob_url": "https://github.com/apache/kafka/blob/9f954ac614cd5dd7efbcabe34799207128f16e63/connect/api/src/main/java/org/apache/kafka/connect/connector/ConnectRecord.java", "sha": "b181209163926978714f7d8d0d313c22da12aa77", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/api/src/main/java/org/apache/kafka/connect/connector/ConnectRecord.java?ref=9f954ac614cd5dd7efbcabe34799207128f16e63", "patch": "@@ -140,9 +140,9 @@ public String toString() {\n                 \"topic='\" + topic + '\\'' +\n                 \", kafkaPartition=\" + kafkaPartition +\n                 \", key=\" + key +\n-                \", keySchema=\" + keySchema.toString() +\n+                \", keySchema=\" + keySchema +\n                 \", value=\" + value +\n-                \", valueSchema=\" + valueSchema.toString() +\n+                \", valueSchema=\" + valueSchema +\n                 \", timestamp=\" + timestamp +\n                 \", headers=\" + headers +\n                 '}';", "filename": "connect/api/src/main/java/org/apache/kafka/connect/connector/ConnectRecord.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/3000fda8b9800f4cddfc9675dfae11f762aabccc", "parent": "https://github.com/apache/kafka/commit/c2bee988faa3c338fdc275aac9f638bb652b38a0", "message": "KAFKA-8277: Fix NPEs in several methods of ConnectHeaders (#6550)\n\nReplace `headers.isEmpty()` by calls to `isEmpty()` as the latter does a null check on heathers (that is lazily created).\r\n\r\nAuthor: Sebasti\u00e1n Ortega <sebastian.ortega@letgo.com>\r\nReviewers: Konstantine Karantasis <konstantine@confluent.io>, Arjun Satish <arjunconfluent.io>, Randall Hauch <rhauch@gmail.com>", "bug_id": "kafka_13", "file": [{"additions": 5, "raw_url": "https://github.com/apache/kafka/raw/3000fda8b9800f4cddfc9675dfae11f762aabccc/connect/api/src/main/java/org/apache/kafka/connect/header/ConnectHeaders.java", "blob_url": "https://github.com/apache/kafka/blob/3000fda8b9800f4cddfc9675dfae11f762aabccc/connect/api/src/main/java/org/apache/kafka/connect/header/ConnectHeaders.java", "sha": "0b5c484b3533e124a76cec330f2c3ec8f6361e8d", "changes": 10, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/api/src/main/java/org/apache/kafka/connect/header/ConnectHeaders.java?ref=3000fda8b9800f4cddfc9675dfae11f762aabccc", "patch": "@@ -274,7 +274,7 @@ public Header lastWithName(String key) {\n     @Override\n     public Headers remove(String key) {\n         checkKey(key);\n-        if (!headers.isEmpty()) {\n+        if (!isEmpty()) {\n             Iterator<Header> iterator = iterator();\n             while (iterator.hasNext()) {\n                 if (iterator.next().key().equals(key)) {\n@@ -287,7 +287,7 @@ public Headers remove(String key) {\n \n     @Override\n     public Headers retainLatest() {\n-        if (!headers.isEmpty()) {\n+        if (!isEmpty()) {\n             Set<String> keys = new HashSet<>();\n             ListIterator<Header> iter = headers.listIterator(headers.size());\n             while (iter.hasPrevious()) {\n@@ -304,7 +304,7 @@ public Headers retainLatest() {\n     @Override\n     public Headers retainLatest(String key) {\n         checkKey(key);\n-        if (!headers.isEmpty()) {\n+        if (!isEmpty()) {\n             boolean found = false;\n             ListIterator<Header> iter = headers.listIterator(headers.size());\n             while (iter.hasPrevious()) {\n@@ -322,7 +322,7 @@ public Headers retainLatest(String key) {\n     @Override\n     public Headers apply(String key, HeaderTransform transform) {\n         checkKey(key);\n-        if (!headers.isEmpty()) {\n+        if (!isEmpty()) {\n             ListIterator<Header> iter = headers.listIterator();\n             while (iter.hasNext()) {\n                 Header orig = iter.next();\n@@ -341,7 +341,7 @@ public Headers apply(String key, HeaderTransform transform) {\n \n     @Override\n     public Headers apply(HeaderTransform transform) {\n-        if (!headers.isEmpty()) {\n+        if (!isEmpty()) {\n             ListIterator<Header> iter = headers.listIterator();\n             while (iter.hasNext()) {\n                 Header orig = iter.next();", "filename": "connect/api/src/main/java/org/apache/kafka/connect/header/ConnectHeaders.java"}, {"additions": 22, "raw_url": "https://github.com/apache/kafka/raw/3000fda8b9800f4cddfc9675dfae11f762aabccc/connect/api/src/test/java/org/apache/kafka/connect/header/ConnectHeadersTest.java", "blob_url": "https://github.com/apache/kafka/blob/3000fda8b9800f4cddfc9675dfae11f762aabccc/connect/api/src/test/java/org/apache/kafka/connect/header/ConnectHeadersTest.java", "sha": "72418ba47ffa7ba84f6fe07b22ca159a07aa7dcc", "changes": 23, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/api/src/test/java/org/apache/kafka/connect/header/ConnectHeadersTest.java?ref=3000fda8b9800f4cddfc9675dfae11f762aabccc", "patch": "@@ -118,6 +118,14 @@ public void shouldHaveToString() {\n         assertNotNull(headers.toString());\n     }\n \n+    @Test\n+    public void shouldRetainLatestWhenEmpty() {\n+        headers.retainLatest(other);\n+        headers.retainLatest(key);\n+        headers.retainLatest();\n+        assertTrue(headers.isEmpty());\n+    }\n+\n     @Test\n     public void shouldAddMultipleHeadersWithSameKeyAndRetainLatest() {\n         populate(headers);\n@@ -179,6 +187,12 @@ public void shouldNotAddHeadersWithObjectValuesAndMismatchedSchema() {\n         attemptAndFailToAddHeader(\"k2\", Schema.OPTIONAL_STRING_SCHEMA, 0L);\n     }\n \n+    @Test\n+    public void shouldRemoveAllHeadersWithSameKeyWhenEmpty() {\n+        headers.remove(key);\n+        assertNoHeaderWithKey(key);\n+    }\n+\n     @Test\n     public void shouldRemoveAllHeadersWithSameKey() {\n         populate(headers);\n@@ -211,6 +225,13 @@ public void shouldRemoveAllHeaders() {\n         assertTrue(headers.isEmpty());\n     }\n \n+    @Test\n+    public void shouldTransformHeadersWhenEmpty() {\n+        headers.apply(appendToKey(\"-suffix\"));\n+        headers.apply(key, appendToKey(\"-suffix\"));\n+        assertTrue(headers.isEmpty());\n+    }\n+\n     @Test\n     public void shouldTransformHeaders() {\n         populate(headers);\n@@ -544,4 +565,4 @@ protected void assertHeader(Header header, String key, Schema schema, Object val\n         assertSame(schema, header.schema());\n         assertSame(value, header.value());\n     }\n-}\n\\ No newline at end of file\n+}", "filename": "connect/api/src/test/java/org/apache/kafka/connect/header/ConnectHeadersTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/b616f913c8a57609476e5cf99b1802ef1dfe1c98", "parent": "https://github.com/apache/kafka/commit/ec501f305e53a09072580fb3824048c170d32a48", "message": "KAFKA-7678: Avoid NPE when closing the RecordCollector (#5993)\n\nReviewers: Matthias J. Sax <matthias@confluent.io>, Bill Bejeck <bill@confluent.io>", "bug_id": "kafka_14", "file": [{"additions": 4, "raw_url": "https://github.com/apache/kafka/raw/b616f913c8a57609476e5cf99b1802ef1dfe1c98/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "blob_url": "https://github.com/apache/kafka/blob/b616f913c8a57609476e5cf99b1802ef1dfe1c98/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java", "sha": "d3a00301d7a933f6e7e75a3efab3c25af74a73ab", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java?ref=b616f913c8a57609476e5cf99b1802ef1dfe1c98", "patch": "@@ -249,8 +249,10 @@ public void flush() {\n     @Override\n     public void close() {\n         log.debug(\"Closing producer\");\n-        producer.close();\n-        producer = null;\n+        if (producer != null) {\n+            producer.close();\n+            producer = null;\n+        }\n         checkForException();\n     }\n ", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/RecordCollectorImpl.java"}, {"additions": 12, "raw_url": "https://github.com/apache/kafka/raw/b616f913c8a57609476e5cf99b1802ef1dfe1c98/streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java", "blob_url": "https://github.com/apache/kafka/blob/b616f913c8a57609476e5cf99b1802ef1dfe1c98/streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java", "sha": "0bc65ccbe106e7a69764b20cd9b78006c626cb51", "changes": 12, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java?ref=b616f913c8a57609476e5cf99b1802ef1dfe1c98", "patch": "@@ -387,6 +387,18 @@ public void testRecordHeaderPassThroughSerializer() {\n         }\n     }\n \n+    @Test\n+    public void testShouldNotThrowNPEOnCloseIfProducerIsNotInitialized() {\n+        final RecordCollectorImpl collector = new RecordCollectorImpl(\n+                \"NoNPE\",\n+                logContext,\n+                new DefaultProductionExceptionHandler(),\n+                new Metrics().sensor(\"skipped-records\")\n+        );\n+\n+        collector.close();\n+    }\n+\n     private static class CustomStringSerializer extends StringSerializer {\n \n         private boolean isKey;", "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/RecordCollectorTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/289ac092923a32e5a839750fcaef0ae0856d05b3", "parent": "https://github.com/apache/kafka/commit/05cba28ca7aafd3974e9e818be08f239b6162855", "message": "KAFKA-8591; WorkerConfigTransformer NPE on connector configuration reloading (#6991)\n\nA bug in `WorkerConfigTransformer` prevents the connector configuration reload when the ConfigData TTL expires. \r\n\r\nThe issue boils down to the fact that `worker.herder().restartConnector` is receiving a null callback. \r\n\r\n```\r\n[2019-06-17 14:34:12,320] INFO Scheduling a restart of connector workshop-incremental in 60000 ms (org.apache.kafka.connect.runtime.WorkerConfigTransformer:88)\r\n[2019-06-17 14:34:12,321] ERROR Uncaught exception in herder work thread, exiting:  (org.apache.kafka.connect.runtime.distributed.DistributedHerder:227)\r\njava.lang.NullPointerException\r\n        at org.apache.kafka.connect.runtime.distributed.DistributedHerder$19.onCompletion(DistributedHerder.java:1187)\r\n        at org.apache.kafka.connect.runtime.distributed.DistributedHerder$19.onCompletion(DistributedHerder.java:1183)\r\n        at org.apache.kafka.connect.runtime.distributed.DistributedHerder.tick(DistributedHerder.java:273)\r\n        at org.apache.kafka.connect.runtime.distributed.DistributedHerder.run(DistributedHerder.java:219)\r\n```\r\nThis patch adds a callback which just logs the error.\r\n\r\nReviewers: Robert Yokota <rayokota@gmail.com>, Jason Gustafson <jason@confluent.io>", "bug_id": "kafka_15", "file": [{"additions": 10, "raw_url": "https://github.com/apache/kafka/raw/289ac092923a32e5a839750fcaef0ae0856d05b3/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java", "blob_url": "https://github.com/apache/kafka/blob/289ac092923a32e5a839750fcaef0ae0856d05b3/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java", "sha": "1a799bb37297699106c4ac8df3098af73898b165", "changes": 11, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java?ref=289ac092923a32e5a839750fcaef0ae0856d05b3", "patch": "@@ -21,6 +21,7 @@\n import org.apache.kafka.common.config.ConfigTransformer;\n import org.apache.kafka.common.config.ConfigTransformerResult;\n import org.apache.kafka.connect.runtime.Herder.ConfigReloadAction;\n+import org.apache.kafka.connect.util.Callback;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -86,7 +87,15 @@ private void scheduleReload(String connectorName, String path, long ttl) {\n             }\n         }\n         log.info(\"Scheduling a restart of connector {} in {} ms\", connectorName, ttl);\n-        HerderRequest request = worker.herder().restartConnector(ttl, connectorName, null);\n+        Callback<Void> cb = new Callback<Void>() {\n+            @Override\n+            public void onCompletion(Throwable error, Void result) {\n+                if (error != null) {\n+                    log.error(\"Unexpected error during connector restart: \", error);\n+                }\n+            }\n+        };\n+        HerderRequest request = worker.herder().restartConnector(ttl, connectorName, cb);\n         connectorRequests.put(path, request);\n     }\n }", "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java"}, {"additions": 5, "raw_url": "https://github.com/apache/kafka/raw/289ac092923a32e5a839750fcaef0ae0856d05b3/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java", "blob_url": "https://github.com/apache/kafka/blob/289ac092923a32e5a839750fcaef0ae0856d05b3/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java", "sha": "6f4bda66904d7fc25e903ede7d35a440b082b3c0", "changes": 9, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java?ref=289ac092923a32e5a839750fcaef0ae0856d05b3", "patch": "@@ -20,6 +20,7 @@\n import org.apache.kafka.common.config.ConfigData;\n import org.apache.kafka.common.config.provider.ConfigProvider;\n import org.easymock.EasyMock;\n+import static org.easymock.EasyMock.eq;\n import org.junit.Before;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n@@ -34,6 +35,7 @@\n \n import static org.apache.kafka.connect.runtime.ConnectorConfig.CONFIG_RELOAD_ACTION_CONFIG;\n import static org.apache.kafka.connect.runtime.ConnectorConfig.CONFIG_RELOAD_ACTION_NONE;\n+import static org.easymock.EasyMock.notNull;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNull;\n import static org.powermock.api.easymock.PowerMock.replayAll;\n@@ -84,8 +86,7 @@ public void testReplaceVariableWithTTL() {\n     @Test\n     public void testReplaceVariableWithTTLAndScheduleRestart() {\n         EasyMock.expect(worker.herder()).andReturn(herder);\n-        EasyMock.expect(herder.restartConnector(1L, MY_CONNECTOR, null)).andReturn(requestId);\n-\n+        EasyMock.expect(herder.restartConnector(eq(1L), eq(MY_CONNECTOR), notNull())).andReturn(requestId);\n         replayAll();\n \n         Map<String, String> result = configTransformer.transform(MY_CONNECTOR, Collections.singletonMap(MY_KEY, \"${test:testPath:testKeyWithTTL}\"));\n@@ -95,13 +96,13 @@ public void testReplaceVariableWithTTLAndScheduleRestart() {\n     @Test\n     public void testReplaceVariableWithTTLFirstCancelThenScheduleRestart() {\n         EasyMock.expect(worker.herder()).andReturn(herder);\n-        EasyMock.expect(herder.restartConnector(1L, MY_CONNECTOR, null)).andReturn(requestId);\n+        EasyMock.expect(herder.restartConnector(eq(1L), eq(MY_CONNECTOR), notNull())).andReturn(requestId);\n \n         EasyMock.expect(worker.herder()).andReturn(herder);\n         EasyMock.expectLastCall();\n         requestId.cancel();\n         EasyMock.expectLastCall();\n-        EasyMock.expect(herder.restartConnector(10L, MY_CONNECTOR, null)).andReturn(requestId);\n+        EasyMock.expect(herder.restartConnector(eq(10L), eq(MY_CONNECTOR), notNull())).andReturn(requestId);\n \n         replayAll();\n ", "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/a1f7925d23be0b81cb77561d2113443df52c6f74", "parent": "https://github.com/apache/kafka/commit/f667f573ff12321f2913ccf17cdf9e9bcc97b550", "message": "KAFKA-7962: Avoid NPE for StickyAssignor (#6308)\n\n* KAFKA-7962: StickyAssignor: throws NullPointerException during assignments if topic is deleted\r\n\r\nhttps://issues.apache.org/jira/browse/KAFKA-7962\r\n\r\nConsumer using StickyAssignor throws NullPointerException if a subscribed topic was removed.\r\n\r\n* addressed vahidhashemian's comments\r\n\r\n* lower NPath Complexity\r\n\r\n* added a unit test", "bug_id": "kafka_16", "file": [{"additions": 4, "raw_url": "https://github.com/apache/kafka/raw/a1f7925d23be0b81cb77561d2113443df52c6f74/clients/src/main/java/org/apache/kafka/clients/consumer/StickyAssignor.java", "blob_url": "https://github.com/apache/kafka/blob/a1f7925d23be0b81cb77561d2113443df52c6f74/clients/src/main/java/org/apache/kafka/clients/consumer/StickyAssignor.java", "sha": "ee537eba78802f3c9c1ea4304d5a1dea557ff9b2", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/consumer/StickyAssignor.java?ref=a1f7925d23be0b81cb77561d2113443df52c6f74", "patch": "@@ -219,13 +219,13 @@\n         for (Entry<String, Subscription> entry: subscriptions.entrySet()) {\n             String consumer = entry.getKey();\n             consumer2AllPotentialPartitions.put(consumer, new ArrayList<TopicPartition>());\n-            for (String topic: entry.getValue().topics()) {\n+            entry.getValue().topics().stream().filter(topic -> partitionsPerTopic.get(topic) != null).forEach(topic -> {\n                 for (int i = 0; i < partitionsPerTopic.get(topic); ++i) {\n                     TopicPartition topicPartition = new TopicPartition(topic, i);\n                     consumer2AllPotentialPartitions.get(consumer).add(topicPartition);\n                     partition2AllPotentialConsumers.get(topicPartition).add(consumer);\n                 }\n-            }\n+            });\n \n             // add this consumer to currentAssignment (with an empty topic partition assignment) if it does not already exist\n             if (!currentAssignment.containsKey(consumer))\n@@ -705,6 +705,8 @@ static ByteBuffer serializeTopicPartitionAssignment(List<TopicPartition> partiti\n      */\n     private <T> boolean hasIdenticalListElements(Collection<List<T>> col) {\n         Iterator<List<T>> it = col.iterator();\n+        if (!it.hasNext())\n+            return true;\n         List<T> cur = it.next();\n         while (it.hasNext()) {\n             List<T> next = it.next();", "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/StickyAssignor.java"}, {"additions": 32, "raw_url": "https://github.com/apache/kafka/raw/a1f7925d23be0b81cb77561d2113443df52c6f74/clients/src/test/java/org/apache/kafka/clients/consumer/StickyAssignorTest.java", "blob_url": "https://github.com/apache/kafka/blob/a1f7925d23be0b81cb77561d2113443df52c6f74/clients/src/test/java/org/apache/kafka/clients/consumer/StickyAssignorTest.java", "sha": "32ba16a482023cafc66a8fbf917b3bb268163300", "changes": 32, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/consumer/StickyAssignorTest.java?ref=a1f7925d23be0b81cb77561d2113443df52c6f74", "patch": "@@ -606,6 +606,38 @@ public void testStickiness() {\n         }\n     }\n \n+    @Test\n+    public void testAssignmentUpdatedForDeletedTopic() {\n+        String consumerId = \"consumer\";\n+\n+        Map<String, Integer> partitionsPerTopic = new HashMap<>();\n+        partitionsPerTopic.put(\"topic01\", 1);\n+        partitionsPerTopic.put(\"topic03\", 100);\n+        Map<String, Subscription> subscriptions =\n+                Collections.singletonMap(consumerId, new Subscription(topics(\"topic01\", \"topic02\", \"topic03\")));\n+\n+        Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, subscriptions);\n+        assertEquals(assignment.values().stream().mapToInt(topicPartitions -> topicPartitions.size()).sum(), 1 + 100);\n+        assertEquals(Collections.singleton(consumerId), assignment.keySet());\n+        assertTrue(isFullyBalanced(assignment));\n+    }\n+\n+    @Test\n+    public void testNoExceptionThrownWhenOnlySubscribedTopicDeleted() {\n+        String topic = \"topic01\";\n+        String consumer = \"consumer01\";\n+        Map<String, Integer> partitionsPerTopic = new HashMap<>();\n+        partitionsPerTopic.put(topic, 3);\n+        Map<String, Subscription> subscriptions = new HashMap<>();\n+        subscriptions.put(consumer, new Subscription(topics(topic)));\n+        Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic, subscriptions);\n+        subscriptions.put(consumer, new Subscription(topics(topic), StickyAssignor.serializeTopicPartitionAssignment(assignment.get(consumer))));\n+\n+        assignment = assignor.assign(Collections.emptyMap(), subscriptions);\n+        assertEquals(assignment.size(), 1);\n+        assertTrue(assignment.get(consumer).isEmpty());\n+    }\n+\n     private String getTopicName(int i, int maxNum) {\n         return getCanonicalName(\"t\", i, maxNum);\n     }", "filename": "clients/src/test/java/org/apache/kafka/clients/consumer/StickyAssignorTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/14314e3d687b4c7b77750d27a085b803c934e3e9", "parent": "https://github.com/apache/kafka/commit/a1b1e088b98763818e933dce335b580d02916640", "message": "[HOT FIX] in-memory store behavior should match rocksDB (#6657)\n\nWhile working on consolidating the various store unit tests I uncovered some minor \"bugs\" in the in-memory stores (inconsistencies with the behavior as established by the RocksDB stores).\r\n\r\nopen iterators should be properly closed in the case the store is closed\r\nfetch/findSessions should always throw NPE if key is null\r\nwindow end time should be truncated at Long.MAX_VALUE rather than throw exception\r\n(Verified in-memory stores pass all applicable rocksDB tests now, unified unit tests coming in another PR)\r\n\r\nReviewers: Guozhang Wang <wangguoz@gmail.com>, Bill Bejeck <bbejeck@gmail.com>", "bug_id": "kafka_17", "file": [{"additions": 25, "raw_url": "https://github.com/apache/kafka/raw/14314e3d687b4c7b77750d27a085b803c934e3e9/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemorySessionStore.java", "blob_url": "https://github.com/apache/kafka/blob/14314e3d687b4c7b77750d27a085b803c934e3e9/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemorySessionStore.java", "sha": "f3b85657278dc19df103049bf5143681bfb63f50", "changes": 25, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemorySessionStore.java?ref=14314e3d687b4c7b77750d27a085b803c934e3e9", "patch": "@@ -23,6 +23,7 @@\n import java.util.Map;\n import java.util.Map.Entry;\n import java.util.NoSuchElementException;\n+import java.util.Objects;\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentNavigableMap;\n@@ -134,6 +135,8 @@ public void remove(final Windowed<Bytes> sessionKey) {\n     public byte[] fetchSession(final Bytes key, final long startTime, final long endTime) {\n         removeExpiredSegments();\n \n+        Objects.requireNonNull(key, \"key cannot be null\");\n+\n         // Only need to search if the record hasn't expired yet\n         if (endTime > observedStreamTime - retentionPeriod) {\n             final ConcurrentNavigableMap<Bytes, ConcurrentNavigableMap<Long, byte[]>> keyMap = endTimeMap.get(endTime);\n@@ -152,6 +155,8 @@ public void remove(final Windowed<Bytes> sessionKey) {\n     public KeyValueIterator<Windowed<Bytes>, byte[]> findSessions(final Bytes key,\n                                                                   final long earliestSessionEndTime,\n                                                                   final long latestSessionStartTime) {\n+        Objects.requireNonNull(key, \"key cannot be null\");\n+\n         removeExpiredSegments();\n \n         return registerNewIterator(key,\n@@ -166,6 +171,9 @@ public void remove(final Windowed<Bytes> sessionKey) {\n                                                                   final Bytes keyTo,\n                                                                   final long earliestSessionEndTime,\n                                                                   final long latestSessionStartTime) {\n+        Objects.requireNonNull(keyFrom, \"from key cannot be null\");\n+        Objects.requireNonNull(keyTo, \"to key cannot be null\");\n+\n         removeExpiredSegments();\n \n         if (keyFrom.compareTo(keyTo) > 0) {\n@@ -183,15 +191,23 @@ public void remove(final Windowed<Bytes> sessionKey) {\n \n     @Override\n     public KeyValueIterator<Windowed<Bytes>, byte[]> fetch(final Bytes key) {\n+\n+        Objects.requireNonNull(key, \"key cannot be null\");\n+\n         removeExpiredSegments();\n \n         return registerNewIterator(key, key, Long.MAX_VALUE, endTimeMap.entrySet().iterator());\n     }\n \n     @Override\n     public KeyValueIterator<Windowed<Bytes>, byte[]> fetch(final Bytes from, final Bytes to) {\n+\n+        Objects.requireNonNull(from, \"from key cannot be null\");\n+        Objects.requireNonNull(to, \"to key cannot be null\");\n+\n         removeExpiredSegments();\n \n+\n         return registerNewIterator(from, to, Long.MAX_VALUE, endTimeMap.entrySet().iterator());\n     }\n \n@@ -212,6 +228,13 @@ public void flush() {\n \n     @Override\n     public void close() {\n+        if (openIterators.size() != 0) {\n+            LOG.warn(\"Closing {} open iterators for store {}\", openIterators.size(), name);\n+            for (final InMemorySessionStoreIterator it : openIterators) {\n+                it.close();\n+            }\n+        }\n+\n         endTimeMap.clear();\n         openIterators.clear();\n         open = false;\n@@ -303,6 +326,8 @@ public boolean hasNext() {\n \n         @Override\n         public void close() {\n+            next = null;\n+            recordIterator = null;\n             callback.deregisterIterator(this);\n         }\n ", "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemorySessionStore.java"}, {"additions": 55, "raw_url": "https://github.com/apache/kafka/raw/14314e3d687b4c7b77750d27a085b803c934e3e9/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowStore.java", "blob_url": "https://github.com/apache/kafka/blob/14314e3d687b4c7b77750d27a085b803c934e3e9/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowStore.java", "sha": "8063410212eb679e8557e5f1a04a37a1efc5add8", "changes": 70, "status": "modified", "deletions": 15, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowStore.java?ref=14314e3d687b4c7b77750d27a085b803c934e3e9", "patch": "@@ -18,6 +18,7 @@\n \n import java.nio.ByteBuffer;\n import java.util.Iterator;\n+import java.util.Objects;\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentNavigableMap;\n@@ -135,6 +136,9 @@ public void put(final Bytes key, final byte[] value, final long windowStartTimes\n             } else {\n                 segmentMap.computeIfPresent(windowStartTimestamp, (t, kvMap) -> {\n                     kvMap.remove(keyBytes);\n+                    if (kvMap.isEmpty()) {\n+                        segmentMap.remove(windowStartTimestamp);\n+                    }\n                     return kvMap;\n                 });\n             }\n@@ -143,6 +147,9 @@ public void put(final Bytes key, final byte[] value, final long windowStartTimes\n \n     @Override\n     public byte[] fetch(final Bytes key, final long windowStartTimestamp) {\n+\n+        Objects.requireNonNull(key, \"key cannot be null\");\n+\n         removeExpiredSegments();\n \n         if (windowStartTimestamp <= observedStreamTime - retentionPeriod) {\n@@ -160,6 +167,9 @@ public void put(final Bytes key, final byte[] value, final long windowStartTimes\n     @Deprecated\n     @Override\n     public WindowStoreIterator<byte[]> fetch(final Bytes key, final long timeFrom, final long timeTo) {\n+\n+        Objects.requireNonNull(key, \"key cannot be null\");\n+\n         removeExpiredSegments();\n \n         // add one b/c records expire exactly retentionPeriod ms after created\n@@ -179,6 +189,9 @@ public void put(final Bytes key, final byte[] value, final long windowStartTimes\n                                                            final Bytes to,\n                                                            final long timeFrom,\n                                                            final long timeTo) {\n+        Objects.requireNonNull(from, \"from key cannot be null\");\n+        Objects.requireNonNull(to, \"to key cannot be null\");\n+\n         removeExpiredSegments();\n \n         if (from.compareTo(to) > 0) {\n@@ -242,6 +255,13 @@ public void flush() {\n \n     @Override\n     public void close() {\n+        if (openIterators.size() != 0) {\n+            LOG.warn(\"Closing {} open iterators for store {}\", openIterators.size(), name);\n+            for (final InMemoryWindowStoreIteratorWrapper it : openIterators) {\n+                it.close();\n+            }\n+        }\n+        \n         segmentMap.clear();\n         open = false;\n     }\n@@ -281,7 +301,7 @@ private WrappedInMemoryWindowStoreIterator registerNewWindowStoreIterator(final\n         final Bytes keyTo = retainDuplicates ? wrapForDups(key, Integer.MAX_VALUE) : key;\n \n         final WrappedInMemoryWindowStoreIterator iterator =\n-            new WrappedInMemoryWindowStoreIterator(keyFrom, keyTo, segmentIterator, openIterators::remove);\n+            new WrappedInMemoryWindowStoreIterator(keyFrom, keyTo, segmentIterator, openIterators::remove, retainDuplicates);\n \n         openIterators.add(iterator);\n         return iterator;\n@@ -319,15 +339,18 @@ private WrappedWindowedKeyValueIterator registerNewWindowedKeyValueIterator(fina\n         private final boolean allKeys;\n         private final Bytes keyFrom;\n         private final Bytes keyTo;\n+        private final boolean retainDuplicates;\n         private final ClosingCallback callback;\n \n         InMemoryWindowStoreIteratorWrapper(final Bytes keyFrom,\n                                            final Bytes keyTo,\n                                            final Iterator<Map.Entry<Long, ConcurrentNavigableMap<Bytes, byte[]>>> segmentIterator,\n-                                           final ClosingCallback callback) {\n+                                           final ClosingCallback callback,\n+                                           final boolean retainDuplicates) {\n             this.keyFrom = keyFrom;\n             this.keyTo = keyTo;\n             allKeys = (keyFrom == null) && (keyTo == null);\n+            this.retainDuplicates = retainDuplicates;\n \n             this.segmentIterator = segmentIterator;\n             this.callback = callback;\n@@ -343,15 +366,26 @@ public boolean hasNext() {\n             }\n \n             next = getNext();\n-            return next != null;\n-        }\n+            if (next == null) {\n+                return false;\n+            }\n \n-        public void remove() {\n-            throw new UnsupportedOperationException(\n-                \"remove() is not supported in \" + getClass().getName());\n+            if (allKeys || !retainDuplicates) {\n+                return true;\n+            }\n+\n+            final Bytes key = getKey(next.key);\n+            if (key.compareTo(getKey(keyFrom)) >= 0 && key.compareTo(getKey(keyTo)) <= 0) {\n+                return true;\n+            } else {\n+                next = null;\n+                return hasNext();\n+            }\n         }\n \n         public void close() {\n+            next = null;\n+            recordIterator = null;\n             callback.deregisterIterator(this);\n         }\n \n@@ -395,8 +429,9 @@ Long minTime() {\n         WrappedInMemoryWindowStoreIterator(final Bytes keyFrom,\n                                            final Bytes keyTo,\n                                            final Iterator<Map.Entry<Long, ConcurrentNavigableMap<Bytes, byte[]>>> segmentIterator,\n-                                           final ClosingCallback callback)  {\n-            super(keyFrom, keyTo, segmentIterator, callback);\n+                                           final ClosingCallback callback,\n+                                           final boolean retainDuplicates)  {\n+            super(keyFrom, keyTo, segmentIterator, callback, retainDuplicates);\n         }\n \n         @Override\n@@ -419,13 +454,12 @@ public Long peekNextKey() {\n         }\n \n         public static WrappedInMemoryWindowStoreIterator emptyIterator() {\n-            return new WrappedInMemoryWindowStoreIterator(null, null, null, it -> { });\n+            return new WrappedInMemoryWindowStoreIterator(null, null, null, it -> { }, false);\n         }\n     }\n \n     private static class WrappedWindowedKeyValueIterator extends InMemoryWindowStoreIteratorWrapper implements KeyValueIterator<Windowed<Bytes>, byte[]> {\n \n-        private final boolean retainDuplicates;\n         private final long windowSize;\n \n         WrappedWindowedKeyValueIterator(final Bytes keyFrom,\n@@ -434,8 +468,7 @@ public static WrappedInMemoryWindowStoreIterator emptyIterator() {\n                                         final ClosingCallback callback,\n                                         final boolean retainDuplicates,\n                                         final long windowSize) {\n-            super(keyFrom, keyTo, segmentIterator, callback);\n-            this.retainDuplicates = retainDuplicates;\n+            super(keyFrom, keyTo, segmentIterator, callback, retainDuplicates);\n             this.windowSize = windowSize;\n         }\n \n@@ -457,8 +490,15 @@ public static WrappedInMemoryWindowStoreIterator emptyIterator() {\n         }\n \n         private Windowed<Bytes> getWindowedKey() {\n-            final Bytes key = retainDuplicates ? getKey(super.next.key) : super.next.key;\n-            final TimeWindow timeWindow = new TimeWindow(super.currentTime, super.currentTime + windowSize);\n+            final Bytes key = super.retainDuplicates ? getKey(super.next.key) : super.next.key;\n+            long endTime = super.currentTime + windowSize;\n+\n+            if (endTime < 0) {\n+                LOG.warn(\"Warning: window end time was truncated to Long.MAX\");\n+                endTime = Long.MAX_VALUE;\n+            }\n+\n+            final TimeWindow timeWindow = new TimeWindow(super.currentTime, endTime);\n             return new Windowed<>(key, timeWindow);\n         }\n     }", "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/InMemoryWindowStore.java"}, {"additions": 11, "raw_url": "https://github.com/apache/kafka/raw/14314e3d687b4c7b77750d27a085b803c934e3e9/streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java", "blob_url": "https://github.com/apache/kafka/blob/14314e3d687b4c7b77750d27a085b803c934e3e9/streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java", "sha": "9218ccf0a7752d6084c1e80fc9cb3971b3a9357b", "changes": 13, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java?ref=14314e3d687b4c7b77750d27a085b803c934e3e9", "patch": "@@ -26,9 +26,13 @@\n \n import java.nio.ByteBuffer;\n import java.util.List;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n public class WindowKeySchema implements RocksDBSegmentedBytesStore.KeySchema {\n \n+    private static final Logger LOG = LoggerFactory.getLogger(WindowKeySchema.class);\n+\n     private static final int SEQNUM_SIZE = 4;\n     private static final int TIMESTAMP_SIZE = 8;\n     private static final int SUFFIX_SIZE = TIMESTAMP_SIZE + SEQNUM_SIZE;\n@@ -99,8 +103,13 @@ public HasNextCondition hasNextCondition(final Bytes binaryKeyFrom,\n      */\n     static TimeWindow timeWindowForSize(final long startMs,\n                                         final long windowSize) {\n-        final long endMs = startMs + windowSize;\n-        return new TimeWindow(startMs, endMs < 0 ? Long.MAX_VALUE : endMs);\n+        long endMs = startMs + windowSize;\n+\n+        if (endMs < 0) {\n+            LOG.warn(\"Warning: window end time was truncated to Long.MAX\");\n+            endMs = Long.MAX_VALUE;\n+        }\n+        return new TimeWindow(startMs, endMs);\n     }\n \n     // for pipe serdes", "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/WindowKeySchema.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/c7836307c3588ed5d267f32eabcd7dfc0dbeec80", "parent": "https://github.com/apache/kafka/commit/555d05971e16123a497ad423ef4786e0831b2101", "message": "[HOT FIX] Check for null before deserializing in MeteredSessionStore  (#6575)\n\nThe fetchSession() method of SessionStore searches for a (single) specific session and returns null if none are found. This is analogous to fetch(key, time) in WindowStore or get(key) in KeyValueStore. MeteredWindowStore and MeteredKeyValueStore both check for a null result before attempting to deserialize, however MeteredSessionStore just blindly deserializes and as a result NPE is thrown when we search for a record that does not exist.\r\n\r\nReviewers: Guozhang Wang <wangguoz@gmail.com>, Bill Bejeck <bbejeck@gmail.com>, Bruno Cadonna <bruno@confluent.io>", "bug_id": "kafka_18", "file": [{"additions": 17, "raw_url": "https://github.com/apache/kafka/raw/c7836307c3588ed5d267f32eabcd7dfc0dbeec80/streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredSessionStore.java", "blob_url": "https://github.com/apache/kafka/blob/c7836307c3588ed5d267f32eabcd7dfc0dbeec80/streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredSessionStore.java", "sha": "94b004e330e75d310eaa3d77851b85ef47426797", "changes": 23, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredSessionStore.java?ref=c7836307c3588ed5d267f32eabcd7dfc0dbeec80", "patch": "@@ -151,30 +151,41 @@ public void remove(final Windowed<K> sessionKey) {\n     @Override\n     public V fetchSession(final K key, final long startTime, final long endTime) {\n         Objects.requireNonNull(key, \"key cannot be null\");\n-        final V value;\n         final Bytes bytesKey = keyBytes(key);\n         final long startNs = time.nanoseconds();\n         try {\n-            value = serdes.valueFrom(wrapped().fetchSession(bytesKey, startTime, endTime));\n+            final byte[] result = wrapped().fetchSession(bytesKey, startTime, endTime);\n+            if (result == null) {\n+                return null;\n+            }\n+            return serdes.valueFrom(result);\n         } finally {\n             metrics.recordLatency(flushTime, startNs, time.nanoseconds());\n         }\n-\n-        return value;\n     }\n \n     @Override\n     public KeyValueIterator<Windowed<K>, V> fetch(final K key) {\n         Objects.requireNonNull(key, \"key cannot be null\");\n-        return findSessions(key, 0, Long.MAX_VALUE);\n+        return new MeteredWindowedKeyValueIterator<>(\n+            wrapped().fetch(keyBytes(key)),\n+            fetchTime,\n+            metrics,\n+            serdes,\n+            time);\n     }\n \n     @Override\n     public KeyValueIterator<Windowed<K>, V> fetch(final K from,\n                                                   final K to) {\n         Objects.requireNonNull(from, \"from cannot be null\");\n         Objects.requireNonNull(to, \"to cannot be null\");\n-        return findSessions(from, to, 0, Long.MAX_VALUE);\n+        return new MeteredWindowedKeyValueIterator<>(\n+            wrapped().fetch(keyBytes(from), keyBytes(to)),\n+            fetchTime,\n+            metrics,\n+            serdes,\n+            time);\n     }\n \n     @Override", "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/MeteredSessionStore.java"}, {"additions": 9, "raw_url": "https://github.com/apache/kafka/raw/c7836307c3588ed5d267f32eabcd7dfc0dbeec80/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "blob_url": "https://github.com/apache/kafka/blob/c7836307c3588ed5d267f32eabcd7dfc0dbeec80/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java", "sha": "5cbe95cea2d37c0da2f0ec7cc1b599f199e36cd0", "changes": 9, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java?ref=c7836307c3588ed5d267f32eabcd7dfc0dbeec80", "patch": "@@ -55,6 +55,7 @@\n import static org.hamcrest.CoreMatchers.equalTo;\n import static org.hamcrest.MatcherAssert.assertThat;\n import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n \n @RunWith(EasyMockRunner.class)\n@@ -243,6 +244,14 @@ public void shouldSetFlushListenerOnWrappedCachingStore() {\n         verify(cachedKeyValueStore);\n     }\n \n+    @Test\n+    public void shouldNotThrowNullPointerExceptionIfGetReturnsNull() {\n+        expect(inner.get(Bytes.wrap(\"a\".getBytes()))).andReturn(null);\n+\n+        init();\n+        assertNull(metered.get(\"a\"));\n+    }\n+\n     @Test\n     public void shouldNotSetFlushListenerOnWrappedNoneCachingStore() {\n         assertFalse(metered.setFlushListener(null, false));", "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredKeyValueStoreTest.java"}, {"additions": 11, "raw_url": "https://github.com/apache/kafka/raw/c7836307c3588ed5d267f32eabcd7dfc0dbeec80/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredSessionStoreTest.java", "blob_url": "https://github.com/apache/kafka/blob/c7836307c3588ed5d267f32eabcd7dfc0dbeec80/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredSessionStoreTest.java", "sha": "30c382b19c1c9b5fc259bd1e54503584a955eb95", "changes": 13, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredSessionStoreTest.java?ref=c7836307c3588ed5d267f32eabcd7dfc0dbeec80", "patch": "@@ -57,6 +57,7 @@\n import static org.hamcrest.CoreMatchers.equalTo;\n import static org.hamcrest.MatcherAssert.assertThat;\n import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n \n @RunWith(EasyMockRunner.class)\n@@ -172,7 +173,7 @@ public void shouldRemoveFromStoreAndRecordRemoveMetric() {\n \n     @Test\n     public void shouldFetchForKeyAndRecordFetchMetric() {\n-        expect(inner.findSessions(Bytes.wrap(keyBytes), 0, Long.MAX_VALUE))\n+        expect(inner.fetch(Bytes.wrap(keyBytes)))\n                 .andReturn(new KeyValueIteratorStub<>(\n                         Collections.singleton(KeyValue.pair(windowedKeyBytes, keyBytes)).iterator()));\n         init();\n@@ -189,7 +190,7 @@ public void shouldFetchForKeyAndRecordFetchMetric() {\n \n     @Test\n     public void shouldFetchRangeFromStoreAndRecordFetchMetric() {\n-        expect(inner.findSessions(Bytes.wrap(keyBytes), Bytes.wrap(keyBytes), 0, Long.MAX_VALUE))\n+        expect(inner.fetch(Bytes.wrap(keyBytes), Bytes.wrap(keyBytes)))\n                 .andReturn(new KeyValueIteratorStub<>(\n                         Collections.singleton(KeyValue.pair(windowedKeyBytes, keyBytes)).iterator()));\n         init();\n@@ -211,6 +212,14 @@ public void shouldRecordRestoreTimeOnInit() {\n         assertTrue((Double) metric.metricValue() > 0);\n     }\n \n+    @Test\n+    public void shouldNotThrowNullPointerExceptionIfFetchSessionReturnsNull() {\n+        expect(inner.fetchSession(Bytes.wrap(\"a\".getBytes()), 0, Long.MAX_VALUE)).andReturn(null);\n+\n+        init();\n+        assertNull(metered.fetchSession(\"a\", 0, Long.MAX_VALUE));\n+    }\n+\n     @Test(expected = NullPointerException.class)\n     public void shouldThrowNullPointerOnPutIfKeyIsNull() {\n         metered.put(null, \"a\");", "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredSessionStoreTest.java"}, {"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/c7836307c3588ed5d267f32eabcd7dfc0dbeec80/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredWindowStoreTest.java", "blob_url": "https://github.com/apache/kafka/blob/c7836307c3588ed5d267f32eabcd7dfc0dbeec80/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredWindowStoreTest.java", "sha": "c0ed7f634017effccd61044325ff7eb26c9504fe", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredWindowStoreTest.java?ref=c7836307c3588ed5d267f32eabcd7dfc0dbeec80", "patch": "@@ -178,7 +178,7 @@ public void shouldCloseUnderlyingStore() {\n     }\n \n     @Test\n-    public void shouldNotExceptionIfFetchReturnsNull() {\n+    public void shouldNotThrowNullPointerExceptionIfFetchReturnsNull() {\n         expect(innerStoreMock.fetch(Bytes.wrap(\"a\".getBytes()), 0)).andReturn(null);\n         replay(innerStoreMock);\n ", "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/MeteredWindowStoreTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/1918f5cb5b5ebbcfb80d19521e2ee7734346a32b", "parent": "https://github.com/apache/kafka/commit/2514a42e84c4e4bad9ac560c09e3d397707436d8", "message": "MINOR: Fix swallowed NPE in KafkaServer.close() (#5339)\n\nIf the server fails to connect to zk, `kafkaScheduler` will be `null`\r\nwhen closing  `KafkaServer`. The NPE is swallowed so it's\r\nharmless apart from the confusing log noise.\r\n\r\nReviewers: Ismael Juma <ismael@juma.me.uk>", "bug_id": "kafka_19", "file": [{"additions": 2, "raw_url": "https://github.com/apache/kafka/raw/1918f5cb5b5ebbcfb80d19521e2ee7734346a32b/core/src/main/scala/kafka/server/KafkaServer.scala", "blob_url": "https://github.com/apache/kafka/blob/1918f5cb5b5ebbcfb80d19521e2ee7734346a32b/core/src/main/scala/kafka/server/KafkaServer.scala", "sha": "0d13d9e0730c0191fca8b314837b9964422d3616", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/KafkaServer.scala?ref=1918f5cb5b5ebbcfb80d19521e2ee7734346a32b", "patch": "@@ -576,7 +576,8 @@ class KafkaServer(val config: KafkaConfig, time: Time = Time.SYSTEM, threadNameP\n         if (requestHandlerPool != null)\n           CoreUtils.swallow(requestHandlerPool.shutdown(), this)\n \n-        CoreUtils.swallow(kafkaScheduler.shutdown(), this)\n+        if (kafkaScheduler != null)\n+          CoreUtils.swallow(kafkaScheduler.shutdown(), this)\n \n         if (apis != null)\n           CoreUtils.swallow(apis.close(), this)", "filename": "core/src/main/scala/kafka/server/KafkaServer.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/bf2c46a0b8266562bf4697dc8e3e9443059f9abb", "parent": "https://github.com/apache/kafka/commit/7c4e6724699bb6fc65112b5513848c733a03019e", "message": "MINOR: Change order of Property Check To Avoid NPE (#5528)\n\nReviewer: Matthias J. Sax <matthias@confluent.io>", "bug_id": "kafka_20", "file": [{"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/bf2c46a0b8266562bf4697dc8e3e9443059f9abb/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java", "blob_url": "https://github.com/apache/kafka/blob/bf2c46a0b8266562bf4697dc8e3e9443059f9abb/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java", "sha": "c1532af97f2748eb08d177a583565f9eb9f465a2", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java?ref=bf2c46a0b8266562bf4697dc8e3e9443059f9abb", "patch": "@@ -292,7 +292,7 @@ public void buildAndOptimizeTopology(final Properties props) {\n \n     private void maybePerformOptimizations(final Properties props) {\n \n-        if (props != null && props.getProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION).equals(StreamsConfig.OPTIMIZE)) {\n+        if (props != null && StreamsConfig.OPTIMIZE.equals(props.getProperty(StreamsConfig.TOPOLOGY_OPTIMIZATION))) {\n             LOG.debug(\"Optimizing the Kafka Streams graph for repartition nodes\");\n             maybeOptimizeRepartitionOperations();\n         }", "filename": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/InternalStreamsBuilder.java"}, {"additions": 8, "raw_url": "https://github.com/apache/kafka/raw/bf2c46a0b8266562bf4697dc8e3e9443059f9abb/streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java", "blob_url": "https://github.com/apache/kafka/blob/bf2c46a0b8266562bf4697dc8e3e9443059f9abb/streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java", "sha": "2531591869daaad968f321dd984abee8f4dfef0c", "changes": 8, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java?ref=bf2c46a0b8266562bf4697dc8e3e9443059f9abb", "patch": "@@ -55,6 +55,14 @@\n     private final StreamsBuilder builder = new StreamsBuilder();\n     private final Properties props = StreamsTestUtils.getStreamsConfig(Serdes.String(), Serdes.String());\n \n+    @Test\n+    public void shouldNotThrowNullPointerIfOptimizationsNotSpecified() {\n+        final Properties properties = new Properties();\n+\n+        final StreamsBuilder builder = new StreamsBuilder();\n+        builder.build(properties);\n+    }\n+\n     @Test\n     public void shouldAllowJoinUnmaterializedFilteredKTable() {\n         final KTable<Bytes, String> filteredKTable = builder.<Bytes, String>table(\"table-topic\").filter(MockPredicate.<Bytes, String>allGoodPredicate());", "filename": "streams/src/test/java/org/apache/kafka/streams/StreamsBuilderTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/938580ff6c5c27b1d7a3baf9cc09029ef3c2eb68", "parent": "https://github.com/apache/kafka/commit/74e755fdb1ea98c2f90f2127160f905ad3c00486", "message": "KAFKA-7813: JmxTool throws NPE when --object-name is omitted\n\nhttps://issues.apache.org/jira/browse/KAFKA-7813\n\nRunning the JMX tool without --object-name parameter, results in a NullPointerException.\n\n*More detailed description of your change,\nif necessary. The PR title and PR message become\nthe squashed commit message, so use a separate\ncomment to ping reviewers.*\n\n*Summary of testing strategy (including rationale)\nfor the feature or bug fix. Unit and/or integration\ntests are expected for any behaviour change and\nsystem tests should be considered for larger changes.*\n\nAuthor: huxihx <huxi_2b@hotmail.com>\n\nReviewers: Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #6139 from huxihx/KAFKA-7813", "bug_id": "kafka_21", "file": [{"additions": 15, "raw_url": "https://github.com/apache/kafka/raw/938580ff6c5c27b1d7a3baf9cc09029ef3c2eb68/core/src/main/scala/kafka/tools/JmxTool.scala", "blob_url": "https://github.com/apache/kafka/blob/938580ff6c5c27b1d7a3baf9cc09029ef3c2eb68/core/src/main/scala/kafka/tools/JmxTool.scala", "sha": "9451cc2983b8c933153abb38addb9003cd069e75", "changes": 22, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/tools/JmxTool.scala?ref=938580ff6c5c27b1d7a3baf9cc09029ef3c2eb68", "patch": "@@ -18,7 +18,7 @@\n  */\n package kafka.tools\n \n-import java.util.Date\n+import java.util.{Date, Objects}\n import java.text.SimpleDateFormat\n import javax.management._\n import javax.management.remote._\n@@ -28,7 +28,7 @@ import joptsimple.OptionParser\n import scala.collection.JavaConverters._\n import scala.collection.mutable\n import scala.math._\n-import kafka.utils.{CommandLineUtils , Exit, Logging}\n+import kafka.utils.{CommandLineUtils, Exit, Logging}\n \n \n /**\n@@ -140,7 +140,7 @@ object JmxTool extends Logging {\n       else\n         List(null)\n \n-    val hasPatternQueries = queries.exists((name: ObjectName) => name.isPattern)\n+    val hasPatternQueries = queries.filterNot(Objects.isNull).exists((name: ObjectName) => name.isPattern)\n \n     var names: Iterable[ObjectName] = null\n     def namesSet = Option(names).toSet.flatten\n@@ -165,12 +165,20 @@ object JmxTool extends Logging {\n     }\n \n     val numExpectedAttributes: Map[ObjectName, Int] =\n-      if (attributesWhitelistExists)\n-        queries.map((_, attributesWhitelist.get.length)).toMap\n-      else {\n-        names.map{(name: ObjectName) =>\n+      if (!attributesWhitelistExists)\n+        names.map{name: ObjectName =>\n           val mbean = mbsc.getMBeanInfo(name)\n           (name, mbsc.getAttributes(name, mbean.getAttributes.map(_.getName)).size)}.toMap\n+      else {\n+        if (!hasPatternQueries)\n+          names.map{name: ObjectName =>\n+            val mbean = mbsc.getMBeanInfo(name)\n+            val attributes = mbsc.getAttributes(name, mbean.getAttributes.map(_.getName))\n+            val expectedAttributes = attributes.asScala.asInstanceOf[mutable.Buffer[Attribute]]\n+              .filter(attr => attributesWhitelist.get.contains(attr.getName))\n+            (name, expectedAttributes.size)}.toMap.filter(_._2 > 0)\n+        else\n+          queries.map((_, attributesWhitelist.get.length)).toMap\n       }\n \n     if(numExpectedAttributes.isEmpty) {", "filename": "core/src/main/scala/kafka/tools/JmxTool.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/70828cea49ab8a3ceb54a9017618b84e0d9c1420", "parent": "https://github.com/apache/kafka/commit/45adc80366e6e61187484d7e88467df05af2470f", "message": "KAFKA-8012; Ensure partitionStates have not been removed before truncating. (#6333)\n\nThis patch fixes a regression in the replica fetcher which occurs when the replica fetcher manager simultaneously calls `removeFetcherForPartitions`, removing the corresponding partitionStates, while a replica fetcher thread attempts to truncate the same partition(s) in `truncateToHighWatermark`. This causes an NPE which causes the fetcher to crash.\r\n\r\nThis change simply checks that the `partitionState` is not null first. Note that a similar guard exists in `truncateToEpochEndOffsets`.\r\n\r\nReviewers: Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Jason Gustafson <jason@confluent.io>", "bug_id": "kafka_22", "file": [{"additions": 17, "raw_url": "https://github.com/apache/kafka/raw/70828cea49ab8a3ceb54a9017618b84e0d9c1420/core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "blob_url": "https://github.com/apache/kafka/blob/70828cea49ab8a3ceb54a9017618b84e0d9c1420/core/src/main/scala/kafka/server/AbstractFetcherThread.scala", "sha": "3cc6137fcb732fff7c5613666b42195eaf9a9a16", "changes": 30, "status": "modified", "deletions": 13, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/AbstractFetcherThread.scala?ref=70828cea49ab8a3ceb54a9017618b84e0d9c1420", "patch": "@@ -205,23 +205,27 @@ abstract class AbstractFetcherThread(name: String,\n     }\n   }\n \n-  private def truncateToHighWatermark(partitions: Set[TopicPartition]): Unit = inLock(partitionMapLock) {\n+  // Visible for testing\n+  private[server] def truncateToHighWatermark(partitions: Set[TopicPartition]): Unit = inLock(partitionMapLock) {\n     val fetchOffsets = mutable.HashMap.empty[TopicPartition, OffsetTruncationState]\n     val partitionsWithError = mutable.HashSet.empty[TopicPartition]\n \n     for (tp <- partitions) {\n-      try {\n-        val highWatermark = partitionStates.stateValue(tp).fetchOffset\n-        val truncationState = OffsetTruncationState(highWatermark, truncationCompleted = true)\n-\n-        info(s\"Truncating partition $tp to local high watermark $highWatermark\")\n-        truncate(tp, truncationState)\n-\n-        fetchOffsets.put(tp, truncationState)\n-      } catch {\n-        case e: KafkaStorageException =>\n-          info(s\"Failed to truncate $tp\", e)\n-          partitionsWithError += tp\n+      val partitionState = partitionStates.stateValue(tp)\n+      if (partitionState != null) {\n+        try {\n+          val highWatermark = partitionState.fetchOffset\n+          val truncationState = OffsetTruncationState(highWatermark, truncationCompleted = true)\n+\n+          info(s\"Truncating partition $tp to local high watermark $highWatermark\")\n+          truncate(tp, truncationState)\n+\n+          fetchOffsets.put(tp, truncationState)\n+        } catch {\n+          case e: KafkaStorageException =>\n+            info(s\"Failed to truncate $tp\", e)\n+            partitionsWithError += tp\n+        }\n       }\n     }\n ", "filename": "core/src/main/scala/kafka/server/AbstractFetcherThread.scala"}, {"additions": 72, "raw_url": "https://github.com/apache/kafka/raw/70828cea49ab8a3ceb54a9017618b84e0d9c1420/core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala", "blob_url": "https://github.com/apache/kafka/blob/70828cea49ab8a3ceb54a9017618b84e0d9c1420/core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala", "sha": "1fc079dde5387066606bd1f808315779a75797c9", "changes": 72, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala?ref=70828cea49ab8a3ceb54a9017618b84e0d9c1420", "patch": "@@ -41,6 +41,8 @@ import scala.collection.{Map, Set, mutable}\n import scala.util.Random\n import org.scalatest.Assertions.assertThrows\n \n+import scala.collection.mutable.ArrayBuffer\n+\n class AbstractFetcherThreadTest {\n \n   @Before\n@@ -346,6 +348,34 @@ class AbstractFetcherThreadTest {\n     assertTrue(fetcher.fetchState(partition).get.isReadyForFetch)\n   }\n \n+  @Test\n+  def testTruncateToHighWatermarkDuringRemovePartitions(): Unit = {\n+    val highWatermark = 2L\n+    val partition = new TopicPartition(\"topic\", 0)\n+    val fetcher = new MockFetcherThread {\n+      override def truncateToHighWatermark(partitions: Set[TopicPartition]): Unit = {\n+        removePartitions(Set(partition))\n+        super.truncateToHighWatermark(partitions)\n+      }\n+\n+      override def latestEpoch(topicPartition: TopicPartition): Option[Int] = None\n+    }\n+\n+    val replicaLog = Seq(\n+      mkBatch(baseOffset = 0, leaderEpoch = 0, new SimpleRecord(\"a\".getBytes)),\n+      mkBatch(baseOffset = 1, leaderEpoch = 2, new SimpleRecord(\"b\".getBytes)),\n+      mkBatch(baseOffset = 2, leaderEpoch = 4, new SimpleRecord(\"c\".getBytes)))\n+\n+    val replicaState = MockFetcherThread.PartitionState(replicaLog, leaderEpoch = 5, highWatermark)\n+    fetcher.setReplicaState(partition, replicaState)\n+    fetcher.addPartitions(Map(partition -> offsetAndEpoch(highWatermark, leaderEpoch = 5)))\n+\n+    fetcher.doWork()\n+\n+    assertEquals(replicaLog.last.nextOffset(), replicaState.logEndOffset)\n+    assertTrue(fetcher.fetchState(partition).isEmpty)\n+  }\n+\n   @Test\n   def testTruncationSkippedIfNoEpochChange(): Unit = {\n     val partition = new TopicPartition(\"topic\", 0)\n@@ -630,6 +660,48 @@ class AbstractFetcherThreadTest {\n     assertEquals(fetcher.leaderPartitionState(partition).log, fetcher.replicaPartitionState(partition).log)\n   }\n \n+  @Test\n+  def testTruncateToEpochEndOffsetsDuringRemovePartitions(): Unit = {\n+    val partition = new TopicPartition(\"topic\", 0)\n+    val leaderEpochOnLeader = 0\n+    val initialLeaderEpochOnFollower = 0\n+    val nextLeaderEpochOnFollower = initialLeaderEpochOnFollower + 1\n+\n+    val fetcher = new MockFetcherThread {\n+      override def fetchEpochEndOffsets(partitions: Map[TopicPartition, EpochData]): Map[TopicPartition, EpochEndOffset] = {\n+        val fetchedEpochs = super.fetchEpochEndOffsets(partitions)\n+        // leader epoch changes while fetching epochs from leader\n+        // at the same time, the replica fetcher manager removes the partition\n+        removePartitions(Set(partition))\n+        setReplicaState(partition, MockFetcherThread.PartitionState(leaderEpoch = nextLeaderEpochOnFollower))\n+        fetchedEpochs\n+      }\n+    }\n+\n+    fetcher.setReplicaState(partition, MockFetcherThread.PartitionState(leaderEpoch = initialLeaderEpochOnFollower))\n+    fetcher.addPartitions(Map(partition -> offsetAndEpoch(0L, leaderEpoch = initialLeaderEpochOnFollower)))\n+\n+    val leaderLog = Seq(\n+      mkBatch(baseOffset = 0, leaderEpoch = initialLeaderEpochOnFollower, new SimpleRecord(\"c\".getBytes)))\n+    val leaderState = MockFetcherThread.PartitionState(leaderLog, leaderEpochOnLeader, highWatermark = 0L)\n+    fetcher.setLeaderState(partition, leaderState)\n+\n+    // first round of work\n+    fetcher.doWork()\n+\n+    // since the partition was removed before the fetched endOffsets were filtered against the leader epoch,\n+    // we do not expect the partition to be in Truncating state\n+    assertEquals(None, fetcher.fetchState(partition).map(_.state))\n+    assertEquals(None, fetcher.fetchState(partition).map(_.currentLeaderEpoch))\n+\n+    fetcher.setLeaderState(\n+      partition, MockFetcherThread.PartitionState(leaderLog, nextLeaderEpochOnFollower, highWatermark = 0L))\n+\n+    // make sure the fetcher is able to continue work\n+    fetcher.doWork()\n+    assertEquals(ArrayBuffer.empty, fetcher.replicaPartitionState(partition).log)\n+  }\n+\n   @Test\n   def testTruncationThrowsExceptionIfLeaderReturnsPartitionsNotRequestedInFetchEpochs(): Unit = {\n     val partition = new TopicPartition(\"topic\", 0)", "filename": "core/src/test/scala/unit/kafka/server/AbstractFetcherThreadTest.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/7cef37cf55353f542db3562157547d26c992e782", "parent": "https://github.com/apache/kafka/commit/9e23af3115ddfbf311f6b37c273327b3fa8e9a14", "message": "KAFKA-7324: NPE due to lack of SASLExtensions in SASL/OAUTHBEARER (#5552)\n\nSet empty extensions if null is passed in.\r\n\r\nReviewers: Satish Duggana <sduggana@hortonworks.com>, Stanislav Kozlovski <stanislav_kozlovski@outlook.com>, Rajini Sivaram <rajinisivaram@googlemail.com>", "bug_id": "kafka_23", "file": [{"additions": 4, "raw_url": "https://github.com/apache/kafka/raw/7cef37cf55353f542db3562157547d26c992e782/clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensions.java", "blob_url": "https://github.com/apache/kafka/blob/7cef37cf55353f542db3562157547d26c992e782/clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensions.java", "sha": "c129f1ec400f7e1c7637c5a9ce30310c2d0f380d", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensions.java?ref=7cef37cf55353f542db3562157547d26c992e782", "patch": "@@ -24,6 +24,10 @@\n  * A simple immutable value object class holding customizable SASL extensions\n  */\n public class SaslExtensions {\n+    /**\n+     * An \"empty\" instance indicating no SASL extensions\n+     */\n+    public static final SaslExtensions NO_SASL_EXTENSIONS = new SaslExtensions(Collections.emptyMap());\n     private final Map<String, String> extensionsMap;\n \n     public SaslExtensions(Map<String, String> extensionsMap) {", "filename": "clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensions.java"}, {"additions": 12, "raw_url": "https://github.com/apache/kafka/raw/7cef37cf55353f542db3562157547d26c992e782/clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensionsCallback.java", "blob_url": "https://github.com/apache/kafka/blob/7cef37cf55353f542db3562157547d26c992e782/clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensionsCallback.java", "sha": "c5bd449e0cc0804e9dea54813c02c8fc66829b9a", "changes": 16, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensionsCallback.java?ref=7cef37cf55353f542db3562157547d26c992e782", "patch": "@@ -17,27 +17,35 @@\n \n package org.apache.kafka.common.security.auth;\n \n+import java.util.Objects;\n+\n import javax.security.auth.callback.Callback;\n \n /**\n  * Optional callback used for SASL mechanisms if any extensions need to be set\n  * in the SASL exchange.\n  */\n public class SaslExtensionsCallback implements Callback {\n-    private SaslExtensions extensions;\n+    private SaslExtensions extensions = SaslExtensions.NO_SASL_EXTENSIONS;\n \n     /**\n-     * Returns a {@link SaslExtensions} consisting of the extension names and values that are sent by the client to\n-     * the server in the initial client SASL authentication message.\n+     * Returns always non-null {@link SaslExtensions} consisting of the extension\n+     * names and values that are sent by the client to the server in the initial\n+     * client SASL authentication message. The default value is\n+     * {@link SaslExtensions#NO_SASL_EXTENSIONS} so that if this callback is\n+     * unhandled the client will see a non-null value.\n      */\n     public SaslExtensions extensions() {\n         return extensions;\n     }\n \n     /**\n      * Sets the SASL extensions on this callback.\n+     * \n+     * @param extensions\n+     *            the mandatory extensions to set\n      */\n     public void extensions(SaslExtensions extensions) {\n-        this.extensions = extensions;\n+        this.extensions = Objects.requireNonNull(extensions, \"extensions must not be null\");\n     }\n }", "filename": "clients/src/main/java/org/apache/kafka/common/security/auth/SaslExtensionsCallback.java"}, {"additions": 58, "raw_url": "https://github.com/apache/kafka/raw/7cef37cf55353f542db3562157547d26c992e782/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponse.java", "blob_url": "https://github.com/apache/kafka/blob/7cef37cf55353f542db3562157547d26c992e782/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponse.java", "sha": "a356f0da3ddb941f6473bffaaf781f51318046d4", "changes": 63, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponse.java?ref=7cef37cf55353f542db3562157547d26c992e782", "patch": "@@ -22,6 +22,7 @@\n import javax.security.sasl.SaslException;\n import java.nio.charset.StandardCharsets;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n \n@@ -58,7 +59,9 @@ public OAuthBearerClientInitialResponse(byte[] response) throws SaslException {\n         if (auth == null)\n             throw new SaslException(\"Invalid OAUTHBEARER client first message: 'auth' not specified\");\n         properties.remove(AUTH_KEY);\n-        this.saslExtensions = validateExtensions(new SaslExtensions(properties));\n+        SaslExtensions extensions = new SaslExtensions(properties);\n+        validateExtensions(extensions);\n+        this.saslExtensions = extensions;\n \n         Matcher authMatcher = AUTH_PATTERN.matcher(auth);\n         if (!authMatcher.matches())\n@@ -71,16 +74,48 @@ public OAuthBearerClientInitialResponse(byte[] response) throws SaslException {\n         this.tokenValue = authMatcher.group(\"token\");\n     }\n \n+    /**\n+     * Constructor\n+     * \n+     * @param tokenValue\n+     *            the mandatory token value\n+     * @param extensions\n+     *            the optional extensions\n+     * @throws SaslException\n+     *             if any extension name or value fails to conform to the required\n+     *             regular expression as defined by the specification, or if the\n+     *             reserved {@code auth} appears as a key\n+     */\n     public OAuthBearerClientInitialResponse(String tokenValue, SaslExtensions extensions) throws SaslException {\n         this(tokenValue, \"\", extensions);\n     }\n \n+    /**\n+     * Constructor\n+     * \n+     * @param tokenValue\n+     *            the mandatory token value\n+     * @param authorizationId\n+     *            the optional authorization ID\n+     * @param extensions\n+     *            the optional extensions\n+     * @throws SaslException\n+     *             if any extension name or value fails to conform to the required\n+     *             regular expression as defined by the specification, or if the\n+     *             reserved {@code auth} appears as a key\n+     */\n     public OAuthBearerClientInitialResponse(String tokenValue, String authorizationId, SaslExtensions extensions) throws SaslException {\n-        this.tokenValue = tokenValue;\n+        this.tokenValue = Objects.requireNonNull(tokenValue, \"token value must not be null\");\n         this.authorizationId = authorizationId == null ? \"\" : authorizationId;\n-        this.saslExtensions = validateExtensions(extensions);\n+        validateExtensions(extensions);\n+        this.saslExtensions = extensions != null ? extensions : SaslExtensions.NO_SASL_EXTENSIONS;\n     }\n \n+    /**\n+     * Return the always non-null extensions\n+     * \n+     * @return the always non-null extensions\n+     */\n     public SaslExtensions extensions() {\n         return saslExtensions;\n     }\n@@ -97,21 +132,40 @@ public SaslExtensions extensions() {\n         return message.getBytes(StandardCharsets.UTF_8);\n     }\n \n+    /**\n+     * Return the always non-null token value\n+     * \n+     * @return the always non-null toklen value\n+     */\n     public String tokenValue() {\n         return tokenValue;\n     }\n \n+    /**\n+     * Return the always non-null authorization ID\n+     * \n+     * @return the always non-null authorization ID\n+     */\n     public String authorizationId() {\n         return authorizationId;\n     }\n \n     /**\n      * Validates that the given extensions conform to the standard. They should also not contain the reserve key name {@link OAuthBearerClientInitialResponse#AUTH_KEY}\n      *\n+     * @param extensions\n+     *            optional extensions to validate\n+     * @throws SaslException\n+     *             if any extension name or value fails to conform to the required\n+     *             regular expression as defined by the specification, or if the\n+     *             reserved {@code auth} appears as a key\n+     *\n      * @see <a href=\"https://tools.ietf.org/html/rfc7628#section-3.1\">RFC 7628,\n      *  Section 3.1</a>\n      */\n-    public static SaslExtensions validateExtensions(SaslExtensions extensions) throws SaslException {\n+    public static void validateExtensions(SaslExtensions extensions) throws SaslException {\n+        if (extensions == null)\n+            return;\n         if (extensions.map().containsKey(OAuthBearerClientInitialResponse.AUTH_KEY))\n             throw new SaslException(\"Extension name \" + OAuthBearerClientInitialResponse.AUTH_KEY + \" is invalid\");\n \n@@ -124,7 +178,6 @@ public static SaslExtensions validateExtensions(SaslExtensions extensions) throw\n             if (!EXTENSION_VALUE_PATTERN.matcher(extensionValue).matches())\n                 throw new SaslException(\"Extension value (\" + extensionValue + \") for extension \" + extensionName + \" is invalid\");\n         }\n-        return extensions;\n     }\n \n     /**", "filename": "clients/src/main/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponse.java"}, {"additions": 22, "raw_url": "https://github.com/apache/kafka/raw/7cef37cf55353f542db3562157547d26c992e782/clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponseTest.java", "blob_url": "https://github.com/apache/kafka/blob/7cef37cf55353f542db3562157547d26c992e782/clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponseTest.java", "sha": "0ba956561dfd52d59e3685fdce94b359c99d8e94", "changes": 22, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponseTest.java?ref=7cef37cf55353f542db3562157547d26c992e782", "patch": "@@ -17,6 +17,7 @@\n package org.apache.kafka.common.security.oauthbearer.internals;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n \n import org.apache.kafka.common.security.auth.SaslExtensions;\n import org.junit.Test;\n@@ -99,4 +100,25 @@ public void testRfc7688Example() throws Exception {\n         assertEquals(\"server.example.com\", response.extensions().map().get(\"host\"));\n         assertEquals(\"143\", response.extensions().map().get(\"port\"));\n     }\n+\n+    @Test\n+    public void testNoExtensionsFromByteArray() throws Exception {\n+        String message = \"n,a=user@example.com,\\u0001\" +\n+                \"auth=Bearer vF9dft4qmTc2Nvb3RlckBhbHRhdmlzdGEuY29tCg\\u0001\\u0001\";\n+        OAuthBearerClientInitialResponse response = new OAuthBearerClientInitialResponse(message.getBytes(StandardCharsets.UTF_8));\n+        assertEquals(\"vF9dft4qmTc2Nvb3RlckBhbHRhdmlzdGEuY29tCg\", response.tokenValue());\n+        assertEquals(\"user@example.com\", response.authorizationId());\n+        assertTrue(response.extensions().map().isEmpty());\n+    }\n+\n+    @Test\n+    public void testNoExtensionsFromTokenAndNullExtensions() throws Exception {\n+        OAuthBearerClientInitialResponse response = new OAuthBearerClientInitialResponse(\"token\", null);\n+        assertTrue(response.extensions().map().isEmpty());\n+    }\n+\n+    @Test\n+    public void testValidateNullExtensions() throws Exception {\n+        OAuthBearerClientInitialResponse.validateExtensions(null);\n+    }\n }", "filename": "clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerClientInitialResponseTest.java"}, {"additions": 31, "raw_url": "https://github.com/apache/kafka/raw/7cef37cf55353f542db3562157547d26c992e782/clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslClientTest.java", "blob_url": "https://github.com/apache/kafka/blob/7cef37cf55353f542db3562157547d26c992e782/clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslClientTest.java", "sha": "fad743136f33b556014a75d38514e33ea3e3f08e", "changes": 35, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslClientTest.java?ref=7cef37cf55353f542db3562157547d26c992e782", "patch": "@@ -20,8 +20,8 @@\n import org.apache.kafka.common.security.auth.SaslExtensionsCallback;\n import org.apache.kafka.common.security.auth.AuthenticateCallbackHandler;\n import org.apache.kafka.common.security.auth.SaslExtensions;\n+import org.apache.kafka.common.security.oauthbearer.OAuthBearerToken;\n import org.apache.kafka.common.security.oauthbearer.OAuthBearerTokenCallback;\n-import org.apache.kafka.common.security.oauthbearer.internals.unsecured.OAuthBearerUnsecuredJws;\n import org.easymock.EasyMockSupport;\n import org.junit.Test;\n \n@@ -30,9 +30,11 @@\n import javax.security.auth.login.AppConfigurationEntry;\n import javax.security.sasl.SaslException;\n import java.nio.charset.StandardCharsets;\n+import java.util.Collections;\n import java.util.LinkedHashMap;\n import java.util.List;\n import java.util.Map;\n+import java.util.Set;\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.fail;\n@@ -70,7 +72,32 @@ public void configure(Map<String, ?> configs, String saslMechanism, List<AppConf\n         public void handle(Callback[] callbacks) throws UnsupportedCallbackException {\n             for (Callback callback : callbacks) {\n                 if (callback instanceof OAuthBearerTokenCallback)\n-                    ((OAuthBearerTokenCallback) callback).token(createMock(OAuthBearerUnsecuredJws.class));\n+                    ((OAuthBearerTokenCallback) callback).token(new OAuthBearerToken() {\n+                        @Override\n+                        public String value() {\n+                            return \"\";\n+                        }\n+\n+                        @Override\n+                        public Set<String> scope() {\n+                            return Collections.emptySet();\n+                        }\n+\n+                        @Override\n+                        public long lifetimeMs() {\n+                            return 100;\n+                        }\n+\n+                        @Override\n+                        public String principalName() {\n+                            return \"principalName\";\n+                        }\n+\n+                        @Override\n+                        public Long startTimeMs() {\n+                            return null;\n+                        }\n+                    });\n                 else if (callback instanceof SaslExtensionsCallback) {\n                     if (toThrow)\n                         throw new ConfigException(errorMessage);\n@@ -88,7 +115,7 @@ public void close() {\n \n     @Test\n     public void testAttachesExtensionsToFirstClientMessage() throws Exception {\n-        String expectedToken = new String(new OAuthBearerClientInitialResponse(null, testExtensions).toBytes(), StandardCharsets.UTF_8);\n+        String expectedToken = new String(new OAuthBearerClientInitialResponse(\"\", testExtensions).toBytes(), StandardCharsets.UTF_8);\n \n         OAuthBearerSaslClient client = new OAuthBearerSaslClient(new ExtensionsCallbackHandler(false));\n \n@@ -101,7 +128,7 @@ public void testAttachesExtensionsToFirstClientMessage() throws Exception {\n     public void testNoExtensionsDoesNotAttachAnythingToFirstClientMessage() throws Exception {\n         TEST_PROPERTIES.clear();\n         testExtensions = new SaslExtensions(TEST_PROPERTIES);\n-        String expectedToken = new String(new OAuthBearerClientInitialResponse(null, new SaslExtensions(TEST_PROPERTIES)).toBytes(), StandardCharsets.UTF_8);\n+        String expectedToken = new String(new OAuthBearerClientInitialResponse(\"\", new SaslExtensions(TEST_PROPERTIES)).toBytes(), StandardCharsets.UTF_8);\n         OAuthBearerSaslClient client = new OAuthBearerSaslClient(new ExtensionsCallbackHandler(false));\n \n         String message = new String(client.evaluateChallenge(\"\".getBytes()), StandardCharsets.UTF_8);", "filename": "clients/src/test/java/org/apache/kafka/common/security/oauthbearer/internals/OAuthBearerSaslClientTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/6810617179222ae659343efb02ef7e6cefb15662", "parent": "https://github.com/apache/kafka/commit/d06da1b7f424ebad16ea5eca11b58b7c2ca3fa34", "message": "KAFKA-7048 NPE when creating connector (#5202)\n\nReviewers: Robert Yokota <rayokota@gmail.com>, Konstantine Karantasis <konstantine@confluent.io>, Guozhang Wang <wangguoz@gmail.com>", "bug_id": "kafka_24", "file": [{"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/6810617179222ae659343efb02ef7e6cefb15662/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java", "blob_url": "https://github.com/apache/kafka/blob/6810617179222ae659343efb02ef7e6cefb15662/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java", "sha": "7efb481ac75fb9225d33a5dd242639d86713828d", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java?ref=6810617179222ae659343efb02ef7e6cefb15662", "patch": "@@ -39,6 +39,7 @@ public WorkerConfigTransformer(Worker worker, Map<String, ConfigProvider> config\n     }\n \n     public Map<String, String> transform(String connectorName, Map<String, String> configs) {\n+        if (configs == null) return null;\n         ConfigTransformerResult result = configTransformer.transform(configs);\n         scheduleReload(connectorName, result.ttls());\n         return result.data();", "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/WorkerConfigTransformer.java"}, {"additions": 6, "raw_url": "https://github.com/apache/kafka/raw/6810617179222ae659343efb02ef7e6cefb15662/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java", "blob_url": "https://github.com/apache/kafka/blob/6810617179222ae659343efb02ef7e6cefb15662/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java", "sha": "300022de76ebd5300b56a730456cf9601b78f3d4", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java?ref=6810617179222ae659343efb02ef7e6cefb15662", "patch": "@@ -32,6 +32,7 @@\n import java.util.Set;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n import static org.powermock.api.easymock.PowerMock.replayAll;\n \n @RunWith(PowerMockRunner.class)\n@@ -110,6 +111,11 @@ public void testReplaceVariableWithTTLFirstCancelThenScheduleRestart() throws Ex\n         assertEquals(TEST_RESULT_WITH_LONGER_TTL, result.get(MY_KEY));\n     }\n \n+    @Test\n+    public void testTransformNullConfiguration() {\n+        assertNull(configTransformer.transform(MY_CONNECTOR, null));\n+    }\n+\n     public static class TestConfigProvider implements ConfigProvider {\n \n         public void configure(Map<String, ?> configs) {", "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/WorkerConfigTransformerTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/ff96d574371811c75f4f454847f67508d1de98c0", "parent": "https://github.com/apache/kafka/commit/ce7fe8fe5f8333e1e6936e2198713211bfb0dfe5", "message": "KAFKA-6860: Fix NPE in Kafka Streams with EOS enabled (#5187)\n\nReviewers: John Roesler <john@confluent.io>, Ko Byoung Kwon, Bill Bejeck <bill@confluent.io>, Guozhang Wang <guozhang@confluent.io>", "bug_id": "kafka_25", "file": [{"additions": 13, "raw_url": "https://github.com/apache/kafka/raw/ff96d574371811c75f4f454847f67508d1de98c0/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractStateManager.java", "blob_url": "https://github.com/apache/kafka/blob/ff96d574371811c75f4f454847f67508d1de98c0/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractStateManager.java", "sha": "66ddec950c8d9809f8d87ef93dcf79feba06ed6a", "changes": 22, "status": "modified", "deletions": 9, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractStateManager.java?ref=ff96d574371811c75f4f454847f67508d1de98c0", "patch": "@@ -36,17 +36,18 @@\n     static final String CHECKPOINT_FILE_NAME = \".checkpoint\";\n \n     final File baseDir;\n-    final Map<TopicPartition, Long> checkpointableOffsets = new HashMap<>();\n-\n+    private final boolean eosEnabled;\n     OffsetCheckpoint checkpoint;\n \n+    final Map<TopicPartition, Long> checkpointableOffsets = new HashMap<>();\n     final Map<String, StateStore> stores = new LinkedHashMap<>();\n     final Map<String, StateStore> globalStores = new LinkedHashMap<>();\n \n-    AbstractStateManager(final File baseDir) {\n+    AbstractStateManager(final File baseDir,\n+                         final boolean eosEnabled) {\n         this.baseDir = baseDir;\n+        this.eosEnabled = eosEnabled;\n         this.checkpoint = new OffsetCheckpoint(new File(baseDir, CHECKPOINT_FILE_NAME));\n-\n     }\n \n     public void reinitializeStateStoresForPartitions(final Logger log,\n@@ -62,11 +63,14 @@ public void reinitializeStateStoresForPartitions(final Logger log,\n             checkpointableOffsets.remove(topicPartition);\n             storeToBeReinitialized.add(changelogTopicToStore.get(topicPartition.topic()));\n         }\n-        try {\n-            checkpoint.write(checkpointableOffsets);\n-        } catch (final IOException fatalException) {\n-            log.error(\"Failed to write offset checkpoint file to {} while re-initializing {}: {}\", checkpoint, stateStores, fatalException);\n-            throw new StreamsException(\"Failed to reinitialize global store.\", fatalException);\n+\n+        if (!eosEnabled) {\n+            try {\n+                checkpoint.write(checkpointableOffsets);\n+            } catch (final IOException fatalException) {\n+                log.error(\"Failed to write offset checkpoint file to {} while re-initializing {}: {}\", checkpoint, stateStores, fatalException);\n+                throw new StreamsException(\"Failed to reinitialize global store.\", fatalException);\n+            }\n         }\n \n         for (final Map.Entry<String, StateStore> entry : storesCopy.entrySet()) {", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/AbstractStateManager.java"}, {"additions": 11, "raw_url": "https://github.com/apache/kafka/raw/ff96d574371811c75f4f454847f67508d1de98c0/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java", "blob_url": "https://github.com/apache/kafka/blob/ff96d574371811c75f4f454847f67508d1de98c0/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java", "sha": "78c4a363f29382041f7dce6229395f3e6a433e9a", "changes": 23, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java?ref=ff96d574371811c75f4f454847f67508d1de98c0", "patch": "@@ -69,7 +69,7 @@ public GlobalStateManagerImpl(final LogContext logContext,\n                                   final StateDirectory stateDirectory,\n                                   final StateRestoreListener stateRestoreListener,\n                                   final StreamsConfig config) {\n-        super(stateDirectory.globalStateDir());\n+        super(stateDirectory.globalStateDir(), StreamsConfig.EXACTLY_ONCE.equals(config.getString(StreamsConfig.PROCESSING_GUARANTEE_CONFIG)));\n \n         this.log = logContext.logger(GlobalStateManagerImpl.class);\n         this.topology = topology;\n@@ -92,16 +92,16 @@ public void setGlobalProcessorContext(final InternalProcessorContext processorCo\n             if (!stateDirectory.lockGlobalState()) {\n                 throw new LockException(String.format(\"Failed to lock the global state directory: %s\", baseDir));\n             }\n-        } catch (IOException e) {\n+        } catch (final IOException e) {\n             throw new LockException(String.format(\"Failed to lock the global state directory: %s\", baseDir));\n         }\n \n         try {\n             this.checkpointableOffsets.putAll(checkpoint.read());\n-        } catch (IOException e) {\n+        } catch (final IOException e) {\n             try {\n                 stateDirectory.unlockGlobalState();\n-            } catch (IOException e1) {\n+            } catch (final IOException e1) {\n                 log.error(\"Failed to unlock the global state directory\", e);\n             }\n             throw new StreamsException(\"Failed to read checkpoints for global state globalStores\", e);\n@@ -232,7 +232,7 @@ public void register(final StateStore store,\n         }\n \n         final List<TopicPartition> topicPartitions = new ArrayList<>();\n-        for (PartitionInfo partition : partitionInfos) {\n+        for (final PartitionInfo partition : partitionInfos) {\n             topicPartitions.add(new TopicPartition(partition.topic(), partition.partition()));\n         }\n         return topicPartitions;\n@@ -253,8 +253,7 @@ private void restoreState(final StateRestoreCallback stateRestoreCallback,\n \n             long offset = globalConsumer.position(topicPartition);\n             final Long highWatermark = highWatermarks.get(topicPartition);\n-            BatchingStateRestoreCallback\n-                stateRestoreAdapter =\n+            final BatchingStateRestoreCallback stateRestoreAdapter =\n                 (BatchingStateRestoreCallback) ((stateRestoreCallback instanceof\n                                                      BatchingStateRestoreCallback)\n                                                 ? stateRestoreCallback\n@@ -267,7 +266,7 @@ private void restoreState(final StateRestoreCallback stateRestoreCallback,\n                 try {\n                     final ConsumerRecords<byte[], byte[]> records = globalConsumer.poll(pollTime);\n                     final List<KeyValue<byte[], byte[]>> restoreRecords = new ArrayList<>();\n-                    for (ConsumerRecord<byte[], byte[]> record : records) {\n+                    for (final ConsumerRecord<byte[], byte[]> record : records) {\n                         if (record.key() != null) {\n                             restoreRecords.add(KeyValue.pair(record.key(), record.value()));\n                         }\n@@ -294,11 +293,11 @@ private void restoreState(final StateRestoreCallback stateRestoreCallback,\n     @Override\n     public void flush() {\n         log.debug(\"Flushing all global globalStores registered in the state manager\");\n-        for (StateStore store : this.globalStores.values()) {\n+        for (final StateStore store : this.globalStores.values()) {\n             try {\n                 log.trace(\"Flushing global store={}\", store.name());\n                 store.flush();\n-            } catch (Exception e) {\n+            } catch (final Exception e) {\n                 throw new ProcessorStateException(String.format(\"Failed to flush global state store %s\", store.name()), e);\n             }\n         }\n@@ -316,7 +315,7 @@ public void close(final Map<TopicPartition, Long> offsets) throws IOException {\n                 log.debug(\"Closing global storage engine {}\", entry.getKey());\n                 try {\n                     entry.getValue().close();\n-                } catch (Exception e) {\n+                } catch (final Exception e) {\n                     log.error(\"Failed to close global state store {}\", entry.getKey(), e);\n                     closeFailed.append(\"Failed to close global state store:\")\n                             .append(entry.getKey())\n@@ -341,7 +340,7 @@ public void checkpoint(final Map<TopicPartition, Long> offsets) {\n         if (!checkpointableOffsets.isEmpty()) {\n             try {\n                 checkpoint.write(checkpointableOffsets);\n-            } catch (IOException e) {\n+            } catch (final IOException e) {\n                 log.warn(\"Failed to write offset checkpoint file to {} for global stores: {}\", checkpoint, e);\n             }\n         }", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/GlobalStateManagerImpl.java"}, {"additions": 4, "raw_url": "https://github.com/apache/kafka/raw/ff96d574371811c75f4f454847f67508d1de98c0/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "blob_url": "https://github.com/apache/kafka/blob/ff96d574371811c75f4f454847f67508d1de98c0/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "sha": "afb56c1ac1b19015ede570d5a928488823e2b258", "changes": 13, "status": "modified", "deletions": 9, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java?ref=ff96d574371811c75f4f454847f67508d1de98c0", "patch": "@@ -67,7 +67,7 @@ public ProcessorStateManager(final TaskId taskId,\n                                  final ChangelogReader changelogReader,\n                                  final boolean eosEnabled,\n                                  final LogContext logContext) throws IOException {\n-        super(stateDirectory.directoryForTask(taskId));\n+        super(stateDirectory.directoryForTask(taskId), eosEnabled);\n \n         this.log = logContext.logger(ProcessorStateManager.class);\n         this.taskId = taskId;\n@@ -81,12 +81,11 @@ public ProcessorStateManager(final TaskId taskId,\n         offsetLimits = new HashMap<>();\n         standbyRestoredOffsets = new HashMap<>();\n         this.isStandby = isStandby;\n-        restoreCallbacks = isStandby ? new HashMap<String, StateRestoreCallback>() : null;\n+        restoreCallbacks = isStandby ? new HashMap<>() : null;\n         this.storeToChangelogTopic = storeToChangelogTopic;\n \n         // load the checkpoint information\n         checkpointableOffsets.putAll(checkpoint.read());\n-\n         if (eosEnabled) {\n             // delete the checkpoint file after finish loading its stored offsets\n             checkpoint.delete();\n@@ -169,11 +168,7 @@ public void reinitializeStateStoresForPartitions(final Collection<TopicPartition\n             final int partition = getPartition(topicName);\n             final TopicPartition storePartition = new TopicPartition(topicName, partition);\n \n-            if (checkpointableOffsets.containsKey(storePartition)) {\n-                partitionsAndOffsets.put(storePartition, checkpointableOffsets.get(storePartition));\n-            } else {\n-                partitionsAndOffsets.put(storePartition, -1L);\n-            }\n+            partitionsAndOffsets.put(storePartition, checkpointableOffsets.getOrDefault(storePartition, -1L));\n         }\n         return partitionsAndOffsets;\n     }\n@@ -340,7 +335,7 @@ public StateStore getGlobalStore(final String name) {\n         return globalStores.get(name);\n     }\n \n-    private BatchingStateRestoreCallback getBatchingRestoreCallback(StateRestoreCallback callback) {\n+    private BatchingStateRestoreCallback getBatchingRestoreCallback(final StateRestoreCallback callback) {\n         if (callback instanceof BatchingStateRestoreCallback) {\n             return (BatchingStateRestoreCallback) callback;\n         }", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java"}, {"additions": 39, "raw_url": "https://github.com/apache/kafka/raw/ff96d574371811c75f4f454847f67508d1de98c0/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java", "blob_url": "https://github.com/apache/kafka/blob/ff96d574371811c75f4f454847f67508d1de98c0/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java", "sha": "1b03cd4f2949aa600b52fe7267291b9b689ebf0a", "changes": 71, "status": "modified", "deletions": 32, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java?ref=ff96d574371811c75f4f454847f67508d1de98c0", "patch": "@@ -123,7 +123,7 @@ public void shouldRestoreStoreWithBatchingRestoreSpecification() throws Exceptio\n             assertThat(batchingRestoreCallback.getRestoredRecords().size(), is(1));\n             assertTrue(batchingRestoreCallback.getRestoredRecords().contains(expectedKeyValue));\n         } finally {\n-            stateMgr.close(Collections.<TopicPartition, Long>emptyMap());\n+            stateMgr.close(Collections.emptyMap());\n         }\n     }\n \n@@ -141,7 +141,7 @@ public void shouldRestoreStoreWithSinglePutRestoreSpecification() throws Excepti\n             assertThat(persistentStore.keys.size(), is(1));\n             assertTrue(persistentStore.keys.contains(intKey));\n         } finally {\n-            stateMgr.close(Collections.<TopicPartition, Long>emptyMap());\n+            stateMgr.close(Collections.emptyMap());\n         }\n     }\n \n@@ -169,7 +169,7 @@ public void testRegisterPersistentStore() throws IOException {\n             stateMgr.register(persistentStore, persistentStore.stateRestoreCallback);\n             assertTrue(changelogReader.wasRegistered(new TopicPartition(persistentStoreTopicName, 2)));\n         } finally {\n-            stateMgr.close(Collections.<TopicPartition, Long>emptyMap());\n+            stateMgr.close(Collections.emptyMap());\n         }\n     }\n \n@@ -196,7 +196,7 @@ public void testRegisterNonPersistentStore() throws IOException {\n             stateMgr.register(nonPersistentStore, nonPersistentStore.stateRestoreCallback);\n             assertTrue(changelogReader.wasRegistered(new TopicPartition(nonPersistentStoreTopicName, 2)));\n         } finally {\n-            stateMgr.close(Collections.<TopicPartition, Long>emptyMap());\n+            stateMgr.close(Collections.emptyMap());\n         }\n     }\n \n@@ -257,7 +257,7 @@ public void testChangeLogOffsets() throws IOException {\n             assertEquals(-1L, (long) changeLogOffsets.get(partition3));\n \n         } finally {\n-            stateMgr.close(Collections.<TopicPartition, Long>emptyMap());\n+            stateMgr.close(Collections.emptyMap());\n         }\n     }\n \n@@ -269,7 +269,7 @@ public void testGetStore() throws IOException {\n             noPartitions,\n             false,\n             stateDirectory,\n-            Collections.<String, String>emptyMap(),\n+            Collections.emptyMap(),\n             changelogReader,\n             false,\n             logContext);\n@@ -280,13 +280,13 @@ public void testGetStore() throws IOException {\n             assertEquals(mockStateStore, stateMgr.getStore(nonPersistentStoreName));\n \n         } finally {\n-            stateMgr.close(Collections.<TopicPartition, Long>emptyMap());\n+            stateMgr.close(Collections.emptyMap());\n         }\n     }\n \n     @Test\n     public void testFlushAndClose() throws IOException {\n-        checkpoint.write(Collections.<TopicPartition, Long>emptyMap());\n+        checkpoint.write(Collections.emptyMap());\n \n         // set up ack'ed offsets\n         final HashMap<TopicPartition, Long> ackedOffsets = new HashMap<>();\n@@ -339,7 +339,7 @@ public void shouldRegisterStoreWithoutLoggingEnabledAndNotBackedByATopic() throw\n             noPartitions,\n             false,\n             stateDirectory,\n-            Collections.<String, String>emptyMap(),\n+            Collections.emptyMap(),\n             changelogReader,\n             false,\n             logContext);\n@@ -358,7 +358,7 @@ public void shouldNotChangeOffsetsIfAckedOffsetsIsNull() throws IOException {\n             noPartitions,\n             false,\n             stateDirectory,\n-            Collections.<String, String>emptyMap(),\n+            Collections.emptyMap(),\n             changelogReader,\n             false,\n             logContext);\n@@ -408,7 +408,7 @@ public void shouldWriteCheckpointForStandbyReplica() throws IOException {\n                                                                   bytes,\n                                                                   bytes)));\n \n-        stateMgr.checkpoint(Collections.<TopicPartition, Long>emptyMap());\n+        stateMgr.checkpoint(Collections.emptyMap());\n \n         final Map<TopicPartition, Long> read = checkpoint.read();\n         assertThat(read, equalTo(Collections.singletonMap(persistentStorePartition, 889L)));\n@@ -433,7 +433,7 @@ public void shouldNotWriteCheckpointForNonPersistent() throws IOException {\n         stateMgr.checkpoint(Collections.singletonMap(topicPartition, 876L));\n \n         final Map<TopicPartition, Long> read = checkpoint.read();\n-        assertThat(read, equalTo(Collections.<TopicPartition, Long>emptyMap()));\n+        assertThat(read, equalTo(Collections.emptyMap()));\n     }\n \n     @Test\n@@ -443,7 +443,7 @@ public void shouldNotWriteCheckpointForStoresWithoutChangelogTopic() throws IOEx\n             noPartitions,\n             true, // standby\n             stateDirectory,\n-            Collections.<String, String>emptyMap(),\n+            Collections.emptyMap(),\n             changelogReader,\n             false,\n             logContext);\n@@ -453,18 +453,17 @@ public void shouldNotWriteCheckpointForStoresWithoutChangelogTopic() throws IOEx\n         stateMgr.checkpoint(Collections.singletonMap(persistentStorePartition, 987L));\n \n         final Map<TopicPartition, Long> read = checkpoint.read();\n-        assertThat(read, equalTo(Collections.<TopicPartition, Long>emptyMap()));\n+        assertThat(read, equalTo(Collections.emptyMap()));\n     }\n \n-\n     @Test\n     public void shouldThrowIllegalArgumentExceptionIfStoreNameIsSameAsCheckpointFileName() throws IOException {\n         final ProcessorStateManager stateManager = new ProcessorStateManager(\n             taskId,\n             noPartitions,\n             false,\n             stateDirectory,\n-            Collections.<String, String>emptyMap(),\n+            Collections.emptyMap(),\n             changelogReader,\n             false,\n             logContext);\n@@ -484,7 +483,7 @@ public void shouldThrowIllegalArgumentExceptionOnRegisterWhenStoreHasAlreadyBeen\n             noPartitions,\n             false,\n             stateDirectory,\n-            Collections.<String, String>emptyMap(),\n+            Collections.emptyMap(),\n             changelogReader,\n             false,\n             logContext);\n@@ -551,7 +550,7 @@ public void close() {\n         stateManager.register(stateStore, stateStore.stateRestoreCallback);\n \n         try {\n-            stateManager.close(Collections.<TopicPartition, Long>emptyMap());\n+            stateManager.close(Collections.emptyMap());\n             fail(\"Should throw ProcessorStateException if store close throws exception\");\n         } catch (final ProcessorStateException e) {\n             // pass\n@@ -623,7 +622,7 @@ public void close() {\n         stateManager.register(stateStore2, stateStore2.stateRestoreCallback);\n \n         try {\n-            stateManager.close(Collections.<TopicPartition, Long>emptyMap());\n+            stateManager.close(Collections.emptyMap());\n         } catch (final ProcessorStateException expected) { /* ignode */ }\n         Assert.assertTrue(closedStore.get());\n     }\n@@ -640,7 +639,7 @@ public void shouldDeleteCheckpointFileOnCreationIfEosEnabled() throws IOExceptio\n                 noPartitions,\n                 false,\n                 stateDirectory,\n-                Collections.<String, String>emptyMap(),\n+                Collections.emptyMap(),\n                 changelogReader,\n                 true,\n                 logContext);\n@@ -653,28 +652,36 @@ public void shouldDeleteCheckpointFileOnCreationIfEosEnabled() throws IOExceptio\n         }\n     }\n \n-    @SuppressWarnings(\"unchecked\")\n     @Test\n-    public void shouldSuccessfullyReInitializeStateStores() throws IOException {\n+    public void shouldSuccessfullyReInitializeStateStoresWithEosDisable() throws Exception {\n+        shouldSuccessfullyReInitializeStateStores(false);\n+    }\n+\n+    @Test\n+    public void shouldSuccessfullyReInitializeStateStoresWithEosEnable() throws Exception {\n+        shouldSuccessfullyReInitializeStateStores(true);\n+    }\n+\n+    private void shouldSuccessfullyReInitializeStateStores(final boolean eosEnabled) throws Exception {\n         final String store2Name = \"store2\";\n         final String store2Changelog = \"store2-changelog\";\n         final TopicPartition store2Partition = new TopicPartition(store2Changelog, 0);\n         final List<TopicPartition> changelogPartitions = Arrays.asList(changelogTopicPartition, store2Partition);\n-        Map<String, String> storeToChangelog = new HashMap() {\n+        final Map<String, String> storeToChangelog = new HashMap<String, String>() {\n             {\n                 put(storeName, changelogTopic);\n                 put(store2Name, store2Changelog);\n             }\n         };\n         final ProcessorStateManager stateManager = new ProcessorStateManager(\n-                taskId,\n-                changelogPartitions,\n-                false,\n-                stateDirectory,\n-                storeToChangelog,\n-                changelogReader,\n-                false,\n-                logContext);\n+            taskId,\n+            changelogPartitions,\n+            false,\n+            stateDirectory,\n+            storeToChangelog,\n+            changelogReader,\n+            eosEnabled,\n+            logContext);\n \n         final MockStateStore stateStore = new MockStateStore(storeName, true);\n         final MockStateStore stateStore2 = new MockStateStore(store2Name, true);\n@@ -696,7 +703,7 @@ public void register(final StateStore store, final StateRestoreCallback stateRes\n         assertTrue(stateStore2.initialized);\n     }\n \n-    private ProcessorStateManager getStandByStateManager(TaskId taskId) throws IOException {\n+    private ProcessorStateManager getStandByStateManager(final TaskId taskId) throws IOException {\n         return new ProcessorStateManager(\n             taskId,\n             noPartitions,", "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/8f90fd6530ce2c4f7e2fdfe3541c61d0178289d5", "parent": "https://github.com/apache/kafka/commit/d60f011d77ce80a44b02d43bf0889a50a8797dcd", "message": "KAFKA-5959; Fix NPE in Sender.canRetry when idempotence is not enabled\n\nAuthor: Apurva Mehta <apurva@confluent.io>\n\nReviewers: tedyu <yuzhihong@gmail.com>, Jason Gustafson <jason@confluent.io>\n\nCloses #3947 from apurvam/KAFKA-5959-npe-in-sender", "bug_id": "kafka_26", "file": [{"additions": 2, "raw_url": "https://github.com/apache/kafka/raw/8f90fd6530ce2c4f7e2fdfe3541c61d0178289d5/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java", "blob_url": "https://github.com/apache/kafka/blob/8f90fd6530ce2c4f7e2fdfe3541c61d0178289d5/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java", "sha": "45a29197b4c7ec3b801e1855ea98a25d5d55147b", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java?ref=8f90fd6530ce2c4f7e2fdfe3541c61d0178289d5", "patch": "@@ -636,7 +636,8 @@ private void failBatch(ProducerBatch batch, long baseOffset, long logAppendTime,\n      */\n     private boolean canRetry(ProducerBatch batch, ProduceResponse.PartitionResponse response) {\n         return batch.attempts() < this.retries &&\n-                ((response.error.exception() instanceof RetriableException) || transactionManager.canRetry(response, batch));\n+                ((response.error.exception() instanceof RetriableException) ||\n+                        (transactionManager != null && transactionManager.canRetry(response, batch)));\n     }\n \n     /**", "filename": "clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java"}, {"additions": 30, "raw_url": "https://github.com/apache/kafka/raw/8f90fd6530ce2c4f7e2fdfe3541c61d0178289d5/clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java", "blob_url": "https://github.com/apache/kafka/blob/8f90fd6530ce2c4f7e2fdfe3541c61d0178289d5/clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java", "sha": "ecf77aa794052a35608c31897c262416636dd5c4", "changes": 30, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java?ref=8f90fd6530ce2c4f7e2fdfe3541c61d0178289d5", "patch": "@@ -33,6 +33,7 @@\n import org.apache.kafka.common.errors.OutOfOrderSequenceException;\n import org.apache.kafka.common.errors.RecordTooLargeException;\n import org.apache.kafka.common.errors.TimeoutException;\n+import org.apache.kafka.common.errors.TopicAuthorizationException;\n import org.apache.kafka.common.errors.UnsupportedForMessageFormatException;\n import org.apache.kafka.common.errors.UnsupportedVersionException;\n import org.apache.kafka.common.internals.ClusterResourceListeners;\n@@ -500,6 +501,35 @@ public void testClusterAuthorizationExceptionInInitProducerIdRequest() throws Ex\n         assertSendFailure(ClusterAuthorizationException.class);\n     }\n \n+    @Test\n+    public void testCanRetryWithoutIdempotence() throws Exception {\n+        // do a successful retry\n+        Future<RecordMetadata> future = accumulator.append(tp0, 0L, \"key\".getBytes(), \"value\".getBytes(), null, null, MAX_BLOCK_TIMEOUT).future;\n+        sender.run(time.milliseconds()); // connect\n+        sender.run(time.milliseconds()); // send produce request\n+        String id = client.requests().peek().destination();\n+        Node node = new Node(Integer.parseInt(id), \"localhost\", 0);\n+        assertEquals(1, client.inFlightRequestCount());\n+        assertTrue(client.hasInFlightRequests());\n+        assertTrue(\"Client ready status should be true\", client.isReady(node, 0L));\n+        assertFalse(future.isDone());\n+\n+        client.respond(new MockClient.RequestMatcher() {\n+            @Override\n+            public boolean matches(AbstractRequest body) {\n+                ProduceRequest request = (ProduceRequest) body;\n+                assertFalse(request.isIdempotent());\n+                return true;\n+            }\n+        }, produceResponse(tp0, -1L, Errors.TOPIC_AUTHORIZATION_FAILED, 0));\n+        sender.run(time.milliseconds());\n+        assertTrue(future.isDone());\n+        try {\n+            future.get();\n+        } catch (Exception e) {\n+            assertTrue(e.getCause() instanceof TopicAuthorizationException);\n+        }\n+    }\n \n     @Test\n     public void testIdempotenceWithMultipleInflights() throws Exception {", "filename": "clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/e2ec2d79c8d5adefc0c764583cec47144dbc5705", "parent": "https://github.com/apache/kafka/commit/79ad9026a667469a2013ce82961c0c90f3bb0877", "message": "KAFKA-7044; Fix Fetcher.fetchOffsetsByTimes and NPE in describe consumer group (#5627)\n\nA call to `kafka-consumer-groups --describe --group ...` can result in NullPointerException for two reasons:\r\n1)  `Fetcher.fetchOffsetsByTimes()` may return too early, without sending list offsets request for topic partitions that are not in cached metadata.\r\n2) `ConsumerGroupCommand.getLogEndOffsets()` and `getLogStartOffsets()` assumed that endOffsets()/beginningOffsets() which eventually call Fetcher.fetchOffsetsByTimes(), would return a map with all the topic partitions passed to endOffsets()/beginningOffsets() and that values are not null. Because of (1), null values were possible if some of the topic partitions were already known (in metadata cache) and some not (metadata cache did not have entries for some of the topic partitions). However, even with fixing (1), endOffsets()/beginningOffsets() may return a map with some topic partitions missing, when list offset request returns a non-retriable error. This happens in corner cases such as message format on broker is before 0.10, or maybe in cases of some other errors. \r\n\r\nTesting:\r\n-- added unit test to verify fix in Fetcher.fetchOffsetsByTimes() \r\n-- did some manual testing with `kafka-consumer-groups --describe`, causing NPE. Was not able to reproduce any NPE cases with DescribeConsumerGroupTest.scala,\r\n\r\nReviewers: Jason Gustafson <jason@confluent.io>", "bug_id": "kafka_27", "file": [{"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/e2ec2d79c8d5adefc0c764583cec47144dbc5705/checkstyle/suppressions.xml", "blob_url": "https://github.com/apache/kafka/blob/e2ec2d79c8d5adefc0c764583cec47144dbc5705/checkstyle/suppressions.xml", "sha": "2fa499c21655e85a971cf4fd27c0cb76bd84a820", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/checkstyle/suppressions.xml?ref=e2ec2d79c8d5adefc0c764583cec47144dbc5705", "patch": "@@ -70,7 +70,7 @@\n               files=\"MockAdminClient.java\"/>\n \n     <suppress checks=\"JavaNCSS\"\n-              files=\"RequestResponseTest.java\"/>\n+              files=\"RequestResponseTest.java|FetcherTest.java\"/>\n \n     <suppress checks=\"NPathComplexity\"\n               files=\"MemoryRecordsTest|MetricsTest\"/>", "filename": "checkstyle/suppressions.xml"}, {"additions": 18, "raw_url": "https://github.com/apache/kafka/raw/e2ec2d79c8d5adefc0c764583cec47144dbc5705/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "blob_url": "https://github.com/apache/kafka/blob/e2ec2d79c8d5adefc0c764583cec47144dbc5705/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java", "sha": "c84dd6f9b7b5adbadcb6c3fe4358e46421c48b64", "changes": 25, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java?ref=e2ec2d79c8d5adefc0c764583cec47144dbc5705", "patch": "@@ -414,7 +414,7 @@ private ListOffsetResult fetchOffsetsByTimes(Map<TopicPartition, Long> timestamp\n                 if (value.partitionsToRetry.isEmpty())\n                     return result;\n \n-                remainingToSearch.keySet().removeAll(result.fetchedOffsets.keySet());\n+                remainingToSearch.keySet().retainAll(value.partitionsToRetry);\n             } else if (!future.isRetriable()) {\n                 throw future.exception();\n             }\n@@ -575,7 +575,7 @@ private void resetOffsetsAsync(Map<TopicPartition, Long> partitionResetTimestamp\n             metadata.add(tp.topic());\n \n         Map<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>> timestampsToSearchByNode =\n-                groupListOffsetRequests(partitionResetTimestamps);\n+                groupListOffsetRequests(partitionResetTimestamps, new HashSet<>());\n         for (Map.Entry<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>> entry : timestampsToSearchByNode.entrySet()) {\n             Node node = entry.getKey();\n             final Map<TopicPartition, ListOffsetRequest.PartitionData> resetTimestamps = entry.getValue();\n@@ -624,19 +624,19 @@ public void onFailure(RuntimeException e) {\n         for (TopicPartition tp : timestampsToSearch.keySet())\n             metadata.add(tp.topic());\n \n+        final Set<TopicPartition> partitionsToRetry = new HashSet<>();\n         Map<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>> timestampsToSearchByNode =\n-                groupListOffsetRequests(timestampsToSearch);\n+                groupListOffsetRequests(timestampsToSearch, partitionsToRetry);\n         if (timestampsToSearchByNode.isEmpty())\n             return RequestFuture.failure(new StaleMetadataException());\n \n         final RequestFuture<ListOffsetResult> listOffsetRequestsFuture = new RequestFuture<>();\n         final Map<TopicPartition, OffsetData> fetchedTimestampOffsets = new HashMap<>();\n-        final Set<TopicPartition> partitionsToRetry = new HashSet<>();\n         final AtomicInteger remainingResponses = new AtomicInteger(timestampsToSearchByNode.size());\n \n         for (Map.Entry<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>> entry : timestampsToSearchByNode.entrySet()) {\n             RequestFuture<ListOffsetResult> future =\n-                    sendListOffsetRequest(entry.getKey(), entry.getValue(), requireTimestamps);\n+                sendListOffsetRequest(entry.getKey(), entry.getValue(), requireTimestamps);\n             future.addListener(new RequestFutureListener<ListOffsetResult>() {\n                 @Override\n                 public void onSuccess(ListOffsetResult partialResult) {\n@@ -663,8 +663,16 @@ public void onFailure(RuntimeException e) {\n         return listOffsetRequestsFuture;\n     }\n \n+    /**\n+     * Groups timestamps to search by node for topic partitions in `timestampsToSearch` that have\n+     * leaders available. Topic partitions from `timestampsToSearch` that do not have their leader\n+     * available are added to `partitionsToRetry`\n+     * @param timestampsToSearch The mapping from partitions ot the target timestamps\n+     * @param partitionsToRetry A set of topic partitions that will be extended with partitions\n+     *                          that need metadata update or re-connect to the leader.\n+     */\n     private Map<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>> groupListOffsetRequests(\n-            Map<TopicPartition, Long> timestampsToSearch) {\n+            Map<TopicPartition, Long> timestampsToSearch, Set<TopicPartition> partitionsToRetry) {\n         final Map<Node, Map<TopicPartition, ListOffsetRequest.PartitionData>> timestampsToSearchByNode = new HashMap<>();\n         for (Map.Entry<TopicPartition, Long> entry: timestampsToSearch.entrySet()) {\n             TopicPartition tp  = entry.getKey();\n@@ -673,17 +681,20 @@ public void onFailure(RuntimeException e) {\n                 metadata.add(tp.topic());\n                 log.debug(\"Leader for partition {} is unknown for fetching offset\", tp);\n                 metadata.requestUpdate();\n+                partitionsToRetry.add(tp);\n             } else if (info.leader() == null) {\n                 log.debug(\"Leader for partition {} is unavailable for fetching offset\", tp);\n                 metadata.requestUpdate();\n+                partitionsToRetry.add(tp);\n             } else if (client.isUnavailable(info.leader())) {\n                 client.maybeThrowAuthFailure(info.leader());\n \n                 // The connection has failed and we need to await the blackout period before we can\n                 // try again. No need to request a metadata update since the disconnect will have\n                 // done so already.\n                 log.debug(\"Leader {} for partition {} is unavailable for fetching offset until reconnect backoff expires\",\n-                        info.leader(), tp);\n+                          info.leader(), tp);\n+                partitionsToRetry.add(tp);\n             } else {\n                 Node node = info.leader();\n                 Map<TopicPartition, ListOffsetRequest.PartitionData> topicData =", "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/Fetcher.java"}, {"additions": 43, "raw_url": "https://github.com/apache/kafka/raw/e2ec2d79c8d5adefc0c764583cec47144dbc5705/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "blob_url": "https://github.com/apache/kafka/blob/e2ec2d79c8d5adefc0c764583cec47144dbc5705/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java", "sha": "3bf3deba1aa6c69227854ce9603e7759a3ccb317", "changes": 43, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java?ref=e2ec2d79c8d5adefc0c764583cec47144dbc5705", "patch": "@@ -108,6 +108,7 @@\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n \n@@ -1938,6 +1939,48 @@ public void testGetOffsetsForTimes() {\n         testGetOffsetsForTimesWithError(Errors.BROKER_NOT_AVAILABLE, Errors.NONE, 10L, 100L, 10L, 100L);\n     }\n \n+    @Test\n+    public void testGetOffsetsForTimesWhenSomeTopicPartitionLeadersNotKnownInitially() {\n+        final String anotherTopic = \"another-topic\";\n+        final TopicPartition t2p0 = new TopicPartition(anotherTopic, 0);\n+\n+        client.reset();\n+\n+        // Metadata initially has one topic\n+        Cluster cluster = TestUtils.clusterWith(3, topicName, 2);\n+        metadata.update(cluster, Collections.<String>emptySet(), time.milliseconds());\n+\n+        // The first metadata refresh should contain one topic\n+        client.prepareMetadataUpdate(cluster, Collections.<String>emptySet(), false);\n+        client.prepareResponseFrom(listOffsetResponse(tp0, Errors.NONE, 1000L, 11L), cluster.leaderFor(tp0));\n+        client.prepareResponseFrom(listOffsetResponse(tp1, Errors.NONE, 1000L, 32L), cluster.leaderFor(tp1));\n+\n+        // Second metadata refresh should contain two topics\n+        Map<String, Integer> partitionNumByTopic = new HashMap<>();\n+        partitionNumByTopic.put(topicName, 2);\n+        partitionNumByTopic.put(anotherTopic, 1);\n+        Cluster updatedCluster = TestUtils.clusterWith(3, partitionNumByTopic);\n+        client.prepareMetadataUpdate(updatedCluster, Collections.<String>emptySet(), false);\n+        client.prepareResponseFrom(listOffsetResponse(t2p0, Errors.NONE, 1000L, 54L), cluster.leaderFor(t2p0));\n+\n+        Map<TopicPartition, Long> timestampToSearch = new HashMap<>();\n+        timestampToSearch.put(tp0, ListOffsetRequest.LATEST_TIMESTAMP);\n+        timestampToSearch.put(tp1, ListOffsetRequest.LATEST_TIMESTAMP);\n+        timestampToSearch.put(t2p0, ListOffsetRequest.LATEST_TIMESTAMP);\n+        Map<TopicPartition, OffsetAndTimestamp> offsetAndTimestampMap =\n+            fetcher.offsetsByTimes(timestampToSearch, time.timer(Long.MAX_VALUE));\n+\n+        assertNotNull(\"Expect Fetcher.offsetsByTimes() to return non-null result for \" + tp0,\n+                      offsetAndTimestampMap.get(tp0));\n+        assertNotNull(\"Expect Fetcher.offsetsByTimes() to return non-null result for \" + tp1,\n+                      offsetAndTimestampMap.get(tp1));\n+        assertNotNull(\"Expect Fetcher.offsetsByTimes() to return non-null result for \" + t2p0,\n+                      offsetAndTimestampMap.get(t2p0));\n+        assertEquals(11L, offsetAndTimestampMap.get(tp0).offset());\n+        assertEquals(32L, offsetAndTimestampMap.get(tp1).offset());\n+        assertEquals(54L, offsetAndTimestampMap.get(t2p0).offset());\n+    }\n+\n     @Test(expected = TimeoutException.class)\n     public void testBatchedListOffsetsMetadataErrors() {\n         Map<TopicPartition, ListOffsetResponse.PartitionData> partitionData = new HashMap<>();", "filename": "clients/src/test/java/org/apache/kafka/clients/consumer/internals/FetcherTest.java"}, {"additions": 10, "raw_url": "https://github.com/apache/kafka/raw/e2ec2d79c8d5adefc0c764583cec47144dbc5705/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala", "blob_url": "https://github.com/apache/kafka/blob/e2ec2d79c8d5adefc0c764583cec47144dbc5705/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala", "sha": "c0f6797eddd073a029d0042f94fb0158ff1622b2", "changes": 20, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala?ref=e2ec2d79c8d5adefc0c764583cec47144dbc5705", "patch": "@@ -292,12 +292,8 @@ object ConsumerGroupCommand extends Logging {\n       }\n \n       getLogEndOffsets(topicPartitions).map {\n-        logEndOffsetResult =>\n-          logEndOffsetResult._2 match {\n-            case LogOffsetResult.LogOffset(logEndOffset) => getDescribePartitionResult(logEndOffsetResult._1, Some(logEndOffset))\n-            case LogOffsetResult.Unknown => getDescribePartitionResult(logEndOffsetResult._1, None)\n-            case LogOffsetResult.Ignore => null\n-          }\n+        case (topicPartition, LogOffsetResult.LogOffset(offset)) => getDescribePartitionResult(topicPartition, Some(offset))\n+        case (topicPartition, _) => getDescribePartitionResult(topicPartition, None)\n       }.toArray\n     }\n \n@@ -399,16 +395,20 @@ object ConsumerGroupCommand extends Logging {\n     private def getLogEndOffsets(topicPartitions: Seq[TopicPartition]): Map[TopicPartition, LogOffsetResult] = {\n       val offsets = getConsumer.endOffsets(topicPartitions.asJava)\n       topicPartitions.map { topicPartition =>\n-        val logEndOffset = offsets.get(topicPartition)\n-        topicPartition -> LogOffsetResult.LogOffset(logEndOffset)\n+        Option(offsets.get(topicPartition)) match {\n+          case Some(logEndOffset) => topicPartition -> LogOffsetResult.LogOffset(logEndOffset)\n+          case _ => topicPartition -> LogOffsetResult.Unknown\n+        }\n       }.toMap\n     }\n \n     private def getLogStartOffsets(topicPartitions: Seq[TopicPartition]): Map[TopicPartition, LogOffsetResult] = {\n       val offsets = getConsumer.beginningOffsets(topicPartitions.asJava)\n       topicPartitions.map { topicPartition =>\n-        val logStartOffset = offsets.get(topicPartition)\n-        topicPartition -> LogOffsetResult.LogOffset(logStartOffset)\n+        Option(offsets.get(topicPartition)) match {\n+          case Some(logStartOffset) => topicPartition -> LogOffsetResult.LogOffset(logStartOffset)\n+          case _ => topicPartition -> LogOffsetResult.Unknown\n+        }\n       }.toMap\n     }\n ", "filename": "core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/e32dcb9a669bc354cc97cb45f14f0dbad9657693", "parent": "https://github.com/apache/kafka/commit/5ca9ed5ede4f03cdef54cdbce70be3fdf052157d", "message": "KAFKA-6878: NPE when querying global state store not in READY state (#4978)\n\nCheck whether cache is null before retrieving from cache.\r\n\r\nReviewers: Guozhang Wang <guozhang@confluent.io>, Bill Bejeck <bill@confluent.io>", "bug_id": "kafka_28", "file": [{"additions": 4, "raw_url": "https://github.com/apache/kafka/raw/e32dcb9a669bc354cc97cb45f14f0dbad9657693/streams/src/main/java/org/apache/kafka/streams/state/internals/CachingKeyValueStore.java", "blob_url": "https://github.com/apache/kafka/blob/e32dcb9a669bc354cc97cb45f14f0dbad9657693/streams/src/main/java/org/apache/kafka/streams/state/internals/CachingKeyValueStore.java", "sha": "16684e39c1ac2e83bbe126678688c43478093986", "changes": 5, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/CachingKeyValueStore.java?ref=e32dcb9a669bc354cc97cb45f14f0dbad9657693", "patch": "@@ -163,7 +163,10 @@ public boolean isOpen() {\n     }\n \n     private byte[] getInternal(final Bytes key) {\n-        final LRUCacheEntry entry = cache.get(cacheName, key);\n+        LRUCacheEntry entry = null;\n+        if (cache != null) {\n+            entry = cache.get(cacheName, key);\n+        }\n         if (entry == null) {\n             final byte[] rawValue = underlying.get(key);\n             if (rawValue == null) {", "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/CachingKeyValueStore.java"}, {"additions": 3, "raw_url": "https://github.com/apache/kafka/raw/e32dcb9a669bc354cc97cb45f14f0dbad9657693/streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java", "blob_url": "https://github.com/apache/kafka/blob/e32dcb9a669bc354cc97cb45f14f0dbad9657693/streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java", "sha": "1d0455b2b6f164f7c2f7739e90b2fa269b8694ff", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java?ref=e32dcb9a669bc354cc97cb45f14f0dbad9657693", "patch": "@@ -160,6 +160,9 @@ public synchronized void put(final Bytes key, final byte[] value, final long tim\n         validateStoreOpen();\n         final Bytes bytesKey = WindowKeySchema.toStoreKeyBinary(key, timestamp, 0);\n         final Bytes cacheKey = cacheFunction.cacheKey(bytesKey);\n+        if (cache == null) {\n+            return underlying.fetch(key, timestamp);\n+        }\n         final LRUCacheEntry entry = cache.get(name, cacheKey);\n         if (entry == null) {\n             return underlying.fetch(key, timestamp);", "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/CachingWindowStore.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/d06da1b7f424ebad16ea5eca11b58b7c2ca3fa34", "parent": "https://github.com/apache/kafka/commit/be846d833caade74f1d0536ecf9d540855cde758", "message": "KAFKA-7068: Handle null config values during transform (KIP-297)\n\nFix NPE when processing null config values during transform.\n\nAuthor: Robert Yokota <rayokota@gmail.com>\n\nReviewers: Magesh Nandakumar <magesh.n.kumar@gmail.com>, Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #5241 from rayokota/KIP-297-null-config-values", "bug_id": "kafka_29", "file": [{"additions": 10, "raw_url": "https://github.com/apache/kafka/raw/d06da1b7f424ebad16ea5eca11b58b7c2ca3fa34/clients/src/main/java/org/apache/kafka/common/config/ConfigTransformer.java", "blob_url": "https://github.com/apache/kafka/blob/d06da1b7f424ebad16ea5eca11b58b7c2ca3fa34/clients/src/main/java/org/apache/kafka/common/config/ConfigTransformer.java", "sha": "f5a3737d3347552f582b0d1e025e0cf3470ec53a", "changes": 15, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/config/ConfigTransformer.java?ref=d06da1b7f424ebad16ea5eca11b58b7c2ca3fa34", "patch": "@@ -80,11 +80,13 @@ public ConfigTransformerResult transform(Map<String, String> configs) {\n \n         // Collect the variables from the given configs that need transformation\n         for (Map.Entry<String, String> config : configs.entrySet()) {\n-            List<ConfigVariable> vars = getVars(config.getKey(), config.getValue(), DEFAULT_PATTERN);\n-            for (ConfigVariable var : vars) {\n-                Map<String, Set<String>> keysByPath = keysByProvider.computeIfAbsent(var.providerName, k -> new HashMap<>());\n-                Set<String> keys = keysByPath.computeIfAbsent(var.path, k -> new HashSet<>());\n-                keys.add(var.variable);\n+            if (config.getValue() != null) {\n+                List<ConfigVariable> vars = getVars(config.getKey(), config.getValue(), DEFAULT_PATTERN);\n+                for (ConfigVariable var : vars) {\n+                    Map<String, Set<String>> keysByPath = keysByProvider.computeIfAbsent(var.providerName, k -> new HashMap<>());\n+                    Set<String> keys = keysByPath.computeIfAbsent(var.path, k -> new HashSet<>());\n+                    keys.add(var.variable);\n+                }\n             }\n         }\n \n@@ -131,6 +133,9 @@ public ConfigTransformerResult transform(Map<String, String> configs) {\n     private static String replace(Map<String, Map<String, Map<String, String>>> lookupsByProvider,\n                                   String value,\n                                   Pattern pattern) {\n+        if (value == null) {\n+            return null;\n+        }\n         Matcher matcher = pattern.matcher(value);\n         StringBuilder builder = new StringBuilder();\n         int i = 0;", "filename": "clients/src/main/java/org/apache/kafka/common/config/ConfigTransformer.java"}, {"additions": 25, "raw_url": "https://github.com/apache/kafka/raw/d06da1b7f424ebad16ea5eca11b58b7c2ca3fa34/clients/src/test/java/org/apache/kafka/common/config/ConfigTransformerTest.java", "blob_url": "https://github.com/apache/kafka/blob/d06da1b7f424ebad16ea5eca11b58b7c2ca3fa34/clients/src/test/java/org/apache/kafka/common/config/ConfigTransformerTest.java", "sha": "e2b9f6b001cee3c2f1f9f2b5f576a98d5c9e6954", "changes": 26, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/config/ConfigTransformerTest.java?ref=d06da1b7f424ebad16ea5eca11b58b7c2ca3fa34", "patch": "@@ -26,6 +26,7 @@\n import java.util.Set;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n \n public class ConfigTransformerTest {\n@@ -37,6 +38,7 @@\n     public static final String TEST_PATH = \"testPath\";\n     public static final String TEST_RESULT = \"testResult\";\n     public static final String TEST_RESULT_WITH_TTL = \"testResultWithTTL\";\n+    public static final String TEST_RESULT_NO_PATH = \"testResultNoPath\";\n \n     private ConfigTransformer configTransformer;\n \n@@ -84,6 +86,24 @@ public void testSingleLevelOfIndirection() throws Exception {\n         assertEquals(\"${test:testPath:testResult}\", data.get(MY_KEY));\n     }\n \n+    @Test\n+    public void testReplaceVariableNoPath() throws Exception {\n+        ConfigTransformerResult result = configTransformer.transform(Collections.singletonMap(MY_KEY, \"${test:testKey}\"));\n+        Map<String, String> data = result.data();\n+        Map<String, Long> ttls = result.ttls();\n+        assertEquals(TEST_RESULT_NO_PATH, data.get(MY_KEY));\n+        assertTrue(ttls.isEmpty());\n+    }\n+\n+    @Test\n+    public void testNullConfigValue() throws Exception {\n+        ConfigTransformerResult result = configTransformer.transform(Collections.singletonMap(MY_KEY, null));\n+        Map<String, String> data = result.data();\n+        Map<String, Long> ttls = result.ttls();\n+        assertNull(data.get(MY_KEY));\n+        assertTrue(ttls.isEmpty());\n+    }\n+\n     public static class TestConfigProvider implements ConfigProvider {\n \n         public void configure(Map<String, ?> configs) {\n@@ -96,7 +116,7 @@ public ConfigData get(String path) {\n         public ConfigData get(String path, Set<String> keys) {\n             Map<String, String> data = new HashMap<>();\n             Long ttl = null;\n-            if (path.equals(TEST_PATH)) {\n+            if (TEST_PATH.equals(path)) {\n                 if (keys.contains(TEST_KEY)) {\n                     data.put(TEST_KEY, TEST_RESULT);\n                 }\n@@ -107,6 +127,10 @@ public ConfigData get(String path, Set<String> keys) {\n                 if (keys.contains(TEST_INDIRECTION)) {\n                     data.put(TEST_INDIRECTION, \"${test:testPath:testResult}\");\n                 }\n+            } else {\n+                if (keys.contains(TEST_KEY)) {\n+                    data.put(TEST_KEY, TEST_RESULT_NO_PATH);\n+                }\n             }\n             return new ConfigData(data, ttl);\n         }", "filename": "clients/src/test/java/org/apache/kafka/common/config/ConfigTransformerTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/22f1724123c267352116c18db1abdee25c31b382", "parent": "https://github.com/apache/kafka/commit/cb21bca400359e6b7e61555578b7539f27d9d2ff", "message": "KAFKA-7434: Fix NPE in DeadLetterQueueReporter\n\n*More detailed description of your change,\nif necessary. The PR title and PR message become\nthe squashed commit message, so use a separate\ncomment to ping reviewers.*\n\n*Summary of testing strategy (including rationale)\nfor the feature or bug fix. Unit and/or integration\ntests are expected for any behaviour change and\nsystem tests should be considered for larger changes.*\n\nAuthor: Micha\u0142 Borowiecki <mbor81@gmail.com>\n\nReviewers: Arjun Satish <arjun@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #5700 from mihbor/KAFKA-7434", "bug_id": "kafka_30", "file": [{"additions": 5, "raw_url": "https://github.com/apache/kafka/raw/22f1724123c267352116c18db1abdee25c31b382/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java", "blob_url": "https://github.com/apache/kafka/blob/22f1724123c267352116c18db1abdee25c31b382/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java", "sha": "231226997833dc10e0106d0192afcc8087eedd61", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java?ref=22f1724123c267352116c18db1abdee25c31b382", "patch": "@@ -199,6 +199,10 @@ void populateContextHeaders(ProducerRecord<byte[], byte[]> producerRecord, Proce\n     }\n \n     private byte[] toBytes(String value) {\n-        return value.getBytes(StandardCharsets.UTF_8);\n+        if (value != null) {\n+            return value.getBytes(StandardCharsets.UTF_8);\n+        } else {\n+            return null;\n+        }\n     }\n }", "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java"}, {"additions": 30, "raw_url": "https://github.com/apache/kafka/raw/22f1724123c267352116c18db1abdee25c31b382/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java", "blob_url": "https://github.com/apache/kafka/blob/22f1724123c267352116c18db1abdee25c31b382/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java", "sha": "00a922f76ad97938238c71e47070bd76d355d813", "changes": 30, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java?ref=22f1724123c267352116c18db1abdee25c31b382", "patch": "@@ -59,6 +59,7 @@\n import static org.apache.kafka.connect.runtime.errors.DeadLetterQueueReporter.ERROR_HEADER_TASK_ID;\n import static org.easymock.EasyMock.replay;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n \n @RunWith(PowerMockRunner.class)\n@@ -205,6 +206,7 @@ public void testSetDLQConfigs() {\n         assertEquals(configuration.dlqTopicReplicationFactor(), 7);\n     }\n \n+    @Test\n     public void testDlqHeaderConsumerRecord() {\n         Map<String, String> props = new HashMap<>();\n         props.put(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC);\n@@ -232,6 +234,34 @@ public void testDlqHeaderConsumerRecord() {\n         assertTrue(headerValue(producerRecord, ERROR_HEADER_EXCEPTION_STACK_TRACE).startsWith(\"org.apache.kafka.connect.errors.ConnectException: Test Exception\"));\n     }\n \n+    @Test\n+    public void testDlqHeaderOnNullExceptionMessage() {\n+        Map<String, String> props = new HashMap<>();\n+        props.put(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC);\n+        props.put(SinkConnectorConfig.DLQ_CONTEXT_HEADERS_ENABLE_CONFIG, \"true\");\n+        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(props), TASK_ID, errorHandlingMetrics);\n+\n+        ProcessingContext context = new ProcessingContext();\n+        context.consumerRecord(new ConsumerRecord<>(\"source-topic\", 7, 10, \"source-key\".getBytes(), \"source-value\".getBytes()));\n+        context.currentContext(Stage.TRANSFORMATION, Transformation.class);\n+        context.error(new NullPointerException());\n+\n+        ProducerRecord<byte[], byte[]> producerRecord = new ProducerRecord<>(DLQ_TOPIC, \"source-key\".getBytes(), \"source-value\".getBytes());\n+\n+        deadLetterQueueReporter.populateContextHeaders(producerRecord, context);\n+        assertEquals(\"source-topic\", headerValue(producerRecord, ERROR_HEADER_ORIG_TOPIC));\n+        assertEquals(\"7\", headerValue(producerRecord, ERROR_HEADER_ORIG_PARTITION));\n+        assertEquals(\"10\", headerValue(producerRecord, ERROR_HEADER_ORIG_OFFSET));\n+        assertEquals(TASK_ID.connector(), headerValue(producerRecord, ERROR_HEADER_CONNECTOR_NAME));\n+        assertEquals(String.valueOf(TASK_ID.task()), headerValue(producerRecord, ERROR_HEADER_TASK_ID));\n+        assertEquals(Stage.TRANSFORMATION.name(), headerValue(producerRecord, ERROR_HEADER_STAGE));\n+        assertEquals(Transformation.class.getName(), headerValue(producerRecord, ERROR_HEADER_EXECUTING_CLASS));\n+        assertEquals(NullPointerException.class.getName(), headerValue(producerRecord, ERROR_HEADER_EXCEPTION));\n+        assertNull(producerRecord.headers().lastHeader(ERROR_HEADER_EXCEPTION_MESSAGE).value());\n+        assertTrue(headerValue(producerRecord, ERROR_HEADER_EXCEPTION_STACK_TRACE).length() > 0);\n+        assertTrue(headerValue(producerRecord, ERROR_HEADER_EXCEPTION_STACK_TRACE).startsWith(\"java.lang.NullPointerException\"));\n+    }\n+\n     @Test\n     public void testDlqHeaderIsAppended() {\n         Map<String, String> props = new HashMap<>();", "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/783900c259511f86f5af03cbd96f2e74833447b9", "parent": "https://github.com/apache/kafka/commit/9e787716b013595851b4a6c1ddf8b8af1ec0f42e", "message": "MINOR: Guard against NPE when throwing StreamsException on serializer mismatch\n\nAuthor: Michael G. Noll <michael@confluent.io>\n\nReviewers: Damian Guy, Guozhang Wang\n\nCloses #2696 from miguno/trunk-sinknode-NPE", "bug_id": "kafka_31", "file": [{"additions": 4, "raw_url": "https://github.com/apache/kafka/raw/783900c259511f86f5af03cbd96f2e74833447b9/streams/src/main/java/org/apache/kafka/streams/processor/internals/SinkNode.java", "blob_url": "https://github.com/apache/kafka/blob/783900c259511f86f5af03cbd96f2e74833447b9/streams/src/main/java/org/apache/kafka/streams/processor/internals/SinkNode.java", "sha": "3d4f2829cfeb1f46f905ff4ce80c011ba572c222", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/SinkNode.java?ref=783900c259511f86f5af03cbd96f2e74833447b9", "patch": "@@ -77,14 +77,16 @@ public void process(final K key, final V value) {\n         try {\n             collector.send(topic, key, value, null, timestamp, keySerializer, valSerializer, partitioner);\n         } catch (ClassCastException e) {\n+            final String keyClass = key == null ? \"unknown because key is null\" : key.getClass().getName();\n+            final String valueClass = value == null ? \"unknown because value is null\" : value.getClass().getName();\n             throw new StreamsException(\n                     String.format(\"A serializer (key: %s / value: %s) is not compatible to the actual key or value type \" +\n                                     \"(key type: %s / value type: %s). Change the default Serdes in StreamConfig or \" +\n                                     \"provide correct Serdes via method parameters.\",\n                                     keySerializer.getClass().getName(),\n                                     valSerializer.getClass().getName(),\n-                                    key.getClass().getName(),\n-                                    value.getClass().getName()),\n+                                    keyClass,\n+                                    valueClass),\n                     e);\n         }\n     }", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/SinkNode.java"}, {"additions": 74, "raw_url": "https://github.com/apache/kafka/raw/783900c259511f86f5af03cbd96f2e74833447b9/streams/src/test/java/org/apache/kafka/streams/processor/internals/SinkNodeTest.java", "blob_url": "https://github.com/apache/kafka/blob/783900c259511f86f5af03cbd96f2e74833447b9/streams/src/test/java/org/apache/kafka/streams/processor/internals/SinkNodeTest.java", "sha": "dc9129ab5ade262e6fe732a9f5229fc006bd83ae", "changes": 94, "status": "modified", "deletions": 20, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/SinkNodeTest.java?ref=783900c259511f86f5af03cbd96f2e74833447b9", "patch": "@@ -20,52 +20,106 @@\n import org.apache.kafka.common.serialization.Serdes;\n import org.apache.kafka.common.serialization.Serializer;\n import org.apache.kafka.common.utils.Bytes;\n-import org.apache.kafka.streams.StreamsConfig;\n import org.apache.kafka.streams.errors.StreamsException;\n import org.apache.kafka.streams.state.StateSerdes;\n import org.apache.kafka.test.MockProcessorContext;\n import org.junit.Test;\n \n-import java.util.Properties;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.hamcrest.CoreMatchers.instanceOf;\n+import static org.hamcrest.MatcherAssert.assertThat;\n+import static org.junit.Assert.fail;\n \n public class SinkNodeTest {\n \n-    @Test(expected = StreamsException.class)\n+    @Test\n     @SuppressWarnings(\"unchecked\")\n-    public void invalidInputRecordTimestampTest() {\n+    public void shouldThrowStreamsExceptionOnInputRecordWithInvalidTimestamp() {\n+        // Given\n         final Serializer anySerializer = Serdes.Bytes().serializer();\n         final StateSerdes anyStateSerde = StateSerdes.withBuiltinTypes(\"anyName\", Bytes.class, Bytes.class);\n+        final MockProcessorContext context = new MockProcessorContext(anyStateSerde,\n+            new RecordCollectorImpl(new MockProducer<byte[], byte[]>(true, anySerializer, anySerializer), null));\n+        final SinkNode sink = new SinkNode<>(\"anyNodeName\", \"any-output-topic\", anySerializer, anySerializer, null);\n+        sink.init(context);\n+        final Bytes anyKey = new Bytes(\"any key\".getBytes());\n+        final Bytes anyValue = new Bytes(\"any value\".getBytes());\n \n-        final MockProcessorContext context = new MockProcessorContext(anyStateSerde,  new RecordCollectorImpl(null, null));\n-        context.setTime(-1);\n+        // When/Then\n+        context.setTime(-1); // ensures a negative timestamp is set for the record we send next\n+        try {\n+            sink.process(anyKey, anyValue);\n+            fail(\"Should have thrown StreamsException\");\n+        } catch (final StreamsException ignored) {\n+        }\n+    }\n \n-        final SinkNode sink = new SinkNode<>(\"name\", \"output-topic\", anySerializer, anySerializer, null);\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldThrowStreamsExceptionOnKeyValueTypeSerializerMismatch() {\n+        // Given\n+        final Serializer anySerializer = Serdes.Bytes().serializer();\n+        final StateSerdes anyStateSerde = StateSerdes.withBuiltinTypes(\"anyName\", Bytes.class, Bytes.class);\n+        final MockProcessorContext context = new MockProcessorContext(anyStateSerde,\n+            new RecordCollectorImpl(new MockProducer<byte[], byte[]>(true, anySerializer, anySerializer), null));\n+        context.setTime(0);\n+        final SinkNode sink = new SinkNode<>(\"anyNodeName\", \"any-output-topic\", anySerializer, anySerializer, null);\n         sink.init(context);\n+        final String keyOfDifferentTypeThanSerializer = \"key with different type\";\n+        final String valueOfDifferentTypeThanSerializer = \"value with different type\";\n \n-        sink.process(null, null);\n+        // When/Then\n+        try {\n+            sink.process(keyOfDifferentTypeThanSerializer, valueOfDifferentTypeThanSerializer);\n+            fail(\"Should have thrown StreamsException\");\n+        } catch (final StreamsException e) {\n+            assertThat(e.getCause(), instanceOf(ClassCastException.class));\n+        }\n     }\n \n-    @Test(expected = StreamsException.class)\n+    @Test\n     @SuppressWarnings(\"unchecked\")\n-    public void shouldThrowStreamsExceptionOnKeyValyeTypeSerializerMissmatch() {\n+    public void shouldHandleNullKeysWhenThrowingStreamsExceptionOnKeyValueTypeSerializerMismatch() {\n+        // Given\n         final Serializer anySerializer = Serdes.Bytes().serializer();\n         final StateSerdes anyStateSerde = StateSerdes.withBuiltinTypes(\"anyName\", Bytes.class, Bytes.class);\n+        final MockProcessorContext context = new MockProcessorContext(anyStateSerde,\n+            new RecordCollectorImpl(new MockProducer<byte[], byte[]>(true, anySerializer, anySerializer), null));\n+        context.setTime(1);\n+        final SinkNode sink = new SinkNode<>(\"anyNodeName\", \"any-output-topic\", anySerializer, anySerializer, null);\n+        sink.init(context);\n+        final String invalidValueToTriggerSerializerMismatch = \"\";\n \n-        Properties config = new Properties();\n-        config.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\");\n-        final MockProcessorContext context = new MockProcessorContext(anyStateSerde, new RecordCollectorImpl(new MockProducer<byte[], byte[]>(true, anySerializer, anySerializer), null));\n-        context.setTime(0);\n+        // When/Then\n+        try {\n+            sink.process(null, invalidValueToTriggerSerializerMismatch);\n+            fail(\"Should have thrown StreamsException\");\n+        } catch (final StreamsException e) {\n+            assertThat(e.getCause(), instanceOf(ClassCastException.class));\n+            assertThat(e.getMessage(), containsString(\"unknown because key is null\"));\n+        }\n+    }\n \n-        final SinkNode sink = new SinkNode<>(\"name\", \"output-topic\", anySerializer, anySerializer, null);\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void shouldHandleNullValuesWhenThrowingStreamsExceptionOnKeyValueTypeSerializerMismatch() {\n+        // Given\n+        final Serializer anySerializer = Serdes.Bytes().serializer();\n+        final StateSerdes anyStateSerde = StateSerdes.withBuiltinTypes(\"anyName\", Bytes.class, Bytes.class);\n+        final MockProcessorContext context = new MockProcessorContext(anyStateSerde,\n+            new RecordCollectorImpl(new MockProducer<byte[], byte[]>(true, anySerializer, anySerializer), null));\n+        context.setTime(1);\n+        final SinkNode sink = new SinkNode<>(\"anyNodeName\", \"any-output-topic\", anySerializer, anySerializer, null);\n         sink.init(context);\n+        final String invalidKeyToTriggerSerializerMismatch = \"\";\n \n+        // When/Then\n         try {\n-            sink.process(\"\", \"\");\n+            sink.process(invalidKeyToTriggerSerializerMismatch, null);\n+            fail(\"Should have thrown StreamsException\");\n         } catch (final StreamsException e) {\n-            if (e.getCause() instanceof ClassCastException) {\n-                throw e;\n-            }\n-            throw new RuntimeException(e);\n+            assertThat(e.getCause(), instanceOf(ClassCastException.class));\n+            assertThat(e.getMessage(), containsString(\"unknown because value is null\"));\n         }\n     }\n ", "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/SinkNodeTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/3b8b7a4be39cb4a3f7257f8a17f74a887572e627", "parent": "https://github.com/apache/kafka/commit/e41e782006d807a1ca8098dbfb95b8ab2295d6af", "message": "MINOR: Fix NPE handling unknown APIs in NodeApiVersions.toString\n\nAuthor: Jason Gustafson <jason@confluent.io>\n\nReviewers: Colin P. Mccabe <cmccabe@confluent.io>, Ismael Juma <ismael@juma.me.uk>\n\nCloses #2561 from hachikuji/fix-npe-api-version-tostring", "bug_id": "kafka_32", "file": [{"additions": 5, "raw_url": "https://github.com/apache/kafka/raw/3b8b7a4be39cb4a3f7257f8a17f74a887572e627/clients/src/main/java/org/apache/kafka/clients/NodeApiVersions.java", "blob_url": "https://github.com/apache/kafka/blob/3b8b7a4be39cb4a3f7257f8a17f74a887572e627/clients/src/main/java/org/apache/kafka/clients/NodeApiVersions.java", "sha": "906c2264c55ceb319f4767124f0f2f0eb370126f", "changes": 11, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/NodeApiVersions.java?ref=3b8b7a4be39cb4a3f7257f8a17f74a887572e627", "patch": "@@ -31,7 +31,6 @@\n  * An internal class which represents the API versions supported by a particular node.\n  */\n public class NodeApiVersions {\n-    private static final Short API_NOT_ON_NODE = null;\n     private static final short NODE_TOO_OLD = (short) -1;\n     private static final short NODE_TOO_NEW = (short) -2;\n     private final Collection<ApiVersion> nodeApiVersions;\n@@ -47,7 +46,7 @@\n      * @return A new NodeApiVersions object.\n      */\n     public static NodeApiVersions create() {\n-        return create(Collections.EMPTY_LIST);\n+        return create(Collections.<ApiVersion>emptyList());\n     }\n \n     /**\n@@ -98,7 +97,7 @@ public NodeApiVersions(Collection<ApiVersion> nodeApiVersions) {\n      */\n     public short usableVersion(ApiKeys apiKey) {\n         Short usableVersion = usableVersions.get(apiKey);\n-        if (usableVersion == API_NOT_ON_NODE)\n+        if (usableVersion == null)\n             throw new UnsupportedVersionException(\"The broker does not support \" + apiKey);\n         else if (usableVersion == NODE_TOO_OLD)\n             throw new UnsupportedVersionException(\"The broker is too old to support \" + apiKey +\n@@ -160,17 +159,17 @@ private String apiVersionToText(ApiVersion apiVersion) {\n         ApiKeys apiKey = null;\n         if (ApiKeys.hasId(apiVersion.apiKey)) {\n             apiKey = ApiKeys.forId(apiVersion.apiKey);\n-        }\n-        if (apiKey != null) {\n             bld.append(apiKey.name).append(\"(\").append(apiKey.id).append(\"): \");\n         } else {\n-            bld.append(\"UNKNOWN(\").append(apiKey.id).append(\"): \");\n+            bld.append(\"UNKNOWN(\").append(apiVersion.apiKey).append(\"): \");\n         }\n+\n         if (apiVersion.minVersion == apiVersion.maxVersion) {\n             bld.append(apiVersion.minVersion);\n         } else {\n             bld.append(apiVersion.minVersion).append(\" to \").append(apiVersion.maxVersion);\n         }\n+\n         if (apiKey != null) {\n             Short usableVersion = usableVersions.get(apiKey);\n             if (usableVersion == NODE_TOO_OLD)", "filename": "clients/src/main/java/org/apache/kafka/clients/NodeApiVersions.java"}, {"additions": 8, "raw_url": "https://github.com/apache/kafka/raw/3b8b7a4be39cb4a3f7257f8a17f74a887572e627/clients/src/test/java/org/apache/kafka/clients/NodeApiVersionsTest.java", "blob_url": "https://github.com/apache/kafka/blob/3b8b7a4be39cb4a3f7257f8a17f74a887572e627/clients/src/test/java/org/apache/kafka/clients/NodeApiVersionsTest.java", "sha": "53c47c833bfb7b91c68f761e8acfeae7225072c3", "changes": 8, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/NodeApiVersionsTest.java?ref=3b8b7a4be39cb4a3f7257f8a17f74a887572e627", "patch": "@@ -30,6 +30,7 @@\n import java.util.List;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n \n public class NodeApiVersionsTest {\n \n@@ -47,6 +48,13 @@ public void testUnsupportedVersionsToString() {\n         assertEquals(bld.toString(), versions.toString());\n     }\n \n+    @Test\n+    public void testUnknownApiVersionsToString() {\n+        ApiVersion unknownApiVersion = new ApiVersion((short) 337, (short) 0, (short) 1);\n+        NodeApiVersions versions = new NodeApiVersions(Collections.singleton(unknownApiVersion));\n+        assertTrue(versions.toString().endsWith(\"UNKNOWN(337): 0 to 1)\"));\n+    }\n+\n     @Test\n     public void testVersionsToString() {\n         List<ApiVersion> versionList = new ArrayList<>();", "filename": "clients/src/test/java/org/apache/kafka/clients/NodeApiVersionsTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/80d78f81470f109dc6d221f755b039c7332bb93b", "parent": "https://github.com/apache/kafka/commit/d4d5920ed40736d21f056188efa8a86c93e22506", "message": "HOTFIX: fix NPE in changelogger\n\nFix NPE in StoreChangeLogger caused by a record out of window retention period.\nguozhangwang\n\nAuthor: Yasuhiro Matsuda <yasuhiro@confluent.io>\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>\n\nCloses #1124 from ymatsuda/logger_npe", "bug_id": "kafka_33", "file": [{"additions": 2, "raw_url": "https://github.com/apache/kafka/raw/80d78f81470f109dc6d221f755b039c7332bb93b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBWindowStore.java", "blob_url": "https://github.com/apache/kafka/blob/80d78f81470f109dc6d221f755b039c7332bb93b/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBWindowStore.java", "sha": "9851c0489b8864932d975e81656758ce4a2dd985", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBWindowStore.java?ref=80d78f81470f109dc6d221f755b039c7332bb93b", "patch": "@@ -245,7 +245,7 @@ public void close() {\n     public void put(K key, V value) {\n         byte[] rawKey = putAndReturnInternalKey(key, value, USE_CURRENT_TIMESTAMP);\n \n-        if (loggingEnabled) {\n+        if (rawKey != null && loggingEnabled) {\n             changeLogger.add(rawKey);\n             changeLogger.maybeLogChange(this.getter);\n         }\n@@ -255,7 +255,7 @@ public void put(K key, V value) {\n     public void put(K key, V value, long timestamp) {\n         byte[] rawKey = putAndReturnInternalKey(key, value, timestamp);\n \n-        if (loggingEnabled) {\n+        if (rawKey != null && loggingEnabled) {\n             changeLogger.add(rawKey);\n             changeLogger.maybeLogChange(this.getter);\n         }", "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/RocksDBWindowStore.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/4c76b5fa6a72412efa5936c284800148c2c69c24", "parent": "https://github.com/apache/kafka/commit/2885bc33daaf75477bf39a92d1d1da02c0e03eaa", "message": "KAFKA-3629; KStreamImpl.to(...) throws NPE when the value SerDe is null\n\nguozhangwang\n\nAuthor: Damian Guy <damian.guy@gmail.com>\n\nReviewers: Matthias J. Sax, Guozhang Wang\n\nCloses #1272 from dguy/kstreamimpl-to-npe and squashes the following commits:\n\n49d48fb [Damian Guy] actually commit the fix\n07ce589 [Damian Guy] fix npe in KStreamImpl.to(..)\n74d396d [Damian Guy] fix npe in KStreamImpl.to(..)", "bug_id": "kafka_34", "file": [{"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/4c76b5fa6a72412efa5936c284800148c2c69c24/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java", "blob_url": "https://github.com/apache/kafka/blob/4c76b5fa6a72412efa5936c284800148c2c69c24/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java", "sha": "91bcef94eb35d0175ae5a82f07cf2526d90b0a09", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java?ref=4c76b5fa6a72412efa5936c284800148c2c69c24", "patch": "@@ -298,7 +298,7 @@ public void to(Serde<K> keySerde, Serde<V> valSerde, StreamPartitioner<K, V> par\n         String name = topology.newName(SINK_NAME);\n \n         Serializer<K> keySerializer = keySerde == null ? null : keySerde.serializer();\n-        Serializer<V> valSerializer = keySerde == null ? null : valSerde.serializer();\n+        Serializer<V> valSerializer = valSerde == null ? null : valSerde.serializer();\n         \n         if (partitioner == null && keySerializer != null && keySerializer instanceof WindowedSerializer) {\n             WindowedSerializer<Object> windowedSerializer = (WindowedSerializer<Object>) keySerializer;", "filename": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java"}, {"additions": 7, "raw_url": "https://github.com/apache/kafka/raw/4c76b5fa6a72412efa5936c284800148c2c69c24/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplTest.java", "blob_url": "https://github.com/apache/kafka/blob/4c76b5fa6a72412efa5936c284800148c2c69c24/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplTest.java", "sha": "3d45d1dcc8a290892b7bbb59b9e716243c6bc7c2", "changes": 7, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplTest.java?ref=4c76b5fa6a72412efa5936c284800148c2c69c24", "patch": "@@ -133,4 +133,11 @@ public Integer apply(Integer value1, Integer value2) {\n             1, // process\n             builder.build(\"X\", null).processors().size());\n     }\n+\n+    @Test\n+    public void testToWithNullValueSerdeDoesntNPE() {\n+        final KStreamBuilder builder = new KStreamBuilder();\n+        final KStream<String, String> inputStream = builder.stream(stringSerde, stringSerde, \"input\");\n+        inputStream.to(stringSerde, null, \"output\");\n+    }\n }", "filename": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/006630fd93d8efb823e5b5f7d61584138df984a6", "parent": "https://github.com/apache/kafka/commit/1949a76bc4189534b853e21c476bb11172fa3fc9", "message": "MINOR: Fix metric collection NPE during shutdown\n\nCollecting socket server metrics during shutdown may throw NullPointerException\n\nAuthor: Xavier L\u00e9aut\u00e9 <xavier@confluent.io>\n\nReviewers: Ismael Juma <ismael@juma.me.uk>\n\nCloses #2221 from xvrl/fix-metrics-npe-on-shutdown", "bug_id": "kafka_35", "file": [{"additions": 4, "raw_url": "https://github.com/apache/kafka/raw/006630fd93d8efb823e5b5f7d61584138df984a6/clients/src/main/java/org/apache/kafka/common/metrics/Metrics.java", "blob_url": "https://github.com/apache/kafka/blob/006630fd93d8efb823e5b5f7d61584138df984a6/clients/src/main/java/org/apache/kafka/common/metrics/Metrics.java", "sha": "78dad18577310a644d062873a1accadd8ad9746a", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/metrics/Metrics.java?ref=006630fd93d8efb823e5b5f7d61584138df984a6", "patch": "@@ -389,6 +389,10 @@ synchronized void registerMetric(KafkaMetric metric) {\n         return this.metrics;\n     }\n \n+    public KafkaMetric metric(MetricName metricName) {\n+        return this.metrics.get(metricName);\n+    }\n+\n     /**\n      * This iterates over every Sensor and triggers a removeSensor if it has expired\n      * Package private for testing", "filename": "clients/src/main/java/org/apache/kafka/common/metrics/Metrics.java"}, {"additions": 4, "raw_url": "https://github.com/apache/kafka/raw/006630fd93d8efb823e5b5f7d61584138df984a6/core/src/main/scala/kafka/network/SocketServer.scala", "blob_url": "https://github.com/apache/kafka/blob/006630fd93d8efb823e5b5f7d61584138df984a6/core/src/main/scala/kafka/network/SocketServer.scala", "sha": "55061edb68f2da698879a794608deea1309bb787", "changes": 7, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/network/SocketServer.scala?ref=006630fd93d8efb823e5b5f7d61584138df984a6", "patch": "@@ -105,8 +105,9 @@ class SocketServer(val config: KafkaConfig, val metrics: Metrics, val time: Time\n \n     newGauge(\"NetworkProcessorAvgIdlePercent\",\n       new Gauge[Double] {\n-        def value = allMetricNames.map( metricName =>\n-          metrics.metrics().get(metricName).value()).sum / totalProcessorThreads\n+        def value = allMetricNames.map { metricName =>\n+          Option(metrics.metric(metricName)).fold(0.0)(_.value)\n+        }.sum / totalProcessorThreads\n       }\n     )\n \n@@ -389,7 +390,7 @@ private[kafka] class Processor(val id: Int,\n   newGauge(\"IdlePercent\",\n     new Gauge[Double] {\n       def value = {\n-        metrics.metrics().get(metrics.metricName(\"io-wait-ratio\", \"socket-server-metrics\", metricTags)).value()\n+        Option(metrics.metric(metrics.metricName(\"io-wait-ratio\", \"socket-server-metrics\", metricTags))).fold(0.0)(_.value)\n       }\n     },\n     metricTags.asScala", "filename": "core/src/main/scala/kafka/network/SocketServer.scala"}, {"additions": 25, "raw_url": "https://github.com/apache/kafka/raw/006630fd93d8efb823e5b5f7d61584138df984a6/core/src/test/scala/unit/kafka/network/SocketServerTest.scala", "blob_url": "https://github.com/apache/kafka/blob/006630fd93d8efb823e5b5f7d61584138df984a6/core/src/test/scala/unit/kafka/network/SocketServerTest.scala", "sha": "c6f90ff6f30cff7e89bdbf265729c467fbaa55c9", "changes": 34, "status": "modified", "deletions": 9, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/network/SocketServerTest.scala?ref=006630fd93d8efb823e5b5f7d61584138df984a6", "patch": "@@ -17,27 +17,29 @@\n \n package kafka.network\n \n-import java.net._\n-import javax.net.ssl._\n import java.io._\n-import java.util.HashMap\n-import java.util.Random\n+import java.net._\n import java.nio.ByteBuffer\n+import java.util.{HashMap, Random}\n+import javax.net.ssl._\n \n+import com.yammer.metrics.core.Gauge\n+import com.yammer.metrics.{Metrics => YammerMetrics}\n+import kafka.server.KafkaConfig\n+import kafka.utils.TestUtils\n+import org.apache.kafka.common.TopicPartition\n import org.apache.kafka.common.metrics.Metrics\n import org.apache.kafka.common.network.NetworkSend\n import org.apache.kafka.common.protocol.{ApiKeys, SecurityProtocol}\n-import org.apache.kafka.common.security.auth.KafkaPrincipal\n-import org.apache.kafka.common.TopicPartition\n+import org.apache.kafka.common.record.MemoryRecords\n import org.apache.kafka.common.requests.{ProduceRequest, RequestHeader}\n+import org.apache.kafka.common.security.auth.KafkaPrincipal\n import org.apache.kafka.common.utils.Time\n-import kafka.server.KafkaConfig\n-import kafka.utils.TestUtils\n-import org.apache.kafka.common.record.MemoryRecords\n import org.junit.Assert._\n import org.junit._\n import org.scalatest.junit.JUnitSuite\n \n+import scala.collection.JavaConverters.mapAsScalaMapConverter\n import scala.collection.mutable.ArrayBuffer\n \n class SocketServerTest extends JUnitSuite {\n@@ -395,4 +397,18 @@ class SocketServerTest extends JUnitSuite {\n \n   }\n \n+  @Test\n+  def testMetricCollectionAfterShutdown(): Unit = {\n+    server.shutdown()\n+\n+    val sum = YammerMetrics\n+      .defaultRegistry\n+      .allMetrics.asScala\n+      .filterKeys(k => k.getName.endsWith(\"IdlePercent\") || k.getName.endsWith(\"NetworkProcessorAvgIdlePercent\"))\n+      .collect { case (_, metric: Gauge[_]) => metric.value.asInstanceOf[Double] }\n+      .sum\n+\n+    assertEquals(0, sum, 0)\n+  }\n+\n }", "filename": "core/src/test/scala/unit/kafka/network/SocketServerTest.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/e43bbce493192ea5f936867e8cdae163ab850650", "parent": "https://github.com/apache/kafka/commit/b06fc322bf008f55bb8b14c5b02c9af5a90a92ed", "message": "KAFKA-4483; Fix NPE in `Log` constructor if log level is INFO or finer\n\nAuthor: Ismael Juma <ismael@juma.me.uk>\n\nReviewers: Jason Gustafson <jason@confluent.io>\n\nCloses #2207 from ijuma/kafka-4483-npe-in-log-constructor", "bug_id": "kafka_36", "file": [{"additions": 7, "raw_url": "https://github.com/apache/kafka/raw/e43bbce493192ea5f936867e8cdae163ab850650/core/src/main/scala/kafka/log/Log.scala", "blob_url": "https://github.com/apache/kafka/blob/e43bbce493192ea5f936867e8cdae163ab850650/core/src/main/scala/kafka/log/Log.scala", "sha": "122f8be8c7430c9dd8691b383fbf4e72bb769c96", "changes": 11, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/log/Log.scala?ref=e43bbce493192ea5f936867e8cdae163ab850650", "patch": "@@ -104,19 +104,22 @@ class Log(@volatile var dir: File,\n       0\n   }\n \n+  @volatile private var nextOffsetMetadata: LogOffsetMetadata = _\n+\n   /* the actual segments of the log */\n   private val segments: ConcurrentNavigableMap[java.lang.Long, LogSegment] = new ConcurrentSkipListMap[java.lang.Long, LogSegment]\n   locally {\n     val startMs = time.milliseconds\n+\n     loadSegments()\n+    /* Calculate the offset of the next message */\n+    nextOffsetMetadata = new LogOffsetMetadata(activeSegment.nextOffset(), activeSegment.baseOffset,\n+      activeSegment.size.toInt)\n+\n     info(\"Completed load of log %s with %d log segments and log end offset %d in %d ms\"\n       .format(name, segments.size(), logEndOffset, time.milliseconds - startMs))\n   }\n \n-  /* Calculate the offset of the next message */\n-  @volatile private var nextOffsetMetadata = new LogOffsetMetadata(activeSegment.nextOffset(), activeSegment.baseOffset,\n-    activeSegment.size.toInt)\n-\n   val topicAndPartition: TopicAndPartition = Log.parseTopicPartitionName(dir)\n \n   private val tags = Map(\"topic\" -> topicAndPartition.topic, \"partition\" -> topicAndPartition.partition.toString)", "filename": "core/src/main/scala/kafka/log/Log.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/feab5a374a33a7b7b8e96c6a88b872c4db33dcf1", "parent": "https://github.com/apache/kafka/commit/f643d1b75d17bb27a378c7e66fcc49607454e445", "message": "KAFKA-3781; Errors.exceptionName() can throw NPE\n\nAuthor: Ismael Juma <ismael@juma.me.uk>\n\nReviewers: Grant Henke <granthenke@gmail.com>, Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #1476 from ijuma/kafka-3781-exception-name-npe", "bug_id": "kafka_37", "file": [{"additions": 2, "raw_url": "https://github.com/apache/kafka/raw/feab5a374a33a7b7b8e96c6a88b872c4db33dcf1/clients/src/main/java/org/apache/kafka/common/protocol/Errors.java", "blob_url": "https://github.com/apache/kafka/blob/feab5a374a33a7b7b8e96c6a88b872c4db33dcf1/clients/src/main/java/org/apache/kafka/common/protocol/Errors.java", "sha": "bd7310ba4551f1e9d2ce3ebfcdc76ec3931adf1f", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/protocol/Errors.java?ref=feab5a374a33a7b7b8e96c6a88b872c4db33dcf1", "patch": "@@ -170,10 +170,10 @@ public ApiException exception() {\n     }\n \n     /**\n-     * Returns the class name of the exception\n+     * Returns the class name of the exception or null if this is {@code Errors.NONE}.\n      */\n     public String exceptionName() {\n-        return exception.getClass().getName();\n+        return exception == null ? null : exception.getClass().getName();\n     }\n \n     /**", "filename": "clients/src/main/java/org/apache/kafka/common/protocol/Errors.java"}, {"additions": 10, "raw_url": "https://github.com/apache/kafka/raw/feab5a374a33a7b7b8e96c6a88b872c4db33dcf1/clients/src/test/java/org/apache/kafka/common/protocol/ErrorsTest.java", "blob_url": "https://github.com/apache/kafka/blob/feab5a374a33a7b7b8e96c6a88b872c4db33dcf1/clients/src/test/java/org/apache/kafka/common/protocol/ErrorsTest.java", "sha": "e198e739ac96daca7923a0fb4a64e4c98429c1aa", "changes": 10, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/protocol/ErrorsTest.java?ref=feab5a374a33a7b7b8e96c6a88b872c4db33dcf1", "patch": "@@ -77,4 +77,14 @@ public void testForExceptionDefault() {\n         assertEquals(\"forException should default to unknown\", Errors.UNKNOWN, error);\n     }\n \n+    @Test\n+    public void testExceptionName() {\n+        String exceptionName = Errors.UNKNOWN.exceptionName();\n+        assertEquals(\"org.apache.kafka.common.errors.UnknownServerException\", exceptionName);\n+        exceptionName = Errors.NONE.exceptionName();\n+        assertNull(exceptionName);\n+        exceptionName = Errors.INVALID_TOPIC_EXCEPTION.exceptionName();\n+        assertEquals(\"org.apache.kafka.common.errors.InvalidTopicException\", exceptionName);\n+    }\n+\n }", "filename": "clients/src/test/java/org/apache/kafka/common/protocol/ErrorsTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/62f03ba2cd27880562dbf85c5ef6699d25bc5b43", "parent": "https://github.com/apache/kafka/commit/e7f7d4093968d8de494e371a6d3c85e555332cbb", "message": "MINOR: Avoid duplicate processing of notifications in ZkNodeChangeNotificationListener\n\nAlso fix potential NPE.\n\nAuthor: Manikumar Reddy <manikumar.reddy@gmail.com>\n\nReviewers: Ismael Juma <ismael@juma.me.uk>\n\nCloses #3615 from omkreddy/zk-notif-duplicates", "bug_id": "kafka_38", "file": [{"additions": 5, "raw_url": "https://github.com/apache/kafka/raw/62f03ba2cd27880562dbf85c5ef6699d25bc5b43/core/src/main/scala/kafka/common/ZkNodeChangeNotificationListener.scala", "blob_url": "https://github.com/apache/kafka/blob/62f03ba2cd27880562dbf85c5ef6699d25bc5b43/core/src/main/scala/kafka/common/ZkNodeChangeNotificationListener.scala", "sha": "450707bd4cf91367fe08696856c1f268e753e1d2", "changes": 8, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/common/ZkNodeChangeNotificationListener.scala?ref=62f03ba2cd27880562dbf85c5ef6699d25bc5b43", "patch": "@@ -93,12 +93,14 @@ class ZkNodeChangeNotificationListener(private val zkUtils: ZkUtils,\n           val changeId = changeNumber(notification)\n           if (changeId > lastExecutedChange) {\n             val changeZnode = seqNodeRoot + \"/\" + notification\n-            val (data, _) = zkUtils.readDataMaybeNull(changeZnode)\n-            data.map(notificationHandler.processNotification(_)).getOrElse {\n+            val data = zkUtils.readDataMaybeNull(changeZnode)._1.orNull\n+            if (data != null) {\n+              notificationHandler.processNotification(data)\n+            } else {\n               logger.warn(s\"read null data from $changeZnode when processing notification $notification\")\n             }\n+            lastExecutedChange = changeId\n           }\n-          lastExecutedChange = changeId\n         }\n         purgeObsoleteNotifications(now, notifications)\n       } catch {", "filename": "core/src/main/scala/kafka/common/ZkNodeChangeNotificationListener.scala"}, {"additions": 17, "raw_url": "https://github.com/apache/kafka/raw/62f03ba2cd27880562dbf85c5ef6699d25bc5b43/core/src/test/scala/unit/kafka/common/ZkNodeChangeNotificationListenerTest.scala", "blob_url": "https://github.com/apache/kafka/blob/62f03ba2cd27880562dbf85c5ef6699d25bc5b43/core/src/test/scala/unit/kafka/common/ZkNodeChangeNotificationListenerTest.scala", "sha": "a646ced14eec87de628f58cf50f4a7ee0a20a2ca", "changes": 24, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/common/ZkNodeChangeNotificationListenerTest.scala?ref=62f03ba2cd27880562dbf85c5ef6699d25bc5b43", "patch": "@@ -41,21 +41,31 @@ class ZkNodeChangeNotificationListenerTest extends KafkaServerTestHarness {\n     val seqNodePath = seqNodeRoot + \"/\" + seqNodePrefix\n     val notificationMessage1 = \"message1\"\n     val notificationMessage2 = \"message2\"\n-    val changeExpirationMs = 100\n+    val changeExpirationMs = 1000\n \n     val notificationListener = new ZkNodeChangeNotificationListener(zkUtils, seqNodeRoot, seqNodePrefix, notificationHandler, changeExpirationMs)\n     notificationListener.init()\n \n     zkUtils.createSequentialPersistentPath(seqNodePath, notificationMessage1)\n \n-    TestUtils.waitUntilTrue(() => invocationCount == 1 && notification == notificationMessage1, \"failed to send/process notification message in the timeout period.\")\n+    TestUtils.waitUntilTrue(() => invocationCount == 1 && notification == notificationMessage1,\n+      \"Failed to send/process notification message in the timeout period.\")\n \n-    /*There is no easy way to test that purging. Even if we mock kafka time with MockTime, the purging compares kafka time with the time stored in zookeeper stat and the\n-    embeded zookeeper server does not provide a way to mock time. so to test purging we will have to use Time.SYSTEM.sleep(changeExpirationMs + 1) issue a write and check\n-    Assert.assertEquals(1, ZkUtils.getChildren(zkClient, seqNodeRoot).size) however even after that the assertion can fail as the second node it self can be deleted\n-    depending on how threads get scheduled.*/\n+    /*\n+     * There is no easy way to test purging. Even if we mock kafka time with MockTime, the purging compares kafka time\n+     * with the time stored in zookeeper stat and the embedded zookeeper server does not provide a way to mock time.\n+     * So to test purging we would have to use Time.SYSTEM.sleep(changeExpirationMs + 1) issue a write and check\n+     * Assert.assertEquals(1, ZkUtils.getChildren(zkClient, seqNodeRoot).size). However even that the assertion\n+     * can fail as the second node can be deleted depending on how threads get scheduled.\n+     */\n \n     zkUtils.createSequentialPersistentPath(seqNodePath, notificationMessage2)\n-    TestUtils.waitUntilTrue(() => invocationCount == 2 && notification == notificationMessage2, \"failed to send/process notification message in the timeout period.\")\n+    TestUtils.waitUntilTrue(() => invocationCount == 2 && notification == notificationMessage2,\n+      \"Failed to send/process notification message in the timeout period.\")\n+\n+    (3 to 10).foreach(i => zkUtils.createSequentialPersistentPath(seqNodePath, \"message\" + i))\n+\n+    TestUtils.waitUntilTrue(() => invocationCount == 10 ,\n+      s\"Expected 10 invocations of processNotifications, but there were $invocationCount\")\n   }\n }", "filename": "core/src/test/scala/unit/kafka/common/ZkNodeChangeNotificationListenerTest.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/b380a82d5be7c68141590467911ecb61db45ed1e", "parent": "https://github.com/apache/kafka/commit/0fba529608a5eb829feb66a499c89ead40b79694", "message": "MINOR: improve MinTimestampTrackerTest and fix NPE when null element removed\n\nAuthor: Damian Guy <damian.guy@gmail.com>\n\nReviewers: Matthias J. Sax, Guozhang Wang\n\nCloses #2611 from dguy/testing", "bug_id": "kafka_39", "file": [{"additions": 11, "raw_url": "https://github.com/apache/kafka/raw/b380a82d5be7c68141590467911ecb61db45ed1e/streams/src/main/java/org/apache/kafka/streams/processor/internals/MinTimestampTracker.java", "blob_url": "https://github.com/apache/kafka/blob/b380a82d5be7c68141590467911ecb61db45ed1e/streams/src/main/java/org/apache/kafka/streams/processor/internals/MinTimestampTracker.java", "sha": "a67675c7ad7dee54d02b9d98ddc1a32d28ffe5ca", "changes": 15, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/MinTimestampTracker.java?ref=b380a82d5be7c68141590467911ecb61db45ed1e", "patch": "@@ -34,7 +34,7 @@\n     /**\n      * @throws NullPointerException if the element is null\n      */\n-    public void addElement(Stamped<E> elem) {\n+    public void addElement(final Stamped<E> elem) {\n         if (elem == null) throw new NullPointerException();\n \n         Stamped<E> minElem = descendingSubsequence.peekLast();\n@@ -45,12 +45,19 @@ public void addElement(Stamped<E> elem) {\n         descendingSubsequence.offerLast(elem);\n     }\n \n-    public void removeElement(Stamped<E> elem) {\n-        if (elem != null && descendingSubsequence.peekFirst() == elem)\n+    public void removeElement(final Stamped<E> elem) {\n+        if (elem == null) {\n+            return;\n+        }\n+\n+        if (descendingSubsequence.peekFirst() == elem) {\n             descendingSubsequence.removeFirst();\n+        }\n \n-        if (descendingSubsequence.isEmpty())\n+        if (descendingSubsequence.isEmpty()) {\n             lastKnownTime = elem.timestamp;\n+        }\n+\n     }\n \n     public int size() {", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/MinTimestampTracker.java"}, {"additions": 44, "raw_url": "https://github.com/apache/kafka/raw/b380a82d5be7c68141590467911ecb61db45ed1e/streams/src/test/java/org/apache/kafka/streams/processor/internals/MinTimestampTrackerTest.java", "blob_url": "https://github.com/apache/kafka/blob/b380a82d5be7c68141590467911ecb61db45ed1e/streams/src/test/java/org/apache/kafka/streams/processor/internals/MinTimestampTrackerTest.java", "sha": "f6a1518066fecb384b4fd48909fd73d73ddfb876", "changes": 102, "status": "modified", "deletions": 58, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/MinTimestampTrackerTest.java?ref=b380a82d5be7c68141590467911ecb61db45ed1e", "patch": "@@ -16,77 +16,63 @@\n  */\n package org.apache.kafka.streams.processor.internals;\n \n-import static org.junit.Assert.assertEquals;\n+import static org.hamcrest.CoreMatchers.equalTo;\n+import static org.hamcrest.MatcherAssert.assertThat;\n \n import org.junit.Test;\n \n public class MinTimestampTrackerTest {\n \n-    private Stamped<String> elem(long timestamp) {\n-        return new Stamped<>(\"\", timestamp);\n-    }\n+    private MinTimestampTracker<String> tracker = new MinTimestampTracker<>();\n \n-    @SuppressWarnings(\"unchecked\")\n     @Test\n-    public void testTracking() {\n-        TimestampTracker<String> tracker = new MinTimestampTracker<>();\n-\n-        Object[] elems = new Object[]{\n-            elem(100), elem(101), elem(102), elem(98), elem(99), elem(100)\n-        };\n-\n-        int insertionIndex = 0;\n-        int removalIndex = 0;\n-\n-        // add 100\n-        tracker.addElement((Stamped<String>) elems[insertionIndex++]);\n-        assertEquals(100L, tracker.get());\n-\n-        // add 101\n-        tracker.addElement((Stamped<String>) elems[insertionIndex++]);\n-        assertEquals(100L, tracker.get());\n-\n-        // remove 100\n-        tracker.removeElement((Stamped<String>) elems[removalIndex++]);\n-        assertEquals(101L, tracker.get());\n-\n-        // add 102\n-        tracker.addElement((Stamped<String>) elems[insertionIndex++]);\n-        assertEquals(101L, tracker.get());\n-\n-        // add 98\n-        tracker.addElement((Stamped<String>) elems[insertionIndex++]);\n-        assertEquals(98L, tracker.get());\n-\n-        // add 99\n-        tracker.addElement((Stamped<String>) elems[insertionIndex++]);\n-        assertEquals(98L, tracker.get());\n-\n-        // add 100\n-        tracker.addElement((Stamped<String>) elems[insertionIndex++]);\n-        assertEquals(98L, tracker.get());\n+    public void shouldReturnNotKnownTimestampWhenNoRecordsEverAdded() throws Exception {\n+        assertThat(tracker.get(), equalTo(TimestampTracker.NOT_KNOWN));\n+    }\n \n-        // remove 101\n-        tracker.removeElement((Stamped<String>) elems[removalIndex++]);\n-        assertEquals(98L, tracker.get());\n+    @Test\n+    public void shouldReturnTimestampOfOnlyRecord() throws Exception {\n+        tracker.addElement(elem(100));\n+        assertThat(tracker.get(), equalTo(100L));\n+    }\n \n-        // remove 102\n-        tracker.removeElement((Stamped<String>) elems[removalIndex++]);\n-        assertEquals(98L, tracker.get());\n+    @Test\n+    public void shouldReturnLowestAvailableTimestampFromAllInputs() throws Exception {\n+        tracker.addElement(elem(100));\n+        tracker.addElement(elem(99));\n+        tracker.addElement(elem(102));\n+        assertThat(tracker.get(), equalTo(99L));\n+    }\n \n-        // remove 98\n-        tracker.removeElement((Stamped<String>) elems[removalIndex++]);\n-        assertEquals(99L, tracker.get());\n+    @Test\n+    public void shouldReturnLowestAvailableTimestampAfterPreviousLowestRemoved() throws Exception {\n+        final Stamped<String> lowest = elem(88);\n+        tracker.addElement(lowest);\n+        tracker.addElement(elem(101));\n+        tracker.addElement(elem(99));\n+        tracker.removeElement(lowest);\n+        assertThat(tracker.get(), equalTo(99L));\n+    }\n \n-        // remove 99\n-        tracker.removeElement((Stamped<String>) elems[removalIndex++]);\n-        assertEquals(100L, tracker.get());\n+    @Test\n+    public void shouldReturnLastKnownTimestampWhenAllElementsHaveBeenRemoved() throws Exception {\n+        final Stamped<String> record = elem(98);\n+        tracker.addElement(record);\n+        tracker.removeElement(record);\n+        assertThat(tracker.get(), equalTo(98L));\n+    }\n \n-        // remove 100\n-        tracker.removeElement((Stamped<String>) elems[removalIndex++]);\n-        assertEquals(100L, tracker.get());\n+    @Test\n+    public void shouldIgnoreNullRecordOnRemove() throws Exception {\n+        tracker.removeElement(null);\n+    }\n \n-        assertEquals(insertionIndex, removalIndex);\n+    @Test(expected = NullPointerException.class)\n+    public void shouldThrowNullPointerExceptionWhenTryingToAddNullElement() throws Exception {\n+        tracker.addElement(null);\n     }\n \n+    private Stamped<String> elem(final long timestamp) {\n+        return new Stamped<>(\"\", timestamp);\n+    }\n }\n\\ No newline at end of file", "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/MinTimestampTrackerTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/08b775c322d359e20506069d54f5b92a5a37ce63", "parent": "https://github.com/apache/kafka/commit/1110f66fa7278212160c6e8392b6e6ab09560452", "message": "KAFKA-5608; Follow-up to fix potential NPE and clarify method name\n\nAuthor: Ismael Juma <ismael@juma.me.uk>\n\nReviewers: Ewen Cheslack-Postava <me@ewencp.org>\n\nCloses #3553 from ijuma/kafka-5608-follow-up", "bug_id": "kafka_40", "file": [{"additions": 4, "raw_url": "https://github.com/apache/kafka/raw/08b775c322d359e20506069d54f5b92a5a37ce63/core/src/main/scala/kafka/tools/JmxTool.scala", "blob_url": "https://github.com/apache/kafka/blob/08b775c322d359e20506069d54f5b92a5a37ce63/core/src/main/scala/kafka/tools/JmxTool.scala", "sha": "32738215b761aee5de42cd94626f32ed41518c6c", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/tools/JmxTool.scala?ref=08b775c322d359e20506069d54f5b92a5a37ce63", "patch": "@@ -127,8 +127,8 @@ object JmxTool extends Logging {\n     val hasPatternQueries = queries.exists((name: ObjectName) => name.isPattern)\n \n     var names: Iterable[ObjectName] = null\n-    def namesSet = if (names == null) null else names.toSet\n-    def foundAllObjects() = !queries.toSet.equals(namesSet)\n+    def namesSet = Option(names).toSet.flatten\n+    def foundAllObjects = queries.toSet == namesSet\n     val waitTimeoutMs = 10000\n     if (!hasPatternQueries) {\n       val start = System.currentTimeMillis\n@@ -138,10 +138,10 @@ object JmxTool extends Logging {\n           Thread.sleep(100)\n         }\n         names = queries.flatMap((name: ObjectName) => mbsc.queryNames(name, null).asScala)\n-      } while (wait && System.currentTimeMillis - start < waitTimeoutMs && foundAllObjects)\n+      } while (wait && System.currentTimeMillis - start < waitTimeoutMs && !foundAllObjects)\n     }\n \n-    if (wait && foundAllObjects) {\n+    if (wait && !foundAllObjects) {\n       val missing = (queries.toSet - namesSet).mkString(\", \")\n       System.err.println(s\"Could not find all requested object names after $waitTimeoutMs ms. Missing $missing\")\n       System.err.println(\"Exiting.\")", "filename": "core/src/main/scala/kafka/tools/JmxTool.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/70d882861e1bf3eb503c84a31834e8b628de2df9", "parent": "https://github.com/apache/kafka/commit/596c6c0c0b27c13f2017c770ea37cd39e27e5dcf", "message": "KAFKA-7228: Set errorHandlingMetrics for dead letter queue\n\nDLQ reporter does not get a `errorHandlingMetrics` object when created by the worker. This results in an NPE.\n\nSigned-off-by: Arjun Satish <arjunconfluent.io>\n\n*More detailed description of your change,\nif necessary. The PR title and PR message become\nthe squashed commit message, so use a separate\ncomment to ping reviewers.*\n\n*Summary of testing strategy (including rationale)\nfor the feature or bug fix. Unit and/or integration\ntests are expected for any behaviour change and\nsystem tests should be considered for larger changes.*\n\nAuthor: Arjun Satish <arjun@confluent.io>\n\nReviewers: Konstantine Karantasis <konstantine@confluent.io>, Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #5440 from wicknicks/KAFKA-7228", "bug_id": "kafka_41", "file": [{"additions": 3, "raw_url": "https://github.com/apache/kafka/raw/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java", "blob_url": "https://github.com/apache/kafka/blob/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java", "sha": "df73a434d31b2859ba83a05348bf5b835d9211e0", "changes": 8, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java?ref=70d882861e1bf3eb503c84a31834e8b628de2df9", "patch": "@@ -523,14 +523,13 @@ ErrorHandlingMetrics errorHandlingMetrics(ConnectorTaskId id) {\n     private List<ErrorReporter> sinkTaskReporters(ConnectorTaskId id, SinkConnectorConfig connConfig,\n                                                   ErrorHandlingMetrics errorHandlingMetrics) {\n         ArrayList<ErrorReporter> reporters = new ArrayList<>();\n-        LogReporter logReporter = new LogReporter(id, connConfig);\n-        logReporter.metrics(errorHandlingMetrics);\n+        LogReporter logReporter = new LogReporter(id, connConfig, errorHandlingMetrics);\n         reporters.add(logReporter);\n \n         // check if topic for dead letter queue exists\n         String topic = connConfig.dlqTopicName();\n         if (topic != null && !topic.isEmpty()) {\n-            DeadLetterQueueReporter reporter = DeadLetterQueueReporter.createAndSetup(config, id, connConfig, producerProps);\n+            DeadLetterQueueReporter reporter = DeadLetterQueueReporter.createAndSetup(config, id, connConfig, producerProps, errorHandlingMetrics);\n             reporters.add(reporter);\n         }\n \n@@ -540,8 +539,7 @@ ErrorHandlingMetrics errorHandlingMetrics(ConnectorTaskId id) {\n     private List<ErrorReporter> sourceTaskReporters(ConnectorTaskId id, ConnectorConfig connConfig,\n                                                       ErrorHandlingMetrics errorHandlingMetrics) {\n         List<ErrorReporter> reporters = new ArrayList<>();\n-        LogReporter logReporter = new LogReporter(id, connConfig);\n-        logReporter.metrics(errorHandlingMetrics);\n+        LogReporter logReporter = new LogReporter(id, connConfig, errorHandlingMetrics);\n         reporters.add(logReporter);\n \n         return reporters;", "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java"}, {"additions": 12, "raw_url": "https://github.com/apache/kafka/raw/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java", "blob_url": "https://github.com/apache/kafka/blob/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java", "sha": "c059dcff793a0ec18e3287d35fd17c7266407f63", "changes": 20, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java?ref=70d882861e1bf3eb503c84a31834e8b628de2df9", "patch": "@@ -36,6 +36,7 @@\n import java.io.PrintStream;\n import java.nio.charset.StandardCharsets;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.concurrent.ExecutionException;\n \n import static java.util.Collections.singleton;\n@@ -66,13 +67,14 @@\n \n     private final SinkConnectorConfig connConfig;\n     private final ConnectorTaskId connectorTaskId;\n+    private final ErrorHandlingMetrics errorHandlingMetrics;\n \n     private KafkaProducer<byte[], byte[]> kafkaProducer;\n-    private ErrorHandlingMetrics errorHandlingMetrics;\n \n     public static DeadLetterQueueReporter createAndSetup(WorkerConfig workerConfig,\n                                                          ConnectorTaskId id,\n-                                                         SinkConnectorConfig sinkConfig, Map<String, Object> producerProps) {\n+                                                         SinkConnectorConfig sinkConfig, Map<String, Object> producerProps,\n+                                                         ErrorHandlingMetrics errorHandlingMetrics) {\n         String topic = sinkConfig.dlqTopicName();\n \n         try (AdminClient admin = AdminClient.create(workerConfig.originals())) {\n@@ -90,7 +92,7 @@ public static DeadLetterQueueReporter createAndSetup(WorkerConfig workerConfig,\n         }\n \n         KafkaProducer<byte[], byte[]> dlqProducer = new KafkaProducer<>(producerProps);\n-        return new DeadLetterQueueReporter(dlqProducer, sinkConfig, id);\n+        return new DeadLetterQueueReporter(dlqProducer, sinkConfig, id, errorHandlingMetrics);\n     }\n \n     /**\n@@ -99,14 +101,16 @@ public static DeadLetterQueueReporter createAndSetup(WorkerConfig workerConfig,\n      * @param kafkaProducer a Kafka Producer to produce the original consumed records.\n      */\n     // Visible for testing\n-    DeadLetterQueueReporter(KafkaProducer<byte[], byte[]> kafkaProducer, SinkConnectorConfig connConfig, ConnectorTaskId id) {\n+    DeadLetterQueueReporter(KafkaProducer<byte[], byte[]> kafkaProducer, SinkConnectorConfig connConfig,\n+                            ConnectorTaskId id, ErrorHandlingMetrics errorHandlingMetrics) {\n+        Objects.requireNonNull(kafkaProducer);\n+        Objects.requireNonNull(connConfig);\n+        Objects.requireNonNull(id);\n+        Objects.requireNonNull(errorHandlingMetrics);\n+\n         this.kafkaProducer = kafkaProducer;\n         this.connConfig = connConfig;\n         this.connectorTaskId = id;\n-    }\n-\n-    @Override\n-    public void metrics(ErrorHandlingMetrics errorHandlingMetrics) {\n         this.errorHandlingMetrics = errorHandlingMetrics;\n     }\n ", "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/DeadLetterQueueReporter.java"}, {"additions": 0, "raw_url": "https://github.com/apache/kafka/raw/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java", "blob_url": "https://github.com/apache/kafka/blob/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java", "sha": "58336163fbf4a84176a77b5d2716143640dc992b", "changes": 8, "status": "modified", "deletions": 8, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java?ref=70d882861e1bf3eb503c84a31834e8b628de2df9", "patch": "@@ -28,12 +28,4 @@\n      */\n     void report(ProcessingContext context);\n \n-    /**\n-     * Provides the container for error handling metrics to implementations. This method will be called once the error\n-     * reporter object is instantiated.\n-     *\n-     * @param errorHandlingMetrics metrics for error handling (cannot be null).\n-     */\n-    void metrics(ErrorHandlingMetrics errorHandlingMetrics);\n-\n }", "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/ErrorReporter.java"}, {"additions": 8, "raw_url": "https://github.com/apache/kafka/raw/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java", "blob_url": "https://github.com/apache/kafka/blob/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java", "sha": "8b07adf8e499288c76041f515a7d5897006ee65f", "changes": 15, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java?ref=70d882861e1bf3eb503c84a31834e8b628de2df9", "patch": "@@ -21,6 +21,8 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+import java.util.Objects;\n+\n /**\n  * Writes errors and their context to application logs.\n  */\n@@ -30,12 +32,16 @@\n \n     private final ConnectorTaskId id;\n     private final ConnectorConfig connConfig;\n+    private final ErrorHandlingMetrics errorHandlingMetrics;\n \n-    private ErrorHandlingMetrics errorHandlingMetrics;\n+    public LogReporter(ConnectorTaskId id, ConnectorConfig connConfig, ErrorHandlingMetrics errorHandlingMetrics) {\n+        Objects.requireNonNull(id);\n+        Objects.requireNonNull(connConfig);\n+        Objects.requireNonNull(errorHandlingMetrics);\n \n-    public LogReporter(ConnectorTaskId id, ConnectorConfig connConfig) {\n         this.id = id;\n         this.connConfig = connConfig;\n+        this.errorHandlingMetrics = errorHandlingMetrics;\n     }\n \n     /**\n@@ -57,11 +63,6 @@ public void report(ProcessingContext context) {\n         errorHandlingMetrics.recordErrorLogged();\n     }\n \n-    @Override\n-    public void metrics(ErrorHandlingMetrics errorHandlingMetrics) {\n-        this.errorHandlingMetrics = errorHandlingMetrics;\n-    }\n-\n     // Visible for testing\n     String message(ProcessingContext context) {\n         return String.format(\"Error encountered in task %s. %s\", String.valueOf(id),", "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/errors/LogReporter.java"}, {"additions": 3, "raw_url": "https://github.com/apache/kafka/raw/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java", "blob_url": "https://github.com/apache/kafka/blob/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java", "sha": "1bf9c717068e3098da95d231e665f74c782fc0fc", "changes": 9, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java?ref=70d882861e1bf3eb503c84a31834e8b628de2df9", "patch": "@@ -166,8 +166,7 @@ public void testErrorHandlingInSinkTasks() throws Exception {\n         Map<String, String> reportProps = new HashMap<>();\n         reportProps.put(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, \"true\");\n         reportProps.put(ConnectorConfig.ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, \"true\");\n-        LogReporter reporter = new LogReporter(taskId, connConfig(reportProps));\n-        reporter.metrics(errorHandlingMetrics);\n+        LogReporter reporter = new LogReporter(taskId, connConfig(reportProps), errorHandlingMetrics);\n \n         RetryWithToleranceOperator retryWithToleranceOperator = operator();\n         retryWithToleranceOperator.metrics(errorHandlingMetrics);\n@@ -218,8 +217,7 @@ public void testErrorHandlingInSourceTasks() throws Exception {\n         Map<String, String> reportProps = new HashMap<>();\n         reportProps.put(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, \"true\");\n         reportProps.put(ConnectorConfig.ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, \"true\");\n-        LogReporter reporter = new LogReporter(taskId, connConfig(reportProps));\n-        reporter.metrics(errorHandlingMetrics);\n+        LogReporter reporter = new LogReporter(taskId, connConfig(reportProps), errorHandlingMetrics);\n \n         RetryWithToleranceOperator retryWithToleranceOperator = operator();\n         retryWithToleranceOperator.metrics(errorHandlingMetrics);\n@@ -283,8 +281,7 @@ public void testErrorHandlingInSourceTasksWthBadConverter() throws Exception {\n         Map<String, String> reportProps = new HashMap<>();\n         reportProps.put(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, \"true\");\n         reportProps.put(ConnectorConfig.ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, \"true\");\n-        LogReporter reporter = new LogReporter(taskId, connConfig(reportProps));\n-        reporter.metrics(errorHandlingMetrics);\n+        LogReporter reporter = new LogReporter(taskId, connConfig(reportProps), errorHandlingMetrics);\n \n         RetryWithToleranceOperator retryWithToleranceOperator = operator();\n         retryWithToleranceOperator.metrics(errorHandlingMetrics);", "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/ErrorHandlingTaskTest.java"}, {"additions": 17, "raw_url": "https://github.com/apache/kafka/raw/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java", "blob_url": "https://github.com/apache/kafka/blob/70d882861e1bf3eb503c84a31834e8b628de2df9/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java", "sha": "fa628b0984080b2f61b1cafecacf0f57a982d891", "changes": 33, "status": "modified", "deletions": 16, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java?ref=70d882861e1bf3eb503c84a31834e8b628de2df9", "patch": "@@ -94,10 +94,15 @@ public void tearDown() {\n         }\n     }\n \n+    @Test(expected = NullPointerException.class)\n+    public void initializeDLQWithNullMetrics() {\n+        new DeadLetterQueueReporter(producer, config(emptyMap()), TASK_ID, null);\n+    }\n+\n     @Test\n     public void testDLQConfigWithEmptyTopicName() {\n-        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(emptyMap()), TASK_ID);\n-        deadLetterQueueReporter.metrics(errorHandlingMetrics);\n+        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(\n+                producer, config(emptyMap()), TASK_ID, errorHandlingMetrics);\n \n         ProcessingContext context = processingContext();\n \n@@ -111,8 +116,8 @@ public void testDLQConfigWithEmptyTopicName() {\n \n     @Test\n     public void testDLQConfigWithValidTopicName() {\n-        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(singletonMap(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC)), TASK_ID);\n-        deadLetterQueueReporter.metrics(errorHandlingMetrics);\n+        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(\n+                producer, config(singletonMap(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC)), TASK_ID, errorHandlingMetrics);\n \n         ProcessingContext context = processingContext();\n \n@@ -126,8 +131,8 @@ public void testDLQConfigWithValidTopicName() {\n \n     @Test\n     public void testReportDLQTwice() {\n-        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(singletonMap(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC)), TASK_ID);\n-        deadLetterQueueReporter.metrics(errorHandlingMetrics);\n+        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(\n+                producer, config(singletonMap(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC)), TASK_ID, errorHandlingMetrics);\n \n         ProcessingContext context = processingContext();\n \n@@ -142,8 +147,7 @@ public void testReportDLQTwice() {\n \n     @Test\n     public void testLogOnDisabledLogReporter() {\n-        LogReporter logReporter = new LogReporter(TASK_ID, config(emptyMap()));\n-        logReporter.metrics(errorHandlingMetrics);\n+        LogReporter logReporter = new LogReporter(TASK_ID, config(emptyMap()), errorHandlingMetrics);\n \n         ProcessingContext context = processingContext();\n         context.error(new RuntimeException());\n@@ -155,8 +159,7 @@ public void testLogOnDisabledLogReporter() {\n \n     @Test\n     public void testLogOnEnabledLogReporter() {\n-        LogReporter logReporter = new LogReporter(TASK_ID, config(singletonMap(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, \"true\")));\n-        logReporter.metrics(errorHandlingMetrics);\n+        LogReporter logReporter = new LogReporter(TASK_ID, config(singletonMap(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, \"true\")), errorHandlingMetrics);\n \n         ProcessingContext context = processingContext();\n         context.error(new RuntimeException());\n@@ -168,8 +171,7 @@ public void testLogOnEnabledLogReporter() {\n \n     @Test\n     public void testLogMessageWithNoRecords() {\n-        LogReporter logReporter = new LogReporter(TASK_ID, config(singletonMap(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, \"true\")));\n-        logReporter.metrics(errorHandlingMetrics);\n+        LogReporter logReporter = new LogReporter(TASK_ID, config(singletonMap(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, \"true\")), errorHandlingMetrics);\n \n         ProcessingContext context = processingContext();\n \n@@ -184,8 +186,7 @@ public void testLogMessageWithSinkRecords() {\n         props.put(ConnectorConfig.ERRORS_LOG_ENABLE_CONFIG, \"true\");\n         props.put(ConnectorConfig.ERRORS_LOG_INCLUDE_MESSAGES_CONFIG, \"true\");\n \n-        LogReporter logReporter = new LogReporter(TASK_ID, config(props));\n-        logReporter.metrics(errorHandlingMetrics);\n+        LogReporter logReporter = new LogReporter(TASK_ID, config(props), errorHandlingMetrics);\n \n         ProcessingContext context = processingContext();\n \n@@ -208,7 +209,7 @@ public void testDlqHeaderConsumerRecord() {\n         Map<String, String> props = new HashMap<>();\n         props.put(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC);\n         props.put(SinkConnectorConfig.DLQ_CONTEXT_HEADERS_ENABLE_CONFIG, \"true\");\n-        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(props), TASK_ID);\n+        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(props), TASK_ID, errorHandlingMetrics);\n \n         ProcessingContext context = new ProcessingContext();\n         context.consumerRecord(new ConsumerRecord<>(\"source-topic\", 7, 10, \"source-key\".getBytes(), \"source-value\".getBytes()));\n@@ -236,7 +237,7 @@ public void testDlqHeaderIsAppended() {\n         Map<String, String> props = new HashMap<>();\n         props.put(SinkConnectorConfig.DLQ_TOPIC_NAME_CONFIG, DLQ_TOPIC);\n         props.put(SinkConnectorConfig.DLQ_CONTEXT_HEADERS_ENABLE_CONFIG, \"true\");\n-        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(props), TASK_ID);\n+        DeadLetterQueueReporter deadLetterQueueReporter = new DeadLetterQueueReporter(producer, config(props), TASK_ID, errorHandlingMetrics);\n \n         ProcessingContext context = new ProcessingContext();\n         context.consumerRecord(new ConsumerRecord<>(\"source-topic\", 7, 10, \"source-key\".getBytes(), \"source-value\".getBytes()));", "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/errors/ErrorReporterTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/0c25c73782e6e70b8f37e3dda2fa2a5b0b1c8c65", "parent": "https://github.com/apache/kafka/commit/61d3378bc84914a521a65cdfffb7299928fa8671", "message": "HOTFIX: fix npe in StreamsMetadataState when onChange has not been called\n\nIf some StreamsMetadataState methods are called before the onChange method is called a NullPointerException was being thrown. Added null check for cluster in isInitialized method\n\nAuthor: Damian Guy <damian.guy@gmail.com>\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>\n\nCloses #1920 from dguy/fix-npe-streamsmetadata", "bug_id": "kafka_42", "file": [{"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/0c25c73782e6e70b8f37e3dda2fa2a5b0b1c8c65/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java", "blob_url": "https://github.com/apache/kafka/blob/0c25c73782e6e70b8f37e3dda2fa2a5b0b1c8c65/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java", "sha": "ccb2cdafeeb733ab8eb4908b79f4648718a3858a", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java?ref=0c25c73782e6e70b8f37e3dda2fa2a5b0b1c8c65", "patch": "@@ -234,7 +234,7 @@ private SourceTopicsInfo getSourceTopicsInfo(final String storeName) {\n     }\n \n     private boolean isInitialized() {\n-        return !clusterMetadata.topics().isEmpty();\n+        return clusterMetadata != null && !clusterMetadata.topics().isEmpty();\n     }\n \n     private class SourceTopicsInfo {", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java"}, {"additions": 5, "raw_url": "https://github.com/apache/kafka/raw/0c25c73782e6e70b8f37e3dda2fa2a5b0b1c8c65/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsMetadataStateTest.java", "blob_url": "https://github.com/apache/kafka/blob/0c25c73782e6e70b8f37e3dda2fa2a5b0b1c8c65/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsMetadataStateTest.java", "sha": "411e02db9b9e1cc0e653af8d3ed6bd3e911dbc4b", "changes": 5, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsMetadataStateTest.java?ref=0c25c73782e6e70b8f37e3dda2fa2a5b0b1c8c65", "patch": "@@ -114,6 +114,11 @@ public Object apply(final Object value) {\n         discovery.onChange(hostToPartitions, cluster);\n     }\n \n+    @Test\n+    public void shouldNotThrowNPEWhenOnChangeNotCalled() throws Exception {\n+        new StreamsMetadataState(builder).getAllMetadataForStore(\"store\");\n+    }\n+\n     @Test\n     public void shouldGetAllStreamInstances() throws Exception {\n         final StreamsMetadata one = new StreamsMetadata(hostOne, Utils.mkSet(\"table-one\", \"table-two\", \"merged-table\"),", "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsMetadataStateTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/0a7f2bf335e426b12f717ec7f6d51779d56fe59b", "parent": "https://github.com/apache/kafka/commit/b1d325b3c09cd95d69a66fac4a3760f57d3062c9", "message": "MINOR: MemoryRecords.sizeInBytes throws NPE when non-writable.\n\nI just noticed that `MemoryRecords.sizeInBytes` throws NPE when MemoryRecords is non-writable. `compressor` is explicitly set to null when `writable` is false (L56) at the construction time, for instance when `MemoryRecords.readableRecords` is used.\n\nguozhangwang Could you take a look when you have time?\n\nAuthor: David Jacot <david.jacot@gmail.com>\n\nReviewers: Guozhang Wang\n\nCloses #786 from dajac/kafka-npe", "bug_id": "kafka_43", "file": [{"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/0a7f2bf335e426b12f717ec7f6d51779d56fe59b/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java", "blob_url": "https://github.com/apache/kafka/blob/0a7f2bf335e426b12f717ec7f6d51779d56fe59b/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java", "sha": "971f0a2131259babdfde7947149a50102f60ccc5", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java?ref=0a7f2bf335e426b12f717ec7f6d51779d56fe59b", "patch": "@@ -145,7 +145,7 @@ public int sizeInBytes() {\n         if (writable) {\n             return compressor.buffer().position();\n         } else {\n-            return compressor.buffer().limit();\n+            return buffer.limit();\n         }\n     }\n ", "filename": "clients/src/main/java/org/apache/kafka/common/record/MemoryRecords.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/1f8527b331f3d4c8e3ea16993d96caae2ea18fc5", "parent": "https://github.com/apache/kafka/commit/1aea0834d346f0afa16f946c47e51fefac37612b", "message": "KAFKA-7136: Avoid deadlocks in synchronized metrics reporters (#5341)\n\nWe need to use the same lock for metric update and read to avoid NPE and concurrent modification exceptions. Sensor add/remove/update are synchronized on Sensor since they access lists and maps that are not thread-safe. Reporters are notified of metrics add/remove while holding (Sensor, Metrics) locks and reporters may synchronize on the reporter lock. Metric read may be invoked by metrics reporters while holding a reporter lock. So read/update cannot be synchronized using Sensor since that could lead to deadlock. This PR introduces a new lock in Sensor for update/read.\r\nLocking order:\r\n\r\n- Sensor#add: Sensor -> Metrics -> MetricsReporter\r\n- Metrics#removeSensor: Sensor -> Metrics -> MetricsReporter\r\n- KafkaMetric#metricValue: MetricsReporter -> Sensor#metricLock\r\n- Sensor#record: Sensor -> Sensor#metricLock\r\n\r\n\r\nReviewers: Jun Rao <junrao@gmail.com>, Guozhang Wang <wangguoz@gmail.com>", "bug_id": "kafka_44", "file": [{"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/1f8527b331f3d4c8e3ea16993d96caae2ea18fc5/checkstyle/suppressions.xml", "blob_url": "https://github.com/apache/kafka/blob/1f8527b331f3d4c8e3ea16993d96caae2ea18fc5/checkstyle/suppressions.xml", "sha": "e80d5bf24c1cfa2dbe1869393142a040a9c2e36a", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/checkstyle/suppressions.xml?ref=1f8527b331f3d4c8e3ea16993d96caae2ea18fc5", "patch": "@@ -73,7 +73,7 @@\n               files=\"RequestResponseTest.java\"/>\n \n     <suppress checks=\"NPathComplexity\"\n-              files=\"MemoryRecordsTest.java\"/>\n+              files=\"MemoryRecordsTest|MetricsTest\"/>\n \n     <!-- Connect -->\n     <suppress checks=\"ClassFanOutComplexity\"", "filename": "checkstyle/suppressions.xml"}, {"additions": 29, "raw_url": "https://github.com/apache/kafka/raw/1f8527b331f3d4c8e3ea16993d96caae2ea18fc5/clients/src/main/java/org/apache/kafka/common/metrics/Sensor.java", "blob_url": "https://github.com/apache/kafka/blob/1f8527b331f3d4c8e3ea16993d96caae2ea18fc5/clients/src/main/java/org/apache/kafka/common/metrics/Sensor.java", "sha": "ccbe8aad9cde51ee9c5bc7cdc4a20f913b6a0ade", "changes": 38, "status": "modified", "deletions": 9, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/metrics/Sensor.java?ref=1f8527b331f3d4c8e3ea16993d96caae2ea18fc5", "patch": "@@ -48,6 +48,7 @@\n     private final Time time;\n     private volatile long lastRecordTime;\n     private final long inactiveSensorExpirationTimeMs;\n+    private final Object metricLock;\n \n     public enum RecordingLevel {\n         INFO(0, \"INFO\"), DEBUG(1, \"DEBUG\");\n@@ -113,6 +114,7 @@ public boolean shouldRecord(final int configId) {\n         this.inactiveSensorExpirationTimeMs = TimeUnit.MILLISECONDS.convert(inactiveSensorExpirationTimeSeconds, TimeUnit.SECONDS);\n         this.lastRecordTime = time.milliseconds();\n         this.recordingLevel = recordingLevel;\n+        this.metricLock = new Object();\n         checkForest(new HashSet<Sensor>());\n     }\n \n@@ -174,9 +176,11 @@ public void record(double value, long timeMs, boolean checkQuotas) {\n         if (shouldRecord()) {\n             this.lastRecordTime = timeMs;\n             synchronized (this) {\n-                // increment all the stats\n-                for (Stat stat : this.stats)\n-                    stat.record(config, value, timeMs);\n+                synchronized (metricLock()) {\n+                    // increment all the stats\n+                    for (Stat stat : this.stats)\n+                        stat.record(config, value, timeMs);\n+                }\n                 if (checkQuotas)\n                     checkQuotas(timeMs);\n             }\n@@ -229,7 +233,7 @@ public synchronized boolean add(CompoundStat stat, MetricConfig config) {\n             return false;\n \n         this.stats.add(Utils.notNull(stat));\n-        Object lock = metricLock(stat);\n+        Object lock = metricLock();\n         for (NamedMeasurable m : stat.stats()) {\n             final KafkaMetric metric = new KafkaMetric(lock, m.name(), m.stat(), config == null ? this.config : config, time);\n             if (!metrics.containsKey(metric.metricName())) {\n@@ -265,7 +269,7 @@ public synchronized boolean add(final MetricName metricName, final MeasurableSta\n             return true;\n         } else {\n             final KafkaMetric metric = new KafkaMetric(\n-                metricLock(stat),\n+                metricLock(),\n                 Utils.notNull(metricName),\n                 Utils.notNull(stat),\n                 config == null ? this.config : config,\n@@ -291,10 +295,26 @@ public boolean hasExpired() {\n     }\n \n     /**\n-     * KafkaMetrics of sensors which use SampledStat should be synchronized on the Sensor object\n-     * to allow concurrent reads and updates. For simplicity, all sensors are synchronized on Sensor.\n+     * KafkaMetrics of sensors which use SampledStat should be synchronized on the same lock\n+     * for sensor record and metric value read to allow concurrent reads and updates. For simplicity,\n+     * all sensors are synchronized on this object.\n+     * <p>\n+     * Sensor object is not used as a lock for reading metric value since metrics reporter is\n+     * invoked while holding Sensor and Metrics locks to report addition and removal of metrics\n+     * and synchronized reporters may deadlock if Sensor lock is used for reading metrics values.\n+     * Note that Sensor object itself is used as a lock to protect the access to stats and metrics\n+     * while recording metric values, adding and deleting sensors.\n+     * </p><p>\n+     * Locking order (assume all MetricsReporter methods may be synchronized):\n+     * <ul>\n+     *   <li>Sensor#add: Sensor -> Metrics -> MetricsReporter</li>\n+     *   <li>Metrics#removeSensor: Sensor -> Metrics -> MetricsReporter</li>\n+     *   <li>KafkaMetric#metricValue: MetricsReporter -> Sensor#metricLock</li>\n+     *   <li>Sensor#record: Sensor -> Sensor#metricLock</li>\n+     * </ul>\n+     * </p>\n      */\n-    private Object metricLock(Stat stat) {\n-        return this;\n+    private Object metricLock() {\n+        return metricLock;\n     }\n }", "filename": "clients/src/main/java/org/apache/kafka/common/metrics/Sensor.java"}, {"additions": 105, "raw_url": "https://github.com/apache/kafka/raw/1f8527b331f3d4c8e3ea16993d96caae2ea18fc5/clients/src/test/java/org/apache/kafka/common/metrics/MetricsTest.java", "blob_url": "https://github.com/apache/kafka/blob/1f8527b331f3d4c8e3ea16993d96caae2ea18fc5/clients/src/test/java/org/apache/kafka/common/metrics/MetricsTest.java", "sha": "59bc84e40decf49ea22902f572e84f32af21ee29", "changes": 117, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/metrics/MetricsTest.java?ref=1f8527b331f3d4c8e3ea16993d96caae2ea18fc5", "patch": "@@ -26,13 +26,16 @@\n import java.util.Arrays;\n import java.util.Collections;\n import java.util.Deque;\n+import java.util.List;\n import java.util.HashMap;\n import java.util.Map;\n import java.util.Random;\n import java.util.concurrent.ConcurrentLinkedDeque;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n import java.util.concurrent.TimeUnit;\n+\n import java.util.concurrent.atomic.AtomicBoolean;\n \n import org.apache.kafka.common.Metric;\n@@ -54,9 +57,12 @@\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n @SuppressWarnings(\"deprecation\")\n public class MetricsTest {\n+    private static final Logger log = LoggerFactory.getLogger(MetricsTest.class);\n \n     private static final double EPS = 0.000001;\n     private MockTime time = new MockTime();\n@@ -604,25 +610,21 @@ public void testMetricInstances() {\n         }\n     }\n \n+    /**\n+     * Verifies that concurrent sensor add, remove, updates and read don't result\n+     * in errors or deadlock.\n+     */\n     @Test\n-    public void testConcurrentAccess() throws Exception {\n+    public void testConcurrentReadUpdate() throws Exception {\n         final Random random = new Random();\n         final Deque<Sensor> sensors = new ConcurrentLinkedDeque<>();\n         metrics = new Metrics(new MockTime(10));\n         SensorCreator sensorCreator = new SensorCreator(metrics);\n \n         final AtomicBoolean alive = new AtomicBoolean(true);\n         executorService = Executors.newSingleThreadExecutor();\n-        executorService.submit(new Runnable() {\n-            @Override\n-            public void run() {\n-                while (alive.get()) {\n-                    for (Sensor sensor : sensors) {\n-                        sensor.record(random.nextInt(10000));\n-                    }\n-                }\n-            }\n-        });\n+        executorService.submit(new ConcurrentMetricOperation(alive, \"record\",\n+            () -> sensors.forEach(sensor -> sensor.record(random.nextInt(10000)))));\n \n         for (int i = 0; i < 10000; i++) {\n             if (sensors.size() > 5) {\n@@ -640,6 +642,97 @@ public void run() {\n         alive.set(false);\n     }\n \n+    /**\n+     * Verifies that concurrent sensor add, remove, updates and read with a metrics reporter\n+     * that synchronizes on every reporter method doesn't result in errors or deadlock.\n+     */\n+    @Test\n+    public void testConcurrentReadUpdateReport() throws Exception {\n+\n+        class LockingReporter implements MetricsReporter {\n+            Map<MetricName, KafkaMetric> activeMetrics = new HashMap<>();\n+            @Override\n+            public synchronized void init(List<KafkaMetric> metrics) {\n+            }\n+\n+            @Override\n+            public synchronized void metricChange(KafkaMetric metric) {\n+                activeMetrics.put(metric.metricName(), metric);\n+            }\n+\n+            @Override\n+            public synchronized void metricRemoval(KafkaMetric metric) {\n+                activeMetrics.remove(metric.metricName(), metric);\n+            }\n+\n+            @Override\n+            public synchronized void close() {\n+            }\n+\n+            @Override\n+            public void configure(Map<String, ?> configs) {\n+            }\n+\n+            synchronized void processMetrics() {\n+                for (KafkaMetric metric : activeMetrics.values()) {\n+                    assertNotNull(\"Invalid metric value\", metric.metricValue());\n+                }\n+            }\n+        }\n+\n+        final LockingReporter reporter = new LockingReporter();\n+        this.metrics.close();\n+        this.metrics = new Metrics(config, Arrays.asList((MetricsReporter) reporter), new MockTime(10), true);\n+        final Deque<Sensor> sensors = new ConcurrentLinkedDeque<>();\n+        SensorCreator sensorCreator = new SensorCreator(metrics);\n+\n+        final Random random = new Random();\n+        final AtomicBoolean alive = new AtomicBoolean(true);\n+        executorService = Executors.newFixedThreadPool(3);\n+\n+        Future<?> writeFuture = executorService.submit(new ConcurrentMetricOperation(alive, \"record\",\n+            () -> sensors.forEach(sensor -> sensor.record(random.nextInt(10000)))));\n+        Future<?> readFuture = executorService.submit(new ConcurrentMetricOperation(alive, \"read\",\n+            () -> sensors.forEach(sensor -> sensor.metrics().forEach(metric ->\n+                assertNotNull(\"Invalid metric value\", metric.metricValue())))));\n+        Future<?> reportFuture = executorService.submit(new ConcurrentMetricOperation(alive, \"report\",\n+            () -> reporter.processMetrics()));\n+\n+        for (int i = 0; i < 10000; i++) {\n+            if (sensors.size() > 10) {\n+                Sensor sensor = random.nextBoolean() ? sensors.removeFirst() : sensors.removeLast();\n+                metrics.removeSensor(sensor.name());\n+            }\n+            StatType statType = StatType.forId(random.nextInt(StatType.values().length));\n+            sensors.add(sensorCreator.createSensor(statType, i));\n+        }\n+        assertFalse(\"Read failed\", readFuture.isDone());\n+        assertFalse(\"Write failed\", writeFuture.isDone());\n+        assertFalse(\"Report failed\", reportFuture.isDone());\n+\n+        alive.set(false);\n+    }\n+\n+    private class ConcurrentMetricOperation implements Runnable {\n+        private final AtomicBoolean alive;\n+        private final String opName;\n+        private final Runnable op;\n+        ConcurrentMetricOperation(AtomicBoolean alive, String opName, Runnable op) {\n+            this.alive = alive;\n+            this.opName = opName;\n+            this.op = op;\n+        }\n+        public void run() {\n+            try {\n+                while (alive.get()) {\n+                    op.run();\n+                }\n+            } catch (Throwable t) {\n+                log.error(\"Metric {} failed with exception\", opName, t);\n+            }\n+        }\n+    }\n+\n     enum StatType {\n         AVG(0),\n         TOTAL(1),\n@@ -676,7 +769,7 @@ static StatType forId(int id) {\n         }\n \n         private Sensor createSensor(StatType statType, int index) {\n-            Sensor sensor = metrics.sensor(\"kafka.requests\");\n+            Sensor sensor = metrics.sensor(\"kafka.requests.\" + index);\n             Map<String, String> tags = Collections.singletonMap(\"tag\", \"tag\" + index);\n             switch (statType) {\n                 case AVG:", "filename": "clients/src/test/java/org/apache/kafka/common/metrics/MetricsTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/a5c47db1382c14720106ae1da20d2b332f89c22c", "parent": "https://github.com/apache/kafka/commit/ee5eac715d58e6b16a115692ede93ae481ae7785", "message": "KAFKA-5506; Fix NPE in OffsetFetchRequest.toString and logging improvements\n\nNetworkClient's logging improvements:\n- Include correlation id in a number of log statements\n- Avoid eager toString call in parameter passed to log.debug\n- Use node.toString instead of passing a subset of fields to the\nlogger\n- Use requestBuilder instead of clientRequest in one of the log\nstatements\n\nAuthor: Ismael Juma <ismael@juma.me.uk>\n\nReviewers: Damian Guy <damian.guy@gmail.com>, Jason Gustafson <jason@confluent.io>\n\nCloses #3420 from ijuma/kafka-5506-offset-fetch-request-to-string-npe", "bug_id": "kafka_45", "file": [{"additions": 23, "raw_url": "https://github.com/apache/kafka/raw/a5c47db1382c14720106ae1da20d2b332f89c22c/clients/src/main/java/org/apache/kafka/clients/NetworkClient.java", "blob_url": "https://github.com/apache/kafka/blob/a5c47db1382c14720106ae1da20d2b332f89c22c/clients/src/main/java/org/apache/kafka/clients/NetworkClient.java", "sha": "59c606f8fabd77000896ae2e57ee5176e746e8f0", "changes": 44, "status": "modified", "deletions": 21, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/NetworkClient.java?ref=a5c47db1382c14720106ae1da20d2b332f89c22c", "patch": "@@ -251,7 +251,7 @@ public void disconnect(String nodeId) {\n         }\n         connectionStates.disconnected(nodeId, now);\n         if (log.isDebugEnabled()) {\n-            log.debug(\"Manually disconnected from {}.  Removed requests: {}.\", nodeId,\n+            log.debug(\"Manually disconnected from {}. Removed requests: {}.\", nodeId,\n                 Utils.join(requestTypes, \", \"));\n         }\n     }\n@@ -360,8 +360,8 @@ private void doSend(ClientRequest clientRequest, boolean isInternalRequest, long\n             if (versionInfo == null) {\n                 version = builder.desiredOrLatestVersion();\n                 if (discoverBrokerVersions && log.isTraceEnabled())\n-                    log.trace(\"No version information found when sending message of type {} to node {}. \" +\n-                            \"Assuming version {}.\", clientRequest.apiKey(), nodeId, version);\n+                    log.trace(\"No version information found when sending {} with correlation id {} to node {}. \" +\n+                            \"Assuming version {}.\", clientRequest.apiKey(), clientRequest.correlationId(), nodeId, version);\n             } else {\n                 version = versionInfo.usableVersion(clientRequest.apiKey(), builder.desiredVersion());\n             }\n@@ -371,8 +371,8 @@ private void doSend(ClientRequest clientRequest, boolean isInternalRequest, long\n         } catch (UnsupportedVersionException e) {\n             // If the version is not supported, skip sending the request over the wire.\n             // Instead, simply add it to the local queue of aborted requests.\n-            log.debug(\"Version mismatch when attempting to send {} to {}\",\n-                    clientRequest.toString(), clientRequest.destination(), e);\n+            log.debug(\"Version mismatch when attempting to send {} with correlation id {} to {}\", builder,\n+                    clientRequest.correlationId(), clientRequest.destination(), e);\n             ClientResponse clientResponse = new ClientResponse(clientRequest.makeHeader(builder.desiredOrLatestVersion()),\n                     clientRequest.callback(), clientRequest.destination(), now, now,\n                     false, e, null);\n@@ -386,10 +386,11 @@ private void doSend(ClientRequest clientRequest, boolean isInternalRequest, long\n         if (log.isDebugEnabled()) {\n             int latestClientVersion = clientRequest.apiKey().latestVersion();\n             if (header.apiVersion() == latestClientVersion) {\n-                log.trace(\"Sending {} {} to node {}.\", clientRequest.apiKey(), request, nodeId);\n+                log.trace(\"Sending {} {} with correlation id {} to node {}\", clientRequest.apiKey(), request,\n+                        clientRequest.correlationId(), nodeId);\n             } else {\n-                log.debug(\"Using older server API v{} to send {} {} to node {}.\",\n-                        header.apiVersion(), clientRequest.apiKey(), request, nodeId);\n+                log.debug(\"Using older server API v{} to send {} {} with correlation id {} to node {}\",\n+                        header.apiVersion(), clientRequest.apiKey(), request, clientRequest.correlationId(), nodeId);\n             }\n         }\n         Send send = request.toSend(nodeId, header);\n@@ -554,8 +555,8 @@ public static AbstractResponse parseResponse(ByteBuffer responseBuffer, RequestH\n     private static Struct parseStructMaybeUpdateThrottleTimeMetrics(ByteBuffer responseBuffer, RequestHeader requestHeader,\n                                                                     Sensor throttleTimeSensor, long now) {\n         ResponseHeader responseHeader = ResponseHeader.parse(responseBuffer);\n-        // Always expect the response version id to be the same as the request version id\n         ApiKeys apiKey = ApiKeys.forId(requestHeader.apiKey());\n+        // Always expect the response version id to be the same as the request version id\n         Struct responseBody = apiKey.parseResponse(requestHeader.apiVersion(), responseBuffer);\n         correlate(requestHeader, responseHeader);\n         if (throttleTimeSensor != null && responseBody.hasField(AbstractResponse.THROTTLE_TIME_KEY_NAME))\n@@ -591,7 +592,8 @@ private void processDisconnection(List<ClientResponse> responses, String nodeId,\n                 break; // Disconnections in other states are logged at debug level in Selector\n         }\n         for (InFlightRequest request : this.inFlightRequests.clearAll(nodeId)) {\n-            log.trace(\"Cancelled request {} due to node {} being disconnected\", request.request, nodeId);\n+            log.trace(\"Cancelled request {} with correlation id {} due to node {} being disconnected\", request.request,\n+                    request.header.correlationId(), nodeId);\n             if (request.isInternalRequest && request.header.apiKey() == ApiKeys.METADATA.id)\n                 metadataUpdater.handleDisconnection(request.destination);\n             else\n@@ -655,8 +657,8 @@ private void handleCompletedReceives(List<ClientResponse> responses, long now) {\n             Struct responseStruct = parseStructMaybeUpdateThrottleTimeMetrics(receive.payload(), req.header,\n                 throttleTimeSensor, now);\n             if (log.isTraceEnabled()) {\n-                log.trace(\"Completed receive from node {}, for key {}, received {}\", req.destination,\n-                    req.header.apiKey(), responseStruct.toString());\n+                log.trace(\"Completed receive from node {} for {} with correlation id {}, received {}\", req.destination,\n+                    ApiKeys.forId(req.header.apiKey()), req.header.correlationId(), responseStruct);\n             }\n             AbstractResponse body = createResponse(responseStruct, req.header);\n             if (req.isInternalRequest && body instanceof MetadataResponse)\n@@ -673,8 +675,8 @@ private void handleApiVersionsResponse(List<ClientResponse> responses,\n         final String node = req.destination;\n         if (apiVersionsResponse.error() != Errors.NONE) {\n             if (req.request.version() == 0 || apiVersionsResponse.error() != Errors.UNSUPPORTED_VERSION) {\n-                log.warn(\"Node {} got error {} when making an ApiVersionsRequest.  Disconnecting.\",\n-                        node, apiVersionsResponse.error());\n+                log.warn(\"Received error {} from node {} when making an ApiVersionsRequest with correlation id {}. Disconnecting.\",\n+                        apiVersionsResponse.error(), node, req.header.correlationId());\n                 this.selector.close(node);\n                 processDisconnection(responses, node, now, ChannelState.LOCAL_CLOSE);\n             } else {\n@@ -719,10 +721,10 @@ private void handleConnections() {\n             if (discoverBrokerVersions) {\n                 this.connectionStates.checkingApiVersions(node);\n                 nodesNeedingApiVersionsFetch.put(node, new ApiVersionsRequest.Builder());\n-                log.debug(\"Completed connection to node {}.  Fetching API versions.\", node);\n+                log.debug(\"Completed connection to node {}. Fetching API versions.\", node);\n             } else {\n                 this.connectionStates.ready(node);\n-                log.debug(\"Completed connection to node {}.  Ready.\", node);\n+                log.debug(\"Completed connection to node {}. Ready.\", node);\n             }\n         }\n     }\n@@ -757,7 +759,7 @@ private static void correlate(RequestHeader requestHeader, ResponseHeader respon\n     private void initiateConnect(Node node, long now) {\n         String nodeConnectionId = node.idString();\n         try {\n-            log.debug(\"Initiating connection to node {} at {}:{}.\", node.id(), node.host(), node.port());\n+            log.debug(\"Initiating connection to node {}\", node);\n             this.connectionStates.connecting(nodeConnectionId, now);\n             selector.connect(nodeConnectionId,\n                              new InetSocketAddress(node.host(), node.port()),\n@@ -768,7 +770,7 @@ private void initiateConnect(Node node, long now) {\n             connectionStates.disconnected(nodeConnectionId, now);\n             /* maybe the problem is our metadata, update it */\n             metadataUpdater.requestUpdate();\n-            log.debug(\"Error connecting to node {} at {}:{}:\", node.id(), node.host(), node.port(), e);\n+            log.debug(\"Error connecting to node {}\", node, e);\n         }\n     }\n \n@@ -828,7 +830,7 @@ public void handleDisconnection(String destination) {\n                 int nodeId = Integer.parseInt(destination);\n                 Node node = cluster.nodeById(nodeId);\n                 if (node != null)\n-                    log.warn(\"Bootstrap broker {}:{} disconnected\", node.host(), node.port());\n+                    log.warn(\"Bootstrap broker {} disconnected\", node);\n             }\n \n             metadataFetchInProgress = false;\n@@ -886,7 +888,7 @@ private long maybeUpdate(long now, Node node) {\n                             metadata.allowAutoTopicCreation());\n \n \n-                log.debug(\"Sending metadata request {} to node {}\", metadataRequest, node.id());\n+                log.debug(\"Sending metadata request {} to node {}\", metadataRequest, node);\n                 sendInternalMetadataRequest(metadataRequest, nodeConnectionId, now);\n                 return requestTimeoutMs;\n             }\n@@ -902,7 +904,7 @@ private long maybeUpdate(long now, Node node) {\n \n             if (connectionStates.canConnect(nodeConnectionId, now)) {\n                 // we don't have a connection to this node right now, make one\n-                log.debug(\"Initialize connection to node {} for sending metadata request\", node.id());\n+                log.debug(\"Initialize connection to node {} for sending metadata request\", node);\n                 initiateConnect(node, now);\n                 return reconnectBackoffMs;\n             }", "filename": "clients/src/main/java/org/apache/kafka/clients/NetworkClient.java"}, {"additions": 2, "raw_url": "https://github.com/apache/kafka/raw/a5c47db1382c14720106ae1da20d2b332f89c22c/clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchRequest.java", "blob_url": "https://github.com/apache/kafka/blob/a5c47db1382c14720106ae1da20d2b332f89c22c/clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchRequest.java", "sha": "15fdf57dcf5c64278f67875d9ad011bc55a34f4d", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchRequest.java?ref=a5c47db1382c14720106ae1da20d2b332f89c22c", "patch": "@@ -71,9 +71,10 @@ public OffsetFetchRequest build(short version) {\n         @Override\n         public String toString() {\n             StringBuilder bld = new StringBuilder();\n+            String partitionsString = partitions == null ? \"<ALL>\" : Utils.join(partitions, \",\");\n             bld.append(\"(type=OffsetFetchRequest, \").\n                     append(\"groupId=\").append(groupId).\n-                    append(\", partitions=\").append(Utils.join(partitions, \",\")).\n+                    append(\", partitions=\").append(partitionsString).\n                     append(\")\");\n             return bld.toString();\n         }", "filename": "clients/src/main/java/org/apache/kafka/common/requests/OffsetFetchRequest.java"}, {"additions": 10, "raw_url": "https://github.com/apache/kafka/raw/a5c47db1382c14720106ae1da20d2b332f89c22c/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "blob_url": "https://github.com/apache/kafka/blob/a5c47db1382c14720106ae1da20d2b332f89c22c/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java", "sha": "a3c277f247c3ea535be450e0c0f9a7339bf948d0", "changes": 10, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java?ref=a5c47db1382c14720106ae1da20d2b332f89c22c", "patch": "@@ -554,6 +554,16 @@ public void testJoinGroupRequestVersion0RebalanceTimeout() throws Exception {\n         assertEquals(jgr2.rebalanceTimeout(), jgr.rebalanceTimeout());\n     }\n \n+    @Test\n+    public void testOffsetFetchRequestBuilderToString() {\n+        String allTopicPartitionsString = OffsetFetchRequest.Builder.allTopicPartitions(\"someGroup\").toString();\n+        assertTrue(allTopicPartitionsString.contains(\"<ALL>\"));\n+        String string = new OffsetFetchRequest.Builder(\"group1\",\n+                singletonList(new TopicPartition(\"test11\", 1))).toString();\n+        assertTrue(string.contains(\"test11\"));\n+        assertTrue(string.contains(\"group1\"));\n+    }\n+\n     private RequestHeader createRequestHeader() {\n         return new RequestHeader((short) 10, (short) 1, \"\", 10);\n     }", "filename": "clients/src/test/java/org/apache/kafka/common/requests/RequestResponseTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/1f528815de8d0e094ee5446794ab7325629ca7ed", "parent": "https://github.com/apache/kafka/commit/62253539d87e1ccb353673ed96adc98fb2d854ae", "message": "KAFKA-3642: Fix NPE from ProcessorStateManager when the changelog topic not exists\n\nIssue: https://issues.apache.org/jira/browse/KAFKA-3642\n\nAuthor: Yuto Kawamura <kawamuray.dadada@gmail.com>\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>\n\nCloses #1289 from kawamuray/KAFKA-3642-streams-NPE", "bug_id": "kafka_46", "file": [{"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/1f528815de8d0e094ee5446794ab7325629ca7ed/clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java", "blob_url": "https://github.com/apache/kafka/blob/1f528815de8d0e094ee5446794ab7325629ca7ed/clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java", "sha": "9ab4c29493da28d05cf745906537f7d33c780475", "changes": 6, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java?ref=1f528815de8d0e094ee5446794ab7325629ca7ed", "patch": "@@ -268,11 +268,7 @@ public void updateEndOffsets(Map<TopicPartition, Long> newOffsets) {\n     @Override\n     public List<PartitionInfo> partitionsFor(String topic) {\n         ensureNotClosed();\n-        List<PartitionInfo> parts = this.partitions.get(topic);\n-        if (parts == null)\n-            return Collections.emptyList();\n-        else\n-            return parts;\n+        return this.partitions.get(topic);\n     }\n \n     @Override", "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/MockConsumer.java"}, {"additions": 5, "raw_url": "https://github.com/apache/kafka/raw/1f528815de8d0e094ee5446794ab7325629ca7ed/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "blob_url": "https://github.com/apache/kafka/blob/1f528815de8d0e094ee5446794ab7325629ca7ed/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java", "sha": "1d97384a9bf571be2cbd0f841fa13651c66b2a11", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java?ref=1f528815de8d0e094ee5446794ab7325629ca7ed", "patch": "@@ -186,7 +186,11 @@ public void register(StateStore store, boolean loggingEnabled, StateRestoreCallb\n                 // ignore\n             }\n \n-            for (PartitionInfo partitionInfo : restoreConsumer.partitionsFor(topic)) {\n+            List<PartitionInfo> partitionInfos = restoreConsumer.partitionsFor(topic);\n+            if (partitionInfos == null) {\n+                throw new StreamsException(\"Could not find partition info for topic: \" + topic);\n+            }\n+            for (PartitionInfo partitionInfo : partitionInfos) {\n                 if (partitionInfo.partition() == partition) {\n                     partitionNotFound = false;\n                     break;", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorStateManager.java"}, {"additions": 18, "raw_url": "https://github.com/apache/kafka/raw/1f528815de8d0e094ee5446794ab7325629ca7ed/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java", "blob_url": "https://github.com/apache/kafka/blob/1f528815de8d0e094ee5446794ab7325629ca7ed/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java", "sha": "f2eea36c11c1da3df58d1b67e3c26cb98f2f90e3", "changes": 27, "status": "modified", "deletions": 9, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java?ref=1f528815de8d0e094ee5446794ab7325629ca7ed", "patch": "@@ -152,15 +152,13 @@ public Subscription subscription(Set<String> topics) {\n      * @param topicToTaskIds Map that contains the topic names to be created\n      * @param compactTopic If true, the topic should be a compacted topic. This is used for\n      *                     change log topics usually.\n-     * @param outPartitionInfo If true, compute and return all partitions created\n      * @param postPartitionPhase If true, the computation for calculating the number of partitions\n      *                           is slightly different. Set to true after the initial topic-to-partition\n      *                           assignment.\n      * @return\n      */\n     private Map<TopicPartition, PartitionInfo> prepareTopic(Map<String, Set<TaskId>> topicToTaskIds,\n                                                             boolean compactTopic,\n-                                                            boolean outPartitionInfo,\n                                                             boolean postPartitionPhase) {\n         Map<TopicPartition, PartitionInfo> partitionInfos = new HashMap<>();\n         // if ZK is specified, prepare the internal source topic before calling partition grouper\n@@ -192,13 +190,24 @@ public Subscription subscription(Set<String> topics) {\n                     partitions = streamThread.restoreConsumer.partitionsFor(topic);\n                 } while (partitions == null || partitions.size() != numPartitions);\n \n-                if (outPartitionInfo) {\n-                    for (PartitionInfo partition : partitions)\n-                        partitionInfos.put(new TopicPartition(partition.topic(), partition.partition()), partition);\n-                }\n+                for (PartitionInfo partition : partitions)\n+                    partitionInfos.put(new TopicPartition(partition.topic(), partition.partition()), partition);\n             }\n \n             log.info(\"Completed validating internal topics in partition assignor.\");\n+        } else {\n+            List<String> missingTopics = new ArrayList<>();\n+            for (String topic : topicToTaskIds.keySet()) {\n+                List<PartitionInfo> partitions = streamThread.restoreConsumer.partitionsFor(topic);\n+                if (partitions == null) {\n+                    missingTopics.add(topic);\n+                }\n+            }\n+            if (!missingTopics.isEmpty()) {\n+                log.warn(\"Topic {} do not exists but couldn't created as the config '{}' isn't supplied\",\n+                         missingTopics, StreamsConfig.ZOOKEEPER_CONNECT_CONFIG);\n+\n+            }\n         }\n \n         return partitionInfos;\n@@ -284,7 +293,7 @@ public Subscription subscription(Set<String> topics) {\n             }\n         }\n \n-        Map<TopicPartition, PartitionInfo> internalPartitionInfos = prepareTopic(internalSourceTopicToTaskIds, false, true, false);\n+        Map<TopicPartition, PartitionInfo> internalPartitionInfos = prepareTopic(internalSourceTopicToTaskIds, false, false);\n         internalSourceTopicToTaskIds.clear();\n \n         Cluster metadataWithInternalTopics = metadata;\n@@ -380,9 +389,9 @@ public Subscription subscription(Set<String> topics) {\n         }\n \n         // if ZK is specified, validate the internal topics again\n-        prepareTopic(internalSourceTopicToTaskIds, false /* compactTopic */, false, true);\n+        prepareTopic(internalSourceTopicToTaskIds, false /* compactTopic */, true);\n         // change log topics should be compacted\n-        prepareTopic(stateChangelogTopicToTaskIds, true /* compactTopic */, false, true);\n+        prepareTopic(stateChangelogTopicToTaskIds, true /* compactTopic */, true);\n \n         return assignment;\n     }", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java"}, {"additions": 2, "raw_url": "https://github.com/apache/kafka/raw/1f528815de8d0e094ee5446794ab7325629ca7ed/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java", "blob_url": "https://github.com/apache/kafka/blob/1f528815de8d0e094ee5446794ab7325629ca7ed/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java", "sha": "890af0fc1e864487fa005d7692f8ef706afd7a06", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java?ref=1f528815de8d0e094ee5446794ab7325629ca7ed", "patch": "@@ -21,14 +21,14 @@\n import org.apache.kafka.clients.consumer.ConsumerRecords;\n import org.apache.kafka.clients.consumer.MockConsumer;\n import org.apache.kafka.clients.consumer.OffsetResetStrategy;\n-import org.apache.kafka.common.KafkaException;\n import org.apache.kafka.common.Node;\n import org.apache.kafka.common.PartitionInfo;\n import org.apache.kafka.common.TopicPartition;\n import org.apache.kafka.common.record.TimestampType;\n import org.apache.kafka.common.serialization.IntegerSerializer;\n import org.apache.kafka.common.serialization.Serializer;\n import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.errors.StreamsException;\n import org.apache.kafka.streams.state.internals.OffsetCheckpoint;\n import org.apache.kafka.test.MockStateStoreSupplier;\n import org.junit.Test;\n@@ -223,7 +223,7 @@ public void testLockStateDirectory() throws IOException {\n         }\n     }\n \n-    @Test(expected = KafkaException.class)\n+    @Test(expected = StreamsException.class)\n     public void testNoTopic() throws IOException {\n         File baseDir = Files.createTempDirectory(stateDir).toFile();\n         try {", "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/ProcessorStateManagerTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/218e6a53c1385be897d9f8a3a39baa38b68d7992", "parent": "https://github.com/apache/kafka/commit/e7edb5e1e933f5535378d546bcf4d8b178d2e69c", "message": "KAFKA-771 NPE in handleOffsetCommitRequest; reviewed by Neha Narkhede", "bug_id": "kafka_47", "file": [{"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/218e6a53c1385be897d9f8a3a39baa38b68d7992/core/src/main/scala/kafka/server/KafkaApis.scala", "blob_url": "https://github.com/apache/kafka/blob/218e6a53c1385be897d9f8a3a39baa38b68d7992/core/src/main/scala/kafka/server/KafkaApis.scala", "sha": "c059981887f0675cb7a10519142bfa3e08c1462a", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/KafkaApis.scala?ref=218e6a53c1385be897d9f8a3a39baa38b68d7992", "patch": "@@ -476,7 +476,7 @@ class KafkaApis(val requestChannel: RequestChannel,\n     val responseInfo = offsetCommitRequest.requestInfo.map( t => {\n       val topicDirs = new ZKGroupTopicDirs(offsetCommitRequest.groupId, t._1.topic)\n       try {\n-        if(t._2.metadata.length > config.offsetMetadataMaxSize) {\n+        if(t._2.metadata != null && t._2.metadata.length > config.offsetMetadataMaxSize) {\n           (t._1, ErrorMapping.OffsetMetadataTooLargeCode)\n         } else {\n           ZkUtils.updatePersistentPath(zkClient, topicDirs.consumerOffsetDir + \"/\" +", "filename": "core/src/main/scala/kafka/server/KafkaApis.scala"}, {"additions": 10, "raw_url": "https://github.com/apache/kafka/raw/218e6a53c1385be897d9f8a3a39baa38b68d7992/core/src/test/scala/unit/kafka/server/OffsetCommitTest.scala", "blob_url": "https://github.com/apache/kafka/blob/218e6a53c1385be897d9f8a3a39baa38b68d7992/core/src/test/scala/unit/kafka/server/OffsetCommitTest.scala", "sha": "6989c95e611b937fdb401c37be98c6c0e6a0ccfb", "changes": 10, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/server/OffsetCommitTest.scala?ref=218e6a53c1385be897d9f8a3a39baa38b68d7992", "patch": "@@ -171,4 +171,14 @@ class OffsetCommitTest extends JUnit3Suite with ZooKeeperTestHarness {\n \n   }\n \n+  @Test\n+  def testNullMetadata() {\n+    val topicAndPartition = TopicAndPartition(\"null-metadata\", 0)\n+    val commitRequest = OffsetCommitRequest(\"test-group\", Map(topicAndPartition -> OffsetMetadataAndError(\n+      offset=42L,\n+      metadata=null\n+    )))\n+    val commitResponse = simpleConsumer.commitOffsets(commitRequest)\n+    assertEquals(ErrorMapping.NoError, commitResponse.requestInfo.get(topicAndPartition).get)\n+  }\n }", "filename": "core/src/test/scala/unit/kafka/server/OffsetCommitTest.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/65f6a7964f5e59e789eae3cdd4d301bb6a649064", "parent": "https://github.com/apache/kafka/commit/83116c733d09f5c85ced56701cdb614ce6efc847", "message": "KAFKA-4300: NamedCache throws an NPE when evict is called and the cache is empty\n\nIf evict is called on a NamedCache and the cache is empty an NPE is thrown. This was reported on the user list from a developer running 0.10.1.\n\nAuthor: Damian Guy <damian.guy@gmail.com>\n\nReviewers: Eno Thereska, Matthias J. Sax, Guozhang Wang\n\nCloses #2024 from dguy/cache-bug", "bug_id": "kafka_48", "file": [{"additions": 3, "raw_url": "https://github.com/apache/kafka/raw/65f6a7964f5e59e789eae3cdd4d301bb6a649064/streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java", "blob_url": "https://github.com/apache/kafka/blob/65f6a7964f5e59e789eae3cdd4d301bb6a649064/streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java", "sha": "65a836eb9d9d4eff1b65a908631961b677575d84", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java?ref=65f6a7964f5e59e789eae3cdd4d301bb6a649064", "patch": "@@ -190,6 +190,9 @@ private void putHead(LRUNode node) {\n     }\n \n     synchronized void evict() {\n+        if (tail == null) {\n+            return;\n+        }\n         final LRUNode eldest = tail;\n         currentSizeBytes -= eldest.size();\n         if (eldest.entry.isDirty()) {", "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java"}, {"additions": 5, "raw_url": "https://github.com/apache/kafka/raw/65f6a7964f5e59e789eae3cdd4d301bb6a649064/streams/src/test/java/org/apache/kafka/streams/state/internals/NamedCacheTest.java", "blob_url": "https://github.com/apache/kafka/blob/65f6a7964f5e59e789eae3cdd4d301bb6a649064/streams/src/test/java/org/apache/kafka/streams/state/internals/NamedCacheTest.java", "sha": "3067256b5a994b8f50ce20b338328da66a21f142", "changes": 5, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/NamedCacheTest.java?ref=65f6a7964f5e59e789eae3cdd4d301bb6a649064", "patch": "@@ -186,4 +186,9 @@ public void shouldGetIteratorOverAllKeys() throws Exception {\n         assertEquals(Bytes.wrap(new byte[]{2}), iterator.next());\n         assertFalse(iterator.hasNext());\n     }\n+\n+    @Test\n+    public void shouldNotThrowNullPointerWhenCacheIsEmptyAndEvictionCalled() throws Exception {\n+        cache.evict();\n+    }\n }\n\\ No newline at end of file", "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/NamedCacheTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/d01f01ec63f61bd4742f02abff2ab6cf339e2897", "parent": "https://github.com/apache/kafka/commit/18e34482e685098ebef90233d78ae803f906633a", "message": "KAFKA-6260; Ensure selection keys are removed from all collections on socket close\n\nWhen a socket is closed, we must remove corresponding selection keys from internal collections. This fixes an NPE which is caused by attempting to access the selection key's attached channel after it had been cleared after disconnecting.\n\nAuthor: Jason Gustafson <jason@confluent.io>\n\nReviewers: Ismael Juma <ismael@juma.me.uk>, Rajini Sivaram <rajinisivaram@googlemail.com>\n\nCloses #4276 from hachikuji/KAFKA-6260", "bug_id": "kafka_49", "file": [{"additions": 4, "raw_url": "https://github.com/apache/kafka/raw/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/KafkaChannel.java", "blob_url": "https://github.com/apache/kafka/blob/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/KafkaChannel.java", "sha": "e125bbca10e82876c35b60e8c236d65ebcad8e96", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/network/KafkaChannel.java?ref=d01f01ec63f61bd4742f02abff2ab6cf339e2897", "patch": "@@ -117,6 +117,10 @@ public String id() {\n         return id;\n     }\n \n+    public SelectionKey selectionKey() {\n+        return transportLayer.selectionKey();\n+    }\n+\n     /**\n      * externally muting a channel should be done via selector to ensure proper state handling\n      */", "filename": "clients/src/main/java/org/apache/kafka/common/network/KafkaChannel.java"}, {"additions": 7, "raw_url": "https://github.com/apache/kafka/raw/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/PlaintextTransportLayer.java", "blob_url": "https://github.com/apache/kafka/blob/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/PlaintextTransportLayer.java", "sha": "ccb9c606185faf8bfe37b2e8938292b8a049ed67", "changes": 21, "status": "modified", "deletions": 14, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/network/PlaintextTransportLayer.java?ref=d01f01ec63f61bd4742f02abff2ab6cf339e2897", "patch": "@@ -63,6 +63,11 @@ public SocketChannel socketChannel() {\n         return socketChannel;\n     }\n \n+    @Override\n+    public SelectionKey selectionKey() {\n+        return key;\n+    }\n+\n     @Override\n     public boolean isOpen() {\n         return socketChannel.isOpen();\n@@ -73,20 +78,10 @@ public boolean isConnected() {\n         return socketChannel.isConnected();\n     }\n \n-    /**\n-     * Closes this channel\n-     *\n-     * @throws IOException If I/O error occurs\n-     */\n     @Override\n     public void close() throws IOException {\n-        try {\n-            socketChannel.socket().close();\n-            socketChannel.close();\n-        } finally {\n-            key.attach(null);\n-            key.cancel();\n-        }\n+        socketChannel.socket().close();\n+        socketChannel.close();\n     }\n \n     /**\n@@ -191,7 +186,6 @@ public Principal peerPrincipal() throws IOException {\n \n     /**\n      * Adds the interestOps to selectionKey.\n-     * @param ops\n      */\n     @Override\n     public void addInterestOps(int ops) {\n@@ -201,7 +195,6 @@ public void addInterestOps(int ops) {\n \n     /**\n      * Removes the interestOps from selectionKey.\n-     * @param ops\n      */\n     @Override\n     public void removeInterestOps(int ops) {", "filename": "clients/src/main/java/org/apache/kafka/common/network/PlaintextTransportLayer.java"}, {"additions": 88, "raw_url": "https://github.com/apache/kafka/raw/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/Selector.java", "blob_url": "https://github.com/apache/kafka/blob/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/Selector.java", "sha": "6bfcfd21a90e89a90b3eecdd7acce205d5a8308d", "changes": 162, "status": "modified", "deletions": 74, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/network/Selector.java?ref=d01f01ec63f61bd4742f02abff2ab6cf339e2897", "patch": "@@ -16,6 +16,23 @@\n  */\n package org.apache.kafka.common.network;\n \n+import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.MetricName;\n+import org.apache.kafka.common.errors.AuthenticationException;\n+import org.apache.kafka.common.memory.MemoryPool;\n+import org.apache.kafka.common.metrics.Measurable;\n+import org.apache.kafka.common.metrics.MetricConfig;\n+import org.apache.kafka.common.metrics.Metrics;\n+import org.apache.kafka.common.metrics.Sensor;\n+import org.apache.kafka.common.metrics.stats.Avg;\n+import org.apache.kafka.common.metrics.stats.Count;\n+import org.apache.kafka.common.metrics.stats.Max;\n+import org.apache.kafka.common.metrics.stats.Meter;\n+import org.apache.kafka.common.metrics.stats.SampledStat;\n+import org.apache.kafka.common.utils.LogContext;\n+import org.apache.kafka.common.utils.Time;\n+import org.slf4j.Logger;\n+\n import java.io.IOException;\n import java.net.InetSocketAddress;\n import java.net.Socket;\n@@ -37,23 +54,6 @@\n import java.util.Set;\n import java.util.concurrent.TimeUnit;\n \n-import org.apache.kafka.common.KafkaException;\n-import org.apache.kafka.common.memory.MemoryPool;\n-import org.apache.kafka.common.metrics.Measurable;\n-import org.apache.kafka.common.metrics.MetricConfig;\n-import org.apache.kafka.common.MetricName;\n-import org.apache.kafka.common.errors.AuthenticationException;\n-import org.apache.kafka.common.metrics.Metrics;\n-import org.apache.kafka.common.metrics.Sensor;\n-import org.apache.kafka.common.metrics.stats.Avg;\n-import org.apache.kafka.common.metrics.stats.Meter;\n-import org.apache.kafka.common.metrics.stats.SampledStat;\n-import org.apache.kafka.common.metrics.stats.Count;\n-import org.apache.kafka.common.metrics.stats.Max;\n-import org.apache.kafka.common.utils.LogContext;\n-import org.apache.kafka.common.utils.Time;\n-import org.slf4j.Logger;\n-\n /**\n  * A nioSelector interface for doing non-blocking multi-connection network I/O.\n  * <p>\n@@ -195,12 +195,37 @@ public Selector(long connectionMaxIdleMS, Metrics metrics, Time time, String met\n      */\n     @Override\n     public void connect(String id, InetSocketAddress address, int sendBufferSize, int receiveBufferSize) throws IOException {\n-        if (this.channels.containsKey(id))\n-            throw new IllegalStateException(\"There is already a connection for id \" + id);\n-        if (this.closingChannels.containsKey(id))\n-            throw new IllegalStateException(\"There is already a connection for id \" + id + \" that is still being closed\");\n-\n+        ensureNotRegistered(id);\n         SocketChannel socketChannel = SocketChannel.open();\n+        try {\n+            configureSocketChannel(socketChannel, sendBufferSize, receiveBufferSize);\n+            boolean connected = doConnect(socketChannel, address);\n+            SelectionKey key = registerChannel(id, socketChannel, SelectionKey.OP_CONNECT);\n+\n+            if (connected) {\n+                // OP_CONNECT won't trigger for immediately connected channels\n+                log.debug(\"Immediately connected to node {}\", id);\n+                immediatelyConnectedKeys.add(key);\n+                key.interestOps(0);\n+            }\n+        } catch (IOException | RuntimeException e) {\n+            socketChannel.close();\n+            throw e;\n+        }\n+    }\n+\n+    // Visible to allow test cases to override. In particular, we use this to implement a blocking connect\n+    // in order to simulate \"immediately connected\" sockets.\n+    protected boolean doConnect(SocketChannel channel, InetSocketAddress address) throws IOException {\n+        try {\n+            return channel.connect(address);\n+        } catch (UnresolvedAddressException e) {\n+            throw new IOException(\"Can't resolve address: \" + address, e);\n+        }\n+    }\n+\n+    private void configureSocketChannel(SocketChannel socketChannel, int sendBufferSize, int receiveBufferSize)\n+            throws IOException {\n         socketChannel.configureBlocking(false);\n         Socket socket = socketChannel.socket();\n         socket.setKeepAlive(true);\n@@ -209,25 +234,6 @@ public void connect(String id, InetSocketAddress address, int sendBufferSize, in\n         if (receiveBufferSize != Selectable.USE_DEFAULT_BUFFER_SIZE)\n             socket.setReceiveBufferSize(receiveBufferSize);\n         socket.setTcpNoDelay(true);\n-        boolean connected;\n-        try {\n-            connected = socketChannel.connect(address);\n-        } catch (UnresolvedAddressException e) {\n-            socketChannel.close();\n-            throw new IOException(\"Can't resolve address: \" + address, e);\n-        } catch (IOException e) {\n-            socketChannel.close();\n-            throw e;\n-        }\n-        SelectionKey key = socketChannel.register(nioSelector, SelectionKey.OP_CONNECT);\n-        KafkaChannel channel = buildChannel(socketChannel, id, key);\n-\n-        if (connected) {\n-            // OP_CONNECT won't trigger for immediately connected channels\n-            log.debug(\"Immediately connected to node {}\", channel.id());\n-            immediatelyConnectedKeys.add(key);\n-            key.interestOps(0);\n-        }\n     }\n \n     /**\n@@ -245,19 +251,29 @@ public void connect(String id, InetSocketAddress address, int sendBufferSize, in\n      * </p>\n      */\n     public void register(String id, SocketChannel socketChannel) throws IOException {\n+        ensureNotRegistered(id);\n+        registerChannel(id, socketChannel, SelectionKey.OP_READ);\n+    }\n+\n+    private void ensureNotRegistered(String id) {\n         if (this.channels.containsKey(id))\n             throw new IllegalStateException(\"There is already a connection for id \" + id);\n         if (this.closingChannels.containsKey(id))\n             throw new IllegalStateException(\"There is already a connection for id \" + id + \" that is still being closed\");\n+    }\n \n-        SelectionKey key = socketChannel.register(nioSelector, SelectionKey.OP_READ);\n-        buildChannel(socketChannel, id, key);\n+    private SelectionKey registerChannel(String id, SocketChannel socketChannel, int interestedOps) throws IOException {\n+        SelectionKey key = socketChannel.register(nioSelector, interestedOps);\n+        KafkaChannel channel = buildAndAttachKafkaChannel(socketChannel, id, key);\n+        this.channels.put(id, channel);\n+        return key;\n     }\n \n-    private KafkaChannel buildChannel(SocketChannel socketChannel, String id, SelectionKey key) throws IOException {\n-        KafkaChannel channel;\n+    private KafkaChannel buildAndAttachKafkaChannel(SocketChannel socketChannel, String id, SelectionKey key) throws IOException {\n         try {\n-            channel = channelBuilder.buildChannel(id, key, maxReceiveSize, memoryPool);\n+            KafkaChannel channel = channelBuilder.buildChannel(id, key, maxReceiveSize, memoryPool);\n+            key.attach(channel);\n+            return channel;\n         } catch (Exception e) {\n             try {\n                 socketChannel.close();\n@@ -266,9 +282,6 @@ private KafkaChannel buildChannel(SocketChannel socketChannel, String id, Select\n             }\n             throw new IOException(\"Channel could not be created for socket \" + socketChannel, e);\n         }\n-        key.attach(channel);\n-        this.channels.put(id, channel);\n-        return channel;\n     }\n \n     /**\n@@ -386,20 +399,22 @@ public void poll(long timeout) throws IOException {\n \n         if (numReadyKeys > 0 || !immediatelyConnectedKeys.isEmpty() || dataInBuffers) {\n             Set<SelectionKey> readyKeys = this.nioSelector.selectedKeys();\n-            keysWithBufferedRead.removeAll(readyKeys); //so no channel gets polled twice\n \n-            //poll from channels that have buffered data (but nothing more from the underlying socket)\n-            if (!keysWithBufferedRead.isEmpty()) {\n+            // Poll from channels that have buffered data (but nothing more from the underlying socket)\n+            if (dataInBuffers) {\n+                keysWithBufferedRead.removeAll(readyKeys); //so no channel gets polled twice\n                 Set<SelectionKey> toPoll = keysWithBufferedRead;\n                 keysWithBufferedRead = new HashSet<>(); //poll() calls will repopulate if needed\n                 pollSelectionKeys(toPoll, false, endSelect);\n             }\n-            //poll from channels where the underlying socket has more data\n-            pollSelectionKeys(readyKeys, false, endSelect);\n-            pollSelectionKeys(immediatelyConnectedKeys, true, endSelect);\n \n+            // Poll from channels where the underlying socket has more data\n+            pollSelectionKeys(readyKeys, false, endSelect);\n             // Clear all selected keys so that they are included in the ready count for the next select\n             readyKeys.clear();\n+\n+            pollSelectionKeys(immediatelyConnectedKeys, true, endSelect);\n+            immediatelyConnectedKeys.clear();\n         } else {\n             madeReadProgressLastPoll = true; //no work is also \"progress\"\n         }\n@@ -424,11 +439,9 @@ public void poll(long timeout) throws IOException {\n      */\n     // package-private for testing\n     void pollSelectionKeys(Set<SelectionKey> selectionKeys,\n-                                   boolean isImmediatelyConnected,\n-                                   long currentTimeNanos) {\n-        Iterator<SelectionKey> iterator = determineHandlingOrder(selectionKeys).iterator();\n-        while (iterator.hasNext()) {\n-            SelectionKey key = iterator.next();\n+                           boolean isImmediatelyConnected,\n+                           long currentTimeNanos) {\n+        for (SelectionKey key : determineHandlingOrder(selectionKeys)) {\n             KafkaChannel channel = channel(key);\n             long channelStartTimeNanos = recordTimePerConnection ? time.nanoseconds() : 0;\n \n@@ -507,16 +520,13 @@ else if (e instanceof AuthenticationException) // will be logged later as error\n     private Collection<SelectionKey> determineHandlingOrder(Set<SelectionKey> selectionKeys) {\n         //it is possible that the iteration order over selectionKeys is the same every invocation.\n         //this may cause starvation of reads when memory is low. to address this we shuffle the keys if memory is low.\n-        Collection<SelectionKey> inHandlingOrder;\n-\n         if (!outOfMemory && memoryPool.availableMemory() < lowMemThreshold) {\n-            List<SelectionKey> temp = new ArrayList<>(selectionKeys);\n-            Collections.shuffle(temp);\n-            inHandlingOrder = temp;\n+            List<SelectionKey> shuffledKeys = new ArrayList<>(selectionKeys);\n+            Collections.shuffle(shuffledKeys);\n+            return shuffledKeys;\n         } else {\n-            inHandlingOrder = selectionKeys;\n+            return selectionKeys;\n         }\n-        return inHandlingOrder;\n     }\n \n     private void attemptRead(SelectionKey key, KafkaChannel channel) throws IOException {\n@@ -642,19 +652,17 @@ private void clear() {\n     /**\n      * Check for data, waiting up to the given timeout.\n      *\n-     * @param ms Length of time to wait, in milliseconds, which must be non-negative\n+     * @param timeoutMs Length of time to wait, in milliseconds, which must be non-negative\n      * @return The number of keys ready\n-     * @throws IllegalArgumentException\n-     * @throws IOException\n      */\n-    private int select(long ms) throws IOException {\n-        if (ms < 0L)\n+    private int select(long timeoutMs) throws IOException {\n+        if (timeoutMs < 0L)\n             throw new IllegalArgumentException(\"timeout should be >= 0\");\n \n-        if (ms == 0L)\n+        if (timeoutMs == 0L)\n             return this.nioSelector.selectNow();\n         else\n-            return this.nioSelector.select(ms);\n+            return this.nioSelector.select(timeoutMs);\n     }\n \n     /**\n@@ -713,10 +721,16 @@ private void close(KafkaChannel channel, boolean processOutstanding) {\n     }\n \n     private void doClose(KafkaChannel channel, boolean notifyDisconnect) {\n+        SelectionKey key = channel.selectionKey();\n         try {\n+            immediatelyConnectedKeys.remove(key);\n+            keysWithBufferedRead.remove(key);\n             channel.close();\n         } catch (IOException e) {\n             log.error(\"Exception closing connection to node {}:\", channel.id(), e);\n+        } finally {\n+            key.cancel();\n+            key.attach(null);\n         }\n         this.sensors.connectionClosed.record();\n         this.stagedReceives.remove(channel);", "filename": "clients/src/main/java/org/apache/kafka/common/network/Selector.java"}, {"additions": 7, "raw_url": "https://github.com/apache/kafka/raw/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/SslTransportLayer.java", "blob_url": "https://github.com/apache/kafka/blob/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/SslTransportLayer.java", "sha": "69ca037266d9beb7917db6e79061586da2980f4a", "changes": 14, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/network/SslTransportLayer.java?ref=d01f01ec63f61bd4742f02abff2ab6cf339e2897", "patch": "@@ -130,6 +130,11 @@ public SocketChannel socketChannel() {\n         return socketChannel;\n     }\n \n+    @Override\n+    public SelectionKey selectionKey() {\n+        return key;\n+    }\n+\n     @Override\n     public boolean isOpen() {\n         return socketChannel.isOpen();\n@@ -169,13 +174,8 @@ public void close() throws IOException {\n         } catch (IOException ie) {\n             log.warn(\"Failed to send SSL Close message \", ie);\n         } finally {\n-            try {\n-                socketChannel.socket().close();\n-                socketChannel.close();\n-            } finally {\n-                key.attach(null);\n-                key.cancel();\n-            }\n+            socketChannel.socket().close();\n+            socketChannel.close();\n         }\n     }\n ", "filename": "clients/src/main/java/org/apache/kafka/common/network/SslTransportLayer.java"}, {"additions": 7, "raw_url": "https://github.com/apache/kafka/raw/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/TransportLayer.java", "blob_url": "https://github.com/apache/kafka/blob/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/main/java/org/apache/kafka/common/network/TransportLayer.java", "sha": "3673d21dae6b57b173074341aae2cc9d6f4cf5f9", "changes": 10, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/common/network/TransportLayer.java?ref=d01f01ec63f61bd4742f02abff2ab6cf339e2897", "patch": "@@ -25,6 +25,7 @@\n  */\n import java.io.IOException;\n import java.nio.channels.FileChannel;\n+import java.nio.channels.SelectionKey;\n import java.nio.channels.SocketChannel;\n import java.nio.channels.ScatteringByteChannel;\n import java.nio.channels.GatheringByteChannel;\n@@ -60,13 +61,16 @@\n      */\n     SocketChannel socketChannel();\n \n+    /**\n+     * Get the underlying selection key\n+     */\n+    SelectionKey selectionKey();\n \n     /**\n      * This a no-op for the non-secure PLAINTEXT implementation. For SSL, this performs\n      * SSL handshake. The SSL handshake includes client authentication if configured using\n-     * {@link org.apache.kafka.common.config.SslConfigsSslConfigs#SSL_CLIENT_AUTH_CONFIG}.\n-     * @throws AuthenticationException if handshake fails due to an\n-     *         {@link javax.net.ssl.SSLExceptionSSLException}.\n+     * {@link org.apache.kafka.common.config.SslConfigs#SSL_CLIENT_AUTH_CONFIG}.\n+     * @throws AuthenticationException if handshake fails due to an {@link javax.net.ssl.SSLException}.\n      * @throws IOException if read or write fails with an I/O error.\n     */\n     void handshake() throws AuthenticationException, IOException;", "filename": "clients/src/main/java/org/apache/kafka/common/network/TransportLayer.java"}, {"additions": 92, "raw_url": "https://github.com/apache/kafka/raw/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/test/java/org/apache/kafka/common/network/SelectorTest.java", "blob_url": "https://github.com/apache/kafka/blob/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/test/java/org/apache/kafka/common/network/SelectorTest.java", "sha": "f1c6a5af1420befb23877c1a2cfcdb8b321b09d2", "changes": 115, "status": "modified", "deletions": 23, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/network/SelectorTest.java?ref=d01f01ec63f61bd4742f02abff2ab6cf339e2897", "patch": "@@ -25,6 +25,7 @@\n import org.apache.kafka.common.utils.MockTime;\n import org.apache.kafka.common.utils.Time;\n import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.test.TestCondition;\n import org.apache.kafka.test.TestUtils;\n import org.easymock.IMocksControl;\n import org.junit.After;\n@@ -41,6 +42,7 @@\n import java.nio.channels.SelectionKey;\n import java.nio.channels.ServerSocketChannel;\n import java.nio.channels.SocketChannel;\n+import java.util.Collection;\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.List;\n@@ -104,16 +106,29 @@ public SecurityProtocol securityProtocol() {\n      */\n     @Test\n     public void testServerDisconnect() throws Exception {\n-        String node = \"0\";\n+        final String node = \"0\";\n \n         // connect and do a simple request\n         blockingConnect(node);\n         assertEquals(\"hello\", blockingRequest(node, \"hello\"));\n \n+        KafkaChannel channel = selector.channel(node);\n+\n         // disconnect\n         this.server.closeConnections();\n-        while (!selector.disconnected().containsKey(node))\n-            selector.poll(1000L);\n+        TestUtils.waitForCondition(new TestCondition() {\n+            @Override\n+            public boolean conditionMet() {\n+                try {\n+                    selector.poll(1000L);\n+                    return selector.disconnected().containsKey(node);\n+                } catch (IOException e) {\n+                    throw new RuntimeException(e);\n+                }\n+            }\n+        }, 5000, \"Failed to observe disconnected node in disconnected set\");\n+\n+        assertNull(channel.selectionKey().attachment());\n \n         // reconnect and do another request\n         blockingConnect(node);\n@@ -186,8 +201,8 @@ public void testNormalOperation() throws Exception {\n         for (int i = 0; i < conns; i++)\n             connect(Integer.toString(i), addr);\n         // send echo requests and receive responses\n-        Map<String, Integer> requests = new HashMap<String, Integer>();\n-        Map<String, Integer> responses = new HashMap<String, Integer>();\n+        Map<String, Integer> requests = new HashMap<>();\n+        Map<String, Integer> responses = new HashMap<>();\n         int responseCount = 0;\n         for (int i = 0; i < conns; i++) {\n             String node = Integer.toString(i);\n@@ -252,11 +267,6 @@ public void testLargeMessageSequence() throws Exception {\n         sendAndReceive(node, requestPrefix, 0, reqs);\n     }\n \n-\n-\n-    /**\n-     * Test sending an empty string\n-     */\n     @Test\n     public void testEmptyRequest() throws Exception {\n         String node = \"0\";\n@@ -333,6 +343,7 @@ public void testCloseConnectionInClosingState() throws Exception {\n         assertNull(\"Channel not removed from closingChannels\", selector.closingChannel(id));\n         assertTrue(\"Unexpected disconnect notification\", selector.disconnected().isEmpty());\n         assertEquals(ChannelState.EXPIRED, channel.state());\n+        assertNull(channel.selectionKey().attachment());\n         selector.poll(0);\n         assertTrue(\"Unexpected disconnect notification\", selector.disconnected().isEmpty());\n     }\n@@ -349,6 +360,41 @@ public void testCloseOldestConnection() throws Exception {\n         assertEquals(ChannelState.EXPIRED, selector.disconnected().get(id));\n     }\n \n+    @Test\n+    public void testImmediatelyConnectedCleaned() throws Exception {\n+        Metrics metrics = new Metrics(); // new metrics object to avoid metric registration conflicts\n+        Selector selector = new Selector(5000, metrics, time, \"MetricGroup\", channelBuilder, new LogContext()) {\n+            @Override\n+            protected boolean doConnect(SocketChannel channel, InetSocketAddress address) throws IOException {\n+                // Use a blocking connect to trigger the immediately connected path\n+                channel.configureBlocking(true);\n+                boolean connected = super.doConnect(channel, address);\n+                channel.configureBlocking(false);\n+                return connected;\n+            }\n+        };\n+\n+        try {\n+            testImmediatelyConnectedCleaned(selector, true);\n+            testImmediatelyConnectedCleaned(selector, false);\n+        } finally {\n+            selector.close();\n+            metrics.close();\n+        }\n+    }\n+\n+    private void testImmediatelyConnectedCleaned(Selector selector, boolean closeAfterFirstPoll) throws Exception {\n+        String id = \"0\";\n+        selector.connect(id, new InetSocketAddress(\"localhost\", server.port), BUFFER_SIZE, BUFFER_SIZE);\n+        verifyNonEmptyImmediatelyConnectedKeys(selector);\n+        if (closeAfterFirstPoll) {\n+            selector.poll(0);\n+            verifyEmptyImmediatelyConnectedKeys(selector);\n+        }\n+        selector.close(id);\n+        verifySelectorEmpty(selector);\n+    }\n+\n     @Test\n     public void testCloseOldestConnectionWithOneStagedReceive() throws Exception {\n         verifyCloseOldestConnectionWithStagedReceives(1);\n@@ -410,8 +456,6 @@ private void verifyCloseOldestConnectionWithStagedReceives(int maxStagedReceives\n         assertTrue(\"Unexpected receive\", selector.completedReceives().isEmpty());\n     }\n \n-\n-\n     @Test\n     public void testMuteOnOOM() throws Exception {\n         //clean up default selector, replace it with one that uses a finite mem pool\n@@ -517,8 +561,11 @@ public void testConnectDisconnectDuringInSinglePoll() throws Exception {\n         expectLastCall().andThrow(new IOException());\n \n         SelectionKey selectionKey = control.createMock(SelectionKey.class);\n+        expect(kafkaChannel.selectionKey()).andStubReturn(selectionKey);\n         expect(selectionKey.channel()).andReturn(SocketChannel.open());\n         expect(selectionKey.readyOps()).andStubReturn(SelectionKey.OP_CONNECT);\n+        selectionKey.cancel();\n+        expectLastCall();\n \n         control.replay();\n \n@@ -528,6 +575,7 @@ public void testConnectDisconnectDuringInSinglePoll() throws Exception {\n \n         assertFalse(selector.connected().contains(kafkaChannel.id()));\n         assertTrue(selector.disconnected().containsKey(kafkaChannel.id()));\n+        assertNull(selectionKey.attachment());\n \n         control.verify();\n     }\n@@ -551,6 +599,7 @@ protected void connect(String node, InetSocketAddress serverAddr) throws IOExcep\n     private void blockingConnect(String node) throws IOException {\n         blockingConnect(node, new InetSocketAddress(\"localhost\", server.port));\n     }\n+\n     protected void blockingConnect(String node, InetSocketAddress serverAddr) throws IOException {\n         selector.connect(node, serverAddr, BUFFER_SIZE, BUFFER_SIZE);\n         while (!selector.connected().contains(node))\n@@ -589,21 +638,41 @@ private void sendAndReceive(String node, String requestPrefix, int startIndex, i\n         }\n     }\n \n-    private void verifySelectorEmpty() throws Exception {\n-        for (KafkaChannel channel : selector.channels())\n+    private void verifyNonEmptyImmediatelyConnectedKeys(Selector selector) throws Exception {\n+        Field field = Selector.class.getDeclaredField(\"immediatelyConnectedKeys\");\n+        field.setAccessible(true);\n+        Collection<?> immediatelyConnectedKeys = (Collection<?>) field.get(selector);\n+        assertFalse(immediatelyConnectedKeys.isEmpty());\n+    }\n+\n+    private void verifyEmptyImmediatelyConnectedKeys(Selector selector) throws Exception {\n+        Field field = Selector.class.getDeclaredField(\"immediatelyConnectedKeys\");\n+        ensureEmptySelectorField(selector, field);\n+    }\n+\n+    protected void verifySelectorEmpty() throws Exception {\n+        verifySelectorEmpty(this.selector);\n+    }\n+\n+    private void verifySelectorEmpty(Selector selector) throws Exception {\n+        for (KafkaChannel channel : selector.channels()) {\n             selector.close(channel.id());\n+            assertNull(channel.selectionKey().attachment());\n+        }\n         selector.poll(0);\n         selector.poll(0); // Poll a second time to clear everything\n-        for (Field field : selector.getClass().getDeclaredFields()) {\n-            field.setAccessible(true);\n-            Object obj = field.get(selector);\n-            if (obj instanceof Set)\n-                assertTrue(\"Field not empty: \" + field + \" \" + obj, ((Set<?>) obj).isEmpty());\n-            else if (obj instanceof Map)\n-                assertTrue(\"Field not empty: \" + field + \" \" + obj, ((Map<?, ?>) obj).isEmpty());\n-            else if (obj instanceof List)\n-                assertTrue(\"Field not empty: \" + field + \" \" + obj, ((List<?>) obj).isEmpty());\n+        for (Field field : Selector.class.getDeclaredFields()) {\n+            ensureEmptySelectorField(selector, field);\n         }\n     }\n \n+    private void ensureEmptySelectorField(Selector selector, Field field) throws Exception {\n+        field.setAccessible(true);\n+        Object obj = field.get(selector);\n+        if (obj instanceof Collection)\n+            assertTrue(\"Field not empty: \" + field + \" \" + obj, ((Collection<?>) obj).isEmpty());\n+        else if (obj instanceof Map)\n+            assertTrue(\"Field not empty: \" + field + \" \" + obj, ((Map<?, ?>) obj).isEmpty());\n+    }\n+\n }", "filename": "clients/src/test/java/org/apache/kafka/common/network/SelectorTest.java"}, {"additions": 27, "raw_url": "https://github.com/apache/kafka/raw/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/test/java/org/apache/kafka/common/network/SslSelectorTest.java", "blob_url": "https://github.com/apache/kafka/blob/d01f01ec63f61bd4742f02abff2ab6cf339e2897/clients/src/test/java/org/apache/kafka/common/network/SslSelectorTest.java", "sha": "dc062ea3b6857494b90fe3726be962b603018be9", "changes": 27, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/common/network/SslSelectorTest.java?ref=d01f01ec63f61bd4742f02abff2ab6cf339e2897", "patch": "@@ -22,7 +22,9 @@\n import org.apache.kafka.common.security.auth.SecurityProtocol;\n import org.apache.kafka.common.utils.LogContext;\n import org.apache.kafka.common.utils.MockTime;\n+import org.apache.kafka.test.TestCondition;\n import org.apache.kafka.test.TestSslUtils;\n+import org.apache.kafka.test.TestUtils;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n@@ -77,6 +79,30 @@ public SecurityProtocol securityProtocol() {\n         return SecurityProtocol.PLAINTEXT;\n     }\n \n+    @Test\n+    public void testDisconnectWithIntermediateBufferedBytes() throws Exception {\n+        int requestSize = 100 * 1024;\n+        final String node = \"0\";\n+        connect(node, new InetSocketAddress(\"localhost\", server.port));\n+        String request = TestUtils.randomString(requestSize);\n+        selector.send(createSend(node, request));\n+\n+        TestUtils.waitForCondition(new TestCondition() {\n+            @Override\n+            public boolean conditionMet() {\n+                try {\n+                    selector.poll(0L);\n+                    return selector.channel(node).hasBytesBuffered();\n+                } catch (IOException e) {\n+                    throw new RuntimeException(e);\n+                }\n+            }\n+        }, 2000L, \"Failed to reach socket state with bytes buffered\");\n+\n+        selector.close(node);\n+        verifySelectorEmpty();\n+    }\n+\n     /**\n      * Renegotiation is not supported since it is potentially unsafe and it has been removed in TLS 1.3\n      */\n@@ -197,4 +223,5 @@ protected void connect(String node, InetSocketAddress serverAddr) throws IOExcep\n     private SslSender createSender(InetSocketAddress serverAddress, byte[] payload) {\n         return new SslSender(serverAddress, payload);\n     }\n+\n }", "filename": "clients/src/test/java/org/apache/kafka/common/network/SslSelectorTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/3d74196f205c53946b6fc3dd0501aa2095f0031a", "parent": "https://github.com/apache/kafka/commit/70afd5f9dd2eddc784a24fa2518992ef3371f0a4", "message": "KAFKA-4163: NPE in StreamsMetadataState during re-balance operations\n\nDuring rebalance operations the Cluster object gets set to Cluster.empty(). This can result in NPEs when doing certain operation on StreamsMetadataState. This should throw a StreamsException if the Cluster is empty as it is not yet (re-)initialized\n\nAuthor: Damian Guy <damian.guy@gmail.com>\n\nReviewers: Eno Thereska, Guozhang Wang\n\nCloses #1845 from dguy/streams-meta-hotfix", "bug_id": "kafka_50", "file": [{"additions": 6, "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java", "sha": "d88d09ef1d90d690bb749bf0dd453650e24696bb", "changes": 8, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a", "patch": "@@ -329,7 +329,8 @@ public void setUncaughtExceptionHandler(final Thread.UncaughtExceptionHandler eh\n      * @param key               Key to use to for partition\n      * @param keySerializer     Serializer for the key\n      * @param <K>               key type\n-     * @return  The {@link StreamsMetadata} for the storeName and key\n+     * @return  The {@link StreamsMetadata} for the storeName and key or {@link StreamsMetadata#NOT_AVAILABLE}\n+     * if streams is (re-)initializing\n      */\n     public <K> StreamsMetadata metadataForKey(final String storeName,\n                                               final K key,\n@@ -350,7 +351,8 @@ public void setUncaughtExceptionHandler(final Thread.UncaughtExceptionHandler eh\n      * @param key               Key to use to for partition\n      * @param partitioner       Partitioner for the store\n      * @param <K>               key type\n-     * @return  The {@link StreamsMetadata} for the storeName and key\n+     * @return  The {@link StreamsMetadata} for the storeName and key or {@link StreamsMetadata#NOT_AVAILABLE}\n+     * if streams is (re-)initializing\n      */\n     public <K> StreamsMetadata metadataForKey(final String storeName,\n                                               final K key,\n@@ -368,6 +370,8 @@ public void setUncaughtExceptionHandler(final Thread.UncaughtExceptionHandler eh\n      * @param queryableStoreType    accept only stores that are accepted by {@link QueryableStoreType#accepts(StateStore)}\n      * @param <T>                   return type\n      * @return  A facade wrapping the {@link org.apache.kafka.streams.processor.StateStore} instances\n+     * @throws org.apache.kafka.streams.errors.InvalidStateStoreException if the streams are (re-)initializing or\n+     * a store with storeName and queryableStoreType doesnt' exist.\n      */\n     public <T> T store(final String storeName, final QueryableStoreType<T> queryableStoreType) {\n         validateIsRunning();", "filename": "streams/src/main/java/org/apache/kafka/streams/KafkaStreams.java"}, {"additions": 21, "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java", "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java", "sha": "6f9bea69af2356ffafc38cdb5abe45161f932b82", "changes": 23, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a", "patch": "@@ -70,6 +70,10 @@ public StreamsMetadataState(final TopologyBuilder builder) {\n     public synchronized Collection<StreamsMetadata> getAllMetadataForStore(final String storeName) {\n         Objects.requireNonNull(storeName, \"storeName cannot be null\");\n \n+        if (!isInitialized()) {\n+            return Collections.emptyList();\n+        }\n+\n         final Set<String> sourceTopics = builder.stateStoreNameToSourceTopics().get(storeName);\n         if (sourceTopics == null) {\n             return Collections.emptyList();\n@@ -96,7 +100,8 @@ public StreamsMetadataState(final TopologyBuilder builder) {\n      * @param key           Key to use\n      * @param keySerializer Serializer for the key\n      * @param <K>           key type\n-     * @return The {@link StreamsMetadata} for the storeName and key\n+     * @return The {@link StreamsMetadata} for the storeName and key or {@link StreamsMetadata#NOT_AVAILABLE}\n+     * if streams is (re-)initializing\n      */\n     public synchronized <K> StreamsMetadata getMetadataWithKey(final String storeName,\n                                                                final K key,\n@@ -105,10 +110,15 @@ public StreamsMetadataState(final TopologyBuilder builder) {\n         Objects.requireNonNull(storeName, \"storeName can't be null\");\n         Objects.requireNonNull(key, \"key can't be null\");\n \n+        if (!isInitialized()) {\n+            return StreamsMetadata.NOT_AVAILABLE;\n+        }\n+\n         final SourceTopicsInfo sourceTopicsInfo = getSourceTopicsInfo(storeName);\n         if (sourceTopicsInfo == null) {\n             return null;\n         }\n+\n         return getStreamsMetadataForKey(storeName,\n                                         key,\n                                         new DefaultStreamPartitioner<>(keySerializer,\n@@ -131,7 +141,8 @@ public StreamsMetadataState(final TopologyBuilder builder) {\n      * @param key         Key to use\n      * @param partitioner partitioner to use to find correct partition for key\n      * @param <K>         key type\n-     * @return The {@link StreamsMetadata} for the storeName and key\n+     * @return The {@link StreamsMetadata} for the storeName and key or {@link StreamsMetadata#NOT_AVAILABLE}\n+     * if streams is (re-)initializing\n      */\n     public synchronized <K> StreamsMetadata getMetadataWithKey(final String storeName,\n                                                                final K key,\n@@ -140,6 +151,10 @@ public StreamsMetadataState(final TopologyBuilder builder) {\n         Objects.requireNonNull(key, \"key can't be null\");\n         Objects.requireNonNull(partitioner, \"partitioner can't be null\");\n \n+        if (!isInitialized()) {\n+            return StreamsMetadata.NOT_AVAILABLE;\n+        }\n+\n         SourceTopicsInfo sourceTopicsInfo = getSourceTopicsInfo(storeName);\n         if (sourceTopicsInfo == null) {\n             return null;\n@@ -218,6 +233,10 @@ private SourceTopicsInfo getSourceTopicsInfo(final String storeName) {\n         return new SourceTopicsInfo(sourceTopics);\n     }\n \n+    private boolean isInitialized() {\n+        return !clusterMetadata.topics().isEmpty();\n+    }\n+\n     private class SourceTopicsInfo {\n         private final Set<String> sourceTopics;\n         private int maxPartitions;", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsMetadataState.java"}, {"additions": 9, "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/StreamsMetadata.java", "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/StreamsMetadata.java", "sha": "9602bfe10a5ab18064712f3c04894ca4139063fb", "changes": 9, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/StreamsMetadata.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a", "patch": "@@ -19,6 +19,7 @@\n import org.apache.kafka.common.TopicPartition;\n import org.apache.kafka.streams.KafkaStreams;\n \n+import java.util.Collections;\n import java.util.Set;\n \n /**\n@@ -29,6 +30,14 @@\n  * NOTE: This is a point in time view. It may change when rebalances happen.\n  */\n public class StreamsMetadata {\n+    /**\n+     * Sentinel to indicate that the StreamsMetadata is currently unavailable. This can occur during rebalance\n+     * operations.\n+     */\n+    public final static StreamsMetadata NOT_AVAILABLE = new StreamsMetadata(new HostInfo(\"unavailable\", -1),\n+                                                                            Collections.<String>emptySet(),\n+                                                                            Collections.<TopicPartition>emptySet());\n+\n     private final HostInfo hostInfo;\n     private final Set<String> stateStoreNames;\n     private final Set<TopicPartition> topicPartitions;", "filename": "streams/src/main/java/org/apache/kafka/streams/state/StreamsMetadata.java"}, {"additions": 19, "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStore.java", "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStore.java", "sha": "5c47419a1bc2539831260ed92bd819cff67e5e77", "changes": 24, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStore.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a", "patch": "@@ -15,6 +15,7 @@\n package org.apache.kafka.streams.state.internals;\n \n import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n import org.apache.kafka.streams.state.KeyValueIterator;\n import org.apache.kafka.streams.state.QueryableStoreType;\n import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n@@ -48,10 +49,15 @@ public CompositeReadOnlyKeyValueStore(final StateStoreProvider storeProvider,\n     public V get(final K key) {\n         final List<ReadOnlyKeyValueStore<K, V>> stores = storeProvider.stores(storeName, storeType);\n         for (ReadOnlyKeyValueStore<K, V> store : stores) {\n-            V result = store.get(key);\n-            if (result != null) {\n-                return result;\n+            try {\n+                final V result = store.get(key);\n+                if (result != null) {\n+                    return result;\n+                }\n+            } catch (InvalidStateStoreException e) {\n+                throw new InvalidStateStoreException(\"State store is not available anymore and may have been migrated to another instance; please re-discover its location from the state metadata.\");\n             }\n+\n         }\n         return null;\n     }\n@@ -61,7 +67,11 @@ public V get(final K key) {\n         final NextIteratorFunction<K, V> nextIteratorFunction = new NextIteratorFunction<K, V>() {\n             @Override\n             public KeyValueIterator<K, V> apply(final ReadOnlyKeyValueStore<K, V> store) {\n-                return store.range(from, to);\n+                try {\n+                    return store.range(from, to);\n+                } catch (InvalidStateStoreException e) {\n+                    throw new InvalidStateStoreException(\"State store is not available anymore and may have been migrated to another instance; please re-discover its location from the state metadata.\");\n+                }\n             }\n         };\n         final List<ReadOnlyKeyValueStore<K, V>> stores = storeProvider.stores(storeName, storeType);\n@@ -73,7 +83,11 @@ public V get(final K key) {\n         final NextIteratorFunction<K, V> nextIteratorFunction = new NextIteratorFunction<K, V>() {\n             @Override\n             public KeyValueIterator<K, V> apply(final ReadOnlyKeyValueStore<K, V> store) {\n-                return store.all();\n+                try {\n+                    return store.all();\n+                } catch (InvalidStateStoreException e) {\n+                    throw new InvalidStateStoreException(\"State store is not available anymore and may have been migrated to another instance; please re-discover its location from the state metadata.\");\n+                }\n             }\n         };\n         final List<ReadOnlyKeyValueStore<K, V>> stores = storeProvider.stores(storeName, storeType);", "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStore.java"}, {"additions": 10, "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStore.java", "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStore.java", "sha": "b33c0f0ceab3dc4d1b6854381bde22a14c068a6c", "changes": 15, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStore.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a", "patch": "@@ -15,6 +15,7 @@\n package org.apache.kafka.streams.state.internals;\n \n import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n import org.apache.kafka.streams.state.QueryableStoreType;\n import org.apache.kafka.streams.state.ReadOnlyWindowStore;\n import org.apache.kafka.streams.state.WindowStoreIterator;\n@@ -44,11 +45,15 @@ public CompositeReadOnlyWindowStore(final StateStoreProvider provider,\n     public WindowStoreIterator<V> fetch(final K key, final long timeFrom, final long timeTo) {\n         final List<ReadOnlyWindowStore<K, V>> stores = provider.stores(storeName, windowStoreType);\n         for (ReadOnlyWindowStore<K, V> windowStore : stores) {\n-            final WindowStoreIterator<V> result = windowStore.fetch(key, timeFrom, timeTo);\n-            if (!result.hasNext()) {\n-                result.close();\n-            } else {\n-                return result;\n+            try {\n+                final WindowStoreIterator<V> result = windowStore.fetch(key, timeFrom, timeTo);\n+                if (!result.hasNext()) {\n+                    result.close();\n+                } else {\n+                    return result;\n+                }\n+            } catch (InvalidStateStoreException e) {\n+                throw new InvalidStateStoreException(\"State store is not available anymore and may have been migrated to another instance; please re-discover its location from the state metadata.\");\n             }\n         }\n         return new WindowStoreIterator<V>() {", "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStore.java"}, {"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/QueryableStoreProvider.java", "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/QueryableStoreProvider.java", "sha": "64dac1f24b124f82aa407652114078fd6da867af", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/QueryableStoreProvider.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a", "patch": "@@ -46,7 +46,7 @@ public QueryableStoreProvider(final List<StateStoreProvider> storeProviders) {\n             allStores.addAll(storeProvider.stores(storeName, queryableStoreType));\n         }\n         if (allStores.isEmpty()) {\n-            throw new InvalidStateStoreException(\"Store: \" + storeName + \" is currently not available\");\n+            throw new InvalidStateStoreException(\"the state store, \" + storeName + \", may have migrated to another instance.\");\n         }\n         return queryableStoreType.create(\n                 new WrappingStoreProvider(storeProviders),", "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/QueryableStoreProvider.java"}, {"additions": 2, "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProvider.java", "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProvider.java", "sha": "3a50a68a7d09bbab9e42b29bd882ff4b4ddce3b1", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProvider.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a", "patch": "@@ -39,14 +39,14 @@ public StreamThreadStateStoreProvider(final StreamThread streamThread) {\n     @Override\n     public <T> List<T> stores(final String storeName, final QueryableStoreType<T> queryableStoreType) {\n         if (!streamThread.isInitialized()) {\n-            throw new InvalidStateStoreException(\"Store: \" + storeName + \" is currently not available as the stream thread has not (re-)initialized yet\");\n+            throw new InvalidStateStoreException(\"the state store, \" + storeName + \", may have migrated to another instance.\");\n         }\n         final List<T> stores = new ArrayList<>();\n         for (StreamTask streamTask : streamThread.tasks().values()) {\n             final StateStore store = streamTask.getStore(storeName);\n             if (store != null && queryableStoreType.accepts(store)) {\n                 if (!store.isOpen()) {\n-                    throw new InvalidStateStoreException(\"Store: \" + storeName + \" isn't isOpen\");\n+                    throw new InvalidStateStoreException(\"the state store, \" + storeName + \", may have migrated to another instance.\");\n                 }\n                 stores.add((T) store);\n             }", "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/StreamThreadStateStoreProvider.java"}, {"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/WrappingStoreProvider.java", "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/main/java/org/apache/kafka/streams/state/internals/WrappingStoreProvider.java", "sha": "eb1bc6473870a00cd1b68df1de82673fe4c0c72e", "changes": 3, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/WrappingStoreProvider.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a", "patch": "@@ -48,8 +48,7 @@ public WrappingStoreProvider(final List<StateStoreProvider> storeProviders) {\n             allStores.addAll(stores);\n         }\n         if (allStores.isEmpty()) {\n-            throw new InvalidStateStoreException(\"Store \" + storeName + \" is currently \"\n-                                                 + \"unavailable\");\n+            throw new InvalidStateStoreException(\"the state store, \" + storeName + \", may have migrated to another instance.\");\n         }\n         return allStores;\n     }", "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/WrappingStoreProvider.java"}, {"additions": 21, "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java", "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java", "sha": "e6d7be8129ec3b5f8f08ff4fd576c094c2ca817a", "changes": 45, "status": "modified", "deletions": 24, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a", "patch": "@@ -74,8 +74,6 @@\n import static org.hamcrest.MatcherAssert.assertThat;\n import static org.hamcrest.core.IsEqual.equalTo;\n \n-\n-\n @RunWith(Parameterized.class)\n public class QueryableStateIntegrationTest {\n     private static final int NUM_BROKERS = 1;\n@@ -265,24 +263,23 @@ private void verifyAllKVKeys(final StreamRunnable[] streamRunnables, final Kafka\n             TestUtils.waitForCondition(new TestCondition() {\n                 @Override\n                 public boolean conditionMet() {\n-                    final StreamsMetadata metadata = streams.metadataForKey(storeName, key, new StringSerializer());\n-                    if (metadata == null) {\n-                        return false;\n-                    }\n-                    final int index = metadata.hostInfo().port();\n-                    final KafkaStreams streamsWithKey = streamRunnables[index].getStream();\n-                    final ReadOnlyKeyValueStore<String, Long> store;\n                     try {\n-                        store = streamsWithKey.store(storeName, QueryableStoreTypes.<String, Long>keyValueStore());\n-                    } catch (final InvalidStateStoreException e) {\n-                        // rebalance\n-                        return false;\n+                        final StreamsMetadata metadata = streams.metadataForKey(storeName, key, new StringSerializer());\n+                        if (metadata == null) {\n+                            return false;\n+                        }\n+                        final int index = metadata.hostInfo().port();\n+                        final KafkaStreams streamsWithKey = streamRunnables[index].getStream();\n+                        final ReadOnlyKeyValueStore<String, Long> store = streamsWithKey.store(storeName, QueryableStoreTypes.<String, Long>keyValueStore());\n+                        return store != null && store.get(key) != null;\n                     } catch (final IllegalStateException e) {\n                         // Kafka Streams instance may have closed but rebalance hasn't happened\n                         return false;\n-                    } \n+                    } catch (final InvalidStateStoreException e) {\n+                        // rebalance\n+                        return false;\n+                    }\n \n-                    return store != null && store.get(key) != null;\n                 }\n             }, 30000, \"waiting for metadata, store and value to be non null\");\n         }\n@@ -296,23 +293,23 @@ private void verifyAllWindowedKeys(final StreamRunnable[] streamRunnables, final\n             TestUtils.waitForCondition(new TestCondition() {\n                 @Override\n                 public boolean conditionMet() {\n-                    final StreamsMetadata metadata = streams.metadataForKey(storeName, key, new StringSerializer());\n-                    if (metadata == null) {\n-                        return false;\n-                    }\n-                    final int index = metadata.hostInfo().port();\n-                    final KafkaStreams streamsWithKey = streamRunnables[index].getStream();\n-                    final ReadOnlyWindowStore<String, Long> store;\n                     try {\n-                        store = streamsWithKey.store(storeName, QueryableStoreTypes.<String, Long>windowStore());\n+                        final StreamsMetadata metadata = streams.metadataForKey(storeName, key, new StringSerializer());\n+                        if (metadata == null) {\n+                            return false;\n+                        }\n+                        final int index = metadata.hostInfo().port();\n+                        final KafkaStreams streamsWithKey = streamRunnables[index].getStream();\n+                        final ReadOnlyWindowStore<String, Long> store = streamsWithKey.store(storeName, QueryableStoreTypes.<String, Long>windowStore());\n+                        return store != null && store.fetch(key, from, to) != null;\n                     } catch (final IllegalStateException e) {\n                         // Kafka Streams instance may have closed but rebalance hasn't happened\n                         return false;\n                     } catch (InvalidStateStoreException e) {\n                         // rebalance\n                         return false;\n                     }\n-                    return store != null && store.fetch(key, from, to) != null;\n+\n                 }\n             }, 30000, \"waiting for metadata, store and value to be non null\");\n         }", "filename": "streams/src/test/java/org/apache/kafka/streams/integration/QueryableStateIntegrationTest.java"}, {"additions": 7, "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsMetadataStateTest.java", "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsMetadataStateTest.java", "sha": "03280a831a58ea98eacceb8c70ff1672b4338005", "changes": 7, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsMetadataStateTest.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a", "patch": "@@ -209,6 +209,13 @@ public Integer partition(final String key, final Object value, final int numPart\n         assertEquals(expected, actual);\n     }\n \n+    @Test\n+    public void shouldReturnNotAvailableWhenClusterIsEmpty() throws Exception {\n+        discovery.onChange(Collections.<HostInfo, Set<TopicPartition>>emptyMap(), Cluster.empty());\n+        final StreamsMetadata result = discovery.getMetadataWithKey(\"table-one\", \"a\", Serdes.String().serializer());\n+        assertEquals(StreamsMetadata.NOT_AVAILABLE, result);\n+    }\n+\n     @Test\n     public void shouldGetInstanceWithKeyWithMergedStreams() throws Exception {\n         final TopicPartition topic2P2 = new TopicPartition(\"topic-two\", 2);", "filename": "streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamsMetadataStateTest.java"}, {"additions": 11, "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStoreTest.java", "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStoreTest.java", "sha": "05c32f02d7d8c840567f8f3eb8a5cf515e076365", "changes": 21, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStoreTest.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a", "patch": "@@ -18,6 +18,7 @@\n import org.apache.kafka.streams.errors.InvalidStateStoreException;\n import org.apache.kafka.streams.state.KeyValueStore;\n import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.test.StateStoreProviderStub;\n import org.junit.Before;\n import org.junit.Test;\n \n@@ -42,8 +43,8 @@\n     @SuppressWarnings(\"unchecked\")\n     @Before\n     public void before() {\n-        final StateStoreProviderStub stubProviderOne = new StateStoreProviderStub();\n-        stubProviderTwo = new StateStoreProviderStub();\n+        final StateStoreProviderStub stubProviderOne = new StateStoreProviderStub(false);\n+        stubProviderTwo = new StateStoreProviderStub(false);\n \n         stubOneUnderlying = newStoreInstance();\n         stubProviderOne.addStore(storeName, stubOneUnderlying);\n@@ -148,19 +149,19 @@ public void shouldSupportAllAcrossMultipleStores() throws Exception {\n     }\n \n     @Test(expected = InvalidStateStoreException.class)\n-    public void shouldThrowInvalidStoreExceptionIfNoStoresExistOnGet() throws Exception {\n-        noStores().get(\"anything\");\n+    public void shouldThrowInvalidStoreExceptionDuringRebalance() throws Exception {\n+        rebalancing().get(\"anything\");\n     }\n \n \n     @Test(expected = InvalidStateStoreException.class)\n-    public void shouldThrowInvalidStoreExceptionIfNoStoresExistOnRange() throws Exception {\n-        noStores().range(\"anything\", \"something\");\n+    public void shouldThrowInvalidStoreExceptionOnRangeDuringRebalance() throws Exception {\n+        rebalancing().range(\"anything\", \"something\");\n     }\n \n     @Test(expected = InvalidStateStoreException.class)\n-    public void shouldThrowInvalidStoreExceptionIfNoStoresExistOnAll() throws Exception {\n-        noStores().all();\n+    public void shouldThrowInvalidStoreExceptionOnAllDuringRebalance() throws Exception {\n+        rebalancing().all();\n     }\n \n     @Test\n@@ -192,8 +193,8 @@ public long approximateNumEntries() {\n         assertEquals(Long.MAX_VALUE, theStore.approximateNumEntries());\n     }\n \n-    private CompositeReadOnlyKeyValueStore<Object, Object> noStores() {\n-        return new CompositeReadOnlyKeyValueStore<>(new WrappingStoreProvider(Collections.<StateStoreProvider>emptyList()),\n+    private CompositeReadOnlyKeyValueStore<Object, Object> rebalancing() {\n+        return new CompositeReadOnlyKeyValueStore<>(new WrappingStoreProvider(Collections.<StateStoreProvider>singletonList(new StateStoreProviderStub(true))),\n                 QueryableStoreTypes.keyValueStore(), storeName);\n     }\n ", "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyKeyValueStoreTest.java"}, {"additions": 17, "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStoreTest.java", "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStoreTest.java", "sha": "d098429e28a833c1d2a4dbf48b3beb293cde94f1", "changes": 19, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStoreTest.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a", "patch": "@@ -15,8 +15,10 @@\n package org.apache.kafka.streams.state.internals;\n \n import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n import org.apache.kafka.streams.state.QueryableStoreTypes;\n import org.apache.kafka.streams.state.WindowStoreIterator;\n+import org.apache.kafka.test.StateStoreProviderStub;\n import org.junit.Before;\n import org.junit.Test;\n \n@@ -42,8 +44,8 @@\n \n     @Before\n     public void before() {\n-        stubProviderOne = new StateStoreProviderStub();\n-        stubProviderTwo = new StateStoreProviderStub();\n+        stubProviderOne = new StateStoreProviderStub(false);\n+        stubProviderTwo = new StateStoreProviderStub(false);\n         underlyingWindowStore = new ReadOnlyWindowStoreStub<>();\n         stubProviderOne.addStore(storeName, underlyingWindowStore);\n \n@@ -103,6 +105,19 @@ public void shouldNotGetValuesFromOtherStores() throws Exception {\n         assertEquals(Collections.singletonList(new KeyValue<>(1L, \"my-value\")), results);\n     }\n \n+\n+    @Test(expected = InvalidStateStoreException.class)\n+    public void shouldThrowInvalidStateStoreExceptionOnRebalance() throws Exception {\n+        final CompositeReadOnlyWindowStore<Object, Object> store = new CompositeReadOnlyWindowStore<>(new StateStoreProviderStub(true), QueryableStoreTypes.windowStore(), \"foo\");\n+        store.fetch(\"key\", 1, 10);\n+    }\n+\n+    @Test(expected = InvalidStateStoreException.class)\n+    public void shouldThrowInvalidStateStoreExceptionIfFetchThrows() throws Exception {\n+        underlyingWindowStore.setOpen(false);\n+        underlyingWindowStore.fetch(\"key\", 1, 10);\n+    }\n+\n     static <K, V> List<KeyValue<K, V>> toList(final Iterator<KeyValue<K, V>> iterator) {\n         final List<KeyValue<K, V>> results = new ArrayList<>();\n ", "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/CompositeReadOnlyWindowStoreTest.java"}, {"additions": 2, "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/QueryableStoreProviderTest.java", "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/QueryableStoreProviderTest.java", "sha": "3660e8eb2a0b6af8ca107546aac13803313fd979", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/QueryableStoreProviderTest.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a", "patch": "@@ -18,6 +18,7 @@\n import org.apache.kafka.streams.errors.InvalidStateStoreException;\n import org.apache.kafka.streams.state.NoOpWindowStore;\n import org.apache.kafka.streams.state.QueryableStoreTypes;\n+import org.apache.kafka.test.StateStoreProviderStub;\n import org.junit.Before;\n import org.junit.Test;\n \n@@ -33,7 +34,7 @@\n \n     @Before\n     public void before() {\n-        final StateStoreProviderStub theStoreProvider = new StateStoreProviderStub();\n+        final StateStoreProviderStub theStoreProvider = new StateStoreProviderStub(false);\n         theStoreProvider.addStore(keyValueStore, new StateStoreTestUtils.NoOpReadOnlyStore<>());\n         theStoreProvider.addStore(windowStore, new NoOpWindowStore());\n         storeProvider =", "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/QueryableStoreProviderTest.java"}, {"additions": 10, "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/ReadOnlyWindowStoreStub.java", "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/ReadOnlyWindowStoreStub.java", "sha": "2082e00cba0bc9ce63e066562b04cc32a2b5c780", "changes": 11, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/ReadOnlyWindowStoreStub.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a", "patch": "@@ -15,6 +15,7 @@\n package org.apache.kafka.streams.state.internals;\n \n import org.apache.kafka.streams.KeyValue;\n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n import org.apache.kafka.streams.processor.ProcessorContext;\n import org.apache.kafka.streams.processor.StateStore;\n import org.apache.kafka.streams.state.ReadOnlyWindowStore;\n@@ -32,9 +33,13 @@\n public class ReadOnlyWindowStoreStub<K, V> implements ReadOnlyWindowStore<K, V>, StateStore {\n \n     private final Map<Long, Map<K, V>> data = new HashMap<>();\n+    private boolean open  = true;\n \n     @Override\n     public WindowStoreIterator<V> fetch(final K key, final long timeFrom, final long timeTo) {\n+        if (!open) {\n+            throw new InvalidStateStoreException(\"Store is not open\");\n+        }\n         final List<KeyValue<Long, V>> results = new ArrayList<>();\n         for (long now = timeFrom; now <= timeTo; now++) {\n             final Map<K, V> kvMap = data.get(now);\n@@ -79,7 +84,11 @@ public boolean persistent() {\n \n     @Override\n     public boolean isOpen() {\n-        return false;\n+        return open;\n+    }\n+\n+    public void setOpen(final boolean open) {\n+        this.open = open;\n     }\n \n     private class TheWindowStoreIterator<E> implements WindowStoreIterator<E> {", "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/ReadOnlyWindowStoreStub.java"}, {"additions": 3, "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/WrappingStoreProviderTest.java", "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/streams/state/internals/WrappingStoreProviderTest.java", "sha": "708e1534dcc1e22e9ddf5a8f206ac395eafdf05d", "changes": 5, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/WrappingStoreProviderTest.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a", "patch": "@@ -22,6 +22,7 @@\n import org.apache.kafka.streams.state.QueryableStoreTypes;\n import org.apache.kafka.streams.state.ReadOnlyKeyValueStore;\n import org.apache.kafka.streams.state.ReadOnlyWindowStore;\n+import org.apache.kafka.test.StateStoreProviderStub;\n import org.junit.Before;\n import org.junit.Test;\n \n@@ -37,8 +38,8 @@\n \n     @Before\n     public void before() {\n-        final StateStoreProviderStub stubProviderOne = new StateStoreProviderStub();\n-        final StateStoreProviderStub stubProviderTwo = new StateStoreProviderStub();\n+        final StateStoreProviderStub stubProviderOne = new StateStoreProviderStub(false);\n+        final StateStoreProviderStub stubProviderTwo = new StateStoreProviderStub(false);\n \n \n         stubProviderOne.addStore(\"kv\", StateStoreTestUtils.newKeyValueStore(\"kv\", String.class, String.class));", "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/WrappingStoreProviderTest.java"}, {"additions": 12, "raw_url": "https://github.com/apache/kafka/raw/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/test/StateStoreProviderStub.java", "blob_url": "https://github.com/apache/kafka/blob/3d74196f205c53946b6fc3dd0501aa2095f0031a/streams/src/test/java/org/apache/kafka/test/StateStoreProviderStub.java", "previous_filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/StateStoreProviderStub.java", "sha": "f17777fab3e5b359b10d28c6687063dcf86ea49d", "changes": 13, "status": "renamed", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/test/StateStoreProviderStub.java?ref=3d74196f205c53946b6fc3dd0501aa2095f0031a", "patch": "@@ -12,10 +12,12 @@\n  * or implied. See the License for the specific language governing permissions and limitations under\n  * the License.\n  */\n-package org.apache.kafka.streams.state.internals;\n+package org.apache.kafka.test;\n \n+import org.apache.kafka.streams.errors.InvalidStateStoreException;\n import org.apache.kafka.streams.processor.StateStore;\n import org.apache.kafka.streams.state.QueryableStoreType;\n+import org.apache.kafka.streams.state.internals.StateStoreProvider;\n \n import java.util.Collections;\n import java.util.HashMap;\n@@ -25,10 +27,19 @@\n public class StateStoreProviderStub implements StateStoreProvider {\n \n     private final Map<String, StateStore> stores = new HashMap<>();\n+    private final boolean throwException;\n+\n+    public StateStoreProviderStub(final boolean throwException) {\n+\n+        this.throwException = throwException;\n+    }\n \n     @SuppressWarnings(\"unchecked\")\n     @Override\n     public <T> List<T> stores(final String storeName, final QueryableStoreType<T> queryableStoreType) {\n+        if (throwException) {\n+            throw new InvalidStateStoreException(\"store is unavailable\");\n+        }\n         if (stores.containsKey(storeName) && queryableStoreType.accepts(stores.get(storeName))) {\n             return (List<T>) Collections.singletonList(stores.get(storeName));\n         }", "filename": "streams/src/test/java/org/apache/kafka/test/StateStoreProviderStub.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/58e58529b350a3da860b1f51fdfa356dfc42761f", "parent": "https://github.com/apache/kafka/commit/a314461fa04b3249608607d1040c8e0cf8724bac", "message": "KAFKA-1648; Robin consumer balance throws an NPE when there are no topics", "bug_id": "kafka_51", "file": [{"additions": 32, "raw_url": "https://github.com/apache/kafka/raw/58e58529b350a3da860b1f51fdfa356dfc42761f/core/src/main/scala/kafka/consumer/PartitionAssignor.scala", "blob_url": "https://github.com/apache/kafka/blob/58e58529b350a3da860b1f51fdfa356dfc42761f/core/src/main/scala/kafka/consumer/PartitionAssignor.scala", "sha": "e6ff7683a0df4a7d221e949767e57c34703d5aad", "changes": 62, "status": "modified", "deletions": 30, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/consumer/PartitionAssignor.scala?ref=58e58529b350a3da860b1f51fdfa356dfc42761f", "patch": "@@ -71,39 +71,41 @@ class RoundRobinAssignor() extends PartitionAssignor with Logging {\n   def assign(ctx: AssignmentContext) = {\n     val partitionOwnershipDecision = collection.mutable.Map[TopicAndPartition, ConsumerThreadId]()\n \n-    // check conditions (a) and (b)\n-    val (headTopic, headThreadIdSet) = (ctx.consumersForTopic.head._1, ctx.consumersForTopic.head._2.toSet)\n-    ctx.consumersForTopic.foreach { case (topic, threadIds) =>\n-      val threadIdSet = threadIds.toSet\n-      require(threadIdSet == headThreadIdSet,\n-              \"Round-robin assignment is allowed only if all consumers in the group subscribe to the same topics, \" +\n-              \"AND if the stream counts across topics are identical for a given consumer instance.\\n\" +\n-              \"Topic %s has the following available consumer streams: %s\\n\".format(topic, threadIdSet) +\n-              \"Topic %s has the following available consumer streams: %s\\n\".format(headTopic, headThreadIdSet))\n-    }\n+    if (ctx.consumersForTopic.size > 0) {\n+      // check conditions (a) and (b)\n+      val (headTopic, headThreadIdSet) = (ctx.consumersForTopic.head._1, ctx.consumersForTopic.head._2.toSet)\n+      ctx.consumersForTopic.foreach { case (topic, threadIds) =>\n+        val threadIdSet = threadIds.toSet\n+        require(threadIdSet == headThreadIdSet,\n+          \"Round-robin assignment is allowed only if all consumers in the group subscribe to the same topics, \" +\n+            \"AND if the stream counts across topics are identical for a given consumer instance.\\n\" +\n+            \"Topic %s has the following available consumer streams: %s\\n\".format(topic, threadIdSet) +\n+            \"Topic %s has the following available consumer streams: %s\\n\".format(headTopic, headThreadIdSet))\n+      }\n \n-    val threadAssignor = Utils.circularIterator(headThreadIdSet.toSeq.sorted)\n+      val threadAssignor = Utils.circularIterator(headThreadIdSet.toSeq.sorted)\n+\n+      info(\"Starting round-robin assignment with consumers \" + ctx.consumers)\n+      val allTopicPartitions = ctx.partitionsForTopic.flatMap { case (topic, partitions) =>\n+        info(\"Consumer %s rebalancing the following partitions for topic %s: %s\"\n+          .format(ctx.consumerId, topic, partitions))\n+        partitions.map(partition => {\n+          TopicAndPartition(topic, partition)\n+        })\n+      }.toSeq.sortWith((topicPartition1, topicPartition2) => {\n+        /*\n+         * Randomize the order by taking the hashcode to reduce the likelihood of all partitions of a given topic ending\n+         * up on one consumer (if it has a high enough stream count).\n+         */\n+        topicPartition1.toString.hashCode < topicPartition2.toString.hashCode\n+      })\n \n-    info(\"Starting round-robin assignment with consumers \" + ctx.consumers)\n-    val allTopicPartitions = ctx.partitionsForTopic.flatMap { case(topic, partitions) =>\n-      info(\"Consumer %s rebalancing the following partitions for topic %s: %s\"\n-           .format(ctx.consumerId, topic, partitions))\n-      partitions.map(partition => {\n-        TopicAndPartition(topic, partition)\n+      allTopicPartitions.foreach(topicPartition => {\n+        val threadId = threadAssignor.next()\n+        if (threadId.consumer == ctx.consumerId)\n+          partitionOwnershipDecision += (topicPartition -> threadId)\n       })\n-    }.toSeq.sortWith((topicPartition1, topicPartition2) => {\n-      /*\n-       * Randomize the order by taking the hashcode to reduce the likelihood of all partitions of a given topic ending\n-       * up on one consumer (if it has a high enough stream count).\n-       */\n-      topicPartition1.toString.hashCode < topicPartition2.toString.hashCode\n-    })\n-\n-    allTopicPartitions.foreach(topicPartition => {\n-      val threadId = threadAssignor.next()\n-      if (threadId.consumer == ctx.consumerId)\n-        partitionOwnershipDecision += (topicPartition -> threadId)\n-    })\n+    }\n \n     partitionOwnershipDecision\n   }", "filename": "core/src/main/scala/kafka/consumer/PartitionAssignor.scala"}, {"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/58e58529b350a3da860b1f51fdfa356dfc42761f/core/src/test/scala/unit/kafka/consumer/PartitionAssignorTest.scala", "blob_url": "https://github.com/apache/kafka/blob/58e58529b350a3da860b1f51fdfa356dfc42761f/core/src/test/scala/unit/kafka/consumer/PartitionAssignorTest.scala", "sha": "24954de66ccc5158696166b7e2aabad0f1b1f287", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/consumer/PartitionAssignorTest.scala?ref=58e58529b350a3da860b1f51fdfa356dfc42761f", "patch": "@@ -87,7 +87,7 @@ private object PartitionAssignorTest extends Logging {\n   private val MaxConsumerCount = 10\n   private val MaxStreamCount = 8\n   private val MaxTopicCount = 100\n-  private val MinTopicCount = 20\n+  private val MinTopicCount = 0\n   private val MaxPartitionCount = 120\n   private val MinPartitionCount = 8\n ", "filename": "core/src/test/scala/unit/kafka/consumer/PartitionAssignorTest.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/1d19ac9fea5b3344a0cdd00ee8c58ab8a22a5579", "parent": "https://github.com/apache/kafka/commit/079c88178dff4b3a4c9de55629e7d15b60e5f562", "message": "HOTFIX: Avoid NPE in StreamsPartitionAssignor\n\nAuthor: Guozhang Wang <wangguoz@gmail.com>\n\nReviewers: Michael G. Noll <michael@confluent.io>\n\nCloses #1004 from guozhangwang/KStreamPANPE", "bug_id": "kafka_52", "file": [{"additions": 17, "raw_url": "https://github.com/apache/kafka/raw/1d19ac9fea5b3344a0cdd00ee8c58ab8a22a5579/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java", "blob_url": "https://github.com/apache/kafka/blob/1d19ac9fea5b3344a0cdd00ee8c58ab8a22a5579/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java", "sha": "1b3bf101b65404a996ed8c0d60ec7a19b7524a46", "changes": 35, "status": "modified", "deletions": 18, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java?ref=1d19ac9fea5b3344a0cdd00ee8c58ab8a22a5579", "patch": "@@ -188,6 +188,7 @@ public Subscription subscription(Set<String> topics) {\n \n         // ensure the co-partitioning topics within the group have the same number of partitions,\n         // and enforce the number of partitions for those internal topics.\n+        internalSourceTopicToTaskIds = new HashMap<>();\n         Map<Integer, Set<String>> sourceTopicGroups = new HashMap<>();\n         Map<Integer, Set<String>> internalSourceTopicGroups = new HashMap<>();\n         for (Map.Entry<Integer, TopologyBuilder.TopicsInfo> entry : topicGroups.entrySet()) {\n@@ -229,37 +230,35 @@ public Subscription subscription(Set<String> topics) {\n         if (internalTopicManager != null) {\n             log.debug(\"Starting to validate internal source topics in partition assignor.\");\n \n-            if (internalSourceTopicToTaskIds != null) {\n-                for (Map.Entry<String, Set<TaskId>> entry : internalSourceTopicToTaskIds.entrySet()) {\n-                    String topic = streamThread.jobId + \"-\" + entry.getKey();\n+            for (Map.Entry<String, Set<TaskId>> entry : internalSourceTopicToTaskIds.entrySet()) {\n+                String topic = streamThread.jobId + \"-\" + entry.getKey();\n \n-                    // should have size 1 only\n-                    int numPartitions = -1;\n-                    for (TaskId task : entry.getValue()) {\n-                        numPartitions = task.partition;\n-                    }\n+                // should have size 1 only\n+                int numPartitions = -1;\n+                for (TaskId task : entry.getValue()) {\n+                    numPartitions = task.partition;\n+                }\n \n-                    internalTopicManager.makeReady(topic, numPartitions);\n+                internalTopicManager.makeReady(topic, numPartitions);\n \n-                    // wait until the topic metadata has been propagated to all brokers\n-                    List<PartitionInfo> partitions;\n-                    do {\n-                        partitions = streamThread.restoreConsumer.partitionsFor(topic);\n-                    } while (partitions == null || partitions.size() != numPartitions);\n+                // wait until the topic metadata has been propagated to all brokers\n+                List<PartitionInfo> partitions;\n+                do {\n+                    partitions = streamThread.restoreConsumer.partitionsFor(topic);\n+                } while (partitions == null || partitions.size() != numPartitions);\n \n-                    metadata.update(topic, partitions);\n-                }\n+                metadata.update(topic, partitions);\n             }\n \n             log.info(\"Completed validating internal source topics in partition assignor.\");\n         }\n+        internalSourceTopicToTaskIds.clear();\n \n         // get the tasks as partition groups from the partition grouper\n         Map<TaskId, Set<TopicPartition>> partitionsForTask = streamThread.partitionGrouper.partitionGroups(sourceTopicGroups, metadata);\n \n-        // add tasks to state topic subscribers\n+        // add tasks to state change log topic subscribers\n         stateChangelogTopicToTaskIds = new HashMap<>();\n-        internalSourceTopicToTaskIds = new HashMap<>();\n         for (TaskId task : partitionsForTask.keySet()) {\n             for (String topicName : topicGroups.get(task.topicGroupId).stateChangelogTopics) {\n                 Set<TaskId> tasks = stateChangelogTopicToTaskIds.get(topicName);", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/d4366610241321a58d62c0b919d60ccfe3c3d698", "parent": "https://github.com/apache/kafka/commit/f3bb10413d11525a396851eb7e6b9d75b8c6fe23", "message": "KAFKA-4131; Multiple Regex KStream-Consumers cause Null pointer exception\n\nFix for bug outlined in KAFKA-4131\n\nAuthor: bbejeck <bbejeck@gmail.com>\n\nReviewers: Damian Guy, Guozhang Wang\n\nCloses #1843 from bbejeck/KAFKA-4131_mulitple_regex_consumers_cause_npe", "bug_id": "kafka_53", "file": [{"additions": 9, "raw_url": "https://github.com/apache/kafka/raw/d4366610241321a58d62c0b919d60ccfe3c3d698/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java", "blob_url": "https://github.com/apache/kafka/blob/d4366610241321a58d62c0b919d60ccfe3c3d698/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java", "sha": "b6cebf4f66f92dc4af4bffad7331a70d08486174", "changes": 15, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java?ref=d4366610241321a58d62c0b919d60ccfe3c3d698", "patch": "@@ -176,6 +176,14 @@ public Subscription subscription(Set<String> topics) {\n         standbyTasks.removeAll(prevTasks);\n         SubscriptionInfo data = new SubscriptionInfo(streamThread.processId, prevTasks, standbyTasks, this.userEndPointConfig);\n \n+        if (streamThread.builder.sourceTopicPattern() != null) {\n+            SubscriptionUpdates subscriptionUpdates = new SubscriptionUpdates();\n+            log.debug(\"have {} topics matching regex\", topics);\n+            // update the topic groups with the returned subscription set for regex pattern subscriptions\n+            subscriptionUpdates.updateTopics(topics);\n+            streamThread.builder.updateSubscriptions(subscriptionUpdates);\n+        }\n+\n         return new Subscription(new ArrayList<>(topics), data.encode());\n     }\n \n@@ -255,17 +263,12 @@ public Subscription subscription(Set<String> topics) {\n         // 2. within each client, tasks are assigned to consumer clients in round-robin manner.\n         Map<UUID, Set<String>> consumersByClient = new HashMap<>();\n         Map<UUID, ClientState<TaskId>> states = new HashMap<>();\n-        SubscriptionUpdates subscriptionUpdates = new SubscriptionUpdates();\n         Map<UUID, HostInfo> consumerEndPointMap = new HashMap<>();\n         // decode subscription info\n         for (Map.Entry<String, Subscription> entry : subscriptions.entrySet()) {\n             String consumerId = entry.getKey();\n             Subscription subscription = entry.getValue();\n \n-            if (streamThread.builder.sourceTopicPattern() != null) {\n-               // update the topic groups with the returned subscription list for regex pattern subscriptions\n-                subscriptionUpdates.updateTopics(subscription.topics());\n-            }\n \n             SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());\n             if (info.userEndPoint != null) {\n@@ -291,7 +294,7 @@ public Subscription subscription(Set<String> topics) {\n             state.capacity = state.capacity + 1d;\n         }\n \n-        streamThread.builder.updateSubscriptions(subscriptionUpdates);\n+\n         this.topicGroups = streamThread.builder.topicGroups();\n \n         // ensure the co-partitioning topics within the group have the same number of partitions,", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java"}, {"additions": 76, "raw_url": "https://github.com/apache/kafka/raw/d4366610241321a58d62c0b919d60ccfe3c3d698/streams/src/test/java/org/apache/kafka/streams/integration/RegexSourceIntegrationTest.java", "blob_url": "https://github.com/apache/kafka/blob/d4366610241321a58d62c0b919d60ccfe3c3d698/streams/src/test/java/org/apache/kafka/streams/integration/RegexSourceIntegrationTest.java", "sha": "8487674ce5e1965bc0680276d329cceec2560900", "changes": 76, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/integration/RegexSourceIntegrationTest.java?ref=d4366610241321a58d62c0b919d60ccfe3c3d698", "patch": "@@ -80,6 +80,8 @@\n     private static final String TOPIC_Z = \"topic-Z\";\n     private static final String FA_TOPIC = \"fa\";\n     private static final String FOO_TOPIC = \"foo\";\n+    private static final String PARTITIONED_TOPIC_1 = \"partitioned-1\";\n+    private static final String PARTITIONED_TOPIC_2 = \"partitioned-2\";\n \n     private static final String DEFAULT_OUTPUT_TOPIC = \"outputTopic\";\n     private static final String STRING_SERDE_CLASSNAME = Serdes.String().getClass().getName();\n@@ -97,6 +99,8 @@ public static void startKafkaCluster() throws Exception {\n         CLUSTER.createTopic(TOPIC_Z);\n         CLUSTER.createTopic(FA_TOPIC);\n         CLUSTER.createTopic(FOO_TOPIC);\n+        CLUSTER.createTopic(PARTITIONED_TOPIC_1, 2, 1);\n+        CLUSTER.createTopic(PARTITIONED_TOPIC_2, 2, 1);\n \n     }\n \n@@ -275,6 +279,78 @@ public void testShouldReadFromRegexAndNamedTopics() throws Exception {\n         assertThat(actualValues, equalTo(expectedReceivedValues));\n     }\n \n+    @Test\n+    public void testMultipleConsumersCanReadFromPartitionedTopic() throws Exception {\n+\n+        final Serde<String> stringSerde = Serdes.String();\n+        final KStreamBuilder builderLeader = new KStreamBuilder();\n+        final KStreamBuilder builderFollower = new KStreamBuilder();\n+        final List<String> expectedAssignment = Arrays.asList(PARTITIONED_TOPIC_1,  PARTITIONED_TOPIC_2);\n+\n+        final KStream<String, String> partitionedStreamLeader = builderLeader.stream(Pattern.compile(\"partitioned-\\\\d\"));\n+        final KStream<String, String> partitionedStreamFollower = builderFollower.stream(Pattern.compile(\"partitioned-\\\\d\"));\n+\n+\n+        partitionedStreamLeader.to(stringSerde, stringSerde, DEFAULT_OUTPUT_TOPIC);\n+        partitionedStreamFollower.to(stringSerde, stringSerde, DEFAULT_OUTPUT_TOPIC);\n+\n+        final KafkaStreams partitionedStreamsLeader  = new KafkaStreams(builderLeader, streamsConfiguration);\n+        final KafkaStreams partitionedStreamsFollower  = new KafkaStreams(builderFollower, streamsConfiguration);\n+\n+        final StreamsConfig streamsConfig = new StreamsConfig(streamsConfiguration);\n+\n+\n+        final Field leaderStreamThreadsField = partitionedStreamsLeader.getClass().getDeclaredField(\"threads\");\n+        leaderStreamThreadsField.setAccessible(true);\n+        final StreamThread[] leaderStreamThreads = (StreamThread[]) leaderStreamThreadsField.get(partitionedStreamsLeader);\n+        final StreamThread originalLeaderThread = leaderStreamThreads[0];\n+\n+        final TestStreamThread leaderTestStreamThread = new TestStreamThread(builderLeader, streamsConfig,\n+                new DefaultKafkaClientSupplier(),\n+                originalLeaderThread.applicationId, originalLeaderThread.clientId, originalLeaderThread.processId, new Metrics(), new SystemTime());\n+\n+        leaderStreamThreads[0] = leaderTestStreamThread;\n+\n+        final TestCondition bothTopicsAddedToLeader = new TestCondition() {\n+            @Override\n+            public boolean conditionMet() {\n+                return leaderTestStreamThread.assignedTopicPartitions.equals(expectedAssignment);\n+            }\n+        };\n+\n+\n+\n+        final Field followerStreamThreadsField = partitionedStreamsFollower.getClass().getDeclaredField(\"threads\");\n+        followerStreamThreadsField.setAccessible(true);\n+        final StreamThread[] followerStreamThreads = (StreamThread[]) followerStreamThreadsField.get(partitionedStreamsFollower);\n+        final StreamThread originalFollowerThread = followerStreamThreads[0];\n+\n+        final TestStreamThread followerTestStreamThread = new TestStreamThread(builderFollower, streamsConfig,\n+                new DefaultKafkaClientSupplier(),\n+                originalFollowerThread.applicationId, originalFollowerThread.clientId, originalFollowerThread.processId, new Metrics(), new SystemTime());\n+\n+        followerStreamThreads[0] = followerTestStreamThread;\n+\n+\n+        final TestCondition bothTopicsAddedToFollower = new TestCondition() {\n+            @Override\n+            public boolean conditionMet() {\n+                return followerTestStreamThread.assignedTopicPartitions.equals(expectedAssignment);\n+            }\n+        };\n+\n+        partitionedStreamsLeader.start();\n+        TestUtils.waitForCondition(bothTopicsAddedToLeader, \"Topics never assigned to leader stream\");\n+\n+\n+        partitionedStreamsFollower.start();\n+        TestUtils.waitForCondition(bothTopicsAddedToFollower, \"Topics never assigned to follower stream\");\n+\n+        partitionedStreamsLeader.close();\n+        partitionedStreamsFollower.close();\n+\n+    }\n+\n     // TODO should be updated to expected = TopologyBuilderException after KAFKA-3708\n     @Test(expected = AssertionError.class)\n     public void testNoMessagesSentExceptionFromOverlappingPatterns() throws Exception {", "filename": "streams/src/test/java/org/apache/kafka/streams/integration/RegexSourceIntegrationTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/c455e608c1f2c7be6ff0a721f49c1fe3ede0165f", "parent": "https://github.com/apache/kafka/commit/1d884d1f60aec9ec7ea334761bead4c60b13c7a9", "message": "KAFKA-2795: fix potential NPE in GroupMetadataManager.addGroup\n\nAuthor: Jason Gustafson <jason@confluent.io>\n\nReviewers: Onur Karaman, Guozhang Wang\n\nCloses #488 from hachikuji/KAFKA-2795", "bug_id": "kafka_54", "file": [{"additions": 6, "raw_url": "https://github.com/apache/kafka/raw/c455e608c1f2c7be6ff0a721f49c1fe3ede0165f/core/src/main/scala/kafka/coordinator/GroupMetadataManager.scala", "blob_url": "https://github.com/apache/kafka/blob/c455e608c1f2c7be6ff0a721f49c1fe3ede0165f/core/src/main/scala/kafka/coordinator/GroupMetadataManager.scala", "sha": "047970e3bd6df802ec48acfae2882b33bee5dbfe", "changes": 15, "status": "modified", "deletions": 9, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/coordinator/GroupMetadataManager.scala?ref=c455e608c1f2c7be6ff0a721f49c1fe3ede0165f", "patch": "@@ -67,9 +67,6 @@ class GroupMetadataManager(val brokerId: Int,\n   /* lock for expiring stale offsets, it should be always called BEFORE the group lock if needed */\n   private val offsetExpireLock = new ReentrantReadWriteLock()\n \n-  /* lock for removing offsets of a range partition, it should be always called BEFORE the group lock if needed */\n-  private val offsetRemoveLock = new ReentrantReadWriteLock()\n-\n   /* shutting down flag */\n   private val shuttingDown = new AtomicBoolean(false)\n \n@@ -116,12 +113,12 @@ class GroupMetadataManager(val brokerId: Int,\n    * Add a group or get the group associated with the given groupId if it already exists\n    */\n   def addGroup(groupId: String, protocolType: String): GroupMetadata = {\n-    addGroup(groupId, new GroupMetadata(groupId, protocolType))\n-  }\n-\n-  private def addGroup(groupId: String, group: GroupMetadata): GroupMetadata = {\n-    groupsCache.putIfNotExists(groupId, group)\n-    groupsCache.get(groupId)\n+    val newGroup = new GroupMetadata(groupId, protocolType)\n+    val currentGroup = groupsCache.putIfNotExists(groupId, newGroup)\n+    if (currentGroup != null)\n+      currentGroup\n+    else\n+      newGroup\n   }\n \n   /**", "filename": "core/src/main/scala/kafka/coordinator/GroupMetadataManager.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/490bbc91183d1468a164d80f826317f137026684", "parent": "https://github.com/apache/kafka/commit/7eee11451e1f0d17efa27775becfb370a9894d56", "message": "KAFKA-2755: Suspicious reference forwarding cause NPE on handleDescribeGroup.\n\n\u2026xistent consumer group\n\nAuthor: Ashish Singh <asingh@cloudera.com>\n\nReviewers: Guozhang Wang\n\nCloses #435 from SinghAsDev/KAFKA-2755", "bug_id": "kafka_55", "file": [{"additions": 3, "raw_url": "https://github.com/apache/kafka/raw/490bbc91183d1468a164d80f826317f137026684/core/src/main/scala/kafka/coordinator/GroupCoordinator.scala", "blob_url": "https://github.com/apache/kafka/blob/490bbc91183d1468a164d80f826317f137026684/core/src/main/scala/kafka/coordinator/GroupCoordinator.scala", "sha": "900830fdf702bb2eb3336c55ac1a33a10c2822f1", "changes": 6, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/coordinator/GroupCoordinator.scala?ref=490bbc91183d1468a164d80f826317f137026684", "patch": "@@ -662,13 +662,13 @@ class GroupCoordinator(val brokerId: Int,\n \n object GroupCoordinator {\n \n-  val EmptyGroup = GroupSummary(NoState, NoProtocolType, NoProtocol, NoMembers)\n-  val DeadGroup = GroupSummary(Dead.toString, NoProtocolType, NoProtocol, NoMembers)\n-  val NoMembers = List[MemberSummary]()\n   val NoState = \"\"\n   val NoProtocolType = \"\"\n   val NoProtocol = \"\"\n   val NoLeader = \"\"\n+  val NoMembers = List[MemberSummary]()\n+  val EmptyGroup = GroupSummary(NoState, NoProtocolType, NoProtocol, NoMembers)\n+  val DeadGroup = GroupSummary(Dead.toString, NoProtocolType, NoProtocol, NoMembers)\n \n   // TODO: we store both group metadata and offset data here despite the topic name being offsets only\n   val GroupMetadataTopicName = \"__consumer_offsets\"", "filename": "core/src/main/scala/kafka/coordinator/GroupCoordinator.scala"}, {"additions": 11, "raw_url": "https://github.com/apache/kafka/raw/490bbc91183d1468a164d80f826317f137026684/core/src/test/scala/integration/kafka/api/AdminClientTest.scala", "blob_url": "https://github.com/apache/kafka/blob/490bbc91183d1468a164d80f826317f137026684/core/src/test/scala/integration/kafka/api/AdminClientTest.scala", "sha": "7d529ec8c4fddc5c76687326833ba12fbf11870f", "changes": 11, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/integration/kafka/api/AdminClientTest.scala?ref=490bbc91183d1468a164d80f826317f137026684", "patch": "@@ -111,4 +111,15 @@ class AdminClientTest extends IntegrationTestHarness with Logging {\n       assertEquals(Set(tp, tp2), partitions.toSet)\n   }\n \n+  @Test\n+  def testDescribeConsumerGroupForNonExistentGroup() {\n+    val nonExistentGroup = \"non\" + groupId\n+    try {\n+      client.describeConsumerGroup(nonExistentGroup)\n+      fail(\"Should have failed for non existent group.\")\n+    } catch {\n+      case ex: IllegalArgumentException => // Pass\n+      case _: Throwable => fail(\"Should have failed for non existent group with IllegalArgumentException.\")\n+    }\n+  }\n }", "filename": "core/src/test/scala/integration/kafka/api/AdminClientTest.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/b46cb3b2975afd8a3e82a0265c57760d8b9910da", "parent": "https://github.com/apache/kafka/commit/726e23ef491637e03b0ba90c21992a639dcd158d", "message": "KAFKA-2599: Fix Metadata.getClusterForCurrentTopics throws NPE\n\n\u2026h null checking\n\nAuthor: Edward Ribeiro <edward.ribeiro@gmail.com>\n\nReviewers: Ismael Juma, Guozhang Wang\n\nCloses #262 from eribeiro/KAFKA-2599", "bug_id": "kafka_56", "file": [{"additions": 5, "raw_url": "https://github.com/apache/kafka/raw/b46cb3b2975afd8a3e82a0265c57760d8b9910da/clients/src/main/java/org/apache/kafka/clients/Metadata.java", "blob_url": "https://github.com/apache/kafka/blob/b46cb3b2975afd8a3e82a0265c57760d8b9910da/clients/src/main/java/org/apache/kafka/clients/Metadata.java", "sha": "f2fca12e09d527fd63aa4700e60a6e1aa56dfa03", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/Metadata.java?ref=b46cb3b2975afd8a3e82a0265c57760d8b9910da", "patch": "@@ -14,10 +14,12 @@\n \n import java.util.ArrayList;\n import java.util.Collection;\n+import java.util.Collections;\n import java.util.HashSet;\n import java.util.List;\n import java.util.Set;\n import org.apache.kafka.common.Cluster;\n+import org.apache.kafka.common.Node;\n import org.apache.kafka.common.PartitionInfo;\n import org.apache.kafka.common.errors.TimeoutException;\n import org.slf4j.Logger;\n@@ -237,11 +239,13 @@ public void removeListener(Listener listener) {\n \n     private Cluster getClusterForCurrentTopics(Cluster cluster) {\n         Collection<PartitionInfo> partitionInfos = new ArrayList<>();\n+        List<Node> nodes = Collections.emptyList();\n         if (cluster != null) {\n             for (String topic : this.topics) {\n                 partitionInfos.addAll(cluster.partitionsForTopic(topic));\n             }\n+            nodes = cluster.nodes();\n         }\n-        return new Cluster(cluster.nodes(), partitionInfos);\n+        return new Cluster(nodes, partitionInfos);\n     }\n }", "filename": "clients/src/main/java/org/apache/kafka/clients/Metadata.java"}, {"additions": 3, "raw_url": "https://github.com/apache/kafka/raw/b46cb3b2975afd8a3e82a0265c57760d8b9910da/clients/src/test/java/org/apache/kafka/clients/MetadataTest.java", "blob_url": "https://github.com/apache/kafka/blob/b46cb3b2975afd8a3e82a0265c57760d8b9910da/clients/src/test/java/org/apache/kafka/clients/MetadataTest.java", "sha": "b7160a1996e8e7ed37dce0b6028f089751d7c234", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/MetadataTest.java?ref=b46cb3b2975afd8a3e82a0265c57760d8b9910da", "patch": "@@ -111,6 +111,9 @@ public void testFailedUpdate() {\n         assertEquals(100, metadata.timeToNextUpdate(1100));\n         assertEquals(100, metadata.lastSuccessfulUpdate());\n \n+        metadata.needMetadataForAllTopics(true);\n+        metadata.update(null, time);\n+        assertEquals(100, metadata.timeToNextUpdate(1000));\n     }\n \n     @Test", "filename": "clients/src/test/java/org/apache/kafka/clients/MetadataTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/b982eefd37595b30600b7a979915db5da22271fe", "parent": "https://github.com/apache/kafka/commit/8ace736f7121161518953b4ce2a03f36a8306e18", "message": "KAFKA-4927: Fix KStreamsTestDriver to not throw NPE when KStream.to() sinks are used\n\na KStream.to() sink is also a topic\n... so the KStreamTestDriver to fetch it when required\n\nAuthor: Wim Van Leuven <wim.vanleuven@bigboards.io>\nAuthor: Wim Van Leuven <wim.vanleuven@highestpoint.biz>\n\nReviewers: Eno Thereska, Matthias J. Sax, Guozhang Wang\n\nCloses #2716 from wimvanleuven/KAFKA-4927", "bug_id": "kafka_57", "file": [{"additions": 46, "raw_url": "https://github.com/apache/kafka/raw/b982eefd37595b30600b7a979915db5da22271fe/streams/src/test/java/org/apache/kafka/streams/kstream/KStreamBuilderTest.java", "blob_url": "https://github.com/apache/kafka/blob/b982eefd37595b30600b7a979915db5da22271fe/streams/src/test/java/org/apache/kafka/streams/kstream/KStreamBuilderTest.java", "sha": "e7cb6691f8733e6d50c8fee4434dba13fb0b434a", "changes": 53, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/kstream/KStreamBuilderTest.java?ref=b982eefd37595b30600b7a979915db5da22271fe", "patch": "@@ -79,13 +79,52 @@ public void testNewName() {\n         assertEquals(\"Y-0000000001\", builder.newName(\"Y-\"));\n         assertEquals(\"Z-0000000002\", builder.newName(\"Z-\"));\n \n-        KStreamBuilder newBuilder = new KStreamBuilder();\n+        final KStreamBuilder newBuilder = new KStreamBuilder();\n \n         assertEquals(\"X-0000000000\", newBuilder.newName(\"X-\"));\n         assertEquals(\"Y-0000000001\", newBuilder.newName(\"Y-\"));\n         assertEquals(\"Z-0000000002\", newBuilder.newName(\"Z-\"));\n     }\n \n+\n+    @Test\n+    public void shouldNotTryProcessingFromSinkTopic() {\n+        final KStream<String, String> source = builder.stream(\"topic-source\");\n+        source.to(\"topic-sink\");\n+\n+        final MockProcessorSupplier<String, String> processorSupplier = new MockProcessorSupplier<>();\n+\n+        source.process(processorSupplier);\n+\n+        driver = new KStreamTestDriver(builder);\n+        driver.setTime(0L);\n+\n+        driver.process(\"topic-source\", \"A\", \"aa\");\n+\n+        // no exception was thrown\n+        assertEquals(Utils.mkList(\"A:aa\"), processorSupplier.processed);\n+    }\n+\n+    @Test\n+    public void shouldTryProcessingFromThoughTopic() {\n+        final KStream<String, String> source = builder.stream(\"topic-source\");\n+        final KStream<String, String> through = source.through(\"topic-sink\");\n+\n+        final MockProcessorSupplier<String, String> sourceProcessorSupplier = new MockProcessorSupplier<>();\n+        final MockProcessorSupplier<String, String> throughProcessorSupplier = new MockProcessorSupplier<>();\n+\n+        source.process(sourceProcessorSupplier);\n+        through.process(throughProcessorSupplier);\n+\n+        driver = new KStreamTestDriver(builder);\n+        driver.setTime(0L);\n+\n+        driver.process(\"topic-source\", \"A\", \"aa\");\n+\n+        assertEquals(Utils.mkList(\"A:aa\"), sourceProcessorSupplier.processed);\n+        assertEquals(Utils.mkList(\"A:aa\"), throughProcessorSupplier.processed);\n+    }\n+\n     @Test\n     public void testNewStoreName() {\n         assertEquals(\"X-STATE-STORE-0000000000\", builder.newStoreName(\"X-\"));\n@@ -101,14 +140,14 @@ public void testNewStoreName() {\n \n     @Test\n     public void testMerge() {\n-        String topic1 = \"topic-1\";\n-        String topic2 = \"topic-2\";\n+        final String topic1 = \"topic-1\";\n+        final String topic2 = \"topic-2\";\n \n-        KStream<String, String> source1 = builder.stream(topic1);\n-        KStream<String, String> source2 = builder.stream(topic2);\n-        KStream<String, String> merged = builder.merge(source1, source2);\n+        final KStream<String, String> source1 = builder.stream(topic1);\n+        final KStream<String, String> source2 = builder.stream(topic2);\n+        final KStream<String, String> merged = builder.merge(source1, source2);\n \n-        MockProcessorSupplier<String, String> processorSupplier = new MockProcessorSupplier<>();\n+        final MockProcessorSupplier<String, String> processorSupplier = new MockProcessorSupplier<>();\n         merged.process(processorSupplier);\n \n         driver = new KStreamTestDriver(builder);", "filename": "streams/src/test/java/org/apache/kafka/streams/kstream/KStreamBuilderTest.java"}, {"additions": 26, "raw_url": "https://github.com/apache/kafka/raw/b982eefd37595b30600b7a979915db5da22271fe/streams/src/test/java/org/apache/kafka/test/KStreamTestDriver.java", "blob_url": "https://github.com/apache/kafka/blob/b982eefd37595b30600b7a979915db5da22271fe/streams/src/test/java/org/apache/kafka/test/KStreamTestDriver.java", "sha": "9fb83ba5a49d8de247ee76df8a3b1044d1525c27", "changes": 45, "status": "modified", "deletions": 19, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/test/KStreamTestDriver.java?ref=b982eefd37595b30600b7a979915db5da22271fe", "patch": "@@ -22,12 +22,11 @@\n import org.apache.kafka.common.serialization.Serializer;\n import org.apache.kafka.streams.kstream.KStreamBuilder;\n import org.apache.kafka.streams.processor.ProcessorContext;\n-import org.apache.kafka.streams.processor.internals.MockStreamsMetrics;\n-import org.apache.kafka.streams.processor.internals.ProcessorRecordContext;\n import org.apache.kafka.streams.processor.StateStore;\n import org.apache.kafka.streams.processor.StreamPartitioner;\n+import org.apache.kafka.streams.processor.internals.MockStreamsMetrics;\n import org.apache.kafka.streams.processor.internals.ProcessorNode;\n-import org.apache.kafka.streams.processor.internals.ProcessorStateManager;\n+import org.apache.kafka.streams.processor.internals.ProcessorRecordContext;\n import org.apache.kafka.streams.processor.internals.ProcessorTopology;\n import org.apache.kafka.streams.processor.internals.RecordCollectorImpl;\n import org.apache.kafka.streams.state.internals.ThreadCache;\n@@ -109,23 +108,27 @@ public ProcessorContext context() {\n \n     public void process(final String topicName, final Object key, final Object value) {\n         final ProcessorNode prevNode = context.currentNode();\n-        ProcessorNode currNode = topology.source(topicName);\n-        if (currNode == null && globalTopology != null) {\n-            currNode = globalTopology.source(topicName);\n-        }\n+        final ProcessorNode currNode = sourceNodeByTopicName(topicName);\n \n-        // if currNode is null, check if this topic is a changelog topic;\n-        // if yes, skip\n-        if (topicName.endsWith(ProcessorStateManager.STATE_CHANGELOG_TOPIC_SUFFIX)) {\n-            return;\n+        if (currNode != null) {\n+            context.setRecordContext(createRecordContext(context.timestamp()));\n+            context.setCurrentNode(currNode);\n+            try {\n+                context.forward(key, value);\n+            } finally {\n+                context.setCurrentNode(prevNode);\n+            }\n         }\n-        context.setRecordContext(createRecordContext(context.timestamp()));\n-        context.setCurrentNode(currNode);\n-        try {\n-            context.forward(key, value);\n-        } finally {\n-            context.setCurrentNode(prevNode);\n+    }\n+\n+    private ProcessorNode sourceNodeByTopicName(final String topicName) {\n+        ProcessorNode topicNode = topology.source(topicName);\n+\n+        if (topicNode == null && globalTopology != null) {\n+            topicNode = globalTopology.source(topicName);\n         }\n+\n+        return topicNode;\n     }\n \n     public void punctuate(final long timestamp) {\n@@ -224,7 +227,9 @@ private ProcessorRecordContext createRecordContext(final long timestamp) {\n                                 final Serializer<V> valueSerializer,\n                                 final StreamPartitioner<? super K, ? super V> partitioner) {\n             // The serialization is skipped.\n-            process(topic, key, value);\n+            if (sourceNodeByTopicName(topic) != null) {\n+                process(topic, key, value);\n+            }\n         }\n \n         @Override\n@@ -236,7 +241,9 @@ private ProcessorRecordContext createRecordContext(final long timestamp) {\n                                 final Serializer<K> keySerializer,\n                                 final Serializer<V> valueSerializer) {\n         // The serialization is skipped.\n-            process(topic, key, value);\n+            if (sourceNodeByTopicName(topic) != null) {\n+                process(topic, key, value);\n+            }\n         }\n \n         @Override", "filename": "streams/src/test/java/org/apache/kafka/test/KStreamTestDriver.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/7b16b4731666ff321fbe46828d526872ff5f56d7", "parent": "https://github.com/apache/kafka/commit/6ed3e6b1cb8a73b1f5f78926ccb247a8953a554c", "message": "KAFKA-4066; Fix NPE in consumer due to multi-threaded updates\n\nAuthor: Rajini Sivaram <rajinisivaram@googlemail.com>\n\nReviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>\n\nCloses #1763 from rajinisivaram/KAFKA-4066", "bug_id": "kafka_58", "file": [{"additions": 15, "raw_url": "https://github.com/apache/kafka/raw/7b16b4731666ff321fbe46828d526872ff5f56d7/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java", "blob_url": "https://github.com/apache/kafka/blob/7b16b4731666ff321fbe46828d526872ff5f56d7/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java", "sha": "f2e15ca6f943dfca017b4dce72ee221a85f55ee4", "changes": 32, "status": "modified", "deletions": 17, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java?ref=7b16b4731666ff321fbe46828d526872ff5f56d7", "patch": "@@ -206,24 +206,16 @@ public synchronized void ensureCoordinatorReady() {\n         }\n     }\n \n-    protected RequestFuture<Void> lookupCoordinator() {\n-        if (findCoordinatorFuture == null) {\n+    protected synchronized RequestFuture<Void> lookupCoordinator() {\n+        if (findCoordinatorFuture == null)\n             findCoordinatorFuture = sendGroupCoordinatorRequest();\n-            findCoordinatorFuture.addListener(new RequestFutureListener<Void>() {\n-                @Override\n-                public void onSuccess(Void value) {\n-                    findCoordinatorFuture = null;\n-                }\n-\n-                @Override\n-                public void onFailure(RuntimeException e) {\n-                    findCoordinatorFuture = null;\n-                }\n-            });\n-        }\n         return findCoordinatorFuture;\n     }\n \n+    private synchronized void clearFindCoordinatorFuture() {\n+        findCoordinatorFuture = null;\n+    }\n+\n     /**\n      * Check whether the group should be rejoined (e.g. if metadata changes)\n      * @return true if it should, false otherwise\n@@ -532,6 +524,7 @@ public void onSuccess(ClientResponse resp, RequestFuture<Void> future) {\n             // for the coordinator in the underlying network client layer\n             // TODO: this needs to be better handled in KAFKA-1935\n             Errors error = Errors.forCode(groupCoordinatorResponse.errorCode());\n+            clearFindCoordinatorFuture();\n             if (error == Errors.NONE) {\n                 synchronized (AbstractCoordinator.this) {\n                     AbstractCoordinator.this.coordinator = new Node(\n@@ -550,6 +543,12 @@ public void onSuccess(ClientResponse resp, RequestFuture<Void> future) {\n                 future.raise(error);\n             }\n         }\n+\n+        @Override\n+        public void onFailure(RuntimeException e, RequestFuture<Void> future) {\n+            clearFindCoordinatorFuture();\n+            super.onFailure(e, future);\n+        }\n     }\n \n     /**\n@@ -820,7 +819,6 @@ private RuntimeException failureCause() {\n         @Override\n         public void run() {\n             try {\n-                RequestFuture findCoordinatorFuture = null;\n \n                 while (true) {\n                     synchronized (AbstractCoordinator.this) {\n@@ -843,8 +841,8 @@ public void run() {\n                         long now = time.milliseconds();\n \n                         if (coordinatorUnknown()) {\n-                            if (findCoordinatorFuture == null || findCoordinatorFuture.isDone())\n-                                findCoordinatorFuture = lookupCoordinator();\n+                            if (findCoordinatorFuture == null)\n+                                lookupCoordinator();\n                             else\n                                 AbstractCoordinator.this.wait(retryBackoffMs);\n                         } else if (heartbeat.sessionTimeoutExpired(now)) {", "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/3537063a52b97b6d46f6bd17e3f03e1b03630a3e", "parent": "https://github.com/apache/kafka/commit/98dfc4b307c2e41a7fb0fff330048aa9ff78addd", "message": "MINOR: Check null in SmokeTestDriver to avoid NPE\n\nAuthor: Guozhang Wang <wangguoz@gmail.com>\n\nReviewers: Damian Guy <damian.guy@gmail.com>\n\nCloses #1611 from guozhangwang/Kminor-check-null-smokedriver", "bug_id": "kafka_59", "file": [{"additions": 2, "raw_url": "https://github.com/apache/kafka/raw/3537063a52b97b6d46f6bd17e3f03e1b03630a3e/streams/src/test/java/org/apache/kafka/streams/smoketest/SmokeTestDriver.java", "blob_url": "https://github.com/apache/kafka/blob/3537063a52b97b6d46f6bd17e3f03e1b03630a3e/streams/src/test/java/org/apache/kafka/streams/smoketest/SmokeTestDriver.java", "sha": "b22d8a776701ff612d6b97e8ccf7b857c6ed6793", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/smoketest/SmokeTestDriver.java?ref=3537063a52b97b6d46f6bd17e3f03e1b03630a3e", "patch": "@@ -379,7 +379,7 @@ private static boolean verifyDif(Map<String, Integer> map, Map<String, Set<Integ\n                 int min = getMin(entry.getKey());\n                 int max = getMax(entry.getKey());\n                 int expected = max - min;\n-                if (expected != entry.getValue()) {\n+                if (entry.getValue() == null || expected != entry.getValue()) {\n                     System.out.println(\"fail: key=\" + entry.getKey() + \" dif=\" + entry.getValue() + \" expected=\" + expected);\n                     success = false;\n                 }\n@@ -455,7 +455,7 @@ private static boolean verifyAvg(Map<String, Double> map, Map<String, Set<Intege\n                 int max = getMax(entry.getKey());\n                 double expected = ((long) min + (long) max) / 2.0;\n \n-                if (expected != entry.getValue()) {\n+                if (entry.getValue() == null || expected != entry.getValue()) {\n                     System.out.println(\"fail: key=\" + entry.getKey() + \" avg=\" + entry.getValue() + \" expected=\" + expected);\n                     success = false;\n                 }", "filename": "streams/src/test/java/org/apache/kafka/streams/smoketest/SmokeTestDriver.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/6352a30f46f2da11a8dc3e58912d0a2db8284c35", "parent": "https://github.com/apache/kafka/commit/9f5a1f87667c23db557a712d51c45541372f3c5d", "message": "HOTFIX: Fix NPE after standby task reassignment\n\nBuffered records of change logs must be cleared upon reassignment of standby tasks.\n\nAuthor: Yasuhiro Matsuda <yasuhiro@confluent.io>\n\nReviewers: Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #889 from ymatsuda/hotfix", "bug_id": "kafka_60", "file": [{"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/6352a30f46f2da11a8dc3e58912d0a2db8284c35/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "blob_url": "https://github.com/apache/kafka/blob/6352a30f46f2da11a8dc3e58912d0a2db8284c35/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java", "sha": "d51974a51f15924007620545c5dadf4149ff4b96", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java?ref=6352a30f46f2da11a8dc3e58912d0a2db8284c35", "patch": "@@ -682,6 +682,7 @@ private void removeStandbyTasks() {\n \n         standbyTasks.clear();\n         standbyTasksByPartition.clear();\n+        standbyRecords.clear();\n     }\n \n     private void ensureCopartitioning(Collection<Set<String>> copartitionGroups) {", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/961ebca57fdd4f8f84df8cf26a835f07ea1718c9", "parent": "https://github.com/apache/kafka/commit/ccb183f9fca20a37b9b39761dedcd197e3cf2033", "message": "HOTFIX: ChangeLoggingKeyValueStore.name() returns null\n\nThis class doesn't need to override this method as it is handled appropriately by the super class\n\nAuthor: Damian Guy <damian.guy@gmail.com>\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>\n\nCloses #2397 from dguy/hotfix-npe-state-store", "bug_id": "kafka_61", "file": [{"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/961ebca57fdd4f8f84df8cf26a835f07ea1718c9/streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueStore.java", "blob_url": "https://github.com/apache/kafka/blob/961ebca57fdd4f8f84df8cf26a835f07ea1718c9/streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueStore.java", "sha": "cd63d1ab30b75e23de00dfa3b1d64e0c04173c04", "changes": 7, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueStore.java?ref=961ebca57fdd4f8f84df8cf26a835f07ea1718c9", "patch": "@@ -48,12 +48,7 @@ private ChangeLoggingKeyValueStore(final ChangeLoggingKeyValueBytesStore bytesSt\n         this.keySerde = keySerde;\n         this.valueSerde = valueSerde;\n     }\n-\n-    @Override\n-    public String name() {\n-        return null;\n-    }\n-\n+    \n     @SuppressWarnings(\"unchecked\")\n     @Override\n     public void init(final ProcessorContext context, final StateStore root) {", "filename": "streams/src/main/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueStore.java"}, {"additions": 5, "raw_url": "https://github.com/apache/kafka/raw/961ebca57fdd4f8f84df8cf26a835f07ea1718c9/streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueStoreTest.java", "blob_url": "https://github.com/apache/kafka/blob/961ebca57fdd4f8f84df8cf26a835f07ea1718c9/streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueStoreTest.java", "sha": "2dc2017fd48627cdd12ebe27b912d374e31d3724", "changes": 5, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueStoreTest.java?ref=961ebca57fdd4f8f84df8cf26a835f07ea1718c9", "patch": "@@ -200,6 +200,11 @@ public void shouldReturnNullOnGetWhenDoesntExist() throws Exception {\n         assertThat(store.get(hello), is(nullValue()));\n     }\n \n+    @Test\n+    public void shouldReturnInnerStoreName() throws Exception {\n+        assertThat(store.name(), equalTo(\"kv\"));\n+    }\n+\n     private String deserializedValueFromInner(final String key) {\n         return valueSerde.deserializer().deserialize(\"blah\", inner.get(Bytes.wrap(key.getBytes())));\n     }", "filename": "streams/src/test/java/org/apache/kafka/streams/state/internals/ChangeLoggingKeyValueStoreTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/ff300c9d4f45e4a355db11258965c3a3a6f6bbf7", "parent": "https://github.com/apache/kafka/commit/49ddc897b8feda9c4786d5bcd03814b91ede7124", "message": "KAFKA-3645; Fix ConsumerGroupCommand and ConsumerOffsetChecker to correctly read endpoint info from ZK\n\nThe host and port entries under /brokers/ids/<bid> gets filled only for PLAINTEXT security protocol. For other protocols the host is null and the actual endpoint is under \"endpoints\". This causes NPE when running the consumer group and offset checker scripts in a kerberized env. By always reading the host and port values from the \"endpoint\", a more meaningful exception would be thrown rather than a NPE.\n\nAuthor: Arun Mahadevan <aiyer@hortonworks.com>\n\nReviewers: Sriharsha Chintalapani <schintalapani@hortonworks.com>, Ismael Juma <ismael@juma.me.uk>\n\nCloses #1301 from arunmahadevan/cg_kerb_fix", "bug_id": "kafka_62", "file": [{"additions": 5, "raw_url": "https://github.com/apache/kafka/raw/ff300c9d4f45e4a355db11258965c3a3a6f6bbf7/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala", "blob_url": "https://github.com/apache/kafka/blob/ff300c9d4f45e4a355db11258965c3a3a6f6bbf7/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala", "sha": "f0c817fd8823ab5e7b0e44615670cc9d353a907d", "changes": 20, "status": "modified", "deletions": 15, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala?ref=ff300c9d4f45e4a355db11258965c3a3a6f6bbf7", "patch": "@@ -30,7 +30,7 @@ import org.apache.kafka.clients.CommonClientConfigs\n import org.apache.kafka.clients.consumer.{ConsumerConfig, KafkaConsumer}\n import org.apache.kafka.common.TopicPartition\n import org.apache.kafka.common.errors.BrokerNotAvailableException\n-import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.protocol.{Errors, SecurityProtocol}\n import org.apache.kafka.common.security.JaasUtils\n import org.apache.kafka.common.serialization.StringDeserializer\n import org.apache.kafka.common.utils.Utils\n@@ -277,20 +277,10 @@ object ConsumerGroupCommand {\n \n     private def getZkConsumer(brokerId: Int): Option[SimpleConsumer] = {\n       try {\n-        zkUtils.readDataMaybeNull(ZkUtils.BrokerIdsPath + \"/\" + brokerId)._1 match {\n-          case Some(brokerInfoString) =>\n-            Json.parseFull(brokerInfoString) match {\n-              case Some(m) =>\n-                val brokerInfo = m.asInstanceOf[Map[String, Any]]\n-                val host = brokerInfo.get(\"host\").get.asInstanceOf[String]\n-                val port = brokerInfo.get(\"port\").get.asInstanceOf[Int]\n-                Some(new SimpleConsumer(host, port, 10000, 100000, \"ConsumerGroupCommand\"))\n-              case None =>\n-                throw new BrokerNotAvailableException(\"Broker id %d does not exist\".format(brokerId))\n-            }\n-          case None =>\n-            throw new BrokerNotAvailableException(\"Broker id %d does not exist\".format(brokerId))\n-        }\n+        zkUtils.getBrokerInfo(brokerId)\n+          .map(_.getBrokerEndPoint(SecurityProtocol.PLAINTEXT))\n+          .map(endPoint => new SimpleConsumer(endPoint.host, endPoint.port, 10000, 100000, \"ConsumerGroupCommand\"))\n+          .orElse(throw new BrokerNotAvailableException(\"Broker id %d does not exist\".format(brokerId)))\n       } catch {\n         case t: Throwable =>\n           println(\"Could not parse broker info due to \" + t.getMessage)", "filename": "core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala"}, {"additions": 7, "raw_url": "https://github.com/apache/kafka/raw/ff300c9d4f45e4a355db11258965c3a3a6f6bbf7/core/src/main/scala/kafka/tools/ConsumerOffsetChecker.scala", "blob_url": "https://github.com/apache/kafka/blob/ff300c9d4f45e4a355db11258965c3a3a6f6bbf7/core/src/main/scala/kafka/tools/ConsumerOffsetChecker.scala", "sha": "8f86f6612df56d6c0bdd77a08b40babfb8a545dd", "changes": 23, "status": "modified", "deletions": 16, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/tools/ConsumerOffsetChecker.scala?ref=ff300c9d4f45e4a355db11258965c3a3a6f6bbf7", "patch": "@@ -21,11 +21,12 @@ package kafka.tools\n import joptsimple._\n import kafka.utils._\n import kafka.consumer.SimpleConsumer\n-import kafka.api.{OffsetFetchResponse, OffsetFetchRequest, OffsetRequest}\n+import kafka.api.{OffsetFetchRequest, OffsetFetchResponse, OffsetRequest}\n import kafka.common.{OffsetMetadataAndError, TopicAndPartition}\n import org.apache.kafka.common.errors.BrokerNotAvailableException\n-import org.apache.kafka.common.protocol.Errors\n+import org.apache.kafka.common.protocol.{Errors, SecurityProtocol}\n import org.apache.kafka.common.security.JaasUtils\n+\n import scala.collection._\n import kafka.client.ClientUtils\n import kafka.network.BlockingChannel\n@@ -40,20 +41,10 @@ object ConsumerOffsetChecker extends Logging {\n \n   private def getConsumer(zkUtils: ZkUtils, bid: Int): Option[SimpleConsumer] = {\n     try {\n-      zkUtils.readDataMaybeNull(ZkUtils.BrokerIdsPath + \"/\" + bid)._1 match {\n-        case Some(brokerInfoString) =>\n-          Json.parseFull(brokerInfoString) match {\n-            case Some(m) =>\n-              val brokerInfo = m.asInstanceOf[Map[String, Any]]\n-              val host = brokerInfo.get(\"host\").get.asInstanceOf[String]\n-              val port = brokerInfo.get(\"port\").get.asInstanceOf[Int]\n-              Some(new SimpleConsumer(host, port, 10000, 100000, \"ConsumerOffsetChecker\"))\n-            case None =>\n-              throw new BrokerNotAvailableException(\"Broker id %d does not exist\".format(bid))\n-          }\n-        case None =>\n-          throw new BrokerNotAvailableException(\"Broker id %d does not exist\".format(bid))\n-      }\n+      zkUtils.getBrokerInfo(bid)\n+        .map(_.getBrokerEndPoint(SecurityProtocol.PLAINTEXT))\n+        .map(endPoint => new SimpleConsumer(endPoint.host, endPoint.port, 10000, 100000, \"ConsumerOffsetChecker\"))\n+        .orElse(throw new BrokerNotAvailableException(\"Broker id %d does not exist\".format(bid)))\n     } catch {\n       case t: Throwable =>\n         println(\"Could not parse broker info due to \" + t.getCause)", "filename": "core/src/main/scala/kafka/tools/ConsumerOffsetChecker.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/105ab47ed90c8a0e83c159c97a8f2294c5582657", "parent": "https://github.com/apache/kafka/commit/10cd98cc894b88c5d1e24fc54c66361ad9914df2", "message": "KAFKA-6015; Fix NPE in RecordAccumulator after ProducerId reset\n\nIt is possible for batches with sequence numbers to be in the `deque` while at the same time the in flight batches in the `TransactionManager` are removed due to a producerId reset.\n\nIn this case, when the batches in the `deque` are drained, we will get a `NullPointerException` in the background thread due to this line:\n\n```java\nif (first.hasSequence() && first.baseSequence() != transactionManager.nextBatchBySequence(first.topicPartition).baseSequence())\n```\n\nParticularly, `transactionManager.nextBatchBySequence` will return null, because there no inflight batches being tracked.\n\nIn this patch, we simply allow the batches in the `deque` to be drained if there are no in flight batches being tracked in the TransactionManager. If they succeed, well and good. If the responses come back with an error, the batces will be ultimately failed in the producer with an `OutOfOrderSequenceException` when the response comes back.\n\nAuthor: Apurva Mehta <apurva@confluent.io>\n\nReviewers: Jason Gustafson <jason@confluent.io>\n\nCloses #4022 from apurvam/KAFKA-6015-npe-in-record-accumulator", "bug_id": "kafka_63", "file": [{"additions": 3, "raw_url": "https://github.com/apache/kafka/raw/105ab47ed90c8a0e83c159c97a8f2294c5582657/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java", "blob_url": "https://github.com/apache/kafka/blob/105ab47ed90c8a0e83c159c97a8f2294c5582657/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java", "sha": "ba8c28ece27f65c0fdcaf4ba3448746e59dfa242", "changes": 5, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java?ref=105ab47ed90c8a0e83c159c97a8f2294c5582657", "patch": "@@ -538,8 +538,9 @@ public boolean hasUndrained() {\n                                                 // on the client after being sent to the broker at least once.\n                                                 break;\n \n-                                            if (first.hasSequence()\n-                                                    && first.baseSequence() != transactionManager.nextBatchBySequence(first.topicPartition).baseSequence())\n+                                            int firstInFlightSequence = transactionManager.firstInFlightSequence(first.topicPartition);\n+                                            if (firstInFlightSequence != RecordBatch.NO_SEQUENCE && first.hasSequence()\n+                                                    && first.baseSequence() != firstInFlightSequence)\n                                                 // If the queued batch already has an assigned sequence, then it is being\n                                                 // retried. In this case, we wait until the next immediate batch is ready\n                                                 // and drain that. We only move on when the next in line batch is complete (either successfully", "filename": "clients/src/main/java/org/apache/kafka/clients/producer/internals/RecordAccumulator.java"}, {"additions": 2, "raw_url": "https://github.com/apache/kafka/raw/105ab47ed90c8a0e83c159c97a8f2294c5582657/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java", "blob_url": "https://github.com/apache/kafka/blob/105ab47ed90c8a0e83c159c97a8f2294c5582657/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java", "sha": "7eea4992b33151ee99b82db823c7c0b510a70a60", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java?ref=105ab47ed90c8a0e83c159c97a8f2294c5582657", "patch": "@@ -528,8 +528,8 @@ private void completeBatch(ProducerBatch batch, ProduceResponse.PartitionRespons\n                 } else if (transactionManager.hasProducerIdAndEpoch(batch.producerId(), batch.producerEpoch())) {\n                     // If idempotence is enabled only retry the request if the current producer id is the same as\n                     // the producer id of the batch.\n-                    log.debug(\"Retrying batch to topic-partition {}. Sequence number : {}\", batch.topicPartition,\n-                            batch.baseSequence());\n+                    log.debug(\"Retrying batch to topic-partition {}. ProducerId: {}; Sequence number : {}\",\n+                            batch.topicPartition, batch.producerId(), batch.baseSequence());\n                     reenqueueBatch(batch, now);\n                 } else {\n                     failBatch(batch, response, new OutOfOrderSequenceException(\"Attempted to retry sending a \" +", "filename": "clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java"}, {"additions": 19, "raw_url": "https://github.com/apache/kafka/raw/105ab47ed90c8a0e83c159c97a8f2294c5582657/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java", "blob_url": "https://github.com/apache/kafka/blob/105ab47ed90c8a0e83c159c97a8f2294c5582657/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java", "sha": "006a12b1bfd4ca73035e7d69ab894a32b87fb27e", "changes": 19, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java?ref=105ab47ed90c8a0e83c159c97a8f2294c5582657", "patch": "@@ -26,6 +26,7 @@\n import org.apache.kafka.common.errors.GroupAuthorizationException;\n import org.apache.kafka.common.errors.TopicAuthorizationException;\n import org.apache.kafka.common.protocol.Errors;\n+import org.apache.kafka.common.record.RecordBatch;\n import org.apache.kafka.common.requests.AbstractRequest;\n import org.apache.kafka.common.requests.AbstractResponse;\n import org.apache.kafka.common.requests.AddOffsetsToTxnRequest;\n@@ -435,6 +436,24 @@ public int compare(ProducerBatch o1, ProducerBatch o2) {\n         inflightBatchesBySequence.get(batch.topicPartition).offer(batch);\n     }\n \n+    /**\n+     * Returns the first inflight sequence for a given partition. This is the base sequence of an inflight batch with\n+     * the lowest sequence number.\n+     * @return the lowest inflight sequence if the transaction manager is tracking inflight requests for this partition.\n+     *         If there are no inflight requests being tracked for this partition, this method will return\n+     *         RecordBatch.NO_SEQUENCE.\n+     */\n+    synchronized int firstInFlightSequence(TopicPartition topicPartition) {\n+        PriorityQueue<ProducerBatch> inFlightBatches = inflightBatchesBySequence.get(topicPartition);\n+        if (inFlightBatches == null)\n+            return RecordBatch.NO_SEQUENCE;\n+\n+        ProducerBatch firstInFlightBatch = inFlightBatches.peek();\n+        if (firstInFlightBatch == null)\n+            return RecordBatch.NO_SEQUENCE;\n+\n+        return firstInFlightBatch.baseSequence();\n+    }\n \n     synchronized ProducerBatch nextBatchBySequence(TopicPartition topicPartition) {\n         PriorityQueue<ProducerBatch> queue = inflightBatchesBySequence.get(topicPartition);", "filename": "clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java"}, {"additions": 116, "raw_url": "https://github.com/apache/kafka/raw/105ab47ed90c8a0e83c159c97a8f2294c5582657/clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java", "blob_url": "https://github.com/apache/kafka/blob/105ab47ed90c8a0e83c159c97a8f2294c5582657/clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java", "sha": "1ce8e5a4bbdb48024aa23c2b8ce390999fb0bae2", "changes": 116, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java?ref=105ab47ed90c8a0e83c159c97a8f2294c5582657", "patch": "@@ -1104,6 +1104,103 @@ public void testExpiryOfAllSentBatchesShouldCauseUnresolvedSequences() throws Ex\n         assertTrue(transactionManager.hasProducerId(producerId + 1));\n     }\n \n+    @Test\n+    public void testResetOfProducerStateShouldAllowQueuedBatchesToDrain() throws Exception {\n+        final long producerId = 343434L;\n+        TransactionManager transactionManager = new TransactionManager();\n+        transactionManager.setProducerIdAndEpoch(new ProducerIdAndEpoch(producerId, (short) 0));\n+        setupWithTransactionState(transactionManager);\n+        client.setNode(new Node(1, \"localhost\", 33343));\n+\n+        int maxRetries = 10;\n+        Metrics m = new Metrics();\n+        SenderMetricsRegistry senderMetrics = new SenderMetricsRegistry(m);\n+\n+        Sender sender = new Sender(logContext, client, metadata, this.accumulator, true, MAX_REQUEST_SIZE, ACKS_ALL, maxRetries,\n+                senderMetrics, time, REQUEST_TIMEOUT, 50, transactionManager, apiVersions);\n+\n+        Future<RecordMetadata> failedResponse = accumulator.append(tp0, time.milliseconds(), \"key\".getBytes(),\n+                \"value\".getBytes(), null, null, MAX_BLOCK_TIMEOUT).future;\n+        Future<RecordMetadata> successfulResponse = accumulator.append(tp1, time.milliseconds(), \"key\".getBytes(),\n+                \"value\".getBytes(), null, null, MAX_BLOCK_TIMEOUT).future;\n+        sender.run(time.milliseconds());  // connect.\n+        sender.run(time.milliseconds());  // send.\n+\n+        assertEquals(1, client.inFlightRequestCount());\n+\n+        Map<TopicPartition, OffsetAndError> responses = new LinkedHashMap<>();\n+        responses.put(tp1, new OffsetAndError(-1, Errors.NOT_LEADER_FOR_PARTITION));\n+        responses.put(tp0, new OffsetAndError(-1, Errors.OUT_OF_ORDER_SEQUENCE_NUMBER));\n+        client.respond(produceResponse(responses));\n+        sender.run(time.milliseconds());\n+        assertTrue(failedResponse.isDone());\n+        assertFalse(\"Expected transaction state to be reset upon receiving an OutOfOrderSequenceException\", transactionManager.hasProducerId());\n+        prepareAndReceiveInitProducerId(producerId + 1, Errors.NONE);\n+        assertEquals(producerId + 1, transactionManager.producerIdAndEpoch().producerId);\n+        sender.run(time.milliseconds());  // send request to tp1\n+\n+        assertFalse(successfulResponse.isDone());\n+        client.respond(produceResponse(tp1, 10, Errors.NONE, -1));\n+        sender.run(time.milliseconds());\n+\n+        assertTrue(successfulResponse.isDone());\n+        assertEquals(10, successfulResponse.get().offset());\n+\n+        // Since the response came back for the old producer id, we shouldn't update the next sequence.\n+        assertEquals(0, transactionManager.sequenceNumber(tp1).longValue());\n+    }\n+\n+    @Test\n+    public void testBatchesDrainedWithOldProducerIdShouldFailWithOutOfOrderSequenceOnSubsequentRetry() throws Exception {\n+        final long producerId = 343434L;\n+        TransactionManager transactionManager = new TransactionManager();\n+        transactionManager.setProducerIdAndEpoch(new ProducerIdAndEpoch(producerId, (short) 0));\n+        setupWithTransactionState(transactionManager);\n+        client.setNode(new Node(1, \"localhost\", 33343));\n+\n+        int maxRetries = 10;\n+        Metrics m = new Metrics();\n+        SenderMetricsRegistry senderMetrics = new SenderMetricsRegistry(m);\n+\n+        Sender sender = new Sender(logContext, client, metadata, this.accumulator, true, MAX_REQUEST_SIZE, ACKS_ALL, maxRetries,\n+                senderMetrics, time, REQUEST_TIMEOUT, 50, transactionManager, apiVersions);\n+\n+        Future<RecordMetadata> failedResponse = accumulator.append(tp0, time.milliseconds(), \"key\".getBytes(),\n+                \"value\".getBytes(), null, null, MAX_BLOCK_TIMEOUT).future;\n+        Future<RecordMetadata> successfulResponse = accumulator.append(tp1, time.milliseconds(), \"key\".getBytes(),\n+                \"value\".getBytes(), null, null, MAX_BLOCK_TIMEOUT).future;\n+        sender.run(time.milliseconds());  // connect.\n+        sender.run(time.milliseconds());  // send.\n+\n+        assertEquals(1, client.inFlightRequestCount());\n+\n+        Map<TopicPartition, OffsetAndError> responses = new LinkedHashMap<>();\n+        responses.put(tp1, new OffsetAndError(-1, Errors.NOT_LEADER_FOR_PARTITION));\n+        responses.put(tp0, new OffsetAndError(-1, Errors.OUT_OF_ORDER_SEQUENCE_NUMBER));\n+        client.respond(produceResponse(responses));\n+        sender.run(time.milliseconds());\n+        assertTrue(failedResponse.isDone());\n+        assertFalse(\"Expected transaction state to be reset upon receiving an OutOfOrderSequenceException\", transactionManager.hasProducerId());\n+        prepareAndReceiveInitProducerId(producerId + 1, Errors.NONE);\n+        assertEquals(producerId + 1, transactionManager.producerIdAndEpoch().producerId);\n+        sender.run(time.milliseconds());  // send request to tp1 with the old producerId\n+\n+        assertFalse(successfulResponse.isDone());\n+        // The response comes back with a retriable error.\n+        client.respond(produceResponse(tp1, 0, Errors.NOT_LEADER_FOR_PARTITION, -1));\n+        sender.run(time.milliseconds());\n+\n+        assertTrue(successfulResponse.isDone());\n+        // Since the batch has an old producerId, it will not be retried yet again, but will be failed with a Fatal\n+        // exception.\n+        try {\n+            successfulResponse.get();\n+            fail(\"Should have raised an OutOfOrderSequenceException\");\n+        } catch (Exception e) {\n+            assertTrue(e.getCause() instanceof OutOfOrderSequenceException);\n+        }\n+    }\n+\n     @Test\n     public void testCorrectHandlingOfDuplicateSequenceError() throws Exception {\n         final long producerId = 343434L;\n@@ -1799,12 +1896,31 @@ public boolean matches(AbstractRequest body) {\n         };\n     }\n \n+    class OffsetAndError {\n+        long offset;\n+        Errors error;\n+        OffsetAndError(long offset, Errors error) {\n+            this.offset = offset;\n+            this.error = error;\n+        }\n+    }\n+\n     private ProduceResponse produceResponse(TopicPartition tp, long offset, Errors error, int throttleTimeMs, long logStartOffset) {\n         ProduceResponse.PartitionResponse resp = new ProduceResponse.PartitionResponse(error, offset, RecordBatch.NO_TIMESTAMP, logStartOffset);\n         Map<TopicPartition, ProduceResponse.PartitionResponse> partResp = Collections.singletonMap(tp, resp);\n         return new ProduceResponse(partResp, throttleTimeMs);\n     }\n \n+    private ProduceResponse produceResponse(Map<TopicPartition, OffsetAndError> responses) {\n+        Map<TopicPartition, ProduceResponse.PartitionResponse> partResponses = new LinkedHashMap<>();\n+        for (Map.Entry<TopicPartition, OffsetAndError> entry : responses.entrySet()) {\n+            ProduceResponse.PartitionResponse response = new ProduceResponse.PartitionResponse(entry.getValue().error,\n+                    entry.getValue().offset, RecordBatch.NO_TIMESTAMP, -1);\n+            partResponses.put(entry.getKey(), response);\n+        }\n+        return new ProduceResponse(partResponses);\n+\n+    }\n     private ProduceResponse produceResponse(TopicPartition tp, long offset, Errors error, int throttleTimeMs) {\n         return produceResponse(tp, offset, error, throttleTimeMs, -1L);\n     }", "filename": "clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java"}, {"additions": 17, "raw_url": "https://github.com/apache/kafka/raw/105ab47ed90c8a0e83c159c97a8f2294c5582657/core/src/main/scala/kafka/log/ProducerStateManager.scala", "blob_url": "https://github.com/apache/kafka/blob/105ab47ed90c8a0e83c159c97a8f2294c5582657/core/src/main/scala/kafka/log/ProducerStateManager.scala", "sha": "7c0a3da6d6adc2ed67f150e0dc0d33fbfb3614fd", "changes": 20, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/log/ProducerStateManager.scala?ref=105ab47ed90c8a0e83c159c97a8f2294c5582657", "patch": "@@ -40,8 +40,22 @@ class CorruptSnapshotException(msg: String) extends KafkaException(msg)\n // ValidationType and its subtypes define the extent of the validation to perform on a given ProducerAppendInfo instance\n private[log] sealed trait ValidationType\n private[log] object ValidationType {\n+\n+  /**\n+    * This indicates no validation should be performed on the incoming append. This is the case for all appends on\n+    * a replica, as well as appends when the producer state is being built from the log.\n+    */\n   case object None extends ValidationType\n+\n+  /**\n+    * We only validate the epoch (and not the sequence numbers) for offset commit requests coming from the transactional\n+    * producer. These appends will not have sequence numbers, so we can't validate them.\n+    */\n   case object EpochOnly extends ValidationType\n+\n+  /**\n+    * Perform the full validation. This should be used fo regular produce requests coming to the leader.\n+    */\n   case object Full extends ValidationType\n }\n \n@@ -148,9 +162,9 @@ private[log] class ProducerIdEntry(val producerId: Long, val batchMetadata: muta\n  *                      be made against the lastest append in the current entry. New appends will replace older appends\n  *                      in the current entry so that the space overhead is constant.\n  * @param validationType Indicates the extent of validation to perform on the appends on this instance. Offset commits\n- *                       coming from the producer should have EpochOnlyValidation. Appends which aren't from a client\n- *                       will not be validated at all, and should be set to NoValidation. All other appends should\n- *                       have FullValidation.\n+ *                       coming from the producer should have ValidationType.EpochOnly. Appends which aren't from a client\n+ *                       should have ValidationType.None. Appends coming from a client for produce requests should have\n+ *                       ValidationType.Full.\n  */\n private[log] class ProducerAppendInfo(val producerId: Long,\n                                       currentEntry: ProducerIdEntry,", "filename": "core/src/main/scala/kafka/log/ProducerStateManager.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/b51002c576ea9758132d75a8a0fe454e1bc270a2", "parent": "https://github.com/apache/kafka/commit/c2a8b86117ede2ffda4cc4a8800b46f65ef9922d", "message": "KAFKA-4312: If filePath is empty string writeAsText should have more meaningful error message\n\n\u2026eaningful error message\n\nAuthor: bbejeck <bbejeck@gmail.com>\n\nReviewers: Guozhang Wang <wangguoz@gmail.com>\n\nCloses #2042 from bbejeck/KAFKA-4312_write_as_text_throws_NPE_empty_string", "bug_id": "kafka_64", "file": [{"additions": 3, "raw_url": "https://github.com/apache/kafka/raw/b51002c576ea9758132d75a8a0fe454e1bc270a2/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java", "blob_url": "https://github.com/apache/kafka/blob/b51002c576ea9758132d75a8a0fe454e1bc270a2/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java", "sha": "bb77e963655e35db9456b7941bbec84bc99b07ee", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java?ref=b51002c576ea9758132d75a8a0fe454e1bc270a2", "patch": "@@ -207,6 +207,9 @@ public void writeAsText(String filePath, Serde<K> keySerde, Serde<V> valSerde) {\n     @Override\n     public void writeAsText(String filePath, String streamName, Serde<K> keySerde, Serde<V> valSerde) {\n         Objects.requireNonNull(filePath, \"filePath can't be null\");\n+        if (filePath.trim().isEmpty()) {\n+            throw new TopologyBuilderException(\"filePath can't be an empty string\");\n+        }\n         String name = topology.newName(PRINTING_NAME);\n         streamName = (streamName == null) ? this.name : streamName;\n         try {", "filename": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KStreamImpl.java"}, {"additions": 4, "raw_url": "https://github.com/apache/kafka/raw/b51002c576ea9758132d75a8a0fe454e1bc270a2/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java", "blob_url": "https://github.com/apache/kafka/blob/b51002c576ea9758132d75a8a0fe454e1bc270a2/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java", "sha": "fc1c07674c8b5425ade4df71c4f6d92f56af235d", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java?ref=b51002c576ea9758132d75a8a0fe454e1bc270a2", "patch": "@@ -189,6 +189,10 @@ public void writeAsText(String filePath, Serde<K> keySerde, Serde<V> valSerde) {\n      */\n     @Override\n     public void writeAsText(String filePath, String streamName, Serde<K> keySerde, Serde<V> valSerde) {\n+        Objects.requireNonNull(filePath, \"filePath can't be null\");\n+        if (filePath.trim().isEmpty()) {\n+            throw new TopologyBuilderException(\"filePath can't be an empty string\");\n+        }\n         String name = topology.newName(PRINTING_NAME);\n         streamName = (streamName == null) ? this.name : streamName;\n         try {", "filename": "streams/src/main/java/org/apache/kafka/streams/kstream/internals/KTableImpl.java"}, {"additions": 6, "raw_url": "https://github.com/apache/kafka/raw/b51002c576ea9758132d75a8a0fe454e1bc270a2/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplTest.java", "blob_url": "https://github.com/apache/kafka/blob/b51002c576ea9758132d75a8a0fe454e1bc270a2/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplTest.java", "sha": "e5e334c48b49f456eac4b0b6ada772321a2b6415", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplTest.java?ref=b51002c576ea9758132d75a8a0fe454e1bc270a2", "patch": "@@ -19,6 +19,7 @@\n \n import org.apache.kafka.common.serialization.Serde;\n import org.apache.kafka.common.serialization.Serdes;\n+import org.apache.kafka.streams.errors.TopologyBuilderException;\n import org.apache.kafka.streams.kstream.JoinWindows;\n import org.apache.kafka.streams.kstream.KStream;\n import org.apache.kafka.streams.kstream.KStreamBuilder;\n@@ -183,6 +184,11 @@ public void shouldNotAllowNullFilePathOnWriteAsText() throws Exception {\n         testStream.writeAsText(null);\n     }\n \n+    @Test(expected = TopologyBuilderException.class)\n+    public void shouldNotAllowEmptyFilePathOnWriteAsText() throws Exception {\n+        testStream.writeAsText(\"\\t    \\t\");\n+    }\n+\n     @Test(expected = NullPointerException.class)\n     public void shouldNotAllowNullMapperOnFlatMap() throws Exception {\n         testStream.flatMap(null);", "filename": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KStreamImplTest.java"}, {"additions": 6, "raw_url": "https://github.com/apache/kafka/raw/b51002c576ea9758132d75a8a0fe454e1bc270a2/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableImplTest.java", "blob_url": "https://github.com/apache/kafka/blob/b51002c576ea9758132d75a8a0fe454e1bc270a2/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableImplTest.java", "sha": "afa1033d732a0d85520435c845a2ada7b883bf55", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableImplTest.java?ref=b51002c576ea9758132d75a8a0fe454e1bc270a2", "patch": "@@ -20,6 +20,7 @@\n import org.apache.kafka.common.serialization.Serde;\n import org.apache.kafka.common.serialization.Serdes;\n import org.apache.kafka.common.utils.Utils;\n+import org.apache.kafka.streams.errors.TopologyBuilderException;\n import org.apache.kafka.streams.kstream.KStreamBuilder;\n import org.apache.kafka.streams.kstream.KTable;\n import org.apache.kafka.streams.kstream.Predicate;\n@@ -402,6 +403,11 @@ public void shouldNotAllowNullFilePathOnWriteAsText() throws Exception {\n         table.writeAsText(null);\n     }\n \n+    @Test(expected = TopologyBuilderException.class)\n+    public void shouldNotAllowEmptyFilePathOnWriteAsText() throws Exception {\n+        table.writeAsText(\"\\t  \\t\");\n+    }\n+\n     @Test(expected = NullPointerException.class)\n     public void shouldNotAllowNullActionOnForEach() throws Exception {\n         table.foreach(null);", "filename": "streams/src/test/java/org/apache/kafka/streams/kstream/internals/KTableImplTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/00a58f8e1e0c82c2948a8fdfacf812ec4865b339", "parent": "https://github.com/apache/kafka/commit/9cd65c46a71cb07276f23838aa55747efc32674c", "message": "KAFKA-2944: Replaced the NPE with a nicer error and clean exit and added debug message to assist with figuring this out.\n\n\u2026ssage to assist with figuring this out.\n\nAuthor: Gwen Shapira <cshapi@gmail.com>\n\nReviewers: Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #993 from gwenshap/KAFKA-2944", "bug_id": "kafka_65", "file": [{"additions": 16, "raw_url": "https://github.com/apache/kafka/raw/00a58f8e1e0c82c2948a8fdfacf812ec4865b339/connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigStorage.java", "blob_url": "https://github.com/apache/kafka/blob/00a58f8e1e0c82c2948a8fdfacf812ec4865b339/connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigStorage.java", "sha": "6a06fecb4b4346eb2b58489f9f63855fa2d8b84e", "changes": 20, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigStorage.java?ref=00a58f8e1e0c82c2948a8fdfacf812ec4865b339", "patch": "@@ -286,6 +286,7 @@ public void putConnectorConfig(String connector, Map<String, String> properties)\n         }\n \n         try {\n+            log.debug(\"Writing connector configuration for connector \" + connector + \" configuration: \" + properties);\n             configLog.send(CONNECTOR_KEY(connector), serializedConfig);\n             configLog.readToEnd().get(READ_TO_END_TIMEOUT_MS, TimeUnit.MILLISECONDS);\n         } catch (InterruptedException | ExecutionException | TimeoutException e) {\n@@ -334,6 +335,7 @@ public void putTaskConfigs(Map<ConnectorTaskId, Map<String, String>> configs) {\n             Struct connectConfig = new Struct(TASK_CONFIGURATION_V0);\n             connectConfig.put(\"properties\", taskConfigEntry.getValue());\n             byte[] serializedConfig = converter.fromConnectData(topic, TASK_CONFIGURATION_V0, connectConfig);\n+            log.debug(\"Writing configuration for task \" + taskConfigEntry.getKey() + \" configuration: \" + taskConfigEntry.getValue());\n             configLog.send(TASK_KEY(taskConfigEntry.getKey()), serializedConfig);\n         }\n \n@@ -348,6 +350,7 @@ public void putTaskConfigs(Map<ConnectorTaskId, Map<String, String>> configs) {\n                 Struct connectConfig = new Struct(CONNECTOR_TASKS_COMMIT_V0);\n                 connectConfig.put(\"tasks\", taskCountEntry.getValue());\n                 byte[] serializedConfig = converter.fromConnectData(topic, CONNECTOR_TASKS_COMMIT_V0, connectConfig);\n+                log.debug(\"Writing commit for connector \" + taskCountEntry.getKey() + \" with \" + taskCountEntry.getValue() + \" tasks.\");\n                 configLog.send(COMMIT_TASKS_KEY(taskCountEntry.getKey()), serializedConfig);\n             }\n \n@@ -396,6 +399,7 @@ public void onCompletion(Throwable error, ConsumerRecord<String, byte[]> record)\n                 synchronized (lock) {\n                     if (value.value() == null) {\n                         // Connector deletion will be written as a null value\n+                        log.info(\"Removed connector \" + connectorName + \" due to null configuration. This is usually intentional and does not indicate an issue.\");\n                         connectorConfigs.remove(connectorName);\n                     } else {\n                         // Connector configs can be applied and callbacks invoked immediately\n@@ -405,9 +409,10 @@ public void onCompletion(Throwable error, ConsumerRecord<String, byte[]> record)\n                         }\n                         Object newConnectorConfig = ((Map<String, Object>) value.value()).get(\"properties\");\n                         if (!(newConnectorConfig instanceof Map)) {\n-                            log.error(\"Invalid data for connector config: properties filed should be a Map but is \" + newConnectorConfig.getClass());\n+                            log.error(\"Invalid data for connector config (\" + connectorName + \"): properties filed should be a Map but is \" + newConnectorConfig.getClass());\n                             return;\n                         }\n+                        log.debug(\"Updating configuration for connector \" + connectorName + \" configuation: \" + newConnectorConfig);\n                         connectorConfigs.put(connectorName, (Map<String, String>) newConnectorConfig);\n                     }\n                 }\n@@ -421,13 +426,13 @@ public void onCompletion(Throwable error, ConsumerRecord<String, byte[]> record)\n                         return;\n                     }\n                     if (!(value.value() instanceof Map)) {\n-                        log.error(\"Ignoring task configuration because it is in the wrong format: \" + value.value());\n+                        log.error(\"Ignoring task configuration for task \" + taskId + \" because it is in the wrong format: \" + value.value());\n                         return;\n                     }\n \n                     Object newTaskConfig = ((Map<String, Object>) value.value()).get(\"properties\");\n                     if (!(newTaskConfig instanceof Map)) {\n-                        log.error(\"Invalid data for task config: properties filed should be a Map but is \" + newTaskConfig.getClass());\n+                        log.error(\"Invalid data for task config (\" + taskId + \"): properties filed should be a Map but is \" + newTaskConfig.getClass());\n                         return;\n                     }\n \n@@ -436,6 +441,7 @@ public void onCompletion(Throwable error, ConsumerRecord<String, byte[]> record)\n                         deferred = new HashMap<>();\n                         deferredTaskUpdates.put(taskId.connector(), deferred);\n                     }\n+                    log.debug(\"Storing new config for task \" + taskId + \" this will wait for a commit message before the new config will take effect. New config: \" + newTaskConfig);\n                     deferred.put(taskId, (Map<String, String>) newTaskConfig);\n                 }\n             } else if (record.key().startsWith(COMMIT_TASKS_PREFIX)) {\n@@ -464,7 +470,7 @@ public void onCompletion(Throwable error, ConsumerRecord<String, byte[]> record)\n                     // resolve this (i.e., get the connector to recommit its configuration). This inconsistent state is\n                     // exposed in the snapshots provided via ClusterConfigState so they are easy to handle.\n                     if (!(value.value() instanceof Map)) { // Schema-less, so we get maps instead of structs\n-                        log.error(\"Ignoring connector tasks configuration commit because it is in the wrong format: \" + value.value());\n+                        log.error(\"Ignoring connector tasks configuration commit for connector \" + connectorName + \" because it is in the wrong format: \" + value.value());\n                         return;\n                     }\n \n@@ -476,11 +482,17 @@ public void onCompletion(Throwable error, ConsumerRecord<String, byte[]> record)\n                     // update of all tasks that are expected based on the number of tasks in the commit message.\n                     Map<String, Set<Integer>> updatedConfigIdsByConnector = taskIdsByConnector(deferred);\n                     Set<Integer> taskIdSet = updatedConfigIdsByConnector.get(connectorName);\n+                    if (taskIdSet == null) {\n+                        //TODO: Figure out why this happens (KAFKA-3321)\n+                        log.error(\"Received a commit message for connector \" + connectorName + \" but there is no matching configuration for tasks in this connector. This should never happen.\");\n+                        return;\n+                    }\n                     if (!completeTaskIdSet(taskIdSet, newTaskCount)) {\n                         // Given the logic for writing commit messages, we should only hit this condition due to compacted\n                         // historical data, in which case we would not have applied any updates yet and there will be no\n                         // task config data already committed for the connector, so we shouldn't have to clear any data\n                         // out. All we need to do is add the flag marking it inconsistent.\n+                        log.debug(\"We have an incomplete set of task configs for connector \" + connectorName + \" probably due to compaction. So we are not doing anything with the new configuration.\");\n                         inconsistent.add(connectorName);\n                     } else {\n                         if (deferred != null) {", "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/storage/KafkaConfigStorage.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/4c295a78446be6eba24ca4a9b7e506657e55c875", "parent": "https://github.com/apache/kafka/commit/17cb4fe52f74fcc1b7c43baa4649e0a4aba80fa3", "message": "KAFKA-4269: Update topic subscription when regex pattern specified out of topicGroups method\n\n\u2026d out of topicGroups method. The topicGroups method only called from StreamPartitionAssignor when KafkaStreams object  is the leader, needs to be executed for clients.\n\nAuthor: bbejeck <bbejeck@gmail.com>\n\nReviewers: Damian Guy <damian.guy@gmail.com>, Guozhang Wang <wangguoz@gmail.com>\n\nCloses #2005 from bbejeck/KAFKA-4269_multiple_kstream_instances_mult_consumers_npe", "bug_id": "kafka_66", "file": [{"additions": 18, "raw_url": "https://github.com/apache/kafka/raw/4c295a78446be6eba24ca4a9b7e506657e55c875/streams/src/main/java/org/apache/kafka/streams/processor/TopologyBuilder.java", "blob_url": "https://github.com/apache/kafka/blob/4c295a78446be6eba24ca4a9b7e506657e55c875/streams/src/main/java/org/apache/kafka/streams/processor/TopologyBuilder.java", "sha": "81f1f63078fcdae68b1d729fed253b3eab49cb2b", "changes": 27, "status": "modified", "deletions": 9, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/TopologyBuilder.java?ref=4c295a78446be6eba24ca4a9b7e506657e55c875", "patch": "@@ -30,6 +30,8 @@\n import org.apache.kafka.streams.processor.internals.SourceNode;\n import org.apache.kafka.streams.processor.internals.StreamPartitionAssignor.SubscriptionUpdates;\n import org.apache.kafka.streams.state.internals.RocksDBWindowStoreSupplier;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n import java.util.ArrayList;\n import java.util.Arrays;\n@@ -101,6 +103,8 @@\n \n     private Map<Integer, Set<String>> nodeGroups = null;\n \n+    private static final Logger log = LoggerFactory.getLogger(TopologyBuilder.class);\n+\n     private static class StateStoreFactory {\n         public final Set<String> users;\n \n@@ -831,14 +835,6 @@ private ProcessorTopology build(Set<String> nodeGroup) {\n     public synchronized Map<Integer, TopicsInfo> topicGroups() {\n         Map<Integer, TopicsInfo> topicGroups = new LinkedHashMap<>();\n \n-        if (subscriptionUpdates.hasUpdates()) {\n-            for (Map.Entry<String, Pattern> stringPatternEntry : nodeToSourcePatterns.entrySet()) {\n-                SourceNodeFactory sourceNode = (SourceNodeFactory) nodeFactories.get(stringPatternEntry.getKey());\n-                //need to update nodeToSourceTopics with topics matched from given regex\n-                nodeToSourceTopics.put(stringPatternEntry.getKey(), sourceNode.getTopics(subscriptionUpdates.getUpdates()));\n-            }\n-        }\n-\n         if (nodeGroups == null)\n             nodeGroups = makeNodeGroups();\n \n@@ -897,6 +893,17 @@ private ProcessorTopology build(Set<String> nodeGroup) {\n         return Collections.unmodifiableMap(topicGroups);\n     }\n \n+    private void setRegexMatchedTopicsToSourceNodes() {\n+        if (subscriptionUpdates.hasUpdates()) {\n+            for (Map.Entry<String, Pattern> stringPatternEntry : nodeToSourcePatterns.entrySet()) {\n+                SourceNodeFactory sourceNode = (SourceNodeFactory) nodeFactories.get(stringPatternEntry.getKey());\n+                //need to update nodeToSourceTopics with topics matched from given regex\n+                nodeToSourceTopics.put(stringPatternEntry.getKey(), sourceNode.getTopics(subscriptionUpdates.getUpdates()));\n+                log.debug(\"nodeToSourceTopics {}\", nodeToSourceTopics);\n+            }\n+        }\n+    }\n+\n     private InternalTopicConfig createInternalTopicConfig(final StateStoreSupplier supplier, final String name) {\n         if (!(supplier instanceof RocksDBWindowStoreSupplier)) {\n             return new InternalTopicConfig(name, Collections.singleton(InternalTopicConfig.CleanupPolicy.compact), supplier.logConfig());\n@@ -999,7 +1006,9 @@ public synchronized Pattern sourceTopicPattern() {\n         return this.topicPattern;\n     }\n \n-    public synchronized void updateSubscriptions(SubscriptionUpdates subscriptionUpdates) {\n+    public synchronized void updateSubscriptions(SubscriptionUpdates subscriptionUpdates, String threadId) {\n+        log.debug(\"stream-thread [{}] updating builder with {} topic(s) with possible matching regex subscription(s)\", threadId, subscriptionUpdates);\n         this.subscriptionUpdates = subscriptionUpdates;\n+        setRegexMatchedTopicsToSourceNodes();\n     }\n }", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/TopologyBuilder.java"}, {"additions": 8, "raw_url": "https://github.com/apache/kafka/raw/4c295a78446be6eba24ca4a9b7e506657e55c875/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java", "blob_url": "https://github.com/apache/kafka/blob/4c295a78446be6eba24ca4a9b7e506657e55c875/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java", "sha": "dcba5437bf52c725a1be7d64e5e4d4f7706b5a74", "changes": 10, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java?ref=4c295a78446be6eba24ca4a9b7e506657e55c875", "patch": "@@ -178,10 +178,10 @@ public Subscription subscription(Set<String> topics) {\n \n         if (streamThread.builder.sourceTopicPattern() != null) {\n             SubscriptionUpdates subscriptionUpdates = new SubscriptionUpdates();\n-            log.debug(\"have {} topics matching regex\", topics);\n+            log.debug(\"stream-thread [{}] found {} topics possibly matching regex\", streamThread.getName(), topics);\n             // update the topic groups with the returned subscription set for regex pattern subscriptions\n             subscriptionUpdates.updateTopics(topics);\n-            streamThread.builder.updateSubscriptions(subscriptionUpdates);\n+            streamThread.builder.updateSubscriptions(subscriptionUpdates, streamThread.getName());\n         }\n \n         return new Subscription(new ArrayList<>(topics), data.encode());\n@@ -669,6 +669,12 @@ public boolean hasUpdates() {\n             return !updatedTopicSubscriptions.isEmpty();\n         }\n \n+        @Override\n+        public String toString() {\n+            return \"SubscriptionUpdates{\" +\n+                    \"updatedTopicSubscriptions=\" + updatedTopicSubscriptions +\n+                    '}';\n+        }\n     }\n \n }", "filename": "streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamPartitionAssignor.java"}, {"additions": 31, "raw_url": "https://github.com/apache/kafka/raw/4c295a78446be6eba24ca4a9b7e506657e55c875/streams/src/test/java/org/apache/kafka/streams/processor/TopologyBuilderTest.java", "blob_url": "https://github.com/apache/kafka/blob/4c295a78446be6eba24ca4a9b7e506657e55c875/streams/src/test/java/org/apache/kafka/streams/processor/TopologyBuilderTest.java", "sha": "d2609377933ea92f12db257e0ccd14d6f2b8af41", "changes": 31, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/streams/src/test/java/org/apache/kafka/streams/processor/TopologyBuilderTest.java?ref=4c295a78446be6eba24ca4a9b7e506657e55c875", "patch": "@@ -27,12 +27,14 @@\n import org.apache.kafka.streams.processor.internals.ProcessorStateManager;\n import org.apache.kafka.streams.processor.internals.ProcessorTopology;\n import org.apache.kafka.streams.state.Stores;\n+import org.apache.kafka.streams.processor.internals.StreamPartitionAssignor;\n import org.apache.kafka.streams.state.internals.RocksDBWindowStoreSupplier;\n import org.apache.kafka.test.MockProcessorSupplier;\n import org.apache.kafka.test.MockStateStoreSupplier;\n import org.apache.kafka.test.ProcessorTopologyTestDriver;\n import org.junit.Test;\n \n+import java.lang.reflect.Field;\n import java.util.Arrays;\n import java.util.Collection;\n import java.util.Collections;\n@@ -521,6 +523,7 @@ public void shouldAddInternalTopicConfigWithCleanupPolicyDeleteForInternalTopics\n         assertEquals(1, properties.size());\n     }\n \n+\n     @Test(expected = TopologyBuilderException.class)\n     public void shouldThroughOnUnassignedStateStoreAccess() {\n         final String sourceNodeName = \"source\";\n@@ -583,4 +586,32 @@ public void close() {\n         }\n     }\n \n+    @SuppressWarnings(\"unchecked\")\n+    @Test\n+    public void shouldSetCorrectSourceNodesWithRegexUpdatedTopics() throws Exception {\n+        final TopologyBuilder builder = new TopologyBuilder();\n+        builder.addSource(\"source-1\", \"topic-foo\");\n+        builder.addSource(\"source-2\", Pattern.compile(\"topic-[A-C]\"));\n+        builder.addSource(\"source-3\", Pattern.compile(\"topic-\\\\d\"));\n+\n+        StreamPartitionAssignor.SubscriptionUpdates subscriptionUpdates = new StreamPartitionAssignor.SubscriptionUpdates();\n+        Field updatedTopicsField  = subscriptionUpdates.getClass().getDeclaredField(\"updatedTopicSubscriptions\");\n+        updatedTopicsField.setAccessible(true);\n+\n+        Set<String> updatedTopics = (Set<String>) updatedTopicsField.get(subscriptionUpdates);\n+\n+        updatedTopics.add(\"topic-B\");\n+        updatedTopics.add(\"topic-3\");\n+        updatedTopics.add(\"topic-A\");\n+\n+        builder.updateSubscriptions(subscriptionUpdates, null);\n+        builder.setApplicationId(\"test-id\");\n+\n+        Map<Integer, TopicsInfo> topicGroups = builder.topicGroups();\n+        assertTrue(topicGroups.get(0).sourceTopics.contains(\"topic-foo\"));\n+        assertTrue(topicGroups.get(1).sourceTopics.contains(\"topic-A\"));\n+        assertTrue(topicGroups.get(1).sourceTopics.contains(\"topic-B\"));\n+        assertTrue(topicGroups.get(2).sourceTopics.contains(\"topic-3\"));\n+\n+    }\n }", "filename": "streams/src/test/java/org/apache/kafka/streams/processor/TopologyBuilderTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/12a3ccafa1cbc333539b98d83d7d6d572700f05b", "parent": "https://github.com/apache/kafka/commit/60735b7ad6ef15f25ae9e432c2fc566bec535e31", "message": "KafkaController NPE in SessionExpireListener; patched by Yang Ye; reviewed by Jun Rao, Neha Narkhede; KAFKA-464\n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/kafka/branches/0.8@1374467 13f79535-47bb-0310-9956-ffa450edef68", "bug_id": "kafka_67", "file": [{"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/12a3ccafa1cbc333539b98d83d7d6d572700f05b/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala", "blob_url": "https://github.com/apache/kafka/blob/12a3ccafa1cbc333539b98d83d7d6d572700f05b/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala", "sha": "6d38fc5011382db7234b501d87367173d04b16bf", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala?ref=12a3ccafa1cbc333539b98d83d7d6d572700f05b", "patch": "@@ -120,7 +120,7 @@ private[kafka] class ZookeeperConsumerConnector(val config: ConsumerConfig,\n   connectZk()\n   createFetcher()\n   if (config.autoCommit) {\n-    scheduler.startUp\n+    scheduler.startup\n     info(\"starting auto committer every \" + config.autoCommitIntervalMs + \" ms\")\n     scheduler.scheduleWithRate(autoCommit, \"Kafka-consumer-autocommit-\", config.autoCommitIntervalMs,\n       config.autoCommitIntervalMs, false)", "filename": "core/src/main/scala/kafka/consumer/ZookeeperConsumerConnector.scala"}, {"additions": 25, "raw_url": "https://github.com/apache/kafka/raw/12a3ccafa1cbc333539b98d83d7d6d572700f05b/core/src/main/scala/kafka/server/KafkaController.scala", "blob_url": "https://github.com/apache/kafka/blob/12a3ccafa1cbc333539b98d83d7d6d572700f05b/core/src/main/scala/kafka/server/KafkaController.scala", "sha": "68f5dd2ee1ea0e29e59349660d4e8b20fa40a05d", "changes": 46, "status": "modified", "deletions": 21, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/KafkaController.scala?ref=12a3ccafa1cbc333539b98d83d7d6d572700f05b", "patch": "@@ -38,14 +38,14 @@ class RequestSendThread(val controllerId: Int,\n         extends Thread(\"requestSendThread-\" + toBrokerId) with Logging {\n   this.logIdent = \"Controller %d, request send thread to broker %d, \".format(controllerId, toBrokerId)\n   val isRunning: AtomicBoolean = new AtomicBoolean(true)\n-  private val shutDownLatch = new CountDownLatch(1)\n+  private val shutdownLatch = new CountDownLatch(1)\n   private val lock = new Object()\n \n-  def shutDown(): Unit = {\n+  def shutdown(): Unit = {\n     info(\"shutting down\")\n     isRunning.set(false)\n     interrupt()\n-    shutDownLatch.await()\n+    shutdownLatch.await()\n     info(\"shutted down completed\")\n   }\n \n@@ -84,7 +84,7 @@ class RequestSendThread(val controllerId: Int,\n       case e: InterruptedException => warn(\"intterrupted. Shutting down\")\n       case e1 => error(\"Error due to \", e1)\n     }\n-    shutDownLatch.countDown()\n+    shutdownLatch.countDown()\n   }\n }\n \n@@ -107,7 +107,7 @@ class ControllerChannelManager(allBrokers: Set[Broker], config : KafkaConfig) ex\n     messageQueues.put(broker.id, new LinkedBlockingQueue[(RequestOrResponse, (RequestOrResponse) => Unit)](config.controllerMessageQueueSize))\n   }\n \n-  def startUp() = {\n+  def startup() = {\n     for((brokerId, broker) <- brokers){\n       val thread = new RequestSendThread(config.brokerId, brokerId, messageQueues(brokerId), messageChannels(brokerId))\n       thread.setDaemon(false)\n@@ -116,7 +116,7 @@ class ControllerChannelManager(allBrokers: Set[Broker], config : KafkaConfig) ex\n     }\n   }\n \n-  def shutDown() = {\n+  def shutdown() = {\n     lock synchronized {\n       for((brokerId, broker) <- brokers){\n         removeBroker(brokerId)\n@@ -152,7 +152,7 @@ class ControllerChannelManager(allBrokers: Set[Broker], config : KafkaConfig) ex\n         messageChannels(brokerId).disconnect()\n         messageChannels.remove(brokerId)\n         messageQueues.remove(brokerId)\n-        messageThreads(brokerId).shutDown()\n+        messageThreads(brokerId).shutdown()\n         messageThreads.remove(brokerId)\n       }catch {\n         case e => error(\"Error while removing broker by the controller\", e)\n@@ -163,7 +163,8 @@ class ControllerChannelManager(allBrokers: Set[Broker], config : KafkaConfig) ex\n \n class KafkaController(config : KafkaConfig, zkClient: ZkClient) extends Logging {\n   this.logIdent = \"Controller \" + config.brokerId + \", \"\n-  info(\"startup\");\n+  info(\"startup\")\n+  private var isRunning = true\n   private val controllerLock = new Object\n   private var controllerChannelManager: ControllerChannelManager = null\n   private var allBrokers : Set[Broker] = null\n@@ -189,7 +190,7 @@ class KafkaController(config : KafkaConfig, zkClient: ZkClient) extends Logging\n       info(\"allPartitionReplicaAssignment: %s\".format(allPartitionReplicaAssignment))\n       allLeaders = new mutable.HashMap[(String, Int), Int]\n       controllerChannelManager = new ControllerChannelManager(allBrokers, config)\n-      controllerChannelManager.startUp()\n+      controllerChannelManager.startup()\n       return true\n     } catch {\n       case e: ZkNodeExistsException =>\n@@ -201,6 +202,10 @@ class KafkaController(config : KafkaConfig, zkClient: ZkClient) extends Logging\n   }\n \n   private def controllerRegisterOrFailover(){\n+    if(!isRunning){\n+      info(\"controller has already been shut down, don't need to compete for lead controller any more\")\n+      return\n+    }\n     info(\"try to become controller\")\n     if(tryToBecomeController() == true){\n       info(\"won the controller competition and work on leader and isr recovery\")\n@@ -209,12 +214,7 @@ class KafkaController(config : KafkaConfig, zkClient: ZkClient) extends Logging\n       onBrokerChange()\n \n       // If there are some partition with leader not initialized, init the leader for them\n-      val partitionReplicaAssignment = allPartitionReplicaAssignment.clone()\n-      for((topicPartition, replicas) <- partitionReplicaAssignment){\n-        if (allLeaders.contains(topicPartition)){\n-          partitionReplicaAssignment.remove(topicPartition)\n-        }\n-      }\n+      val partitionReplicaAssignment = allPartitionReplicaAssignment.filter(m => !allLeaders.contains(m._1))\n       debug(\"work on init leaders: %s, current cache for all leader is: %s\".format(partitionReplicaAssignment.toString(), allLeaders))\n       initLeaders(partitionReplicaAssignment)\n     }\n@@ -228,18 +228,20 @@ class KafkaController(config : KafkaConfig, zkClient: ZkClient) extends Logging\n     controllerLock synchronized {\n       registerSessionExpirationListener()\n       registerControllerExistListener()\n+      isRunning = true\n       controllerRegisterOrFailover()\n     }\n   }\n \n-  def shutDown() = {\n+  def shutdown() = {\n     controllerLock synchronized {\n       if(controllerChannelManager != null){\n         info(\"shut down\")\n-        controllerChannelManager.shutDown()\n+        controllerChannelManager.shutdown()\n         controllerChannelManager = null\n         info(\"shutted down completely\")\n       }\n+      isRunning = false\n     }\n   }\n \n@@ -280,11 +282,13 @@ class KafkaController(config : KafkaConfig, zkClient: ZkClient) extends Logging\n     @throws(classOf[Exception])\n     def handleNewSession() {\n       controllerLock synchronized {\n-        info(\"session expires, clean up the state\")\n-        controllerChannelManager.shutDown()\n-        controllerChannelManager = null\n-        controllerRegisterOrFailover()\n+        if(controllerChannelManager != null){\n+          info(\"session expires, clean up the state\")\n+          controllerChannelManager.shutdown()\n+          controllerChannelManager = null\n+        }\n       }\n+      controllerRegisterOrFailover()\n     }\n   }\n ", "filename": "core/src/main/scala/kafka/server/KafkaController.scala"}, {"additions": 2, "raw_url": "https://github.com/apache/kafka/raw/12a3ccafa1cbc333539b98d83d7d6d572700f05b/core/src/main/scala/kafka/server/KafkaServer.scala", "blob_url": "https://github.com/apache/kafka/blob/12a3ccafa1cbc333539b98d83d7d6d572700f05b/core/src/main/scala/kafka/server/KafkaServer.scala", "sha": "3038399270f26f779edf7556ef81ae010794393d", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/KafkaServer.scala?ref=12a3ccafa1cbc333539b98d83d7d6d572700f05b", "patch": "@@ -65,7 +65,7 @@ class KafkaServer(val config: KafkaConfig, time: Time = SystemTime) extends Logg\n     }\n \n     /* start scheduler */\n-    kafkaScheduler.startUp\n+    kafkaScheduler.startup\n \n     /* start log manager */\n     logManager = new LogManager(config,\n@@ -132,7 +132,7 @@ class KafkaServer(val config: KafkaConfig, time: Time = SystemTime) extends Logg\n         logManager.shutdown()\n \n       if(kafkaController != null)\n-        kafkaController.shutDown()\n+        kafkaController.shutdown()\n \n       val cleanShutDownFile = new File(new File(config.logDir), CleanShutdownFile)\n       debug(\"creating clean shutdown file \" + cleanShutDownFile.getAbsolutePath())", "filename": "core/src/main/scala/kafka/server/KafkaServer.scala"}, {"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/12a3ccafa1cbc333539b98d83d7d6d572700f05b/core/src/main/scala/kafka/utils/KafkaScheduler.scala", "blob_url": "https://github.com/apache/kafka/blob/12a3ccafa1cbc333539b98d83d7d6d572700f05b/core/src/main/scala/kafka/utils/KafkaScheduler.scala", "sha": "4a70e8173eab05ab7242907a60c300b11385e71a", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/utils/KafkaScheduler.scala?ref=12a3ccafa1cbc333539b98d83d7d6d572700f05b", "patch": "@@ -34,7 +34,7 @@ class KafkaScheduler(val numThreads: Int) extends Logging {\n     }\n   private val threadNamesAndIds = new HashMap[String, AtomicInteger]()\n \n-  def startUp = {\n+  def startup = {\n     executor = new ScheduledThreadPoolExecutor(numThreads)\n     executor.setContinueExistingPeriodicTasksAfterShutdownPolicy(false)\n     executor.setExecuteExistingDelayedTasksAfterShutdownPolicy(false)", "filename": "core/src/main/scala/kafka/utils/KafkaScheduler.scala"}, {"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/12a3ccafa1cbc333539b98d83d7d6d572700f05b/core/src/test/scala/unit/kafka/log/LogManagerTest.scala", "blob_url": "https://github.com/apache/kafka/blob/12a3ccafa1cbc333539b98d83d7d6d572700f05b/core/src/test/scala/unit/kafka/log/LogManagerTest.scala", "sha": "1fd176b028422e0c29f6f8b1615e8d495b13a58f", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/log/LogManagerTest.scala?ref=12a3ccafa1cbc333539b98d83d7d6d572700f05b", "patch": "@@ -46,7 +46,7 @@ class LogManagerTest extends JUnit3Suite with ZooKeeperTestHarness {\n                    override val logFileSize = 1024\n                    override val flushInterval = 100\n                  }\n-    scheduler.startUp\n+    scheduler.startup\n     logManager = new LogManager(config, scheduler, time, veryLargeLogFlushInterval, maxLogAge, false)\n     logManager.startup\n     logDir = logManager.logDir", "filename": "core/src/test/scala/unit/kafka/log/LogManagerTest.scala"}, {"additions": 2, "raw_url": "https://github.com/apache/kafka/raw/12a3ccafa1cbc333539b98d83d7d6d572700f05b/core/src/test/scala/unit/kafka/server/HighwatermarkPersistenceTest.scala", "blob_url": "https://github.com/apache/kafka/blob/12a3ccafa1cbc333539b98d83d7d6d572700f05b/core/src/test/scala/unit/kafka/server/HighwatermarkPersistenceTest.scala", "sha": "2b9f86a11e435e9d8a664b1cba87a2d11f91449d", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/server/HighwatermarkPersistenceTest.scala?ref=12a3ccafa1cbc333539b98d83d7d6d572700f05b", "patch": "@@ -37,7 +37,7 @@ class HighwatermarkPersistenceTest extends JUnit3Suite {\n     EasyMock.replay(zkClient)\n     // create kafka scheduler\n     val scheduler = new KafkaScheduler(2)\n-    scheduler.startUp\n+    scheduler.startup\n     // create replica manager\n     val replicaManager = new ReplicaManager(configs.head, new MockTime(), zkClient, scheduler, null)\n     replicaManager.startup()\n@@ -80,7 +80,7 @@ class HighwatermarkPersistenceTest extends JUnit3Suite {\n     EasyMock.replay(zkClient)\n     // create kafka scheduler\n     val scheduler = new KafkaScheduler(2)\n-    scheduler.startUp\n+    scheduler.startup\n     // create replica manager\n     val replicaManager = new ReplicaManager(configs.head, new MockTime(), zkClient, scheduler, null)\n     replicaManager.startup()", "filename": "core/src/test/scala/unit/kafka/server/HighwatermarkPersistenceTest.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/8429db937e2134d9935d9dccd2ed0febc474fd66", "parent": "https://github.com/apache/kafka/commit/7f4e3ccde820eedd962b4cfd3abaecd8a49b83a8", "message": "KAFKA-3661; fix NPE in o.a.k.c.c.RoundRobinAssignor when topic metadata not found\n\nAbstractPartitionAssignor.assign has an ambiguous line in its documentation:\n> param partitionsPerTopic The number of partitions for each subscribed topic (may be empty for some topics)\n\nDoes empty mean the topic has an entry with value zero, or that the entry is excluded from the map altogether? The current implementation in AbstractPartitionAssignor excludes the entry from partitionsPerTopic if the topic isn't in the metadata.\n\nRoundRobinAssignorTest.testOneConsumerNonexistentTopic interprets emptiness as providing the topic with a zero value.\nRangeAssignor interprets emptiness as excluding the entry from the map.\nRangeAssignorTest.testOneConsumerNonexistentTopic interprets emptiness as providing the topic with a zero value.\n\nThis implementation chooses to solve the NPE by deciding to exclude topics from partitionsPerTopic when the topic is not in the metadata.\n\nAuthor: Onur Karaman <okaraman@linkedin.com>\n\nReviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>\n\nCloses #1326 from onurkaraman/KAFKA-3661", "bug_id": "kafka_68", "file": [{"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/main/java/org/apache/kafka/clients/consumer/RangeAssignor.java", "blob_url": "https://github.com/apache/kafka/blob/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/main/java/org/apache/kafka/clients/consumer/RangeAssignor.java", "sha": "16c1d77c429a7147456d0bb572414fd85f7b8390", "changes": 10, "status": "modified", "deletions": 9, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/consumer/RangeAssignor.java?ref=8429db937e2134d9935d9dccd2ed0febc474fd66", "patch": "@@ -45,14 +45,6 @@ public String name() {\n         return \"range\";\n     }\n \n-    private List<TopicPartition> partitions(String topic,\n-                                            int numPartitions) {\n-        List<TopicPartition> partitions = new ArrayList<>();\n-        for (int i = 0; i < numPartitions; i++)\n-            partitions.add(new TopicPartition(topic, i));\n-        return partitions;\n-    }\n-\n     private Map<String, List<String>> consumersPerTopic(Map<String, List<String>> consumerMetadata) {\n         Map<String, List<String>> res = new HashMap<>();\n         for (Map.Entry<String, List<String>> subscriptionEntry : consumerMetadata.entrySet()) {\n@@ -84,7 +76,7 @@ public String name() {\n             int numPartitionsPerConsumer = numPartitionsForTopic / consumersForTopic.size();\n             int consumersWithExtraPartition = numPartitionsForTopic % consumersForTopic.size();\n \n-            List<TopicPartition> partitions = partitions(topic, numPartitionsForTopic);\n+            List<TopicPartition> partitions = AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic);\n             for (int i = 0, n = consumersForTopic.size(); i < n; i++) {\n                 int start = numPartitionsPerConsumer * i + Math.min(i, consumersWithExtraPartition);\n                 int length = numPartitionsPerConsumer + (i + 1 > consumersWithExtraPartition ? 0 : 1);", "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/RangeAssignor.java"}, {"additions": 3, "raw_url": "https://github.com/apache/kafka/raw/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/main/java/org/apache/kafka/clients/consumer/RoundRobinAssignor.java", "blob_url": "https://github.com/apache/kafka/blob/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/main/java/org/apache/kafka/clients/consumer/RoundRobinAssignor.java", "sha": "a5de595cd361c5b6a48f7dc1f817b4d288d487d3", "changes": 7, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/consumer/RoundRobinAssignor.java?ref=8429db937e2134d9935d9dccd2ed0febc474fd66", "patch": "@@ -65,10 +65,9 @@\n \n         List<TopicPartition> allPartitions = new ArrayList<>();\n         for (String topic : topics) {\n-            Integer partitions = partitionsPerTopic.get(topic);\n-            for (int partition = 0; partition < partitions; partition++) {\n-                allPartitions.add(new TopicPartition(topic, partition));\n-            }\n+            Integer numPartitionsForTopic = partitionsPerTopic.get(topic);\n+            if (numPartitionsForTopic != null)\n+                allPartitions.addAll(AbstractPartitionAssignor.partitions(topic, numPartitionsForTopic));\n         }\n         return allPartitions;\n     }", "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/RoundRobinAssignor.java"}, {"additions": 10, "raw_url": "https://github.com/apache/kafka/raw/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractPartitionAssignor.java", "blob_url": "https://github.com/apache/kafka/blob/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractPartitionAssignor.java", "sha": "4f90e66f2794ca9e7ab84e9a1dc523789ead4f40", "changes": 13, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractPartitionAssignor.java?ref=8429db937e2134d9935d9dccd2ed0febc474fd66", "patch": "@@ -33,9 +33,10 @@\n \n     /**\n      * Perform the group assignment given the partition counts and member subscriptions\n-     * @param partitionsPerTopic The number of partitions for each subscribed topic (may be empty for some topics)\n+     * @param partitionsPerTopic The number of partitions for each subscribed topic. Topics not in metadata will be excluded\n+     *                           from this map.\n      * @param subscriptions Map from the memberId to their respective topic subscription\n-     * @return Map from each member to the\n+     * @return Map from each member to the list of partitions assigned to them.\n      */\n     public abstract Map<String, List<TopicPartition>> assign(Map<String, Integer> partitionsPerTopic,\n                                                              Map<String, List<String>> subscriptions);\n@@ -58,7 +59,7 @@ public Subscription subscription(Set<String> topics) {\n         Map<String, Integer> partitionsPerTopic = new HashMap<>();\n         for (String topic : allSubscribedTopics) {\n             Integer numPartitions = metadata.partitionCountForTopic(topic);\n-            if (numPartitions != null)\n+            if (numPartitions != null && numPartitions > 0)\n                 partitionsPerTopic.put(topic, numPartitions);\n             else\n                 log.debug(\"Skipping assignment for topic {} since no metadata is available\", topic);\n@@ -87,4 +88,10 @@ public void onAssignment(Assignment assignment) {\n         list.add(value);\n     }\n \n+    protected static List<TopicPartition> partitions(String topic, int numPartitions) {\n+        List<TopicPartition> partitions = new ArrayList<>(numPartitions);\n+        for (int i = 0; i < numPartitions; i++)\n+            partitions.add(new TopicPartition(topic, i));\n+        return partitions;\n+    }\n }", "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractPartitionAssignor.java"}, {"additions": 0, "raw_url": "https://github.com/apache/kafka/raw/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/test/java/org/apache/kafka/clients/consumer/RangeAssignorTest.java", "blob_url": "https://github.com/apache/kafka/blob/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/test/java/org/apache/kafka/clients/consumer/RangeAssignorTest.java", "sha": "72febb02ca6efad00d7fe5fc9d8d49638af540d1", "changes": 2, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/consumer/RangeAssignorTest.java?ref=8429db937e2134d9935d9dccd2ed0febc474fd66", "patch": "@@ -53,8 +53,6 @@ public void testOneConsumerNonexistentTopic() {\n         String consumerId = \"consumer\";\n \n         Map<String, Integer> partitionsPerTopic = new HashMap<>();\n-        partitionsPerTopic.put(topic, 0);\n-\n         Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic,\n                 Collections.singletonMap(consumerId, Arrays.asList(topic)));\n         assertEquals(Collections.singleton(consumerId), assignment.keySet());", "filename": "clients/src/test/java/org/apache/kafka/clients/consumer/RangeAssignorTest.java"}, {"additions": 0, "raw_url": "https://github.com/apache/kafka/raw/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/test/java/org/apache/kafka/clients/consumer/RoundRobinAssignorTest.java", "blob_url": "https://github.com/apache/kafka/blob/8429db937e2134d9935d9dccd2ed0febc474fd66/clients/src/test/java/org/apache/kafka/clients/consumer/RoundRobinAssignorTest.java", "sha": "1d62700e5cbe1d43f4c8dcb4070ca42a8f8f8bc5", "changes": 2, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/consumer/RoundRobinAssignorTest.java?ref=8429db937e2134d9935d9dccd2ed0febc474fd66", "patch": "@@ -47,8 +47,6 @@ public void testOneConsumerNonexistentTopic() {\n         String consumerId = \"consumer\";\n \n         Map<String, Integer> partitionsPerTopic = new HashMap<>();\n-        partitionsPerTopic.put(topic, 0);\n-\n         Map<String, List<TopicPartition>> assignment = assignor.assign(partitionsPerTopic,\n                 Collections.singletonMap(consumerId, Arrays.asList(topic)));\n ", "filename": "clients/src/test/java/org/apache/kafka/clients/consumer/RoundRobinAssignorTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/c439268224e3178002bfa28bc048722870f992e3", "parent": "https://github.com/apache/kafka/commit/383cec9cf38607cdfbda0256d3447d253dcdde7d", "message": "KAFKA-3562; Handle topic deletion during a send\n\nFix timing window in producer by holding onto cluster object while processing send requests so that changes to cluster during metadata refresh don't cause NPE if a topic is deleted.\n\nAuthor: Rajini Sivaram <rajinisivaram@googlemail.com>\n\nReviewers: Sriharsha Chintalapani <harsha@hortonworks.com>, Ewen Cheslack-Postava <ewen@confluent.io>, Ismael Juma <ismael@juma.me.uk>\n\nCloses #1478 from rajinisivaram/KAFKA-3562", "bug_id": "kafka_69", "file": [{"additions": 3, "raw_url": "https://github.com/apache/kafka/raw/c439268224e3178002bfa28bc048722870f992e3/build.gradle", "blob_url": "https://github.com/apache/kafka/blob/c439268224e3178002bfa28bc048722870f992e3/build.gradle", "sha": "b3be020cbd7117b163ab920cdf6a5002d534fafe", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/build.gradle?ref=c439268224e3178002bfa28bc048722870f992e3", "patch": "@@ -576,6 +576,9 @@ project(':clients') {\n \n     testCompile libs.bcpkix\n     testCompile libs.junit\n+    testCompile libs.easymock\n+    testCompile libs.powermock\n+    testCompile libs.powermockEasymock\n \n     testRuntime libs.slf4jlog4j\n   }", "filename": "build.gradle"}, {"additions": 28, "raw_url": "https://github.com/apache/kafka/raw/c439268224e3178002bfa28bc048722870f992e3/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java", "blob_url": "https://github.com/apache/kafka/blob/c439268224e3178002bfa28bc048722870f992e3/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java", "sha": "05d937754aad8f150809e4db69c71b291aff2b45", "changes": 41, "status": "modified", "deletions": 13, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java?ref=c439268224e3178002bfa28bc048722870f992e3", "patch": "@@ -437,8 +437,9 @@ private static int parseAcks(String acksString) {\n         TopicPartition tp = null;\n         try {\n             // first make sure the metadata for the topic is available\n-            long waitedOnMetadataMs = waitOnMetadata(record.topic(), this.maxBlockTimeMs);\n-            long remainingWaitMs = Math.max(0, this.maxBlockTimeMs - waitedOnMetadataMs);\n+            ClusterAndWaitTime clusterAndWaitTime = waitOnMetadata(record.topic(), this.maxBlockTimeMs);\n+            long remainingWaitMs = Math.max(0, this.maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);\n+            Cluster cluster = clusterAndWaitTime.cluster;\n             byte[] serializedKey;\n             try {\n                 serializedKey = keySerializer.serialize(record.topic(), record.key());\n@@ -455,7 +456,8 @@ private static int parseAcks(String acksString) {\n                         \" to class \" + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +\n                         \" specified in value.serializer\");\n             }\n-            int partition = partition(record, serializedKey, serializedValue, metadata.fetch());\n+\n+            int partition = partition(record, serializedKey, serializedValue, cluster);\n             int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue);\n             ensureValidRecordSize(serializedSize);\n             tp = new TopicPartition(record.topic(), partition);\n@@ -508,17 +510,19 @@ private static int parseAcks(String acksString) {\n      * Wait for cluster metadata including partitions for the given topic to be available.\n      * @param topic The topic we want metadata for\n      * @param maxWaitMs The maximum time in ms for waiting on the metadata\n-     * @return The amount of time we waited in ms\n+     * @return The cluster containing topic metadata and the amount of time we waited in ms\n      */\n-    private long waitOnMetadata(String topic, long maxWaitMs) throws InterruptedException {\n+    private ClusterAndWaitTime waitOnMetadata(String topic, long maxWaitMs) throws InterruptedException {\n         // add topic to metadata topic list if it is not there already and reset expiry\n         this.metadata.add(topic);\n-        if (metadata.fetch().partitionsForTopic(topic) != null)\n-            return 0;\n+        Cluster cluster = metadata.fetch();\n+        if (cluster.partitionsForTopic(topic) != null)\n+            return new ClusterAndWaitTime(cluster, 0);\n \n         long begin = time.milliseconds();\n         long remainingWaitMs = maxWaitMs;\n-        while (metadata.fetch().partitionsForTopic(topic) == null) {\n+        long elapsed = 0;\n+        while (cluster.partitionsForTopic(topic) == null) {\n             log.trace(\"Requesting metadata update for topic {}.\", topic);\n             int version = metadata.requestUpdate();\n             sender.wakeup();\n@@ -528,14 +532,15 @@ private long waitOnMetadata(String topic, long maxWaitMs) throws InterruptedExce\n                 // Rethrow with original maxWaitMs to prevent logging exception with remainingWaitMs\n                 throw new TimeoutException(\"Failed to update metadata after \" + maxWaitMs + \" ms.\");\n             }\n-            long elapsed = time.milliseconds() - begin;\n+            cluster = metadata.fetch();\n+            elapsed = time.milliseconds() - begin;\n             if (elapsed >= maxWaitMs)\n                 throw new TimeoutException(\"Failed to update metadata after \" + maxWaitMs + \" ms.\");\n-            if (metadata.fetch().unauthorizedTopics().contains(topic))\n+            if (cluster.unauthorizedTopics().contains(topic))\n                 throw new TopicAuthorizationException(topic);\n             remainingWaitMs = maxWaitMs - elapsed;\n         }\n-        return time.milliseconds() - begin;\n+        return new ClusterAndWaitTime(cluster, elapsed);\n     }\n \n     /**\n@@ -600,12 +605,13 @@ public void flush() {\n      */\n     @Override\n     public List<PartitionInfo> partitionsFor(String topic) {\n+        Cluster cluster;\n         try {\n-            waitOnMetadata(topic, this.maxBlockTimeMs);\n+            cluster = waitOnMetadata(topic, this.maxBlockTimeMs).cluster;\n         } catch (InterruptedException e) {\n             throw new InterruptException(e);\n         }\n-        return this.metadata.fetch().partitionsForTopic(topic);\n+        return cluster.partitionsForTopic(topic);\n     }\n \n     /**\n@@ -724,6 +730,15 @@ private int partition(ProducerRecord<K, V> record, byte[] serializedKey , byte[]\n             cluster);\n     }\n \n+    private static class ClusterAndWaitTime {\n+        final Cluster cluster;\n+        final long waitedOnMetadataMs;\n+        ClusterAndWaitTime(Cluster cluster, long waitedOnMetadataMs) {\n+            this.cluster = cluster;\n+            this.waitedOnMetadataMs = waitedOnMetadataMs;\n+        }\n+    }\n+\n     private static class FutureFailure implements Future<RecordMetadata> {\n \n         private final ExecutionException exception;", "filename": "clients/src/main/java/org/apache/kafka/clients/producer/KafkaProducer.java"}, {"additions": 62, "raw_url": "https://github.com/apache/kafka/raw/c439268224e3178002bfa28bc048722870f992e3/clients/src/test/java/org/apache/kafka/clients/producer/KafkaProducerTest.java", "blob_url": "https://github.com/apache/kafka/blob/c439268224e3178002bfa28bc048722870f992e3/clients/src/test/java/org/apache/kafka/clients/producer/KafkaProducerTest.java", "sha": "1780e2f823fef915b85f0f89d3a88334472095e9", "changes": 62, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/producer/KafkaProducerTest.java?ref=c439268224e3178002bfa28bc048722870f992e3", "patch": "@@ -16,21 +16,37 @@\n  */\n package org.apache.kafka.clients.producer;\n \n+import org.apache.kafka.common.Cluster;\n import org.apache.kafka.common.KafkaException;\n+import org.apache.kafka.common.Node;\n+import org.apache.kafka.common.PartitionInfo;\n import org.apache.kafka.common.network.Selectable;\n import org.apache.kafka.common.serialization.ByteArraySerializer;\n import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.Metadata;\n import org.apache.kafka.common.serialization.StringSerializer;\n import org.apache.kafka.test.MockMetricsReporter;\n import org.apache.kafka.test.MockProducerInterceptor;\n import org.apache.kafka.test.MockSerializer;\n+import org.easymock.EasyMock;\n import org.junit.Assert;\n import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.powermock.api.easymock.PowerMock;\n+import org.powermock.api.support.membermodification.MemberModifier;\n+import org.powermock.core.classloader.annotations.PowerMockIgnore;\n+import org.powermock.core.classloader.annotations.PrepareOnlyThisForTest;\n+import org.powermock.modules.junit4.PowerMockRunner;\n \n import java.util.Properties;\n import java.util.Map;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Collections;\n import java.util.HashMap;\n \n+@RunWith(PowerMockRunner.class)\n+@PowerMockIgnore(\"javax.management.*\")\n public class KafkaProducerTest {\n \n     @Test\n@@ -123,4 +139,50 @@ public void testInvalidSocketReceiveBufferSize() throws Exception {\n         config.put(ProducerConfig.RECEIVE_BUFFER_CONFIG, -2);\n         new KafkaProducer<>(config, new ByteArraySerializer(), new ByteArraySerializer());\n     }\n+\n+    @PrepareOnlyThisForTest(Metadata.class)\n+    @Test\n+    public void testMetadataFetch() throws Exception {\n+        Properties props = new Properties();\n+        props.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9999\");\n+        KafkaProducer<String, String> producer = new KafkaProducer<>(props, new StringSerializer(), new StringSerializer());\n+        Metadata metadata = PowerMock.createNiceMock(Metadata.class);\n+        MemberModifier.field(KafkaProducer.class, \"metadata\").set(producer, metadata);\n+\n+        String topic = \"topic\";\n+        ProducerRecord<String, String> record = new ProducerRecord<>(topic, \"value\");\n+        Collection<Node> nodes = Collections.singletonList(new Node(0, \"host1\", 1000));\n+        final Cluster emptyCluster = new Cluster(nodes,\n+                Collections.<PartitionInfo>emptySet(),\n+                Collections.<String>emptySet());\n+        final Cluster cluster = new Cluster(\n+                Collections.singletonList(new Node(0, \"host1\", 1000)),\n+                Arrays.asList(new PartitionInfo(topic, 0, null, null, null)),\n+                Collections.<String>emptySet());\n+\n+        // Expect exactly one fetch for each attempt to refresh while topic metadata is not available\n+        final int refreshAttempts = 5;\n+        EasyMock.expect(metadata.fetch()).andReturn(emptyCluster).times(refreshAttempts - 1);\n+        EasyMock.expect(metadata.fetch()).andReturn(cluster).once();\n+        EasyMock.expect(metadata.fetch()).andThrow(new IllegalStateException(\"Unexpected call to metadata.fetch()\")).anyTimes();\n+        PowerMock.replay(metadata);\n+        producer.send(record);\n+        PowerMock.verify(metadata);\n+\n+        // Expect exactly one fetch if topic metadata is available\n+        PowerMock.reset(metadata);\n+        EasyMock.expect(metadata.fetch()).andReturn(cluster).once();\n+        EasyMock.expect(metadata.fetch()).andThrow(new IllegalStateException(\"Unexpected call to metadata.fetch()\")).anyTimes();\n+        PowerMock.replay(metadata);\n+        producer.send(record, null);\n+        PowerMock.verify(metadata);\n+\n+        // Expect exactly one fetch if topic metadata is available\n+        PowerMock.reset(metadata);\n+        EasyMock.expect(metadata.fetch()).andReturn(cluster).once();\n+        EasyMock.expect(metadata.fetch()).andThrow(new IllegalStateException(\"Unexpected call to metadata.fetch()\")).anyTimes();\n+        PowerMock.replay(metadata);\n+        producer.partitionsFor(topic);\n+        PowerMock.verify(metadata);\n+    }\n }", "filename": "clients/src/test/java/org/apache/kafka/clients/producer/KafkaProducerTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/5d893654489647e8be65c4d54864ab63b7285faa", "parent": "https://github.com/apache/kafka/commit/f25fe02d994a663b6a5a0d01ea290785d63e1e52", "message": "KAFKA-3310; Fix for NPEs observed when throttling clients.\n\nThe fix basically ensures that the throttleTimeSensor is non-null before handing off to record the metric value. We also record the throttle time to 0 so that we don't recreate the sensor always.\n\nAuthor: Aditya Auradkar <aauradkar@linkedin.com>\n\nReviewers: Jiangjie Qin <becket.qin@gmail.com>, Jun Rao <junrao@gmail.com>\n\nCloses #989 from auradkar/KAFKA-3310", "bug_id": "kafka_70", "file": [{"additions": 7, "raw_url": "https://github.com/apache/kafka/raw/5d893654489647e8be65c4d54864ab63b7285faa/core/src/main/scala/kafka/server/ClientQuotaManager.scala", "blob_url": "https://github.com/apache/kafka/blob/5d893654489647e8be65c4d54864ab63b7285faa/core/src/main/scala/kafka/server/ClientQuotaManager.scala", "sha": "5863c72c037399725674fb18a8083c98100627db", "changes": 11, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/ClientQuotaManager.scala?ref=5d893654489647e8be65c4d54864ab63b7285faa", "patch": "@@ -120,9 +120,9 @@ class ClientQuotaManager(private val config: ClientQuotaManagerConfig,\n         val clientMetric = metrics.metrics().get(clientRateMetricName(clientId))\n         throttleTimeMs = throttleTime(clientMetric, getQuotaMetricConfig(quota(clientId)))\n         clientSensors.throttleTimeSensor.record(throttleTimeMs)\n+        // If delayed, add the element to the delayQueue\n         delayQueue.add(new ThrottledResponse(time, throttleTimeMs, callback))\n         delayQueueSensor.record()\n-        // If delayed, add the element to the delayQueue\n         logger.debug(\"Quota violated for sensor (%s). Delay time: (%d)\".format(clientSensors.quotaSensor.name(), throttleTimeMs))\n     }\n     throttleTimeMs\n@@ -189,9 +189,9 @@ class ClientQuotaManager(private val config: ClientQuotaManagerConfig,\n     }\n \n     /* If the sensor is null, try to create it else return the created sensor\n-     * Also if quota sensor is null, the throttle time sensor must be null\n+     * Either of the sensors can be null, hence null checks on both\n      */\n-    if (quotaSensor == null) {\n+    if (quotaSensor == null || throttleTimeSensor == null) {\n       /* Acquire a write lock because the sensor may not have been created and we only want one thread to create it.\n        * Note that multiple threads may acquire the write lock if they all see a null sensor initially\n        * In this case, the writer checks the sensor after acquiring the lock again.\n@@ -204,7 +204,7 @@ class ClientQuotaManager(private val config: ClientQuotaManagerConfig,\n         // ensure that we initialise `ClientSensors` with non-null parameters.\n         quotaSensor = metrics.getSensor(quotaSensorName)\n         throttleTimeSensor = metrics.getSensor(throttleTimeSensorName)\n-        if (quotaSensor == null) {\n+        if (throttleTimeSensor == null) {\n           // create the throttle time sensor also. Use default metric config\n           throttleTimeSensor = metrics.sensor(throttleTimeSensorName,\n                                               null,\n@@ -214,7 +214,10 @@ class ClientQuotaManager(private val config: ClientQuotaManagerConfig,\n                                                 \"Tracking average throttle-time per client\",\n                                                 \"client-id\",\n                                                 clientId), new Avg())\n+        }\n+\n \n+        if (quotaSensor == null) {\n           quotaSensor = metrics.sensor(quotaSensorName,\n                                        getQuotaMetricConfig(quota(clientId)),\n                                        ClientQuotaManagerConfig.InactiveSensorExpirationTimeSeconds)", "filename": "core/src/main/scala/kafka/server/ClientQuotaManager.scala"}, {"additions": 47, "raw_url": "https://github.com/apache/kafka/raw/5d893654489647e8be65c4d54864ab63b7285faa/core/src/test/scala/unit/kafka/server/ClientQuotaManagerTest.scala", "blob_url": "https://github.com/apache/kafka/blob/5d893654489647e8be65c4d54864ab63b7285faa/core/src/test/scala/unit/kafka/server/ClientQuotaManagerTest.scala", "sha": "193acfd2bb77159bf504f322e48f9c74772b8c33", "changes": 52, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/server/ClientQuotaManagerTest.scala?ref=5d893654489647e8be65c4d54864ab63b7285faa", "patch": "@@ -18,7 +18,6 @@ package kafka.server\n \n import java.util.Collections\n \n-import org.apache.kafka.common.MetricName\n import org.apache.kafka.common.metrics.{MetricConfig, Metrics, Quota}\n import org.apache.kafka.common.utils.MockTime\n import org.junit.Assert.{assertEquals, assertTrue}\n@@ -44,8 +43,8 @@ class ClientQuotaManagerTest {\n     val clientMetrics = new ClientQuotaManager(config, newMetrics, \"producer\", time)\n \n     // Case 1: Update the quota. Assert that the new quota value is returned\n-    clientMetrics.updateQuota(\"p1\", new Quota(2000, true));\n-    clientMetrics.updateQuota(\"p2\", new Quota(4000, true));\n+    clientMetrics.updateQuota(\"p1\", new Quota(2000, true))\n+    clientMetrics.updateQuota(\"p2\", new Quota(4000, true))\n \n     try {\n       assertEquals(\"Default producer quota should be 500\", new Quota(500, true), clientMetrics.quota(\"random-client-id\"))\n@@ -58,14 +57,14 @@ class ClientQuotaManagerTest {\n \n       // Case 2: Change quota again. The quota should be updated within KafkaMetrics as well since the sensor was created.\n       // p1 should not longer be throttled after the quota change\n-      clientMetrics.updateQuota(\"p1\", new Quota(3000, true));\n+      clientMetrics.updateQuota(\"p1\", new Quota(3000, true))\n       assertEquals(\"Should return the newly overridden value (3000)\", new Quota(3000, true), clientMetrics.quota(\"p1\"))\n \n       throttleTimeMs = clientMetrics.recordAndMaybeThrottle(\"p1\", 0, this.callback)\n       assertEquals(s\"throttleTimeMs should be 0. was $throttleTimeMs\", 0, throttleTimeMs)\n \n       // Case 3: Change quota back to default. Should be throttled again\n-      clientMetrics.updateQuota(\"p1\", new Quota(500, true));\n+      clientMetrics.updateQuota(\"p1\", new Quota(500, true))\n       assertEquals(\"Should return the default value (500)\", new Quota(500, true), clientMetrics.quota(\"p1\"))\n \n       throttleTimeMs = clientMetrics.recordAndMaybeThrottle(\"p1\", 0, this.callback)\n@@ -123,6 +122,49 @@ class ClientQuotaManagerTest {\n     }\n   }\n \n+  @Test\n+  def testExpireThrottleTimeSensor() {\n+    val metrics = newMetrics\n+    val clientMetrics = new ClientQuotaManager(config, metrics, \"producer\", time)\n+    try {\n+      clientMetrics.recordAndMaybeThrottle(\"client1\", 100, callback)\n+      // remove the throttle time sensor\n+      metrics.removeSensor(\"producerThrottleTime-client1\")\n+      // should not throw an exception even if the throttle time sensor does not exist.\n+      val throttleTime = clientMetrics.recordAndMaybeThrottle(\"client1\", 10000, callback)\n+      assertTrue(\"Should be throttled\", throttleTime > 0)\n+      // the sensor should get recreated\n+      val throttleTimeSensor = metrics.getSensor(\"producerThrottleTime-client1\")\n+      assertTrue(\"Throttle time sensor should exist\", throttleTimeSensor != null)\n+    } finally {\n+      clientMetrics.shutdown()\n+    }\n+  }\n+\n+  @Test\n+  def testExpireQuotaSensors() {\n+    val metrics = newMetrics\n+    val clientMetrics = new ClientQuotaManager(config, metrics, \"producer\", time)\n+    try {\n+      clientMetrics.recordAndMaybeThrottle(\"client1\", 100, callback)\n+      // remove all the sensors\n+      metrics.removeSensor(\"producerThrottleTime-client1\")\n+      metrics.removeSensor(\"producer-client1\")\n+      // should not throw an exception\n+      val throttleTime = clientMetrics.recordAndMaybeThrottle(\"client1\", 10000, callback)\n+      assertTrue(\"Should be throttled\", throttleTime > 0)\n+\n+      // all the sensors should get recreated\n+      val throttleTimeSensor = metrics.getSensor(\"producerThrottleTime-client1\")\n+      assertTrue(\"Throttle time sensor should exist\", throttleTimeSensor != null)\n+\n+      val byteRateSensor = metrics.getSensor(\"producer-client1\")\n+      assertTrue(\"Byte rate sensor should exist\", byteRateSensor != null)\n+    } finally {\n+      clientMetrics.shutdown()\n+    }\n+  }\n+\n   def newMetrics: Metrics = {\n     new Metrics(new MetricConfig(), Collections.emptyList(), time)\n   }", "filename": "core/src/test/scala/unit/kafka/server/ClientQuotaManagerTest.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/d9369de8f2c6435843fb7577d313bb24e3b09cba", "parent": "https://github.com/apache/kafka/commit/67077ebbcf24448e48d6223dc7090d5d94ea7780", "message": "KAFKA-6728: Corrected the worker\u2019s instantiation of the HeaderConverter\n\n## Summary of the problem\nWhen the `header.converter` is not specified in the worker config or the connector config, a bug in the `Plugins` test causes it to never instantiate the `HeaderConverter` instance, even though there is a default value.\n\nThis is a problem as soon as the connector deals with headers, either in records created by a source connector or in messages on the Kafka topics consumed by a sink connector. As soon as that happens, a NPE occurs.\n\nA workaround is to explicitly set the `header.converter` configuration property, but this was added in AK 1.1 and thus means that upgrading to AK 1.1 will not be backward compatible and will require this configuration change.\n\n## The Changes\n\nThe `Plugins.newHeaderConverter` methods were always returning null if the `header.converter` configuration value was not specified in the supplied connector or worker configuration. Thus, even though the `header.converter` property has a default, it was never being used.\n\nThe fix was to only check whether a `header.converter` property was specified when the connector configuration was being used, and if no such property exists in the connector configuration to return null. Then, when the worker configuration is being used, the method simply gets the `header.converter` value (or the default if no value was explicitly set).\n\nAlso, the ConnectorConfig had the same default value for the `header.converter` property as the WorkerConfig, but this resulted in very confusing log messages that implied the default header converter should be used even when the worker config specified the `header.converter` value. By removing the default, the log messages now make sense, and the Worker still properly instantiates the correct header converter.\n\nFinally, updated comments and added log messages to make it more clear which converters are being used and how they are being converted.\n\n## Testing\n\nSeveral new unit tests for `Plugins.newHeaderConverter` were added to check the various behavior. Additionally, the runtime JAR with these changes was built and inserted into an AK 1.1 installation, and a source connector was manually tested with 8 different combinations of settings for the `header.converter` configuration:\n\n1. default value\n1. worker configuration has `header.converter` explicitly set to the default\n1. worker configuration has `header.converter` set to a custom `HeaderConverter` implementation in the same plugin\n1. worker configuration has `header.converter` set to a custom `HeaderConverter` implementation in a _different_ plugin\n1. connector configuration has `header.converter` explicitly set to the default\n1. connector configuration has `header.converter` set to a custom `HeaderConverter` implementation in the same plugin\n1. connector configuration has `header.converter` set to a custom `HeaderConverter` implementation in a _different_ plugin\n1. worker configuration has `header.converter` explicitly set to the default, and the connector configuration has `header.converter` set to a custom `HeaderConverter` implementation in a _different_ plugin\n\nThe worker created the correct `HeaderConverter` implementation with the correct configuration in all of these tests.\n\nFinally, the default configuration was used with the aforementioned custom source connector that generated records with headers, and an S3 connector that consumes the records with headers (but didn't do anything with them). This test also passed.\n\nAuthor: Randall Hauch <rhauch@gmail.com>\n\nReviewers: Ewen Cheslack-Postava <ewen@confluent.io>\n\nCloses #4815 from rhauch/kafka-6728", "bug_id": "kafka_71", "file": [{"additions": 3, "raw_url": "https://github.com/apache/kafka/raw/d9369de8f2c6435843fb7577d313bb24e3b09cba/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java", "blob_url": "https://github.com/apache/kafka/blob/d9369de8f2c6435843fb7577d313bb24e3b09cba/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java", "sha": "fd05af57a648c2682879896d91a27a4f60e8b523", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java?ref=d9369de8f2c6435843fb7577d313bb24e3b09cba", "patch": "@@ -76,7 +76,9 @@\n     public static final String HEADER_CONVERTER_CLASS_CONFIG = WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG;\n     public static final String HEADER_CONVERTER_CLASS_DOC = WorkerConfig.HEADER_CONVERTER_CLASS_DOC;\n     public static final String HEADER_CONVERTER_CLASS_DISPLAY = \"Header converter class\";\n-    public static final String HEADER_CONVERTER_CLASS_DEFAULT = WorkerConfig.HEADER_CONVERTER_CLASS_DEFAULT;\n+    // The Connector config should not have a default for the header converter, since the absence of a config property means that\n+    // the worker config settings should be used. Thus, we set the default to null here.\n+    public static final String HEADER_CONVERTER_CLASS_DEFAULT = null;\n \n     public static final String TASKS_MAX_CONFIG = \"tasks.max\";\n     private static final String TASKS_MAX_DOC = \"Maximum number of tasks to use for this connector.\";", "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/ConnectorConfig.java"}, {"additions": 9, "raw_url": "https://github.com/apache/kafka/raw/d9369de8f2c6435843fb7577d313bb24e3b09cba/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java", "blob_url": "https://github.com/apache/kafka/blob/d9369de8f2c6435843fb7577d313bb24e3b09cba/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java", "sha": "1c6465855ff7bd3392a7911eaffa3017850fdac5", "changes": 9, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java?ref=d9369de8f2c6435843fb7577d313bb24e3b09cba", "patch": "@@ -397,12 +397,21 @@ public boolean startTask(\n             );\n             if (keyConverter == null) {\n                 keyConverter = plugins.newConverter(config, WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);\n+                log.info(\"Set up the key converter {} for task {} using the worker config\", keyConverter.getClass(), id);\n+            } else {\n+                log.info(\"Set up the key converter {} for task {} using the connector config\", keyConverter.getClass(), id);\n             }\n             if (valueConverter == null) {\n                 valueConverter = plugins.newConverter(config, WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);\n+                log.info(\"Set up the value converter {} for task {} using the worker config\", valueConverter.getClass(), id);\n+            } else {\n+                log.info(\"Set up the value converter {} for task {} using the connector config\", valueConverter.getClass(), id);\n             }\n             if (headerConverter == null) {\n                 headerConverter = plugins.newHeaderConverter(config, WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG, ClassLoaderUsage.PLUGINS);\n+                log.info(\"Set up the header converter {} for task {} using the worker config\", headerConverter.getClass(), id);\n+            } else {\n+                log.info(\"Set up the header converter {} for task {} using the connector config\", headerConverter.getClass(), id);\n             }\n \n             workerTask = buildWorkerTask(connConfig, id, task, statusListener, initialState, keyConverter, valueConverter, headerConverter, connectorLoader);", "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/Worker.java"}, {"additions": 9, "raw_url": "https://github.com/apache/kafka/raw/d9369de8f2c6435843fb7577d313bb24e3b09cba/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/Plugins.java", "blob_url": "https://github.com/apache/kafka/blob/d9369de8f2c6435843fb7577d313bb24e3b09cba/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/Plugins.java", "sha": "f4cd2ba14b0de6d95efeb33c2d023e692951a73a", "changes": 14, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/Plugins.java?ref=d9369de8f2c6435843fb7577d313bb24e3b09cba", "patch": "@@ -234,6 +234,8 @@ public Converter newConverter(AbstractConfig config, String classPropertyName, C\n         // Configure the Converter using only the old configuration mechanism ...\n         String configPrefix = classPropertyName + \".\";\n         Map<String, Object> converterConfig = config.originalsWithPrefix(configPrefix);\n+        log.debug(\"Configuring the {} converter with configuration:{}{}\",\n+                  isKeyConverter ? \"key\" : \"value\", System.lineSeparator(), converterConfig);\n         plugin.configure(converterConfig, isKeyConverter);\n         return plugin;\n     }\n@@ -249,20 +251,21 @@ public Converter newConverter(AbstractConfig config, String classPropertyName, C\n      * @throws ConnectException if the {@link HeaderConverter} implementation class could not be found\n      */\n     public HeaderConverter newHeaderConverter(AbstractConfig config, String classPropertyName, ClassLoaderUsage classLoaderUsage) {\n-        if (!config.originals().containsKey(classPropertyName)) {\n-            // This configuration does not define the header converter via the specified property name\n-            return null;\n-        }\n         HeaderConverter plugin = null;\n         switch (classLoaderUsage) {\n             case CURRENT_CLASSLOADER:\n+                if (!config.originals().containsKey(classPropertyName)) {\n+                    // This connector configuration does not define the header converter via the specified property name\n+                    return null;\n+                }\n                 // Attempt to load first with the current classloader, and plugins as a fallback.\n                 // Note: we can't use config.getConfiguredInstance because we have to remove the property prefixes\n                 // before calling config(...)\n                 plugin = getInstance(config, classPropertyName, HeaderConverter.class);\n                 break;\n             case PLUGINS:\n-                // Attempt to load with the plugin class loader, which uses the current classloader as a fallback\n+                // Attempt to load with the plugin class loader, which uses the current classloader as a fallback.\n+                // Note that there will always be at least a default header converter for the worker\n                 String converterClassOrAlias = config.getClass(classPropertyName).getName();\n                 Class<? extends HeaderConverter> klass;\n                 try {\n@@ -288,6 +291,7 @@ public HeaderConverter newHeaderConverter(AbstractConfig config, String classPro\n         String configPrefix = classPropertyName + \".\";\n         Map<String, Object> converterConfig = config.originalsWithPrefix(configPrefix);\n         converterConfig.put(ConverterConfig.TYPE_CONFIG, ConverterType.HEADER.getName());\n+        log.debug(\"Configuring the header converter with configuration:{}{}\", System.lineSeparator(), converterConfig);\n         plugin.configure(converterConfig);\n         return plugin;\n     }", "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/Plugins.java"}, {"additions": 59, "raw_url": "https://github.com/apache/kafka/raw/d9369de8f2c6435843fb7577d313bb24e3b09cba/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/PluginsTest.java", "blob_url": "https://github.com/apache/kafka/blob/d9369de8f2c6435843fb7577d313bb24e3b09cba/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/PluginsTest.java", "sha": "a9a944fa3604cdb34033b05a50bb60a4264f80c8", "changes": 76, "status": "modified", "deletions": 17, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/PluginsTest.java?ref=d9369de8f2c6435843fb7577d313bb24e3b09cba", "patch": "@@ -29,6 +29,7 @@\n import org.apache.kafka.connect.storage.ConverterConfig;\n import org.apache.kafka.connect.storage.ConverterType;\n import org.apache.kafka.connect.storage.HeaderConverter;\n+import org.apache.kafka.connect.storage.SimpleHeaderConverter;\n import org.junit.Before;\n import org.junit.BeforeClass;\n import org.junit.Test;\n@@ -39,18 +40,31 @@\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n \n public class PluginsTest {\n \n-    private static Map<String, String> props;\n+    private static Map<String, String> pluginProps;\n     private static Plugins plugins;\n+    private Map<String, String> props;\n     private AbstractConfig config;\n     private TestConverter converter;\n     private TestHeaderConverter headerConverter;\n \n     @BeforeClass\n     public static void beforeAll() {\n-        props = new HashMap<>();\n+        pluginProps = new HashMap<>();\n+\n+        // Set up the plugins to have no additional plugin directories.\n+        // This won't allow us to test classpath isolation, but it will allow us to test some of the utility methods.\n+        pluginProps.put(WorkerConfig.PLUGIN_PATH_CONFIG, \"\");\n+        plugins = new Plugins(pluginProps);\n+    }\n+\n+    @Before\n+    public void setup() {\n+        props = new HashMap<>(pluginProps);\n         props.put(WorkerConfig.KEY_CONVERTER_CLASS_CONFIG, TestConverter.class.getName());\n         props.put(WorkerConfig.VALUE_CONVERTER_CLASS_CONFIG, TestConverter.class.getName());\n         props.put(\"key.converter.\" + JsonConverterConfig.SCHEMAS_ENABLE_CONFIG, \"true\");\n@@ -66,14 +80,10 @@ public static void beforeAll() {\n         props.put(WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG, TestHeaderConverter.class.getName());\n         props.put(\"header.converter.extra.config\", \"baz\");\n \n-        // Set up the plugins to have no additional plugin directories.\n-        // This won't allow us to test classpath isolation, but it will allow us to test some of the utility methods.\n-        props.put(WorkerConfig.PLUGIN_PATH_CONFIG, \"\");\n-        plugins = new Plugins(props);\n+        createConfig();\n     }\n \n-    @Before\n-    public void setup() {\n+    protected void createConfig() {\n         this.config = new TestableWorkerConfig(props);\n     }\n \n@@ -104,23 +114,55 @@ public void shouldInstantiateAndConfigureInternalConverters() {\n     }\n \n     @Test\n-    public void shouldInstantiateAndConfigureHeaderConverter() {\n-        instantiateAndConfigureHeaderConverter(WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG);\n+    public void shouldInstantiateAndConfigureExplicitlySetHeaderConverterWithCurrentClassLoader() {\n+        assertNotNull(props.get(WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG));\n+        HeaderConverter headerConverter = plugins.newHeaderConverter(config,\n+                                                                     WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,\n+                                                                     ClassLoaderUsage.CURRENT_CLASSLOADER);\n+        assertNotNull(headerConverter);\n+        assertTrue(headerConverter instanceof TestHeaderConverter);\n+        this.headerConverter = (TestHeaderConverter) headerConverter;\n+\n+        // Validate extra configs got passed through to overridden converters\n+        assertConverterType(ConverterType.HEADER, this.headerConverter.configs);\n+        assertEquals(\"baz\", this.headerConverter.configs.get(\"extra.config\"));\n+\n+        headerConverter = plugins.newHeaderConverter(config,\n+                                                     WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,\n+                                                     ClassLoaderUsage.PLUGINS);\n+        assertNotNull(headerConverter);\n+        assertTrue(headerConverter instanceof TestHeaderConverter);\n+        this.headerConverter = (TestHeaderConverter) headerConverter;\n+\n         // Validate extra configs got passed through to overridden converters\n-        assertConverterType(ConverterType.HEADER, headerConverter.configs);\n-        assertEquals(\"baz\", headerConverter.configs.get(\"extra.config\"));\n+        assertConverterType(ConverterType.HEADER, this.headerConverter.configs);\n+        assertEquals(\"baz\", this.headerConverter.configs.get(\"extra.config\"));\n+    }\n+\n+    @Test\n+    public void shouldInstantiateAndConfigureDefaultHeaderConverter() {\n+        props.remove(WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG);\n+        createConfig();\n+\n+        // Because it's not explicitly set on the supplied configuration, the logic to use the current classloader for the connector\n+        // will exit immediately, and so this method always returns null\n+        HeaderConverter headerConverter = plugins.newHeaderConverter(config,\n+                                                                     WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,\n+                                                                     ClassLoaderUsage.CURRENT_CLASSLOADER);\n+        assertNull(headerConverter);\n+        // But we should always find it (or the worker's default) when using the plugins classloader ...\n+        headerConverter = plugins.newHeaderConverter(config,\n+                                                     WorkerConfig.HEADER_CONVERTER_CLASS_CONFIG,\n+                                                     ClassLoaderUsage.PLUGINS);\n+        assertNotNull(headerConverter);\n+        assertTrue(headerConverter instanceof SimpleHeaderConverter);\n     }\n \n     protected void instantiateAndConfigureConverter(String configPropName, ClassLoaderUsage classLoaderUsage) {\n         converter = (TestConverter) plugins.newConverter(config, configPropName, classLoaderUsage);\n         assertNotNull(converter);\n     }\n \n-    protected void instantiateAndConfigureHeaderConverter(String configPropName) {\n-        headerConverter = (TestHeaderConverter) plugins.newHeaderConverter(config, configPropName, ClassLoaderUsage.CURRENT_CLASSLOADER);\n-        assertNotNull(headerConverter);\n-    }\n-\n     protected void assertConverterType(ConverterType type, Map<String, ?> props) {\n         assertEquals(type.getName(), props.get(ConverterConfig.TYPE_CONFIG));\n     }", "filename": "connect/runtime/src/test/java/org/apache/kafka/connect/runtime/isolation/PluginsTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/2d19ad4bb07b4f10f2a314e86bbf4171cdcd7765", "parent": "https://github.com/apache/kafka/commit/e43bbce493192ea5f936867e8cdae163ab850650", "message": "KAFKA-4205; KafkaApis: fix NPE caused by conversion to array\n\nNPE was caused by `log.logSegments.toArray` resulting in array containing `null` values. The exact reason still remains somewhat a mystery to me, but it seems that the culprit is `JavaConverters` in combination with concurrent data structure access.\n\nHere's a simple code example to prove that:\n```scala\nimport java.util.concurrent.ConcurrentSkipListMap\n// Same as `JavaConversions`, but allows explicit conversions via `asScala`/`asJava` methods.\nimport scala.collection.JavaConverters._\n\ncase object Value\nval m = new ConcurrentSkipListMap[Int, Value.type]\nnew Thread { override def run() = { while (true) m.put(9000, Value) } }.start()\nnew Thread { override def run() = { while (true) m.remove(9000) } }.start()\nnew Thread { override def run() = { while (true) { println(m.values.asScala.toArray.headOption) } } }.start()\n```\n\nRunning the example will occasionally print `Some(null)` indicating that there's something shady going on during `toArray` conversion.\n\n`null`s magically disappear by making the following change:\n```diff\n- println(m.values.asScala.toArray.headOption)\n+ println(m.values.asScala.toSeq.headOption)\n```\n\nAuthor: Anton Karamanov <ataraxer@yandex-team.ru>\n\nReviewers: Ismael Juma <ismael@juma.me.uk>, Guozhang Wang <wangguoz@gmail.com>\n\nCloses #2204 from ataraxer/KAFKA-4205", "bug_id": "kafka_72", "file": [{"additions": 7, "raw_url": "https://github.com/apache/kafka/raw/2d19ad4bb07b4f10f2a314e86bbf4171cdcd7765/core/src/main/scala/kafka/log/Log.scala", "blob_url": "https://github.com/apache/kafka/blob/2d19ad4bb07b4f10f2a314e86bbf4171cdcd7765/core/src/main/scala/kafka/log/Log.scala", "sha": "6acc8d274a1379b5523b6bb71bb842534743f530", "changes": 12, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/log/Log.scala?ref=2d19ad4bb07b4f10f2a314e86bbf4171cdcd7765", "patch": "@@ -605,19 +605,21 @@ class Log(@volatile var dir: File,\n           s\"for partition $topicAndPartition is ${config.messageFormatVersion} which is earlier than the minimum \" +\n           s\"required version $KAFKA_0_10_0_IV0\")\n \n+    // Cache to avoid race conditions. `toBuffer` is faster than most alternatives and provides\n+    // constant time access while being safe to use with concurrent collections unlike `toArray`.\n+    val segmentsCopy = logSegments.toBuffer\n     // For the earliest and latest, we do not need to return the timestamp.\n-    val segsArray = logSegments.toArray\n     if (targetTimestamp == ListOffsetRequest.EARLIEST_TIMESTAMP)\n-        return Some(TimestampOffset(Message.NoTimestamp, segsArray(0).baseOffset))\n+        return Some(TimestampOffset(Message.NoTimestamp, segmentsCopy.head.baseOffset))\n     else if (targetTimestamp == ListOffsetRequest.LATEST_TIMESTAMP)\n         return Some(TimestampOffset(Message.NoTimestamp, logEndOffset))\n \n     val targetSeg = {\n       // Get all the segments whose largest timestamp is smaller than target timestamp\n-      val earlierSegs = segsArray.takeWhile(_.largestTimestamp < targetTimestamp)\n+      val earlierSegs = segmentsCopy.takeWhile(_.largestTimestamp < targetTimestamp)\n       // We need to search the first segment whose largest timestamp is greater than the target timestamp if there is one.\n-      if (earlierSegs.length < segsArray.length)\n-        Some(segsArray(earlierSegs.length))\n+      if (earlierSegs.length < segmentsCopy.length)\n+        Some(segmentsCopy(earlierSegs.length))\n       else\n         None\n     }", "filename": "core/src/main/scala/kafka/log/Log.scala"}, {"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/2d19ad4bb07b4f10f2a314e86bbf4171cdcd7765/core/src/main/scala/kafka/log/LogCleanerManager.scala", "blob_url": "https://github.com/apache/kafka/blob/2d19ad4bb07b4f10f2a314e86bbf4171cdcd7765/core/src/main/scala/kafka/log/LogCleanerManager.scala", "sha": "681042ef4987d297712c9e280b369d6201afe6d6", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/log/LogCleanerManager.scala?ref=2d19ad4bb07b4f10f2a314e86bbf4171cdcd7765", "patch": "@@ -289,7 +289,7 @@ private[log] object LogCleanerManager extends Logging {\n     }\n \n     // dirty log segments\n-    val dirtyNonActiveSegments = log.logSegments(firstDirtyOffset, log.activeSegment.baseOffset).toArray\n+    val dirtyNonActiveSegments = log.logSegments(firstDirtyOffset, log.activeSegment.baseOffset)\n \n     val compactionLagMs = math.max(log.config.compactionLagMs, 0L)\n ", "filename": "core/src/main/scala/kafka/log/LogCleanerManager.scala"}, {"additions": 13, "raw_url": "https://github.com/apache/kafka/raw/2d19ad4bb07b4f10f2a314e86bbf4171cdcd7765/core/src/main/scala/kafka/server/KafkaApis.scala", "blob_url": "https://github.com/apache/kafka/blob/2d19ad4bb07b4f10f2a314e86bbf4171cdcd7765/core/src/main/scala/kafka/server/KafkaApis.scala", "sha": "447e4e2b4bb90a2f799f0af501980878b5ce015e", "changes": 23, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/KafkaApis.scala?ref=2d19ad4bb07b4f10f2a314e86bbf4171cdcd7765", "patch": "@@ -717,18 +717,21 @@ class KafkaApis(val requestChannel: RequestChannel,\n   }\n \n   private[server] def fetchOffsetsBefore(log: Log, timestamp: Long, maxNumOffsets: Int): Seq[Long] = {\n-    val segsArray = log.logSegments.toArray\n-    var offsetTimeArray: Array[(Long, Long)] = null\n-    val lastSegmentHasSize = segsArray.last.size > 0\n-    if (lastSegmentHasSize)\n-      offsetTimeArray = new Array[(Long, Long)](segsArray.length + 1)\n-    else\n-      offsetTimeArray = new Array[(Long, Long)](segsArray.length)\n+    // Cache to avoid race conditions. `toBuffer` is faster than most alternatives and provides\n+    // constant time access while being safe to use with concurrent collections unlike `toArray`.\n+    val segments = log.logSegments.toBuffer\n+    val lastSegmentHasSize = segments.last.size > 0\n+\n+    val offsetTimeArray =\n+      if (lastSegmentHasSize)\n+        new Array[(Long, Long)](segments.length + 1)\n+      else\n+        new Array[(Long, Long)](segments.length)\n \n-    for (i <- segsArray.indices)\n-      offsetTimeArray(i) = (segsArray(i).baseOffset, segsArray(i).lastModified)\n+    for (i <- segments.indices)\n+      offsetTimeArray(i) = (segments(i).baseOffset, segments(i).lastModified)\n     if (lastSegmentHasSize)\n-      offsetTimeArray(segsArray.length) = (log.logEndOffset, time.milliseconds)\n+      offsetTimeArray(segments.length) = (log.logEndOffset, time.milliseconds)\n \n     var startIndex = -1\n     timestamp match {", "filename": "core/src/main/scala/kafka/server/KafkaApis.scala"}, {"additions": 19, "raw_url": "https://github.com/apache/kafka/raw/2d19ad4bb07b4f10f2a314e86bbf4171cdcd7765/core/src/test/scala/unit/kafka/server/LogOffsetTest.scala", "blob_url": "https://github.com/apache/kafka/blob/2d19ad4bb07b4f10f2a314e86bbf4171cdcd7765/core/src/test/scala/unit/kafka/server/LogOffsetTest.scala", "sha": "70445d78a1594f404190b8ee10cc03b7644b67d9", "changes": 19, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/server/LogOffsetTest.scala?ref=2d19ad4bb07b4f10f2a314e86bbf4171cdcd7765", "patch": "@@ -214,6 +214,25 @@ class LogOffsetTest extends ZooKeeperTestHarness {\n     server.apis.fetchOffsetsBefore(log, System.currentTimeMillis, 100)\n   }\n \n+  /* We test that `fetchOffsetsBefore` works correctly if `Log.logSegments` content and size are\n+   * different (simulating a race condition) */\n+  @Test\n+  def testFetchOffsetsBeforeWithChangingSegments() {\n+    val log = EasyMock.niceMock(classOf[Log])\n+    val logSegment = EasyMock.niceMock(classOf[LogSegment])\n+    EasyMock.expect(log.logSegments).andStubAnswer {\n+      new IAnswer[Iterable[LogSegment]] {\n+        def answer = new Iterable[LogSegment] {\n+          override def size = 2\n+          def iterator = Seq(logSegment).iterator\n+        }\n+      }\n+    }\n+    EasyMock.replay(logSegment)\n+    EasyMock.replay(log)\n+    server.apis.fetchOffsetsBefore(log, System.currentTimeMillis, 100)\n+  }\n+\n   private def createBrokerConfig(nodeId: Int): Properties = {\n     val props = new Properties\n     props.put(\"broker.id\", nodeId.toString)", "filename": "core/src/test/scala/unit/kafka/server/LogOffsetTest.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/9f94a7752a590c36186a7eb1eee16e992ec68c97", "parent": "https://github.com/apache/kafka/commit/f17790cd9952aecef67fbec58122c55c72e5c2b2", "message": "KAFKA-3905: Handling null/empty topics and collections, patterns when subscription with list of topics or with patterns, and with assignments.\n\n- Added validity checks for input parameters on subscribe, assign to avoid NPE, and provide an argument exception instead\n- Updated behavior for subscription with null collection to be same as when subscription with emptyList.i.e., unsubscribes.\n- Added tests on subscription, assign\n\nAuthor: Rekha Joshi <rekhajoshm@gmail.com>\n\nReviewers: Jason Gustafson <jason@confluent.io>, Ismael Juma <ismael@juma.me.uk>\n\nCloses #1601 from rekhajoshm/KAFKA-3905-1", "bug_id": "kafka_73", "file": [{"additions": 34, "raw_url": "https://github.com/apache/kafka/raw/9f94a7752a590c36186a7eb1eee16e992ec68c97/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java", "blob_url": "https://github.com/apache/kafka/blob/9f94a7752a590c36186a7eb1eee16e992ec68c97/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java", "sha": "ff94dc81e80ff60b6e9adfaf1502883d9ca05e99", "changes": 41, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java?ref=9f94a7752a590c36186a7eb1eee16e992ec68c97", "patch": "@@ -790,15 +790,22 @@ private KafkaConsumer(ConsumerConfig config,\n      * @param topics The list of topics to subscribe to\n      * @param listener Non-null listener instance to get notifications on partition assignment/revocation for the\n      *                 subscribed topics\n+     * @throws IllegalArgumentException If topics is null or contains null or empty elements\n      */\n     @Override\n     public void subscribe(Collection<String> topics, ConsumerRebalanceListener listener) {\n         acquire();\n         try {\n-            if (topics.isEmpty()) {\n+            if (topics == null) {\n+                throw new IllegalArgumentException(\"Topic collection to subscribe to cannot be null\");\n+            } else if (topics.isEmpty()) {\n                 // treat subscribing to empty topic list as the same as unsubscribing\n                 this.unsubscribe();\n             } else {\n+                for (String topic : topics) {\n+                    if (topic == null || topic.trim().isEmpty())\n+                        throw new IllegalArgumentException(\"Topic collection to subscribe to cannot contain null or empty topic\");\n+                }\n                 log.debug(\"Subscribed to topic(s): {}\", Utils.join(topics, \", \"));\n                 this.subscriptions.subscribe(topics, listener);\n                 metadata.setTopics(subscriptions.groupSubscription());\n@@ -824,6 +831,7 @@ public void subscribe(Collection<String> topics, ConsumerRebalanceListener liste\n      * management since the listener gives you an opportunity to commit offsets before a rebalance finishes.\n      *\n      * @param topics The list of topics to subscribe to\n+     * @throws IllegalArgumentException If topics is null or contains null or empty elements\n      */\n     @Override\n     public void subscribe(Collection<String> topics) {\n@@ -833,6 +841,7 @@ public void subscribe(Collection<String> topics) {\n     /**\n      * Subscribe to all topics matching specified pattern to get dynamically assigned partitions. The pattern matching will be done periodically against topics\n      * existing at the time of check.\n+     *\n      * <p>\n      * As part of group management, the consumer will keep track of the list of consumers that\n      * belong to a particular group and will trigger a rebalance operation if one of the\n@@ -845,11 +854,14 @@ public void subscribe(Collection<String> topics) {\n      * </ul>\n      *\n      * @param pattern Pattern to subscribe to\n+     * @throws IllegalArgumentException If pattern is null\n      */\n     @Override\n     public void subscribe(Pattern pattern, ConsumerRebalanceListener listener) {\n         acquire();\n         try {\n+            if (pattern == null)\n+                throw new IllegalArgumentException(\"Topic pattern to subscribe to cannot be null\");\n             log.debug(\"Subscribed to pattern: {}\", pattern);\n             this.subscriptions.subscribe(pattern, listener);\n             this.metadata.needMetadataForAllTopics(true);\n@@ -878,24 +890,39 @@ public void unsubscribe() {\n     /**\n      * Manually assign a list of partition to this consumer. This interface does not allow for incremental assignment\n      * and will replace the previous assignment (if there is one).\n+     *\n+     * If the given list of topic partition is empty, it is treated the same as {@link #unsubscribe()}.\n+     *\n      * <p>\n      * Manual topic assignment through this method does not use the consumer's group management\n      * functionality. As such, there will be no rebalance operation triggered when group membership or cluster and topic\n      * metadata change. Note that it is not possible to use both manual partition assignment with {@link #assign(Collection)}\n      * and group assignment with {@link #subscribe(Collection, ConsumerRebalanceListener)}.\n      *\n      * @param partitions The list of partitions to assign this consumer\n+     * @throws IllegalArgumentException If partitions is null or contains null or empty topics\n      */\n     @Override\n     public void assign(Collection<TopicPartition> partitions) {\n         acquire();\n         try {\n-            log.debug(\"Subscribed to partition(s): {}\", Utils.join(partitions, \", \"));\n-            this.subscriptions.assignFromUser(partitions);\n-            Set<String> topics = new HashSet<>();\n-            for (TopicPartition tp : partitions)\n-                topics.add(tp.topic());\n-            metadata.setTopics(topics);\n+            if (partitions == null) {\n+                throw new IllegalArgumentException(\"Topic partition collection to assign to cannot be null\");\n+            } else if (partitions.isEmpty()) {\n+                this.unsubscribe();\n+            } else {\n+                Set<String> topics = new HashSet<>();\n+                for (TopicPartition tp : partitions) {\n+                    String topic = (tp != null) ? tp.topic() : null;\n+                    if (topic == null || topic.trim().isEmpty())\n+                        throw new IllegalArgumentException(\"Topic partitions to assign to cannot have null or empty topic\");\n+                    topics.add(topic);\n+                }\n+\n+                log.debug(\"Subscribed to partition(s): {}\", Utils.join(partitions, \", \"));\n+                this.subscriptions.assignFromUser(partitions);\n+                metadata.setTopics(topics);\n+            }\n         } finally {\n             release();\n         }", "filename": "clients/src/main/java/org/apache/kafka/clients/consumer/KafkaConsumer.java"}, {"additions": 102, "raw_url": "https://github.com/apache/kafka/raw/9f94a7752a590c36186a7eb1eee16e992ec68c97/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java", "blob_url": "https://github.com/apache/kafka/blob/9f94a7752a590c36186a7eb1eee16e992ec68c97/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java", "sha": "b5a5fcab6aa26c40fecbb80c65258134a1cf1771", "changes": 102, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java?ref=9f94a7752a590c36186a7eb1eee16e992ec68c97", "patch": "@@ -25,6 +25,7 @@\n import org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient;\n import org.apache.kafka.clients.consumer.internals.ConsumerProtocol;\n import org.apache.kafka.clients.consumer.internals.Fetcher;\n+import org.apache.kafka.clients.consumer.internals.NoOpConsumerRebalanceListener;\n import org.apache.kafka.clients.consumer.internals.PartitionAssignor;\n import org.apache.kafka.clients.consumer.internals.SubscriptionState;\n import org.apache.kafka.common.Cluster;\n@@ -65,6 +66,7 @@\n import java.util.Map;\n import java.util.Properties;\n import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.regex.Pattern;\n \n import static java.util.Collections.singleton;\n import static org.junit.Assert.assertEquals;\n@@ -147,6 +149,53 @@ public void testSubscription() {\n         consumer.close();\n     }\n \n+    @Test(expected = IllegalArgumentException.class)\n+    public void testSubscriptionOnNullTopicCollection() {\n+        KafkaConsumer<byte[], byte[]> consumer = newConsumer();\n+\n+        try {\n+            consumer.subscribe(null);\n+        } finally {\n+            consumer.close();\n+        }\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void testSubscriptionOnNullTopic() {\n+        KafkaConsumer<byte[], byte[]> consumer = newConsumer();\n+        String nullTopic = null;\n+\n+        try {\n+            consumer.subscribe(Collections.singletonList(nullTopic));\n+        } finally {\n+            consumer.close();\n+        }\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void testSubscriptionOnEmptyTopic() {\n+        KafkaConsumer<byte[], byte[]> consumer = newConsumer();\n+        String emptyTopic = \"  \";\n+\n+        try {\n+            consumer.subscribe(Collections.singletonList(emptyTopic));\n+        } finally {\n+            consumer.close();\n+        }\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void testSubscriptionOnNullPattern() {\n+        KafkaConsumer<byte[], byte[]> consumer = newConsumer();\n+        Pattern pattern = null;\n+\n+        try {\n+            consumer.subscribe(pattern, new NoOpConsumerRebalanceListener());\n+        } finally {\n+            consumer.close();\n+        }\n+    }\n+\n     @Test(expected = IllegalArgumentException.class)\n     public void testSeekNegative() {\n         Properties props = new Properties();\n@@ -162,6 +211,59 @@ public void testSeekNegative() {\n         }\n     }\n \n+    @Test(expected = IllegalArgumentException.class)\n+    public void testAssignOnNullTopicPartition() {\n+        Properties props = new Properties();\n+        props.setProperty(ConsumerConfig.CLIENT_ID_CONFIG, \"testAssignOnNullTopicPartition\");\n+        props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9999\");\n+        props.setProperty(ConsumerConfig.METRIC_REPORTER_CLASSES_CONFIG, MockMetricsReporter.class.getName());\n+        KafkaConsumer<byte[], byte[]> consumer = newConsumer();\n+        try {\n+            consumer.assign(null);\n+        } finally {\n+            consumer.close();\n+        }\n+    }\n+\n+    @Test\n+    public void testAssignOnEmptyTopicPartition() {\n+        KafkaConsumer<byte[], byte[]> consumer = newConsumer();\n+\n+        consumer.assign(Collections.<TopicPartition>emptyList());\n+        assertTrue(consumer.subscription().isEmpty());\n+        assertTrue(consumer.assignment().isEmpty());\n+\n+        consumer.close();\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void testAssignOnNullTopicInPartition() {\n+        Properties props = new Properties();\n+        props.setProperty(ConsumerConfig.CLIENT_ID_CONFIG, \"testAssignOnNullTopicInPartition\");\n+        props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9999\");\n+        props.setProperty(ConsumerConfig.METRIC_REPORTER_CLASSES_CONFIG, MockMetricsReporter.class.getName());\n+        KafkaConsumer<byte[], byte[]> consumer = newConsumer();\n+        try {\n+            consumer.assign(Arrays.asList(new TopicPartition(null, 0)));\n+        } finally {\n+            consumer.close();\n+        }\n+    }\n+\n+    @Test(expected = IllegalArgumentException.class)\n+    public void testAssignOnEmptyTopicInPartition() {\n+        Properties props = new Properties();\n+        props.setProperty(ConsumerConfig.CLIENT_ID_CONFIG, \"testAssignOnEmptyTopicInPartition\");\n+        props.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9999\");\n+        props.setProperty(ConsumerConfig.METRIC_REPORTER_CLASSES_CONFIG, MockMetricsReporter.class.getName());\n+        KafkaConsumer<byte[], byte[]> consumer = newConsumer();\n+        try {\n+            consumer.assign(Arrays.asList(new TopicPartition(\"  \", 0)));\n+        } finally {\n+            consumer.close();\n+        }\n+    }\n+\n     @Test\n     public void testInterceptorConstructorClose() throws Exception {\n         try {", "filename": "clients/src/test/java/org/apache/kafka/clients/consumer/KafkaConsumerTest.java"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/bc2c17a9b85c8cba546ec41129de8055d46f740d", "parent": "https://github.com/apache/kafka/commit/38c5d7fba7387b797a10c9c6ed71bf99c6d417bc", "message": "MINOR: Shutdown ControllerEventThread via event instead of interruption\n\nIf the ControllerEventThread is interrupted when a request is\nbeing sent, it may lead to an IllegalStateException being thrown.\nThis, in turn, can lead to a NullPointerException in\nunregisterPartitionReassignmentIsrChangeHandlers,\n\nTo avoid these issues, we make the ControllerEventThread\nuninterruptable and we shut it down by clearing the queue\nand enqueuing a special event.\n\nTo make the code more robust, we also set\nReassignedPartitionsContext.reassignIsrChangeHandler\nduring construction instead of setting it to null first.\n\nFinally, misleading log messages in ephemeral node\ncreation have been clarified.\n\nFor reference, the relevant log lines from the relevant\nflaky test:\n\n```text\n[2017-11-15 10:30:13,869] ERROR Error while creating ephemeral at /controller with return code: OK (kafka.zk.KafkaZkClient$CheckedEphemeral:101)\n[2017-11-15 10:30:14,155] ERROR Haven't been able to send leader and isr requests, current state of the map is Map(101 -> Map(topic1-0 -> PartitionState(controllerEpoch=2, leader=101, leaderEpoch=3, isr=101, zkVersion=3, replicas=100,102,101, isNew=false)), 100 -> Map(topic1-0 -> PartitionState(controllerEpoch=2, leader=101, leaderEpoch=3, isr=101, zkVersion=3, replicas=100,102,101, isNew=false)), 102 -> Map(topic1-0 -> PartitionState(controllerEpoch=2, leader=101, leaderEpoch=3, isr=101, zkVersion=3, replicas=100,102,101, isNew=false))). Exception message: java.lang.InterruptedException (kafka.controller.ControllerBrokerRequestBatch:101)\n[2017-11-15 10:30:14,156] ERROR Haven't been able to send metadata update requests to brokers Set(102, 103, 104, 101, 105), current state of the partition info is Map(topic1-0 -> PartitionState(controllerEpoch=1, leader=101, leaderEpoch=2, isr=[101], zkVersion=2, replicas=[100, 102, 101], offlineReplicas=[100])). Exception message: java.lang.InterruptedException (kafka.controller.ControllerBrokerRequestBatch:101)\n[2017-11-15 10:30:14,158] ERROR [Controller id=101] Forcing the controller to resign (kafka.controller.KafkaController:101)\n[2017-11-15 10:30:14,158] ERROR [Controller id=101] Error completing reassignment of partition topic1-0 (kafka.controller.KafkaController:107)\njava.lang.NullPointerException\n\tat kafka.controller.KafkaController$$anonfun$unregisterPartitionReassignmentIsrChangeHandlers$1.apply(KafkaController.scala:784)\n\tat kafka.controller.KafkaController$$anonfun$unregisterPartitionReassignmentIsrChangeHandlers$1.apply(KafkaController.scala:783)\n```\n\nAuthor: Ismael Juma <ismael@juma.me.uk>\n\nReviewers: Jun Rao <junrao@gmail.com>\n\nCloses #4219 from ijuma/fix-npe-unregister-zk-listener", "bug_id": "kafka_74", "file": [{"additions": 24, "raw_url": "https://github.com/apache/kafka/raw/bc2c17a9b85c8cba546ec41129de8055d46f740d/core/src/main/scala/kafka/controller/ControllerEventManager.scala", "blob_url": "https://github.com/apache/kafka/blob/bc2c17a9b85c8cba546ec41129de8055d46f740d/core/src/main/scala/kafka/controller/ControllerEventManager.scala", "sha": "8ccbfb5665de87e3ae9e15e29a5f747bb14b9b8c", "changes": 42, "status": "modified", "deletions": 18, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/controller/ControllerEventManager.scala?ref=bc2c17a9b85c8cba546ec41129de8055d46f740d", "patch": "@@ -41,7 +41,10 @@ class ControllerEventManager(rateAndTimeMetrics: Map[ControllerState, KafkaTimer\n \n   def start(): Unit = thread.start()\n \n-  def close(): Unit = thread.shutdown()\n+  def close(): Unit = {\n+    clearAndPut(KafkaController.ShutdownEventThread)\n+    thread.awaitShutdown()\n+  }\n \n   def put(event: ControllerEvent): Unit = inLock(putLock) {\n     queue.put(event)\n@@ -52,25 +55,28 @@ class ControllerEventManager(rateAndTimeMetrics: Map[ControllerState, KafkaTimer\n     queue.put(event)\n   }\n \n-  class ControllerEventThread(name: String) extends ShutdownableThread(name = name) {\n+  class ControllerEventThread(name: String) extends ShutdownableThread(name = name, isInterruptible = false) {\n     override def doWork(): Unit = {\n-      val controllerEvent = queue.take()\n-      _state = controllerEvent.state\n-\n-      try {\n-        rateAndTimeMetrics(state).time {\n-          controllerEvent.process()\n-        }\n-      } catch {\n-        case e: Throwable => error(s\"Error processing event $controllerEvent\", e)\n-      }\n-\n-      try eventProcessedListener(controllerEvent)\n-      catch {\n-        case e: Throwable => error(s\"Error while invoking listener for processed event $controllerEvent\", e)\n+      queue.take() match {\n+        case KafkaController.ShutdownEventThread => initiateShutdown()\n+        case controllerEvent =>\n+          _state = controllerEvent.state\n+\n+          try {\n+            rateAndTimeMetrics(state).time {\n+              controllerEvent.process()\n+            }\n+          } catch {\n+            case e: Throwable => error(s\"Error processing event $controllerEvent\", e)\n+          }\n+\n+          try eventProcessedListener(controllerEvent)\n+          catch {\n+            case e: Throwable => error(s\"Error while invoking listener for processed event $controllerEvent\", e)\n+          }\n+\n+          _state = ControllerState.Idle\n       }\n-\n-      _state = ControllerState.Idle\n     }\n   }\n ", "filename": "core/src/main/scala/kafka/controller/ControllerEventManager.scala"}, {"additions": 5, "raw_url": "https://github.com/apache/kafka/raw/bc2c17a9b85c8cba546ec41129de8055d46f740d/core/src/main/scala/kafka/controller/ControllerState.scala", "blob_url": "https://github.com/apache/kafka/blob/bc2c17a9b85c8cba546ec41129de8055d46f740d/core/src/main/scala/kafka/controller/ControllerState.scala", "sha": "17af77759019b0833da17c93d40b1ce9316818fc", "changes": 6, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/controller/ControllerState.scala?ref=bc2c17a9b85c8cba546ec41129de8055d46f740d", "patch": "@@ -86,7 +86,11 @@ object ControllerState {\n     def value = 11\n   }\n \n+  case object ControllerShutdown extends ControllerState {\n+    def value = 12\n+  }\n+\n   val values: Seq[ControllerState] = Seq(Idle, ControllerChange, BrokerChange, TopicChange, TopicDeletion,\n     PartitionReassignment, AutoLeaderBalance, ManualLeaderBalance, ControlledShutdown, IsrChange, LeaderAndIsrResponseReceived,\n-    LogDirChange)\n+    LogDirChange, ControllerShutdown)\n }", "filename": "core/src/main/scala/kafka/controller/ControllerState.scala"}, {"additions": 31, "raw_url": "https://github.com/apache/kafka/raw/bc2c17a9b85c8cba546ec41129de8055d46f740d/core/src/main/scala/kafka/controller/KafkaController.scala", "blob_url": "https://github.com/apache/kafka/blob/bc2c17a9b85c8cba546ec41129de8055d46f740d/core/src/main/scala/kafka/controller/KafkaController.scala", "sha": "42c66f6ed62af02ca5c70c12cb228aa68c6d45ac", "changes": 52, "status": "modified", "deletions": 21, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/controller/KafkaController.scala?ref=bc2c17a9b85c8cba546ec41129de8055d46f740d", "patch": "@@ -43,6 +43,15 @@ import scala.util.Try\n object KafkaController extends Logging {\n   val InitialControllerEpoch = 1\n   val InitialControllerEpochZkVersion = 1\n+\n+  /**\n+   * ControllerEventThread will shutdown once it sees this event\n+   */\n+  private[controller] case object ShutdownEventThread extends ControllerEvent {\n+    def state = ControllerState.ControllerShutdown\n+    override def process(): Unit = ()\n+  }\n+\n }\n \n class KafkaController(val config: KafkaConfig, zkClient: KafkaZkClient, time: Time, metrics: Metrics, threadNamePrefix: Option[String] = None) extends Logging with KafkaMetricsGroup {\n@@ -489,16 +498,8 @@ class KafkaController(val config: KafkaConfig, zkClient: KafkaZkClient, time: Ti\n     }\n   }\n \n-  private def watchIsrChangesForReassignedPartition(partition: TopicPartition,\n-                                                    reassignedPartitionContext: ReassignedPartitionsContext) {\n-    val reassignIsrChangeHandler = new PartitionReassignmentIsrChangeHandler(this, eventManager, partition)\n-    reassignedPartitionContext.reassignIsrChangeHandler = reassignIsrChangeHandler\n-    // register listener on the leader and isr path to wait until they catch up with the current leader\n-    zkClient.registerZNodeChangeHandler(reassignIsrChangeHandler)\n-  }\n-\n   private def initiateReassignReplicasForTopicPartition(topicPartition: TopicPartition,\n-                                                reassignedPartitionContext: ReassignedPartitionsContext) {\n+                                                        reassignedPartitionContext: ReassignedPartitionsContext) {\n     val newReplicas = reassignedPartitionContext.newReplicas\n     val topic = topicPartition.topic\n     try {\n@@ -511,7 +512,7 @@ class KafkaController(val config: KafkaConfig, zkClient: KafkaZkClient, time: Ti\n           } else {\n             info(s\"Handling reassignment of partition $topicPartition to new replicas ${newReplicas.mkString(\",\")}\")\n             // first register ISR change listener\n-            watchIsrChangesForReassignedPartition(topicPartition, reassignedPartitionContext)\n+            reassignedPartitionContext.registerReassignIsrChangeHandler(zkClient)\n             controllerContext.partitionsBeingReassigned.put(topicPartition, reassignedPartitionContext)\n             // mark topic ineligible for deletion for the partitions being reassigned\n             topicDeletionManager.markTopicIneligibleForDeletion(Set(topic))\n@@ -632,7 +633,10 @@ class KafkaController(val config: KafkaConfig, zkClient: KafkaZkClient, time: Ti\n     }.keys\n     reassignedPartitions.foreach(removePartitionFromReassignedPartitions)\n     val partitionsToReassign = partitionsBeingReassigned -- reassignedPartitions\n-    controllerContext.partitionsBeingReassigned ++= partitionsToReassign.mapValues(new ReassignedPartitionsContext(_))\n+    controllerContext.partitionsBeingReassigned ++= partitionsToReassign.map { case (tp, newReplicas) =>\n+      val reassignIsrChangeHandler = new PartitionReassignmentIsrChangeHandler(this, eventManager, tp)\n+      tp -> new ReassignedPartitionsContext(newReplicas, reassignIsrChangeHandler)\n+    }\n     info(s\"Partitions being reassigned: $partitionsBeingReassigned\")\n     info(s\"Partitions already reassigned: $reassignedPartitions\")\n     info(s\"Resuming reassignment of partitions: $partitionsToReassign\")\n@@ -773,15 +777,12 @@ class KafkaController(val config: KafkaConfig, zkClient: KafkaZkClient, time: Ti\n \n   private[controller] def unregisterPartitionModificationsHandlers(topics: Seq[String]) = {\n     topics.foreach { topic =>\n-      partitionModificationsHandlers.remove(topic)\n-        .foreach(handler => zkClient.unregisterZNodeChangeHandler(handler.path))\n+      partitionModificationsHandlers.remove(topic).foreach(handler => zkClient.unregisterZNodeChangeHandler(handler.path))\n     }\n   }\n \n   private def unregisterPartitionReassignmentIsrChangeHandlers() {\n-    controllerContext.partitionsBeingReassigned.values.foreach { reassignedPartitionsContext =>\n-      zkClient.unregisterZNodeChangeHandler(reassignedPartitionsContext.reassignIsrChangeHandler.path)\n-    }\n+    controllerContext.partitionsBeingReassigned.values.foreach(_.unregisterReassignIsrChangeHandler(zkClient))\n   }\n \n   private def readControllerEpochFromZooKeeper() {\n@@ -796,8 +797,7 @@ class KafkaController(val config: KafkaConfig, zkClient: KafkaZkClient, time: Ti\n \n   private def removePartitionFromReassignedPartitions(topicPartition: TopicPartition) {\n     controllerContext.partitionsBeingReassigned.get(topicPartition).foreach { reassignContext =>\n-      // stop watching the ISR changes for this partition\n-      zkClient.unregisterZNodeChangeHandler(reassignContext.reassignIsrChangeHandler.path)\n+      reassignContext.unregisterReassignIsrChangeHandler(zkClient)\n     }\n \n     val updatedPartitionsBeingReassigned = controllerContext.partitionsBeingReassigned - topicPartition\n@@ -1282,12 +1282,14 @@ class KafkaController(val config: KafkaConfig, zkClient: KafkaZkClient, time: Ti\n       if (zkClient.registerZNodeChangeHandlerAndCheckExistence(partitionReassignmentHandler)) {\n         val partitionReassignment = zkClient.getPartitionReassignment\n         val partitionsToBeReassigned = partitionReassignment -- controllerContext.partitionsBeingReassigned.keys\n-        partitionsToBeReassigned.foreach { case (tp, context) =>\n+        partitionsToBeReassigned.foreach { case (tp, newReplicas) =>\n           if (topicDeletionManager.isTopicQueuedUpForDeletion(tp.topic)) {\n             error(s\"Skipping reassignment of $tp since the topic is currently being deleted\")\n             removePartitionFromReassignedPartitions(tp)\n           } else {\n-            initiateReassignReplicasForTopicPartition(tp, ReassignedPartitionsContext(context))\n+            val reassignIsrChangeHandler = new PartitionReassignmentIsrChangeHandler(KafkaController.this, eventManager,\n+              tp)\n+            initiateReassignReplicasForTopicPartition(tp, ReassignedPartitionsContext(newReplicas, reassignIsrChangeHandler))\n           }\n         }\n       }\n@@ -1488,7 +1490,15 @@ class ControllerChangeHandler(controller: KafkaController, eventManager: Control\n }\n \n case class ReassignedPartitionsContext(var newReplicas: Seq[Int] = Seq.empty,\n-                                       var reassignIsrChangeHandler: PartitionReassignmentIsrChangeHandler = null)\n+                                       val reassignIsrChangeHandler: PartitionReassignmentIsrChangeHandler) {\n+\n+  def registerReassignIsrChangeHandler(zkClient: KafkaZkClient): Unit =\n+    zkClient.registerZNodeChangeHandler(reassignIsrChangeHandler)\n+\n+  def unregisterReassignIsrChangeHandler(zkClient: KafkaZkClient): Unit =\n+    zkClient.unregisterZNodeChangeHandler(reassignIsrChangeHandler.path)\n+\n+}\n \n case class PartitionAndReplica(topicPartition: TopicPartition, replica: Int) {\n   def topic: String = topicPartition.topic", "filename": "core/src/main/scala/kafka/controller/KafkaController.scala"}, {"additions": 0, "raw_url": "https://github.com/apache/kafka/raw/bc2c17a9b85c8cba546ec41129de8055d46f740d/core/src/main/scala/kafka/tools/ProducerPerformance.scala", "blob_url": "https://github.com/apache/kafka/blob/bc2c17a9b85c8cba546ec41129de8055d46f740d/core/src/main/scala/kafka/tools/ProducerPerformance.scala", "sha": "f96200d2fcb2bfd576374b9e442dcd2aabf1d1d8", "changes": 1, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/tools/ProducerPerformance.scala?ref=bc2c17a9b85c8cba546ec41129de8055d46f740d", "patch": "@@ -31,7 +31,6 @@ import java.math.BigInteger\n import java.nio.charset.StandardCharsets\n \n import org.apache.kafka.common.utils.Utils\n-import org.slf4j.LoggerFactory\n \n /**\n  * Load test for the producer", "filename": "core/src/main/scala/kafka/tools/ProducerPerformance.scala"}, {"additions": 3, "raw_url": "https://github.com/apache/kafka/raw/bc2c17a9b85c8cba546ec41129de8055d46f740d/core/src/main/scala/kafka/utils/ZkUtils.scala", "blob_url": "https://github.com/apache/kafka/blob/bc2c17a9b85c8cba546ec41129de8055d46f740d/core/src/main/scala/kafka/utils/ZkUtils.scala", "sha": "004a408a303b27790db8f49e10ab62f8f9752276", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/utils/ZkUtils.scala?ref=bc2c17a9b85c8cba546ec41129de8055d46f740d", "patch": "@@ -821,7 +821,9 @@ class ZkUtils(zkClientWrap: ZooKeeperClientWrapper,\n     jsonPartitionMapOpt match {\n       case Some(jsonPartitionMap) =>\n         val reassignedPartitions = parsePartitionReassignmentData(jsonPartitionMap)\n-        reassignedPartitions.map(p => p._1 -> new ReassignedPartitionsContext(p._2))\n+        reassignedPartitions.map { case (tp, newReplicas) =>\n+          tp -> new ReassignedPartitionsContext(newReplicas, null)\n+        }\n       case None => Map.empty[TopicAndPartition, ReassignedPartitionsContext]\n     }\n   }", "filename": "core/src/main/scala/kafka/utils/ZkUtils.scala"}, {"additions": 17, "raw_url": "https://github.com/apache/kafka/raw/bc2c17a9b85c8cba546ec41129de8055d46f740d/core/src/main/scala/kafka/zk/KafkaZkClient.scala", "blob_url": "https://github.com/apache/kafka/blob/bc2c17a9b85c8cba546ec41129de8055d46f740d/core/src/main/scala/kafka/zk/KafkaZkClient.scala", "sha": "d4c7daa7ac44250af4c4bba714d14a0df0bd1925", "changes": 48, "status": "modified", "deletions": 31, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/zk/KafkaZkClient.scala?ref=bc2c17a9b85c8cba546ec41129de8055d46f740d", "patch": "@@ -21,7 +21,7 @@ import java.util.Properties\n \n import kafka.api.LeaderAndIsr\n import kafka.cluster.Broker\n-import kafka.controller.{LeaderIsrAndControllerEpoch, ReassignedPartitionsContext}\n+import kafka.controller.LeaderIsrAndControllerEpoch\n import kafka.log.LogConfig\n import kafka.security.auth.SimpleAclAuthorizer.VersionedAcls\n import kafka.security.auth.{Acl, Resource, ResourceType}\n@@ -680,14 +680,6 @@ class KafkaZkClient(zooKeeperClient: ZooKeeperClient, isSecure: Boolean) extends\n     pathExists(ReassignPartitionsZNode.path)\n   }\n \n-  /**\n-   * Gets the partitions being reassigned for given topics\n-   * @return ReassignedPartitionsContexts for each topic which are being reassigned.\n-   */\n-  def getPartitionsBeingReassigned(): Map[TopicPartition, ReassignedPartitionsContext] = {\n-    getPartitionReassignment.mapValues(replicas => ReassignedPartitionsContext(replicas))\n-  }\n-\n   /**\n    * Gets topic partition states for the given partitions.\n    * @param partitions the partitions for which we want to get states.\n@@ -1285,43 +1277,37 @@ class KafkaZkClient(zooKeeperClient: ZooKeeperClient, isSecure: Boolean) extends\n     info(s\"Creating $path (is it secure? $isSecure)\")\n     val code = checkedEphemeral.create()\n     info(s\"Result of znode creation at $path is: $code\")\n-    code match {\n-      case Code.OK =>\n-      case _ => throw KeeperException.create(code)\n-    }\n+    if (code != Code.OK)\n+      throw KeeperException.create(code)\n   }\n \n   private class CheckedEphemeral(path: String, data: Array[Byte]) extends Logging {\n     def create(): Code = {\n       val createRequest = CreateRequest(path, data, acls(path), CreateMode.EPHEMERAL)\n       val createResponse = retryRequestUntilConnected(createRequest)\n-      val code = createResponse.resultCode\n-      code match {\n-        case Code.OK => code\n-        case Code.NODEEXISTS => get()\n-        case _ =>\n+      createResponse.resultCode match {\n+        case code@ Code.OK => code\n+        case Code.NODEEXISTS => getAfterNodeExists()\n+        case code =>\n           error(s\"Error while creating ephemeral at $path with return code: $code\")\n           code\n       }\n     }\n \n-    private def get(): Code = {\n+    private def getAfterNodeExists(): Code = {\n       val getDataRequest = GetDataRequest(path)\n       val getDataResponse = retryRequestUntilConnected(getDataRequest)\n-      val code = getDataResponse.resultCode\n-      code match {\n-        case Code.OK =>\n-          if (getDataResponse.stat.getEphemeralOwner != zooKeeperClient.sessionId) {\n-            error(s\"Error while creating ephemeral at $path with return code: $code\")\n-            Code.NODEEXISTS\n-          } else {\n-            code\n-          }\n+      getDataResponse.resultCode match {\n+        case Code.OK if getDataResponse.stat.getEphemeralOwner != zooKeeperClient.sessionId =>\n+          error(s\"Error while creating ephemeral at $path, node already exists and owner \" +\n+            s\"'${getDataResponse.stat.getEphemeralOwner}' does not match current session '${zooKeeperClient.sessionId}'\")\n+          Code.NODEEXISTS\n+        case code@ Code.OK => code\n         case Code.NONODE =>\n-          info(s\"The ephemeral node at $path went away while reading it\")\n+          info(s\"The ephemeral node at $path went away while reading it, attempting create() again\")\n           create()\n-        case _ =>\n-          error(s\"Error while creating ephemeral at $path with return code: $code\")\n+        case code =>\n+          error(s\"Error while creating ephemeral at $path as it already exists and error getting the node data due to $code\")\n           code\n       }\n     }", "filename": "core/src/main/scala/kafka/zk/KafkaZkClient.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/6021618f9dafa3478104575d307e7bcd2cb4cca9", "parent": "https://github.com/apache/kafka/commit/b38b74bb77e6d12e4ea74b51216bdc15e47dbbce", "message": "MINOR: onControllerResignation should be invoked if triggerControllerMove is called\n\nAlso update the test to be simpler since we can use a mock event to simulate the issue\nmore easily (thanks Jun for the suggestion). This should fix two issues:\n\n1. A transient test failure due to a NPE in ControllerFailoverTest.testMetadataUpdate:\n\n```text\nCaused by: java.lang.NullPointerException\n\tat kafka.controller.ControllerBrokerRequestBatch.addUpdateMetadataRequestForBrokers(ControllerChannelManager.scala:338)\n\tat kafka.controller.KafkaController.sendUpdateMetadataRequest(KafkaController.scala:975)\n\tat kafka.controller.ControllerFailoverTest.testMetadataUpdate(ControllerFailoverTest.scala:141)\n```\n\nThe test was creating an additional thread and it does not seem like it was doing the\nappropriate synchronization (perhaps this became more of an issue after we changed\nthe Controller to be single-threaded and changed the locking)\n\n2. Setting `activeControllerId.set(-1)` in `triggerControllerMove` causes `Reelect` not to invoke `onControllerResignation`. Among other things, this causes an `IllegalStateException` to be thrown when `KafkaScheduler.startup` is invoked for the second time without the corresponding `shutdown`. We now simply call `onControllerResignation` as part of `triggerControllerMove`.\n\nFinally, I included a few clean-ups:\n\n1. No longer update the broker state in `onControllerFailover`. This is no longer needed\nsince we removed the `RunningAsController` state (KAFKA-3761).\n2. Trivial clean-ups in KafkaController\n3. Removed unused parameter in `ZkUtils.getPartitionLeaderAndIsrForTopics`\n\nAuthor: Ismael Juma <ismael@juma.me.uk>\n\nReviewers: Jun Rao <junrao@gmail.com>\n\nCloses #2935 from ijuma/on-controller-resignation-if-trigger-controller-move", "bug_id": "kafka_75", "file": [{"additions": 3, "raw_url": "https://github.com/apache/kafka/raw/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "blob_url": "https://github.com/apache/kafka/blob/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/main/scala/kafka/controller/ControllerChannelManager.scala", "sha": "8f98a8c26c246cf92c88db2bd1abf6a8125b66cb", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/controller/ControllerChannelManager.scala?ref=6021618f9dafa3478104575d307e7bcd2cb4cca9", "patch": "@@ -381,7 +381,9 @@ class ControllerBrokerRequestBatch(controller: KafkaController) extends  Logging\n     try {\n       leaderAndIsrRequestMap.foreach { case (broker, partitionStateInfos) =>\n         partitionStateInfos.foreach { case (topicPartition, state) =>\n-          val typeOfRequest = if (broker == state.leaderIsrAndControllerEpoch.leaderAndIsr.leader) \"become-leader\" else \"become-follower\"\n+          val typeOfRequest =\n+            if (broker == state.leaderIsrAndControllerEpoch.leaderAndIsr.leader) \"become-leader\"\n+            else \"become-follower\"\n           stateChangeLogger.trace((\"Controller %d epoch %d sending %s LeaderAndIsr request %s to broker %d \" +\n                                    \"for partition [%s,%d]\").format(controllerId, controllerEpoch, typeOfRequest,\n                                                                    state.leaderIsrAndControllerEpoch, broker,", "filename": "core/src/main/scala/kafka/controller/ControllerChannelManager.scala"}, {"additions": 50, "raw_url": "https://github.com/apache/kafka/raw/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/main/scala/kafka/controller/KafkaController.scala", "blob_url": "https://github.com/apache/kafka/blob/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/main/scala/kafka/controller/KafkaController.scala", "sha": "956d1caba126464a30371de3df62353bfc65c9bf", "changes": 113, "status": "modified", "deletions": 63, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/controller/KafkaController.scala?ref=6021618f9dafa3478104575d307e7bcd2cb4cca9", "patch": "@@ -146,18 +146,20 @@ object KafkaController extends Logging {\n   }\n }\n \n-class KafkaController(val config: KafkaConfig, zkUtils: ZkUtils, val brokerState: BrokerState, time: Time, metrics: Metrics, threadNamePrefix: Option[String] = None) extends Logging with KafkaMetricsGroup {\n+class KafkaController(val config: KafkaConfig, zkUtils: ZkUtils, time: Time, metrics: Metrics, threadNamePrefix: Option[String] = None) extends Logging with KafkaMetricsGroup {\n   this.logIdent = \"[Controller \" + config.brokerId + \"]: \"\n   private val stateChangeLogger = KafkaController.stateChangeLogger\n   val controllerContext = new ControllerContext(zkUtils)\n   val partitionStateMachine = new PartitionStateMachine(this)\n   val replicaStateMachine = new ReplicaStateMachine(this)\n \n-  // have a separate scheduler for the controller to be able to start and stop independently of the\n-  // kafka server\n-  private val kafkaScheduler = new KafkaScheduler(1)\n+  // have a separate scheduler for the controller to be able to start and stop independently of the kafka server\n+  // visible for testing\n+  private[controller] val kafkaScheduler = new KafkaScheduler(1)\n \n-  private val eventManager = new ControllerEventManager(controllerContext.stats.rateAndTimeMetrics, _ => updateMetrics())\n+  // visible for testing\n+  private[controller] val eventManager = new ControllerEventManager(controllerContext.stats.rateAndTimeMetrics,\n+    _ => updateMetrics())\n \n   val topicDeletionManager = new TopicDeletionManager(this, eventManager)\n   val offlinePartitionSelector = new OfflinePartitionLeaderSelector(controllerContext, config)\n@@ -290,7 +292,6 @@ class KafkaController(val config: KafkaConfig, zkUtils: ZkUtils, val brokerState\n   /**\n    * This callback is invoked by the zookeeper leader elector when the current broker resigns as the controller. This is\n    * required to clean up internal controller data structures\n-   * Note:We need to resign as a controller out of the controller lock to avoid potential deadlock issue\n    */\n   def onControllerResignation() {\n     debug(\"Controller resigning, broker id %d\".format(config.brokerId))\n@@ -318,9 +319,7 @@ class KafkaController(val config: KafkaConfig, zkUtils: ZkUtils, val brokerState\n     replicaStateMachine.shutdown()\n     deregisterBrokerChangeListener()\n \n-    // reset controller context\n     resetControllerContext()\n-    brokerState.newState(RunningAsBroker)\n \n     info(\"Broker %d resigned as the controller\".format(config.brokerId))\n   }\n@@ -746,18 +745,15 @@ class KafkaController(val config: KafkaConfig, zkUtils: ZkUtils, val brokerState\n   }\n \n   def updateLeaderAndIsrCache(topicAndPartitions: Set[TopicAndPartition] = controllerContext.partitionReplicaAssignment.keySet) {\n-    val leaderAndIsrInfo = zkUtils.getPartitionLeaderAndIsrForTopics(zkUtils.zkClient, topicAndPartitions)\n-    for((topicPartition, leaderIsrAndControllerEpoch) <- leaderAndIsrInfo)\n+    val leaderAndIsrInfo = zkUtils.getPartitionLeaderAndIsrForTopics(topicAndPartitions)\n+    for ((topicPartition, leaderIsrAndControllerEpoch) <- leaderAndIsrInfo)\n       controllerContext.partitionLeadershipInfo.put(topicPartition, leaderIsrAndControllerEpoch)\n   }\n \n   private def areReplicasInIsr(topic: String, partition: Int, replicas: Seq[Int]): Boolean = {\n-    zkUtils.getLeaderAndIsrForPartition(topic, partition) match {\n-      case Some(leaderAndIsr) =>\n-        val replicasNotInIsr = replicas.filterNot(r => leaderAndIsr.isr.contains(r))\n-        replicasNotInIsr.isEmpty\n-      case None => false\n-    }\n+    zkUtils.getLeaderAndIsrForPartition(topic, partition).map { leaderAndIsr =>\n+      replicas.forall(leaderAndIsr.isr.contains)\n+    }.getOrElse(false)\n   }\n \n   private def moveReassignedPartitionLeaderIfRequired(topicAndPartition: TopicAndPartition,\n@@ -824,22 +820,16 @@ class KafkaController(val config: KafkaConfig, zkUtils: ZkUtils, val brokerState\n   }\n \n   private def updateLeaderEpochAndSendRequest(topicAndPartition: TopicAndPartition, replicasToReceiveRequest: Seq[Int], newAssignedReplicas: Seq[Int]) {\n-    brokerRequestBatch.newBatch()\n     updateLeaderEpoch(topicAndPartition.topic, topicAndPartition.partition) match {\n       case Some(updatedLeaderIsrAndControllerEpoch) =>\n         try {\n+          brokerRequestBatch.newBatch()\n           brokerRequestBatch.addLeaderAndIsrRequestForBrokers(replicasToReceiveRequest, topicAndPartition.topic,\n             topicAndPartition.partition, updatedLeaderIsrAndControllerEpoch, newAssignedReplicas)\n           brokerRequestBatch.sendRequestsToBrokers(controllerContext.epoch)\n         } catch {\n-          case e : IllegalStateException => {\n-            // Resign if the controller is in an illegal state\n-            error(\"Forcing the controller to resign\")\n-            brokerRequestBatch.clear()\n-            triggerControllerMove()\n-\n-            throw e\n-          }\n+          case e: IllegalStateException =>\n+            handleIllegalState(e)\n         }\n         stateChangeLogger.trace((\"Controller %d epoch %d sent LeaderAndIsr request %s with new assigned replica list %s \" +\n           \"to leader %d for partition being reassigned %s\").format(config.brokerId, controllerContext.epoch, updatedLeaderIsrAndControllerEpoch,\n@@ -986,14 +976,8 @@ class KafkaController(val config: KafkaConfig, zkUtils: ZkUtils, val brokerState\n       brokerRequestBatch.addUpdateMetadataRequestForBrokers(brokers, partitions)\n       brokerRequestBatch.sendRequestsToBrokers(epoch)\n     } catch {\n-      case e : IllegalStateException => {\n-        // Resign if the controller is in an illegal state\n-        error(\"Forcing the controller to resign\")\n-        brokerRequestBatch.clear()\n-        triggerControllerMove()\n-\n-        throw e\n-      }\n+      case e: IllegalStateException =>\n+        handleIllegalState(e)\n     }\n   }\n \n@@ -1425,39 +1409,32 @@ class KafkaController(val config: KafkaConfig, zkUtils: ZkUtils, val brokerState\n           controllerContext.partitionsOnBroker(id)\n             .map(topicAndPartition => (topicAndPartition, controllerContext.partitionReplicaAssignment(topicAndPartition).size))\n \n-      allPartitionsAndReplicationFactorOnBroker.foreach {\n-        case(topicAndPartition, replicationFactor) =>\n-          controllerContext.partitionLeadershipInfo.get(topicAndPartition).foreach { currLeaderIsrAndControllerEpoch =>\n-            if (replicationFactor > 1) {\n-              if (currLeaderIsrAndControllerEpoch.leaderAndIsr.leader == id) {\n-                // If the broker leads the topic partition, transition the leader and update isr. Updates zk and\n-                // notifies all affected brokers\n-                partitionStateMachine.handleStateChanges(Set(topicAndPartition), OnlinePartition,\n-                  controlledShutdownPartitionLeaderSelector)\n-              } else {\n-                // Stop the replica first. The state change below initiates ZK changes which should take some time\n-                // before which the stop replica request should be completed (in most cases)\n-                try {\n-                  brokerRequestBatch.newBatch()\n-                  brokerRequestBatch.addStopReplicaRequestForBrokers(Seq(id), topicAndPartition.topic,\n-                    topicAndPartition.partition, deletePartition = false)\n-                  brokerRequestBatch.sendRequestsToBrokers(epoch)\n-                } catch {\n-                  case e : IllegalStateException => {\n-                    // Resign if the controller is in an illegal state\n-                    error(\"Forcing the controller to resign\")\n-                    brokerRequestBatch.clear()\n-                    triggerControllerMove()\n-\n-                    throw e\n-                  }\n-                }\n-                // If the broker is a follower, updates the isr in ZK and notifies the current leader\n-                replicaStateMachine.handleStateChanges(Set(PartitionAndReplica(topicAndPartition.topic,\n-                  topicAndPartition.partition, id)), OfflineReplica)\n+      allPartitionsAndReplicationFactorOnBroker.foreach { case (topicAndPartition, replicationFactor) =>\n+        controllerContext.partitionLeadershipInfo.get(topicAndPartition).foreach { currLeaderIsrAndControllerEpoch =>\n+          if (replicationFactor > 1) {\n+            if (currLeaderIsrAndControllerEpoch.leaderAndIsr.leader == id) {\n+              // If the broker leads the topic partition, transition the leader and update isr. Updates zk and\n+              // notifies all affected brokers\n+              partitionStateMachine.handleStateChanges(Set(topicAndPartition), OnlinePartition,\n+                controlledShutdownPartitionLeaderSelector)\n+            } else {\n+              // Stop the replica first. The state change below initiates ZK changes which should take some time\n+              // before which the stop replica request should be completed (in most cases)\n+              try {\n+                brokerRequestBatch.newBatch()\n+                brokerRequestBatch.addStopReplicaRequestForBrokers(Seq(id), topicAndPartition.topic,\n+                  topicAndPartition.partition, deletePartition = false)\n+                brokerRequestBatch.sendRequestsToBrokers(epoch)\n+              } catch {\n+                case e: IllegalStateException =>\n+                  handleIllegalState(e)\n               }\n+              // If the broker is a follower, updates the isr in ZK and notifies the current leader\n+              replicaStateMachine.handleStateChanges(Set(PartitionAndReplica(topicAndPartition.topic,\n+                topicAndPartition.partition, id)), OfflineReplica)\n             }\n           }\n+        }\n       }\n       def replicatedPartitionsBrokerLeads() = {\n         trace(\"All leaders = \" + controllerContext.partitionLeadershipInfo.mkString(\",\"))\n@@ -1557,7 +1534,17 @@ class KafkaController(val config: KafkaConfig, zkUtils: ZkUtils, val brokerState\n       }\n   }\n \n+  // visible for testing\n+  private[controller] def handleIllegalState(e: IllegalStateException): Nothing = {\n+    // Resign if the controller is in an illegal state\n+    error(\"Forcing the controller to resign\")\n+    brokerRequestBatch.clear()\n+    triggerControllerMove()\n+    throw e\n+  }\n+\n   private def triggerControllerMove(): Unit = {\n+    onControllerResignation()\n     activeControllerId = -1\n     controllerContext.zkUtils.deletePath(ZkUtils.ControllerPath)\n   }", "filename": "core/src/main/scala/kafka/controller/KafkaController.scala"}, {"additions": 3, "raw_url": "https://github.com/apache/kafka/raw/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/main/scala/kafka/controller/PartitionStateMachine.scala", "blob_url": "https://github.com/apache/kafka/blob/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/main/scala/kafka/controller/PartitionStateMachine.scala", "sha": "5751e17de0c05b987b5baed76f1707afec51b699", "changes": 6, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/controller/PartitionStateMachine.scala?ref=6021618f9dafa3478104575d307e7bcd2cb4cca9", "patch": "@@ -79,9 +79,9 @@ class PartitionStateMachine(controller: KafkaController) extends Logging {\n       brokerRequestBatch.newBatch()\n       // try to move all partitions in NewPartition or OfflinePartition state to OnlinePartition state except partitions\n       // that belong to topics to be deleted\n-      for((topicAndPartition, partitionState) <- partitionState\n+      for ((topicAndPartition, partitionState) <- partitionState\n           if !controller.topicDeletionManager.isTopicQueuedUpForDeletion(topicAndPartition.topic)) {\n-        if(partitionState.equals(OfflinePartition) || partitionState.equals(NewPartition))\n+        if (partitionState.equals(OfflinePartition) || partitionState.equals(NewPartition))\n           handleStateChange(topicAndPartition.topic, topicAndPartition.partition, OnlinePartition, controller.offlinePartitionSelector,\n                             (new CallbackBuilder).build)\n       }\n@@ -111,7 +111,7 @@ class PartitionStateMachine(controller: KafkaController) extends Logging {\n         handleStateChange(topicAndPartition.topic, topicAndPartition.partition, targetState, leaderSelector, callbacks)\n       }\n       brokerRequestBatch.sendRequestsToBrokers(controller.epoch)\n-    }catch {\n+    } catch {\n       case e: Throwable => error(\"Error while moving some partitions to %s state\".format(targetState), e)\n       // TODO: It is not enough to bail out and log an error, it is important to trigger state changes for those partitions\n     }", "filename": "core/src/main/scala/kafka/controller/PartitionStateMachine.scala"}, {"additions": 2, "raw_url": "https://github.com/apache/kafka/raw/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/main/scala/kafka/controller/ReplicaStateMachine.scala", "blob_url": "https://github.com/apache/kafka/blob/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/main/scala/kafka/controller/ReplicaStateMachine.scala", "sha": "60b99908d5635543e5b986b85539856731d195e0", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/controller/ReplicaStateMachine.scala?ref=6021618f9dafa3478104575d307e7bcd2cb4cca9", "patch": "@@ -81,13 +81,13 @@ class ReplicaStateMachine(controller: KafkaController) extends Logging {\n    */\n   def handleStateChanges(replicas: Set[PartitionAndReplica], targetState: ReplicaState,\n                          callbacks: Callbacks = (new CallbackBuilder).build) {\n-    if(replicas.nonEmpty) {\n+    if (replicas.nonEmpty) {\n       info(\"Invoking state change to %s for replicas %s\".format(targetState, replicas.mkString(\",\")))\n       try {\n         brokerRequestBatch.newBatch()\n         replicas.foreach(r => handleStateChange(r, targetState, callbacks))\n         brokerRequestBatch.sendRequestsToBrokers(controller.epoch)\n-      }catch {\n+      } catch {\n         case e: Throwable => error(\"Error while moving some replicas to %s state\".format(targetState), e)\n       }\n     }", "filename": "core/src/main/scala/kafka/controller/ReplicaStateMachine.scala"}, {"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/main/scala/kafka/server/KafkaServer.scala", "blob_url": "https://github.com/apache/kafka/blob/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/main/scala/kafka/server/KafkaServer.scala", "sha": "0a87750d9495b3365d1e8cad4f21aef4f2c8fed4", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/server/KafkaServer.scala?ref=6021618f9dafa3478104575d307e7bcd2cb4cca9", "patch": "@@ -226,7 +226,7 @@ class KafkaServer(val config: KafkaConfig, time: Time = Time.SYSTEM, threadNameP\n         replicaManager.startup()\n \n         /* start kafka controller */\n-        kafkaController = new KafkaController(config, zkUtils, brokerState, time, metrics, threadNamePrefix)\n+        kafkaController = new KafkaController(config, zkUtils, time, metrics, threadNamePrefix)\n         kafkaController.startup()\n \n         adminManager = new AdminManager(config, metrics, metadataCache, zkUtils)", "filename": "core/src/main/scala/kafka/server/KafkaServer.scala"}, {"additions": 1, "raw_url": "https://github.com/apache/kafka/raw/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/main/scala/kafka/utils/ZkUtils.scala", "blob_url": "https://github.com/apache/kafka/blob/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/main/scala/kafka/utils/ZkUtils.scala", "sha": "ac497c4eb9139d68d8f4b3ab2ed94f56ad050571", "changes": 3, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/main/scala/kafka/utils/ZkUtils.scala?ref=6021618f9dafa3478104575d307e7bcd2cb4cca9", "patch": "@@ -701,8 +701,7 @@ class ZkUtils(val zkClient: ZkClient,\n     cluster\n   }\n \n-  def getPartitionLeaderAndIsrForTopics(zkClient: ZkClient, topicAndPartitions: Set[TopicAndPartition])\n-  : mutable.Map[TopicAndPartition, LeaderIsrAndControllerEpoch] = {\n+  def getPartitionLeaderAndIsrForTopics(topicAndPartitions: Set[TopicAndPartition]): mutable.Map[TopicAndPartition, LeaderIsrAndControllerEpoch] = {\n     val ret = new mutable.HashMap[TopicAndPartition, LeaderIsrAndControllerEpoch]\n     for(topicAndPartition <- topicAndPartitions) {\n       ReplicationUtils.getLeaderIsrAndEpochForPartition(this, topicAndPartition.topic, topicAndPartition.partition) match {", "filename": "core/src/main/scala/kafka/utils/ZkUtils.scala"}, {"additions": 3, "raw_url": "https://github.com/apache/kafka/raw/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/test/scala/unit/kafka/controller/ControllerEventManagerTest.scala", "blob_url": "https://github.com/apache/kafka/blob/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/test/scala/unit/kafka/controller/ControllerEventManagerTest.scala", "sha": "ec9343e042560242b61453e2f97dd04c1b7d1632", "changes": 15, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/controller/ControllerEventManagerTest.scala?ref=6021618f9dafa3478104575d307e7bcd2cb4cca9", "patch": "@@ -23,7 +23,6 @@ import java.util.concurrent.atomic.AtomicInteger\n import com.yammer.metrics.Metrics\n import com.yammer.metrics.core.Timer\n import kafka.utils.TestUtils\n-import org.easymock.{EasyMock, IAnswer}\n import org.junit.{After, Test}\n import org.junit.Assert.{assertEquals, fail}\n \n@@ -60,21 +59,13 @@ class ControllerEventManagerTest {\n \n     val initialTimerCount = timer(metricName).count\n \n-    // `ControllerEvent` is sealed so we use EasyMock to create a subclass\n-    val eventMock = EasyMock.createMock(classOf[ControllerEvent])\n-    EasyMock.expect(eventMock.state).andReturn(controllerState)\n-\n     // Only return from `process()` once we have checked `controllerEventManager.state`\n     val latch = new CountDownLatch(1)\n-    EasyMock.expect(eventMock.process()).andAnswer(new IAnswer[Unit]() {\n-      def answer(): Unit = {\n-        latch.await()\n-        process()\n-      }\n+    val eventMock = ControllerTestUtils.createMockControllerEvent(controllerState, { () =>\n+      latch.await()\n+      process()\n     })\n \n-    EasyMock.replay(eventMock)\n-\n     controllerEventManager.put(eventMock)\n     TestUtils.waitUntilTrue(() => controllerEventManager.state == controllerState,\n       s\"Controller state is not $controllerState\")", "filename": "core/src/test/scala/unit/kafka/controller/ControllerEventManagerTest.scala"}, {"additions": 36, "raw_url": "https://github.com/apache/kafka/raw/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/test/scala/unit/kafka/controller/ControllerFailoverTest.scala", "blob_url": "https://github.com/apache/kafka/blob/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/test/scala/unit/kafka/controller/ControllerFailoverTest.scala", "sha": "7a91bef570d788fd32bdfb23ad72c4bf1ee15556", "changes": 148, "status": "modified", "deletions": 112, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/controller/ControllerFailoverTest.scala?ref=6021618f9dafa3478104575d307e7bcd2cb4cca9", "patch": "@@ -18,18 +18,17 @@\n package kafka.controller\n \n import java.util.Properties\n-import java.util.concurrent.LinkedBlockingQueue\n+import java.util.concurrent.CountDownLatch\n \n+import kafka.admin.AdminUtils\n import kafka.common.TopicAndPartition\n import kafka.integration.KafkaServerTestHarness\n-import kafka.server.{KafkaConfig, KafkaServer}\n+import kafka.server.KafkaConfig\n import kafka.utils._\n import org.apache.kafka.common.metrics.Metrics\n-import org.apache.kafka.common.utils.Time\n-import org.apache.log4j.{Level, Logger}\n-import org.junit.{After, Ignore, Test}\n-\n-import scala.collection.mutable\n+import org.apache.log4j.Logger\n+import org.junit.{After, Test}\n+import org.junit.Assert._\n \n class ControllerFailoverTest extends KafkaServerTestHarness with Logging {\n   val log = Logger.getLogger(classOf[ControllerFailoverTest])\n@@ -54,119 +53,44 @@ class ControllerFailoverTest extends KafkaServerTestHarness with Logging {\n    * See @link{https://issues.apache.org/jira/browse/KAFKA-2300}\n    * for the background of this test case\n    */\n-  @Ignore // This needs to be reworked as described here: https://github.com/apache/kafka/pull/2935#discussion_r114374412\n   @Test\n-  def testMetadataUpdate() {\n-    log.setLevel(Level.INFO)\n-    var controller: KafkaServer = this.servers.head\n-    // Find the current controller\n-    val epochMap: mutable.Map[Int, Int] = mutable.Map.empty\n-    for (server <- this.servers) {\n-      epochMap += (server.config.brokerId -> server.kafkaController.epoch)\n-      if(server.kafkaController.isActive) {\n-        controller = server\n-      }\n+  def testHandleIllegalStateException() {\n+    val initialController = servers.find(_.kafkaController.isActive).map(_.kafkaController).getOrElse {\n+      fail(\"Could not find controller\")\n     }\n+    val initialEpoch = initialController.epoch\n     // Create topic with one partition\n-    kafka.admin.AdminUtils.createTopic(controller.zkUtils, topic, 1, 1)\n+    AdminUtils.createTopic(servers.head.zkUtils, topic, 1, 1)\n     val topicPartition = TopicAndPartition(\"topic1\", 0)\n-    var partitions = controller.kafkaController.partitionStateMachine.partitionsInState(OnlinePartition)\n-    while (!partitions.contains(topicPartition)) {\n-      partitions = controller.kafkaController.partitionStateMachine.partitionsInState(OnlinePartition)\n-      Thread.sleep(100)\n-    }\n-    // Replace channel manager with our mock manager\n-    controller.kafkaController.controllerContext.controllerChannelManager.shutdown()\n-    val channelManager = new MockChannelManager(controller.kafkaController.controllerContext, \n-                                                  controller.kafkaController.config, metrics)\n-    channelManager.startup()\n-    controller.kafkaController.controllerContext.controllerChannelManager = channelManager\n-    channelManager.shrinkBlockingQueue(0)\n-    channelManager.stopSendThread(0)\n-    // Spawn a new thread to block on the outgoing channel\n-    // queue\n-    val thread = new Thread(new Runnable {\n-      def run() {\n-        try {\n-          controller.kafkaController.sendUpdateMetadataRequest(Seq(0), Set(topicPartition))\n-          log.info(\"Queue state %d %d\".format(channelManager.queueCapacity(0), channelManager.queueSize(0)))\n-          controller.kafkaController.sendUpdateMetadataRequest(Seq(0), Set(topicPartition))\n-          log.info(\"Queue state %d %d\".format(channelManager.queueCapacity(0), channelManager.queueSize(0)))\n-        } catch {\n-          case _: Exception => log.info(\"Thread interrupted\")\n-        }\n+    TestUtils.waitUntilTrue(() =>\n+      initialController.partitionStateMachine.partitionsInState(OnlinePartition).contains(topicPartition),\n+      s\"Partition $topicPartition did not transition to online state\")\n+\n+    // Wait until we have verified that we have resigned\n+    val latch = new CountDownLatch(1)\n+    @volatile var expectedExceptionThrown = false\n+    @volatile var unexpectedExceptionThrown: Option[Throwable] = None\n+    val illegalStateEvent = ControllerTestUtils.createMockControllerEvent(ControllerState.BrokerChange, { () =>\n+      try initialController.handleIllegalState(new IllegalStateException(\"Thrown for test purposes\"))\n+      catch {\n+        case _: IllegalStateException => expectedExceptionThrown = true\n+        case t: Throwable => unexpectedExceptionThrown = Some(t)\n       }\n+      latch.await()\n     })\n-    thread.setName(\"mythread\")\n-    thread.start()\n-    while (thread.getState() != Thread.State.WAITING) {\n-      Thread.sleep(100)\n-    }\n-    // Assume that the thread is WAITING because it is\n-    // blocked on the queue, so interrupt and move forward\n-    thread.interrupt()\n-    thread.join()\n-    channelManager.resumeSendThread(0)\n-    // Wait and find current controller\n-    var found = false\n-    var counter = 0\n-    while (!found && counter < 10) {\n-      for (server <- this.servers) {\n-        val previousEpoch = epochMap get server.config.brokerId match {\n-          case Some(epoch) =>\n-            epoch\n-          case None =>\n-            val msg = String.format(\"Missing element in epoch map %s\", epochMap.mkString(\", \"))\n-            throw new IllegalStateException(msg)\n-        }\n+    initialController.eventManager.put(illegalStateEvent)\n+    // Check that we have shutdown the scheduler (via onControllerResigned)\n+    TestUtils.waitUntilTrue(() => !initialController.kafkaScheduler.isStarted, \"Scheduler was not shutdown\")\n+    TestUtils.waitUntilTrue(() => !initialController.isActive, \"Controller did not become inactive\")\n+    latch.countDown()\n+    assertTrue(\"IllegalStateException was not thrown\", expectedExceptionThrown)\n+    assertEquals(\"Unexpected exception thrown\", None, unexpectedExceptionThrown)\n \n-        if (server.kafkaController.isActive\n-            && previousEpoch < server.kafkaController.epoch) {\n-          controller = server\n-          found = true\n-        }\n+    TestUtils.waitUntilTrue(() => {\n+      servers.exists { server =>\n+        server.kafkaController.isActive && server.kafkaController.epoch > initialController.epoch\n       }\n-      if (!found) {\n-          Thread.sleep(100)\n-          counter += 1\n-      }\n-    }\n-    // Give it a shot to make sure that sending isn't blocking\n-    try {\n-      controller.kafkaController.sendUpdateMetadataRequest(Seq(0), Set(topicPartition))\n-    } catch {\n-      case e : Throwable => {\n-        fail(e)\n-      }\n-    }\n-  }\n-}\n-\n-class MockChannelManager(private val controllerContext: ControllerContext, config: KafkaConfig, metrics: Metrics)\n-  extends ControllerChannelManager(controllerContext, config, Time.SYSTEM, metrics) {\n-\n-  def stopSendThread(brokerId: Int) {\n-    val requestThread = brokerStateInfo(brokerId).requestSendThread\n-    requestThread.isRunning.set(false)\n-    requestThread.interrupt\n-    requestThread.join\n-  }\n-\n-  def shrinkBlockingQueue(brokerId: Int) {\n-    val messageQueue = new LinkedBlockingQueue[QueueItem](1)\n-    val brokerInfo = this.brokerStateInfo(brokerId)\n-    this.brokerStateInfo.put(brokerId, brokerInfo.copy(messageQueue = messageQueue))\n-  }\n-\n-  def resumeSendThread (brokerId: Int) {\n-    this.startRequestSendThread(0)\n-  }\n-\n-  def queueCapacity(brokerId: Int): Int = {\n-    this.brokerStateInfo(brokerId).messageQueue.remainingCapacity\n-  }\n+    }, \"Failed to find controller\")\n \n-  def queueSize(brokerId: Int): Int = {\n-    this.brokerStateInfo(brokerId).messageQueue.size\n   }\n }", "filename": "core/src/test/scala/unit/kafka/controller/ControllerFailoverTest.scala"}, {"additions": 3, "raw_url": "https://github.com/apache/kafka/raw/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/test/scala/unit/kafka/controller/ControllerIntegrationTest.scala", "blob_url": "https://github.com/apache/kafka/blob/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/test/scala/unit/kafka/controller/ControllerIntegrationTest.scala", "sha": "d5f2fe0a2d612c654e5de6b8ae862e5ad953de0f", "changes": 6, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/controller/ControllerIntegrationTest.scala?ref=6021618f9dafa3478104575d307e7bcd2cb4cca9", "patch": "@@ -264,7 +264,7 @@ class ControllerIntegrationTest extends ZooKeeperTestHarness {\n     servers(otherBrokerId).shutdown()\n     servers(otherBrokerId).awaitShutdown()\n     TestUtils.waitUntilTrue(() => {\n-      val leaderIsrAndControllerEpochMap = zkUtils.getPartitionLeaderAndIsrForTopics(null, Set(tp))\n+      val leaderIsrAndControllerEpochMap = zkUtils.getPartitionLeaderAndIsrForTopics(Set(tp))\n       leaderIsrAndControllerEpochMap.contains(tp) &&\n         isExpectedPartitionState(leaderIsrAndControllerEpochMap(tp), KafkaController.InitialControllerEpoch, LeaderAndIsr.NoLeader, LeaderAndIsr.initialLeaderEpoch + 1) &&\n         leaderIsrAndControllerEpochMap(tp).leaderAndIsr.isr == List(otherBrokerId)\n@@ -284,7 +284,7 @@ class ControllerIntegrationTest extends ZooKeeperTestHarness {\n     servers(1).shutdown()\n     servers(1).awaitShutdown()\n     TestUtils.waitUntilTrue(() => {\n-      val leaderIsrAndControllerEpochMap = zkUtils.getPartitionLeaderAndIsrForTopics(null, Set(tp))\n+      val leaderIsrAndControllerEpochMap = zkUtils.getPartitionLeaderAndIsrForTopics(Set(tp))\n       leaderIsrAndControllerEpochMap.contains(tp) &&\n         isExpectedPartitionState(leaderIsrAndControllerEpochMap(tp), KafkaController.InitialControllerEpoch, LeaderAndIsr.NoLeader, LeaderAndIsr.initialLeaderEpoch + 1) &&\n         leaderIsrAndControllerEpochMap(tp).leaderAndIsr.isr == List.empty\n@@ -301,7 +301,7 @@ class ControllerIntegrationTest extends ZooKeeperTestHarness {\n                                     leaderEpoch: Int,\n                                     message: String): Unit = {\n     TestUtils.waitUntilTrue(() => {\n-      val leaderIsrAndControllerEpochMap = zkUtils.getPartitionLeaderAndIsrForTopics(null, Set(tp))\n+      val leaderIsrAndControllerEpochMap = zkUtils.getPartitionLeaderAndIsrForTopics(Set(tp))\n       leaderIsrAndControllerEpochMap.contains(tp) &&\n         isExpectedPartitionState(leaderIsrAndControllerEpochMap(tp), controllerEpoch, leader, leaderEpoch)\n     }, message)", "filename": "core/src/test/scala/unit/kafka/controller/ControllerIntegrationTest.scala"}, {"additions": 35, "raw_url": "https://github.com/apache/kafka/raw/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/test/scala/unit/kafka/controller/ControllerTestUtils.scala", "blob_url": "https://github.com/apache/kafka/blob/6021618f9dafa3478104575d307e7bcd2cb4cca9/core/src/test/scala/unit/kafka/controller/ControllerTestUtils.scala", "sha": "407297a87131427d0077d3720ef62ff4f713c1b9", "changes": 35, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/core/src/test/scala/unit/kafka/controller/ControllerTestUtils.scala?ref=6021618f9dafa3478104575d307e7bcd2cb4cca9", "patch": "@@ -0,0 +1,35 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements. See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License. You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package kafka.controller\n+\n+import org.easymock.{EasyMock, IAnswer}\n+\n+object ControllerTestUtils {\n+\n+  /** Since ControllerEvent is sealed, return a subclass of ControllerEvent created with EasyMock */\n+  def createMockControllerEvent(controllerState: ControllerState, process: () => Unit): ControllerEvent = {\n+    val mockEvent = EasyMock.createMock(classOf[ControllerEvent])\n+    EasyMock.expect(mockEvent.state).andReturn(controllerState)\n+    EasyMock.expect(mockEvent.process()).andAnswer(new IAnswer[Unit]() {\n+      def answer(): Unit = {\n+        process()\n+      }\n+    })\n+    EasyMock.replay(mockEvent)\n+    mockEvent\n+  }\n+}", "filename": "core/src/test/scala/unit/kafka/controller/ControllerTestUtils.scala"}], "repo": "kafka"}, {"commit": "https://github.com/apache/kafka/commit/508711d09e7c6070868aadfff1b2627e77f7f3d9", "parent": "https://github.com/apache/kafka/commit/2959bc2ad382fa786bd6209fe742b8218edfa4a8", "message": "MINOR: Fix NPE when Connect offset contains non-primitive type\n\nWhen storing a non-primitive type in a Connect offset, the following NullPointerException will occur:\n\n```\n07:18:23.702 [pool-3-thread-1] ERROR o.a.k.c.storage.OffsetStorageWriter - CRITICAL: Failed to serialize offset data, making it impossible to commit offsets under namespace tenant-db-bootstrap-source. This likely won't recover unless the unserializable partition or offset information is overwritten.\n07:18:23.702 [pool-3-thread-1] ERROR o.a.k.c.storage.OffsetStorageWriter - Cause of serialization failure:\njava.lang.NullPointerException: null\n\tat org.apache.kafka.connect.storage.OffsetUtils.validateFormat(OffsetUtils.java:51)\n\tat org.apache.kafka.connect.storage.OffsetStorageWriter.doFlush(OffsetStorageWriter.java:143)\n\tat org.apache.kafka.connect.runtime.WorkerSourceTask.commitOffsets(WorkerSourceTask.java:319)\n... snip ...\n```\n\nThe attached patch fixes the specific case where OffsetUtils.validateFormat is attempting to provide a useful error message, but fails to because the schemaType method could return null.\n\nThis contribution is my original work and I license the work to the project under the project's open source license.\n\nAuthor: Mathieu Fenniak <mathieu.fenniak@replicon.com>\n\nReviewers: Gwen Shapira\n\nCloses #2087 from mfenniak/fix-npr-with-clearer-error-message", "bug_id": "kafka_76", "file": [{"additions": 2, "raw_url": "https://github.com/apache/kafka/raw/508711d09e7c6070868aadfff1b2627e77f7f3d9/connect/runtime/src/main/java/org/apache/kafka/connect/storage/OffsetUtils.java", "blob_url": "https://github.com/apache/kafka/blob/508711d09e7c6070868aadfff1b2627e77f7f3d9/connect/runtime/src/main/java/org/apache/kafka/connect/storage/OffsetUtils.java", "sha": "9fdcfc3ee9c4bce50bdafe985fbf1490613bb508", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/kafka/contents/connect/runtime/src/main/java/org/apache/kafka/connect/storage/OffsetUtils.java?ref=508711d09e7c6070868aadfff1b2627e77f7f3d9", "patch": "@@ -48,6 +48,8 @@ public static void validateFormat(Object offsetData) {\n             if (value == null)\n                 continue;\n             Schema.Type schemaType = ConnectSchema.schemaType(value.getClass());\n+            if (schemaType == null)\n+                throw new DataException(\"Offsets may only contain primitive types as values, but field \" + entry.getKey() + \" contains \" + value.getClass());\n             if (!schemaType.isPrimitive())\n                 throw new DataException(\"Offsets may only contain primitive types as values, but field \" + entry.getKey() + \" contains \" + schemaType);\n         }", "filename": "connect/runtime/src/main/java/org/apache/kafka/connect/storage/OffsetUtils.java"}], "repo": "kafka"}]
