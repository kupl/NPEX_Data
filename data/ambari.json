{
    "ambari_004e4f9": {
        "bug_id": "ambari_004e4f9",
        "commit": "https://github.com/apache/ambari/commit/004e4f98e7d20dbecdd31a7b889f55bfd1008897",
        "file": [
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/ambari/blob/004e4f98e7d20dbecdd31a7b889f55bfd1008897/ambari-server/src/main/java/org/apache/ambari/server/serveraction/upgrades/ConfigureAction.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/serveraction/upgrades/ConfigureAction.java?ref=004e4f98e7d20dbecdd31a7b889f55bfd1008897",
                "deletions": 11,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/serveraction/upgrades/ConfigureAction.java",
                "patch": "@@ -45,11 +45,11 @@\n import org.apache.ambari.server.state.DesiredConfig;\n import org.apache.ambari.server.state.PropertyInfo;\n import org.apache.ambari.server.state.StackId;\n-import org.apache.ambari.server.state.stack.upgrade.ConfigureTask;\n import org.apache.ambari.server.state.stack.upgrade.ConfigUpgradeChangeDefinition.ConfigurationKeyValue;\n-import org.apache.ambari.server.state.stack.upgrade.ConfigUpgradeChangeDefinition.Transfer;\n-import org.apache.ambari.server.state.stack.upgrade.ConfigUpgradeChangeDefinition.Replace;\n import org.apache.ambari.server.state.stack.upgrade.ConfigUpgradeChangeDefinition.Masked;\n+import org.apache.ambari.server.state.stack.upgrade.ConfigUpgradeChangeDefinition.Replace;\n+import org.apache.ambari.server.state.stack.upgrade.ConfigUpgradeChangeDefinition.Transfer;\n+import org.apache.ambari.server.state.stack.upgrade.ConfigureTask;\n import org.apache.commons.lang.StringUtils;\n \n import com.google.gson.Gson;\n@@ -404,9 +404,10 @@ public CommandReport execute(\n \n     // !!! string replacements happen only on the new values.\n     for (Replace replacement : replacements) {\n-      if (newValues.containsKey(replacement.key)) {\n-        String toReplace = newValues.get(replacement.key);\n-\n+      // the key might exist but might be null, so we need to check this\n+      // condition when replacing a part of the value\n+      String toReplace = newValues.get(replacement.key);\n+      if (StringUtils.isNotBlank(toReplace)) {\n         if (!toReplace.contains(replacement.find)) {\n           outputBuffer.append(MessageFormat.format(\"String \\\"{0}\\\" was not found in {1}/{2}\\n\",\n               replacement.find, configType, replacement.key));\n@@ -415,16 +416,20 @@ public CommandReport execute(\n \n           newValues.put(replacement.key, replaced);\n \n-          outputBuffer.append(MessageFormat.format(\"Replaced {0}/{1} containing \\\"{2}\\\" with \\\"{3}\\\"\\n\",\n-            configType, replacement.key, replacement.find, replacement.replaceWith));\n+          outputBuffer.append(\n+              MessageFormat.format(\"Replaced {0}/{1} containing \\\"{2}\\\" with \\\"{3}\\\"\", configType,\n+                  replacement.key, replacement.find, replacement.replaceWith));\n+\n+          outputBuffer.append(System.lineSeparator());\n         }\n       } else {\n-        outputBuffer.append(MessageFormat.format(\"Property \\\"{0}\\\" was not found in {1} to replace content\\n\",\n-            replacement.key, configType));\n+        outputBuffer.append(MessageFormat.format(\n+            \"Skipping replacement for {0}/{1} because it does not exist or is empty.\",\n+            configType, replacement.key));\n+        outputBuffer.append(System.lineSeparator());\n       }\n     }\n \n-\n     // !!! check to see if we're going to a new stack and double check the\n     // configs are for the target.  Then simply update the new properties instead\n     // of creating a whole new history record since it was already done",
                "raw_url": "https://github.com/apache/ambari/raw/004e4f98e7d20dbecdd31a7b889f55bfd1008897/ambari-server/src/main/java/org/apache/ambari/server/serveraction/upgrades/ConfigureAction.java",
                "sha": "706f9c679111a234df115525407d58c1a722ff71",
                "status": "modified"
            },
            {
                "additions": 70,
                "blob_url": "https://github.com/apache/ambari/blob/004e4f98e7d20dbecdd31a7b889f55bfd1008897/ambari-server/src/test/java/org/apache/ambari/server/serveraction/upgrades/ConfigureActionTest.java",
                "changes": 72,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/serveraction/upgrades/ConfigureActionTest.java?ref=004e4f98e7d20dbecdd31a7b889f55bfd1008897",
                "deletions": 2,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/serveraction/upgrades/ConfigureActionTest.java",
                "patch": "@@ -55,8 +55,10 @@\n import org.apache.ambari.server.state.Service;\n import org.apache.ambari.server.state.ServiceFactory;\n import org.apache.ambari.server.state.StackId;\n+import org.apache.ambari.server.state.stack.upgrade.ConfigUpgradeChangeDefinition.ConfigurationKeyValue;\n+import org.apache.ambari.server.state.stack.upgrade.ConfigUpgradeChangeDefinition.Replace;\n+import org.apache.ambari.server.state.stack.upgrade.ConfigUpgradeChangeDefinition.Transfer;\n import org.apache.ambari.server.state.stack.upgrade.ConfigureTask;\n-import org.apache.ambari.server.state.stack.upgrade.ConfigUpgradeChangeDefinition.*;\n import org.apache.ambari.server.state.stack.upgrade.TransferCoercionType;\n import org.apache.ambari.server.state.stack.upgrade.TransferOperation;\n import org.junit.After;\n@@ -73,7 +75,7 @@\n  * Tests upgrade-related server side actions\n  */\n public class ConfigureActionTest {\n-  \n+\n   private static final String HDP_2_2_0_0 = \"2.2.0.0-2041\";\n   private static final String HDP_2_2_0_1 = \"2.2.0.1-2270\";\n   private static final StackId HDP_211_STACK = new StackId(\"HDP-2.1.1\");\n@@ -514,6 +516,72 @@ public void testValueReplacement() throws Exception {\n     assertEquals(\"WxyAndZ\", config.getProperties().get(\"key_with_no_match\"));\n   }\n \n+  /**\n+   * Tests that replacing a {@code null} value works.\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testValueReplacementWithMissingConfigurations() throws Exception {\n+    makeUpgradeCluster();\n+\n+    Cluster c = m_injector.getInstance(Clusters.class).getCluster(\"c1\");\n+    assertEquals(1, c.getConfigsByType(\"zoo.cfg\").size());\n+\n+    c.setDesiredStackVersion(HDP_220_STACK);\n+    ConfigFactory cf = m_injector.getInstance(ConfigFactory.class);\n+    Config config = cf.createNew(c, \"zoo.cfg\", new HashMap<String, String>() {\n+      {\n+        put(\"existing\", \"This exists!\");\n+        put(\"missing\", null);\n+      }\n+    }, new HashMap<String, Map<String, String>>());\n+    config.setTag(\"version2\");\n+    config.persist();\n+\n+    c.addConfig(config);\n+    c.addDesiredConfig(\"user\", Collections.singleton(config));\n+    assertEquals(2, c.getConfigsByType(\"zoo.cfg\").size());\n+\n+    Map<String, String> commandParams = new HashMap<String, String>();\n+    commandParams.put(\"upgrade_direction\", \"upgrade\");\n+    commandParams.put(\"version\", HDP_2_2_0_1);\n+    commandParams.put(\"clusterName\", \"c1\");\n+    commandParams.put(ConfigureTask.PARAMETER_CONFIG_TYPE, \"zoo.cfg\");\n+\n+    // Replacement task\n+    List<Replace> replacements = new ArrayList<Replace>();\n+    Replace replace = new Replace();\n+    replace.key = \"missing\";\n+    replace.find = \"foo\";\n+    replace.replaceWith = \"bar\";\n+    replacements.add(replace);\n+\n+    commandParams.put(ConfigureTask.PARAMETER_REPLACEMENTS, new Gson().toJson(replacements));\n+\n+    ExecutionCommand executionCommand = new ExecutionCommand();\n+    executionCommand.setCommandParams(commandParams);\n+    executionCommand.setClusterName(\"c1\");\n+    executionCommand.setRoleParams(new HashMap<String, String>());\n+    executionCommand.getRoleParams().put(ServerAction.ACTION_USER_NAME, \"username\");\n+\n+    HostRoleCommand hostRoleCommand = hostRoleCommandFactory.create(null, null, null, null);\n+\n+    hostRoleCommand.setExecutionCommandWrapper(new ExecutionCommandWrapper(executionCommand));\n+\n+    ConfigureAction action = m_injector.getInstance(ConfigureAction.class);\n+    action.setExecutionCommand(executionCommand);\n+    action.setHostRoleCommand(hostRoleCommand);\n+\n+    CommandReport report = action.execute(null);\n+    assertNotNull(report);\n+\n+    assertEquals(3, c.getConfigsByType(\"zoo.cfg\").size());\n+\n+    config = c.getDesiredConfigByType(\"zoo.cfg\");\n+    assertEquals(null, config.getProperties().get(\"missing\"));\n+  }\n+\n   @Test\n   public void testMultipleKeyValuesPerTask() throws Exception {\n     makeUpgradeCluster();",
                "raw_url": "https://github.com/apache/ambari/raw/004e4f98e7d20dbecdd31a7b889f55bfd1008897/ambari-server/src/test/java/org/apache/ambari/server/serveraction/upgrades/ConfigureActionTest.java",
                "sha": "d7a2ad9e4009f115983b640ae701dfda59d0f87d",
                "status": "modified"
            }
        ],
        "message": "AMBARI-13915 - Storm Upgrade Causes NPE Due To null Property Value Replacement (jonathanhurley)",
        "parent": "https://github.com/apache/ambari/commit/2d82360b1428da13379118a2f75f618a6beea9ca",
        "repo": "ambari",
        "unit_tests": [
            "ConfigureActionTest.java"
        ]
    },
    "ambari_080991d": {
        "bug_id": "ambari_080991d",
        "commit": "https://github.com/apache/ambari/commit/080991d131768ece5e0d7065df2bb8443e14053f",
        "file": [
            {
                "additions": 32,
                "blob_url": "https://github.com/apache/ambari/blob/080991d131768ece5e0d7065df2bb8443e14053f/ambari-server/src/main/java/org/apache/ambari/server/topology/ClusterConfigurationRequest.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/topology/ClusterConfigurationRequest.java?ref=080991d131768ece5e0d7065df2bb8443e14053f",
                "deletions": 3,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/topology/ClusterConfigurationRequest.java",
                "patch": "@@ -161,9 +161,8 @@ public void process() throws AmbariException, ConfigurationTopologyException {\n         Map<String, String> clusterConfigProperties = existingConfigurations.get(configType);\n         Map<String, String> stackDefaultConfigProperties = stackDefaultProps.get(configType);\n         for (String property : propertyMap.keySet()) {\n-          if (clusterConfigProperties == null || !clusterConfigProperties.containsKey(property)\n-                 || (clusterConfigProperties.get(property) == null && stackDefaultConfigProperties.get(property) == null)\n-                 || (clusterConfigProperties.get(property) != null && clusterConfigProperties.get(property).equals(stackDefaultConfigProperties.get(property)))) {\n+          // update value only if property value configured in Blueprint /ClusterTemplate is not a custom one\n+          if (!propertyHasCustomValue(clusterConfigProperties, stackDefaultConfigProperties, property)) {\n             LOG.debug(\"Update Kerberos related config property: {} {} {}\", configType, property, propertyMap.get\n               (property));\n             clusterConfiguration.setProperty(configType, property, propertyMap.get(property));\n@@ -179,6 +178,36 @@ public void process() throws AmbariException, ConfigurationTopologyException {\n     return updatedConfigTypes;\n   }\n \n+  /**\n+   * Returns true if the property exists in clusterConfigProperties and has a custom user defined value. Property has\n+   * custom value in case we there's no stack default value for it or it's not equal to stack default value.\n+   * @param clusterConfigProperties\n+   * @param stackDefaultConfigProperties\n+   * @param property\n+   * @return\n+   */\n+  private boolean propertyHasCustomValue(Map<String, String> clusterConfigProperties, Map<String, String>\n+    stackDefaultConfigProperties, String property) {\n+\n+    boolean propertyHasCustomValue = false;\n+    if (clusterConfigProperties != null) {\n+      String propertyValue = clusterConfigProperties.get(property);\n+      if (propertyValue != null) {\n+        if (stackDefaultConfigProperties != null) {\n+          String stackDefaultValue = stackDefaultConfigProperties.get(property);\n+          if (stackDefaultValue != null) {\n+            propertyHasCustomValue = !propertyValue.equals(stackDefaultValue);\n+          } else {\n+            propertyHasCustomValue = true;\n+          }\n+        } else {\n+          propertyHasCustomValue = true;\n+        }\n+      }\n+    }\n+    return propertyHasCustomValue;\n+  }\n+\n   private Map<String, String> createComponentHostMap(Blueprint blueprint) {\n     Map<String, String> componentHostsMap = new HashMap<String, String>();\n     for (String service : blueprint.getServices()) {",
                "raw_url": "https://github.com/apache/ambari/raw/080991d131768ece5e0d7065df2bb8443e14053f/ambari-server/src/main/java/org/apache/ambari/server/topology/ClusterConfigurationRequest.java",
                "sha": "70fa8801cf4166ffbe4c0e6933a7defc1affe1cc",
                "status": "modified"
            },
            {
                "additions": 93,
                "blob_url": "https://github.com/apache/ambari/blob/080991d131768ece5e0d7065df2bb8443e14053f/ambari-server/src/test/java/org/apache/ambari/server/topology/ClusterConfigurationRequestTest.java",
                "changes": 101,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/topology/ClusterConfigurationRequestTest.java?ref=080991d131768ece5e0d7065df2bb8443e14053f",
                "deletions": 8,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/topology/ClusterConfigurationRequestTest.java",
                "patch": "@@ -18,13 +18,18 @@\n \n package org.apache.ambari.server.topology;\n \n+import org.apache.ambari.server.AmbariException;\n import org.apache.ambari.server.api.services.stackadvisor.StackAdvisorBlueprintProcessor;\n import org.apache.ambari.server.controller.AmbariManagementController;\n import org.apache.ambari.server.controller.ConfigurationRequest;\n import org.apache.ambari.server.controller.KerberosHelper;\n+import org.apache.ambari.server.controller.internal.ConfigurationTopologyException;\n import org.apache.ambari.server.controller.internal.Stack;\n+import org.apache.ambari.server.serveraction.kerberos.KerberosInvalidConfigurationException;\n import org.apache.ambari.server.state.Cluster;\n import org.apache.ambari.server.state.Clusters;\n+import org.easymock.Capture;\n+import org.easymock.CaptureType;\n import org.easymock.EasyMockRule;\n import org.easymock.Mock;\n import org.easymock.MockType;\n@@ -49,7 +54,10 @@\n import static org.easymock.EasyMock.expect;\n import static org.easymock.EasyMock.expectLastCall;\n import static org.easymock.EasyMock.eq;\n+import static org.easymock.EasyMock.newCapture;\n import static org.easymock.EasyMock.verify;\n+import static org.easymock.EasyMock.capture;\n+import static org.junit.Assert.assertEquals;\n \n /**\n  * ClusterConfigurationRequest unit tests\n@@ -89,12 +97,82 @@\n   @Mock(type = MockType.NICE)\n   private KerberosHelper kerberosHelper;\n \n+  /**\n+   * testConfigType config type should be in updatedConfigTypes, as no custom property in Blueprint\n+   * ==> Kerberos config property should be updated\n+   * @throws Exception\n+   */\n   @Test\n-  public void testProcessClusterConfigRequestIncludeKererosConfigs() throws Exception {\n+  public void testProcessWithKerberos_UpdateKererosConfigProperty_WithNoCustomValue() throws Exception {\n+\n+    Capture<? extends Set<String>> captureUpdatedConfigTypes = testProcessWithKerberos(null, \"defaultTestValue\");\n+\n+    Set<String> updatedConfigTypes = captureUpdatedConfigTypes.getValue();\n+    assertEquals(2, updatedConfigTypes.size());\n+  }\n+\n+  /**\n+   * testConfigType config type should be in updatedConfigTypes, as testProperty in Blueprint is equal to stack\n+   * default ==> Kerberos config property should be updated\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testProcessWithKerberos_UpdateKererosConfigProperty_WithCustomValueEqualToStackDefault() throws\n+    Exception {\n+\n+    Capture<? extends Set<String>> captureUpdatedConfigTypes = testProcessWithKerberos(\"defaultTestValue\", \"defaultTestValue\");\n+\n+    Set<String> updatedConfigTypes = captureUpdatedConfigTypes.getValue();\n+    assertEquals(2, updatedConfigTypes.size());\n+\n+  }\n+\n+  /**\n+   * testConfigType config type shouldn't be in updatedConfigTypes, as testProperty in Blueprint is different that\n+   * stack default (custom value) ==> Kerberos config property shouldn't be updated\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testProcessWithKerberos_DontUpdateKererosConfigProperty_WithCustomValueDifferentThanStackDefault() throws\n+    Exception {\n+\n+    Capture<? extends Set<String>> captureUpdatedConfigTypes = testProcessWithKerberos(\"testPropertyValue\", \"defaultTestValue\");\n+\n+    Set<String> updatedConfigTypes = captureUpdatedConfigTypes.getValue();\n+    assertEquals(1, updatedConfigTypes.size());\n+  }\n+\n+  /**\n+   * testConfigType config type shouldn't be in updatedConfigTypes, as testProperty in Blueprint is a custom value\n+   * (no default value in stack for testProperty)\n+   * ==> Kerberos config property shouldn't be updated\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testProcessWithKerberos_DontUpdateKererosConfigProperty_WithCustomValueNoStackDefault() throws Exception {\n+\n+    Capture<? extends Set<String>> captureUpdatedConfigTypes = testProcessWithKerberos(\"testPropertyValue\", null);\n+\n+    Set<String> updatedConfigTypes = captureUpdatedConfigTypes.getValue();\n+    assertEquals(1, updatedConfigTypes.size());\n+  }\n+\n+  private Capture<? extends Set<String>> testProcessWithKerberos(String blueprintPropertyValue, String\n+    stackPropertyValue) throws AmbariException, KerberosInvalidConfigurationException, ConfigurationTopologyException {\n+\n \n     Map<String, Map<String, String>> existingConfig = new HashMap<String, Map<String, String>>();\n-    Configuration stackConfig = new Configuration(existingConfig,\n+    Configuration stackDefaultConfig = new Configuration(existingConfig,\n       new HashMap<String, Map<String, Map<String, String>>>());\n+    if (stackPropertyValue != null) {\n+      stackDefaultConfig.setProperty(\"testConfigType\", \"testProperty\", stackPropertyValue);\n+    }\n+\n+    Configuration blueprintConfig = new Configuration(stackDefaultConfig.getFullProperties(),\n+      new HashMap<String, Map<String, Map<String, String>>>());\n+    if (blueprintPropertyValue != null) {\n+      blueprintConfig.setProperty(\"testConfigType\", \"testProperty\", blueprintPropertyValue);\n+    }\n \n     PowerMock.mockStatic(AmbariContext.class);\n     AmbariContext.getController();\n@@ -117,7 +195,7 @@ public void testProcessClusterConfigRequestIncludeKererosConfigs() throws Except\n     services.add(\"KERBEROS\");\n     services.add(\"ZOOKEPER\");\n     expect(blueprint.getServices()).andReturn(services).anyTimes();\n-    expect(stack.getConfiguration(services)).andReturn(stackConfig).once();\n+    expect(stack.getConfiguration(services)).andReturn(stackDefaultConfig).once();\n \n     List<String> hdfsComponents = new ArrayList<>();\n     hdfsComponents.add(\"NAMENODE\");\n@@ -132,7 +210,7 @@ public void testProcessClusterConfigRequestIncludeKererosConfigs() throws Except\n \n     expect(topology.getConfigRecommendationStrategy()).andReturn(ConfigRecommendationStrategy.NEVER_APPLY).anyTimes();\n     expect(topology.getBlueprint()).andReturn(blueprint).anyTimes();\n-    expect(topology.getConfiguration()).andReturn(stackConfig).anyTimes();\n+    expect(topology.getConfiguration()).andReturn(blueprintConfig).anyTimes();\n     expect(topology.getHostGroupInfo()).andReturn(Collections.<String, HostGroupInfo>emptyMap());\n     expect(topology.getClusterId()).andReturn(Long.valueOf(1)).anyTimes();\n     expect(ambariContext.getClusterName(Long.valueOf(1))).andReturn(\"testCluster\").anyTimes();\n@@ -141,24 +219,33 @@ public void testProcessClusterConfigRequestIncludeKererosConfigs() throws Except\n \n     Map<String, Map<String, String>> kerberosConfig = new HashMap<String, Map<String, String>>();\n     Map<String, String> properties = new HashMap<>();\n-    properties.put(\"testPorperty\", \"testValue\");\n+    properties.put(\"testProperty\", \"KERBEROStestValue\");\n     kerberosConfig.put(\"testConfigType\", properties);\n     expect(kerberosHelper.ensureHeadlessIdentities(anyObject(Cluster.class), anyObject(Map.class), anyObject\n       (Set.class))).andReturn(true).once();\n     expect(kerberosHelper.getServiceConfigurationUpdates(anyObject(Cluster.class), anyObject(Map.class), anyObject\n       (Set.class), anyBoolean(), anyBoolean(), eq(false))).andReturn(kerberosConfig).once();\n \n+    Capture<? extends String> captureClusterName = newCapture(CaptureType.ALL);\n+    Capture<? extends Set<String>> captureUpdatedConfigTypes = newCapture(CaptureType.ALL);\n+    ambariContext.waitForConfigurationResolution(capture(captureClusterName), capture\n+      (captureUpdatedConfigTypes));\n+    expectLastCall();\n \n     PowerMock.replay(stack, blueprint, topology, controller, clusters, kerberosHelper, ambariContext,\n       AmbariContext\n-      .class);\n+        .class);\n \n     ClusterConfigurationRequest clusterConfigurationRequest = new ClusterConfigurationRequest(\n       ambariContext, topology, false, stackAdvisorBlueprintProcessor, true);\n     clusterConfigurationRequest.process();\n \n     verify(blueprint, topology, ambariContext, controller, kerberosHelper);\n \n+\n+    String clusterName = captureClusterName.getValue();\n+    assertEquals(\"testCluster\", clusterName);\n+    return captureUpdatedConfigTypes;\n   }\n \n   @Test\n@@ -221,6 +308,4 @@ public void testProcessClusterConfigRequestDontIncludeKererosConfigs() throws Ex\n \n   }\n \n-\n-\n }",
                "raw_url": "https://github.com/apache/ambari/raw/080991d131768ece5e0d7065df2bb8443e14053f/ambari-server/src/test/java/org/apache/ambari/server/topology/ClusterConfigurationRequestTest.java",
                "sha": "2e7bd3b609e01e075163b3a05e59c6f9bf518edb",
                "status": "modified"
            }
        ],
        "message": "AMBARI-14901. NPE when configuring Kerberos at provisioning secure cluster with Blueprint. (Sandor Magyari via rnettleton)",
        "parent": "https://github.com/apache/ambari/commit/f026accf2df9f2be6e6f4dac049593c90f37bbc9",
        "repo": "ambari",
        "unit_tests": [
            "ClusterConfigurationRequestTest.java"
        ]
    },
    "ambari_0a8c397": {
        "bug_id": "ambari_0a8c397",
        "commit": "https://github.com/apache/ambari/commit/0a8c397bd1944b8787befdff08bf6b95b9afb225",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/ambari/blob/0a8c397bd1944b8787befdff08bf6b95b9afb225/ambari-server/src/main/java/org/apache/ambari/server/state/kerberos/KerberosDescriptorUpdateHelper.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/state/kerberos/KerberosDescriptorUpdateHelper.java?ref=0a8c397bd1944b8787befdff08bf6b95b9afb225",
                "deletions": 2,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/state/kerberos/KerberosDescriptorUpdateHelper.java",
                "patch": "@@ -340,6 +340,11 @@ private static KerberosComponentDescriptor processComponent(KerberosComponentDes\n   /**\n    * Processes a {@link KerberosIdentityDescriptor} to change the user-supplied data based on the changes\n    * observed between the previous stack version's data and the new stack version's data.\n+   * <p>\n+   * It is expected that <code>newStackIdentities</code> and <code>userIdentities</code> are not null.\n+   * However, <code>previousStackIdentities</code> may be null in the event the user added a Kerberos\n+   * identity that was then added in the new Kerberos descriptor.  In this case, the user's values\n+   * for the principal name and keytab file are kept while adding any other changes from tne new stack.\n    *\n    * @param previousStackIdentity a {@link KerberosIdentityDescriptor} from the previous stack version's Kerberos descriptor\n    * @param newStackIdentity      a {@link KerberosIdentityDescriptor} from the new stack version's Kerberos descriptor\n@@ -357,7 +362,7 @@ private static KerberosIdentityDescriptor processIdentity(KerberosIdentityDescri\n     // If the new identity definition is a reference and no longer has a principal definition,\n     // Ignore any user changes to the old principal definition.\n     if (updatedValuePrincipal != null) {\n-      KerberosPrincipalDescriptor oldValuePrincipal = previousStackIdentity.getPrincipalDescriptor();\n+      KerberosPrincipalDescriptor oldValuePrincipal = (previousStackIdentity == null) ? null : previousStackIdentity.getPrincipalDescriptor();\n       String previousValuePrincipalValue = null;\n       KerberosPrincipalDescriptor userValuePrincipal = userIdentity.getPrincipalDescriptor();\n       String userValuePrincipalValue = null;\n@@ -380,7 +385,7 @@ private static KerberosIdentityDescriptor processIdentity(KerberosIdentityDescri\n     // If the new identity definition is a reference and no longer has a keytab definition,\n     // Ignore any user changes to the old keytab definition.\n     if (updatedValueKeytab != null) {\n-      KerberosKeytabDescriptor oldValueKeytab = previousStackIdentity.getKeytabDescriptor();\n+      KerberosKeytabDescriptor oldValueKeytab = (previousStackIdentity == null) ? null : previousStackIdentity.getKeytabDescriptor();\n       String previousValueKeytabFile = null;\n       KerberosKeytabDescriptor userValueKeytab = userIdentity.getKeytabDescriptor();\n       String userValueKeytabFile = null;",
                "raw_url": "https://github.com/apache/ambari/raw/0a8c397bd1944b8787befdff08bf6b95b9afb225/ambari-server/src/main/java/org/apache/ambari/server/state/kerberos/KerberosDescriptorUpdateHelper.java",
                "sha": "dd865be39ce5e22793fb9d98424fe82688a43bdc",
                "status": "modified"
            },
            {
                "additions": 70,
                "blob_url": "https://github.com/apache/ambari/blob/0a8c397bd1944b8787befdff08bf6b95b9afb225/ambari-server/src/test/java/org/apache/ambari/server/state/kerberos/KerberosDescriptorUpdateHelperTest.java",
                "changes": 70,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/state/kerberos/KerberosDescriptorUpdateHelperTest.java?ref=0a8c397bd1944b8787befdff08bf6b95b9afb225",
                "deletions": 0,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/state/kerberos/KerberosDescriptorUpdateHelperTest.java",
                "patch": "@@ -272,6 +272,16 @@ public void testUpdateIdentities() throws AmbariException {\n             \"      \\\"keytab\\\": {\" +\n             \"        \\\"file\\\": \\\"${keytab_dir}/ambari.server.keytab\\\"\" +\n             \"      }\" +\n+            \"    },\" +\n+            \"    {\" +\n+            \"      \\\"name\\\": \\\"future_identity\\\",\" +\n+            \"      \\\"principal\\\": {\" +\n+            \"        \\\"value\\\": \\\"CHANGED_future${principal_suffix}@${realm}\\\",\" +\n+            \"        \\\"type\\\": \\\"user\\\"\" +\n+            \"      },\" +\n+            \"      \\\"keytab\\\": {\" +\n+            \"        \\\"file\\\": \\\"${keytab_dir}/future.user.keytab\\\"\" +\n+            \"      }\" +\n             \"    }\" +\n             \"  ]\" +\n             \"}\");\n@@ -328,6 +338,26 @@ public void testUpdateIdentities() throws AmbariException {\n             \"      \\\"keytab\\\": {\" +\n             \"        \\\"file\\\": \\\"${keytab_dir}/ambari.server.keytab\\\"\" +\n             \"      }\" +\n+            \"    },\" +\n+            \"    {\" +\n+            \"      \\\"name\\\": \\\"custom_identity\\\",\" +\n+            \"      \\\"principal\\\": {\" +\n+            \"        \\\"value\\\": \\\"custom${principal_suffix}@${realm}\\\",\" +\n+            \"        \\\"type\\\": \\\"user\\\"\" +\n+            \"      },\" +\n+            \"      \\\"keytab\\\": {\" +\n+            \"        \\\"file\\\": \\\"${keytab_dir}/custom.user.keytab\\\"\" +\n+            \"      }\" +\n+            \"    },\" +\n+            \"    {\" +\n+            \"      \\\"name\\\": \\\"future_identity\\\",\" +\n+            \"      \\\"principal\\\": {\" +\n+            \"        \\\"value\\\": \\\"future${principal_suffix}@${realm}\\\",\" +\n+            \"        \\\"type\\\": \\\"user\\\"\" +\n+            \"      },\" +\n+            \"      \\\"keytab\\\": {\" +\n+            \"        \\\"file\\\": \\\"${keytab_dir}/future.user.keytab\\\"\" +\n+            \"      }\" +\n             \"    }\" +\n             \"  ]\" +\n             \"}\");\n@@ -343,6 +373,26 @@ public void testUpdateIdentities() throws AmbariException {\n             \"{\\n\" +\n                 \"  \\\"identities\\\": [\\n\" +\n                 \"    {\\n\" +\n+                \"      \\\"name\\\": \\\"future_identity\\\",\\n\" +\n+                \"      \\\"principal\\\": {\\n\" +\n+                \"        \\\"value\\\": \\\"future${principal_suffix}@${realm}\\\",\\n\" +\n+                \"        \\\"type\\\": \\\"user\\\"\\n\" +\n+                \"      },\\n\" +\n+                \"      \\\"keytab\\\": {\\n\" +\n+                \"        \\\"file\\\": \\\"${keytab_dir}/future.user.keytab\\\"\\n\" +\n+                \"      }\\n\" +\n+                \"    },\\n\" +\n+                \"    {\\n\" +\n+                \"      \\\"name\\\": \\\"custom_identity\\\",\\n\" +\n+                \"      \\\"principal\\\": {\\n\" +\n+                \"        \\\"value\\\": \\\"custom${principal_suffix}@${realm}\\\",\\n\" +\n+                \"        \\\"type\\\": \\\"user\\\"\\n\" +\n+                \"      },\\n\" +\n+                \"      \\\"keytab\\\": {\\n\" +\n+                \"        \\\"file\\\": \\\"${keytab_dir}/custom.user.keytab\\\"\\n\" +\n+                \"      }\\n\" +\n+                \"    },\\n\" +\n+                \"    {\\n\" +\n                 \"      \\\"name\\\": \\\"spnego\\\",\\n\" +\n                 \"      \\\"principal\\\": {\\n\" +\n                 \"        \\\"value\\\": \\\"CHANGED_HTTP/_HOST@${realm}\\\",\\n\" +\n@@ -405,6 +455,26 @@ public void testUpdateIdentities() throws AmbariException {\n                 \"      }\\n\" +\n                 \"    },\\n\" +\n                 \"    {\\n\" +\n+                \"      \\\"name\\\": \\\"custom_identity\\\",\\n\" +\n+                \"      \\\"principal\\\": {\\n\" +\n+                \"        \\\"value\\\": \\\"custom${principal_suffix}@${realm}\\\",\\n\" +\n+                \"        \\\"type\\\": \\\"user\\\"\\n\" +\n+                \"      },\\n\" +\n+                \"      \\\"keytab\\\": {\\n\" +\n+                \"        \\\"file\\\": \\\"${keytab_dir}/custom.user.keytab\\\"\\n\" +\n+                \"      }\\n\" +\n+                \"    },\\n\" +\n+                \"    {\\n\" +\n+                \"      \\\"name\\\": \\\"future_identity\\\",\\n\" +\n+                \"      \\\"principal\\\": {\\n\" +\n+                \"        \\\"value\\\": \\\"future${principal_suffix}@${realm}\\\",\\n\" +\n+                \"        \\\"type\\\": \\\"user\\\"\\n\" +\n+                \"      },\\n\" +\n+                \"      \\\"keytab\\\": {\\n\" +\n+                \"        \\\"file\\\": \\\"${keytab_dir}/future.user.keytab\\\"\\n\" +\n+                \"      }\\n\" +\n+                \"    },\\n\" +\n+                \"    {\\n\" +\n                 \"      \\\"name\\\": \\\"spnego\\\",\\n\" +\n                 \"      \\\"principal\\\": {\\n\" +\n                 \"        \\\"value\\\": \\\"CHANGED_HTTP/_HOST@${realm}\\\",\\n\" +",
                "raw_url": "https://github.com/apache/ambari/raw/0a8c397bd1944b8787befdff08bf6b95b9afb225/ambari-server/src/test/java/org/apache/ambari/server/state/kerberos/KerberosDescriptorUpdateHelperTest.java",
                "sha": "e717190060874686dea474f930ec6f95811188c5",
                "status": "modified"
            }
        ],
        "message": "AMBARI-21480. NPE during \"Update Kerberos Descriptor\" (rlevas)",
        "parent": "https://github.com/apache/ambari/commit/4e1da58a9479889f1624e9887bb681a54e2ae6ae",
        "repo": "ambari",
        "unit_tests": [
            "KerberosDescriptorUpdateHelperTest.java"
        ]
    },
    "ambari_135b00e": {
        "bug_id": "ambari_135b00e",
        "commit": "https://github.com/apache/ambari/commit/135b00e22cd116ee54413ef8acc049a1ac75ad32",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/135b00e22cd116ee54413ef8acc049a1ac75ad32/ambari-server/src/main/java/org/apache/ambari/server/state/host/HostImpl.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/state/host/HostImpl.java?ref=135b00e22cd116ee54413ef8acc049a1ac75ad32",
                "deletions": 1,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/state/host/HostImpl.java",
                "patch": "@@ -121,7 +121,7 @@\n   private Map<Long, MaintenanceState> maintMap = null;\n \n   // In-memory status, based on host components states\n-  private String status;\n+  private String status = HealthStatus.UNKNOWN.name();\n \n   // In-memory prefix of log file paths that is retrieved when the agent registers with the server\n   private String prefix;",
                "raw_url": "https://github.com/apache/ambari/raw/135b00e22cd116ee54413ef8acc049a1ac75ad32/ambari-server/src/main/java/org/apache/ambari/server/state/host/HostImpl.java",
                "sha": "9ba38ebc60b750b8db8e8f4ff6a0445e1974b5b7",
                "status": "modified"
            }
        ],
        "message": "AMBARI-8337. Add wizard hangs adding hosts with NPE. (Nate via mahadev)",
        "parent": "https://github.com/apache/ambari/commit/c7aea4b32bd0bf2a92cbe8f76547a67eb2965202",
        "repo": "ambari",
        "unit_tests": [
            "HostImplTest.java"
        ]
    },
    "ambari_1a6dd84": {
        "bug_id": "ambari_1a6dd84",
        "commit": "https://github.com/apache/ambari/commit/1a6dd84d242fcdd0e26c96b5ce3e564049b5095f",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/ambari/blob/1a6dd84d242fcdd0e26c96b5ce3e564049b5095f/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackDefinedPropertyProvider.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackDefinedPropertyProvider.java?ref=1a6dd84d242fcdd0e26c96b5ce3e564049b5095f",
                "deletions": 5,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackDefinedPropertyProvider.java",
                "patch": "@@ -248,11 +248,13 @@ public StackDefinedPropertyProvider(Resource.Type type,\n \n     for (Entry<String, Metric> entry : def.getMetrics().entrySet()) {\n       Metric metric = entry.getValue();\n-      PropertyInfo propertyInfo = new PropertyInfo(metric.getName(),\n-        metric.isTemporal(), metric.isPointInTime());\n-      propertyInfo.setAmsHostMetric(metric.isAmsHostMetric());\n-      propertyInfo.setUnit(metric.getUnit());\n-      defs.put(entry.getKey(), propertyInfo);\n+      if (metric.getName() != null) {\n+        PropertyInfo propertyInfo = new PropertyInfo(metric.getName(),\n+                metric.isTemporal(), metric.isPointInTime());\n+        propertyInfo.setAmsHostMetric(metric.isAmsHostMetric());\n+        propertyInfo.setUnit(metric.getUnit());\n+        defs.put(entry.getKey(), propertyInfo);\n+      }\n     }\n \n     return defs;",
                "raw_url": "https://github.com/apache/ambari/raw/1a6dd84d242fcdd0e26c96b5ce3e564049b5095f/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackDefinedPropertyProvider.java",
                "sha": "3a6d30b8c48e7a44d501bd865c7be944c2d7d7cf",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/1a6dd84d242fcdd0e26c96b5ce3e564049b5095f/ambari-server/src/test/java/org/apache/ambari/server/controller/metrics/RestMetricsPropertyProviderTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/controller/metrics/RestMetricsPropertyProviderTest.java?ref=1a6dd84d242fcdd0e26c96b5ce3e564049b5095f",
                "deletions": 0,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/controller/metrics/RestMetricsPropertyProviderTest.java",
                "patch": "@@ -82,6 +82,7 @@\n     componentMetrics.put(\"metrics/api/cluster/summary/slots.used\", new Metric(\"/api/cluster/summary##slots.used\", false, false, false, \"unitless\"));\n     componentMetrics.put(\"metrics/api/cluster/summary/topologies\", new Metric(\"/api/cluster/summary##topologies\", false, false, false, \"unitless\"));\n     componentMetrics.put(\"metrics/api/cluster/summary/nimbus.uptime\", new Metric(\"/api/cluster/summary##nimbus.uptime\", false, false, false, \"unitless\"));\n+    componentMetrics.put(\"metrics/api/cluster/summary/wrong.metric\", new Metric(null, false, false, false, \"unitless\"));\n   }\n \n \n@@ -132,6 +133,7 @@ public void testPopulateResources() throws Exception {\n     Request request = PropertyHelper.getReadRequest(Collections.<String>emptySet());\n \n     Assert.assertEquals(1, restMetricsPropertyProvider.populateResources(Collections.singleton(resource), request, null).size());\n+    Assert.assertNull(resource.getPropertyValue(PropertyHelper.getPropertyId(\"metrics/api/cluster/summary\", \"wrong.metric\")));\n \n     //STORM_REST_API\n     Assert.assertEquals(28.0, resource.getPropertyValue(PropertyHelper.getPropertyId(\"metrics/api/cluster/summary\", \"tasks.total\")));",
                "raw_url": "https://github.com/apache/ambari/raw/1a6dd84d242fcdd0e26c96b5ce3e564049b5095f/ambari-server/src/test/java/org/apache/ambari/server/controller/metrics/RestMetricsPropertyProviderTest.java",
                "sha": "500eea4271978c5f6e82ad73f0d852a778d77b20",
                "status": "modified"
            }
        ],
        "message": "AMBARI-11434. After cluster installation accessing DATANODE host component results in NPE due to bad metrics JSON (dlysnichenko)",
        "parent": "https://github.com/apache/ambari/commit/766b934c086024a65e53b5441f561828e388578c",
        "repo": "ambari",
        "unit_tests": [
            "StackDefinedPropertyProviderTest.java"
        ]
    },
    "ambari_1aa13da": {
        "bug_id": "ambari_1aa13da",
        "commit": "https://github.com/apache/ambari/commit/1aa13da878a0ccc6a0f40efae823ab767417462f",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/ambari/blob/1aa13da878a0ccc6a0f40efae823ab767417462f/ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java?ref=1aa13da878a0ccc6a0f40efae823ab767417462f",
                "deletions": 3,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java",
                "patch": "@@ -546,9 +546,15 @@ public static void checkServiceConfigs()  {\n         Map<String, ServiceInfo> serviceInfoMap = ambariMetaInfo.getServices(stackName, stackVersion);\n         for (String serviceName : serviceNames) {\n           ServiceInfo serviceInfo = serviceInfoMap.get(serviceName);\n-          Set<String> configTypes = serviceInfo.getConfigTypeAttributes().keySet();\n-          for (String configType : configTypes) {\n-            stackServiceConfigs.put(serviceName, configType);\n+          if (serviceInfo != null) {\n+            Set<String> configTypes = serviceInfo.getConfigTypeAttributes().keySet();\n+            for (String configType : configTypes) {\n+              stackServiceConfigs.put(serviceName, configType);\n+            }\n+          } else {\n+            LOG.warn(\"Service {} is not available for stack {} in cluster {}\",\n+                    serviceName, stackName + \"-\" + stackVersion, clusterName);\n+            warningAvailable = true;\n           }\n         }\n ",
                "raw_url": "https://github.com/apache/ambari/raw/1aa13da878a0ccc6a0f40efae823ab767417462f/ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java",
                "sha": "f302b8b30642035085d2930b86f1cd3c43340040",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/ambari/blob/1aa13da878a0ccc6a0f40efae823ab767417462f/ambari-server/src/test/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelperTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelperTest.java?ref=1aa13da878a0ccc6a0f40efae823ab767417462f",
                "deletions": 3,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelperTest.java",
                "patch": "@@ -251,9 +251,9 @@ protected void configure() {\n \n     expect(mockHDFSServiceInfo.getConfigTypeAttributes()).andReturn(configAttributes);\n     expect(mockAmbariMetainfo.getServices(\"HDP\", \"2.2\")).andReturn(services);\n-    expect(serviceConfigResultSet.next()).andReturn(true);\n-    expect(serviceConfigResultSet.getString(\"service_name\")).andReturn(\"HDFS\");\n-    expect(serviceConfigResultSet.getString(\"type_name\")).andReturn(\"core-site\");\n+    expect(serviceConfigResultSet.next()).andReturn(true).times(2);\n+    expect(serviceConfigResultSet.getString(\"service_name\")).andReturn(\"HDFS\").andReturn(\"HBASE\");\n+    expect(serviceConfigResultSet.getString(\"type_name\")).andReturn(\"core-site\").andReturn(\"hbase-env\");\n     expect(stackResultSet.next()).andReturn(true);\n     expect(stackResultSet.getString(\"stack_name\")).andReturn(\"HDP\");\n     expect(stackResultSet.getString(\"stack_version\")).andReturn(\"2.2\");",
                "raw_url": "https://github.com/apache/ambari/raw/1aa13da878a0ccc6a0f40efae823ab767417462f/ambari-server/src/test/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelperTest.java",
                "sha": "46633101e4eb8bac1f6189050d658c7766305cc0",
                "status": "modified"
            }
        ],
        "message": "AMBARI-18296. Database Consistency Check Fails With NPE With Missing Service From Stack.(vbrodetskyi)",
        "parent": "https://github.com/apache/ambari/commit/a6b96f835db0326cb448f4b215074055ba93bfa5",
        "repo": "ambari",
        "unit_tests": [
            "DatabaseConsistencyCheckHelperTest.java"
        ]
    },
    "ambari_21c3f58": {
        "bug_id": "ambari_21c3f58",
        "commit": "https://github.com/apache/ambari/commit/21c3f58d24ece92c42f0054f9e57e76f04927e91",
        "file": [
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/ambari/blob/21c3f58d24ece92c42f0054f9e57e76f04927e91/ambari-server/src/main/java/org/apache/ambari/server/view/ViewRegistry.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/view/ViewRegistry.java?ref=21c3f58d24ece92c42f0054f9e57e76f04927e91",
                "deletions": 17,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/view/ViewRegistry.java",
                "patch": "@@ -40,6 +40,7 @@\n import org.apache.ambari.server.view.configuration.EntityConfig;\n import org.apache.ambari.server.view.configuration.InstanceConfig;\n import org.apache.ambari.server.view.configuration.ParameterConfig;\n+import org.apache.ambari.server.view.configuration.PersistenceConfig;\n import org.apache.ambari.server.view.configuration.PropertyConfig;\n import org.apache.ambari.server.view.configuration.ResourceConfig;\n import org.apache.ambari.server.view.configuration.ViewConfig;\n@@ -266,11 +267,11 @@ public static ViewRegistry getInstance() {\n           try {\n             ClassLoader cl = URLClassLoader.newInstance(new URL[]{fileEntry.toURI().toURL()});\n \n-            InputStream    configStream     = cl.getResourceAsStream(VIEW_XML);\n-            JAXBContext    jaxbContext      = JAXBContext.newInstance(ViewConfig.class);\n-            Unmarshaller   jaxbUnmarshaller = jaxbContext.createUnmarshaller();\n-            ViewConfig     viewConfig       = (ViewConfig) jaxbUnmarshaller.unmarshal(configStream);\n-            ViewEntity viewDefinition       = installView(viewConfig, configuration, cl, fileEntry.getAbsolutePath());\n+            InputStream  configStream     = cl.getResourceAsStream(VIEW_XML);\n+            JAXBContext  jaxbContext      = JAXBContext.newInstance(ViewConfig.class);\n+            Unmarshaller jaxbUnmarshaller = jaxbContext.createUnmarshaller();\n+            ViewConfig   viewConfig       = (ViewConfig) jaxbUnmarshaller.unmarshal(configStream);\n+            ViewEntity   viewDefinition   = installView(viewConfig, configuration, cl, fileEntry.getAbsolutePath());\n \n             for (InstanceConfig instanceConfig : viewConfig.getInstances()) {\n               ViewInstanceEntity viewInstanceDefinition = new ViewInstanceEntity(viewDefinition, instanceConfig);\n@@ -288,7 +289,6 @@ public static ViewRegistry getInstance() {\n         }\n       }\n       try {\n-\n         instanceDefinitions.addAll(persistViews());\n       } catch (ClassNotFoundException e) {\n         LOG.error(\"Caught exception persisting views.\", e);\n@@ -528,20 +528,27 @@ private void installViewInstance(ViewEntity viewDefinition,\n \n   // Set the entities defined in the view persistence element for the given view instance\n   private static void setPersistenceEntities(ViewInstanceEntity viewInstanceDefinition) {\n-    ViewEntity viewDefinition = viewInstanceDefinition.getViewEntity();\n+    ViewEntity        viewDefinition    = viewInstanceDefinition.getViewEntity();\n+    ViewConfig        viewConfig        = viewDefinition.getConfiguration();\n+\n     Collection<ViewEntityEntity> entities = new HashSet<ViewEntityEntity>();\n \n-    ViewConfig viewConfig = viewDefinition.getConfiguration();\n-    for (EntityConfig entityConfiguration : viewConfig.getPersistence().getEntities()) {\n-      ViewEntityEntity viewEntityEntity = new ViewEntityEntity();\n+    if (viewConfig != null) {\n+      PersistenceConfig persistenceConfig = viewConfig.getPersistence();\n \n-      viewEntityEntity.setViewName(viewDefinition.getName());\n-      viewEntityEntity.setViewInstanceName(viewInstanceDefinition.getName());\n-      viewEntityEntity.setClassName(entityConfiguration.getClassName());\n-      viewEntityEntity.setIdProperty(entityConfiguration.getIdProperty());\n-      viewEntityEntity.setViewInstance(viewInstanceDefinition);\n+      if (persistenceConfig != null) {\n+        for (EntityConfig entityConfiguration : persistenceConfig.getEntities()) {\n+          ViewEntityEntity viewEntityEntity = new ViewEntityEntity();\n \n-      entities.add(viewEntityEntity);\n+          viewEntityEntity.setViewName(viewDefinition.getName());\n+          viewEntityEntity.setViewInstanceName(viewInstanceDefinition.getName());\n+          viewEntityEntity.setClassName(entityConfiguration.getClassName());\n+          viewEntityEntity.setIdProperty(entityConfiguration.getIdProperty());\n+          viewEntityEntity.setViewInstance(viewInstanceDefinition);\n+\n+          entities.add(viewEntityEntity);\n+        }\n+      }\n     }\n     viewInstanceDefinition.setEntities(entities);\n   }\n@@ -580,7 +587,7 @@ protected void configure() {\n   private Set<ViewInstanceEntity> persistViews() throws ClassNotFoundException {\n \n     Set<ViewInstanceEntity> instanceDefinitions = new HashSet<ViewInstanceEntity>();\n-    Set<String> persistedViews = new HashSet<String>();\n+    Set<String>             persistedViews      = new HashSet<String>();\n \n     for (ViewEntity viewEntity : viewDAO.findAll()) {\n       String name = viewEntity.getName();",
                "raw_url": "https://github.com/apache/ambari/raw/21c3f58d24ece92c42f0054f9e57e76f04927e91/ambari-server/src/main/java/org/apache/ambari/server/view/ViewRegistry.java",
                "sha": "c8da1ae7725a3345011166d4d16b3bded0d2b154",
                "status": "modified"
            }
        ],
        "message": "AMBARI-5642 - Ambari Views : NPE deploying view",
        "parent": "https://github.com/apache/ambari/commit/3c799274aea8d73aa50a5d76fde32ee6573beee7",
        "repo": "ambari",
        "unit_tests": [
            "ViewRegistryTest.java"
        ]
    },
    "ambari_2574ed4": {
        "bug_id": "ambari_2574ed4",
        "commit": "https://github.com/apache/ambari/commit/2574ed43e4cb52807d9d4b0e35257fcfbb7815ad",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/ambari/blob/2574ed43e4cb52807d9d4b0e35257fcfbb7815ad/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java?ref=2574ed43e4cb52807d9d4b0e35257fcfbb7815ad",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java",
                "patch": "@@ -1767,6 +1767,11 @@ public String updateForClusterCreate(String propertyName,\n                                          Map<String, Map<String, String>> properties,\n                                          ClusterTopology topology) {\n \n+      if (origValue == null) {\n+        LOG.info(\"Property {} is null, skipping search for host group placeholder\", propertyName);\n+        return null;\n+      }\n+\n       HostGroups hostGroups = new HostGroups(topology, propertyName);\n \n       //todo: getHostStrings (?)\n@@ -1807,6 +1812,11 @@ public String updateForClusterCreate(String propertyName,\n                                                     String origValue,\n                                                     Map<String, Map<String, String>> properties,\n                                                     ClusterTopology topology) {\n+      if (origValue == null) {\n+        LOG.info(\"Property {} is null, skipping search for host group placeholder\", propertyName);\n+        return Collections.emptyList();\n+      }\n+\n       //todo: getHostStrings\n       Matcher m = HostGroup.HOSTGROUP_REGEX.matcher(origValue);\n       Set<String> hostGroups = new HashSet<>();",
                "raw_url": "https://github.com/apache/ambari/raw/2574ed43e4cb52807d9d4b0e35257fcfbb7815ad/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java",
                "sha": "1b62c29cdda08193aae25a9cc4338b13bb68da28",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/ambari/blob/2574ed43e4cb52807d9d4b0e35257fcfbb7815ad/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessorTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessorTest.java?ref=2574ed43e4cb52807d9d4b0e35257fcfbb7815ad",
                "deletions": 0,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessorTest.java",
                "patch": "@@ -2249,6 +2249,7 @@ public void testDoUpdateForClusterCreate_HostgroupReplacement_DefaultUpdater() t\n         \"dfs.https.address\", \"testhost3\")),\n       \"myservice-site\", new HashMap<>(ImmutableMap.of(\n         \"myservice_slave_address\", \"%HOSTGROUP::group1%:8080\"))));\n+    hostGroupProperties.get(\"hdfs-site\").put(\"null_property\", null);\n \n     Configuration clusterConfig = new Configuration(new HashMap<>(), new HashMap<>());\n     clusterConfig.setParentConfiguration(new Configuration(stackProperties, emptyMap()));\n@@ -2387,6 +2388,9 @@ public void testHostgroupUpdater_getRequiredHostgroups() throws Exception {\n           \"%HOSTGROUP::master3%:8080\",\n           clusterConfig.getProperties(),\n           topology)));\n+\n+    assertEquals(emptyList(),\n+      updater.getRequiredHostGroups(\"mycomponent.urls\", null, clusterConfig.getProperties(), topology));\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/ambari/raw/2574ed43e4cb52807d9d4b0e35257fcfbb7815ad/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessorTest.java",
                "sha": "f7188620649d1dcec2d2b415d70eac7d5fefdc0c",
                "status": "modified"
            }
        ],
        "message": "AMBARI-24852. NPE in default host group replacement (#2571)",
        "parent": "https://github.com/apache/ambari/commit/4945cd0acf89d286abbe326191ce352d3ae08516",
        "repo": "ambari",
        "unit_tests": [
            "BlueprintConfigurationProcessorTest.java"
        ]
    },
    "ambari_2c660d3": {
        "bug_id": "ambari_2c660d3",
        "commit": "https://github.com/apache/ambari/commit/2c660d32ea1b499cb8ee7908d43fec25f011f96f",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/ambari/blob/2c660d32ea1b499cb8ee7908d43fec25f011f96f/ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClustersImpl.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClustersImpl.java?ref=2c660d32ea1b499cb8ee7908d43fec25f011f96f",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClustersImpl.java",
                "patch": "@@ -840,6 +840,10 @@ public void deleteHost(String hostname) throws AmbariException {\n     // a copy of this to ensure that we can pass in the original set of\n     // clusters that the host belonged to to the host removal event\n     Set<Cluster> clusters = hostClusterMap.get(hostname);\n+    if (clusters == null) {\n+      throw new HostNotFoundException(hostname);\n+    }\n+\n     Set<Cluster> hostsClusters = new HashSet<>(clusters);\n \n     deleteHostEntityRelationships(hostname);",
                "raw_url": "https://github.com/apache/ambari/raw/2c660d32ea1b499cb8ee7908d43fec25f011f96f/ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClustersImpl.java",
                "sha": "caab7dfc93a1eba2cb1ab8a165f3dee164611265",
                "status": "modified"
            }
        ],
        "message": "AMBARI-15726. Removing unknown host from cluster throws NPE (ajit)",
        "parent": "https://github.com/apache/ambari/commit/df00fd6e4b769276a72373980aa819c26465b12f",
        "repo": "ambari",
        "unit_tests": [
            "ClustersImplTest.java"
        ]
    },
    "ambari_30cd715": {
        "bug_id": "ambari_30cd715",
        "commit": "https://github.com/apache/ambari/commit/30cd715793183e0c00ea7fdb900482cfcdc13a8a",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/ambari/blob/30cd715793183e0c00ea7fdb900482cfcdc13a8a/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java?ref=30cd715793183e0c00ea7fdb900482cfcdc13a8a",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java",
                "patch": "@@ -2707,6 +2707,9 @@ protected RequestStageContainer doStageCreation(RequestStageContainer requestSta\n       return requestStages;\n     }\n \n+    // check all stack configs are present in desired configs\n+    configHelper.checkAllStageConfigsPresentInDesiredConfigs(cluster);\n+\n     // caching upgrade suspended\n     boolean isUpgradeSuspended = cluster.isUpgradeSuspended();\n ",
                "raw_url": "https://github.com/apache/ambari/raw/30cd715793183e0c00ea7fdb900482cfcdc13a8a/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java",
                "sha": "fac7b94791417a511484a85361fce5e1e1c8bb47",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/ambari/blob/30cd715793183e0c00ea7fdb900482cfcdc13a8a/ambari-server/src/main/java/org/apache/ambari/server/state/ConfigHelper.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/state/ConfigHelper.java?ref=30cd715793183e0c00ea7fdb900482cfcdc13a8a",
                "deletions": 2,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/state/ConfigHelper.java",
                "patch": "@@ -699,7 +699,9 @@ public void removeConfigsByType(Cluster cluster, String type) {\n     for (PropertyInfo stackProperty : stackProperties) {\n       if (stackProperty.getPropertyTypes().contains(propertyType)) {\n         String stackPropertyConfigType = fileNameToConfigType(stackProperty.getFilename());\n-        result.put(stackProperty, actualConfigs.get(stackPropertyConfigType).getProperties().get(stackProperty.getName()));\n+        if (actualConfigs.containsKey(stackPropertyConfigType)) {\n+          result.put(stackProperty, actualConfigs.get(stackPropertyConfigType).getProperties().get(stackProperty.getName()));\n+        }\n       }\n     }\n \n@@ -776,13 +778,36 @@ public void removeConfigsByType(Cluster cluster, String type) {\n     for (PropertyInfo stackProperty : stackProperties) {\n       if (stackProperty.getPropertyTypes().contains(propertyType)) {\n         String stackPropertyConfigType = fileNameToConfigType(stackProperty.getFilename());\n-        result.add(actualConfigs.get(stackPropertyConfigType).getProperties().get(stackProperty.getName()));\n+        if (actualConfigs.containsKey(stackPropertyConfigType)) {\n+          result.add(actualConfigs.get(stackPropertyConfigType).getProperties().get(stackProperty.getName()));\n+        }\n       }\n     }\n \n     return result;\n   }\n \n+  public void checkAllStageConfigsPresentInDesiredConfigs(Cluster cluster) throws AmbariException {\n+    StackId stackId = cluster.getDesiredStackVersion();\n+    Set<String> stackConfigTypes = ambariMetaInfo.getStack(stackId.getStackName(),\n+            stackId.getStackVersion()).getConfigTypeAttributes().keySet();\n+    Map<String, Config> actualConfigs = new HashMap<>();\n+    Map<String, DesiredConfig> desiredConfigs = cluster.getDesiredConfigs();\n+\n+    for (Map.Entry<String, DesiredConfig> desiredConfigEntry : desiredConfigs.entrySet()) {\n+      String configType = desiredConfigEntry.getKey();\n+      DesiredConfig desiredConfig = desiredConfigEntry.getValue();\n+      actualConfigs.put(configType, cluster.getConfig(configType, desiredConfig.getTag()));\n+    }\n+\n+    for (String stackConfigType : stackConfigTypes) {\n+      if (!actualConfigs.containsKey(stackConfigType)) {\n+        LOG.error(String.format(\"Unable to find stack configuration %s in ambari configs!\", stackConfigType));\n+      }\n+    }\n+\n+  }\n+\n   /***\n    * Fetch all the config values of a given PropertyType. For eg: Fetch all stack configs that are of type \"user\"\n    * @param cluster",
                "raw_url": "https://github.com/apache/ambari/raw/30cd715793183e0c00ea7fdb900482cfcdc13a8a/ambari-server/src/main/java/org/apache/ambari/server/state/ConfigHelper.java",
                "sha": "e8250fefa12924750ae17681b41b79096022b2c6",
                "status": "modified"
            }
        ],
        "message": "AMBARI-21520. Ambari server logs NPE with no additional stack trace on any host component start/stop command.(vbrodetskyi)",
        "parent": "https://github.com/apache/ambari/commit/4b189a1131a387f1a74e45624af95525e984d30a",
        "repo": "ambari",
        "unit_tests": [
            "AmbariManagementControllerImplTest.java",
            "ConfigHelperTest.java"
        ]
    },
    "ambari_33a573c": {
        "bug_id": "ambari_33a573c",
        "commit": "https://github.com/apache/ambari/commit/33a573cdb54229cee37e1f49837be783de3cba7b",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/ambari/blob/33a573cdb54229cee37e1f49837be783de3cba7b/ambari-server/src/main/java/org/apache/ambari/server/serveraction/kerberos/ConfigureAmbariIdentitiesServerAction.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/serveraction/kerberos/ConfigureAmbariIdentitiesServerAction.java?ref=33a573cdb54229cee37e1f49837be783de3cba7b",
                "deletions": 2,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/serveraction/kerberos/ConfigureAmbariIdentitiesServerAction.java",
                "patch": "@@ -184,12 +184,16 @@ public boolean installAmbariServerIdentity(String principal,\n                   \"  This is not an error if an Ambari agent is not installed on the Ambari server host.\",\n               principal, ambariServerHostName);\n           LOG.warn(message);\n-          actionLog.writeStdErr(message);\n+          if(actionLog != null) {\n+            actionLog.writeStdErr(message);\n+          }\n         } else if (!kerberosPrincipalHostDAO.exists(principal, ambariServerHostID)) {\n           kerberosPrincipalHostDAO.create(principal, ambariServerHostID);\n         }\n \n-        actionLog.writeStdOut(String.format(\"Created Ambari server keytab file for %s at %s\", principal, destKeytabFile));\n+        if(actionLog != null) {\n+          actionLog.writeStdOut(String.format(\"Created Ambari server keytab file for %s at %s\", principal, destKeytabFile));\n+        }\n       }\n     } catch (InterruptedException | IOException e) {\n       throw new AmbariException(e.getLocalizedMessage(), e);",
                "raw_url": "https://github.com/apache/ambari/raw/33a573cdb54229cee37e1f49837be783de3cba7b/ambari-server/src/main/java/org/apache/ambari/server/serveraction/kerberos/ConfigureAmbariIdentitiesServerAction.java",
                "sha": "10647e8a01370dea90e7d102b18d26589f198590",
                "status": "modified"
            }
        ],
        "message": "AMBARI-18448. NPE when installing secure cluster via Blueprints due to null logger (rlevas)",
        "parent": "https://github.com/apache/ambari/commit/ac0c66e986e14db6a746dfe1b84f36a662dacfbb",
        "repo": "ambari",
        "unit_tests": [
            "ConfigureAmbariIdentitiesServerActionTest.java"
        ]
    },
    "ambari_33f95a1": {
        "bug_id": "ambari_33f95a1",
        "commit": "https://github.com/apache/ambari/commit/33f95a12a31bfd01e5135ab8e4586affb6b815c9",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/33f95a12a31bfd01e5135ab8e4586affb6b815c9/ambari-server/src/main/java/org/apache/ambari/server/state/ServiceInfo.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/state/ServiceInfo.java?ref=33f95a12a31bfd01e5135ab8e4586affb6b815c9",
                "deletions": 1,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/state/ServiceInfo.java",
                "patch": "@@ -233,7 +233,7 @@ public boolean hasPropertyFor(String type, Collection<String> keyNames) {\n     Set<String> keys = configLayout.get(type);\n \n     for (String staleCheck : keyNames) {\n-      if (keys.contains(staleCheck))\n+      if (keys != null && keys.contains(staleCheck))\n         return true;\n     }\n     ",
                "raw_url": "https://github.com/apache/ambari/raw/33f95a12a31bfd01e5135ab8e4586affb6b815c9/ambari-server/src/main/java/org/apache/ambari/server/state/ServiceInfo.java",
                "sha": "c45531f1b0d7f129a551b4149f82d335d6926d81",
                "status": "modified"
            }
        ],
        "message": "AMBARI-4191. NPE exceptions / errors in server log. (swagle)",
        "parent": "https://github.com/apache/ambari/commit/f0be471404f90af602b79d368cc8091560a891b1",
        "repo": "ambari",
        "unit_tests": [
            "ServiceInfoTest.java"
        ]
    },
    "ambari_358dc1d": {
        "bug_id": "ambari_358dc1d",
        "commit": "https://github.com/apache/ambari/commit/358dc1d8eb361480932380d8546d58b264a25c38",
        "file": [
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/ambari/blob/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java?ref=358dc1d8eb361480932380d8546d58b264a25c38",
                "deletions": 2,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java",
                "patch": "@@ -1060,6 +1060,11 @@ private Stage createNewStage(long id, Cluster cluster, long requestId,\n \n             ServiceComponentHost sch = serviceComponentHostMap.get(request.getHostname());\n \n+            if (null == sch) {\n+              // It's possible that the host was deleted during the time that the request was generated.\n+              continue;\n+            }\n+\n             if (checkDesiredState && (desiredStateToCheck != sch.getDesiredState())) {\n               continue;\n             }\n@@ -1077,7 +1082,7 @@ private Stage createNewStage(long id, Cluster cluster, long requestId,\n             }\n \n             ServiceComponentHostResponse r = sch.convertToResponse();\n-            if (filterBasedConfigStaleness && r.isStaleConfig() != staleConfig) {\n+            if (null == r || (filterBasedConfigStaleness && r.isStaleConfig() != staleConfig)) {\n               continue;\n             }\n \n@@ -1106,6 +1111,11 @@ private Stage createNewStage(long id, Cluster cluster, long requestId,\n           }\n         } else {\n           for (ServiceComponentHost sch : serviceComponentHostMap.values()) {\n+            if (null == sch) {\n+              // It's possible that the host was deleted during the time that the request was generated.\n+              continue;\n+            }\n+\n             if (checkDesiredState && (desiredStateToCheck != sch.getDesiredState())) {\n               continue;\n             }\n@@ -1123,7 +1133,7 @@ private Stage createNewStage(long id, Cluster cluster, long requestId,\n             }\n \n             ServiceComponentHostResponse r = sch.convertToResponse();\n-            if (filterBasedConfigStaleness && r.isStaleConfig() != staleConfig) {\n+            if (null == r || (filterBasedConfigStaleness && r.isStaleConfig() != staleConfig)) {\n               continue;\n             }\n ",
                "raw_url": "https://github.com/apache/ambari/raw/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java",
                "sha": "5aec7eb791d2deb0372d1b0c93cf4426d425d1de",
                "status": "modified"
            },
            {
                "additions": 35,
                "blob_url": "https://github.com/apache/ambari/blob/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/HostResourceProvider.java",
                "changes": 47,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/HostResourceProvider.java?ref=358dc1d8eb361480932380d8546d58b264a25c38",
                "deletions": 12,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/HostResourceProvider.java",
                "patch": "@@ -55,12 +55,15 @@\n import org.apache.ambari.server.state.DesiredConfig;\n import org.apache.ambari.server.state.Host;\n import org.apache.ambari.server.state.MaintenanceState;\n+import org.apache.ambari.server.state.Service;\n+import org.apache.ambari.server.state.ServiceComponent;\n import org.apache.ambari.server.state.ServiceComponentHost;\n import org.apache.ambari.server.state.stack.OsFamily;\n import org.apache.ambari.server.topology.InvalidTopologyException;\n import org.apache.ambari.server.topology.InvalidTopologyTemplateException;\n import org.apache.ambari.server.topology.TopologyManager;\n import org.apache.ambari.server.topology.TopologyRequest;\n+import org.apache.commons.lang.StringUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -775,27 +778,47 @@ protected void deleteHosts(Set<HostRequest> requests)\n         continue;\n       }\n \n+      Set<String> clusterNamesForHost = new HashSet<String>();\n       if (null != hostRequest.getClusterName()) {\n-        Cluster cluster = clusters.getCluster(hostRequest.getClusterName());\n+        clusterNamesForHost.add(hostRequest.getClusterName());\n+      } else {\n+        Set<Cluster> clustersForHost = clusters.getClustersForHost(hostRequest.getHostname());\n+        if (null != clustersForHost) {\n+          for (Cluster c : clustersForHost) {\n+            clusterNamesForHost.add(c.getClusterName());\n+          }\n+        }\n+      }\n+\n+      for (String clusterName : clusterNamesForHost) {\n+        Cluster cluster = clusters.getCluster(clusterName);\n \n         List<ServiceComponentHost> list = cluster.getServiceComponentHosts(hostName);\n \n-        if (0 != list.size()) {\n-          StringBuilder reason = new StringBuilder(\"Cannot remove host \")\n-              .append(hostName)\n-              .append(\" from \")\n-              .append(hostRequest.getClusterName())\n-              .append(\".  The following roles exist: \");\n+        if (!list.isEmpty()) {\n \n-          int i = 0;\n+          List<String> componentsToRemove = new ArrayList<String>();\n           for (ServiceComponentHost sch : list) {\n-            if ((i++) > 0) {\n-              reason.append(\", \");\n+            Service s = cluster.getService(sch.getServiceName());\n+            ServiceComponent sc = s.getServiceComponent(sch.getServiceComponentName());\n+\n+            // Masters and Slaves must be deleted first. Clients are ok.\n+            if (!sc.isClientComponent()) {\n+              componentsToRemove.add(sch.getServiceComponentName());\n             }\n-            reason.append(sch.getServiceComponentName());\n           }\n \n-          throw new AmbariException(reason.toString());\n+          if (!componentsToRemove.isEmpty()) {\n+            StringBuilder reason = new StringBuilder(\"Cannot remove host \")\n+                .append(hostName)\n+                .append(\" from \")\n+                .append(hostRequest.getClusterName())\n+                .append(\".  The following roles exist, and these components must be stopped if running, and then deleted: \");\n+\n+            reason.append(StringUtils.join(componentsToRemove, \", \"));\n+\n+            throw new AmbariException(reason.toString());\n+          }\n         }\n       }\n       okToRemove.add(hostRequest);",
                "raw_url": "https://github.com/apache/ambari/raw/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/HostResourceProvider.java",
                "sha": "c7f0e9b94d77ea8d27bcf7c792192be1660006ed",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/RequestOperationLevel.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/RequestOperationLevel.java?ref=358dc1d8eb361480932380d8546d58b264a25c38",
                "deletions": 2,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/RequestOperationLevel.java",
                "patch": "@@ -48,7 +48,7 @@\n   public static final String OPERATION_CLUSTER_ID = \"operation_level/cluster_name\";\n   public static final String OPERATION_SERVICE_ID = \"operation_level/service_name\";\n   public static final String OPERATION_HOSTCOMPONENT_ID = \"operation_level/hostcomponent_name\";\n-  public static final String OPERATION_HOST_ID = \"operation_level/host_name\";\n+  public static final String OPERATION_HOST_NAME = \"operation_level/host_name\";\n \n   /**\n    * Converts external operation level alias to an internal name\n@@ -117,7 +117,7 @@ public RequestOperationLevel(Map<String, String> requestInfoProperties)\n     this.serviceName = requestInfoProperties.get(OPERATION_SERVICE_ID);\n     this.hostComponentName =\n             requestInfoProperties.get(OPERATION_HOSTCOMPONENT_ID);\n-    this.hostName = requestInfoProperties.get(OPERATION_HOST_ID);\n+    this.hostName = requestInfoProperties.get(OPERATION_HOST_NAME);\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/ambari/raw/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/RequestOperationLevel.java",
                "sha": "ae57c5035a972f6e100c4e442498ea64c6b5eea2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/RequestResourceProvider.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/RequestResourceProvider.java?ref=358dc1d8eb361480932380d8546d58b264a25c38",
                "deletions": 1,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/RequestResourceProvider.java",
                "patch": "@@ -99,7 +99,7 @@\n   protected static final String COMMAND_ID = \"command\";\n   protected static final String SERVICE_ID = \"service_name\";\n   protected static final String COMPONENT_ID = \"component_name\";\n-  protected static final String HOSTS_ID = \"hosts\";\n+  protected static final String HOSTS_ID = \"hosts\";                           // This is actually a list of hosts\n   protected static final String ACTION_ID = \"action\";\n   protected static final String INPUTS_ID = \"parameters\";\n   protected static final String EXLUSIVE_ID = \"exclusive\";",
                "raw_url": "https://github.com/apache/ambari/raw/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/RequestResourceProvider.java",
                "sha": "911e3cda77d0b37b97275e4d4a6b3a117361998a",
                "status": "modified"
            },
            {
                "additions": 82,
                "blob_url": "https://github.com/apache/ambari/blob/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/main/java/org/apache/ambari/server/orm/dao/RequestOperationLevelDAO.java",
                "changes": 82,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/orm/dao/RequestOperationLevelDAO.java?ref=358dc1d8eb361480932380d8546d58b264a25c38",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/orm/dao/RequestOperationLevelDAO.java",
                "patch": "@@ -0,0 +1,82 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.ambari.server.orm.dao;\n+\n+import com.google.inject.Inject;\n+import com.google.inject.Provider;\n+import com.google.inject.Singleton;\n+import com.google.inject.persist.Transactional;\n+import org.apache.ambari.server.orm.RequiresSession;\n+import org.apache.ambari.server.orm.entities.RequestOperationLevelEntity;\n+\n+import javax.persistence.EntityManager;\n+import javax.persistence.TypedQuery;\n+import java.util.Collection;\n+import java.util.List;\n+\n+@Singleton\n+public class RequestOperationLevelDAO {\n+  @Inject\n+  Provider<EntityManager> entityManagerProvider;\n+  @Inject\n+  DaoUtils daoUtils;\n+\n+\n+  @RequiresSession\n+  public List<RequestOperationLevelEntity> findByHostId(Long hostId) {\n+    final TypedQuery<RequestOperationLevelEntity> query = entityManagerProvider.get()\n+        .createNamedQuery(\"requestOperationLevelByHostId\", RequestOperationLevelEntity.class);\n+    query.setParameter(\"hostId\", hostId);\n+    return daoUtils.selectList(query);\n+  }\n+\n+  @RequiresSession\n+  public List<RequestOperationLevelEntity> findAll() {\n+    return daoUtils.selectAll(entityManagerProvider.get(), RequestOperationLevelEntity.class);\n+  }\n+\n+  @Transactional\n+  public void refresh(RequestOperationLevelEntity requestOperationLevelEntity) {\n+    entityManagerProvider.get().refresh(requestOperationLevelEntity);\n+  }\n+\n+  @Transactional\n+  public void create(RequestOperationLevelEntity requestOperationLevelEntity) {\n+    entityManagerProvider.get().persist(requestOperationLevelEntity);\n+  }\n+\n+  @Transactional\n+  public RequestOperationLevelEntity merge(RequestOperationLevelEntity requestOperationLevelEntity) {\n+    return entityManagerProvider.get().merge(requestOperationLevelEntity);\n+  }\n+\n+  @Transactional\n+  public void remove(RequestOperationLevelEntity requestOperationLevelEntity) {\n+    entityManagerProvider.get().remove(merge(requestOperationLevelEntity));\n+  }\n+\n+  @Transactional\n+  public void removeByHostId(Long hostId) {\n+    Collection<RequestOperationLevelEntity> hostRequestOpLevels = this.findByHostId(hostId);\n+    for (RequestOperationLevelEntity hostRequestOpLevel : hostRequestOpLevels) {\n+      this.remove(hostRequestOpLevel);\n+    }\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/ambari/raw/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/main/java/org/apache/ambari/server/orm/dao/RequestOperationLevelDAO.java",
                "sha": "346b87b39150e1fdcd68b6fbfab123a58eb12f87",
                "status": "added"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/ambari/blob/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/main/java/org/apache/ambari/server/orm/entities/RequestOperationLevelEntity.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/orm/entities/RequestOperationLevelEntity.java?ref=358dc1d8eb361480932380d8546d58b264a25c38",
                "deletions": 3,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/orm/entities/RequestOperationLevelEntity.java",
                "patch": "@@ -22,13 +22,12 @@\n import javax.persistence.Basic;\n import javax.persistence.Column;\n import javax.persistence.Entity;\n-import javax.persistence.EnumType;\n-import javax.persistence.Enumerated;\n import javax.persistence.GeneratedValue;\n import javax.persistence.GenerationType;\n import javax.persistence.Id;\n import javax.persistence.JoinColumn;\n-import javax.persistence.ManyToOne;\n+import javax.persistence.NamedQueries;\n+import javax.persistence.NamedQuery;\n import javax.persistence.OneToOne;\n import javax.persistence.Table;\n import javax.persistence.TableGenerator;\n@@ -40,6 +39,11 @@\n   , pkColumnValue = \"operation_level_id_seq\"\n   , initialValue = 1\n )\n+@NamedQueries({\n+    @NamedQuery(name = \"requestOperationLevelByHostId\", query =\n+        \"SELECT requestOperationLevel FROM RequestOperationLevelEntity requestOperationLevel \" +\n+            \"WHERE requestOperationLevel.hostId=:hostId\")\n+})\n public class RequestOperationLevelEntity {\n \n   @Id",
                "raw_url": "https://github.com/apache/ambari/raw/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/main/java/org/apache/ambari/server/orm/entities/RequestOperationLevelEntity.java",
                "sha": "c03816e84a3a4da1053215d8a5f33fabef35c71e",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/ambari/blob/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClustersImpl.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClustersImpl.java?ref=358dc1d8eb361480932380d8546d58b264a25c38",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClustersImpl.java",
                "patch": "@@ -51,6 +51,7 @@\n import org.apache.ambari.server.orm.dao.HostStateDAO;\n import org.apache.ambari.server.orm.dao.HostVersionDAO;\n import org.apache.ambari.server.orm.dao.KerberosPrincipalHostDAO;\n+import org.apache.ambari.server.orm.dao.RequestOperationLevelDAO;\n import org.apache.ambari.server.orm.dao.ResourceTypeDAO;\n import org.apache.ambari.server.orm.dao.ServiceConfigDAO;\n import org.apache.ambari.server.orm.dao.StackDAO;\n@@ -117,6 +118,8 @@\n   @Inject\n   private ResourceTypeDAO resourceTypeDAO;\n   @Inject\n+  private RequestOperationLevelDAO requestOperationLevelDAO;\n+  @Inject\n   private KerberosPrincipalHostDAO kerberosPrincipalHostDAO;\n   @Inject\n   private HostConfigMappingDAO hostConfigMappingDAO;\n@@ -811,6 +814,7 @@ public void deleteHost(String hostname) throws AmbariException {\n       hostStateDAO.removeByHostId(entity.getHostId());\n       hostConfigMappingDAO.removeByHostId(entity.getHostId());\n       serviceConfigDAO.removeHostFromServiceConfigs(entity.getHostId());\n+      requestOperationLevelDAO.removeByHostId(entity.getHostId());\n \n       // Remove from dictionaries\n       hosts.remove(hostname);",
                "raw_url": "https://github.com/apache/ambari/raw/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClustersImpl.java",
                "sha": "90fdbec5d377beef6cc7664f0bbbbaf1797330b5",
                "status": "modified"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/ambari/blob/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/main/java/org/apache/ambari/server/state/svccomphost/ServiceComponentHostImpl.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/state/svccomphost/ServiceComponentHostImpl.java?ref=358dc1d8eb361480932380d8546d58b264a25c38",
                "deletions": 5,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/state/svccomphost/ServiceComponentHostImpl.java",
                "patch": "@@ -1142,14 +1142,31 @@ public void setComponentAdminState(HostComponentAdminState attribute) {\n   public ServiceComponentHostResponse convertToResponse() {\n     readLock.lock();\n     try {\n+      HostComponentStateEntity hostComponentStateEntity = getStateEntity();\n+      if (null == hostComponentStateEntity) {\n+        LOG.warn(\"Could not convert ServiceComponentHostResponse to a response. It's possible that Host \" + getHostName() + \" was deleted.\");\n+        return null;\n+      }\n+\n+      String clusterName = serviceComponent.getClusterName();\n+      String serviceName = serviceComponent.getServiceName();\n+      String serviceComponentName = serviceComponent.getName();\n+      String hostName = getHostName();\n+      String state = getState().toString();\n+      String stackId = getStackVersion().getStackId();\n+      String desiredState = getDesiredState().toString();\n+      String desiredStackId = getDesiredStackVersion().getStackId();\n+      HostComponentAdminState componentAdminState = getComponentAdminState();\n+      UpgradeState upgradeState = hostComponentStateEntity.getUpgradeState();\n+\n       ServiceComponentHostResponse r = new ServiceComponentHostResponse(\n-          serviceComponent.getClusterName(), serviceComponent.getServiceName(),\n-          serviceComponent.getName(), getHostName(), getState().toString(),\n-          getStackVersion().getStackId(), getDesiredState().toString(),\n-          getDesiredStackVersion().getStackId(), getComponentAdminState());\n+          clusterName, serviceName,\n+          serviceComponentName, hostName, state,\n+          stackId, desiredState,\n+          desiredStackId, componentAdminState);\n \n       r.setActualConfigs(actualConfigs);\n-      r.setUpgradeState(getStateEntity().getUpgradeState());\n+      r.setUpgradeState(upgradeState);\n \n       try {\n         r.setStaleConfig(helper.isStaleConfigs(this));",
                "raw_url": "https://github.com/apache/ambari/raw/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/main/java/org/apache/ambari/server/state/svccomphost/ServiceComponentHostImpl.java",
                "sha": "b6234797ac83c08b9738ca560d02a01b1b89ba92",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/RequestOperationLevelTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/RequestOperationLevelTest.java?ref=358dc1d8eb361480932380d8546d58b264a25c38",
                "deletions": 1,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/controller/internal/RequestOperationLevelTest.java",
                "patch": "@@ -50,7 +50,7 @@ public void test_ConstructionFromRequestProperties() throws Exception {\n             service_id);\n     requestInfoProperties.put(RequestOperationLevel.OPERATION_HOSTCOMPONENT_ID,\n             hostcomponent_id);\n-    requestInfoProperties.put(RequestOperationLevel.OPERATION_HOST_ID,\n+    requestInfoProperties.put(RequestOperationLevel.OPERATION_HOST_NAME,\n             host_id);\n \n     // Check normal creation",
                "raw_url": "https://github.com/apache/ambari/raw/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/RequestOperationLevelTest.java",
                "sha": "6a38b1dd8571b361e401de8025caee415c2b9385",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/ambari/blob/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/RequestResourceProviderTest.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/RequestResourceProviderTest.java?ref=358dc1d8eb361480932380d8546d58b264a25c38",
                "deletions": 5,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/controller/internal/RequestResourceProviderTest.java",
                "patch": "@@ -1047,14 +1047,14 @@ public void testCreateResourcesForCommandsWithOpLvl() throws Exception {\n     String host_component = \"HOST_COMPONENT\";\n     String service_id = \"HDFS\";\n     String hostcomponent_id = \"Namenode\";\n-    String host_id = \"host1\";\n+    String host_name = \"host1\";\n \n     properties.put(RequestResourceProvider.REQUEST_CLUSTER_NAME_PROPERTY_ID, c1);\n \n     Set<Map<String, Object>> filterSet = new HashSet<Map<String, Object>>();\n     Map<String, Object> filterMap = new HashMap<String, Object>();\n     filterMap.put(RequestResourceProvider.SERVICE_ID, service_id);\n-    filterMap.put(RequestResourceProvider.HOSTS_ID, host_id);\n+    filterMap.put(RequestResourceProvider.HOSTS_ID, host_name);\n     filterSet.add(filterMap);\n \n     properties.put(RequestResourceProvider.REQUEST_RESOURCE_FILTER_ID, filterSet);\n@@ -1071,8 +1071,8 @@ public void testCreateResourcesForCommandsWithOpLvl() throws Exception {\n             service_id);\n     requestInfoProperties.put(RequestOperationLevel.OPERATION_HOSTCOMPONENT_ID,\n             hostcomponent_id);\n-    requestInfoProperties.put(RequestOperationLevel.OPERATION_HOST_ID,\n-            host_id);\n+    requestInfoProperties.put(RequestOperationLevel.OPERATION_HOST_NAME,\n+        host_name);\n \n     Request request = PropertyHelper.getCreateRequest(propertySet, requestInfoProperties);\n     ResourceProvider provider = AbstractControllerResourceProvider.getResourceProvider(\n@@ -1092,7 +1092,7 @@ public void testCreateResourcesForCommandsWithOpLvl() throws Exception {\n     Assert.assertEquals(level.getClusterName(), c1);\n     Assert.assertEquals(level.getServiceName(), service_id);\n     Assert.assertEquals(level.getHostComponentName(), hostcomponent_id);\n-    Assert.assertEquals(level.getHostName(), host_id);\n+    Assert.assertEquals(level.getHostName(), host_name);\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/ambari/raw/358dc1d8eb361480932380d8546d58b264a25c38/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/RequestResourceProviderTest.java",
                "sha": "d150ab129ed72a1a03fa8386e8da1e8c21bc7b10",
                "status": "modified"
            }
        ],
        "message": "AMBARI-12429. Deleting a host using the API causes NPE (alejandro)",
        "parent": "https://github.com/apache/ambari/commit/a56a5912ac6947ba534a75e3b43e46ae35c0cbdb",
        "repo": "ambari",
        "unit_tests": [
            "AmbariManagementControllerImplTest.java",
            "HostResourceProviderTest.java",
            "RequestOperationLevelTest.java",
            "RequestResourceProviderTest.java",
            "ClustersImplTest.java"
        ]
    },
    "ambari_3ae844f": {
        "bug_id": "ambari_3ae844f",
        "commit": "https://github.com/apache/ambari/commit/3ae844f0a9439cb29945e16c8f9963d4261e7c40",
        "file": [
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/ambari/blob/3ae844f0a9439cb29945e16c8f9963d4261e7c40/ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java?ref=3ae844f0a9439cb29945e16c8f9963d4261e7c40",
                "deletions": 11,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java",
                "patch": "@@ -511,17 +511,21 @@ public static void checkServiceConfigs()  {\n \n         //compare required service configs from stack with mapped service configs from db\n         Map<Integer, Multimap<String, String>> dbServiceVersionConfigs = dbClusterServiceVersionConfigs.get(clusterName);\n-        for (Integer serviceVersion : dbServiceVersionConfigs.keySet()) {\n-          Multimap<String, String> dbServiceConfigs = dbServiceVersionConfigs.get(serviceVersion);\n-          for (String serviceName : dbServiceConfigs.keySet()) {\n-            Collection<String> serviceConfigsFromStack = stackServiceConfigs.get(serviceName);\n-            Collection<String> serviceConfigsFromDB = dbServiceConfigs.get(serviceName);\n-            if (serviceConfigsFromDB != null && serviceConfigsFromStack != null) {\n-              serviceConfigsFromStack.removeAll(serviceConfigsFromDB);\n-              if (!serviceConfigsFromStack.isEmpty()) {\n-                LOG.error(\"Required config(s): {} is(are) not available for service {} with service config version {} in cluster {}\",\n-                        StringUtils.join(serviceConfigsFromStack, \",\"), serviceName, Integer.toString(serviceVersion), clusterName);\n-                errorAvailable = true;\n+        if (dbServiceVersionConfigs != null) {\n+          for (Integer serviceVersion : dbServiceVersionConfigs.keySet()) {\n+            Multimap<String, String> dbServiceConfigs = dbServiceVersionConfigs.get(serviceVersion);\n+            if (dbServiceConfigs != null) {\n+              for (String serviceName : dbServiceConfigs.keySet()) {\n+                Collection<String> serviceConfigsFromStack = stackServiceConfigs.get(serviceName);\n+                Collection<String> serviceConfigsFromDB = dbServiceConfigs.get(serviceName);\n+                if (serviceConfigsFromDB != null && serviceConfigsFromStack != null) {\n+                  serviceConfigsFromStack.removeAll(serviceConfigsFromDB);\n+                  if (!serviceConfigsFromStack.isEmpty()) {\n+                    LOG.error(\"Required config(s): {} is(are) not available for service {} with service config version {} in cluster {}\",\n+                            StringUtils.join(serviceConfigsFromStack, \",\"), serviceName, Integer.toString(serviceVersion), clusterName);\n+                    errorAvailable = true;\n+                  }\n+                }\n               }\n             }\n           }",
                "raw_url": "https://github.com/apache/ambari/raw/3ae844f0a9439cb29945e16c8f9963d4261e7c40/ambari-server/src/main/java/org/apache/ambari/server/checks/DatabaseConsistencyCheckHelper.java",
                "sha": "3035de98756059dad194323208c6a4a5e417e7be",
                "status": "modified"
            }
        ],
        "message": "AMBARI-17430. Fix serviceconfig NPE in db consistency checker.(vbrodetskyi)",
        "parent": "https://github.com/apache/ambari/commit/0fc5b8fd3c474881cff2eaf2ebdafa91068336ae",
        "repo": "ambari",
        "unit_tests": [
            "DatabaseConsistencyCheckHelperTest.java"
        ]
    },
    "ambari_3b1fcf3": {
        "bug_id": "ambari_3b1fcf3",
        "commit": "https://github.com/apache/ambari/commit/3b1fcf36a6771a989f4b188f7018ea2a574f68df",
        "file": [
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/ambari/blob/3b1fcf36a6771a989f4b188f7018ea2a574f68df/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ClusterResourceProvider.java",
                "changes": 31,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ClusterResourceProvider.java?ref=3b1fcf36a6771a989f4b188f7018ea2a574f68df",
                "deletions": 8,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ClusterResourceProvider.java",
                "patch": "@@ -720,15 +720,30 @@ private void processConfigurations(Map<String, Map<String, String>> blueprintCon\n     }\n     // AMBARI-4921\n     //todo: hard-coding default values for required global config properties which are not in stack definition\n-    Map<String, String> globalProperties = mapClusterConfigurations.get(\"global\");\n-    if (globalProperties == null) {\n-      globalProperties = new HashMap<String, String>();\n-      mapClusterConfigurations.put(\"global\", globalProperties);\n+    ensureProperty(\"global\", \"user_group\", \"hadoop\");\n+    ensureProperty(\"global\", \"nagios_contact\", \"default@REPLACEME.NOWHERE\");\n+    ensureProperty(\"global\", \"nagios_web_password\", \"admin\");\n+    ensureProperty(\"global\", \"smokeuser\", \"ambari-qa\");\n+  }\n+\n+  /**\n+   * Ensure that the specified property exists.\n+   * If not, set a default value.\n+   *\n+   * @param type          config type\n+   * @param property      property name\n+   * @param defaultValue  default value\n+   */\n+  private void ensureProperty(String type, String property, String defaultValue) {\n+    Map<String, String> properties = mapClusterConfigurations.get(type);\n+    if (properties == null) {\n+      properties = new HashMap<String, String>();\n+      mapClusterConfigurations.put(type, properties);\n+    }\n+\n+    if (! properties.containsKey(property)) {\n+      properties.put(property, defaultValue);\n     }\n-    globalProperties.put(\"user_group\", \"hadoop\");\n-    globalProperties.put(\"smokeuser\", \"ambari-qa\");\n-    globalProperties.put(\"nagios_contact\", \"default@REPLACEME.NOWHERE\");\n-    globalProperties.put(\"nagios_web_password\", \"admin\");\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/ambari/raw/3b1fcf36a6771a989f4b188f7018ea2a574f68df/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ClusterResourceProvider.java",
                "sha": "8afee6e13ff6ee36485eaf83ea1acaaf5dd8a21d",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/ambari/blob/3b1fcf36a6771a989f4b188f7018ea2a574f68df/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/RequestStageContainer.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/RequestStageContainer.java?ref=3b1fcf36a6771a989f4b188f7018ea2a574f68df",
                "deletions": 3,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/RequestStageContainer.java",
                "patch": "@@ -33,6 +33,7 @@\n import java.util.ArrayList;\n import java.util.List;\n import java.util.ListIterator;\n+import java.util.Map;\n \n /**\n  * Contains stages associated with a request.\n@@ -132,9 +133,16 @@ public State getProjectedState(String host, String component) {\n     ListIterator<Stage> iterator = stages.listIterator(stages.size());\n     while (lastCommand == null && iterator.hasPrevious()) {\n       Stage stage = iterator.previous();\n-      HostRoleCommand hostRoleCommand = stage.getHostRoleCommand(host, component);\n-      if (hostRoleCommand != null && hostRoleCommand.getRoleCommand() != RoleCommand.SERVICE_CHECK) {\n-        lastCommand = hostRoleCommand.getRoleCommand();\n+\n+      Map<String, Map<String, HostRoleCommand>> stageCommands = stage.getHostRoleCommands();\n+      if (stageCommands != null) {\n+        Map<String, HostRoleCommand> hostCommands = stageCommands.get(host);\n+        if (hostCommands != null) {\n+          HostRoleCommand roleCommand = hostCommands.get(component);\n+          if (roleCommand != null && roleCommand.getRoleCommand() != RoleCommand.SERVICE_CHECK) {\n+            lastCommand = roleCommand.getRoleCommand();\n+          }\n+        }\n       }\n     }\n ",
                "raw_url": "https://github.com/apache/ambari/raw/3b1fcf36a6771a989f4b188f7018ea2a574f68df/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/RequestStageContainer.java",
                "sha": "e8b9adf866ad41d6f0077e15cd52c411e4a18fb6",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/ambari/blob/3b1fcf36a6771a989f4b188f7018ea2a574f68df/ambari-server/src/main/resources/Ambari-DDL-Oracle-CREATE.sql",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/resources/Ambari-DDL-Oracle-CREATE.sql?ref=3b1fcf36a6771a989f4b188f7018ea2a574f68df",
                "deletions": 3,
                "filename": "ambari-server/src/main/resources/Ambari-DDL-Oracle-CREATE.sql",
                "patch": "@@ -81,9 +81,9 @@ ALTER TABLE confgroupclusterconfigmapping ADD CONSTRAINT FK_cgccm_gid FOREIGN KE\n ALTER TABLE configgrouphostmapping ADD CONSTRAINT FK_cghm_cgid FOREIGN KEY (config_group_id) REFERENCES configgroup (group_id);\n ALTER TABLE configgrouphostmapping ADD CONSTRAINT FK_cghm_hname FOREIGN KEY (host_name) REFERENCES hosts (host_name);\n ALTER TABLE requestschedulebatchrequest ADD CONSTRAINT FK_rsbatchrequest_schedule_id FOREIGN KEY (schedule_id) REFERENCES requestschedule (schedule_id);\n-ALTER TABLE hostgroup ADD FOREIGN KEY (blueprint_name) REFERENCES ambari.blueprint(blueprint_name);\n-ALTER TABLE hostgroup_component ADD FOREIGN KEY (blueprint_name, hostgroup_name) REFERENCES ambari.hostgroup(blueprint_name, name);\n-ALTER TABLE blueprint_configuration ADD FOREIGN KEY (blueprint_name) REFERENCES ambari.blueprint(blueprint_name);\n+ALTER TABLE hostgroup ADD FOREIGN KEY (blueprint_name) REFERENCES blueprint(blueprint_name);\n+ALTER TABLE hostgroup_component ADD FOREIGN KEY (blueprint_name, hostgroup_name) REFERENCES hostgroup(blueprint_name, name);\n+ALTER TABLE blueprint_configuration ADD FOREIGN KEY (blueprint_name) REFERENCES blueprint(blueprint_name);\n ALTER TABLE requestresourcefilter ADD CONSTRAINT FK_requestresourcefilter_req_id FOREIGN KEY (request_id) REFERENCES request (request_id);\n \n INSERT INTO ambari_sequences(sequence_name, value) values ('host_role_command_id_seq', 0);",
                "raw_url": "https://github.com/apache/ambari/raw/3b1fcf36a6771a989f4b188f7018ea2a574f68df/ambari-server/src/main/resources/Ambari-DDL-Oracle-CREATE.sql",
                "sha": "afe64977e3200ce3df72c422e6c9db0f757a4f52",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/ambari/blob/3b1fcf36a6771a989f4b188f7018ea2a574f68df/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/RequestStageContainerTest.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/RequestStageContainerTest.java?ref=3b1fcf36a6771a989f4b188f7018ea2a574f68df",
                "deletions": 4,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/controller/internal/RequestStageContainerTest.java",
                "patch": "@@ -34,6 +34,7 @@\n import java.util.ArrayList;\n import java.util.Collections;\n import java.util.List;\n+import java.util.Map;\n \n import static org.easymock.EasyMock.createNiceMock;\n import static org.easymock.EasyMock.createStrictMock;\n@@ -114,10 +115,10 @@ public void testGetProjectedState() {\n     stages.add(stage4);\n \n     //expectations\n-    expect(stage1.getHostRoleCommand(hostname, componentName)).andReturn(command1).anyTimes();\n-    expect(stage2.getHostRoleCommand(hostname, componentName)).andReturn(command2).anyTimes();\n-    expect(stage3.getHostRoleCommand(hostname, componentName)).andReturn(command3).anyTimes();\n-    expect(stage4.getHostRoleCommand(hostname, componentName)).andReturn(null).anyTimes();\n+    expect(stage1.getHostRoleCommands()).andReturn(Collections.singletonMap(hostname, Collections.singletonMap(componentName, command1))).anyTimes();\n+    expect(stage2.getHostRoleCommands()).andReturn(Collections.singletonMap(hostname, Collections.singletonMap(componentName, command2))).anyTimes();\n+    expect(stage3.getHostRoleCommands()).andReturn(Collections.singletonMap(hostname, Collections.singletonMap(componentName, command3))).anyTimes();\n+    expect(stage4.getHostRoleCommands()).andReturn(Collections.<String, Map<String, HostRoleCommand>>emptyMap()).anyTimes();\n \n     expect(command3.getRoleCommand()).andReturn(RoleCommand.SERVICE_CHECK).anyTimes();\n     expect(command2.getRoleCommand()).andReturn(RoleCommand.INSTALL).anyTimes();",
                "raw_url": "https://github.com/apache/ambari/raw/3b1fcf36a6771a989f4b188f7018ea2a574f68df/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/RequestStageContainerTest.java",
                "sha": "8c58517efbbf83d33ce99e40da015c19b6152755",
                "status": "modified"
            }
        ],
        "message": "AMBARI-5145.  Fixed an issue that could result in a NPE when deploying a cluster via a blueprint.",
        "parent": "https://github.com/apache/ambari/commit/5b72f0b54cbf0350cfbbad78ae9125174ff9afd2",
        "repo": "ambari",
        "unit_tests": [
            "ClusterResourceProviderTest.java",
            "RequestStageContainerTest.java"
        ]
    },
    "ambari_40637b1": {
        "bug_id": "ambari_40637b1",
        "commit": "https://github.com/apache/ambari/commit/40637b16746ea4e2eb772e909cee83989ec08d7e",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/ambari/blob/40637b16746ea4e2eb772e909cee83989ec08d7e/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariCustomCommandExecutionHelper.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariCustomCommandExecutionHelper.java?ref=40637b16746ea4e2eb772e909cee83989ec08d7e",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariCustomCommandExecutionHelper.java",
                "patch": "@@ -557,6 +557,16 @@ private void findHostAndAddServiceCheckAction(final ActionExecutionContext actio\n       candidateHosts = serviceHostComponents.keySet();\n     }\n \n+    // check if all hostnames are valid.\n+    for(String candidateHostName: candidateHosts) {\n+      ServiceComponentHost serviceComponentHost = serviceHostComponents.get(candidateHostName);\n+\n+      if (serviceComponentHost == null) {\n+        throw new AmbariException(\"Provided hostname = \"\n+            + candidateHostName + \" is either not a valid cluster host or does not satisfy the filter condition.\");\n+      }\n+    }\n+\n     // Filter out hosts that are in maintenance mode - they should never be included in service checks\n     Set<String> hostsInMaintenanceMode = new HashSet<String>();\n     if (actionExecutionContext.isMaintenanceModeHostExcluded()) {",
                "raw_url": "https://github.com/apache/ambari/raw/40637b16746ea4e2eb772e909cee83989ec08d7e/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariCustomCommandExecutionHelper.java",
                "sha": "0cf46df5e70adb60a12474e4a130dc7949b698c2",
                "status": "modified"
            }
        ],
        "message": "AMBARI-18623. NPE when a non-existent host is provided as part of the host filter (aonishuk)",
        "parent": "https://github.com/apache/ambari/commit/8d5e76fe84ffdc7eee43b6a9a08c984a8bcf4c5b",
        "repo": "ambari",
        "unit_tests": [
            "AmbariCustomCommandExecutionHelperTest.java"
        ]
    },
    "ambari_417a1f0": {
        "bug_id": "ambari_417a1f0",
        "commit": "https://github.com/apache/ambari/commit/417a1f03c3bf908265881313da835cad2dab3d4d",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/417a1f03c3bf908265881313da835cad2dab3d4d/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java?ref=417a1f03c3bf908265881313da835cad2dab3d4d",
                "deletions": 2,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java",
                "patch": "@@ -215,7 +215,7 @@ public BlueprintConfigurationProcessor(ClusterTopology clusterTopology) {\n           // cluster scoped configuration which also includes all default and BP properties\n           Map<String, Map<String, String>> clusterProps = clusterTopology.getConfiguration().getFullProperties();\n           Map<String, String> typeMap = clusterProps.get(type);\n-          if (typeMap != null && typeMap.containsKey(propertyName)) {\n+          if (typeMap != null && typeMap.containsKey(propertyName) && typeMap.get(propertyName) != null) {\n             requiredHostGroups.addAll(updater.getRequiredHostGroups(\n                 propertyName, typeMap.get(propertyName), clusterProps, clusterTopology));\n           }\n@@ -266,7 +266,7 @@ public BlueprintConfigurationProcessor(ClusterTopology clusterTopology) {\n \n           // topo cluster scoped configuration which also includes all default and BP properties\n           Map<String, String> typeMap = clusterProps.get(type);\n-          if (typeMap != null && typeMap.containsKey(propertyName)) {\n+          if (typeMap != null && typeMap.containsKey(propertyName) && typeMap.get(propertyName) != null) {\n             final String originalValue = typeMap.get(propertyName);\n             final String updatedValue =\n               updater.updateForClusterCreate(propertyName, originalValue, clusterProps, clusterTopology);",
                "raw_url": "https://github.com/apache/ambari/raw/417a1f03c3bf908265881313da835cad2dab3d4d/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java",
                "sha": "a295bf77ec0d930c000c1740a05aff999c03c0e1",
                "status": "modified"
            }
        ],
        "message": "AMBARI-17793: Blueprint deployment throwing NPE when there is a config marked as deleted (jluniya)",
        "parent": "https://github.com/apache/ambari/commit/9f79fa9f5ec5ccb50db7e6a28656f85bc2e3c3ff",
        "repo": "ambari",
        "unit_tests": [
            "BlueprintConfigurationProcessorTest.java"
        ]
    },
    "ambari_4ecb3c1": {
        "bug_id": "ambari_4ecb3c1",
        "commit": "https://github.com/apache/ambari/commit/4ecb3c10fede992fde82e54cf252f28160ba5f07",
        "file": [
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/ambari/blob/4ecb3c10fede992fde82e54cf252f28160ba5f07/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/Stack.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/Stack.java?ref=4ecb3c10fede992fde82e54cf252f28160ba5f07",
                "deletions": 3,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/Stack.java",
                "patch": "@@ -168,7 +168,7 @@ public Stack(String name, String version, AmbariManagementController controller)\n       String serviceName = stackService.getServiceName();\n       parseComponents(serviceName);\n       parseExcludedConfigurations(stackService);\n-      parseConfigurations(serviceName);\n+      parseConfigurations(stackService);\n       registerConditionalDependencies();\n     }\n \n@@ -646,14 +646,16 @@ private void parseComponents(String service) throws AmbariException{\n   /**\n    * Parse configurations for the specified service from the stack definition.\n    *\n-   * @param service  service name\n+   * @param stackService  service to parse the stack configuration for\n    *\n    * @throws AmbariException an exception occurred getting configurations from the stack definition\n    */\n-  private void parseConfigurations(String service) throws AmbariException {\n+  private void parseConfigurations(StackServiceResponse stackService) throws AmbariException {\n+    String service = stackService.getServiceName();\n     Map<String, Map<String, ConfigProperty>> mapServiceConfig = new HashMap<String, Map<String, ConfigProperty>>();\n     Map<String, Map<String, ConfigProperty>> mapRequiredServiceConfig = new HashMap<String, Map<String, ConfigProperty>>();\n \n+\n     serviceConfigurations.put(service, mapServiceConfig);\n     requiredServiceConfigurations.put(service, mapRequiredServiceConfig);\n \n@@ -684,6 +686,16 @@ private void parseConfigurations(String service) throws AmbariException {\n         requiredTypeConfig.put(config.getPropertyName(), configProperty);\n       }\n     }\n+\n+    // So far we added only config types that have properties defined\n+    // in stack service definition. Since there might be config types\n+    // with no properties defined we need to add those separately\n+    Set<String> configTypes = stackService.getConfigTypes().keySet();\n+    for (String configType: configTypes) {\n+      if (!mapServiceConfig.containsKey(configType)) {\n+        mapServiceConfig.put(configType, Collections.<String, ConfigProperty>emptyMap());\n+      }\n+    }\n   }\n \n   private void parseStackConfigurations () throws AmbariException {",
                "raw_url": "https://github.com/apache/ambari/raw/4ecb3c10fede992fde82e54cf252f28160ba5f07/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/Stack.java",
                "sha": "986dce901a7d4797b49e24a2033aa80227252912",
                "status": "modified"
            },
            {
                "additions": 86,
                "blob_url": "https://github.com/apache/ambari/blob/4ecb3c10fede992fde82e54cf252f28160ba5f07/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/StackTest.java",
                "changes": 86,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/StackTest.java?ref=4ecb3c10fede992fde82e54cf252f28160ba5f07",
                "deletions": 0,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/controller/internal/StackTest.java",
                "patch": "@@ -33,22 +33,33 @@\n import org.easymock.Capture;\n import org.easymock.EasyMock;\n import org.easymock.EasyMockSupport;\n+import org.hamcrest.CoreMatchers;\n+import org.hamcrest.Matcher;\n import org.junit.Test;\n+import org.mockito.Matchers;\n \n import java.util.Arrays;\n import java.util.Collection;\n import java.util.Collections;\n import java.util.HashSet;\n+import java.util.Map;\n import java.util.Set;\n \n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.ImmutableSet;\n+import com.google.common.collect.Sets;\n+\n import static org.easymock.EasyMock.capture;\n import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.anyObject;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertSame;\n+import static org.junit.Assert.assertThat;\n import static org.junit.Assert.assertTrue;\n import static org.powermock.api.easymock.PowerMock.createNiceMock;\n import static org.powermock.api.easymock.PowerMock.replay;\n+import static org.powermock.api.easymock.PowerMock.verifyAll;\n \n /**\n  * Stack unit tests.\n@@ -74,6 +85,7 @@ public void testTestXmlExtensionStrippedOff() throws Exception {\n \n     expect(stackServiceResponse.getServiceName()).andReturn(\"service1\").anyTimes();\n     expect(stackServiceResponse.getExcludedConfigTypes()).andReturn(Collections.<String>emptySet());\n+    expect(stackServiceResponse.getConfigTypes()).andReturn(Collections.<String, Map<String,Map<String,String>>>emptyMap());\n \n     expect(controller.getStackComponents(capture(stackComponentRequestCapture))).\n         andReturn(Collections.singleton(stackComponentResponse)).anyTimes();\n@@ -168,6 +180,7 @@ public void testGetRequiredProperties_serviceAndPropertyType() throws Exception\n \n     expect(stackServiceResponse.getServiceName()).andReturn(\"service1\").anyTimes();\n     expect(stackServiceResponse.getExcludedConfigTypes()).andReturn(Collections.<String>emptySet());\n+    expect(stackServiceResponse.getConfigTypes()).andReturn(Collections.<String, Map<String,Map<String,String>>>emptyMap());\n \n     expect(controller.getStackComponents(capture(stackComponentRequestCapture))).\n         andReturn(Collections.singleton(stackComponentResponse)).anyTimes();\n@@ -230,4 +243,77 @@ public void testGetRequiredProperties_serviceAndPropertyType() throws Exception\n     assertNull(stackComponentRequest.getComponentName());\n   }\n \n+  // Test that getAllConfigurationTypes returns beside the configuration types that have\n+  // service config properties defined also the empty ones that doesn't have any config\n+  // property defined.\n+  @Test\n+  public void testGetAllConfigurationTypesWithEmptyStackServiceConfigType() throws Exception {\n+    // Given\n+    AmbariManagementController controller = createNiceMock(AmbariManagementController.class);\n+    AmbariMetaInfo metaInfo = createNiceMock(AmbariMetaInfo.class);\n+    StackServiceResponse stackServiceResponse = createNiceMock(StackServiceResponse.class);\n+    StackServiceComponentResponse stackComponentResponse = createNiceMock(StackServiceComponentResponse.class);\n+    StackConfigurationResponse stackConfigurationResponse1 = createNiceMock(StackConfigurationResponse.class);\n+    StackConfigurationResponse stackConfigurationResponse2 = createNiceMock(StackConfigurationResponse.class);\n+\n+    String testServiceName = \"service1\";\n+    String testEmptyConfigType = \"test-empty-config-type\";\n+    String testSiteConfigFile = \"test-site.xml\";\n+    String testSiteConfigType = \"test-site\";\n+\n+\n+    expect(controller.getAmbariMetaInfo()).andReturn(metaInfo).anyTimes();\n+\n+    expect(controller.getStackServices(anyObject(Set.class))).andReturn(Collections.singleton(stackServiceResponse)).anyTimes();\n+    expect(stackServiceResponse.getServiceName()).andReturn(testServiceName).anyTimes();\n+    expect(stackServiceResponse.getExcludedConfigTypes()).andReturn(Collections.<String>emptySet());\n+\n+    // stack components\n+    expect(stackComponentResponse.getComponentName()).andReturn(\"component1\").anyTimes();\n+    expect(stackComponentResponse.getComponentCategory()).andReturn(testSiteConfigFile).anyTimes();\n+    expect(controller.getStackComponents(anyObject(Set.class))).andReturn(Collections.singleton(stackComponentResponse)).anyTimes();\n+\n+    // stack configurations\n+\n+    // two properties with config type 'test-site'\n+    expect(stackConfigurationResponse1.getPropertyName()).andReturn(\"prop1\").anyTimes();\n+    expect(stackConfigurationResponse1.getPropertyValue()).andReturn(null).anyTimes();\n+    expect(stackConfigurationResponse1.getType()).andReturn(testSiteConfigFile).anyTimes();\n+    expect(stackConfigurationResponse1.getPropertyType()).andReturn(Collections.singleton(PropertyInfo.PropertyType.TEXT)).anyTimes();\n+    expect(stackConfigurationResponse1.getPropertyAttributes()).andReturn(Collections.<String, String>emptyMap()).anyTimes();\n+    expect(stackConfigurationResponse1.isRequired()).andReturn(true).anyTimes();\n+\n+    expect(stackConfigurationResponse2.getPropertyName()).andReturn(\"prop2\").anyTimes();\n+    expect(stackConfigurationResponse2.getPropertyValue()).andReturn(null).anyTimes();\n+    expect(stackConfigurationResponse2.getType()).andReturn(testSiteConfigFile).anyTimes();\n+    expect(stackConfigurationResponse2.getPropertyType()).andReturn(Collections.singleton(PropertyInfo.PropertyType.USER)).anyTimes();\n+    expect(stackConfigurationResponse2.getPropertyAttributes()).andReturn(Collections.<String, String>emptyMap()).anyTimes();\n+    expect(stackConfigurationResponse2.isRequired()).andReturn(true).anyTimes();\n+\n+    expect(controller.getStackConfigurations(anyObject(Set.class))).andReturn(Sets.newHashSet(stackConfigurationResponse1, stackConfigurationResponse2)).anyTimes();\n+\n+    // empty stack service config type\n+    expect(stackServiceResponse.getConfigTypes()).andReturn(Collections.singletonMap(testEmptyConfigType, Collections.<String, Map<String,String>>emptyMap()));\n+\n+    // no stack level configs for this test\n+    expect(controller.getStackLevelConfigurations(anyObject(Set.class))).andReturn(Collections.<StackConfigurationResponse>emptySet()).anyTimes();\n+\n+    expect(metaInfo.getComponentDependencies(\"test\", \"1.0\", \"service1\", \"component1\")).andReturn(Collections.<DependencyInfo>emptyList()).anyTimes();\n+\n+    replay(controller, stackServiceResponse, stackComponentResponse, stackConfigurationResponse1, stackConfigurationResponse2, metaInfo);\n+\n+\n+    Stack stack = new Stack(\"test\", \"1.0\", controller);\n+\n+    // When\n+    Collection<String> allServiceConfigTypes = stack.getAllConfigurationTypes(testServiceName);\n+\n+    // Then\n+\n+    assertTrue(allServiceConfigTypes.containsAll(ImmutableSet.of(testSiteConfigType, testEmptyConfigType)));\n+    assertEquals(2, allServiceConfigTypes.size());\n+\n+    verifyAll();\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/ambari/raw/4ecb3c10fede992fde82e54cf252f28160ba5f07/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/StackTest.java",
                "sha": "79f111ec29ad5797464c1bfc82dcec7b2f6b30af",
                "status": "modified"
            }
        ],
        "message": "AMBARI-15220. NPE thrown when secure cluster is created with Blueprint that includes ACCUMULO. (stoader)",
        "parent": "https://github.com/apache/ambari/commit/863c1d17f4f2c8901967dd5f07632290b0aa873e",
        "repo": "ambari",
        "unit_tests": [
            "StackTest.java"
        ]
    },
    "ambari_6237724": {
        "bug_id": "ambari_6237724",
        "commit": "https://github.com/apache/ambari/commit/6237724d4f5cb02bd1541deba6542a72be04af2a",
        "file": [
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/ambari/blob/6237724d4f5cb02bd1541deba6542a72be04af2a/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BaseBlueprintProcessor.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BaseBlueprintProcessor.java?ref=6237724d4f5cb02bd1541deba6542a72be04af2a",
                "deletions": 12,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BaseBlueprintProcessor.java",
                "patch": "@@ -106,7 +106,7 @@ protected BaseBlueprintProcessor(Set<String> propertyIds,\n     Map<String, HostGroupImpl> mapHostGroups = new HashMap<String, HostGroupImpl>();\n \n     for (HostGroupEntity hostGroup : blueprint.getHostGroups()) {\n-      mapHostGroups.put(hostGroup.getName(), new HostGroupImpl(hostGroup, stack));\n+      mapHostGroups.put(hostGroup.getName(), new HostGroupImpl(hostGroup, stack, this));\n     }\n     return mapHostGroups;\n   }\n@@ -123,7 +123,7 @@ protected BaseBlueprintProcessor(Set<String> propertyIds,\n   protected Stack parseStack(BlueprintEntity blueprint) throws SystemException {\n     Stack stack;\n     try {\n-      stack = new Stack(blueprint.getStackName(), blueprint.getStackVersion());\n+      stack = new Stack(blueprint.getStackName(), blueprint.getStackVersion(), getManagementController());\n     } catch (StackAccessException e) {\n       throw new IllegalArgumentException(\"Invalid stack information provided for cluster.  \" +\n           \"stack name: \" + blueprint.getStackName() +\n@@ -148,7 +148,7 @@ protected Stack parseStack(BlueprintEntity blueprint) throws SystemException {\n    * @throws IllegalArgumentException when validation fails\n    */\n   protected BlueprintEntity validateTopology(BlueprintEntity blueprint) throws AmbariException {\n-    Stack stack = new Stack(blueprint.getStackName(), blueprint.getStackVersion());\n+    Stack stack = new Stack(blueprint.getStackName(), blueprint.getStackVersion(), getManagementController());\n     Map<String, HostGroupImpl> hostGroupMap = parseBlueprintHostGroups(blueprint, stack);\n     Collection<HostGroupImpl> hostGroups = hostGroupMap.values();\n     Map<String, Map<String, String>> clusterConfig = processBlueprintConfigurations(blueprint, null);\n@@ -455,7 +455,7 @@ private void generateInvalidTopologyException(Map<String, Map<String, Collection\n   /**\n    * Encapsulates stack information.\n    */\n-  protected class Stack {\n+  protected static class Stack {\n     /**\n      * Stack name\n      */\n@@ -514,6 +514,12 @@ private void generateInvalidTopologyException(Map<String, Map<String, Collection\n     private Map<String, Map<String, Map<String, ConfigProperty>>> serviceConfigurations =\n         new HashMap<String, Map<String, Map<String, ConfigProperty>>>();\n \n+\n+    /**\n+     * Ambari Management Controller, used to obtain Stack definitions\n+     */\n+    private final AmbariManagementController ambariManagementController;\n+\n     /**\n      * Contains a configuration property's value and attributes.\n      */\n@@ -553,11 +559,12 @@ public void setAttributes(Map<String, String> attributes) {\n      * @throws AmbariException an exception occurred getting stack information\n      *                         for the specified name and version\n      */\n-    public Stack(String name, String version) throws AmbariException {\n+    public Stack(String name, String version, AmbariManagementController ambariManagementController) throws AmbariException {\n       this.name = name;\n       this.version = version;\n+      this.ambariManagementController = ambariManagementController;\n \n-      Set<StackServiceResponse> stackServices = getManagementController().getStackServices(\n+      Set<StackServiceResponse> stackServices = ambariManagementController.getStackServices(\n           Collections.singleton(new StackServiceRequest(name, version, null)));\n \n       for (StackServiceResponse stackService : stackServices) {\n@@ -767,7 +774,7 @@ public AutoDeployInfo getAutoDeployInfo(String component) {\n     private void parseComponents(String service) throws AmbariException{\n       Collection<String> componentSet = new HashSet<String>();\n \n-      Set<StackServiceComponentResponse> components = getManagementController().getStackComponents(\n+      Set<StackServiceComponentResponse> components = ambariManagementController.getStackComponents(\n           Collections.singleton(new StackServiceComponentRequest(name, version, service, null)));\n \n       // stack service components\n@@ -807,7 +814,7 @@ private void parseConfigurations(String service) throws AmbariException {\n \n       serviceConfigurations.put(service, mapServiceConfig);\n \n-      Set<StackConfigurationResponse> serviceConfigs = getManagementController().getStackConfigurations(\n+      Set<StackConfigurationResponse> serviceConfigs = ambariManagementController.getStackConfigurations(\n           Collections.singleton(new StackConfigurationRequest(name, version, service, null)));\n \n       for (StackConfigurationResponse config : serviceConfigs) {\n@@ -846,7 +853,7 @@ private void registerConditionalDependencies() {\n   /**\n    * Host group representation.\n    */\n-  protected class HostGroupImpl implements HostGroup {\n+  protected static class HostGroupImpl implements HostGroup {\n     /**\n      * Host group entity\n      */\n@@ -879,15 +886,21 @@ private void registerConditionalDependencies() {\n      */\n     private Stack stack;\n \n+    /**\n+     * The Blueprint processor associated with this HostGroupImpl instance\n+     */\n+    private final BaseBlueprintProcessor blueprintProcessor;\n+\n     /**\n      * Constructor.\n      *\n      * @param hostGroup  host group\n      * @param stack      stack\n      */\n-    public HostGroupImpl(HostGroupEntity hostGroup, Stack stack) {\n+    public HostGroupImpl(HostGroupEntity hostGroup, Stack stack, BaseBlueprintProcessor blueprintProcessor) {\n       this.hostGroup = hostGroup;\n       this.stack = stack;\n+      this.blueprintProcessor = blueprintProcessor;\n       parseComponents();\n       parseConfigurations();\n     }\n@@ -1009,14 +1022,14 @@ public HostGroupEntity getEntity() {\n           boolean           resolved        = false;\n \n           if (dependencyScope.equals(\"cluster\")) {\n-            Collection<String> missingDependencyInfo = verifyComponentCardinalityCount(entity, hostGroups,\n+            Collection<String> missingDependencyInfo = blueprintProcessor.verifyComponentCardinalityCount(entity, hostGroups,\n                 componentName, new Cardinality(\"1+\"), autoDeployInfo, stack, clusterConfig);\n             resolved = missingDependencyInfo.isEmpty();\n           } else if (dependencyScope.equals(\"host\")) {\n             if (components.contains(component) || (autoDeployInfo != null && autoDeployInfo.isEnabled())) {\n               resolved = true;\n               if (addComponent(componentName)) {\n-                addComponentToBlueprint(hostGroup.getBlueprintEntity(), getEntity().getName(), componentName);\n+                blueprintProcessor.addComponentToBlueprint(hostGroup.getBlueprintEntity(), getEntity().getName(), componentName);\n               }\n             }\n           }",
                "raw_url": "https://github.com/apache/ambari/raw/6237724d4f5cb02bd1541deba6542a72be04af2a/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BaseBlueprintProcessor.java",
                "sha": "c9f0124183c09cdb238bfc0c38fa93361371fbca",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/ambari/blob/6237724d4f5cb02bd1541deba6542a72be04af2a/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java?ref=6237724d4f5cb02bd1541deba6542a72be04af2a",
                "deletions": 2,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java",
                "patch": "@@ -150,7 +150,8 @@ private void doSingleHostExportUpdate(Collection<? extends HostGroup> hostGroups\n           for (HostGroup group : hostGroups) {\n             Collection<String> hosts = group.getHostInfo();\n             for (String host : hosts) {\n-              if (propValue.contains(host)) {    //todo: need to use regular expression to avoid matching a host which is a superset.  Can this be fixed???\n+              //todo: need to use regular expression to avoid matching a host which is a superset.\n+              if (propValue.contains(host)) {\n                 matchedHost = true;\n                 typeProperties.put(propertyName, propValue.replace(\n                     host, \"%HOSTGROUP::\" + group.getName() + \"%\"));\n@@ -697,4 +698,4 @@ public String doFormat(String origValue) {\n     hbaseEnvMap.put(\"hbase_master_heapsize\", new MPropertyUpdater());\n     hbaseEnvMap.put(\"hbase_regionserver_heapsize\", new MPropertyUpdater());\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/ambari/raw/6237724d4f5cb02bd1541deba6542a72be04af2a/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java",
                "sha": "c246f83b086df445fb7d6bdf3479b50569f1034d",
                "status": "modified"
            },
            {
                "additions": 62,
                "blob_url": "https://github.com/apache/ambari/blob/6237724d4f5cb02bd1541deba6542a72be04af2a/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ClusterResourceProvider.java",
                "changes": 73,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ClusterResourceProvider.java?ref=6237724d4f5cb02bd1541deba6542a72be04af2a",
                "deletions": 11,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ClusterResourceProvider.java",
                "patch": "@@ -303,6 +303,16 @@ public static void init(BlueprintDAO dao, AmbariMetaInfo metaInfo, ConfigHelper\n   }\n \n \n+  /**\n+   * Package-level access for cluster config\n+   * @return cluster config map\n+   */\n+  Map<String, Map<String, String>> getClusterConfigurations() {\n+    return mapClusterConfigurations;\n+  }\n+\n+\n+\n   // ----- utility methods ---------------------------------------------------\n \n   /**\n@@ -844,7 +854,7 @@ private void processConfigurations(Map<String, Map<String, String>> blueprintCon\n \n     BlueprintConfigurationProcessor configurationProcessor = new BlueprintConfigurationProcessor(mapClusterConfigurations);\n     configurationProcessor.doUpdateForClusterCreate(blueprintHostGroups);\n-    setMissingConfigurations();\n+    setMissingConfigurations(blueprintHostGroups);\n   }\n \n   /**\n@@ -908,29 +918,70 @@ private void processBlueprintClusterConfigAttributes(Map<String, Map<String, Map\n   /**\n    * Explicitly set any properties that are required but not currently provided in the stack definition.\n    */\n-  private void setMissingConfigurations() {\n+  void setMissingConfigurations(Map<String, HostGroupImpl> blueprintHostGroups) {\n     // AMBARI-5206\n     final Map<String , String> userProps = new HashMap<String , String>();\n-    userProps.put(\"oozie_user\", \"oozie-env\");\n-    userProps.put(\"hive_user\", \"hive-env\");\n-    userProps.put(\"hcat_user\", \"hive-env\");\n-    userProps.put(\"hbase_user\", \"hbase-env\");\n-    userProps.put(\"falcon_user\", \"falcon-env\");\n+\n+    // only add user properties to the map for\n+    // services actually included in the blueprint definition\n+    if (isServiceIncluded(\"OOZIE\", blueprintHostGroups)) {\n+      userProps.put(\"oozie_user\", \"oozie-env\");\n+    }\n+\n+    if (isServiceIncluded(\"HIVE\", blueprintHostGroups)) {\n+      userProps.put(\"hive_user\", \"hive-env\");\n+      userProps.put(\"hcat_user\", \"hive-env\");\n+    }\n+\n+    if (isServiceIncluded(\"HBASE\", blueprintHostGroups)) {\n+      userProps.put(\"hbase_user\", \"hbase-env\");\n+    }\n+\n+    if (isServiceIncluded(\"FALCON\", blueprintHostGroups)) {\n+      userProps.put(\"falcon_user\", \"falcon-env\");\n+    }\n+\n \n     String proxyUserHosts  = \"hadoop.proxyuser.%s.hosts\";\n     String proxyUserGroups = \"hadoop.proxyuser.%s.groups\";\n \n     for (String property : userProps.keySet()) {\n       String configType = userProps.get(property);\n       Map<String, String> configs = mapClusterConfigurations.get(configType);\n-      String user = configs.get(property);\n-      if (user != null && !user.isEmpty()) {\n-        ensureProperty(\"core-site\", String.format(proxyUserHosts, user), \"*\");\n-        ensureProperty(\"core-site\", String.format(proxyUserGroups, user), \"users\");\n+      if (configs != null) {\n+        String user = configs.get(property);\n+        if (user != null && !user.isEmpty()) {\n+          ensureProperty(\"core-site\", String.format(proxyUserHosts, user), \"*\");\n+          ensureProperty(\"core-site\", String.format(proxyUserGroups, user), \"users\");\n+        }\n+      } else {\n+        LOG.debug(\"setMissingConfigurations: no user configuration found for type = \" + configType + \".  This may be caused by an error in the blueprint configuration.\");\n       }\n+\n     }\n   }\n \n+\n+  /**\n+   * Determines if any components in the specified service are\n+   *   included in the current blueprint's host group definitions.\n+   *\n+   * @param serviceName the Hadoop service name to query on\n+   * @param blueprintHostGroups the map of Host Groups in the current blueprint\n+   * @return true if the named service is included in the blueprint\n+   *         false if the named service it not included in the blueprint\n+   */\n+  protected boolean isServiceIncluded(String serviceName, Map<String, HostGroupImpl> blueprintHostGroups) {\n+    for (String hostGroupName : blueprintHostGroups.keySet()) {\n+      HostGroupImpl hostGroup = blueprintHostGroups.get(hostGroupName);\n+      if (hostGroup.getServices().contains(serviceName)) {\n+        return true;\n+      }\n+    }\n+\n+    return false;\n+  }\n+\n   /**\n    * Ensure that the specified property exists.\n    * If not, set a default value.",
                "raw_url": "https://github.com/apache/ambari/raw/6237724d4f5cb02bd1541deba6542a72be04af2a/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ClusterResourceProvider.java",
                "sha": "3498ffb0e0428648c0ab5a27899019015b44cdae",
                "status": "modified"
            },
            {
                "additions": 494,
                "blob_url": "https://github.com/apache/ambari/blob/6237724d4f5cb02bd1541deba6542a72be04af2a/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/ClusterResourceProviderTest.java",
                "changes": 497,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/ClusterResourceProviderTest.java?ref=6237724d4f5cb02bd1541deba6542a72be04af2a",
                "deletions": 3,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/controller/internal/ClusterResourceProviderTest.java",
                "patch": "@@ -25,6 +25,7 @@\n import static org.easymock.EasyMock.createStrictMock;\n import static org.easymock.EasyMock.eq;\n import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.isA;\n import static org.easymock.EasyMock.replay;\n import static org.easymock.EasyMock.verify;\n import static org.junit.Assert.assertEquals;\n@@ -71,14 +72,17 @@\n import org.apache.ambari.server.orm.entities.HostGroupComponentEntity;\n import org.apache.ambari.server.orm.entities.HostGroupConfigEntity;\n import org.apache.ambari.server.orm.entities.HostGroupEntity;\n+import org.apache.ambari.server.state.AutoDeployInfo;\n import org.apache.ambari.server.state.Clusters;\n import org.apache.ambari.server.state.ConfigHelper;\n import org.apache.ambari.server.state.DependencyInfo;\n import org.apache.ambari.server.state.PropertyInfo;\n import org.apache.ambari.server.state.State;\n import org.easymock.Capture;\n import org.easymock.EasyMock;\n+import org.easymock.EasyMockSupport;\n import org.junit.Assert;\n+import org.junit.Before;\n import org.junit.Test;\n \n import com.google.gson.Gson;\n@@ -87,6 +91,14 @@\n  * ClusterResourceProvider tests.\n  */\n public class ClusterResourceProviderTest {\n+\n+  @Before\n+  public void setup() throws Exception {\n+    // reset this static field, to allow unit tests to function independently\n+    BaseBlueprintProcessor.stackInfo = null;\n+  }\n+\n+\n   @Test\n   public void testCreateResources() throws Exception{\n     Resource.Type type = Resource.Type.Cluster;\n@@ -440,7 +452,12 @@ public void testCreateResource_blueprint() throws Exception {\n     PersistKeyValueService.init(persistKeyValue);\n     ResourceProvider provider = new TestClusterResourceProvider(\n         managementController, serviceResourceProvider, componentResourceProvider,\n-        hostResourceProvider, hostComponentResourceProvider, configGroupResourceProvider);\n+        hostResourceProvider, hostComponentResourceProvider, configGroupResourceProvider) {\n+      @Override\n+      protected boolean isServiceIncluded(String serviceName, Map<String, HostGroupImpl> blueprintHostGroups) {\n+        return true;\n+      }\n+    };\n \n     RequestStatus requestStatus = provider.createResources(request);\n \n@@ -1644,7 +1661,12 @@ public void testCreateResource_blueprint__defaultPassword() throws Exception {\n     PersistKeyValueService.init(persistKeyValue);\n     ResourceProvider provider = new TestClusterResourceProvider(\n         managementController, serviceResourceProvider, componentResourceProvider,\n-        hostResourceProvider, hostComponentResourceProvider, configGroupResourceProvider);\n+        hostResourceProvider, hostComponentResourceProvider, configGroupResourceProvider) {\n+      @Override\n+      protected boolean isServiceIncluded(String serviceName, Map<String, HostGroupImpl> blueprintHostGroups) {\n+        return true;\n+      }\n+    };\n \n     RequestStatus requestStatus = provider.createResources(request);\n \n@@ -2336,7 +2358,12 @@ public void testCreateResource_blueprint_attrbiutesProvided() throws Exception {\n     PersistKeyValueService.init(persistKeyValue);\n     ResourceProvider provider = new TestClusterResourceProvider(\n         managementController, serviceResourceProvider, componentResourceProvider,\n-        hostResourceProvider, hostComponentResourceProvider, configGroupResourceProvider);\n+        hostResourceProvider, hostComponentResourceProvider, configGroupResourceProvider) {\n+      @Override\n+      protected boolean isServiceIncluded(String serviceName, Map<String, HostGroupImpl> blueprintHostGroups) {\n+        return true;\n+      }\n+    };\n \n     RequestStatus requestStatus = provider.createResources(request);\n \n@@ -2826,6 +2853,470 @@ public void testDeleteResources() throws Exception{\n     verify(managementController, response, clusters);\n   }\n \n+  @Test\n+  public void testSetMissingConfigurationsOozieIncluded() throws Exception {\n+    EasyMockSupport mockSupport = new EasyMockSupport();\n+\n+    AmbariManagementController mockMgmtController =\n+      mockSupport.createMock(AmbariManagementController.class);\n+    ResourceProvider mockServiceProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockComponentProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockHostProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockHostComponentProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockConfigGroupProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    AmbariManagementController mockManagementController =\n+      mockSupport.createMock(AmbariManagementController.class);\n+    StackServiceResponse mockStackServiceResponseOne =\n+      mockSupport.createMock(StackServiceResponse.class);\n+    StackServiceComponentResponse mockStackComponentResponse =\n+      mockSupport.createMock(StackServiceComponentResponse.class);\n+    AmbariMetaInfo mockAmbariMetaInfo =\n+      mockSupport.createMock(AmbariMetaInfo.class);\n+\n+    expect(mockStackComponentResponse.getComponentName()).andReturn(\"OOZIE_SERVER\");\n+    expect(mockStackComponentResponse.getCardinality()).andReturn(\"1\");\n+    expect(mockStackComponentResponse.getAutoDeploy()).andReturn(new AutoDeployInfo());\n+\n+\n+    expect(mockStackServiceResponseOne.getServiceName()).andReturn(\"OOZIE\");\n+    expect(mockManagementController.getStackServices(isA(Set.class))).andReturn(Collections.singleton(mockStackServiceResponseOne));\n+    expect(mockManagementController.getStackComponents(isA(Set.class))).andReturn(Collections.singleton(mockStackComponentResponse));\n+    expect(mockManagementController.getStackConfigurations(isA(Set.class))).andReturn(Collections.<StackConfigurationResponse>emptySet());\n+\n+    expect(mockAmbariMetaInfo.getComponentDependencies(\"HDP\", \"2.1\", \"OOZIE\", \"OOZIE_SERVER\")).andReturn(Collections.<DependencyInfo>emptyList());\n+\n+    mockSupport.replayAll();\n+\n+\n+    ClusterResourceProvider.init(null, mockAmbariMetaInfo, null);\n+\n+    BaseBlueprintProcessor.Stack stack =\n+      new BaseBlueprintProcessor.Stack(\"HDP\", \"2.1\", mockManagementController);\n+\n+    ClusterResourceProvider clusterResourceProvider =\n+      new TestClusterResourceProvider(mockMgmtController, mockServiceProvider,\n+        mockComponentProvider, mockHostProvider, mockHostComponentProvider, mockConfigGroupProvider);\n+\n+\n+    HostGroupEntity hostGroup = new HostGroupEntity();\n+    hostGroup.setComponents(Collections.<HostGroupComponentEntity>emptyList());\n+    HostGroupConfigEntity configEntity = new HostGroupConfigEntity();\n+    configEntity.setConfigData(\"\");\n+\n+    hostGroup.setConfigurations(Collections.singletonList(configEntity));\n+    BaseBlueprintProcessor.HostGroupImpl hostGroupImpl =\n+      new BaseBlueprintProcessor.HostGroupImpl(hostGroup, stack, null);\n+    hostGroupImpl.addComponent(\"OOZIE_SERVER\");\n+\n+    // add empty map for core-site, to simulate this configuration entry\n+    clusterResourceProvider.getClusterConfigurations().put(\"core-site\", new HashMap<String, String>());\n+    clusterResourceProvider.getClusterConfigurations().put(\"oozie-env\", new HashMap<String, String>());\n+    clusterResourceProvider.getClusterConfigurations().get(\"oozie-env\").put(\"oozie_user\", \"oozie\");\n+\n+    clusterResourceProvider.setMissingConfigurations(Collections.singletonMap(\"host_group_one\", hostGroupImpl));\n+\n+    Map<String, String> mapCoreSiteConfig =\n+      clusterResourceProvider.getClusterConfigurations().get(\"core-site\");\n+\n+    assertNotNull(\"core-site map was null.\", mapCoreSiteConfig);\n+    assertEquals(\"Incorrect number of entries in the core-site config map\",\n+                 2, mapCoreSiteConfig.size());\n+    assertEquals(\"Incorrect value for proxy hosts\",\n+                 \"*\", mapCoreSiteConfig.get(\"hadoop.proxyuser.oozie.hosts\"));\n+    assertEquals(\"Incorrect value for proxy hosts\",\n+      \"users\", mapCoreSiteConfig.get(\"hadoop.proxyuser.oozie.groups\"));\n+\n+    mockSupport.verifyAll();\n+  }\n+\n+\n+  @Test\n+  public void testSetMissingConfigurationsFalconIncluded() throws Exception {\n+    EasyMockSupport mockSupport = new EasyMockSupport();\n+\n+    AmbariManagementController mockMgmtController =\n+      mockSupport.createMock(AmbariManagementController.class);\n+    ResourceProvider mockServiceProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockComponentProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockHostProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockHostComponentProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockConfigGroupProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    AmbariManagementController mockManagementController =\n+      mockSupport.createMock(AmbariManagementController.class);\n+    StackServiceResponse mockStackServiceResponseOne =\n+      mockSupport.createMock(StackServiceResponse.class);\n+    StackServiceComponentResponse mockStackComponentResponse =\n+      mockSupport.createMock(StackServiceComponentResponse.class);\n+    AmbariMetaInfo mockAmbariMetaInfo =\n+      mockSupport.createMock(AmbariMetaInfo.class);\n+\n+    expect(mockStackComponentResponse.getComponentName()).andReturn(\"FALCON_SERVER\");\n+    expect(mockStackComponentResponse.getCardinality()).andReturn(\"1\");\n+    expect(mockStackComponentResponse.getAutoDeploy()).andReturn(new AutoDeployInfo());\n+\n+\n+    expect(mockStackServiceResponseOne.getServiceName()).andReturn(\"FALCON\");\n+    expect(mockManagementController.getStackServices(isA(Set.class))).andReturn(Collections.singleton(mockStackServiceResponseOne));\n+    expect(mockManagementController.getStackComponents(isA(Set.class))).andReturn(Collections.singleton(mockStackComponentResponse));\n+    expect(mockManagementController.getStackConfigurations(isA(Set.class))).andReturn(Collections.<StackConfigurationResponse>emptySet());\n+\n+    expect(mockAmbariMetaInfo.getComponentDependencies(\"HDP\", \"2.1\", \"FALCON\", \"FALCON_SERVER\")).andReturn(Collections.<DependencyInfo>emptyList());\n+\n+    mockSupport.replayAll();\n+\n+    ClusterResourceProvider.init(null, mockAmbariMetaInfo, null);\n+\n+    BaseBlueprintProcessor.Stack stack =\n+      new BaseBlueprintProcessor.Stack(\"HDP\", \"2.1\", mockManagementController);\n+\n+    ClusterResourceProvider clusterResourceProvider =\n+      new TestClusterResourceProvider(mockMgmtController, mockServiceProvider,\n+        mockComponentProvider, mockHostProvider, mockHostComponentProvider, mockConfigGroupProvider);\n+\n+    HostGroupEntity hostGroup = new HostGroupEntity();\n+    hostGroup.setComponents(Collections.<HostGroupComponentEntity>emptyList());\n+    HostGroupConfigEntity configEntity = new HostGroupConfigEntity();\n+    configEntity.setConfigData(\"\");\n+\n+    hostGroup.setConfigurations(Collections.singletonList(configEntity));\n+    BaseBlueprintProcessor.HostGroupImpl hostGroupImpl =\n+      new BaseBlueprintProcessor.HostGroupImpl(hostGroup, stack, null);\n+    hostGroupImpl.addComponent(\"FALCON_SERVER\");\n+\n+    // add empty map for core-site, to simulate this configuration entry\n+    clusterResourceProvider.getClusterConfigurations().put(\"core-site\", new HashMap<String, String>());\n+    clusterResourceProvider.getClusterConfigurations().put(\"falcon-env\", new HashMap<String, String>());\n+    clusterResourceProvider.getClusterConfigurations().get(\"falcon-env\").put(\"falcon_user\", \"falcon\");\n+\n+    clusterResourceProvider.setMissingConfigurations(Collections.singletonMap(\"host_group_one\", hostGroupImpl));\n+\n+    Map<String, String> mapCoreSiteConfig =\n+      clusterResourceProvider.getClusterConfigurations().get(\"core-site\");\n+\n+    assertNotNull(\"core-site map was null.\", mapCoreSiteConfig);\n+    assertEquals(\"Incorrect number of entries in the core-site config map\",\n+      2, mapCoreSiteConfig.size());\n+    assertEquals(\"Incorrect value for proxy hosts\",\n+      \"*\", mapCoreSiteConfig.get(\"hadoop.proxyuser.falcon.hosts\"));\n+    assertEquals(\"Incorrect value for proxy hosts\",\n+      \"users\", mapCoreSiteConfig.get(\"hadoop.proxyuser.falcon.groups\"));\n+\n+    mockSupport.verifyAll();\n+  }\n+\n+\n+  @Test\n+  public void testSetMissingConfigurationsOozieNotIncluded() throws Exception {\n+    EasyMockSupport mockSupport = new EasyMockSupport();\n+\n+    AmbariManagementController mockMgmtController =\n+      mockSupport.createMock(AmbariManagementController.class);\n+    ResourceProvider mockServiceProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockComponentProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockHostProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockHostComponentProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockConfigGroupProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    AmbariManagementController mockManagementController =\n+      mockSupport.createMock(AmbariManagementController.class);\n+    StackServiceResponse mockStackServiceResponseOne =\n+      mockSupport.createMock(StackServiceResponse.class);\n+    StackServiceComponentResponse mockStackComponentResponse =\n+      mockSupport.createMock(StackServiceComponentResponse.class);\n+    AmbariMetaInfo mockAmbariMetaInfo =\n+      mockSupport.createMock(AmbariMetaInfo.class);\n+\n+    expect(mockStackComponentResponse.getComponentName()).andReturn(\"OOZIE_SERVER\");\n+    expect(mockStackComponentResponse.getCardinality()).andReturn(\"1\");\n+    expect(mockStackComponentResponse.getAutoDeploy()).andReturn(new AutoDeployInfo());\n+\n+\n+    expect(mockStackServiceResponseOne.getServiceName()).andReturn(\"OOZIE\");\n+    expect(mockManagementController.getStackServices(isA(Set.class))).andReturn(Collections.singleton(mockStackServiceResponseOne));\n+    expect(mockManagementController.getStackComponents(isA(Set.class))).andReturn(Collections.singleton(mockStackComponentResponse));\n+    expect(mockManagementController.getStackConfigurations(isA(Set.class))).andReturn(Collections.<StackConfigurationResponse>emptySet());\n+\n+    expect(mockAmbariMetaInfo.getComponentDependencies(\"HDP\", \"2.1\", \"OOZIE\", \"OOZIE_SERVER\")).andReturn(Collections.<DependencyInfo>emptyList());\n+\n+    mockSupport.replayAll();\n+\n+    ClusterResourceProvider.init(null, mockAmbariMetaInfo, null);\n+\n+    BaseBlueprintProcessor.Stack stack =\n+      new BaseBlueprintProcessor.Stack(\"HDP\", \"2.1\", mockManagementController);\n+\n+    ClusterResourceProvider clusterResourceProvider =\n+      new TestClusterResourceProvider(mockMgmtController, mockServiceProvider,\n+        mockComponentProvider, mockHostProvider, mockHostComponentProvider, mockConfigGroupProvider);\n+\n+\n+    HostGroupEntity hostGroup = new HostGroupEntity();\n+    hostGroup.setComponents(Collections.<HostGroupComponentEntity>emptyList());\n+    HostGroupConfigEntity configEntity = new HostGroupConfigEntity();\n+    configEntity.setConfigData(\"\");\n+\n+    hostGroup.setConfigurations(Collections.singletonList(configEntity));\n+    BaseBlueprintProcessor.HostGroupImpl hostGroupImpl =\n+      new BaseBlueprintProcessor.HostGroupImpl(hostGroup, stack, null);\n+    hostGroupImpl.addComponent(\"COMPONENT_ONE\");\n+\n+    // add empty map for core-site, to simulate this configuration entry\n+    clusterResourceProvider.getClusterConfigurations().put(\"core-site\", new HashMap<String, String>());\n+\n+    clusterResourceProvider.setMissingConfigurations(Collections.singletonMap(\"host_group_one\", hostGroupImpl));\n+\n+    Map<String, String> mapCoreSiteConfig =\n+      clusterResourceProvider.getClusterConfigurations().get(\"core-site\");\n+\n+    assertNotNull(\"core-site map was null.\", mapCoreSiteConfig);\n+    assertEquals(\"Incorrect number of entries in the core-site config map\",\n+                0, mapCoreSiteConfig.size());\n+\n+    mockSupport.verifyAll();\n+\n+  }\n+\n+\n+  @Test\n+  public void testSetMissingConfigurationsFalconNotIncluded() throws Exception {\n+    EasyMockSupport mockSupport = new EasyMockSupport();\n+\n+    AmbariManagementController mockMgmtController =\n+      mockSupport.createMock(AmbariManagementController.class);\n+    ResourceProvider mockServiceProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockComponentProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockHostProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockHostComponentProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockConfigGroupProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    AmbariManagementController mockManagementController =\n+      mockSupport.createMock(AmbariManagementController.class);\n+    StackServiceResponse mockStackServiceResponseOne =\n+      mockSupport.createMock(StackServiceResponse.class);\n+    StackServiceComponentResponse mockStackComponentResponse =\n+      mockSupport.createMock(StackServiceComponentResponse.class);\n+    AmbariMetaInfo mockAmbariMetaInfo =\n+      mockSupport.createMock(AmbariMetaInfo.class);\n+\n+    expect(mockStackComponentResponse.getComponentName()).andReturn(\"FALCON_SERVER\");\n+    expect(mockStackComponentResponse.getCardinality()).andReturn(\"1\");\n+    expect(mockStackComponentResponse.getAutoDeploy()).andReturn(new AutoDeployInfo());\n+\n+\n+    expect(mockStackServiceResponseOne.getServiceName()).andReturn(\"FALCON\");\n+    expect(mockManagementController.getStackServices(isA(Set.class))).andReturn(Collections.singleton(mockStackServiceResponseOne));\n+    expect(mockManagementController.getStackComponents(isA(Set.class))).andReturn(Collections.singleton(mockStackComponentResponse));\n+    expect(mockManagementController.getStackConfigurations(isA(Set.class))).andReturn(Collections.<StackConfigurationResponse>emptySet());\n+\n+    expect(mockAmbariMetaInfo.getComponentDependencies(\"HDP\", \"2.1\", \"FALCON\", \"FALCON_SERVER\")).andReturn(Collections.<DependencyInfo>emptyList());\n+\n+    mockSupport.replayAll();\n+\n+    ClusterResourceProvider.init(null, mockAmbariMetaInfo, null);\n+\n+    BaseBlueprintProcessor.Stack stack =\n+      new BaseBlueprintProcessor.Stack(\"HDP\", \"2.1\", mockManagementController);\n+\n+    ClusterResourceProvider clusterResourceProvider =\n+      new TestClusterResourceProvider(mockMgmtController, mockServiceProvider,\n+        mockComponentProvider, mockHostProvider, mockHostComponentProvider, mockConfigGroupProvider);\n+\n+    HostGroupEntity hostGroup = new HostGroupEntity();\n+    hostGroup.setComponents(Collections.<HostGroupComponentEntity>emptyList());\n+    HostGroupConfigEntity configEntity = new HostGroupConfigEntity();\n+    configEntity.setConfigData(\"\");\n+\n+    hostGroup.setConfigurations(Collections.singletonList(configEntity));\n+    BaseBlueprintProcessor.HostGroupImpl hostGroupImpl =\n+      new BaseBlueprintProcessor.HostGroupImpl(hostGroup, stack, null);\n+    // blueprint request will not include a reference to FALCON_SERVER\n+    hostGroupImpl.addComponent(\"COMPONENT_ONE\");\n+\n+    // add empty map for core-site, to simulate this configuration entry\n+    clusterResourceProvider.getClusterConfigurations().put(\"core-site\", new HashMap<String, String>());\n+\n+    clusterResourceProvider.setMissingConfigurations(Collections.singletonMap(\"host_group_one\", hostGroupImpl));\n+\n+    Map<String, String> mapCoreSiteConfig =\n+      clusterResourceProvider.getClusterConfigurations().get(\"core-site\");\n+\n+    assertNotNull(\"core-site map was null.\", mapCoreSiteConfig);\n+    assertEquals(\"Incorrect number of entries in the core-site config map\",\n+      0, mapCoreSiteConfig.size());\n+\n+    mockSupport.verifyAll();\n+\n+  }\n+\n+\n+  @Test\n+  public void testSetMissingConfigurationsHiveNotIncluded() throws Exception {\n+    EasyMockSupport mockSupport = new EasyMockSupport();\n+\n+    AmbariManagementController mockMgmtController =\n+      mockSupport.createMock(AmbariManagementController.class);\n+    ResourceProvider mockServiceProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockComponentProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockHostProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockHostComponentProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockConfigGroupProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    AmbariManagementController mockManagementController =\n+      mockSupport.createMock(AmbariManagementController.class);\n+    StackServiceResponse mockStackServiceResponseOne =\n+      mockSupport.createMock(StackServiceResponse.class);\n+    StackServiceComponentResponse mockStackComponentResponse =\n+      mockSupport.createMock(StackServiceComponentResponse.class);\n+    AmbariMetaInfo mockAmbariMetaInfo =\n+      mockSupport.createMock(AmbariMetaInfo.class);\n+\n+    expect(mockStackComponentResponse.getComponentName()).andReturn(\"HIVE_SERVER\");\n+    expect(mockStackComponentResponse.getCardinality()).andReturn(\"1\");\n+    expect(mockStackComponentResponse.getAutoDeploy()).andReturn(new AutoDeployInfo());\n+\n+    expect(mockStackServiceResponseOne.getServiceName()).andReturn(\"HIVE\");\n+    expect(mockManagementController.getStackServices(isA(Set.class))).andReturn(Collections.singleton(mockStackServiceResponseOne));\n+    expect(mockManagementController.getStackComponents(isA(Set.class))).andReturn(Collections.singleton(mockStackComponentResponse));\n+    expect(mockManagementController.getStackConfigurations(isA(Set.class))).andReturn(Collections.<StackConfigurationResponse>emptySet());\n+\n+    expect(mockAmbariMetaInfo.getComponentDependencies(\"HDP\", \"2.1\", \"HIVE\", \"HIVE_SERVER\")).andReturn(Collections.<DependencyInfo>emptyList());\n+\n+    mockSupport.replayAll();\n+\n+    ClusterResourceProvider.init(null, mockAmbariMetaInfo, null);\n+\n+    BaseBlueprintProcessor.Stack stack =\n+      new BaseBlueprintProcessor.Stack(\"HDP\", \"2.1\", mockManagementController);\n+\n+    ClusterResourceProvider clusterResourceProvider =\n+      new TestClusterResourceProvider(mockMgmtController, mockServiceProvider,\n+        mockComponentProvider, mockHostProvider, mockHostComponentProvider, mockConfigGroupProvider);\n+\n+    HostGroupEntity hostGroup = new HostGroupEntity();\n+    hostGroup.setComponents(Collections.<HostGroupComponentEntity>emptyList());\n+    HostGroupConfigEntity configEntity = new HostGroupConfigEntity();\n+    configEntity.setConfigData(\"\");\n+\n+    hostGroup.setConfigurations(Collections.singletonList(configEntity));\n+    BaseBlueprintProcessor.HostGroupImpl hostGroupImpl =\n+      new BaseBlueprintProcessor.HostGroupImpl(hostGroup, stack, null);\n+    // blueprint request will not include a reference to a HIVE component\n+    hostGroupImpl.addComponent(\"COMPONENT_ONE\");\n+\n+    // add empty map for core-site, to simulate this configuration entry\n+    clusterResourceProvider.getClusterConfigurations().put(\"core-site\", new HashMap<String, String>());\n+\n+    clusterResourceProvider.setMissingConfigurations(Collections.singletonMap(\"host_group_one\", hostGroupImpl));\n+\n+    Map<String, String> mapCoreSiteConfig =\n+      clusterResourceProvider.getClusterConfigurations().get(\"core-site\");\n+\n+    assertNotNull(\"core-site map was null.\", mapCoreSiteConfig);\n+    assertEquals(\"Incorrect number of entries in the core-site config map\",\n+      0, mapCoreSiteConfig.size());\n+\n+    mockSupport.verifyAll();\n+\n+  }\n+\n+\n+  @Test\n+  public void testSetMissingConfigurationsHBaseNotIncluded() throws Exception {\n+    EasyMockSupport mockSupport = new EasyMockSupport();\n+\n+    AmbariManagementController mockMgmtController =\n+      mockSupport.createMock(AmbariManagementController.class);\n+    ResourceProvider mockServiceProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockComponentProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockHostProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockHostComponentProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    ResourceProvider mockConfigGroupProvider =\n+      mockSupport.createMock(ResourceProvider.class);\n+    AmbariManagementController mockManagementController =\n+      mockSupport.createMock(AmbariManagementController.class);\n+    StackServiceResponse mockStackServiceResponseOne =\n+      mockSupport.createMock(StackServiceResponse.class);\n+    StackServiceComponentResponse mockStackComponentResponse =\n+      mockSupport.createMock(StackServiceComponentResponse.class);\n+    AmbariMetaInfo mockAmbariMetaInfo =\n+      mockSupport.createMock(AmbariMetaInfo.class);\n+\n+    expect(mockStackComponentResponse.getComponentName()).andReturn(\"HBASE_SERVER\");\n+    expect(mockStackComponentResponse.getCardinality()).andReturn(\"1\");\n+    expect(mockStackComponentResponse.getAutoDeploy()).andReturn(new AutoDeployInfo());\n+\n+    expect(mockStackServiceResponseOne.getServiceName()).andReturn(\"HBASE\");\n+    expect(mockManagementController.getStackServices(isA(Set.class))).andReturn(Collections.singleton(mockStackServiceResponseOne));\n+    expect(mockManagementController.getStackComponents(isA(Set.class))).andReturn(Collections.singleton(mockStackComponentResponse));\n+    expect(mockManagementController.getStackConfigurations(isA(Set.class))).andReturn(Collections.<StackConfigurationResponse>emptySet());\n+\n+    expect(mockAmbariMetaInfo.getComponentDependencies(\"HDP\", \"2.1\", \"HBASE\", \"HBASE_SERVER\")).andReturn(Collections.<DependencyInfo>emptyList());\n+\n+    mockSupport.replayAll();\n+\n+    ClusterResourceProvider.init(null, mockAmbariMetaInfo, null);\n+\n+    BaseBlueprintProcessor.Stack stack =\n+      new BaseBlueprintProcessor.Stack(\"HDP\", \"2.1\", mockManagementController);\n+\n+    ClusterResourceProvider clusterResourceProvider =\n+      new TestClusterResourceProvider(mockMgmtController, mockServiceProvider,\n+        mockComponentProvider, mockHostProvider, mockHostComponentProvider, mockConfigGroupProvider);\n+\n+    HostGroupEntity hostGroup = new HostGroupEntity();\n+    hostGroup.setComponents(Collections.<HostGroupComponentEntity>emptyList());\n+    HostGroupConfigEntity configEntity = new HostGroupConfigEntity();\n+    configEntity.setConfigData(\"\");\n+\n+    hostGroup.setConfigurations(Collections.singletonList(configEntity));\n+    BaseBlueprintProcessor.HostGroupImpl hostGroupImpl =\n+      new BaseBlueprintProcessor.HostGroupImpl(hostGroup, stack, null);\n+    // blueprint request will not include a reference to an HBASE component\n+    hostGroupImpl.addComponent(\"COMPONENT_ONE\");\n+\n+    // add empty map for core-site, to simulate this configuration entry\n+    clusterResourceProvider.getClusterConfigurations().put(\"core-site\", new HashMap<String, String>());\n+\n+    clusterResourceProvider.setMissingConfigurations(Collections.singletonMap(\"host_group_one\", hostGroupImpl));\n+\n+    Map<String, String> mapCoreSiteConfig =\n+      clusterResourceProvider.getClusterConfigurations().get(\"core-site\");\n+\n+    assertNotNull(\"core-site map was null.\", mapCoreSiteConfig);\n+    assertEquals(\"Incorrect number of entries in the core-site config map\",\n+      0, mapCoreSiteConfig.size());\n+\n+    mockSupport.verifyAll();\n+\n+  }\n+\n+\n   private class TestClusterResourceProvider extends ClusterResourceProvider {\n \n     private ResourceProvider serviceResourceProvider;",
                "raw_url": "https://github.com/apache/ambari/raw/6237724d4f5cb02bd1541deba6542a72be04af2a/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/ClusterResourceProviderTest.java",
                "sha": "06d8bb72a7c85428d5a4d3fc514cb3de8fce50ff",
                "status": "modified"
            }
        ],
        "message": "AMBARI-6957.  Fixes NPE during cluster create with blueprint.",
        "parent": "https://github.com/apache/ambari/commit/2cceee2ea3933b05cef97e6d48f029dcebe9c200",
        "repo": "ambari",
        "unit_tests": [
            "BaseBlueprintProcessorTest.java",
            "BlueprintConfigurationProcessorTest.java",
            "ClusterResourceProviderTest.java"
        ]
    },
    "ambari_65e8193": {
        "bug_id": "ambari_65e8193",
        "commit": "https://github.com/apache/ambari/commit/65e8193187cf550071ecaaab228f4470dcb96bef",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/ambari/blob/65e8193187cf550071ecaaab228f4470dcb96bef/ambari-server/src/main/java/org/apache/ambari/server/view/ViewRegistry.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/view/ViewRegistry.java?ref=65e8193187cf550071ecaaab228f4470dcb96bef",
                "deletions": 1,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/view/ViewRegistry.java",
                "patch": "@@ -58,6 +58,7 @@\n import java.net.URL;\n import java.net.URLClassLoader;\n import java.util.Collection;\n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.List;\n@@ -170,7 +171,13 @@ public void addDefinition(ViewEntity definition) {\n    * @return the collection of view instances for the view definition\n    */\n   public Collection<ViewInstanceEntity> getInstanceDefinitions(ViewEntity definition) {\n-    return definition == null ? null : viewInstanceDefinitions.get(definition).values();\n+    if (definition != null) {\n+      Map<String, ViewInstanceEntity> instanceEntityMap = viewInstanceDefinitions.get(definition);\n+      if (instanceEntityMap != null) {\n+        return instanceEntityMap.values();\n+      }\n+    }\n+    return Collections.emptyList();\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/ambari/raw/65e8193187cf550071ecaaab228f4470dcb96bef/ambari-server/src/main/java/org/apache/ambari/server/view/ViewRegistry.java",
                "sha": "a45cb3952d97e4a5a69484e85e87c713167ac626",
                "status": "modified"
            }
        ],
        "message": "AMBARI-5651 - Ambari Views : NPE deploying view with no instances defined",
        "parent": "https://github.com/apache/ambari/commit/60db51efe596d617fe40779dce9dd610d25e8217",
        "repo": "ambari",
        "unit_tests": [
            "ViewRegistryTest.java"
        ]
    },
    "ambari_73f1dea": {
        "bug_id": "ambari_73f1dea",
        "commit": "https://github.com/apache/ambari/commit/73f1dea9674359ecbfd723917c83913b20986906",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/ambari/blob/73f1dea9674359ecbfd723917c83913b20986906/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/JsonHttpPropertyRequest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/JsonHttpPropertyRequest.java?ref=73f1dea9674359ecbfd723917c83913b20986906",
                "deletions": 2,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/JsonHttpPropertyRequest.java",
                "patch": "@@ -62,7 +62,9 @@ public void populateResource(Resource resource, InputStream inputStream) throws\n \n     try {\n       Map<String, Object> responseMap = GSON.fromJson(IOUtils.toString(inputStream, \"UTF-8\"), MAP_TYPE);\n-\n+      if (responseMap == null){\n+        LOG.error(\"Properties map from HTTP response is null\");\n+      }\n       for (Map.Entry<String, String> entry : getPropertyMappings().entrySet()) {\n         Object propertyValueToSet = getPropertyValue(responseMap, entry.getKey());\n         resource.setProperty(entry.getValue(), propertyValueToSet);\n@@ -77,7 +79,7 @@ public void populateResource(Resource resource, InputStream inputStream) throws\n \n   // get the property value from the response map for the given property name\n   private Object getPropertyValue(Map<String, Object> responseMap, String property) throws SystemException {\n-    if (property == null) {\n+    if (property == null || responseMap == null) {\n       return null;\n     }\n ",
                "raw_url": "https://github.com/apache/ambari/raw/73f1dea9674359ecbfd723917c83913b20986906/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/JsonHttpPropertyRequest.java",
                "sha": "16a7ae7be1a8f167ce9f9614e3d1a6ce9d5111d6",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/ambari/blob/73f1dea9674359ecbfd723917c83913b20986906/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/URLStreamProvider.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/URLStreamProvider.java?ref=73f1dea9674359ecbfd723917c83913b20986906",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/URLStreamProvider.java",
                "patch": "@@ -241,6 +241,9 @@ public HttpURLConnection processURL(String spec, String requestMethod, byte[] bo\n     } else {\n         // not a 401 Unauthorized status code\n         // we would let the original response propagate\n+        if (statusCode == HttpStatus.SC_NOT_FOUND || statusCode == HttpStatus.SC_FORBIDDEN){\n+          LOG.error(String.format(\"Received HTTP %s response from URL: %s\", statusCode, spec));\n+        }\n         return connection;\n     }\n   }",
                "raw_url": "https://github.com/apache/ambari/raw/73f1dea9674359ecbfd723917c83913b20986906/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/URLStreamProvider.java",
                "sha": "501aa0426a7980ae0f6246787d4ea2772109f13a",
                "status": "modified"
            }
        ],
        "message": "AMBARI-18588 Ambari server should not crash with NPE when parsing HTTP response for YARN RM properties (dili)",
        "parent": "https://github.com/apache/ambari/commit/41c49e162f156320a0c366f487beea0280b95e38",
        "repo": "ambari",
        "unit_tests": [
            "URLStreamProviderTest.java"
        ]
    },
    "ambari_773541e": {
        "bug_id": "ambari_773541e",
        "commit": "https://github.com/apache/ambari/commit/773541ea06916d63aa2cd2c701b2759c6bd6384f",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/ambari/blob/773541ea06916d63aa2cd2c701b2759c6bd6384f/ambari-server/src/main/java/org/apache/ambari/server/controller/ganglia/GangliaPropertyProvider.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/ganglia/GangliaPropertyProvider.java?ref=773541ea06916d63aa2cd2c701b2759c6bd6384f",
                "deletions": 2,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/ganglia/GangliaPropertyProvider.java",
                "patch": "@@ -487,8 +487,16 @@ public void putPropertyId(String metric, String id) {\n           metric.setHost_name(reader.readLine());\n           metric.setMetric_name(reader.readLine());\n \n-          int time = convertToNumber(reader.readLine()).intValue();\n-          int step = convertToNumber(reader.readLine()).intValue();\n+          String timeStr = reader.readLine();\n+          String stepStr = reader.readLine();\n+          if (timeStr == null || timeStr.isEmpty() || stepStr == null\n+              || stepStr.isEmpty()) {\n+            LOG.info(\"Unexpected end of stream reached while getting ganglia \" +\n+                \"metrics for spec => \" + spec);\n+            return Collections.emptySet();\n+          }\n+          int time = convertToNumber(timeStr).intValue();\n+          int step = convertToNumber(stepStr).intValue();\n \n           String val     = reader.readLine();\n           String lastVal = null;",
                "raw_url": "https://github.com/apache/ambari/raw/773541ea06916d63aa2cd2c701b2759c6bd6384f/ambari-server/src/main/java/org/apache/ambari/server/controller/ganglia/GangliaPropertyProvider.java",
                "sha": "a0c9b3ce60de45bf396714165cbac01c27913e04",
                "status": "modified"
            }
        ],
        "message": "AMBARI-5510. NPE in ganglia property provider (Eugene Chekanskiy via smohanty)",
        "parent": "https://github.com/apache/ambari/commit/08d499f9ea87adc89ca0ee7509dd8f408b82a1ca",
        "repo": "ambari",
        "unit_tests": [
            "GangliaPropertyProviderTest.java"
        ]
    },
    "ambari_7e1c33a": {
        "bug_id": "ambari_7e1c33a",
        "commit": "https://github.com/apache/ambari/commit/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/stackVersions/StackVersionsCreateCtrl.js",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/stackVersions/StackVersionsCreateCtrl.js?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 2,
                "filename": "ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/stackVersions/StackVersionsCreateCtrl.js",
                "patch": "@@ -346,7 +346,7 @@ angular.module('ambariAdminConsole')\n                 $t('versions.register.error.body'),\n                 null,\n                 null,\n-                true\n+                {hideCancelButton: true}\n               )\n             });\n           }\n@@ -419,7 +419,7 @@ angular.module('ambariAdminConsole')\n       },\n       $t('common.controls.ok'),\n       $t('common.controls.cancel'),\n-      true\n+      {hideCancelButton: true}\n     )\n   };\n ",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/stackVersions/StackVersionsCreateCtrl.js",
                "sha": "a2c21c32ac166dc45af3a3e1209625647f6fc4cb",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/stackVersions/StackVersionsListCtrl.js",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/stackVersions/StackVersionsListCtrl.js?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/stackVersions/StackVersionsListCtrl.js",
                "patch": "@@ -164,6 +164,6 @@ angular.module('ambariAdminConsole')\n     };\n \n     $scope.isHideCheckBoxEnabled = function ( repo ) {\n-      return !repo.isProccessing && ( !repo.cluster || repo.isPatch && ( repo.status === 'installed' || repo.status === 'install_failed') );\n+      return !repo.isProccessing && ( (!repo.cluster && repo.status !== 'OUT_OF_SYNC') || repo.isPatch && ( repo.status === 'INSTALLED' || repo.status === 'INSTALL_FAILED') );\n     }\n   }]);",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/stackVersions/StackVersionsListCtrl.js",
                "sha": "1ef50b9e23959a2d49bec68343f590efb85c8d0c",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/userManagement/GroupEditCtrl.js",
                "changes": 46,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/userManagement/GroupEditCtrl.js?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 46,
                "filename": "ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/userManagement/GroupEditCtrl.js",
                "patch": "@@ -71,52 +71,6 @@ function($scope, $rootScope, Group, $routeParams, Cluster, View, Alert, Confirma\n     $scope.isMembersEditing = false;\n   };\n \n-  $scope.deleteGroup = function(group) {\n-    ConfirmationModal.show(\n-      $t('common.delete', {\n-        term: $t('common.group')\n-      }),\n-      $t('common.deleteConfirmation', {\n-        instanceType: $t('common.group').toLowerCase(),\n-        instanceName: '\"' + group.group_name + '\"'\n-      })\n-    ).then(function() {\n-      Cluster.getPrivilegesForResource({\n-        nameFilter : group.group_name,\n-        typeFilter : {value: 'GROUP'}\n-      }).then(function(data) {\n-        var clusterPrivilegesIds = [];\n-        var viewsPrivileges = [];\n-        if (data.items && data.items.length) {\n-          angular.forEach(data.items[0].privileges, function(privilege) {\n-            if (privilege.PrivilegeInfo.principal_type === 'GROUP') {\n-              if (privilege.PrivilegeInfo.type === 'VIEW') {\n-                viewsPrivileges.push({\n-                  id: privilege.PrivilegeInfo.privilege_id,\n-                  view_name: privilege.PrivilegeInfo.view_name,\n-                  version: privilege.PrivilegeInfo.version,\n-                  instance_name: privilege.PrivilegeInfo.instance_name\n-                });\n-              } else {\n-                clusterPrivilegesIds.push(privilege.PrivilegeInfo.privilege_id);\n-              }\n-            }\n-          });\n-        }\n-        group.destroy().then(function() {\n-          $location.path('/userManagement?tab=groups');\n-          if (clusterPrivilegesIds.length) {\n-            Cluster.deleteMultiplePrivileges($rootScope.cluster.Clusters.cluster_name, clusterPrivilegesIds);\n-          }\n-          angular.forEach(viewsPrivileges, function(privilege) {\n-            View.deletePrivilege(privilege);\n-          });\n-        });\n-      });\n-    });\n-  };\n-\n-\n   $scope.removeViewPrivilege = function(name, privilege) {\n     var privilegeObject = {\n         id: privilege.privilege_id,",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/userManagement/GroupEditCtrl.js",
                "sha": "f963992d740b60587bce925879c80ec76666cf6d",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/userManagement/GroupsListCtrl.js",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/userManagement/GroupsListCtrl.js?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/userManagement/GroupsListCtrl.js",
                "patch": "@@ -107,7 +107,10 @@ function($scope, Group, $modal, ConfirmationModal, $rootScope, $translate, Setti\n       $t('common.deleteConfirmation', {\n         instanceType: $t('common.group').toLowerCase(),\n         instanceName: '\"' + group.group_name + '\"'\n-      })\n+      }),\n+      null,\n+      null,\n+      {primaryClass: 'btn-danger'}\n     ).then(function() {\n       Cluster.getPrivilegesForResource({\n         nameFilter : group.group_name,",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/userManagement/GroupsListCtrl.js",
                "sha": "32b0ade7d4d8a28dceebc4299860d64644899082",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/userManagement/UserEditCtrl.js",
                "changes": 45,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/userManagement/UserEditCtrl.js?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 45,
                "filename": "ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/userManagement/UserEditCtrl.js",
                "patch": "@@ -218,51 +218,6 @@ function($scope, $rootScope, $routeParams, Cluster, User, View, $modal, $locatio\n     });\n   };\n \n-  $scope.deleteUser = function () {\n-    ConfirmationModal.show(\n-      $t('common.delete', {\n-        term: $t('common.user')\n-      }),\n-      $t('common.deleteConfirmation', {\n-        instanceType: $t('common.user').toLowerCase(),\n-        instanceName: '\"' + $scope.user.user_name + '\"'\n-      })\n-    ).then(function () {\n-      Cluster.getPrivilegesForResource({\n-        nameFilter: $scope.user.user_name,\n-        typeFilter: {value: 'USER'}\n-      }).then(function (data) {\n-        var clusterPrivilegesIds = [];\n-        var viewsPrivileges = [];\n-        if (data.items && data.items.length) {\n-          angular.forEach(data.items[0].privileges, function (privilege) {\n-            if (privilege.PrivilegeInfo.principal_type === 'USER') {\n-              if (privilege.PrivilegeInfo.type === 'VIEW') {\n-                viewsPrivileges.push({\n-                  id: privilege.PrivilegeInfo.privilege_id,\n-                  view_name: privilege.PrivilegeInfo.view_name,\n-                  version: privilege.PrivilegeInfo.version,\n-                  instance_name: privilege.PrivilegeInfo.instance_name\n-                });\n-              } else {\n-                clusterPrivilegesIds.push(privilege.PrivilegeInfo.privilege_id);\n-              }\n-            }\n-          });\n-        }\n-        User.delete($scope.user.user_name).then(function () {\n-          $location.path('/userManagement?tab=users');\n-          if (clusterPrivilegesIds.length) {\n-            Cluster.deleteMultiplePrivileges($rootScope.cluster.Clusters.cluster_name, clusterPrivilegesIds);\n-          }\n-          angular.forEach(viewsPrivileges, function (privilege) {\n-            View.deletePrivilege(privilege);\n-          });\n-        });\n-      });\n-    });\n-  };\n-\n   function deleteUserRoles(clusterName, user, ignoreAlert) {\n     return Cluster.deleteMultiplePrivileges(\n       clusterName,",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/userManagement/UserEditCtrl.js",
                "sha": "f8633ce38acec88a07435fecbcd46cae56b16b86",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/userManagement/UsersListCtrl.js",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/userManagement/UsersListCtrl.js?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/userManagement/UsersListCtrl.js",
                "patch": "@@ -138,14 +138,20 @@ function($scope, User, $modal, $rootScope, UserConstants, $translate, Cluster, V\n   };\n \n   $scope.deleteUser = function (user) {\n+    if (!user.isDeletable) {\n+      return false;\n+    }\n     ConfirmationModal.show(\n       $t('common.delete', {\n         term: $t('common.user')\n       }),\n       $t('common.deleteConfirmation', {\n         instanceType: $t('common.user').toLowerCase(),\n         instanceName: '\"' + user.user_name + '\"'\n-      })\n+      }),\n+      null,\n+      null,\n+      {primaryClass: 'btn-danger'}\n     ).then(function () {\n       Cluster.getPrivilegesForResource({\n         nameFilter: user.user_name,",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/controllers/userManagement/UsersListCtrl.js",
                "sha": "012a1cc0e04cf9ba2fc79b8b94e8a6ef7a99f58e",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/directives/editableList.js",
                "changes": 82,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-admin/src/main/resources/ui/admin-web/app/scripts/directives/editableList.js?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 29,
                "filename": "ambari-admin/src/main/resources/ui/admin-web/app/scripts/directives/editableList.js",
                "patch": "@@ -98,8 +98,9 @@ angular.module('ambariAdminConsole')\n             break;\n           case 13: // Enter\n             $scope.$apply(function() {\n-              $scope.addItem();\n-              $scope.focusOnInput();\n+              if ($scope.addItem()) {\n+                $scope.focusOnInput();\n+              }\n             });\n             return false;\n             break;\n@@ -129,6 +130,10 @@ angular.module('ambariAdminConsole')\n       $scope.input = '';\n       $scope.typeahead = [];\n       $scope.selectedTypeahed = 0;\n+      $scope.resources = [];\n+      $scope.invalidInput = false;\n+\n+      preloadResources();\n \n       // Watch source of items\n       $scope.$watch(function() {\n@@ -141,37 +146,33 @@ angular.module('ambariAdminConsole')\n       $scope.$watch(function() {\n         return $scope.input;\n       }, function(newValue) {\n+        $scope.invalidInput = false;\n         if(newValue){\n           var newValue = newValue.split(',').filter(function(i){ \n             i = i.replace('&nbsp;', ''); // Sanitize from spaces\n             return !!i.trim();\n           }).map(function(i) { return i.trim(); });\n           if( newValue.length > 1){\n+            var validInput = true;\n             // If someone paste coma separated string, then just add all items to list\n             angular.forEach(newValue, function(item) {\n-              $scope.addItem(item);\n+              if (validInput) {\n+                validInput = $scope.addItem(item);\n+              }\n             });\n-            $scope.clearInput();\n-            $scope.focusOnInput();\n-            \n+            if (validInput) {\n+              $scope.clearInput();\n+              $scope.focusOnInput();\n+            }\n           } else {\n-            // Load typeahed items based on current input\n-            $resource.listByName(encodeURIComponent(newValue)).then(function(data) {\n-              var items = [];\n-              angular.forEach(data.data.items, function(item) {\n-                var name;\n-                if($scope.resourceType === 'User'){\n-                  name = item.Users.user_name;\n-                } else if($scope.resourceType === 'Group'){\n-                  name = item.Groups.group_name;\n-                }\n-                if($scope.items.indexOf(name) < 0){ // Only if item not in list\n-                  items.push(name);\n-                }\n-              });\n-              $scope.typeahead = items.slice(0, 5);\n-              $scope.selectedTypeahed = 0;\n+            var items = [];\n+            angular.forEach($scope.resources, function (name) {\n+              if (name.indexOf(newValue) !== -1 && $scope.items.indexOf(name) === -1) {\n+                items.push(name);\n+              }\n             });\n+            $scope.typeahead = items.slice(0, 5);\n+            $scope.selectedTypeahed = 0;\n           }\n         } else {\n           $scope.typeahead = [];\n@@ -180,6 +181,20 @@ angular.module('ambariAdminConsole')\n         }\n       });\n \n+      function preloadResources() {\n+        $resource.listByName('').then(function(data) {\n+          if (data && data.data.items) {\n+            $scope.resources = data.data.items.map(function(item) {\n+              if ($scope.resourceType === 'User') {\n+                return item.Users.user_name;\n+              } else if ($scope.resourceType === 'Group') {\n+                return item.Groups.group_name;\n+              }\n+            });\n+          }\n+        });\n+      }\n+\n       $scope.enableEditMode = function(event) {\n         if( $scope.editable && !$scope.editMode){\n           //only one editable-list could be in edit mode at once\n@@ -197,12 +212,15 @@ angular.module('ambariAdminConsole')\n         event.stopPropagation();\n       };\n       $scope.save = function(event) {\n+        var validInput = true;\n         if( $scope.input ){\n-          $scope.addItem($scope.input);\n+          validInput = $scope.addItem($scope.input);\n+        }\n+        if (validInput) {\n+          $scope.itemsSource = $scope.items;\n+          $scope.editMode = false;\n+          $scope.input = '';\n         }\n-        $scope.itemsSource = $scope.items;\n-        $scope.editMode = false;\n-        $scope.input = '';\n         if(event){\n           event.stopPropagation();\n         }\n@@ -224,10 +242,16 @@ angular.module('ambariAdminConsole')\n       $scope.addItem = function(item) {\n         item = item ? item : $scope.typeahead.length ? $scope.typeahead[$scope.selectedTypeahed] : $scope.input;\n         \n-        if(item && $scope.items.indexOf(item) < 0){\n-          $scope.items.push(item);\n-          $scope.input = '';\n+        if (item && $scope.items.indexOf(item) === -1){\n+          if ($scope.resources.indexOf(item) !== -1) {\n+            $scope.items.push(item);\n+            $scope.input = '';\n+          } else {\n+            $scope.invalidInput = true;\n+            return false;\n+          }\n         }\n+        return true;\n       };\n \n       $scope.removeFromItems = function(item) {",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/directives/editableList.js",
                "sha": "4a2ec441eada3df73614e92d25c327e4e246344a",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/i18n.config.js",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-admin/src/main/resources/ui/admin-web/app/scripts/i18n.config.js?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-admin/src/main/resources/ui/admin-web/app/scripts/i18n.config.js",
                "patch": "@@ -314,6 +314,8 @@ angular.module('ambariAdminConsole')\n     'users.showAdmin': 'Show only admin users',\n     'users.groupMembership': 'Group Membership',\n     'users.userNameTip': 'Maximum length is 80 characters. \\\\, &, |, <, >, ` are not allowed.',\n+    'users.adminTip': 'An Ambari Admin can create new clusters and other Ambari Admin Users.',\n+    'users.deactivateTip': 'Active Users can log in to Ambari. Inactive Users cannot.',\n \n     'users.changeStatusConfirmation.title': 'Change Status',\n     'users.changeStatusConfirmation.message': 'Are you sure you want to change status for user \"{{userName}}\" to {{status}}?',",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/i18n.config.js",
                "sha": "58b068d098d7201932fe6bdcac0858fbb9f865d2",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/services/ConfirmationModal.js",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-admin/src/main/resources/ui/admin-web/app/scripts/services/ConfirmationModal.js?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 2,
                "filename": "ambari-admin/src/main/resources/ui/admin-web/app/scripts/services/ConfirmationModal.js",
                "patch": "@@ -23,7 +23,7 @@ angular.module('ambariAdminConsole')\n   var $t = $translate.instant;\n \n \treturn {\n-\t\tshow: function(header, body, confirmText, cancelText, hideCancelButton) {\n+\t\tshow: function(header, body, confirmText, cancelText, options) {\n \t\t\tvar deferred = $q.defer();\n \n \t\t\tvar modalInstance = $modal.open({\n@@ -35,7 +35,8 @@ angular.module('ambariAdminConsole')\n           $scope.innerScope = body.scope;\n           $scope.confirmText = confirmText || $t('common.controls.ok');\n           $scope.cancelText = cancelText || $t('common.controls.cancel');\n-\t\t\t\t\t$scope.showCancelButton = !hideCancelButton;\n+          $scope.primaryClass = options.primaryClass || 'btn-primary',\n+\t\t\t\t\t$scope.showCancelButton = !options.hideCancelButton;\n \n \t\t\t\t\t$scope.ok = function() {\n \t\t\t\t\t\t$modalInstance.close();",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/services/ConfirmationModal.js",
                "sha": "dd3e3203eb646da5b552068480cfa1cc1dc6224f",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/services/User.js",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-admin/src/main/resources/ui/admin-web/app/scripts/services/User.js?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-admin/src/main/resources/ui/admin-web/app/scripts/services/User.js",
                "patch": "@@ -18,7 +18,7 @@\n 'use strict';\n \n angular.module('ambariAdminConsole')\n-.factory('User', ['Restangular', '$http', 'Settings', 'UserConstants', '$translate', 'Cluster', function(Restangular, $http, Settings, UserConstants, $translate, Cluster) {\n+.factory('User', ['Restangular', '$http', 'Settings', 'UserConstants', '$translate', 'Cluster', 'Auth', function(Restangular, $http, Settings, UserConstants, $translate, Cluster, Auth) {\n   Restangular.addResponseInterceptor(function(data, operation, what, url, response, deferred) {\n     var extractedData;\n     if(operation === 'getList'){\n@@ -99,6 +99,7 @@ angular.module('ambariAdminConsole')\n      * @returns {Object}\n      */\n     makeUser: function(user) {\n+      user.Users.isDeletable = !(user.Users.user_name === Auth.getCurrentUser() || user.Users.user_type !== 'LOCAL');\n       user.Users.encodedName = encodeURIComponent(user.Users.user_name);\n       user.Users.userTypeName = $t(UserConstants.TYPES[user.Users.user_type].LABEL_KEY);\n       user.Users.ldapUser = user.Users.user_type === UserConstants.TYPES.LDAP.VALUE;",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/scripts/services/User.js",
                "sha": "e9e52e5093daeee6f7fbad60648802ab46d09c09",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/styles/bootstrap-overrides.css",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-admin/src/main/resources/ui/admin-web/app/styles/bootstrap-overrides.css?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 3,
                "filename": "ambari-admin/src/main/resources/ui/admin-web/app/styles/bootstrap-overrides.css",
                "patch": "@@ -15,6 +15,7 @@\n  * See the License for the specific language governing permissions and\n  * limitations under the License.\n  */\n-@InterfaceAudience.Private\n-package org.apache.hadoop.yarn.server.applicationhistoryservice.timeline;\n-import org.apache.hadoop.classification.InterfaceAudience;\n+\n+.modal-body .form-group label {\n+  font-weight: normal;\n+}",
                "previous_filename": "ambari-metrics/ambari-metrics-timelineservice/src/main/java/org/apache/hadoop/yarn/server/applicationhistoryservice/timeline/package-info.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/styles/bootstrap-overrides.css",
                "sha": "8f30045358d568e49bfd72b767476e48d7688b51",
                "status": "renamed"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/styles/main.css",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-admin/src/main/resources/ui/admin-web/app/styles/main.css?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 8,
                "filename": "ambari-admin/src/main/resources/ui/admin-web/app/styles/main.css",
                "patch": "@@ -139,6 +139,12 @@\n   position: relative;\n   cursor: pointer;\n }\n+\n+.has-error.add-item-input {\n+  color: #666;\n+  border: 1px solid #EF6162 !important;\n+}\n+\n .add-item-input span:focus{\n   cursor: default;\n }\n@@ -754,8 +760,9 @@ accordion .panel-group .panel{\n   padding-left: 19px;\n }\n \n-.repo-version-inline .dot {\n-  padding-right: 15px;\n+.repo-version-inline .form-group {\n+  margin-left: 0;\n+  margin-right: 0;\n }\n \n .status-CURRENT {\n@@ -984,12 +991,14 @@ thead.view-permission-header > tr > th {\n }\n \n #current-stack-details {\n-  bottom: 20px;\n-  padding-left: 0px;\n+  padding-left: 0;\n+  margin-right: 15px;\n }\n \n-#current-stack-details .table-borderless {\n- border: transparent;\n+#current-stack-details .table {\n+  position: absolute;\n+  top: 2px;\n+  border: transparent;\n }\n \n #current-stack-details .table-borderless tbody tr td, .table-borderless tbody tr th, .table-borderless thead tr th {\n@@ -1084,7 +1093,7 @@ thead.view-permission-header > tr > th {\n   overflow: auto;\n   border: 1px solid #ddd;\n   padding: 8px 25px;\n-  margin: 8px;\n+  margin: 2px;\n }\n \n .register-version-form .details-panel .control-label {\n@@ -1374,5 +1383,15 @@ th.entity-actions {\n   transform: rotate(45deg);\n   top: 10px;\n   left: 2px;\n-  box-shadow: -2px -2px 10px -3px rgba(0, 0, 0, 0.5);\n+  border: 1px solid #ccc;\n+}\n+\n+a.disabled i:before,\n+a[disabled] i:before {\n+  color: #ccc;\n+  cursor: not-allowed;\n+}\n+\n+.display-inline {\n+  display: inline;\n }",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/styles/main.css",
                "sha": "08f8960658d0e7a6b455547bae869e8fc8803d42",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/views/directives/editableList.html",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-admin/src/main/resources/ui/admin-web/app/views/directives/editableList.html?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-admin/src/main/resources/ui/admin-web/app/views/directives/editableList.html",
                "patch": "@@ -19,7 +19,16 @@\n <div class=\"editable-list-container well\" ng-class=\"{'edit-mode' : editMode, 'disabled' : !editable}\" ng-click=\"enableEditMode($event)\">\n   <div class=\"items-box\">\n     <ul class=\"items-list\">\n-      <li class=\"item\" ng-repeat=\"item in items | orderBy:identity\"><span><a href>{{item}}</a><button ng-click=\"removeFromItems(item)\" type=\"button\" class=\"close\"><span aria-hidden=\"true\">&times;</span><span class=\"sr-only\">{{'common.controls.close' | translate}}</span></button></span></li><li class=\"item add-item-input\" ng-show=\"editMode\">\n+      <li class=\"item\" ng-repeat=\"item in items | orderBy:identity\">\n+        <span>\n+          <a href>{{item}}</a>\n+          <button ng-click=\"removeFromItems(item)\" type=\"button\" class=\"close\">\n+            <span aria-hidden=\"true\">&times;</span>\n+            <span class=\"sr-only\">{{'common.controls.close' | translate}}</span>\n+          </button>\n+        </span>\n+      </li>\n+      <li class=\"item add-item-input\" ng-class=\"{'has-error': invalidInput}\" ng-show=\"editMode\">\n         <span contenteditable></span>\n         <div class=\"typeahead-box\" ng-show=\"typeahead.length != 0\">\n           <ul>",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/views/directives/editableList.html",
                "sha": "fdf9fce6cd95e51f81295748ca25cbdee3e1aa1e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/views/modals/ConfirmationModal.html",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-admin/src/main/resources/ui/admin-web/app/views/modals/ConfirmationModal.html?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-admin/src/main/resources/ui/admin-web/app/views/modals/ConfirmationModal.html",
                "patch": "@@ -24,5 +24,5 @@ <h3 class=\"modal-title\">{{header}}</h3>\n </div>\n <div class=\"modal-footer\">\n     <button class=\"btn btn-default\" ng-if=\"showCancelButton\" ng-click=\"cancel()\">{{cancelText}}</button>\n-    <button class=\"btn btn-primary\" ng-click=\"ok()\">{{confirmText}}</button>\n+    <button class=\"btn\" ng-class=\"primaryClass\" ng-click=\"ok()\">{{confirmText}}</button>\n </div>\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/views/modals/ConfirmationModal.html",
                "sha": "308d9f21d6de0f242414f1182cf8aae9c9f33346",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/views/stackVersions/stackVersionPage.html",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-admin/src/main/resources/ui/admin-web/app/views/stackVersions/stackVersionPage.html?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 11,
                "filename": "ambari-admin/src/main/resources/ui/admin-web/app/views/stackVersions/stackVersionPage.html",
                "patch": "@@ -50,7 +50,7 @@\n     <div class=\"tab-content\">\n       <div class=\"panel panel-default details-panel\">\n         <div class=\"panel-body\">\n-          <div class=\"col-sm-3 .pull-left\" id=\"current-stack-details\" ng-if=\"editController\">\u2028\n+          <div class=\"col-sm-3\" id=\"current-stack-details\" ng-if=\"editController\">\u2028\n             <table class='table table-borderless alert alert-info'>\n               <tr>\n                 <td>{{'common.stack' | translate}}</td>\n@@ -88,13 +88,11 @@\n               </ul>\n             </div>\n             <div class=\"pull-right form-inline repo-version-inline\" ng-if=\"activeStackVersion.isNonXMLdata\">\n-              <label class=\"control-label col-sm-1 repo-version-label\">{{'common.name' | translate}}:</label>\n-              <div class=\"col-sm-11\">\n-                <span class=\"control-label\">\n-                  {{activeStackVersion.stackNameVersion}}\n-                </span>\n-                <span class=\"dot\">.</span>\n-                <div class=\"form-group\" ng-class=\"{'has-error' : versionRegForm.version.$error.pattern}\">\n+              <div>\n+                <label class=\"control-label repo-version-label\">\n+                  {{'common.name' | translate}}:&nbsp;{{activeStackVersion.stackNameVersion}}&nbsp;.\n+                </label>\n+                <div class=\"form-group reset-horizontal-margin\" ng-class=\"{'has-error' : versionRegForm.version.$error.pattern}\">\n                   <input class=\"form-control\" name=\"version\" type=\"text\" ng-model=\"activeStackVersion.editableDisplayName\" ng-pattern=\"activeStackVersion.subVersionPattern\"\n                          placeholder=\"{{'versions.placeholder' | translate:{pattern: activeStackVersion.pattern} }}\" ng-change=\"updateCurrentVersionInput()\" required/>\n                 </div>\n@@ -104,7 +102,6 @@\n               </div>\n             </div>\n           </div>\n-          <br>\n           <div class=\"version-contents-section\" ng-class=\"{'version-contents-section-register-version': createController}\">\n             <table class=\"table table-striped table-condensed\">\n               <tr ng-repeat=\"service in activeStackVersion.services\">\n@@ -199,8 +196,10 @@ <h3 class=\"panel-title\">\n                     </div>\n                   </div>\n                 </div>\n-                <div class=\"col-sm-1 remove-icon\" ng-click=\"removeOS()\" ng-class=\"{'disabled' : useRedhatSatellite}\"><i\n-                        class=\"fa fa-minus\" aria-hidden=\"true\"></i>{{'common.controls.remove' | translate}}\n+                <div class=\"col-sm-1 remove-icon\" ng-click=\"removeOS()\" ng-class=\"{'disabled' : useRedhatSatellite}\">\n+                  <span>\n+                    <i class=\"fa fa-minus display-inline\" aria-hidden=\"true\"></i>&nbsp;{{'common.controls.remove' | translate}}\n+                  </span>\n                 </div>\n               </div>\n             </div>",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/views/stackVersions/stackVersionPage.html",
                "sha": "1eb247c37f7073d3b28e487a5d076217c16bae7c",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/views/userManagement/groupEdit.html",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-admin/src/main/resources/ui/admin-web/app/views/userManagement/groupEdit.html?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 13,
                "filename": "ambari-admin/src/main/resources/ui/admin-web/app/views/userManagement/groupEdit.html",
                "patch": "@@ -22,19 +22,6 @@\n       <li><a href=\"#/userManagement?tab=groups\">{{'common.groups' | translate}}</a></li>\n       <li class=\"active\">{{group.group_name}}</li>\n     </ol>\n-    <div class=\"pull-right\">\n-      <div ng-switch=\"group.group_type != 'LOCAL'\">\n-        <button\n-          ng-switch-when=\"true\"\n-          class=\"btn disabled deletegroup-btn\"\n-          tooltip=\"{{'common.cannotDelete' | translate:{term: constants.group} }}\">\n-          {{'common.delete' | translate:{term: constants.group} }}\n-        </button>\n-        <button ng-switch-when=\"false\" class=\"btn btn-danger deletegroup-btn\" ng-click=\"deleteGroup(group)\">\n-          {{'common.delete' | translate:{term: constants.group} }}\n-        </button>\n-      </div>\n-    </div>\n   </div>\n \n   <form class=\"form-horizontal\" role=\"form\" novalidate name=\"form\" >",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/views/userManagement/groupEdit.html",
                "sha": "fbab9de84fe8c96dc05c5c38cbba34a9b32d7b1d",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/views/userManagement/modals/userCreate.html",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-admin/src/main/resources/ui/admin-web/app/views/userManagement/modals/userCreate.html?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 2,
                "filename": "ambari-admin/src/main/resources/ui/admin-web/app/views/userManagement/modals/userCreate.html",
                "patch": "@@ -124,7 +124,11 @@ <h1 class=\"modal-title col-sm-8\">\n     <div class=\"form-group\">\n       <label>\n         {{'users.isAmbariAdmin' | translate}}<span>&nbsp;*</span>\n-        <i class=\"fa fa-question-circle\" aria-hidden=\"true\"></i>\n+        <i class=\"fa fa-question-circle\"\n+           aria-hidden=\"true\"\n+           tooltip=\"{{'users.adminTip' | translate}}\"\n+           tooltip-trigger=\"click\"\n+           tooltip-placement=\"top\"></i>\n       </label>\n       <div>\n         <toggle-switch model=\"formData.isAdmin\" class=\"switch-success\" data-off-color=\"danger\"></toggle-switch>\n@@ -136,7 +140,11 @@ <h1 class=\"modal-title col-sm-8\">\n     <div class=\"form-group\">\n       <label>\n         {{'users.isActive' | translate}}<span>&nbsp;*</span>\n-        <i class=\"fa fa-question-circle\" aria-hidden=\"true\"></i>\n+        <i class=\"fa fa-question-circle\"\n+           aria-hidden=\"true\"\n+           tooltip=\"{{'users.deactivateTip' | translate}}\"\n+           tooltip-trigger=\"click\"\n+           tooltip-placement=\"top\"></i>\n       </label>\n       <div>\n         <toggle-switch model=\"formData.isActive\" class=\"switch-success\" data-off-color=\"danger\"></toggle-switch>",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/views/userManagement/modals/userCreate.html",
                "sha": "01893628ebe268adf6e279188b8e856d02924b90",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/views/userManagement/userEdit.html",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-admin/src/main/resources/ui/admin-web/app/views/userManagement/userEdit.html?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 6,
                "filename": "ambari-admin/src/main/resources/ui/admin-web/app/views/userManagement/userEdit.html",
                "patch": "@@ -22,12 +22,6 @@\n       <li><a href=\"#/userManagement?tab=users\">{{'common.users' | translate}}</a></li>\n       <li class=\"active\">{{user.user_name}}</li>\n     </ol>\n-    <div class=\"pull-right\">\n-      <div ng-switch=\"isCurrentUser || user.user_type != 'LOCAL'\">\n-        <button class=\"btn deleteuser-btn disabled btn-default\" ng-switch-when=\"true\" tooltip=\"{{'common.cannotDelete' | translate:{term: constants.user} }}\">{{'common.delete' | translate:{term: constants.user} }}</button>\n-        <button class=\"btn deleteuser-btn btn-danger\" ng-switch-when=\"false\" ng-click=\"deleteUser()\">{{'common.delete' | translate:{term: constants.user} }}</button>\n-      </div>\n-    </div>\n   </div>\n   <hr>\n   <form class=\"form-horizontal\" role=\"form\" >",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/views/userManagement/userEdit.html",
                "sha": "9c15041821b375d1c7637861cf79d16036d0d813",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/views/userManagement/usersList.html",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-admin/src/main/resources/ui/admin-web/app/views/userManagement/usersList.html?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-admin/src/main/resources/ui/admin-web/app/views/userManagement/usersList.html",
                "patch": "@@ -77,7 +77,7 @@\n           <a href=\"#/users/{{user.Users.encodedName}}/edit\">\n             <i class=\"fa fa-pencil\"></i>\n           </a>\n-          <a href ng-click=\"deleteUser(user.Users)\">\n+          <a href ng-click=\"deleteUser(user.Users)\" ng-disabled=\"!user.Users.isDeletable\">\n             <i class=\"fa fa-trash-o\"></i>\n           </a>\n         </td>",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-admin/src/main/resources/ui/admin-web/app/views/userManagement/usersList.html",
                "sha": "8b93eb58cfb3ae607afa197507a152281a6d128f",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/pom.xml",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/pom.xml?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 6,
                "filename": "ambari-agent/pom.xml",
                "patch": "@@ -63,12 +63,10 @@\n     <dependency>\n       <groupId>commons-cli</groupId>\n       <artifactId>commons-cli</artifactId>\n-      <version>1.3.1</version>\n     </dependency>\n     <dependency>\n       <groupId>commons-collections</groupId>\n       <artifactId>commons-collections</artifactId>\n-      <version>3.2.2</version>\n     </dependency>\n     <dependency>\n       <groupId>commons-configuration</groupId>\n@@ -165,7 +163,6 @@\n       <plugin>\n         <groupId>org.codehaus.mojo</groupId>\n         <artifactId>exec-maven-plugin</artifactId>\n-        <version>1.2.1</version>\n         <executions>\n           <execution>\n             <configuration>\n@@ -488,7 +485,7 @@\n           </execution>\n            <execution>\n             <id>copy-resources-filter</id>\n-            <phase>generate-resources</phase>\n+            <phase>process-resources</phase>\n             <goals>\n               <goal>copy-resources</goal>\n             </goals>\n@@ -709,7 +706,6 @@\n           <plugin>\n             <groupId>org.codehaus.mojo</groupId>\n             <artifactId>exec-maven-plugin</artifactId>\n-            <version>1.2.1</version>\n             <executions>\n               <execution>\n                 <id>build-choco-package</id>\n@@ -755,7 +751,6 @@\n           <plugin>\n             <groupId>org.codehaus.mojo</groupId>\n             <artifactId>exec-maven-plugin</artifactId>\n-            <version>1.2.1</version>\n             <executions>\n               <execution>\n                 <configuration>",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/pom.xml",
                "sha": "33160468daca8d6965542b67a45dd64d0f2556aa",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/AlertStatusReporter.py",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/AlertStatusReporter.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-agent/src/main/python/ambari_agent/AlertStatusReporter.py",
                "patch": "@@ -59,7 +59,7 @@ def run(self):\n           changed_alerts = self.get_changed_alerts(alerts)\n \n           if changed_alerts and self.initializer_module.is_registered:\n-            self.initializer_module.connection.send(message=changed_alerts, destination=Constants.ALERTS_STATUS_REPORTS_ENDPOINT)\n+            self.initializer_module.connection.send(message=changed_alerts, destination=Constants.ALERTS_STATUS_REPORTS_ENDPOINT, log_message_function=AlertStatusReporter.log_sending)\n             self.save_results(changed_alerts)\n       except ConnectionIsAlreadyClosed: # server and agent disconnected during sending data. Not an issue\n         pass\n@@ -93,3 +93,18 @@ def get_changed_alerts(self, alerts):\n         changed_alerts.append(alert)\n \n     return changed_alerts\n+    \n+  @staticmethod\n+  def log_sending(message_dict):\n+    \"\"\"\n+    Returned dictionary will be used while logging sent alert status.\n+    Used because full dict is too big for logs and should be shortened\n+    \"\"\"\n+    try:\n+      for alert_status in message_dict:\n+        if 'text' in alert_status:\n+          alert_status['text'] = '...'\n+    except KeyError:\n+      pass\n+      \n+    return message_dict",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/AlertStatusReporter.py",
                "sha": "bfb0e4f637c626a847917005d751fd6d9dc56844",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/AmbariAgent.py",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/AmbariAgent.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-agent/src/main/python/ambari_agent/AmbariAgent.py",
                "patch": "@@ -22,7 +22,8 @@\n import sys\n from ambari_commons import subprocess32\n import signal\n-from Controller import AGENT_AUTO_RESTART_EXIT_CODE\n+\n+AGENT_AUTO_RESTART_EXIT_CODE = 77\n \n if os.environ.has_key(\"PYTHON_BIN\"):\n   AGENT_SCRIPT = os.path.join(os.environ[\"PYTHON_BIN\"],\"site-packages/ambari_agent/main.py\")",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/AmbariAgent.py",
                "sha": "6f5a14efcf7f7e2433d857602c32417b8579a71d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/AmbariConfig.py",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/AmbariConfig.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 2,
                "filename": "ambari-agent/src/main/python/ambari_agent/AmbariConfig.py",
                "patch": "@@ -319,9 +319,9 @@ def get_force_https_protocol_name(self):\n     \"\"\"\n     Get forced https protocol name.\n \n-    :return: protocol name, PROTOCOL_TLSv1 by default\n+    :return: protocol name, PROTOCOL_TLSv1_2 by default\n     \"\"\"\n-    return self.get('security', 'force_https_protocol', default=\"PROTOCOL_TLSv1\")\n+    return self.get('security', 'force_https_protocol', default=\"PROTOCOL_TLSv1_2\")\n \n   def get_force_https_protocol_value(self):\n     \"\"\"",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/AmbariConfig.py",
                "sha": "d98212543b599c95f6c72fe98fe82c981289c07c",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/ClusterTopologyCache.py",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/ClusterTopologyCache.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 4,
                "filename": "ambari-agent/src/main/python/ambari_agent/ClusterTopologyCache.py",
                "patch": "@@ -80,8 +80,9 @@ def on_cache_update(self):\n \n       current_host_id = self.current_host_ids_to_cluster[cluster_id]\n       for component_dict in self[cluster_id].components:\n-        if current_host_id in component_dict.hostIds:\n-          self.cluster_local_components[cluster_id].append(component_dict.componentName)\n+        if 'hostIds' in component_dict and current_host_id in component_dict.hostIds:\n+          if current_host_id in component_dict.hostIds:\n+            self.cluster_local_components[cluster_id].append(component_dict.componentName)\n \n \n     self.hosts_to_id = ImmutableDictionary(hosts_to_id)\n@@ -188,8 +189,11 @@ def cache_update(self, cache_update, cache_hash):\n         for component_updates_dict in cluster_updates_dict['components']:\n           component_mutable_dict = ClusterTopologyCache._find_component_in_dict(components_mutable_list, component_updates_dict['serviceName'], component_updates_dict['componentName'])\n           if component_mutable_dict is not None:\n-            component_updates_dict['hostIds'] += component_mutable_dict['hostIds']\n-            component_updates_dict['hostIds'] = list(set(component_updates_dict['hostIds']))\n+            if 'hostIds' in component_updates_dict:\n+              if not 'hostIds' in component_mutable_dict:\n+                component_mutable_dict['hostIds'] = []\n+              component_updates_dict['hostIds'] += component_mutable_dict['hostIds']\n+              component_updates_dict['hostIds'] = list(set(component_updates_dict['hostIds']))\n             component_mutable_dict.update(component_updates_dict)\n           else:\n             components_mutable_list.append(component_updates_dict)",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/ClusterTopologyCache.py",
                "sha": "f23c7f7ba3f7d36566a6d4b7399b05a348333084",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/CommandStatusDict.py",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/CommandStatusDict.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 12,
                "filename": "ambari-agent/src/main/python/ambari_agent/CommandStatusDict.py",
                "patch": "@@ -18,6 +18,7 @@\n limitations under the License.\n '''\n \n+import os\n import logging\n import threading\n import copy\n@@ -76,7 +77,7 @@ def force_update_to_server(self, reports_dict):\n       return False\n \n     try:\n-      self.initializer_module.connection.send(message={'clusters':reports_dict}, destination=Constants.COMMANDS_STATUS_REPORTS_ENDPOINT)\n+      self.initializer_module.connection.send(message={'clusters':reports_dict}, destination=Constants.COMMANDS_STATUS_REPORTS_ENDPOINT, log_message_function=CommandStatusDict.log_sending)\n       return True\n     except ConnectionIsAlreadyClosed:\n       return False\n@@ -131,17 +132,18 @@ def generate_in_progress_report(self, command, report):\n     and populates other fields of report.\n     \"\"\"\n     from ActionQueue import ActionQueue\n-    try:\n-      tmpout = open(report['tmpout'], 'r').read()\n-      tmperr = open(report['tmperr'], 'r').read()\n-    except Exception, err:\n-      logger.warn(err)\n-      tmpout = '...'\n-      tmperr = '...'\n-    try:\n-      tmpstructuredout = open(report['structuredOut'], 'r').read()\n-    except Exception:\n-      tmpstructuredout = '{}'\n+    \n+    files_to_read = [report['tmpout'], report['tmperr'], report['structuredOut']]\n+    files_content = ['...', '...', '{}']\n+\n+    for i in xrange(len(files_to_read)):\n+      filename = files_to_read[i]\n+      if os.path.exists(filename):\n+        with open(filename, 'r') as fp:\n+          files_content[i] = fp.read()\n+          \n+    tmpout, tmperr, tmpstructuredout = files_content\n+\n     grep = Grep()\n     output = grep.tail(tmpout, Grep.OUTPUT_LAST_LINES)\n     inprogress = self.generate_report_template(command)\n@@ -169,5 +171,21 @@ def generate_report_template(self, command):\n       'roleCommand': command['roleCommand']\n     }\n     return stub\n+    \n+  @staticmethod\n+  def log_sending(message_dict):\n+    \"\"\"\n+    Returned dictionary will be used while logging sent component status.\n+    Used because full dict is too big for logs and should be shortened\n+    \"\"\"\n+    try:\n+      for cluster_id in message_dict['clusters']:\n+        for command_status in message_dict['clusters'][cluster_id]:\n+          if 'stdout' in command_status:\n+            command_status['stdout'] = '...'\n+    except KeyError:\n+      pass\n+      \n+    return message_dict\n \n ",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/CommandStatusDict.py",
                "sha": "0ddc1d9b6d3c6016356699e5ac4800659ab18797",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/ComponentStatusExecutor.py",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/ComponentStatusExecutor.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-agent/src/main/python/ambari_agent/ComponentStatusExecutor.py",
                "patch": "@@ -93,6 +93,11 @@ def run(self):\n               service_name = component_dict.serviceName\n               component_name = component_dict.componentName\n \n+              # do not run status commands for the component which is starting/stopping or doing other action\n+              if self.customServiceOrchestrator.commandsRunningForComponent(cluster_id, component_name):\n+                logger.info(\"Skipping status command for {0}. Since command for it is running\".format(component_name))\n+                continue\n+\n               command_dict = {\n                 'serviceName': service_name,\n                 'role': component_name,\n@@ -103,6 +108,11 @@ def run(self):\n               component_status_result = self.customServiceOrchestrator.requestComponentStatus(command_dict)\n               status = LiveStatus.LIVE_STATUS if component_status_result['exitcode'] == 0 else LiveStatus.DEAD_STATUS\n \n+              # if exec command for component started to run after status command completion\n+              if self.customServiceOrchestrator.commandsRunningForComponent(cluster_id, component_name):\n+                logger.info(\"Skipped status command result for {0}. Since command for it is running\".format(component_name))\n+                continue\n+\n               # log if status command failed\n               if status == LiveStatus.DEAD_STATUS:\n                 stderr = component_status_result['stderr']",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/ComponentStatusExecutor.py",
                "sha": "cebf19041bf9116de4bbca32ec1551424ed82592",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/ConfigurationBuilder.py",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/ConfigurationBuilder.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-agent/src/main/python/ambari_agent/ConfigurationBuilder.py",
                "patch": "@@ -30,7 +30,7 @@ def __init__(self, initializer_module):\n \n   def get_configuration(self, cluster_id, service_name, component_name, configurations_timestamp=None):\n     if cluster_id:\n-      if configurations_timestamp and configurations_timestamp != self.configurations_cache.timestamp:\n+      if configurations_timestamp and self.configurations_cache.timestamp < configurations_timestamp:\n         raise Exception(\"Command requires configs with timestamp={0} but configs on agent have timestamp={1}\".format(configurations_timestamp, self.configurations_cache.timestamp))\n \n       metadata_cache = self.metadata_cache[cluster_id]",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/ConfigurationBuilder.py",
                "sha": "fb912fb853a42f0da98732d8f4d8341f50571e51",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/Constants.py",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/Constants.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-agent/src/main/python/ambari_agent/Constants.py",
                "patch": "@@ -26,9 +26,10 @@\n METADATA_TOPIC = '/events/metadata'\n TOPOLOGIES_TOPIC = '/events/topologies'\n SERVER_RESPONSES_TOPIC = '/user/'\n+AGENT_ACTIONS_TOPIC = '/user/agent_actions'\n \n PRE_REGISTRATION_TOPICS_TO_SUBSCRIBE = [SERVER_RESPONSES_TOPIC]\n-POST_REGISTRATION_TOPICS_TO_SUBSCRIBE = [COMMANDS_TOPIC, CONFIGURATIONS_TOPIC, METADATA_TOPIC, TOPOLOGIES_TOPIC, HOST_LEVEL_PARAMS_TOPIC, ALERTS_DEFINITIONS_TOPIC]\n+POST_REGISTRATION_TOPICS_TO_SUBSCRIBE = [COMMANDS_TOPIC, CONFIGURATIONS_TOPIC, METADATA_TOPIC, TOPOLOGIES_TOPIC, HOST_LEVEL_PARAMS_TOPIC, ALERTS_DEFINITIONS_TOPIC, AGENT_ACTIONS_TOPIC]\n \n TOPOLOGY_REQUEST_ENDPOINT = '/agents/topologies'\n METADATA_REQUEST_ENDPOINT = '/agents/metadata'",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/Constants.py",
                "sha": "93c2a4920a72d03b4c36272f4fdf6185e877f0a9",
                "status": "modified"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/CustomServiceOrchestrator.py",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/CustomServiceOrchestrator.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 5,
                "filename": "ambari-agent/src/main/python/ambari_agent/CustomServiceOrchestrator.py",
                "patch": "@@ -24,6 +24,7 @@\n import sys\n from ambari_commons import shell\n import threading\n+from collections import defaultdict\n \n from AgentException import AgentException\n from PythonExecutor import PythonExecutor\n@@ -32,8 +33,6 @@\n from ambari_commons import subprocess32\n from ambari_agent.Utils import Utils\n from ambari_commons.constants import AGENT_TMP_DIR\n-import hostname\n-import Constants\n \n \n logger = logging.getLogger()\n@@ -108,6 +107,9 @@ def __init__(self, initializer_module):\n     self.commands_in_progress_lock = threading.RLock()\n     self.commands_in_progress = {}\n \n+    # save count (not boolean) for parallel execution cases\n+    self.commands_for_component_in_progress = defaultdict(lambda:defaultdict(lambda:0))\n+\n   def map_task_to_process(self, task_id, processId):\n     with self.commands_in_progress_lock:\n       logger.debug('Maps taskId=%s to pid=%s', task_id, processId)\n@@ -148,6 +150,9 @@ def getProviderDirectory(self, service_name):\n     conf_dir = os.path.join(self.credential_conf_dir, service_name.lower())\n     return conf_dir\n \n+  def commandsRunningForComponent(self, clusterId, componentName):\n+    return self.commands_for_component_in_progress[clusterId][componentName] > 0\n+\n   def getConfigTypeCredentials(self, commandJson):\n     \"\"\"\n     Gets the affected config types for the service in this command\n@@ -241,7 +246,7 @@ def getConfigTypeCredentials(self, commandJson):\n             config.pop(value_name, None)\n     return configtype_credentials\n \n-  def qJceks(self, commandJson):\n+  def generateJceks(self, commandJson):\n     \"\"\"\n     Generates the JCEKS file with passwords for the service specified in commandJson\n \n@@ -308,11 +313,14 @@ def runCommand(self, command_header, tmpoutfile, tmperrfile, forced_command_name\n     forced_command_name may be specified manually. In this case, value, defined at\n     command json, is ignored.\n     \"\"\"\n+    incremented_commands_for_component = False\n+\n     try:\n       command = self.generate_command(command_header)\n       script_type = command['commandParams']['script_type']\n       script = command['commandParams']['script']\n       timeout = int(command['commandParams']['command_timeout'])\n+      cluster_id = str(command['clusterId'])\n \n       server_url_prefix = command['ambariLevelParams']['jdk_location']\n \n@@ -363,9 +371,9 @@ def runCommand(self, command_header, tmpoutfile, tmperrfile, forced_command_name\n       # generate the JCEKS file for the configurations.\n       credentialStoreEnabled = False\n       if 'serviceLevelParams' in command and 'credentialStoreEnabled' in command['serviceLevelParams']:\n-        credentialStoreEnabled = (command['serviceLevelParams']['credentialStoreEnabled'] == \"true\")\n+        credentialStoreEnabled = command['serviceLevelParams']['credentialStoreEnabled']\n \n-      if credentialStoreEnabled == True:\n+      if credentialStoreEnabled and command_name != self.COMMAND_NAME_STATUS:\n         if 'commandBeingRetried' not in command['agentLevelParams'] or command['agentLevelParams']['commandBeingRetried'] != \"true\":\n           self.generateJceks(command)\n         else:\n@@ -393,6 +401,10 @@ def runCommand(self, command_header, tmpoutfile, tmperrfile, forced_command_name\n       backup_log_files = not command_name in self.DONT_BACKUP_LOGS_FOR_COMMANDS\n       log_out_files = self.config.get(\"logging\",\"log_out_files\", default=\"0\") != \"0\"\n \n+      if cluster_id != '-1' and cluster_id != 'null':\n+        self.commands_for_component_in_progress[cluster_id][command['role']] += 1\n+        incremented_commands_for_component = True\n+\n       for py_file, current_base_dir in filtered_py_file_list:\n         log_info_on_failure = not command_name in self.DONT_DEBUG_FAILURES_FOR_COMMANDS\n         script_params = [command_name, json_path, current_base_dir, tmpstrucoutfile, logger_level, self.exec_tmp_dir,\n@@ -437,6 +449,10 @@ def runCommand(self, command_header, tmpoutfile, tmperrfile, forced_command_name\n         'structuredOut' : '{}',\n         'exitcode': 1,\n       }\n+    finally:\n+      if incremented_commands_for_component:\n+        self.commands_for_component_in_progress[cluster_id][command['role']] -= 1\n+\n     return ret\n \n   def command_canceled_reason(self, task_id):",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/CustomServiceOrchestrator.py",
                "sha": "604d01cb896ef136786698a681c664346da50f35",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/ExitHelper.py",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/ExitHelper.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 3,
                "filename": "ambari-agent/src/main/python/ambari_agent/ExitHelper.py",
                "patch": "@@ -48,6 +48,7 @@ class ExitHelper(object):\n   def __init__(self):\n     self.exit_functions = []\n     self.exit_functions_executed = False\n+    self.exitcode = 0\n     atexit.register(self.execute_cleanup)\n \n   def execute_cleanup(self):\n@@ -65,10 +66,10 @@ def execute_cleanup(self):\n   def register(self, func, *args, **kwargs):\n     self.exit_functions.append((func, args, kwargs))\n \n-  def exit(self, code):\n+  def exit(self):\n     self.execute_cleanup()\n-    logger.info(\"Cleanup finished, exiting with code:\" + str(code))\n-    os._exit(code)\n+    logger.info(\"Cleanup finished, exiting with code:\" + str(self.exitcode))\n+    os._exit(self.exitcode)\n \n \n if __name__ == '__main__':",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/ExitHelper.py",
                "sha": "e61ccc4f9c99603eaf6bf450ad0af46804b6017b",
                "status": "modified"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/FileCache.py",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/FileCache.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 4,
                "filename": "ambari-agent/src/main/python/ambari_agent/FileCache.py",
                "patch": "@@ -25,6 +25,10 @@\n import zipfile\n import urllib2\n import urllib\n+import time\n+import threading\n+\n+from ambari_agent.Utils import execute_with_retries\n \n logger = logging.getLogger()\n \n@@ -61,6 +65,8 @@ def __init__(self, config):\n     # from the server is not possible or agent should rollback to local copy\n     self.tolerate_download_failures = \\\n           config.get('agent','tolerate_download_failures').lower() == 'true'\n+    self.currently_providing_dict_lock = threading.RLock()\n+    self.currently_providing = {}\n     self.reset()\n \n \n@@ -72,7 +78,10 @@ def get_service_base_dir(self, command, server_url_prefix):\n     \"\"\"\n     Returns a base directory for service\n     \"\"\"\n-    service_subpath = command['serviceLevelParams']['service_package_folder']\n+    if 'service_package_folder' in command['commandParams']:\n+      service_subpath = command['commandParams']['service_package_folder']\n+    else:\n+      service_subpath = command['serviceLevelParams']['service_package_folder']\n     return self.provide_directory(self.cache_dir, service_subpath,\n                                   server_url_prefix)\n \n@@ -139,14 +148,24 @@ def provide_directory(self, cache_path, subdirectory, server_url_prefix):\n       subdirectory: subpath inside cache\n       server_url_prefix: url of \"resources\" folder at the server\n     \"\"\"\n-\n     full_path = os.path.join(cache_path, subdirectory)\n     logger.debug(\"Trying to provide directory {0}\".format(subdirectory))\n \n     if not self.auto_cache_update_enabled():\n       logger.debug(\"Auto cache update is disabled.\")\n       return full_path\n \n+    wait_for_another_execution_event = None\n+    with self.currently_providing_dict_lock:\n+      if full_path in self.currently_providing:\n+        wait_for_another_execution_event = self.currently_providing[full_path]\n+      else:\n+        self.currently_providing[full_path] = threading.Event()\n+\n+    if wait_for_another_execution_event:\n+      wait_for_another_execution_event.wait()\n+      return full_path\n+\n     try:\n       if full_path not in self.uptodate_paths:\n         logger.debug(\"Checking if update is available for \"\n@@ -183,6 +202,10 @@ def provide_directory(self, cache_path, subdirectory, server_url_prefix):\n                     \"Error details: {0}\".format(str(e)))\n       else:\n         raise # we are not tolerant to exceptions, command execution will fail\n+    finally:\n+      self.currently_providing[full_path].set()\n+      del self.currently_providing[full_path]\n+\n     return full_path\n \n \n@@ -245,7 +268,7 @@ def write_hash_sum(self, directory, new_hash):\n     try:\n       with open(hash_file, \"w\") as fh:\n         fh.write(new_hash)\n-      os.chmod(hash_file, 0o666)\n+      os.chmod(hash_file, 0o644)\n     except Exception, err:\n       raise CachingException(\"Can not write to file {0} : {1}\".format(hash_file,\n                                                                  str(err)))\n@@ -257,16 +280,25 @@ def invalidate_directory(self, directory):\n     directory and any parent directories if needed. May throw exceptions\n     on permission problems\n     \"\"\"\n+    CLEAN_DIRECTORY_TRIES = 5\n+    CLEAN_DIRECTORY_TRY_SLEEP = 0.25\n+\n     logger.debug(\"Invalidating directory {0}\".format(directory))\n     try:\n       if os.path.exists(directory):\n         if os.path.isfile(directory): # It would be a strange situation\n           os.unlink(directory)\n         elif os.path.isdir(directory):\n-          shutil.rmtree(directory)\n+          \"\"\"\n+          Execute shutil.rmtree(directory) multiple times.\n+          Reason: race condition, where a file (e.g. *.pyc) in deleted directory\n+          is created during function is running, causing it to fail.\n+          \"\"\"\n+          execute_with_retries(CLEAN_DIRECTORY_TRIES, CLEAN_DIRECTORY_TRY_SLEEP, OSError, shutil.rmtree, directory)\n         # create directory itself and any parent directories\n       os.makedirs(directory)\n     except Exception, err:\n+      logger.exception(\"Can not invalidate cache directory {0}\".format(directory))\n       raise CachingException(\"Can not invalidate cache directory {0}: {1}\",\n                              directory, str(err))\n ",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/FileCache.py",
                "sha": "780a12b2f48585090a84ca104cfd225ba655fa4c",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/HeartbeatThread.py",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/HeartbeatThread.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 10,
                "filename": "ambari-agent/src/main/python/ambari_agent/HeartbeatThread.py",
                "patch": "@@ -30,6 +30,7 @@\n from ambari_agent.listeners.ServerResponsesListener import ServerResponsesListener\n from ambari_agent.listeners.TopologyEventListener import TopologyEventListener\n from ambari_agent.listeners.ConfigurationEventListener import ConfigurationEventListener\n+from ambari_agent.listeners.AgentActionsListener import AgentActionsListener\n from ambari_agent.listeners.MetadataEventListener import MetadataEventListener\n from ambari_agent.listeners.CommandsEventListener import CommandsEventListener\n from ambari_agent.listeners.HostLevelParamsEventListener import HostLevelParamsEventListener\n@@ -64,7 +65,8 @@ def __init__(self, initializer_module):\n     self.configuration_events_listener = ConfigurationEventListener(initializer_module.configurations_cache)\n     self.host_level_params_events_listener = HostLevelParamsEventListener(initializer_module.host_level_params_cache, initializer_module.recovery_manager)\n     self.alert_definitions_events_listener = AlertDefinitionsEventListener(initializer_module.alert_definitions_cache, initializer_module.alert_scheduler_handler)\n-    self.listeners = [self.server_responses_listener, self.commands_events_listener, self.metadata_events_listener, self.topology_events_listener, self.configuration_events_listener, self.host_level_params_events_listener, self.alert_definitions_events_listener]\n+    self.agent_actions_events_listener = AgentActionsListener(initializer_module)\n+    self.listeners = [self.server_responses_listener, self.commands_events_listener, self.metadata_events_listener, self.topology_events_listener, self.configuration_events_listener, self.host_level_params_events_listener, self.alert_definitions_events_listener, self.agent_actions_events_listener]\n \n     self.post_registration_requests = [\n     (Constants.TOPOLOGY_REQUEST_ENDPOINT, initializer_module.topology_cache, self.topology_events_listener),\n@@ -93,7 +95,11 @@ def run(self):\n         logger.debug(\"Heartbeat response is {0}\".format(response))\n         self.handle_heartbeat_reponse(response)\n       except Exception as ex:\n-        if not isinstance(ex, (socket_error, ConnectionIsAlreadyClosed)):\n+        if isinstance(ex, (ConnectionIsAlreadyClosed)):\n+          logger.info(\"Connection was closed. Re-running the registration\")\n+        elif isinstance(ex, (socket_error)):\n+          logger.info(\"Connection error \\\"{0}\\\". Re-running the registration\".format(str(ex)))\n+        else:\n           logger.exception(\"Exception in HeartbeatThread. Re-running the registration\")\n \n         self.unregister()\n@@ -125,7 +131,7 @@ def register(self):\n \n     for endpoint, cache, listener in self.post_registration_requests:\n       # should not hang forever on these requests\n-      response = self.blocking_request({'hash': cache.hash}, endpoint)\n+      response = self.blocking_request({'hash': cache.hash}, endpoint, log_handler=listener.get_log_message)\n       try:\n         listener.on_event({}, response)\n       except:\n@@ -185,10 +191,6 @@ def handle_heartbeat_reponse(self, response):\n     else:\n       self.responseId = serverId\n \n-    if 'restartAgent' in response and response['restartAgent'].lower() == \"true\":\n-      logger.warn(\"Restarting the agent by the request from server\")\n-      Utils.restartAgent(self.stop_event)\n-\n   def get_heartbeat_body(self):\n     \"\"\"\n     Heartbeat body to be send to server\n@@ -213,7 +215,8 @@ def establish_connection(self):\n     Create a stomp connection\n     \"\"\"\n     connection_url = 'wss://{0}:{1}/agent/stomp/v1'.format(self.config.server_hostname, self.config.secured_url_port)\n-    self.connection = security.establish_connection(connection_url)\n+    connection_helper = security.VerifiedHTTPSConnection(self.config.server_hostname, connection_url, self.config)\n+    self.connection = connection_helper.connect()\n \n   def add_listeners(self):\n     \"\"\"\n@@ -226,12 +229,16 @@ def subscribe_to_topics(self, topics_list):\n     for topic_name in topics_list:\n       self.connection.subscribe(destination=topic_name, id='sub', ack='client-individual')\n \n-  def blocking_request(self, message, destination, timeout=REQUEST_RESPONSE_TIMEOUT):\n+  def blocking_request(self, message, destination, log_handler=None, timeout=REQUEST_RESPONSE_TIMEOUT):\n     \"\"\"\n     Send a request to server and waits for the response from it. The response it detected by the correspondence of correlation_id.\n     \"\"\"\n+    def presend_hook(correlation_id):\n+      if log_handler:\n+        self.server_responses_listener.logging_handlers[str(correlation_id)] = log_handler \n+           \n     try:\n-      correlation_id = self.connection.send(message=message, destination=destination)\n+      correlation_id = self.connection.send(message=message, destination=destination, presend_hook=presend_hook)\n     except ConnectionIsAlreadyClosed:\n       # this happens when trying to connect to broken connection. Happens if ambari-server is restarted.\n       logger.warn(\"Connection failed while trying to connect to {0}\".format(destination))",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/HeartbeatThread.py",
                "sha": "6a7fbb7637bacd7b89edf058ecfdc4c7779dc1e7",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/HostCleanup.py",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/HostCleanup.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 2,
                "filename": "ambari-agent/src/main/python/ambari_agent/HostCleanup.py",
                "patch": "@@ -511,8 +511,13 @@ def do_delete_by_owner(self, userIds, folders):\n     for folder in folders:\n       for filename in os.listdir(folder):\n         fileToCheck = os.path.join(folder, filename)\n-        stat = os.stat(fileToCheck)\n-        if stat.st_uid in userIds:\n+        try:\n+          stat = os.stat(fileToCheck)\n+        except OSError:\n+          stat = None\n+          logger.warn(\"Cannot stat file, skipping: \" + fileToCheck)\n+\n+        if stat and stat.st_uid in userIds:\n           self.do_erase_dir_silent([fileToCheck])\n           logger.info(\"Deleting file/folder: \" + fileToCheck)\n ",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/HostCleanup.py",
                "sha": "c6f5bffbd83a18f41f269b8c92f39b0f4ec002c2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/RecoveryManager.py",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/RecoveryManager.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-agent/src/main/python/ambari_agent/RecoveryManager.py",
                "patch": "@@ -683,7 +683,7 @@ def process_execution_command_result(self, command, status):\n       return\n \n     if status == ActionQueue.COMPLETED_STATUS:\n-      if command[self.ROLE_COMMAND] == self.ROLE_COMMAND_START:\n+      if command[self.ROLE_COMMAND] == ActionQueue.ROLE_COMMAND_START:\n         self.update_current_status(command[self.ROLE], LiveStatus.LIVE_STATUS)\n         #self.update_config_staleness(command['role'], False)\n         logger.info(\"After EXECUTION_COMMAND (START), with taskId=\" + str(command['taskId']) +",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/RecoveryManager.py",
                "sha": "3a20b20107811182e498f88234bfff5344e497e6",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/Register.py",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/Register.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-agent/src/main/python/ambari_agent/Register.py",
                "patch": "@@ -30,6 +30,7 @@ class Register:\n   def __init__(self, config):\n     self.config = config\n     self.hardware = Hardware(self.config)\n+    self.init_time_ms = int(1000*time.time())\n \n   def build(self, response_id='-1'):\n     timestamp = int(time.time()*1000)\n@@ -48,6 +49,7 @@ def build(self, response_id='-1'):\n                  'hardwareProfile'   : self.hardware.get(),\n                  'agentEnv'          : agentEnv,\n                  'agentVersion'      : Utils.read_agent_version(self.config),\n-                 'prefix'            : self.config.get('agent', 'prefix')\n+                 'prefix'            : self.config.get('agent', 'prefix'),\n+                 'agentStartTime'    : self.init_time_ms\n                }\n     return register",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/Register.py",
                "sha": "cf63b5d6d135d4320c61ec60a45c76ebfe78930c",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/Utils.py",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/Utils.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 3,
                "filename": "ambari-agent/src/main/python/ambari_agent/Utils.py",
                "patch": "@@ -18,6 +18,7 @@\n limitations under the License.\n \"\"\"\n import os\n+import time\n import threading\n import collections\n from functools import wraps\n@@ -154,8 +155,7 @@ def read_agent_version(config):\n \n   @staticmethod\n   def restartAgent(stop_event, graceful_stop_timeout=30):\n-    from ambari_agent import main\n-    main.EXIT_CODE_ON_STOP = AGENT_AUTO_RESTART_EXIT_CODE\n+    ExitHelper().exitcode = AGENT_AUTO_RESTART_EXIT_CODE\n     stop_event.set()\n \n     t = threading.Timer( graceful_stop_timeout, ExitHelper().exit, [AGENT_AUTO_RESTART_EXIT_CODE])\n@@ -214,4 +214,14 @@ def decorated(self):\n       setattr(self, name, v)\n       return v\n \n-  return decorated\n\\ No newline at end of file\n+  return decorated\n+\n+def execute_with_retries(tries, try_sleep, retry_exception_class, func, *args, **kwargs):\n+  for i in range(tries):\n+    try:\n+      func(*args, **kwargs)\n+      break\n+    except retry_exception_class:\n+      if i==tries-1:\n+        raise\n+      time.sleep(try_sleep)\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/Utils.py",
                "sha": "774e957de326300c35f8c7036552c749f5d946fe",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/alerts/ams_alert.py",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/alerts/ams_alert.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-agent/src/main/python/ambari_agent/alerts/ams_alert.py",
                "patch": "@@ -31,7 +31,7 @@\n from resource_management.libraries.functions.get_port_from_url import get_port_from_url\n from ambari_commons import inet_utils\n \n-logger = logging.getLogger()\n+logger = logging.getLogger(__name__)\n \n AMS_METRICS_GET_URL = \"/ws/v1/timeline/metrics?%s\"\n ",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/alerts/ams_alert.py",
                "sha": "4f5914318908896792c3cab08c7e174f1a1e5c08",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/alerts/script_alert.py",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/alerts/script_alert.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-agent/src/main/python/ambari_agent/alerts/script_alert.py",
                "patch": "@@ -115,7 +115,7 @@ def _collect(self):\n       matchObj = re.match( r'((.*)services(.*)package)', self.path_to_script)\n       if matchObj:\n         basedir = matchObj.group(1)\n-        with Environment(basedir, tmp_dir=AGENT_TMP_DIR, logger=logging.getLogger('ambari_alerts')) as env:\n+        with Environment(basedir, tmp_dir=AGENT_TMP_DIR, logger=logging.getLogger('alerts')) as env:\n           result = cmd_module.execute(configurations, self.parameters, self.host_name)\n       else:\n         result = cmd_module.execute(configurations, self.parameters, self.host_name)",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/alerts/script_alert.py",
                "sha": "073d6c0d7c7fa2c74edaf57edf818ea08b6eee42",
                "status": "modified"
            },
            {
                "additions": 66,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/listeners/AgentActionsListener.py",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/listeners/AgentActionsListener.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-agent/src/main/python/ambari_agent/listeners/AgentActionsListener.py",
                "patch": "@@ -0,0 +1,66 @@\n+#!/usr/bin/env python\n+\n+'''\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+'''\n+\n+import logging\n+import ambari_stomp\n+\n+from ambari_agent.listeners import EventListener\n+from ambari_agent.Utils import Utils\n+from ambari_agent import Constants\n+\n+logger = logging.getLogger(__name__)\n+\n+class AgentActionsListener(EventListener):\n+  \"\"\"\n+  Listener of Constants.AGENT_ACTIONS_TOPIC events from server.\n+  \"\"\"\n+  ACTION_NAME = 'actionName'\n+  RESTART_AGENT_ACTION = 'RESTART_AGENT'\n+  \n+  def __init__(self, initializer_module):\n+    self.initializer_module = initializer_module\n+    self.stop_event = initializer_module.stop_event\n+\n+  def on_event(self, headers, message):\n+    \"\"\"\n+    Is triggered when an event to Constants.AGENT_ACTIONS_TOPIC topic is received from server.\n+    It contains some small actions which server can ask agent to do.\n+\n+    For bigger actions containing a lot of info and special workflow and a new topic would be\n+    required. Small actions like restart_agent/clean_cache make sense to be in a general event\n+\n+    @param headers: headers dictionary\n+    @param message: message payload dictionary\n+    \"\"\"\n+    action_name = message[self.ACTION_NAME]\n+\n+    if action_name == self.RESTART_AGENT_ACTION:\n+      self.restart_agent()\n+    else:\n+      logger.warn(\"Unknown action '{0}' requested by server. Ignoring it\".format(action_name))\n+\n+  def restart_agent(self):\n+    logger.warn(\"Restarting the agent by the request from server\")\n+    Utils.restartAgent(self.stop_event)\n+\n+  def get_handled_path(self):\n+    return Constants.AGENT_ACTIONS_TOPIC\n+\n+",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/listeners/AgentActionsListener.py",
                "sha": "07cf8ed8133cfaca0dde60b1e1b8c39834f899f1",
                "status": "added"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/listeners/AlertDefinitionsEventListener.py",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/listeners/AlertDefinitionsEventListener.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-agent/src/main/python/ambari_agent/listeners/AlertDefinitionsEventListener.py",
                "patch": "@@ -60,3 +60,18 @@ def on_event(self, headers, message):\n \n   def get_handled_path(self):\n     return Constants.ALERTS_DEFINITIONS_TOPIC\n+    \n+  def get_log_message(self, headers, message_json):\n+    \"\"\"\n+    This string will be used to log received messsage of this type.\n+    Usually should be used if full dict is too big for logs and should shortened or made more readable\n+    \"\"\"\n+    try:\n+      for cluster_id in message_json['clusters']:\n+        for alert_definition in message_json['clusters'][cluster_id]['alertDefinitions']:\n+          if 'source' in alert_definition:\n+            alert_definition['source'] = '...'\n+    except KeyError:\n+      pass\n+      \n+    return super(AlertDefinitionsEventListener, self).get_log_message(headers, message_json)",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/listeners/AlertDefinitionsEventListener.py",
                "sha": "4ef998241b9a6c415d84e2327b483122a9663dec",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/listeners/CommandsEventListener.py",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/listeners/CommandsEventListener.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-agent/src/main/python/ambari_agent/listeners/CommandsEventListener.py",
                "patch": "@@ -59,4 +59,21 @@ def on_event(self, headers, message):\n       self.action_queue.put(commands)\n \n   def get_handled_path(self):\n-    return Constants.COMMANDS_TOPIC\n\\ No newline at end of file\n+    return Constants.COMMANDS_TOPIC\n+    \n+  def get_log_message(self, headers, message_json):\n+    \"\"\"\n+    This string will be used to log received messsage of this type.\n+    Usually should be used if full dict is too big for logs and should shortened or made more readable\n+    \"\"\"\n+    try:\n+      for cluster_id in message_json['clusters']:\n+        for command in message_json['clusters'][cluster_id]['commands']:\n+          if 'repositoryFile' in command:\n+            command['repositoryFile'] = '...'\n+          if 'commandParams' in command:\n+            command['commandParams'] = '...'\n+    except KeyError:\n+      pass\n+      \n+    return super(CommandsEventListener, self).get_log_message(headers, message_json)\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/listeners/CommandsEventListener.py",
                "sha": "49ba20fd3b87f9a1bf78d7a6ae709f787e8c312d",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/listeners/ConfigurationEventListener.py",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/listeners/ConfigurationEventListener.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-agent/src/main/python/ambari_agent/listeners/ConfigurationEventListener.py",
                "patch": "@@ -49,4 +49,18 @@ def on_event(self, headers, message):\n     self.configuration_cache.rewrite_cache(message['clusters'], message['hash'])\n \n   def get_handled_path(self):\n-    return Constants.CONFIGURATIONS_TOPIC\n\\ No newline at end of file\n+    return Constants.CONFIGURATIONS_TOPIC\n+    \n+  def get_log_message(self, headers, message_json):\n+    \"\"\"\n+    This string will be used to log received messsage of this type.\n+    Usually should be used if full dict is too big for logs and should shortened shortened or made more readable\n+    \"\"\"\n+    try:\n+      for cluster_id in message_json['clusters']:\n+        for config_type in message_json['clusters'][cluster_id]['configurations']:\n+          message_json['clusters'][cluster_id]['configurations'][config_type] = '...'\n+    except KeyError:\n+      pass\n+      \n+    return super(ConfigurationEventListener, self).get_log_message(headers, message_json)\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/listeners/ConfigurationEventListener.py",
                "sha": "880ffd30fa9a909f8e15dfb341e036d064d17f28",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/listeners/ServerResponsesListener.py",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/listeners/ServerResponsesListener.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-agent/src/main/python/ambari_agent/listeners/ServerResponsesListener.py",
                "patch": "@@ -33,6 +33,7 @@ class ServerResponsesListener(EventListener):\n   \"\"\"\n   def __init__(self):\n     self.listener_functions = {}\n+    self.logging_handlers = {}\n     self.reset_responses()\n \n   def on_event(self, headers, message):\n@@ -63,6 +64,13 @@ def get_log_message(self, headers, message_json):\n     \"\"\"\n     if Constants.CORRELATION_ID_STRING in headers:\n       correlation_id = headers[Constants.CORRELATION_ID_STRING]\n+      \n+      if correlation_id in self.logging_handlers:\n+        message_json = self.logging_handlers[correlation_id](headers, message_json)\n+        if message_json.startswith(\" :\"):\n+          message_json = message_json[2:]\n+        del self.logging_handlers[correlation_id]\n+      \n       return \" (correlation_id={0}): {1}\".format(correlation_id, message_json)\n     return str(message_json)\n ",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/listeners/ServerResponsesListener.py",
                "sha": "bb0a746873fddc0b5148ae5a77ed33f186c08ab0",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/listeners/TopologyEventListener.py",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/listeners/TopologyEventListener.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-agent/src/main/python/ambari_agent/listeners/TopologyEventListener.py",
                "patch": "@@ -56,4 +56,21 @@ def on_event(self, headers, message):\n       logger.error(\"Unknown event type '{0}' for topology event\")\n \n   def get_handled_path(self):\n-    return Constants.TOPOLOGIES_TOPIC\n\\ No newline at end of file\n+    return Constants.TOPOLOGIES_TOPIC\n+    \n+  def get_log_message(self, headers, message_json):\n+    \"\"\"\n+    This string will be used to log received messsage of this type.\n+    Usually should be used if full dict is too big for logs and should shortened or made more readable\n+    \"\"\"\n+    try:\n+      for cluster_id in message_json['clusters']:\n+        for component_info in message_json['clusters'][cluster_id]['components']:\n+          if 'componentLevelParams' in component_info:\n+            component_info['componentLevelParams'] = '...'\n+          if 'commandParams' in component_info:\n+            component_info['commandParams'] = '...'\n+    except KeyError:\n+      pass\n+      \n+    return super(TopologyEventListener, self).get_log_message(headers, message_json)\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/listeners/TopologyEventListener.py",
                "sha": "df4ea03f19f5c6f4964f1d8742d185b69730c3b3",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/listeners/__init__.py",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/listeners/__init__.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-agent/src/main/python/ambari_agent/listeners/__init__.py",
                "patch": "@@ -20,6 +20,7 @@\n import ambari_simplejson as json\n import ambari_stomp\n import logging\n+import copy\n \n logger = logging.getLogger(__name__)\n \n@@ -44,7 +45,8 @@ def on_message(self, headers, message):\n       except ValueError:\n         logger.exception(\"Received from server event is not a valid message json. Message is:\\n{0}\".format(message))\n         return\n-      logger.info(\"Event from server at {0}{1}\".format(destination, self.get_log_message(headers, message_json)))\n+\n+      logger.info(\"Event from server at {0}{1}\".format(destination, self.get_log_message(headers, copy.deepcopy(message_json))))\n       try:\n         self.on_event(headers, message_json)\n       except:",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/listeners/__init__.py",
                "sha": "1134cb9daf4914b284f28d442ce28a7ad73dd34a",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/main.py",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/main.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 3,
                "filename": "ambari-agent/src/main/python/ambari_agent/main.py",
                "patch": "@@ -103,6 +103,8 @@ def mp_locked_init(self, *a, **kw):\n from HeartbeatHandlers import bind_signal_handlers\n from ambari_commons.constants import AMBARI_SUDO_BINARY\n from resource_management.core.logger import Logger\n+#from resource_management.core.resources.system import File\n+#from resource_management.core.environment import Environment\n \n from ambari_agent import HeartbeatThread\n from ambari_agent.InitializerModule import InitializerModule\n@@ -140,8 +142,6 @@ def mp_locked_init(self, *a, **kw):\n \n _file_logging_handlers ={}\n \n-EXIT_CODE_ON_STOP = 0\n-\n def setup_logging(logger, filename, logging_level):\n   logger.propagate = False\n   formatter = logging.Formatter(formatstr)\n@@ -416,6 +416,8 @@ def main(initializer_module, heartbeat_stop_callback=None):\n   setup_logging(apscheduler_logger, AmbariConfig.AmbariConfig.getAlertsLogFile(), logging_level)\n   setup_logging(apscheduler_logger_global, AmbariConfig.AmbariConfig.getAlertsLogFile(), logging_level)\n   Logger.initialize_logger('resource_management', logging_level=logging_level)\n+  #with Environment() as env:\n+  #  File(\"/abc\")\n \n   # init data, once loggers are setup to see exceptions/errors of initialization.\n   initializer_module.init()\n@@ -514,7 +516,7 @@ def main(initializer_module, heartbeat_stop_callback=None):\n       # Clean up if not Windows OS\n       #\n       if connected or stopped:\n-        ExitHelper().exit(EXIT_CODE_ON_STOP)\n+        ExitHelper().exit()\n         logger.info(\"finished\")\n         break\n     pass # for server_hostname in server_hostnames",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/main.py",
                "sha": "6280a4ce47a2e273dad00852be429d6f309f3f7f",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/security.py",
                "changes": 103,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/security.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 62,
                "filename": "ambari-agent/src/main/python/ambari_agent/security.py",
                "patch": "@@ -20,6 +20,7 @@\n import httplib\n import urllib2\n import socket\n+import copy\n import ssl\n import os\n import logging\n@@ -42,12 +43,12 @@\n KEY_FILENAME = '%(hostname)s.key'\n \n \n-class VerifiedHTTPSConnection(httplib.HTTPSConnection):\n+class VerifiedHTTPSConnection:\n   \"\"\" Connecting using ssl wrapped sockets \"\"\"\n-  def __init__(self, host, port=None, config=None):\n-    httplib.HTTPSConnection.__init__(self, host, port=port)\n+  def __init__(self, host, connection_url, config):\n     self.two_way_ssl_required = False\n     self.host = host\n+    self.connection_url = connection_url\n     self.config = config\n \n   def connect(self):\n@@ -57,86 +58,64 @@ def connect(self):\n       logger.info(\n         'Server require two-way SSL authentication. Use it instead of one-way...')\n \n+    logging.info(\"Connecting to {0}\".format(self.connection_url))\n+\n+\n     if not self.two_way_ssl_required:\n-      sock = self.create_connection()\n-      self.sock = ssl.wrap_socket(sock, cert_reqs=ssl.CERT_NONE)\n-      logger.info('SSL connection established. Two-way SSL authentication is '\n-                  'turned off on the server.')\n+      conn = AmbariStompConnection(self.connection_url)\n     else:\n       self.certMan = CertificateManager(self.config, self.host)\n       self.certMan.initSecurity()\n       agent_key = self.certMan.getAgentKeyName()\n       agent_crt = self.certMan.getAgentCrtName()\n       server_crt = self.certMan.getSrvrCrtName()\n \n-      sock = self.create_connection()\n+      ssl_options = {\n+        'keyfile': agent_key,\n+        'certfile': agent_crt,\n+        'cert_reqs': ssl.CERT_REQUIRED,\n+        'ca_certs': server_crt\n+      }\n \n-      try:\n-        self.sock = ssl.wrap_socket(sock,\n-                                    keyfile=agent_key,\n-                                    certfile=agent_crt,\n-                                    cert_reqs=ssl.CERT_REQUIRED,\n-                                    ca_certs=server_crt)\n-        logger.info('SSL connection established. Two-way SSL authentication '\n-                    'completed successfully.')\n-      except ssl.SSLError as err:\n-        logger.error('Two-way SSL authentication failed. Ensure that '\n-                     'server and agent certificates were signed by the same CA '\n-                     'and restart the agent. '\n-                     '\\nIn order to receive a new agent certificate, remove '\n-                     'existing certificate file from keys directory. As a '\n-                     'workaround you can turn off two-way SSL authentication in '\n-                     'server configuration(ambari.properties) '\n-                     '\\nExiting..')\n-        raise err\n-\n-  def create_connection(self):\n-    if self.sock:\n-      self.sock.close()\n-    logger.info(\"SSL Connect being called.. connecting to the server\")\n-    sock = socket.create_connection((self.host, self.port), 60)\n-    sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)\n-    if self._tunnel_host:\n-      self.sock = sock\n-      self._tunnel()\n-\n-    return sock\n-\n-def establish_connection(connection_url):\n-  \"\"\"\n-  Create a stomp connection\n-  \"\"\"\n-  logging.info(\"Connecting to {0}\".format(connection_url))\n-\n-  conn = AmbariStompConnection(connection_url)\n-  try:\n-    conn.start()\n-    conn.connect(wait=True)\n-  except Exception as ex:\n-    try:\n-      conn.disconnect()\n-    except:\n-      logger.exception(\"Exception during conn.disconnect()\")\n+      conn = AmbariStompConnection(self.connection_url, ssl_options=ssl_options)\n \n-    if isinstance(ex, socket_error):\n-      logger.warn(\"Could not connect to {0}\".format(connection_url))\n+    self.establish_connection(conn)\n+    return conn\n+\n+  def establish_connection(self, conn):\n+    \"\"\"\n+    Create a stomp connection\n+    \"\"\"\n+    try:\n+      conn.start()\n+      conn.connect(wait=True)\n+    except Exception as ex:\n+      try:\n+        conn.disconnect()\n+      except:\n+        logger.exception(\"Exception during conn.disconnect()\")\n \n-    raise\n+      if isinstance(ex, socket_error):\n+        logger.warn(\"Could not connect to {0}. {1}\".format(self.connection_url, str(ex)))\n \n-  return conn\n+      raise\n \n class AmbariStompConnection(WsConnection):\n-  def __init__(self, url):\n+  def __init__(self, *args, **kwargs):\n     self.lock = threading.RLock()\n     self.correlation_id = -1\n-    WsConnection.__init__(self, url)\n+    WsConnection.__init__(self, *args, **kwargs)\n \n-  def send(self, destination, message, content_type=None, headers=None, **keyword_headers):\n+  def send(self, destination, message, content_type=None, headers=None, log_message_function=lambda x:x, presend_hook=None, **keyword_headers):\n     with self.lock:\n       self.correlation_id += 1\n       correlation_id = self.correlation_id\n+      \n+    if presend_hook:\n+      presend_hook(self.correlation_id)\n \n-    logger.info(\"Event to server at {0} (correlation_id={1}): {2}\".format(destination, correlation_id, message))\n+    logged_message = log_message_function(copy.deepcopy(message))\n+    logger.info(\"Event to server at {0} (correlation_id={1}): {2}\".format(destination, correlation_id, logged_message))\n \n     body = json.dumps(message)\n     WsConnection.send(self, destination, body, content_type=content_type, headers=headers, correlationId=correlation_id, **keyword_headers)",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/main/python/ambari_agent/security.py",
                "sha": "563771aa6c037f2674abd0f4f75166d914eaed8b",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/test/python/ambari_agent/TestAgentStompResponses.py",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/test/python/ambari_agent/TestAgentStompResponses.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-agent/src/test/python/ambari_agent/TestAgentStompResponses.py",
                "patch": "@@ -39,7 +39,7 @@\n \n @patch(\"socket.gethostbyname\", new=MagicMock(return_value=\"192.168.64.101\"))\n @patch(\"ambari_agent.hostname.hostname\", new=MagicMock(return_value=\"c6401.ambari.apache.org\"))\n-class TestAgentStompResponses(BaseStompServerTestCase):\n+class TestAgentStompResponses:#(BaseStompServerTestCase):\n   def setUp(self):\n     self.maxDiff = None\n     self.initializer_module = None",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/test/python/ambari_agent/TestAgentStompResponses.py",
                "sha": "8a2d18a21d48f7bd93abffa33c73795339bddbde",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/test/python/ambari_agent/TestAlertSchedulerHandler.py",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/test/python/ambari_agent/TestAlertSchedulerHandler.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 8,
                "filename": "ambari-agent/src/test/python/ambari_agent/TestAlertSchedulerHandler.py",
                "patch": "@@ -41,13 +41,6 @@ class TestAlertSchedulerHandler(TestCase):\n   def setUp(self):\n     self.config = AmbariConfig()\n \n-  def test_load_definitions(self):\n-    scheduler = AlertSchedulerHandler(TEST_PATH, TEST_PATH, TEST_PATH, TEST_PATH, TEST_PATH, None)\n-\n-    definitions = scheduler._AlertSchedulerHandler__load_definitions()\n-\n-    self.assertEquals(len(definitions), 1)\n-\n   @patch(\"ambari_commons.network.reconfigure_urllib2_opener\")\n   def test_job_context_injector(self, reconfigure_urllib2_opener_mock):\n     self.config.use_system_proxy_setting = lambda: False\n@@ -302,7 +295,8 @@ def test_load_definitions(self):\n   def test_load_definitions_noFile(self):\n     initializer_module = InitializerModule()\n     initializer_module.init()\n-    \n+    initializer_module.alert_definitions_cache.rewrite_cluster_cache('0', {'alertDefinitions':[]})\n+\n     scheduler = AlertSchedulerHandler(initializer_module)\n     #('wrong_path', 'wrong_path', 'wrong_path', 'wrong_path', 'wrong_path', None, self.config, None)\n     scheduler._AlertSchedulerHandler__config_maps = {",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/test/python/ambari_agent/TestAlertSchedulerHandler.py",
                "sha": "7647070c106fdc886c2df28acfda09aa1511aa61",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/test/python/ambari_agent/TestFileCache.py",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/test/python/ambari_agent/TestFileCache.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-agent/src/test/python/ambari_agent/TestFileCache.py",
                "patch": "@@ -65,7 +65,7 @@ def test_get_service_base_dir(self, provide_directory_mock):\n     provide_directory_mock.return_value = \"dummy value\"\n     fileCache = FileCache(self.config)\n     command = {\n-      'serviceLevelParams' : {\n+      'commandParams' : {\n         'service_package_folder' : os.path.join('stacks', 'HDP', '2.1.1', 'services', 'ZOOKEEPER', 'package')\n       }\n     }",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/test/python/ambari_agent/TestFileCache.py",
                "sha": "33ddfeb56293c87b71d6d546027040496b21a997",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/test/python/ambari_agent/TestSecurity.py",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/test/python/ambari_agent/TestSecurity.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 53,
                "filename": "ambari-agent/src/test/python/ambari_agent/TestSecurity.py",
                "patch": "@@ -59,59 +59,6 @@ def setUp(self):\n   def tearDown(self):\n     # enable stdout\n     sys.stdout = sys.__stdout__\n-\n-\n-  ### VerifiedHTTPSConnection ###\n-\n-  @patch.object(security.CertificateManager, \"initSecurity\")\n-  @patch(\"socket.create_connection\")\n-  @patch(\"ssl.wrap_socket\")\n-  def test_VerifiedHTTPSConnection_connect(self, wrap_socket_mock,\n-                                           create_connection_mock,\n-                                            init_security_mock):\n-    init_security_mock.return_value = None\n-    self.config.set('security', 'keysdir', '/dummy-keysdir')\n-    connection = security.VerifiedHTTPSConnection(\"example.com\",\n-      self.config.get('server', 'secured_url_port'), self.config)\n-    connection._tunnel_host = False\n-    connection.sock = None\n-    connection.connect()\n-    self.assertTrue(wrap_socket_mock.called)\n-\n-  ### VerifiedHTTPSConnection with no certificates creation\n-  @patch.object(security.CertificateManager, \"initSecurity\")\n-  @patch(\"socket.create_connection\")\n-  @patch(\"ssl.wrap_socket\")\n-  def test_Verified_HTTPSConnection_non_secure_connect(self, wrap_socket_mock,\n-                                                    create_connection_mock,\n-                                                    init_security_mock):\n-    connection = security.VerifiedHTTPSConnection(\"example.com\",\n-      self.config.get('server', 'secured_url_port'), self.config)\n-    connection._tunnel_host = False\n-    connection.sock = None\n-    connection.connect()\n-    self.assertFalse(init_security_mock.called)\n-\n-  ### VerifiedHTTPSConnection with two-way SSL authentication enabled\n-  @patch.object(security.CertificateManager, \"initSecurity\")\n-  @patch(\"socket.create_connection\")\n-  @patch(\"ssl.wrap_socket\")\n-  def test_Verified_HTTPSConnection_two_way_ssl_connect(self, wrap_socket_mock,\n-                                                    create_connection_mock,\n-                                                    init_security_mock):\n-    wrap_socket_mock.side_effect=ssl.SSLError()\n-    connection = security.VerifiedHTTPSConnection(\"example.com\",\n-      self.config.get('server', 'secured_url_port'), self.config)\n-    self.config.isTwoWaySSLConnection = MagicMock(return_value=True)\n-\n-    connection._tunnel_host = False\n-    connection.sock = None\n-    try:\n-      connection.connect()\n-    except ssl.SSLError:\n-      pass\n-    self.assertTrue(init_security_mock.called)\n-\n   ### CachedHTTPSConnection ###\n \n   @patch.object(security.VerifiedHTTPSConnection, \"connect\")",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/test/python/ambari_agent/TestSecurity.py",
                "sha": "3387895eb85a9c9eb56c8f24ffc61ee6569e49d8",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/test/python/ambari_agent/dummy_files/alert_definitions.json",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/test/python/ambari_agent/dummy_files/alert_definitions.json?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 2,
                "filename": "ambari-agent/src/test/python/ambari_agent/dummy_files/alert_definitions.json",
                "patch": "@@ -7,9 +7,9 @@\n       {\n         \"name\": \"namenode_process\", \n         \"service\": \"HDFS\", \n-        \"component\": \"NAMENODE\", \n-        \"interval\": 6, \n         \"enabled\": true, \n+        \"interval\": 6, \n+        \"component\": \"NAMENODE\", \n         \"label\": \"NameNode process\", \n         \"source\": {\n           \"reporting\": {",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/test/python/ambari_agent/dummy_files/alert_definitions.json",
                "sha": "d9a82a7d04c1e47ecf2f937cadbb559805443e1a",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/test/python/resource_management/TestPackageResource.py",
                "changes": 172,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/test/python/resource_management/TestPackageResource.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 147,
                "filename": "ambari-agent/src/test/python/resource_management/TestPackageResource.py",
                "patch": "@@ -20,101 +20,36 @@\n import sys\n from unittest import TestCase\n from mock.mock import patch, MagicMock, call\n+from ambari_commons.os_check import OSConst\n+from ambari_commons.repo_manager import ManagerFactory\n \n from resource_management.core import Environment, Fail\n from resource_management.core.system import System\n from resource_management.core.resources import Package\n \n from resource_management.core import shell\n-from resource_management.core.providers.package.apt import replace_underscores\n+from ambari_commons import shell as ac_shell\n+from ambari_commons.repo_manager.apt_manager import replace_underscores\n \n @patch.object(os, \"geteuid\", new=MagicMock(return_value=1234))\n class TestPackageResource(TestCase):\n-  @patch.object(shell, \"call\")\n-  @patch.object(shell, \"checked_call\")\n-  @patch.object(System, \"os_family\", new = 'ubuntu')\n-  def test_action_install_ubuntu_update(self, shell_mock, call_mock):\n-    shell_mock.return_value= (0, '')\n-    call_mock.return_value= (1, '')\n-    with Environment('/') as env:\n-      Package(\"some_package\",\n-        logoutput = False\n-      )\n-    call_mock.assert_has_calls([call(\"dpkg --get-selections | grep -v deinstall | awk '{print $1}' | grep ^some-package$\")])\n-    \n-    shell_mock.assert_has_calls([call(['/usr/bin/apt-get', '-q', '-o', 'Dpkg::Options::=--force-confdef', \n-                                       '--allow-unauthenticated', '--assume-yes', 'install', 'some-package'], logoutput=False, sudo=True,  env={'DEBIAN_FRONTEND': 'noninteractive'})])\n-  \n-  @patch.object(shell, \"call\")\n-  @patch.object(shell, \"checked_call\")\n-  @patch.object(System, \"os_family\", new = 'ubuntu')\n-  def test_action_install_ubuntu(self, shell_mock, call_mock):\n-    call_mock.side_effect = [(1, ''), (0, '')]\n-    shell_mock.return_value = (0, '')\n-    with Environment('/') as env:\n-      Package(\"some_package\",\n-        logoutput = False\n-      )\n-    call_mock.assert_has_calls([call(\"dpkg --get-selections | grep -v deinstall | awk '{print $1}' | grep ^some-package$\")])\n-\n-    shell_mock.assert_has_call([call(['/usr/bin/apt-get', '-q', '-o', 'Dpkg::Options::=--force-confdef', '--allow-unauthenticated', '--assume-yes', 'install', 'some-package'], logoutput=False, sudo=True, env={'DEBIAN_FRONTEND': 'noninteractive'})])\n \n-  @patch.object(shell, \"call\")\n-  @patch.object(shell, \"checked_call\")\n-  @patch.object(System, \"os_family\", new = 'ubuntu')\n-  def test_action_install_regex_ubuntu(self, shell_mock, call_mock):\n-    call_mock.side_effect = [(0, None)]\n-    with Environment('/') as env:\n-      Package(\"some_package.*\",\n-      )\n-    call_mock.assert_has_calls([call(\"dpkg --get-selections | grep -v deinstall | awk '{print $1}' | grep ^some-package.*$\")])\n-    self.assertEqual(shell_mock.call_count, 0, \"shell.checked_call shouldn't be called\")\n-\n-  @patch.object(shell, \"call\")\n-  @patch.object(shell, \"checked_call\")\n-  @patch.object(System, \"os_family\", new = 'ubuntu')\n-  def test_action_install_regex_installed_ubuntu(self, shell_mock, call_mock):\n-    call_mock.side_effect = [(0, None),\n-                             (0, \"some-package1\\nsome-package2\"),\n-                             (0, \"Some text.\\nStatus: install ok installed\\nSome text\"),\n-                             (0, \"Some text.\\nStatus: install ok installed\\nSome text\"),\n-                             (0, None)]\n-    with Environment('/') as env:\n-      Package(\"some_package.*\",\n-              )\n-    call_mock.assert_has_calls([call(\"dpkg --get-selections | grep -v deinstall | awk '{print $1}' | grep ^some-package.*$\")])\n-    self.assertEqual(call_mock.call_count, 1, \"Package should not be installed\")\n-    self.assertEqual(shell_mock.call_count, 0, \"shell.checked_call shouldn't be called\")\n-\n-  @patch.object(shell, \"checked_call\")\n-  @patch.object(System, \"os_family\", new = 'redhat')\n-  def test_action_install_rhel(self, shell_mock):\n-    shell_mock.return_value = (0,'')\n-    sys.modules['rpm'] = MagicMock()\n-    sys.modules['rpm'].TransactionSet.return_value = MagicMock()\n-    sys.modules['rpm'].TransactionSet.return_value.dbMatch.return_value = [{'name':'some_packag'}]\n-    with Environment('/') as env:\n-      Package(\"some_package\",\n-        logoutput = False\n-      )\n-    self.assertTrue(sys.modules['rpm'].TransactionSet.return_value.dbMatch.called)\n-    shell_mock.assert_called_with(['/usr/bin/yum', '-d', '0', '-e', '0', '-y', 'install', 'some_package'], logoutput=False, sudo=True)\n-\n-  @patch.object(shell, \"checked_call\")\n-  @patch.object(System, \"os_family\", new = 'redhat')\n+  @patch.object(ac_shell, \"process_executor\")\n+  @patch.object(ManagerFactory, \"get\", new=MagicMock(return_value=ManagerFactory.get_new_instance(OSConst.REDHAT_FAMILY)))\n   def test_action_install_pattern_rhel(self, shell_mock):\n-    shell_mock.return_value = (0,'')\n+    shell_mock.return_value.__enter__.return_value = []\n     sys.modules['rpm'] = MagicMock()\n     sys.modules['rpm'].TransactionSet.return_value = MagicMock()\n     sys.modules['rpm'].TransactionSet.return_value.dbMatch.return_value = [{'name':'some_packag'}]\n     with Environment('/') as env:\n       Package(\"some_package*\",\n         logoutput = False\n       )\n-    shell_mock.assert_called_with(['/usr/bin/yum', '-d', '0', '-e', '0', '-y', 'install', 'some_package*'], logoutput=False, sudo=True)\n \n-  @patch.object(shell, \"checked_call\")\n-  @patch.object(System, \"os_family\", new = 'redhat')\n+    self.assertEquals(shell_mock.call_args_list[0][0][0],['/usr/bin/yum', '-d', '0', '-e', '0', '-y', 'install', 'some_package*'])\n+\n+  @patch.object(ac_shell, \"process_executor\")\n+  @patch.object(ManagerFactory, \"get\", new=MagicMock(return_value=ManagerFactory.get_new_instance(OSConst.REDHAT_FAMILY)))\n   def test_action_install_pattern_installed_rhel(self, shell_mock):\n     shell_mock.return_value = (0,'')\n     sys.modules['yum'] = MagicMock()\n@@ -127,33 +62,19 @@ def test_action_install_pattern_installed_rhel(self, shell_mock):\n       )\n     self.assertEqual(shell_mock.call_count, 0, \"shell.checked_call shouldn't be called\")\n \n-  @patch.object(shell, \"checked_call\")\n-  @patch.object(System, \"os_family\", new = 'suse')\n-  def test_action_install_suse(self, shell_mock):\n-    shell_mock.return_value = (0,'')\n-    sys.modules['rpm'] = MagicMock()\n-    sys.modules['rpm'].TransactionSet.return_value = MagicMock()\n-    sys.modules['rpm'].TransactionSet.return_value.dbMatch.return_value = [{'name':'some_packages'}]\n-    with Environment('/') as env:\n-      Package(\"some_package\",\n-        logoutput = False\n-      )\n-    shell_mock.assert_called_with(['/usr/bin/zypper', '--quiet', 'install', '--auto-agree-with-licenses', '--no-confirm', 'some_package'], logoutput=False, sudo=True)\n-\n-  @patch.object(shell, \"call\")\n-  @patch.object(shell, \"checked_call\")\n-  @patch.object(System, \"os_family\", new = 'suse')\n+  @patch.object(ac_shell, \"process_executor\")\n+  @patch.object(ManagerFactory, \"get\", new=MagicMock(return_value=ManagerFactory.get_new_instance(OSConst.SUSE_FAMILY)))\n   def test_action_install_pattern_suse(self, shell_mock, call_mock):\n     call_mock.side_effect=[(0, None), (0, \"Loading repository data...\\nReading installed packages...\\n\\nS | Name\\n--+-----\\n  | Pack\")]\n     with Environment('/') as env:\n       Package(\"some_package*\",\n               )\n     call_mock.assert_has_calls([call(\"installed_pkgs=`rpm -qa 'some_package*'` ; [ ! -z \\\"$installed_pkgs\\\" ]\"),\n                                 call(\"zypper --non-interactive search --type package --uninstalled-only --match-exact 'some_package*'\")])\n-    shell_mock.assert_called_with(['/usr/bin/zypper', '--quiet', 'install', '--auto-agree-with-licenses', '--no-confirm', 'some_package*'], logoutput=False, sudo=True)\n+    self.assertEquals(shell_mock.call_args_list[0][0][0],['/usr/bin/zypper', '--quiet', 'install', '--auto-agree-with-licenses', '--no-confirm', 'some_package*'])\n \n-  @patch.object(shell, \"checked_call\")\n-  @patch.object(System, \"os_family\", new = 'suse')\n+  @patch.object(ac_shell, \"process_executor\")\n+  @patch.object(ManagerFactory, \"get\", new=MagicMock(return_value=ManagerFactory.get_new_instance(OSConst.SUSE_FAMILY)))\n   def test_action_install_pattern_suse(self, shell_mock):\n     sys.modules['rpm'] = MagicMock()\n     sys.modules['rpm'].TransactionSet.return_value = MagicMock()\n@@ -163,47 +84,30 @@ def test_action_install_pattern_suse(self, shell_mock):\n               )\n     self.assertEqual(shell_mock.call_count, 0, \"shell.checked_call shouldn't be called\")\n \n-  @patch.object(shell, \"call\", new = MagicMock(return_value=(0, None)))\n-  @patch.object(shell, \"checked_call\")\n-  @patch.object(System, \"os_family\", new = 'redhat')\n+  @patch.object(ac_shell, \"process_executor\")\n+  @patch.object(ManagerFactory, \"get\", new=MagicMock(return_value=ManagerFactory.get_new_instance(OSConst.REDHAT_FAMILY)))\n   def test_action_install_existent_rhel(self, shell_mock):\n     sys.modules['rpm'] = MagicMock()\n     sys.modules['rpm'].TransactionSet.return_value = MagicMock()\n     sys.modules['rpm'].TransactionSet.return_value.dbMatch.return_value = [{'name':'some_package'}]\n     with Environment('/') as env:\n       Package(\"some_package\",\n               )\n-    self.assertFalse(shell_mock.mock_calls)\n+    self.assertFalse(shell_mock.call_count > 0)\n \n-  @patch.object(shell, \"call\", new = MagicMock(return_value=(0, None)))\n-  @patch.object(shell, \"checked_call\")\n-  @patch.object(System, \"os_family\", new = 'redhat')\n-  def test_action_install_use_repos_rhel(self, shell_mock):\n-    shell_mock.return_value = (0,'')\n-    with Environment('/') as env:\n-      Package(\"some_package\", use_repos={'HDP-UTILS-2.2.0.1-885': 'ambari-hdp-1', 'HDP-2.2.0.1-885': 'ambari-hdp-1'},\n-              logoutput = False\n-              )\n-    self.assertEquals(shell_mock.call_args[0][0],\n-                      ['/usr/bin/yum', '-d', '0', '-e', '0', '-y', 'install',\n-                       '--disablerepo=*',\n-                       '--enablerepo=HDP-2.2.0.1-885,HDP-UTILS-2.2.0.1-885', 'some_package'])\n-\n-  @patch.object(shell, \"call\", new = MagicMock(return_value=(0, None)))\n-  @patch.object(shell, \"checked_call\")\n-  @patch.object(System, \"os_family\", new = 'suse')\n+  @patch.object(ac_shell, \"process_executor\")\n+  @patch.object(ManagerFactory, \"get\", new=MagicMock(return_value=ManagerFactory.get_new_instance(OSConst.SUSE_FAMILY)))\n   def test_action_install_existent_suse(self, shell_mock):\n     sys.modules['rpm'] = MagicMock()\n     sys.modules['rpm'].TransactionSet.return_value = MagicMock()\n     sys.modules['rpm'].TransactionSet.return_value.dbMatch.return_value = [{'name':'some_package'}]\n     with Environment('/') as env:\n       Package(\"some_package\",\n               )\n-    self.assertFalse(shell_mock.mock_calls)\n+    self.assertFalse(shell_mock.call_count > 0)\n \n-  @patch.object(shell, \"call\", new = MagicMock(return_value=(0, None)))\n-  @patch.object(shell, \"checked_call\")\n-  @patch.object(System, \"os_family\", new = 'redhat')\n+  @patch.object(ac_shell, \"process_executor\")\n+  @patch.object(ManagerFactory, \"get\", new=MagicMock(return_value=ManagerFactory.get_new_instance(OSConst.REDHAT_FAMILY)))\n   def test_action_remove_rhel(self, shell_mock):\n     sys.modules['rpm'] = MagicMock()\n     sys.modules['rpm'].TransactionSet.return_value = MagicMock()\n@@ -213,34 +117,8 @@ def test_action_remove_rhel(self, shell_mock):\n               action = \"remove\",\n               logoutput = False\n       )\n-    shell_mock.assert_called_with(['/usr/bin/yum', '-d', '0', '-e', '0', '-y', 'erase', 'some_package'], logoutput=False, sudo=True)\n+    self.assertEquals(shell_mock.call_args_list[0][0][0], ['/usr/bin/yum', '-d', '0', '-e', '0', '-y', 'erase', 'some_package'])\n \n-  @patch.object(shell, \"call\", new = MagicMock(return_value=(0, None)))\n-  @patch.object(shell, \"checked_call\")\n-  @patch.object(System, \"os_family\", new = 'suse')\n-  def test_action_remove_suse(self, shell_mock):\n-    shell_mock.return_value = (0, '')\n-    sys.modules['rpm'] = MagicMock()\n-    sys.modules['rpm'].TransactionSet.return_value = MagicMock()\n-    sys.modules['rpm'].TransactionSet.return_value.dbMatch.return_value = [{'name':'some_package'}]\n-    with Environment('/') as env:\n-      Package(\"some_package\",\n-              action = \"remove\",\n-              logoutput = False\n-      )\n-    shell_mock.assert_called_with(['/usr/bin/zypper', '--quiet', 'remove', '--no-confirm', 'some_package'], logoutput=False, sudo=True)\n-\n-  @patch.object(shell, \"call\", new = MagicMock(return_value=(1, None)))\n-  @patch.object(shell, \"checked_call\")\n-  @patch.object(System, \"os_family\", new = 'redhat')\n-  def test_action_install_version_attr(self, shell_mock):\n-    shell_mock.return_value = (0,'')\n-    with Environment('/') as env:\n-      Package(\"some_package\",\n-              version = \"3.5.0\",\n-              logoutput = False\n-      )\n-    shell_mock.assert_called_with(['/usr/bin/yum', '-d', '0', '-e', '0', '-y', 'install', 'some_package-3.5.0'], logoutput=False, sudo=True)\n \n   @replace_underscores\n   def func_to_test(self, name):",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/test/python/resource_management/TestPackageResource.py",
                "sha": "2e40052d66071d2ce8c0ab32170bb2a7cf1965f0",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/test/python/resource_management/TestScript.py",
                "changes": 63,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/test/python/resource_management/TestScript.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 63,
                "filename": "ambari-agent/src/test/python/resource_management/TestScript.py",
                "patch": "@@ -32,69 +32,6 @@ def setUp(self):\n     out = StringIO.StringIO()\n     sys.stdout = out\n \n-\n-  @patch(\"resource_management.core.providers.package.PackageProvider\")\n-  def test_install_packages(self, package_provider_mock):\n-    no_packages_config = {\n-      'commandParams': {\n-        'package_list' : ''\n-      },\n-      'ambariLevelParams' : {\n-        'repo_info' : \"[{\\\"baseUrl\\\":\\\"http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0.6.0\\\",\\\"osType\\\":\\\"centos6\\\",\\\"repoId\\\":\\\"HDP-2.0._\\\",\\\"repoName\\\":\\\"HDP\\\",\\\"defaultBaseUrl\\\":\\\"http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0.6.0\\\"}]\",\n-        'agent_stack_retry_count': '5',\n-        'agent_stack_retry_on_unavailability': 'false'\n-      }\n-    }\n-    empty_config = {\n-      'commandParams': {\n-        'package_list' : ''\n-      },\n-      'ambariLevelParams' : {\n-        'repo_info' : \"[{\\\"baseUrl\\\":\\\"http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0.6.0\\\",\\\"osType\\\":\\\"centos6\\\",\\\"repoId\\\":\\\"HDP-2.0._\\\",\\\"repoName\\\":\\\"HDP\\\",\\\"defaultBaseUrl\\\":\\\"http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0.6.0\\\"}]\",\n-        'agent_stack_retry_count': '5',\n-        'agent_stack_retry_on_unavailability': 'false'\n-      }\n-    }\n-    dummy_config = {\n-      'commandParams' : {\n-        'package_list' : \"[{\\\"type\\\":\\\"rpm\\\",\\\"name\\\":\\\"hbase\\\", \\\"condition\\\": \\\"\\\"},\"\n-                         \"{\\\"type\\\":\\\"rpm\\\",\\\"name\\\":\\\"yet-another-package\\\", \\\"condition\\\": \\\"\\\"}]\",\n-      },                 \n-      'ambariLevelParams' : {\n-        'repo_info' : \"[{\\\"baseUrl\\\":\\\"http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0.6.0\\\",\\\"osType\\\":\\\"centos6\\\",\\\"repoId\\\":\\\"HDP-2.0._\\\",\\\"repoName\\\":\\\"HDP\\\",\\\"defaultBaseUrl\\\":\\\"http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0.6.0\\\"}]\",\n-        'service_repo_info' : \"[{\\\"mirrorsList\\\":\\\"abc\\\",\\\"osType\\\":\\\"centos6\\\",\\\"repoId\\\":\\\"HDP-2.0._\\\",\\\"repoName\\\":\\\"HDP\\\",\\\"defaultBaseUrl\\\":\\\"http://public-repo-1.hortonworks.com/HDP/centos6/2.x/updates/2.0.6.0\\\"}]\",\n-        'agent_stack_retry_count': '5',\n-        'agent_stack_retry_on_unavailability': 'false'\n-      }\n-    }\n-\n-    # Testing config without any keys\n-    with Environment(\".\", test_mode=True) as env:\n-      with patch(\"resource_management.libraries.script.get_provider\", return_value=MagicMock()):\n-        script = Script()\n-        Script.config = no_packages_config\n-        script.install_packages(env)\n-    resource_dump = pprint.pformat(env.resource_list)\n-    self.assertEquals(resource_dump, \"[]\")\n-\n-    # Testing empty package list\n-    with Environment(\".\", test_mode=True) as env:\n-      with patch(\"resource_management.libraries.script.get_provider\", return_value=MagicMock()):\n-        script = Script()\n-        Script.config = empty_config\n-        script.install_packages(env)\n-    resource_dump = pprint.pformat(env.resource_list)\n-    self.assertEquals(resource_dump, \"[]\")\n-\n-    # Testing installing of a list of packages\n-    with Environment(\".\", test_mode=True) as env:\n-      with patch(\"resource_management.libraries.script.get_provider\", return_value=MagicMock()):\n-        script = Script()\n-        Script.config = dummy_config\n-        script.install_packages(\"env\")\n-    resource_dump = pprint.pformat(env.resource_list)\n-    self.assertEqual(resource_dump, '[Package[\\'hbase\\'], Package[\\'yet-another-package\\']]')\n-\n   @patch(\"__builtin__.open\")\n   def test_structured_out(self, open_mock):\n     script = Script()",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-agent/src/test/python/resource_management/TestScript.py",
                "sha": "cc5c171d9631e78ee64d2f21707ca37e6d862739",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/assemblies/client.xml",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/assemblies/client.xml?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 20,
                "filename": "ambari-client/assemblies/client.xml",
                "patch": "@@ -1,20 +0,0 @@\n-<?xml version=\"1.0\"?>\n-<!--\n-  Licensed to the Apache Software Foundation (ASF) under one\n-  or more contributor license agreements.  See the NOTICE file\n-  distributed with this work for additional information\n-  regarding copyright ownership.  The ASF licenses this file\n-  to you under the Apache License, Version 2.0 (the\n-  \"License\"); you may not use this file except in compliance\n-  with the License.  You may obtain a copy of the License at\n-  \n-       http://www.apache.org/licenses/LICENSE-2.0\n-  \n-  Unless required by applicable law or agreed to in writing, software\n-  distributed under the License is distributed on an \"AS IS\" BASIS,\n-  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-  See the License for the specific language governing permissions and\n-  limitations under the License.\n--->\n-<assembly>\n-</assembly>",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/assemblies/client.xml",
                "sha": "20670cddb10a6cad53e9fe1e22d7f20279c1c653",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/pom.xml",
                "changes": 130,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/pom.xml?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 130,
                "filename": "ambari-client/groovy-client/pom.xml",
                "patch": "@@ -1,130 +0,0 @@\n-<?xml version=\"1.0\"?>\n-<!-- Licensed under the Apache License, Version 2.0 (the \"License\"); you\n-  may not use this file except in compliance with the License. You may obtain\n-  a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless\n-  required by applicable law or agreed to in writing, software distributed\n-  under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES\n-  OR CONDITIONS OF ANY KIND, either express or implied. See the License for\n-  the specific language governing permissions and limitations under the License.\n-  See accompanying LICENSE file. -->\n-<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n-         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\">\n-  <parent>\n-    <artifactId>ambari-client</artifactId>\n-    <groupId>org.apache.ambari</groupId>\n-    <version>2.0.0.0-SNAPSHOT</version>\n-    <relativePath>../../ambari-client/</relativePath>\n-  </parent>\n-  <modelVersion>4.0.0</modelVersion>\n-  <groupId>org.apache.ambari</groupId>\n-  <artifactId>groovy-client</artifactId>\n-  <packaging>jar</packaging>\n-  <version>2.0.0.0-SNAPSHOT</version>\n-  <name>Ambari Groovy Client</name>\n-  <description>Ambari Groovy client</description>\n-  <properties>\n-    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n-  </properties>\n-  <dependencies>\n-    <dependency>\n-      <groupId>org.slf4j</groupId>\n-      <artifactId>slf4j-api</artifactId>\n-      <version>1.7.20</version>\n-    </dependency>\n-    <dependency>\n-      <groupId>org.slf4j</groupId>\n-      <artifactId>slf4j-log4j12</artifactId>\n-      <version>1.7.20</version>\n-    </dependency>\n-    <dependency>\n-      <groupId>org.codehaus.groovy</groupId>\n-      <artifactId>groovy-all</artifactId>\n-    </dependency>\n-    <dependency>\n-      <groupId>org.codehaus.groovy.modules.http-builder</groupId>\n-      <artifactId>http-builder</artifactId>\n-    </dependency>\n-    <dependency>\n-      <groupId>org.easymock</groupId>\n-      <artifactId>easymock</artifactId>\n-      <version>3.2</version>\n-      <scope>test</scope>\n-    </dependency>\n-    <dependency>\n-      <groupId>org.spockframework</groupId>\n-      <artifactId>spock-core</artifactId>\n-      <version>0.7-groovy-2.0</version>\n-    </dependency>\n-    <dependency>\n-      <groupId>commons-io</groupId>\n-      <artifactId>commons-io</artifactId>\n-    </dependency>\n-  </dependencies>\n-  <build>\n-    <plugins>\n-      <plugin>\n-        <artifactId>maven-compiler-plugin</artifactId>\n-        <!-- 2.8.0-01 and later require maven-compiler-plugin 3.1 or higher -->\n-        <version>3.1</version>\n-        <configuration>\n-          <compilerId>groovy-eclipse-compiler</compilerId>\n-        </configuration>\n-        <dependencies>\n-          <dependency>\n-            <groupId>org.codehaus.groovy</groupId>\n-            <artifactId>groovy-eclipse-compiler</artifactId>\n-            <version>2.8.0-01</version>\n-          </dependency>\n-          <!-- for 2.8.0-01 and later you must have an explicit dependency on\n-            groovy-eclipse-batch -->\n-          <dependency>\n-            <groupId>org.codehaus.groovy</groupId>\n-            <artifactId>groovy-eclipse-batch</artifactId>\n-            <version>2.1.8-01</version>\n-          </dependency>\n-        </dependencies>\n-      </plugin>\n-      <plugin>\n-        <groupId>org.apache.rat</groupId>\n-        <artifactId>apache-rat-plugin</artifactId>\n-        <configuration>\n-          <excludes>\n-            <exclude>**/*.iml</exclude>\n-            <exclude>src/main/resources/blueprints/**</exclude>\n-            <exclude>src/test/resources/**</exclude>\n-          </excludes>\n-        </configuration>\n-        <executions>\n-          <execution>\n-            <phase>test</phase>\n-            <goals>\n-              <goal>check</goal>\n-            </goals>\n-          </execution>\n-        </executions>\n-      </plugin>\n-      <plugin>\n-        <artifactId>maven-assembly-plugin</artifactId>\n-        <configuration>\n-          <skipAssembly>true</skipAssembly>\n-        </configuration>\n-      </plugin>\n-      <plugin>\n-        <groupId>org.vafer</groupId>\n-        <artifactId>jdeb</artifactId>\n-        <version>1.0.1</version>\n-        <executions>\n-          <execution>\n-            <phase>none</phase>\n-            <goals>\n-              <goal>jdeb</goal>\n-            </goals>\n-          </execution>\n-        </executions>\n-        <configuration>\n-          <controlDir>${basedir}/../../ambari-project/src/main/package/deb/control</controlDir>\n-        </configuration>\n-      </plugin>\n-    </plugins>\n-  </build>\n-</project>",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/pom.xml",
                "sha": "8fafdec2f59fc10d3b5cf3d5547825748fa683bb",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/main/groovy/org/apache/ambari/groovy/client/AmbariClient.groovy",
                "changes": 1170,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/main/groovy/org/apache/ambari/groovy/client/AmbariClient.groovy?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 1170,
                "filename": "ambari-client/groovy-client/src/main/groovy/org/apache/ambari/groovy/client/AmbariClient.groovy",
                "patch": "@@ -1,1170 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.ambari.groovy.client\n-\n-import groovy.json.JsonBuilder\n-import groovy.json.JsonSlurper\n-import groovy.util.logging.Slf4j\n-import groovyx.net.http.ContentType\n-import groovyx.net.http.HttpResponseException\n-import groovyx.net.http.RESTClient\n-import org.apache.commons.io.IOUtils\n-import org.apache.http.NoHttpResponseException\n-import org.apache.http.client.ClientProtocolException\n-import java.net.ConnectException\n-import java.net.NoRouteToHostException\n-import java.net.UnknownHostException\n-\n-/**\n- * Basic client to send requests to the Ambari server.\n- */\n-@Slf4j\n-class AmbariClient {\n-\n-  private static final int PAD = 30\n-  private static final int OK_RESPONSE = 200\n-  private static final String SLAVE = \"slave_\"\n-  boolean debugEnabled = false;\n-  def RESTClient ambari\n-  def slurper = new JsonSlurper()\n-  def clusterName\n-\n-  /**\n-   * Connects to the ambari server.\n-   *\n-   * @param host host of the Ambari server; default value is localhost\n-   * @param port port of the Ambari server; default value is 8080\n-   * @param user username of the Ambari server; default is admin\n-   * @param password password fom the Ambari server; default is admin\n-   */\n-  AmbariClient(host = 'localhost', port = '8080', user = 'admin', password = 'admin') {\n-    ambari = new RESTClient(\"http://${host}:${port}/api/v1/\" as String)\n-    ambari.headers['Authorization'] = 'Basic ' + \"$user:$password\".getBytes('iso-8859-1').encodeBase64()\n-    ambari.headers['X-Requested-By'] = 'ambari'\n-  }\n-\n-  /**\n-   * Connects to the ambari server.\n-   *\n-   * @param restClient underlying client\n-   * @param slurper slurper to parse responses\n-   */\n-  AmbariClient(RESTClient restClient, JsonSlurper slurper) {\n-    this.ambari = restClient\n-    this.slurper = slurper\n-  }\n-\n-  /**\n-   * Sets the debug variable. Used by printing the API calls for the Ambari Shell.\n-   *\n-   * @param enabled enable or disable\n-   */\n-  def setDebugEnabled(boolean enabled) {\n-    debugEnabled = enabled;\n-  }\n-\n-  /**\n-   * Returns the name of the cluster.\n-   *\n-   * @return the name of the cluster of null if no cluster yet\n-   */\n-  def String getClusterName() {\n-    if (!clusterName) {\n-      def clusters = getClusters();\n-      if (clusters) {\n-        clusterName = clusters.items[0]?.Clusters?.cluster_name\n-      }\n-    }\n-    return clusterName\n-  }\n-\n-  /**\n-   * Adds a registered host to the cluster.\n-   *\n-   * @param hostName new node's hostname\n-   * @throws HttpResponseException if the node is not registered with ambari\n-   */\n-  def addHost(String hostName) throws HttpResponseException {\n-    if (debugEnabled) {\n-      println \"[DEBUG] POST ${ambari.getUri()}clusters/${getClusterName()}/hosts/$hostName\"\n-    }\n-    ambari.post(path: \"clusters/${getClusterName()}/hosts/$hostName\", { it })\n-  }\n-\n-  /**\n-   * Decommission and remove a host from the cluster.\n-   * NOTE: this is a synchronous call, it wont return until all\n-   * requests are finished\n-   *\n-   * Steps:\n-   *  1, decommission services\n-   *  2, stop services\n-   *  3, delete host components\n-   *  4, delete host\n-   *  5, restart services\n-   *\n-   * @param hostName host to be deleted\n-   */\n-  def removeHost(String hostName) {\n-    def components = getHostComponentsMap(hostName).keySet() as List\n-\n-    // decommission\n-    if (components.contains(\"NODEMANAGER\")) {\n-      decommissionNodeManager(hostName)\n-    }\n-    if (components.contains(\"DATANODE\")) {\n-      decommissionDataNode(hostName)\n-    }\n-\n-    // stop services\n-    def requests = stopComponentsOnHost(hostName, components)\n-    waitForRequestsToFinish(requests.values() as List)\n-\n-    // delete host components\n-    deleteHostComponents(hostName, components)\n-\n-    // delete host\n-    deleteHost(hostName)\n-\n-    // restart zookeper\n-    def id = restartServiceComponents(\"ZOOKEEPER\", [\"ZOOKEEPER_SERVER\"])\n-    waitForRequestsToFinish([id])\n-\n-    // restart nagios\n-    if (getServiceComponentsMap().containsKey(\"NAGIOS\")) {\n-      id = restartServiceComponents(\"NAGIOS\", [\"NAGIOS_SERVER\"])\n-      waitForRequestsToFinish([id])\n-    }\n-  }\n-\n-  /**\n-   * Does not return until all the requests are finished.\n-   * @param requestIds ids of the requests\n-   */\n-  def waitForRequestsToFinish(List<Integer> requestIds) {\n-    def stopped = false\n-    while (!stopped) {\n-      def state = true\n-      for (int id : requestIds) {\n-        if (getRequestProgress(id) != 100.0) {\n-          state = false;\n-          break;\n-        }\n-      }\n-      stopped = state\n-      Thread.sleep(2000)\n-    }\n-  }\n-\n-  /**\n-   * Decommission the data node on a given host.\n-   *\n-   * @return id of the request to keep track its progress\n-   */\n-  def int decommissionDataNode(String host) {\n-    decommission(host, \"DATANODE\", \"HDFS\", \"NAMENODE\")\n-  }\n-\n-  /**\n-   * Decommission the node manager on a given host.\n-   *\n-   * @return id of the request to keep track its progress\n-   */\n-  def int decommissionNodeManager(String host) {\n-    decommission(host, \"NODEMANAGER\", \"YARN\", \"RESOURCEMANAGER\")\n-  }\n-\n-  /**\n-   * Decommission a host component on a given host.\n-   *\n-   * @param host hostName where the component is installed to\n-   * @param slaveName slave to be decommissioned\n-   * @param serviceName where the slave belongs to\n-   * @param componentName where the slave belongs to\n-   * @return id of the request to keep track its progress\n-   */\n-  def int decommission(String host, String slaveName, String serviceName, String componentName) {\n-    def requestInfo = [\n-      command   : \"DECOMMISSION\",\n-      context   : \"Decommission $slaveName\",\n-      parameters: [\"slave_type\": slaveName, \"excluded_hosts\": host]\n-    ]\n-    def filter = [\n-      [\"service_name\": serviceName, \"component_name\": componentName]\n-    ]\n-    Map bodyMap = [\n-      \"RequestInfo\"              : requestInfo,\n-      \"Requests/resource_filters\": filter\n-    ]\n-    ambari.post(path: \"clusters/${getClusterName()}/requests\", body: new JsonBuilder(bodyMap).toPrettyString(), {\n-      getRequestId(it)\n-    })\n-  }\n-\n-  /**\n-   * Deletes the components from the host.\n-   */\n-  def deleteHostComponents(String hostName, List<String> components) {\n-    components.each {\n-      ambari.delete(path: \"clusters/${getClusterName()}/hosts/$hostName/host_components/$it\")\n-    }\n-  }\n-\n-  /**\n-   * Deletes the host from the cluster.\n-   */\n-  def deleteHost(String hostName) {\n-    ambari.delete(path: \"clusters/${getClusterName()}/hosts/$hostName\")\n-  }\n-\n-  /**\n-   * Install all the components from a given blueprint's host group. The services must be installed\n-   * in order to install its components. It is recommended to use the same blueprint's host group from which\n-   * the cluster was created.\n-   *\n-   * @param hostName components will be installed on this host\n-   * @param blueprint id of the blueprint\n-   * @param hostGroup host group of the blueprint\n-   * @return map of the component names and their request id since its an async call\n-   */\n-  def Map<String, Integer> installComponentsToHost(String hostName, String blueprint, String hostGroup) throws HttpResponseException {\n-    def bpMap = getBlueprint(blueprint)\n-    def components = bpMap?.host_groups?.find { it.name.equals(hostGroup) }?.components?.collect { it.name }\n-    if (components) {\n-      return installComponentsToHost(hostName, components)\n-    } else {\n-      return [:]\n-    }\n-  }\n-\n-  /**\n-   * Installs the given components to the given host.\n-   * Only existing service components can be installed.\n-   *\n-   * @param hostName host to install the component to\n-   * @param components components to be installed\n-   * @throws HttpResponseException in case the component's service is not installed\n-   * @return map of the component names and their request id since its an async call\n-   */\n-  def Map<String, Integer> installComponentsToHost(String hostName, List<String> components) throws HttpResponseException {\n-    def resp = [:]\n-    components.each {\n-      addComponentToHost(hostName, it)\n-      resp << [(it): setComponentState(hostName, it, \"INSTALLED\")]\n-    }\n-    resp\n-  }\n-\n-  /**\n-   * Starts the given components on a host.\n-   *\n-   * @return map of the component names and their request id since its an async call\n-   * @throws HttpResponseException in case the component is not found\n-   */\n-  def Map<String, Integer> startComponentsOnHost(String hostName, List<String> components) throws HttpResponseException {\n-    setComponentsState(hostName, components, \"STARTED\")\n-  }\n-\n-  /**\n-   * Stops the given components on a host.\n-   *\n-   * @return map of the component names and their request id since its an async call\n-   * @throws HttpResponseException in case the component is not found\n-   */\n-  def Map<String, Integer> stopComponentsOnHost(String hostName, List<String> components) throws HttpResponseException {\n-    setComponentsState(hostName, components, \"INSTALLED\")\n-  }\n-\n-  /**\n-   * Checks whether the blueprint exists or not.\n-   *\n-   * @param id id of the blueprint\n-   * @return true if exists false otherwise\n-   */\n-  def boolean doesBlueprintExist(String id) {\n-    def result = false\n-    try {\n-      def Map resourceRequest = getResourceRequestMap(\"blueprints/$id\", ['fields': \"Blueprints\"])\n-      def jsonResponse = getSlurpedResource(resourceRequest)\n-      result = !(jsonResponse.status)\n-    } catch (e) {\n-      log.info(\"Blueprint does not exist\", e)\n-    }\n-    return result\n-  }\n-\n-  /**\n-   * Checks whether there are available blueprints or not.\n-   *\n-   * @return true if blueprints are available false otherwise\n-   */\n-  def boolean isBlueprintAvailable() {\n-    return getBlueprints().items?.size > 0\n-  }\n-\n-  /**\n-   * Returns a pre-formatted String of the blueprint.\n-   *\n-   * @param id id of the blueprint\n-   * @return formatted String\n-   */\n-  def String showBlueprint(String id) {\n-    def resp = getBlueprint(id)\n-    if (resp) {\n-      def groups = resp.host_groups.collect {\n-        def name = it.name\n-        def comps = it.components.collect { it.name.padRight(PAD).padLeft(PAD + 10) }.join(\"\\n\")\n-        return \"HOSTGROUP: $name\\n$comps\"\n-      }.join(\"\\n\")\n-      return \"[${resp.Blueprints.stack_name}:${resp.Blueprints.stack_version}]\\n$groups\"\n-    }\n-    return \"Not found\"\n-  }\n-\n-  /**\n-   * Returns the host group - components mapping of the blueprint.\n-   *\n-   * @param id id of the blueprint\n-   * @return Map where the key is the host group and the value is the list of components\n-   */\n-  def Map<String, List<String>> getBlueprintMap(String id) {\n-    def result = getBlueprint(id)?.host_groups?.collectEntries { [(it.name): it.components.collect { it.name }] }\n-    result ?: [:]\n-  }\n-\n-  /**\n-   * Returns a pre-formatted String of the blueprints.\n-   *\n-   * @return formatted blueprint list\n-   */\n-  def String showBlueprints() {\n-    getBlueprints().items.collect {\n-      \"${it.Blueprints.blueprint_name.padRight(PAD)} [${it.Blueprints.stack_name}:${it.Blueprints.stack_version}]\"\n-    }.join(\"\\n\")\n-  }\n-\n-  /**\n-   * Returns a Map containing the blueprint name - stack association.\n-   *\n-   * @return Map where the key is the blueprint's name value is the used stack\n-   */\n-  def Map<String, String> getBlueprintsMap() {\n-    def result = getBlueprints().items?.collectEntries {\n-      [(it.Blueprints.blueprint_name): it.Blueprints.stack_name + \":\" + it.Blueprints.stack_version]\n-    }\n-    result ?: new HashMap()\n-  }\n-\n-  /**\n-   * Recommends a host - host group assignment based on the provided blueprint\n-   * and the available hosts.\n-   *\n-   * @param blueprint id of the blueprint\n-   * @return recommended assignments\n-   */\n-  def Map<String, List<String>> recommendAssignments(String blueprint) throws InvalidHostGroupHostAssociation {\n-    def result = [:]\n-    def hostNames = getHostNames().keySet() as List\n-    def groups = getBlueprint(blueprint)?.host_groups?.collect { [\"name\": it.name, \"cardinality\": it.cardinality] }\n-    if (hostNames && groups) {\n-      def groupSize = groups.size()\n-      def hostSize = hostNames.size()\n-      if (hostSize == 1 && groupSize == 1) {\n-        result = [(groups[0].name): [hostNames[0]]]\n-      } else if (hostSize >= groupSize) {\n-        int i = 0\n-        groups.findAll { !it.name.toLowerCase().startsWith(SLAVE) }.each {\n-          result << [(it.name): [hostNames[i++]]]\n-        }\n-        def slaves = groups.findAll { it.name.toLowerCase().startsWith(SLAVE) }\n-        if (slaves) {\n-          int k = 0\n-          for (int j = i; j < hostSize; j++) {\n-            result[slaves[k].name] = result[slaves[k].name] ?: []\n-            result[slaves[k].name] << hostNames[j]\n-            result << [(slaves[k].name): result[slaves[k++].name]]\n-            k = k == slaves.size ? 0 : k\n-          }\n-        } else {\n-          throw new InvalidHostGroupHostAssociation(\"At least one '$SLAVE' is required\", groupSize)\n-        }\n-      } else {\n-        throw new InvalidHostGroupHostAssociation(\"At least $groupSize host is required\", groupSize)\n-      }\n-    }\n-    return result\n-  }\n-\n-  /**\n-   * Returns the name of the host groups for a given blueprint.\n-   *\n-   * @param blueprint id of the blueprint\n-   * @return host group list or empty list\n-   */\n-  def List<String> getHostGroups(String blueprint) {\n-    def result = getBlueprint(blueprint)\n-    result ? result.host_groups.collect { it.name } : new ArrayList<String>()\n-  }\n-\n-  /**\n-   * Returns a pre-formatted String of a blueprint used by the cluster.\n-   *\n-   * @return formatted String\n-   */\n-  def String showClusterBlueprint() {\n-    ambari.get(path: \"clusters/${getClusterName()}\", query: ['format': \"blueprint\"]).data.text\n-  }\n-\n-  /**\n-   * Adds a blueprint to the Ambari server. Exception is thrown if fails.\n-   *\n-   * @param json blueprint as json\n-   * @return blueprint json\n-   * @throws HttpResponseException in case of error\n-   */\n-  def String addBlueprint(String json) throws HttpResponseException {\n-    addBlueprint(json, [:])\n-  }\n-\n-  /**\n-   * Adds a blueprint with the desired configurations.\n-   *\n-   * @param json blueprint to be added\n-   * @param configurations blueprint will be extended with these configurations\n-   * @return the extended blueprint as json\n-   */\n-  def String addBlueprint(String json, Map<String, Map<String, String>> configurations) throws HttpResponseException {\n-    if (json) {\n-      def text = slurper.parseText(json)\n-      def bpMap = extendBlueprintConfiguration(text, configurations)\n-      def builder = new JsonBuilder(bpMap)\n-      def resultJson = builder.toPrettyString()\n-      postBlueprint(resultJson)\n-      resultJson\n-    }\n-  }\n-\n-  /**\n-   * Only validates the multinode blueprints, at least 1 slave host group must exist.\n-   * Throws an exception if the blueprint is not valid.\n-   *\n-   * @param json blueprint json\n-   * @throws InvalidBlueprintException if the blueprint is not valid\n-   */\n-  def void validateBlueprint(String json) throws InvalidBlueprintException {\n-    if (json) {\n-      def bpMap = slurper.parseText(json)\n-      if (bpMap?.host_groups?.size > 1) {\n-        def find = bpMap.host_groups.find { it.name.toLowerCase().startsWith(SLAVE) }\n-        if (!find) {\n-          throw new InvalidBlueprintException(\"At least one '$SLAVE' host group is required.\")\n-        }\n-      }\n-    } else {\n-      throw new InvalidBlueprintException(\"No blueprint specified\")\n-    }\n-  }\n-\n-  /**\n-   * Adds the default blueprints.\n-   *\n-   * @throws HttpResponseException in case of error\n-   */\n-  def void addDefaultBlueprints() throws HttpResponseException {\n-    addBlueprint(getResourceContent(\"blueprints/multi-node-hdfs-yarn\"))\n-    addBlueprint(getResourceContent(\"blueprints/single-node-hdfs-yarn\"))\n-    addBlueprint(getResourceContent(\"blueprints/lambda-architecture\"))\n-    addBlueprint(getResourceContent(\"blueprints/warmup\"))\n-    addBlueprint(getResourceContent(\"blueprints/hdp-singlenode-default\"))\n-    addBlueprint(getResourceContent(\"blueprints/hdp-multinode-default\"))\n-  }\n-\n-  /**\n-   * Creates a cluster with the given blueprint and host group - host association.\n-   *\n-   * @param clusterName name of the cluster\n-   * @param blueprintName blueprint id used to create this cluster\n-   * @param hostGroups Map<String, List<String> key - host group, value - host list\n-   * @return true if the creation was successful false otherwise\n-   * @throws HttpResponseException in case of error\n-   */\n-  def void createCluster(String clusterName, String blueprintName, Map<String, List<String>> hostGroups) throws HttpResponseException {\n-    if (debugEnabled) {\n-      println \"[DEBUG] POST ${ambari.getUri()}clusters/$clusterName\"\n-    }\n-    ambari.post(path: \"clusters/$clusterName\", body: createClusterJson(blueprintName, hostGroups), { it })\n-  }\n-\n-  /**\n-   * Deletes the cluster.\n-   *\n-   * @param clusterName name of the cluster\n-   * @throws HttpResponseException in case of error\n-   */\n-  def void deleteCluster(String clusterName) throws HttpResponseException {\n-    if (debugEnabled) {\n-      println \"[DEBUG] DELETE ${ambari.getUri()}clusters/$clusterName\"\n-    }\n-    ambari.delete(path: \"clusters/$clusterName\")\n-  }\n-\n-  /**\n-   * Returns the active cluster as json\n-   *\n-   * @return cluster as json String\n-   * @throws HttpResponseException in case of error\n-   */\n-  def String getClusterAsJson() throws HttpResponseException {\n-    String path = \"clusters/\" + getClusterName();\n-    Map resourceRequestMap = getResourceRequestMap(path, null)\n-    return getRawResource(resourceRequestMap)\n-  }\n-\n-  /**\n-   * Returns all clusters as json\n-   *\n-   * @return json String\n-   * @throws HttpResponseException in case of error\n-   */\n-  def getClustersAsJson() throws HttpResponseException {\n-    Map resourceRequestMap = getResourceRequestMap(\"clusters\", null)\n-    return getRawResource(resourceRequestMap)\n-  }\n-\n-  /**\n-   * Modify an existing configuration. Be ware you'll have to provide the whole configuration\n-   * otherwise properties might get lost.\n-   *\n-   * @param type type of the configuration e.g capacity-scheduler\n-   * @param properties properties to be used\n-   */\n-  def modifyConfiguration(String type, Map<String, String> properties) {\n-    Map bodyMap = [\n-      \"Clusters\": [\"desired_config\": [\"type\": type, \"tag\": \"version${System.currentTimeMillis()}\", \"properties\": properties]]\n-    ]\n-    def Map<String, ?> putRequestMap = [:]\n-    putRequestMap.put('requestContentType', ContentType.URLENC)\n-    putRequestMap.put('path', \"clusters/${getClusterName()}\")\n-    putRequestMap.put('body', new JsonBuilder(bodyMap).toPrettyString());\n-    ambari.put(putRequestMap)\n-  }\n-\n-  /**\n-   * Returns a pre-formatted String of the clusters.\n-   *\n-   * @return pre-formatted cluster list\n-   */\n-  def String showClusterList() {\n-    getClusters().items.collect {\n-      \"[$it.Clusters.cluster_id] $it.Clusters.cluster_name:$it.Clusters.version\"\n-    }.join(\"\\n\")\n-  }\n-\n-  /**\n-   * Returns the task properties as Map.\n-   *\n-   * @param request request id; default is 1\n-   * @return property Map or empty Map\n-   */\n-  def getTasks(request = 1) {\n-    getAllResources(\"requests/$request\", \"tasks/Tasks\")\n-  }\n-\n-  /**\n-   * Returns the requests progress.\n-   *\n-   * @param request request id; default is 1\n-   * @return progress in percentage\n-   */\n-  def BigDecimal getRequestProgress(request = 1) {\n-    def response = getAllResources(\"requests/$request\", \"Requests\")\n-    def String status = response?.Requests?.request_status\n-    if (status && status.equals(\"FAILED\")) {\n-      return new BigDecimal(-1)\n-    }\n-    return response?.Requests?.progress_percent\n-  }\n-\n-  /**\n-   * Returns a pre-formatted task list.\n-   *\n-   * @param request request id; default is 1\n-   * @return pre-formatted task list\n-   */\n-  def String showTaskList(request = 1) {\n-    getTasks(request)?.tasks.collect { \"${it.Tasks.command_detail.padRight(PAD)} [${it.Tasks.status}]\" }.join(\"\\n\")\n-  }\n-\n-  /**\n-   * Returns a Map containing the task's command detail as key and the task's status as value.\n-   *\n-   * @param request request id; default is 1\n-   * @return key task command detail; task value status\n-   */\n-  def Map<String, String> getTaskMap(request = 1) {\n-    def result = getTasks(request).tasks?.collectEntries { [(it.Tasks.command_detail): it.Tasks.status] }\n-    result ?: new HashMap()\n-  }\n-\n-  /**\n-   * Returns the available host names and their states. It also\n-   * contains hosts which are not part of the cluster, but are connected\n-   * to ambari.\n-   *\n-   * @return hostname state association\n-   */\n-  def Map<String, String> getHostNames() {\n-    getHosts().items.collectEntries { [(it.Hosts.host_name): it.Hosts.host_status] }\n-  }\n-\n-  /**\n-   * Returns the names of the hosts which have the given state. It also\n-   * contains hosts which are not part of the cluster, but are connected\n-   * to ambari.\n-   */\n-  def Map<String, String> getHostNamesByState(String state) {\n-    getHostNames().findAll { it.value == state }\n-  }\n-\n-  /**\n-   * Returns a pre-formatted list of the hosts.\n-   *\n-   * @return pre-formatted String\n-   */\n-  def String showHostList() {\n-    getHosts().items.collect {\n-      \"$it.Hosts.host_name [$it.Hosts.host_status] $it.Hosts.ip $it.Hosts.os_type:$it.Hosts.os_arch\"\n-    }.join(\"\\n\")\n-  }\n-\n-  /**\n-   * Returns a pre-formatted list of the service components.\n-   *\n-   * @return pre-formatted String\n-   */\n-  def String showServiceComponents() {\n-    getServices().items.collect {\n-      def name = it.ServiceInfo.service_name\n-      def state = it.ServiceInfo.state\n-      def componentList = getServiceComponents(name).items.collect {\n-        \"    ${it.ServiceComponentInfo.component_name.padRight(PAD)}  [$it.ServiceComponentInfo.state]\"\n-      }.join(\"\\n\")\n-      \"${name.padRight(PAD)} [$state]\\n$componentList\"\n-    }.join(\"\\n\")\n-  }\n-\n-  /**\n-   * Returns the service components properties as Map where the key is the service name and\n-   * value is a component - state association.\n-   *\n-   * @return service name - [component name - status]\n-   */\n-  def Map<String, Map<String, String>> getServiceComponentsMap() {\n-    def result = getServices().items.collectEntries {\n-      def name = it.ServiceInfo.service_name\n-      def componentList = getServiceComponents(name).items.collectEntries {\n-        [(it.ServiceComponentInfo.component_name): it.ServiceComponentInfo.state]\n-      }\n-      [(name): componentList]\n-    }\n-    result ?: new HashMap()\n-  }\n-\n-  /**\n-   * Performs a health check on the Ambari server.\n-   *\n-   * @return status\n-   */\n-  def String healthCheck() {\n-    ambari.get(path: \"check\", headers: [\"Accept\": ContentType.TEXT]).data.text\n-  }\n-\n-  /**\n-   * Returns a pre-formatted service list.\n-   *\n-   * @return formatted String\n-   */\n-  def String showServiceList() {\n-    getServices().items.collect { \"${it.ServiceInfo.service_name.padRight(PAD)} [$it.ServiceInfo.state]\" }.join(\"\\n\")\n-  }\n-\n-  /**\n-   * Returns the services properties as Map where the key is the service's name and the values is the service's state.\n-   *\n-   * @return service name - service state association as Map\n-   */\n-  def Map<String, String> getServicesMap() {\n-    def result = getServices().items.collectEntries { [(it.ServiceInfo.service_name): it.ServiceInfo.state] }\n-    result ?: new HashMap()\n-  }\n-\n-  /**\n-   * Returns a pre-formatted component list of a host.\n-   *\n-   * @param host which host's components are requested\n-   * @return formatted String\n-   */\n-  def String showHostComponentList(host) {\n-    getHostComponents(host).items.collect {\n-      \"${it.HostRoles.component_name.padRight(PAD)} [$it.HostRoles.state]\"\n-    }.join(\"\\n\")\n-  }\n-\n-  /**\n-   * Returns the host's components as Map where the key is the component's name and values is its state.\n-   *\n-   * @param host which host's components are requested\n-   * @return component name - state association\n-   */\n-  def Map<String, String> getHostComponentsMap(host) {\n-    def result = getHostComponents(host)?.items?.collectEntries { [(it.HostRoles.component_name): it.HostRoles.state] }\n-    result ?: [:]\n-  }\n-\n-  /**\n-   * Returns the blueprint json as String.\n-   *\n-   * @param id id of the blueprint\n-   * @return json as String, exception if thrown is it fails\n-   */\n-  def String getBlueprintAsJson(id) {\n-    Map resourceRequestMap = getResourceRequestMap(\"blueprints/$id\", ['fields': \"host_groups,Blueprints\"])\n-    return getRawResource(resourceRequestMap)\n-  }\n-\n-  /**\n-   * Returns a map with service configurations. The keys are the service names, values are maps with <propertyName, propertyValue> entries\n-   *\n-   * @return a Map with entries of format <servicename, Map<property, value>>\n-   */\n-  def Map<String, Map<String, String>> getServiceConfigMap(String type = \"\") {\n-    def Map<String, Integer> serviceToTags = new HashMap<>()\n-\n-    //get services and last versions configurations\n-    def path = \"clusters/${getClusterName()}/configurations\"\n-    Map<String, ?> configsResourceRequestMap = getResourceRequestMap(path, type ? [\"type\": type] : [:])\n-    def rawConfigs = getSlurpedResource(configsResourceRequestMap)\n-\n-    rawConfigs?.items.collect { object ->\n-      // selecting the latest versions\n-      processServiceVersions(serviceToTags, object.type, object.tag)\n-    }\n-\n-    // collect properties for every service\n-    def finalMap = serviceToTags.collectEntries { entry ->\n-      // collect config props for every service\n-      def propsMap = collectConfigPropertiesForService(entry.getKey(), entry.getValue())\n-      // put them in the final map\n-      [(entry.key): propsMap]\n-    }\n-    return finalMap\n-  }\n-\n-  /**\n-   * Starts all the services.\n-   *\n-   * @return id of the request since its an async call\n-   */\n-  def int startAllServices() {\n-    log.debug(\"Starting all services ...\")\n-    manageService(\"Start All Services\", \"STARTED\")\n-  }\n-\n-  /**\n-   * Stops all the services.\n-   *\n-   * @return id of the request since its an async call\n-   */\n-  def int stopAllServices() {\n-    log.debug(\"Stopping all services ...\")\n-    manageService(\"Stop All Services\", \"INSTALLED\")\n-  }\n-\n-  /**\n-   * Starts the given service.\n-   *\n-   * @param service name of the service\n-   * @return id of the request\n-   */\n-  def int startService(String service) {\n-    manageService(\"Starting $service\", \"STARTED\", service)\n-  }\n-\n-  /**\n-   * Stops the given service.\n-   *\n-   * @param service name of the service\n-   * @return id of the request\n-   */\n-  def int stopService(String service) {\n-    manageService(\"Stopping $service\", \"INSTALLED\", service)\n-  }\n-\n-  def boolean servicesStarted() {\n-    return servicesStatus(true)\n-  }\n-\n-  def boolean servicesStopped() {\n-    return servicesStatus(false)\n-  }\n-\n-  /**\n-   * Returns the public hostnames of the hosts which the host components are installed to.\n-   */\n-  def List<String> getPublicHostNames(String hostComponent) {\n-    def hosts = getInternalHostNames(hostComponent)\n-    if (hosts) {\n-      return hosts.collect() { resolveInternalHostName(it) }\n-    } else {\n-      return []\n-    }\n-  }\n-\n-  /**\n-   * Returns the internal hostnames of the hosts which the host components are installed to.\n-   */\n-  def List<String> getInternalHostNames(String hostComponent) {\n-    def hosts = []\n-    getClusterHosts().each {\n-      if (getHostComponentsMap(it).keySet().contains(hostComponent)) {\n-        hosts << it\n-      }\n-    }\n-    hosts\n-  }\n-\n-  /**\n-   * Restarts the given components of a service.\n-   */\n-  def int restartServiceComponents(String service, List<String> components) {\n-    def filter = components.collect {\n-      [\"service_name\": service, \"component_name\": it, \"hosts\": getInternalHostNames(it).join(\",\")]\n-    }\n-    Map bodyMap = [\n-      \"RequestInfo\"              : [command: \"RESTART\", context: \"Restart $service components $components\"],\n-      \"Requests/resource_filters\": filter\n-    ]\n-    ambari.post(path: \"clusters/${getClusterName()}/requests\", body: new JsonBuilder(bodyMap).toPrettyString(), {\n-      getRequestId(it)\n-    })\n-  }\n-\n-  /**\n-   * Returns the names of the hosts which are in the cluster.\n-   */\n-  def List<String> getClusterHosts() {\n-    slurp(\"clusters/${getClusterName()}\")?.hosts?.Hosts?.host_name\n-  }\n-\n-  /**\n-   * Resolves an internal hostname to a public one.\n-   */\n-  def String resolveInternalHostName(String internalHostName) {\n-    slurp(\"clusters/${getClusterName()}/hosts/$internalHostName\")?.Hosts?.public_host_name\n-  }\n-\n-  def private boolean servicesStatus(boolean starting) {\n-    def String status = (starting) ? \"STARTED\" : \"INSTALLED\"\n-    Map serviceComponents = getServicesMap();\n-    boolean allInState = true;\n-    serviceComponents.values().each { val ->\n-      log.debug(\"Service: {}\", val)\n-      allInState = allInState && val.equals(status)\n-    }\n-    return allInState;\n-  }\n-\n-  def private manageService(String context, String state, String service = \"\") {\n-    Map bodyMap = [\n-      RequestInfo: [context: context],\n-      ServiceInfo: [state: state]\n-    ]\n-    JsonBuilder builder = new JsonBuilder(bodyMap)\n-    def path = \"${ambari.getUri()}clusters/${getClusterName()}/services\"\n-    if (service) {\n-      path += \"/$service\"\n-    }\n-    def Map<String, ?> putRequestMap = [:]\n-    putRequestMap.put('requestContentType', ContentType.URLENC)\n-    putRequestMap.put('path', path)\n-    putRequestMap.put('query', ['params/run_smoke_test': 'false'])\n-    putRequestMap.put('body', builder.toPrettyString());\n-\n-    def reponse = ambari.put(putRequestMap)\n-    slurper.parseText(reponse.getAt(\"responseData\")?.getAt(\"str\"))?.Requests?.id\n-  }\n-\n-  private def processServiceVersions(Map<String, Integer> serviceToVersions, String service, def version) {\n-    boolean change = false\n-    log.debug(\"Handling service version <{}:{}>\", service, version)\n-    if (serviceToVersions.containsKey(service)) {\n-      log.debug(\"Entry already added, checking versions ...\")\n-      def newVersion = Long.valueOf(version.minus(\"version\")).longValue()\n-      def oldVersion = Long.valueOf(serviceToVersions.get(service).minus(\"version\")).longValue()\n-      change = oldVersion < newVersion\n-    } else {\n-      change = true;\n-    }\n-    if (change) {\n-      log.debug(\"Adding / updating service version <{}:{}>\", service, version)\n-      serviceToVersions.put(service, version);\n-    }\n-  }\n-\n-  private def Map<String, String> collectConfigPropertiesForService(String service, def tag) {\n-    Map<String, String> serviceConfigProperties\n-\n-    def Map resourceRequestMap = getResourceRequestMap(\"clusters/${getClusterName()}/configurations\",\n-      ['type': \"$service\", 'tag': \"$tag\"])\n-    def rawResource = getSlurpedResource(resourceRequestMap);\n-\n-    if (rawResource) {\n-      serviceConfigProperties = rawResource.items?.collectEntries { it -> it.properties }\n-    } else {\n-      log.debug(\"No resource object has been returned for the resource request map: {}\", resourceRequestMap)\n-    }\n-    return serviceConfigProperties\n-  }\n-\n-  private Map<String, ?> getResourceRequestMap(String path, Map<String, String> queryParams) {\n-    def Map requestMap = [:]\n-    if (queryParams) {\n-      requestMap = ['path': \"${ambari.getUri()}\" + path, 'query': queryParams]\n-    } else {\n-      requestMap = ['path': \"${ambari.getUri()}\" + path]\n-    }\n-    return requestMap\n-  }\n-\n-  /**\n-   * Gets the resource as a text as it;s returned by the server.\n-   *\n-   * @param resourceRequestMap\n-   */\n-  private getRawResource(Map resourceRequestMap) {\n-    def rawResource = null;\n-    try {\n-      if (debugEnabled) {\n-        println \"[DEBUG] GET ${resourceRequestMap.get('path')}\"\n-      }\n-      rawResource = ambari.get(resourceRequestMap)?.data?.text\n-    } catch (e) {\n-      def clazz = e.class\n-      log.error(\"Error occurred during GET request to {}, exception: \", resourceRequestMap.get('path'), e)\n-      if (clazz == NoHttpResponseException.class || clazz == ConnectException.class\n-        || clazz == ClientProtocolException.class || clazz == NoRouteToHostException.class\n-        || clazz == UnknownHostException.class || (clazz == HttpResponseException.class && e.message == \"Bad credentials\")) {\n-        throw new AmbariConnectionException(\"Cannot connect to Ambari ${ambari.getUri()}\")\n-      }\n-    }\n-    return rawResource\n-  }\n-\n-  /**\n-   * Slurps the response text.\n-   *\n-   * @param resourceRequestMap a map wrapping the resource request components\n-   * @return an Object as it's created by the JsonSlurper\n-   */\n-  private getSlurpedResource(Map resourceRequestMap) {\n-    def rawResource = getRawResource(resourceRequestMap)\n-    def slurpedResource = (rawResource != null) ? slurper.parseText(rawResource) : rawResource\n-    return slurpedResource\n-  }\n-\n-\n-  private def getAllResources(resourceName, fields = \"\") {\n-    slurp(\"clusters/${getClusterName()}/$resourceName\", fields ? \"$fields/*\" : \"\")\n-  }\n-\n-  /**\n-   * Posts the blueprint JSON to Ambari with name 'bp' in the URL\n-   * because it does not matter here. The blueprint's name is\n-   * provided in the JSON.\n-   *\n-   * @param blueprint json\n-   * @return response message\n-   */\n-  private void postBlueprint(String blueprint) {\n-    if (debugEnabled) {\n-      println \"[DEBUG] POST ${ambari.getUri()}blueprints/bp\"\n-    }\n-    ambari.post(path: \"blueprints/bp\", body: blueprint, { it })\n-  }\n-\n-  private def createClusterJson(String name, Map hostGroups) {\n-    def builder = new JsonBuilder()\n-    def groups = hostGroups.collect {\n-      def hostList = it.value.collect { ['fqdn': it] }\n-      [name: it.key, hosts: hostList]\n-    }\n-    builder { \"blueprint\" name; \"default_password\" \"admin\"; \"host_groups\" groups }\n-    builder.toPrettyString()\n-  }\n-\n-  private def slurp(path, fields = \"\") {\n-\n-    def fieldsMap = fields ? ['fields': fields] : [:]\n-    def Map resourceReqMap = getResourceRequestMap(path, fieldsMap)\n-    def result = getSlurpedResource(resourceReqMap)\n-\n-    return result\n-  }\n-\n-  /**\n-   * Return the blueprint's properties as a Map.\n-   *\n-   * @param id id of the blueprint\n-   * @return properties as Map\n-   */\n-  private def getBlueprint(id) {\n-    slurp(\"blueprints/$id\", \"host_groups,Blueprints\")\n-  }\n-\n-  /**\n-   * Returns a Map containing the blueprint's properties parsed from the Ambari response json.\n-   *\n-   * @return blueprint's properties as Map or empty Map\n-   */\n-  private def getBlueprints() {\n-    slurp(\"blueprints\", \"Blueprints\")\n-  }\n-\n-  /**\n-   * Returns a Map containing the cluster's properties parsed from the Ambari response json.\n-   *\n-   * @return cluster's properties as Map or empty Map\n-   */\n-  private def getClusters() {\n-    slurp(\"clusters\", \"Clusters\")\n-  }\n-\n-  /**\n-   * Returns the available hosts properties as a Map.\n-   *\n-   * @return Map containing the hosts properties\n-   */\n-  private def getHosts() {\n-    slurp(\"hosts\", \"Hosts\")\n-  }\n-\n-  /**\n-   * Returns the service components properties as Map.\n-   *\n-   * @param service id of the service\n-   * @return service component properties as Map\n-   */\n-  private def getServiceComponents(service) {\n-    getAllResources(\"services/$service/components\", \"ServiceComponentInfo\")\n-  }\n-\n-  /**\n-   * Returns the services properties as Map parsed from Ambari response json.\n-   *\n-   * @return service properties as Map\n-   */\n-  private def getServices() {\n-    getAllResources(\"services\", \"ServiceInfo\")\n-  }\n-\n-  private def addComponentToHost(String hostName, String component) {\n-    if (debugEnabled) {\n-      println \"[DEBUG] POST ${ambari.getUri()}clusters/${getClusterName()}/hosts/$hostName/host_components\"\n-    }\n-    ambari.post(path: \"clusters/${getClusterName()}/hosts/$hostName/host_components/${component.toUpperCase()}\", { it })\n-  }\n-\n-  private def Map<String, Integer> setComponentsState(String hostName, List<String> components, String state)\n-    throws HttpResponseException {\n-    def resp = [:]\n-    components.each {\n-      resp << [(it): setComponentState(hostName, it, state)]\n-    }\n-    return resp\n-  }\n-\n-  private def setComponentState(String hostName, String component, String state) {\n-    if (debugEnabled) {\n-      println \"[DEBUG] PUT ${ambari.getUri()}clusters/${getClusterName()}/hosts/$hostName/host_components/$component\"\n-    }\n-    Map bodyMap = [\n-      HostRoles  : [state: state.toUpperCase()],\n-      RequestInfo: [context: \"${component.toUpperCase()} ${state.toUpperCase()}\"]\n-    ]\n-    def Map<String, ?> putRequestMap = [:]\n-    putRequestMap.put('requestContentType', ContentType.URLENC)\n-    putRequestMap.put('path', \"clusters/${getClusterName()}/hosts/$hostName/host_components/${component.toUpperCase()}\")\n-    putRequestMap.put('body', new JsonBuilder(bodyMap).toPrettyString());\n-    def reponse = ambari.put(putRequestMap)\n-    slurper.parseText(reponse.getAt(\"responseData\")?.getAt(\"str\"))?.Requests?.id\n-  }\n-\n-  /**\n-   * Returns the properties of the host components as a Map parsed from the Ambari response json.\n-   *\n-   * @param host which host's components are requested\n-   * @return component properties as Map\n-   */\n-  private def getHostComponents(host) {\n-    getAllResources(\"hosts/$host/host_components\", \"HostRoles\")\n-  }\n-\n-  private String getResourceContent(name) {\n-    getClass().getClassLoader().getResourceAsStream(name)?.text\n-  }\n-\n-  private def extendBlueprintConfiguration(Map blueprintMap, Map newConfigs) {\n-    def configurations = blueprintMap.configurations\n-    if (!configurations) {\n-      if (newConfigs) {\n-        def conf = []\n-        newConfigs.each { conf << [(it.key): it.value] }\n-        blueprintMap << [\"configurations\": conf]\n-      }\n-      return blueprintMap\n-    }\n-    newConfigs.each {\n-      def site = it.key\n-      def index = indexOfConfig(configurations, site)\n-      if (index == -1) {\n-        configurations << [\"$site\": it.value]\n-      } else {\n-        def existingConf = configurations.get(index)\n-        existingConf.\"$site\" << it.value\n-      }\n-    }\n-    blueprintMap\n-  }\n-\n-  private int indexOfConfig(List<Map> configurations, String site) {\n-    def index = 0\n-    for (Map conf : configurations) {\n-      if (conf.containsKey(site)) {\n-        return index;\n-      }\n-      index++\n-    }\n-    return -1;\n-  }\n-\n-  private def int getRequestId(def responseDecorator) {\n-    def resp = IOUtils.toString(new InputStreamReader(responseDecorator.entity.content.wrappedStream))\n-    slurper.parseText(resp)?.Requests?.id\n-  }\n-\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/main/groovy/org/apache/ambari/groovy/client/AmbariClient.groovy",
                "sha": "6d94c5d717e484f0daf55f6d9ba6da5580c14f9d",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/main/resources/blueprints/hdp-multinode-default",
                "changes": 179,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/main/resources/blueprints/hdp-multinode-default?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 179,
                "filename": "ambari-client/groovy-client/src/main/resources/blueprints/hdp-multinode-default",
                "patch": "@@ -1,179 +0,0 @@\n-{\n-    \"configurations\" : [\n-        {\n-            \"nagios-env\" : {\n-                \"nagios_contact\" : \"admin@localhost\"\n-            }\n-        }\n-    ],\n-    \"host_groups\" : [\n-        {\n-            \"name\" : \"master_1\",\n-            \"components\" : [\n-                {\n-                    \"name\" : \"NAMENODE\"\n-                },\n-                {\n-                    \"name\" : \"ZOOKEEPER_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"HBASE_MASTER\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"HDFS_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"YARN_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HCAT\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_MONITOR\"\n-                }\n-            ],\n-            \"cardinality\" : \"1\"\n-        },\n-        {\n-            \"name\" : \"master_2\",\n-            \"components\" : [\n-\n-                {\n-                    \"name\" : \"ZOOKEEPER_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HISTORYSERVER\"\n-                },\n-                {\n-                    \"name\" : \"HIVE_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"SECONDARY_NAMENODE\"\n-                },\n-                {\n-                    \"name\" : \"HIVE_METASTORE\"\n-                },\n-                {\n-                    \"name\" : \"HDFS_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HIVE_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"YARN_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"MYSQL_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_MONITOR\"\n-                },\n-                {\n-                    \"name\" : \"WEBHCAT_SERVER\"\n-                }\n-            ],\n-            \"cardinality\" : \"1\"\n-        },\n-        {\n-            \"name\" : \"master_3\",\n-            \"components\" : [\n-                {\n-                    \"name\" : \"RESOURCEMANAGER\"\n-                },\n-                {\n-                    \"name\" : \"ZOOKEEPER_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_MONITOR\"\n-                }\n-            ],\n-            \"cardinality\" : \"1\"\n-        },\n-        {\n-            \"name\" : \"master_4\",\n-            \"components\" : [\n-                {\n-                    \"name\" : \"OOZIE_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"ZOOKEEPER_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_MONITOR\"\n-                }\n-            ],\n-            \"cardinality\" : \"1\"\n-        },\n-        {\n-            \"name\" : \"slave_1\",\n-            \"components\" : [\n-                {\n-                    \"name\" : \"HBASE_REGIONSERVER\"\n-                },\n-                {\n-                    \"name\" : \"NODEMANAGER\"\n-                },\n-                {\n-                    \"name\" : \"DATANODE\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_MONITOR\"\n-                }\n-            ],\n-            \"cardinality\" : \"${slavesCount}\"\n-        },\n-        {\n-            \"name\" : \"gateway\",\n-            \"components\" : [\n-                {\n-                    \"name\" : \"AMBARI_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"NAGIOS_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"ZOOKEEPER_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"PIG\"\n-                },\n-                {\n-                    \"name\" : \"OOZIE_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HBASE_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HCAT\"\n-                },\n-                {\n-                    \"name\" : \"SQOOP\"\n-                },\n-                {\n-                    \"name\" : \"HDFS_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HIVE_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"YARN_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"MAPREDUCE2_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_MONITOR\"\n-                }\n-            ],\n-            \"cardinality\" : \"1\"\n-        }\n-    ],\n-    \"Blueprints\" : {\n-        \"blueprint_name\" : \"hdp-multinode-default\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"2.1\"\n-    }\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/main/resources/blueprints/hdp-multinode-default",
                "sha": "0a7680851b26f5916b2bde94d3a5f4ec16f21e38",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/main/resources/blueprints/hdp-singlenode-default",
                "changes": 133,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/main/resources/blueprints/hdp-singlenode-default?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 133,
                "filename": "ambari-client/groovy-client/src/main/resources/blueprints/hdp-singlenode-default",
                "patch": "@@ -1,133 +0,0 @@\n-{\n-    \"configurations\" : [\n-        {\n-            \"nagios-env\" : {\n-                \"nagios_contact\" : \"admin@localhost\"\n-            }\n-        }\n-    ],\n-    \"host_groups\" : [\n-        {\n-            \"name\" : \"master\",\n-            \"components\" : [\n-                {\n-                    \"name\" : \"STORM_REST_API\"\n-                },\n-                {\n-                    \"name\" : \"PIG\"\n-                },\n-                {\n-                    \"name\" : \"HISTORYSERVER\"\n-                },\n-                {\n-                    \"name\" : \"HBASE_REGIONSERVER\"\n-                },\n-                {\n-                    \"name\" : \"OOZIE_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HBASE_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"NAMENODE\"\n-                },\n-                {\n-                    \"name\" : \"SUPERVISOR\"\n-                },\n-                {\n-                    \"name\" : \"FALCON_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"HCAT\"\n-                },\n-                {\n-                    \"name\" : \"AMBARI_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"APP_TIMELINE_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"HDFS_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HIVE_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"NODEMANAGER\"\n-                },\n-                {\n-                    \"name\" : \"DATANODE\"\n-                },\n-                {\n-                    \"name\" : \"WEBHCAT_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"RESOURCEMANAGER\"\n-                },\n-                {\n-                    \"name\" : \"ZOOKEEPER_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"ZOOKEEPER_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"STORM_UI_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"HBASE_MASTER\"\n-                },\n-                {\n-                    \"name\" : \"HIVE_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"OOZIE_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"FALCON_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"NAGIOS_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"SECONDARY_NAMENODE\"\n-                },\n-                {\n-                    \"name\" : \"TEZ_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HIVE_METASTORE\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"SQOOP\"\n-                },\n-                {\n-                    \"name\" : \"YARN_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"MAPREDUCE2_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"MYSQL_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_MONITOR\"\n-                },\n-                {\n-                    \"name\" : \"DRPC_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"NIMBUS\"\n-                }\n-            ],\n-            \"cardinality\" : \"1\"\n-        }\n-    ],\n-    \"Blueprints\" : {\n-        \"blueprint_name\" : \"hdp-singlenode-default\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"2.1\"\n-    }\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/main/resources/blueprints/hdp-singlenode-default",
                "sha": "34fa5a0f81ba92f44f45543f09b725093a9de1d6",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/main/resources/blueprints/lambda-architecture",
                "changes": 169,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/main/resources/blueprints/lambda-architecture?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 169,
                "filename": "ambari-client/groovy-client/src/main/resources/blueprints/lambda-architecture",
                "patch": "@@ -1,169 +0,0 @@\n-{\n-  \"configurations\": [\n-    {\n-        \"nagios-env\" : {\n-            \"nagios_contact\" : \"admin@localhost\"\n-        }\n-    }\n-  ],\n-  \"host_groups\": [\n-    {\n-      \"name\": \"master_1\",\n-      \"components\": [\n-        {\n-          \"name\": \"ZOOKEEPER_SERVER\"\n-        },\n-        {\n-          \"name\": \"ZOOKEEPER_CLIENT\"\n-        },\n-        {\n-          \"name\": \"PIG\"\n-        },\n-        {\n-          \"name\": \"HISTORYSERVER\"\n-        },\n-        {\n-          \"name\": \"SUPERVISOR\"\n-        },\n-        {\n-          \"name\": \"NAGIOS_SERVER\"\n-        },\n-        {\n-          \"name\": \"TEZ_CLIENT\"\n-        },\n-        {\n-          \"name\": \"AMBARI_SERVER\"\n-        },\n-        {\n-          \"name\": \"APP_TIMELINE_SERVER\"\n-        },\n-        {\n-          \"name\": \"GANGLIA_SERVER\"\n-        },\n-        {\n-          \"name\": \"HDFS_CLIENT\"\n-        },\n-        {\n-          \"name\": \"NODEMANAGER\"\n-        },\n-        {\n-          \"name\": \"YARN_CLIENT\"\n-        },\n-        {\n-          \"name\": \"MAPREDUCE2_CLIENT\"\n-        },\n-        {\n-          \"name\": \"DATANODE\"\n-        },\n-        {\n-          \"name\": \"GANGLIA_MONITOR\"\n-        },\n-        {\n-          \"name\": \"RESOURCEMANAGER\"\n-        }\n-      ],\n-      \"cardinality\": \"1\"\n-    },\n-    {\n-      \"name\": \"slave_1\",\n-      \"components\": [\n-        {\n-          \"name\": \"ZOOKEEPER_SERVER\"\n-        },\n-        {\n-          \"name\": \"ZOOKEEPER_CLIENT\"\n-        },\n-        {\n-          \"name\": \"PIG\"\n-        },\n-        {\n-          \"name\": \"STORM_REST_API\"\n-        },\n-        {\n-          \"name\": \"STORM_UI_SERVER\"\n-        },\n-        {\n-          \"name\": \"SUPERVISOR\"\n-        },\n-        {\n-          \"name\": \"SECONDARY_NAMENODE\"\n-        },\n-        {\n-          \"name\": \"TEZ_CLIENT\"\n-        },\n-        {\n-          \"name\": \"HDFS_CLIENT\"\n-        },\n-        {\n-          \"name\": \"NODEMANAGER\"\n-        },\n-        {\n-          \"name\": \"YARN_CLIENT\"\n-        },\n-        {\n-          \"name\": \"MAPREDUCE2_CLIENT\"\n-        },\n-        {\n-          \"name\": \"DATANODE\"\n-        },\n-        {\n-          \"name\": \"GANGLIA_MONITOR\"\n-        },\n-        {\n-          \"name\": \"DRPC_SERVER\"\n-        },\n-        {\n-          \"name\": \"NIMBUS\"\n-        }\n-      ],\n-      \"cardinality\": \"1\"\n-    },\n-    {\n-      \"name\": \"slave_2\",\n-      \"components\": [\n-        {\n-          \"name\": \"ZOOKEEPER_SERVER\"\n-        },\n-        {\n-          \"name\": \"ZOOKEEPER_CLIENT\"\n-        },\n-        {\n-          \"name\": \"PIG\"\n-        },\n-        {\n-          \"name\": \"NAMENODE\"\n-        },\n-        {\n-          \"name\": \"SUPERVISOR\"\n-        },\n-        {\n-          \"name\": \"TEZ_CLIENT\"\n-        },\n-        {\n-          \"name\": \"HDFS_CLIENT\"\n-        },\n-        {\n-          \"name\": \"NODEMANAGER\"\n-        },\n-        {\n-          \"name\": \"YARN_CLIENT\"\n-        },\n-        {\n-          \"name\": \"MAPREDUCE2_CLIENT\"\n-        },\n-        {\n-          \"name\": \"DATANODE\"\n-        },\n-        {\n-          \"name\": \"GANGLIA_MONITOR\"\n-        }\n-      ],\n-      \"cardinality\": \"1\"\n-    }\n-  ],\n-  \"Blueprints\": {\n-    \"blueprint_name\": \"lambda-architecture\",\n-    \"stack_name\": \"HDP\",\n-    \"stack_version\": \"2.1\"\n-  }\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/main/resources/blueprints/lambda-architecture",
                "sha": "26fa576202cebc97d113edc5282d8b4abd4ee633",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/main/resources/blueprints/multi-node-hdfs-yarn",
                "changes": 67,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/main/resources/blueprints/multi-node-hdfs-yarn?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 67,
                "filename": "ambari-client/groovy-client/src/main/resources/blueprints/multi-node-hdfs-yarn",
                "patch": "@@ -1,67 +0,0 @@\n-{\n-  \"configurations\": [\n-    {\n-        \"nagios-env\" : {\n-            \"nagios_contact\" : \"admin@localhost\"\n-        }\n-    }\n-  ],\n-  \"host_groups\": [\n-    {\n-      \"name\": \"master\",\n-      \"components\": [\n-        {\n-          \"name\": \"NAMENODE\"\n-        },\n-        {\n-          \"name\": \"SECONDARY_NAMENODE\"\n-        },\n-        {\n-          \"name\": \"RESOURCEMANAGER\"\n-        },\n-        {\n-          \"name\": \"HISTORYSERVER\"\n-        },\n-        {\n-          \"name\": \"NAGIOS_SERVER\"\n-        },\n-        {\n-          \"name\": \"ZOOKEEPER_SERVER\"\n-        }\n-      ],\n-      \"cardinality\": \"1\"\n-    },\n-    {\n-      \"name\": \"slave_1\",\n-      \"components\": [\n-        {\n-          \"name\": \"DATANODE\"\n-        },\n-        {\n-          \"name\": \"HDFS_CLIENT\"\n-        },\n-        {\n-          \"name\": \"NODEMANAGER\"\n-        },\n-        {\n-          \"name\": \"YARN_CLIENT\"\n-        },\n-        {\n-          \"name\" : \"APP_TIMELINE_SERVER\"\n-        },\n-        {\n-          \"name\": \"MAPREDUCE2_CLIENT\"\n-        },\n-        {\n-          \"name\": \"ZOOKEEPER_CLIENT\"\n-        }\n-      ],\n-      \"cardinality\": \"2\"\n-    }\n-  ],\n-  \"Blueprints\": {\n-    \"blueprint_name\": \"multi-node-hdfs-yarn\",\n-    \"stack_name\": \"HDP\",\n-    \"stack_version\": \"2.1\"\n-  }\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/main/resources/blueprints/multi-node-hdfs-yarn",
                "sha": "82f30421d40863054009177c587631c277629d82",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/main/resources/blueprints/single-node-hdfs-yarn",
                "changes": 51,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/main/resources/blueprints/single-node-hdfs-yarn?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 51,
                "filename": "ambari-client/groovy-client/src/main/resources/blueprints/single-node-hdfs-yarn",
                "patch": "@@ -1,51 +0,0 @@\n-{\n-  \"host_groups\" : [\n-    {\n-      \"name\" : \"master\",\n-      \"components\" : [\n-      {\n-        \"name\" : \"NAMENODE\"\n-      },\n-      {\n-        \"name\" : \"SECONDARY_NAMENODE\"\n-      },\n-      {\n-        \"name\" : \"DATANODE\"\n-      },\n-      {\n-        \"name\" : \"HDFS_CLIENT\"\n-      },\n-      {\n-        \"name\" : \"RESOURCEMANAGER\"\n-      },\n-      {\n-        \"name\" : \"NODEMANAGER\"\n-      },\n-      {\n-        \"name\" : \"YARN_CLIENT\"\n-      },\n-      {\n-        \"name\" : \"HISTORYSERVER\"\n-      },\n-      {\n-        \"name\" : \"MAPREDUCE2_CLIENT\"\n-      },\n-      {\n-        \"name\" : \"ZOOKEEPER_SERVER\"\n-      },\n-      {\n-        \"name\" : \"APP_TIMELINE_SERVER\"\n-      },\n-      {\n-        \"name\" : \"ZOOKEEPER_CLIENT\"\n-      }\n-      ],\n-      \"cardinality\" : \"1\"\n-    }\n-  ],\n-  \"Blueprints\" : {\n-    \"blueprint_name\" : \"single-node-hdfs-yarn\",\n-    \"stack_name\" : \"HDP\",\n-    \"stack_version\" : \"2.1\"\n-  }\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/main/resources/blueprints/single-node-hdfs-yarn",
                "sha": "768741b4c7014afbdb51a8389f07b0689d95f3bc",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/main/resources/blueprints/warmup",
                "changes": 94,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/main/resources/blueprints/warmup?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 94,
                "filename": "ambari-client/groovy-client/src/main/resources/blueprints/warmup",
                "patch": "@@ -1,94 +0,0 @@\n-{\n-  \"configurations\": [\n-    {\n-        \"nagios-env\" : {\n-            \"nagios_contact\" : \"admin@localhost\"\n-        }\n-    }\n-  ],\n-  \"host_groups\": [\n-    {\n-      \"name\": \"host_group_1\",\n-      \"components\": [\n-        {\n-          \"name\": \"ZOOKEEPER_SERVER\"\n-        },\n-        {\n-          \"name\": \"ZOOKEEPER_CLIENT\"\n-        },\n-        {\n-          \"name\": \"PIG\"\n-        },\n-        {\n-          \"name\": \"HISTORYSERVER\"\n-        },\n-        {\n-          \"name\": \"SUPERVISOR\"\n-        },\n-        {\n-          \"name\": \"NAGIOS_SERVER\"\n-        },\n-        {\n-          \"name\": \"TEZ_CLIENT\"\n-        },\n-        {\n-          \"name\": \"AMBARI_SERVER\"\n-        },\n-        {\n-          \"name\": \"APP_TIMELINE_SERVER\"\n-        },\n-        {\n-          \"name\": \"GANGLIA_SERVER\"\n-        },\n-        {\n-          \"name\": \"HDFS_CLIENT\"\n-        },\n-        {\n-          \"name\": \"NODEMANAGER\"\n-        },\n-        {\n-          \"name\": \"YARN_CLIENT\"\n-        },\n-        {\n-          \"name\" : \"APP_TIMELINE_SERVER\"\n-        },\n-        {\n-          \"name\": \"MAPREDUCE2_CLIENT\"\n-        },\n-        {\n-          \"name\": \"DATANODE\"\n-        },\n-        {\n-          \"name\": \"GANGLIA_MONITOR\"\n-        },\n-        {\n-          \"name\": \"RESOURCEMANAGER\"\n-        },\n-        {\n-          \"name\": \"STORM_REST_API\"\n-        },\n-        {\n-          \"name\": \"STORM_UI_SERVER\"\n-        },\n-        {\n-          \"name\": \"SECONDARY_NAMENODE\"\n-        },\n-        {\n-          \"name\": \"DRPC_SERVER\"\n-        },\n-        {\n-          \"name\": \"NIMBUS\"\n-        },\n-        {\n-          \"name\": \"NAMENODE\"\n-        }\n-      ],\n-      \"cardinality\": \"1\"\n-    }\n-  ],\n-  \"Blueprints\": {\n-    \"blueprint_name\": \"warmup\",\n-    \"stack_name\": \"HDP\",\n-    \"stack_version\": \"2.1\"\n-  }\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/main/resources/blueprints/warmup",
                "sha": "e6587c1d8356b687ecb53b8ffa9c3457dca32bdc",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariBlueprintsTest.groovy",
                "changes": 286,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariBlueprintsTest.groovy?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 286,
                "filename": "ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariBlueprintsTest.groovy",
                "patch": "@@ -1,286 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.ambari.groovy.client\n-\n-import groovy.json.JsonSlurper\n-import groovy.util.logging.Slf4j\n-\n-@Slf4j\n-class AmbariBlueprintsTest extends AbstractAmbariClientTest {\n-\n-  def slurper = new JsonSlurper()\n-\n-  private enum Scenario {\n-    CLUSTERS, NO_CLUSTERS, BLUEPRINT_EXISTS, NO_BLUEPRINT, HOSTS, NO_HOSTS\n-  }\n-\n-  def \"test get the name of the cluster\"() {\n-    given:\n-    mockResponses(Scenario.CLUSTERS.name())\n-\n-    when:\n-    def result = ambari.getClusterName()\n-\n-    then:\n-    \"MySingleNodeCluster\" == result\n-  }\n-\n-  def \"test get the name when there is no cluster\"() {\n-    given:\n-    mockResponses(Scenario.NO_CLUSTERS.name())\n-\n-    when:\n-    def result = ambari.getClusterName()\n-\n-    then:\n-    null == result\n-  }\n-\n-  def \"test blueprint doesn't exist\"() {\n-    given:\n-    mockResponses(Scenario.NO_BLUEPRINT.name())\n-\n-    when:\n-    def result = ambari.doesBlueprintExist(\"inexistent-blueprint\")\n-\n-    then:\n-    !result\n-  }\n-\n-  def \"test blueprint exists\"() {\n-    given:\n-    mockResponses(Scenario.BLUEPRINT_EXISTS.name())\n-\n-    when:\n-    def result = ambari.doesBlueprintExist(\"single-node-hdfs-yarn\")\n-\n-    then:\n-    result\n-  }\n-\n-  def \"test get blueprint as map\"() {\n-    given:\n-    mockResponses(Scenario.BLUEPRINT_EXISTS.name())\n-\n-    when:\n-    def response = ambari.getBlueprintMap(\"single-node-hdfs-yarn\")\n-\n-    then:\n-    response.keySet().size() == 1\n-    response.containsKey('host_group_1')\n-  }\n-\n-\n-  def \"test get blueprint as map when there's no blueprint\"() {\n-    given:\n-    mockResponses(Scenario.NO_BLUEPRINT.name())\n-    when:\n-    def response = ambari.getBlueprintMap(\"inexistent-blueprint\")\n-\n-    then:\n-    [:] == response\n-  }\n-\n-  def \"test get host groups\"() {\n-    given:\n-    mockResponses(Scenario.BLUEPRINT_EXISTS.name())\n-\n-    when:\n-    def result = ambari.getHostGroups(\"single-node-hdfs-yarn\")\n-\n-    then:\n-    [\"host_group_1\"] == result\n-  }\n-\n-  def \"test get host groups for no groups\"() {\n-    given:\n-    mockResponses(Scenario.NO_BLUEPRINT.name())\n-\n-    when:\n-    def result = ambari.getHostGroups(\"inexistent-blueprint\")\n-\n-    then:\n-    [] == result\n-  }\n-\n-  def \"test get host names\"() {\n-    given:\n-    mockResponses(Scenario.HOSTS.name())\n-\n-    when:\n-    def result = ambari.getHostNames()\n-\n-    then:\n-    \"UNHEALTHY\" == result[\"server.ambari.com\"]\n-    1 == result.size()\n-  }\n-\n-  def \"test get host names for empty result\"() {\n-    given:\n-    mockResponses(Scenario.NO_HOSTS.name())\n-\n-    when:\n-    def result = ambari.getHostNames()\n-\n-    then:\n-    [:] == result\n-  }\n-\n-  def \"test validate blueprint\"() {\n-    given:\n-    def json = getClass().getClassLoader().getResourceAsStream(\"blueprint.json\").text\n-\n-    when:\n-    ambari.validateBlueprint(json)\n-\n-    then:\n-    noExceptionThrown()\n-  }\n-\n-  def \"test validate blueprint no slaves_\"() {\n-    given:\n-    def json = getClass().getClassLoader().getResourceAsStream(\"hdp-multinode-default2.json\").text\n-\n-    when:\n-    ambari.validateBlueprint(json)\n-\n-    then:\n-    thrown(InvalidBlueprintException)\n-  }\n-\n-  def \"test validate blueprint with uppercase SLAVE_\"() {\n-    given:\n-    def json = getClass().getClassLoader().getResourceAsStream(\"hdp-multinode-default.json\").text\n-\n-    when:\n-    ambari.validateBlueprint(json)\n-\n-    then:\n-    notThrown(InvalidBlueprintException)\n-  }\n-\n-  def \"test validate blueprint for null json\"() {\n-    when:\n-    ambari.validateBlueprint(null)\n-\n-    then:\n-    thrown(InvalidBlueprintException)\n-  }\n-\n-  def \"test add blueprint with configuration\"() {\n-    given:\n-    def json = getClass().getClassLoader().getResourceAsStream(\"blueprint.json\").text\n-    ambari.metaClass.postBlueprint = { String blueprint -> return }\n-\n-    when:\n-    def config = [\n-      \"yarn-site\": [\"property-key\": \"property-value\", \"yarn.nodemanager.local-dirs\": \"/mnt/fs1/,/mnt/fs2/\"],\n-      \"hdfs-site\": [\"dfs.datanode.data.dir\": \"/mnt/fs1/,/mnt/fs2/\"]\n-    ]\n-    def blueprint = ambari.addBlueprint(json, config)\n-\n-    then:\n-    def expected = slurper.parseText(getClass().getClassLoader().getResourceAsStream(\"blueprint-config.json\").text)\n-    def actual = slurper.parseText(blueprint)\n-    actual == expected\n-  }\n-\n-  def \"test add blueprint with existing configuration\"() {\n-    given:\n-    def json = getClass().getClassLoader().getResourceAsStream(\"multi-node-hdfs-yarn.json\").text\n-    ambari.metaClass.postBlueprint = { String blueprint -> return }\n-\n-    when:\n-    def config = [\n-      \"yarn-site\": [\"property-key\": \"property-value\", \"yarn.nodemanager.local-dirs\": \"apple\"],\n-      \"hdfs-site\": [\"dfs.datanode.data.dir\": \"/mnt/fs1/,/mnt/fs2/\"],\n-      \"core-site\": [\"fs.defaultFS\": \"localhost:9000\"]\n-    ]\n-    def blueprint = ambari.addBlueprint(json, config)\n-\n-    then:\n-    def expected = slurper.parseText(getClass().getClassLoader().getResourceAsStream(\"multi-node-hdfs-yarn-config.json\").text)\n-    def actual = slurper.parseText(blueprint)\n-    actual == expected\n-  }\n-\n-  def \"test add blueprint with empty configuration\"() {\n-    given:\n-    def json = getClass().getClassLoader().getResourceAsStream(\"blueprint.json\").text\n-    ambari.metaClass.postBlueprint = { String blueprint -> return }\n-\n-    when:\n-    def blueprint = ambari.addBlueprint(json, [:])\n-\n-    then:\n-    def expected = slurper.parseText(json)\n-    def actual = slurper.parseText(blueprint)\n-    actual == expected\n-  }\n-\n-  def protected String selectResponseJson(Map resourceRequestMap, String scenarioStr) {\n-    def thePath = resourceRequestMap.get(\"path\");\n-    def query = resourceRequestMap.get(\"query\");\n-    def Scenario scenario = Scenario.valueOf(scenarioStr)\n-    def json = null\n-    if (thePath == TestResources.CLUSTERS.uri()) {\n-      switch (scenario) {\n-        case Scenario.CLUSTERS: json = \"clusters.json\"\n-          break\n-        case Scenario.NO_CLUSTERS: json = \"no-clusters.json\"\n-          break\n-      }\n-    } else if (thePath == TestResources.BLUEPRINTS.uri) {\n-      switch (scenario) {\n-        case Scenario.BLUEPRINT_EXISTS: json = \"blueprints.json\"\n-          break\n-        case Scenario.NO_BLUEPRINT: json = \"no-blueprints.json\"\n-          break\n-      }\n-    } else if (thePath == TestResources.BLUEPRINT.uri) {\n-      switch (scenario) {\n-        case Scenario.BLUEPRINT_EXISTS: json = \"blueprint.json\"\n-          break\n-        case Scenario.NO_BLUEPRINT: json = \"no-blueprint.json\"\n-          break\n-      }\n-    } else if (thePath == TestResources.INEXISTENT_BLUEPRINT.uri) {\n-      switch (scenario) {\n-        case Scenario.NO_BLUEPRINT: json = \"no-blueprint.json\"\n-          break\n-      }\n-    } else if (thePath == TestResources.CONFIGURATIONS.uri()) {\n-      if (query) {\n-        json = \"service-config.json\"\n-      } else {\n-        json = \"service-versions.json\"\n-      }\n-    } else if (thePath == TestResources.HOSTS.uri()) {\n-      switch (scenario) {\n-        case Scenario.HOSTS: json = \"hosts.json\"\n-          break\n-        case Scenario.NO_HOSTS: json = \"no-hosts.json\"\n-          break\n-      }\n-    } else {\n-      log.error(\"Unsupported resource path: {}\", thePath)\n-    }\n-    return json\n-  }\n-\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariBlueprintsTest.groovy",
                "sha": "d47b1a2e336ad40fbe29f3477e159c081c3a5f6f",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariClustersTest.groovy",
                "changes": 65,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariClustersTest.groovy?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 65,
                "filename": "ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariClustersTest.groovy",
                "patch": "@@ -1,65 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.ambari.groovy.client\n-\n-import groovy.util.logging.Slf4j\n-\n-@Slf4j\n-class AmbariClustersTest extends AbstractAmbariClientTest {\n-\n-  private enum Scenario {\n-    CLUSTERS, CLUSTER\n-  }\n-\n-  def \"test get cluster as JSON\"() {\n-    given:\n-    mockResponses(Scenario.CLUSTER.name())\n-\n-    expect:\n-    String json = ambari.getClusterAsJson();\n-    log.debug(\"JSON: {}\", json)\n-\n-  }\n-\n-  def \"test get clusters as JSON\"() {\n-    given:\n-    mockResponses(Scenario.CLUSTERS.name())\n-\n-    expect:\n-    String json = ambari.getClustersAsJson();\n-    log.debug(\"JSON: {}\", json)\n-  }\n-\n-  def protected String selectResponseJson(Map resourceRequestMap, String scenarioStr) {\n-    def thePath = resourceRequestMap.get(\"path\");\n-    def query = resourceRequestMap.get(\"query\");\n-    def Scenario scenario = Scenario.valueOf(scenarioStr)\n-    def json = null\n-    if (thePath == TestResources.CLUSTERS.uri()) {\n-      json = \"clusters.json\"\n-    } else if (thePath == TestResources.CLUSTER.uri()) {\n-      switch (scenario) {\n-        case Scenario.CLUSTER: json = \"clusterAll.json\"\n-          break\n-      }\n-    } else {\n-      log.error(\"Unsupported resource path: {}\", thePath)\n-    }\n-    return json\n-  }\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariClustersTest.groovy",
                "sha": "e6aee4b121dbd9f9ce5ef762bea4232c2420757b",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariHostsTest.groovy",
                "changes": 112,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariHostsTest.groovy?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 112,
                "filename": "ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariHostsTest.groovy",
                "patch": "@@ -1,112 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.ambari.groovy.client\n-\n-import groovy.util.logging.Slf4j\n-\n-@Slf4j\n-class AmbariHostsTest extends AbstractAmbariClientTest {\n-\n-  private enum Scenario {\n-    CLUSTERS\n-  }\n-\n-  def \"test get host components as map when there is no cluster yet\"() {\n-    given:\n-    ambari.metaClass.getHostComponenets = { return null }\n-\n-    when:\n-    def result = ambari.getHostComponentsMap(\"host\")\n-\n-    then:\n-    [:] == result\n-  }\n-\n-  def \"test get host components as map when there is a cluster\"() {\n-    given:\n-    mockResponses(Scenario.CLUSTERS.name())\n-\n-    when:\n-    def result = ambari.getHostComponentsMap(\"host\")\n-\n-    then:\n-    [\"DATANODE\"          : \"STARTED\",\n-     \"HDFS_CLIENT\"       : \"INSTALLED\",\n-     \"HISTORYSERVER\"     : \"STARTED\",\n-     \"MAPREDUCE2_CLIENT\" : \"INSTALLED\",\n-     \"NAMENODE\"          : \"STARTED\",\n-     \"NODEMANAGER\"       : \"STARTED\",\n-     \"RESOURCEMANAGER\"   : \"STARTED\",\n-     \"SECONDARY_NAMENODE\": \"STARTED\",\n-     \"YARN_CLIENT\"       : \"INSTALLED\",\n-     \"ZOOKEEPER_CLIENT\"  : \"INSTALLED\",\n-     \"ZOOKEEPER_SERVER\"  : \"STARTED\"\n-    ] == result\n-  }\n-\n-  def \"install host components to a host from an existing valid blueprint\"() {\n-    given:\n-    mockResponses(Scenario.CLUSTERS.name())\n-    ambari.metaClass.addComponentToHost = { String host, String component -> return null }\n-    ambari.metaClass.setComponentState = { String host, String component, String state -> return 10 }\n-\n-    when:\n-    def result = ambari.installComponentsToHost(\"amb0\", \"hdp-multinode-default\", \"slave_1\")\n-\n-    then:\n-    [\n-      \"HBASE_REGIONSERVER\": 10,\n-      \"NODEMANAGER\"       : 10,\n-      \"DATANODE\"          : 10,\n-      \"GANGLIA_MONITOR\"   : 10\n-    ] == result\n-  }\n-\n-  def \"install host components to a host from an existing valid blueprint but invalid group\"() {\n-    given:\n-    mockResponses(Scenario.CLUSTERS.name())\n-    ambari.metaClass.addComponentToHost = { String host, String component -> return null }\n-    ambari.metaClass.setComponentState = { String host, String component, String state -> return null }\n-\n-    when:\n-    def result = ambari.installComponentsToHost(\"amb0\", \"hdp-multinode-default\", \"slave_2\")\n-\n-    then:\n-    [:] == result\n-  }\n-\n-  def protected String selectResponseJson(Map resourceRequestMap, String scenarioStr) {\n-    def thePath = resourceRequestMap.get(\"path\");\n-    def Scenario scenario = Scenario.valueOf(scenarioStr)\n-    def json = null\n-    if (thePath == TestResources.CLUSTERS.uri()) {\n-      switch (scenario) {\n-        case Scenario.CLUSTERS: json = \"clusters.json\"\n-          break\n-      }\n-    } else if (thePath == TestResources.HOST_COMPONENTS.uri()) {\n-      json = \"host-components.json\"\n-    } else if (thePath == TestResources.BLUEPRINT_MULTI.uri) {\n-      json = \"hdp-multinode-default.json\"\n-    } else {\n-      log.error(\"Unsupported resource path: {}\", thePath)\n-    }\n-    return json\n-  }\n-\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariHostsTest.groovy",
                "sha": "7efb95dd07530ac7baf1f2ebd613b7baff03056d",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariRecommendTest.groovy",
                "changes": 150,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariRecommendTest.groovy?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 150,
                "filename": "ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariRecommendTest.groovy",
                "patch": "@@ -1,150 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.ambari.groovy.client\n-\n-import groovy.util.logging.Slf4j\n-\n-@Slf4j\n-class AmbariRecommendTest extends AbstractAmbariClientTest {\n-\n-  private enum Scenario {\n-    SINGLE_NODE_BLUEPRINT, MULTI_NODE_BLUEPRINT, MULTI_NODE_BLUEPRINT2\n-  }\n-\n-  def \"test recommend for single node\"() {\n-    given:\n-    mockResponses(Scenario.SINGLE_NODE_BLUEPRINT.name())\n-    ambari.metaClass.getHostNames = { return [\"amb0\": \"HEALTHY\"] }\n-\n-    when:\n-    def result = ambari.recommendAssignments(\"single-node-hdfs-yarn\")\n-\n-    then:\n-    [host_group_1: [\"amb0\"]] == result\n-  }\n-\n-  def \"test recommend for invalid host number\"() {\n-    given:\n-    mockResponses(Scenario.MULTI_NODE_BLUEPRINT.name())\n-    ambari.metaClass.getHostNames = { return [\"amb0\": \"HEALTHY\"] }\n-\n-    when:\n-    def result\n-    try {\n-      result = ambari.recommendAssignments(\"hdp-multinode-default\")\n-    } catch (InvalidHostGroupHostAssociation e) {\n-      result = e.getMinRequiredHost()\n-    }\n-\n-    then:\n-    result == 7\n-  }\n-\n-  def \"test recommend for no slave group\"() {\n-    given:\n-    mockResponses(Scenario.MULTI_NODE_BLUEPRINT2.name())\n-    ambari.metaClass.getHostNames = {\n-      return [\n-        \"amb0\": \"HEALTHY\",\n-        \"amb1\": \"HEALTHY\",\n-        \"amb2\": \"HEALTHY\",\n-        \"amb3\": \"HEALTHY\",\n-        \"amb4\": \"HEALTHY\",\n-        \"amb5\": \"HEALTHY\",\n-        \"amb6\": \"HEALTHY\",\n-        \"amb7\": \"HEALTHY\",\n-        \"amb8\": \"HEALTHY\",\n-        \"amb9\": \"HEALTHY\",\n-        \"am10\": \"HEALTHY\",\n-        \"am10\": \"HEALTHY\",\n-        \"am20\": \"HEALTHY\",\n-        \"am30\": \"HEALTHY\",\n-        \"am40\": \"HEALTHY\",\n-      ]\n-    }\n-\n-    when:\n-    def result\n-    def msg\n-    try {\n-      result = ambari.recommendAssignments(\"hdp-multinode-default2\")\n-    } catch (InvalidHostGroupHostAssociation e) {\n-      msg = e.getMessage()\n-      result = e.getMinRequiredHost()\n-    }\n-\n-    then:\n-    result == 5\n-    msg == \"At least one 'slave_' is required\"\n-  }\n-\n-  def \"test recommend for multi node\"() {\n-    given:\n-    mockResponses(Scenario.MULTI_NODE_BLUEPRINT.name())\n-    ambari.metaClass.getHostNames = {\n-      return [\n-        \"amb0\": \"HEALTHY\",\n-        \"amb1\": \"HEALTHY\",\n-        \"amb2\": \"HEALTHY\",\n-        \"amb3\": \"HEALTHY\",\n-        \"amb4\": \"HEALTHY\",\n-        \"amb5\": \"HEALTHY\",\n-        \"amb6\": \"HEALTHY\",\n-        \"amb7\": \"HEALTHY\",\n-        \"amb8\": \"HEALTHY\",\n-        \"amb9\": \"HEALTHY\",\n-        \"am10\": \"HEALTHY\",\n-        \"am10\": \"HEALTHY\",\n-        \"am20\": \"HEALTHY\",\n-        \"am30\": \"HEALTHY\",\n-        \"am40\": \"HEALTHY\",\n-      ]\n-    }\n-\n-    when:\n-    def result = ambari.recommendAssignments(\"hdp-multinode-default\")\n-\n-    then:\n-    [master_1: [\"amb0\"],\n-     master_2: [\"amb1\"],\n-     master_3: [\"amb2\"],\n-     master_4: [\"amb3\"],\n-     gateway : [\"amb4\"],\n-     slave_1 : [\"amb5\", \"amb7\", \"amb9\", \"am20\", \"am40\"],\n-     SLAVE_2 : [\"amb6\", \"amb8\", \"am10\", \"am30\"]\n-    ] == result\n-  }\n-\n-  def protected String selectResponseJson(Map resourceRequestMap, String scenarioStr) {\n-    def thePath = resourceRequestMap.get(\"path\");\n-    def query = resourceRequestMap.get(\"query\");\n-    def Scenario scenario = Scenario.valueOf(scenarioStr)\n-    def json = null\n-    if (thePath == TestResources.BLUEPRINT.uri()) {\n-      json = \"blueprint.json\"\n-    } else if (thePath == TestResources.BLUEPRINT_MULTI.uri()) {\n-      json = \"hdp-multinode-default.json\"\n-    } else if (thePath == TestResources.BLUEPRINT_MULTI2.uri()) {\n-      json = \"hdp-multinode-default2.json\"\n-    } else {\n-      log.error(\"Unsupported resource path: {}\", thePath)\n-    }\n-    return json\n-  }\n-\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariRecommendTest.groovy",
                "sha": "9f7ff032096cf4ad8cf5e47cf9f6be5a527128d6",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariServiceConfigurationTest.groovy",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariServiceConfigurationTest.groovy?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 83,
                "filename": "ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariServiceConfigurationTest.groovy",
                "patch": "@@ -1,83 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.ambari.groovy.client\n-\n-import groovy.util.logging.Slf4j\n-\n-@Slf4j\n-class AmbariServiceConfigurationTest extends AbstractAmbariClientTest {\n-\n-  private enum Scenario {\n-    CONFIGURATIONS, MULTIPLE_VERSIONS\n-  }\n-\n-  def \"test request service configurations map\"() {\n-    given:\n-    mockResponses(Scenario.CONFIGURATIONS.name())\n-\n-    when:\n-    Map<String, Map<String, String>> serviceConfigMap = ambari.getServiceConfigMap();\n-\n-    then:\n-    serviceConfigMap != [:]\n-    serviceConfigMap.get(\"yarn-site\") != [:]\n-  }\n-\n-  def \"test request service configurations with multiple versions\"() {\n-    given:\n-    mockResponses(Scenario.MULTIPLE_VERSIONS.name())\n-\n-    when:\n-    Map<String, Map<String, String>> serviceConfigMap = ambari.getServiceConfigMap();\n-\n-    then:\n-    serviceConfigMap != [:]\n-    serviceConfigMap.get(\"yarn-site\") != [:]\n-  }\n-\n-  // ---- helper method definitions\n-\n-  def protected String selectResponseJson(Map resourceRequestMap, String scenarioStr) {\n-    def thePath = resourceRequestMap.get(\"path\")\n-    def theQuery = resourceRequestMap.get(\"query\")\n-    def Scenario scenario = Scenario.valueOf(scenarioStr)\n-\n-    def json = null\n-    if (thePath == TestResources.CLUSTERS.uri()) {\n-      json = \"clusters.json\"\n-    } else if (thePath == TestResources.CONFIGURATIONS.uri()) {\n-      if (!theQuery) {\n-        switch (scenario) {\n-          case Scenario.MULTIPLE_VERSIONS:\n-            json = \"service-versions-multiple.json\"\n-            break\n-          case Scenario.CONFIGURATIONS:\n-            json = \"service-versions.json\"\n-            break\n-        }\n-      } else {\n-        json = \"service-config.json\"\n-      }\n-\n-    } else {\n-      log.error(\"Unsupported resource path: {}\", thePath)\n-    }\n-    return json\n-  }\n-\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariServiceConfigurationTest.groovy",
                "sha": "909955af2c1381562e171b16c674c26dda200699",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariServicesTest.groovy",
                "changes": 210,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariServicesTest.groovy?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 210,
                "filename": "ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariServicesTest.groovy",
                "patch": "@@ -1,210 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.ambari.groovy.client\n-\n-import groovy.json.JsonSlurper\n-import groovy.util.logging.Slf4j\n-\n-@Slf4j\n-class AmbariServicesTest extends AbstractAmbariClientTest {\n-\n-  def slurper = new JsonSlurper()\n-\n-  private enum Scenario {\n-    SERVICES, NO_SERVICES, NO_SERVICE_COMPONENTS\n-  }\n-\n-  def \"test get service components map\"() {\n-    given:\n-    // get the name of the cluster\n-    super.mockResponses(Scenario.SERVICES.name())\n-\n-    when:\n-    def Map result = ambari.getServiceComponentsMap()\n-\n-    then:\n-    [HDFS: [\n-      DATANODE          : \"STARTED\",\n-      HDFS_CLIENT       : \"INSTALLED\",\n-      NAMENODE          : \"STARTED\",\n-      SECONDARY_NAMENODE: \"STARTED\"]\n-    ] == result\n-  }\n-\n-\n-  def \"test get service components map for empty services\"() {\n-    given:\n-    mockResponses(Scenario.NO_SERVICES.name())\n-\n-    when:\n-    def Map result = ambari.getServiceComponentsMap()\n-\n-    then:\n-    [:] == result\n-  }\n-\n-  def \"test get service components map for empty components\"() {\n-    given:\n-    // get the name of the cluster\n-    mockResponses(Scenario.NO_SERVICE_COMPONENTS.name())\n-\n-    when:\n-    def Map result = ambari.getServiceComponentsMap()\n-\n-    then:\n-    [HDFS: [:]] == result\n-  }\n-\n-  def \"test get services as map\"() {\n-    given:\n-    super.mockResponses(Scenario.SERVICES.name())\n-\n-    when:\n-    def result = ambari.getServicesMap()\n-\n-    then:\n-    [HDFS: \"STARTED\"] == result\n-  }\n-\n-  def \"test get services as map for empty result\"() {\n-    given:\n-    mockResponses(Scenario.NO_SERVICES.name())\n-\n-    when:\n-    def result = ambari.getServicesMap()\n-\n-    then:\n-    [:] == result\n-  }\n-\n-\n-  def \"test services started\"() {\n-    given:\n-    // get the name of the cluster\n-    super.mockResponses(Scenario.SERVICES.name())\n-\n-    when:\n-    def boolean result = ambari.servicesStarted()\n-\n-    then:\n-    result\n-  }\n-\n-  def \"test services stopped\"() {\n-    given:\n-    // get the name of the cluster\n-    super.mockResponses(Scenario.SERVICES.name())\n-\n-    when:\n-    def boolean result = ambari.servicesStopped()\n-\n-    then:\n-    !result\n-  }\n-\n-  def \"test stop all services\"() {\n-    given:\n-    def context\n-    ambari.metaClass.getClusterName = { return \"cluster\" }\n-    ambari.getAmbari().metaClass.put = { Map request ->\n-      context = request\n-    }\n-    ambari.getSlurper().metaClass.parseText { String text -> return [\"Requests\": [\"id\": 1]] }\n-\n-    when:\n-    def id = ambari.stopAllServices()\n-\n-    then:\n-    1 == id\n-    context.path == \"http://localhost:8080/api/v1/clusters/cluster/services\"\n-    def body = slurper.parseText(context.body)\n-    body.RequestInfo.context == \"Stop All Services\"\n-    body.ServiceInfo.state == \"INSTALLED\"\n-  }\n-\n-  def \"test start service ZOOKEEPER\"() {\n-    given:\n-    def context\n-    ambari.metaClass.getClusterName = { return \"cluster\" }\n-    ambari.getAmbari().metaClass.put = { Map request ->\n-      context = request\n-    }\n-    ambari.getSlurper().metaClass.parseText { String text -> return [\"Requests\": [\"id\": 1]] }\n-\n-    when:\n-    def id = ambari.startService(\"ZOOKEEPER\")\n-\n-    then:\n-    1 == id\n-    context.path == \"http://localhost:8080/api/v1/clusters/cluster/services/ZOOKEEPER\"\n-    def body = slurper.parseText(context.body)\n-    body.RequestInfo.context == \"Starting ZOOKEEPER\"\n-    body.ServiceInfo.state == \"STARTED\"\n-  }\n-\n-  def \"test stop service ZOOKEEPER\"() {\n-    given:\n-    def context\n-    ambari.metaClass.getClusterName = { return \"cluster\" }\n-    ambari.getAmbari().metaClass.put = { Map request ->\n-      context = request\n-    }\n-    ambari.getSlurper().metaClass.parseText { String text -> return [\"Requests\": [\"id\": 1]] }\n-\n-    when:\n-    def id = ambari.stopService(\"ZOOKEEPER\")\n-\n-    then:\n-    1 == id\n-    context.path == \"http://localhost:8080/api/v1/clusters/cluster/services/ZOOKEEPER\"\n-    def body = slurper.parseText(context.body)\n-    body.RequestInfo.context == \"Stopping ZOOKEEPER\"\n-    body.ServiceInfo.state == \"INSTALLED\"\n-  }\n-\n-  def private String selectResponseJson(Map resourceRequestMap, String scenarioStr) {\n-    def thePath = resourceRequestMap.get(\"path\");\n-    def Scenario scenario = Scenario.valueOf(scenarioStr)\n-\n-    def json = null\n-    if (thePath == TestResources.CLUSTERS.uri()) {\n-      json = \"clusters.json\"\n-    } else if (thePath == TestResources.SERVICES.uri()) {\n-      switch (scenario) {\n-        case Scenario.SERVICES: json = \"services.json\"\n-          break\n-        case Scenario.NO_SERVICES: json = \"no-services.json\"\n-          break\n-        case Scenario.NO_SERVICE_COMPONENTS:\n-          json = \"services.json\"\n-          break\n-      }\n-    } else if (thePath == TestResources.SERVICE_COMPONENTS.uri()) {\n-      switch (scenario) {\n-        case Scenario.NO_SERVICE_COMPONENTS: json = \"no-service-components-hdfs.json\"\n-          break\n-        default:\n-          json = \"service-components-hdfs.json\"\n-          break\n-      }\n-    } else {\n-      log.error(\"Unsupported resource path: {}\", thePath)\n-    }\n-    return json\n-  }\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariServicesTest.groovy",
                "sha": "e798789a4344ad7ca8d3505260be1df4d61adbbb",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariTasksTest.groovy",
                "changes": 93,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariTasksTest.groovy?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 93,
                "filename": "ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariTasksTest.groovy",
                "patch": "@@ -1,93 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.ambari.groovy.client\n-\n-import groovy.util.logging.Slf4j\n-\n-@Slf4j\n-class AmbariTasksTest extends AbstractAmbariClientTest {\n-\n-  private enum Scenario {\n-    TASKS, NO_TASKS\n-  }\n-\n-  def \"test get task as map\"() {\n-    given:\n-    mockResponses(Scenario.TASKS.name())\n-\n-    when:\n-    def result = ambari.getTaskMap()\n-\n-    then:\n-    [\n-      'DATANODE INSTALL'          : 'COMPLETED',\n-      'GANGLIA_MONITOR INSTALL'   : 'COMPLETED',\n-      'GANGLIA_SERVER INSTALL'    : 'COMPLETED',\n-      'HDFS_CLIENT INSTALL'       : 'COMPLETED',\n-      'HISTORYSERVER INSTALL'     : 'COMPLETED',\n-      'MAPREDUCE2_CLIENT INSTALL' : 'COMPLETED',\n-      'NAMENODE INSTALL'          : 'COMPLETED',\n-      'NODEMANAGER INSTALL'       : 'COMPLETED',\n-      'RESOURCEMANAGER INSTALL'   : 'COMPLETED',\n-      'SECONDARY_NAMENODE INSTALL': 'COMPLETED',\n-      'YARN_CLIENT INSTALL'       : 'COMPLETED',\n-      'ZOOKEEPER_CLIENT INSTALL'  : 'COMPLETED',\n-      'ZOOKEEPER_SERVER INSTALL'  : 'COMPLETED',\n-      'DATANODE START'            : 'COMPLETED',\n-      'GANGLIA_MONITOR START'     : 'COMPLETED',\n-      'GANGLIA_SERVER START'      : 'COMPLETED',\n-      'NAMENODE START'            : 'COMPLETED',\n-      'ZOOKEEPER_SERVER START'    : 'COMPLETED',\n-      'HISTORYSERVER START'       : 'COMPLETED',\n-      'RESOURCEMANAGER START'     : 'COMPLETED',\n-      'SECONDARY_NAMENODE START'  : 'COMPLETED',\n-      'NODEMANAGER START'         : 'COMPLETED'] == result\n-  }\n-\n-  def \"test get task as map for no tasks\"() {\n-    given:\n-    mockResponses(Scenario.NO_TASKS.name())\n-\n-    when:\n-    def result = ambari.getTaskMap()\n-\n-    then:\n-    [:] == result\n-  }\n-\n-  def protected String selectResponseJson(Map resourceRequestMap, String scenarioStr) {\n-    def thePath = resourceRequestMap.get(\"path\");\n-    def Scenario scenario = Scenario.valueOf(scenarioStr)\n-\n-    def json = null\n-    if (thePath == TestResources.CLUSTERS.uri()) {\n-      json = \"clusters.json\"\n-    } else if (thePath == TestResources.TASKS.uri()) {\n-      switch (scenario) {\n-        case Scenario.TASKS: json = \"request-tasks.json\"\n-          break\n-        case Scenario.NO_TASKS: json = \"no-request-tasks.json\"\n-          break\n-      }\n-    } else {\n-      log.error(\"Unsupported resource path: {}\", thePath)\n-    }\n-    return json\n-  }\n-\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/AmbariTasksTest.groovy",
                "sha": "8b5e47fa69a5997f6186601eef39368abfac0e2c",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/TestResources.groovy",
                "changes": 45,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/TestResources.groovy?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 45,
                "filename": "ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/TestResources.groovy",
                "patch": "@@ -1,45 +0,0 @@\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *     http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing, software\n- * distributed under the License is distributed on an \"AS IS\" BASIS,\n- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- * See the License for the specific language governing permissions and\n- * limitations under the License.\n- */\n-package org.apache.ambari.groovy.client\n-\n-enum TestResources {\n-  CLUSTERS(\"http://localhost:8080/api/v1/clusters\"),\n-  CLUSTER(\"http://localhost:8080/api/v1/clusters/MySingleNodeCluster\"),\n-  CONFIGURATIONS(\"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/configurations\"),\n-  BLUEPRINTS(\"http://localhost:8080/api/v1/blueprints\"),\n-  BLUEPRINT(\"http://localhost:8080/api/v1/blueprints/single-node-hdfs-yarn\"),\n-  BLUEPRINT_MULTI(\"http://localhost:8080/api/v1/blueprints/hdp-multinode-default\"),\n-  BLUEPRINT_MULTI2(\"http://localhost:8080/api/v1/blueprints/hdp-multinode-default2\"),\n-  INEXISTENT_BLUEPRINT(\"http://localhost:8080/api/v1/blueprints/inexistent-blueprint\"),\n-  HOSTS(\"http://localhost:8080/api/v1/hosts\"),\n-  TASKS(\"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/requests/1\"),\n-  SERVICES(\"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/services\"),\n-  SERVICE_COMPONENTS(\"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/services/HDFS/components\"),\n-  HOST_COMPONENTS(\"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/hosts/host/host_components\")\n-\n-  String uri;\n-\n-  TestResources(String uri) {\n-    this.uri = uri\n-  }\n-\n-  public uri() {\n-    return this.uri\n-  }\n-\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/groovy/org/apache/ambari/groovy/client/TestResources.groovy",
                "sha": "716d700f9c7cf3077bd158b2e1d9e5b10b4155a7",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/blueprint-config.json",
                "changes": 61,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/blueprint-config.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 61,
                "filename": "ambari-client/groovy-client/src/test/resources/blueprint-config.json",
                "patch": "@@ -1,61 +0,0 @@\n-{\n-    \"Blueprints\": {\n-        \"blueprint_name\": \"single-node-hdfs-yarn\",\n-        \"stack_version\": \"2.0\",\n-        \"stack_name\": \"HDP\"\n-    },\n-    \"configurations\": [\n-        {\n-            \"yarn-site\": {\n-                \"property-key\": \"property-value\",\n-                \"yarn.nodemanager.local-dirs\": \"/mnt/fs1/,/mnt/fs2/\"\n-            }\n-        },\n-        {\n-            \"hdfs-site\": {\n-                \"dfs.datanode.data.dir\": \"/mnt/fs1/,/mnt/fs2/\"\n-            }\n-        }\n-    ],\n-    \"host_groups\": [\n-        {\n-            \"name\": \"host_group_1\",\n-            \"components\": [\n-                {\n-                    \"name\": \"NAMENODE\"\n-                },\n-                {\n-                    \"name\": \"SECONDARY_NAMENODE\"\n-                },\n-                {\n-                    \"name\": \"DATANODE\"\n-                },\n-                {\n-                    \"name\": \"HDFS_CLIENT\"\n-                },\n-                {\n-                    \"name\": \"RESOURCEMANAGER\"\n-                },\n-                {\n-                    \"name\": \"NODEMANAGER\"\n-                },\n-                {\n-                    \"name\": \"YARN_CLIENT\"\n-                },\n-                {\n-                    \"name\": \"HISTORYSERVER\"\n-                },\n-                {\n-                    \"name\": \"MAPREDUCE2_CLIENT\"\n-                },\n-                {\n-                    \"name\": \"ZOOKEEPER_SERVER\"\n-                },\n-                {\n-                    \"name\": \"ZOOKEEPER_CLIENT\"\n-                }\n-            ],\n-            \"cardinality\": \"1\"\n-        }\n-    ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/blueprint-config.json",
                "sha": "bf3f67d4f54a2ad8969ccaf74366601e42e4d092",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/blueprint.json",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/blueprint.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 48,
                "filename": "ambari-client/groovy-client/src/test/resources/blueprint.json",
                "patch": "@@ -1,48 +0,0 @@\n-{\n-  \"host_groups\" : [\n-    {\n-      \"name\" : \"host_group_1\",\n-      \"components\" : [\n-      {\n-        \"name\" : \"NAMENODE\"\n-      },\n-      {\n-        \"name\" : \"SECONDARY_NAMENODE\"\n-      },       \n-      {\n-        \"name\" : \"DATANODE\"\n-      },\n-      {\n-        \"name\" : \"HDFS_CLIENT\"\n-      },\n-      {\n-        \"name\" : \"RESOURCEMANAGER\"\n-      },\n-      {\n-        \"name\" : \"NODEMANAGER\"\n-      },\n-      {\n-        \"name\" : \"YARN_CLIENT\"\n-      },\n-      {\n-        \"name\" : \"HISTORYSERVER\"\n-      },\n-      {\n-        \"name\" : \"MAPREDUCE2_CLIENT\"\n-      },\n-      {\n-        \"name\" : \"ZOOKEEPER_SERVER\"\n-      },\n-      {\n-        \"name\" : \"ZOOKEEPER_CLIENT\"\n-      }\n-      ],\n-      \"cardinality\" : \"1\"\n-    }\n-  ],\n-  \"Blueprints\" : {\n-    \"blueprint_name\" : \"single-node-hdfs-yarn\",\n-    \"stack_name\" : \"HDP\",\n-    \"stack_version\" : \"2.0\"\n-  }\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/blueprint.json",
                "sha": "1a7d302c162ee5fb91866766ec1a1fd18fd98ed3",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/blueprints.json",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/blueprints.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 21,
                "filename": "ambari-client/groovy-client/src/test/resources/blueprints.json",
                "patch": "@@ -1,21 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:49233/api/v1/blueprints?fields=Blueprints\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:49233/api/v1/blueprints/multi-node-hdfs-yarn\",\n-      \"Blueprints\" : {\n-        \"blueprint_name\" : \"multi-node-hdfs-yarn\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"2.0\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:49233/api/v1/blueprints/single-node-hdfs-yarn\",\n-      \"Blueprints\" : {\n-        \"blueprint_name\" : \"single-node-hdfs-yarn\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"2.0\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/blueprints.json",
                "sha": "208a95463ba07b3d932c2888430f5619fded0617",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/cluster.json",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/cluster.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 16,
                "filename": "ambari-client/groovy-client/src/test/resources/cluster.json",
                "patch": "@@ -1,16 +0,0 @@\n-{\n-    \"blueprint\": \"c1\",\n-    \"host-groups\": [\n-        {\n-            \"name\": \"host_group_1\",\n-            \"hosts\": [\n-                {\n-                    \"fqdn\": \"server.ambari.com\"\n-                },\n-                {\n-                    \"fqdn\": \"server2.ambari.com\"\n-                }\n-            ]\n-        }\n-    ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/cluster.json",
                "sha": "886a960f6d4efb1c8a103fec5aa4bdbaffd093e4",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/clusterAll.json",
                "changes": 194,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/clusterAll.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 194,
                "filename": "ambari-client/groovy-client/src/test/resources/clusterAll.json",
                "patch": "@@ -1,194 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/clusters/MySingleNodeCluster\",\n-  \"Clusters\" : {\n-    \"cluster_id\" : 2,\n-    \"cluster_name\" : \"MySingleNodeCluster\",\n-    \"version\" : \"HDP-2.0\",\n-    \"desired_configs\" : {\n-      \"capacity-scheduler\" : {\n-        \"user\" : \"admin\",\n-        \"tag\" : \"1\"\n-      },\n-      \"core-site\" : {\n-        \"user\" : \"admin\",\n-        \"tag\" : \"1\"\n-      },\n-      \"global\" : {\n-        \"user\" : \"admin\",\n-        \"tag\" : \"1\"\n-      },\n-      \"hadoop-policy\" : {\n-        \"user\" : \"admin\",\n-        \"tag\" : \"1\"\n-      },\n-      \"hdfs-log4j\" : {\n-        \"user\" : \"admin\",\n-        \"tag\" : \"1\"\n-      },\n-      \"hdfs-site\" : {\n-        \"user\" : \"admin\",\n-        \"tag\" : \"1\"\n-      },\n-      \"mapred-queue-acls\" : {\n-        \"user\" : \"admin\",\n-        \"tag\" : \"1\"\n-      },\n-      \"mapred-site\" : {\n-        \"user\" : \"admin\",\n-        \"tag\" : \"1\"\n-      },\n-      \"yarn-log4j\" : {\n-        \"user\" : \"admin\",\n-        \"tag\" : \"1\"\n-      },\n-      \"yarn-site\" : {\n-        \"user\" : \"admin\",\n-        \"tag\" : \"1\"\n-      },\n-      \"zookeeper-log4j\" : {\n-        \"user\" : \"admin\",\n-        \"tag\" : \"1\"\n-      }\n-    }\n-  },\n-  \"requests\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/requests/1\",\n-      \"Requests\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\",\n-        \"id\" : 1\n-      }\n-    }\n-  ],\n-  \"config_groups\" : [ ],\n-  \"services\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/services/HDFS\",\n-      \"ServiceInfo\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\",\n-        \"service_name\" : \"HDFS\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/services/MAPREDUCE2\",\n-      \"ServiceInfo\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\",\n-        \"service_name\" : \"MAPREDUCE2\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/services/YARN\",\n-      \"ServiceInfo\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\",\n-        \"service_name\" : \"YARN\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/services/ZOOKEEPER\",\n-      \"ServiceInfo\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\",\n-        \"service_name\" : \"ZOOKEEPER\"\n-      }\n-    }\n-  ],\n-  \"workflows\" : [ ],\n-  \"hosts\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/hosts/server.ambari.com\",\n-      \"Hosts\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\",\n-        \"host_name\" : \"server.ambari.com\"\n-      }\n-    }\n-  ],\n-  \"configurations\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/configurations?type=capacity-scheduler&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"capacity-scheduler\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/configurations?type=core-site&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"core-site\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/configurations?type=global&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"global\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/configurations?type=hadoop-policy&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"hadoop-policy\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/configurations?type=hdfs-log4j&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"hdfs-log4j\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/configurations?type=hdfs-site&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"hdfs-site\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/configurations?type=mapred-queue-acls&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"mapred-queue-acls\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/configurations?type=mapred-site&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"mapred-site\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/configurations?type=yarn-log4j&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"yarn-log4j\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/configurations?type=yarn-site&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"yarn-site\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/MySingleNodeCluster/configurations?type=zookeeper-log4j&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"zookeeper-log4j\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/clusterAll.json",
                "sha": "05e4a137529e1600b2bf1282db63c9901928b8a5",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/clusters.json",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/clusters.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 12,
                "filename": "ambari-client/groovy-client/src/test/resources/clusters.json",
                "patch": "@@ -1,12 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:49178/api/v1/clusters\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster\",\n-      \"Clusters\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\",\n-        \"version\" : \"HDP-2.1\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/clusters.json",
                "sha": "abaf084165187a9fb4ee0c804b5320e8c2f78436",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/hdfsServiceComponents.json",
                "changes": 63,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/hdfsServiceComponents.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 63,
                "filename": "ambari-client/groovy-client/src/test/resources/hdfsServiceComponents.json",
                "patch": "@@ -1,63 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:49271/api/v1/clusters/single-node-hdfs-yarn/services/HDFS/components?fields=ServiceComponentInfo/*\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:49271/api/v1/clusters/single-node-hdfs-yarn/services/HDFS/components/DATANODE\",\n-      \"ServiceComponentInfo\" : {\n-        \"category\" : \"SLAVE\",\n-        \"cluster_name\" : \"single-node-hdfs-yarn\",\n-        \"component_name\" : \"DATANODE\",\n-        \"service_name\" : \"HDFS\",\n-        \"state\" : \"STARTED\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:49271/api/v1/clusters/single-node-hdfs-yarn/services/HDFS/components/HDFS_CLIENT\",\n-      \"ServiceComponentInfo\" : {\n-        \"category\" : \"CLIENT\",\n-        \"cluster_name\" : \"single-node-hdfs-yarn\",\n-        \"component_name\" : \"HDFS_CLIENT\",\n-        \"service_name\" : \"HDFS\",\n-        \"state\" : \"INSTALLED\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:49271/api/v1/clusters/single-node-hdfs-yarn/services/HDFS/components/NAMENODE\",\n-      \"ServiceComponentInfo\" : {\n-        \"CapacityRemaining\" : 31924920320,\n-        \"CapacityTotal\" : 39078707200,\n-        \"CapacityUsed\" : 24576,\n-        \"DeadNodes\" : \"{}\",\n-        \"DecomNodes\" : \"{}\",\n-        \"HeapMemoryMax\" : 1052770304,\n-        \"HeapMemoryUsed\" : 100962448,\n-        \"LiveNodes\" : \"{\\\"server.ambari.com\\\":{\\\"infoAddr\\\":\\\"172.17.0.2:50075\\\",\\\"infoSecureAddr\\\":\\\"172.17.0.2:0\\\",\\\"xferaddr\\\":\\\"172.17.0.2:50010\\\",\\\"lastContact\\\":0,\\\"usedSpace\\\":24576,\\\"adminState\\\":\\\"In Service\\\",\\\"nonDfsUsedSpace\\\":7153762304,\\\"capacity\\\":39078707200,\\\"numBlocks\\\":0,\\\"version\\\":\\\"2.4.0.2.1.1.0-385\\\",\\\"used\\\":24576,\\\"remaining\\\":31924920320,\\\"blockScheduled\\\":0,\\\"blockPoolUsed\\\":24576,\\\"blockPoolUsedPercent\\\":6.2888466E-5,\\\"volfails\\\":0}}\",\n-        \"NonDfsUsedSpace\" : 7153762304,\n-        \"NonHeapMemoryMax\" : 136314880,\n-        \"NonHeapMemoryUsed\" : 33860904,\n-        \"PercentRemaining\" : 81.6939,\n-        \"PercentUsed\" : 6.2888466E-5,\n-        \"Safemode\" : \"\",\n-        \"StartTime\" : 1400572832732,\n-        \"TotalFiles\" : 10,\n-        \"UpgradeFinalized\" : true,\n-        \"Version\" : \"2.4.0.2.1.1.0-385, r68ceccf06a4441273e81a5ec856d41fc7e11c792\",\n-        \"category\" : \"MASTER\",\n-        \"cluster_name\" : \"single-node-hdfs-yarn\",\n-        \"component_name\" : \"NAMENODE\",\n-        \"service_name\" : \"HDFS\",\n-        \"state\" : \"STARTED\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:49271/api/v1/clusters/single-node-hdfs-yarn/services/HDFS/components/SECONDARY_NAMENODE\",\n-      \"ServiceComponentInfo\" : {\n-        \"category\" : \"MASTER\",\n-        \"cluster_name\" : \"single-node-hdfs-yarn\",\n-        \"component_name\" : \"SECONDARY_NAMENODE\",\n-        \"service_name\" : \"HDFS\",\n-        \"state\" : \"STARTED\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/hdfsServiceComponents.json",
                "sha": "b4f8e507d2b784a2d1533e5336cd7e30d3159932",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/hdp-multinode-default.json",
                "changes": 200,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/hdp-multinode-default.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 200,
                "filename": "ambari-client/groovy-client/src/test/resources/hdp-multinode-default.json",
                "patch": "@@ -1,200 +0,0 @@\n-{\n-    \"configurations\" : [\n-        {\n-            \"global\" : {\n-                \"nagios_contact\" : \"admin@localhost\"\n-            }\n-        }\n-    ],\n-    \"host_groups\" : [\n-        {\n-            \"name\" : \"master_1\",\n-            \"components\" : [\n-                {\n-                    \"name\" : \"NAMENODE\"\n-                },\n-                {\n-                    \"name\" : \"ZOOKEEPER_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"HBASE_MASTER\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"HDFS_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"YARN_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HCAT\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_MONITOR\"\n-                }\n-            ],\n-            \"cardinality\" : \"1\"\n-        },\n-        {\n-            \"name\" : \"master_2\",\n-            \"components\" : [\n-\n-                {\n-                    \"name\" : \"ZOOKEEPER_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HISTORYSERVER\"\n-                },\n-                {\n-                    \"name\" : \"HIVE_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"SECONDARY_NAMENODE\"\n-                },\n-                {\n-                    \"name\" : \"HIVE_METASTORE\"\n-                },\n-                {\n-                    \"name\" : \"HDFS_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HIVE_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"YARN_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"MYSQL_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_MONITOR\"\n-                },\n-                {\n-                    \"name\" : \"WEBHCAT_SERVER\"\n-                }\n-            ],\n-            \"cardinality\" : \"1\"\n-        },\n-        {\n-            \"name\" : \"master_3\",\n-            \"components\" : [\n-                {\n-                    \"name\" : \"RESOURCEMANAGER\"\n-                },\n-                {\n-                    \"name\" : \"ZOOKEEPER_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_MONITOR\"\n-                }\n-            ],\n-            \"cardinality\" : \"1\"\n-        },\n-        {\n-            \"name\" : \"master_4\",\n-            \"components\" : [\n-                {\n-                    \"name\" : \"OOZIE_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"ZOOKEEPER_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_MONITOR\"\n-                }\n-            ],\n-            \"cardinality\" : \"1\"\n-        },\n-        {\n-            \"name\" : \"slave_1\",\n-            \"components\" : [\n-                {\n-                    \"name\" : \"HBASE_REGIONSERVER\"\n-                },\n-                {\n-                    \"name\" : \"NODEMANAGER\"\n-                },\n-                {\n-                    \"name\" : \"DATANODE\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_MONITOR\"\n-                }\n-            ],\n-            \"cardinality\" : \"${slavesCount}\"\n-        },\n-        {\n-                    \"name\" : \"SLAVE_2\",\n-                    \"components\" : [\n-                        {\n-                            \"name\" : \"HBASE_REGIONSERVER\"\n-                        },\n-                        {\n-                            \"name\" : \"NODEMANAGER\"\n-                        },\n-                        {\n-                            \"name\" : \"DATANODE\"\n-                        },\n-                        {\n-                            \"name\" : \"GANGLIA_MONITOR\"\n-                        }\n-                    ],\n-                    \"cardinality\" : \"${slavesCount}\"\n-                },\n-        {\n-            \"name\" : \"gateway\",\n-            \"components\" : [\n-                {\n-                    \"name\" : \"AMBARI_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"NAGIOS_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"ZOOKEEPER_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"PIG\"\n-                },\n-                {\n-                    \"name\" : \"OOZIE_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HBASE_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HCAT\"\n-                },\n-                {\n-                    \"name\" : \"SQOOP\"\n-                },\n-                {\n-                    \"name\" : \"HDFS_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HIVE_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"YARN_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"MAPREDUCE2_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_MONITOR\"\n-                }\n-            ],\n-            \"cardinality\" : \"1\"\n-        }\n-    ],\n-    \"Blueprints\" : {\n-        \"blueprint_name\" : \"hdp-multinode-default\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"2.1\"\n-    }\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/hdp-multinode-default.json",
                "sha": "f6c7f3f55492fa442e50f0a4a496394ab75a315b",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/hdp-multinode-default2.json",
                "changes": 164,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/hdp-multinode-default2.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 164,
                "filename": "ambari-client/groovy-client/src/test/resources/hdp-multinode-default2.json",
                "patch": "@@ -1,164 +0,0 @@\n-{\n-    \"configurations\" : [\n-        {\n-            \"global\" : {\n-                \"nagios_contact\" : \"admin@localhost\"\n-            }\n-        }\n-    ],\n-    \"host_groups\" : [\n-        {\n-            \"name\" : \"master_1\",\n-            \"components\" : [\n-                {\n-                    \"name\" : \"NAMENODE\"\n-                },\n-                {\n-                    \"name\" : \"ZOOKEEPER_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"HBASE_MASTER\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"HDFS_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"YARN_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HCAT\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_MONITOR\"\n-                }\n-            ],\n-            \"cardinality\" : \"1\"\n-        },\n-        {\n-            \"name\" : \"master_2\",\n-            \"components\" : [\n-\n-                {\n-                    \"name\" : \"ZOOKEEPER_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HISTORYSERVER\"\n-                },\n-                {\n-                    \"name\" : \"HIVE_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"SECONDARY_NAMENODE\"\n-                },\n-                {\n-                    \"name\" : \"HIVE_METASTORE\"\n-                },\n-                {\n-                    \"name\" : \"HDFS_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HIVE_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"YARN_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"MYSQL_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_MONITOR\"\n-                },\n-                {\n-                    \"name\" : \"WEBHCAT_SERVER\"\n-                }\n-            ],\n-            \"cardinality\" : \"1\"\n-        },\n-        {\n-            \"name\" : \"master_3\",\n-            \"components\" : [\n-                {\n-                    \"name\" : \"RESOURCEMANAGER\"\n-                },\n-                {\n-                    \"name\" : \"ZOOKEEPER_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_MONITOR\"\n-                }\n-            ],\n-            \"cardinality\" : \"1\"\n-        },\n-        {\n-            \"name\" : \"master_4\",\n-            \"components\" : [\n-                {\n-                    \"name\" : \"OOZIE_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"ZOOKEEPER_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_MONITOR\"\n-                }\n-            ],\n-            \"cardinality\" : \"1\"\n-        },\n-        {\n-            \"name\" : \"gateway\",\n-            \"components\" : [\n-                {\n-                    \"name\" : \"AMBARI_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"NAGIOS_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_SERVER\"\n-                },\n-                {\n-                    \"name\" : \"ZOOKEEPER_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"PIG\"\n-                },\n-                {\n-                    \"name\" : \"OOZIE_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HBASE_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HCAT\"\n-                },\n-                {\n-                    \"name\" : \"SQOOP\"\n-                },\n-                {\n-                    \"name\" : \"HDFS_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"HIVE_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"YARN_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"MAPREDUCE2_CLIENT\"\n-                },\n-                {\n-                    \"name\" : \"GANGLIA_MONITOR\"\n-                }\n-            ],\n-            \"cardinality\" : \"1\"\n-        }\n-    ],\n-    \"Blueprints\" : {\n-        \"blueprint_name\" : \"hdp-multinode-default\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"2.1\"\n-    }\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/hdp-multinode-default2.json",
                "sha": "4db36802f3d8331b2c295b8e04c7291ca9c1397e",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/host-components.json",
                "changes": 590,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/host-components.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 590,
                "filename": "ambari-client/groovy-client/src/test/resources/host-components.json",
                "patch": "@@ -1,590 +0,0 @@\n-{\n-  \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom/host_components?fields=HostRoles/*\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom/host_components/DATANODE\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"single-node-hdfs-yarn\",\n-        \"component_name\" : \"DATANODE\",\n-        \"desired_admin_state\" : \"INSERVICE\",\n-        \"desired_stack_id\" : \"HDP-2.0\",\n-        \"desired_state\" : \"STARTED\",\n-        \"host_name\" : \"amb0.mycorp.kom\",\n-        \"maintenance_state\" : \"OFF\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_id\" : \"HDP-2.0\",\n-        \"stale_configs\" : false,\n-        \"state\" : \"STARTED\",\n-        \"actual_configs\" : {\n-          \"capacity-scheduler\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"core-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"global\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hadoop-policy\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-queue-acls\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"zookeeper-log4j\" : {\n-            \"default\" : \"1\"\n-          }\n-        }\n-      },\n-      \"host\" : {\n-        \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom/host_components/HDFS_CLIENT\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"single-node-hdfs-yarn\",\n-        \"component_name\" : \"HDFS_CLIENT\",\n-        \"desired_stack_id\" : \"HDP-2.0\",\n-        \"desired_state\" : \"INSTALLED\",\n-        \"host_name\" : \"amb0.mycorp.kom\",\n-        \"maintenance_state\" : \"OFF\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_id\" : \"HDP-2.0\",\n-        \"stale_configs\" : false,\n-        \"state\" : \"INSTALLED\",\n-        \"actual_configs\" : {\n-          \"capacity-scheduler\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"core-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"global\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hadoop-policy\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-queue-acls\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"zookeeper-log4j\" : {\n-            \"default\" : \"1\"\n-          }\n-        }\n-      },\n-      \"host\" : {\n-        \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom/host_components/HISTORYSERVER\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"single-node-hdfs-yarn\",\n-        \"component_name\" : \"HISTORYSERVER\",\n-        \"desired_stack_id\" : \"HDP-2.0\",\n-        \"desired_state\" : \"STARTED\",\n-        \"host_name\" : \"amb0.mycorp.kom\",\n-        \"maintenance_state\" : \"OFF\",\n-        \"service_name\" : \"MAPREDUCE2\",\n-        \"stack_id\" : \"HDP-2.0\",\n-        \"stale_configs\" : false,\n-        \"state\" : \"STARTED\",\n-        \"actual_configs\" : {\n-          \"capacity-scheduler\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"core-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"global\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hadoop-policy\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-queue-acls\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"zookeeper-log4j\" : {\n-            \"default\" : \"1\"\n-          }\n-        }\n-      },\n-      \"host\" : {\n-        \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom/host_components/MAPREDUCE2_CLIENT\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"single-node-hdfs-yarn\",\n-        \"component_name\" : \"MAPREDUCE2_CLIENT\",\n-        \"desired_stack_id\" : \"HDP-2.0\",\n-        \"desired_state\" : \"INSTALLED\",\n-        \"host_name\" : \"amb0.mycorp.kom\",\n-        \"maintenance_state\" : \"OFF\",\n-        \"service_name\" : \"MAPREDUCE2\",\n-        \"stack_id\" : \"HDP-2.0\",\n-        \"stale_configs\" : false,\n-        \"state\" : \"INSTALLED\",\n-        \"actual_configs\" : {\n-          \"capacity-scheduler\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"core-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"global\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hadoop-policy\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-queue-acls\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"zookeeper-log4j\" : {\n-            \"default\" : \"1\"\n-          }\n-        }\n-      },\n-      \"host\" : {\n-        \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom/host_components/NAMENODE\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"single-node-hdfs-yarn\",\n-        \"component_name\" : \"NAMENODE\",\n-        \"desired_stack_id\" : \"HDP-2.0\",\n-        \"desired_state\" : \"STARTED\",\n-        \"host_name\" : \"amb0.mycorp.kom\",\n-        \"maintenance_state\" : \"OFF\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_id\" : \"HDP-2.0\",\n-        \"stale_configs\" : false,\n-        \"state\" : \"STARTED\",\n-        \"actual_configs\" : {\n-          \"capacity-scheduler\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"core-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"global\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hadoop-policy\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-queue-acls\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"zookeeper-log4j\" : {\n-            \"default\" : \"1\"\n-          }\n-        }\n-      },\n-      \"host\" : {\n-        \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom/host_components/NODEMANAGER\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"single-node-hdfs-yarn\",\n-        \"component_name\" : \"NODEMANAGER\",\n-        \"desired_admin_state\" : \"INSERVICE\",\n-        \"desired_stack_id\" : \"HDP-2.0\",\n-        \"desired_state\" : \"STARTED\",\n-        \"host_name\" : \"amb0.mycorp.kom\",\n-        \"maintenance_state\" : \"OFF\",\n-        \"service_name\" : \"YARN\",\n-        \"stack_id\" : \"HDP-2.0\",\n-        \"stale_configs\" : false,\n-        \"state\" : \"STARTED\",\n-        \"actual_configs\" : {\n-          \"capacity-scheduler\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"core-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"global\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hadoop-policy\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-queue-acls\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"zookeeper-log4j\" : {\n-            \"default\" : \"1\"\n-          }\n-        }\n-      },\n-      \"host\" : {\n-        \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom/host_components/RESOURCEMANAGER\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"single-node-hdfs-yarn\",\n-        \"component_name\" : \"RESOURCEMANAGER\",\n-        \"desired_stack_id\" : \"HDP-2.0\",\n-        \"desired_state\" : \"STARTED\",\n-        \"host_name\" : \"amb0.mycorp.kom\",\n-        \"maintenance_state\" : \"OFF\",\n-        \"service_name\" : \"YARN\",\n-        \"stack_id\" : \"HDP-2.0\",\n-        \"stale_configs\" : false,\n-        \"state\" : \"STARTED\",\n-        \"actual_configs\" : {\n-          \"capacity-scheduler\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"core-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"global\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hadoop-policy\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-queue-acls\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"zookeeper-log4j\" : {\n-            \"default\" : \"1\"\n-          }\n-        }\n-      },\n-      \"host\" : {\n-        \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom/host_components/SECONDARY_NAMENODE\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"single-node-hdfs-yarn\",\n-        \"component_name\" : \"SECONDARY_NAMENODE\",\n-        \"desired_stack_id\" : \"HDP-2.0\",\n-        \"desired_state\" : \"STARTED\",\n-        \"host_name\" : \"amb0.mycorp.kom\",\n-        \"maintenance_state\" : \"OFF\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_id\" : \"HDP-2.0\",\n-        \"stale_configs\" : false,\n-        \"state\" : \"STARTED\",\n-        \"actual_configs\" : {\n-          \"capacity-scheduler\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"core-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"global\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hadoop-policy\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-queue-acls\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"zookeeper-log4j\" : {\n-            \"default\" : \"1\"\n-          }\n-        }\n-      },\n-      \"host\" : {\n-        \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom/host_components/YARN_CLIENT\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"single-node-hdfs-yarn\",\n-        \"component_name\" : \"YARN_CLIENT\",\n-        \"desired_stack_id\" : \"HDP-2.0\",\n-        \"desired_state\" : \"INSTALLED\",\n-        \"host_name\" : \"amb0.mycorp.kom\",\n-        \"maintenance_state\" : \"OFF\",\n-        \"service_name\" : \"YARN\",\n-        \"stack_id\" : \"HDP-2.0\",\n-        \"stale_configs\" : false,\n-        \"state\" : \"INSTALLED\",\n-        \"actual_configs\" : {\n-          \"capacity-scheduler\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"core-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"global\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hadoop-policy\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-queue-acls\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"zookeeper-log4j\" : {\n-            \"default\" : \"1\"\n-          }\n-        }\n-      },\n-      \"host\" : {\n-        \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom/host_components/ZOOKEEPER_CLIENT\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"single-node-hdfs-yarn\",\n-        \"component_name\" : \"ZOOKEEPER_CLIENT\",\n-        \"desired_stack_id\" : \"HDP-2.0\",\n-        \"desired_state\" : \"INSTALLED\",\n-        \"host_name\" : \"amb0.mycorp.kom\",\n-        \"maintenance_state\" : \"OFF\",\n-        \"service_name\" : \"ZOOKEEPER\",\n-        \"stack_id\" : \"HDP-2.0\",\n-        \"stale_configs\" : false,\n-        \"state\" : \"INSTALLED\",\n-        \"actual_configs\" : {\n-          \"capacity-scheduler\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"core-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"global\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hadoop-policy\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-queue-acls\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"zookeeper-log4j\" : {\n-            \"default\" : \"1\"\n-          }\n-        }\n-      },\n-      \"host\" : {\n-        \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom/host_components/ZOOKEEPER_SERVER\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"single-node-hdfs-yarn\",\n-        \"component_name\" : \"ZOOKEEPER_SERVER\",\n-        \"desired_stack_id\" : \"HDP-2.0\",\n-        \"desired_state\" : \"STARTED\",\n-        \"host_name\" : \"amb0.mycorp.kom\",\n-        \"maintenance_state\" : \"OFF\",\n-        \"service_name\" : \"ZOOKEEPER\",\n-        \"stack_id\" : \"HDP-2.0\",\n-        \"stale_configs\" : false,\n-        \"state\" : \"STARTED\",\n-        \"actual_configs\" : {\n-          \"capacity-scheduler\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"core-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"global\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hadoop-policy\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"hdfs-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-queue-acls\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"mapred-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-log4j\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"yarn-site\" : {\n-            \"default\" : \"1\"\n-          },\n-          \"zookeeper-log4j\" : {\n-            \"default\" : \"1\"\n-          }\n-        }\n-      },\n-      \"host\" : {\n-        \"href\" : \"http://172.24.0.2:8080/api/v1/clusters/single-node-hdfs-yarn/hosts/amb0.mycorp.kom\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/host-components.json",
                "sha": "1c44e7ab48c573f8f378e4dc2458a12f41d5a83c",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/hosts.json",
                "changes": 92,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/hosts.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 92,
                "filename": "ambari-client/groovy-client/src/test/resources/hosts.json",
                "patch": "@@ -1,92 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:49178/api/v1/hosts?fields=Hosts\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/hosts/server.ambari.com\",\n-      \"Hosts\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\",\n-        \"cpu_count\" : 2,\n-        \"desired_configs\" : null,\n-        \"disk_info\" : [\n-          {\n-            \"available\" : \"30467416\",\n-            \"used\" : \"6729092\",\n-            \"percent\" : \"19%\",\n-            \"size\" : \"39211376\",\n-            \"type\" : \"rootfs\",\n-            \"mountpoint\" : \"/\"\n-          },\n-          {\n-            \"available\" : \"30467416\",\n-            \"used\" : \"6729092\",\n-            \"percent\" : \"19%\",\n-            \"size\" : \"39211376\",\n-            \"type\" : \"aufs\",\n-            \"mountpoint\" : \"/\"\n-          },\n-          {\n-            \"available\" : \"2027492\",\n-            \"used\" : \"0\",\n-            \"percent\" : \"0%\",\n-            \"size\" : \"2027492\",\n-            \"type\" : \"tmpfs\",\n-            \"mountpoint\" : \"/dev\"\n-          },\n-          {\n-            \"available\" : \"65536\",\n-            \"used\" : \"0\",\n-            \"percent\" : \"0%\",\n-            \"size\" : \"65536\",\n-            \"type\" : \"tmpfs\",\n-            \"mountpoint\" : \"/dev/shm\"\n-          },\n-          {\n-            \"available\" : \"2027492\",\n-            \"used\" : \"0\",\n-            \"percent\" : \"0%\",\n-            \"size\" : \"2027492\",\n-            \"type\" : \"tmpfs\",\n-            \"mountpoint\" : \"/proc/kcore\"\n-          }\n-        ],\n-        \"host_health_report\" : \"\",\n-        \"host_name\" : \"server.ambari.com\",\n-        \"host_state\" : \"HEALTHY\",\n-        \"host_status\" : \"UNHEALTHY\",\n-        \"ip\" : \"172.17.0.2\",\n-        \"last_agent_env\" : {\n-          \"stackFoldersAndFiles\" : [ ],\n-          \"alternatives\" : [ ],\n-          \"existingUsers\" : [ ],\n-          \"existingRepos\" : [\n-            \"unable_to_determine\"\n-          ],\n-          \"installedPackages\" : [ ],\n-          \"hostHealth\" : {\n-            \"activeJavaProcs\" : [ ],\n-            \"agentTimeStampAtReporting\" : 1401222525617,\n-            \"serverTimeStampAtReporting\" : 1401222525661,\n-            \"liveServices\" : [\n-              {\n-                \"desc\" : \"ntpd: unrecognized service\\n\",\n-                \"status\" : \"Unhealthy\",\n-                \"name\" : \"ntpd\"\n-              }\n-            ]\n-          },\n-          \"umask\" : 18,\n-          \"firewallRunning\" : true,\n-          \"firewallName\" : \"iptables\"\n-        },\n-        \"last_heartbeat_time\" : 1401222535701,\n-        \"last_registration_time\" : 1401180386121,\n-        \"os_arch\" : \"x86_64\",\n-        \"os_type\" : \"redhat6\",\n-        \"ph_cpu_count\" : 2,\n-        \"public_host_name\" : \"server.ambari.com\",\n-        \"rack_info\" : \"/default-rack\",\n-        \"total_mem\" : 4054984\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/hosts.json",
                "sha": "b1fa47ee580e9af2a174fee2f8276ae89056cf84",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/multi-node-hdfs-yarn-config.json",
                "changes": 89,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/multi-node-hdfs-yarn-config.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 89,
                "filename": "ambari-client/groovy-client/src/test/resources/multi-node-hdfs-yarn-config.json",
                "patch": "@@ -1,89 +0,0 @@\n-{\n-  \"configurations\": [\n-    {\n-      \"global\": {\n-        \"nagios_contact\": \"admin@localhost\"\n-      }\n-    },\n-    {\n-      \"hdfs-site\": {\n-        \"dfs.datanode.data.dir\": \"/mnt/fs1/,/mnt/fs2/\"\n-      }\n-    },\n-    {\n-      \"yarn-site\": {\n-        \"yarn.nodemanager.local-dirs\": \"apple\",\n-        \"property-key\": \"property-value\"\n-      }\n-    },\n-    {\n-          \"core-site\": {\n-            \"fs.defaultFS\": \"localhost:9000\"\n-          }\n-        }\n-  ],\n-  \"host_groups\": [\n-    {\n-      \"name\": \"master\",\n-      \"components\": [\n-        {\n-          \"name\": \"NAMENODE\"\n-        },\n-        {\n-          \"name\": \"GANGLIA_SERVER\"\n-        },\n-        {\n-          \"name\": \"HISTORYSERVER\"\n-        },\n-        {\n-          \"name\": \"SECONDARY_NAMENODE\"\n-        },\n-        {\n-          \"name\": \"RESOURCEMANAGER\"\n-        },\n-        {\n-          \"name\": \"HISTORYSERVER\"\n-        },\n-        {\n-          \"name\": \"NAGIOS_SERVER\"\n-        },\n-        {\n-          \"name\": \"ZOOKEEPER_SERVER\"\n-        }\n-      ],\n-      \"cardinality\": \"1\"\n-    },\n-    {\n-      \"name\": \"slave_1\",\n-      \"components\": [\n-        {\n-          \"name\": \"DATANODE\"\n-        },\n-        {\n-          \"name\": \"GANGLIA_MONITOR\"\n-        },\n-        {\n-          \"name\": \"HDFS_CLIENT\"\n-        },\n-        {\n-          \"name\": \"NODEMANAGER\"\n-        },\n-        {\n-          \"name\": \"YARN_CLIENT\"\n-        },\n-        {\n-          \"name\": \"MAPREDUCE2_CLIENT\"\n-        },\n-        {\n-          \"name\": \"ZOOKEEPER_CLIENT\"\n-        }\n-      ],\n-      \"cardinality\": \"2\"\n-    }\n-  ],\n-  \"Blueprints\": {\n-    \"blueprint_name\": \"multi-node-hdfs-yarn\",\n-    \"stack_name\": \"HDP\",\n-    \"stack_version\": \"2.1\"\n-  }\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/multi-node-hdfs-yarn-config.json",
                "sha": "16f4938f42106330832708b29ac0d0a2543517d5",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/multi-node-hdfs-yarn.json",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/multi-node-hdfs-yarn.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 83,
                "filename": "ambari-client/groovy-client/src/test/resources/multi-node-hdfs-yarn.json",
                "patch": "@@ -1,83 +0,0 @@\n-{\n-  \"configurations\": [\n-    {\n-      \"global\": {\n-        \"nagios_contact\": \"admin@localhost\"\n-      }\n-    },\n-    {\n-      \"hdfs-site\": {\n-        \"dfs.datanode.data.dir\": \"/mnt/fs1/,/mnt/fs2/\"\n-      }\n-    },\n-    {\n-      \"yarn-site\": {\n-        \"yarn.nodemanager.local-dirs\": \"/mnt/fs1/,/mnt/fs2/\"\n-      }\n-    }\n-  ],\n-  \"host_groups\": [\n-    {\n-      \"name\": \"master\",\n-      \"components\": [\n-        {\n-          \"name\": \"NAMENODE\"\n-        },\n-        {\n-          \"name\": \"GANGLIA_SERVER\"\n-        },\n-        {\n-          \"name\": \"HISTORYSERVER\"\n-        },\n-        {\n-          \"name\": \"SECONDARY_NAMENODE\"\n-        },\n-        {\n-          \"name\": \"RESOURCEMANAGER\"\n-        },\n-        {\n-          \"name\": \"HISTORYSERVER\"\n-        },\n-        {\n-          \"name\": \"NAGIOS_SERVER\"\n-        },\n-        {\n-          \"name\": \"ZOOKEEPER_SERVER\"\n-        }\n-      ],\n-      \"cardinality\": \"1\"\n-    },\n-    {\n-      \"name\": \"slave_1\",\n-      \"components\": [\n-        {\n-          \"name\": \"DATANODE\"\n-        },\n-        {\n-          \"name\": \"GANGLIA_MONITOR\"\n-        },\n-        {\n-          \"name\": \"HDFS_CLIENT\"\n-        },\n-        {\n-          \"name\": \"NODEMANAGER\"\n-        },\n-        {\n-          \"name\": \"YARN_CLIENT\"\n-        },\n-        {\n-          \"name\": \"MAPREDUCE2_CLIENT\"\n-        },\n-        {\n-          \"name\": \"ZOOKEEPER_CLIENT\"\n-        }\n-      ],\n-      \"cardinality\": \"2\"\n-    }\n-  ],\n-  \"Blueprints\": {\n-    \"blueprint_name\": \"multi-node-hdfs-yarn\",\n-    \"stack_name\": \"HDP\",\n-    \"stack_version\": \"2.1\"\n-  }\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/multi-node-hdfs-yarn.json",
                "sha": "fce02add677f16540031515758cb804da119b6fc",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/no-blueprint.json",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/no-blueprint.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 4,
                "filename": "ambari-client/groovy-client/src/test/resources/no-blueprint.json",
                "patch": "@@ -1,4 +0,0 @@\n-{\n-  \"status\" : 404,\n-  \"message\" : \"The requested resource doesn't exist: Blueprint not found, Blueprints/blueprint_name=azaz\"\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/no-blueprint.json",
                "sha": "2a18b761b5a9bd0f3b584a20d965c6d44ad82f89",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/no-clusters.json",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/no-clusters.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 4,
                "filename": "ambari-client/groovy-client/src/test/resources/no-clusters.json",
                "patch": "@@ -1,4 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:49178/api/v1/clusters\",\n-  \"items\" : [  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/no-clusters.json",
                "sha": "56751b469cc36a4d7c44e9f5d488fc7795bb9e61",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/no-hosts.json",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/no-hosts.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 4,
                "filename": "ambari-client/groovy-client/src/test/resources/no-hosts.json",
                "patch": "@@ -1,4 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:49178/api/v1/hosts?fields=Hosts\",\n-  \"items\" : [  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/no-hosts.json",
                "sha": "5f69b8709df2a060533ac88bf13cce7652181400",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/no-request-tasks.json",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/no-request-tasks.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 10,
                "filename": "ambari-client/groovy-client/src/test/resources/no-request-tasks.json",
                "patch": "@@ -1,10 +0,0 @@\n-{\n-  \"href\" : \"http://172.18.0.2:8080/api/v1/clusters/MySingleNodeCluster/requests/1\",\n-  \"Requests\" : {\n-    \"cluster_name\" : \"MySingleNodeCluster\",\n-    \"id\" : 1\n-  },\n-  \"tasks\" : [\n-\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/no-request-tasks.json",
                "sha": "0235adb209d0ff8c68e9ba880cd2002948e06c73",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/no-service-components-hdfs.json",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/no-service-components-hdfs.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 6,
                "filename": "ambari-client/groovy-client/src/test/resources/no-service-components-hdfs.json",
                "patch": "@@ -1,6 +0,0 @@\n-{\n-  \"href\" : \"http://172.18.0.2:8080/api/v1/clusters/MySingleNodeCluster/services/HDFS/components?fields=ServiceComponentInfo\",\n-  \"items\" : [\n-\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/no-service-components-hdfs.json",
                "sha": "501826cd04ed694435a269cc4098e33f3c935c1c",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/no-services.json",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/no-services.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 5,
                "filename": "ambari-client/groovy-client/src/test/resources/no-services.json",
                "patch": "@@ -1,5 +0,0 @@\n-{\n-  \"href\" : \"http://172.18.0.2:8080/api/v1/clusters/MySingleNodeCluster/services?fields=ServiceInfo\",\n-  \"items\" : [\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/no-services.json",
                "sha": "09c08568482a0e8b40696eb25ebefa163adea09a",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/request-tasks.json",
                "changes": 555,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/request-tasks.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 555,
                "filename": "ambari-client/groovy-client/src/test/resources/request-tasks.json",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/request-tasks.json",
                "sha": "2bdb99fac99d2f5281461b74980e1d8fe18523db",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/service-components-hdfs.json",
                "changes": 63,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/service-components-hdfs.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 63,
                "filename": "ambari-client/groovy-client/src/test/resources/service-components-hdfs.json",
                "patch": "@@ -1,63 +0,0 @@\n-{\n-  \"href\" : \"http://172.18.0.2:8080/api/v1/clusters/MySingleNodeCluster/services/HDFS/components?fields=ServiceComponentInfo\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://172.18.0.2:8080/api/v1/clusters/MySingleNodeCluster/services/HDFS/components/DATANODE\",\n-      \"ServiceComponentInfo\" : {\n-        \"category\" : \"SLAVE\",\n-        \"cluster_name\" : \"MySingleNodeCluster\",\n-        \"component_name\" : \"DATANODE\",\n-        \"service_name\" : \"HDFS\",\n-        \"state\" : \"STARTED\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://172.18.0.2:8080/api/v1/clusters/MySingleNodeCluster/services/HDFS/components/HDFS_CLIENT\",\n-      \"ServiceComponentInfo\" : {\n-        \"category\" : \"CLIENT\",\n-        \"cluster_name\" : \"MySingleNodeCluster\",\n-        \"component_name\" : \"HDFS_CLIENT\",\n-        \"service_name\" : \"HDFS\",\n-        \"state\" : \"INSTALLED\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://172.18.0.2:8080/api/v1/clusters/MySingleNodeCluster/services/HDFS/components/NAMENODE\",\n-      \"ServiceComponentInfo\" : {\n-        \"CapacityRemaining\" : 15762272256,\n-        \"CapacityTotal\" : 18433347584,\n-        \"CapacityUsed\" : 24576,\n-        \"DeadNodes\" : \"{}\",\n-        \"DecomNodes\" : \"{}\",\n-        \"HeapMemoryMax\" : 1052770304,\n-        \"HeapMemoryUsed\" : 161373832,\n-        \"LiveNodes\" : \"{\\\"server.ambari.com\\\":{\\\"infoAddr\\\":\\\"172.18.0.2:50075\\\",\\\"infoSecureAddr\\\":\\\"172.18.0.2:0\\\",\\\"xferaddr\\\":\\\"172.18.0.2:50010\\\",\\\"lastContact\\\":2,\\\"usedSpace\\\":24576,\\\"adminState\\\":\\\"In Service\\\",\\\"nonDfsUsedSpace\\\":2671050752,\\\"capacity\\\":18433347584,\\\"numBlocks\\\":0,\\\"version\\\":\\\"2.4.0.2.1.1.0-390\\\",\\\"used\\\":24576,\\\"remaining\\\":15762272256,\\\"blockScheduled\\\":0,\\\"blockPoolUsed\\\":24576,\\\"blockPoolUsedPercent\\\":1.3332359E-4,\\\"volfails\\\":0}}\",\n-        \"NonDfsUsedSpace\" : 2671050752,\n-        \"NonHeapMemoryMax\" : 136314880,\n-        \"NonHeapMemoryUsed\" : 34330384,\n-        \"PercentRemaining\" : 85.509544,\n-        \"PercentUsed\" : 1.3332359E-4,\n-        \"Safemode\" : \"\",\n-        \"StartTime\" : 1401353117084,\n-        \"TotalFiles\" : 10,\n-        \"UpgradeFinalized\" : true,\n-        \"Version\" : \"2.4.0.2.1.1.0-390, r68ceccf06a4441273e81a5ec856d41fc7e11c792\",\n-        \"category\" : \"MASTER\",\n-        \"cluster_name\" : \"MySingleNodeCluster\",\n-        \"component_name\" : \"NAMENODE\",\n-        \"service_name\" : \"HDFS\",\n-        \"state\" : \"STARTED\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://172.18.0.2:8080/api/v1/clusters/MySingleNodeCluster/services/HDFS/components/SECONDARY_NAMENODE\",\n-      \"ServiceComponentInfo\" : {\n-        \"category\" : \"MASTER\",\n-        \"cluster_name\" : \"MySingleNodeCluster\",\n-        \"component_name\" : \"SECONDARY_NAMENODE\",\n-        \"service_name\" : \"HDFS\",\n-        \"state\" : \"STARTED\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/service-components-hdfs.json",
                "sha": "9262c53662e868b4f7c7011689604b009d842225",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/service-config.json",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/service-config.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 58,
                "filename": "ambari-client/groovy-client/src/test/resources/service-config.json",
                "patch": "@@ -1,58 +0,0 @@\n-{\n-  \"zk_user\": \"zookeeper\",\n-  \"zk_pid_file\": \"/var/run/zookeeper/zookeeper_server.pid\",\n-  \"zk_pid_dir\": \"/var/run/zookeeper\",\n-  \"zk_log_dir\": \"/var/log/zookeeper\",\n-  \"zk_data_dir\": \"/hadoop/zookeeper\",\n-  \"yarn_user\": \"yarn\",\n-  \"yarn_pid_dir_prefix\": \"/var/run/hadoop-yarn\",\n-  \"yarn_log_dir_prefix\": \"/var/log/hadoop-yarn\",\n-  \"yarn_heapsize\": \"1024\",\n-  \"user_group\": \"hadoop\",\n-  \"tickTime\": \"2000\",\n-  \"syncLimit\": \"5\",\n-  \"smokeuser\": \"ambari-qa\",\n-  \"security_enabled\": \"false\",\n-  \"rrdcached_base_dir\": \"/var/lib/ganglia/rrds\",\n-  \"resourcemanager_heapsize\": \"1024\",\n-  \"proxyuser_group\": \"users\",\n-  \"nodemanager_heapsize\": \"1024\",\n-  \"namenode_opt_newsize\": \"200m\",\n-  \"namenode_opt_maxnewsize\": \"200m\",\n-  \"namenode_opt_permsize\" : \"128m\",\n-  \"namenode_opt_maxpermsize\" : \"256m\",\n-  \"namenode_heapsize\": \"1024m\",\n-  \"namenode_formatted_mark_dir\": \"/var/run/hadoop/hdfs/namenode/formatted/\",\n-  \"ganglia_conf_dir\": \"/etc/ganglia/hdp\",\n-  \"dtnode_heapsize\": \"1024m\",\n-  \"dfs_webhdfs_enabled\": \"true\",\n-  \"dfs_replication\": \"3\",\n-  \"dfs_namenode_name_dir\": \"/hadoop/hdfs/namenode\",\n-  \"dfs_namenode_checkpoint_period\": \"21600\",\n-  \"dfs_namenode_checkpoint_dir\": \"/hadoop/hdfs/namesecondary\",\n-  \"dfs_datanode_http_address\": \"50075\",\n-  \"apptimelineserver_heapsize\": \"1024\",\n-  \"clientPort\": \"2181\",\n-  \"datanode_du_reserved\": \"1073741824\",\n-  \"dfs_block_local_path_access_user\": \"hbase\",\n-  \"dfs_datanode_address\": \"50010\",\n-  \"dfs_datanode_data_dir\": \"/hadoop/hdfs/data\",\n-  \"dfs_datanode_data_dir_perm\": \"750\",\n-  \"dfs_datanode_failed_volume_tolerated\": \"0\",\n-  \"ganglia_runtime_dir\": \"/var/run/ganglia/hdp\",\n-  \"gmetad_user\": \"nobody\",\n-  \"gmond_user\": \"nobody\",\n-  \"hadoop_heapsize\": \"1024\",\n-  \"hadoop_pid_dir_prefix\": \"/var/run/hadoop\",\n-  \"hdfs_log_dir_prefix\": \"/var/log/hadoop\",\n-  \"hdfs_user\": \"hdfs\",\n-  \"initLimit\": \"10\",\n-  \"kerberos_domain\": \"EXAMPLE.COM\",\n-  \"keytab_path\": \"/etc/security/keytabs\",\n-  \"lzo_enabled\": \"true\",\n-  \"mapred_log_dir_prefix\": \"/var/log/hadoop-mapreduce\",\n-  \"mapred_pid_dir_prefix\": \"/var/run/hadoop-mapreduce\",\n-  \"mapred_user\": \"mapred\",\n-  \"nagios_contact\": \"default@REPLACEME.NOWHERE\",\n-  \"nagios_web_password\": \"admin\"\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/service-config.json",
                "sha": "d18b5db6f6e65b1657d6bc224e0b76b6cbdf1ac0",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/service-versions-multiple.json",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/service-versions-multiple.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 37,
                "filename": "ambari-client/groovy-client/src/test/resources/service-versions-multiple.json",
                "patch": "@@ -1,37 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations?type=yarn-log4j&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"yarn-log4j\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations?type=yarn-site&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"yarn-site\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations?type=yarn-site&tag=version1402400472475\",\n-      \"tag\" : \"version1402400472475\",\n-      \"type\" : \"yarn-site\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations?type=zookeeper-log4j&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"zookeeper-log4j\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/service-versions-multiple.json",
                "sha": "b320e97f4127495374400093099df54d9544d298",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/service-versions.json",
                "changes": 93,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/service-versions.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 93,
                "filename": "ambari-client/groovy-client/src/test/resources/service-versions.json",
                "patch": "@@ -1,93 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations?type=capacity-scheduler&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"capacity-scheduler\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations?type=core-site&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"core-site\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations?type=global&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"global\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations?type=hadoop-policy&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"hadoop-policy\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations?type=hdfs-log4j&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"hdfs-log4j\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations?type=hdfs-site&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"hdfs-site\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations?type=mapred-queue-acls&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"mapred-queue-acls\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations?type=mapred-site&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"mapred-site\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations?type=yarn-log4j&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"yarn-log4j\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations?type=yarn-site&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"yarn-site\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations?type=zookeeper-log4j&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"zookeeper-log4j\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/service-versions.json",
                "sha": "049337a21d4cd66d0eb84799bef2488afb13ecd6",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/services.json",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/services.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 15,
                "filename": "ambari-client/groovy-client/src/test/resources/services.json",
                "patch": "@@ -1,15 +0,0 @@\n-{\n-  \"href\" : \"http://172.18.0.2:8080/api/v1/clusters/MySingleNodeCluster/services?fields=ServiceInfo\",\n-  \"items\" : [\n-\n-    {\n-      \"href\" : \"http://172.18.0.2:8080/api/v1/clusters/MySingleNodeCluster/services/HDFS\",\n-      \"ServiceInfo\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\",\n-        \"maintenance_state\" : \"OFF\",\n-        \"service_name\" : \"HDFS\",\n-        \"state\" : \"STARTED\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/services.json",
                "sha": "25581a5f7ad0704e7e0f3c014a78a8dde2b9012e",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/tasks.json",
                "changes": 471,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/tasks.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 471,
                "filename": "ambari-client/groovy-client/src/test/resources/tasks.json",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/tasks.json",
                "sha": "aa4cb1a49966e92f224f327f25b29a712762f8bb",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/versions/clusters.json",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/versions/clusters.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 12,
                "filename": "ambari-client/groovy-client/src/test/resources/versions/clusters.json",
                "patch": "@@ -1,12 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:49178/api/v1/clusters\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster\",\n-      \"Clusters\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\",\n-        \"version\" : \"HDP-2.1\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/versions/clusters.json",
                "sha": "abaf084165187a9fb4ee0c804b5320e8c2f78436",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/versions/service-config.json",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/versions/service-config.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 58,
                "filename": "ambari-client/groovy-client/src/test/resources/versions/service-config.json",
                "patch": "@@ -1,58 +0,0 @@\n-{\n-  \"zk_user\": \"zookeeper-me\",\n-  \"zk_pid_file\": \"/var/run/zookeeper/zookeeper_server.pid\",\n-  \"zk_pid_dir\": \"/var/run/zookeeper\",\n-  \"zk_log_dir\": \"/var/log/zookeeper\",\n-  \"zk_data_dir\": \"/hadoop/zookeeper\",\n-  \"yarn_user\": \"yarn\",\n-  \"yarn_pid_dir_prefix\": \"/var/run/hadoop-yarn\",\n-  \"yarn_log_dir_prefix\": \"/var/log/hadoop-yarn\",\n-  \"yarn_heapsize\": \"1024\",\n-  \"user_group\": \"hadoop\",\n-  \"tickTime\": \"2000\",\n-  \"syncLimit\": \"5\",\n-  \"smokeuser\": \"ambari-qa\",\n-  \"security_enabled\": \"false\",\n-  \"rrdcached_base_dir\": \"/var/lib/ganglia/rrds\",\n-  \"resourcemanager_heapsize\": \"1024\",\n-  \"proxyuser_group\": \"users\",\n-  \"nodemanager_heapsize\": \"1024\",\n-  \"namenode_opt_newsize\": \"200m\",\n-  \"namenode_opt_maxnewsize\": \"200m\",\n-  \"namenode_opt_permsize\" : \"128m\",\n-  \"namenode_opt_maxpermsize\" : \"256m\",\n-  \"namenode_heapsize\": \"1024m\",\n-  \"namenode_formatted_mark_dir\": \"/var/run/hadoop/hdfs/namenode/formatted/\",\n-  \"ganglia_conf_dir\": \"/etc/ganglia/hdp\",\n-  \"dtnode_heapsize\": \"1024m\",\n-  \"dfs_webhdfs_enabled\": \"true\",\n-  \"dfs_replication\": \"3\",\n-  \"dfs_namenode_name_dir\": \"/hadoop/hdfs/namenode\",\n-  \"dfs_namenode_checkpoint_period\": \"21600\",\n-  \"dfs_namenode_checkpoint_dir\": \"/hadoop/hdfs/namesecondary\",\n-  \"dfs_datanode_http_address\": \"50075\",\n-  \"apptimelineserver_heapsize\": \"1024\",\n-  \"clientPort\": \"2181\",\n-  \"datanode_du_reserved\": \"1073741824\",\n-  \"dfs_block_local_path_access_user\": \"hbase\",\n-  \"dfs_datanode_address\": \"50010\",\n-  \"dfs_datanode_data_dir\": \"/hadoop/hdfs/data\",\n-  \"dfs_datanode_data_dir_perm\": \"750\",\n-  \"dfs_datanode_failed_volume_tolerated\": \"0\",\n-  \"ganglia_runtime_dir\": \"/var/run/ganglia/hdp\",\n-  \"gmetad_user\": \"nobody\",\n-  \"gmond_user\": \"nobody\",\n-  \"hadoop_heapsize\": \"1024\",\n-  \"hadoop_pid_dir_prefix\": \"/var/run/hadoop\",\n-  \"hdfs_log_dir_prefix\": \"/var/log/hadoop\",\n-  \"hdfs_user\": \"hdfs\",\n-  \"initLimit\": \"10\",\n-  \"kerberos_domain\": \"EXAMPLE.COM\",\n-  \"keytab_path\": \"/etc/security/keytabs\",\n-  \"lzo_enabled\": \"true\",\n-  \"mapred_log_dir_prefix\": \"/var/log/hadoop-mapreduce\",\n-  \"mapred_pid_dir_prefix\": \"/var/run/hadoop-mapreduce\",\n-  \"mapred_user\": \"mapred\",\n-  \"nagios_contact\": \"default@REPLACEME.NOWHERE\",\n-  \"nagios_web_password\": \"admin\"\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/versions/service-config.json",
                "sha": "c6efa66da9334b9f338ad80c7db50443c2640596",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/versions/service-versions.json",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/groovy-client/src/test/resources/versions/service-versions.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 37,
                "filename": "ambari-client/groovy-client/src/test/resources/versions/service-versions.json",
                "patch": "@@ -1,37 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations?type=yarn-log4j&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"yarn-log4j\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations?type=yarn-site&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"yarn-site\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    },\n-    {\n-          \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations?type=yarn-site&tag=2\",\n-          \"tag\" : \"2\",\n-          \"type\" : \"yarn-site\",\n-          \"Config\" : {\n-            \"cluster_name\" : \"MySingleNodeCluster\"\n-          }\n-    },\n-    {\n-      \"href\" : \"http://localhost:49178/api/v1/clusters/MySingleNodeCluster/configurations?type=zookeeper-log4j&tag=1\",\n-      \"tag\" : \"1\",\n-      \"type\" : \"zookeeper-log4j\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"MySingleNodeCluster\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/groovy-client/src/test/resources/versions/service-versions.json",
                "sha": "eaed568be642207ab1bda084de6c8b89a8068158",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/pom.xml",
                "changes": 75,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/pom.xml?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 75,
                "filename": "ambari-client/pom.xml",
                "patch": "@@ -1,75 +0,0 @@\n-<?xml version=\"1.0\"?>\n-<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n-         xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\">\n-  <!--\n-   Licensed to the Apache Software Foundation (ASF) under one or more\n-   contributor license agreements.  See the NOTICE file distributed with\n-   this work for additional information regarding copyright ownership.\n-   The ASF licenses this file to You under the Apache License, Version 2.0\n-   (the \"License\"); you may not use this file except in compliance with\n-   the License.  You may obtain a copy of the License at\n-\n-       http://www.apache.org/licenses/LICENSE-2.0\n-\n-   Unless required by applicable law or agreed to in writing, software\n-   distributed under the License is distributed on an \"AS IS\" BASIS,\n-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-   See the License for the specific language governing permissions and\n-   limitations under the License.\n--->\n-  <parent>\n-    <groupId>org.apache.ambari</groupId>\n-    <artifactId>ambari-project</artifactId>\n-    <version>2.0.0.0-SNAPSHOT</version>\n-    <relativePath>../ambari-project</relativePath>\n-  </parent>\n-  <modelVersion>4.0.0</modelVersion>\n-  <groupId>org.apache.ambari</groupId>\n-  <artifactId>ambari-client</artifactId>\n-  <packaging>pom</packaging>\n-  <version>2.0.0.0-SNAPSHOT</version>\n-  <name>Ambari Client</name>\n-  <description>Ambari client</description>\n-  <modules>\n-    <module>python-client</module>\n-    <module>groovy-client</module>\n-  </modules>\n-  <build>\n-    <plugins>\n-      <plugin>\n-        <artifactId>maven-assembly-plugin</artifactId>\n-        <configuration>\n-          <tarLongFileMode>gnu</tarLongFileMode>\n-          <descriptors>\n-            <descriptor>assemblies/client.xml</descriptor>\n-          </descriptors>\n-        </configuration>\n-        <executions>\n-          <execution>\n-            <id>build-tarball</id>\n-            <phase>prepare-package</phase>\n-            <goals>\n-              <goal>single</goal>\n-            </goals>\n-          </execution>\n-        </executions>\n-      </plugin>\n-      <plugin>\n-        <groupId>org.vafer</groupId>\n-        <artifactId>jdeb</artifactId>\n-        <version>1.0.1</version>\n-        <executions>\n-          <execution>\n-            <phase>none</phase>\n-            <goals>\n-              <goal>jdeb</goal>\n-            </goals>\n-          </execution>\n-        </executions>\n-        <configuration>\n-          <controlDir>${basedir}/../ambari-project/src/main/package/deb/control</controlDir>\n-        </configuration>\n-      </plugin>\n-    </plugins>\n-  </build>\n-</project>",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/pom.xml",
                "sha": "3c6fd9939aca97cee7976328bdacca93b2a1453d",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/pom.xml",
                "changes": 207,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/pom.xml?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 207,
                "filename": "ambari-client/python-client/pom.xml",
                "patch": "@@ -1,207 +0,0 @@\n-<?xml version=\"1.0\"?>\n-<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\">\n-  <!--\n-   Licensed to the Apache Software Foundation (ASF) under one or more\n-   contributor license agreements.  See the NOTICE file distributed with\n-   this work for additional information regarding copyright ownership.\n-   The ASF licenses this file to You under the Apache License, Version 2.0\n-   (the \"License\"); you may not use this file except in compliance with\n-   the License.  You may obtain a copy of the License at\n-\n-       http://www.apache.org/licenses/LICENSE-2.0\n-\n-   Unless required by applicable law or agreed to in writing, software\n-   distributed under the License is distributed on an \"AS IS\" BASIS,\n-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-   See the Licenseam for the specific language governing permissions and\n-   limitations under the License.\n--->\n-  <parent>\n-    <groupId>org.apache.ambari</groupId>\n-    <artifactId>ambari-client</artifactId>\n-    <version>2.0.0.0-SNAPSHOT</version>\n-    <relativePath>../../ambari-client</relativePath>\n-  </parent>\n-  <modelVersion>4.0.0</modelVersion>\n-  <groupId>org.apache.ambari</groupId>\n-  <artifactId>python-client</artifactId>\n-  <packaging>pom</packaging>\n-  <version>2.0.0.0-SNAPSHOT</version>\n-  <name>Ambari Python Client</name>\n-  <description>Ambari Python client</description>\n-  <properties>\n-    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n-    <final.name>${project.artifactId}-${project.version}</final.name>\n-    <package.release>1</package.release>\n-    <package.prefix>/usr</package.prefix>\n-    <package.log.dir>/var/log/python-client</package.log.dir>\n-    <package.pid.dir>/var/run/python-client</package.pid.dir>\n-    <skipTests>false</skipTests>\n-    <install.dir>/usr/lib/python2.6/site-packages/ambari_client</install.dir>\n-    <lib.dir>/usr/lib/ambari-client/lib</lib.dir>\n-    <python.ver>python &gt;= 2.6</python.ver>\n-    <deb.python.ver>python (&gt;= 2.6)</deb.python.ver>\n-    <deb.architecture>amd64</deb.architecture>\n-    <deb.dependency.list>openssl, zlibc, ${deb.python.ver}</deb.dependency.list>\n-  </properties>\n-  <build>\n-    <plugins>\n-      <plugin>\n-        <artifactId>maven-compiler-plugin</artifactId>\n-        <version>3.0</version>\n-      </plugin>\n-      <plugin>\n-        <artifactId>maven-assembly-plugin</artifactId>\n-        <configuration>\n-          <tarLongFileMode>gnu</tarLongFileMode>\n-          <descriptors>\n-            <descriptor>src/packages/tarball/all.xml</descriptor>\n-          </descriptors>\n-        </configuration>\n-        <executions>\n-          <execution>\n-            <id>build-tarball</id>\n-            <phase>prepare-package</phase>\n-            <goals>\n-              <goal>single</goal>\n-            </goals>\n-          </execution>\n-        </executions>\n-      </plugin>\n-      <plugin>\n-        <groupId>org.codehaus.mojo</groupId>\n-        <artifactId>exec-maven-plugin</artifactId>\n-        <version>1.2.1</version>\n-        <executions>\n-          <execution>\n-            <configuration>\n-              <executable>python</executable>\n-              <workingDirectory>src/test/python</workingDirectory>\n-              <arguments>\n-                <argument>unitTests.py</argument>\n-              </arguments>\n-              <environmentVariables>\n-                <PYTHONPATH>${project.basedir}/../../ambari-common/src/test/python:${project.basedir}/src/main/python/ambari_client:${project.basedir}/src/test/python/utils:$PYTHONPATH</PYTHONPATH>\n-              </environmentVariables>\n-              <skip>${skipTests}</skip>\n-            </configuration>\n-            <id>python-test</id>\n-            <phase>test</phase>\n-            <goals>\n-              <goal>exec</goal>\n-            </goals>\n-          </execution>\n-          <execution>\n-            <configuration>\n-              <executable>python</executable>\n-              <workingDirectory>target/python-client-${project.version}</workingDirectory>\n-              <arguments>\n-                <argument>${project.basedir}/src/main/python/setup.py</argument>\n-                <argument>clean</argument>\n-                <argument>bdist_dumb</argument>\n-              </arguments>\n-              <environmentVariables>\n-                <PYTHONPATH>target/python-client-${project.version}:$PYTHONPATH</PYTHONPATH>\n-              </environmentVariables>\n-            </configuration>\n-            <id>python-package</id>\n-            <phase>package</phase>\n-            <goals>\n-              <goal>exec</goal>\n-            </goals>\n-          </execution>\n-        </executions>\n-      </plugin>\n-      <plugin>\n-        <groupId>org.codehaus.mojo</groupId>\n-        <artifactId>rpm-maven-plugin</artifactId>\n-        <version>2.1-alpha-2</version>\n-        <executions>\n-          <execution>\n-            <phase>none</phase>\n-            <goals>\n-              <goal>rpm</goal>\n-            </goals>\n-          </execution>\n-        </executions>\n-        <configuration>\n-          <copyright>2012, Apache Software Foundation</copyright>\n-          <group>Development</group>\n-          <description>Maven Recipe: RPM Package.</description>\n-          <requires>\n-            <require>openssl</require>\n-            <require>zlib</require>\n-            <require>${python.ver}</require>\n-          </requires>\n-          <needarch>x86_64</needarch>\n-          <autoRequires>false</autoRequires>\n-          <mappings>\n-            <mapping>\n-              <directory>${install.dir}</directory>\n-              <sources>\n-                <source>\n-                  <location>${project.build.directory}/${project.artifactId}-${project.version}/ambari_client</location>\n-                </source>\n-              </sources>\n-            </mapping>\n-          </mappings>\n-        </configuration>\n-      </plugin>\n-      <plugin>\n-        <groupId>org.apache.rat</groupId>\n-        <artifactId>apache-rat-plugin</artifactId>\n-        <configuration>\n-          <excludes>\n-            <exclude>src/examples/*</exclude>\n-            <exclude>src/test/python/dummy*.txt</exclude>\n-            <exclude>src/main/python/ambari_client/imports.txt</exclude>\n-            <exclude>src/main/puppet/modules/stdlib/**</exclude>\n-            <exclude>**/*.erb</exclude>\n-            <exclude>**/*.json</exclude>\n-          </excludes>\n-        </configuration>\n-        <executions>\n-          <execution>\n-            <phase>test</phase>\n-            <goals>\n-              <goal>check</goal>\n-            </goals>\n-          </execution>\n-        </executions>\n-      </plugin>\n-      <plugin>\n-        <groupId>org.vafer</groupId>\n-        <artifactId>jdeb</artifactId>\n-        <version>1.0.1</version>\n-        <executions>\n-          <execution>\n-            <phase>none</phase>\n-            <goals>\n-              <goal>jdeb</goal>\n-            </goals>\n-          </execution>\n-        </executions>\n-        <configuration>\n-          <controlDir>${basedir}/src/main/package/deb/control</controlDir>\n-          <deb>${basedir}/target/${project.artifactId}_${package-version}-${package-release}.deb</deb>\n-          <dataSet>\n-            <data>\n-              <src>${project.build.directory}/${project.artifactId}-${project.version}/ambari_client</src>\n-              <type>directory</type>\n-              <mapper>\n-                <type>perm</type>\n-                <prefix>${install.dir}</prefix>\n-              </mapper>\n-            </data>\n-          </dataSet>\n-        </configuration>\n-      </plugin>\n-    </plugins>\n-    <extensions>\n-      <extension>\n-        <groupId>org.apache.maven.wagon</groupId>\n-        <artifactId>wagon-ssh-external</artifactId>\n-      </extension>\n-    </extensions>\n-  </build>\n-</project>",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/pom.xml",
                "sha": "b8cfb94250b7ab6dbe887464c9838555d66bbd33",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/examples/create.py",
                "changes": 147,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/examples/create.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 147,
                "filename": "ambari-client/python-client/src/examples/create.py",
                "patch": "@@ -1,147 +0,0 @@\n- #\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-# \n-#      http://www.apache.org/licenses/LICENSE-2.0\n-# \n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-import os \n-import sys \n-import logging\n-from ambari_client.ambari_api import  AmbariClient \n-\n-\n-def main():\n-\n-    path = os.getcwd() ;\n-    print path\n-    sys.path.append(path)\n-    \n-    logging.basicConfig(filename=\"ambari_api.log\", level=logging.DEBUG , filemode=\"w\")\n-    logging.info(\"Program started\")\n-     \n-     \n-     \n-    client = AmbariClient(\"localhost\", 8080, \"admin\", \"admin\", version=1)\n-    print client.version\n-    print client.host_url\n-    print\"\\n\"\n-    \n-    \n-    ###############################\n-    # cluster creation\n-    ###############################\n-    # 1) create cluster\n-    cluster = client.create_cluster(\"test33\", \"HDP-1.3.0\")\n-    print cluster\n-    \n-\n-    cluster = client.get_cluster('test33')\n-    print cluster\n-    print cluster.to_json_dict()\n-    print\"\\n\"\n-    \n-    # 2) create services\n-    services_list = [\"HDFS\", \"MAPREDUCE\", \"NAGIOS\", \"GANGLIA\"]\n-    s2 = cluster.create_services(services_list)\n-    print s2\n-    \n-    s2 = cluster.create_service(\"ZOOKEEPER\")\n-    print s2\n-    \n-    # 3) create global config\n-    s3 = cluster.add_config(\"global\", \"version1\" , {})\n-    print s3\n-    s3 = cluster.add_config(\"core-site\", \"version1\" , {})\n-    print s3\n-    s3 = cluster.add_config(\"hdfs-site\", \"version1\" , {})\n-    print s3\n-    s3 = cluster.add_config(\"mapred-site\", \"version1\" , {})\n-    print s3\n-#    s3 = cluster.add_config(\"hbase-site\", \"version1\" , {})\n-#    print s3\n-#    s3 = cluster.add_config(\"oozie-site\", \"version1\" , {})\n-#    print s3\n-#    s3 = cluster.add_config(\"hive-site\", \"version1\" , {})\n-#    print s3\n-#    s3 = cluster.add_config(\"webhcat-site\", \"version1\" , {})\n-#    print s3\n-    \n-    \n-    \n-    \n-#    hdfs_components = client.get_components(\"1.3.0\", \"HDFS\")\n-#    print hdfs_components\n-#    mr_components = client.get_components(\"1.3.0\", \"MAPREDUCE\")\n-#    print mr_components\n-#    ganglia_components = client.get_components(\"1.3.0\", \"GANGLIA\")\n-#    print ganglia_components\n-#    nagios_components = client.get_components(\"1.3.0\", \"NAGIOS\")\n-#    print nagios_components\n-\n-\n-    # 4) add service components\n-    s2 = cluster.create_service_components(\"1.3.0\", \"HDFS\")\n-    print s2    \n-    s2 = cluster.create_service_components(\"1.3.0\", \"MAPREDUCE\")\n-    print s2 \n-    s2 = cluster.create_service_components(\"1.3.0\", \"GANGLIA\")\n-    print s2 \n-    s2 = cluster.create_service_components(\"1.3.0\", \"NAGIOS\")\n-    print s2 \n-\n-\n-    all_hosts = client.get_all_hosts()\n-    h_l = [x.host_name for x in all_hosts]\n-    print h_l\n-    \n-    # 5) add hosts\n-    s3 = cluster.create_hosts(h_l)\n-    print s3\n-    print\"\\n\"\n-    \n-    # 6) add hosts roles\n-    host1 = cluster.get_host('r01wn01')\n-    print host1\n-    s4 = host1.assign_role(\"NAMENODE\")\n-    print s4\n-    print\"\\n\"\n-     \n-    # 7) add hosts roles\n-    s4 = cluster.start_all_services()\n-    print s4\n-    print\"\\n\"\n-\n-\n-    all_clusters = client.get_all_clusters()\n-    print all_clusters.to_json_dict()\n-    print all_clusters\n-    print\"\\n\"\n-    \n-    all_hosts = client.get_all_hosts()\n-    print all_hosts\n-    print all_hosts.to_json_dict()\n-    print\"\\n\"\n-    \n-    serviceList = cluster.get_all_services()\n-    print serviceList\n-    print serviceList.to_json_dict()\n-    print\"\\n\"\n-########################################################################\n-#\n-# The \"main\" entry\n-#\n-########################################################################\n-if __name__ == '__main__':\n-    main()\n-######################################################################## ",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/examples/create.py",
                "sha": "c5f2658d34f52eac8573f1cb1df1803c9fa45298",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/examples/delete.py",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/examples/delete.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 66,
                "filename": "ambari-client/python-client/src/examples/delete.py",
                "patch": "@@ -1,66 +0,0 @@\n- #\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-# \n-#      http://www.apache.org/licenses/LICENSE-2.0\n-# \n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-import os \n-import sys \n-import logging\n-from ambari_client.ambari_api import  AmbariClient \n-\n-\n-def main():\n-\n-    path = os.getcwd() ;\n-    print path\n-    sys.path.append(path)\n-    \n-    logging.basicConfig(filename=\"ambari_api.log\", level=logging.DEBUG , filemode=\"w\")\n-    logging.info(\"Program started\")\n-     \n-     \n-     \n-    client = AmbariClient(\"localhost\", 8080, \"admin\", \"admin\", version=1)\n-    print client.version\n-    print client.host_url\n-    print\"\\n\"\n-    \n-    ###############################\n-    # cluster delete\n-    ###############################\n-    print dir(client)\n-    \n-    s1 = client.delete_cluster(\"test33\")\n-    print s1\n-    print s1.to_json_dict()\n-    \n-    s1 = client.delete_host(\"test33\")\n-    print s1\n-    print s1.to_json_dict()\n-    \n-    s1 = client.delete_host(\"r01wn01\")\n-    print s1\n-    print s1.to_json_dict()\n-    \n-\n-\n-########################################################################\n-#\n-# The \"main\" entry\n-#\n-########################################################################\n-if __name__ == '__main__':\n-    main()\n-######################################################################## ",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/examples/delete.py",
                "sha": "3debc4c3969b49835060b023f66829e5f94d5320",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/examples/example.py",
                "changes": 133,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/examples/example.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 133,
                "filename": "ambari-client/python-client/src/examples/example.py",
                "patch": "@@ -1,133 +0,0 @@\n-#\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-# \n-#      http://www.apache.org/licenses/LICENSE-2.0\n-# \n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-import os \n-import sys \n-import logging\n-from ambari_client.ambari_api import  AmbariClient \n-\n-\n-def main():\n-\n-    path = os.getcwd() ;\n-    print path\n-    sys.path.append(path)\n-    \n-    logging.basicConfig(filename=\"ambari_api.log\", level=logging.DEBUG , filemode=\"w\")\n-    logging.info(\"Program started\")\n-     \n-     \n-     \n-    client = AmbariClient(\"localhost\", 8080, \"admin\", \"admin\", version=1)\n-    print client.version\n-    print client.host_url\n-    print\"\\n\"\n-    \n-    ######################################\n-    #    High level\n-    ######################################\n-    all_clusters = client.get_all_clusters()\n-    print all_clusters.to_json_dict()\n-    print all_clusters\n-    print\"\\n\"\n-    \n-    all_hosts = client.get_all_hosts()\n-    print all_hosts\n-    print all_hosts.to_json_dict()\n-    print\"\\n\"\n-\n-    \n-    ######################################\n-    #    going into a specific cluster\n-    ######################################\n-    cluster = client.get_cluster('test46')\n-    print cluster\n-    print cluster.to_json_dict()\n-    print\"\\n\"\n-\n-\n-    clusters_hosts = cluster.get_all_hosts()\n-    print clusters_hosts.to_json_dict()\n-    print clusters_hosts\n-    print\"\\n\"\n-\n-\n-    #host1 = cluster.get_host('r01wn01')\n-    host1 = cluster.get_host('r01hn01')\n-    print host1\n-    print host1.clusterRef.cluster_name\n-    print host1.to_json_dict()\n-    print\"\\n\"\n-    \n-    \n-    host1_comp = host1.get_host_components()\n-    print host1_comp\n-    print host1_comp.to_json_dict()\n-    print\"\\n\"\n-\n-\n-    nn = host1.get_host_component(\"NAMENODE\")\n-    print nn\n-    print nn.to_json_dict()\n-    print nn.clusterRef.cluster_name\n-    print\"\\n\"\n-\n-\n-    serviceList = cluster.get_all_services()\n-    print serviceList\n-    print serviceList.to_json_dict()\n-    print\"\\n\"\n-\n-\n-    ganglia = cluster.get_service(\"GANGLIA\")     \n-    print  ganglia  \n-    print ganglia.to_json_dict()\n-    print\"\\n\"\n-   \n-   \n-    ganglia_comps = ganglia.get_service_components()\n-    print ganglia_comps  \n-    print ganglia_comps.to_json_dict()\n-    print\"\\n\"\n-   \n-   \n-    ganglia_comp1 = ganglia.get_service_component('GANGLIA_MONITOR')\n-    print ganglia_comp1  \n-    print ganglia_comp1.to_json_dict()\n-    print ganglia_comp1.clusterRef.cluster_name\n-    print\"\\n\"\n-    \n-    s = client.get_config(\"1.3.0\", \"HDFS\")\n-    print s\n-    print\"\\n\"\n-     \n-    s = client.get_components(\"1.3.0\", \"HDFS\")\n-    print s   \n-    \n-#    ganglia.stop()\n-#    ganglia.start()\n-       \n-\n-    \n-########################################################################\n-#\n-# The \"main\" entry\n-#\n-########################################################################\n-if __name__ == '__main__':\n-    main()\n-######################################################################## ",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/examples/example.py",
                "sha": "6702edac8e6bb06002c0bfb51b72db629d9d64dd",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/examples/main.py",
                "changes": 166,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/examples/main.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 166,
                "filename": "ambari-client/python-client/src/examples/main.py",
                "patch": "@@ -1,166 +0,0 @@\n- #\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-# \n-#      http://www.apache.org/licenses/LICENSE-2.0\n-# \n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-import os \n-import sys \n-import logging\n-from ambari_client.ambari_api import  AmbariClient \n-\n-\n-def main():\n-\n-    path = os.getcwd() ;\n-    print path\n-    sys.path.append(path)\n-    \n-    logging.basicConfig(filename=\"ambari_api.log\", level=logging.DEBUG , filemode=\"w\")\n-    logging.info(\"Program started\")\n-     \n-     \n-     \n-    client = AmbariClient(\"localhost\", 8080, \"admin\", \"admin\", version=1)\n-    print client.version\n-    print client.host_url\n-    print\"\\n\"\n-    \n-    ######################################\n-    #    High level\n-    ######################################\n-    all_clusters = client.get_all_clusters()\n-    print all_clusters.to_json_dict()\n-    print all_clusters\n-\n-    \n-    all_hosts = client.get_all_hosts()\n-    print all_hosts\n-    print all_hosts.to_json_dict()\n-    print\"\\n\"\n-\n-    \n-    ######################################\n-    #    going into a specific cluster\n-    ######################################\n-    cluster = client.get_cluster('test46')\n-    print cluster\n-    print cluster.to_json_dict()\n-    print\"\\n\"\n-\n-\n-    clusters_hosts = cluster.get_all_hosts()\n-    print clusters_hosts.to_json_dict()\n-    print clusters_hosts\n-    print\"\\n\"\n-\n-\n-    host1 = cluster.get_host('r01wn01')\n-    print host1\n-    print host1.clusterRef.cluster_name\n-    print host1.to_json_dict()\n-    print\"\\n\"\n-    \n-    print \"==================== host components ====================\\n\"\n-    host1_comp = host1.get_host_components()\n-    print host1_comp\n-    print host1_comp.to_json_dict()\n-    print\"\\n\"\n-\n-\n-    nn = host1.get_host_component(\"NAMENODE\")\n-    print nn\n-    print nn.to_json_dict()\n-    print nn.clusterRef.cluster_name\n-    metric_json = nn.get_metrics()\n-    print metric_json[\"metrics\"][\"cpu\"]\n-    print\"\\n\"\n-\n-\n-    serviceList = cluster.get_all_services()\n-    print serviceList\n-    print serviceList.to_json_dict()\n-    print\"\\n\"\n-\n-\n-    ganglia = cluster.get_service(\"GANGLIA\")     \n-    print  ganglia  \n-    print ganglia.to_json_dict()\n-    print\"\\n\"\n-   \n-   \n-    print \"==================== service components ====================\\n\"\n-    ganglia_comps = ganglia.get_service_components()\n-    print ganglia_comps  \n-    print ganglia_comps.to_json_dict()\n-    print\"\\n\"\n-   \n-   \n-    ganglia_comp1 = ganglia.get_service_component('GANGLIA_MONITOR')\n-    print ganglia_comp1  \n-    print ganglia_comp1.to_json_dict()\n-    print ganglia_comp1.clusterRef.cluster_name\n-    print\"\\n\"\n-    \n-    mr = cluster.get_service(\"MAPREDUCE\")     \n-    print  mr  \n-    print mr.to_json_dict()\n-    print\"\\n\"\n-    \n-    mr_comp1 = mr.get_service_component('TASKTRACKER')\n-    print mr_comp1  \n-    print mr_comp1.to_json_dict()\n-    print mr_comp1.clusterRef.cluster_name\n-    metric_json = mr_comp1.get_metrics()\n-    print metric_json[\"metrics\"][\"cpu\"]\n-    print\"\\n\"\n-    \n-    ######################################\n-    #    configurations\n-    ######################################\n-    hdfs_config = cluster.get_hdfs_site_config()\n-    print hdfs_config\n-    print hdfs_config.properties\n-    \n-    global_config = cluster.get_global_config()\n-    core_config = cluster.get_core_site_config()\n-    mapred_config = cluster.get_mapred_site_config()\n-    print global_config \n-    print core_config \n-    print mapred_config\n-    print global_config.clusterRef.cluster_name\n-    print core_config.clusterRef.cluster_name\n-    print mapred_config.clusterRef.cluster_name\n-    \n-    hdfs_config.properties[\"dfs.replication.max\"] = 51\n-    #print hdfs_config.properties \n-    hdfs_config1 = cluster.update_hdfs_site_config(hdfs_config)\n-    print hdfs_config1.properties \n-    \n-    \n-    ######################################\n-    #    create cluster\n-    ######################################\n-#    ganglia.stop()\n-#    ganglia.start()\n-       \n-\n-########################################################################\n-#\n-# The \"main\" entry\n-#\n-########################################################################\n-if __name__ == '__main__':\n-    main()\n-######################################################################## ",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/examples/main.py",
                "sha": "04db4c0eecfd25c3a3002da400a3668785c7f371",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/examples/trial.py",
                "changes": 145,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/examples/trial.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 145,
                "filename": "ambari-client/python-client/src/examples/trial.py",
                "patch": "@@ -1,145 +0,0 @@\n- #\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-# \n-#      http://www.apache.org/licenses/LICENSE-2.0\n-# \n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-import os \n-import sys \n-import logging\n-from ambari_client.ambari_api import  AmbariClient \n-\n-\n-def main():\n-\n-    path = os.getcwd() ;\n-    print path\n-    sys.path.append(path)\n-    \n-    logging.basicConfig(filename=\"ambari_api.log\", level=logging.DEBUG , filemode=\"w\")\n-    logging.info(\"Program started\")\n-     \n-     \n-     \n-    client = AmbariClient(\"localhost\", 8080, \"admin\", \"admin\", version=1)\n-    print client.version\n-    print client.host_url\n-    print\"\\n\"\n-    \n-#    s = client.get_config(\"1.3.0\", \"HDFS\")\n-#    print s\n-#    \n-#    s = client.get_components(\"1.3.0\", \"HDFS\")\n-#    print s\n-#\n-#    mycluster = client.create_cluster(\"test46\", \"HDP-1.3.0\")\n-#    print mycluster\n-#\n-    mycluster = client.get_cluster('test46')\n-    print mycluster\n-    print mycluster.to_json_dict()\n-    print\"\\n\"\n-#    \n-#    services_list = [\"HDFS\", \"MAPREDUCE\", \"NAGIOS\", \"GANGLIA\"]\n-#    s2 = mycluster.create_services(services_list)\n-#    print s2\n-#\n-#\n-#    propr_dict = {\"dfs_name_dir\":\"/data/1/hadoop/hdfs/namenode,/data/2/hadoop/hdfs/namenode,/data/3/hadoop/hdfs/namenode,/data/4/hadoop/hdfs/namenode,/data/5/hadoop/hdfs/namenode,/data/6/hadoop/hdfs/namenode,/data/7/hadoop/hdfs/namenode,/data/8/hadoop/hdfs/namenode\", \"namenode_heapsize\":\"1024m\", \"namenode_opt_newsize\":\"200m\", \"fs_checkpoint_dir\":\"/data/1/hadoop/hdfs/namesecondary\", \"dfs_data_dir\":\"/data/1/hadoop/hdfs/data,/data/2/hadoop/hdfs/data,/data/3/hadoop/hdfs/data,/data/4/hadoop/hdfs/data,/data/5/hadoop/hdfs/data,/data/6/hadoop/hdfs/data,/data/7/hadoop/hdfs/data,/data/8/hadoop/hdfs/data,/data/9/hadoop/hdfs/data,/data/10/hadoop/hdfs/data\", \"dtnode_heapsize\":\"1024m\", \"dfs_datanode_failed_volume_tolerated\":\"0\", \"dfs_webhdfs_enabled\":\"true\", \"hadoop_heapsize\":\"1024\", \"datanode_du_reserved\":\"0\", \"fs_checkpoint_period\":\"21600\", \"fs_checkpoint_size\":\"67108864\", \"hdfs_log_dir_prefix\":\"/var/log/hadoop\", \"hadoop_pid_dir_prefix\":\"/var/run/hadoop\", \"namenode_opt_maxnewsize\":\"200m\", \"dfs_exclude\":\"dfs.exclude\", \"dfs_include\":\"dfs.include\", \"dfs_replication\":\"3\", \"dfs_block_local_path_access_user\":\"hbase\", \"dfs_datanode_data_dir_perm\":\"750\", \"security_enabled\":\"false\", \"namenode_formatted_mark_dir\":\"/var/run/hadoop/hdfs/namenode/formatted/\", \"hcat_conf_dir\":\"\", \"jtnode_opt_newsize\":\"200m\", \"jtnode_opt_maxnewsize\":\"200m\", \"jtnode_heapsize\":\"1024m\", \"mapred_local_dir\":\"/data/1/hadoop/mapred,/data/2/hadoop/mapred,/data/3/hadoop/mapred,/data/4/hadoop/mapred,/data/5/hadoop/mapred,/data/6/hadoop/mapred,/data/7/hadoop/mapred,/data/8/hadoop/mapred,/data/9/hadoop/mapred,/data/10/hadoop/mapred\", \"mapred_map_tasks_max\":\"4\", \"mapred_red_tasks_max\":\"2\", \"mapred_child_java_opts_sz\":\"768\", \"scheduler_name\":\"org.apache.hadoop.mapred.CapacityTaskScheduler\", \"mapred_cluster_map_mem_mb\":\"1536\", \"mapred_cluster_red_mem_mb\":\"2048\", \"mapred_cluster_max_map_mem_mb\":\"6144\", \"mapred_cluster_max_red_mem_mb\":\"4096\", \"mapred_job_map_mem_mb\":\"1536\", \"mapred_job_red_mem_mb\":\"2048\", \"io_sort_mb\":\"200\", \"io_sort_spill_percent\":\"0.9\", \"mapreduce_userlog_retainhours\":\"24\", \"maxtasks_per_job\":\"-1\", \"lzo_enabled\":\"true\", \"snappy_enabled\":\"true\", \"rca_enabled\":\"true\", \"mapred_system_dir\":\"/mapred/system\", \"mapred_hosts_exclude\":\"mapred.exclude\", \"mapred_hosts_include\":\"mapred.include\", \"mapred_jobstatus_dir\":\"file:////mapred/jobstatus\", \"nagios_web_login\":\"nagiosadmin\", \"nagios_web_password\":\"admin\", \"nagios_contact\":\"admin@admin.com\", \"nagios_group\":\"nagios\", \"hbase_conf_dir\":\"/etc/hbase\", \"proxyuser_group\":\"users\", \"dfs_datanode_address\":\"50010\", \"dfs_datanode_http_address\":\"50075\", \"apache_artifacts_download_url\":\"\", \"ganglia_runtime_dir\":\"/var/run/ganglia/hdp\", \"java64_home\":\"/usr/jdk/jdk1.6.0_31\", \"run_dir\":\"/var/run/hadoop\", \"hadoop_conf_dir\":\"/etc/hadoop\", \"hdfs_user\":\"hdfs\", \"mapred_user\":\"mapred\", \"hbase_user\":\"hbase\", \"hive_user\":\"hive\", \"hcat_user\":\"hcat\", \"webhcat_user\":\"hcat\", \"oozie_user\":\"oozie\", \"zk_user\":\"zookeeper\", \"gmetad_user\":\"nobody\", \"gmond_user\":\"nobody\", \"nagios_user\":\"nagios\", \"smokeuser\":\"ambari-qa\", \"user_group\":\"hadoop\", \"rrdcached_base_dir\":\"/var/lib/ganglia/rrds\"}\n-#    print propr_dict\n-#    s3 = mycluster.add_config(\"global\", \"version1\" , propr_dict)\n-#    print s3\n-#\n-#    s2 = mycluster.create_service_components(\"1.3.0\", \"HDFS\")\n-#    print s2    \n-#    s2 = mycluster.create_service_components(\"1.3.0\", \"MAPREDUCE\")\n-#    print s2 \n-#    s2 = mycluster.create_service_components(\"1.3.0\", \"GANGLIA\")\n-#    print s2 \n-#    s2 = mycluster.create_service_components(\"1.3.0\", \"NAGIOS\")\n-#    print s2 \n-#\n-#    h_l = ['apspal44-83', 'apspal44-84', 'apspal44-85', 'apspal44-86', 'apspal44-87', 'apspal44-88', 'apspal44-89', 'r01hn01', 'r01wn01', 'r01wn02', 'r01wn03']\n-#    print h_l\n-#    s3 = mycluster.create_hosts(h_l)\n-#    print s3\n-#    print\"\\n\"\n-#\n-#    # 6) add hosts roles\n-#    host1 = mycluster.get_host('r01hn01')\n-#    print host1\n-#    s4 = host1.assign_role(\"NAMENODE\")\n-#    print s4\n-#    print\"\\n\"\n-#\n-#    s4 = mycluster.install_all_services()\n-#    print s4\n-#    print\"\\n\"\n-\n-#    s4 = mycluster.start_all_services(run_smoke_test=True)\n-#    print s4\n-#    print\"\\n\"\n-#\n-#    s4 = mycluster.stop_all_services()\n-#    print s4\n-#    print\"\\n\"\n-\n-#    s2 = mycluster.create_service(\"ZOOKEEPER\")\n-#    print s2\n-    \n-#    s2 = mycluster.create_service_components(\"1.3.0\", \"ZOOKEEPER\")\n-#    print s2\n-#    \n-\n-\n-\n-#    host1 = mycluster.get_host('r01wn01')\n-#    print host1\n-#    s4 = host1.assign_role(\"ZOOKEEPER_SERVER\")\n-#    print s4\n-#    host1 = mycluster.get_host('r01wn02')\n-#    print host1\n-#    s4 = host1.assign_role(\"ZOOKEEPER_SERVER\")\n-#    print s4\n-#    host1 = mycluster.get_host('r01wn03')\n-#    print host1\n-#    s4 = host1.assign_role(\"ZOOKEEPER_SERVER\")\n-#    print s4\n-#    host1 = mycluster.get_host('r01wn03')\n-#    print host1\n-#    s4 = host1.assign_role(\"ZOOKEEPER_CLIENT\")\n-#    print s4\n-\n-    zk = mycluster.get_service(\"ZOOKEEPER\")\n-    print zk\n-    \n-    s = zk.stop()\n-    print s\n-    \n-    \n-    ######################################\n-    #    create cluster\n-    ######################################\n-#    ganglia.stop()\n-#    ganglia.start()\n-       \n-\n-########################################################################\n-#\n-# The \"main\" entry\n-#\n-########################################################################\n-if __name__ == '__main__':\n-    main()\n-######################################################################## ",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/examples/trial.py",
                "sha": "5a7a7aff67ba7bf091556b58b9c40e0be55d9833",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/package/deb/control/control",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/package/deb/control/control?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 22,
                "filename": "ambari-client/python-client/src/main/package/deb/control/control",
                "patch": "@@ -1,22 +0,0 @@\n-# Licensed to the Apache Software Foundation (ASF) under one or more\n-# contributor license agreements.  See the NOTICE file distributed with\n-# this work for additional information regarding copyright ownership.\n-# The ASF licenses this file to You under the Apache License, Version 2.0\n-# (the \"License\"); you may not use this file except in compliance with\n-# the License.  You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License\n-Package: [[artifactId]]\n-Version: [[package-version]]-[[package-release]]\n-Section: [[deb.section]]\n-Priority: [[deb.priority]]\n-Depends: [[deb.dependency.list]]\n-Architecture: [[deb.architecture]]\n-Description: [[description]]\n-Maintainer: [[deb.publisher]]",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/package/deb/control/control",
                "sha": "40cd85513c284d6ad8c639815ee9a2e845fd3425",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/package/deb/control/postinst",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/package/deb/control/postinst?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 15,
                "filename": "ambari-client/python-client/src/main/package/deb/control/postinst",
                "patch": "@@ -1,15 +0,0 @@\n-#!/bin/bash\n-# Licensed to the Apache Software Foundation (ASF) under one or more\n-# contributor license agreements.  See the NOTICE file distributed with\n-# this work for additional information regarding copyright ownership.\n-# The ASF licenses this file to You under the Apache License, Version 2.0\n-# (the \"License\"); you may not use this file except in compliance with\n-# the License.  You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/package/deb/control/postinst",
                "sha": "21a01faa534c3ca8f39ea13d009d596ef973f2dd",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/package/deb/control/postrm",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/package/deb/control/postrm?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 15,
                "filename": "ambari-client/python-client/src/main/package/deb/control/postrm",
                "patch": "@@ -1,15 +0,0 @@\n-#!/bin/bash\n-# Licensed to the Apache Software Foundation (ASF) under one or more\n-# contributor license agreements.  See the NOTICE file distributed with\n-# this work for additional information regarding copyright ownership.\n-# The ASF licenses this file to You under the Apache License, Version 2.0\n-# (the \"License\"); you may not use this file except in compliance with\n-# the License.  You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/package/deb/control/postrm",
                "sha": "21a01faa534c3ca8f39ea13d009d596ef973f2dd",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/package/deb/control/preinst",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/package/deb/control/preinst?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 15,
                "filename": "ambari-client/python-client/src/main/package/deb/control/preinst",
                "patch": "@@ -1,15 +0,0 @@\n-#!/bin/bash\n-# Licensed to the Apache Software Foundation (ASF) under one or more\n-# contributor license agreements.  See the NOTICE file distributed with\n-# this work for additional information regarding copyright ownership.\n-# The ASF licenses this file to You under the Apache License, Version 2.0\n-# (the \"License\"); you may not use this file except in compliance with\n-# the License.  You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/package/deb/control/preinst",
                "sha": "21a01faa534c3ca8f39ea13d009d596ef973f2dd",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/package/deb/control/prerm",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/package/deb/control/prerm?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 15,
                "filename": "ambari-client/python-client/src/main/package/deb/control/prerm",
                "patch": "@@ -1,15 +0,0 @@\n-#!/bin/bash\n-# Licensed to the Apache Software Foundation (ASF) under one or more\n-# contributor license agreements.  See the NOTICE file distributed with\n-# this work for additional information regarding copyright ownership.\n-# The ASF licenses this file to You under the Apache License, Version 2.0\n-# (the \"License\"); you may not use this file except in compliance with\n-# the License.  You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/package/deb/control/prerm",
                "sha": "21a01faa534c3ca8f39ea13d009d596ef973f2dd",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/__init__.py",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/__init__.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 16,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/__init__.py",
                "patch": "@@ -1,16 +0,0 @@\n-#\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/__init__.py",
                "sha": "278df2e7b66ed24abc80868620a7156e055d6b5e",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/ambari_api.py",
                "changes": 301,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/ambari_api.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 301,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/ambari_api.py",
                "patch": "@@ -1,301 +0,0 @@\n-#\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-import logging\n-from ambari_client.core.http_client import HttpClient\n-from ambari_client.core.rest_resource import RestResource\n-from ambari_client.model import blueprint, stack, cluster, host, status\n-\n-__docformat__ = \"epytext\"\n-\n-LOG = logging.getLogger(__name__)\n-\n-\n-API_VERSION = 1\n-\n-\n-class AmbariClient(RestResource):\n-\n-    \"\"\"\n-    AmbariClient top-level root resources.\n-    \"\"\"\n-\n-    def __init__(\n-            self,\n-            host_name,\n-            port=None,\n-            user_name=\"admin\",\n-            password=\"admin\",\n-            use_https=False,\n-            version=API_VERSION,\n-            client=None,\n-            http_header=None):\n-        \"\"\"\n-        Creates a RestResource object.\n-\n-        @param host_name: The hostname  server.\n-        @param port: The port of the server.\n-        @param user_name: Login name.\n-        @param password: Login password.\n-        @param version: API version.\n-        @return RestResource object referring to the root.\n-        \"\"\"\n-\n-        self._version = version\n-\n-        if use_https:\n-            protocol = \"https\"\n-            if port is None:\n-                port = 8443\n-        else:\n-            protocol = \"http\"\n-            if port is None:\n-                port = 8080\n-\n-        if not http_header:\n-            http_header = {'X-Requested-By': 'pythonclient'}\n-        elif 'X-Requested-By' not in http_header.keys():\n-            http_header.update({'X-Requested-By': 'pythonclient'})\n-        else:\n-            pass\n-\n-        host_url = \"%s://%s:%s/api/v%s\" % (protocol, host_name, port, version)\n-        if client is None:\n-            client = HttpClient(host_url, user_name, password)\n-            if http_header:\n-                client.set_headers(http_header)\n-        RestResource.__init__(self, client)\n-\n-    @property\n-    def version(self):\n-        \"\"\"\n-        Returns the API version .\n-        \"\"\"\n-        return self._version\n-\n-    def get_all_clusters(self):\n-        \"\"\"\n-        Get all clusters.\n-        @return : A ModelList of ClusterModel.\n-        \"\"\"\n-        return cluster._get_all_clusters(self)\n-\n-    def get_cluster(self, cluster_name):\n-        \"\"\"\n-        Get a cluster by cluster_name.\n-\n-        @param cluster_name : Cluster's cluster_name.\n-        @return : An ClusterModel.\n-        \"\"\"\n-        return cluster._get_cluster(self, cluster_name)\n-\n-    def get_host(self, host_name):\n-        \"\"\"\n-        Lookup a host by name\n-        @param root_resource: The root Resource.\n-        @param host_name: Host name\n-        @return: A HostModel object\n-        \"\"\"\n-        return host._get_host(self, host_name)\n-\n-    def get_all_hosts(self):\n-        \"\"\"\n-        Get all hosts in the Data Center\n-        @return: A ModelList of HostModel objects.\n-        \"\"\"\n-        return host._get_all_hosts(self)\n-\n-    def get_request_status(self, request_id):\n-        \"\"\"\n-        Get request status\n-        @param request_id : request id for the request\n-        @return: A  StatusModel object.\n-        \"\"\"\n-        return \"TODO\"\n-\n-    def bootstrap_hosts(self, hosts_list, ssh_key=None, ssh_user=None):\n-        \"\"\"\n-        Bootstrap hosts.\n-        @param hosts_list :list of host_names.\n-        @param ssh_key : ssh key for password-less access\n-        @return: A  StatusModel object.\n-        \"\"\"\n-        return host._bootstrap_hosts(self, hosts_list, ssh_key, ssh_user)\n-\n-    def create_cluster(self, cluster_name, version):\n-        \"\"\"\n-        Create a new cluster.\n-        @param cluster_name: Cluster name.\n-        @param version : HDP version.\n-        @return  ClusterModel object.\n-        \"\"\"\n-        return cluster._create_cluster(self, cluster_name, version)\n-\n-    def create_cluster_from_blueprint(self, cluster_name, blueprint_name, \n-                                      host_groups, configurations=None, \n-                                      default_password=None):\n-        \"\"\"\n-        Create a new cluster.\n-        @param cluster_name: Cluster cluster_name\n-        @param blueprint_name: the name of the blueprint\n-        @param host_groups: an array of host_group information\n-        @param configurations: an array of configuration overrides\n-        @param default_password: the default password to use for all password-requiring services\n-        @return  ClusterModel object.\n-        \"\"\"\n-        return cluster._create_cluster_from_blueprint(self, cluster_name, \n-            blueprint_name, host_groups, configurations=configurations, \n-            default_password=default_password)\n-\n-    def delete_cluster(self, cluster_name):\n-        \"\"\"\n-        Delete a cluster\n-        @param cluster_name: Cluster to be deleted\n-        \"\"\"\n-        return cluster._delete_cluster(self, cluster_name)\n-\n-    def delete_host(self, host_name):\n-        \"\"\"\n-        Delete a cluster\n-        @param host_name: host to be deleted\n-        \"\"\"\n-        return host._delete_host(self, host_name)\n-\n-    def get_config(self, version, service_name):\n-        \"\"\"\n-        get configurations from stack\n-        @param version: The HDP version.\n-        @param service_name: service name\n-        @return: A ConfigModel object\n-        \"\"\"\n-        return stack._get_configuration_from_stack(self, version, service_name)\n-\n-    def get_components(self, version, service_name):\n-        \"\"\"\n-        get components from stack\n-        @param version: The HDP version.\n-        @param service_name: service name\n-        @return: A ConfigModel object\n-        \"\"\"\n-        return stack._get_components_from_stack(self, version, service_name)\n-\n-    def get_stacks(self, view_all_versions=None):\n-        \"\"\"\n-        get components from stack\n-        @param version: The HDP version.\n-        @param service_name: service name\n-        @return: A ConfigModel object\n-        \"\"\"\n-        return stack._get_stacks(self, view_all_versions)\n-\n-    def get_stack(self, version):\n-        \"\"\"\n-        get components from stack\n-        @param version: The HDP version.\n-        @param service_name: service name\n-        @return: A ConfigModel object\n-        \"\"\"\n-        return stack._get_stack(self, version)\n-\n-    def get_stack_components(self, version, service_name):\n-        \"\"\"\n-        get components from stack\n-        @param version: The HDP version.\n-        @param service_name: service name\n-        @return: A ConfigModel object\n-        \"\"\"\n-        return stack._get_components_from_stack(self, version, service_name)\n-\n-    def get_stack_configs(self, version, service_name):\n-        \"\"\"\n-        get components from stack\n-        @param version: The HDP version.\n-        @param service_name: service name\n-        @return: A ConfigModel object\n-        \"\"\"\n-        return stack._get_configuration_from_stack(self, version, service_name)\n-\n-    def set_stacks(self, version, os_type, repo):\n-        \"\"\"\n-        get components from stack\n-        @param version: The HDP version.\n-        @param service_name: service name\n-        @return: A ConfigModel object\n-        \"\"\"\n-        return stack._put_stacks_and_repo(self, version, os_type, repo)\n-\n-    def get_task_status(self, cluster_name, requestid=None):\n-        \"\"\"\n-        get components from stack\n-        @param version: The HDP version.\n-        @param service_name: service name\n-        @return: A ConfigModel object\n-        \"\"\"\n-        return cluster._task_status(self, cluster_name, requestid)\n-\n-    def get_requests(self, cluster_name, noOfrequest=3):\n-        \"\"\"\n-        get components from stack\n-        @param version: The HDP version.\n-        @param service_name: service name\n-        @return: A ConfigModel object\n-        \"\"\"\n-        return status._get_N_requests(self, cluster_name, noOfrequest)\n-\n-    def get_blueprint(self, blueprint_name):\n-        \"\"\"\n-        get blueprint\n-        @param blueprint_name:blueprint_name name.\n-        @return: A BlueprintModel object\n-        \"\"\"\n-        return blueprint.get_blueprint(self, blueprint_name)\n-\n-    def get_cluster_blueprint(self, cluster_name):\n-        \"\"\"\n-        get blueprint\n-        @param cluster_name:cluster_name name.\n-        @return: A BlueprintModel object\n-        \"\"\"\n-        return blueprint.get_cluster_blueprint(self, cluster_name)\n-\n-    def delete_blueprint(self, blueprint_name):\n-        \"\"\"\n-        get blueprint\n-        @param blueprint_name:blueprint_name name.\n-        @return: A BlueprintModel object\n-        \"\"\"\n-        return blueprint.delete_blueprint(self, blueprint_name)\n-\n-    def create_blueprint(self, blueprint_name, blueprint_schema):\n-        \"\"\"\n-        get blueprint\n-        @param blueprint_name:blueprint_name name.\n-        @return: A BlueprintModel object\n-        \"\"\"\n-        return blueprint.create_blueprint(self, blueprint_name, blueprint_schema)\n-\n-\n-def get_root_resource(\n-        server_host,\n-        server_port=None,\n-        username=\"admin\",\n-        password=\"admin\",\n-        version=1):\n-    \"\"\"\n-     AmbariClient.\n-    \"\"\"\n-    return AmbariClient(server_host, server_port, username, password, version)",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/ambari_api.py",
                "sha": "d3832c8f16d13a8426612b2308ee6ed71b6c0414",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/core/__init__.py",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/core/__init__.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 16,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/core/__init__.py",
                "patch": "@@ -1,16 +0,0 @@\n-#\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/core/__init__.py",
                "sha": "278df2e7b66ed24abc80868620a7156e055d6b5e",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/core/coreutils.py",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/core/coreutils.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 27,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/core/coreutils.py",
                "patch": "@@ -1,27 +0,0 @@\n-#\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-import re\n-\n-def normalize_all_caps(name):\n-\t\"\"\"\n-\tThis converts all caps words into normal case.  \n-\ti.e. 'NAGIOS_SERVER' becomes 'Nagios Server'\n-\t\"\"\"\n-\tnormalized = name.lower()\n-\tnormalized = re.sub('_(\\w)', lambda match: ' ' + match.group(1).upper(), normalized)\n-\treturn normalized[0].upper() + normalized[1:]",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/core/coreutils.py",
                "sha": "56f10c3f8169fa112f93bcf76e3e01e1727a7f56",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/core/errors.py",
                "changes": 103,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/core/errors.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 103,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/core/errors.py",
                "patch": "@@ -1,103 +0,0 @@\n-#\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-\n-class ResourceError(Exception):\n-\n-    def __init__(self, response, resource_root=None):\n-        \"\"\"\n-        Create new exception based on not successful server response\n-        @param response: StatusModel response\n-        @param resource_root: The resource which sent an error response\n-        \"\"\"\n-        self.response = response\n-        self.resource_root = resource_root\n-        Exception.__init__(self)\n-\n-    def get_message(self):\n-        \"\"\" Get an error message \"\"\"\n-        return self.response.get_message()\n-\n-    def get_status_code(self):\n-        \"\"\" Get a status(error) code from the server response \"\"\"\n-        return self.response.status\n-\n-    def get_reponse(self):\n-        \"\"\" StatusModel object \"\"\"\n-        return self.reponse\n-\n-    def get_root_resource(self):\n-        \"\"\" AmbariClient object \"\"\"\n-        return self.resource_root\n-\n-    def __str__(self):\n-        if self.get_message():\n-            return \"exception: %s. %s\" % (\n-                self.response.status, self.get_message())\n-        try:\n-            return self._fmt % self.__dict__\n-        except (NameError, ValueError, KeyError) as e:\n-            return 'exception %s: %s' \\\n-                   % (self.__class__.__name__, str(e))\n-\n-\n-class ResourceConflict(ResourceError):\n-\n-    \"\"\" 409 status code \"\"\"\n-\n-\n-class ResourceNotFound(ResourceError):\n-\n-    \"\"\" 404 status code \"\"\"\n-\n-\n-class BadRequest(ResourceError):\n-\n-    \"\"\" 400 status code \"\"\"\n-\n-\n-class AuthorizationError(ResourceError):\n-\n-    \"\"\" 401 status code \"\"\"\n-\n-\n-class ForbiddenError(ResourceError):\n-\n-    \"\"\" 403 status code \"\"\"\n-\n-\n-class InternalServerError(ResourceError):\n-\n-    \"\"\" 500 status code \"\"\"\n-\n-\n-class MethodNotAllowed(ResourceError):\n-\n-    \"\"\" 405 status code \"\"\"\n-\n-\n-class UnknownServerError(ResourceError):\n-\n-    \"\"\" Received other response code \"\"\"\n-\n-_exceptions_to_codes = {409: ResourceConflict,\n-                        404: ResourceNotFound,\n-                        400: BadRequest,\n-                        401: AuthorizationError,\n-                        403: ForbiddenError,\n-                        500: InternalServerError,\n-                        405: MethodNotAllowed}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/core/errors.py",
                "sha": "eb3644ac33075dae2ba959327f3de0f3ce970469",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/core/http_client.py",
                "changes": 179,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/core/http_client.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 179,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/core/http_client.py",
                "patch": "@@ -1,179 +0,0 @@\n-#\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-import logging\n-import posixpath\n-import sys\n-try:\n-    import pycurl\n-# pycurl is not necessary for testcases, mock it\n-except ImportError:\n-    from mock.mock import MagicMock\n-    pycurl = MagicMock()\n-import cStringIO\n-import StringIO\n-import pdb\n-try:\n-    import json\n-except ImportError:\n-    import simplejson as json\n-from ambari_client.core.http_utils import uri_encoding\n-\n-__docformat__ = \"epytext\"\n-\n-LOG = logging.getLogger(__name__)\n-\n-\n-class HttpClient(object):\n-\n-    \"\"\"\n-    Basic HTTP client for rest APIs.\n-    \"\"\"\n-\n-    def __init__(self, host_url, user_name, password):\n-        \"\"\"\n-        @param host_url: The base url to the API.\n-\n-        \"\"\"\n-\n-        self._host_url = host_url.rstrip('/')\n-        self._headers = {}\n-        self.c = pycurl.Curl()\n-        if user_name is not None:\n-            self.c.setopt(pycurl.HTTPAUTH, pycurl.HTTPAUTH_BASIC)\n-            userpass = user_name + ':'\n-            if password is not None:\n-                userpass += password\n-        LOG.debug(\"pycurl.USERPWD value = \" + str(userpass))\n-        self.c.setopt(pycurl.USERPWD, userpass)\n-\n-    def set_headers(self, headers):\n-        \"\"\"\n-        Add headers to the request\n-        \"\"\"\n-        self._headers = headers\n-        return self\n-\n-    @property\n-    def host_url(self):\n-        return self._host_url\n-\n-    def _get_headers(self, headers):\n-        res = self._headers.copy()\n-        if headers:\n-            res.update(headers)\n-        return res\n-\n-    def invoke(self, http_method, path, payload=None, headers=None):\n-        \"\"\"\n-        Submit an HTTP request.\n-        @param http_method: GET, POST, PUT, DELETE\n-        @param path: The path of the resource.\n-        @param payload: The payload to attach to the body of the request.\n-        @param headers: The headers to set for this request.\n-\n-        @return: The result of REST request\n-        \"\"\"\n-        # pdb.set_trace()\n-        LOG.debug(\"invoke : http_method = \" + str(http_method))\n-        # Prepare URL and params\n-        url = self._normalize(path)\n-        if http_method in (\"GET\", \"DELETE\"):\n-            if payload is not None:\n-                self.logger.warn(\n-                    \"GET http_method does not pass any payload. Path '%s'\" %\n-                    (path,))\n-                payload = None\n-\n-\tself.c.unsetopt(pycurl.CUSTOMREQUEST)\n-        buf = cStringIO.StringIO()\n-        self.c.setopt(pycurl.WRITEFUNCTION, buf.write)\n-        self.c.setopt(pycurl.SSL_VERIFYPEER, 0)\n-\n-\n-        LOG.debug(\"invoke : url = \" + str(url))\n-        # set http_method\n-        if http_method == \"GET\":\n-            self.c.setopt(pycurl.HTTPGET, 1)\n-        elif http_method == \"HEAD\":\n-            self.c.setopt(pycurl.HTTPGET, 1)\n-            self.c.setopt(pycurl.NOBODY, 1)\n-        elif http_method == \"POST\":\n-            self.c.setopt(pycurl.POST, 1)\n-        elif http_method == \"PUT\":\n-            self.c.setopt(pycurl.UPLOAD, 1)\n-        else:\n-            self.c.setopt(pycurl.CUSTOMREQUEST, http_method)\n-\n-        data = None\n-        if http_method in ('POST', 'PUT'):\n-            LOG.debug(\"data...........\" + str(payload))\n-            data = json.dumps(payload)\n-            # data= data.decode('unicode-escape')\n-            # LOG.debug( \"after unicode decode\")\n-            # LOG.debug( data)\n-            data = self._to_bytestring(data)\n-            LOG.debug(\"after _to_bytestring\")\n-            LOG.debug(data)\n-            content = StringIO.StringIO(data)\n-            LOG.debug(content)\n-            content_length = len(data)\n-            LOG.debug(\"content_length.........\" + str(content_length))\n-\n-            if http_method == 'POST':\n-                self.c.setopt(pycurl.POSTFIELDSIZE, content_length)\n-            else:\n-                self.c.setopt(pycurl.INFILESIZE, content_length)\n-\n-            self.c.setopt(pycurl.READFUNCTION, content.read)\n-\n-        self.c.setopt(self.c.URL, url)\n-        headers = self._get_headers(headers)\n-        headers_l = [\"%s: %s\" % pair for pair in sorted(headers.iteritems())]\n-        LOG.debug(headers_l)\n-        self.c.setopt(pycurl.HTTPHEADER, headers_l)\n-\n-        LOG.debug(\n-            \"invoke : pycurl.EFFECTIVE_URL = \" +\n-            self.c.getinfo(\n-                pycurl.EFFECTIVE_URL))\n-        try:\n-            self.c.perform()\n-        except Exception as ex:\n-            LOG.debug(sys.stderr, str(ex))\n-            raise ex\n-        contents_type = self.c.getinfo(pycurl.CONTENT_TYPE)\n-        LOG.debug(\"invoke : pycurl.CONTENT_TYPE = \" + contents_type)\n-        code = self.c.getinfo(pycurl.RESPONSE_CODE)\n-        LOG.debug(\"invoke : pycurl.RESPONSE_CODE = \" + str(code))\n-        response = buf.getvalue()\n-        buf.close()\n-        LOG.debug(\"invoke : COMPLETED \")\n-        return response, code, contents_type\n-\n-    def _to_bytestring(self, s):\n-        #    if not isinstance(s, basestring):\n-        #      raise TypeError(\"value should be a str or unicode\")\n-        if isinstance(s, unicode):\n-            return s.encode('utf-8')\n-        return s\n-\n-    def _normalize(self, path):\n-        res = self._host_url\n-        if path:\n-            res += posixpath.normpath('/' + path.lstrip('/'))\n-        return uri_encoding(res)",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/core/http_client.py",
                "sha": "7f7e526630524502179dcea4ef031c047f515d12",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/core/http_utils.py",
                "changes": 50,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/core/http_utils.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 50,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/core/http_utils.py",
                "patch": "@@ -1,50 +0,0 @@\n-#\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-\n-import types\n-import urllib\n-import urllib2\n-\n-\n-def uri_encoding(url):\n-    \"\"\"\n-    Returns an ASCII string version of the URL.\n-    \"\"\"\n-    if url is None:\n-        return url\n-    return urllib.quote(get_utf8_str(url), safe=\"/#%[]=:;$&()+,!?*@'~\")\n-\n-\n-def get_utf8_str(strr, encoding='utf-8'):\n-    \"\"\"\n-    Returns a utf8 ecoded 'str'.\n-    \"\"\"\n-    errors = 'strict'\n-    if not isinstance(strr, basestring):\n-        try:\n-            return str(strr)\n-        except UnicodeEncodeError:\n-            if isinstance(strr, Exception):\n-                return ' '.join([get_utf8_str(arg, encoding) for arg in strr])\n-            return unicode(strr).encode(encoding, errors)\n-    elif isinstance(strr, unicode):\n-        return strr.encode(encoding, errors)\n-    elif strr and encoding != 'utf-8':\n-        return strr.decode('utf-8', errors).encode(encoding, errors)\n-    else:\n-        return strr",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/core/http_utils.py",
                "sha": "5c0dc2f96336e0baff983d43f5ebbce50d9169d8",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/core/rest_resource.py",
                "changes": 136,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/core/rest_resource.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 136,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/core/rest_resource.py",
                "patch": "@@ -1,136 +0,0 @@\n-#\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-try:\n-    import json\n-except ImportError:\n-    import simplejson as json\n-import logging\n-import posixpath\n-\n-LOG = logging.getLogger(__name__)\n-\n-\n-class RestResource(object):\n-\n-    \"\"\"\n-    RestResource wrapper.\n-    \"\"\"\n-\n-    def __init__(self, client, path=\"\"):\n-        \"\"\"\n-        @param client: A Client object.\n-        @param path: The relative path of the resource.\n-        \"\"\"\n-        self._client = client\n-        self._path = path.strip('/')\n-\n-    @property\n-    def host_url(self):\n-        return self._client.host_url\n-\n-    def _join_uri(self, relpath):\n-        if relpath is None:\n-            return self._path\n-        return self._path + posixpath.normpath('/' + relpath)\n-\n-    def _set_headers(self, content_type=None):\n-        if content_type:\n-            return {'Content-Type': content_type}\n-        return None\n-\n-    def _make_invoke(self, http_method, payload, headers, path):\n-        return self._client.invoke(\n-            http_method,\n-            path,\n-            payload=payload,\n-            headers=headers)\n-\n-    def invoke(self, http_method, url_path=None, payload=None, headers=None):\n-        \"\"\"\n-        Invoke an API http_method.\n-        \"\"\"\n-        path = self._join_uri(url_path)\n-        resp, code, content = self._make_invoke(\n-            http_method, payload, headers, path)\n-\n-        LOG.debug(\"RESPONSE from the REST request >>>>>>> \\n\" + str(resp))\n-        LOG.debug(\n-            \"\\n===========================================================\")\n-        # take care of REST calls with no response\n-\n-        try:\n-            isOK = (code == 200 or code == 201 or code == 202)\n-\n-            if isOK and not resp:\n-                json_dict = {\"status\": code}\n-            else:\n-                json_dict = json.loads(resp)\n-\n-            return json_dict\n-        except Exception as ex:\n-            LOG.error(\n-                \"Command '%s %s' failed with error %s\\n%s\" %\n-                (http_method, path, code, resp))\n-            return {\n-                \"status\": code, \"message\": \"Command '%s %s' failed with error %s\" %\n-                (http_method, path, code)}\n-\n-    def get(self, path=None):\n-        \"\"\"\n-        Invoke the GET method .\n-        @param path: resource path\n-        @return: A dictionary of the REST result.\n-        \"\"\"\n-        return self.invoke(\"GET\", path)\n-\n-    def put(self, path=None, payload=None, content_type=None):\n-        \"\"\"\n-        Invoke the PUT method on a resource.\n-        @param path: resource path\n-        @param payload: Body of the request.\n-        @param content_type:\n-        @return: A dictionary of the REST result.\n-        \"\"\"\n-        return self.invoke(\n-            \"PUT\",\n-            path,\n-            payload,\n-            self._set_headers(content_type))\n-\n-    def post(self, path=None, payload=None, content_type=None):\n-        \"\"\"\n-        Invoke the POST method on a resource.\n-        @param path: resource path\n-        @param payload: Body of the request.\n-        @param content_type:\n-        @return: A dictionary of the REST result.\n-        \"\"\"\n-        return self.invoke(\n-            \"POST\",\n-            path,\n-            payload,\n-            self._set_headers(content_type))\n-\n-    def delete(self, path=None, payload=None,):\n-        \"\"\"\n-        Invoke the DELETE method on a resource.\n-        @param path: resource path\n-        @param payload: Body of the request.\n-        @return: A dictionary of the REST result.\n-        \"\"\"\n-        return self.invoke(\"DELETE\", path, payload)",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/core/rest_resource.py",
                "sha": "9e98178546e8ded17fd3a9cef07cb04c1c62e9f8",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/__init__.py",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/model/__init__.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 16,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/model/__init__.py",
                "patch": "@@ -1,16 +0,0 @@\n-#\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/__init__.py",
                "sha": "278df2e7b66ed24abc80868620a7156e055d6b5e",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/base_model.py",
                "changes": 109,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/model/base_model.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 109,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/model/base_model.py",
                "patch": "@@ -1,109 +0,0 @@\n-#\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-import sys\n-import logging\n-import time\n-from ambari_client.model.utils import get_REF_object, get_unicode, getREF_var_name, LIST_KEY\n-from operator import itemgetter, attrgetter\n-\n-__docformat__ = \"epytext\"\n-\n-LOG = logging.getLogger(__name__)\n-\n-\n-class BaseModel(object):\n-\n-    \"\"\"\n-    The BaseModel\n-\n-    RW_ATTR - A list of mutable attributes\n-    RO_ATTR - A list of immutable attributes\n-    REF_ATTR - A REF attribute\n-\n-    \"\"\"\n-    RO_ATTR = ()\n-    RW_ATTR = ()\n-    REF_ATTR = ()\n-\n-    def __init__(self, resource_root, **rw_attrs):\n-        # print\" ================== base_model\\n\"\n-        # print locals()\n-        self._resource_root = resource_root\n-        for k, v in rw_attrs.items():\n-            if k not in self.RW_ATTR:\n-                raise ValueError(\"Unknown argument '%s' in %s\" %\n-                                 (k, self.__class__.__name__))\n-            self._setattr(k, v)\n-\n-    def _get_resource_root(self):\n-        return self._resource_root\n-\n-    def to_json_dict(self):\n-        dic = {}\n-        for attr in self.RW_ATTR:\n-            value = getattr(self, attr)\n-            try:\n-                value = value.to_json_dict()\n-            except Exception:\n-                pass\n-            dic[attr] = value\n-        return dic\n-\n-    def _setattr(self, k, v):\n-        \"\"\"Set an attribute. \"\"\"\n-        value = v\n-        if v and k.endswith(\"Ref\"):\n-            cls_name = k[0].upper() + k[1:]\n-            cls_name = cls_name[:-3] + \"ModelRef\"\n-            cls = get_REF_object(cls_name)\n-            LOG.debug(str(cls_name) + \"  -  \" + str(cls))\n-            v = get_unicode(v)\n-            var_name = getREF_var_name(cls_name)\n-            c = {str(var_name): str(v)}\n-            LOG.debug(c)\n-            value = cls(self._get_resource_root(), **c)\n-        setattr(self, k, value)\n-\n-\n-class ModelList(object):\n-\n-    \"\"\"A list of Model objects\"\"\"\n-\n-    def __init__(self, objects):\n-        self.objects = objects\n-\n-    def __str__(self):\n-        return \"<<ModelList>>[size = %d]) = [%s]\" % (\n-            len(self.objects),\n-            \", \".join([str(item) for item in self.objects]))\n-\n-    def to_json_dict(self):\n-        return {LIST_KEY:\n-                [x.to_json_dict() for x in self.objects]}\n-\n-    def __len__(self):\n-        return self.objects.__len__()\n-\n-    def __iter__(self):\n-        return self.objects.__iter__()\n-\n-    def sort(self, sortkey):\n-        self.objects = sorted(self.objects, key=sortkey, reverse=True)\n-\n-    def __getitem__(self, i):\n-        return self.objects.__getitem__(i)",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/base_model.py",
                "sha": "1d4fc151113f123170fa340d94b6f6cbd7929151",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/blueprint.py",
                "changes": 178,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/model/blueprint.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 178,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/model/blueprint.py",
                "patch": "@@ -1,178 +0,0 @@\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-import logging\n-import json\n-from ambari_client.model.base_model import BaseModel, ModelList\n-from ambari_client.model import paths, utils, status\n-\n-\n-LOG = logging.getLogger(__name__)\n-\n-\n-def get_blueprint(resource_root, blueprint_name):\n-    \"\"\"\n-    Get all Blueprint\n-    @param root_resource: The root Resource .\n-    @param name: blueprint_name\n-    @return: A list of BlueprintModel objects.\n-    \"\"\"\n-    if not blueprint_name:\n-        dic = resource_root.get(paths.BLUEPRINT_ALL_PATH)\n-        return utils.ModelUtils.get_model_list(\n-            ModelList,\n-            BlueprintModel,\n-            dic,\n-            resource_root,\n-            \"Blueprints\")\n-    else:\n-        dic = resource_root.get(paths.BLUEPRINT_PATH % blueprint_name)\n-        return utils.ModelUtils.create_model(\n-            BlueprintModel,\n-            dic,\n-            resource_root,\n-            \"Blueprints\")\n-\n-\n-def get_cluster_blueprint(resource_root, cluster_name):\n-    \"\"\"\n-    Get all Blueprint\n-    @param root_resource: The root Resource .\n-    @param name: blueprint_name\n-    @return: A list of BlueprintModel objects.\n-    \"\"\"\n-    resp = resource_root.get(paths.BLUEPRINT_CLUSTER_PATH % cluster_name)\n-    result = json.dumps(resp)\n-    resp_dictt = json.loads(result)\n-    objects = [\n-        utils.ModelUtils.create_model(\n-            BlueprintHostModel,\n-            x,\n-            resource_root,\n-            \"NO_KEY\") for x in resp_dictt['host_groups']]\n-    LOG.debug(objects)\n-\n-    bluep = utils.ModelUtils.create_model(\n-        BlueprintModel,\n-        resp,\n-        resource_root,\n-        \"Blueprints\")\n-\n-    return bluep, ModelList(objects)\n-\n-\n-def delete_blueprint(resource_root, blueprint_name):\n-    \"\"\"\n-    Delete a blueprint by name\n-    @param root_resource: The root Resource .\n-    @param name: blueprint_name\n-    \"\"\"\n-    resp = resource_root.delete(paths.BLUEPRINT_PATH % blueprint_name)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        resource_root,\n-        \"NO_KEY\")\n-\n-\n-def create_blueprint(resource_root, blueprint_name, blueprint_schema):\n-    \"\"\"\n-    Create a blueprint\n-    @param root_resource: The root Resource.\n-    @param blueprint_name: blueprint_name\n-    @param blueprint_schema: blueprint  json\n-    @return: An ClusterModel object\n-    \"\"\"\n-    path = paths.BLUEPRINT_PATH % blueprint_name\n-    resp = resource_root.post(path=path, payload=blueprint_schema)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        resource_root,\n-        \"NO_KEY\")\n-\n-\n-class BlueprintModel(BaseModel):\n-\n-    \"\"\"\n-    The BlueprintModel class\n-    \"\"\"\n-    RO_ATTR = ('stack_name', 'stack_version')\n-    RW_ATTR = ('blueprint_name')\n-    REF_ATTR = ('cluster_name',)\n-\n-    def __init__(self, resource_root, blueprint_name):\n-        utils.retain_self_helper(BaseModel, **locals())\n-\n-    def __str__(self):\n-        return \"<<BlueprintModel>> blueprint_name = %s \" % (\n-            self.blueprint_name)\n-\n-    def _get_cluster_name(self):\n-        if self.clusterRef:\n-            return self.clusterRef.cluster_name\n-        return None\n-\n-\n-class BlueprintHostModel(BaseModel):\n-\n-    \"\"\"\n-    The BlueprintHostModel class\n-    \"\"\"\n-    RO_ATTR = ('blueprint_name',)\n-    RW_ATTR = ('name', 'components', 'cardinality')\n-    REF_ATTR = ()\n-\n-    def __init__(self, resource_root, name, components, cardinality):\n-        utils.retain_self_helper(BaseModel, **locals())\n-\n-    def __str__(self):\n-        return \"<<BlueprintHostModel>> name = %s ,components =%s\" % (\n-            self.name, str(self.components))\n-\n-    def to_json(self):\n-        pass\n-\n-\n-class BlueprintConfigModel(BaseModel):\n-\n-    \"\"\"\n-    The BlueprintConfigModel class\n-    \"\"\"\n-    RO_ATTR = ('stack_name', 'stack_version')\n-    RW_ATTR = ('blueprint_name')\n-    REF_ATTR = ('cluster_name',)\n-\n-    RO_ATTR = ('stack_name', 'type', 'property_description')\n-    RW_ATTR = (\n-        'property_name',\n-        'property_value',\n-        'service_name',\n-        'stack_version')\n-    REF_ATTR = ()\n-\n-    def __init__(\n-            self,\n-            resource_root,\n-            property_name,\n-            property_value=None,\n-            service_name=None,\n-            stack_version=None):\n-        utils.retain_self_helper(BaseModel, **locals())\n-\n-    def __str__(self):\n-        return \"<<BlueprintConfigModel>> property_name=%s; service_name= %s\" % (\n-            self.property_name, self.service_name)",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/blueprint.py",
                "sha": "21b0b33264837b34020502be0045b57a174bf547",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/cluster.py",
                "changes": 540,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/model/cluster.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 540,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/model/cluster.py",
                "patch": "@@ -1,540 +0,0 @@\n-#\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-import logging\n-import time\n-from ambari_client.model.base_model import BaseModel, ModelList\n-from ambari_client.model import service, host, paths, status, configuration, utils\n-\n-\n-LOG = logging.getLogger(__name__)\n-\n-\n-def _get_cluster(resource_root, cluster_name):\n-    \"\"\"\n-    Lookup a cluster by cluster_name\n-    @param resource_root: The root Resource .\n-    @param cluster_name: cluster_name\n-    @return: A ClusterModel object\n-    \"\"\"\n-    dic = resource_root.get(\"%s/%s\" % (paths.CLUSTERS_PATH, cluster_name))\n-    return utils.ModelUtils.create_model(\n-        ClusterModel,\n-        dic,\n-        resource_root,\n-        \"Clusters\")\n-\n-\n-def _get_all_clusters(root_resource):\n-    \"\"\"\n-    Get all clusters\n-    @param root_resource: The root Resource .\n-    @return: A list of ClusterModel objects.\n-    \"\"\"\n-    dic = root_resource.get(paths.CLUSTERS_PATH)\n-    return utils.ModelUtils.get_model_list(\n-        ModelList,\n-        ClusterModel,\n-        dic,\n-        root_resource,\n-        \"Clusters\")\n-\n-\n-def _create_cluster(root_resource, cluster_name, version):\n-    \"\"\"\n-    Create a cluster\n-    @param root_resource: The root Resource.\n-    @param cluster_name: Cluster cluster_name\n-    @param version: HDP version\n-    @return: An ClusterModel object\n-    \"\"\"\n-    data = {\"Clusters\": {\"version\": str(version)}}\n-    path = paths.CLUSTERS_PATH + \"/%s\" % (cluster_name)\n-    resp = root_resource.post(path=path, payload=data)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        root_resource,\n-        \"NO_KEY\")\n-\n-\n-def _create_cluster_from_blueprint(root_resource, cluster_name, blueprint_name, \n-                                   host_groups, configurations=None, \n-                                   default_password=None):\n-    \"\"\"\n-    Create a cluster\n-    @param root_resource: The root Resource.\n-    @param cluster_name: Cluster cluster_name\n-    @param blueprint_name: the name of the blueprint\n-    @param host_groups: an array of host_group information\n-    @param configurations: an array of configuration overrides\n-    @param default_password: the default password to use for all password-requiring services\n-    @return: An StatusModel object\n-    \"\"\"\n-    data = {\n-      \"blueprint\" : blueprint_name,\n-      \"host_groups\" : host_groups,\n-    }\n-    if configurations is not None:\n-        data['configurations'] = configurations\n-    if default_password is not None:\n-        data['default_password'] = default_password\n-\n-    path = paths.CLUSTERS_PATH + \"/%s\" % (cluster_name)\n-    resp = root_resource.post(path=path, payload=data)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        root_resource,\n-        \"NO_KEY\")\n-\n-\n-def _delete_cluster(root_resource, cluster_name):\n-    \"\"\"\n-    Delete a cluster by name\n-    @param root_resource: The root Resource .\n-    @param name: Cluster name\n-    \"\"\"\n-    resp = root_resource.delete(\"%s/%s\" % (paths.CLUSTERS_PATH, cluster_name))\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        root_resource,\n-        \"NO_KEY\")\n-\n-\n-def _install_all_services(root_resource, cluster_name):\n-    \"\"\"\n-    Start all services\n-    @param root_resource: The root Resource .\n-    @param name: Cluster name\n-    \"\"\"\n-    cpath = paths.CLUSTER_START_ALL_SERVICES % cluster_name\n-    data = {\n-        \"RequestInfo\": {\n-            \"context\": \"Install Services\"}, \"Body\": {\n-            \"ServiceInfo\": {\n-                \"state\": \"INSTALLED\"}}}\n-    resp = root_resource.put(path=cpath, payload=data)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        root_resource,\n-        \"NO_KEY\")\n-\n-\n-def _stop_all_services(root_resource, cluster_name):\n-    \"\"\"\n-    Start all services\n-    @param root_resource: The root Resource .\n-    @param name: Cluster name\n-    \"\"\"\n-    cpath = paths.CLUSTER_STOP_ALL_SERVICES % cluster_name\n-    data = {\n-        \"RequestInfo\": {\n-            \"context\": \"Stop All Services\"}, \"Body\": {\n-            \"ServiceInfo\": {\n-                \"state\": \"INSTALLED\"}}}\n-    resp = root_resource.put(path=cpath, payload=data)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        root_resource,\n-        \"NO_KEY\")\n-\n-\n-def _start_all_services(root_resource, cluster_name, run_smoke_test=False):\n-    \"\"\"\n-    Start all services\n-    @param root_resource: The root Resource .\n-    @param name: Cluster name\n-    \"\"\"\n-    cpath = paths.CLUSTER_START_ALL_SERVICES % cluster_name\n-    if run_smoke_test:\n-        cpath = \"%s&%s\" % (\n-            cpath, \"params/run_smoke_test=true&params/reconfigure_client=false\")\n-    data = {\n-        \"RequestInfo\": {\n-            \"context\": \"Start All Services\"}, \"Body\": {\n-            \"ServiceInfo\": {\n-                \"state\": \"STARTED\"}}}\n-    resp = root_resource.put(path=cpath, payload=data)\n-    if isinstance(resp, dict) and \"Requests\" in resp:\n-        resp = resp[\"Requests\"]\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        root_resource,\n-        \"NO_KEY\")\n-\n-\n-def _task_status(root_resource, cluster_name, requestid):\n-    cpath = paths.TASKS_PATH % (cluster_name, requestid)\n-    dic = root_resource.get(cpath)\n-    return utils.ModelUtils.get_model_list(\n-        ModelList,\n-        TaskModel,\n-        dic,\n-        root_resource,\n-        \"Tasks\")\n-\n-\n-class TaskModel(BaseModel):\n-\n-    \"\"\"\n-    The ClusterModel class\n-    \"\"\"\n-    RW_ATTR = ('host_name', 'role', 'status')\n-\n-    def __init__(self, resource_root, host_name, role, status):\n-        utils.retain_self_helper(BaseModel, **locals())\n-\n-    def __str__(self):\n-        return \"<<TaskModel>> host_name = %s; status = %s\" % (\n-            self.host_name, self.status)\n-\n-\n-class ClusterModel(BaseModel):\n-\n-    \"\"\"\n-    The ClusterModel class\n-    \"\"\"\n-    RW_ATTR = ('cluster_name', 'version')\n-\n-    def __init__(self, resource_root, cluster_name, version):\n-        utils.retain_self_helper(BaseModel, **locals())\n-\n-    def __str__(self):\n-        return \"<<ClusterModel>> cluster_name = %s; version = %s\" % (\n-            self.cluster_name, self.version)\n-\n-    def _path(self):\n-        return \"%s/%s\" % (paths.CLUSTERS_PATH, self.cluster_name)\n-\n-    def get_service(self, service_name):\n-        \"\"\"\n-        Get a service by service_name.\n-        @param service_name: Service name\n-        @return: A ServiceModel object\n-        \"\"\"\n-        return service._get_service(\n-            self._get_resource_root(),\n-            service_name,\n-            self.cluster_name)\n-\n-    def get_all_services(self, detail=None):\n-        \"\"\"\n-        Get all services in this cluster.\n-        @return: ModelList containing ServiceModel objects.\n-        \"\"\"\n-        return service._get_all_services(\n-            self._get_resource_root(),\n-            self.cluster_name)\n-\n-    def get_all_hosts(self, detail=None):\n-        \"\"\"\n-        Get all hosts in this cluster.\n-        @return: ModelList containing HostModel objects.\n-        \"\"\"\n-        return host._get_all_cluster_hosts(\n-            self._get_resource_root(),\n-            self.cluster_name)\n-\n-    def get_host(self, hostname, detail=None):\n-        \"\"\"\n-        Get a specific hosts in this cluster.\n-        @return: A HostModel object.\n-        \"\"\"\n-        return host._get_cluster_host(\n-            self._get_resource_root(),\n-            self.cluster_name,\n-            hostname)\n-\n-    def get_global_config(self, detail=None):\n-        \"\"\"\n-        Get global configuration of  cluster.\n-        @return: A ConfigModel object.\n-        \"\"\"\n-        return configuration._get_configuration(\n-            self._get_resource_root(),\n-            self.cluster_name,\n-            \"global\")\n-\n-    def get_core_site_config(self, tag=\"version1\", detail=None):\n-        \"\"\"\n-        Get core-site configuration of  cluster.\n-        @return: A ConfigModel object or ModelList<ConfiObject>\n-        \"\"\"\n-        if(detail == utils.ALL):\n-            return configuration._get_all_configuration(\n-                self._get_resource_root(),\n-                self.cluster_name,\n-                \"core-site\")\n-        else:\n-            return configuration._get_configuration(\n-                self._get_resource_root(),\n-                self.cluster_name,\n-                \"core-site\",\n-                tag)\n-\n-    def get_hdfs_site_config(self, detail=None):\n-        \"\"\"\n-        Get hdfs-site configuration of  cluster.\n-        @return: A ConfigModel object.\n-        \"\"\"\n-        return configuration._get_configuration(\n-            self._get_resource_root(),\n-            self.cluster_name,\n-            \"hdfs-site\")\n-\n-    def get_mapred_site_config(self, detail=None):\n-        \"\"\"\n-        Get mapred-site configuration of  cluster.\n-        @return: A ConfigModel object.\n-        \"\"\"\n-        return configuration._get_configuration(\n-            self._get_resource_root(),\n-            self.cluster_name,\n-            \"mapred-site\")\n-\n-    def update_global_config(self, config_model, tag=\"version1\", detail=None):\n-        \"\"\"\n-        Updates the  global configuration of  cluster.\n-        @param config_model: The configModel object\n-        @return: A ConfigModel object.\n-        \"\"\"\n-        return configuration._update_configuration(\n-            self._get_resource_root(),\n-            self.cluster_name,\n-            \"global\",\n-            tag,\n-            config_model)\n-\n-    def update_core_site_config(\n-            self,\n-            config_model,\n-            tag=\"version1\",\n-            detail=None):\n-        \"\"\"\n-        Updates the  core-site configuration of  cluster.\n-        @param config_model: The configModel object\n-        @return: A ConfigModel object.\n-        \"\"\"\n-        return configuration._update_configuration(\n-            self._get_resource_root(),\n-            self.cluster_name,\n-            \"core-site\",\n-            tag,\n-            config_model)\n-\n-    def update_hdfs_site_config(\n-            self,\n-            config_model,\n-            tag=\"version1\",\n-            detail=None):\n-        \"\"\"\n-        Updates the  hdfs-site configuration of  cluster.\n-        @param config_model: The configModel object\n-        @return: A ConfigModel object.\n-        \"\"\"\n-        return configuration._update_configuration(\n-            self._get_resource_root(),\n-            self.cluster_name,\n-            \"hdfs-site\",\n-            tag,\n-            config_model)\n-\n-    def update_mapred_site_config(\n-            self,\n-            config_model,\n-            tag=\"version1\",\n-            detail=None):\n-        \"\"\"\n-        Updates the  mapred-site configuration of  cluster.\n-        @param config_model: The configModel object\n-        @return: A ConfigModel object.\n-        \"\"\"\n-        return configuration._update_configuration(\n-            self._get_resource_root(),\n-            self.cluster_name,\n-            \"mapred-site\",\n-            tag,\n-            config_model)\n-\n-    def create_services(self, services_list, detail=None):\n-        \"\"\"\n-        Creates services.\n-        @param services_list: list of services\n-        @return: StatusModel.\n-        \"\"\"\n-        return service._create_services(\n-            self._get_resource_root(),\n-            self.cluster_name,\n-            services_list)\n-\n-    def create_service(self, service_name, detail=None):\n-        \"\"\"\n-        Creates a single service\n-        @param service_name: service name\n-        @return: StatusModel.\n-        \"\"\"\n-        return service._create_service(\n-            self._get_resource_root(),\n-            self.cluster_name,\n-            service_name)\n-\n-    def create_service_components(self, version, service_name, detail=None):\n-        \"\"\"\n-        Creates service with components\n-        @param version: version\n-        @param service_name: service_name\n-        @return: StatusModel.\n-        \"\"\"\n-        return service._create_service_components(\n-            self._get_resource_root(),\n-            self.cluster_name,\n-            version,\n-            service_name)\n-\n-    def create_service_component(\n-            self,\n-            version,\n-            service_name,\n-            component_name,\n-            detail=None):\n-        \"\"\"\n-        Create service with component\n-        @param version: version\n-        @param service_name: service_name\n-        @return: StatusModel.\n-        \"\"\"\n-        return service._create_service_component(\n-            self._get_resource_root(),\n-            self.cluster_name,\n-            version,\n-            service_name,\n-            component_name)\n-\n-    def create_hosts(self, host_list, detail=None):\n-        \"\"\"\n-        Creates hosts.\n-        @param host_list: list of HostModel\n-        @return: StatusModel.\n-        \"\"\"\n-        return host._add_hosts(\n-            self._get_resource_root(),\n-            self.cluster_name,\n-            host_list)\n-\n-    def create_host(\n-            self,\n-            host_name,\n-            ip,\n-            rack_info='/default-rack',\n-            detail=None):\n-        \"\"\"\n-        Creates host.\n-        @param host_name: Host name\n-        @param ip: ip of Host\n-        @param rack_info: rack information\n-        @return: StatusModel.\n-        \"\"\"\n-        return host._add_host(\n-            self._get_resource_root(),\n-            self.cluster_name,\n-            host_name,\n-            ip,\n-            rack_info)\n-\n-    def delete_host(self, host_name, detail=None):\n-        \"\"\"\n-        deletes a host.\n-        @param host_name: Host name\n-        @return: StatusModel.\n-        \"\"\"\n-        return host._delete_cluster_host(\n-            self._get_resource_root(),\n-            self.cluster_name,\n-            host_name)\n-\n-    def start_all_services(self, run_smoke_test=False, detail=None):\n-        \"\"\"\n-        Start all the services.\n-        @return: StatusModel.\n-        \"\"\"\n-        return _start_all_services(\n-            self._get_resource_root(),\n-            self.cluster_name,\n-            run_smoke_test)\n-\n-    def stop_all_services(self, detail=None):\n-        \"\"\"\n-        Stop all the services.\n-        @return: StatusModel.\n-        \"\"\"\n-        return _stop_all_services(self._get_resource_root(), self.cluster_name)\n-\n-    def install_all_services(self, detail=None):\n-        \"\"\"\n-        INIT all the services.\n-        @return: StatusModel.\n-        \"\"\"\n-        return _install_all_services(\n-            self._get_resource_root(),\n-            self.cluster_name)\n-\n-    def add_config(self, type, tag, properties):\n-        \"\"\"\n-        add configurations to the cluster\n-        @param type: the type of config\n-        @param tag: tag\n-        @param properties: a dict of properties\n-        @return: A StatusModel object\n-        \"\"\"\n-        return configuration._add_config(\n-            self._get_resource_root(),\n-            self.cluster_name,\n-            type,\n-            tag,\n-            properties)\n-\n-    def create_config(self, type, tag, properties):\n-        \"\"\"\n-        create configurations to the cluster\n-        @param type: the type of config\n-        @param tag: tag\n-        @param properties: a dict of properties\n-        @return: A StatusModel object\n-        \"\"\"\n-        return configuration._create_config(\n-            self._get_resource_root(),\n-            self.cluster_name,\n-            type,\n-            tag,\n-            properties)\n-\n-\n-class ClusterModelRef(BaseModel):\n-\n-    \"\"\"\n-    The ClusterModelRef class\n-      Some models need reference to cluster\n-    \"\"\"\n-    RW_ATTR = ('cluster_name',)\n-\n-    def __init__(self, resource_root, cluster_name=None):\n-        utils.retain_self_helper(BaseModel, **locals())",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/cluster.py",
                "sha": "7e0b37877bc2b927e8e9a658ef613c1fe47df57b",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/component.py",
                "changes": 207,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/model/component.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 207,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/model/component.py",
                "patch": "@@ -1,207 +0,0 @@\n-#\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-import logging\n-from ambari_client.core.coreutils import normalize_all_caps\n-from ambari_client.model.base_model import BaseModel, ModelList\n-from ambari_client.model import paths, utils, status\n-\n-\n-LOG = logging.getLogger(__name__)\n-\n-\n-def get_host_components(resource_root, cluster_name, host_name):\n-    path = paths.HOSTS_COMPONENTS_PATH % (cluster_name, host_name)\n-    dic = resource_root.get(path)\n-    return utils.ModelUtils.get_model_list(\n-        ModelList,\n-        ComponentModel,\n-        dic,\n-        resource_root,\n-        \"HostRoles\")\n-\n-\n-def get_host_component(resource_root, cluster_name, host_name, component_name):\n-    path = paths.HOSTS_COMPONENT_PATH % (\n-        cluster_name, host_name, component_name)\n-    dic = resource_root.get(path)\n-    comp_model = utils.ModelUtils.create_model(\n-        ComponentModel,\n-        dic,\n-        resource_root,\n-        \"HostRoles\",\n-        status.StatusModel)\n-    #comp_model._setattr('host_name', dic[\"items\"][0]['HostRoles']['host_name'])\n-    return comp_model\n-\n-\n-def _get_service_components(resource_root, cluster_name, service_name):\n-    path = paths.SERVICE_COMPONENTS_PATH % (cluster_name, service_name)\n-    dic = resource_root.get(path)\n-    return utils.ModelUtils.get_model_list(\n-        ModelList,\n-        ComponentModel,\n-        dic,\n-        resource_root,\n-        \"ServiceComponentInfo\")\n-\n-\n-def _get_service_component(\n-        resource_root,\n-        cluster_name,\n-        service_name,\n-        component_name):\n-    path = paths.SERVICE_COMPONENT_PATH % (\n-        cluster_name, service_name, component_name)\n-    dic = resource_root.get(path)\n-    return utils.ModelUtils.create_model(\n-        ComponentModel,\n-        dic,\n-        resource_root,\n-        \"ServiceComponentInfo\")\n-\n-def _delete_host_component(\n-        resource_root, \n-        cluster_name , \n-        host_name , \n-        component_name):\n-    path = paths.HOSTS_COMPONENT_PATH % (\n-        cluster_name, host_name , component_name)\n-    resp = resource_root.delete(path)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel, \n-        resp, \n-        resource_root, \n-        \"NO_KEY\")\n-\n-\n-class ComponentModel(BaseModel):\n-    \"\"\"\n-    The ComponentModel class\n-    \"\"\"\n-    RO_ATTR = ('stack_id',)\n-    RW_ATTR = ('host_name', 'component_name', 'service_name', 'state')\n-    REF_ATTR = ('cluster_name',)\n-\n-    def __init__(\n-            self,\n-            resource_root,\n-            component_name,\n-            host_name=None,\n-            service_name=None,\n-            state=None):\n-        utils.retain_self_helper(BaseModel, **locals())\n-\n-    def __str__(self):\n-        return \"<<ComponentModel>> component_name = %s; state = %s\" % (\n-            self.component_name, self.state)\n-\n-    def _get_cluster_name(self):\n-        if self.clusterRef:\n-            return self.clusterRef.cluster_name\n-        return None\n-\n-    def _path(self):\n-        \"\"\"\n-        Return the API path for this service.\n-        \"\"\"\n-        if self._get_cluster_name():\n-            return paths.HOSTS_COMPONENTS_PATH % (\n-                self._get_cluster_name(), self.host_name)\n-        else:\n-            return ''\n-\n-    def get_metrics(self, detail=None):\n-        \"\"\"\n-        Gets the json containing all the metrics data\n-        @return: json.\n-        \"\"\"\n-        if self.service_name:\n-            metricpath = paths.SERVICE_COMPONENT_PATH % (\n-                self._get_cluster_name(), self.service_name, self.component_name) + \"?fields=metrics\"\n-        elif self.host_name:\n-            metricpath = paths.HOSTS_COMPONENT_PATH % (\n-                self._get_cluster_name(), self.host_name, self.component_name) + \"?fields=metrics\"\n-        metricjson = self._get_resource_root().get(metricpath)\n-        return metricjson\n-\n-\n-    def delete(self):\n-        return _delete_host_component(self._get_resource_root(), self._get_cluster_name(), self.host_name, self.component_name)\n-\n-    def install(self):\n-        data = {\n-            \"RequestInfo\": {\n-                \"context\": \"Install %s\" % normalize_all_caps(self.component_name),\n-            },\n-            \"HostRoles\": {\n-                \"state\": \"INSTALLED\",\n-            },\n-        }\n-        root_resource = self._get_resource_root()\n-        resp = root_resource.put(path=self._path() + '/' + self.component_name, payload=data)\n-        return utils.ModelUtils.create_model(status.StatusModel, resp, root_resource, \"NO_KEY\")\n-\n-    def start(self):\n-        data = {\n-            \"RequestInfo\": {\n-                \"context\": \"Start %s\" % normalize_all_caps(self.component_name),\n-            },\n-            \"HostRoles\": {\n-                \"state\": \"STARTED\",\n-            },\n-        }\n-        root_resource = self._get_resource_root()\n-        resp = root_resource.put(path=self._path() + '/' + self.component_name, payload=data)\n-        return utils.ModelUtils.create_model(status.StatusModel, resp, root_resource, \"NO_KEY\")\n-\n-    def stop(self):\n-        data = {\n-            \"RequestInfo\": {\n-                \"context\": \"Stop %s\" % normalize_all_caps(self.component_name),\n-            },\n-            \"HostRoles\": {\n-                \"state\": \"INSTALLED\",\n-            },\n-        }\n-        root_resource = self._get_resource_root()\n-        resp = root_resource.put(path=self._path() + '/' + self.component_name, payload=data)\n-        return utils.ModelUtils.create_model(status.StatusModel, resp, root_resource, \"NO_KEY\")\n-\n-    def restart(self):\n-        # need to move this to utils, handle _ gracefully\n-        data = {\n-            \"RequestInfo\": {\n-                \"command\": \"RESTART\", \n-                \"context\": \"Restart %s\" % normalize_all_caps(self.component_name),\n-                \"operation_level\": {\n-                    \"level\": \"SERVICE\",\n-                    \"cluster_name\": self._get_cluster_name(),\n-                    \"service_name\": self.service_name,\n-\n-                },\n-            },\n-            \"Requests/resource_filters\": [{\n-                \"service_name\": self.service_name,\n-                \"component_name\": self.component_name,\n-                \"hosts\": self.host_name,\n-            }],\n-        }\n-        root_resource = self._get_resource_root()\n-        path = paths.CLUSTER_REQUESTS_PATH % self._get_cluster_name()\n-        resp = root_resource.post(path=path, payload=data)\n-        return utils.ModelUtils.create_model(status.StatusModel, resp, root_resource, \"NO_KEY\")",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/component.py",
                "sha": "c0323abe2e002bf3c4b77d886b86c9512eb487d7",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/configuration.py",
                "changes": 179,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/model/configuration.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 179,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/model/configuration.py",
                "patch": "@@ -1,179 +0,0 @@\n-#\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-\n-from ambari_client.model.base_model import BaseModel, ModelList\n-from ambari_client.model import paths, status, utils\n-\n-\n-def _get_configuration(resource_root, cluster_name, type, tag=\"version1\"):\n-    \"\"\"\n-    Get configuration of a cluster\n-    @param resource_root: The root Resource .\n-    @param cluster_name: cluster_name\n-    @param type: type of config\n-    @return: A ConfigModel object\n-    \"\"\"\n-    dic = resource_root.get(\n-        paths.CONFIGURATION_PATH %\n-        (cluster_name, type, tag))\n-\n-    if len(dic[\"items\"]) == 0:\n-        return None\n-\n-    config_model = utils.ModelUtils.create_model(\n-        ConfigModel,\n-        dic[\"items\"][0],\n-        resource_root,\n-        \"NO_KEY\")\n-    ref_clss = utils.getREF_class_name(\"cluster_name\")\n-    config_model._setattr(ref_clss, dic[\"items\"][0]['Config']['cluster_name'])\n-    return config_model\n-\n-\n-def _get_all_configuration(resource_root, cluster_name, type):\n-    \"\"\"\n-    Gets ALL configuration of a cluster of a given type\n-    @param resource_root: The root Resource .\n-    @param cluster_name: cluster_name\n-    @param type: type of config\n-    @return: A ConfigModel object\n-    \"\"\"\n-    dic = resource_root.get(\n-        paths.CONFIGURATION_ALL_PATH %\n-        (cluster_name, type))\n-\n-    if len(dic[\"items\"]) == 0:\n-        return None\n-\n-    objects = []\n-    for cfgm in dic[\"items\"]:\n-        config_model = utils.ModelUtils.create_model(\n-            ConfigModel,\n-            cfgm,\n-            resource_root,\n-            \"NO_KEY\")\n-        ref_clss = utils.getREF_class_name(\"cluster_name\")\n-        config_model._setattr(ref_clss, cfgm['Config']['cluster_name'])\n-        objects.append(config_model)\n-    return ModelList(objects)\n-\n-\n-def _update_configuration(\n-        resource_root,\n-        cluster_name,\n-        type,\n-        tag,\n-        config_model):\n-    \"\"\"\n-    Update configuration of a cluster\n-    @param resource_root: The root Resource .\n-    @param cluster_name: cluster_name\n-    @param type: type of config\n-    @param config_model: config model object\n-    @return: A ConfigModel object\n-    \"\"\"\n-    data = {\n-        \"Clusters\": {\n-            \"desired_configs\": {\n-                \"type\": type,\n-                \"tag\": tag,\n-                \"properties\": config_model.properties}}}\n-    resp = resource_root.put(\n-        path=paths.UPDATE_CONFIGURATION_PATH %\n-        cluster_name,\n-        payload=data)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        resource_root,\n-        \"NO_KEY\")\n-\n-\n-def _add_config(root_resource, cluster_name, type, tag, properties):\n-    \"\"\"\n-    add configurations\n-    @param type: the type of config\n-    @param tag: tag\n-    @param properties: a dict of properties\n-    @return: A StatusModel object\n-    \"\"\"\n-    cpath = paths.CLUSTERS_CONFIG_PATH % cluster_name\n-    data = {\n-        \"Clusters\": {\n-            \"desired_configs\": {\n-                \"type\": type,\n-                \"tag\": tag,\n-                \"properties\": properties}}}\n-    resp = root_resource.put(path=cpath, payload=data)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        root_resource,\n-        \"NO_KEY\")\n-\n-\n-def _create_config(root_resource, cluster_name, type, tag, properties):\n-    \"\"\"\n-    create a new  configurations\n-    @param type: the type of config\n-    @param tag: tag\n-    @param properties: a dict of properties\n-    @return: A StatusModel object\n-    \"\"\"\n-    cpath = paths.CLUSTERS_CONFIG_PATH % cluster_name\n-    data = {\"type\": type, \"tag\": tag, \"properties\": properties}\n-    resp = root_resource.put(path=cpath, payload=data)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        root_resource,\n-        \"NO_KEY\")\n-\n-\n-class ConfigModel(BaseModel):\n-\n-    \"\"\"\n-    The ConfigModel class\n-    \"\"\"\n-    RO_ATTR = ('properties',)\n-    RW_ATTR = ('tag', 'type')\n-    REF_ATTR = ('cluster_name',)\n-\n-    def __init__(self, resource_root, tag, type=None):\n-        utils.retain_self_helper(BaseModel, **locals())\n-\n-    def __str__(self):\n-        return \"<<ConfigModel>> tag = %s; type = %s\" % (self.tag, self.type)\n-\n-    def _get_cluster_name(self):\n-        if self.clusterRef:\n-            return self.clusterRef.cluster_name\n-        return None\n-\n-    def __lt__(self, other):\n-        return self.tag < other.tag\n-\n-    def _path(self):\n-        \"\"\"\n-        Return the API path for this service.\n-        \"\"\"\n-        if self._get_cluster_name():\n-            return paths.CONFIGURATION_PATH % (\n-                self._get_cluster_name(), self.type, self.tag)\n-        else:\n-            return ''",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/configuration.py",
                "sha": "730ca33adcbff1e700eafc733d0b5c00f0c50718",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/host.py",
                "changes": 403,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/model/host.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 403,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/model/host.py",
                "patch": "@@ -1,403 +0,0 @@\n-#\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-try:\n-    import json\n-except ImportError:\n-    import simplejson as json\n-import logging\n-from ambari_client.model.base_model import BaseModel, ModelList\n-from ambari_client.model import status, component, paths, utils\n-\n-\n-LOG = logging.getLogger(__name__)\n-\n-\n-def _get_host(root_resource, host_name):\n-    \"\"\"\n-    Lookup up by host_name\n-    @param root_resource: The root Resource object.\n-    @param cluster_name: Cluster name\n-    @param host_name: Host name\n-    @return: A HostModel object\n-    \"\"\"\n-    path = paths.HOST_PATH % (host_name)\n-    dic = root_resource.get(path)\n-\n-    return utils.ModelUtils.create_model(\n-        HostModel,\n-        dic,\n-        root_resource,\n-        \"Hosts\")\n-\n-\n-def _get_cluster_host(root_resource, cluster_name, host_name):\n-    \"\"\"\n-    Lookup cluster host up by host_name\n-    @param root_resource: The root Resource object.\n-    @param cluster_name: Cluster name\n-    @param host_name: Host name\n-    @return: A HostModel object\n-    \"\"\"\n-    path = paths.CLUSTER_HOST_PATH % (cluster_name, host_name)\n-    dic = root_resource.get(path)\n-    return utils.ModelUtils.create_model(\n-        HostModel,\n-        dic,\n-        root_resource,\n-        \"Hosts\")\n-\n-\n-def _create_hosts(root_resource, host_list):\n-    \"\"\"\n-    Create hosts from list\n-    @param root_resource: The root Resource.\n-    @param host_name: Host name\n-    @param ip: IP address\n-    @param rack_info: Rack id. Default None\n-    @return: An HostList object\n-    \"\"\"\n-\n-    data = [{\"Hosts\": {\"host_name\": x.host_name,\n-                       \"ip\": x.ip,\n-                       \"rack_info\": x.rack_info}} for x in host_list]\n-    resp = root_resource.post(paths.HOSTS_PATH, payload=data)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        root_resource,\n-        \"NO_KEY\")\n-\n-\n-def _create_host(root_resource, host_name, ip, rack_info=None):\n-    \"\"\"\n-    Create a host\n-    @param root_resource: The root Resource.\n-    @param host_name: Host name\n-    @param ip: IP address\n-    @param rack_info: Rack id. Default None\n-    @return: An HostModel object\n-    \"\"\"\n-    host_list = ModelList([HostModel(root_resource, host_name, ip, rack_info)])\n-    return _create_hosts(root_resource, host_list)\n-\n-\n-def _add_hosts(root_resource, cluster_name, host_list):\n-    \"\"\"\n-    Adds a hosts to a cluster.\n-    @param root_resource: The root Resource object.\n-    @param cluster_name: Cluster name\n-    @param host_list: list of hosts\n-    @return: A StatusModel object\n-    \"\"\"\n-    cpath = paths.HOSTS_CREATE_PATH % (cluster_name)\n-    data = [{\"Hosts\": {\"host_name\": x.host_name,\n-                       \"ip\": x.ip,\n-                       \"rack_info\": x.rack_info}} for x in host_list]\n-    resp = root_resource.post(path=cpath, payload=data)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        root_resource,\n-        \"NO_KEY\")\n-\n-\n-def _add_host(root_resource, cluster_name, host_name, ip, rack_info=None):\n-    \"\"\"\n-    Adds a host to a cluster.\n-    @param host_name: Host name\n-    @param ip: ip of Host\n-    @param rack_info: rack information\n-    @return: StatusModel.\n-    \"\"\"\n-    host_list = ModelList([HostModel(root_resource, host_name, ip, rack_info)])\n-    return _add_hosts(root_resource, cluster_name, host_list)\n-\n-\n-def _assign_role(root_resource, cluster_name, host_name, component_name):\n-    \"\"\"\n-    Add a new component to a node\n-    @param root_resource: The root Resource object.\n-    @param cluster_name: Cluster name\n-    @param component_name : name of component.\n-    @param host_name: name of host\n-    @return: StatusModel\n-    \"\"\"\n-    data = {\"host_components\": [\n-        {\"HostRoles\": {\"component_name\": component_name}}]}\n-    cpath = paths.HOSTS_ASSIGN_ROLE % (cluster_name, host_name)\n-    resp = root_resource.post(path=cpath, payload=data)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        root_resource,\n-        \"NO_KEY\")\n-\n-\n-def _get_all_hosts(root_resource):\n-    \"\"\"\n-    Get all hosts\n-    @param root_resource: The root Resource.\n-    @return: A list of HostModel objects.\n-    \"\"\"\n-    dic = root_resource.get(paths.HOSTS_PATH)\n-    return utils.ModelUtils.get_model_list(\n-        ModelList,\n-        HostModel,\n-        dic,\n-        root_resource,\n-        \"Hosts\")\n-\n-\n-def _get_all_cluster_hosts(root_resource, cluster_name):\n-    \"\"\"\n-    Get all hosts in the cluster\n-    @param root_resource: The root Resource.\n-    @param cluster_name: The name of the cluster.\n-    @return: A list of HostModel objects.\n-    \"\"\"\n-    path = paths.CLUSTER_HOSTS_PATH % (cluster_name)\n-    path = path + '?fields=*'\n-    dic = root_resource.get(path)\n-    return utils.ModelUtils.get_model_list(\n-        ModelList,\n-        HostModel,\n-        dic,\n-        root_resource,\n-        \"Hosts\")\n-\n-\n-def _delete_host(root_resource, host_name):\n-    \"\"\"\n-    Delete a host by id\n-    @param root_resource: The root Resource object.\n-    @param host_name: Host name\n-    @return: StatusModel object\n-    \"\"\"\n-    resp = root_resource.delete(paths.HOST_PATH % (host_name))\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        root_resource,\n-        \"NO_KEY\")\n-\n-\n-def _delete_cluster_host(root_resource, cluster_name, host_name):\n-    \"\"\"\n-    Delete a host by id\n-    @param root_resource: The root Resource object.\n-    @param host_name: Host name\n-    @param cluster_name: cluster name\n-    @return: StatusModel object\n-    \"\"\"\n-    path = paths.CLUSTER_HOST_PATH % (cluster_name, host_name)\n-    resp = root_resource.delete(path)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        root_resource,\n-        \"NO_KEY\")\n-\n-\n-def _bootstrap_hosts(root_resource, hosts_list, ssh_key, ssh_user):\n-    \"\"\"\n-    Bootstrap hosts.\n-    @param hosts_list list of host_names.\n-    @return: A  StatusModel object.\n-    \"\"\"\n-    payload_dic = {\n-        \"verbose\": True,\n-        \"sshKey\": ssh_key,\n-        \"hosts\": hosts_list,\n-        \"user\": ssh_user}\n-    resp = root_resource.post(\n-        paths.BOOTSTRAP_PATH,\n-        payload_dic,\n-        content_type=\"application/json\")\n-    status_dict = _bootstrap_resp_to_status_dict(resp)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        status_dict,\n-        root_resource,\n-        \"NO_KEY\")\n-\n-\n-def _bootstrap_resp_to_status_dict(resp):\n-    \"\"\"\n-    Bootstrap response has a little odd format\n-    that's why we have to convert it to the normal\n-    format to handle it properly later.\n-    \"\"\"\n-\n-    # if we got other response, like an error 400 happened on higher level\n-    if isinstance(resp['status'], int):\n-        return resp\n-\n-    new_resp = {}\n-\n-    if resp['status'] == \"OK\":\n-        new_resp['status'] = 201\n-    else:  # ERROR\n-        new_resp['status'] = 500\n-\n-    new_resp['message'] = resp['log']\n-    new_resp['requestId'] = resp['requestId']\n-    return new_resp\n-\n-\n-class HostModel(BaseModel):\n-\n-    \"\"\"\n-    The HostModel class\n-    \"\"\"\n-    RO_ATTR = ('host_state', 'public_host_name')\n-    RW_ATTR = ('host_name', 'ip', 'rack_info')\n-    REF_ATTR = ('cluster_name',)\n-\n-    def __init__(\n-            self,\n-            resource_root,\n-            host_name,\n-            ip=None,\n-            rack_info='/default-rack'):\n-        utils.retain_self_helper(BaseModel, **locals())\n-\n-    def __str__(self):\n-        return \"<<HostModel>> hostname = %s; ip = %s ; rack_info = %s\" % (\n-            self.host_name, self.ip, self.rack_info)\n-\n-    def _get_cluster_name(self):\n-        if self.clusterRef:\n-            return self.clusterRef.cluster_name\n-        return None\n-\n-    def _path(self):\n-        return paths.HOSTS_PATH + '/' + self.host_name\n-\n-    def get_host_components(self, detail=None):\n-        \"\"\"\n-        Get a specific host's components.\n-        @return: A ModelList containing ComponentModel objects.\n-        \"\"\"\n-        return component.get_host_components(\n-            self._get_resource_root(),\n-            self._get_cluster_name(),\n-            self.host_name)\n-\n-    def get_host_component(self, component_name, detail=None):\n-        \"\"\"\n-        Get a specific host's ,specific component.\n-        @param component_name : name of component.\n-        @return: A ComponentModel object.\n-        \"\"\"\n-        return component.get_host_component(\n-            self._get_resource_root(),\n-            self._get_cluster_name(),\n-            self.host_name,\n-            component_name)\n-\n-    def assign_role(self, component_name, detail=None):\n-        \"\"\"\n-        Assign a component role to the host\n-        @param component_name : name of component.\n-        @return: StatusModel.\n-        \"\"\"\n-        return _assign_role(\n-            self._get_resource_root(),\n-            self._get_cluster_name(),\n-            self.host_name,\n-            component_name)\n-\n-    def install_all_components(self):\n-        root_resource = self._get_resource_root()\n-        path = paths.HOSTS_COMPONENTS_PATH % (self._get_cluster_name(), \n-                                              self.host_name)\n-        data = {\n-            \"RequestInfo\": {\n-                \"context\" :\"Install All Components\",\n-            }, \n-            \"Body\": {\n-                \"HostRoles\": {\"state\": \"INSTALLED\"},\n-            },\n-        }\n-        resp = root_resource.put(path=path, payload=data)\n-        return utils.ModelUtils.create_model(status.StatusModel, resp, \n-                                             root_resource, \"NO_KEY\")\n-\n-    def start_all_components(self):\n-        root_resource = self._get_resource_root()\n-        path = paths.HOSTS_COMPONENTS_PATH % (self._get_cluster_name(), \n-                                              self.host_name)\n-        data = {\n-            \"RequestInfo\": {\n-                \"context\" :\"Start All Components\",\n-            }, \n-            \"Body\": {\n-                \"HostRoles\": {\"state\": \"STARTED\"},\n-            },\n-        }\n-        resp = root_resource.put(path=path, payload=data)\n-        return utils.ModelUtils.create_model(status.StatusModel, resp, \n-                                             root_resource, \"NO_KEY\")\n-\n-    def stop_all_components(self):\n-        root_resource = self._get_resource_root()\n-        path = paths.HOSTS_COMPONENTS_PATH % (self._get_cluster_name(), \n-                                              self.host_name)\n-        data = {\n-            \"RequestInfo\": {\n-                \"context\" :\"Stop All Components\",\n-            }, \n-            \"Body\": {\n-                \"HostRoles\": {\"state\": \"INSTALLED\"},\n-            },\n-        }\n-        resp = root_resource.put(path=path, payload=data)\n-        return utils.ModelUtils.create_model(status.StatusModel, resp, \n-                                             root_resource, \"NO_KEY\")\n-\n-    def enable_maintenance_mode(self):\n-        root_resource = self._get_resource_root()\n-        path = paths.HOSTS_COMPONENTS_PATH % (self._get_cluster_name(), \n-                                              self.host_name)\n-        data = {\n-            \"RequestInfo\": {\n-                \"context\" :\"Start Maintanence Mode\",\n-            }, \n-            \"Body\": {\n-                \"HostRoles\": {\"maintenance_state\": \"ON\"},\n-            },\n-        }\n-        resp = root_resource.put(path=path, payload=data)\n-        return utils.ModelUtils.create_model(status.StatusModel, resp, \n-                                             root_resource, \"NO_KEY\")\n-\n-    def disable_maintenance_mode(self):\n-        root_resource = self._get_resource_root()\n-        path = paths.HOSTS_COMPONENTS_PATH % (self._get_cluster_name(), \n-                                              self.host_name)\n-        data = {\n-            \"RequestInfo\": {\n-                \"context\" :\"Stop Maintanence Mode\",\n-            }, \n-            \"Body\": {\n-                \"HostRoles\": {\"maintenance_state\": \"OFF\"},\n-            },\n-        }\n-        resp = root_resource.put(path=path, payload=data)\n-        return utils.ModelUtils.create_model(status.StatusModel, resp, \n-                                             root_resource, \"NO_KEY\")",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/host.py",
                "sha": "dd4bcb4ce91033433a974323241247ede8ecc105",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/paths.py",
                "changes": 61,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/model/paths.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 61,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/model/paths.py",
                "patch": "@@ -1,61 +0,0 @@\n-#\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-CLUSTERS_PATH = \"/clusters\"\n-CLUSTERS_CONFIG_PATH = \"/clusters/%s\"\n-CLUSTER_HOSTS_PATH = \"/clusters/%s/hosts\"\n-CLUSTER_HOST_PATH = \"/clusters/%s/hosts/%s\"\n-CLUSTER_START_ALL_SERVICES = \"/clusters/%s/services?ServiceInfo/state=INSTALLED\"\n-CLUSTER_STOP_ALL_SERVICES = \"/clusters/%s/services?ServiceInfo\"\n-CLUSTER_REQUESTS_PATH = \"/clusters/%s/requests\"\n-\n-SERVICES_PATH = \"/clusters/%s/services\"\n-SERVICE_PATH = \"/clusters/%s/services/%s\"\n-SERVICE_CREATE_PATH = \"/clusters/%s/services/?ServiceInfo/service_name=%s\"\n-SERVICE_COMPONENTS_PATH = \"/clusters/%s/services/%s/components?fields=*\"\n-SERVICE_COMPONENT_PATH = \"/clusters/%s/services/%s/components/%s\"\n-\n-\n-HOST_PATH = \"/hosts/%s\"\n-HOSTS_PATH = \"/hosts\"\n-HOSTS_CREATE_PATH = \"/clusters/%s/hosts\"\n-HOSTS_COMPONENTS_PATH = \"/clusters/%s/hosts/%s/host_components?fields=HostRoles/state\"\n-HOSTS_COMPONENT_PATH = \"/clusters/%s/hosts/%s/host_components/%s\"\n-HOSTS_ASSIGN_ROLE = \"/clusters/%s/hosts?Hosts/host_name=%s\"\n-\n-BOOTSTRAP_PATH = \"/bootstrap\"\n-REQUEST_STATUS_PATH = \"/clusters/%s/requests/%s?fields=tasks/Tasks/status\"\n-REQUEST_PATH = \"clusters/%s/requests/%s\"\n-REQUEST_N_PATH = \"clusters/%s/requests?to=end&page_size=%s&fields=Requests\"\n-\n-CONFIGURATION_PATH = \"/clusters/%s/configurations?type=%s&tag=%s\"\n-CONFIGURATION_ALL_PATH = \"/clusters/%s/configurations?type=%s\"\n-CREATE_CONFIGURATION_PATH = \"/clusters/%s/configurations\"\n-UPDATE_CONFIGURATION_PATH = \"/clusters/%s\"\n-\n-STACKS_PATH = \"/stacks2\"\n-\n-STACKS_VERSIONS_PATH = \"stacks2/HDP/versions\"\n-STACKS_OS_REPO_PATH = \"stacks2/HDP/versions/%s/operatingSystems/%s/repositories/HDP-%s\"\n-STACK_SERVICES_COMPONENTS_PATH = \"/stacks2/HDP/versions/%s/stackServices/%s/serviceComponents?fields=*\"\n-STACK_SERVICES_CONFIG_PATH = \"/stacks2/HDP/versions/%s/stackServices/%s/configurations?fields=*\"\n-\n-TASKS_PATH = \"clusters/%s/requests/%s/tasks?fields=*\"\n-\n-BLUEPRINT_ALL_PATH = \"blueprints?fields=*\"\n-BLUEPRINT_PATH = \"blueprints/%s\"\n-BLUEPRINT_CLUSTER_PATH = \"clusters/%s?format=blueprint\"",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/paths.py",
                "sha": "17a4633ed9cad6d5e87908b7ae11efbc7f734ba3",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/service.py",
                "changes": 270,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/model/service.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 270,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/model/service.py",
                "patch": "@@ -1,270 +0,0 @@\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-import logging\n-import time\n-from ambari_client.model.base_model import BaseModel, ModelList\n-from ambari_client.model import component, paths, status, stack, utils\n-\n-\n-LOG = logging.getLogger(__name__)\n-\n-\n-def _get_all_services(resource_root, cluster_name):\n-    \"\"\"\n-    Get all services in a cluster.\n-    @param cluster_name :Cluster name.\n-    @return: A  ModelList object.\n-    \"\"\"\n-    path = paths.SERVICES_PATH % (cluster_name,)\n-    path = path + '?fields=*'\n-    dic = resource_root.get(path)\n-    return utils.ModelUtils.get_model_list(\n-        ModelList,\n-        ServiceModel,\n-        dic,\n-        resource_root,\n-        \"ServiceInfo\")\n-\n-\n-def _get_service(resource_root, service_name, cluster_name):\n-    \"\"\"\n-    Get a specific services in a cluster.\n-    @param service_name :Service name.\n-    @param cluster_name :Cluster name.\n-    @return: A  ServiceModel object.\n-    \"\"\"\n-    path = \"%s/%s\" % (paths.SERVICES_PATH % (cluster_name,), service_name)\n-    dic = resource_root.get(path)\n-    return utils.ModelUtils.create_model(\n-        ServiceModel,\n-        dic,\n-        resource_root,\n-        \"ServiceInfo\")\n-\n-\n-def _create_services(root_resource, cluster_name, service_names):\n-    \"\"\"\n-    Create services\n-    @param root_resource: The root Resource object.\n-    @param service_names: list of service_names\n-    @param cluster_name: Cluster name\n-    @return: StatusModel\n-    \"\"\"\n-    data = [{\"ServiceInfo\": {\"service_name\": x}} for x in service_names]\n-    cpath = paths.SERVICES_PATH % cluster_name\n-    resp = root_resource.post(path=cpath, payload=data)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        root_resource,\n-        \"NO_KEY\")\n-\n-\n-def _create_service(root_resource, cluster_name, service_name):\n-    \"\"\"\n-    Create a single service\n-    @param root_resource: The root Resource object.\n-    @param service_name:  service_name\n-    @param cluster_name: Cluster name\n-    @return: StatusModel\n-    \"\"\"\n-    data = {\"ServiceInfo\": {\"service_name\": service_name}}\n-    cpath = paths.SERVICES_PATH % cluster_name\n-    resp = root_resource.post(path=cpath, payload=data)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        root_resource,\n-        \"NO_KEY\")\n-\n-\n-def _create_service_components(\n-        root_resource,\n-        cluster_name,\n-        version,\n-        service_name):\n-    \"\"\"\n-    Create service with components\n-    @param root_resource: The root Resource object.\n-    @param service_name:  service_names\n-    @param cluster_name: Cluster service_name\n-    @return: An ServiceModel object\n-    \"\"\"\n-    components = stack._get_components_from_stack(\n-        root_resource,\n-        version,\n-        service_name)\n-    list_componnetinfo = [\n-        {\"ServiceComponentInfo\": {\"component_name\": x.component_name}} for x in components]\n-    data = {\"components\": list_componnetinfo}\n-    cpath = paths.SERVICE_CREATE_PATH % (cluster_name, service_name)\n-    resp = root_resource.post(path=cpath, payload=data)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        root_resource,\n-        \"NO_KEY\")\n-\n-\n-def _create_service_component(\n-        root_resource,\n-        cluster_name,\n-        version,\n-        service_name,\n-        component_name):\n-    \"\"\"\n-    Create service with single component\n-    @param root_resource: The root Resource object.\n-    @param service_name:  service_names\n-    @param cluster_name: Cluster service_name\n-    @param component_name: name of component\n-    @return: An ServiceModel object\n-    \"\"\"\n-    cpath = paths.SERVICE_COMPONENT_PATH % (\n-        cluster_name, service_name, component_name)\n-    resp = root_resource.post(path=cpath, payload=None)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        root_resource,\n-        \"NO_KEY\")\n-\n-\n-def _delete_service(root_resource, service_name, cluster_name):\n-    \"\"\"\n-    Delete a service by service_name\n-    @param root_resource: The root Resource object.\n-    @param service_name: Service service_name\n-    @param cluster_name: Cluster service_name\n-    @return: The StatusModel object\n-    \"\"\"\n-    resp = root_resource.delete(\n-        \"%s/%s\" %\n-        (paths.SERVICES_PATH %\n-         (cluster_name,), service_name))\n-    time.sleep(3)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        resp,\n-        root_resource,\n-        \"NO_KEY\")\n-\n-\n-class ServiceModel(BaseModel):\n-\n-    \"\"\"\n-    The ServiceModel class\n-    \"\"\"\n-    #RO_ATTR = ('state', 'cluster_name')\n-    RW_ATTR = ('service_name', 'state')\n-    REF_ATTR = ('cluster_name',)\n-\n-    def __init__(self, resource_root, service_name, state):\n-        #BaseModel.__init__(self, **locals())\n-        utils.retain_self_helper(BaseModel, **locals())\n-\n-    def __str__(self):\n-        return \"<<ServiceModel>> = %s ;state = %s ; cluster_name = %s\" % (\n-            self.service_name, self.state, self._get_cluster_name())\n-\n-    def _get_cluster_name(self):\n-        if self.clusterRef:\n-            return self.clusterRef.cluster_name\n-        return None\n-\n-    def _path(self):\n-        \"\"\"\n-        Return the API path for this object.\n-        \"\"\"\n-        if self._get_cluster_name():\n-            return paths.SERVICE_PATH % (\n-                self._get_cluster_name(), self.service_name)\n-        else:\n-            return ''\n-\n-    def _action(self, data=None):\n-        path = self._path()\n-        resp = self._get_resource_root().put(path, payload=data)\n-        status_model = utils.ModelUtils.create_model(\n-            status.StatusModel,\n-            resp,\n-            self._get_resource_root(),\n-            \"NO_KEY\")\n-        if status_model._get_id() is not None:\n-            status_model.request_path = paths.REQUEST_PATH % (\n-                self._get_cluster_name(), status_model._get_id())\n-        else:\n-            status_model.request_path = None\n-        return status_model\n-\n-    def start(self, message=None):\n-        \"\"\"\n-        Start a service.\n-        \"\"\"\n-        data = None\n-        if message:\n-            data = {\n-                \"RequestInfo\": {\n-                    \"context\": message}, \"Body\": {\n-                    \"ServiceInfo\": {\n-                        \"state\": \"STARTED\"}}}\n-        else:\n-            data = {\"ServiceInfo\": {\"state\": \"STARTED\"}}\n-        return self._action(data)\n-\n-    def stop(self, message=None):\n-        \"\"\"\n-        Stop a service.\n-        \"\"\"\n-        data = None\n-        if message:\n-            data = {\n-                \"RequestInfo\": {\n-                    \"context\": message}, \"Body\": {\n-                    \"ServiceInfo\": {\n-                        \"state\": \"INSTALLED\"}}}\n-        else:\n-            data = {\"ServiceInfo\": {\"state\": \"INSTALLED\"}}\n-        return self._action(data)\n-\n-    def install(self):\n-        \"\"\"\n-        Install a service.\n-        \"\"\"\n-        data = {\"ServiceInfo\": {\"state\": \"INSTALLED\"}}\n-        return self._action(data)\n-\n-    def get_service_components(self, detail=None):\n-        \"\"\"\n-        Get a specific services's components.\n-        @return: A ComponentModel object.\n-        \"\"\"\n-        return component._get_service_components(\n-            self._get_resource_root(),\n-            self._get_cluster_name(),\n-            self.service_name)\n-\n-    def get_service_component(self, component_name, detail=None):\n-        \"\"\"\n-        Get a specific services's components.\n-        @return: A ComponentModel object.\n-        \"\"\"\n-        return component._get_service_component(\n-            self._get_resource_root(),\n-            self._get_cluster_name(),\n-            self.service_name,\n-            component_name)",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/service.py",
                "sha": "acd75281698e09cd5c51821d77ca2d2ec6a730b4",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/stack.py",
                "changes": 187,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/model/stack.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 187,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/model/stack.py",
                "patch": "@@ -1,187 +0,0 @@\n-#\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-\n-from ambari_client.model.base_model import BaseModel, ModelList\n-from ambari_client.model import paths, utils, status\n-\n-\n-def _get_configuration_from_stack(\n-        resource_root,\n-        version,\n-        service_name,\n-        tag=\"version1\"):\n-    \"\"\"\n-    Get configuration from stack\n-    @param resource_root: The root Resource .\n-    @param cluster_name: cluster_name\n-    @param type: type of config\n-    @return: A ModelList of ConfigModel object\n-    \"\"\"\n-    dic = resource_root.get(\n-        paths.STACK_SERVICES_CONFIG_PATH %\n-        (version, service_name))\n-    return utils.ModelUtils.get_model_list(\n-        ModelList,\n-        StackConfigModel,\n-        dic,\n-        resource_root,\n-        \"StackConfigurations\")\n-\n-\n-def _get_components_from_stack(\n-        resource_root,\n-        version,\n-        service_name,\n-        tag=\"version1\"):\n-    \"\"\"\n-    Get configuration from stack\n-    @param resource_root: The root Resource .\n-    @param cluster_name: cluster_name\n-    @param type: type of config\n-    @return: A ModelList of ConfigModel object\n-    \"\"\"\n-    path = paths.STACK_SERVICES_COMPONENTS_PATH % (version, service_name)\n-    dic = resource_root.get(path)\n-    return utils.ModelUtils.get_model_list(\n-        ModelList,\n-        StackComponentModel,\n-        dic,\n-        resource_root,\n-        \"StackServiceComponents\")\n-\n-\n-def _get_stacks(resource_root, versions=False):\n-    \"\"\"\n-    Get configuration from stack\n-    @param resource_root: The root Resource .\n-    @param cluster_name: cluster_name\n-    @param type: type of config\n-    @return: A ModelList of StackModel object\n-    \"\"\"\n-    if(versions):\n-        path = paths.STACKS_VERSIONS_PATH\n-        dic = resource_root.get(path)\n-        return utils.ModelUtils.get_model_list(\n-            ModelList,\n-            StackModel,\n-            dic,\n-            resource_root,\n-            \"Versions\")\n-    else:\n-        path = paths.STACKS_PATH\n-        dic = resource_root.get(path)\n-        return utils.ModelUtils.get_model_list(\n-            ModelList,\n-            StackModel,\n-            dic,\n-            resource_root,\n-            \"Stacks\")\n-\n-\n-def _get_stack(resource_root, versions):\n-    \"\"\"\n-    Get configuration from stack\n-    @param resource_root: The root Resource .\n-    @param cluster_name: cluster_name\n-    @param type: type of config\n-    @return: A ModelList of StackModel object\n-    \"\"\"\n-    pass\n-\n-\n-def _put_stacks_and_repo(resource_root, version, os_type, repo):\n-    \"\"\"\n-    Get configuration from stack\n-    @param resource_root: The root Resource .\n-    @param cluster_name: cluster_name\n-    @param type: type of config\n-    @return: A ModelList of StackModel object\n-    \"\"\"\n-    path = (paths.STACKS_OS_REPO_PATH) % (version, os_type, version)\n-    data = '{\"Repositories\":{\"base_url\":' + repo + ',\"verify_base_url\":true}}'\n-    dictt = resource_root.put(path, payload=data)\n-    return utils.ModelUtils.create_model(\n-        status.StatusModel,\n-        dictt,\n-        resource_root,\n-        \"NO_KEY\")\n-\n-\n-class StackModel(BaseModel):\n-\n-    \"\"\"\n-    The StackModel class\n-    \"\"\"\n-    RO_ATTR = ('stack_version')\n-    RW_ATTR = ('stack_name')\n-    REF_ATTR = ()\n-\n-    def __init__(self, resource_root, stack_name):\n-        utils.retain_self_helper(BaseModel, **locals())\n-\n-    def __str__(self):\n-        return \"<<StackConfigModel>> stack_name= %s\" % (self.stack_name)\n-\n-\n-class StackConfigModel(BaseModel):\n-\n-    \"\"\"\n-    The StackConfigModel class\n-    \"\"\"\n-    RO_ATTR = ('stack_name', 'type', 'property_description')\n-    RW_ATTR = (\n-        'property_name',\n-        'property_value',\n-        'service_name',\n-        'stack_version')\n-    REF_ATTR = ()\n-\n-    def __init__(\n-            self,\n-            resource_root,\n-            property_name,\n-            property_value=None,\n-            service_name=None,\n-            stack_version=None):\n-        utils.retain_self_helper(BaseModel, **locals())\n-\n-    def __str__(self):\n-        return \"<<StackConfigModel>> property_name=%s; service_name= %s\" % (\n-            self.property_name, self.service_name)\n-\n-\n-class StackComponentModel(BaseModel):\n-\n-    \"\"\"\n-    The StackComponentModel class\n-    \"\"\"\n-    RO_ATTR = ('stack_name', 'is_master', 'is_client', 'component_category')\n-    RW_ATTR = ('component_name', 'service_name', 'stack_version')\n-    REF_ATTR = ()\n-\n-    def __init__(\n-            self,\n-            resource_root,\n-            component_name,\n-            service_name=None,\n-            stack_version=None):\n-        utils.retain_self_helper(BaseModel, **locals())\n-\n-    def __str__(self):\n-        return \"<<StackComponentModel>> component_name = %s; service_name = %s\" % (\n-            self.component_name, self.service_name)",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/stack.py",
                "sha": "4d633761c1eecd9baf395eee23f0d69508cf72d5",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/status.py",
                "changes": 112,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/model/status.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 112,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/model/status.py",
                "patch": "@@ -1,112 +0,0 @@\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-import logging\n-from ambari_client.model.base_model import BaseModel, ModelList\n-from ambari_client.model import paths, utils\n-\n-\n-LOG = logging.getLogger(__name__)\n-\n-\n-class StatusModel(BaseModel):\n-\n-    \"\"\"\n-    The ServiceModel class\n-    \"\"\"\n-    RO_ATTR = ('id',)\n-    RW_ATTR = ('status', 'requestId', \"message\")\n-    REF_ATTR = ('cluster_name',)\n-\n-    def __init__(self, resource_root, status, requestId=None, message=None):\n-        # BaseModel.__init__(self, **locals())\n-        utils.retain_self_helper(BaseModel, **locals())\n-\n-    def __str__(self):\n-        return \"<<StatusModel>> status = %s ; requestId = %s ;message = %s\" % (\n-            self._get_status(), self._get_id(), self.get_message())\n-\n-    def get_bootstrap_path(self):\n-        return paths.BOOTSTRAP_PATH + '/' + str(self.requestId)\n-\n-    def get_request_path(self):\n-        return self.request_path\n-\n-    def get_message(self):\n-        if hasattr(self, 'message'):\n-            return self.message\n-        else:\n-            None\n-\n-    def is_error(self):\n-        return (\n-            self.status != 200 and self.status != 201 and self.status != 202)\n-\n-    def _get_id(self):\n-        if hasattr(self, 'requestId') and self.requestId:\n-            return self.requestId\n-        elif hasattr(self, 'id') and self.id:\n-            return self.id\n-        else:\n-            None\n-\n-    def _get_status(self):\n-        if hasattr(self, 'status') and isinstance(self.status, basestring):\n-            self.message = self.status\n-            self.status = 200\n-            return self.status\n-        elif hasattr(self, 'status') and isinstance(self.status, int):\n-            return self.status\n-        else:\n-            None\n-\n-\n-def _get_N_requests(resource_root, cluster_name, noOfrequest):\n-    \"\"\"\n-    Get all services in a cluster.\n-    @param cluster_name :Cluster name.\n-    @return: A  ModelList object.\n-    \"\"\"\n-    path = paths.REQUEST_N_PATH % (cluster_name, noOfrequest)\n-    dic = resource_root.get(path)\n-    return utils.ModelUtils.get_model_list(\n-        ModelList,\n-        RequestModel,\n-        dic,\n-        resource_root,\n-        \"Requests\")\n-\n-\n-class RequestModel(BaseModel):\n-\n-    \"\"\"\n-    The RequestModel class\n-    \"\"\"\n-    RO_ATTR = (\"request_context\",)\n-    RW_ATTR = ('id', 'request_status')\n-    REF_ATTR = ('cluster_name',)\n-\n-    def __init__(self, resource_root, id, request_status=None):\n-        # BaseModel.__init__(self, **locals())\n-        utils.retain_self_helper(BaseModel, **locals())\n-\n-    def __str__(self):\n-        return \"<<RequestModel>> id = %s ; request_status = %s\" % (\n-            self.id, self.request_status)\n-\n-    def is_error(self):\n-        return (\n-            self.status != 200 and self.status != 201 and self.status != 202)",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/status.py",
                "sha": "8ef114000887251527e000354399e03f109f89c8",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/utils.py",
                "changes": 316,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/model/utils.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 316,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/model/utils.py",
                "patch": "@@ -1,316 +0,0 @@\n-#\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-import logging\n-import sys\n-import unicodedata\n-from ambari_client.core import errors\n-\n-LOG = logging.getLogger(__name__)\n-\n-\n-ref_dic = {\"cluster_name\": \"clusterRef\"}\n-ref_class_dic = {\"ClusterModelRef\": \"cluster_name\"}\n-ref_pkg_dic = {\"ClusterModelRef\": \"ambari_client.model.cluster\"}\n-LIST_KEY = \"items\"\n-ALL = \"ALL\"\n-\n-\n-class ModelUtils(object):\n-\n-    @staticmethod\n-    def _check_is_error(expected_class, model_dict, resource_root):\n-        from ambari_client.model.status import StatusModel\n-        from ambari_client.model.cluster import TaskModel\n-\n-        if expected_class == TaskModel:\n-            resp = ModelUtils.create_model(\n-                TaskModel,\n-                model_dict.copy(),\n-                resource_root,\n-                \"NO_KEY\",\n-                check_errors=False)\n-            return\n-\n-        if \"status\" in model_dict:\n-            resp = ModelUtils.create_model(\n-                StatusModel,\n-                model_dict.copy(),\n-                resource_root,\n-                \"NO_KEY\",\n-                check_errors=False)\n-\n-            if expected_class != StatusModel or resp.is_error():\n-                if resp.status in errors._exceptions_to_codes:\n-                    raise errors._exceptions_to_codes[\n-                        resp.status](\n-                        resp,\n-                        resource_root)\n-                else:\n-                    raise errors.UnknownServerError(resp, resource_root)\n-\n-    @staticmethod\n-    def get_model_list(\n-            member_list_clss,\n-            member_cls,\n-            collection_dict,\n-            resource_root,\n-            RESOURCE_KEY_WORD,\n-            check_errors=True):\n-        \"\"\"\n-        create a model.\n-        @param member_list_clss : model_list class.\n-        @param model_cls : model class.\n-        @param collection_dict : collection dict used for creating the list of objects.\n-        @param resource_root : resource object.\n-        @param RESOURCE_KEY_WORD : tsake subset of model_dict based on this key.\n-        @return: A  ModelList object.\n-        \"\"\"\n-        tLIST_KEY = LIST_KEY\n-\n-        if check_errors:\n-            ModelUtils._check_is_error(\n-                member_list_clss,\n-                collection_dict,\n-                resource_root)\n-\n-        # print locals()\n-        json_list = []\n-\n-        # remove items\n-        if isinstance(collection_dict, dict) and tLIST_KEY in collection_dict:\n-            json_list = collection_dict[tLIST_KEY]\n-            LOG.debug(\n-                \"get_model_list: collection_dict is dict ? %s ; has_key = %s\" %\n-                (isinstance(\n-                    collection_dict,\n-                    dict),\n-                    LIST_KEY in collection_dict))\n-            LOG.debug(\n-                \"get_model_list: collection_dict has %s ;subset = %s\" %\n-                (tLIST_KEY, str(json_list)))\n-        else:\n-            json_list = collection_dict\n-            LOG.error(\n-                \"get_model_list: collection_dict is dict ? %s ; has_key = %s\" %\n-                (isinstance(\n-                    collection_dict,\n-                    dict),\n-                    LIST_KEY in collection_dict))\n-\n-        LOG.debug(\"get_model_list: json_list  value : \\n\\t\" + str(json_list))\n-        if isinstance(json_list, list):\n-            json_list_new = [x.get(RESOURCE_KEY_WORD) for x in json_list]\n-            LOG.debug(\n-                \"get_model_list: json_list is list ? %s ; \" %\n-                (isinstance(\n-                    json_list,\n-                    list)))\n-        else:\n-            json_list_new = [json_list]\n-            LOG.error(\n-                \"get_model_list: json_list is list ? %s ; \" %\n-                (isinstance(\n-                    json_list,\n-                    list)))\n-\n-        LOG.debug(\n-            \"get_model_list: json_list_new used for creating ModelList  \\n\\t\" +\n-            str(json_list_new))\n-        objects = [\n-            ModelUtils.create_model(\n-                member_cls,\n-                x,\n-                resource_root,\n-                RESOURCE_KEY_WORD) for x in json_list_new]\n-        LOG.debug(objects)\n-        return member_list_clss(objects)\n-\n-    @staticmethod\n-    def create_model(\n-            model_cls,\n-            model_dict,\n-            resource_root,\n-            RESOURCE_KEY_WORD,\n-            check_errors=True):\n-        \"\"\"\n-        create a model.\n-        @param model_cls : model class.\n-        @param model_dict : model dict used for creating the object.\n-        @param resource_root : resource object.\n-        @param RESOURCE_KEY_WORD : tsake subset of model_dict based on this key.\n-        @return: A model_cls object.\n-        \"\"\"\n-        if check_errors:\n-            ModelUtils._check_is_error(model_cls, model_dict, resource_root)\n-\n-        # print locals()\n-        rw_dict = {}\n-        LOG.debug(\"model_dict =   \" + str(model_dict))\n-\n-        # extract model /keyword\n-        if isinstance(model_dict, dict) and RESOURCE_KEY_WORD in model_dict:\n-            model_dict = model_dict[RESOURCE_KEY_WORD]\n-            if not isinstance(model_dict, list):\n-                LOG.debug(\n-                    \"model_dict has %s ;subset = %s\" %\n-                    (RESOURCE_KEY_WORD, str(\n-                        model_dict.items())))\n-            else:\n-                LOG.debug(\n-                    \"model_dict is list and has %s ;subset = %s\" %\n-                    (RESOURCE_KEY_WORD, str(\n-                        model_dict)))\n-        # check for Requests\n-        if isinstance(model_dict, dict) and \"Requests\" in model_dict:\n-            model_dict = model_dict[\"Requests\"]\n-            LOG.debug(\n-                \"model_dict has Requests ;subset = %s\" %\n-                (str(\n-                    model_dict.items())))\n-\n-        # check for composition i.e list of Models\n-        if isinstance(model_dict, list):\n-            LOG.debug(\n-                \"model_dict is list\")\n-        else:\n-            for k, v in model_dict.items():\n-                LOG.debug(\"key = %s ; value = %s \" % (str(k), str(v)))\n-                if k in model_cls.RW_ATTR:\n-                    LOG.debug(k + \" is there in RW_ATTR\")\n-                    rw_dict[k] = v\n-                    del model_dict[k]\n-\n-        rw_dict = get_unicode_kw(rw_dict)\n-        obj = model_cls(resource_root, **rw_dict)\n-\n-        for attr in model_cls.RO_ATTR:\n-            obj._setattr(attr, None)\n-\n-        for k, v in model_dict.items():\n-            if k in model_cls.RO_ATTR:\n-                obj._setattr(k, v)\n-            else:\n-                LOG.debug(\n-                    \"Unexpected attribute '%s' in %s json\" %\n-                    (k, model_cls.__name__))\n-\n-        for attr in model_cls.REF_ATTR:\n-            LOG.debug(\"%s found as reference var\" % (attr))\n-            obj._setattr(getREF_class_name(attr), None)\n-\n-        for k, v in model_dict.items():\n-            if k in model_cls.REF_ATTR:\n-                obj._setattr(getREF_class_name(k), v)\n-            else:\n-                LOG.debug(\n-                    \"Unknown attribute '%s' found in model_dict for %s \" %\n-                    (k, model_cls.__name__))\n-        return obj\n-\n-\n-# get attribute with REF\n-def getREF_class_name(REF_name):\n-    if REF_name in ref_dic:\n-        return ref_dic[str(REF_name)]\n-    else:\n-        return None\n-\n-\n-def getREF_var_name(REF_name):\n-    if REF_name in ref_class_dic:\n-        return ref_class_dic[str(REF_name)]\n-    else:\n-        return None\n-\n-\n-def get_REF_object(ref_class_name):\n-    \"\"\"\n-    Gets the Ref object based on class_name\n-    \"\"\"\n-    class_ref = getattr(\n-        sys.modules[\n-            ref_pkg_dic[ref_class_name]],\n-        ref_class_name)\n-    LOG.debug(class_ref)\n-    return class_ref\n-\n-\n-def get_unicode(v):\n-    # import unicodedata\n-    if v:\n-        if isinstance(v, unicode):\n-            v = unicodedata.normalize('NFKD', v).encode('ascii', 'ignore')\n-            LOG.debug(v)\n-        elif isinstance(v, str):\n-            LOG.debug(\"warning: string found while expecting unicode %s\" % v)\n-    return v\n-\n-\n-def retain_self_helper(memclass, self=None, **kwargs):\n-    # print locals()\n-        # from ambari_client.model.base_model import  BaseModel\n-    memclass.__init__(self, **kwargs)\n-\n-\n-def get_unicode_kw(dic):\n-    \"\"\"\n-    We use unicode strings as keys in kwargs.\n-    \"\"\"\n-    res = {}\n-    for k, v in dic.iteritems():\n-        res[str(k)] = v\n-    return res\n-\n-\n-def get_config_type(service_name):\n-    \"\"\"\n-    get the config tmp_type based on service_name\n-    \"\"\"\n-    if service_name == \"HDFS\":\n-        tmp_type = \"hdfs-site\"\n-    elif service_name == \"HDFS\":\n-        tmp_type = \"core-site\"\n-    elif service_name == \"MAPREDUCE\":\n-        tmp_type = \"mapred-site\"\n-    elif service_name == \"HBASE\":\n-        tmp_type = \"hbase-site\"\n-    elif service_name == \"OOZIE\":\n-        tmp_type = \"oozie-site\"\n-    elif service_name == \"HIVE\":\n-        tmp_type = \"hive-site\"\n-    elif service_name == \"WEBHCAT\":\n-        tmp_type = \"webhcat-site\"\n-    else:\n-        tmp_type = \"global\"\n-    return tmp_type\n-\n-\n-def get_key_value(dictt, key):\n-    \"\"\"\n-    Search for some random key in the dict\n-    \"\"\"\n-    if isinstance(dictt, dict) and key in dictt:\n-        return dictt[key]\n-    elif isinstance(dictt, dict) and key not in dictt:\n-        # check if values has it?\n-        for v in dictt.values():\n-            if isinstance(v, dict):\n-                return get_key_value(v, key)\n-            elif isinstance(v, list):\n-                for l in list:\n-                    return get_key_value(l, key)",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/utils.py",
                "sha": "65dd153e47692d3c743040bf5bc5587924951f49",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/views.py",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/ambari_client/model/views.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 33,
                "filename": "ambari-client/python-client/src/main/python/ambari_client/model/views.py",
                "patch": "@@ -1,33 +0,0 @@\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-#\n-#      http://www.apache.org/licenses/LICENSE-2.0\n-#\n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-import logging\n-from ambari_client.model.base_model import BaseModel, ModelList\n-from ambari_client.model import paths, utils, status\n-\n-\n-LOG = logging.getLogger(__name__)\n-\n-# TODO\n-\n-\n-def get_views(resource_root, blueprint_name):\n-    pass\n-\n-\n-# TODO\n-class ViewModel(BaseModel):\n-    pass",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/ambari_client/model/views.py",
                "sha": "561cdbd320ff5c033ea517933c88f41d1e56f6ff",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/setup.py",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/main/python/setup.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 39,
                "filename": "ambari-client/python-client/src/main/python/setup.py",
                "patch": "@@ -1,39 +0,0 @@\n-#  Licensed to the Apache Software Foundation (ASF) under one\n-#  or more contributor license agreements.  See the NOTICE file\n-#  distributed with this work for additional information\n-#  regarding copyright ownership.  The ASF licenses this file\n-#  to you under the Apache License, Version 2.0 (the\n-#  \"License\"); you may not use this file except in compliance\n-#  with the License.  You may obtain a copy of the License at\n-# \n-#      http://www.apache.org/licenses/LICENSE-2.0\n-# \n-#  Unless required by applicable law or agreed to in writing, software\n-#  distributed under the License is distributed on an \"AS IS\" BASIS,\n-#  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-#  See the License for the specific language governing permissions and\n-#  limitations under the License.\n-\n-\n-from setuptools import setup, find_packages\n-\n-from sys import version_info, platform\n-\n-if version_info[:2] > (2, 5):\n-    install_requires = []\n-else:\n-    install_requires = ['simplejson >= 2.0.0']\n-\n-# Python 2.6 and below requires argparse\n-if version_info[:2] < (2, 7):\n-    install_requires += ['argparse']\n-\n-setup(\n-  name = 'ambari_client',\n-  author_email = \"ambari-dev@incubator.apache.org\",\n-  version = \"1.0.3-SNAPSHOT\",\n-  packages = ['ambari_client'],\n-  install_requires = install_requires,\n-  description = 'Ambari python REST API client',\n-  license = 'Apache License 2.0'\n-)",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/main/python/setup.py",
                "sha": "24942ad53920adcc1c90637b4153ab334ef2e5d3",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/packages/tarball/all.xml",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/packages/tarball/all.xml?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 34,
                "filename": "ambari-client/python-client/src/packages/tarball/all.xml",
                "patch": "@@ -1,34 +0,0 @@\n-<?xml version=\"1.0\"?>\n-<!--\n-   Licensed to the Apache Software Foundation (ASF) under one or more\n-   contributor license agreements.  See the NOTICE file distributed with\n-   this work for additional information regarding copyright ownership.\n-   The ASF licenses this file to You under the Apache License, Version 2.0\n-   (the \"License\"); you may not use this file except in compliance with\n-   the License.  You may obtain a copy of the License at\n-\n-       http://www.apache.org/licenses/LICENSE-2.0\n-\n-   Unless required by applicable law or agreed to in writing, software\n-   distributed under the License is distributed on an \"AS IS\" BASIS,\n-   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-   See the License for the specific language governing permissions and\n-   limitations under the License.\n--->\n-<assembly xmlns=\"http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.1\"\n-          xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n-          xsi:schemaLocation=\"http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.1 http://maven.apache.org/xsd/assembly-1.1.1.xsd\">\n-  <!--This 'all' id is not appended to the produced bundle because we do this:\n-    http://maven.apache.org/plugins/maven-assembly-plugin/faq.html#required-classifiers\n-  -->\n-  <formats>\n-    <format>dir</format>\n-  </formats>\n-  <includeBaseDirectory>false</includeBaseDirectory>\n-  <fileSets>\n-    <fileSet>\n-      <directory>src/main/python</directory>\n-      <outputDirectory>/</outputDirectory>\n-    </fileSet>\n-  </fileSets>\n-</assembly>",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/packages/tarball/all.xml",
                "sha": "0e4f34b5cefae6679455aa6d16ea5a94e29dbb61",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/TestAmbariClient.py",
                "changes": 212,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/TestAmbariClient.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 212,
                "filename": "ambari-client/python-client/src/test/python/TestAmbariClient.py",
                "patch": "@@ -1,212 +0,0 @@\n-#!/usr/bin/env python\n-\n-'''\n-Licensed to the Apache Software Foundation (ASF) under one\n-or more contributor license agreements.  See the NOTICE file\n-distributed with this work for additional information\n-regarding copyright ownership.  The ASF licenses this file\n-to you under the Apache License, Version 2.0 (the\n-\"License\"); you may not use this file except in compliance\n-with the License.  You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-'''\n-\n-\n-from mock.mock import MagicMock, patch\n-from ambari_client.ambari_api import  AmbariClient\n-from HttpClientInvoker import HttpClientInvoker\n-from ambari_client.model.stack import StackConfigModel, StackComponentModel\n-import unittest\n-import logging\n-\n-class TestAmbariClient(unittest.TestCase):\n-\n-  def setUp(self):\n-    http_client_logger = logging.getLogger()\n-    http_client_logger.info('Running test:' + self.id())\n-\n-  def create_client(self, http_client_mock = MagicMock()):\n-    http_client_mock.invoke.side_effect = HttpClientInvoker.http_client_invoke_side_effects\n-    client = AmbariClient(\"localhost\", 8080, \"admin\", \"admin\", version=1, client=http_client_mock)\n-    return client\n-\n-  def test_init(self):\n-    \"\"\"\n-    AmbariClient is the top-level root resources.\n-    This testcase checks if when the  init method was called &\n-    the httpclient was initialized\n-    \"\"\"\n-    client = AmbariClient(\"localhost\", 8080, \"admin\", \"admin\", version=1)\n-    self.assertEqual(client.version, 1, \"version should be 1\")\n-    self.assertEqual(client.host_url, \"http://localhost:8080/api/v1\",\n-                       \"host_url should be http://localhost:8080/api/v1\")\n-\n-    client = AmbariClient(host_name=\"localhost\", user_name=\"admin\", password=\"admin\")\n-    self.assertEqual(client.version, 1, \"version should be 1\")\n-    self.assertEqual(client.host_url, \"http://localhost:8080/api/v1\",\n-                       \"host_url should be http://localhost:8080/api/v1\")\n-\n-    client = AmbariClient(host_name=\"localhost\")\n-    self.assertEqual(client.version, 1, \"version should be 1\")\n-    self.assertEqual(client.host_url, \"http://localhost:8080/api/v1\",\n-                       \"host_url should be http://localhost:8080/api/v1\")\n-\n-\n-    client = AmbariClient(\"localhost\", 8443, \"admin\", \"admin\", use_https=True)\n-    self.assertEqual(client.version, 1, \"version should be 1\")\n-    self.assertEqual(client.host_url, \"https://localhost:8443/api/v1\",\n-                       \"host_url should be https://localhost:8443/api/v1\")\n-\n-  def test_get_all_clusters(self):\n-    \"\"\"\n-    Get all clusters.\n-    This testcase checks if get_all_clusters returns a list of ModelList.\n-    \"\"\"\n-    expected_output = {'items': [{'cluster_name': u'test1', 'version': u'HDP-1.2.1'}]}\n-\n-    client = self.create_client()\n-    all_clusters = client.get_all_clusters()\n-\n-    self.assertEqual(len(all_clusters), 1)\n-    self.assertEqual(all_clusters.to_json_dict(), expected_output)\n-\n-  def test_get_cluster(self):\n-    \"\"\"\n-    Get all clusters.\n-    This testcase checks if get_all_clusters returns a list of ModelList.\n-    \"\"\"\n-    expected_dict_output = {'cluster_name': u'test1', 'version': u'HDP-1.2.1'}\n-\n-    client = self.create_client()\n-    cluster = client.get_cluster('test1')\n-\n-    self.assertEqual(cluster.cluster_name, \"test1\", \"cluster_name should be test1 \")\n-    self.assertEqual(cluster.to_json_dict(), expected_dict_output, \"to_json_dict should convert ClusterModel\")\n-\n-  def test_get_host(self):\n-    \"\"\"\n-    Get host\n-    This testcase checks if client.get_host returns a correct host\n-    \"\"\"\n-    expected_dict_output = {'ip': '10.0.2.15', 'host_name': 'dev06.hortonworks.com', 'rack_info': '/default-rack'}\n-\n-    client = self.create_client()\n-    host = client.get_host('dev06.hortonworks.com')\n-\n-    self.assertEqual(host.to_json_dict(), expected_dict_output)\n-    self.assertEqual(host.host_state, \"HEARTBEAT_LOST\")\n-\n-  def test_get_all_hosts(self):\n-    \"\"\"\n-    Get all hosts.\n-    This testcase checks if get_all_hosts returns a list of ModelList.\n-    \"\"\"\n-    expected_hosts_dict = {'items': [{'ip': None, 'host_name': u'apspal44-83', 'rack_info': '/default-rack'}, {'ip': None, 'host_name': u'apspal44-84', 'rack_info': '/default-rack'}, {'ip': None, 'host_name': u'apspal44-85', 'rack_info': '/default-rack'}, {'ip': None, 'host_name': u'apspal44-86', 'rack_info': '/default-rack'}, {'ip': None, 'host_name': u'apspal44-87', 'rack_info': '/default-rack'}, {'ip': None, 'host_name': u'apspal44-88', 'rack_info': '/default-rack'}, {'ip': None, 'host_name': u'apspal44-89', 'rack_info': '/default-rack'}, {'ip': None, 'host_name': u'r01hn01', 'rack_info': '/default-rack'}, {'ip': None, 'host_name': u'r01mgt', 'rack_info': '/default-rack'}, {'ip': None, 'host_name': u'r01wn01', 'rack_info': '/default-rack'}, {'ip': None, 'host_name': u'r01wn02', 'rack_info': '/default-rack'}, {'ip': None, 'host_name': u'r01wn03', 'rack_info': '/default-rack'}]}\n-\n-    client = self.create_client()\n-    all_hosts = client.get_all_hosts()\n-\n-    self.assertEqual(len(all_hosts), 12, \"There should be 12 hosts from the response\")\n-    self.assertEqual(all_hosts.to_json_dict(), expected_hosts_dict)\n-\n-  def ADisabledtest_bootstrap_hosts(self):\n-    \"\"\"\n-    Test Bootstrap\n-    \"\"\"\n-    http_client_mock = MagicMock()\n-\n-    ssh_key = 'abc!@#$%^&*()_:\"|<>?[];\\'\\\\./'\n-    host_list = ['dev05.hortonworks.com', 'dev06.hortonworks.com']\n-    ssh_user = 'root'\n-\n-    expected_path = '//bootstrap'\n-    expected_headers = {'Content-Type': 'application/json'}\n-    expected_request = {'user': ssh_user, 'hosts': str(host_list), 'verbose': True, 'sshKey': ssh_key}\n-    expected_response = {'status': 201, 'message': u'Running Bootstrap now.', 'requestId': 5}\n-\n-    client = self.create_client(http_client_mock)\n-    resp = client.bootstrap_hosts(host_list, ssh_key, ssh_user)\n-\n-    self.assertEqual(resp.to_json_dict(), expected_response)\n-    http_client_mock.invoke.assert_called_with('POST', expected_path, headers=expected_headers, payload=expected_request)\n-\n-  def test_create_cluster(self):\n-    \"\"\"\n-    Test create cluster\n-    \"\"\"\n-    http_client_mock = MagicMock()\n-\n-    expected_path = '//clusters/c1'\n-    expected_request = {'Clusters': {'version': 'HDP-2.0.5'}}\n-\n-    client = self.create_client(http_client_mock)\n-    resp = client.create_cluster('c1', 'HDP-2.0.5')\n-\n-    http_client_mock.invoke.assert_called_with('POST', expected_path, headers=None, payload=expected_request)\n-\n-  def test_delete_cluster(self):\n-    \"\"\"\n-    Test create cluster\n-    \"\"\"\n-    http_client_mock = MagicMock()\n-\n-    expected_path = '//clusters/c1'\n-    expected_request = None\n-\n-    client = self.create_client(http_client_mock)\n-    resp = client.delete_cluster('c1')\n-\n-    http_client_mock.invoke.assert_called_with('DELETE', expected_path, headers=None, payload=expected_request)\n-\n-  def test_delete_host(self):\n-    \"\"\"\n-    Test delete host\n-    \"\"\"\n-    http_client_mock = MagicMock()\n-\n-    expected_path = '//hosts/abc.abc.abc'\n-    expected_request = None\n-\n-    client = self.create_client(http_client_mock)\n-    resp = client.delete_host('abc.abc.abc')\n-\n-    http_client_mock.invoke.assert_called_with('DELETE', expected_path, headers=None, payload=expected_request)\n-\n-  def test_get_config(self):\n-    \"\"\"\n-    Test get config\n-    \"\"\"\n-    expected_dict = {'items': [{'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'datanode_du_reserved', 'property_value': u'1'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.access.time.precision', 'property_value': u'0'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.balance.bandwidthPerSec', 'property_value': u'6250000'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.block.access.token.enable', 'property_value': u'true'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.block.size', 'property_value': u'134217728'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.blockreport.initialDelay', 'property_value': u'120'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.cluster.administrators', 'property_value': u' hdfs'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.datanode.du.pct', 'property_value': u'0.85f'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.datanode.failed.volumes.tolerated', 'property_value': u'0'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.datanode.ipc.address', 'property_value': u'0.0.0.0:8010'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.datanode.max.xcievers', 'property_value': u'4096'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.datanode.socket.write.timeout', 'property_value': u'0'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.heartbeat.interval', 'property_value': u'3'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.https.port', 'property_value': u'50470'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.namenode.avoid.read.stale.datanode', 'property_value': u'true'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.namenode.avoid.write.stale.datanode', 'property_value': u'true'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.namenode.handler.count', 'property_value': u'100'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.namenode.handler.count', 'property_value': u'40'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.namenode.stale.datanode.interval', 'property_value': u'30000'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.namenode.write.stale.datanode.ratio', 'property_value': u'1.0f'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.permissions', 'property_value': u'true'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.permissions.supergroup', 'property_value': u'hdfs'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.replication.max', 'property_value': u'50'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.safemode.threshold.pct', 'property_value': u'1.0f'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.secondary.https.port', 'property_value': u'50490'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.umaskmode', 'property_value': u'077'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs.web.ugi', 'property_value': u'gopher,gopher'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs_block_local_path_access_user', 'property_value': u'hbase'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs_data_dir', 'property_value': u'/hadoop/hdfs/data'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs_datanode_address', 'property_value': u'50010'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs_datanode_data_dir_perm', 'property_value': u'750'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs_datanode_failed_volume_tolerated', 'property_value': u'0'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs_datanode_http_address', 'property_value': u'50075'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs_name_dir', 'property_value': u'/hadoop/hdfs/namenode'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs_replication', 'property_value': u'3'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dfs_webhdfs_enabled', 'property_value': u'true'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'dtnode_heapsize', 'property_value': u'1024'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'fs.checkpoint.edits.dir', 'property_value': u'${fs.checkpoint.dir}'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'fs.checkpoint.period', 'property_value': u'21600'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'fs.checkpoint.size', 'property_value': u'536870912'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'fs.trash.interval', 'property_value': u'360'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'fs_checkpoint_dir', 'property_value': u'/hadoop/hdfs/namesecondary'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'fs_checkpoint_period', 'property_value': u'21600'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'fs_checkpoint_size', 'property_value': u'0.5'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'hadoop.security.authentication', 'property_value': u'simple'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'hadoop_heapsize', 'property_value': u'1024'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'hadoop_pid_dir_prefix', 'property_value': u'/var/run/hadoop'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'hdfs_log_dir_prefix', 'property_value': u'/var/log/hadoop'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'hdfs_user', 'property_value': u'hdfs'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'io.compression.codec.lzo.class', 'property_value': u'com.hadoop.compression.lzo.LzoCodec'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'io.compression.codecs', 'property_value': u'org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.SnappyCodec'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'io.file.buffer.size', 'property_value': u'131072'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'io.serializations', 'property_value': u'org.apache.hadoop.io.serializer.WritableSerialization'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'ipc.client.connect.max.retries', 'property_value': u'50'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'ipc.client.connection.maxidletime', 'property_value': u'30000'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'ipc.client.idlethreshold', 'property_value': u'8000'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'ipc.server.max.response.size', 'property_value': u'5242880'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'ipc.server.read.threadpool.size', 'property_value': u'5'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'kerberos_domain', 'property_value': u'EXAMPLE.COM'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'keytab_path', 'property_value': u'/etc/security/keytabs'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'namenode_formatted_mark_dir', 'property_value': u'/var/run/hadoop/hdfs/namenode/formatted/'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'namenode_heapsize', 'property_value': u'1024'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'namenode_opt_maxnewsize', 'property_value': u'640'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'namenode_opt_newsize', 'property_value': u'200'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'namenode_opt_maxpermsize', 'property_value': u'256'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'namenode_opt_permsize', 'property_value': u'128'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'proxyuser_group', 'property_value': u'users'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'security.client.datanode.protocol.acl', 'property_value': u'*'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'security.client.protocol.acl', 'property_value': u'*'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'security.datanode.protocol.acl', 'property_value': u'*'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'security.inter.datanode.protocol.acl', 'property_value': u'*'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'security.inter.tracker.protocol.acl', 'property_value': u'*'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'security.job.submission.protocol.acl', 'property_value': u'*'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'security.namenode.protocol.acl', 'property_value': u'*'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'security.task.umbilical.protocol.acl', 'property_value': u'*'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'security_enabled', 'property_value': u'false'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'property_name': u'webinterface.private.actions', 'property_value': u'false'}]}\n-    expected_first_item = StackConfigModel(None, property_name='datanode_du_reserved' , property_value='1' , service_name='HDFS' , stack_version='1.3.0')\n-    expected_request = None\n-\n-    client = self.create_client()\n-    configs = client.get_config('1.3.0','HDFS')\n-\n-\n-    self.assertEquals(len(configs), 77)\n-    self.assertEquals(str(configs[0]),str(expected_first_item))\n-    self.assertEquals(configs.to_json_dict(), expected_dict)\n-\n-  def test_get_components(self):\n-    \"\"\"\n-    Test get components\n-    \"\"\"\n-    expected_dict = {'items': [{'stack_version': u'1.3.0', 'service_name': u'HDFS', 'component_name': u'DATANODE'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'component_name': u'HDFS_CLIENT'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'component_name': u'NAMENODE'}, {'stack_version': u'1.3.0', 'service_name': u'HDFS', 'component_name': u'SECONDARY_NAMENODE'}]}\n-    expected_first_item = StackComponentModel(None, component_name='DATANODE', service_name='HDFS' , stack_version='1.3.0')\n-    expected_request = None\n-\n-    client = self.create_client()\n-    components = client.get_components('1.3.0','HDFS')\n-\n-    self.assertEquals(len(components), 4)\n-    self.assertEquals(str(components[0]),str(expected_first_item))\n-    self.assertEquals(components.to_json_dict(), expected_dict)\n-",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/TestAmbariClient.py",
                "sha": "f96d6b0503bc927d7e3d6aadbeee52a67c2aa230",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/TestClusterModel.py",
                "changes": 428,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/TestClusterModel.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 428,
                "filename": "ambari-client/python-client/src/test/python/TestClusterModel.py",
                "patch": "@@ -1,428 +0,0 @@\n-#!/usr/bin/env python\n-\n-'''\n-Licensed to the Apache Software Foundation (ASF) under one\n-or more contributor license agreements.  See the NOTICE file\n-distributed with this work for additional information\n-regarding copyright ownership.  The ASF licenses this file\n-to you under the Apache License, Version 2.0 (the\n-\"License\"); you may not use this file except in compliance\n-with the License.  You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-'''\n-import logging\n-\n-from mock.mock import MagicMock, patch\n-from HttpClientInvoker import HttpClientInvoker\n-\n-from ambari_client.ambari_api import  AmbariClient\n-from ambari_client.model.host import HostModel\n-from ambari_client.core.errors import BadRequest\n-\n-import unittest\n-\n-class TestClusterModel(unittest.TestCase):\n-\n-  def setUp(self):\n-    http_client_logger = logging.getLogger()\n-    http_client_logger.info('Running test:' + self.id())\n-\n-  def create_cluster(self, http_client_mock = MagicMock()):    \n-    http_client_mock.invoke.side_effect = HttpClientInvoker.http_client_invoke_side_effects\n-    client = AmbariClient(\"localhost\", 8080, \"admin\", \"admin\", version=1, client=http_client_mock)\n-    return client.get_cluster('test1')\n-   \n-  def test_get_service(self):\n-    \"\"\"\n-    Get the service of a cluster\n-    This testcase checks if get_service returns a list of ServiceModel.\n-    \"\"\"   \n-    expected_dict_output = {'cluster_name': 'test1', 'version': 'HDP-1.2.1'}\n-    \n-    cluster = self.create_cluster()\n-    serviceList = cluster.get_all_services()\n-    ganglia = cluster.get_service(\"GANGLIA\")  \n-\n-    self.assertEqual(cluster.cluster_name, \"test1\", \"cluster_name should be test1 \")\n-    self.assertEqual(cluster.to_json_dict(), expected_dict_output, \"to_json_dict should convert ClusterModel\")\n-    self.assertEqual(len(serviceList), 3, \"There should be a 3 services from the response\")\n-    self.assertEqual(str(ganglia.state), \"STARTED\", \"The ganglia service state should be fetched as STARTED\")\n-    self.assertEqual(ganglia.clusterRef.cluster_name, cluster.cluster_name, \"The clusterRef value for  service  should be fetched \")\n-      \n-  def test_get_all_services(self):\n-    \"\"\"\n-    Get all services of a cluster.\n-    This testcase checks if get_all_services returns a list of ModelList.\n-    \"\"\"\n-    expected_dict_output = {'cluster_name': 'test1', 'version': 'HDP-1.2.1'}\n-    \n-    cluster = self.create_cluster()\n-    serviceList = cluster.get_all_services()\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\", \"cluster_name should be test1 \")\n-    self.assertEqual(cluster.to_json_dict(), expected_dict_output, \"to_json_dict should convert ClusterModel\")\n-    self.assertEqual(len(serviceList), 3, \"There should be a 3 services from the response\")\n-\n-  def test_get_all_hosts(self):\n-    \"\"\"\n-    Get all cluster hosts\n-    This testcase checks if get_all_services returns a list of ModelList.\n-    \"\"\"\n-    expected_dict_output = {'items': [{'ip': '10.0.2.15', 'host_name': 'dev05.hortonworks.com', 'rack_info': '/default-rack'}, {'ip': '10.0.2.15', 'host_name': 'dev06.hortonworks.com', 'rack_info': '/default-rack'}]}\n-\n-    cluster = self.create_cluster()\n-    hostlist = cluster.get_all_hosts()\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    self.assertEqual(hostlist.to_json_dict(), expected_dict_output)\n-    self.assertEqual(hostlist[1].host_name, 'dev06.hortonworks.com')\n-    self.assertEqual(len(hostlist), 2)  \n-\n-  def test_get_host(self):\n-    \"\"\"\n-    Get cluster host\n-    This testcase checks if get_host returns correct HostModel\n-    \"\"\"\n-    expected_dict_output = {'ip': '10.104.44.95', 'host_name': 'myhost', 'rack_info': '/default-rack'}\n-    \n-    cluster = self.create_cluster()\n-    host = cluster.get_host('myhost')\n-    \n-    self.assertEqual(host.clusterRef.cluster_name, \"test1\")\n-    self.assertEqual(host.to_json_dict(), expected_dict_output)\n-    self.assertEqual(host.host_state, \"HEALTHY\")\n-    self.assertEqual(host.public_host_name, \"myhost\")\n-     \n-  def test_get_global_config(self):\n-    \"\"\"\n-    Get global config\n-    This testcase checks if get_host returns correct HostModel\n-    \"\"\"\n-    expected_dict_output = {'tag': 'version1', 'type': 'global'}\n-    expected_properties = {'dfs_namenode_name_dir': '/hadoop/hdfs/namenode', 'security_enabled': 'false', 'proxyuser_group': 'users', 'hdfs_log_dir_prefix': '/var/log/hadoop', 'dfs_datanode_data_dir': '/hadoop/hdfs/data', 'namenode_formatted_mark_dir': '/var/run/hadoop/hdfs/namenode/formatted/', 'rrdcached_base_dir': '/var/lib/ganglia/rrds', 'user_group': 'hadoop', 'dfs_namenode_checkpoint_dir': '/hadoop/hdfs/namesecondary', 'dfs_namenode_checkpoint_period': '21600', 'hive_user': 'hive', 'fs_checkpoint_size': '0.5', 'hbase_conf_dir': '/etc/hbase', 'datanode_du_reserved': '1', 'dfs_datanode_http_address': '50075', 'namenode_heapsize': '1024m', 'dfs_webhdfs_enabled': 'true', 'oozie_user': 'oozie', 'hcat_conf_dir': '', 'hadoop_conf_dir': '/etc/hadoop/conf', 'dfs_replication': '3', 'namenode_opt_maxnewsize': '640m', 'apache_artifacts_download_url': '', 'dfs_datanode_address': '50010', 'dfs_exclude': 'dfs.exclude', 'yarn_user': 'yarn', 'zk_user': 'zookeeper', 'smokeuser': 'ambari-qa', 'dtnode_heapsize': '1024m', 'gmond_user': 'nobody', 'dfs_datanode_failed_volume_tolerated': '0', 'java64_home': '/usr/jdk/jdk1.6.0_31', 'run_dir': '/var/run/hadoop', 'ganglia_runtime_dir': '/var/run/ganglia/hdp', 'dfs_datanode_data_dir_perm': '750', 'hdfs_enable_shortcircuit_read': 'true', 'hdfs_user': 'hdfs', 'hbase_user': 'hbase', 'webhcat_user': 'hcat', 'gmetad_user': 'nobody', 'dfs_block_local_path_access_user': 'hbase', 'namenode_opt_newsize': '200m', 'namenode_opt_maxpermsize': '256m', 'namenode_opt_permsize': '128m', 'mapred_user': 'mapred', 'nagios_group': 'nagios', 'hcat_user': 'hcat', 'hadoop_heapsize': '1024', 'hadoop_pid_dir_prefix': '/var/run/hadoop', 'nagios_user': 'nagios'}\n-    \n-    cluster = self.create_cluster()\n-    global_config = cluster.get_global_config()\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    self.assertEqual(global_config.properties, expected_properties)\n-    self.assertEqual(global_config.to_json_dict(), expected_dict_output)\n-    \n-  def test_get_core_site_config(self):\n-    \"\"\"\n-    Get core-site config\n-    \"\"\"\n-    expected_dict_output = {'tag': 'version1', 'type': 'core-site'}\n-    expected_properties = {'io.serializations': 'org.apache.hadoop.io.serializer.WritableSerialization', 'fs.checkpoint.size': '0.5', 'fs.trash.interval': '360', 'hadoop.security.authentication': 'simple', 'io.compression.codecs': 'org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec', 'mapreduce.jobtracker.webinterface.trusted': 'false', 'hadoop.security.authorization': 'false', 'fs.checkpoint.edits.dir': '/hadoop/hdfs/namesecondary', 'ipc.client.connection.maxidletime': '30000', 'ipc.client.connect.max.retries': '50', 'hadoop.security.auth_to_local': '\\n        RULE:[2:$1@$0]([rn]m@.*)s/.*/yarn/\\n        RULE:[2:$1@$0](jhs@.*)s/.*/mapred/\\n        RULE:[2:$1@$0]([nd]n@.*)s/.*/hdfs/\\n        RULE:[2:$1@$0](hm@.*)s/.*/hbase/\\n        RULE:[2:$1@$0](rs@.*)s/.*/hbase/\\n        DEFAULT\\n    ', 'io.file.buffer.size': '131072', 'dfs.namenode.checkpoint.dir': '/hadoop/hdfs/namesecondary', 'ipc.client.idlethreshold': '8000', 'dfs.namenode.checkpoint.edits.dir': '${dfs.namenode.checkpoint.dir}', 'fs.defaultFS': 'hdfs://dev05.hortonworks.com:8020', 'dfs.namenode.checkpoint.period': '21600'}\n-        \n-    cluster = self.create_cluster()\n-    global_config = cluster.get_core_site_config()\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    self.assertEqual(global_config.properties, expected_properties)\n-    self.assertEqual(global_config.to_json_dict(), expected_dict_output)\n-    \n-  def test_get_hdfs_site_config(self):\n-    \"\"\"\n-    Get hdfs config\n-    \"\"\"\n-    expected_dict_output = {'tag': 'version1', 'type': 'hdfs-site'}\n-    expected_properties = {'dfs.namenode.avoid.write.stale.datanode': 'true', 'dfs.webhdfs.enabled': 'true', 'dfs.block.access.token.enable': 'true', 'dfs.datanode.address': '0.0.0.0:50010', 'dfs.cluster.administrators': ' hdfs', 'dfs.datanode.balance.bandwidthPerSec': '6250000', 'dfs.namenode.safemode.threshold-pct': '1.0f', 'dfs.permissions.enabled': 'true', 'dfs.client.read.shortcircuit': 'true', 'dfs.journalnode.edits.dir': '/grid/0/hdfs/journal', 'dfs.blocksize': '134217728', 'dfs.datanode.max.transfer.threads': '1024', 'dfs.datanode.du.reserved': '1', 'dfs.replication': '3', 'dfs.namenode.handler.count': '100', 'fs.permissions.umask-mode': '022', 'dfs.datanode.http.address': '0.0.0.0:50075', 'dfs.datanode.ipc.address': '0.0.0.0:8010', 'dfs.datanode.data.dir': '/hadoop/hdfs/data', 'dfs.namenode.http-address': 'dev05.hortonworks.com:50070', 'dfs.blockreport.initialDelay': '120', 'dfs.datanode.failed.volumes.tolerated': '0', 'dfs.namenode.accesstime.precision': '0', 'dfs.block.local-path-access.user': 'hbase', 'dfs.https.namenode.https-address': 'dev05.hortonworks.com:50470', 'dfs.namenode.secondary.http-address': 'dev05.hortonworks.com:50090', 'dfs.namenode.stale.datanode.interval': '30000', 'dfs.heartbeat.interval': '3', 'dfs.client.read.shortcircuit.streams.cache.size': '4096', 'dfs.permissions.superusergroup': 'hdfs', 'dfs.journalnode.http-address': '0.0.0.0:8480', 'dfs.domain.socket.path': '/var/lib/hadoop-hdfs/dn_socket', 'dfs.namenode.avoid.read.stale.datanode': 'true', 'dfs.hosts.exclude': '/etc/hadoop/conf/dfs.exclude', 'dfs.datanode.data.dir.perm': '750', 'dfs.namenode.write.stale.datanode.ratio': '1.0f', 'dfs.replication.max': '50', 'dfs.namenode.name.dir': '/hadoop/hdfs/namenode'}\n-        \n-    cluster = self.create_cluster()\n-    global_config = cluster.get_hdfs_site_config()\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    self.assertEqual(global_config.properties, expected_properties)\n-    self.assertEqual(global_config.to_json_dict(), expected_dict_output)\n-    \n-  def test_get_mapred_site_config(self):\n-    \"\"\"\n-    Get mapred config\n-    \"\"\"\n-    expected_dict_output = {'tag': 'version1', 'type': 'mapred-site'}\n-    expected_properties = {'mapreduce.jobhistory.address': 'dev05.hortonworks.com:10020', 'mapreduce.reduce.input.buffer.percent': '0.0', 'mapred.jobtracker.maxtasks.per.job': '-1', 'mapreduce.framework.name': 'yarn', 'mapreduce.map.speculative': 'false', 'mapreduce.tasktracker.healthchecker.script.path': 'file:////mapred/jobstatus', 'mapreduce.reduce.shuffle.merge.percent': '0.66', 'mapred.userlog.retain.hours': '24', 'yarn.app.mapreduce.am.resource.mb': '1024', 'mapreduce.reduce.shuffle.parallelcopies': '30', 'mapreduce.map.java.opts': '-Xmx320m', 'mapreduce.task.io.sort.factor': '100', 'mapreduce.application.classpath': '$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*', 'yarn.app.mapreduce.am.command-opts': '-Xmx756m', 'mapreduce.job.reduce.slowstart.completedmaps': '0.05', 'mapreduce.output.fileoutputformat.compress.type': 'BLOCK', 'mapreduce.reduce.speculative': 'false', 'mapreduce.reduce.java.opts': '-Xmx756m', 'mapreduce.am.max-attempts': '2', 'yarn.app.mapreduce.am.admin-command-opts': '-Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN', 'mapreduce.jobtracker.system.dir': '/mapred/system', 'mapreduce.map.sort.spill.percent': '0.1', 'mapreduce.task.timeout': '600000', 'mapreduce.map.memory.mb': '1536', 'mapreduce.reduce.log.level': 'INFO', 'mapreduce.jobhistory.intermediate-done-dir': '/mr-history/tmp', 'mapreduce.reduce.memory.mb': '2048', 'mapreduce.tasktracker.map.tasks.maximum': '4', 'yarn.app.mapreduce.am.log.level': 'INFO', 'mapreduce.map.log.level': 'INFO', 'mapreduce.shuffle.port': '13562', 'mapred.jobtracker.taskScheduler': 'org.apache.hadoop.mapred.CapacityTaskScheduler', 'mapreduce.admin.user.env': 'LD_LIBRARY_PATH=/usr/lib/hadoop/lib/native:/usr/lib/hadoop/lib/native/`$JAVA_HOME/bin/java -d32 -version &amp;&gt; /dev/null;if [ $? -eq 0 ]; then echo Linux-i386-32; else echo Linux-amd64-64;fi`', 'mapreduce.jobhistory.webapp.address': 'dev05.hortonworks.com:19888', 'mapred.hosts.exclude': '/etc/hadoop/conf/mapred.exclude', 'mapreduce.reduce.shuffle.input.buffer.percent': '0.7', 'yarn.app.mapreduce.am.staging-dir': '/user', 'mapred.hosts': '/etc/hadoop/conf/mapred.include', 'mapreduce.jobhistory.done-dir': '/mr-history/done', 'mapreduce.admin.reduce.child.java.opts': '-Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN', 'mapreduce.task.io.sort.mb': '200', 'mapred.task.tracker.task-controller': 'org.apache.hadoop.mapred.DefaultTaskController', 'mapreduce.admin.map.child.java.opts': '-Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN'}\n-        \n-    cluster = self.create_cluster()\n-    global_config = cluster.get_mapred_site_config()\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    self.assertEqual(global_config.properties, expected_properties)\n-    self.assertEqual(global_config.to_json_dict(), expected_dict_output)\n-    \n-  def test_update_global_config(self):\n-    \"\"\"\n-    Update global config\n-    \"\"\"\n-    http_client_mock = MagicMock()\n-    \n-    expected_properties = {'dfs_namenode_name_dir': 'abc', 'security_enabled': 'false', 'proxyuser_group': 'users', 'hdfs_log_dir_prefix': '/var/log/hadoop', 'dfs_datanode_data_dir': '/hadoop/hdfs/data', 'namenode_formatted_mark_dir': '/var/run/hadoop/hdfs/namenode/formatted/', 'rrdcached_base_dir': '/var/lib/ganglia/rrds', 'user_group': 'hadoop', 'dfs_namenode_checkpoint_dir': '/hadoop/hdfs/namesecondary', 'dfs_namenode_checkpoint_period': '21600', 'hive_user': 'hive', 'fs_checkpoint_size': '0.5', 'hbase_conf_dir': '/etc/hbase', 'datanode_du_reserved': '1', 'dfs_datanode_http_address': '50075', 'namenode_heapsize': '1024m', 'dfs_webhdfs_enabled': 'true', 'oozie_user': 'oozie', 'hcat_conf_dir': '', 'hadoop_conf_dir': '/etc/hadoop/conf', 'dfs_replication': '3', 'namenode_opt_maxnewsize': '640m', 'apache_artifacts_download_url': '', 'dfs_datanode_address': '50010', 'dfs_exclude': 'dfs.exclude', 'yarn_user': 'yarn', 'zk_user': 'zookeeper', 'smokeuser': 'ambari-qa', 'dtnode_heapsize': '1024m', 'gmond_user': 'nobody', 'dfs_datanode_failed_volume_tolerated': '0', 'java64_home': '/usr/jdk/jdk1.6.0_31', 'run_dir': '/var/run/hadoop', 'ganglia_runtime_dir': '/var/run/ganglia/hdp', 'dfs_datanode_data_dir_perm': '750', 'hdfs_enable_shortcircuit_read': 'true', 'hdfs_user': 'hdfs', 'hbase_user': 'hbase', 'webhcat_user': 'hcat', 'gmetad_user': 'nobody', 'dfs_block_local_path_access_user': 'hbase', 'namenode_opt_newsize': '200m', 'namenode_opt_maxpermsize': '256m', 'namenode_opt_permsize': '128m', 'mapred_user': 'mapred', 'nagios_group': 'nagios', 'hcat_user': 'hcat', 'hadoop_heapsize': '1024', 'hadoop_pid_dir_prefix': '/var/run/hadoop', 'nagios_user': 'nagios'}\n-    expected_put_path = '//clusters/test1'\n-    expected_post_request = {'Clusters': {'desired_configs': {'tag': 'version1', 'type': 'global', 'properties':expected_properties}}}   \n-    expected_get_path = '//clusters/test1/configurations?type=global&tag=version1'\n-    expected_get_request = None\n-        \n-    cluster = self.create_cluster(http_client_mock)\n-    existant_global_config = cluster.get_global_config()\n-    existant_global_config.properties['dfs_namenode_name_dir'] = 'abc'\n-    cluster.update_global_config(existant_global_config)\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    http_client_mock.invoke.assert_any_call('PUT', expected_put_path, headers=None, payload=expected_post_request)\n-    http_client_mock.invoke.assert_any_call('GET', expected_get_path, headers=None, payload=expected_get_request)\n- \n-  def test_update_core_site_config(self):\n-    \"\"\"\n-    Update core-site config\n-    \"\"\"\n-    http_client_mock = MagicMock()\n-    \n-    expected_properties = {'io.serializations': 'abc', 'fs.checkpoint.size': '0.5', 'fs.trash.interval': '360', 'hadoop.security.authentication': 'simple', 'io.compression.codecs': 'org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec', 'mapreduce.jobtracker.webinterface.trusted': 'false', 'hadoop.security.authorization': 'false', 'fs.checkpoint.edits.dir': '/hadoop/hdfs/namesecondary', 'ipc.client.connection.maxidletime': '30000', 'ipc.client.connect.max.retries': '50', 'hadoop.security.auth_to_local': '\\n        RULE:[2:$1@$0]([rn]m@.*)s/.*/yarn/\\n        RULE:[2:$1@$0](jhs@.*)s/.*/mapred/\\n        RULE:[2:$1@$0]([nd]n@.*)s/.*/hdfs/\\n        RULE:[2:$1@$0](hm@.*)s/.*/hbase/\\n        RULE:[2:$1@$0](rs@.*)s/.*/hbase/\\n        DEFAULT\\n    ', 'io.file.buffer.size': '131072', 'dfs.namenode.checkpoint.dir': '/hadoop/hdfs/namesecondary', 'ipc.client.idlethreshold': '8000', 'dfs.namenode.checkpoint.edits.dir': '${dfs.namenode.checkpoint.dir}', 'fs.defaultFS': 'hdfs://dev05.hortonworks.com:8020', 'dfs.namenode.checkpoint.period': '21600'}\n-    expected_put_path = '//clusters/test1'\n-    expected_post_request = {'Clusters': {'desired_configs': {'tag': 'version1', 'type': 'core-site', 'properties':expected_properties}}}   \n-    expected_get_path = '//clusters/test1/configurations?type=core-site&tag=version1'\n-    expected_get_request = None\n-        \n-    cluster = self.create_cluster(http_client_mock)\n-    existant_global_config = cluster.get_core_site_config()\n-    existant_global_config.properties['io.serializations'] = 'abc'\n-    cluster.update_core_site_config(existant_global_config)\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    http_client_mock.invoke.assert_any_call('PUT', expected_put_path, headers=None, payload=expected_post_request)\n-    http_client_mock.invoke.assert_any_call('GET', expected_get_path, headers=None, payload=expected_get_request)\n-       \n-  def test_update_hdfs_site_config(self):\n-    \"\"\"\n-    Update hdfs-site config\n-    \"\"\"\n-    http_client_mock = MagicMock()\n-    \n-    expected_properties = {'dfs.namenode.avoid.write.stale.datanode': 'abc', 'dfs.webhdfs.enabled': 'true', 'dfs.block.access.token.enable': 'true', 'dfs.datanode.address': '0.0.0.0:50010', 'dfs.cluster.administrators': ' hdfs', 'dfs.datanode.balance.bandwidthPerSec': '6250000', 'dfs.namenode.safemode.threshold-pct': '1.0f', 'dfs.permissions.enabled': 'true', 'dfs.client.read.shortcircuit': 'true', 'dfs.journalnode.edits.dir': '/grid/0/hdfs/journal', 'dfs.blocksize': '134217728', 'dfs.datanode.max.transfer.threads': '1024', 'dfs.datanode.du.reserved': '1', 'dfs.replication': '3', 'dfs.namenode.handler.count': '100', 'fs.permissions.umask-mode': '022', 'dfs.datanode.http.address': '0.0.0.0:50075', 'dfs.datanode.ipc.address': '0.0.0.0:8010', 'dfs.datanode.data.dir': '/hadoop/hdfs/data', 'dfs.namenode.http-address': 'dev05.hortonworks.com:50070', 'dfs.blockreport.initialDelay': '120', 'dfs.datanode.failed.volumes.tolerated': '0', 'dfs.namenode.accesstime.precision': '0', 'dfs.block.local-path-access.user': 'hbase', 'dfs.https.namenode.https-address': 'dev05.hortonworks.com:50470', 'dfs.namenode.secondary.http-address': 'dev05.hortonworks.com:50090', 'dfs.namenode.stale.datanode.interval': '30000', 'dfs.heartbeat.interval': '3', 'dfs.client.read.shortcircuit.streams.cache.size': '4096', 'dfs.permissions.superusergroup': 'hdfs', 'dfs.journalnode.http-address': '0.0.0.0:8480', 'dfs.domain.socket.path': '/var/lib/hadoop-hdfs/dn_socket', 'dfs.namenode.avoid.read.stale.datanode': 'true', 'dfs.hosts.exclude': '/etc/hadoop/conf/dfs.exclude', 'dfs.datanode.data.dir.perm': '750', 'dfs.namenode.write.stale.datanode.ratio': '1.0f', 'dfs.replication.max': '50', 'dfs.namenode.name.dir': '/hadoop/hdfs/namenode'}\n-    expected_put_path = '//clusters/test1'\n-    expected_post_request = {'Clusters': {'desired_configs': {'tag': 'version1', 'type': 'hdfs-site', 'properties':expected_properties}}}   \n-    expected_get_path = '//clusters/test1/configurations?type=hdfs-site&tag=version1'\n-    expected_get_request = None\n-        \n-    cluster = self.create_cluster(http_client_mock)\n-    existant_global_config = cluster.get_hdfs_site_config()\n-    existant_global_config.properties['dfs.namenode.avoid.write.stale.datanode'] = 'abc'\n-    cluster.update_hdfs_site_config(existant_global_config)\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    http_client_mock.invoke.assert_any_call('PUT', expected_put_path, headers=None, payload=expected_post_request)\n-    http_client_mock.invoke.assert_any_call('GET', expected_get_path, headers=None, payload=expected_get_request)\\\n-    \n-  def test_update_mapred_site_config(self):\n-    \"\"\"\n-    Update mapred-site config\n-    \"\"\"\n-    http_client_mock = MagicMock()\n-    \n-    expected_properties = {'mapreduce.jobhistory.address': 'abc', 'mapreduce.reduce.input.buffer.percent': '0.0', 'mapred.jobtracker.maxtasks.per.job': '-1', 'mapreduce.framework.name': 'yarn', 'mapreduce.map.speculative': 'false', 'mapreduce.tasktracker.healthchecker.script.path': 'file:////mapred/jobstatus', 'mapreduce.reduce.shuffle.merge.percent': '0.66', 'mapred.userlog.retain.hours': '24', 'yarn.app.mapreduce.am.resource.mb': '1024', 'mapreduce.reduce.shuffle.parallelcopies': '30', 'mapreduce.map.java.opts': '-Xmx320m', 'mapreduce.task.io.sort.factor': '100', 'mapreduce.application.classpath': '$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*', 'yarn.app.mapreduce.am.command-opts': '-Xmx756m', 'mapreduce.job.reduce.slowstart.completedmaps': '0.05', 'mapreduce.output.fileoutputformat.compress.type': 'BLOCK', 'mapreduce.reduce.speculative': 'false', 'mapreduce.reduce.java.opts': '-Xmx756m', 'mapreduce.am.max-attempts': '2', 'yarn.app.mapreduce.am.admin-command-opts': '-Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN', 'mapreduce.jobtracker.system.dir': '/mapred/system', 'mapreduce.map.sort.spill.percent': '0.1', 'mapreduce.task.timeout': '600000', 'mapreduce.map.memory.mb': '1536', 'mapreduce.reduce.log.level': 'INFO', 'mapreduce.jobhistory.intermediate-done-dir': '/mr-history/tmp', 'mapreduce.reduce.memory.mb': '2048', 'mapreduce.tasktracker.map.tasks.maximum': '4', 'yarn.app.mapreduce.am.log.level': 'INFO', 'mapreduce.map.log.level': 'INFO', 'mapreduce.shuffle.port': '13562', 'mapred.jobtracker.taskScheduler': 'org.apache.hadoop.mapred.CapacityTaskScheduler', 'mapreduce.admin.user.env': 'LD_LIBRARY_PATH=/usr/lib/hadoop/lib/native:/usr/lib/hadoop/lib/native/`$JAVA_HOME/bin/java -d32 -version &amp;&gt; /dev/null;if [ $? -eq 0 ]; then echo Linux-i386-32; else echo Linux-amd64-64;fi`', 'mapreduce.jobhistory.webapp.address': 'dev05.hortonworks.com:19888', 'mapred.hosts.exclude': '/etc/hadoop/conf/mapred.exclude', 'mapreduce.reduce.shuffle.input.buffer.percent': '0.7', 'yarn.app.mapreduce.am.staging-dir': '/user', 'mapred.hosts': '/etc/hadoop/conf/mapred.include', 'mapreduce.jobhistory.done-dir': '/mr-history/done', 'mapreduce.admin.reduce.child.java.opts': '-Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN', 'mapreduce.task.io.sort.mb': '200', 'mapred.task.tracker.task-controller': 'org.apache.hadoop.mapred.DefaultTaskController', 'mapreduce.admin.map.child.java.opts': '-Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN'}\n-    expected_put_path = '//clusters/test1'\n-    expected_post_request = {'Clusters': {'desired_configs': {'tag': 'version1', 'type': 'mapred-site', 'properties':expected_properties}}}   \n-    expected_get_path = '//clusters/test1/configurations?type=mapred-site&tag=version1'\n-    expected_get_request = None\n-        \n-    cluster = self.create_cluster(http_client_mock)\n-    existant_global_config = cluster.get_mapred_site_config()\n-    existant_global_config.properties['mapreduce.jobhistory.address'] = 'abc'\n-    cluster.update_mapred_site_config(existant_global_config)\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    http_client_mock.invoke.assert_any_call('PUT', expected_put_path, headers=None, payload=expected_post_request)\n-    http_client_mock.invoke.assert_any_call('GET', expected_get_path, headers=None, payload=expected_get_request)\n-    \n-  def test_create_services(self):\n-    \"\"\"\n-    Create services\n-    \"\"\"   \n-    http_client_mock = MagicMock()\n-    \n-    expected_path = '//clusters/test1/services'\n-    expected_request = [{'ServiceInfo': {'service_name': 'HDFS'}}, {'ServiceInfo': {'service_name': 'YARN'}}, {'ServiceInfo': {'service_name': 'MAPREDUCEv2'}}, {'ServiceInfo': {'service_name': 'TEZ'}}]\n-    \n-    cluster = self.create_cluster(http_client_mock)\n-    resp = cluster.create_services(['HDFS','YARN','MAPREDUCEv2','TEZ'])\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    http_client_mock.invoke.assert_called_with('POST', expected_path, headers=None, payload=expected_request)\n-    \n-  def test_create_service_components(self):\n-    \"\"\"\n-    Create service components\n-    \"\"\"   \n-    http_client_mock = MagicMock()\n-    \n-    expected_path = '//clusters/test1/services/?ServiceInfo/service_name=HDFS'\n-    expected_request = {'components': [{'ServiceComponentInfo': {'component_name': u'NODEMANAGER'}}, {'ServiceComponentInfo': {'component_name': u'RESOURCEMANAGER'}}, {'ServiceComponentInfo': {'component_name': u'YARN_CLIENT'}}]}\n-    \n-    cluster = self.create_cluster(http_client_mock)\n-    resp = cluster.create_service_components(\"2.0.5\", \"HDFS\")\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    http_client_mock.invoke.assert_called_with('POST', expected_path, headers=None, payload=expected_request)\n-    \n-  def test_create_service_component(self):\n-    \"\"\"\n-    Create service component\n-    \"\"\"   \n-    http_client_mock = MagicMock()\n-    \n-    expected_path = '//clusters/test1/services/HDFS/components/NAMENODE'\n-    \n-    cluster = self.create_cluster(http_client_mock)\n-    resp = cluster.create_service_component(\"2.0.5\", \"HDFS\",\"NAMENODE\")\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    http_client_mock.invoke.assert_called_with('POST', expected_path, headers=None, payload=None)\n-    \n-  def test_create_hosts(self):\n-    \"\"\"\n-    Create cluster hosts\n-    \"\"\"   \n-    http_client_mock = MagicMock()\n-    \n-    expected_path = '//clusters/test1/hosts'\n-    expected_request = [{'Hosts': {'ip': '1.2.3.4', 'host_name': 'hostname01', 'rack_info': '/default-rack'}}, {'Hosts': {'ip': '2.3.1.22', 'host_name': 'hostname02', 'rack_info': 'rack'}}]\n-        \n-    cluster = self.create_cluster(http_client_mock)\n-    host_list = [HostModel(None, 'hostname01','1.2.3.4'), HostModel(None, 'hostname02','2.3.1.22','rack')]\n-    resp = cluster.create_hosts(host_list)\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    http_client_mock.invoke.assert_called_with('POST', expected_path, headers=None, payload=expected_request)\n-    \n-  def test_create_host(self):\n-    \"\"\"\n-    Create cluster host\n-    \"\"\"   \n-    http_client_mock = MagicMock()\n-    \n-    expected_path = '//clusters/test1/hosts'\n-    expected_request = [{'Hosts': {'ip': '1.2.3.4', 'host_name': 'hostname01', 'rack_info': '/default-rack'}}]\n-            \n-    cluster = self.create_cluster(http_client_mock)\n-    resp = cluster.create_host('hostname01','1.2.3.4')\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    http_client_mock.invoke.assert_called_with('POST', expected_path, headers=None, payload=expected_request)\n-    \n-    \n-  def test_delete_host(self):\n-    \"\"\"\n-    Delete cluster host\n-    \"\"\"   \n-    http_client_mock = MagicMock()\n-    \n-    expected_path = '//clusters/test1/hosts/hostname01'\n-            \n-    cluster = self.create_cluster(http_client_mock)\n-    resp = cluster.delete_host('hostname01')\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    http_client_mock.invoke.assert_called_with('DELETE', expected_path, headers=None, payload=None)\n-    \n-    \n-  def test_exceptions(self):\n-    \"\"\"\n-    Test exceptions from ambari.client.core.errors\n-    \"\"\"\n-    cluster = self.create_cluster()\n-    \n-    try:\n-      cluster.delete_host('deleted_nonexistant_cluster')\n-      print http_client_mock.invoke.call_args_list\n-      self.fail('Exception should have been thrown!')\n-    except BadRequest, ex:\n-      self.assertEquals(str(ex), 'exception: 400. Attempted to add unknown hosts to a cluster.  These hosts have not been registered with the server: dev05')\n-    except Exception, ex:\n-      self.fail('Wrong exception thrown!')\n-    \n-  def test_start_all_services(self):\n-    \"\"\"\n-    Start all services\n-    \"\"\"   \n-    http_client_mock = MagicMock()\n-    \n-    expected_path = '//clusters/test1/services?ServiceInfo/state=INSTALLED&params/run_smoke_test=true&params/reconfigure_client=false'\n-    expected_request = {'RequestInfo': {'context': 'Start All Services'}, 'Body': {'ServiceInfo': {'state': 'STARTED'}}}\n-            \n-    cluster = self.create_cluster(http_client_mock)\n-    resp = cluster.start_all_services(True)\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    http_client_mock.invoke.assert_called_with('PUT', expected_path, headers=None, payload=expected_request)\n-    \n-  def test_stop_all_services(self):\n-    \"\"\"\n-    Stop all services\n-    \"\"\"   \n-    http_client_mock = MagicMock()\n-    \n-    expected_path = '//clusters/test1/services?ServiceInfo'\n-    expected_request = {'RequestInfo': {'context': 'Stop All Services'}, 'Body': {'ServiceInfo': {'state': 'INSTALLED'}}}\n-            \n-    cluster = self.create_cluster(http_client_mock)\n-    resp = cluster.stop_all_services()\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    http_client_mock.invoke.assert_called_with('PUT', expected_path, headers=None, payload=expected_request)\n-    \n-    \n-  def test_install_all_services(self):\n-    \"\"\"\n-    Install all services\n-    \"\"\"   \n-    http_client_mock = MagicMock()\n-    \n-    expected_path = '//clusters/test1/services?ServiceInfo/state=INSTALLED'\n-    expected_request = {'RequestInfo': {'context': 'Install Services'}, 'Body': {'ServiceInfo': {'state': 'INSTALLED'}}}\n-            \n-    cluster = self.create_cluster(http_client_mock)\n-    resp = cluster.install_all_services()\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    http_client_mock.invoke.assert_called_with('PUT', expected_path, headers=None, payload=expected_request)\n-    \n-  def test_add_config(self):\n-    \"\"\"\n-    Set desired configurations\n-    \"\"\"   \n-    http_client_mock = MagicMock()\n-    \n-    propr_dict = {\"dfs_name_dir\":\"/data/1/hadoop/hdfs/namenode,/data/2/hadoop/hdfs/namenode,/data/3/hadoop/hdfs/namenode,/data/4/hadoop/hdfs/namenode,/data/5/hadoop/hdfs/namenode,/data/6/hadoop/hdfs/namenode,/data/7/hadoop/hdfs/namenode,/data/8/hadoop/hdfs/namenode\", \"namenode_heapsize\":\"1024m\", \"namenode_opt_newsize\":\"200m\", 'namenode_opt_maxpermsize': '256m', 'namenode_opt_permsize': '128m', \"fs_checkpoint_dir\":\"/data/1/hadoop/hdfs/namesecondary\", \"dfs_data_dir\":\"/data/1/hadoop/hdfs/data,/data/2/hadoop/hdfs/data,/data/3/hadoop/hdfs/data,/data/4/hadoop/hdfs/data,/data/5/hadoop/hdfs/data,/data/6/hadoop/hdfs/data,/data/7/hadoop/hdfs/data,/data/8/hadoop/hdfs/data,/data/9/hadoop/hdfs/data,/data/10/hadoop/hdfs/data\", \"dtnode_heapsize\":\"1024m\", \"dfs_datanode_failed_volume_tolerated\":\"0\", \"dfs_webhdfs_enabled\":\"true\", \"hadoop_heapsize\":\"1024\", \"datanode_du_reserved\":\"0\", \"fs_checkpoint_period\":\"21600\", \"fs_checkpoint_size\":\"67108864\", \"hdfs_log_dir_prefix\":\"/var/log/hadoop\", \"hadoop_pid_dir_prefix\":\"/var/run/hadoop\", \"namenode_opt_maxnewsize\":\"200m\", \"dfs_exclude\":\"dfs.exclude\", \"dfs_include\":\"dfs.include\", \"dfs_replication\":\"3\", \"dfs_block_local_path_access_user\":\"hbase\", \"dfs_datanode_data_dir_perm\":\"750\", \"security_enabled\":\"false\", \"namenode_formatted_mark_dir\":\"/var/run/hadoop/hdfs/namenode/formatted/\", \"hcat_conf_dir\":\"\", \"jtnode_opt_newsize\":\"200m\", \"jtnode_opt_maxnewsize\":\"200m\", \"jtnode_heapsize\":\"1024m\", \"mapred_local_dir\":\"/data/1/hadoop/mapred,/data/2/hadoop/mapred,/data/3/hadoop/mapred,/data/4/hadoop/mapred,/data/5/hadoop/mapred,/data/6/hadoop/mapred,/data/7/hadoop/mapred,/data/8/hadoop/mapred,/data/9/hadoop/mapred,/data/10/hadoop/mapred\", \"mapred_map_tasks_max\":\"4\", \"mapred_red_tasks_max\":\"2\", \"mapred_child_java_opts_sz\":\"768\", \"scheduler_name\":\"org.apache.hadoop.mapred.CapacityTaskScheduler\", \"mapred_cluster_map_mem_mb\":\"1536\", \"mapred_cluster_red_mem_mb\":\"2048\", \"mapred_cluster_max_map_mem_mb\":\"6144\", \"mapred_cluster_max_red_mem_mb\":\"4096\", \"mapred_job_map_mem_mb\":\"1536\", \"mapred_job_red_mem_mb\":\"2048\", \"io_sort_mb\":\"200\", \"io_sort_spill_percent\":\"0.9\", \"mapreduce_userlog_retainhours\":\"24\", \"maxtasks_per_job\":\"-1\", \"lzo_enabled\":\"true\", \"snappy_enabled\":\"true\", \"rca_enabled\":\"true\", \"mapred_system_dir\":\"/mapred/system\", \"mapred_hosts_exclude\":\"mapred.exclude\", \"mapred_hosts_include\":\"mapred.include\", \"mapred_jobstatus_dir\":\"file:////mapred/jobstatus\", \"nagios_web_login\":\"nagiosadmin\", \"nagios_web_password\":\"admin\", \"nagios_contact\":\"admin@admin.com\", \"nagios_group\":\"nagios\", \"hbase_conf_dir\":\"/etc/hbase\", \"proxyuser_group\":\"users\", \"dfs_datanode_address\":\"50010\", \"dfs_datanode_http_address\":\"50075\", \"apache_artifacts_download_url\":\"\", \"ganglia_runtime_dir\":\"/var/run/ganglia/hdp\", \"java64_home\":\"/usr/jdk/jdk1.6.0_31\", \"run_dir\":\"/var/run/hadoop\", \"hadoop_conf_dir\":\"/etc/hadoop\", \"hdfs_user\":\"hdfs\", \"mapred_user\":\"mapred\", \"hbase_user\":\"hbase\", \"hive_user\":\"hive\", \"hcat_user\":\"hcat\", \"webhcat_user\":\"hcat\", \"oozie_user\":\"oozie\", \"zk_user\":\"zookeeper\", \"gmetad_user\":\"nobody\", \"gmond_user\":\"nobody\", \"nagios_user\":\"nagios\", \"smokeuser\":\"ambari-qa\", \"user_group\":\"hadoop\", \"rrdcached_base_dir\":\"/var/lib/ganglia/rrds\"}\n-    expected_path = '//clusters/test1'\n-    expected_request = {'Clusters': {'desired_configs': {'tag':'version1', 'type':'global', 'properties':propr_dict}}}\n-                \n-    cluster = self.create_cluster(http_client_mock)\n-    resp = cluster.add_config(\"global\",\"version1\",propr_dict)\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    http_client_mock.invoke.assert_called_with('PUT', expected_path, headers=None, payload=expected_request)\n-    \n-  def test_create_config(self):\n-    \"\"\"\n-    Add a configuration\n-    \"\"\"   \n-    http_client_mock = MagicMock()\n-    \n-    propr_dict = {\"dfs_name_dir\":\"/data/1/hadoop/hdfs/namenode,/data/2/hadoop/hdfs/namenode,/data/3/hadoop/hdfs/namenode,/data/4/hadoop/hdfs/namenode,/data/5/hadoop/hdfs/namenode,/data/6/hadoop/hdfs/namenode,/data/7/hadoop/hdfs/namenode,/data/8/hadoop/hdfs/namenode\", \"namenode_heapsize\":\"1024m\", \"namenode_opt_newsize\":\"200m\", 'namenode_opt_maxpermsize': '256m', 'namenode_opt_permsize': '128m', \"fs_checkpoint_dir\":\"/data/1/hadoop/hdfs/namesecondary\", \"dfs_data_dir\":\"/data/1/hadoop/hdfs/data,/data/2/hadoop/hdfs/data,/data/3/hadoop/hdfs/data,/data/4/hadoop/hdfs/data,/data/5/hadoop/hdfs/data,/data/6/hadoop/hdfs/data,/data/7/hadoop/hdfs/data,/data/8/hadoop/hdfs/data,/data/9/hadoop/hdfs/data,/data/10/hadoop/hdfs/data\", \"dtnode_heapsize\":\"1024m\", \"dfs_datanode_failed_volume_tolerated\":\"0\", \"dfs_webhdfs_enabled\":\"true\", \"hadoop_heapsize\":\"1024\", \"datanode_du_reserved\":\"0\", \"fs_checkpoint_period\":\"21600\", \"fs_checkpoint_size\":\"67108864\", \"hdfs_log_dir_prefix\":\"/var/log/hadoop\", \"hadoop_pid_dir_prefix\":\"/var/run/hadoop\", \"namenode_opt_maxnewsize\":\"200m\", \"dfs_exclude\":\"dfs.exclude\", \"dfs_include\":\"dfs.include\", \"dfs_replication\":\"3\", \"dfs_block_local_path_access_user\":\"hbase\", \"dfs_datanode_data_dir_perm\":\"750\", \"security_enabled\":\"false\", \"namenode_formatted_mark_dir\":\"/var/run/hadoop/hdfs/namenode/formatted/\", \"hcat_conf_dir\":\"\", \"jtnode_opt_newsize\":\"200m\", \"jtnode_opt_maxnewsize\":\"200m\", \"jtnode_heapsize\":\"1024m\", \"mapred_local_dir\":\"/data/1/hadoop/mapred,/data/2/hadoop/mapred,/data/3/hadoop/mapred,/data/4/hadoop/mapred,/data/5/hadoop/mapred,/data/6/hadoop/mapred,/data/7/hadoop/mapred,/data/8/hadoop/mapred,/data/9/hadoop/mapred,/data/10/hadoop/mapred\", \"mapred_map_tasks_max\":\"4\", \"mapred_red_tasks_max\":\"2\", \"mapred_child_java_opts_sz\":\"768\", \"scheduler_name\":\"org.apache.hadoop.mapred.CapacityTaskScheduler\", \"mapred_cluster_map_mem_mb\":\"1536\", \"mapred_cluster_red_mem_mb\":\"2048\", \"mapred_cluster_max_map_mem_mb\":\"6144\", \"mapred_cluster_max_red_mem_mb\":\"4096\", \"mapred_job_map_mem_mb\":\"1536\", \"mapred_job_red_mem_mb\":\"2048\", \"io_sort_mb\":\"200\", \"io_sort_spill_percent\":\"0.9\", \"mapreduce_userlog_retainhours\":\"24\", \"maxtasks_per_job\":\"-1\", \"lzo_enabled\":\"true\", \"snappy_enabled\":\"true\", \"rca_enabled\":\"true\", \"mapred_system_dir\":\"/mapred/system\", \"mapred_hosts_exclude\":\"mapred.exclude\", \"mapred_hosts_include\":\"mapred.include\", \"mapred_jobstatus_dir\":\"file:////mapred/jobstatus\", \"nagios_web_login\":\"nagiosadmin\", \"nagios_web_password\":\"admin\", \"nagios_contact\":\"admin@admin.com\", \"nagios_group\":\"nagios\", \"hbase_conf_dir\":\"/etc/hbase\", \"proxyuser_group\":\"users\", \"dfs_datanode_address\":\"50010\", \"dfs_datanode_http_address\":\"50075\", \"apache_artifacts_download_url\":\"\", \"ganglia_runtime_dir\":\"/var/run/ganglia/hdp\", \"java64_home\":\"/usr/jdk/jdk1.6.0_31\", \"run_dir\":\"/var/run/hadoop\", \"hadoop_conf_dir\":\"/etc/hadoop\", \"hdfs_user\":\"hdfs\", \"mapred_user\":\"mapred\", \"hbase_user\":\"hbase\", \"hive_user\":\"hive\", \"hcat_user\":\"hcat\", \"webhcat_user\":\"hcat\", \"oozie_user\":\"oozie\", \"zk_user\":\"zookeeper\", \"gmetad_user\":\"nobody\", \"gmond_user\":\"nobody\", \"nagios_user\":\"nagios\", \"smokeuser\":\"ambari-qa\", \"user_group\":\"hadoop\", \"rrdcached_base_dir\":\"/var/lib/ganglia/rrds\"}\n-    expected_path = '//clusters/test1'\n-    expected_request = {'tag':'version1', 'type':'global', 'properties':propr_dict}\n-                \n-    cluster = self.create_cluster(http_client_mock)\n-    resp = cluster.create_config(\"global\",\"version1\",propr_dict)\n-    \n-    self.assertEqual(cluster.cluster_name, \"test1\")\n-    http_client_mock.invoke.assert_called_with('PUT', expected_path, headers=None, payload=expected_request)\n-    \n-    ",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/TestClusterModel.py",
                "sha": "b501d297047c7316288184d101394fc5a95d3551",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/TestComponentModel.py",
                "changes": 54,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/TestComponentModel.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 54,
                "filename": "ambari-client/python-client/src/test/python/TestComponentModel.py",
                "patch": "@@ -1,54 +0,0 @@\n-#!/usr/bin/env python\n-\n-'''\n-Licensed to the Apache Software Foundation (ASF) under one\n-or more contributor license agreements.  See the NOTICE file\n-distributed with this work for additional information\n-regarding copyright ownership.  The ASF licenses this file\n-to you under the Apache License, Version 2.0 (the\n-\"License\"); you may not use this file except in compliance\n-with the License.  You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-'''\n-import logging\n-\n-from mock.mock import MagicMock, patch\n-from HttpClientInvoker import HttpClientInvoker\n-\n-from ambari_client.ambari_api import  AmbariClient\n-\n-import unittest\n-\n-class TestClusterModel(unittest.TestCase):\n-\n-  def setUp(self):\n-    http_client_logger = logging.getLogger()\n-    http_client_logger.info('Running test:' + self.id())\n-\n-  def create_component(self, http_client_mock = MagicMock()):\n-    http_client_mock.invoke.side_effect = HttpClientInvoker.http_client_invoke_side_effects\n-    client = AmbariClient(\"localhost\", 8080, \"admin\", \"admin\", version=1, client=http_client_mock)\n-    cluster = client.get_cluster('test1')\n-    host = cluster.get_host('myhost')\n-    component = host.get_host_component(\"DATANODE\")\n-    return component\n-\n-  def test_component_get_metrics(self):\n-    http_client_mock = MagicMock()\n-\n-    expected_path = '//clusters/cl1/hosts/myhost/host_components/DATANODE?fields=metrics'\n-    expected_json_output = {u'HostRoles': {u'cluster_name': u'cl1', u'host_name': u'myhost', u'component_name': u'DATANODE'}, u'metrics': {u'load': {u'load_one': 0.0125555555556, u'load_five': 0.059277777777800002, u'load_fifteen': 0.069222222222199994}, u'ugi': {u'loginSuccess_avg_time': 0.0, u'loginFailure_avg_time': 0.0, u'loginSuccess_num_ops': 0.0, u'loginFailure_num_ops': 0.0}, u'network': {u'bytes_in': 30989.887416699999, u'pkts_in': 44.982222222200001, u'pkts_out': 214.35891666699999, u'bytes_out': 98799.674277800004}, u'process': {u'proc_total': 682.39722222199998, u'proc_run': 2.0}, u'dfs': {u'datanode': {u'replaceBlockOp_num_ops': 0.0, u'replaceBlockOp_avg_time': 0.0, u'blockChecksumOp_avg_time': 0.0, u'copyBlockOp_avg_time': 0.0, u'copyBlockOp_num_ops': 0.0, u'heartBeats_avg_time': 1.69166666667, u'writes_from_local_client': 0.0, u'blockReports_avg_time': 6.0, u'blocks_written': 0.0, u'writeBlockOp_num_ops': 0.0, u'bytes_read': 0.0, u'writeBlockOp_avg_time': 13.896907216500001, u'writes_from_remote_client': 0.0, u'blocks_read': 0.0, u'readBlockOp_avg_time': 0.0, u'reads_from_remote_client': 0.0, u'block_verification_failures': 0.0, u'reads_from_local_client': 0.0, u'blocks_removed': 0.0, u'blocks_get_local_pathinfo': 0.0, u'blockReports_num_ops': 0.0, u'heartBeats_num_ops': 0.33648148148099999, u'blocks_verified': 0.0, u'bytes_written': 0.0, u'readBlockOp_num_ops': 0.0, u'blocks_replicated': 0.0, u'blockChecksumOp_num_ops': 0.0}, u'FSNamesystem': {u'VolumeInfo': u'{\"/hadoop/hdfs/data/current\":{\"freeSpace\":495195869184,\"usedSpace\":345120768,\"reservedSpace\":1073741824}}', u'HttpPort': None, u'RpcPort': u'8010', u'NamenodeAddress': u'{\"myhost\":\"BP-442795920-192.168.64.101-1383132565020\"}', u'Version': u'2.2.0.2.0.6.0-76'}}, u'rpc': {u'NumOpenConnections': 0.0, u'RpcProcessingTime_avg_time': 0.0, u'rpcAuthorizationFailures': 0.0, u'callQueueLen': 0.0, u'RpcProcessingTime_num_ops': 0.0, u'RpcQueueTime_avg_time': 0.0, u'rpcAuthorizationSuccesses': 0.0, u'rpcAuthenticationSuccesses': 0.0, u'rpcAuthenticationFailures': 0.0, u'ReceivedBytes': 0.0, u'RpcQueueTime_num_ops': 0.0, u'SentBytes': 0.0}, u'boottime': 1383131209.0, u'jvm': {u'NonHeapMemoryMax': 136314880, u'logWarn': 0.0, u'gcCount': 0.011111111111100001, u'threadsRunnable': 8.0416666666700003, u'memHeapCommittedM': 28.5625, u'threadsWaiting': 18.0, u'NonHeapMemoryUsed': 30798600, u'threadsTimedWaiting': 8.9166666666700003, u'threadsNew': 0.0, u'HeapMemoryUsed': 11395264, u'memHeapUsedM': 11.7175731111, u'memNonHeapUsedM': 29.360076750000001, u'threadsTerminated': 0.0, u'logInfo': 0.0, u'logError': 0.0, u'HeapMemoryMax': 1037959168, u'threadsBlocked': 0.0, u'logFatal': 0.0, u'memNonHeapCommittedM': 29.625, u'gcTimeMillis': 388}, u'memory': {u'mem_cached': 160191.85555599999, u'swap_free': 2593920.0, u'mem_free': 183983.85555599999, u'mem_buffers': 23914.266666700001, u'mem_shared': 0.0, u'swap_total': 2621432.0, u'mem_total': 1922680.0}, u'disk': {u'disk_total': 525.78999999999996, u'disk_free': 495.64499999999998, u'part_max_used': 11.6}, u'cpu': {u'cpu_idle': 85.730000000000004, u'cpu_num': 1.0, u'cpu_wio': 0.0041666666666699999, u'cpu_user': 7.4288888888900004, u'cpu_aidle': 0.0, u'cpu_system': 6.8458333333299999, u'cpu_speed': 2967.0, u'cpu_nice': 0.0}}, u'host': {u'href': u'http://192.168.64.101:8080/api/v1/clusters/cl1/hosts/myhost'}, u'href': u'http://192.168.64.101:8080/api/v1/clusters/cl1/hosts/myhost/host_components/DATANODE?fields=metrics'}\n-\n-    component = self.create_component(http_client_mock)\n-    metrics_json = component.get_metrics()\n-\n-    self.assertEqual(expected_json_output,metrics_json)\n-    http_client_mock.invoke.assert_called_with('GET', expected_path, headers=None, payload=None)\n-    pass",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/TestComponentModel.py",
                "sha": "b3f15307cec1132d995b42f0a50001fd4e27a645",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/TestHostModel.py",
                "changes": 80,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/TestHostModel.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 80,
                "filename": "ambari-client/python-client/src/test/python/TestHostModel.py",
                "patch": "@@ -1,80 +0,0 @@\n-#!/usr/bin/env python\n-\n-'''\n-Licensed to the Apache Software Foundation (ASF) under one\n-or more contributor license agreements.  See the NOTICE file\n-distributed with this work for additional information\n-regarding copyright ownership.  The ASF licenses this file\n-to you under the Apache License, Version 2.0 (the\n-\"License\"); you may not use this file except in compliance\n-with the License.  You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-'''\n-import logging\n-\n-from mock.mock import MagicMock, patch\n-from HttpClientInvoker import HttpClientInvoker\n-\n-from ambari_client.ambari_api import  AmbariClient\n-\n-import unittest\n-\n-class TestHostModel(unittest.TestCase):\n-\n-  def setUp(self):\n-    http_client_logger = logging.getLogger()\n-    http_client_logger.info('Running test:' + self.id())\n-\n-  def create_host(self, http_client_mock = MagicMock()):\n-    http_client_mock.invoke.side_effect = HttpClientInvoker.http_client_invoke_side_effects\n-    client = AmbariClient(\"localhost\", 8080, \"admin\", \"admin\", version=1, client=http_client_mock)\n-    cluster = client.get_cluster('test1')\n-    host = cluster.get_host('myhost')\n-    return host\n-\n-  def test_get_host_components(self):\n-    http_client_mock = MagicMock()\n-\n-    expected_path = '//clusters/test1/hosts/myhost/host_components?fields=HostRoles/state'\n-\n-    host = self.create_host(http_client_mock)\n-    host_components = host.get_host_components()\n-\n-    self.assertEqual(host_components[0].component_name,\"DATANODE\")\n-    self.assertEqual(host_components[0].state,\"STARTED\")\n-    self.assertEqual(host_components[3].component_name,\"HBASE_MASTER\")\n-    self.assertEqual(host_components[3].state,\"STARTED\")\n-    http_client_mock.invoke.assert_called_with('GET', expected_path, headers=None, payload=None)\n-\n-  def test_get_host_component(self):\n-    http_client_mock = MagicMock()\n-\n-    expected_path = '//clusters/test1/hosts/myhost/host_components/DATANODE'\n-\n-    host =  self.create_host(http_client_mock)\n-    component = host.get_host_component(\"DATANODE\")\n-\n-    self.assertEqual(component.component_name,\"DATANODE\")\n-    self.assertEqual(component.state,\"STARTED\")\n-    self.assertEqual(component.host_name,\"myhost\")\n-\n-    http_client_mock.invoke.assert_called_with('GET', expected_path, headers=None, payload=None)\n-\n-  def test_assign_role(self):\n-    http_client_mock = MagicMock()\n-\n-    expected_path = '//clusters/test1/hosts?Hosts/host_name=myhost'\n-    expected_payload = {'host_components': [{'HostRoles': {'component_name': 'GANGLIA_SERVER'}}]}\n-\n-    host =  self.create_host(http_client_mock)\n-    status = host.assign_role(\"GANGLIA_SERVER\")\n-\n-    self.assertTrue(status.status, 201)\n-    http_client_mock.invoke.assert_called_with('POST', expected_path, headers=None, payload=expected_payload)",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/TestHostModel.py",
                "sha": "8b3b531e4edb516da32cec1f50893e5ca1aea8a4",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/TestServiceModel.py",
                "changes": 105,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/TestServiceModel.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 105,
                "filename": "ambari-client/python-client/src/test/python/TestServiceModel.py",
                "patch": "@@ -1,105 +0,0 @@\n-#!/usr/bin/env python\n-\n-'''\n-Licensed to the Apache Software Foundation (ASF) under one\n-or more contributor license agreements.  See the NOTICE file\n-distributed with this work for additional information\n-regarding copyright ownership.  The ASF licenses this file\n-to you under the Apache License, Version 2.0 (the\n-\"License\"); you may not use this file except in compliance\n-with the License.  You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-'''\n-import logging\n-\n-from mock.mock import MagicMock, patch\n-from HttpClientInvoker import HttpClientInvoker\n-\n-from ambari_client.ambari_api import  AmbariClient\n-\n-import unittest\n-\n-class TestServiceModel(unittest.TestCase):\n-\n-  def setUp(self):\n-    http_client_logger = logging.getLogger()\n-    http_client_logger.info('Running test:' + self.id())\n-\n-  def create_service(self, http_client_mock = MagicMock()):\n-    http_client_mock.invoke.side_effect = HttpClientInvoker.http_client_invoke_side_effects\n-    client = AmbariClient(\"localhost\", 8080, \"admin\", \"admin\", version=1, client=http_client_mock)\n-    cluster = client.get_cluster('test1')\n-    service = cluster.get_service('GANGLIA')\n-    return service\n-\n-  def test_start(self):\n-    http_client_mock = MagicMock()\n-\n-    expected_path = '//clusters/test1/services/GANGLIA'\n-    expected_payload = {'ServiceInfo': {'state': 'STARTED'}}\n-\n-    service = self.create_service(http_client_mock)\n-    status = service.start()\n-\n-    self.assertEqual(status.get_request_path(), 'clusters/test1/requests/19')\n-    http_client_mock.invoke.assert_called_with('PUT', expected_path, headers=None, payload=expected_payload)\n-\n-  def test_stop(self):\n-    http_client_mock = MagicMock()\n-\n-    expected_path = '//clusters/test1/services/GANGLIA'\n-    expected_payload = {\"ServiceInfo\": {\"state\": \"INSTALLED\"}}\n-\n-    service = self.create_service(http_client_mock)\n-    status = service.stop()\n-\n-    self.assertEqual(status.get_request_path(), 'clusters/test1/requests/19')\n-    http_client_mock.invoke.assert_called_with('PUT', expected_path, headers=None, payload=expected_payload)\n-\n-  def test_install(self):\n-    http_client_mock = MagicMock()\n-\n-    expected_path = '//clusters/test1/services/GANGLIA'\n-    expected_payload = {\"ServiceInfo\": {\"state\": \"INSTALLED\"}}\n-\n-    service = self.create_service(http_client_mock)\n-    status = service.install()\n-\n-    self.assertEqual(status.get_request_path(), 'clusters/test1/requests/19')\n-    http_client_mock.invoke.assert_called_with('PUT', expected_path, headers=None, payload=expected_payload)\n-\n-  def test_get_service_components(self):\n-    http_client_mock = MagicMock()\n-\n-    expected_path = '//clusters/test1/services/GANGLIA/components?fields=*'\n-\n-    service = self.create_service(http_client_mock)\n-    components = service.get_service_components()\n-\n-    self.assertEqual(components[0].component_name, \"GANGLIA_MONITOR\")\n-    self.assertEqual(components[0].state, \"STARTED\")\n-    self.assertEqual(components[1].component_name, \"GANGLIA_SERVER\")\n-    self.assertEqual(components[1].state, \"INSTALLED\")\n-\n-    http_client_mock.invoke.assert_called_with('GET', expected_path, headers=None, payload=None)\n-\n-  def test_get_service_component(self):\n-    http_client_mock = MagicMock()\n-\n-    expected_path = '//clusters/test1/services/GANGLIA/components/GANGLIA_SERVER'\n-\n-    service = self.create_service(http_client_mock)\n-    component = service.get_service_component(\"GANGLIA_SERVER\")\n-\n-    self.assertEqual(component.component_name, \"GANGLIA_SERVER\")\n-    self.assertEqual(component.service_name, \"GANGLIA\")\n-    self.assertEqual(component.state, \"STARTED\")\n-\n-    http_client_mock.invoke.assert_called_with('GET', expected_path, headers=None, payload=None)",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/TestServiceModel.py",
                "sha": "693370151b0ee5578c20a3504569c480ba74c3c1",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/TestStatusModel.py",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/TestStatusModel.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 83,
                "filename": "ambari-client/python-client/src/test/python/TestStatusModel.py",
                "patch": "@@ -1,83 +0,0 @@\n-#!/usr/bin/env python\n-\n-'''\n-Licensed to the Apache Software Foundation (ASF) under one\n-or more contributor license agreements.  See the NOTICE file\n-distributed with this work for additional information\n-regarding copyright ownership.  The ASF licenses this file\n-to you under the Apache License, Version 2.0 (the\n-\"License\"); you may not use this file except in compliance\n-with the License.  You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-'''\n-import logging\n-\n-from ambari_client.model.status import StatusModel\n-from mock.mock import MagicMock\n-from HttpClientInvoker import HttpClientInvoker\n-\n-from ambari_client.ambari_api import AmbariClient\n-import unittest\n-\n-\n-class TestStatusModel(unittest.TestCase):\n-\n-  def setUp(self):\n-    http_client_logger = logging.getLogger()\n-    http_client_logger.info('Running test:' + self.id())\n-\n-  def create_service(self, http_client_mock=MagicMock()):\n-    http_client_mock.invoke.side_effect = HttpClientInvoker.http_client_invoke_side_effects\n-    client = AmbariClient(\"localhost\", 8080, \"admin\", \"admin\", version=1, client=http_client_mock)\n-    cluster = client.get_cluster('test1')\n-    service = cluster.get_service('GANGLIA')\n-    return service\n-\n-  def create_client(self, http_client_mock=MagicMock()):\n-    http_client_mock.invoke.side_effect = HttpClientInvoker.http_client_invoke_side_effects\n-    client = AmbariClient(\"localhost\", 8080, \"admin\", \"admin\", version=1, client=http_client_mock)\n-    return client\n-\n-  def test_get_request_path(self):\n-    http_client_mock = MagicMock()\n-\n-    expected_payload = {'ServiceInfo': {'state': 'INSTALLED'}}\n-    expected_path = '//clusters/test1/services/GANGLIA'\n-    expected_request_path = 'clusters/test1/requests/19'\n-\n-    service = self.create_service(http_client_mock)\n-    status = service.stop()\n-\n-    self.assertEqual(status.get_request_path(), expected_request_path)\n-    http_client_mock.invoke.assert_called_with('PUT', expected_path, headers=None, payload=expected_payload)\n-\n-  def test_is_error(self):\n-    error_model = StatusModel(None, 400)\n-    ok_model = StatusModel(None, 201)\n-\n-    self.assertTrue(error_model.is_error())\n-    self.assertFalse(ok_model.is_error())\n-\n-  def ADisabledtest_get_bootstrap_path(self):\n-    http_client_mock = MagicMock()\n-\n-    ssh_key = 'abc!@#$%^&*()_:\"|<>?[];\\'\\\\./'\n-    host_list = ['dev05.hortonworks.com', 'dev06.hortonworks.com']\n-    ssh_user = 'root'\n-\n-    expected_path = '//bootstrap'\n-    expected_headers = {'Content-Type': 'application/json'}\n-    expected_request = {'user': ssh_user, 'hosts': str(host_list), 'verbose': True, 'sshKey': ssh_key}\n-    expected_bootstrap_path = '/bootstrap/5'\n-    client = self.create_client(http_client_mock)\n-    resp = client.bootstrap_hosts(host_list, ssh_key, ssh_user)\n-\n-    self.assertEqual(resp.get_bootstrap_path(), expected_bootstrap_path)\n-    http_client_mock.invoke.assert_called_with('POST', expected_path, headers=expected_headers, payload=expected_request)",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/TestStatusModel.py",
                "sha": "107bd8401b7bf031c050c3d2e8d9e3740bb3b83f",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/ambariclient_bootstrap_hosts.json",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/ambariclient_bootstrap_hosts.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 5,
                "filename": "ambari-client/python-client/src/test/python/json/ambariclient_bootstrap_hosts.json",
                "patch": "@@ -1,5 +0,0 @@\n-{\n-  \"status\" : \"OK\",\n-  \"log\" : \"Running Bootstrap now.\",\n-  \"requestId\" : 5\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/ambariclient_bootstrap_hosts.json",
                "sha": "1e60bd05f72627b0c6f5df5bc28e9abe144ad3d1",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/ambariclient_get_all_clusters.json",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/ambariclient_get_all_clusters.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 12,
                "filename": "ambari-client/python-client/src/test/python/json/ambariclient_get_all_clusters.json",
                "patch": "@@ -1,12 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/clusters\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1\",\n-      \"Clusters\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"version\" : \"HDP-1.2.1\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/ambariclient_get_all_clusters.json",
                "sha": "abbf61978943956a8e15d4d5ee471531c559f44f",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/ambariclient_get_all_hosts.json",
                "changes": 77,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/ambariclient_get_all_hosts.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 77,
                "filename": "ambari-client/python-client/src/test/python/json/ambariclient_get_all_hosts.json",
                "patch": "@@ -1,77 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/hosts\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/hosts/apspal44-83\",\n-      \"Hosts\" : {\n-        \"host_name\" : \"apspal44-83\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/hosts/apspal44-84\",\n-      \"Hosts\" : {\n-        \"host_name\" : \"apspal44-84\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/hosts/apspal44-85\",\n-      \"Hosts\" : {\n-        \"host_name\" : \"apspal44-85\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/hosts/apspal44-86\",\n-      \"Hosts\" : {\n-        \"host_name\" : \"apspal44-86\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/hosts/apspal44-87\",\n-      \"Hosts\" : {\n-        \"host_name\" : \"apspal44-87\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/hosts/apspal44-88\",\n-      \"Hosts\" : {\n-        \"host_name\" : \"apspal44-88\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/hosts/apspal44-89\",\n-      \"Hosts\" : {\n-        \"host_name\" : \"apspal44-89\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/hosts/r01hn01\",\n-      \"Hosts\" : {\n-        \"host_name\" : \"r01hn01\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/hosts/r01mgt\",\n-      \"Hosts\" : {\n-        \"host_name\" : \"r01mgt\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/hosts/r01wn01\",\n-      \"Hosts\" : {\n-        \"host_name\" : \"r01wn01\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/hosts/r01wn02\",\n-      \"Hosts\" : {\n-        \"host_name\" : \"r01wn02\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/hosts/r01wn03\",\n-      \"Hosts\" : {\n-        \"host_name\" : \"r01wn03\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/ambariclient_get_all_hosts.json",
                "sha": "2aca5c483ae4b3c58a7427b65ccd35c7c3e3229f",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/ambariclient_get_components.json",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/ambariclient_get_components.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 53,
                "filename": "ambari-client/python-client/src/test/python/json/ambariclient_get_components.json",
                "patch": "@@ -1,53 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/serviceComponents?fields=*\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/serviceComponents/DATANODE\",\n-      \"StackServiceComponents\" : {\n-        \"component_category\" : \"SLAVE\",\n-        \"component_name\" : \"DATANODE\",\n-        \"is_client\" : false,\n-        \"is_master\" : false,\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/serviceComponents/HDFS_CLIENT\",\n-      \"StackServiceComponents\" : {\n-        \"component_category\" : \"CLIENT\",\n-        \"component_name\" : \"HDFS_CLIENT\",\n-        \"is_client\" : true,\n-        \"is_master\" : false,\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/serviceComponents/NAMENODE\",\n-      \"StackServiceComponents\" : {\n-        \"component_category\" : \"MASTER\",\n-        \"component_name\" : \"NAMENODE\",\n-        \"is_client\" : false,\n-        \"is_master\" : true,\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/serviceComponents/SECONDARY_NAMENODE\",\n-      \"StackServiceComponents\" : {\n-        \"component_category\" : \"MASTER\",\n-        \"component_name\" : \"SECONDARY_NAMENODE\",\n-        \"is_client\" : false,\n-        \"is_master\" : true,\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/ambariclient_get_components.json",
                "sha": "56bb1b2b5a4e241840633358ce335b88a8dcbc47",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/ambariclient_get_config.json",
                "changes": 929,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/ambariclient_get_config.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 929,
                "filename": "ambari-client/python-client/src/test/python/json/ambariclient_get_config.json",
                "patch": "@@ -1,929 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations?fields=*\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/datanode_du_reserved\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Reserved space for HDFS\",\n-        \"property_name\" : \"datanode_du_reserved\",\n-        \"property_value\" : \"1\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.access.time.precision\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"The access time for HDFS file is precise up to this value.\\n               The default value is 1 hour. Setting a value of 0 disables\\n               access times for HDFS.\\n  \",\n-        \"property_name\" : \"dfs.access.time.precision\",\n-        \"property_value\" : \"0\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.balance.bandwidthPerSec\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"\\n        Specifies the maximum amount of bandwidth that each datanode\\n        can utilize for the balancing purpose in term of\\n        the number of bytes per second.\\n  \",\n-        \"property_name\" : \"dfs.balance.bandwidthPerSec\",\n-        \"property_value\" : \"6250000\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.block.access.token.enable\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"\\nIf \\\"true\\\", access tokens are used as capabilities for accessing datanodes.\\nIf \\\"false\\\", no access tokens are checked on accessing datanodes.\\n\",\n-        \"property_name\" : \"dfs.block.access.token.enable\",\n-        \"property_value\" : \"true\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.block.size\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"The default block size for new files.\",\n-        \"property_name\" : \"dfs.block.size\",\n-        \"property_value\" : \"134217728\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.blockreport.initialDelay\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Delay for first block report in seconds.\",\n-        \"property_name\" : \"dfs.blockreport.initialDelay\",\n-        \"property_value\" : \"120\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.cluster.administrators\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"ACL for who all can view the default servlets in the HDFS\",\n-        \"property_name\" : \"dfs.cluster.administrators\",\n-        \"property_value\" : \" hdfs\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.datanode.du.pct\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"When calculating remaining space, only use this percentage of the real available space\\n\",\n-        \"property_name\" : \"dfs.datanode.du.pct\",\n-        \"property_value\" : \"0.85f\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.datanode.failed.volumes.tolerated\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Number of failed disks datanode would tolerate\",\n-        \"property_name\" : \"dfs.datanode.failed.volumes.tolerated\",\n-        \"property_value\" : \"0\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.datanode.ipc.address\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"\\nThe datanode ipc server address and port.\\nIf the port is 0 then the server will start on a free port.\\n\",\n-        \"property_name\" : \"dfs.datanode.ipc.address\",\n-        \"property_value\" : \"0.0.0.0:8010\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.datanode.max.xcievers\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"PRIVATE CONFIG VARIABLE\",\n-        \"property_name\" : \"dfs.datanode.max.xcievers\",\n-        \"property_value\" : \"4096\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.datanode.socket.write.timeout\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"DFS Client write socket timeout\",\n-        \"property_name\" : \"dfs.datanode.socket.write.timeout\",\n-        \"property_value\" : \"0\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.heartbeat.interval\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Determines datanode heartbeat interval in seconds.\",\n-        \"property_name\" : \"dfs.heartbeat.interval\",\n-        \"property_value\" : \"3\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.https.port\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"The https port where namenode binds\",\n-        \"property_name\" : \"dfs.https.port\",\n-        \"property_value\" : \"50470\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.namenode.avoid.read.stale.datanode\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"\\n      Indicate whether or not to avoid reading from stale datanodes whose\\n      heartbeat messages have not been received by the namenode for more than a\\n      specified time interval.\\n    \",\n-        \"property_name\" : \"dfs.namenode.avoid.read.stale.datanode\",\n-        \"property_value\" : \"true\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.namenode.avoid.write.stale.datanode\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"\\n      Indicate whether or not to avoid writing to stale datanodes whose\\n      heartbeat messages have not been received by the namenode for more than a\\n      specified time interval.\\n    \",\n-        \"property_name\" : \"dfs.namenode.avoid.write.stale.datanode\",\n-        \"property_value\" : \"true\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.namenode.handler.count\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Added to grow Queue size so that more client connections are allowed\",\n-        \"property_name\" : \"dfs.namenode.handler.count\",\n-        \"property_value\" : \"100\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.namenode.handler.count\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"The number of server threads for the namenode.\",\n-        \"property_name\" : \"dfs.namenode.handler.count\",\n-        \"property_value\" : \"40\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.namenode.stale.datanode.interval\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Datanode is stale after not getting a heartbeat in this interval in ms\",\n-        \"property_name\" : \"dfs.namenode.stale.datanode.interval\",\n-        \"property_value\" : \"30000\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.namenode.write.stale.datanode.ratio\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"When the ratio of number stale datanodes to total datanodes marked is greater\\n      than this ratio, stop avoiding writing to stale nodes so as to prevent causing hotspots.\\n    \",\n-        \"property_name\" : \"dfs.namenode.write.stale.datanode.ratio\",\n-        \"property_value\" : \"1.0f\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.permissions\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"\\nIf \\\"true\\\", enable permission checking in HDFS.\\nIf \\\"false\\\", permission checking is turned off,\\nbut all other behavior is unchanged.\\nSwitching from one parameter value to the other does not change the mode,\\nowner or group of files or directories.\\n\",\n-        \"property_name\" : \"dfs.permissions\",\n-        \"property_value\" : \"true\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.permissions.supergroup\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"The name of the group of super-users.\",\n-        \"property_name\" : \"dfs.permissions.supergroup\",\n-        \"property_value\" : \"hdfs\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.replication.max\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Maximal block replication.\\n  \",\n-        \"property_name\" : \"dfs.replication.max\",\n-        \"property_value\" : \"50\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.safemode.threshold.pct\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"\\n        Specifies the percentage of blocks that should satisfy\\n        the minimal replication requirement defined by dfs.replication.min.\\n        Values less than or equal to 0 mean not to start in safe mode.\\n        Values greater than 1 will make safe mode permanent.\\n        \",\n-        \"property_name\" : \"dfs.safemode.threshold.pct\",\n-        \"property_value\" : \"1.0f\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.secondary.https.port\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"The https port where secondary-namenode binds\",\n-        \"property_name\" : \"dfs.secondary.https.port\",\n-        \"property_value\" : \"50490\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.umaskmode\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"\\nThe octal umask used when creating files and directories.\\n\",\n-        \"property_name\" : \"dfs.umaskmode\",\n-        \"property_value\" : \"077\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs.web.ugi\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"The user account used by the web interface.\\nSyntax: USERNAME,GROUP1,GROUP2, ...\\n\",\n-        \"property_name\" : \"dfs.web.ugi\",\n-        \"property_value\" : \"gopher,gopher\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs_block_local_path_access_user\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Default Block Replication.\",\n-        \"property_name\" : \"dfs_block_local_path_access_user\",\n-        \"property_value\" : \"hbase\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs_data_dir\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Data directories for Data Nodes.\",\n-        \"property_name\" : \"dfs_data_dir\",\n-        \"property_value\" : \"/hadoop/hdfs/data\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs_datanode_address\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Port for datanode address.\",\n-        \"property_name\" : \"dfs_datanode_address\",\n-        \"property_value\" : \"50010\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs_datanode_data_dir_perm\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Datanode dir perms.\",\n-        \"property_name\" : \"dfs_datanode_data_dir_perm\",\n-        \"property_value\" : \"750\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs_datanode_failed_volume_tolerated\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"DataNode volumes failure toleration\",\n-        \"property_name\" : \"dfs_datanode_failed_volume_tolerated\",\n-        \"property_value\" : \"0\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs_datanode_http_address\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Port for datanode address.\",\n-        \"property_name\" : \"dfs_datanode_http_address\",\n-        \"property_value\" : \"50075\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs_name_dir\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"NameNode Directories.\",\n-        \"property_name\" : \"dfs_name_dir\",\n-        \"property_value\" : \"/hadoop/hdfs/namenode\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs_replication\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Default Block Replication.\",\n-        \"property_name\" : \"dfs_replication\",\n-        \"property_value\" : \"3\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dfs_webhdfs_enabled\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"WebHDFS enabled\",\n-        \"property_name\" : \"dfs_webhdfs_enabled\",\n-        \"property_value\" : \"true\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/dtnode_heapsize\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"DataNode maximum Java heap size\",\n-        \"property_name\" : \"dtnode_heapsize\",\n-        \"property_value\" : \"1024\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/fs.checkpoint.edits.dir\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Determines where on the local filesystem the DFS secondary\\n        name node should store the temporary edits to merge.\\n        If this is a comma-delimited list of directoires then teh edits is\\n        replicated in all of the directoires for redundancy.\\n        Default value is same as fs.checkpoint.dir\\n    \",\n-        \"property_name\" : \"fs.checkpoint.edits.dir\",\n-        \"property_value\" : \"${fs.checkpoint.dir}\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"core-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/fs.checkpoint.period\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"The number of seconds between two periodic checkpoints.\\n  \",\n-        \"property_name\" : \"fs.checkpoint.period\",\n-        \"property_value\" : \"21600\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"core-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/fs.checkpoint.size\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"The size of the current edit log (in bytes) that triggers\\n       a periodic checkpoint even if the fs.checkpoint.period hasn't expired.\\n  \",\n-        \"property_name\" : \"fs.checkpoint.size\",\n-        \"property_value\" : \"536870912\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"core-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/fs.trash.interval\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Number of minutes between trash checkpoints.\\n  If zero, the trash feature is disabled.\\n  \",\n-        \"property_name\" : \"fs.trash.interval\",\n-        \"property_value\" : \"360\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"core-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/fs_checkpoint_dir\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Secondary NameNode checkpoint dir.\",\n-        \"property_name\" : \"fs_checkpoint_dir\",\n-        \"property_value\" : \"/hadoop/hdfs/namesecondary\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/fs_checkpoint_period\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"HDFS Maximum Checkpoint Delay\",\n-        \"property_name\" : \"fs_checkpoint_period\",\n-        \"property_value\" : \"21600\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/fs_checkpoint_size\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"FS Checkpoint Size.\",\n-        \"property_name\" : \"fs_checkpoint_size\",\n-        \"property_value\" : \"0.5\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/hadoop.security.authentication\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"\\n   Set the authentication for the cluster. Valid values are: simple or\\n   kerberos.\\n   \",\n-        \"property_name\" : \"hadoop.security.authentication\",\n-        \"property_value\" : \"simple\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"core-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/hadoop_heapsize\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Hadoop maximum Java heap size\",\n-        \"property_name\" : \"hadoop_heapsize\",\n-        \"property_value\" : \"1024\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/hadoop_pid_dir_prefix\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Hadoop PID Dir Prefix\",\n-        \"property_name\" : \"hadoop_pid_dir_prefix\",\n-        \"property_value\" : \"/var/run/hadoop\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/hdfs_log_dir_prefix\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Hadoop Log Dir Prefix\",\n-        \"property_name\" : \"hdfs_log_dir_prefix\",\n-        \"property_value\" : \"/var/log/hadoop\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/hdfs_user\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"User and Groups.\",\n-        \"property_name\" : \"hdfs_user\",\n-        \"property_value\" : \"hdfs\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/io.compression.codec.lzo.class\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"The implementation for lzo codec.\",\n-        \"property_name\" : \"io.compression.codec.lzo.class\",\n-        \"property_value\" : \"com.hadoop.compression.lzo.LzoCodec\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"core-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/io.compression.codecs\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"A list of the compression codec classes that can be used\\n                 for compression/decompression.\",\n-        \"property_name\" : \"io.compression.codecs\",\n-        \"property_value\" : \"org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec,com.hadoop.compression.lzo.LzoCodec,com.hadoop.compression.lzo.LzopCodec,org.apache.hadoop.io.compress.SnappyCodec\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"core-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/io.file.buffer.size\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"The size of buffer for use in sequence files.\\n  The size of this buffer should probably be a multiple of hardware\\n  page size (4096 on Intel x86), and it determines how much data is\\n  buffered during read and write operations.\",\n-        \"property_name\" : \"io.file.buffer.size\",\n-        \"property_value\" : \"131072\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"core-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/io.serializations\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : null,\n-        \"property_name\" : \"io.serializations\",\n-        \"property_value\" : \"org.apache.hadoop.io.serializer.WritableSerialization\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"core-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/ipc.client.connect.max.retries\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Defines the maximum number of retries for IPC connections.\",\n-        \"property_name\" : \"ipc.client.connect.max.retries\",\n-        \"property_value\" : \"50\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"core-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/ipc.client.connection.maxidletime\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"The maximum time after which a client will bring down the\\n               connection to the server.\\n  \",\n-        \"property_name\" : \"ipc.client.connection.maxidletime\",\n-        \"property_value\" : \"30000\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"core-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/ipc.client.idlethreshold\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Defines the threshold number of connections after which\\n               connections will be inspected for idleness.\\n  \",\n-        \"property_name\" : \"ipc.client.idlethreshold\",\n-        \"property_value\" : \"8000\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"core-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/ipc.server.max.response.size\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : null,\n-        \"property_name\" : \"ipc.server.max.response.size\",\n-        \"property_value\" : \"5242880\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/ipc.server.read.threadpool.size\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : null,\n-        \"property_name\" : \"ipc.server.read.threadpool.size\",\n-        \"property_value\" : \"5\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hdfs-site.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/kerberos_domain\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Kerberos realm.\",\n-        \"property_name\" : \"kerberos_domain\",\n-        \"property_value\" : \"EXAMPLE.COM\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/keytab_path\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Kerberos keytab path.\",\n-        \"property_name\" : \"keytab_path\",\n-        \"property_value\" : \"/etc/security/keytabs\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/namenode_formatted_mark_dir\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Formatteed Mark Directory.\",\n-        \"property_name\" : \"namenode_formatted_mark_dir\",\n-        \"property_value\" : \"/var/run/hadoop/hdfs/namenode/formatted/\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/namenode_heapsize\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"NameNode Java heap size\",\n-        \"property_name\" : \"namenode_heapsize\",\n-        \"property_value\" : \"1024\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/namenode_opt_maxnewsize\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"NameNode maximum new generation size\",\n-        \"property_name\" : \"namenode_opt_maxnewsize\",\n-        \"property_value\" : \"640\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/namenode_opt_newsize\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"NameNode new generation size\",\n-        \"property_name\" : \"namenode_opt_newsize\",\n-        \"property_value\" : \"200\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/namenode_opt_maxpermsize\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"NameNode maximum permanent generation size\",\n-        \"property_name\" : \"namenode_opt_maxpermsize\",\n-        \"property_value\" : \"256\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/namenode_opt_permsize\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"NameNode permanent generation size\",\n-        \"property_name\" : \"namenode_opt_permsize\",\n-        \"property_value\" : \"128\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/proxyuser_group\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Proxy user group.\",\n-        \"property_name\" : \"proxyuser_group\",\n-        \"property_value\" : \"users\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/security.client.datanode.protocol.acl\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"ACL for ClientDatanodeProtocol, the client-to-datanode protocol\\n    for block recovery.\\n    The ACL is a comma-separated list of user and group names. The user and\\n    group list is separated by a blank. For e.g. \\\"alice,bob users,wheel\\\".\\n    A special value of \\\"*\\\" means all users are allowed.\",\n-        \"property_name\" : \"security.client.datanode.protocol.acl\",\n-        \"property_value\" : \"*\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hadoop-policy.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/security.client.protocol.acl\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"ACL for ClientProtocol, which is used by user code\\n    via the DistributedFileSystem.\\n    The ACL is a comma-separated list of user and group names. The user and\\n    group list is separated by a blank. For e.g. \\\"alice,bob users,wheel\\\".\\n    A special value of \\\"*\\\" means all users are allowed.\",\n-        \"property_name\" : \"security.client.protocol.acl\",\n-        \"property_value\" : \"*\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hadoop-policy.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/security.datanode.protocol.acl\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"ACL for DatanodeProtocol, which is used by datanodes to\\n    communicate with the namenode.\\n    The ACL is a comma-separated list of user and group names. The user and\\n    group list is separated by a blank. For e.g. \\\"alice,bob users,wheel\\\".\\n    A special value of \\\"*\\\" means all users are allowed.\",\n-        \"property_name\" : \"security.datanode.protocol.acl\",\n-        \"property_value\" : \"*\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hadoop-policy.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/security.inter.datanode.protocol.acl\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"ACL for InterDatanodeProtocol, the inter-datanode protocol\\n    for updating generation timestamp.\\n    The ACL is a comma-separated list of user and group names. The user and\\n    group list is separated by a blank. For e.g. \\\"alice,bob users,wheel\\\".\\n    A special value of \\\"*\\\" means all users are allowed.\",\n-        \"property_name\" : \"security.inter.datanode.protocol.acl\",\n-        \"property_value\" : \"*\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hadoop-policy.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/security.inter.tracker.protocol.acl\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"ACL for InterTrackerProtocol, used by the tasktrackers to\\n    communicate with the jobtracker.\\n    The ACL is a comma-separated list of user and group names. The user and\\n    group list is separated by a blank. For e.g. \\\"alice,bob users,wheel\\\".\\n    A special value of \\\"*\\\" means all users are allowed.\",\n-        \"property_name\" : \"security.inter.tracker.protocol.acl\",\n-        \"property_value\" : \"*\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hadoop-policy.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/security.job.submission.protocol.acl\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"ACL for JobSubmissionProtocol, used by job clients to\\n    communciate with the jobtracker for job submission, querying job status etc.\\n    The ACL is a comma-separated list of user and group names. The user and\\n    group list is separated by a blank. For e.g. \\\"alice,bob users,wheel\\\".\\n    A special value of \\\"*\\\" means all users are allowed.\",\n-        \"property_name\" : \"security.job.submission.protocol.acl\",\n-        \"property_value\" : \"*\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hadoop-policy.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/security.namenode.protocol.acl\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"ACL for NamenodeProtocol, the protocol used by the secondary\\n    namenode to communicate with the namenode.\\n    The ACL is a comma-separated list of user and group names. The user and\\n    group list is separated by a blank. For e.g. \\\"alice,bob users,wheel\\\".\\n    A special value of \\\"*\\\" means all users are allowed.\",\n-        \"property_name\" : \"security.namenode.protocol.acl\",\n-        \"property_value\" : \"*\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hadoop-policy.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/security.task.umbilical.protocol.acl\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"ACL for TaskUmbilicalProtocol, used by the map and reduce\\n    tasks to communicate with the parent tasktracker.\\n    The ACL is a comma-separated list of user and group names. The user and\\n    group list is separated by a blank. For e.g. \\\"alice,bob users,wheel\\\".\\n    A special value of \\\"*\\\" means all users are allowed.\",\n-        \"property_name\" : \"security.task.umbilical.protocol.acl\",\n-        \"property_value\" : \"*\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"hadoop-policy.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/security_enabled\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \"Hadoop Security\",\n-        \"property_name\" : \"security_enabled\",\n-        \"property_value\" : \"false\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"global.xml\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations/webinterface.private.actions\",\n-      \"StackConfigurations\" : {\n-        \"property_description\" : \" If set to true, the web interfaces of JT and NN may contain\\n                actions, such as kill job, delete file, etc., that should\\n                not be exposed to public. Enable this option if the interfaces\\n                are only reachable by those who have the right authorization.\\n  \",\n-        \"property_name\" : \"webinterface.private.actions\",\n-        \"property_value\" : \"false\",\n-        \"service_name\" : \"HDFS\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"1.3.0\",\n-        \"type\" : \"core-site.xml\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/ambariclient_get_config.json",
                "sha": "38bba47100cfe2ae30be1bb4c7518e04622de72b",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/ambariclient_get_host.json",
                "changes": 63,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/ambariclient_get_host.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 63,
                "filename": "ambari-client/python-client/src/test/python/json/ambariclient_get_host.json",
                "patch": "@@ -1,63 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/hosts/dev06.hortonworks.com\",\n-  \"Hosts\" : {\n-    \"cpu_count\" : 4,\n-    \"desired_configs\" : null,\n-    \"disk_info\" : [\n-      {\n-        \"available\" : \"45333752\",\n-        \"used\" : \"5748252\",\n-        \"percent\" : \"12%\",\n-        \"size\" : \"51606140\",\n-        \"type\" : \"ext4\",\n-        \"mountpoint\" : \"/\"\n-      },\n-      {\n-        \"available\" : \"5517976\",\n-        \"used\" : \"272\",\n-        \"percent\" : \"1%\",\n-        \"size\" : \"5518248\",\n-        \"type\" : \"tmpfs\",\n-        \"mountpoint\" : \"/dev/shm\"\n-      },\n-      {\n-        \"available\" : \"432210\",\n-        \"used\" : \"38034\",\n-        \"percent\" : \"9%\",\n-        \"size\" : \"495844\",\n-        \"type\" : \"ext4\",\n-        \"mountpoint\" : \"/boot\"\n-      },\n-      {\n-        \"available\" : \"44459840\",\n-        \"used\" : \"184252\",\n-        \"percent\" : \"1%\",\n-        \"size\" : \"47033288\",\n-        \"type\" : \"ext4\",\n-        \"mountpoint\" : \"/home\"\n-      },\n-      {\n-        \"available\" : \"136400692\",\n-        \"used\" : \"840256712\",\n-        \"percent\" : \"87%\",\n-        \"size\" : \"976657404\",\n-        \"type\" : \"vboxsf\",\n-        \"mountpoint\" : \"/media/sf_share\"\n-      }\n-    ],\n-    \"host_health_report\" : \"\",\n-    \"host_name\" : \"dev06.hortonworks.com\",\n-    \"host_state\" : \"HEARTBEAT_LOST\",\n-    \"host_status\" : \"UNKNOWN\",\n-    \"ip\" : \"10.0.2.15\",\n-    \"last_agent_env\" : null,\n-    \"last_heartbeat_time\" : 0,\n-    \"last_registration_time\" : 1378228232506,\n-    \"os_arch\" : \"x86_64\",\n-    \"os_type\" : \"centos6\",\n-    \"ph_cpu_count\" : 1,\n-    \"public_host_name\" : \"dev06.hortonworks.com\",\n-    \"rack_info\" : \"/default-rack\",\n-    \"total_mem\" : 11041505\n-  }\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/ambariclient_get_host.json",
                "sha": "d7106266ab5f263ee7c6a12349741b1f33e6f33b",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_error_deleting_host.json",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/clustermodel_error_deleting_host.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 4,
                "filename": "ambari-client/python-client/src/test/python/json/clustermodel_error_deleting_host.json",
                "patch": "@@ -1,4 +0,0 @@\n-{\n-  \"status\" : 400,\n-  \"message\" : \"Attempted to add unknown hosts to a cluster.  These hosts have not been registered with the server: dev05\"\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_error_deleting_host.json",
                "sha": "06d1bc982a60a063e44c29d69c81bbce838a5f7e",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_get_all_hosts.json",
                "changes": 375,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/clustermodel_get_all_hosts.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 375,
                "filename": "ambari-client/python-client/src/test/python/json/clustermodel_get_all_hosts.json",
                "patch": "@@ -1,375 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/clusters/test1/hosts?fields=*\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/hosts/dev05.hortonworks.com\",\n-      \"Hosts\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"cpu_count\" : 4,\n-        \"disk_info\" : [\n-          {\n-            \"available\" : \"43515968\",\n-            \"used\" : \"7566036\",\n-            \"percent\" : \"15%\",\n-            \"size\" : \"51606140\",\n-            \"type\" : \"ext4\",\n-            \"mountpoint\" : \"/\"\n-          },\n-          {\n-            \"available\" : \"5517988\",\n-            \"used\" : \"260\",\n-            \"percent\" : \"1%\",\n-            \"size\" : \"5518248\",\n-            \"type\" : \"tmpfs\",\n-            \"mountpoint\" : \"/dev/shm\"\n-          },\n-          {\n-            \"available\" : \"432210\",\n-            \"used\" : \"38034\",\n-            \"percent\" : \"9%\",\n-            \"size\" : \"495844\",\n-            \"type\" : \"ext4\",\n-            \"mountpoint\" : \"/boot\"\n-          },\n-          {\n-            \"available\" : \"44459872\",\n-            \"used\" : \"184220\",\n-            \"percent\" : \"1%\",\n-            \"size\" : \"47033288\",\n-            \"type\" : \"ext4\",\n-            \"mountpoint\" : \"/home\"\n-          },\n-          {\n-            \"available\" : \"57923596\",\n-            \"used\" : \"918733808\",\n-            \"percent\" : \"95%\",\n-            \"size\" : \"976657404\",\n-            \"type\" : \"vboxsf\",\n-            \"mountpoint\" : \"/media/sf_share\"\n-          }\n-        ],\n-        \"host_health_report\" : \"\",\n-        \"host_name\" : \"dev05.hortonworks.com\",\n-        \"host_state\" : \"HEALTHY\",\n-        \"host_status\" : \"HEALTHY\",\n-        \"ip\" : \"10.0.2.15\",\n-        \"last_agent_env\" : {\n-          \"stackFoldersAndFiles\" : [\n-            {\n-              \"name\" : \"/etc/hadoop\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/etc/hbase\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/etc/hcatalog\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/etc/hive\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/etc/ganglia\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/etc/nagios\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/etc/oozie\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/etc/zookeeper\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/var/run/hive\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/var/run/hadoop-hdfs\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/var/run/hadoop-yarn\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/var/run/hadoop-mapreduce\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/var/log/hbase\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/var/log/hive\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/var/log/oozie\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/var/log/zookeeper\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/var/log/hadoop-hdfs\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/var/log/hadoop-yarn\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/var/log/hadoop-mapreduce\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/usr/lib/hadoop\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/usr/lib/hbase\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/usr/lib/hive\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/usr/lib/nagios\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/usr/lib/oozie\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/usr/lib/zookeeper\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/usr/lib/hadoop-hdfs\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/usr/lib/hadoop-yarn\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/usr/lib/hadoop-mapreduce\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/var/lib/ganglia\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/var/lib/oozie\",\n-              \"type\" : \"directory\"\n-            },\n-            {\n-              \"name\" : \"/var/lib/zookeeper\",\n-              \"type\" : \"directory\"\n-            }\n-          ],\n-          \"rpms\" : [ ],\n-          \"alternatives\" : [\n-            {\n-              \"name\" : \"hcatalog-conf\",\n-              \"target\" : \"/etc/hcatalog/conf.dist\"\n-            },\n-            {\n-              \"name\" : \"zookeeper-conf\",\n-              \"target\" : \"/etc/zookeeper/conf.dist\"\n-            },\n-            {\n-              \"name\" : \"hadoop-conf\",\n-              \"target\" : \"/etc/hadoop/conf.empty\"\n-            },\n-            {\n-              \"name\" : \"hbase-conf\",\n-              \"target\" : \"/etc/hbase/conf.dist\"\n-            },\n-            {\n-              \"name\" : \"hive-conf\",\n-              \"target\" : \"/etc/hive/conf.dist\"\n-            },\n-            {\n-              \"name\" : \"oozie-conf\",\n-              \"target\" : \"/etc/oozie/conf.dist\"\n-            }\n-          ],\n-          \"existingUsers\" : [\n-            {\n-              \"userName\" : \"rrdcached\",\n-              \"userHomeDir\" : \"/var/rrdtool/rrdcached\",\n-              \"userStatus\" : \"Available\"\n-            },\n-            {\n-              \"userName\" : \"zookeeper\",\n-              \"userHomeDir\" : \"/var/run/zookeeper\",\n-              \"userStatus\" : \"Available\"\n-            },\n-            {\n-              \"userName\" : \"hdfs\",\n-              \"userHomeDir\" : \"/var/lib/hadoop-hdfs\",\n-              \"userStatus\" : \"Available\"\n-            },\n-            {\n-              \"userName\" : \"hbase\",\n-              \"userHomeDir\" : \"/var/run/hbase\",\n-              \"userStatus\" : \"Available\"\n-            },\n-            {\n-              \"userName\" : \"yarn\",\n-              \"userHomeDir\" : \"/var/lib/hadoop-yarn\",\n-              \"userStatus\" : \"Available\"\n-            },\n-            {\n-              \"userName\" : \"mapred\",\n-              \"userHomeDir\" : \"/var/lib/hadoop-mapreduce\",\n-              \"userStatus\" : \"Available\"\n-            },\n-            {\n-              \"userName\" : \"hive\",\n-              \"userHomeDir\" : \"/var/lib/hive\",\n-              \"userStatus\" : \"Available\"\n-            },\n-            {\n-              \"userName\" : \"hcat\",\n-              \"userHomeDir\" : \"/usr/lib/hcatalog\",\n-              \"userStatus\" : \"Available\"\n-            },\n-            {\n-              \"userName\" : \"oozie\",\n-              \"userHomeDir\" : \"/var/run/oozie\",\n-              \"userStatus\" : \"Available\"\n-            },\n-            {\n-              \"userName\" : \"nagios\",\n-              \"userHomeDir\" : \"/var/log/nagios\",\n-              \"userStatus\" : \"Available\"\n-            }\n-          ],\n-          \"existingRepos\" : [\n-            \"HDP-2.0.5\"\n-          ],\n-          \"installedPackages\" : [ ],\n-          \"hostHealth\" : {\n-            \"activeJavaProcs\" : [\n-              {\n-                \"user\" : \"root\",\n-                \"pid\" : 2716,\n-                \"command\" : \"/usr/bin/java -Dosgi.requiredJavaVersion=1.6 -XX:MaxPermSize=256m -Xms40m -Xmx512m -jar /usr/lib/eclipse//plugins/org.eclipse.equinox.launcher_1.3.0.v20130327-1440.jar -os linux -ws gtk -arch x86_64 -showsplash /usr/lib/eclipse//plugins/org.eclipse.platform_4.3.0.v20130605-2000/splash.bmp -launcher /usr/lib/eclipse/eclipse -name Eclipse --launcher.library /usr/lib/eclipse//plugins/org.eclipse.equinox.launcher.gtk.linux.x86_64_1.1.200.v20130521-0416/eclipse_1506.so -startup /usr/lib/eclipse//plugins/org.eclipse.equinox.launcher_1.3.0.v20130327-1440.jar --launcher.appendVmargs -exitdata 8000d -product org.eclipse.epp.package.jee.product -vm /usr/bin/java -vmargs -Dosgi.requiredJavaVersion=1.6 -XX:MaxPermSize=256m -Xms40m -Xmx512m -jar /usr/lib/eclipse//plugins/org.eclipse.equinox.launcher_1.3.0.v20130327-1440.jar\",\n-                \"hadoop\" : false\n-              }\n-            ],\n-            \"agentTimeStampAtReporting\" : 1379683509805,\n-            \"serverTimeStampAtReporting\" : 1379683509851,\n-            \"liveServices\" : [\n-              {\n-                \"name\" : \"ntpd\",\n-                \"desc\" : \"\",\n-                \"status\" : \"Healthy\"\n-              }\n-            ],\n-            \"diskStatus\" : [\n-              {\n-                \"available\" : \"43516404\",\n-                \"used\" : \"7565600\",\n-                \"percent\" : \"15%\",\n-                \"size\" : \"51606140\",\n-                \"type\" : \"ext4\",\n-                \"mountpoint\" : \"/\"\n-              }\n-            ]\n-          },\n-          \"umask\" : 18\n-        },\n-        \"last_heartbeat_time\" : 1379683560126,\n-        \"last_registration_time\" : 1379683395402,\n-        \"os_arch\" : \"x86_64\",\n-        \"os_type\" : \"centos6\",\n-        \"ph_cpu_count\" : 1,\n-        \"public_host_name\" : \"dev05.hortonworks.com\",\n-        \"rack_info\" : \"/default-rack\",\n-        \"total_mem\" : 11041505,\n-        \"desired_configs\" : { }\n-      },\n-      \"host_components\" : [ ]\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/hosts/dev06.hortonworks.com\",\n-      \"Hosts\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"cpu_count\" : 4,\n-        \"disk_info\" : [\n-          {\n-            \"available\" : \"45333752\",\n-            \"used\" : \"5748252\",\n-            \"percent\" : \"12%\",\n-            \"size\" : \"51606140\",\n-            \"type\" : \"ext4\",\n-            \"mountpoint\" : \"/\"\n-          },\n-          {\n-            \"available\" : \"5517976\",\n-            \"used\" : \"272\",\n-            \"percent\" : \"1%\",\n-            \"size\" : \"5518248\",\n-            \"type\" : \"tmpfs\",\n-            \"mountpoint\" : \"/dev/shm\"\n-          },\n-          {\n-            \"available\" : \"432210\",\n-            \"used\" : \"38034\",\n-            \"percent\" : \"9%\",\n-            \"size\" : \"495844\",\n-            \"type\" : \"ext4\",\n-            \"mountpoint\" : \"/boot\"\n-          },\n-          {\n-            \"available\" : \"44459840\",\n-            \"used\" : \"184252\",\n-            \"percent\" : \"1%\",\n-            \"size\" : \"47033288\",\n-            \"type\" : \"ext4\",\n-            \"mountpoint\" : \"/home\"\n-          },\n-          {\n-            \"available\" : \"136400692\",\n-            \"used\" : \"840256712\",\n-            \"percent\" : \"87%\",\n-            \"size\" : \"976657404\",\n-            \"type\" : \"vboxsf\",\n-            \"mountpoint\" : \"/media/sf_share\"\n-          }\n-        ],\n-        \"host_health_report\" : \"\",\n-        \"host_name\" : \"dev06.hortonworks.com\",\n-        \"host_state\" : \"HEARTBEAT_LOST\",\n-        \"host_status\" : \"UNKNOWN\",\n-        \"ip\" : \"10.0.2.15\",\n-        \"last_agent_env\" : null,\n-        \"last_heartbeat_time\" : 0,\n-        \"last_registration_time\" : 1378228232506,\n-        \"os_arch\" : \"x86_64\",\n-        \"os_type\" : \"centos6\",\n-        \"ph_cpu_count\" : 1,\n-        \"public_host_name\" : \"dev06.hortonworks.com\",\n-        \"rack_info\" : \"/default-rack\",\n-        \"total_mem\" : 11041505,\n-        \"desired_configs\" : { }\n-      },\n-      \"host_components\" : [ ]\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_get_all_hosts.json",
                "sha": "614f8d45e9307797dc82f99306c05df9ef1ad160",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_get_all_services.json",
                "changes": 120,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/clustermodel_get_all_services.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 120,
                "filename": "ambari-client/python-client/src/test/python/json/clustermodel_get_all_services.json",
                "patch": "@@ -1,120 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/clusters/test1/services?fields=*\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/services/GANGLIA\",\n-      \"ServiceInfo\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"state\" : \"STARTED\",\n-        \"service_name\" : \"GANGLIA\",\n-        \"desired_configs\" : {\n-          \"global\" : \"version1\"\n-        }\n-      },\n-      \"components\" : [\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test1/services/GANGLIA/components/GANGLIA_MONITOR\",\n-          \"ServiceComponentInfo\" : {\n-            \"cluster_name\" : \"test1\",\n-            \"component_name\" : \"GANGLIA_MONITOR\",\n-            \"service_name\" : \"GANGLIA\"\n-          }\n-        },\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test1/services/GANGLIA/components/GANGLIA_SERVER\",\n-          \"ServiceComponentInfo\" : {\n-            \"cluster_name\" : \"test1\",\n-            \"component_name\" : \"GANGLIA_SERVER\",\n-            \"service_name\" : \"GANGLIA\"\n-          }\n-        }\n-      ]\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/services/MAPREDUCE\",\n-      \"ServiceInfo\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"state\" : \"STARTED\",\n-        \"service_name\" : \"MAPREDUCE\",\n-        \"desired_configs\" : {\n-          \"mapred-site\" : \"version1\",\n-          \"global\" : \"version1\",\n-          \"core-site\" : \"version1\"\n-        }\n-      },\n-      \"components\" : [\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test1/services/MAPREDUCE/components/TASKTRACKER\",\n-          \"ServiceComponentInfo\" : {\n-            \"cluster_name\" : \"test1\",\n-            \"component_name\" : \"TASKTRACKER\",\n-            \"service_name\" : \"MAPREDUCE\"\n-          }\n-        },\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test1/services/MAPREDUCE/components/MAPREDUCE_CLIENT\",\n-          \"ServiceComponentInfo\" : {\n-            \"cluster_name\" : \"test1\",\n-            \"component_name\" : \"MAPREDUCE_CLIENT\",\n-            \"service_name\" : \"MAPREDUCE\"\n-          }\n-        },\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test1/services/MAPREDUCE/components/JOBTRACKER\",\n-          \"ServiceComponentInfo\" : {\n-            \"cluster_name\" : \"test1\",\n-            \"component_name\" : \"JOBTRACKER\",\n-            \"service_name\" : \"MAPREDUCE\"\n-          }\n-        }\n-      ]\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/services/HDFS\",\n-      \"ServiceInfo\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"state\" : \"STARTED\",\n-        \"service_name\" : \"HDFS\",\n-        \"desired_configs\" : {\n-          \"global\" : \"version1\",\n-          \"hdfs-site\" : \"version1\",\n-          \"core-site\" : \"version1\"\n-        }\n-      },\n-      \"components\" : [\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test1/services/HDFS/components/SECONDARY_NAMENODE\",\n-          \"ServiceComponentInfo\" : {\n-            \"cluster_name\" : \"test1\",\n-            \"component_name\" : \"SECONDARY_NAMENODE\",\n-            \"service_name\" : \"HDFS\"\n-          }\n-        },\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test1/services/HDFS/components/HDFS_CLIENT\",\n-          \"ServiceComponentInfo\" : {\n-            \"cluster_name\" : \"test1\",\n-            \"component_name\" : \"HDFS_CLIENT\",\n-            \"service_name\" : \"HDFS\"\n-          }\n-        },\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test1/services/HDFS/components/NAMENODE\",\n-          \"ServiceComponentInfo\" : {\n-            \"cluster_name\" : \"test1\",\n-            \"component_name\" : \"NAMENODE\",\n-            \"service_name\" : \"HDFS\"\n-          }\n-        },\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test1/services/HDFS/components/DATANODE\",\n-          \"ServiceComponentInfo\" : {\n-            \"cluster_name\" : \"test1\",\n-            \"component_name\" : \"DATANODE\",\n-            \"service_name\" : \"HDFS\"\n-          }\n-        }\n-      ]\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_get_all_services.json",
                "sha": "53bb2daa99ad5b9e579341cdf4d5d251b0ea3c8a",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_get_cluster.json",
                "changes": 103,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/clustermodel_get_cluster.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 103,
                "filename": "ambari-client/python-client/src/test/python/json/clustermodel_get_cluster.json",
                "patch": "@@ -1,103 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/clusters/test1\",\n-  \"Clusters\" : {\n-    \"cluster_name\" : \"test1\",\n-    \"cluster_id\" : 1,\n-    \"version\" : \"HDP-1.2.1\"\n-  },\n-  \"requests\" : [ ],\n-  \"services\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/services/GANGLIA\",\n-      \"ServiceInfo\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"service_name\" : \"GANGLIA\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/services/MAPREDUCE\",\n-      \"ServiceInfo\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"service_name\" : \"MAPREDUCE\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/services/HDFS\",\n-      \"ServiceInfo\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"service_name\" : \"HDFS\"\n-      }\n-    }\n-  ],\n-  \"hosts\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/hosts/r01mgt\",\n-      \"Hosts\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"host_name\" : \"r01mgt\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/hosts/r01hn01\",\n-      \"Hosts\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"host_name\" : \"r01hn01\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/hosts/r01wn03\",\n-      \"Hosts\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"host_name\" : \"r01wn03\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/hosts/r01wn02\",\n-      \"Hosts\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"host_name\" : \"r01wn02\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/hosts/r01wn01\",\n-      \"Hosts\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"host_name\" : \"r01wn01\"\n-      }\n-    }\n-  ],\n-  \"configurations\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/configurations?type=mapred-site&tag=version1\",\n-      \"tag\" : \"version1\",\n-      \"type\" : \"mapred-site\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"test1\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/configurations?type=global&tag=version1\",\n-      \"tag\" : \"version1\",\n-      \"type\" : \"global\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"test1\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/configurations?type=hdfs-site&tag=version1\",\n-      \"tag\" : \"version1\",\n-      \"type\" : \"hdfs-site\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"test1\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/configurations?type=core-site&tag=version1\",\n-      \"tag\" : \"version1\",\n-      \"type\" : \"core-site\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"test1\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_get_cluster.json",
                "sha": "4c72dea9ca2cd8e6d0905ebb1b39ef1c1b5badc0",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_get_core_site_config.json",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/clustermodel_get_core_site_config.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 32,
                "filename": "ambari-client/python-client/src/test/python/json/clustermodel_get_core_site_config.json",
                "patch": "@@ -1,32 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/clusters/test1/configurations?type=core-site&tag=version1\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/configurations?type=core-site&tag=version1\",\n-      \"tag\" : \"version1\",\n-      \"type\" : \"core-site\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"test1\"\n-      },\n-      \"properties\" : {\n-        \"dfs.namenode.checkpoint.dir\" : \"/hadoop/hdfs/namesecondary\",\n-        \"dfs.namenode.checkpoint.edits.dir\" : \"${dfs.namenode.checkpoint.dir}\",\n-        \"dfs.namenode.checkpoint.period\" : \"21600\",\n-        \"fs.checkpoint.edits.dir\" : \"/hadoop/hdfs/namesecondary\",\n-        \"fs.checkpoint.size\" : \"0.5\",\n-        \"fs.defaultFS\" : \"hdfs://dev05.hortonworks.com:8020\",\n-        \"fs.trash.interval\" : \"360\",\n-        \"hadoop.security.auth_to_local\" : \"\\n        RULE:[2:$1@$0]([rn]m@.*)s/.*/yarn/\\n        RULE:[2:$1@$0](jhs@.*)s/.*/mapred/\\n        RULE:[2:$1@$0]([nd]n@.*)s/.*/hdfs/\\n        RULE:[2:$1@$0](hm@.*)s/.*/hbase/\\n        RULE:[2:$1@$0](rs@.*)s/.*/hbase/\\n        DEFAULT\\n    \",\n-        \"hadoop.security.authentication\" : \"simple\",\n-        \"hadoop.security.authorization\" : \"false\",\n-        \"io.compression.codecs\" : \"org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.DefaultCodec\",\n-        \"io.file.buffer.size\" : \"131072\",\n-        \"io.serializations\" : \"org.apache.hadoop.io.serializer.WritableSerialization\",\n-        \"ipc.client.connect.max.retries\" : \"50\",\n-        \"ipc.client.connection.maxidletime\" : \"30000\",\n-        \"ipc.client.idlethreshold\" : \"8000\",\n-        \"mapreduce.jobtracker.webinterface.trusted\" : \"false\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_get_core_site_config.json",
                "sha": "309ba13d5e0ac2a8025df44efe8d7b4645153dc4",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_get_global_config.json",
                "changes": 65,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/clustermodel_get_global_config.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 65,
                "filename": "ambari-client/python-client/src/test/python/json/clustermodel_get_global_config.json",
                "patch": "@@ -1,65 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/clusters/test1/configurations?type=global&tag=version1\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/configurations?type=global&tag=version1\",\n-      \"tag\" : \"version1\",\n-      \"type\" : \"global\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"test1\"\n-      },\n-      \"properties\" : {\n-        \"apache_artifacts_download_url\" : \"\",\n-        \"datanode_du_reserved\" : \"1\",\n-        \"dfs_block_local_path_access_user\" : \"hbase\",\n-        \"dfs_datanode_address\" : \"50010\",\n-        \"dfs_datanode_data_dir\" : \"/hadoop/hdfs/data\",\n-        \"dfs_datanode_data_dir_perm\" : \"750\",\n-        \"dfs_datanode_failed_volume_tolerated\" : \"0\",\n-        \"dfs_datanode_http_address\" : \"50075\",\n-        \"dfs_exclude\" : \"dfs.exclude\",\n-        \"dfs_namenode_checkpoint_dir\" : \"/hadoop/hdfs/namesecondary\",\n-        \"dfs_namenode_checkpoint_period\" : \"21600\",\n-        \"dfs_namenode_name_dir\" : \"/hadoop/hdfs/namenode\",\n-        \"dfs_replication\" : \"3\",\n-        \"dfs_webhdfs_enabled\" : \"true\",\n-        \"dtnode_heapsize\" : \"1024m\",\n-        \"fs_checkpoint_size\" : \"0.5\",\n-        \"ganglia_runtime_dir\" : \"/var/run/ganglia/hdp\",\n-        \"gmetad_user\" : \"nobody\",\n-        \"gmond_user\" : \"nobody\",\n-        \"hadoop_conf_dir\" : \"/etc/hadoop/conf\",\n-        \"hadoop_heapsize\" : \"1024\",\n-        \"hadoop_pid_dir_prefix\" : \"/var/run/hadoop\",\n-        \"hbase_conf_dir\" : \"/etc/hbase\",\n-        \"hbase_user\" : \"hbase\",\n-        \"hcat_conf_dir\" : \"\",\n-        \"hcat_user\" : \"hcat\",\n-        \"hdfs_enable_shortcircuit_read\" : \"true\",\n-        \"hdfs_log_dir_prefix\" : \"/var/log/hadoop\",\n-        \"hdfs_user\" : \"hdfs\",\n-        \"hive_user\" : \"hive\",\n-        \"java64_home\" : \"/usr/jdk/jdk1.6.0_31\",\n-        \"mapred_user\" : \"mapred\",\n-        \"nagios_group\" : \"nagios\",\n-        \"nagios_user\" : \"nagios\",\n-        \"namenode_formatted_mark_dir\" : \"/var/run/hadoop/hdfs/namenode/formatted/\",\n-        \"namenode_heapsize\" : \"1024m\",\n-        \"namenode_opt_maxnewsize\" : \"640m\",\n-        \"namenode_opt_newsize\" : \"200m\",\n-        \"namenode_opt_permsize\" : \"128m\",\n-        \"namenode_opt_maxpermsize\" : \"256m\",\n-        \"oozie_user\" : \"oozie\",\n-        \"proxyuser_group\" : \"users\",\n-        \"rrdcached_base_dir\" : \"/var/lib/ganglia/rrds\",\n-        \"run_dir\" : \"/var/run/hadoop\",\n-        \"security_enabled\" : \"false\",\n-        \"smokeuser\" : \"ambari-qa\",\n-        \"user_group\" : \"hadoop\",\n-        \"webhcat_user\" : \"hcat\",\n-        \"yarn_user\" : \"yarn\",\n-        \"zk_user\" : \"zookeeper\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_get_global_config.json",
                "sha": "bb59fb582beec0949ec4c742bab59a5ea0a3cf94",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_get_hdfs_site_config.json",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/clustermodel_get_hdfs_site_config.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 53,
                "filename": "ambari-client/python-client/src/test/python/json/clustermodel_get_hdfs_site_config.json",
                "patch": "@@ -1,53 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/clusters/test1/configurations?type=hdfs-site&tag=version1\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/configurations?type=hdfs-site&tag=version1\",\n-      \"tag\" : \"version1\",\n-      \"type\" : \"hdfs-site\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"test1\"\n-      },\n-      \"properties\" : {\n-        \"dfs.block.access.token.enable\" : \"true\",\n-        \"dfs.block.local-path-access.user\" : \"hbase\",\n-        \"dfs.blockreport.initialDelay\" : \"120\",\n-        \"dfs.blocksize\" : \"134217728\",\n-        \"dfs.client.read.shortcircuit\" : \"true\",\n-        \"dfs.client.read.shortcircuit.streams.cache.size\" : \"4096\",\n-        \"dfs.cluster.administrators\" : \" hdfs\",\n-        \"dfs.datanode.address\" : \"0.0.0.0:50010\",\n-        \"dfs.datanode.balance.bandwidthPerSec\" : \"6250000\",\n-        \"dfs.datanode.data.dir\" : \"/hadoop/hdfs/data\",\n-        \"dfs.datanode.data.dir.perm\" : \"750\",\n-        \"dfs.datanode.du.reserved\" : \"1\",\n-        \"dfs.datanode.failed.volumes.tolerated\" : \"0\",\n-        \"dfs.datanode.http.address\" : \"0.0.0.0:50075\",\n-        \"dfs.datanode.ipc.address\" : \"0.0.0.0:8010\",\n-        \"dfs.datanode.max.transfer.threads\" : \"1024\",\n-        \"dfs.domain.socket.path\" : \"/var/lib/hadoop-hdfs/dn_socket\",\n-        \"dfs.heartbeat.interval\" : \"3\",\n-        \"dfs.hosts.exclude\" : \"/etc/hadoop/conf/dfs.exclude\",\n-        \"dfs.https.namenode.https-address\" : \"dev05.hortonworks.com:50470\",\n-        \"dfs.journalnode.edits.dir\" : \"/grid/0/hdfs/journal\",\n-        \"dfs.journalnode.http-address\" : \"0.0.0.0:8480\",\n-        \"dfs.namenode.accesstime.precision\" : \"0\",\n-        \"dfs.namenode.avoid.read.stale.datanode\" : \"true\",\n-        \"dfs.namenode.avoid.write.stale.datanode\" : \"true\",\n-        \"dfs.namenode.handler.count\" : \"100\",\n-        \"dfs.namenode.http-address\" : \"dev05.hortonworks.com:50070\",\n-        \"dfs.namenode.name.dir\" : \"/hadoop/hdfs/namenode\",\n-        \"dfs.namenode.safemode.threshold-pct\" : \"1.0f\",\n-        \"dfs.namenode.secondary.http-address\" : \"dev05.hortonworks.com:50090\",\n-        \"dfs.namenode.stale.datanode.interval\" : \"30000\",\n-        \"dfs.namenode.write.stale.datanode.ratio\" : \"1.0f\",\n-        \"dfs.permissions.enabled\" : \"true\",\n-        \"dfs.permissions.superusergroup\" : \"hdfs\",\n-        \"dfs.replication\" : \"3\",\n-        \"dfs.replication.max\" : \"50\",\n-        \"dfs.webhdfs.enabled\" : \"true\",\n-        \"fs.permissions.umask-mode\" : \"022\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_get_hdfs_site_config.json",
                "sha": "9108aa727f86928df5bad5f44e05eff7a22514e3",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_get_host.json",
                "changes": 250,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/clustermodel_get_host.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 250,
                "filename": "ambari-client/python-client/src/test/python/json/clustermodel_get_host.json",
                "patch": "@@ -1,250 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/clusters/test1/hosts/myhost\",\n-  \"Hosts\" : {\n-    \"cluster_name\" : \"test1\",\n-    \"cpu_count\" : 24,\n-    \"disk_info\" : [\n-      {\n-        \"available\" : \"25938900\",\n-        \"used\" : \"5743652\",\n-        \"percent\" : \"19%\",\n-        \"size\" : \"33378088\",\n-        \"type\" : \"ext4\",\n-        \"mountpoint\" : \"/\"\n-      },\n-      {\n-        \"available\" : \"49525536\",\n-        \"used\" : \"0\",\n-        \"percent\" : \"0%\",\n-        \"size\" : \"49525536\",\n-        \"type\" : \"tmpfs\",\n-        \"mountpoint\" : \"/dev/shm\"\n-      },\n-      {\n-        \"available\" : \"433221\",\n-        \"used\" : \"37023\",\n-        \"percent\" : \"8%\",\n-        \"size\" : \"495844\",\n-        \"type\" : \"ext4\",\n-        \"mountpoint\" : \"/boot\"\n-      },\n-      {\n-        \"available\" : \"3020752\",\n-        \"used\" : \"71284\",\n-        \"percent\" : \"3%\",\n-        \"size\" : \"3257512\",\n-        \"type\" : \"ext4\",\n-        \"mountpoint\" : \"/home\"\n-      },\n-      {\n-        \"available\" : \"547125892\",\n-        \"used\" : \"404652\",\n-        \"percent\" : \"1%\",\n-        \"size\" : \"576831992\",\n-        \"type\" : \"ext4\",\n-        \"mountpoint\" : \"/data/1\"\n-      },\n-      {\n-        \"available\" : \"547305068\",\n-        \"used\" : \"225476\",\n-        \"percent\" : \"1%\",\n-        \"size\" : \"576831992\",\n-        \"type\" : \"ext4\",\n-        \"mountpoint\" : \"/data/2\"\n-      },\n-      {\n-        \"available\" : \"547325924\",\n-        \"used\" : \"204620\",\n-        \"percent\" : \"1%\",\n-        \"size\" : \"576831992\",\n-        \"type\" : \"ext4\",\n-        \"mountpoint\" : \"/data/3\"\n-      },\n-      {\n-        \"available\" : \"547320928\",\n-        \"used\" : \"209616\",\n-        \"percent\" : \"1%\",\n-        \"size\" : \"576831992\",\n-        \"type\" : \"ext4\",\n-        \"mountpoint\" : \"/data/4\"\n-      },\n-      {\n-        \"available\" : \"547315544\",\n-        \"used\" : \"215000\",\n-        \"percent\" : \"1%\",\n-        \"size\" : \"576831992\",\n-        \"type\" : \"ext4\",\n-        \"mountpoint\" : \"/data/5\"\n-      },\n-      {\n-        \"available\" : \"547327008\",\n-        \"used\" : \"203536\",\n-        \"percent\" : \"1%\",\n-        \"size\" : \"576831992\",\n-        \"type\" : \"ext4\",\n-        \"mountpoint\" : \"/data/6\"\n-      },\n-      {\n-        \"available\" : \"547310644\",\n-        \"used\" : \"219900\",\n-        \"percent\" : \"1%\",\n-        \"size\" : \"576831992\",\n-        \"type\" : \"ext4\",\n-        \"mountpoint\" : \"/data/7\"\n-      },\n-      {\n-        \"available\" : \"547320544\",\n-        \"used\" : \"210000\",\n-        \"percent\" : \"1%\",\n-        \"size\" : \"576831992\",\n-        \"type\" : \"ext4\",\n-        \"mountpoint\" : \"/data/8\"\n-      }\n-    ],\n-    \"host_health_report\" : \"\",\n-    \"host_name\" : \"myhost\",\n-    \"host_state\" : \"HEALTHY\",\n-    \"host_status\" : \"HEALTHY\",\n-    \"ip\" : \"10.104.44.95\",\n-    \"last_agent_env\" : {\n-      \"stackFoldersAndFiles\" : [ ],\n-      \"rpms\" : [\n-        {\n-          \"name\" : \"nagios\",\n-          \"installed\" : true,\n-          \"version\" : \"nagios-3.5.0-99.x86_64\"\n-        },\n-        {\n-          \"name\" : \"ganglia\",\n-          \"installed\" : false\n-        },\n-        {\n-          \"name\" : \"hadoop\",\n-          \"installed\" : true,\n-          \"version\" : \"hadoop-1.2.0.1.3.0.0-107.el6.x86_64\"\n-        },\n-        {\n-          \"name\" : \"hadoop-lzo\",\n-          \"installed\" : true,\n-          \"version\" : \"hadoop-lzo-0.5.0-1.x86_64\"\n-        },\n-        {\n-          \"name\" : \"hbase\",\n-          \"installed\" : false\n-        },\n-        {\n-          \"name\" : \"oozie\",\n-          \"installed\" : true,\n-          \"version\" : \"oozie-3.3.2.1.3.0.0-107.el6.noarch\"\n-        },\n-        {\n-          \"name\" : \"sqoop\",\n-          \"installed\" : false\n-        },\n-        {\n-          \"name\" : \"pig\",\n-          \"installed\" : false\n-        },\n-        {\n-          \"name\" : \"zookeeper\",\n-          \"installed\" : true,\n-          \"version\" : \"zookeeper-3.4.5.1.3.0.0-107.el6.noarch\"\n-        },\n-        {\n-          \"name\" : \"hive\",\n-          \"installed\" : true,\n-          \"version\" : \"hive-0.11.0.1.3.0.0-107.el6.noarch\"\n-        },\n-        {\n-          \"name\" : \"libconfuse\",\n-          \"installed\" : true,\n-          \"version\" : \"libconfuse-2.6-3.el6.x86_64\"\n-        },\n-        {\n-          \"name\" : \"ambari-log4j\",\n-          \"installed\" : true,\n-          \"version\" : \"ambari-log4j-1.2.3.6-1.noarch\"\n-        }\n-      ],\n-      \"alternatives\" : [ ],\n-      \"existingUsers\" : [ ],\n-      \"existingRepos\" : [\n-        \"unable_to_determine\"\n-      ],\n-      \"installedPackages\" : [ ],\n-      \"hostHealth\" : {\n-        \"activeJavaProcs\" : [ ],\n-        \"agentTimeStampAtReporting\" : 1377776815389,\n-        \"serverTimeStampAtReporting\" : 1377776801984,\n-        \"liveServices\" : [\n-          {\n-            \"name\" : \"ntpd\",\n-            \"desc\" : \"ntpd is stopped\\n\",\n-            \"status\" : \"Unhealthy\"\n-          }\n-        ],\n-        \"diskStatus\" : [\n-          {\n-            \"available\" : \"25032456\",\n-            \"used\" : \"6650096\",\n-            \"percent\" : \"21%\",\n-            \"size\" : \"33378088\",\n-            \"type\" : \"ext4\",\n-            \"mountpoint\" : \"/\"\n-          }\n-        ]\n-      }\n-    },\n-    \"last_heartbeat_time\" : 1377776827188,\n-    \"last_registration_time\" : 1377605551901,\n-    \"os_arch\" : \"x86_64\",\n-    \"os_type\" : \"redhat6\",\n-    \"ph_cpu_count\" : 2,\n-    \"public_host_name\" : \"myhost\",\n-    \"rack_info\" : \"/default-rack\",\n-    \"total_mem\" : 99048488,\n-    \"desired_configs\" : { }\n-  },\n-  \"host_components\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/hosts/myhost/host_components/GANGLIA_MONITOR\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"component_name\" : \"GANGLIA_MONITOR\",\n-        \"host_name\" : \"myhost\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/hosts/myhost/host_components/GANGLIA_SERVER\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"component_name\" : \"GANGLIA_SERVER\",\n-        \"host_name\" : \"myhost\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/hosts/myhost/host_components/MAPREDUCE_CLIENT\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"component_name\" : \"MAPREDUCE_CLIENT\",\n-        \"host_name\" : \"myhost\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/hosts/myhost/host_components/NAGIOS_SERVER\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"component_name\" : \"NAGIOS_SERVER\",\n-        \"host_name\" : \"myhost\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/hosts/myhost/host_components/NAMENODE\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"component_name\" : \"NAMENODE\",\n-        \"host_name\" : \"myhost\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_get_host.json",
                "sha": "b1b09d625366792b0d77d3685efd4465bf54ce3d",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_get_mapred_site_config.json",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/clustermodel_get_mapred_site_config.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 58,
                "filename": "ambari-client/python-client/src/test/python/json/clustermodel_get_mapred_site_config.json",
                "patch": "@@ -1,58 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/clusters/test1/configurations?type=mapred-site&tag=version1\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/configurations?type=mapred-site&tag=version1\",\n-      \"tag\" : \"version1\",\n-      \"type\" : \"mapred-site\",\n-      \"Config\" : {\n-        \"cluster_name\" : \"test1\"\n-      },\n-      \"properties\" : {\n-        \"mapred.hosts\" : \"/etc/hadoop/conf/mapred.include\",\n-        \"mapred.hosts.exclude\" : \"/etc/hadoop/conf/mapred.exclude\",\n-        \"mapred.jobtracker.maxtasks.per.job\" : \"-1\",\n-        \"mapred.jobtracker.taskScheduler\" : \"org.apache.hadoop.mapred.CapacityTaskScheduler\",\n-        \"mapred.task.tracker.task-controller\" : \"org.apache.hadoop.mapred.DefaultTaskController\",\n-        \"mapred.userlog.retain.hours\" : \"24\",\n-        \"mapreduce.admin.map.child.java.opts\" : \"-Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN\",\n-        \"mapreduce.admin.reduce.child.java.opts\" : \"-Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN\",\n-        \"mapreduce.admin.user.env\" : \"LD_LIBRARY_PATH=/usr/lib/hadoop/lib/native:/usr/lib/hadoop/lib/native/`$JAVA_HOME/bin/java -d32 -version &amp;&gt; /dev/null;if [ $? -eq 0 ]; then echo Linux-i386-32; else echo Linux-amd64-64;fi`\",\n-        \"mapreduce.am.max-attempts\" : \"2\",\n-        \"mapreduce.application.classpath\" : \"$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/*,$HADOOP_MAPRED_HOME/share/hadoop/mapreduce/lib/*\",\n-        \"mapreduce.framework.name\" : \"yarn\",\n-        \"mapreduce.job.reduce.slowstart.completedmaps\" : \"0.05\",\n-        \"mapreduce.jobhistory.address\" : \"dev05.hortonworks.com:10020\",\n-        \"mapreduce.jobhistory.done-dir\" : \"/mr-history/done\",\n-        \"mapreduce.jobhistory.intermediate-done-dir\" : \"/mr-history/tmp\",\n-        \"mapreduce.jobhistory.webapp.address\" : \"dev05.hortonworks.com:19888\",\n-        \"mapreduce.jobtracker.system.dir\" : \"/mapred/system\",\n-        \"mapreduce.map.java.opts\" : \"-Xmx320m\",\n-        \"mapreduce.map.log.level\" : \"INFO\",\n-        \"mapreduce.map.memory.mb\" : \"1536\",\n-        \"mapreduce.map.sort.spill.percent\" : \"0.1\",\n-        \"mapreduce.map.speculative\" : \"false\",\n-        \"mapreduce.output.fileoutputformat.compress.type\" : \"BLOCK\",\n-        \"mapreduce.reduce.input.buffer.percent\" : \"0.0\",\n-        \"mapreduce.reduce.java.opts\" : \"-Xmx756m\",\n-        \"mapreduce.reduce.log.level\" : \"INFO\",\n-        \"mapreduce.reduce.memory.mb\" : \"2048\",\n-        \"mapreduce.reduce.shuffle.input.buffer.percent\" : \"0.7\",\n-        \"mapreduce.reduce.shuffle.merge.percent\" : \"0.66\",\n-        \"mapreduce.reduce.shuffle.parallelcopies\" : \"30\",\n-        \"mapreduce.reduce.speculative\" : \"false\",\n-        \"mapreduce.shuffle.port\" : \"13562\",\n-        \"mapreduce.task.io.sort.factor\" : \"100\",\n-        \"mapreduce.task.io.sort.mb\" : \"200\",\n-        \"mapreduce.task.timeout\" : \"600000\",\n-        \"mapreduce.tasktracker.healthchecker.script.path\" : \"file:////mapred/jobstatus\",\n-        \"mapreduce.tasktracker.map.tasks.maximum\" : \"4\",\n-        \"yarn.app.mapreduce.am.admin-command-opts\" : \"-Djava.net.preferIPv4Stack=true -Dhadoop.metrics.log.level=WARN\",\n-        \"yarn.app.mapreduce.am.command-opts\" : \"-Xmx756m\",\n-        \"yarn.app.mapreduce.am.log.level\" : \"INFO\",\n-        \"yarn.app.mapreduce.am.resource.mb\" : \"1024\",\n-        \"yarn.app.mapreduce.am.staging-dir\" : \"/user\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_get_mapred_site_config.json",
                "sha": "1080be1ffd269eb56b7beed5b2eb22133254cc77",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_get_service.json",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/clustermodel_get_service.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 29,
                "filename": "ambari-client/python-client/src/test/python/json/clustermodel_get_service.json",
                "patch": "@@ -1,29 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/clusters/test1/services/GANGLIA\",\n-  \"ServiceInfo\" : {\n-    \"cluster_name\" : \"test1\",\n-    \"state\" : \"STARTED\",\n-    \"service_name\" : \"GANGLIA\",\n-    \"desired_configs\" : {\n-      \"global\" : \"version1\"\n-    }\n-  },\n-  \"components\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/services/GANGLIA/components/GANGLIA_MONITOR\",\n-      \"ServiceComponentInfo\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"component_name\" : \"GANGLIA_MONITOR\",\n-        \"service_name\" : \"GANGLIA\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test1/services/GANGLIA/components/GANGLIA_SERVER\",\n-      \"ServiceComponentInfo\" : {\n-        \"cluster_name\" : \"test1\",\n-        \"component_name\" : \"GANGLIA_SERVER\",\n-        \"service_name\" : \"GANGLIA\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/clustermodel_get_service.json",
                "sha": "0c9d5e703475c3fc44f15b48ca582f8e7ce8a3d9",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/componentmodel_get_metrics.json",
                "changes": 133,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/componentmodel_get_metrics.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 133,
                "filename": "ambari-client/python-client/src/test/python/json/componentmodel_get_metrics.json",
                "patch": "@@ -1,133 +0,0 @@\n-{\n-    \"href\" : \"http://192.168.64.101:8080/api/v1/clusters/cl1/hosts/myhost/host_components/DATANODE?fields=metrics\",\n-    \"HostRoles\" : {\n-        \"cluster_name\" : \"cl1\",\n-        \"component_name\" : \"DATANODE\",\n-        \"host_name\" : \"myhost\"\n-    },\n-    \"host\" : {\n-        \"href\" : \"http://192.168.64.101:8080/api/v1/clusters/cl1/hosts/myhost\"\n-    },\n-    \"metrics\" : {\n-        \"boottime\" : 1.383131209E9,\n-        \"cpu\" : {\n-            \"cpu_aidle\" : 0.0,\n-            \"cpu_idle\" : 85.73,\n-            \"cpu_nice\" : 0.0,\n-            \"cpu_num\" : 1.0,\n-            \"cpu_speed\" : 2967.0,\n-            \"cpu_system\" : 6.84583333333,\n-            \"cpu_user\" : 7.42888888889,\n-            \"cpu_wio\" : 0.00416666666667\n-        },\n-        \"dfs\" : {\n-            \"FSNamesystem\" : {\n-                \"HttpPort\" : null,\n-                \"NamenodeAddress\" : \"{\\\"myhost\\\":\\\"BP-442795920-192.168.64.101-1383132565020\\\"}\",\n-                \"RpcPort\" : \"8010\",\n-                \"Version\" : \"2.2.0.2.0.6.0-76\",\n-                \"VolumeInfo\" : \"{\\\"/hadoop/hdfs/data/current\\\":{\\\"freeSpace\\\":495195869184,\\\"usedSpace\\\":345120768,\\\"reservedSpace\\\":1073741824}}\"\n-            },\n-            \"datanode\" : {\n-                \"blockChecksumOp_avg_time\" : 0.0,\n-                \"blockChecksumOp_num_ops\" : 0.0,\n-                \"blockReports_avg_time\" : 6.0,\n-                \"blockReports_num_ops\" : 0.0,\n-                \"block_verification_failures\" : 0.0,\n-                \"blocks_get_local_pathinfo\" : 0.0,\n-                \"blocks_read\" : 0.0,\n-                \"blocks_removed\" : 0.0,\n-                \"blocks_replicated\" : 0.0,\n-                \"blocks_verified\" : 0.0,\n-                \"blocks_written\" : 0.0,\n-                \"bytes_read\" : 0.0,\n-                \"bytes_written\" : 0.0,\n-                \"copyBlockOp_avg_time\" : 0.0,\n-                \"copyBlockOp_num_ops\" : 0.0,\n-                \"heartBeats_avg_time\" : 1.69166666667,\n-                \"heartBeats_num_ops\" : 0.336481481481,\n-                \"readBlockOp_avg_time\" : 0.0,\n-                \"readBlockOp_num_ops\" : 0.0,\n-                \"reads_from_local_client\" : 0.0,\n-                \"reads_from_remote_client\" : 0.0,\n-                \"replaceBlockOp_avg_time\" : 0.0,\n-                \"replaceBlockOp_num_ops\" : 0.0,\n-                \"writeBlockOp_avg_time\" : 13.8969072165,\n-                \"writeBlockOp_num_ops\" : 0.0,\n-                \"writes_from_local_client\" : 0.0,\n-                \"writes_from_remote_client\" : 0.0\n-            }\n-        },\n-        \"disk\" : {\n-            \"disk_free\" : 495.645,\n-            \"disk_total\" : 525.79,\n-            \"part_max_used\" : 11.6\n-        },\n-        \"jvm\" : {\n-            \"HeapMemoryMax\" : 1037959168,\n-            \"HeapMemoryUsed\" : 11395264,\n-            \"NonHeapMemoryMax\" : 136314880,\n-            \"NonHeapMemoryUsed\" : 30798600,\n-            \"gcCount\" : 0.0111111111111,\n-            \"gcTimeMillis\" : 388,\n-            \"logError\" : 0.0,\n-            \"logFatal\" : 0.0,\n-            \"logInfo\" : 0.0,\n-            \"logWarn\" : 0.0,\n-            \"memHeapCommittedM\" : 28.5625,\n-            \"memHeapUsedM\" : 11.7175731111,\n-            \"memNonHeapCommittedM\" : 29.625,\n-            \"memNonHeapUsedM\" : 29.36007675,\n-            \"threadsBlocked\" : 0.0,\n-            \"threadsNew\" : 0.0,\n-            \"threadsRunnable\" : 8.04166666667,\n-            \"threadsTerminated\" : 0.0,\n-            \"threadsTimedWaiting\" : 8.91666666667,\n-            \"threadsWaiting\" : 18.0\n-        },\n-        \"load\" : {\n-            \"load_fifteen\" : 0.0692222222222,\n-            \"load_five\" : 0.0592777777778,\n-            \"load_one\" : 0.0125555555556\n-        },\n-        \"memory\" : {\n-            \"mem_buffers\" : 23914.2666667,\n-            \"mem_cached\" : 160191.855556,\n-            \"mem_free\" : 183983.855556,\n-            \"mem_shared\" : 0.0,\n-            \"mem_total\" : 1922680.0,\n-            \"swap_free\" : 2593920.0,\n-            \"swap_total\" : 2621432.0\n-        },\n-        \"network\" : {\n-            \"bytes_in\" : 30989.8874167,\n-            \"bytes_out\" : 98799.6742778,\n-            \"pkts_in\" : 44.9822222222,\n-            \"pkts_out\" : 214.358916667\n-        },\n-        \"process\" : {\n-            \"proc_run\" : 2.0,\n-            \"proc_total\" : 682.397222222\n-        },\n-        \"rpc\" : {\n-            \"NumOpenConnections\" : 0.0,\n-            \"ReceivedBytes\" : 0.0,\n-            \"RpcProcessingTime_avg_time\" : 0.0,\n-            \"RpcProcessingTime_num_ops\" : 0.0,\n-            \"RpcQueueTime_avg_time\" : 0.0,\n-            \"RpcQueueTime_num_ops\" : 0.0,\n-            \"SentBytes\" : 0.0,\n-            \"callQueueLen\" : 0.0,\n-            \"rpcAuthenticationFailures\" : 0.0,\n-            \"rpcAuthenticationSuccesses\" : 0.0,\n-            \"rpcAuthorizationFailures\" : 0.0,\n-            \"rpcAuthorizationSuccesses\" : 0.0\n-        },\n-        \"ugi\" : {\n-            \"loginFailure_avg_time\" : 0.0,\n-            \"loginFailure_num_ops\" : 0.0,\n-            \"loginSuccess_avg_time\" : 0.0,\n-            \"loginSuccess_num_ops\" : 0.0\n-        }\n-    }\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/componentmodel_get_metrics.json",
                "sha": "5623d8bd2d4a4ba27046b1b03a015e50e2152eef",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/get_cluster_service.json",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/get_cluster_service.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 27,
                "filename": "ambari-client/python-client/src/test/python/json/get_cluster_service.json",
                "patch": "@@ -1,27 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/clusters/test6/services/GANGLIA\",\n-  \"ServiceInfo\" : {\n-    \"cluster_name\" : \"test6\",\n-    \"service_name\" : \"GANGLIA\",\n-    \"state\" : \"INSTALLED\",\n-    \"desired_configs\" : { }\n-  },\n-  \"components\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/services/GANGLIA/components/GANGLIA_MONITOR\",\n-      \"ServiceComponentInfo\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"GANGLIA_MONITOR\",\n-        \"service_name\" : \"GANGLIA\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/services/GANGLIA/components/GANGLIA_SERVER\",\n-      \"ServiceComponentInfo\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"GANGLIA_SERVER\",\n-        \"service_name\" : \"GANGLIA\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/get_cluster_service.json",
                "sha": "5b78e053d8917e34045a78bab7c7356b39fdf457",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/get_components_from_stack.json",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/get_components_from_stack.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 41,
                "filename": "ambari-client/python-client/src/test/python/json/get_components_from_stack.json",
                "patch": "@@ -1,41 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/2.0.5/stackServices/YARN/serviceComponents?fields=*\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/2.0.5/stackServices/YARN/serviceComponents/NODEMANAGER\",\n-      \"StackServiceComponents\" : {\n-        \"component_category\" : \"SLAVE\",\n-        \"component_name\" : \"NODEMANAGER\",\n-        \"is_client\" : false,\n-        \"is_master\" : false,\n-        \"service_name\" : \"YARN\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"2.0.5\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/2.0.5/stackServices/YARN/serviceComponents/RESOURCEMANAGER\",\n-      \"StackServiceComponents\" : {\n-        \"component_category\" : \"MASTER\",\n-        \"component_name\" : \"RESOURCEMANAGER\",\n-        \"is_client\" : false,\n-        \"is_master\" : true,\n-        \"service_name\" : \"YARN\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"2.0.5\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/stacks2/HDP/versions/2.0.5/stackServices/YARN/serviceComponents/YARN_CLIENT\",\n-      \"StackServiceComponents\" : {\n-        \"component_category\" : \"CLIENT\",\n-        \"component_name\" : \"YARN_CLIENT\",\n-        \"is_client\" : true,\n-        \"is_master\" : false,\n-        \"service_name\" : \"YARN\",\n-        \"stack_name\" : \"HDP\",\n-        \"stack_version\" : \"2.0.5\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/get_components_from_stack.json",
                "sha": "55a9c09b1476600658edc51defed707702231a48",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/get_host_component.json",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/get_host_component.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 41,
                "filename": "ambari-client/python-client/src/test/python/json/get_host_component.json",
                "patch": "@@ -1,41 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn01/host_components/NAMENODE\",\n-  \"HostRoles\" : {\n-    \"cluster_name\" : \"test6\",\n-    \"component_name\" : \"NAMENODE\",\n-    \"desired_stack_id\" : \"HDP-1.3.0\",\n-    \"desired_state\" : \"INSTALLED\",\n-    \"host_name\" : \"r01wn01\",\n-    \"stack_id\" : \"HDP-1.3.0\",\n-    \"state\" : \"INSTALLED\",\n-    \"actual_configs\" : {\n-      \"core-site\" : {\n-        \"tag\" : \"version1\"\n-      },\n-      \"global\" : {\n-        \"tag\" : \"version1\"\n-      },\n-      \"hdfs-site\" : {\n-        \"tag\" : \"version1\"\n-      },\n-      \"mapred-site\" : {\n-        \"tag\" : \"version1\"\n-      }\n-    },\n-    \"configs\" : { },\n-    \"desired_configs\" : { }\n-  },\n-  \"host\" : {\n-    \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn01\"\n-  },\n-  \"component\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/services/HDFS/components/NAMENODE\",\n-      \"ServiceComponentInfo\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"NAMENODE\",\n-        \"service_name\" : \"HDFS\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/get_host_component.json",
                "sha": "709b94b7603485f999a96422f3fd6d53d0d9672b",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/get_host_components.json",
                "changes": 60,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/get_host_components.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 60,
                "filename": "ambari-client/python-client/src/test/python/json/get_host_components.json",
                "patch": "@@ -1,60 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn01/host_components?ServiceComponentInfo\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn01/host_components/GANGLIA_MONITOR\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"GANGLIA_MONITOR\",\n-        \"host_name\" : \"r01wn01\"\n-      },\n-      \"host\" : {\n-        \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn01\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn01/host_components/GANGLIA_SERVER\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"GANGLIA_SERVER\",\n-        \"host_name\" : \"r01wn01\"\n-      },\n-      \"host\" : {\n-        \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn01\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn01/host_components/MAPREDUCE_CLIENT\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"MAPREDUCE_CLIENT\",\n-        \"host_name\" : \"r01wn01\"\n-      },\n-      \"host\" : {\n-        \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn01\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn01/host_components/NAGIOS_SERVER\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"NAGIOS_SERVER\",\n-        \"host_name\" : \"r01wn01\"\n-      },\n-      \"host\" : {\n-        \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn01\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn01/host_components/NAMENODE\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"NAMENODE\",\n-        \"host_name\" : \"r01wn01\"\n-      },\n-      \"host\" : {\n-        \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn01\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/get_host_components.json",
                "sha": "94a3027e4c4bc2d104c6edbca2c082f4c7d66806",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/get_service_component.json",
                "changes": 108,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/get_service_component.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 108,
                "filename": "ambari-client/python-client/src/test/python/json/get_service_component.json",
                "patch": "@@ -1,108 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/clusters/test6/services/GANGLIA/components/GANGLIA_MONITOR\",\n-  \"ServiceComponentInfo\" : {\n-    \"cluster_name\" : \"test6\",\n-    \"component_name\" : \"GANGLIA_MONITOR\",\n-    \"service_name\" : \"GANGLIA\",\n-    \"state\" : \"INSTALLED\",\n-    \"desired_configs\" : { }\n-  },\n-  \"host_components\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/apspal44-83/host_components/GANGLIA_MONITOR\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"GANGLIA_MONITOR\",\n-        \"host_name\" : \"apspal44-83\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/apspal44-84/host_components/GANGLIA_MONITOR\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"GANGLIA_MONITOR\",\n-        \"host_name\" : \"apspal44-84\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/apspal44-85/host_components/GANGLIA_MONITOR\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"GANGLIA_MONITOR\",\n-        \"host_name\" : \"apspal44-85\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/apspal44-86/host_components/GANGLIA_MONITOR\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"GANGLIA_MONITOR\",\n-        \"host_name\" : \"apspal44-86\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/apspal44-87/host_components/GANGLIA_MONITOR\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"GANGLIA_MONITOR\",\n-        \"host_name\" : \"apspal44-87\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/apspal44-88/host_components/GANGLIA_MONITOR\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"GANGLIA_MONITOR\",\n-        \"host_name\" : \"apspal44-88\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/apspal44-89/host_components/GANGLIA_MONITOR\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"GANGLIA_MONITOR\",\n-        \"host_name\" : \"apspal44-89\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01hn01/host_components/GANGLIA_MONITOR\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"GANGLIA_MONITOR\",\n-        \"host_name\" : \"r01hn01\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01mgt/host_components/GANGLIA_MONITOR\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"GANGLIA_MONITOR\",\n-        \"host_name\" : \"r01mgt\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn01/host_components/GANGLIA_MONITOR\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"GANGLIA_MONITOR\",\n-        \"host_name\" : \"r01wn01\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn02/host_components/GANGLIA_MONITOR\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"GANGLIA_MONITOR\",\n-        \"host_name\" : \"r01wn02\"\n-      }\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn03/host_components/GANGLIA_MONITOR\",\n-      \"HostRoles\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"GANGLIA_MONITOR\",\n-        \"host_name\" : \"r01wn03\"\n-      }\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/get_service_component.json",
                "sha": "3e993cd476dd05483e4461dfc97e5e020f4e03a0",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/get_service_components.json",
                "changes": 133,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/get_service_components.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 133,
                "filename": "ambari-client/python-client/src/test/python/json/get_service_components.json",
                "patch": "@@ -1,133 +0,0 @@\n-{\n-  \"href\" : \"http://localhost:8080/api/v1/clusters/test6/services/GANGLIA/components?fields=*\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/services/GANGLIA/components/GANGLIA_MONITOR\",\n-      \"ServiceComponentInfo\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"GANGLIA_MONITOR\",\n-        \"service_name\" : \"GANGLIA\",\n-        \"state\" : \"INSTALLED\",\n-        \"desired_configs\" : { }\n-      },\n-      \"host_components\" : [\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/apspal44-83/host_components/GANGLIA_MONITOR\",\n-          \"HostRoles\" : {\n-            \"cluster_name\" : \"test6\",\n-            \"component_name\" : \"GANGLIA_MONITOR\",\n-            \"host_name\" : \"apspal44-83\"\n-          }\n-        },\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/apspal44-84/host_components/GANGLIA_MONITOR\",\n-          \"HostRoles\" : {\n-            \"cluster_name\" : \"test6\",\n-            \"component_name\" : \"GANGLIA_MONITOR\",\n-            \"host_name\" : \"apspal44-84\"\n-          }\n-        },\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/apspal44-85/host_components/GANGLIA_MONITOR\",\n-          \"HostRoles\" : {\n-            \"cluster_name\" : \"test6\",\n-            \"component_name\" : \"GANGLIA_MONITOR\",\n-            \"host_name\" : \"apspal44-85\"\n-          }\n-        },\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/apspal44-86/host_components/GANGLIA_MONITOR\",\n-          \"HostRoles\" : {\n-            \"cluster_name\" : \"test6\",\n-            \"component_name\" : \"GANGLIA_MONITOR\",\n-            \"host_name\" : \"apspal44-86\"\n-          }\n-        },\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/apspal44-87/host_components/GANGLIA_MONITOR\",\n-          \"HostRoles\" : {\n-            \"cluster_name\" : \"test6\",\n-            \"component_name\" : \"GANGLIA_MONITOR\",\n-            \"host_name\" : \"apspal44-87\"\n-          }\n-        },\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/apspal44-88/host_components/GANGLIA_MONITOR\",\n-          \"HostRoles\" : {\n-            \"cluster_name\" : \"test6\",\n-            \"component_name\" : \"GANGLIA_MONITOR\",\n-            \"host_name\" : \"apspal44-88\"\n-          }\n-        },\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/apspal44-89/host_components/GANGLIA_MONITOR\",\n-          \"HostRoles\" : {\n-            \"cluster_name\" : \"test6\",\n-            \"component_name\" : \"GANGLIA_MONITOR\",\n-            \"host_name\" : \"apspal44-89\"\n-          }\n-        },\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01hn01/host_components/GANGLIA_MONITOR\",\n-          \"HostRoles\" : {\n-            \"cluster_name\" : \"test6\",\n-            \"component_name\" : \"GANGLIA_MONITOR\",\n-            \"host_name\" : \"r01hn01\"\n-          }\n-        },\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01mgt/host_components/GANGLIA_MONITOR\",\n-          \"HostRoles\" : {\n-            \"cluster_name\" : \"test6\",\n-            \"component_name\" : \"GANGLIA_MONITOR\",\n-            \"host_name\" : \"r01mgt\"\n-          }\n-        },\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn01/host_components/GANGLIA_MONITOR\",\n-          \"HostRoles\" : {\n-            \"cluster_name\" : \"test6\",\n-            \"component_name\" : \"GANGLIA_MONITOR\",\n-            \"host_name\" : \"r01wn01\"\n-          }\n-        },\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn02/host_components/GANGLIA_MONITOR\",\n-          \"HostRoles\" : {\n-            \"cluster_name\" : \"test6\",\n-            \"component_name\" : \"GANGLIA_MONITOR\",\n-            \"host_name\" : \"r01wn02\"\n-          }\n-        },\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn03/host_components/GANGLIA_MONITOR\",\n-          \"HostRoles\" : {\n-            \"cluster_name\" : \"test6\",\n-            \"component_name\" : \"GANGLIA_MONITOR\",\n-            \"host_name\" : \"r01wn03\"\n-          }\n-        }\n-      ]\n-    },\n-    {\n-      \"href\" : \"http://localhost:8080/api/v1/clusters/test6/services/GANGLIA/components/GANGLIA_SERVER\",\n-      \"ServiceComponentInfo\" : {\n-        \"cluster_name\" : \"test6\",\n-        \"component_name\" : \"GANGLIA_SERVER\",\n-        \"service_name\" : \"GANGLIA\",\n-        \"state\" : \"INSTALLED\",\n-        \"desired_configs\" : { }\n-      },\n-      \"host_components\" : [\n-        {\n-          \"href\" : \"http://localhost:8080/api/v1/clusters/test6/hosts/r01wn01/host_components/GANGLIA_SERVER\",\n-          \"HostRoles\" : {\n-            \"cluster_name\" : \"test6\",\n-            \"component_name\" : \"GANGLIA_SERVER\",\n-            \"host_name\" : \"r01wn01\"\n-          }\n-        }\n-      ]\n-    }\n-  ]\n-}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/get_service_components.json",
                "sha": "967bcbd051e730b1c1e15900870790b0996144cb",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/hostmodel_get_host_component.json",
                "changes": 181,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/hostmodel_get_host_component.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 181,
                "filename": "ambari-client/python-client/src/test/python/json/hostmodel_get_host_component.json",
                "patch": "@@ -1,181 +0,0 @@\n-{\n-    \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components/DATANODE\",\n-    \"HostRoles\" : {\n-        \"cluster_name\" : \"cl1\",\n-        \"component_name\" : \"DATANODE\",\n-        \"desired_stack_id\" : \"HDP-2.0.6\",\n-        \"desired_state\" : \"STARTED\",\n-        \"host_name\" : \"myhost\",\n-        \"stack_id\" : \"HDP-2.0.6\",\n-        \"state\" : \"STARTED\",\n-        \"actual_configs\" : {\n-            \"capacity-scheduler\" : {\n-                \"tag\" : \"version1\"\n-            },\n-            \"core-site\" : {\n-                \"tag\" : \"version1\"\n-            },\n-            \"global\" : {\n-                \"tag\" : \"version1\"\n-            },\n-            \"hbase-site\" : {\n-                \"tag\" : \"version1\"\n-            },\n-            \"hdfs-site\" : {\n-                \"tag\" : \"version1\"\n-            },\n-            \"hive-site\" : {\n-                \"tag\" : \"version1\"\n-            },\n-            \"mapred-site\" : {\n-                \"tag\" : \"version1\"\n-            },\n-            \"oozie-site\" : {\n-                \"tag\" : \"version1\"\n-            },\n-            \"webhcat-site\" : {\n-                \"tag\" : \"version1\"\n-            },\n-            \"yarn-site\" : {\n-                \"tag\" : \"version1\"\n-            }\n-        },\n-        \"configs\" : { },\n-        \"desired_configs\" : { }\n-    },\n-    \"host\" : {\n-        \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost\"\n-    },\n-    \"metrics\" : {\n-        \"boottime\" : 1.383131209E9,\n-        \"cpu\" : {\n-            \"cpu_aidle\" : 0.0,\n-            \"cpu_idle\" : 86.7238888889,\n-            \"cpu_nice\" : 0.0,\n-            \"cpu_num\" : 1.0,\n-            \"cpu_speed\" : 2967.0,\n-            \"cpu_system\" : 6.43527777778,\n-            \"cpu_user\" : 6.81722222222,\n-            \"cpu_wio\" : 0.0263888888889\n-        },\n-        \"dfs\" : {\n-            \"FSNamesystem\" : {\n-                \"HttpPort\" : null,\n-                \"NamenodeAddress\" : \"{\\\"myhost\\\":\\\"BP-442795920-192.168.64.101-1383132565020\\\"}\",\n-                \"RpcPort\" : \"8010\",\n-                \"Version\" : \"2.2.0.2.0.6.0-76\",\n-                \"VolumeInfo\" : \"{\\\"/hadoop/hdfs/data/current\\\":{\\\"freeSpace\\\":495195308032,\\\"usedSpace\\\":345120768,\\\"reservedSpace\\\":1073741824}}\"\n-            },\n-            \"datanode\" : {\n-                \"blockChecksumOp_avg_time\" : 0.0,\n-                \"blockChecksumOp_num_ops\" : 0.0,\n-                \"blockReports_avg_time\" : 6.0,\n-                \"blockReports_num_ops\" : 0.0,\n-                \"block_verification_failures\" : 0.0,\n-                \"blocks_get_local_pathinfo\" : 0.0,\n-                \"blocks_read\" : 0.0,\n-                \"blocks_removed\" : 0.0,\n-                \"blocks_replicated\" : 0.0,\n-                \"blocks_verified\" : 0.0,\n-                \"blocks_written\" : 0.0,\n-                \"bytes_read\" : 0.0,\n-                \"bytes_written\" : 0.0,\n-                \"copyBlockOp_avg_time\" : 0.0,\n-                \"copyBlockOp_num_ops\" : 0.0,\n-                \"heartBeats_avg_time\" : 1.29537037037,\n-                \"heartBeats_num_ops\" : 0.330833333333,\n-                \"readBlockOp_avg_time\" : 0.0,\n-                \"readBlockOp_num_ops\" : 0.0,\n-                \"reads_from_local_client\" : 0.0,\n-                \"reads_from_remote_client\" : 0.0,\n-                \"replaceBlockOp_avg_time\" : 0.0,\n-                \"replaceBlockOp_num_ops\" : 0.0,\n-                \"writeBlockOp_avg_time\" : 13.8969072165,\n-                \"writeBlockOp_num_ops\" : 0.0,\n-                \"writes_from_local_client\" : 0.0,\n-                \"writes_from_remote_client\" : 0.0\n-            }\n-        },\n-        \"disk\" : {\n-            \"disk_free\" : 495.644319444,\n-            \"disk_total\" : 525.79,\n-            \"part_max_used\" : 11.6\n-        },\n-        \"jvm\" : {\n-            \"HeapMemoryMax\" : 1037959168,\n-            \"HeapMemoryUsed\" : 15927976,\n-            \"NonHeapMemoryMax\" : 136314880,\n-            \"NonHeapMemoryUsed\" : 30819952,\n-            \"gcCount\" : 0.0127777777778,\n-            \"gcTimeMillis\" : 409,\n-            \"logError\" : 0.0,\n-            \"logFatal\" : 0.0,\n-            \"logInfo\" : 0.0,\n-            \"logWarn\" : 0.0,\n-            \"memHeapCommittedM\" : 28.5625,\n-            \"memHeapUsedM\" : 11.6042887139,\n-            \"memNonHeapCommittedM\" : 29.625,\n-            \"memNonHeapUsedM\" : 29.3762000417,\n-            \"threadsBlocked\" : 0.0,\n-            \"threadsNew\" : 0.0,\n-            \"threadsRunnable\" : 8.0,\n-            \"threadsTerminated\" : 0.0,\n-            \"threadsTimedWaiting\" : 9.0,\n-            \"threadsWaiting\" : 18.0\n-        },\n-        \"load\" : {\n-            \"load_fifteen\" : 0.19775,\n-            \"load_five\" : 0.211194444444,\n-            \"load_one\" : 0.156638888889\n-        },\n-        \"memory\" : {\n-            \"mem_buffers\" : 25446.6,\n-            \"mem_cached\" : 160915.911111,\n-            \"mem_free\" : 176156.088889,\n-            \"mem_shared\" : 0.0,\n-            \"mem_total\" : 1922680.0,\n-            \"swap_free\" : 2593928.0,\n-            \"swap_total\" : 2621432.0\n-        },\n-        \"network\" : {\n-            \"bytes_in\" : 31580.1931111,\n-            \"bytes_out\" : 102086.367833,\n-            \"pkts_in\" : 45.7961111111,\n-            \"pkts_out\" : 216.703472222\n-        },\n-        \"process\" : {\n-            \"proc_run\" : 2.21111111111,\n-            \"proc_total\" : 681.766666667\n-        },\n-        \"rpc\" : {\n-            \"NumOpenConnections\" : 0.0,\n-            \"ReceivedBytes\" : 0.0,\n-            \"RpcProcessingTime_avg_time\" : 0.0,\n-            \"RpcProcessingTime_num_ops\" : 0.0,\n-            \"RpcQueueTime_avg_time\" : 0.0,\n-            \"RpcQueueTime_num_ops\" : 0.0,\n-            \"SentBytes\" : 0.0,\n-            \"callQueueLen\" : 0.0,\n-            \"rpcAuthenticationFailures\" : 0.0,\n-            \"rpcAuthenticationSuccesses\" : 0.0,\n-            \"rpcAuthorizationFailures\" : 0.0,\n-            \"rpcAuthorizationSuccesses\" : 0.0\n-        },\n-        \"ugi\" : {\n-            \"loginFailure_avg_time\" : 0.0,\n-            \"loginFailure_num_ops\" : 0.0,\n-            \"loginSuccess_avg_time\" : 0.0,\n-            \"loginSuccess_num_ops\" : 0.0\n-        }\n-    },\n-    \"component\" : [\n-        {\n-            \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/services/HDFS/components/DATANODE\",\n-            \"ServiceComponentInfo\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"DATANODE\",\n-                \"service_name\" : \"HDFS\"\n-            }\n-        }\n-    ]\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/hostmodel_get_host_component.json",
                "sha": "436cfda76bbb93250b8e9fb57f84662a63cefa65",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/hostmodel_get_host_components.json",
                "changes": 222,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/hostmodel_get_host_components.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 222,
                "filename": "ambari-client/python-client/src/test/python/json/hostmodel_get_host_components.json",
                "patch": "@@ -1,222 +0,0 @@\n-{\n-    \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components?fields=HostRoles/state\",\n-    \"items\" : [\n-        {\n-            \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components/DATANODE\",\n-            \"HostRoles\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"DATANODE\",\n-                \"host_name\" : \"myhost\",\n-                \"state\" : \"STARTED\"\n-            },\n-            \"host\" : {\n-                \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost\"\n-            }\n-        },\n-        {\n-            \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components/GANGLIA_MONITOR\",\n-            \"HostRoles\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"GANGLIA_MONITOR\",\n-                \"host_name\" : \"myhost\",\n-                \"state\" : \"STARTED\"\n-            },\n-            \"host\" : {\n-                \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost\"\n-            }\n-        },\n-        {\n-            \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components/HBASE_CLIENT\",\n-            \"HostRoles\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"HBASE_CLIENT\",\n-                \"host_name\" : \"myhost\",\n-                \"state\" : \"INSTALLED\"\n-            },\n-            \"host\" : {\n-                \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost\"\n-            }\n-        },\n-        {\n-            \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components/HBASE_MASTER\",\n-            \"HostRoles\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"HBASE_MASTER\",\n-                \"ha_status\" : \"active\",\n-                \"host_name\" : \"myhost\",\n-                \"state\" : \"STARTED\"\n-            },\n-            \"host\" : {\n-                \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost\"\n-            }\n-        },\n-        {\n-            \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components/HBASE_REGIONSERVER\",\n-            \"HostRoles\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"HBASE_REGIONSERVER\",\n-                \"host_name\" : \"myhost\",\n-                \"state\" : \"INSTALLED\"\n-            },\n-            \"host\" : {\n-                \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost\"\n-            }\n-        },\n-        {\n-            \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components/HCAT\",\n-            \"HostRoles\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"HCAT\",\n-                \"host_name\" : \"myhost\",\n-                \"state\" : \"INSTALLED\"\n-            },\n-            \"host\" : {\n-                \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost\"\n-            }\n-        },\n-        {\n-            \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components/HDFS_CLIENT\",\n-            \"HostRoles\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"HDFS_CLIENT\",\n-                \"host_name\" : \"myhost\",\n-                \"state\" : \"INSTALLED\"\n-            },\n-            \"host\" : {\n-                \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost\"\n-            }\n-        },\n-        {\n-            \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components/HIVE_CLIENT\",\n-            \"HostRoles\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"HIVE_CLIENT\",\n-                \"host_name\" : \"myhost\",\n-                \"state\" : \"INSTALLED\"\n-            },\n-            \"host\" : {\n-                \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost\"\n-            }\n-        },\n-        {\n-            \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components/MAPREDUCE2_CLIENT\",\n-            \"HostRoles\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"MAPREDUCE2_CLIENT\",\n-                \"host_name\" : \"myhost\",\n-                \"state\" : \"INSTALLED\"\n-            },\n-            \"host\" : {\n-                \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost\"\n-            }\n-        },\n-        {\n-            \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components/NAMENODE\",\n-            \"HostRoles\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"NAMENODE\",\n-                \"host_name\" : \"myhost\",\n-                \"state\" : \"STARTED\"\n-            },\n-            \"host\" : {\n-                \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost\"\n-            }\n-        },\n-        {\n-            \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components/NODEMANAGER\",\n-            \"HostRoles\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"NODEMANAGER\",\n-                \"host_name\" : \"myhost\",\n-                \"state\" : \"INSTALLED\"\n-            },\n-            \"host\" : {\n-                \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost\"\n-            }\n-        },\n-        {\n-            \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components/OOZIE_CLIENT\",\n-            \"HostRoles\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"OOZIE_CLIENT\",\n-                \"host_name\" : \"myhost\",\n-                \"state\" : \"INSTALLED\"\n-            },\n-            \"host\" : {\n-                \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost\"\n-            }\n-        },\n-        {\n-            \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components/PIG\",\n-            \"HostRoles\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"PIG\",\n-                \"host_name\" : \"myhost\",\n-                \"state\" : \"INSTALLED\"\n-            },\n-            \"host\" : {\n-                \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost\"\n-            }\n-        },\n-        {\n-            \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components/RESOURCEMANAGER\",\n-            \"HostRoles\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"RESOURCEMANAGER\",\n-                \"host_name\" : \"myhost\",\n-                \"state\" : \"STARTED\"\n-            },\n-            \"host\" : {\n-                \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost\"\n-            }\n-        },\n-        {\n-            \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components/SQOOP\",\n-            \"HostRoles\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"SQOOP\",\n-                \"host_name\" : \"myhost\",\n-                \"state\" : \"INSTALLED\"\n-            },\n-            \"host\" : {\n-                \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost\"\n-            }\n-        },\n-        {\n-            \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components/YARN_CLIENT\",\n-            \"HostRoles\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"YARN_CLIENT\",\n-                \"host_name\" : \"myhost\",\n-                \"state\" : \"INSTALLED\"\n-            },\n-            \"host\" : {\n-                \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost\"\n-            }\n-        },\n-        {\n-            \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components/ZOOKEEPER_CLIENT\",\n-            \"HostRoles\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"ZOOKEEPER_CLIENT\",\n-                \"host_name\" : \"myhost\",\n-                \"state\" : \"INSTALLED\"\n-            },\n-            \"host\" : {\n-                \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost\"\n-            }\n-        },\n-        {\n-            \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost/host_components/ZOOKEEPER_SERVER\",\n-            \"HostRoles\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"ZOOKEEPER_SERVER\",\n-                \"host_name\" : \"myhost\",\n-                \"state\" : \"STARTED\"\n-            },\n-            \"host\" : {\n-                \"href\" : \"http://myhost:8080/api/v1/clusters/cl1/hosts/myhost\"\n-            }\n-        }\n-    ]\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/hostmodel_get_host_components.json",
                "sha": "8148d1874f431b5fe3a5bed9a0374a79e4322ff6",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/servicemodel_get_component.json",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/servicemodel_get_component.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 20,
                "filename": "ambari-client/python-client/src/test/python/json/servicemodel_get_component.json",
                "patch": "@@ -1,20 +0,0 @@\n-{\n-    \"href\" : \"http://c6401.ambari.apache.org:8080/api/v1/clusters/cl1/services/GANGLIA/components/GANGLIA_SERVER\",\n-    \"ServiceComponentInfo\" : {\n-        \"cluster_name\" : \"cl1\",\n-        \"component_name\" : \"GANGLIA_SERVER\",\n-        \"service_name\" : \"GANGLIA\",\n-        \"state\" : \"STARTED\",\n-        \"desired_configs\" : { }\n-    },\n-    \"host_components\" : [\n-        {\n-            \"href\" : \"http://c6401.ambari.apache.org:8080/api/v1/clusters/cl1/hosts/c6402.ambari.apache.org/host_components/GANGLIA_SERVER\",\n-            \"HostRoles\" : {\n-                \"cluster_name\" : \"cl1\",\n-                \"component_name\" : \"GANGLIA_SERVER\",\n-                \"host_name\" : \"c6402.ambari.apache.org\"\n-            }\n-        }\n-    ]\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/servicemodel_get_component.json",
                "sha": "c6cbd08c05ec01613a2d7e858d14e42f63377202",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/servicemodel_get_components.json",
                "changes": 61,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/servicemodel_get_components.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 61,
                "filename": "ambari-client/python-client/src/test/python/json/servicemodel_get_components.json",
                "patch": "@@ -1,61 +0,0 @@\n-{\n-  \"href\" : \"http://c6401.ambari.apache.org:8080/api/v1/clusters/cl1/services/GANGLIA/components?fields=*\",\n-  \"items\" : [\n-    {\n-      \"href\" : \"http://c6401.ambari.apache.org:8080/api/v1/clusters/cl1/services/GANGLIA/components/GANGLIA_MONITOR\",\n-      \"ServiceComponentInfo\" : {\n-        \"cluster_name\" : \"cl1\",\n-        \"component_name\" : \"GANGLIA_MONITOR\",\n-        \"service_name\" : \"GANGLIA\",\n-        \"state\" : \"STARTED\",\n-        \"desired_configs\" : { }\n-      },\n-      \"host_components\" : [\n-        {\n-          \"href\" : \"http://c6401.ambari.apache.org:8080/api/v1/clusters/cl1/hosts/c6401.ambari.apache.org/host_components/GANGLIA_MONITOR\",\n-          \"HostRoles\" : {\n-            \"cluster_name\" : \"cl1\",\n-            \"component_name\" : \"GANGLIA_MONITOR\",\n-            \"host_name\" : \"c6401.ambari.apache.org\"\n-          }\n-        },\n-        {\n-          \"href\" : \"http://c6401.ambari.apache.org:8080/api/v1/clusters/cl1/hosts/c6402.ambari.apache.org/host_components/GANGLIA_MONITOR\",\n-          \"HostRoles\" : {\n-            \"cluster_name\" : \"cl1\",\n-            \"component_name\" : \"GANGLIA_MONITOR\",\n-            \"host_name\" : \"c6402.ambari.apache.org\"\n-          }\n-        },\n-        {\n-          \"href\" : \"http://c6401.ambari.apache.org:8080/api/v1/clusters/cl1/hosts/c6403.ambari.apache.org/host_components/GANGLIA_MONITOR\",\n-          \"HostRoles\" : {\n-            \"cluster_name\" : \"cl1\",\n-            \"component_name\" : \"GANGLIA_MONITOR\",\n-            \"host_name\" : \"c6403.ambari.apache.org\"\n-          }\n-        }\n-      ]\n-    },\n-    {\n-      \"href\" : \"http://c6401.ambari.apache.org:8080/api/v1/clusters/cl1/services/GANGLIA/components/GANGLIA_SERVER\",\n-      \"ServiceComponentInfo\" : {\n-        \"cluster_name\" : \"cl1\",\n-        \"component_name\" : \"GANGLIA_SERVER\",\n-        \"service_name\" : \"GANGLIA\",\n-        \"state\" : \"INSTALLED\",\n-        \"desired_configs\" : { }\n-      },\n-      \"host_components\" : [\n-        {\n-          \"href\" : \"http://c6401.ambari.apache.org:8080/api/v1/clusters/cl1/hosts/c6402.ambari.apache.org/host_components/GANGLIA_SERVER\",\n-          \"HostRoles\" : {\n-            \"cluster_name\" : \"cl1\",\n-            \"component_name\" : \"GANGLIA_SERVER\",\n-            \"host_name\" : \"c6402.ambari.apache.org\"\n-          }\n-        }\n-      ]\n-    }\n-  ]\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/servicemodel_get_components.json",
                "sha": "b9efe7368fa4c39bd86a67d220607ac3950b9863",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/status_error_with_message.json",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/status_error_with_message.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 4,
                "filename": "ambari-client/python-client/src/test/python/json/status_error_with_message.json",
                "patch": "@@ -1,4 +0,0 @@\n-{\n-    \"status\" : 400,\n-    \"message\" : \"java.lang.IllegalArgumentException: No enum const class org.apache.ambari.server.state.State.ILLEGAL_STATE\"\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/status_error_with_message.json",
                "sha": "69ca96d46b7d15aad76ff7991d4f500c794282da",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/status_ok_with_id.json",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/json/status_ok_with_id.json?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 7,
                "filename": "ambari-client/python-client/src/test/python/json/status_ok_with_id.json",
                "patch": "@@ -1,7 +0,0 @@\n-{\n-    \"href\" : \"http://localhost:8080/api/v1/clusters/test1/requests/19\",\n-    \"Requests\" : {\n-        \"id\" : 19,\n-        \"status\" : \"InProgress\"\n-    }\n-}",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/json/status_ok_with_id.json",
                "sha": "c805e38a88e098826a6afafead110e162f02670d",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/unitTests.py",
                "changes": 119,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/unitTests.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 119,
                "filename": "ambari-client/python-client/src/test/python/unitTests.py",
                "patch": "@@ -1,119 +0,0 @@\n-#!/usr/bin/env python\n-\n-'''\n-Licensed to the Apache Software Foundation (ASF) under one\n-or more contributor license agreements.  See the NOTICE file\n-distributed with this work for additional information\n-regarding copyright ownership.  The ASF licenses this file\n-to you under the Apache License, Version 2.0 (the\n-\"License\"); you may not use this file except in compliance\n-with the License.  You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-'''\n-\n-import unittest\n-import doctest\n-from os.path import dirname, split, isdir\n-import logging.handlers\n-import logging\n-\n-LOG_FILE_NAME='tests.log'\n-SELECTED_PREFIX = \"_\"\n-PY_EXT='.py'\n-\n-class TestAgent(unittest.TestSuite):\n-  def run(self, result):\n-    run = unittest.TestSuite.run\n-    run(self, result)\n-    return result\n-\n-\n-def parent_dir(path):\n-  if isdir(path):\n-    if path.endswith(os.sep):\n-      path = os.path.dirname(path)\n-    parent_dir = os.path.dirname(path)\n-  else:\n-    parent_dir = os.path.dirname(os.path.dirname(path))\n-\n-  return parent_dir\n-\n-\n-def all_tests_suite():\n-\n-\n-  src_dir = os.getcwd()\n-  files_list=os.listdir(src_dir)\n-  tests_list = []\n-\n-  logger.info('------------------------TESTS LIST:-------------------------------------')\n-  # If test with special name exists, run only this test\n-  selected_test = None\n-  for file_name in files_list:\n-    if file_name.endswith(PY_EXT) and not file_name == __file__ and file_name.startswith(SELECTED_PREFIX):\n-      logger.info(\"Running only selected test \" + str(file_name))\n-      selected_test = file_name\n-  if selected_test is not None:\n-      tests_list.append(selected_test.replace(PY_EXT, ''))\n-  else:\n-    for file_name in files_list:\n-      if file_name.endswith(PY_EXT) and not file_name == __file__:\n-        logger.info(file_name)\n-        tests_list.append(file_name.replace(PY_EXT, ''))\n-  logger.info('------------------------------------------------------------------------')\n-\n-  suite = unittest.TestLoader().loadTestsFromNames(tests_list)\n-  return TestAgent([suite])\n-\n-def main():\n-\n-  logger.info('------------------------------------------------------------------------')\n-  logger.info('PYTHON AGENT TESTS')\n-  logger.info('------------------------------------------------------------------------')\n-  runner = unittest.TextTestRunner(verbosity=2, stream=sys.stdout)\n-  suite = all_tests_suite()\n-  result = runner.run(suite)\n-  for error in result.errors:\n-    logger.error('Failed test:' + error[0]._testMethodName + '\\n' + error[1])\n-  for failure in result.failures:\n-    logger.error('Failed test:' + failure[0]._testMethodName + '\\n' + failure[1])\n-  status = result.wasSuccessful()\n-\n-  if not status:\n-    logger.error('-----------------------------------------------------------------------')\n-    logger.error('Python unit tests failed')\n-    logger.error('Find detailed logs in ' + path)\n-    logger.error('-----------------------------------------------------------------------')\n-    exit(1)\n-  else:\n-    logger.info('------------------------------------------------------------------------')\n-    logger.info('Python unit tests finished succesfully')\n-    logger.info('------------------------------------------------------------------------')\n-\n-if __name__ == '__main__':\n-  import os\n-  import sys\n-  import io\n-  sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))\n-  sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) + os.sep + 'main' + os.sep + 'python')\n-  sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) + os.sep + 'main' + os.sep + 'python' + os.sep + 'ambari_agent')\n-  logger = logging.getLogger()\n-  logger.setLevel(logging.INFO)\n-  formatter = logging.Formatter(\"[%(levelname)s] %(message)s\")\n-  src_dir = os.getcwd()\n-  target_dir = parent_dir(parent_dir(parent_dir(src_dir))) + os.sep + 'target'\n-  if not os.path.exists(target_dir):\n-    os.mkdir(target_dir)\n-  path = target_dir + os.sep + LOG_FILE_NAME\n-  file=open(path, \"w\")\n-  consoleLog = logging.StreamHandler(file)\n-  consoleLog.setFormatter(formatter)\n-  logger.addHandler(consoleLog)\n-  main()",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/unitTests.py",
                "sha": "eadfade005ce93c00ce57b1e0f301a2529d96909",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/utils/HttpClientInvoker.py",
                "changes": 153,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-client/python-client/src/test/python/utils/HttpClientInvoker.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 153,
                "filename": "ambari-client/python-client/src/test/python/utils/HttpClientInvoker.py",
                "patch": "@@ -1,153 +0,0 @@\n-#!/usr/bin/env python\n-\n-'''\n-Licensed to the Apache Software Foundation (ASF) under one\n-or more contributor license agreements.  See the NOTICE file\n-distributed with this work for additional information\n-regarding copyright ownership.  The ASF licenses this file\n-to you under the Apache License, Version 2.0 (the\n-\"License\"); you may not use this file except in compliance\n-with the License.  You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n-'''\n-import logging\n-import unittest\n-class HttpClientInvoker():\n-  @staticmethod\n-  def http_client_invoke_side_effects(*args, **kwargs):\n-      localss = locals()\n-      logger = logging.getLogger()\n-      logger.info(localss)\n-      http_method = args[0]\n-      url = args[1]\n-      payload = kwargs.get(\"payload\",None)\n-\n-      mocked_code = 200 \n-      mocked_content = \"text/plain\"\n-      \n-      if http_method == \"GET\":\n-        # ClusterModel mocking\n-        if url == \"//clusters/test1\":\n-          mocked_response = open('json/clustermodel_get_cluster.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == \"//clusters/test1/hosts/myhost\":\n-          mocked_response = open('json/clustermodel_get_host.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == \"//clusters/test1/hosts?fields=*\":\n-          mocked_response = open('json/clustermodel_get_all_hosts.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == \"//clusters/test1/configurations?type=global&tag=version1\":\n-          mocked_response = open('json/clustermodel_get_global_config.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == \"//clusters/test1/configurations?type=core-site&tag=version1\":\n-          mocked_response = open('json/clustermodel_get_core_site_config.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == \"//clusters/test1/configurations?type=hdfs-site&tag=version1\":\n-          mocked_response = open('json/clustermodel_get_hdfs_site_config.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == \"//clusters/test1/configurations?type=mapred-site&tag=version1\":\n-          mocked_response = open('json/clustermodel_get_mapred_site_config.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        # HostModel mocking\n-        elif url == '//clusters/test1/hosts/myhost/host_components/DATANODE':\n-          mocked_response = open('json/hostmodel_get_host_component.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == '//clusters/test1/hosts/myhost/host_components?fields=HostRoles/state':\n-          mocked_response = open('json/hostmodel_get_host_components.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        # ComponentModel mocking\n-        elif url == \"//clusters/cl1/hosts/myhost/host_components/DATANODE?fields=metrics\":\n-          mocked_response = open('json/componentmodel_get_metrics.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        # ServiceModel mocking\n-        elif url == \"//clusters/test1/services/GANGLIA/components?fields=*\":\n-          mocked_response = open('json/servicemodel_get_components.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == \"//clusters/test1/services/GANGLIA/components/GANGLIA_SERVER\":\n-          mocked_response = open('json/servicemodel_get_component.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        # AmbariClient mocking\n-        elif url == \"//clusters\":\n-          mocked_response = open('json/ambariclient_get_all_clusters.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == \"//hosts\":\n-          mocked_response = open('json/ambariclient_get_all_hosts.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == \"//hosts/dev06.hortonworks.com\":\n-          mocked_response = open('json/ambariclient_get_host.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == \"//stacks2/HDP/versions/1.3.0/stackServices/HDFS/configurations?fields=*\":\n-          mocked_response = open('json/ambariclient_get_config.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == \"//stacks2/HDP/versions/1.3.0/stackServices/HDFS/serviceComponents?fields=*\":\n-          mocked_response = open('json/ambariclient_get_components.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        # others\n-        elif url == \"//clusters/test1/services/GANGLIA\":\n-          mocked_response = open('json/clustermodel_get_service.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == \"//clusters/test1/services?fields=*\":\n-          mocked_response = open('json/clustermodel_get_all_services.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == \"//stacks2/HDP/versions/2.0.5/stackServices/HDFS/serviceComponents?fields=*\":\n-          mocked_response = open('json/get_components_from_stack.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == \"//clusters/test6/services/GANGLIA\":\n-          mocked_response = open('json/get_cluster_service.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == \"//clusters/test6/hosts/r01wn01/host_components/NAMENODE\":\n-          mocked_response = open('json/get_host_component.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == \"//clusters/test6/hosts/r01wn01/host_components?ServiceComponentInfo\":\n-          mocked_response = open('json/get_host_components.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == \"//clusters/test6/services/GANGLIA/components/GANGLIA_MONITOR\":\n-          mocked_response = open('json/get_service_component.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == \"//clusters/test6/services/GANGLIA/components?fields=*\":\n-          mocked_response = open('json/get_service_components.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        else:\n-          print \"Unknown get request on url: %s\" % url\n-      elif http_method == \"DELETE\":\n-        # ClusterModel\n-        if url == \"//clusters/test1/hosts/deleted_nonexistant_cluster\":\n-          mocked_response = open('json/clustermodel_error_deleting_host.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        else: # DELETE (generally does not require any response)\n-          return \"\", mocked_code , mocked_content\n-      elif http_method == \"POST\":\n-        # AmbariClient\n-        if url == \"//bootstrap\":\n-          mocked_response = open('json/ambariclient_bootstrap_hosts.json', 'r').read()\n-          return mocked_response, mocked_code , mocked_content\n-        elif url == '//clusters/test1/hosts?Hosts/host_name=myhost':\n-          mocked_code = 201\n-          return \"\", mocked_code , mocked_content\n-        else: # POST (generally does not require any response)\n-          return \"\", mocked_code , mocked_content\n-      else: # PUT (generally does not require any response)\n-        # ServiceModel mocking\n-        if url == \"//clusters/test1/services/GANGLIA\":\n-          payload_stop = {'ServiceInfo': {'state': 'INSTALLED'}}\n-          payload_started = {'ServiceInfo': {'state': 'STARTED'}}\n-          payload_illegal = {'ServiceInfo': {'state': 'ILLEGAL_STATE'}}\n-          if payload_stop == payload:\n-            mocked_response = open('json/status_ok_with_id.json', 'r').read()\n-            return mocked_response, mocked_code , mocked_content\n-          elif payload_started == payload:\n-            mocked_response = open('json/status_ok_with_id.json', 'r').read()\n-            return mocked_response, mocked_code , mocked_content\n-          elif payload_illegal == payload:\n-            mocked_response = open('json/status_error_with_message.json', 'r').read()\n-            return mocked_response, mocked_code , mocked_content\n-        else:\n-          return \"\", mocked_code , mocked_content\n-        \n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-client/python-client/src/test/python/utils/HttpClientInvoker.py",
                "sha": "dd753b230c1936b4575b365ade4e71419b72d6c9",
                "status": "removed"
            },
            {
                "additions": 100,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/buffered_queue.py",
                "changes": 100,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_commons/buffered_queue.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/ambari_commons/buffered_queue.py",
                "patch": "@@ -0,0 +1,100 @@\n+\"\"\"\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\"\"\"\n+\n+\n+from collections import deque\n+from threading import Event\n+\n+\n+class BufferedQueue(object):\n+  \"\"\"\n+  Thread safe buffered queue\n+  \"\"\"\n+\n+  def __init__(self):\n+    self.__queue = deque()\n+    self.__data_ready_event = Event()\n+\n+    self.__queue_end = False  # sign that buffer is empty\n+    self.__queue_feeder_end = False    # EOF sign\n+\n+  def __notify_ready(self):\n+    \"\"\"\n+    Notify reader that data is ready to be consumed\n+    \"\"\"\n+    self.__queue_end = False\n+    self.__data_ready_event.set()\n+\n+  def notify_end(self):\n+    \"\"\"\n+    Notify queue about end of producer stream, allow consumer to read buffer to the end\n+    \"\"\"\n+    self.__queue_feeder_end = True\n+    self.__notify_ready()\n+\n+  def put(self, item):\n+    \"\"\"\n+    Add object to the buffer\n+    \"\"\"\n+    if self.__queue_feeder_end:\n+      raise IndexError(\"'notify_end' was called, queue is locked for writing\")\n+\n+    self.__queue.append(item)\n+    self.__notify_ready()\n+\n+  def get(self, timeout=None):\n+    \"\"\"\n+    Read data from buffer at least in `timeout` seconds. If no data ready in `timeout`, would be returned None.\n+\n+    :param timeout: amount of time to wait for data availability\n+    :return: data or None if no data were read in `timeout` or no more data available (buffer is empty)\n+    \"\"\"\n+    try:\n+      if not self.__queue_feeder_end:\n+        self.__data_ready_event.wait(timeout)\n+      return self.__queue.popleft()\n+    except IndexError:\n+      if timeout:\n+        return None\n+\n+      self.__queue_end = True\n+    finally:\n+      if self.count == 0:\n+        self.__data_ready_event.clear()\n+        if self.__queue_feeder_end:\n+          self.__queue_end = True\n+\n+  def reset(self):\n+    \"\"\"\n+    Clear instance state and data\n+    \"\"\"\n+    self.__data_ready_event.clear()\n+    self.__queue.clear()\n+    self.__queue_feeder_end = False\n+    self.__queue_end = False\n+\n+  @property\n+  def empty(self):\n+    if self.__queue_feeder_end and self.count == 0:\n+      return True\n+\n+    return self.__queue_end\n+\n+  @property\n+  def count(self):\n+    return len(self.__queue)",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/buffered_queue.py",
                "sha": "23643eff0f7daebfc7bc608ffa39fccd34f4e17f",
                "status": "added"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/inet_utils.py",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_commons/inet_utils.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 18,
                "filename": "ambari-common/src/main/python/ambari_commons/inet_utils.py",
                "patch": "@@ -228,10 +228,9 @@ def resolve_address(address):\n       return '127.0.0.1'\n   return address\n \n-def ensure_ssl_using_protocol(protocol=\"PROTOCOL_TLSv1\", ca_certs=None):\n+def ensure_ssl_using_protocol(protocol=\"PROTOCOL_TLSv1_2\", ca_certs=None):\n   \"\"\"\n-  Monkey patching ssl module to force it use tls_v1. Do this in common module to avoid problems with\n-  PythonReflectiveExecutor.\n+  Patching ssl module to use configured protocol and ca certs\n \n   :param protocol: one of (\"PROTOCOL_SSLv2\", \"PROTOCOL_SSLv3\", \"PROTOCOL_SSLv23\", \"PROTOCOL_TLSv1\", \"PROTOCOL_TLSv1_1\", \"PROTOCOL_TLSv1_2\")\n   :param ca_certs: path to ca_certs file\n@@ -240,21 +239,6 @@ def ensure_ssl_using_protocol(protocol=\"PROTOCOL_TLSv1\", ca_certs=None):\n   from functools import wraps\n   import ssl\n \n-  if not hasattr(ssl.wrap_socket, \"_ambari_patched\"):\n-    def sslwrap(func):\n-      @wraps(func)\n-      def bar(*args, **kw):\n-        import ssl\n-        kw['ssl_version'] = getattr(ssl, protocol)\n-        if ca_certs and not 'ca_certs' in kw:\n-          kw['ca_certs'] = ca_certs\n-          kw['cert_reqs'] = ssl.CERT_REQUIRED\n-        return func(*args, **kw)\n-      bar._ambari_patched = True\n-      return bar\n-    ssl.wrap_socket = sslwrap(ssl.wrap_socket)\n-\n-  # python 2.7 stuff goes here\n   if hasattr(ssl, \"_create_default_https_context\"):\n     if not hasattr(ssl._create_default_https_context, \"_ambari_patched\"):\n       @wraps(ssl._create_default_https_context)",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/inet_utils.py",
                "sha": "0e160c506aca371e8ea2985da5e5af171c971cb4",
                "status": "modified"
            },
            {
                "additions": 71,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/__init__.py",
                "changes": 71,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_commons/repo_manager/__init__.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/ambari_commons/repo_manager/__init__.py",
                "patch": "@@ -0,0 +1,71 @@\n+\"\"\"\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+\"\"\"\n+__all__ = [\"ManagerFactory\", \"GenericManager\"]\n+\n+import threading\n+from ambari_commons import OSCheck, OSConst\n+\n+from .generic_manager import GenericManager\n+from .apt_manager import AptManager\n+from .yum_manager import YumManager\n+from .zypper_manager import ZypperManager\n+from .choco_manager import ChocoManager\n+\n+\n+class ManagerFactory(object):\n+  __lock = threading.Lock()\n+\n+  __repo_manager = None\n+  \"\"\":type GenericManager\"\"\"\n+\n+  @classmethod\n+  def get(cls):\n+    \"\"\"\n+    Return Repository Manager object for current OS in safe manner.\n+\n+    :rtype GenericManager\n+    \"\"\"\n+    if not cls.__repo_manager:\n+      with cls.__lock:\n+        cls.__repo_manager = cls.get_new_instance(OSCheck.get_os_family())\n+\n+    return cls.__repo_manager\n+\n+  @classmethod\n+  def get_new_instance(cls, os_family=None):\n+    \"\"\"\n+    Construct new instance of Repository Manager object. Call is not thread-safe\n+\n+    :param os_family:  os family string; best used in combination with `OSCheck.get_os_family()`\n+    :type os_family str\n+    :rtype GenericManager\n+    \"\"\"\n+    if not os_family:\n+      os_family = OSCheck.get_os_family()\n+\n+    construct_rules = {\n+      OSConst.UBUNTU_FAMILY: AptManager,\n+      OSConst.SUSE_FAMILY: ZypperManager,\n+      OSConst.REDHAT_FAMILY: YumManager,\n+      OSConst.WINSRV_FAMILY: ChocoManager\n+    }\n+    if os_family in construct_rules:\n+      return construct_rules[os_family]()\n+\n+    raise RuntimeError(\"Not able to create Repository Manager object for unsupported OS family {0}\".format(os_family))",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/__init__.py",
                "sha": "7b741a2c2e775e5ecbc535b96cfad1172ab3f6c3",
                "status": "added"
            },
            {
                "additions": 305,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/apt_manager.py",
                "changes": 305,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_commons/repo_manager/apt_manager.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/ambari_commons/repo_manager/apt_manager.py",
                "patch": "@@ -0,0 +1,305 @@\n+\"\"\"\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+\"\"\"\n+\n+import os\n+import tempfile\n+import re\n+from .generic_manager import GenericManager, GenericManagerProperties\n+from .apt_parser import AptParser\n+\n+from ambari_commons.constants import AMBARI_SUDO_BINARY\n+from ambari_commons import shell\n+from resource_management.core.logger import Logger\n+\n+\n+def replace_underscores(function_to_decorate):\n+  def wrapper(*args):\n+    self = args[0]\n+    name = args[1].replace(\"_\", \"-\")\n+    return function_to_decorate(self, name, *args[2:])\n+  return wrapper\n+\n+\n+class AptManagerProperties(GenericManagerProperties):\n+  \"\"\"\n+  Class to keep all Package-manager depended properties\n+  \"\"\"\n+  locked_output = \"Unable to lock the administration directory\"\n+  repo_error = \"Failure when receiving data from the peer\"\n+\n+  repo_manager_bin = \"/usr/bin/apt-get\"\n+  repo_cache_bin = \"/usr/bin/apt-cache\"\n+  pkg_manager_bin = \"/usr/bin/dpkg\"\n+  repo_update_cmd = [repo_manager_bin, 'update', '-qq']\n+\n+  available_packages_cmd = [repo_cache_bin, \"dump\"]\n+  installed_packages_cmd = [pkg_manager_bin, \"-l\"]\n+\n+  repo_definition_location = \"/etc/apt/sources.list.d\"\n+\n+  install_cmd = {\n+    True: [repo_manager_bin, '-o', \"Dpkg::Options::=--force-confdef\", '--allow-unauthenticated', '--assume-yes', 'install'],\n+    False: [repo_manager_bin, '-q', '-o', \"Dpkg::Options::=--force-confdef\", '--allow-unauthenticated', '--assume-yes', 'install']\n+  }\n+\n+  remove_cmd = {\n+    True: [repo_manager_bin, '-y', 'remove'],\n+    False: [repo_manager_bin, '-y', '-q', 'remove']\n+  }\n+\n+  verify_dependency_cmd = [repo_manager_bin, '-qq', 'check']\n+\n+  install_cmd_env = {'DEBIAN_FRONTEND': 'noninteractive'}\n+\n+  check_cmd = pkg_manager_bin + \" --get-selections | grep -v deinstall | awk '{print $1}' | grep ^%s$\"\n+\n+  repo_url_exclude = \"ubuntu.com\"\n+  configuration_dump_cmd = [AMBARI_SUDO_BINARY, \"apt-config\", \"dump\"]\n+\n+\n+class AptManager(GenericManager):\n+\n+  def get_installed_package_version(self, package_name):\n+    r = shell.subprocess_executor(\"dpkg -s {0} | grep Version | awk '{{print $2}}'\".format(package_name))\n+    return r.out.strip(os.linesep)\n+\n+  @property\n+  def properties(self):\n+    return AptManagerProperties\n+\n+  def installed_packages(self, pkg_names=None, repo_filter=None):\n+    \"\"\"\n+    Return all installed packages in the system except packages in REPO_URL_EXCLUDE\n+\n+    :type pkg_names list|set\n+    :type repo_filter str|None\n+\n+    :return formatted list of packages\n+    \"\"\"\n+    packages = []\n+    available_packages = self._available_packages_dict(pkg_names, repo_filter)\n+\n+    with shell.process_executor(self.properties.installed_packages_cmd, error_callback=self._executor_error_handler) as output:\n+      for package, version in AptParser.packages_installed_reader(output):\n+        if package in available_packages:\n+          packages.append(available_packages[package])\n+\n+        if package not in available_packages:\n+          packages.append([package, version, \"installed\"])  # case, when some package not belongs to any known repo\n+\n+    return packages\n+\n+  def _available_packages(self, pkg_names=None, repo_filter=None):\n+    \"\"\"\n+    Returning list of the installed packages with possibility to filter them by name\n+\n+    :type pkg_names list|set\n+    :type repo_filter str|None\n+    \"\"\"\n+\n+    with shell.process_executor(self.properties.available_packages_cmd, error_callback=self._executor_error_handler) as output:\n+      for pkg_item in AptParser.packages_reader(output):\n+        if repo_filter and repo_filter not in pkg_item[2]:\n+          continue\n+        if self.properties.repo_url_exclude in pkg_item[2]:\n+          continue\n+        if pkg_names and pkg_item[0] not in pkg_names:\n+          continue\n+\n+        yield pkg_item\n+\n+  def _available_packages_dict(self, pkg_names=None, repo_filter=None):\n+    \"\"\"\n+    Same as available packages, but result returns as dict and package name as key\n+\n+    :type pkg_names list|set\n+    :type repo_filter str|None\n+    \"\"\"\n+    result = {}\n+\n+    for item in self._available_packages(pkg_names, repo_filter):\n+      result[item[0]] = item\n+\n+    return result\n+\n+  def available_packages(self, pkg_names=None, repo_filter=None):\n+    \"\"\"\n+    Returning list of the installed packages with possibility to filter them by name\n+\n+    :type pkg_names list|set\n+    :type repo_filter str|None\n+    \"\"\"\n+    return [item for item in self._available_packages(pkg_names, repo_filter)]\n+\n+  def all_packages(self, pkg_names=None, repo_filter=None):\n+    return self.available_packages(pkg_names, repo_filter)\n+\n+  def get_available_packages_in_repos(self, repos):\n+    \"\"\"\n+    Gets all (both installed and available) packages that are available at given repositories.\n+    :type repos resource_management.libraries.functions.repository_util.CommandRepository\n+    :return: installed and available packages from these repositories\n+    \"\"\"\n+\n+    filtered_packages = []\n+    packages = self.available_packages()\n+    repo_ids = []\n+\n+    for repo in repos.items:\n+      repo_ids.append(repo.base_url.replace(\"http://\", \"\").replace(\"/\", \"_\"))\n+\n+    if repos.feat.scoped:\n+      Logger.info(\"Looking for matching packages in the following repositories: {0}\".format(\", \".join(repo_ids)))\n+      for repo_id in repo_ids:\n+        for package in packages:\n+          if repo_id in package[2]:\n+            filtered_packages.append(package[0])\n+\n+      return filtered_packages\n+    else:\n+      Logger.info(\"Packages will be queried using all available repositories on the system.\")\n+      return [package[0] for package in packages]\n+\n+  def package_manager_configuration(self):\n+    \"\"\"\n+    Reading apt configuration\n+\n+    :return dict with apt properties\n+    \"\"\"\n+    with shell.process_executor(self.properties.configuration_dump_cmd, error_callback=self._executor_error_handler) as output:\n+      configuration = list(AptParser.config_reader(output))\n+\n+    return dict(configuration)\n+\n+  def verify_dependencies(self):\n+    \"\"\"\n+    Verify that we have no dependency issues in package manager. Dependency issues could appear because of aborted or terminated\n+    package installation process or invalid packages state after manual modification of packages list on the host\n+\n+    :return True if no dependency issues found, False if dependency issue present\n+    :rtype bool\n+    \"\"\"\n+    r = shell.subprocess_executor(self.properties.verify_dependency_cmd)\n+    pattern = re.compile(\"has missing dependency|E:\")\n+\n+    if r.code or (r.outout and pattern.search(r.out)):\n+      err_msg = Logger.filter_text(\"Failed to verify package dependencies. Execution of '%s' returned %s. %s\" % (VERIFY_DEPENDENCY_CMD, code, out))\n+      Logger.error(err_msg)\n+      return False\n+\n+    return True\n+\n+  @replace_underscores\n+  def install_package(self, name, context):\n+    \"\"\"\n+    Install package\n+\n+    :type name str\n+    :type context ambari_commons.shell.RepoCallContext\n+    \"\"\"\n+    from resource_management.core import sudo\n+\n+    apt_sources_list_tmp_dir = None\n+\n+    if context.is_upgrade or context.use_repos or not self._check_existence(name):\n+      cmd = self.properties.install_cmd[context.log_output]\n+      copied_sources_files = []\n+      is_tmp_dir_created = False\n+      if context.use_repos:\n+        if 'base' in context.use_repos:\n+          use_repos = set([v for k, v in context.use_repos.items() if k != 'base'])\n+        else:\n+          cmd = cmd + ['-o', 'Dir::Etc::SourceList={0}'.format(self.properties.empty_file)]\n+          use_repos = set(context.use_repos.values())\n+\n+        if use_repos:\n+          is_tmp_dir_created = True\n+          apt_sources_list_tmp_dir = tempfile.mkdtemp(suffix=\"-ambari-apt-sources-d\")\n+          Logger.info(\"Temporary sources directory was created: %s\" % apt_sources_list_tmp_dir)\n+\n+          for repo in use_repos:\n+            new_sources_file = os.path.join(apt_sources_list_tmp_dir, repo + '.list')\n+            Logger.info(\"Temporary sources file will be copied: {0}\".format(new_sources_file))\n+            sudo.copy(os.path.join(self.properties.repo_definition_location, repo + '.list'), new_sources_file)\n+            copied_sources_files.append(new_sources_file)\n+          cmd = cmd + ['-o', 'Dir::Etc::SourceParts='.format(apt_sources_list_tmp_dir)]\n+\n+      cmd = cmd + [name]\n+      Logger.info(\"Installing package {0} ('{1}')\".format(name, shell.string_cmd_from_args_list(cmd)))\n+      shell.repository_manager_executor(cmd, self.properties, context, env=self.properties.install_cmd_env)\n+\n+      if is_tmp_dir_created:\n+        for temporary_sources_file in copied_sources_files:\n+          Logger.info(\"Removing temporary sources file: {0}\".format(temporary_sources_file))\n+          os.remove(temporary_sources_file)\n+        if apt_sources_list_tmp_dir:\n+          Logger.info(\"Removing temporary sources directory: {0}\".format(apt_sources_list_tmp_dir))\n+          os.rmdir(apt_sources_list_tmp_dir)\n+    else:\n+      Logger.info(\"Skipping installation of existing package {0}\".format(name))\n+\n+  @replace_underscores\n+  def upgrade_package(self, name, context):\n+    \"\"\"\n+    Install package\n+\n+    :type name str\n+    :type context ambari_commons.shell.RepoCallContext\n+    \"\"\"\n+    context.is_upgrade = True\n+    return self.install_package(name, context)\n+\n+  @replace_underscores\n+  def remove_package(self, name, context, ignore_dependencies=False):\n+    \"\"\"\n+    Remove package\n+\n+    :type name str\n+    :type context ambari_commons.shell.RepoCallContext\n+    :type ignore_dependencies bool\n+    \"\"\"\n+    if self._check_existence(name):\n+      cmd = self.properties.remove_cmd[context.log_output] + [name]\n+      Logger.info(\"Removing package {0} ('{1}')\".format(name, shell.string_cmd_from_args_list(cmd)))\n+      shell.repository_manager_executor(cmd, self.properties, context)\n+    else:\n+      Logger.info(\"Skipping removal of non-existing package {0}\".format(name))\n+\n+  @replace_underscores\n+  def _check_existence(self, name):\n+    \"\"\"\n+    For regexp names:\n+    If only part of packages were installed during early canceling.\n+    Let's say:\n+    1. install hbase-2-3-.*\n+    2. Only hbase-2-3-1234 is installed, but is not hbase-2-3-1234-regionserver yet.\n+    3. We cancel the apt-get\n+\n+    In that case this is bug of packages we require.\n+    And hbase-2-3-*-regionserver should be added to metainfo.xml.\n+\n+    Checking existence should never fail in such a case for hbase-2-3-.*, otherwise it\n+    gonna break things like removing packages and some other things.\n+\n+    Note: this method SHOULD NOT use apt-get (apt.cache is using dpkg not apt). Because a lot of issues we have, when customer have\n+    apt-get in inconsistant state (locked, used, having invalid repo). Once packages are installed\n+    we should not rely on that.\n+    \"\"\"\n+    r = shell.subprocess_executor(self.properties.check_cmd % name)\n+    return not bool(r.code)",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/apt_manager.py",
                "sha": "e05ac5f89b6e5e982215aa0655508979eaa49df7",
                "status": "added"
            },
            {
                "additions": 144,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/apt_parser.py",
                "changes": 144,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_commons/repo_manager/apt_parser.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/ambari_commons/repo_manager/apt_parser.py",
                "patch": "@@ -0,0 +1,144 @@\n+\"\"\"\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+\"\"\"\n+\n+from .generic_parser import GenericParser\n+\n+\n+class AptParser(GenericParser):\n+  @staticmethod\n+  def config_reader(stream):\n+    \"\"\"\n+    apt-config dump command parser\n+\n+    Function consumes io.TextIOBase compatible objects as input and return iterator with parsed items\n+\n+    :type stream collections.Iterable\n+    :rtype collections.Iterable\n+    :return tuple(key, value)\n+\n+    Usage:\n+      for key, value in __config_reader(text_stream):\n+        ...\n+\n+    Parsing subject:\n+\n+       PROPERTY \"\";\n+       PROPERTY::ITEM1:: \"value\";\n+       .....\n+\n+    \"\"\"\n+    for line in stream:\n+      key, value = line.strip().split(\" \", 1)\n+      key = key.strip(\"::\")\n+      value = value.strip(\";\").strip(\"\\\"\").strip()\n+      if not value:\n+        continue\n+\n+      yield key, value\n+\n+  @staticmethod\n+  def packages_reader(stream):\n+    \"\"\"\n+    apt-cache dump command parser\n+\n+    Function consumes io.TextIOBase compatible objects as input and return iterator with parsed items\n+\n+    :type stream collections.Iterable\n+    :rtype collections.Iterable\n+    :return tuple(package name, version, parsed repo list file)\n+\n+    Usage:\n+      for package, version, repo_file_path in __packages_reader(text_stream):\n+        ...\n+\n+    Parsing subject:\n+\n+        Package: test_package\n+     Version: 0.1.1-0\n+         File: /var/lib/apt/lists/some_site_dists_apt_main_binary-amd64_Packages.gz\n+     Description Language:\n+                     File: /var/lib/apt/lists/some_site_dists_apt_main_binary-amd64_Packages.gz\n+                      MD5: 000000000000000000000000000\n+    \"\"\"\n+    fields = {\"Package\": 0, \"Version\": 1, \"File\": 2}\n+    field_names = fields.keys()\n+    field_count = len(field_names)\n+    item_set = [None] * field_count\n+\n+    for line in stream:\n+      line = line.strip()\n+\n+      if not line:\n+        continue\n+\n+      values = line.split(\":\", 1)\n+      if len(values) != 2:\n+        continue\n+\n+      field, value = values\n+      value = value[1:]\n+\n+      if field in field_names:\n+        if field == \"File\":\n+          value = value.rpartition(\"/\")[2]\n+        elif field == \"Package\":\n+          item_set = [None] * field_count  # reset fields which were parsed before new block\n+        item_set[fields[field]] = value\n+      else:\n+        continue\n+\n+      if None not in item_set:\n+        yield item_set\n+        item_set = [None] * field_count\n+\n+  @staticmethod\n+  def packages_installed_reader(stream):\n+    \"\"\"\n+    dpkg -l command parser\n+\n+    Function consumes io.TextIOBase compatible objects as input and return iterator with parsed items\n+\n+    :type stream collections.Iterable\n+    :rtype collections.Iterable\n+    :return tuple(package name, version)\n+\n+    Usage:\n+      for package, version in __packages_installed_reader(text_stream):\n+        ...\n+\n+    Parsing subject:\n+\n+      ||/ Name                              Version               Architecture          Description\n+      +++-=================================-=====================-=====================-======================\n+      ii  package1                           version1                all                   description1\n+      ii  package2                           version2                all                   description2\n+    \"\"\"\n+    for line in stream:\n+      line = line.lstrip()\n+\n+      if line[:2] != \"ii\":\n+        continue\n+\n+      line = line[2:].lstrip()\n+      data = line.partition(\" \")\n+      pkg_name = data[0]\n+      version = data[2].strip().partition(\" \")[0]\n+\n+      if pkg_name and version:\n+        yield pkg_name, version",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/apt_parser.py",
                "sha": "03afb86cbc5a1652f6efa22f3f27e3a953c99f03",
                "status": "added"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/choco_manager.py",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_commons/repo_manager/choco_manager.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 22,
                "filename": "ambari-common/src/main/python/ambari_commons/repo_manager/choco_manager.py",
                "patch": "@@ -14,16 +14,14 @@\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n-\n-Ambari Agent\n-\n \"\"\"\n \n-from resource_management.core.providers.package import PackageProvider\n-from resource_management.core.logger import Logger\n+from ambari_commons.repo_manager.generic_manager import GenericManagerProperties, GenericManager\n from ambari_commons.shell import shellRunner\n \n-import os\n+from resource_management.core.logger import Logger\n+#from ambari_commons.shell import shellRunner\n+\n \n INSTALL_CMD = {\n   True: ['cmd', '/c', 'choco', 'install', '--pre', '-y', '-v'],\n@@ -45,12 +43,23 @@\n   False: ['cmd', '/c', 'choco', 'list', '--pre', '--local-only'],\n }\n \n-class ChocoProvider(PackageProvider):\n-  def install_package(self, name, use_repos={}, skip_repos=[]):\n-    if not self._check_existence(name) or use_repos:\n-      cmd = INSTALL_CMD[self.get_logoutput()]\n-      if use_repos:\n-        enable_repo_option = '-s' + \",\".join(sorted(use_repos.keys()))\n+\n+class ChocoManagerProperties(GenericManagerProperties):\n+  pass\n+\n+\n+class ChocoManager(GenericManager):\n+  def install_package(self, name, context):\n+    \"\"\"\n+    Install package\n+\n+    :type name str\n+    :type context ambari_commons.shell.RepoCallContext\n+    \"\"\"\n+    if not self._check_existence(name) or context.use_repos:\n+      cmd = INSTALL_CMD[context.log_output]\n+      if context.use_repos:\n+        enable_repo_option = '-s' + \",\".join(sorted(context.use_repos.keys()))\n         cmd = cmd + [enable_repo_option]\n       cmd = cmd + [name]\n       cmdString = \" \".join(cmd)\n@@ -62,10 +71,16 @@ def install_package(self, name, use_repos={}, skip_repos=[]):\n     else:\n       Logger.info(\"Skipping installation of existing package %s\" % (name))\n \n-  def upgrade_package(self, name, use_repos={}, skip_repos=[]):\n-    cmd = UPGRADE_CMD[self.get_logoutput()]\n-    if use_repos:\n-      enable_repo_option = '-s' + \",\".join(sorted(use_repos.keys()))\n+  def upgrade_package(self, name, context):\n+    \"\"\"\n+    Install package\n+\n+    :type name str\n+    :type context ambari_commons.shell.RepoCallContext\n+    \"\"\"\n+    cmd = UPGRADE_CMD[context.log_output]\n+    if context.use_repos:\n+      enable_repo_option = '-s' + \",\".join(sorted(context.use_repos.keys()))\n       cmd = cmd + [enable_repo_option]\n     cmd = cmd + [name]\n     cmdString = \" \".join(cmd)\n@@ -75,9 +90,16 @@ def upgrade_package(self, name, use_repos={}, skip_repos=[]):\n     if res['exitCode'] != 0:\n       raise Exception(\"Error while upgrading choco package \" + name + \". \" + res['error'] + res['output'])\n \n-  def remove_package(self, name):\n-    if self._check_existence(name):\n-      cmd = REMOVE_CMD[self.get_logoutput()] + [name]\n+  def remove_package(self, name, context, ignore_dependencies=False):\n+    \"\"\"\n+    Remove package\n+\n+    :type name str\n+    :type context ambari_commons.shell.RepoCallContext\n+    :type ignore_dependencies bool\n+    \"\"\"\n+    if self._check_existence(name, context):\n+      cmd = REMOVE_CMD[context.log_output] + [name]\n       cmdString = \" \".join(cmd)\n       Logger.info(\"Removing package %s ('%s')\" % (name, \" \".join(cmd)))\n       runner = shellRunner()\n@@ -87,10 +109,10 @@ def remove_package(self, name):\n     else:\n       Logger.info(\"Skipping removal of non-existing package %s\" % (name))\n \n-  def _check_existence(self, name):\n-    cmd = CHECK_CMD[self.get_logoutput()] + [name]\n+  def _check_existence(self, name, context):\n+    cmd = CHECK_CMD[context.log_output] + [name]\n     runner = shellRunner()\n     res = runner.run(cmd)\n     if name in res['output']:\n       return True\n-    return False\n\\ No newline at end of file\n+    return False",
                "previous_filename": "ambari-common/src/main/python/resource_management/core/providers/package/choco.py",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/choco_manager.py",
                "sha": "edace06faef3456ae3fe3886b043c5346e3f58dd",
                "status": "renamed"
            },
            {
                "additions": 245,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/generic_manager.py",
                "changes": 245,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_commons/repo_manager/generic_manager.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/ambari_commons/repo_manager/generic_manager.py",
                "patch": "@@ -0,0 +1,245 @@\n+\"\"\"\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+\"\"\"\n+\n+import re\n+\n+from resource_management.core.logger import Logger\n+\n+\n+class GenericManagerProperties(object):\n+  \"\"\"\n+  Class to keep all Package-manager depended properties. Each non-generic implementation should override properties\n+  declared here\n+  \"\"\"\n+  empty_file = \"/dev/null\"\n+  locked_output = None\n+  repo_error = None\n+\n+  repo_manager_bin = None\n+  pkg_manager_bin = None\n+  repo_update_cmd = None\n+\n+  available_packages_cmd = None\n+  installed_packages_cmd = None\n+  all_packages_cmd = None\n+\n+  repo_definition_location = None\n+\n+  install_cmd = {\n+    True: None,\n+    False: None\n+  }\n+\n+  remove_cmd = {\n+    True: None,\n+    False: None\n+  }\n+\n+  verify_dependency_cmd = None\n+\n+\n+class GenericManager(object):\n+  \"\"\"\n+  Interface for all custom implementations.  Provides the required base for any custom manager, to be smoothly integrated\n+  \"\"\"\n+\n+  @property\n+  def properties(self):\n+    return GenericManagerProperties\n+\n+  def install_package(self, name, context):\n+    \"\"\"\n+    Install package\n+\n+    :type name str\n+    :type context ambari_commons.shell.RepoCallContext\n+    \"\"\"\n+    raise NotImplementedError()\n+\n+  def remove_package(self, name, context, ignore_dependencies=False):\n+    \"\"\"\n+    Remove package\n+\n+    :type name str\n+    :type context ambari_commons.shell.RepoCallContext\n+    :type ignore_dependencies bool\n+    \"\"\"\n+    raise NotImplementedError()\n+\n+  def upgrade_package(self, name, context):\n+    \"\"\"\n+    Install package\n+\n+    :type name str\n+    :type context ambari_commons.shell.RepoCallContext\n+    \"\"\"\n+    raise NotImplementedError()\n+\n+  def check_uncompleted_transactions(self):\n+    \"\"\"\n+    Check package manager against uncompleted transactions.\n+\n+    :rtype bool\n+    \"\"\"\n+    return False\n+\n+  def print_uncompleted_transaction_hint(self):\n+    \"\"\"\n+    Print friendly message about they way to fix the issue\n+\n+    \"\"\"\n+    pass\n+\n+  def get_available_packages_in_repos(self, repositories):\n+    \"\"\"\n+    Gets all (both installed and available) packages that are available at given repositories.\n+    :type repositories resource_management.libraries.functions.repository_util.CommandRepository\n+    :return: installed and available packages from these repositories\n+    \"\"\"\n+    raise NotImplementedError()\n+\n+  def installed_packages(self, pkg_names=None, repo_filter=None):\n+    raise NotImplementedError()\n+\n+  def available_packages(self, pkg_names=None, repo_filter=None):\n+    raise NotImplementedError()\n+\n+  def all_packages(self, pkg_names=None, repo_filter=None):\n+    raise NotImplementedError()\n+\n+  def get_installed_repos(self, hint_packages, all_packages, ignore_repos):\n+    \"\"\"\n+    Gets all installed repos by name based on repos that provide any package\n+    contained in hintPackages\n+    Repos starting with value in ignoreRepos will not be returned\n+    hintPackages must be regexps.\n+    \"\"\"\n+    all_repos = []\n+    repo_list = []\n+\n+    for hintPackage in hint_packages:\n+      for item in all_packages:\n+        if re.match(hintPackage, item[0]) and not item[2] in all_repos:\n+          all_repos.append(item[2])\n+\n+    for repo in all_repos:\n+      ignore = False\n+      for ignoredRepo in ignore_repos:\n+        if self.name_match(ignoredRepo, repo):\n+          ignore = True\n+      if not ignore:\n+        repo_list.append(repo)\n+\n+    return repo_list\n+\n+  def get_installed_pkgs_by_repo(self, repos, ignore_packages, installed_packages):\n+    \"\"\"\n+    Get all the installed packages from the repos listed in repos\n+    \"\"\"\n+    packages_from_repo = []\n+    packages_to_remove = []\n+    for repo in repos:\n+      sub_result = []\n+      for item in installed_packages:\n+        if repo == item[2]:\n+          sub_result.append(item[0])\n+      packages_from_repo = list(set(packages_from_repo + sub_result))\n+\n+    for package in packages_from_repo:\n+      keep_package = True\n+      for ignorePackage in ignore_packages:\n+        if self.name_match(ignorePackage, package):\n+          keep_package = False\n+          break\n+      if keep_package:\n+        packages_to_remove.append(package)\n+    return packages_to_remove\n+\n+  def get_installed_pkgs_by_names(self, pkg_names, all_packages_list=None):\n+    \"\"\"\n+    Gets all installed packages that start with names in pkgNames\n+    :type pkg_names list[str]\n+    :type all_packages_list list[str]\n+    \"\"\"\n+    return self.installed_packages(pkg_names)\n+\n+  def get_package_details(self, installed_packages, found_packages):\n+    \"\"\"\n+    Gets the name, version, and repoName for the packages\n+    :type installed_packages list[tuple[str,str,str]]\n+    :type found_packages list[str]\n+    \"\"\"\n+    package_details = []\n+\n+    for package in found_packages:\n+      pkg_detail = {}\n+      for installed_package in installed_packages:\n+        if package == installed_package[0]:\n+          pkg_detail['name'] = installed_package[0]\n+          pkg_detail['version'] = installed_package[1]\n+          pkg_detail['repoName'] = installed_package[2]\n+\n+      package_details.append(pkg_detail)\n+\n+    return package_details\n+\n+  def get_repos_to_remove(self, repos, ignore_list):\n+    repos_to_remove = []\n+    for repo in repos:\n+      add_to_remove_list = True\n+      for ignore_repo in ignore_list:\n+        if self.name_match(ignore_repo, repo):\n+          add_to_remove_list = False\n+          continue\n+      if add_to_remove_list:\n+        repos_to_remove.append(repo)\n+    return repos_to_remove\n+\n+  def get_installed_package_version(self, package_name):\n+    raise NotImplementedError()\n+\n+  def verify_dependencies(self):\n+    \"\"\"\n+    Verify that we have no dependency issues in package manager. Dependency issues could appear because of aborted or terminated\n+    package installation process or invalid packages state after manual modification of packages list on the host\n+\n+    :return True if no dependency issues found, False if dependency issue present\n+    :rtype bool\n+    \"\"\"\n+    raise NotImplementedError()\n+\n+  def name_match(self, lookup_name, actual_name):\n+    tokens = actual_name.strip().lower()\n+    lookup_name = lookup_name.lower()\n+\n+    return \" \" not in lookup_name and lookup_name in tokens\n+\n+  def _executor_error_handler(self, command, error_log, exit_code):\n+    \"\"\"\n+    Error handler for ac_shell.process_executor\n+\n+    :type command list|str\n+    :type error_log list\n+    :type exit_code int\n+    \"\"\"\n+    if isinstance(command, (list, tuple)):\n+      command = \" \".join(command)\n+\n+    Logger.error(\"Command execution error: command = \\\"{0}\\\", exit code = {1}, stderr = {2}\".format(\n+      command, exit_code, \"\\n\".join(error_log)))",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/generic_manager.py",
                "sha": "d4e1e082d2a48d44bf231a7bc08edcb47df62388",
                "status": "added"
            },
            {
                "additions": 63,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/generic_parser.py",
                "changes": 63,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_commons/repo_manager/generic_parser.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/ambari_commons/repo_manager/generic_parser.py",
                "patch": "@@ -0,0 +1,63 @@\n+\"\"\"\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+\"\"\"\n+\n+import re\n+\n+# basic base output cleanup patterns\n+ANSI_ESCAPE = re.compile(r'\\x1B\\[[0-?]*[ -/]*[@-~]')  # exclude bash control sequences\n+EXCLUDE_CHARS = [ord(\"\\r\"), ord(\"\\n\"), ord(\"\\t\")]\n+REMOVE_CHARS = \"\".join([chr(i) for i in range(0, 31) if i not in EXCLUDE_CHARS])  # exclude control characters\n+\n+\n+class GenericParser(object):\n+  \"\"\"\n+   Base for the any custom parser. New parser rules:\n+   - no subprocess calls\n+   - as input should be provided iterable text\n+   - result should be returned as ready to consume object, preferably as generator\n+\n+   Samples available for zypper, yum, apt\n+  \"\"\"\n+\n+  @staticmethod\n+  def config_reader(stream):\n+    \"\"\"\n+    :type stream collections.Iterable\n+    :rtype collections.Iterable\n+    :return tuple(key, value)\n+    \"\"\"\n+    raise NotImplementedError()\n+\n+  @staticmethod\n+  def packages_reader(stream):\n+    \"\"\"\n+    :type stream collections.Iterable\n+    :rtype collections.Iterable\n+    :return tuple(package name, version, parsed repo list file)\n+    \"\"\"\n+    raise NotImplementedError()\n+\n+  @staticmethod\n+  def packages_installed_reader(stream):\n+    \"\"\"\n+      :type stream collections.Iterable\n+      :rtype collections.Iterable\n+      :return tuple(package name, version)\n+    \"\"\"\n+    raise NotImplementedError()",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/generic_parser.py",
                "sha": "2941646dc84642acab071ef34cc4cea511c8dd0d",
                "status": "added"
            },
            {
                "additions": 154,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/yum_manager.py",
                "changes": 324,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_commons/repo_manager/yum_manager.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 170,
                "filename": "ambari-common/src/main/python/ambari_commons/repo_manager/yum_manager.py",
                "patch": "@@ -1,4 +1,3 @@\n-#!/usr/bin/env python\n \"\"\"\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n@@ -15,17 +14,14 @@\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n-\n-Ambari Agent\n-\n \"\"\"\n+\n import ConfigParser\n import glob\n \n-from ambari_commons.constants import AMBARI_SUDO_BINARY\n-from resource_management.core.providers.package import RPMBasedPackageProvider\n-from resource_management.core import shell\n-from resource_management.core.shell import string_cmd_from_args_list\n+from .generic_manager import GenericManagerProperties, GenericManager\n+from .yum_parser import YumParser\n+from ambari_commons import shell\n from resource_management.core.logger import Logger\n from resource_management.core.utils import suppress_stdout\n from resource_management.core import sudo\n@@ -35,40 +31,48 @@\n import re\n import os\n \n-INSTALL_CMD = {\n-  True: ['/usr/bin/yum', '-y', 'install'],\n-  False: ['/usr/bin/yum', '-d', '0', '-e', '0', '-y', 'install'],\n-}\n \n-REMOVE_CMD = {\n-  True: ['/usr/bin/yum', '-y', 'erase'],\n-  False: ['/usr/bin/yum', '-d', '0', '-e', '0', '-y', 'erase'],\n-}\n+class YumManagerProperties(GenericManagerProperties):\n+  \"\"\"\n+  Class to keep all Package-manager depended properties\n+  \"\"\"\n+  locked_output = None\n+  repo_error = \"Failure when receiving data from the peer\", \"Nothing to do\"\n+\n+  repo_manager_bin = \"/usr/bin/yum\"\n+  pkg_manager_bin = \"/usr/bin/rpm\"\n+  repo_update_cmd = [repo_manager_bin, \"clean\", \"metadata\"]\n+\n+  available_packages_cmd = [repo_manager_bin, \"list\", \"available\", \"--showduplicates\"]\n+  installed_packages_cmd = [repo_manager_bin, \"list\", \"installed\", \"--showduplicates\"]\n+  all_packages_cmd = [repo_manager_bin, \"list\", \"all\", \"--showduplicates\"]\n \n-REMOVE_WITHOUT_DEPENDENCIES_CMD = ['rpm', '-e', '--nodeps']\n+  yum_lib_dir = \"/var/lib/yum\"\n+  yum_tr_prefix = \"transaction-\"\n \n-YUM_LIB_DIR = \"/var/lib/yum\"\n-YUM_TR_PREFIX = \"transaction-\"\n+  repo_definition_location = \"/etc/yum.repos.d\"\n \n-YUM_REPO_LOCATION = \"/etc/yum.repos.d\"\n-REPO_UPDATE_CMD = ['/usr/bin/yum', 'clean', 'metadata']\n-ALL_INSTALLED_PACKAGES_CMD = [AMBARI_SUDO_BINARY, \"yum\", \"list\", \"installed\"]\n-ALL_AVAILABLE_PACKAGES_CMD = [AMBARI_SUDO_BINARY, \"yum\", \"list\", \"available\"]\n-VERIFY_DEPENDENCY_CMD = ['/usr/bin/yum', '-d', '0', '-e', '0', 'check', 'dependencies']\n+  install_cmd = {\n+    True: [repo_manager_bin, '-y', 'install'],\n+    False: [repo_manager_bin, '-d', '0', '-e', '0', '-y', 'install']\n+  }\n \n-# base command output sample:\n-# -----------------------------\n-# select.noarch                       2.5.6.0-40.el6            REPO-2.5\n-# select.noarch                       2.6.3.0-56                REPO-2.6.3.0-56\n-# select.noarch                       2.6.3.0-57                REPO-2.6.3.0-57\n-# select.noarch                       2.6.3.0-63                REPO-2.6.3.0\n-# select.noarch                       2.6.3.0-63                REPO-2.6.3.0-63\n+  remove_cmd = {\n+    True: [repo_manager_bin, '-y', 'erase'],\n+    False: [repo_manager_bin, '-d', '0', '-e', '0', '-y', 'erase']\n+  }\n \n-LIST_ALL_SELECT_TOOL_PACKAGES_CMD = \"yum list all --showduplicates|grep -v '@' |grep '^{pkg_name}'|awk '{print $2}'\"\n-SELECT_TOOL_VERSION_PATTERN = re.compile(\"(\\d{1,2}\\.\\d{1,2}\\.\\d{1,2}\\.\\d{1,2}-*\\d*).*\")  # xx.xx.xx.xx(-xxxx)\n+  verify_dependency_cmd = [repo_manager_bin, '-d', '0', '-e', '0', 'check', 'dependencies']\n+  installed_package_version_command = [pkg_manager_bin, \"-q\", \"--queryformat\", \"%{{version}}-%{{release}}\"]\n \n+  remove_without_dependencies_cmd = ['rpm', '-e', '--nodeps']\n \n-class YumProvider(RPMBasedPackageProvider):\n+\n+class YumManager(GenericManager):\n+\n+  @property\n+  def properties(self):\n+    return YumManagerProperties\n \n   def get_available_packages_in_repos(self, repos):\n     \"\"\"\n@@ -88,8 +92,8 @@ def get_available_packages_in_repos(self, repos):\n \n     for repo in repo_ids:\n       repo = repo if repos.feat.scoped else None\n-      available_packages.extend(self._get_available_packages(repo))\n-      installed_packages.extend(self._get_installed_packages(repo))\n+      available_packages.extend(self.available_packages(repo_filter=repo))\n+      installed_packages.extend(self.installed_packages(repo_filter=repo))\n \n     # fallback logic\n \n@@ -99,140 +103,79 @@ def get_available_packages_in_repos(self, repos):\n         Logger.info(\"Adding fallback repositories: {0}\".format(\", \".join(fallback_repo_ids)))\n \n         for repo in fallback_repo_ids:\n-          available_packages.extend(self._get_available_packages(repo))\n-          installed_packages.extend(self._get_installed_packages(repo))\n+          available_packages.extend(self.available_packages(repo_filter=repo))\n+          installed_packages.extend(self.installed_packages(repo_filter=repo))\n \n     return [package[0] for package in available_packages + installed_packages]\n \n-  def get_all_package_versions(self, pkg_name):\n-    \"\"\"\n-    :type pkg_name str\n-    \"\"\"\n-    command = LIST_ALL_SELECT_TOOL_PACKAGES_CMD.replace(\"{pkg_name}\", pkg_name)\n-    result = self._call_with_timeout(command)\n-\n-    if result[\"retCode\"] == 0:\n-       return result[\"out\"].split(os.linesep)\n-\n-    return None\n-\n-  def __parse_select_tool_version(self, v):\n-    \"\"\"\n-    :type v str\n-    \"\"\"\n-    matches = SELECT_TOOL_VERSION_PATTERN.findall(v.strip())\n-    return matches[0] if matches else None\n-\n-  def normalize_select_tool_versions(self, versions):\n-    \"\"\"\n-    Function expect output from get_all_package_versions\n-\n-    :type versions str|list|set\n-    :rtype list\n-    \"\"\"\n-    if isinstance(versions, str):\n-      versions = [versions]\n-\n-    return [self.__parse_select_tool_version(i) for i in versions]\n-\n-  def _get_available_packages(self, repo_filter=None):\n+  def available_packages(self, pkg_names=None, repo_filter=None):\n     \"\"\"\n     Returning list of available packages with possibility to filter them by name\n-    :param repo_filter: repository name\n \n+    :type pkg_names list|set\n     :type repo_filter str|None\n     :rtype list[list,]\n     \"\"\"\n \n-    cmd = list(ALL_AVAILABLE_PACKAGES_CMD)\n-\n+    packages = []\n+    cmd = list(self.properties.available_packages_cmd)\n     if repo_filter:\n       cmd.extend([\"--disablerepo=*\", \"--enablerepo=\" + repo_filter])\n \n-    return self._lookup_packages(cmd, 'Available Packages')\n+    with shell.process_executor(cmd, error_callback=self._executor_error_handler) as output:\n+      for pkg in YumParser.packages_reader(output):\n+        if pkg_names and not pkg[0] in pkg_names:\n+          continue\n \n-  def _get_installed_packages(self, repo_filter=None):\n+        packages.append(pkg)\n+\n+    return packages\n+\n+  def installed_packages(self, pkg_names=None, repo_filter=None):\n     \"\"\"\n     Returning list of the installed packages with possibility to filter them by name\n-    :param repo_filter: repository name\n \n+    :type pkg_names list|set\n     :type repo_filter str|None\n     :rtype list[list,]\n     \"\"\"\n \n-    packages = self._lookup_packages(list(ALL_INSTALLED_PACKAGES_CMD), \"Installed Packages\")\n-    if repo_filter:\n-      packages = [item for item in packages if item[2].lower() == repo_filter.lower()]\n-\n-    return packages\n-\n-  def _lookup_packages(self, command, skip_till):\n-    \"\"\"\n-    :type command list[str]\n-    :type skip_till str|None\n-    \"\"\"\n     packages = []\n+    cmd = self.properties.installed_packages_cmd\n \n-    result = self._call_with_timeout(command)\n-\n-    if result and 0 == result['retCode']:\n-      lines = result['out'].split('\\n')\n-      lines = [line.strip() for line in lines]\n-      items = []\n-      if skip_till:\n-        skip_index = 3\n-        for index in range(len(lines)):\n-          if skip_till in lines[index]:\n-            skip_index = index + 1\n-            break\n-      else:\n-        skip_index = 0\n-\n-      for line in lines[skip_index:]:\n-        items = items + line.strip(' \\t\\n\\r').split()\n-\n-      items_count = len(items)\n+    with shell.process_executor(cmd, error_callback=self._executor_error_handler) as output:\n+      for pkg in YumParser.packages_reader(output):\n+        if pkg_names and not pkg[0] in pkg_names:\n+          continue\n \n-      for i in range(0, items_count, 3):\n-\n-        # check if we reach the end\n-        if i+3 > items_count:\n-          break\n-\n-        if '.' in items[i]:\n-          items[i] = items[i][:items[i].rindex('.')]\n-        if items[i + 2].find('@') == 0:\n-          items[i + 2] = items[i + 2][1:]\n-        packages.append(items[i:i + 3])\n+        if repo_filter and repo_filter.lower() != pkg[2].lower():\n+          continue\n+        packages.append(pkg)\n \n     return packages\n \n-  def all_available_packages(self, result_type=list, group_by_index=-1):\n+  def all_packages(self, pkg_names=None, repo_filter=None):\n     \"\"\"\n-    Return all available packages in the system except packages in REPO_URL_EXCLUDE\n-\n-    :arg result_type Could be list or dict, defines type of returning value\n-    :arg group_by_index index of element in the __packages_reader result, which would be used as key\n-    :return result_type formatted list of packages, including installed and available in repos\n+    Returning list of the installed packages with possibility to filter them by name\n \n-    :type result_type type\n-    :type group_by_index int\n-    :rtype list|dict\n+    :type pkg_names list|set\n+    :type repo_filter str|None\n+    :rtype list[list,]\n     \"\"\"\n-    #  ToDo: move to iterative package lookup (check apt provider for details)\n-    return self._get_available_packages(None)\n \n-  def all_installed_packages(self, from_unknown_repo=False):\n-    \"\"\"\n-    Return all installed packages in the system except packages in REPO_URL_EXCLUDE\n+    packages = []\n+    cmd = self.properties.all_packages_cmd\n \n-    :arg from_unknown_repo return packages from unknown repos\n-    :type from_unknown_repo bool\n+    with shell.process_executor(cmd, error_callback=self._executor_error_handler) as output:\n+      for pkg in YumParser.packages_reader(output):\n+        if pkg_names and not pkg[0] in pkg_names:\n+          continue\n \n-    :return result_type formatted list of packages\n-    \"\"\"\n-    #  ToDo: move to iterative package lookup (check apt provider for details)\n-    return self._get_installed_packages(None)\n+        if repo_filter and repo_filter.lower() != pkg[2].lower():\n+          continue\n+        packages.append(pkg)\n+\n+    return packages\n \n   def verify_dependencies(self):\n     \"\"\"\n@@ -242,42 +185,64 @@ def verify_dependencies(self):\n     :return True if no dependency issues found, False if dependency issue present\n     :rtype bool\n     \"\"\"\n-    code, out = self.checked_call(VERIFY_DEPENDENCY_CMD, sudo=True)\n+    ret = shell.subprocess_executor(self.properties.verify_dependency_cmd)\n     pattern = re.compile(\"has missing requires|Error:\")\n \n-    if code or (out and pattern.search(out)):\n-      err_msg = Logger.filter_text(\"Failed to verify package dependencies. Execution of '%s' returned %s. %s\" % (VERIFY_DEPENDENCY_CMD, code, out))\n+    if ret.code or (ret.out and pattern.search(ret.out)):\n+      err_msg = Logger.filter_text(\"Failed to verify package dependencies. Execution of '{0}' returned {1}. {2}\".format(\n+        self.properties.verify_dependency_cmd, ret.code, ret.out))\n       Logger.error(err_msg)\n       return False\n \n     return True\n \n-  def install_package(self, name, use_repos={}, skip_repos=set(), is_upgrade=False):\n-    if is_upgrade or use_repos or not self._check_existence(name):\n-      cmd = INSTALL_CMD[self.get_logoutput()]\n-      if use_repos:\n-        enable_repo_option = '--enablerepo=' + \",\".join(sorted(use_repos.keys()))\n-        disable_repo_option = '--disablerepo=' + \"*\" if len(skip_repos) == 0 else ','.join(skip_repos)\n+  def install_package(self, name, context):\n+    \"\"\"\n+    Install package\n+\n+    :type name str\n+    :type context ambari_commons.shell.RepoCallContext\n+    \"\"\"\n+\n+    if context.is_upgrade or context.use_repos or not self._check_existence(name):\n+      cmd = self.properties.install_cmd[context.log_output]\n+      if context.use_repos:\n+        enable_repo_option = '--enablerepo=' + \",\".join(sorted(context.use_repos.keys()))\n+        disable_repo_option = '--disablerepo=' + \"*\" if not context.skip_repos or len(context.skip_repos) == 0 else ','.join(context.skip_repos)\n         cmd = cmd + [disable_repo_option, enable_repo_option]\n       cmd = cmd + [name]\n-      Logger.info(\"Installing package %s ('%s')\" % (name, string_cmd_from_args_list(cmd)))\n-      self.checked_call_with_retries(cmd, sudo=True, logoutput=self.get_logoutput())\n+      Logger.info(\"Installing package {0} ('{1}')\".format(name, shell.string_cmd_from_args_list(cmd)))\n+      shell.repository_manager_executor(cmd, self.properties, context)\n     else:\n-      Logger.info(\"Skipping installation of existing package %s\" % (name))\n+      Logger.info(\"Skipping installation of existing package {0}\".format(name))\n+\n+  def upgrade_package(self, name, context):\n+    \"\"\"\n+    Install package\n \n-  def upgrade_package(self, name, use_repos={}, skip_repos=set(), is_upgrade=True):\n-    return self.install_package(name, use_repos, skip_repos, is_upgrade)\n+    :type name str\n+    :type context ambari_commons.shell.RepoCallContext\n+    \"\"\"\n+    context.is_upgrade = True\n+    return self.install_package(name, context)\n \n-  def remove_package(self, name, ignore_dependencies=False):\n+  def remove_package(self, name, context, ignore_dependencies=False):\n+    \"\"\"\n+    Remove package\n+\n+    :type name str\n+    :type context ambari_commons.shell.RepoCallContext\n+    :type ignore_dependencies bool\n+    \"\"\"\n     if self._check_existence(name):\n       if ignore_dependencies:\n-        cmd = REMOVE_WITHOUT_DEPENDENCIES_CMD + [name]\n+        cmd = self.properties.remove_without_dependencies_cmd + [name]\n       else:\n-        cmd = REMOVE_CMD[self.get_logoutput()] + [name]\n-      Logger.info(\"Removing package %s ('%s')\" % (name, string_cmd_from_args_list(cmd)))\n-      shell.checked_call(cmd, sudo=True, logoutput=self.get_logoutput())\n+        cmd = self.properties.remove_cmd[context.log_output] + [name]\n+      Logger.info(\"Removing package {0} ('{1}')\".format(name, shell.string_cmd_from_args_list(cmd)))\n+      shell.repository_manager_executor(cmd, self.properties, context)\n     else:\n-      Logger.info(\"Skipping removal of non-existing package %s\" % (name))\n+      Logger.info(\"Skipping removal of non-existing package {0}\".format(name))\n \n   def _check_existence(self, name):\n     \"\"\"\n@@ -322,14 +287,6 @@ def yum_check_package_available(self, name):\n \n     return False\n \n-  def is_repo_error_output(self, out):\n-    return \"Failure when receiving data from the peer\" in out or \\\n-           \"Nothing to do\" in out\n-\n-  def get_repo_update_cmd(self):\n-    return REPO_UPDATE_CMD\n-\n-\n   @staticmethod\n   def _build_repos_ids(repos):\n     \"\"\"\n@@ -356,7 +313,7 @@ def _build_repos_ids(repos):\n     # for every repo file, find any which match the base URLs we're trying to write out\n     # if there are any matches, it means the repo already exists and we should use it to search\n     # for packages to install\n-    for repo_file in glob.glob(os.path.join(YUM_REPO_LOCATION, \"*.repo\")):\n+    for repo_file in glob.glob(os.path.join(YumManagerProperties.repo_definition_location, \"*.repo\")):\n       config_parser = ConfigParser.ConfigParser()\n       config_parser.read(repo_file)\n       sections = config_parser.sections()\n@@ -373,6 +330,33 @@ def _build_repos_ids(repos):\n \n     return set(repo_ids)\n \n+  def rpm_check_package_available(self, name):\n+    import rpm # this is faster then calling 'rpm'-binary externally.\n+    ts = rpm.TransactionSet()\n+    packages = ts.dbMatch()\n+\n+    name_regex = re.escape(name).replace(\"\\\\?\", \".\").replace(\"\\\\*\", \".*\") + '$'\n+    regex = re.compile(name_regex)\n+\n+    for package in packages:\n+      if regex.match(package['name']):\n+        return True\n+    return False\n+\n+  def get_installed_package_version(self, package_name):\n+    version = None\n+    cmd = list(self.properties.installed_package_version_command) + [\"\\\"{0}\\\"\".format(package_name)]\n+\n+    result = shell.subprocess_executor(cmd)\n+\n+    try:\n+      if result.code == 0:\n+        version = result.out.strip().partition(\".el\")[0]\n+    except IndexError:\n+      pass\n+\n+    return version\n+\n   def __extract_transaction_id(self, filename):\n     \"\"\"\n     :type filename str\n@@ -395,12 +379,12 @@ def uncomplete_transactions(self):\n     \"\"\"\n     transactions = {}\n \n-    prefix_len = len(YUM_TR_PREFIX)\n-    for item in sudo.listdir(YUM_LIB_DIR):\n-      if YUM_TR_PREFIX == item[:prefix_len]:\n+    prefix_len = len(self.properties.yum_tr_prefix)\n+    for item in sudo.listdir(self.properties.yum_lib_dir):\n+      if self.properties.yum_tr_prefix == item[:prefix_len]:\n         tr_id = self.__extract_transaction_id(item)\n \n-        f = StringIO(sudo.read_file(os.path.join(YUM_LIB_DIR, item)))\n+        f = StringIO(sudo.read_file(os.path.join(self.properties.yum_lib_dir, item)))\n         pkgs_in_transaction = list(self.__transaction_file_parser(f))\n \n         if tr_id not in transactions:",
                "previous_filename": "ambari-common/src/main/python/resource_management/core/providers/package/yumrpm.py",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/yum_manager.py",
                "sha": "5e404b13b7a7b7c949e4d255b92054537008ed8c",
                "status": "renamed"
            },
            {
                "additions": 147,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/yum_parser.py",
                "changes": 147,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_commons/repo_manager/yum_parser.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/ambari_commons/repo_manager/yum_parser.py",
                "patch": "@@ -0,0 +1,147 @@\n+\"\"\"\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+\"\"\"\n+\n+from .generic_parser import GenericParser, ANSI_ESCAPE, REMOVE_CHARS\n+\n+\n+class YumParser(GenericParser):\n+  @staticmethod\n+  def config_reader(stream):\n+    # ToDo: future feature\n+    raise NotImplementedError()\n+\n+  @staticmethod\n+  def packages_installed_reader(stream):\n+    \"\"\"\n+    For yum, the output content for `yum list available|all|installed` is the same\n+\n+    :param stream collections.Iterable\n+    \"\"\"\n+    return YumParser.packages_reader(stream)\n+\n+  @staticmethod\n+  def list_all_select_tool_packages_reader(stream):\n+    \"\"\"\n+    apt-config dump command parser\n+\n+    Function consumes io.TextIOBase compatible objects as input and return iterator with parsed items\n+\n+    :type stream collections.Iterable\n+    :rtype collections.Iterable\n+    :return tuple(key, value)\n+\n+    Usage:\n+      for key, value in __config_reader(text_stream):\n+        ...\n+\n+    Parsing subject:\n+\n+       PROPERTY \"\";\n+       PROPERTY::ITEM1:: \"value\";\n+       .....\n+\n+    \"\"\"\n+    for line in stream:\n+      key, value = line.strip().split(\" \", 1)\n+      key = key.strip(\"::\")\n+      value = value.strip(\";\").strip(\"\\\"\").strip()\n+      if not value:\n+        continue\n+\n+      yield key, value\n+\n+  @staticmethod\n+  def lookup_packages(lines, skip_till):\n+    \"\"\"\n+    Deprecated and not used function, would be removed in later releases.\n+\n+    :type lines str\n+    :type skip_till str|None\n+    \"\"\"\n+    packages = []\n+\n+    lines = [line.strip() for line in lines]\n+    items = []\n+    if skip_till:\n+      skip_index = 3\n+      for index in range(len(lines)):\n+        if skip_till in lines[index]:\n+          skip_index = index + 1\n+          break\n+    else:\n+      skip_index = 0\n+\n+    for line in lines[skip_index:]:\n+      items = items + line.strip(' \\t\\n\\r').split()\n+\n+    items_count = len(items)\n+\n+    for i in range(0, items_count, 3):\n+\n+      # check if we reach the end\n+      if i+3 > items_count:\n+        break\n+\n+      if '.' in items[i]:\n+        items[i] = items[i][:items[i].rindex('.')]\n+      if items[i + 2].find('@') == 0:\n+        items[i + 2] = items[i + 2][1:]\n+      packages.append(items[i:i + 3])\n+\n+    return packages\n+\n+  @staticmethod\n+  def packages_reader(stream):\n+    \"\"\"\n+    :type stream collections.Iterable\n+\n+    Notice:  yum contains bug, which makes it format output by using default term with 80 on non-pty terminal.\n+      It makes it to use \"Work Wrap\" for long lines. To avoid such behaviour, use execution mechanic which allow\n+      to set proper terminal geometry.\n+\n+\n+    Sample text:\n+\n+    Loaded plugins: fastestmirror, ovl\n+    Loading mirror speeds from cached hostfile\n+     * base: centos.colocall.net\n+     * extras: centos.colocall.net\n+     * updates: centos.colocall.net\n+    Installed Packages\n+    MAKEDEV.x86_64                           3.24-6.el6                  @CentOS/6.9\n+    audit-libs.x86_64                        2.4.5-6.el6                 @CentOS/6.9\n+    basesystem.noarch                        10.0-4.el6                  @CentOS/6.9\n+    bash.x86_64                              4.1.2-48.el6                @CentOS/6.9\n+    bind-libs.x86_64                         32:9.8.2-0.62.rc1.el6_9.4   @Updates/6.9\n+    \"\"\"\n+\n+    for line in stream:\n+      line = ANSI_ESCAPE.sub('', line).translate(None, REMOVE_CHARS)  # strip sh control seq. and ascii control ch.\n+      pkg_name, _, line = line.lstrip().partition(\" \")\n+      pkg_name, _, _ = str(pkg_name).rpartition(\".\")  # cut architecture from package name\n+      pkg_version, _, line = line.lstrip().partition(\" \")\n+      if not pkg_version[:1].isdigit():\n+        continue\n+      repo_name, _, line = line.lstrip().partition(\" \")\n+\n+      if repo_name[:1] == \"@\":\n+        repo_name = repo_name[1:]\n+\n+      if pkg_name and pkg_version and repo_name:\n+        yield pkg_name, pkg_version, repo_name",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/yum_parser.py",
                "sha": "567a39a6840a6c18aead6b566207f8ea2b7eb0ee",
                "status": "added"
            },
            {
                "additions": 285,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/zypper_manager.py",
                "changes": 285,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_commons/repo_manager/zypper_manager.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/ambari_commons/repo_manager/zypper_manager.py",
                "patch": "@@ -0,0 +1,285 @@\n+\"\"\"\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\"\"\"\n+from ambari_commons import shell\n+from .generic_manager import GenericManagerProperties, GenericManager\n+from .zypper_parser import ZypperParser\n+from resource_management.core.logger import Logger\n+\n+import re\n+\n+\n+class ZypperManagerProperties(GenericManagerProperties):\n+  \"\"\"\n+  Class to keep all Package-manager depended properties\n+  \"\"\"\n+  locked_output = \"System management is locked by the application\"\n+  repo_error = \"Failure when receiving data from the peer\"\n+\n+  repo_manager_bin = \"/usr/bin/zypper\"\n+  pkg_manager_bin = \"/usr/bin/rpm\"\n+  repo_update_cmd = [repo_manager_bin, \"clean\"]\n+\n+  available_packages_cmd = [repo_manager_bin, \"--no-gpg-checks\", \"search\", \"--uninstalled-only\", \"--details\"]\n+  installed_packages_cmd = [repo_manager_bin, \"--no-gpg-checks\", \"search\", \"--installed-only\", \"--details\"]\n+  all_packages_cmd = [repo_manager_bin, \"--no-gpg-checks\", \"search\", \"--details\"]\n+\n+  repo_definition_location = \"/etc/zypp/repos.d\"\n+\n+  install_cmd = {\n+    True: [repo_manager_bin, \"install\", \"--auto-agree-with-licenses\", \"--no-confirm\"],\n+    False: [repo_manager_bin, \"--quiet\", \"install\", \"--auto-agree-with-licenses\", \"--no-confirm\"]\n+  }\n+\n+  remove_cmd = {\n+    True: [repo_manager_bin, \"remove\", \"--no-confirm\"],\n+    False: [repo_manager_bin, \"--quiet\", \"remove\", \"--no-confirm\"]\n+  }\n+\n+  verify_dependency_cmd = [repo_manager_bin, \"--quiet\", \"--non-interactive\", \"verify\", \"--dry-run\"]\n+  list_active_repos_cmd = ['/usr/bin/zypper', 'repos']\n+  installed_package_version_command = [pkg_manager_bin, \"-q\", \"--queryformat\", \"%{{version}}-%{{release}}\"]\n+\n+\n+class ZypperManager(GenericManager):\n+\n+  @property\n+  def properties(self):\n+    return ZypperManagerProperties\n+\n+  def get_available_packages_in_repos(self, repos):\n+    \"\"\"\n+    Gets all (both installed and available) packages that are available at given repositories.\n+    :type repos resource_management.libraries.functions.repository_util.CommandRepository\n+    :return: installed and available packages from these repositories\n+    \"\"\"\n+\n+    available_packages = []\n+    repo_ids = [repository.repo_id for repository in repos.items]\n+\n+    # zypper cant tell from which repository were installed package, as repo r matching by pkg_name\n+    # as result repository would be matched if it contains package with same meta info\n+\n+    if repos.feat.scoped:\n+      Logger.info(\"Looking for matching packages in the following repositories: {0}\".format(\", \".join(repo_ids)))\n+      for repo in repo_ids:\n+        available_packages.extend(self.all_packages(repo_filter=repo))\n+    else:\n+      Logger.info(\"Packages will be queried using all available repositories on the system.\")\n+      available_packages.extend(self.all_packages())\n+\n+    return [package[0] for package in available_packages]\n+\n+  def installed_packages(self, pkg_names=None, repo_filter=None):\n+    \"\"\"\n+    Returning list of the installed packages with possibility to filter them by name\n+\n+    :type pkg_names list|set\n+    :type repo_filter str|None\n+    :rtype list[list,]\n+    \"\"\"\n+    packages = []\n+    cmd = list(self.properties.installed_packages_cmd)\n+\n+    if repo_filter:\n+      cmd.extend([\"--repo=\" + repo_filter])\n+\n+    with shell.process_executor(cmd, error_callback=self._executor_error_handler) as output:\n+      for pkg in ZypperParser.packages_reader(output):\n+        if pkg_names and not pkg[0] in pkg_names:\n+          continue\n+\n+        packages.append(pkg)\n+\n+    return packages\n+\n+  def available_packages(self, pkg_names=None, repo_filter=None):\n+    \"\"\"\n+    Returning list of the available packages with possibility to filter them by name\n+\n+    :type pkg_names list|set\n+    :type repo_filter str|None\n+    :rtype list[list,]\n+    \"\"\"\n+    packages = []\n+    cmd = list(self.properties.available_packages_cmd)\n+\n+    if repo_filter:\n+      cmd.extend([\"--repo=\" + repo_filter])\n+\n+    with shell.process_executor(cmd, error_callback=self._executor_error_handler) as output:\n+      for pkg in ZypperParser.packages_reader(output):\n+        if pkg_names and not pkg[0] in pkg_names:\n+          continue\n+\n+        packages.append(pkg)\n+\n+    return packages\n+\n+  def all_packages(self, pkg_names=None, repo_filter=None):\n+    \"\"\"\n+    Returning list of the all packages with possibility to filter them by name\n+\n+    :type pkg_names list|set\n+    :type repo_filter str|None\n+    :rtype list[list,]\n+    \"\"\"\n+    packages = []\n+    cmd = list(self.properties.all_packages_cmd)\n+\n+    if repo_filter:\n+      cmd.extend([\"--repo=\" + repo_filter])\n+\n+    with shell.process_executor(cmd, error_callback=self._executor_error_handler) as output:\n+      for pkg in ZypperParser.packages_reader(output):\n+        if pkg_names and not pkg[0] in pkg_names:\n+          continue\n+\n+        packages.append(pkg)\n+\n+    return packages\n+\n+  def verify_dependencies(self):\n+    \"\"\"\n+    Verify that we have no dependency issues in package manager. Dependency issues could appear because of aborted or terminated\n+    package installation process or invalid packages state after manual modification of packages list on the host\n+\n+    :return True if no dependency issues found, False if dependency issue present\n+    :rtype bool\n+    \"\"\"\n+    r = shell.subprocess_executor(self.properties.verify_dependency_cmd)\n+    pattern = re.compile(\"\\d+ new package(s)? to install\")\n+\n+    if r.code or (r.out and pattern.search(r.out)):\n+      err_msg = Logger.filter_text(\"Failed to verify package dependencies. Execution of '{0}' returned {1}. {2}\".format(\n+        self.properties.verify_dependency_cmd, r.code, r.out))\n+      Logger.error(err_msg)\n+      return False\n+\n+    return True\n+\n+  def install_package(self, name, context):\n+    \"\"\"\n+    Install package\n+\n+    :type name str\n+    :type context ambari_commons.shell.RepoCallContext\n+    \"\"\"\n+    if context.is_upgrade or context.use_repos or not self._check_existence(name):\n+      cmd = self.properties.install_cmd[context.log_output]\n+\n+      if context.use_repos:\n+        active_base_repos = self.get_active_base_repos()\n+        if 'base' in context.use_repos:\n+          # Remove 'base' from use_repos list\n+          use_repos = filter(lambda x: x != 'base', context.use_repos)\n+          use_repos.extend(active_base_repos)\n+        use_repos_options = []\n+        for repo in sorted(context.use_repos):\n+          use_repos_options = use_repos_options + ['--repo', repo]\n+        cmd = cmd + use_repos_options\n+\n+      cmd = cmd + [name]\n+      Logger.info(\"Installing package {0} ('{1}')\".format(name, shell.string_cmd_from_args_list(cmd)))\n+\n+      shell.repository_manager_executor(cmd, self.properties, context)\n+    else:\n+      Logger.info(\"Skipping installation of existing package {0}\".format(name))\n+\n+  def upgrade_package(self, name, context):\n+    \"\"\"\n+    Install package\n+\n+    :type name str\n+    :type context ambari_commons.shell.RepoCallContext\n+    \"\"\"\n+    context.is_upgrade = True\n+    return self.install_package(name, context)\n+\n+  def remove_package(self, name, context, ignore_dependencies=False):\n+    \"\"\"\n+    Remove package\n+\n+    :type name str\n+    :type context ambari_commons.shell.RepoCallContext\n+    :type ignore_dependencies bool\n+    \"\"\"\n+    if self._check_existence(name):\n+      cmd = self.properties.remove_cmd[context.log_output] + [name]\n+      Logger.info(\"Removing package {0} ('{1}')\".format(name, shell.string_cmd_from_args_list(cmd)))\n+      shell.repository_manager_executor(cmd, self.properties, context)\n+    else:\n+      Logger.info(\"Skipping removal of non-existing package {0}\".format(name))\n+\n+  def get_active_base_repos(self):\n+    enabled_repos = []\n+\n+    with shell.process_executor(self.properties.list_active_repos_cmd, error_callback=self._executor_error_handler) as output:\n+      for _, repo_name, repo_enabled, _ in ZypperParser.repo_list_reader(output):\n+        if repo_enabled and repo_name.startswith(\"SUSE-\"):\n+          enabled_repos.append(repo_name)\n+\n+        if repo_enabled and (\"OSS\" in repo_name) or (\"OpenSuse\" in repo_name):\n+          enabled_repos.append(repo_name)\n+\n+    return enabled_repos\n+\n+  def rpm_check_package_available(self, name):\n+    import rpm # this is faster then calling 'rpm'-binary externally.\n+    ts = rpm.TransactionSet()\n+    packages = ts.dbMatch()\n+\n+    name_regex = re.escape(name).replace(\"\\\\?\", \".\").replace(\"\\\\*\", \".*\") + '$'\n+    regex = re.compile(name_regex)\n+\n+    for package in packages:\n+      if regex.match(package['name']):\n+        return True\n+    return False\n+\n+  def get_installed_package_version(self, package_name):\n+    version = None\n+    cmd = list(self.properties.installed_package_version_command) + [\"\\\"{0}\\\"\".format(package_name)]\n+    result = shell.subprocess_executor(cmd)\n+    try:\n+      if result.code == 0:\n+        version = result.out.strip().partition(\".el\")[0]\n+    except IndexError:\n+      pass\n+\n+    return version\n+\n+  def _check_existence(self, name):\n+    \"\"\"\n+    For regexp names:\n+    If only part of packages were installed during early canceling.\n+    Let's say:\n+    1. install hbase_2_3_*\n+    2. Only hbase_2_3_1234 is installed, but is not hbase_2_3_1234_regionserver yet.\n+    3. We cancel the zypper\n+\n+    In that case this is bug of packages we require.\n+    And hbase_2_3_*_regionserver should be added to metainfo.xml.\n+\n+    Checking existence should never fail in such a case for hbase_2_3_*, otherwise it\n+    gonna break things like removing packages and some other things.\n+\n+    Note: this method SHOULD NOT use zypper. Because a lot of issues we have, when customer have\n+    zypper in inconsistant state (locked, used, having invalid repo). Once packages are installed\n+    we should not rely on that.\n+    \"\"\"\n+    return self.rpm_check_package_available(name)",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/zypper_manager.py",
                "sha": "8617ef9388ef9fb6debc6d08429324a5ea9dd97e",
                "status": "added"
            },
            {
                "additions": 149,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/zypper_parser.py",
                "changes": 149,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_commons/repo_manager/zypper_parser.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/ambari_commons/repo_manager/zypper_parser.py",
                "patch": "@@ -0,0 +1,149 @@\n+\"\"\"\n+Licensed to the Apache Software Foundation (ASF) under one\n+or more contributor license agreements.  See the NOTICE file\n+distributed with this work for additional information\n+regarding copyright ownership.  The ASF licenses this file\n+to you under the Apache License, Version 2.0 (the\n+\"License\"); you may not use this file except in compliance\n+with the License.  You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+\"\"\"\n+\n+from .generic_parser import GenericParser, ANSI_ESCAPE, REMOVE_CHARS\n+\n+\n+class ZypperParser(GenericParser):\n+  @staticmethod\n+  def config_reader(stream):\n+    # ToDo: future feature\n+    raise NotImplementedError()\n+\n+  @staticmethod\n+  def packages_reader(stream):\n+    \"\"\"\n+    :type stream collections.Iterable\n+\n+    Sample text:\n+\n+    Loading repository data...\n+    Reading installed packages...\n+\n+    S | Name                 | Type    | Version                  | Arch   | Repository\n+    --+----------------------+---------+--------------------------+--------+-------------------------------------------------\n+    i | ConsoleKit           | package | 0.2.10-64.67.1           | x86_64 | SLES11-Pool\n+    i | ConsoleKit           | package | 0.2.10-64.67.1           | x86_64 | SUSE11.4.4-1.109\n+    i | PolicyKit            | package | 0.9-14.43.1              | x86_64 | SLES11-Pool\n+    i | PolicyKit            | package | 0.9-14.43.1              | x86_64 | SUSE11.4.4-1.109\n+    i | PolicyKit-doc        | package | 0.9-14.43.1              | x86_64 | SLES11-Pool\n+    i | PolicyKit-doc        | package | 0.9-14.43.1              | x86_64 | SUSE11 11.4.4-1.109\n+    \"\"\"\n+    split_ch = \"|\"\n+\n+    for line in stream:\n+      line = ANSI_ESCAPE.sub('', line).translate(None, REMOVE_CHARS)\n+\n+      # reading second column instantly, first one is not interesting for us\n+      pkg_name, _, line = line.partition(split_ch)[2].partition(split_ch)\n+\n+      pkg_version, _, line = line.lstrip().partition(split_ch)[2].partition(split_ch)\n+      if not pkg_version.strip()[:1].isdigit():\n+        continue\n+\n+      repo_name, _, line = line.lstrip().partition(split_ch)[2].partition(split_ch)\n+\n+      pkg_name = pkg_name.strip()\n+      repo_name = repo_name.strip()\n+      pkg_version = pkg_version.strip()\n+\n+      if pkg_name and pkg_version and repo_name:\n+        yield pkg_name, pkg_version, repo_name\n+\n+  @staticmethod\n+  def packages_installed_reader(stream):\n+    yield ZypperParser.packages_reader(stream)\n+\n+  @staticmethod\n+  def lookup_packages(lines, skip_till=\"--+--\"):\n+    \"\"\"\n+    Deprecated and not used function, would be removed in later releases.\n+\n+    :type lines str\n+    :type skip_till str|None\n+    \"\"\"\n+    packages = []\n+    skip_index = None\n+\n+    lines = [line.strip() for line in lines]\n+    for index in range(len(lines)):\n+      if skip_till in lines[index]:\n+        skip_index = index + 1\n+        break\n+\n+    if skip_index:\n+      for line in lines[skip_index:]:\n+        items = line.strip(' \\t\\n\\r').split('|')\n+        packages.append([items[1].strip(), items[3].strip(), items[5].strip()])\n+\n+    return packages\n+\n+  @staticmethod\n+  def repo_list_reader(stream):\n+    \"\"\"\n+    :type stream collections.Iterable\n+\n+    Sample text:\n+\n+    Novel SuSe\n+\n+    # | Alias                               | Name                                             | Enabled | Refresh\n+    --+-------------------------------------+--------------------------------------------------+---------+--------\n+    1 | SUSE-Linux                          | SUSE-Linux                                       | Yes     | No\n+    2 | SLE11-Public-Cloud-Module           | SLE11-Public-Cloud-Module                        | No      | Yes\n+    3 | SLE11-SP4-Debuginfo-Pool            | SLE11-SP4-Debuginfo-Pool                         | No      | Yes\n+    4 | SLE11-SP4-Debuginfo-Updates         | SLE11-SP4-Debuginfo-Updates                      | No      | Yes\n+    5 | SLE11-Security-Module               | SLE11-Security-Module                            | No      | Yes\n+    6 | SLES11-Extras                       | SLES11-Extras                                    | No      | Yes\n+    7 | SLES11-SP4-Pool                     | SLES11-SP4-Pool                                  | Yes     | Yes\n+    8 | SLES11-SP4-Updates                  | SLES11-SP4-Updates                               | Yes     | Yes\n+\n+\n+    OpenSuse (have extra columns)\n+\n+    # | Alias          | Name           | Enabled | GPG Check | Refresh\n+    --+----------------+----------------+---------+-----------+--------\n+    1 | NON OSS        | NON OSS        | Yes     | (r ) Yes  | Yes\n+    2 | NON OSS Update | NON OSS Update | Yes     | (r ) Yes  | Yes\n+    3 | OSS            | OSS            | Yes     | (r ) Yes  | Yes\n+    4 | OSS Update     | OSS Update     | Yes     | (r ) Yes  | Yes\n+    \"\"\"\n+    split_ch = \"|\"\n+\n+    for line in stream:\n+      line = ANSI_ESCAPE.sub('', line).translate(None, REMOVE_CHARS)\n+      repo_seq, _, line = line.partition(split_ch)\n+      if not repo_seq.strip()[:1].isdigit():\n+        continue\n+\n+      repo_alias, _, line = line.lstrip().partition(split_ch)\n+      repo_name, _, line = line.lstrip().partition(split_ch)\n+      repo_enabled, _, line = line.lstrip().partition(split_ch)\n+      repo_refresh, _, line = line.lstrip().partition(split_ch)\n+\n+      if \"(\" in repo_refresh:  # skip \"GPG Check column\"\n+        repo_refresh, _, line = line.lstrip().partition(split_ch)\n+\n+      repo_alias = repo_alias.strip()\n+      repo_name = repo_name.strip()\n+      repo_enabled = repo_enabled.lower() == \"yes\"\n+      repo_refresh = repo_refresh.lower() == \"yes\"\n+\n+      if repo_alias and repo_name:\n+        yield repo_alias, repo_name, repo_enabled, repo_refresh",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/repo_manager/zypper_parser.py",
                "sha": "43feabc7974fd22ce1492da67d743da5f9527d90",
                "status": "added"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/resources/os_family.json",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_commons/resources/os_family.json?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 2,
                "filename": "ambari-common/src/main/python/ambari_commons/resources/os_family.json",
                "patch": "@@ -39,7 +39,8 @@\n           \"debian\"\n         ],\n         \"versions\": [\n-          7\n+          7,\n+          9\n         ]\n       },\n       \"ubuntu\": {\n@@ -79,7 +80,7 @@\n     \"aliases\": {\n       \"amazon2015\": \"amazon6\",\n       \"amazon2016\": \"amazon6\",\n-      \"amazon2017\": \"amazon6\",\n+      \"amazon2017\": \"redhat7\",\n       \"suse11sp3\": \"suse11\"\n     }\n }",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/resources/os_family.json",
                "sha": "70eac01d7c83fd148be2569d2b608342e3fc3fd7",
                "status": "modified"
            },
            {
                "additions": 546,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/shell.py",
                "changes": 700,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_commons/shell.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 154,
                "filename": "ambari-common/src/main/python/ambari_commons/shell.py",
                "patch": "@@ -20,68 +20,377 @@\n \n import logging\n import os\n+import string\n import signal\n-from ambari_commons import subprocess32\n+from ambari_commons import subprocess32 as subprocess\n import threading\n from contextlib import contextmanager\n+import copy\n+\n+import time\n+\n+import sys\n \n from ambari_commons import OSConst\n+from ambari_commons.buffered_queue import BufferedQueue\n+from ambari_commons.constants import AMBARI_SUDO_BINARY\n from ambari_commons.os_family_impl import OsFamilyImpl, OsFamilyFuncImpl\n-from resource_management.core import sudo\n \n-logger = logging.getLogger()\n+_logger = logging.getLogger()\n \n-threadLocal = threading.local()\n+PIPE_PTY = -3  # Popen stdout-only mode implemented via PopenEx\n \n # default timeout for async invoked processes\n-TIMEOUT_SECONDS = 300\n+__TIMEOUT_SECONDS = 300\n \n-tempFiles = []\n+__PACKAGE_MANAGER_LOCK_ACQUIRED_MSG = \"Cannot obtain lock for Package manager. Retrying after {0} seconds. Reason: {1}\"\n+__PACKAGE_MANAGER_REPO_ERROR_MSG = \"Cannot download the package due to repository unavailability. Retrying after {0} seconds. Reason: {1}\"\n \n \n-def noteTempFile(filename):\n-  tempFiles.append(filename)\n+\"\"\"\n+Module hits:\n+ - subprocess_executor - executing application and returning `SubprocessCallResult`\n+ - process_executor - context manager, targeted for application output reading instantly and\n+   allow data processing in the same time as output from application still coming\n \n \n-def getTempFiles():\n-  return tempFiles\n+Testing ints:\n+ - universal way to block any real application execution is to mock `launch_subprocess` call, which\n+   return Popen-like object\n+  \n+\"\"\"\n \n \n-class _dict_to_object:\n-  def __init__(self, entries):\n-    self.__dict__.update(entries)\n+class ReaderStrategy(object):\n+  BufferedQueue = 0\n+  BufferedChunks = 1\n \n-  def __getitem__(self, item):\n-    return self.__dict__[item]\n \n+class SubprocessCallResult(object):\n+  \"\"\"\n+  Holds response from subprocess call\n+  \"\"\"\n \n-# windows specific code\n-@OsFamilyFuncImpl(os_family=OSConst.WINSRV_FAMILY)\n-def kill_process_with_children(parent_pid):\n-  shellRunnerWindows().run([\"taskkill\", \"/F\", \"/T\", \"/PID\", \"{0}\".format(parent_pid)])\n+  def __init__(self, out=\"\", error=\"\", code=0):\n+    \"\"\"\n+    :type out str\n+    :type error str\n+    :type code int\n+    \"\"\"\n+    self.out = out\n+    self.error = error\n+    self.code = code\n+\n+  @property\n+  def all_out(self):\n+    r = \"\"\n+    if self.out:\n+      r += self.out\n+    if self.error:\n+      r += self.error\n+    return r\n+\n+\n+class RepoCallContext(object):\n+  ignore_errors = True\n+  __retry_count = 2\n+  retry_sleep = 30\n+\n+  retry_on_repo_unavailability = False\n+  retry_on_locked = True\n+\n+  log_output = True\n+\n+  use_repos = None\n+  skip_repos = None\n+  is_upgrade = False\n+\n+  def __init__(self, ignore_errors=True, retry_count=2, retry_sleep=30, retry_on_repo_unavailability=False,\n+               retry_on_locked=True, log_output=True, use_repos=None, skip_repos=None,\n+               is_upgrade=False):\n+    \"\"\"\n+    :type ignore_errors bool\n+    :type retry_count int\n+    :type retry_sleep int\n+    :type retry_on_repo_unavailability bool\n+    :type retry_on_locked bool\n+    :type log_output bool\n+    :type use_repos dict\n+    :type skip_repos set|list\n+    :type is_upgrade bool\n+    \"\"\"\n+    self.ignore_errors = ignore_errors\n+    self.retry_count = retry_count\n+    self.retry_sleep = retry_sleep\n+    self.retry_on_repo_unavailability = retry_on_repo_unavailability\n+    self.retry_on_locked = retry_on_locked\n+    self.log_output = log_output\n+    self.use_repos = use_repos\n+    self.skip_repos = skip_repos\n+    self.is_upgrade = is_upgrade\n+\n+  @property\n+  def retry_count(self):\n+    return self.__retry_count\n+\n+  @retry_count.setter\n+  def retry_count(self, value):\n+    \"\"\"\n+    :type value int\n+    \"\"\"\n+    # at least do one retry, to run after repository is cleaned\n+    self.__retry_count = 2 if value < 2 else value\n+\n+\n+def __set_winsize(fd, row, col):\n+  \"\"\"\n+  Set geometry of terminal, fd should be a tty or pty or ioctl will fail.\n+  \"\"\"\n+  import termios\n+  import struct\n+  import fcntl\n+  winsize = struct.pack(\"HHHH\", row, col, 0, 0)\n+  fcntl.ioctl(fd, termios.TIOCSWINSZ, winsize)\n \n \n-class shellRunner(object):\n-  def run(self, script, user=None):\n-    pass\n+def __terminal_width(fd=1):\n+  \"\"\"\n+  Checking terminal width in the way yum output.py doing it, existing for debug purposes\n \n-  def runPowershell(self, file=None, script_block=None, args=[]):\n-    raise NotImplementedError()\n+  Source:\n+    https://github.com/rpm-software-management/yum/blob/master/output.py#L65\n+  \"\"\"\n+  import termios\n+  import struct\n+  import fcntl\n+  try:\n+    buf = 'abcdefgh'\n+    buf = fcntl.ioctl(fd, termios.TIOCGWINSZ, buf)\n+    ret = struct.unpack('hhhh', buf)[1]\n+    if ret == 0:\n+      return 80\n+    # Add minimum too?\n+    return ret\n+  except:   # IOError\n+    return 80\n+\n+\n+class PopenEx(subprocess.Popen):\n+  \"\"\"\n+  Same nice Popen with stdout handles hack to allow pty instead of pipe. This will allow to control terminal geometry\n+  to eliminate some applications bugs with output formatting according to terminal width.\n+\n+  TODO: move the code directly to subprocess32.py\n+  \"\"\"\n+\n+  def _get_handles(self, stdin, stdout, stderr):\n+    \"\"\"\n+    Replace Pipe with pty fd for stdout\n+    \"\"\"\n+    import pty\n+    import tty\n+\n+    is_extended_pipe = stdout == PIPE_PTY\n+    fd_read, fd_write = None, None\n+\n+    if is_extended_pipe:\n+      fd_read, fd_write = pty.openpty()\n+      tty.setraw(fd_read)   # do not interpret control characters\n+      tty.setraw(fd_write)  # do not interpret control characters\n+      stdout = fd_write\n+\n+    handles = super(PopenEx, self)._get_handles(stdin, stdout, stderr)\n+    if len(handles) == 2:  # python 2.7+\n+      handles, to_close = handles\n+    else:\n+      to_close = None\n+\n+    p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite = handles\n+\n+    if is_extended_pipe:\n+      c2pread = fd_read\n+      if to_close:\n+        to_close.add(fd_read)\n+\n+    handles = p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite\n+\n+    if to_close:\n+      return handles, to_close\n \n+    return handles\n \n-def launch_subprocess(command):\n+\n+def quote_bash_args(command):\n+  \"\"\"\n+  Copied from resource_manager shell module to remove dependency between modules\n+  \"\"\"\n+  if not command:\n+    return \"''\"\n+\n+  if not isinstance(command, basestring):\n+    raise ValueError(\"Command should be a list of strings, found '{0}' in command list elements\".format(str(command)))\n+\n+  valid = set(string.ascii_letters + string.digits + '@%_-+=:,./')\n+  for char in command:\n+    if char not in valid:\n+      return \"'\" + command.replace(\"'\", \"'\\\"'\\\"'\") + \"'\"\n+  return command\n+\n+\n+def string_cmd_from_args_list(command, auto_escape=True):\n+  \"\"\"\n+  Copied from resource_manager shell module to remove dependency between modules\n+  \"\"\"\n+  if auto_escape:\n+    return ' '.join(quote_bash_args(x) for x in command)\n+  else:\n+    return ' '.join(command)\n+\n+\n+def is_under_root():\n+  \"\"\"\n+  Check if current script is executed with root permissions\n+\n+  :rtype bool\n+  :return checks state\n+  \"\"\"\n+  return os.geteuid() == 0\n+\n+\n+def launch_subprocess(command, term_geometry=(42, 255), env=None):\n   \"\"\"\n   Process launch helper\n \n   :param command Command to execute\n   :type command list[str]|str\n-  :return Popen object\n+  :type term_geometry set|list\n+  :type env dict\n+  :rtype PopenEx\n   \"\"\"\n+  def _geometry_helper():\n+    \"\"\"\n+    Setting proper terminal geometry\n+    \"\"\"\n+    if term_geometry:\n+      __set_winsize(sys.stdout.fileno(), *term_geometry)\n+    # check term geometry\n+    # print \"terminal_width: \", __terminal_width()\n+\n+  # no way to execute shell command with bash pipes under sudo, it is fully dev responsibility\n+  if not is_under_root() and not isinstance(command, str):\n+    command = \"{0} -H -E {1}\".format(AMBARI_SUDO_BINARY, string_cmd_from_args_list(command))  # core.shell.as_sudo\n+  elif not is_under_root() and isinstance(command, str):\n+    _logger.debug(\"Warning, command  \\\"{0}\\\" doesn't support sudo appending\".format(command))\n+\n   is_shell = not isinstance(command, (list, tuple))\n-  return subprocess32.Popen(command, stdout=subprocess32.PIPE, stderr=subprocess32.PIPE, shell=is_shell, close_fds=True)\n+  environ = copy.deepcopy(os.environ)\n+\n+  if env:\n+    environ.update(env)\n+\n+  return PopenEx(command, stdout=PIPE_PTY, stderr=subprocess.PIPE,\n+                 shell=is_shell, preexec_fn=_geometry_helper, close_fds=True, env=environ)\n+\n+\n+def chunks_reader(cmd, kill_timer):\n+  \"\"\"\n+  Low level reader based on os.read.\n \n+  Notice: Could stuck, if were killed called process pid without killing children processes, which still holding stdout\n \n-def watchdog_func(event, cmd, exec_timeout):\n+  :type cmd PopenEx\n+  :type kill_timer threading.Timer\n+  \"\"\"\n+  import select\n+  str_buffer = \"\"\n+  read_queue = [cmd.stdout]\n+  chunk_size = 1024\n+\n+  while True:\n+    is_cmd_up = cmd.poll() is None\n+\n+    if not is_cmd_up:\n+      kill_timer.cancel()\n+\n+    ready, _, _ = select.select(read_queue, [], [], 1)\n+\n+    if not is_cmd_up and not ready:\n+      for line in str_buffer.strip(os.linesep).split(os.linesep):\n+        yield line\n+      break\n+\n+    # process one chunk of the data\n+    try:\n+      data_chunk = os.read(read_queue[0].fileno(), chunk_size)\n+    except OSError:\n+      break\n+\n+    if not data_chunk:\n+      break\n+    str_buffer += data_chunk\n+\n+    if os.linesep in str_buffer:\n+      copy_offset = str_buffer.rindex(os.linesep)\n+      buff_lines = str_buffer[:copy_offset]\n+      str_buffer = str_buffer[copy_offset:]\n+      for line in buff_lines.strip(os.linesep).split(os.linesep):\n+        yield line\n+\n+  kill_timer.cancel()\n+\n+\n+def queue_reader(cmd, q, timeout, timer):\n+  \"\"\"\n+  Hungry process output reader, which reads stdout as it come and allow consumer to get it by request.\n+\n+  :arg cmd Popen object\n+  :arg q read/write queue\n+  :arg timeout read timeout\n+  :arg timer kill timer, used to cancel it when process already exited\n+\n+  :type cmd subprocess.Popen\n+  :type q BufferedQueue\n+  :type timeout int\n+  :type timer threading.Timer\n+  \"\"\"\n+\n+  def _reader():\n+    try:\n+      while True:\n+        data_chunk = cmd.stdout.readline()\n+        \"\"\"\n+        data_chunk could be:\n+        \n+        null   - no input received and read were non-blocking, means that parent process died or stdout of\n+                 parent process was closed\n+        \\n     - empty line\n+        [data] - any string \n+        \"\"\"\n+        if not data_chunk:\n+          break\n+        q.put(data_chunk)\n+    except IOError:\n+      pass\n+    finally:\n+      q.notify_end()\n+      try:\n+        cmd.stdout.close()  # close stdout if any exception appear before, triggering by this parent process shutdown\n+      except (IOError, OSError):\n+        pass\n+\n+  reader_th = threading.Thread(target=_reader)\n+  reader_th.start()\n+\n+  while not q.empty:\n+    str_line = q.get(timeout)\n+    if str_line:\n+      yield str_line.strip(os.linesep)\n+\n+    if cmd.poll() is not None and timer.is_alive():\n+      timer.cancel()\n+\n+\n+def __watchdog_func(event, cmd, exec_timeout):\n   \"\"\"\n   Watchdog function for subprocess executors\n \n@@ -106,57 +415,57 @@ def watchdog_func(event, cmd, exec_timeout):\n   \"\"\"\n   event.wait(exec_timeout)\n   if cmd.returncode is None:\n-    logger.error(\"Task timed out and will be killed\")\n+    _logger.error(\"Task timed out and will be killed\")\n     kill_process_with_children(cmd.pid)\n \n \n-def subprocess_with_timeout(command, execution_timeout=None):\n+def subprocess_executor(command, timeout=__TIMEOUT_SECONDS, strategy=ReaderStrategy.BufferedQueue, env=None):\n   \"\"\"\n   Run command with limited time for execution, after timeout command would be killed\n \n-  :param command Command to execute\n-  :param execution_timeout execution time limit in seconds. Defaulting to TIMEOUT_SECONDS global constant\n+  :param command Command to execute. non-shell commands able to use sudo automatically, while shell ones not.\n+                 Prefer tuple or list command format and use shell command style (str) at own risk\n+  :param timeout execution time limit in seconds. If None will default to TIMEOUT_SECONDS, -1 disable feature\n+  :param strategy the way how to process output. Available methods listed in ReaderStrategy\n+  :param env Environment variables for new process\n \n   :type command list[str]|str\n-  :type execution_timeout int\n-  :rtype dict\n-  \"\"\"\n-  event = threading.Event()\n-\n-  if execution_timeout is None:\n-    execution_timeout = TIMEOUT_SECONDS\n-\n-  os_stat = launch_subprocess(command)\n-  logger.debug(\"Launching watchdog thread\")\n+  :type timeout int\n+  :type strategy int\n+  :type env dict\n+  :rtype SubprocessCallResult\n \n-  event.clear()\n-\n-  thread = threading.Thread(target=watchdog_func, args=(event, os_stat, execution_timeout,))\n-  thread.start()\n+  \"\"\"\n+  r = SubprocessCallResult()\n \n-  out, err = os_stat.communicate()\n+  def _error_handler(_command, _error_log, _exit_code):\n+    r.error = os.linesep.join(_error_log)\n+    r.code = _exit_code\n \n-  result = {\n-    \"out\": out,\n-    \"err\": err,\n-    \"retCode\": os_stat.returncode\n-  }\n+  with process_executor(command, timeout, _error_handler, strategy, env=env) as output:\n+    lines = [line for line in output]\n \n-  event.set()\n-  thread.join()\n-  return result\n+  r.out = os.linesep.join(lines)\n+  return r\n \n \n @contextmanager\n-def process_executor(command, timeout=None, error_callback=None):\n+def process_executor(command, timeout=__TIMEOUT_SECONDS, error_callback=None, strategy=ReaderStrategy.BufferedQueue, env=None):\n   \"\"\"\n   Context manager for command execution\n \n+  :param command Command to execute\n+  :param timeout execution time limit in seconds. If None will default to TIMEOUT_SECONDS, -1 disable feature\n+  :param strategy the way how to process output. Available methods listed in ReaderStrategy\n+  :param env Environment variable for new spawned process\n+\n   :type command list|str\n   :type timeout None|int\n   :type error_callback func\n+  :type strategy int\n+  :type env dict\n \n-  :return stdout stream\n+  :rtype stdout collections.Iterable\n \n   Usage example:\n \n@@ -175,68 +484,51 @@ def error_handler(command, error_log, exit_code):\n          print line\n \n   \"\"\"\n-  if not timeout:\n-    timeout = TIMEOUT_SECONDS\n-\n-  event = threading.Event()\n-  cmd = launch_subprocess(command)\n \n-  thread = threading.Thread(target=watchdog_func, args=(event, cmd, timeout,))\n-  thread.start()\n+  buff_queue = None\n+  kill_timer = None\n \n-  yield cmd.stdout\n-\n-  exit_code = cmd.poll()\n-  event.set()\n-  thread.join()\n-\n-  if exit_code is None:\n-    kill_process_with_children(cmd.pid)\n-\n-  if error_callback and exit_code and exit_code > 0:\n-    error_callback(command, cmd.stderr.readlines(), exit_code)\n+  try:\n+    cmd = launch_subprocess(command, env=env)\n+    if not isinstance(cmd, PopenEx):  # ToDo: make better solution to avoid issues in UT\n+      yield []\n+      return\n+    kill_timer = threading.Timer(timeout, lambda: cmd.kill())\n+    if timeout > -1:\n+      kill_timer.start()\n+\n+    if strategy == ReaderStrategy.BufferedQueue:\n+      buff_queue = BufferedQueue()\n+      yield queue_reader(cmd, buff_queue, timeout, kill_timer)\n+    elif strategy == ReaderStrategy.BufferedChunks:\n+      yield chunks_reader(cmd, kill_timer)\n+    else:\n+      raise TypeError(\"Unknown reader strategy selected: {0}\".format(strategy))\n \n+    _exit_code = cmd.poll()\n+    if _exit_code is None:\n+      cmd.terminate()\n \n-@OsFamilyImpl(os_family=OSConst.WINSRV_FAMILY)\n-class shellRunnerWindows(shellRunner):\n-  # Run any command\n-  def run(self, script, user=None):\n-    logger.warn(\"user argument ignored on windows\")\n-    code = 0\n-    if isinstance(script, list):\n-      cmd = \" \".join(script)\n-    else:\n-      cmd = script\n-    p = subprocess32.Popen(cmd, stdout=subprocess32.PIPE,\n-                         stderr=subprocess32.PIPE, shell=False)\n-    out, err = p.communicate()\n-    code = p.wait()\n-    logger.debug(\"Exitcode for %s is %d\" % (cmd, code))\n-    return {'exitCode': code, 'output': out, 'error': err}\n+    if error_callback and cmd.returncode and cmd.returncode > 0:\n+      error_callback(command, cmd.stderr.readlines(), cmd.returncode)\n+  except Exception as e:\n+    if kill_timer:\n+      kill_timer.cancel()\n+    _logger.error(\"Exception during command '{0}' execution: {1}\".format(command, str(e)))\n+    if error_callback:\n+      error_callback(command, [str(e)], -1)\n \n-  def runPowershell(self, file=None, script_block=None, args=[]):\n-    logger.warn(\"user argument ignored on windows\")\n-    code = 0\n-    cmd = None\n-    if file:\n-      cmd = ['powershell', '-WindowStyle', 'Hidden', '-File', file] + args\n-    elif script_block:\n-      cmd = ['powershell', '-WindowStyle', 'Hidden', '-Command', script_block] + args\n-    p = subprocess32.Popen(cmd, stdout=subprocess32.PIPE,\n-                         stderr=subprocess32.PIPE, shell=False)\n-    out, err = p.communicate()\n-    code = p.wait()\n-    logger.debug(\"Exitcode for %s is %d\" % (cmd, code))\n-    return _dict_to_object({'exitCode': code, 'output': out, 'error': err})\n+    yield []\n+  finally:\n+    if buff_queue:\n+      buff_queue.notify_end()\n \n \n def get_all_children(base_pid):\n   \"\"\"\n   Return all child PIDs of base_pid process\n-\n   :param base_pid starting PID to scan for children\n   :return tuple of the following: pid, binary name, command line incl. binary\n-\n   :type base_pid int\n   :rtype list[(int, str, str)]\n   \"\"\"\n@@ -278,30 +570,12 @@ def read_cmdline(pid):\n   return pids\n \n \n-def is_pid_exists(pid):\n-  \"\"\"\n-  Check if process with PID still exist (not counting it real state)\n-\n-  :type pid int\n-  :rtype bool\n-  \"\"\"\n-  pid_path = \"/proc/{0}\"\n-  try:\n-    return os.path.exists(pid_path.format(pid))\n-  except (OSError, IOError):\n-    logger.debug(\"Failed to check PID existence\")\n-    return False\n-\n-\n def get_existing_pids(pids):\n   \"\"\"\n   Check if process with pid still exists (not counting it real state).\n-\n   Optimized to check PID list at once.\n-\n   :param pids list of PIDs to filter\n   :return list of still existing PID\n-\n   :type pids list[int]\n   :rtype list[int]\n   \"\"\"\n@@ -311,7 +585,7 @@ def get_existing_pids(pids):\n   try:\n     all_existing_pid_list = [int(item) for item in os.listdir(\"/proc\") if item.isdigit()]\n   except (OSError, IOError):\n-    logger.debug(\"Failed to check PIDs existence\")\n+    _logger.debug(\"Failed to check PIDs existence\")\n     return existing_pid_list\n \n   for pid_item in pids:\n@@ -324,11 +598,9 @@ def get_existing_pids(pids):\n def wait_for_process_list_kill(pids, timeout=5, check_step_time=0.1):\n   \"\"\"\n   Process tree waiter\n-\n   :type pids list[int]\n   :type timeout int|float\n   :type check_step_time int|float\n-\n   :param pids list of PIDs to watch\n   :param timeout how long wait till giving up, seconds. Set 0 for nowait or None for infinite time\n   :param check_step_time how often scan for existing PIDs, seconds\n@@ -362,6 +634,8 @@ def kill_process_with_children(base_pid):\n \n   :type base_pid int\n   \"\"\"\n+  from resource_management.core import sudo\n+\n   exception_list = [\"apt-get\", \"apt\", \"yum\", \"zypper\", \"zypp\"]\n   signals_to_post = {\n     \"SIGTERM\": signal.SIGTERM,\n@@ -384,59 +658,177 @@ def kill_process_with_children(base_pid):\n       wait_for_process_list_kill(pids_to_kill)\n       still_existing_pids = get_existing_pids(pids_to_kill)\n       if still_existing_pids:\n-        logger.warn(\"These PIDs {0} did not respond to {1} signal. Detailed commands list:\\n {2}\".format(\n+        _logger.warning(\"These PIDs {0} did not respond to {1} signal. Detailed commands list:\\n {2}\".format(\n           \", \".join([str(i) for i in still_existing_pids]),\n           sig_name,\n           \"\\n\".join([i[2] for i in full_child_pids if i[0] in still_existing_pids])\n         ))\n \n   if get_existing_pids(all_child_pids) and error_log:  # we're unable to kill all requested PIDs\n-    logger.warn(\"Process termination error log:\\n\")\n+    _logger.warn(\"Process termination error log:\\n\")\n     for error_item in error_log:\n-      logger.warn(\"PID: {0}, Process: {1}, Exception message: {2}\".format(*error_item))\n+      _logger.warn(\"PID: {0}, Process: {1}, Exception message: {2}\".format(*error_item))\n \n \n-def _changeUid():\n-  try:\n-    os.setuid(threadLocal.uid)\n-  except Exception:\n-    logger.warn(\"can not switch user for running command.\")\n+def __handle_retries(cmd, repo_properties, context, call_result, is_first_time, is_last_time):\n+  \"\"\"\n+  :type cmd list\n+  :type repo_properties ambari_commons.repo_manager.generic_manager.GenericManagerProperties\n+  :type context RepoCallContext\n+  :type call_result SubprocessCallResult\n+  :type is_first_time bool\n+  :type is_last_time bool\n+  \"\"\"\n+  out = call_result.all_out\n+  # handle first failure in a special way (update repo metadata after it, so next try has a better chance to succeed)\n+  if is_first_time and call_result.code and repo_properties.locked_output and repo_properties.locked_output not in out:\n+    __update_repo_metadata_after_bad_try(cmd, context, repo_properties, call_result.code, call_result.out)\n+    return False\n+\n+  err_log_msg = None\n+  if context.retry_on_locked and repo_properties.locked_output and repo_properties.locked_output in out:\n+    err_log_msg = __PACKAGE_MANAGER_LOCK_ACQUIRED_MSG.format(context.retry_sleep, call_result.out)\n+  elif context.retry_on_repo_unavailability and repo_properties.repo_error:\n+    for err in repo_properties.repo_error:\n+      if err in call_result.all_out:\n+        err_log_msg = __PACKAGE_MANAGER_REPO_ERROR_MSG.format(context.retry_sleep, call_result.out)\n+\n+  if err_log_msg and not is_last_time:\n+    _logger.info(err_log_msg)\n+\n+  return is_last_time or not call_result.code or not err_log_msg\n+\n+\n+def __update_repo_metadata_after_bad_try(cmd, context, repo_properties, code, out):\n+  \"\"\"\n+  :type cmd list\n+  :type context RepoCallContext\n+  :type repo_properties ambari_commons.repo_manager.generic_manager.GenericManagerProperties\n+  :type code str|int\n+  :type out str\n+  \"\"\"\n+  repo_update_cmd = repo_properties.repo_update_cmd\n+  _logger.info(\"Execution of '%s' failed and returned %d. %s\" % (string_cmd_from_args_list(cmd), code, out))\n+\n+  call_result = subprocess_executor(repo_update_cmd, timeout=-1)\n+\n+  if call_result.code:\n+    _logger.info(\"Execution of '%s' returned %d. %s\" % (repo_update_cmd, call_result.code, call_result.out))\n+\n+  _logger.info(\"Retrying to execute command after %d seconds\" % context.retry_sleep)\n+\n+\n+def repository_manager_executor(cmd, repo_properties, context=RepoCallContext(), env=None):\n+  \"\"\"\n+  Repository Manager execution call for install, remove, update commands with possibility to retry calls\n+\n+  :type cmd list\n+  :type repo_properties ambari_commons.repo_manager.generic_manager.GenericManagerProperties | YumManagerProperties | ZypperManagerProperties | AptManagerProperties\n+  :type context RepoCallContext\n+  :type env dict\n+\n+  :rtype SubprocessCallResult\n+  \"\"\"\n+  call_result = None\n+\n+  for i in range(context.retry_count):\n+    is_first_time = (i == 0)\n+    is_last_time = (i == context.retry_count - 1)\n+\n+    call_result = subprocess_executor(cmd, timeout=-1, env=env)\n+\n+    should_stop_retries = __handle_retries(cmd, repo_properties, context, call_result, is_first_time, is_last_time)\n+    if (is_last_time or should_stop_retries) and call_result.code != 0:\n+      message = \"Failed to execute command '{0}', exited with code '{1}' with message: {2}\".format(\n+        cmd, call_result.code, call_result.error)\n+\n+      if context.ignore_errors:\n+        _logger.warning(message)\n+      else:\n+        raise RuntimeError(message)\n+    if should_stop_retries:\n+      break\n+\n+    time.sleep(context.retry_sleep)\n+\n+  return call_result\n+\n+\n+class shellRunner(object):\n+  def run(self, script, user=None):\n+    pass\n+\n+  def runPowershell(self, f=None, script_block=None, args=set()):\n+    raise NotImplementedError()\n+\n+\n+@OsFamilyImpl(os_family=OSConst.WINSRV_FAMILY)\n+class shellRunnerWindows(shellRunner):\n+  # Run any command\n+  def run(self, script, user=None):\n+    global _logger\n+    _logger.warn(\"user argument ignored on windows\")\n+    code = 0\n+    if isinstance(script, list):\n+      cmd = \" \".join(script)\n+    else:\n+      cmd = script\n+    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False)\n+    out, err = p.communicate()\n+    code = p.wait()\n+    _logger.debug(\"Exitcode for %s is %d\" % (cmd, code))\n+    return SubprocessCallResult(out=out, error=err, code=code)\n+\n+  def runPowershell(self, f=None, script_block=None, args=set()):\n+    global _logger\n+    _logger.warn(\"user argument ignored on windows\")\n+\n+    cmd = None\n+    if f:\n+      cmd = ['powershell', '-WindowStyle', 'Hidden', '-File', f] + list(args)\n+    elif script_block:\n+      cmd = ['powershell', '-WindowStyle', 'Hidden', '-Command', script_block] + list(args)\n+\n+    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=False)\n+    out, err = p.communicate()\n+    code = p.wait()\n+    _logger.debug(\"Exitcode for %s is %d\" % (cmd, code))\n+    return {'exitCode': code, 'output': out, 'error': err}\n \n \n @OsFamilyImpl(os_family=OsFamilyImpl.DEFAULT)\n class shellRunnerLinux(shellRunner):\n+  _threadLocal = None\n+\n+  def _change_uid(self):\n+    try:\n+      if self._threadLocal is not None:\n+        os.setuid(self._threadLocal.uid)\n+    except Exception:\n+      _logger.warn(\"can not switch user for running command.\")\n+\n   # Run any command\n   def run(self, script, user=None):\n     import pwd\n \n     try:\n-      if user != None:\n+      if user is not None:\n         user = pwd.getpwnam(user)[2]\n       else:\n         user = os.getuid()\n-      threadLocal.uid = user\n+      self._threadLocal.uid = user\n     except Exception:\n-      logger.warn(\"can not switch user for RUN_COMMAND.\")\n+      _logger.warn(\"can not switch user for RUN_COMMAND.\")\n \n     cmd = script\n-    \n+\n     if isinstance(script, list):\n       cmd = \" \".join(script)\n \n-    cmd_list = [\"/bin/bash\",\"--login\",\"--noprofile\",\"-c\", cmd]\n-    p = subprocess32.Popen(cmd_list, preexec_fn=_changeUid, stdout=subprocess32.PIPE,\n-                         stderr=subprocess32.PIPE, shell=False, close_fds=True)\n+    cmd_list = [\"/bin/bash\", \"--login\", \"--noprofile\", \"-c\", cmd]\n+    p = subprocess.Popen(cmd_list, preexec_fn=self._change_uid, stdout=subprocess.PIPE,\n+                           stderr=subprocess.PIPE, shell=False, close_fds=True)\n     out, err = p.communicate()\n     code = p.wait()\n-    logger.debug(\"Exitcode for %s is %d\" % (cmd, code))\n+    _logger.debug(\"Exitcode for %s is %d\" % (cmd, code))\n     return {'exitCode': code, 'output': out, 'error': err}\n-\n-\n-@OsFamilyFuncImpl(os_family=OSConst.WINSRV_FAMILY)\n-def changeUid():\n-  #No Windows implementation\n-  pass\n-\n-@OsFamilyFuncImpl(os_family=OsFamilyImpl.DEFAULT)\n-def changeUid():\n-  _changeUid()",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/shell.py",
                "sha": "d387837a0cf38dc5b289dfd535999e62b33af5aa",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/unicode_tolerant_fs.py",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_commons/unicode_tolerant_fs.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 8,
                "filename": "ambari-common/src/main/python/ambari_commons/unicode_tolerant_fs.py",
                "patch": "@@ -6,19 +6,20 @@\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n-\n     http://www.apache.org/licenses/LICENSE-2.0\n-\n Unless required by applicable law or agreed to in writing, software\n distributed under the License is distributed on an \"AS IS\" BASIS,\n WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n See the License for the specific language governing permissions and\n limitations under the License.\n-\n Ambari Agent\n-\n \"\"\"\n \n+def get_encoded_string(data):\n+  try:\n+    return data.encode(\"utf8\")\n+  except UnicodeDecodeError:\n+    return data\n \n def unicode_walk(top, topdown=True, onerror=None, followlinks=False):\n   \"\"\"\n@@ -31,7 +32,7 @@ def unicode_walk(top, topdown=True, onerror=None, followlinks=False):\n \n   islink, join, isdir = os.path.islink, os.path.join, os.path.isdir\n \n-  top = top.encode(\"utf8\")\n+  top = get_encoded_string(top)\n \n   try:\n     # Note that listdir and error are globals in this module due\n@@ -44,7 +45,7 @@ def unicode_walk(top, topdown=True, onerror=None, followlinks=False):\n \n   dirs, nondirs = [], []\n   for name in names:\n-    name = name.encode(\"utf8\")\n+    name = get_encoded_string(name)\n     if isdir(join(top, name)):\n       dirs.append(name)\n     else:\n@@ -53,11 +54,10 @@ def unicode_walk(top, topdown=True, onerror=None, followlinks=False):\n   if topdown:\n     yield top, dirs, nondirs\n   for name in dirs:\n-    name = name.encode(\"utf8\")\n+    name = get_encoded_string(name)\n     new_path = join(top, name)\n     if followlinks or not islink(new_path):\n       for x in unicode_walk(new_path, topdown, onerror, followlinks):\n         yield x\n   if not topdown:\n     yield top, dirs, nondirs\n-",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_commons/unicode_tolerant_fs.py",
                "sha": "fe0c0bbc22462ce3c4b45f3aa5a7f4ebdeb75597",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_stomp/adapter/websocket.py",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_stomp/adapter/websocket.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 4,
                "filename": "ambari-common/src/main/python/ambari_stomp/adapter/websocket.py",
                "patch": "@@ -64,11 +64,11 @@ def closed(self, code, reason=None):\n     self.messages.put(StopIteration)\n \n class WsTransport(Transport):\n-  def __init__(self, url):\n+  def __init__(self, url, ssl_options=None):\n     Transport.__init__(self, (0, 0), False, False, 0.0, 0.0, 0.0, 0.0, 0, False, None, None, None, None, False,\n     DEFAULT_SSL_VERSION, None, None, None)\n     self.current_host_and_port = (0, 0) # mocking\n-    self.ws = QueuedWebSocketClient(url, protocols=['http-only', 'chat'])\n+    self.ws = QueuedWebSocketClient(url, protocols=['http-only', 'chat'], ssl_options=ssl_options)\n     self.ws.daemon = False\n \n   def wait_for_connection(self, timeout=DEFAULT_CONNECTION_TIMEOUT):\n@@ -124,8 +124,8 @@ def stop(self):\n       logger.exception(\"Exception during Transport.stop(self)\")\n \n class WsConnection(BaseConnection, Protocol12):\n-  def __init__(self, url):\n-    self.transport = WsTransport(url)\n+  def __init__(self, url, ssl_options=None):\n+    self.transport = WsTransport(url, ssl_options=ssl_options)\n     self.transport.set_listener('ws-listener', self)\n     self.transactions = {}\n     Protocol12.__init__(self, self.transport, (0, 0))",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_stomp/adapter/websocket.py",
                "sha": "ad61866b0fde87c6b7f13fe9010cfc67cbed66fb",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_stomp/transport.py",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_stomp/transport.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 4,
                "filename": "ambari-common/src/main/python/ambari_stomp/transport.py",
                "patch": "@@ -17,7 +17,7 @@\n     import ssl\n     from ssl import SSLError\n \n-    DEFAULT_SSL_VERSION = ssl.PROTOCOL_TLSv1\n+    DEFAULT_SSL_VERSION = ssl.PROTOCOL_TLSv1_2\n except (ImportError, AttributeError):  # python version < 2.6 without the backported ssl module\n     ssl = None\n \n@@ -188,7 +188,7 @@ def process_frame(self, f, frame_str):\n             if log.isEnabledFor(logging.DEBUG):\n                 log.debug(\"Received frame: %r, headers=%r, body=%r\", f.cmd, f.headers, f.body)\n             else:\n-                log.info(\"Received frame: %r, headers=%r, len(body)=%r\", f.cmd, f.headers, utils.length(f.body))\n+                log.debug(\"Received frame: %r, headers=%r, len(body)=%r\", f.cmd, f.headers, utils.length(f.body))\n             self.notify(frame_type, f.headers, f.body)\n         else:\n             log.warning(\"Unknown response frame type: '%s' (frame length was %d)\", frame_type, utils.length(frame_str))\n@@ -268,7 +268,7 @@ def transmit(self, frame):\n         if log.isEnabledFor(logging.DEBUG):\n             log.debug(\"Sending frame: %s\", lines)\n         else:\n-            log.info(\"Sending frame: %r, headers=%r\", frame.cmd or \"heartbeat\", frame.headers)\n+            log.debug(\"Sending frame: %r, headers=%r\", frame.cmd or \"heartbeat\", frame.headers)\n \n         self.send(encode(packed_frame))\n \n@@ -814,7 +814,7 @@ def set_ssl(self,\n                                where OK is a boolean, and cert is a certificate structure\n                                as returned by ssl.SSLSocket.getpeercert()\n         :param ssl_version: SSL protocol to use for the connection. This should be one of the PROTOCOL_x\n-                            constants provided by the ssl module. The default is ssl.PROTOCOL_TLSv1\n+                            constants provided by the ssl module. The default is ssl.PROTOCOL_TLSv1_2\n         \"\"\"\n         if not ssl:\n             raise Exception(\"SSL connection requested, but SSL library not found\")",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_stomp/transport.py",
                "sha": "0896ef26b0f060e3b967bd657b3904cca9725451",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/__init__.py",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_ws4py/__init__.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 2,
                "filename": "ambari-common/src/main/python/ambari_ws4py/__init__.py",
                "patch": "@@ -10,7 +10,7 @@\n # copyright notice, this list of conditions and the following disclaimer\n # in the documentation and/or other materials provided with the\n # distribution.\n-#     * Neither the name of ambari_ws4py nor the names of its\n+#     * Neither the name of ws4py nor the names of its\n # contributors may be used to endorse or promote products derived from\n # this software without specific prior written permission.\n #\n@@ -30,7 +30,7 @@\n import logging.handlers as handlers\n \n __author__ = \"Sylvain Hellegouarch\"\n-__version__ = \"0.4.2\"\n+__version__ = \"0.5.1\"\n __all__ = ['WS_KEY', 'WS_VERSION', 'configure_logger', 'format_addresses']\n \n WS_KEY = b\"258EAFA5-E914-47DA-95CA-C5AB0DC85B11\"",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/__init__.py",
                "sha": "bcd60f998fe3d15471ac27bc887367fbf77caeee",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/client/__init__.py",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_ws4py/client/__init__.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 10,
                "filename": "ambari-common/src/main/python/ambari_ws4py/client/__init__.py",
                "patch": "@@ -14,7 +14,7 @@\n \n class WebSocketBaseClient(WebSocket):\n     def __init__(self, url, protocols=None, extensions=None,\n-                 heartbeat_freq=None, ssl_options=None, headers=None):\n+                 heartbeat_freq=None, ssl_options=None, headers=None, exclude_headers=None):\n         \"\"\"\n         A websocket client that implements :rfc:`6455` and provides a simple\n         interface to communicate with a websocket server.\n@@ -36,23 +36,23 @@ def __init__(self, url, protocols=None, extensions=None,\n \n         .. code-block:: python\n \n-           >>> from websocket.client import WebSocketBaseClient\n+           >>> from ambari_ws4py.client import WebSocketBaseClient\n            >>> ws = WebSocketBaseClient('ws://localhost/ws')\n \n \n         Here is an example for a TCP client over SSL:\n \n         .. code-block:: python\n \n-           >>> from websocket.client import WebSocketBaseClient\n+           >>> from ambari_ws4py.client import WebSocketBaseClient\n            >>> ws = WebSocketBaseClient('wss://localhost/ws')\n \n \n         Finally an example of a Unix-domain connection:\n \n         .. code-block:: python\n \n-           >>> from websocket.client import WebSocketBaseClient\n+           >>> from ambari_ws4py.client import WebSocketBaseClient\n            >>> ws = WebSocketBaseClient('ws+unix:///tmp/my.sock')\n \n         Note that in this case, the initial Upgrade request\n@@ -61,7 +61,7 @@ def __init__(self, url, protocols=None, extensions=None,\n \n         .. code-block:: python\n \n-           >>> from websocket.client import WebSocketBaseClient\n+           >>> from ambari_ws4py.client import WebSocketBaseClient\n            >>> ws = WebSocketBaseClient('ws+unix:///tmp/my.sock')\n            >>> ws.resource = '/ws'\n            >>> ws.connect()\n@@ -78,6 +78,8 @@ def __init__(self, url, protocols=None, extensions=None,\n         self.resource = None\n         self.ssl_options = ssl_options or {}\n         self.extra_headers = headers or []\n+        self.exclude_headers = exclude_headers or []\n+        self.exclude_headers = [x.lower() for x in self.exclude_headers]\n \n         if self.scheme == \"wss\":\n             # Prevent check_hostname requires server_hostname (ref #187)\n@@ -211,7 +213,7 @@ def connect(self):\n             # default port is now 443; upgrade self.sender to send ssl\n             self.sock = ssl.wrap_socket(self.sock, **self.ssl_options)\n             self._is_secure = True\n-            \n+\n         self.sock.settimeout(10.0)\n         self.sock.connect(self.bind_addr)\n \n@@ -258,14 +260,15 @@ def handshake_headers(self):\n             ('Sec-WebSocket-Key', self.key.decode('utf-8')),\n             ('Sec-WebSocket-Version', str(max(WS_VERSION)))\n             ]\n-        \n+\n         if self.protocols:\n             headers.append(('Sec-WebSocket-Protocol', ','.join(self.protocols)))\n \n         if self.extra_headers:\n             headers.extend(self.extra_headers)\n \n-        if not any(x for x in headers if x[0].lower() == 'origin'):\n+        if not any(x for x in headers if x[0].lower() == 'origin') and \\\n+           'origin' not in self.exclude_headers:\n \n             scheme, url = self.url.split(\":\", 1)\n             parsed = urlsplit(url, scheme=\"http\")\n@@ -278,6 +281,8 @@ def handshake_headers(self):\n                 origin = origin + ':' + str(parsed.port)\n             headers.append(('Origin', origin))\n \n+        headers = [x for x in headers if x[0].lower() not in self.exclude_headers]\n+\n         return headers\n \n     @property\n@@ -329,10 +334,10 @@ def process_handshake_header(self, headers):\n                     raise HandshakeError(\"Invalid challenge response: %s\" % value)\n \n             elif header == b'sec-websocket-protocol':\n-                protocols = ','.join(value)\n+                protocols.extend([x.strip() for x in value.split(b',')])\n \n             elif header == b'sec-websocket-extensions':\n-                extensions = ','.join(value)\n+                extensions.extend([x.strip() for x in value.split(b',')])\n \n         return protocols, extensions\n ",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/client/__init__.py",
                "sha": "ce1c98fbbf90b00e51a0cc6324d884b48d4a2c2f",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/client/geventclient.py",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_ws4py/client/geventclient.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 5,
                "filename": "ambari-common/src/main/python/ambari_ws4py/client/geventclient.py",
                "patch": "@@ -10,10 +10,10 @@\n __all__ = ['WebSocketClient']\n \n class WebSocketClient(WebSocketBaseClient):\n-    def __init__(self, url, protocols=None, extensions=None, heartbeat_freq=None, ssl_options=None, headers=None):\n+    def __init__(self, url, protocols=None, extensions=None, heartbeat_freq=None, ssl_options=None, headers=None, exclude_headers=None):\n         \"\"\"\n         WebSocket client that executes the\n-        :meth:`run() <ambari_ws4py.websocket.WebSocket.run>` into a gevent greenlet.\n+        :meth:`run() <ws4py.websocket.WebSocket.run>` into a gevent greenlet.\n \n         .. code-block:: python\n \n@@ -41,7 +41,7 @@ def outgoing():\n           gevent.joinall(greenlets)\n         \"\"\"\n         WebSocketBaseClient.__init__(self, url, protocols, extensions, heartbeat_freq,\n-                                     ssl_options=ssl_options, headers=headers)\n+                                     ssl_options=ssl_options, headers=headers, exclude_headers=exclude_headers)\n         self._th = Greenlet(self.run)\n \n         self.messages = Queue()\n@@ -75,18 +75,22 @@ def closed(self, code, reason=None):\n         # to wait for\n         self.messages.put(StopIteration)\n \n-    def receive(self):\n+    def receive(self, block=True):\n         \"\"\"\n         Returns messages that were stored into the\n         `messages` queue and returns `None` when the\n         websocket is terminated or closed.\n+        `block` is passed though the gevent queue `.get()` method, which if \n+        True will block until an item in the queue is available. Set this to \n+        False if you just want to check the queue, which will raise an \n+        Empty exception you need to handle if there is no message to return.\n         \"\"\"\n         # If the websocket was terminated and there are no messages\n         # left in the queue, return None immediately otherwise the client\n         # will block forever\n         if self.terminated and self.messages.empty():\n             return None\n-        message = self.messages.get()\n+        message = self.messages.get(block=block)\n         if message is StopIteration:\n             return None\n         return message",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/client/geventclient.py",
                "sha": "ace6fcc015326d2b92b8091aea790c01355b98e7",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/client/threadedclient.py",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_ws4py/client/threadedclient.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 2,
                "filename": "ambari-common/src/main/python/ambari_ws4py/client/threadedclient.py",
                "patch": "@@ -7,7 +7,7 @@\n \n class WebSocketClient(WebSocketBaseClient):\n     def __init__(self, url, protocols=None, extensions=None, heartbeat_freq=None,\n-                 ssl_options=None, headers=None):\n+                 ssl_options=None, headers=None, exclude_headers=None):\n         \"\"\"\n         .. code-block:: python\n \n@@ -32,7 +32,7 @@ def received_message(self, m):\n \n         \"\"\"\n         WebSocketBaseClient.__init__(self, url, protocols, extensions, heartbeat_freq,\n-                                     ssl_options, headers=headers)\n+                                     ssl_options, headers=headers, exclude_headers=exclude_headers)\n         self._th = threading.Thread(target=self.run, name='WebSocketClient')\n         self._th.daemon = True\n ",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/client/threadedclient.py",
                "sha": "dc7b2e01ee6e84b95d3dae44b7d0f007fbff1927",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/client/tornadoclient.py",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_ws4py/client/tornadoclient.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 2,
                "filename": "ambari-common/src/main/python/ambari_ws4py/client/tornadoclient.py",
                "patch": "@@ -9,7 +9,7 @@\n \n class TornadoWebSocketClient(WebSocketBaseClient):\n     def __init__(self, url, protocols=None, extensions=None,\n-                 io_loop=None, ssl_options=None, headers=None):\n+                 io_loop=None, ssl_options=None, headers=None, exclude_headers=None):\n         \"\"\"\n         .. code-block:: python\n \n@@ -32,7 +32,7 @@ def closed(self, code, reason=None):\n             ioloop.IOLoop.instance().start()\n         \"\"\"\n         WebSocketBaseClient.__init__(self, url, protocols, extensions,\n-                                     ssl_options=ssl_options, headers=headers)\n+                                     ssl_options=ssl_options, headers=headers, exclude_headers=exclude_headers)\n         if self.scheme == \"wss\":\n             self.sock = ssl.wrap_socket(self.sock, do_handshake_on_connect=False, **self.ssl_options)\n             self._is_secure = True",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/client/tornadoclient.py",
                "sha": "5f4d67d2d77708b60614c7cd9f0e247502bf8b1c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/compat.py",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_ws4py/compat.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-common/src/main/python/ambari_ws4py/compat.py",
                "patch": "@@ -2,7 +2,7 @@\n __doc__ = \"\"\"\n This compatibility module is inspired by the one found\n in CherryPy. It provides a common entry point for the various\n-functions and types that are used with ambari_ws4py but which\n+functions and types that are used with ws4py but which\n differ from Python 2.x to Python 3.x\n \n There are likely better ways for some of them so feel",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/compat.py",
                "sha": "e986e338a1385e70c91cd7241f5146b070ac2974",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/manager.py",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_ws4py/manager.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 5,
                "filename": "ambari-common/src/main/python/ambari_ws4py/manager.py",
                "patch": "@@ -243,7 +243,7 @@ def add(self, websocket):\n         \"\"\"\n         Manage a new websocket.\n \n-        First calls its :meth:`opened() <ambari_ws4py.websocket.WebSocket.opened>`\n+        First calls its :meth:`opened() <ws4py.websocket.WebSocket.opened>`\n         method and register its socket against the poller\n         for reading events.\n         \"\"\"\n@@ -261,7 +261,7 @@ def remove(self, websocket):\n         \"\"\"\n         Remove the given ``websocket`` from the manager.\n \n-        This does not call its :meth:`closed() <ambari_ws4py.websocket.WebSocket.closed>`\n+        This does not call its :meth:`closed() <ws4py.websocket.WebSocket.closed>`\n         method as it's out-of-band by your application\n         or from within the manager's run loop.\n         \"\"\"\n@@ -292,8 +292,8 @@ def run(self):\n         call related websockets' `once` method to\n         read and process the incoming data.\n \n-        If the :meth:`once() <ambari_ws4py.websocket.WebSocket.once>`\n-        method returns a `False` value, its :meth:`terminate() <ambari_ws4py.websocket.WebSocket.terminate>`\n+        If the :meth:`once() <ws4py.websocket.WebSocket.once>`\n+        method returns a `False` value, its :meth:`terminate() <ws4py.websocket.WebSocket.terminate>`\n         method is also applied to properly close\n         the websocket and its socket is unregistered from the poller.\n \n@@ -335,7 +335,7 @@ def run(self):\n \n     def close_all(self, code=1001, message='Server is shutting down'):\n         \"\"\"\n-        Execute the :meth:`close() <ambari_ws4py.websocket.WebSocket.close>`\n+        Execute the :meth:`close() <ws4py.websocket.WebSocket.close>`\n         method of each registered websockets to initiate the closing handshake.\n         It doesn't wait for the handshake to complete properly.\n         \"\"\"",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/manager.py",
                "sha": "386f8a27785ea6bb0044b74cf2edbc8723b740cf",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/messaging.py",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_ws4py/messaging.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 1,
                "filename": "ambari-common/src/main/python/ambari_ws4py/messaging.py",
                "patch": "@@ -56,7 +56,7 @@ def single(self, mask=False):\n \n     def fragment(self, first=False, last=False, mask=False):\n         \"\"\"\n-        Returns a :class:`ambari_ws4py.framing.Frame` bytes.\n+        Returns a :class:`ws4py.framing.Frame` bytes.\n \n         The behavior depends on the given flags:\n ",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/messaging.py",
                "sha": "f8a86adf09a5ea157e6fb9476d6ebfd509349b20",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/server/__init__.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_ws4py/server/__init__.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/ambari_ws4py/server/__init__.py",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/server/__init__.py",
                "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/server/cherrypyserver.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_ws4py/server/cherrypyserver.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/ambari_ws4py/server/cherrypyserver.py",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/server/cherrypyserver.py",
                "sha": "98e26d417c43e949376f7fb804a4c732a706f14f",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/server/geventserver.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_ws4py/server/geventserver.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/ambari_ws4py/server/geventserver.py",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/server/geventserver.py",
                "sha": "239f86c1eb4474d95cda293c7bbd515ea88177bb",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/server/wsgirefserver.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_ws4py/server/wsgirefserver.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/ambari_ws4py/server/wsgirefserver.py",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/server/wsgirefserver.py",
                "sha": "3973dd69ac248f6a6cc6ffadb8dc7952d142d0ff",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/server/wsgiutils.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_ws4py/server/wsgiutils.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/ambari_ws4py/server/wsgiutils.py",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/server/wsgiutils.py",
                "sha": "b3eac11b7475e5d0f8ebdff74161cbb543714275",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/streaming.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_ws4py/streaming.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/ambari_ws4py/streaming.py",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/streaming.py",
                "sha": "5f075082b78768d91fd133de51fa3ee613682be9",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/websocket.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/ambari_ws4py/websocket.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/ambari_ws4py/websocket.py",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/ambari_ws4py/websocket.py",
                "sha": "936f333026bc579dd939e866aa67eb925183d89b",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/resource_management/core/providers/__init__.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/resource_management/core/providers/__init__.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/resource_management/core/providers/__init__.py",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/resource_management/core/providers/__init__.py",
                "sha": "cf8d0a6a484d4179bb40069acf01761f5a8b7ddd",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-common/src/main/python/resource_management/core/providers/package/__init__.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/resource_management/core/providers/package/__init__.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/resource_management/core/providers/package/__init__.py",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-common/src/main/python/resource_management/core/providers/package/__init__.py",
                "sha": "f2a375fe6e65f4a606829bcd0b5b1a98a8cf0495",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-common/src/main/python/resource_management/core/providers/package/apt.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/resource_management/core/providers/package/apt.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/resource_management/core/providers/package/apt.py",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-common/src/main/python/resource_management/core/providers/package/apt.py",
                "sha": "e658daf7de27aa1c4098f61a86f6cf4a4536b354",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-common/src/main/python/resource_management/core/providers/package/zypper.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/resource_management/core/providers/package/zypper.py?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/resource_management/core/providers/package/zypper.py",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-common/src/main/python/resource_management/core/providers/package/zypper.py",
                "sha": "504ec0d0fa36d0c793c57c597500788c92fe653d",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/resource_management/core/providers/packaging.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/resource_management/core/providers/packaging.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/resource_management/core/providers/packaging.py",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/resource_management/core/providers/packaging.py",
                "sha": "19f8d465e7a8b5b6b97cebdfbf13274ec2b7c12a",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/resource_management/core/providers/system.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/resource_management/core/providers/system.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/resource_management/core/providers/system.py",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/resource_management/core/providers/system.py",
                "sha": "6293436628f17ae0f1511633d5011f9532ce8c84",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/resource_management/core/shell.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/resource_management/core/shell.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/resource_management/core/shell.py",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/resource_management/core/shell.py",
                "sha": "e0c71f8a37ef90b68c056ddfc4e5b735fbc1d9ac",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/resource_management/libraries/functions/namenode_ha_utils.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/resource_management/libraries/functions/namenode_ha_utils.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/resource_management/libraries/functions/namenode_ha_utils.py",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/resource_management/libraries/functions/namenode_ha_utils.py",
                "sha": "ae1a6814eff1b4d58e9104a2faa5ca947b413e71",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/resource_management/libraries/functions/security_commons.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/resource_management/libraries/functions/security_commons.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/resource_management/libraries/functions/security_commons.py",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/resource_management/libraries/functions/security_commons.py",
                "sha": "3e74b757d8381b346d6d9eacf5bf1637e2912ed6",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/resource_management/libraries/providers/hdfs_resource.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/resource_management/libraries/providers/hdfs_resource.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/resource_management/libraries/providers/hdfs_resource.py",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/resource_management/libraries/providers/hdfs_resource.py",
                "sha": "897e83c659ccd686946866f698c6b3fb11ea9252",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/resource_management/libraries/script/script.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-common/src/main/python/resource_management/libraries/script/script.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-common/src/main/python/resource_management/libraries/script/script.py",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-common/src/main/python/resource_management/libraries/script/script.py",
                "sha": "626aa19d86844f4777db9e0b3bbeae2b77643491",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-funtest/pom.xml",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-funtest/pom.xml?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-funtest/pom.xml",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-funtest/pom.xml",
                "sha": "813afa7eb7bdffdce64059c33435e49113281fc4",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-assembly/pom.xml",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-assembly/pom.xml?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-assembly/pom.xml",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-assembly/pom.xml",
                "sha": "9ecf9fa421af689ca5f7498281ca520498aeebca",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-manager-it/pom.xml",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-manager-it/pom.xml?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-manager-it/pom.xml",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-manager-it/pom.xml",
                "sha": "c9b8bad50193fb12d61baa5ff50aaf94b740eead",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/Solr.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/Solr.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/Solr.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/Solr.java",
                "sha": "1ffdb2a2da4abcbff98829a63b71672b2b3a6ee7",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/TestUtil.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/TestUtil.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/TestUtil.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/TestUtil.java",
                "sha": "f48e10734a78cff31bfd065cf405b1ca5cf79658",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/solr/metrics/MetricsIT.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/solr/metrics/MetricsIT.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/solr/metrics/MetricsIT.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/solr/metrics/MetricsIT.java",
                "sha": "3016d67c0e85b2be2e642b0474d147f704083333",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/solr/metrics/MockMetricsServer.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/solr/metrics/MockMetricsServer.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/solr/metrics/MockMetricsServer.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/solr/metrics/MockMetricsServer.java",
                "sha": "9d2734fcb60c2db9f0dab29a6978434a891ac7e4",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/steps/AbstractInfraSteps.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/steps/AbstractInfraSteps.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/steps/AbstractInfraSteps.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/steps/AbstractInfraSteps.java",
                "sha": "f219ce5cfab20a41ef2deb84c12af7f8f528f3aa",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/steps/ExportJobsSteps.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/steps/ExportJobsSteps.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/steps/ExportJobsSteps.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-manager-it/src/test/java/org/apache/ambari/infra/steps/ExportJobsSteps.java",
                "sha": "d84c23fb69fd1c5e2edaea228b8771047988d159",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-manager/docker/docker-compose.yml",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-manager/docker/docker-compose.yml?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-manager/docker/docker-compose.yml",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-manager/docker/docker-compose.yml",
                "sha": "5644f58783682de6401b29955ab4984646d13e8e",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-manager/docker/infra-manager-docker-compose.sh",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-manager/docker/infra-manager-docker-compose.sh?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-manager/docker/infra-manager-docker-compose.sh",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-manager/docker/infra-manager-docker-compose.sh",
                "sha": "01ade7f05821bd19ad831fcfae4fb20e5a966792",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-client/build.xml",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-client/build.xml?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-client/build.xml",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-client/build.xml",
                "sha": "a5fdfbf73569a58e11c844c1c07aae0dce695b62",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-client/src/main/python/migrationHelper.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-client/src/main/python/migrationHelper.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-client/src/main/python/migrationHelper.py",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-client/src/main/python/migrationHelper.py",
                "sha": "1964f1069f6a6eed8c4e5758c653d1b677d92792",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-client/src/main/python/solrDataManager.py",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-client/src/main/python/solrDataManager.py?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-client/src/main/python/solrDataManager.py",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-client/src/main/python/solrDataManager.py",
                "sha": "5d2d2fa44006827153b278f0e4e74639b55ea2f0",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-client/src/main/resources/solrIndexHelper.sh",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-client/src/main/resources/solrIndexHelper.sh?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-client/src/main/resources/solrIndexHelper.sh",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-client/src/main/resources/solrIndexHelper.sh",
                "sha": "4ba342a151c58abdf20dcf6e6a3c75ebfece98d8",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/.gitignore",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-plugin/.gitignore?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-plugin/.gitignore",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/.gitignore",
                "sha": "379dea75037c3b0877333415ef8b94498c38be24",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/docker/Dockerfile",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-plugin/docker/Dockerfile?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-plugin/docker/Dockerfile",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/docker/Dockerfile",
                "sha": "d1835106a36d17718277743c311e33b7d16c76da",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/docker/docker-compose.yml",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-plugin/docker/docker-compose.yml?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-plugin/docker/docker-compose.yml",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/docker/docker-compose.yml",
                "sha": "98469383657cdf5004fadb2b18494aaf9ada6128",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/docker/infra-solr-docker-compose.sh",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-plugin/docker/infra-solr-docker-compose.sh?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-plugin/docker/infra-solr-docker-compose.sh",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/docker/infra-solr-docker-compose.sh",
                "sha": "770ff7c3186894c1ac5a01af85888d8ffef660ff",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/docker/infra-solr.sh",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-plugin/docker/infra-solr.sh?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-plugin/docker/infra-solr.sh",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/docker/infra-solr.sh",
                "sha": "bb5a6bd054efcb47882b56ed6097d23905abe3f8",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/docker/solr.xml",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-plugin/docker/solr.xml?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-plugin/docker/solr.xml",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/docker/solr.xml",
                "sha": "65aa8a41e192c3d658eda64e8e4a7d76059c2a0f",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/pom.xml",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-plugin/pom.xml?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-plugin/pom.xml",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/pom.xml",
                "sha": "c8e84d5ceb488e056a4d49bcd8cd00948f627175",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/AMSCacheReporter.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/AMSCacheReporter.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/AMSCacheReporter.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/AMSCacheReporter.java",
                "sha": "e25b80effda39f68377e8e884f3a5183c6578454",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/AMSProtocol.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/AMSProtocol.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/AMSProtocol.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/AMSProtocol.java",
                "sha": "404fc5c57561588b85dbbb21edb1055427635409",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/AMSReporter.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/AMSReporter.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/AMSReporter.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/AMSReporter.java",
                "sha": "6e31fd473d35a513715356848506fa3630ffcec8",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/GaugeConverter.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/GaugeConverter.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/GaugeConverter.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/GaugeConverter.java",
                "sha": "31d452b99ec1c2df9f8b8bdf8ba58ae2f387a437",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/MetricsUtils.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/MetricsUtils.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/MetricsUtils.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/MetricsUtils.java",
                "sha": "525c41924f0ec1db38012c13ed44d2addb7984ab",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/ScheduledAMSReporter.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/ScheduledAMSReporter.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/ScheduledAMSReporter.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/ScheduledAMSReporter.java",
                "sha": "9a837faa91c96807146c3d1cad058894f0c873d7",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/ScheduledAMSReporterBuilder.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/ScheduledAMSReporterBuilder.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/ScheduledAMSReporterBuilder.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/ScheduledAMSReporterBuilder.java",
                "sha": "b76325202dd4b7820e2fd796bbd08ef070830c52",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/SimpleAMSReporter.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/SimpleAMSReporter.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/SimpleAMSReporter.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/SimpleAMSReporter.java",
                "sha": "af6fd15da64e2e01f846e3ce56e87a185007bcec",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/SolrMetricsSecurityConfig.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/SolrMetricsSecurityConfig.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/SolrMetricsSecurityConfig.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/SolrMetricsSecurityConfig.java",
                "sha": "247bd33bab3060e7f5267b96d273a85cf26bbffc",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/SolrMetricsSink.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/SolrMetricsSink.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/SolrMetricsSink.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/java/org/apache/ambari/infra/solr/metrics/reporters/SolrMetricsSink.java",
                "sha": "201c7977867872b8654c7164cfa6afdd7b0b1539",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/resources/apispec/cluster.security.InfraRuleBasedAuthorization.json",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/ambari-infra-solr-plugin/src/main/resources/apispec/cluster.security.InfraRuleBasedAuthorization.json?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/ambari-infra-solr-plugin/src/main/resources/apispec/cluster.security.InfraRuleBasedAuthorization.json",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/ambari-infra-solr-plugin/src/main/resources/apispec/cluster.security.InfraRuleBasedAuthorization.json",
                "sha": "4a7fdbebabd77a9298333d6bd50ddbd5738bc9cf",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/pom.xml",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-infra/pom.xml?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-infra/pom.xml",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-infra/pom.xml",
                "sha": "51f5b75efb481d942b506a7d3f058acb2dc803d8",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/README.md",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/README.md?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/README.md",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/README.md",
                "sha": "fb3b56b3c4801b41a36813cd4f04302156937d3b",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-assembly/src/main/package/deb/logfeeder/postinst",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-assembly/src/main/package/deb/logfeeder/postinst?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-assembly/src/main/package/deb/logfeeder/postinst",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-assembly/src/main/package/deb/logfeeder/postinst",
                "sha": "d50dbd1a4ad43ff99d439d1c31b87748eb54ada0",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-assembly/src/main/package/deb/portal/postinst",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-assembly/src/main/package/deb/portal/postinst?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-assembly/src/main/package/deb/portal/postinst",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-assembly/src/main/package/deb/portal/postinst",
                "sha": "e71445b41cdd63cab1b043cd270e433985bcd92f",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/logfeeder/postinstall.sh",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/logfeeder/postinstall.sh?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/logfeeder/postinstall.sh",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/logfeeder/postinstall.sh",
                "sha": "2db0d52aa57d91703a9ed7b2eced856b22bbad9f",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/logfeeder/postremove.sh~HEAD",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/logfeeder/postremove.sh~HEAD?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/logfeeder/postremove.sh~HEAD",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/logfeeder/postremove.sh~HEAD",
                "sha": "5f1c623d65e0e0a0ff235d98fa22ae527990f210",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/logfeeder/postremove.sh~trunk",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/logfeeder/postremove.sh~trunk?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/logfeeder/postremove.sh~trunk",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/logfeeder/postremove.sh~trunk",
                "sha": "5f1c623d65e0e0a0ff235d98fa22ae527990f210",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/portal/postinstall.sh",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/portal/postinstall.sh?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/portal/postinstall.sh",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/portal/postinstall.sh",
                "sha": "97523f4f10e6ce63e035b8064806528016feaa94",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/portal/postremove.sh~HEAD",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/portal/postremove.sh~HEAD?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/portal/postremove.sh~HEAD",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/portal/postremove.sh~HEAD",
                "sha": "8de05c12990ca706c557a2b16aa30ddc12aecbc3",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/portal/postremove.sh~trunk",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/portal/postremove.sh~trunk?ref=a05762d212300684c0c4e6f0436b23bd32958ea8",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/portal/postremove.sh~trunk",
                "raw_url": "https://github.com/apache/ambari/raw/a05762d212300684c0c4e6f0436b23bd32958ea8/ambari-logsearch/ambari-logsearch-assembly/src/main/package/rpm/portal/postremove.sh~trunk",
                "sha": "8de05c12990ca706c557a2b16aa30ddc12aecbc3",
                "status": "removed"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-config-api/src/main/java/org/apache/ambari/logsearch/config/api/model/inputconfig/FilterGrokDescriptor.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-config-api/src/main/java/org/apache/ambari/logsearch/config/api/model/inputconfig/FilterGrokDescriptor.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-config-api/src/main/java/org/apache/ambari/logsearch/config/api/model/inputconfig/FilterGrokDescriptor.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-config-api/src/main/java/org/apache/ambari/logsearch/config/api/model/inputconfig/FilterGrokDescriptor.java",
                "sha": "9fc8eb465fc2f6e2356b137b32495b65921dc467",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-config-api/src/main/java/org/apache/ambari/logsearch/config/api/model/inputconfig/InputDescriptor.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-config-api/src/main/java/org/apache/ambari/logsearch/config/api/model/inputconfig/InputDescriptor.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-config-api/src/main/java/org/apache/ambari/logsearch/config/api/model/inputconfig/InputDescriptor.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-config-api/src/main/java/org/apache/ambari/logsearch/config/api/model/inputconfig/InputDescriptor.java",
                "sha": "71f3cd1a8031f852d6ad756bd4a91d55c71c9645",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-config-api/src/main/java/org/apache/ambari/logsearch/config/api/model/inputconfig/InputFileDescriptor.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-config-api/src/main/java/org/apache/ambari/logsearch/config/api/model/inputconfig/InputFileDescriptor.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-config-api/src/main/java/org/apache/ambari/logsearch/config/api/model/inputconfig/InputFileDescriptor.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-config-api/src/main/java/org/apache/ambari/logsearch/config/api/model/inputconfig/InputFileDescriptor.java",
                "sha": "b58db6aa26e9328c855070a3fea8e287bf38a11b",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-config-zookeeper/src/main/java/org/apache/ambari/logsearch/config/zookeeper/model/inputconfig/impl/FilterGrokDescriptorImpl.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-config-zookeeper/src/main/java/org/apache/ambari/logsearch/config/zookeeper/model/inputconfig/impl/FilterGrokDescriptorImpl.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-config-zookeeper/src/main/java/org/apache/ambari/logsearch/config/zookeeper/model/inputconfig/impl/FilterGrokDescriptorImpl.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-config-zookeeper/src/main/java/org/apache/ambari/logsearch/config/zookeeper/model/inputconfig/impl/FilterGrokDescriptorImpl.java",
                "sha": "9823163209b38ba8312f982542d25fc046b36693",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-config-zookeeper/src/main/java/org/apache/ambari/logsearch/config/zookeeper/model/inputconfig/impl/InputDescriptorImpl.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-config-zookeeper/src/main/java/org/apache/ambari/logsearch/config/zookeeper/model/inputconfig/impl/InputDescriptorImpl.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-config-zookeeper/src/main/java/org/apache/ambari/logsearch/config/zookeeper/model/inputconfig/impl/InputDescriptorImpl.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-config-zookeeper/src/main/java/org/apache/ambari/logsearch/config/zookeeper/model/inputconfig/impl/InputDescriptorImpl.java",
                "sha": "40886be2be67ab49a395541087c3d15edd671d95",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-config-zookeeper/src/main/java/org/apache/ambari/logsearch/config/zookeeper/model/inputconfig/impl/InputFileDescriptorImpl.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-config-zookeeper/src/main/java/org/apache/ambari/logsearch/config/zookeeper/model/inputconfig/impl/InputFileDescriptorImpl.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-config-zookeeper/src/main/java/org/apache/ambari/logsearch/config/zookeeper/model/inputconfig/impl/InputFileDescriptorImpl.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-config-zookeeper/src/main/java/org/apache/ambari/logsearch/config/zookeeper/model/inputconfig/impl/InputFileDescriptorImpl.java",
                "sha": "99b42fe8c31aaf03cd7b41e35f9dfea840f9a0a0",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-it/src/test/java/org/apache/ambari/logsearch/domain/StoryDataRegistry.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-it/src/test/java/org/apache/ambari/logsearch/domain/StoryDataRegistry.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-it/src/test/java/org/apache/ambari/logsearch/domain/StoryDataRegistry.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-it/src/test/java/org/apache/ambari/logsearch/domain/StoryDataRegistry.java",
                "sha": "58398819844e3804dc03d9de86a7938cb06d3216",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-it/src/test/java/org/apache/ambari/logsearch/steps/AbstractLogSearchSteps.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-it/src/test/java/org/apache/ambari/logsearch/steps/AbstractLogSearchSteps.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-it/src/test/java/org/apache/ambari/logsearch/steps/AbstractLogSearchSteps.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-it/src/test/java/org/apache/ambari/logsearch/steps/AbstractLogSearchSteps.java",
                "sha": "f92516f062a0347f190cf06fd8dc5bbf0ebbc26f",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-it/src/test/java/org/apache/ambari/logsearch/steps/LogSearchDockerSteps.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-it/src/test/java/org/apache/ambari/logsearch/steps/LogSearchDockerSteps.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-it/src/test/java/org/apache/ambari/logsearch/steps/LogSearchDockerSteps.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-it/src/test/java/org/apache/ambari/logsearch/steps/LogSearchDockerSteps.java",
                "sha": "cbbeb2d4fb6f8142017f1a13d7bee27fa6e17925",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/common/ConfigItem.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/common/ConfigItem.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/common/ConfigItem.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/common/ConfigItem.java",
                "sha": "1cbbfd5c0e3401578cc08024e891dc1aa8592db6",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/common/LogFeederProperties.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/common/LogFeederProperties.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/common/LogFeederProperties.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/common/LogFeederProperties.java",
                "sha": "7fac01a96f0892d4a67d75331f9acad898129558",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/common/MetricData.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/common/MetricData.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/common/MetricData.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/common/MetricData.java",
                "sha": "54cdb7e7dece8df3c59c8463ecd19fd980629f20",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/filter/Filter.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/filter/Filter.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/filter/Filter.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/filter/Filter.java",
                "sha": "f0982452891ea9575db19e5fb34e250df88863f2",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/input/Input.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/input/Input.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/input/Input.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/input/Input.java",
                "sha": "a586510d194bddb283455e2e12e18801a7dd2010",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/input/cache/LRUCache.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/input/cache/LRUCache.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/input/cache/LRUCache.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder-plugin-api/src/main/java/org/apache/ambari/logfeeder/plugin/input/cache/LRUCache.java",
                "sha": "9ee874319bb73d7e8c3f9568e49489354c22fda5",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/pom.xml",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder/pom.xml?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder/pom.xml",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/pom.xml",
                "sha": "688c944317f4933e0ebc95528c02736a827234c5",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/common/LogFeederConstants.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/common/LogFeederConstants.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/common/LogFeederConstants.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/common/LogFeederConstants.java",
                "sha": "80dc163b5a26b7b97534be3a3bcd6fa0bf04d43f",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/conf/LogFeederProps.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/conf/LogFeederProps.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/conf/LogFeederProps.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/conf/LogFeederProps.java",
                "sha": "9a40e703ad93560efcacc34742df7908689cb0e3",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/conf/LogFeederSecurityConfig.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/conf/LogFeederSecurityConfig.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/conf/LogFeederSecurityConfig.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/conf/LogFeederSecurityConfig.java",
                "sha": "4c5b4dd665c373c2014ff98e464059bd61590438",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/filter/FilterGrok.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/filter/FilterGrok.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/filter/FilterGrok.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/filter/FilterGrok.java",
                "sha": "2074f93d460665ea21c94dbcd10e2ff5ceec0a10",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/InputFile.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/InputFile.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/InputFile.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/InputFile.java",
                "sha": "726a237c15a4c388a0cfa5320fe58a81c4187a9a",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/InputManagerImpl.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/InputManagerImpl.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/InputManagerImpl.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/InputManagerImpl.java",
                "sha": "40475c6705e35983ce627a394f37b1baa63e1000",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/file/FileCheckInHelper.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/file/FileCheckInHelper.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/file/FileCheckInHelper.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/file/FileCheckInHelper.java",
                "sha": "7b8f0cd1898263e7a5bcdad6a6ad1f9b02fd0902",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/monitor/AbstractLogFileMonitor.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/monitor/AbstractLogFileMonitor.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/monitor/AbstractLogFileMonitor.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/monitor/AbstractLogFileMonitor.java",
                "sha": "e0acde12c333fbd2804635020ebd56fb1d082884",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/monitor/CheckpointCleanupMonitor.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/monitor/CheckpointCleanupMonitor.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/monitor/CheckpointCleanupMonitor.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/monitor/CheckpointCleanupMonitor.java",
                "sha": "65a314ea8f69154b5d667cc1883e6658465a9ec4",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/monitor/LogFileDetachMonitor.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/monitor/LogFileDetachMonitor.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/monitor/LogFileDetachMonitor.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/monitor/LogFileDetachMonitor.java",
                "sha": "a40e118a7a17f84b62e13c59eff0fa7d482dc140",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/monitor/LogFilePathUpdateMonitor.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/monitor/LogFilePathUpdateMonitor.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/monitor/LogFilePathUpdateMonitor.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/input/monitor/LogFilePathUpdateMonitor.java",
                "sha": "bfcab5dd64124f11c06d0a48d3074944acc5d4c2",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/metrics/LogFeederAMSClient.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/metrics/LogFeederAMSClient.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/metrics/LogFeederAMSClient.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/metrics/LogFeederAMSClient.java",
                "sha": "0ccdff34c2a978d65db568ae9ebb9e9d0c693d8e",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/output/OutputManagerImpl.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/output/OutputManagerImpl.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/output/OutputManagerImpl.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/output/OutputManagerImpl.java",
                "sha": "834cf7e178ec4a1d640878e261c30427b19b0816",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/output/OutputSolr.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/output/OutputSolr.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/output/OutputSolr.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/output/OutputSolr.java",
                "sha": "12a804d2b7b532868b2301c5c7d08f3cd3285491",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/util/FileUtil.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/util/FileUtil.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/util/FileUtil.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/util/FileUtil.java",
                "sha": "cc6b1e0a0d9706eabc92543ca959bb8d0ca4f3ab",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/util/LogFeederUtil.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/util/LogFeederUtil.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/util/LogFeederUtil.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/java/org/apache/ambari/logfeeder/util/LogFeederUtil.java",
                "sha": "9b0b0e889df11077c29645bc84d951e60edc11ee",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/scripts/logfeeder.sh",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-logfeeder/src/main/scripts/logfeeder.sh?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-logfeeder/src/main/scripts/logfeeder.sh",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-logfeeder/src/main/scripts/logfeeder.sh",
                "sha": "e750b2a275789df8db33edb952d3ac89d2700469",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/auth/filter/AbstractJWTFilter.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/auth/filter/AbstractJWTFilter.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/auth/filter/AbstractJWTFilter.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/auth/filter/AbstractJWTFilter.java",
                "sha": "81b0ffb7d554fe1708f8186b0d82863a53eb1d28",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/conf/SecurityConfig.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/conf/SecurityConfig.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/conf/SecurityConfig.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/conf/SecurityConfig.java",
                "sha": "9ca91650fab69dea75c2a51e8ca694da0b246c5c",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/conf/SolrAuditLogPropsConfig.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/conf/SolrAuditLogPropsConfig.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/conf/SolrAuditLogPropsConfig.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/conf/SolrAuditLogPropsConfig.java",
                "sha": "43349a658c0a11af095dd38ea24f17207d756c00",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/conf/SolrConnectionPropsConfig.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/conf/SolrConnectionPropsConfig.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/conf/SolrConnectionPropsConfig.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/conf/SolrConnectionPropsConfig.java",
                "sha": "87b77bf3f5208089793f653f113b74a225b030dd",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/conf/SolrPropsConfig.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/conf/SolrPropsConfig.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/conf/SolrPropsConfig.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/conf/SolrPropsConfig.java",
                "sha": "ebb1acbadaccc524fab21b8a21565abfc26d14a5",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/configurer/SolrCollectionConfigurer.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/configurer/SolrCollectionConfigurer.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/configurer/SolrCollectionConfigurer.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/configurer/SolrCollectionConfigurer.java",
                "sha": "67f73638c319b95a7b354749504503553c644210",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/handler/CreateCollectionHandler.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/handler/CreateCollectionHandler.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/handler/CreateCollectionHandler.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/handler/CreateCollectionHandler.java",
                "sha": "ba5432f8524091f1842c8f12f3e00a97c2c1a99e",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/model/common/LSServerFilterGrok.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/model/common/LSServerFilterGrok.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/model/common/LSServerFilterGrok.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/model/common/LSServerFilterGrok.java",
                "sha": "215c572a57bb0db8c80bc1c4f06713d8f3989ed6",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/model/common/LSServerInput.java",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/model/common/LSServerInput.java?ref=7e1c33a681c79bdbe786ecf10fb5bf050ccc6188",
                "deletions": 0,
                "filename": "ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/model/common/LSServerInput.java",
                "raw_url": "https://github.com/apache/ambari/raw/7e1c33a681c79bdbe786ecf10fb5bf050ccc6188/ambari-logsearch/ambari-logsearch-server/src/main/java/org/apache/ambari/logsearch/model/common/LSServerInput.java",
                "sha": "af28f1787c509a321c938524d2fb8aa40f5f0445",
                "status": "modified"
            }
        ],
        "message": "Merge remote-tracking branch 'upstream/trunk' into trunk\n\n* upstream/trunk: (325 commits)\n  AMBARI-23416 Background Operations: icons have incorrect color\n  AMBARI-23411 Invalid websocket client port when connected over https\n  AMBARI-23482. NN Federation: service summary widgets should show correct metrics data (akovalenko)\n  AMBARI-23472 NameNode HA: QuickLinks section issues. (ababiichuk)\n  AMBARI-23490. NN Federation Wizard: Bootstrap NameNode failed (aonishuk)\n  AMBARI-23199. Infra Solr custom commands: disable host checks by default for unsecure env\n  AMBARI-23421. solrDataManager.py: Add _version_ to exclude list by default\n  AMBARI-23477. Utility to parse initial active namenode for blueprint deployment (#900)\n  AMBARI-23454. Integrate ConfigGroupService to Swagger (#899)\n  [AMBARI-23467] Initial changes for Blueprint configuration processing of NameNode Federation Clusters (#890)\n  [AMBARI-23471] Ubuntu16: Install components exited with code '100' with message: debconf: delaying package configuration, since apt-utils is not installed (dgrinenko)\n  [AMBARI-23461] Fix TestHeartbeatHandler.testComponents NPE\n  AMBARI-23390 Dashboard: rationalize default widgets\n  AMBARI-23207 The tooltip for Overriden properties shown under manage Config Groups page has '<br/>' in the tooltip text\n  AMBARI-23421. ADDENDUM: fix spaces before content type header\n  AMBARI-23399. Disabled use case until it's fixed for any user\n  AMBARI-23142 - Add AMS Metrics publisher to Infra Solr (#556)\n  AMBARI-23464. Debian9 is not shown on UI Install Wizard Select Version page (amagyar)\n  AMBARI-23462 NN federation related fixes for host details page. (ababiichuk)\n  AMBARI-23453. Integrate ConfigurationService to Swagger (#883)\n  ...",
        "parent": "https://github.com/apache/ambari/commit/a05762d212300684c0c4e6f0436b23bd32958ea8",
        "repo": "ambari",
        "unit_tests": [
            "ConfigurationBuilderTest.java",
            "UtilsTest.java"
        ]
    },
    "ambari_8263f2f": {
        "bug_id": "ambari_8263f2f",
        "commit": "https://github.com/apache/ambari/commit/8263f2f1b5de393f2f3eeefb3fc1c4112f6da8d2",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/8263f2f1b5de393f2f3eeefb3fc1c4112f6da8d2/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java?ref=8263f2f1b5de393f2f3eeefb3fc1c4112f6da8d2",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java",
                "patch": "@@ -1282,6 +1282,7 @@ private synchronized RequestStatusResponse updateCluster(ClusterRequest request,\n           clusterConfigProperties = clusterConfig.getProperties();\n         } else {\n           isConfigurationCreationNeeded = true;\n+          break;\n         }\n         if (requestConfigProperties == null || requestConfigProperties.isEmpty()) {\n           Config existingConfig = cluster.getConfig(desiredConfig.getType(), desiredConfig.getVersionTag());",
                "raw_url": "https://github.com/apache/ambari/raw/8263f2f1b5de393f2f3eeefb3fc1c4112f6da8d2/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java",
                "sha": "d1aebaef10835f33a48889456ce90c67fafcaf38",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/ambari/blob/8263f2f1b5de393f2f3eeefb3fc1c4112f6da8d2/ambari-server/src/test/java/org/apache/ambari/server/controller/AmbariManagementControllerTest.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/controller/AmbariManagementControllerTest.java?ref=8263f2f1b5de393f2f3eeefb3fc1c4112f6da8d2",
                "deletions": 3,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/controller/AmbariManagementControllerTest.java",
                "patch": "@@ -5246,25 +5246,38 @@ public void testReConfigureServiceClient() throws AmbariException {\n     configs.put(\"a\", \"b\");\n     Map<String, String> configs2 = new HashMap<String, String>();\n     configs2.put(\"c\", \"d\");\n+    Map<String, String> configs3 = new HashMap<String, String>();\n \n-    ConfigurationRequest cr1,cr2,cr3;\n+    ConfigurationRequest cr1,cr2,cr3,cr4;\n     cr1 = new ConfigurationRequest(clusterName, \"core-site\",\"version1\",\n       configs, null);\n     cr2 = new ConfigurationRequest(clusterName, \"hdfs-site\",\"version1\",\n       configs, null);\n+    cr4 = new ConfigurationRequest(clusterName, \"kerberos-env\", \"version1\",\n+      configs3, null);\n+\n+    ConfigFactory cf = injector.getInstance(ConfigFactory.class);\n+    Config config1 = cf.createNew(cluster, \"kerberos-env\",\n+        new HashMap<String, String>(), new HashMap<String, Map<String,String>>());\n+    config1.setTag(\"version1\");\n+\n+    cluster.addConfig(config1);\n \n     ClusterRequest crReq = new ClusterRequest(cluster.getClusterId(), clusterName, null, null);\n     crReq.setDesiredConfig(Collections.singletonList(cr1));\n     controller.updateClusters(Collections.singleton(crReq), null);\n     crReq = new ClusterRequest(cluster.getClusterId(), clusterName, null, null);\n     crReq.setDesiredConfig(Collections.singletonList(cr2));\n     controller.updateClusters(Collections.singleton(crReq), null);\n+    crReq = new ClusterRequest(cluster.getClusterId(), clusterName, null, null);\n+    crReq.setDesiredConfig(Collections.singletonList(cr4));\n+    controller.updateClusters(Collections.singleton(crReq), null);\n \n     // Install\n     long requestId1 = installService(clusterName, serviceName1, true, false);\n \n     List<Stage> stages = actionDB.getAllStages(requestId1);\n-    Assert.assertEquals(2, stages.get(0).getOrderedHostRoleCommands().get(0)\n+    Assert.assertEquals(3, stages.get(0).getOrderedHostRoleCommands().get(0)\n       .getExecutionCommandWrapper().getExecutionCommand()\n       .getConfigurationTags().size());\n \n@@ -5320,7 +5333,7 @@ public void testReConfigureServiceClient() throws AmbariException {\n     Assert.assertNotNull(hdfsCmdHost2);\n     ExecutionCommand execCmd = hdfsCmdHost3.getExecutionCommandWrapper()\n       .getExecutionCommand();\n-    Assert.assertEquals(2, execCmd.getConfigurationTags().size());\n+    Assert.assertEquals(3, execCmd.getConfigurationTags().size());\n     Assert.assertEquals(\"version122\", execCmd.getConfigurationTags().get\n       (\"core-site\").get(\"tag\"));\n     Assert.assertEquals(\"d\", execCmd.getConfigurations().get(\"core-site\")",
                "raw_url": "https://github.com/apache/ambari/raw/8263f2f1b5de393f2f3eeefb3fc1c4112f6da8d2/ambari-server/src/test/java/org/apache/ambari/server/controller/AmbariManagementControllerTest.java",
                "sha": "00f9bedcb93ec8ecf7d135de965ad3c0745f0764",
                "status": "modified"
            }
        ],
        "message": "AMBARI-11139. NPE when setting desired config via REST API.(vbrodetskyi)",
        "parent": "https://github.com/apache/ambari/commit/e50a2ac31b28e90ba1cfee70bf337c65910b7ee7",
        "repo": "ambari",
        "unit_tests": [
            "AmbariManagementControllerImplTest.java"
        ]
    },
    "ambari_83bef41": {
        "bug_id": "ambari_83bef41",
        "commit": "https://github.com/apache/ambari/commit/83bef413c47c76cbf54bc64847620769168389ac",
        "file": [
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/ambari/blob/83bef413c47c76cbf54bc64847620769168389ac/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariServer.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariServer.java?ref=83bef413c47c76cbf54bc64847620769168389ac",
                "deletions": 4,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariServer.java",
                "patch": "@@ -794,10 +794,14 @@ protected void initDB() throws AmbariException {\n   }\n \n   public void stop() throws Exception {\n-    try {\n-      server.stop();\n-    } catch (Exception e) {\n-      LOG.error(\"Error stopping the server\", e);\n+    if (server == null) {\n+      throw new AmbariException(\"Error stopping the server\");\n+    } else {\n+      try {\n+        server.stop();\n+      } catch (Exception e) {\n+        LOG.error(\"Error stopping the server\", e);\n+      }\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/ambari/raw/83bef413c47c76cbf54bc64847620769168389ac/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariServer.java",
                "sha": "b4161298c7d450037e315806816b714f80d5e323",
                "status": "modified"
            }
        ],
        "message": "AMBARI-14928 Ambari throws NPE in kerberos mode U14/S11 (dsen)",
        "parent": "https://github.com/apache/ambari/commit/ed492292cd4fedd41ebe95ba97a73e57cceb0bae",
        "repo": "ambari",
        "unit_tests": [
            "AmbariServerTest.java"
        ]
    },
    "ambari_962a973": {
        "bug_id": "ambari_962a973",
        "commit": "https://github.com/apache/ambari/commit/962a9730454f1b771d08ff3280f773a8a1251b7e",
        "file": [
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/ambari/blob/962a9730454f1b771d08ff3280f773a8a1251b7e/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/AbstractProviderModule.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/AbstractProviderModule.java?ref=962a9730454f1b771d08ff3280f773a8a1251b7e",
                "deletions": 9,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/AbstractProviderModule.java",
                "patch": "@@ -66,6 +66,7 @@\n import java.util.Map.Entry;\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n \n import static org.apache.ambari.server.controller.metrics.MetricsServiceProvider.MetricsService.GANGLIA;\n import static org.apache.ambari.server.controller.metrics.MetricsServiceProvider.MetricsService.TIMELINE_METRICS;\n@@ -199,8 +200,8 @@\n   /**\n    * JMX ports read from the configs\n    */\n-  private final Map<String, Map<String, Map<String, String>> >jmxPortMap =\n-      new HashMap<String, Map<String, Map<String, String>>>();\n+  private final Map<String, ConcurrentMap<String, ConcurrentMap<String, String>> >jmxPortMap =\n+      Collections.synchronizedMap(new HashMap<String, ConcurrentMap<String, ConcurrentMap<String, String>>>());\n \n   private volatile boolean initialized = false;\n \n@@ -494,12 +495,12 @@ public String getPort(String clusterName, String componentName, String hostName)\n   @Override\n   public String getPort(String clusterName, String componentName, String hostName, boolean httpsEnabled) throws SystemException {\n     // Parent map need not be synchronized\n-    Map<String, Map<String, String>> clusterJmxPorts = jmxPortMap.get(clusterName);\n+    ConcurrentMap<String, ConcurrentMap<String, String>> clusterJmxPorts = jmxPortMap.get(clusterName);\n     if (clusterJmxPorts == null) {\n       synchronized (jmxPortMap) {\n         clusterJmxPorts = jmxPortMap.get(clusterName);\n         if (clusterJmxPorts == null) {\n-          clusterJmxPorts = new ConcurrentHashMap<String, Map<String, String>>();\n+          clusterJmxPorts = new ConcurrentHashMap<String, ConcurrentMap<String, String>>();\n           jmxPortMap.put(clusterName, clusterJmxPorts);\n         }\n       }\n@@ -547,20 +548,28 @@ public String getPort(String clusterName, String componentName, String hostName,\n             // this will trigger using the default port for the component\n             String portString = getPortString(entry.getValue());\n             if (null != portString) {\n-              if(!clusterJmxPorts.containsKey(hostName)) {\n-                clusterJmxPorts.put(hostName, new ConcurrentHashMap<String, String>());\n-              }\n+              clusterJmxPorts.putIfAbsent(hostName, new ConcurrentHashMap<String, String>());\n               clusterJmxPorts.get(hostName).put(entry.getKey(), portString);\n             }\n           }\n         }\n       } catch (Exception e) {\n-        LOG.error(\"Exception initializing jmx port maps. \" + e);\n+        LOG.error(\"Exception initializing jmx port maps. \", e);\n       }\n     }\n \n     LOG.debug(\"jmxPortMap -> \" + jmxPortMap);\n-    return clusterJmxPorts.get(hostName).get(componentName);\n+\n+    ConcurrentMap<String, String> hostJmxPorts = clusterJmxPorts.get(hostName);\n+    if (hostJmxPorts == null) {\n+      LOG.debug(\"Jmx ports not loaded from properties: clusterName={}, componentName={}, hostName={}, \" +\n+          \"clusterJmxPorts={}, jmxPortMap.get(clusterName)={}\",\n+          clusterName, componentName, hostName, clusterJmxPorts, jmxPortMap.get(clusterName));\n+      //returning null is acceptable in cases when property with port not present\n+      //or loading from property is not supported for specific component\n+      return null;\n+    }\n+    return hostJmxPorts.get(componentName);\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/ambari/raw/962a9730454f1b771d08ff3280f773a8a1251b7e/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/AbstractProviderModule.java",
                "sha": "30e5b9ccca6ef1fec70a9615c5c287e57b495b4f",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/962a9730454f1b771d08ff3280f773a8a1251b7e/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackDefinedPropertyProvider.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackDefinedPropertyProvider.java?ref=962a9730454f1b771d08ff3280f773a8a1251b7e",
                "deletions": 2,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackDefinedPropertyProvider.java",
                "patch": "@@ -209,8 +209,8 @@ public StackDefinedPropertyProvider(Resource.Type type,\n     } catch (AuthorizationException e) {\n       // Need to rethrow the catched 'AuthorizationException'.\n       throw e;\n-    }\n-    catch (Exception e) {\n+    } catch (Exception e) {\n+      LOG.error(\"Error loading deferred resources\", e);\n       throw new SystemException(\"Error loading deferred resources\", e);\n     }\n ",
                "raw_url": "https://github.com/apache/ambari/raw/962a9730454f1b771d08ff3280f773a8a1251b7e/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackDefinedPropertyProvider.java",
                "sha": "6c40d14c744259526d6f0798d9bdf538793d2d00",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/ambari/blob/962a9730454f1b771d08ff3280f773a8a1251b7e/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/JMXHostProviderTest.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/JMXHostProviderTest.java?ref=962a9730454f1b771d08ff3280f773a8a1251b7e",
                "deletions": 0,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/controller/internal/JMXHostProviderTest.java",
                "patch": "@@ -543,6 +543,9 @@ public void testJMXPortMapUpdate() throws\n     //Unrelated ports\n     Assert.assertEquals(\"70070\", providerModule.getPort(\"c1\", \"NAMENODE\", \"localhost\"));\n     Assert.assertEquals(null, providerModule.getPort(\"c1\", \"JOBTRACKER\", \"localhost\"));\n+\n+    //test another host and component without property\n+    Assert.assertNull(providerModule.getPort(\"c1\", \"HBASE_REGIONSERVER\", \"remotehost1\"));\n   }\n \n   private static class JMXHostProviderModule extends AbstractProviderModule {",
                "raw_url": "https://github.com/apache/ambari/raw/962a9730454f1b771d08ff3280f773a8a1251b7e/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/JMXHostProviderTest.java",
                "sha": "70e21d3c7f577fd3a165ae7cfe15ca8e7ca4abc9",
                "status": "modified"
            }
        ],
        "message": "AMBARI-15375. NPE while gathering JMX ports from configs. (mpapirkovskyy)",
        "parent": "https://github.com/apache/ambari/commit/39eded40b1dcf063c351f4bb6832117d14fd0583",
        "repo": "ambari",
        "unit_tests": [
            "StackDefinedPropertyProviderTest.java"
        ]
    },
    "ambari_966f303": {
        "bug_id": "ambari_966f303",
        "commit": "https://github.com/apache/ambari/commit/966f3031d50b0106dfe5d19abbdd1e3b84d1f059",
        "file": [
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/ambari/blob/966f3031d50b0106dfe5d19abbdd1e3b84d1f059/ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClusterImpl.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClusterImpl.java?ref=966f3031d50b0106dfe5d19abbdd1e3b84d1f059",
                "deletions": 1,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClusterImpl.java",
                "patch": "@@ -2267,7 +2267,17 @@ public DesiredConfig transformEntry(@Nullable String key, @Nullable Set<DesiredC\n                 \" unknown configType=\" + e.getType());\n             continue;\n           }\n-          c.setVersion(allConfigs.get(e.getType()).get(e.getTag()).getVersion());\n+\n+          Map<String, Config> configMap = allConfigs.get(e.getType());\n+          if(!configMap.containsKey(e.getTag())) {\n+            LOG.debug(\"Config inconsistency exists for typeName=\" +\n+                    e.getType() +\n+                    \", unknown versionTag=\" + e.getTag());\n+            continue;\n+          }\n+\n+          Config config = configMap.get(e.getTag());\n+          c.setVersion(config.getVersion());\n \n           Set<DesiredConfig> configs = map.get(e.getType());\n           if (configs == null) {",
                "raw_url": "https://github.com/apache/ambari/raw/966f3031d50b0106dfe5d19abbdd1e3b84d1f059/ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClusterImpl.java",
                "sha": "57941d01e9795c903c036d400aeae72228fb3d76",
                "status": "modified"
            }
        ],
        "message": "AMBARI-14988: DB consistency - Add consistency check on clusterconfigmapping in ClusterImpl::getDesiredConfigs() for NPE.",
        "parent": "https://github.com/apache/ambari/commit/f247ddcfa6c7cc48508cbcfa1e681169dce3b495",
        "repo": "ambari",
        "unit_tests": [
            "ClusterImplTest.java"
        ]
    },
    "ambari_96cf16f": {
        "bug_id": "ambari_96cf16f",
        "commit": "https://github.com/apache/ambari/commit/96cf16f195c35aa6617e74e9c2893c53bea1a8b5",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/ambari/blob/96cf16f195c35aa6617e74e9c2893c53bea1a8b5/AMBARI-666-CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/AMBARI-666-CHANGES.txt?ref=96cf16f195c35aa6617e74e9c2893c53bea1a8b5",
                "deletions": 0,
                "filename": "AMBARI-666-CHANGES.txt",
                "patch": "@@ -12,6 +12,9 @@ AMBARI-666 branch (unreleased changes)\n \n   NEW FEATURES\n \n+  AMBARI-845. Fix NPE in the server to be able to run the server api's.\n+  (mahadev)\n+\n   AMBARI-844. Mock JMX provider for manual tests. (Tom Beerbower via mahadev)\n \n   AMBARI-841. Fix comparison predicates in case where resource does not have",
                "raw_url": "https://github.com/apache/ambari/raw/96cf16f195c35aa6617e74e9c2893c53bea1a8b5/AMBARI-666-CHANGES.txt",
                "sha": "794ecf116fc80b0935be22a697f7ac4c16af8ae6",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/ambari/blob/96cf16f195c35aa6617e74e9c2893c53bea1a8b5/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java?ref=96cf16f195c35aa6617e74e9c2893c53bea1a8b5",
                "deletions": 5,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java",
                "patch": "@@ -251,7 +251,7 @@ public TrackActionResponse createHost(HostRequest request)\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Received a createHost request\"\n           + \", hostname=\" + request.getHostname()\n-          + \", request=\" + request);\n+          );\n     }\n \n     Host h = clusters.getHost(request.getHostname());\n@@ -283,23 +283,27 @@ public TrackActionResponse createHostComponent(ServiceComponentHostRequest\n       request) throws AmbariException {\n     if (request.getClusterName() == null\n         || request.getClusterName().isEmpty()\n-        || request.getServiceName() == null\n-        || request.getServiceName().isEmpty()\n         || request.getComponentName() == null\n         || request.getComponentName().isEmpty()\n         || request.getHostname() == null\n         || request.getHostname().isEmpty()) {\n       // FIXME throw correct error\n       throw new AmbariException(\"Invalid arguments\");\n     }\n-\n+    \n+    // FIXME Hard coded stuff --- needs to be fixed.\n+    if (request.getServiceName() == null\n+        || request.getServiceName().isEmpty()\n+      ) {\n+      request.setServiceName(\"HDFS\");\n+    }\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Received a createHostComponent request\"\n           + \", clusterName=\" + request.getClusterName()\n           + \", serviceName=\" + request.getServiceName()\n           + \", componentName=\" + request.getComponentName()\n           + \", hostname=\" + request.getHostname()\n-          + \", request=\" + request);\n+          );\n     }\n \n     final Cluster cluster = clusters.getCluster(request.getClusterName());",
                "raw_url": "https://github.com/apache/ambari/raw/96cf16f195c35aa6617e74e9c2893c53bea1a8b5/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java",
                "sha": "529e55449840a146f8d8e300c5803a694cd85668",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/ambari/blob/96cf16f195c35aa6617e74e9c2893c53bea1a8b5/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariServer.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariServer.java?ref=96cf16f195c35aa6617e74e9c2893c53bea1a8b5",
                "deletions": 8,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariServer.java",
                "patch": "@@ -32,7 +32,6 @@\n import org.mortbay.jetty.security.SslSocketConnector;\n import org.mortbay.jetty.servlet.Context;\n import org.mortbay.jetty.servlet.DefaultServlet;\n-import org.mortbay.jetty.servlet.FilterHolder;\n import org.mortbay.jetty.servlet.ServletHolder;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -71,7 +70,12 @@\n   CertificateManager certMan;\n   @Inject\n   Injector injector;\n-\n+  private static AmbariManagementController clusterController = null;\n+  \n+  public static AmbariManagementController getController() {\n+    return clusterController;\n+  }\n+  \n   public void run() {\n     server = new Server(CLIENT_API_PORT);\n     serverForAgent = new Server();\n@@ -113,7 +117,7 @@ public void run() {\n       //Spring Security Filter initialization\n       DelegatingFilterProxy springSecurityFilter = new DelegatingFilterProxy();\n       springSecurityFilter.setTargetBeanName(\"springSecurityFilterChain\");\n-      root.addFilter(new FilterHolder(springSecurityFilter), \"/*\", 1);\n+      //root.addFilter(new FilterHolder(springSecurityFilter), \"/*\", 1);\n \n       //Secured connector for 2-way auth\n       SslSocketConnector sslConnectorTwoWay = new SslSocketConnector();\n@@ -155,7 +159,8 @@ public void run() {\n       sh.setInitParameter(\"com.sun.jersey.config.property.resourceConfigClass\",\n               \"com.sun.jersey.api.core.PackagesResourceConfig\");\n       sh.setInitParameter(\"com.sun.jersey.config.property.packages\",\n-              \"org.apache.ambari.server.api.rest\");\n+              \"org.apache.ambari.server.api.rest;\" +\n+              \"org.apache.ambari.server.api.services\");\n       root.addServlet(sh, \"/api/*\");\n       sh.setInitOrder(2);\n \n@@ -197,7 +202,8 @@ public void run() {\n       AmbariManagementController controller = injector.getInstance(\n           AmbariManagementController.class);\n \n-\n+      clusterController = controller;\n+      \n       // FIXME need to figure out correct order of starting things to\n       // handle restart-recovery correctly\n \n@@ -211,9 +217,9 @@ public void run() {\n       manager.start();\n       LOG.info(\"********* Started ActionManager **********\");\n \n-      RequestInjectorForTest testInjector = new RequestInjectorForTest(controller, clusters);\n-      Thread testInjectorThread = new Thread(testInjector);\n-      testInjectorThread.start();\n+      //RequestInjectorForTest testInjector = new RequestInjectorForTest(controller, clusters);\n+      //Thread testInjectorThread = new Thread(testInjector);\n+      //testInjectorThread.start();\n       \n       server.join();\n       LOG.info(\"Joined the Server\");",
                "raw_url": "https://github.com/apache/ambari/raw/96cf16f195c35aa6617e74e9c2893c53bea1a8b5/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariServer.java",
                "sha": "3621919f652a912ac7074c640206aabd156016df",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/96cf16f195c35aa6617e74e9c2893c53bea1a8b5/ambari-server/src/main/java/org/apache/ambari/server/controller/ControllerModule.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/ControllerModule.java?ref=96cf16f195c35aa6617e74e9c2893c53bea1a8b5",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/ControllerModule.java",
                "patch": "@@ -28,6 +28,7 @@\n import org.apache.ambari.server.state.cluster.ClustersImpl;\n \n import com.google.inject.AbstractModule;\n+import com.google.inject.Injector;\n import com.google.inject.name.Names;\n \n /**",
                "raw_url": "https://github.com/apache/ambari/raw/96cf16f195c35aa6617e74e9c2893c53bea1a8b5/ambari-server/src/main/java/org/apache/ambari/server/controller/ControllerModule.java",
                "sha": "f2a7689d5980ef8b1c847a8e5fea9d2fee04254c",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/ambari/blob/96cf16f195c35aa6617e74e9c2893c53bea1a8b5/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/DefaultProviderModule.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/DefaultProviderModule.java?ref=96cf16f195c35aa6617e74e9c2893c53bea1a8b5",
                "deletions": 11,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/DefaultProviderModule.java",
                "patch": "@@ -18,31 +18,28 @@\n \n package org.apache.ambari.server.controller.internal;\n \n-import org.apache.ambari.server.controller.spi.ProviderModule;\n-import org.apache.ambari.server.controller.utilities.PropertyHelper;\n+import java.util.LinkedList;\n+import java.util.List;\n+\n import org.apache.ambari.server.controller.AmbariManagementController;\n-import org.apache.ambari.server.controller.AmbariManagementControllerImpl;\n+import org.apache.ambari.server.controller.AmbariServer;\n import org.apache.ambari.server.controller.spi.PropertyProvider;\n+import org.apache.ambari.server.controller.spi.ProviderModule;\n import org.apache.ambari.server.controller.spi.Resource;\n import org.apache.ambari.server.controller.spi.ResourceProvider;\n-\n-import java.util.LinkedList;\n-import java.util.List;\n-\n+import org.apache.ambari.server.controller.utilities.PropertyHelper;\n /**\n  *\n  */\n public class DefaultProviderModule implements ProviderModule {\n \n   private static final List<PropertyProvider> PROPERTY_PROVIDERS =\n       new LinkedList<PropertyProvider>();\n-\n+ \n   @Override\n   public ResourceProvider getResourceProvider(Resource.Type type) {\n-    AmbariManagementController managementController =\n-        new AmbariManagementControllerImpl(null, null);\n     return ResourceProviderImpl.getResourceProvider(type,\n         PROPERTY_PROVIDERS, PropertyHelper.getPropertyIds(type, \"DB\"),\n-        PropertyHelper.getKeyPropertyIds(type), managementController);\n+        PropertyHelper.getKeyPropertyIds(type), AmbariServer.getController());\n   }\n }",
                "raw_url": "https://github.com/apache/ambari/raw/96cf16f195c35aa6617e74e9c2893c53bea1a8b5/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/DefaultProviderModule.java",
                "sha": "3fd5672018043573bc49dd8839bab087b0f1ecb5",
                "status": "modified"
            }
        ],
        "message": "AMBARI-845. Fix NPE in the server to be able to run the server api's. (mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/ambari/branches/AMBARI-666@1396501 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/ambari/commit/947a9d5acc9e22aa1b9613bfdc5cdab74a3e78e0",
        "repo": "ambari",
        "unit_tests": [
            "AmbariManagementControllerImplTest.java",
            "AmbariServerTest.java"
        ]
    },
    "ambari_a62c4b8": {
        "bug_id": "ambari_a62c4b8",
        "commit": "https://github.com/apache/ambari/commit/a62c4b8aad56fce846603b9ad45ceead0300ae29",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/a62c4b8aad56fce846603b9ad45ceead0300ae29/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java?ref=a62c4b8aad56fce846603b9ad45ceead0300ae29",
                "deletions": 2,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java",
                "patch": "@@ -1432,8 +1432,8 @@ private synchronized RequestStatusResponse updateCluster(ClusterRequest request,\n             break;\n           } else {\n             for (Entry<String, String> property : requestConfigProperties.entrySet()) {\n-              if (!StringUtils.equals(property.getValue(),clusterConfigProperties.get(property.getKey()))) {\n-                isConfigurationCreationNeeded =true;\n+              if (!StringUtils.equals(property.getValue(), clusterConfigProperties.get(property.getKey()))) {\n+                isConfigurationCreationNeeded = true;\n                 break;\n               }\n             }",
                "raw_url": "https://github.com/apache/ambari/raw/a62c4b8aad56fce846603b9ad45ceead0300ae29/ambari-server/src/main/java/org/apache/ambari/server/controller/AmbariManagementControllerImpl.java",
                "sha": "c0dc34275a1ab98722a2ab8e0cb8f462d311fa7a",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/ambari/blob/a62c4b8aad56fce846603b9ad45ceead0300ae29/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackAdvisorResourceProvider.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackAdvisorResourceProvider.java?ref=a62c4b8aad56fce846603b9ad45ceead0300ae29",
                "deletions": 2,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackAdvisorResourceProvider.java",
                "patch": "@@ -266,8 +266,12 @@ protected StackAdvisorRequest prepareStackAdvisorRequest(Request request) {\n             siteMap.put(propertiesProperty, propertiesMap);\n           }\n \n-          String value = properties.get(property).toString();\n-          propertiesMap.put(propertyName, value);\n+          Object propVal = properties.get(property);\n+          if (propVal != null)\n+            propertiesMap.put(propertyName, propVal.toString());\n+          else\n+            LOG.info(String.format(\"No value specified for configuration property, name = %s \", property));\n+\n         } catch (Exception e) {\n           LOG.debug(String.format(\"Error handling configuration property, name = %s\", property), e);\n           // do nothing",
                "raw_url": "https://github.com/apache/ambari/raw/a62c4b8aad56fce846603b9ad45ceead0300ae29/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/StackAdvisorResourceProvider.java",
                "sha": "0ad91265b6f6a914a1b46e3147da0e9c957f4fd9",
                "status": "modified"
            },
            {
                "additions": 86,
                "blob_url": "https://github.com/apache/ambari/blob/a62c4b8aad56fce846603b9ad45ceead0300ae29/ambari-server/src/test/java/org/apache/ambari/server/controller/AmbariManagementControllerImplTest.java",
                "changes": 90,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/controller/AmbariManagementControllerImplTest.java?ref=a62c4b8aad56fce846603b9ad45ceead0300ae29",
                "deletions": 4,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/controller/AmbariManagementControllerImplTest.java",
                "patch": "@@ -18,6 +18,8 @@\n \n package org.apache.ambari.server.controller;\n \n+import com.google.common.collect.ImmutableList;\n+import com.google.common.collect.Maps;\n import com.google.gson.Gson;\n import com.google.inject.Binder;\n import com.google.inject.Guice;\n@@ -52,8 +54,10 @@\n import org.apache.ambari.server.state.Cluster;\n import org.apache.ambari.server.state.Clusters;\n import org.apache.ambari.server.state.ComponentInfo;\n+import org.apache.ambari.server.state.ConfigImpl;\n import org.apache.ambari.server.state.Host;\n import org.apache.ambari.server.state.MaintenanceState;\n+import org.apache.ambari.server.state.PropertyInfo;\n import org.apache.ambari.server.state.RepositoryInfo;\n import org.apache.ambari.server.state.SecurityType;\n import org.apache.ambari.server.state.Service;\n@@ -85,7 +89,20 @@\n import static org.apache.ambari.server.agent.ExecutionCommand.KeyNames.JAVA_VERSION;\n import static org.apache.ambari.server.agent.ExecutionCommand.KeyNames.STACK_NAME;\n import static org.apache.ambari.server.agent.ExecutionCommand.KeyNames.STACK_VERSION;\n-import static org.easymock.EasyMock.*;\n+import static org.easymock.EasyMock.anyBoolean;\n+import static org.easymock.EasyMock.anyObject;\n+import static org.easymock.EasyMock.capture;\n+import static org.easymock.EasyMock.captureBoolean;\n+import static org.easymock.EasyMock.createMock;\n+import static org.easymock.EasyMock.createMockBuilder;\n+import static org.easymock.EasyMock.createNiceMock;\n+import static org.easymock.EasyMock.createStrictMock;\n+import static org.easymock.EasyMock.eq;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.expectLastCall;\n+import static org.easymock.EasyMock.replay;\n+import static org.easymock.EasyMock.reset;\n+import static org.easymock.EasyMock.verify;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertNull;\n@@ -346,7 +363,7 @@ public void testGetClientHostForRunningAction_clientComponentThrowsException() t\n \n     expect(service.getName()).andReturn(\"service\");\n     expect(service.getServiceComponent(\"component\")).andThrow(\n-        new ServiceComponentNotFoundException(\"cluster\", \"service\", \"component\"));\n+      new ServiceComponentNotFoundException(\"cluster\", \"service\", \"component\"));\n     expect(service.getDesiredStackVersion()).andReturn(stackId);\n     expect(stackId.getStackName()).andReturn(\"stack\");\n     expect(stackId.getStackVersion()).andReturn(\"1.0\");\n@@ -356,7 +373,7 @@ public void testGetClientHostForRunningAction_clientComponentThrowsException() t\n     expect(service.getServiceComponents()).andReturn(componentsMap);\n     expect(component1.getServiceComponentHosts()).andReturn(Collections.EMPTY_MAP);\n     expect(component2.getServiceComponentHosts()).andReturn(\n-        Collections.<String, ServiceComponentHost>singletonMap(\"anyHost\", null));\n+      Collections.<String, ServiceComponentHost>singletonMap(\"anyHost\", null));\n \n     ServiceInfo serviceInfo = createNiceMock(ServiceInfo.class);\n     ComponentInfo compInfo = createNiceMock(ComponentInfo.class);\n@@ -394,7 +411,7 @@ public void testGetClientHostForRunningAction_noClientComponent() throws Excepti\n     expect(service.getServiceComponents()).andReturn(componentsMap);\n     expect(component1.getServiceComponentHosts()).andReturn(Collections.EMPTY_MAP);\n     expect(component2.getServiceComponentHosts()).andReturn(\n-        Collections.<String, ServiceComponentHost>singletonMap(\"anyHost\", null));\n+      Collections.<String, ServiceComponentHost>singletonMap(\"anyHost\", null));\n \n     ServiceInfo serviceInfo = createNiceMock(ServiceInfo.class);\n     expect(serviceInfo.getClientComponent()).andReturn(null);\n@@ -565,6 +582,71 @@ public void testUpdateClusters() throws Exception {\n     verify(actionManager, cluster, clusters, injector, clusterRequest, sessionManager);\n   }\n \n+  /**\n+   * Ensure that processing update request does not fail on configuration\n+   * properties with no value specified (no value = null reference value)\n+   */\n+  @Test\n+  public void testUpdateClustersWithNullConfigPropertyValues() throws Exception {\n+    // member state mocks\n+    Capture<AmbariManagementController> controllerCapture = new Capture<AmbariManagementController>();\n+    Injector injector = createStrictMock(Injector.class);\n+    Cluster cluster = createNiceMock(Cluster.class);\n+    ActionManager actionManager = createNiceMock(ActionManager.class);\n+    ClusterRequest clusterRequest = createNiceMock(ClusterRequest.class);\n+\n+    // requests\n+    Set<ClusterRequest> setRequests = Collections.singleton(clusterRequest);\n+\n+    KerberosHelper kerberosHelper = createStrictMock(KerberosHelper.class);\n+    // expectations\n+    injector.injectMembers(capture(controllerCapture));\n+    expect(injector.getInstance(Gson.class)).andReturn(null);\n+    expect(injector.getInstance(MaintenanceStateHelper.class)).andReturn(null);\n+    expect(injector.getInstance(KerberosHelper.class)).andReturn(kerberosHelper);\n+    expect(clusterRequest.getClusterName()).andReturn(\"clusterNew\").anyTimes();\n+    expect(clusterRequest.getClusterId()).andReturn(1L).anyTimes();\n+\n+    ConfigurationRequest configReq = new ConfigurationRequest();\n+    final Map<String, String> configReqProps = Maps.newHashMap();\n+    configReqProps.put(\"p1\", null);\n+    configReq.setProperties(configReqProps);\n+\n+    expect(clusterRequest.getDesiredConfig()).andReturn(ImmutableList.of(configReq)).anyTimes();\n+    expect(clusters.getClusterById(1L)).andReturn(cluster).anyTimes();\n+    expect(cluster.getClusterName()).andReturn(\"clusterOld\").anyTimes();\n+    expect(cluster.getConfigPropertiesTypes(anyObject(String.class))).andReturn(Maps.<PropertyInfo.PropertyType, Set<String>>newHashMap()).anyTimes();\n+    expect(cluster.getDesiredConfigByType(anyObject(String.class))).andReturn(new ConfigImpl(\"config-type\") {\n+      @Override\n+      public Map<String, Map<String, String>> getPropertiesAttributes() {\n+        return Maps.newHashMap();\n+      }\n+\n+      @Override\n+      public Map<String, String> getProperties() {\n+        return configReqProps;\n+      }\n+\n+    }).anyTimes();\n+\n+    cluster.addSessionAttributes(anyObject(Map.class));\n+    expectLastCall().once();\n+\n+    cluster.setClusterName(\"clusterNew\");\n+    expectLastCall();\n+\n+    // replay mocks\n+    replay(actionManager, cluster, clusters, injector, clusterRequest, sessionManager);\n+\n+    // test\n+    AmbariManagementController controller = new AmbariManagementControllerImpl(actionManager, clusters, injector);\n+    controller.updateClusters(setRequests, null);\n+\n+    // assert and verify\n+    assertSame(controller, controllerCapture.getValue());\n+    verify(actionManager, cluster, clusters, injector, clusterRequest, sessionManager);\n+  }\n+\n   /**\n    * Ensure that when the cluster is updated KerberosHandler.toggleKerberos is not invoked unless\n    * the security type is altered",
                "raw_url": "https://github.com/apache/ambari/raw/a62c4b8aad56fce846603b9ad45ceead0300ae29/ambari-server/src/test/java/org/apache/ambari/server/controller/AmbariManagementControllerImplTest.java",
                "sha": "e2ec5e0394624e1670c67e42730d973e47e5c111",
                "status": "modified"
            },
            {
                "additions": 40,
                "blob_url": "https://github.com/apache/ambari/blob/a62c4b8aad56fce846603b9ad45ceead0300ae29/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/StackAdvisorResourceProviderTest.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/StackAdvisorResourceProviderTest.java?ref=a62c4b8aad56fce846603b9ad45ceead0300ae29",
                "deletions": 0,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/controller/internal/StackAdvisorResourceProviderTest.java",
                "patch": "@@ -33,6 +33,7 @@\n \n import static org.apache.ambari.server.controller.internal.StackAdvisorResourceProvider.CONFIGURATIONS_PROPERTY_ID;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNotNull;\n import static org.mockito.Mockito.doReturn;\n import static org.mockito.Mockito.mock;\n@@ -73,4 +74,43 @@ public void testCalculateConfigurations() throws Exception {\n     assertEquals(\"string\", properties.get(\"string_prop\"));\n     assertEquals(\"[array1, array2]\", properties.get(\"array_prop\"));\n   }\n+\n+  @Test\n+  public void testCalculateConfigurationsWithNullPropertyValues() throws Exception {\n+\n+    Map<Resource.Type, String> keyPropertyIds = Collections.emptyMap();\n+    Set<String> propertyIds = Collections.emptySet();\n+    AmbariManagementController ambariManagementController = mock(AmbariManagementController.class);\n+    RecommendationResourceProvider provider = new RecommendationResourceProvider(propertyIds,\n+      keyPropertyIds, ambariManagementController);\n+\n+    Request request = mock(Request.class);\n+    Set<Map<String, Object>> propertiesSet = new HashSet<Map<String, Object>>();\n+    Map<String, Object> propertiesMap = new HashMap<String, Object>();\n+    propertiesMap.put(CONFIGURATIONS_PROPERTY_ID + \"site/properties/string_prop\", null); //null value means no value specified for the property\n+    List<Object> array = new ArrayList<Object>();\n+    array.add(\"array1\");\n+    array.add(\"array2\");\n+    propertiesMap.put(CONFIGURATIONS_PROPERTY_ID + \"site/properties/array_prop\", array);\n+    propertiesSet.add(propertiesMap);\n+\n+    doReturn(propertiesSet).when(request).getProperties();\n+\n+    Map<String, Map<String, Map<String, String>>> calculatedConfigurations = provider.calculateConfigurations(request);\n+\n+    assertNotNull(calculatedConfigurations);\n+    assertEquals(1, calculatedConfigurations.size());\n+    Map<String, Map<String, String>> site = calculatedConfigurations.get(\"site\");\n+    assertNotNull(site);\n+    assertEquals(1, site.size());\n+    Map<String, String> properties = site.get(\"properties\");\n+    assertNotNull(properties);\n+\n+    assertEquals(\"[array1, array2]\", properties.get(\"array_prop\"));\n+\n+\n+    // config properties with null values should be ignored\n+    assertFalse(properties.containsKey(\"string_prop\"));\n+\n+  }\n }",
                "raw_url": "https://github.com/apache/ambari/raw/a62c4b8aad56fce846603b9ad45ceead0300ae29/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/StackAdvisorResourceProviderTest.java",
                "sha": "e3b89b818d68f80ccfa32dbc9d5840b16fb1a387",
                "status": "modified"
            }
        ],
        "message": "AMBARI-14040. Update cluster fails with NPE if the request contains null configuration property values (Sebastian Toader via alejandro)",
        "parent": "https://github.com/apache/ambari/commit/0ada28abc97e1b54833b98340a43e09c948a7e25",
        "repo": "ambari",
        "unit_tests": [
            "AmbariManagementControllerImplTest.java",
            "StackAdvisorResourceProviderTest.java"
        ]
    },
    "ambari_ae95ee6": {
        "bug_id": "ambari_ae95ee6",
        "commit": "https://github.com/apache/ambari/commit/ae95ee68f88f96eb8395b6fdd3e3c91abcbc2ac1",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/ambari/blob/ae95ee68f88f96eb8395b6fdd3e3c91abcbc2ac1/ambari-server/src/main/java/org/apache/ambari/server/view/ViewContextImpl.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/view/ViewContextImpl.java?ref=ae95ee68f88f96eb8395b6fdd3e3c91abcbc2ac1",
                "deletions": 3,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/view/ViewContextImpl.java",
                "patch": "@@ -405,9 +405,12 @@ private Masker getMasker(ClassLoader cl, ViewConfig viewConfig) {\n    * @throws ParseErrorException if original string cannot be parsed by Velocity\n    */\n   private String parameterize(String raw) throws ParseErrorException {\n-    Writer templateWriter = new StringWriter();\n-    Velocity.evaluate(velocityContext, templateWriter, raw, raw);\n-    return templateWriter.toString();\n+    if (raw != null) {\n+      Writer templateWriter = new StringWriter();\n+      Velocity.evaluate(velocityContext, templateWriter, raw, raw);\n+      return templateWriter.toString();\n+    }\n+    return null;\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/ambari/raw/ae95ee68f88f96eb8395b6fdd3e3c91abcbc2ac1/ambari-server/src/main/java/org/apache/ambari/server/view/ViewContextImpl.java",
                "sha": "3c736cc94e7cffed3c6018e0c08fefed65f43ac6",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/ambari/blob/ae95ee68f88f96eb8395b6fdd3e3c91abcbc2ac1/ambari-server/src/test/java/org/apache/ambari/server/view/ViewContextImplTest.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/view/ViewContextImplTest.java?ref=ae95ee68f88f96eb8395b6fdd3e3c91abcbc2ac1",
                "deletions": 2,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/view/ViewContextImplTest.java",
                "patch": "@@ -121,28 +121,31 @@ public void testGetPropertiesWithParameters() throws Exception {\n     viewInstanceDefinition.putProperty(\"p5\", \"/path/to/${incorrect_parameter\");\n     viewInstanceDefinition.putProperty(\"p6\", \"/path/to/\\\\${username}\");\n     viewInstanceDefinition.putProperty(\"p7\", \"/path/to/\\\\$viewName\");\n+    viewInstanceDefinition.putProperty(\"p8\", null);\n \n     ViewContextImpl viewContext = new ViewContextImpl(viewInstanceDefinition, viewRegistry);\n \n     Map<String, String> properties = viewContext.getProperties();\n-    Assert.assertEquals(7, properties.size());\n+    Assert.assertEquals(8, properties.size());\n     Assert.assertEquals(\"/tmp/some/path/User\", properties.get(\"p1\"));\n     Assert.assertEquals(\"/tmp/path/View\", properties.get(\"p2\"));\n     Assert.assertEquals(\"/path/Instance\", properties.get(\"p3\"));\n     Assert.assertEquals(\"/path/to/${unspecified_parameter}\", properties.get(\"p4\"));\n     Assert.assertEquals(\"/path/to/${incorrect_parameter\", properties.get(\"p5\"));\n     Assert.assertEquals(\"/path/to/${username}\", properties.get(\"p6\"));\n     Assert.assertEquals(\"/path/to/$viewName\", properties.get(\"p7\"));\n+    Assert.assertNull(properties.get(\"p8\"));\n \n     properties = viewContext.getProperties();\n-    Assert.assertEquals(7, properties.size());\n+    Assert.assertEquals(8, properties.size());\n     Assert.assertEquals(\"/tmp/some/path/User2\", properties.get(\"p1\"));\n     Assert.assertEquals(\"/tmp/path/View\", properties.get(\"p2\"));\n     Assert.assertEquals(\"/path/Instance\", properties.get(\"p3\"));\n     Assert.assertEquals(\"/path/to/${unspecified_parameter}\", properties.get(\"p4\"));\n     Assert.assertEquals(\"/path/to/${incorrect_parameter\", properties.get(\"p5\"));\n     Assert.assertEquals(\"/path/to/${username}\", properties.get(\"p6\"));\n     Assert.assertEquals(\"/path/to/$viewName\", properties.get(\"p7\"));\n+    Assert.assertNull(properties.get(\"p8\"));\n   }\n \n   @Test",
                "raw_url": "https://github.com/apache/ambari/raw/ae95ee68f88f96eb8395b6fdd3e3c91abcbc2ac1/ambari-server/src/test/java/org/apache/ambari/server/view/ViewContextImplTest.java",
                "sha": "88110f7c8bfb78845a019088759d40e092229aa3",
                "status": "modified"
            }
        ],
        "message": "AMBARI-9307 - Views: NPE on getProperties (tbeerbower)",
        "parent": "https://github.com/apache/ambari/commit/acf9018a745b68a6e5e939ee71b86d60b986b7f7",
        "repo": "ambari",
        "unit_tests": [
            "ViewContextImplTest.java"
        ]
    },
    "ambari_aebe216": {
        "bug_id": "ambari_aebe216",
        "commit": "https://github.com/apache/ambari/commit/aebe216d0dd54a1104337bbdd93495bef9a9a461",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/ambari/blob/aebe216d0dd54a1104337bbdd93495bef9a9a461/ambari-server/src/main/java/org/apache/ambari/server/topology/LogicalRequest.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/topology/LogicalRequest.java?ref=aebe216d0dd54a1104337bbdd93495bef9a9a461",
                "deletions": 2,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/topology/LogicalRequest.java",
                "patch": "@@ -25,6 +25,7 @@\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.Set;\n import java.util.TreeSet;\n import java.util.concurrent.atomic.AtomicLong;\n@@ -343,7 +344,7 @@ public void removeHostRequestByHostName(String hostName) {\n \n         Iterator<HostRequest> hostRequestIterator = outstandingHostRequests.iterator();\n         while (hostRequestIterator.hasNext()) {\n-          if (hostRequestIterator.next().getHostName().equals(hostName)) {\n+          if (Objects.equals(hostRequestIterator.next().getHostName(), hostName)) {\n             hostRequestIterator.remove();\n             break;\n           }\n@@ -352,7 +353,7 @@ public void removeHostRequestByHostName(String hostName) {\n         //todo: synchronization\n         Iterator<HostRequest> allHostRequesIterator = allHostRequests.iterator();\n         while (allHostRequesIterator.hasNext()) {\n-          if (allHostRequesIterator.next().getHostName().equals(hostName)) {\n+          if (Objects.equals(allHostRequesIterator.next().getHostName(), hostName)) {\n             allHostRequesIterator.remove();\n             break;\n           }",
                "raw_url": "https://github.com/apache/ambari/raw/aebe216d0dd54a1104337bbdd93495bef9a9a461/ambari-server/src/main/java/org/apache/ambari/server/topology/LogicalRequest.java",
                "sha": "de4211f8180abb6c15e7b24382e89bf7f9ecd337",
                "status": "modified"
            },
            {
                "additions": 122,
                "blob_url": "https://github.com/apache/ambari/blob/aebe216d0dd54a1104337bbdd93495bef9a9a461/ambari-server/src/test/java/org/apache/ambari/server/topology/LogicalRequestTest.java",
                "changes": 122,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/topology/LogicalRequestTest.java?ref=aebe216d0dd54a1104337bbdd93495bef9a9a461",
                "deletions": 0,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/topology/LogicalRequestTest.java",
                "patch": "@@ -339,4 +339,126 @@ public boolean apply(@Nullable HostRequest input) {\n     assertTrue(completedHostReq1.isPresent() && completedHostReq2.isPresent());\n \n   }\n+\n+  @Test\n+  public void testRemoveHostRequestByHostName() throws Exception {\n+    // Given\n+    Long requestId = 1L;\n+\n+    final TopologyHostInfoEntity host1 = new TopologyHostInfoEntity();\n+    host1.setId(100L);\n+    host1.setFqdn(\"host1\");\n+\n+    final TopologyHostInfoEntity host2 = new TopologyHostInfoEntity();\n+    host2.setId(101L);\n+    host2.setFqdn(\"host2\");\n+\n+    final TopologyHostInfoEntity host3 = new TopologyHostInfoEntity();\n+    host3.setId(103L);\n+    host3.setFqdn(\"host3\");\n+\n+    final TopologyHostInfoEntity host4 = new TopologyHostInfoEntity();\n+    host4.setId(104L);\n+    host4.setFqdn(\"host4\");\n+\n+    TopologyHostGroupEntity hostGroupEntity1 = new TopologyHostGroupEntity();\n+    hostGroupEntity1.setTopologyHostInfoEntities(ImmutableSet.of(host1, host2, host3));\n+    hostGroupEntity1.setName(\"host_group_1\");\n+\n+    // host request matched to a registered host\n+    TopologyHostRequestEntity hostRequestEntityHost1Matched = new TopologyHostRequestEntity();\n+    hostRequestEntityHost1Matched.setId(1L);\n+    hostRequestEntityHost1Matched.setHostName(host1.getFqdn()); //host request matched host1\n+    hostRequestEntityHost1Matched.setTopologyHostGroupEntity(hostGroupEntity1);\n+    hostRequestEntityHost1Matched.setTopologyHostTaskEntities(Collections.<TopologyHostTaskEntity>emptySet());\n+    expect(ambariContext.isHostRegisteredWithCluster(eq(clusterId), eq(host1.getFqdn()))).andReturn(true).anyTimes();\n+\n+\n+    TopologyHostRequestEntity hostRequestEntityHost2Matched = new TopologyHostRequestEntity();\n+    hostRequestEntityHost2Matched.setId(2L);\n+    hostRequestEntityHost2Matched.setHostName(host2.getFqdn()); // host request matched host2\n+    hostRequestEntityHost2Matched.setTopologyHostGroupEntity(hostGroupEntity1);\n+    hostRequestEntityHost2Matched.setTopologyHostTaskEntities(Collections.<TopologyHostTaskEntity>emptySet());\n+    expect(ambariContext.isHostRegisteredWithCluster(eq(clusterId), eq(host2.getFqdn()))).andReturn(true).anyTimes();\n+\n+    // host request that hasn't been matched to any registered host yet\n+    TopologyHostRequestEntity hostRequestEntityHost3NotMatched = new TopologyHostRequestEntity();\n+    hostRequestEntityHost3NotMatched.setId(3L);\n+    hostRequestEntityHost3NotMatched.setTopologyHostGroupEntity(hostGroupEntity1);\n+    hostRequestEntityHost3NotMatched.setTopologyHostTaskEntities(Collections.<TopologyHostTaskEntity>emptySet());\n+    expect(ambariContext.isHostRegisteredWithCluster(eq(clusterId), eq(host3.getFqdn()))).andReturn(false).anyTimes();\n+\n+\n+    TopologyHostRequestEntity hostRequestEntityHost4Matched = new TopologyHostRequestEntity();\n+    hostRequestEntityHost4Matched.setId(4L);\n+    hostRequestEntityHost4Matched.setHostName(host4.getFqdn()); // host request matched host4\n+    hostRequestEntityHost4Matched.setTopologyHostGroupEntity(hostGroupEntity1);\n+    hostRequestEntityHost4Matched.setTopologyHostTaskEntities(Collections.<TopologyHostTaskEntity>emptySet());\n+    expect(ambariContext.isHostRegisteredWithCluster(eq(clusterId), eq(host4.getFqdn()))).andReturn(true).anyTimes();\n+\n+    Collection<TopologyHostRequestEntity> reservedHostRequestEntities = ImmutableSet.of(\n+      hostRequestEntityHost1Matched,\n+      hostRequestEntityHost2Matched,\n+      hostRequestEntityHost3NotMatched,\n+      hostRequestEntityHost4Matched);\n+\n+    hostGroupEntity1.setTopologyHostRequestEntities(reservedHostRequestEntities);\n+\n+    TopologyRequestEntity topologyRequestEntity = new TopologyRequestEntity();\n+    topologyRequestEntity.setTopologyHostGroupEntities(Collections.singleton(hostGroupEntity1));\n+\n+\n+    expect(logicalRequestEntity.getTopologyRequestEntity()).andReturn(topologyRequestEntity).atLeastOnce();\n+    expect(logicalRequestEntity.getTopologyHostRequestEntities()).andReturn(reservedHostRequestEntities).atLeastOnce();\n+    expect(blueprint.getHostGroup(eq(\"host_group_1\"))).andReturn(hostGroup1).atLeastOnce();\n+    expect(hostGroup1.containsMasterComponent()).andReturn(false).atLeastOnce();\n+\n+\n+    replayAll();\n+\n+    // When\n+\n+    LogicalRequest req = new LogicalRequest(requestId, replayedTopologyRequest, clusterTopology, logicalRequestEntity);\n+    req.removeHostRequestByHostName(host4.getFqdn());\n+\n+\n+    // Then\n+    verifyAll();\n+\n+    Collection<HostRequest>  hostRequests = req.getHostRequests();\n+    assertEquals(3, hostRequests.size());\n+\n+    Optional<HostRequest> hostReqHost1 = Iterables.tryFind(hostRequests, new Predicate<HostRequest>() {\n+      @Override\n+      public boolean apply(@Nullable HostRequest input) {\n+        return host1.getFqdn().equals(input.getHostName());\n+      }\n+    });\n+\n+    Optional<HostRequest> hostReqHost2 = Iterables.tryFind(hostRequests, new Predicate<HostRequest>() {\n+      @Override\n+      public boolean apply(@Nullable HostRequest input) {\n+        return host2.getFqdn().equals(input.getHostName());\n+      }\n+    });\n+\n+\n+    Optional<HostRequest> hostReqHost3 = Iterables.tryFind(hostRequests, new Predicate<HostRequest>() {\n+      @Override\n+      public boolean apply(@Nullable HostRequest input) {\n+        return input.getHostName() == null;\n+      }\n+    });\n+\n+\n+    Optional<HostRequest> hostReqHost4 = Iterables.tryFind(hostRequests, new Predicate<HostRequest>() {\n+      @Override\n+      public boolean apply(@Nullable HostRequest input) {\n+        return host4.getFqdn().equals(input.getHostName());\n+      }\n+    });\n+\n+\n+    assertTrue(hostReqHost1.isPresent() && hostReqHost2.isPresent() && hostReqHost3.isPresent() && !hostReqHost4.isPresent());\n+  }\n }",
                "raw_url": "https://github.com/apache/ambari/raw/aebe216d0dd54a1104337bbdd93495bef9a9a461/ambari-server/src/test/java/org/apache/ambari/server/topology/LogicalRequestTest.java",
                "sha": "42ef020735d522dd4b8cca397f9b19f4ae338bd0",
                "status": "modified"
            }
        ],
        "message": "AMBARI-20270. NPE thrown if downscale is performed before all hosts registered with cluster. (stoader)",
        "parent": "https://github.com/apache/ambari/commit/56af5741fa0fb7398438fe5bac2cb207c0f93df5",
        "repo": "ambari",
        "unit_tests": [
            "LogicalRequestTest.java"
        ]
    },
    "ambari_af3cb35": {
        "bug_id": "ambari_af3cb35",
        "commit": "https://github.com/apache/ambari/commit/af3cb35229d53803231e395df0df989a0cb9ebb9",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/af3cb35229d53803231e395df0df989a0cb9ebb9/client/src/main/java/org/apache/ambari/event/AsyncDispatcher.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/client/src/main/java/org/apache/ambari/event/AsyncDispatcher.java?ref=af3cb35229d53803231e395df0df989a0cb9ebb9",
                "deletions": 0,
                "filename": "client/src/main/java/org/apache/ambari/event/AsyncDispatcher.java",
                "patch": "@@ -76,6 +76,7 @@ public void run() {\n     };\n   }\n \n+  @Override\n   public void start() {\n     eventHandlingThread = new Thread(createThread());\n     eventHandlingThread.start();",
                "raw_url": "https://github.com/apache/ambari/raw/af3cb35229d53803231e395df0df989a0cb9ebb9/client/src/main/java/org/apache/ambari/event/AsyncDispatcher.java",
                "sha": "eb19876996d1c3c78d7222c521d382052648f969",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/af3cb35229d53803231e395df0df989a0cb9ebb9/client/src/main/java/org/apache/ambari/event/Dispatcher.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/client/src/main/java/org/apache/ambari/event/Dispatcher.java?ref=af3cb35229d53803231e395df0df989a0cb9ebb9",
                "deletions": 0,
                "filename": "client/src/main/java/org/apache/ambari/event/Dispatcher.java",
                "patch": "@@ -28,5 +28,7 @@\n   EventHandler getEventHandler();\n \n   void register(Class<? extends Enum> eventType, EventHandler handler);\n+  \n+  void start();\n \n }",
                "raw_url": "https://github.com/apache/ambari/raw/af3cb35229d53803231e395df0df989a0cb9ebb9/client/src/main/java/org/apache/ambari/event/Dispatcher.java",
                "sha": "5bb4cf07b1ef035dd5506096b1068269f00c2f47",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/java/org/apache/ambari/components/impl/XmlComponentDefinition.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/controller/src/main/java/org/apache/ambari/components/impl/XmlComponentDefinition.java?ref=af3cb35229d53803231e395df0df989a0cb9ebb9",
                "deletions": 2,
                "filename": "controller/src/main/java/org/apache/ambari/components/impl/XmlComponentDefinition.java",
                "patch": "@@ -89,7 +89,7 @@\n   @XmlAccessorType(XmlAccessType.FIELD)\n   @XmlType(name = \"prestart\")\n   public static class Prestart extends ScriptCommand {\n-    @XmlAttribute String runPrestartOn;\n+    @XmlAttribute String runOn;\n   }\n   \n   @XmlAccessorType(XmlAccessType.FIELD)\n@@ -284,7 +284,7 @@ private static String getUser(ScriptCommand cmd, String user) {\n         checkRole = null;\n       }\n       if (component.prestart != null) {\n-        prestartRole = component.prestart.runPrestartOn;\n+        prestartRole = component.prestart.runOn;\n       } else {\n         prestartRole = null;\n       }",
                "raw_url": "https://github.com/apache/ambari/raw/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/java/org/apache/ambari/components/impl/XmlComponentDefinition.java",
                "sha": "e3592a8c78cc3fbad2f928348e299e2748294d17",
                "status": "modified"
            },
            {
                "additions": 118,
                "blob_url": "https://github.com/apache/ambari/blob/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/java/org/apache/ambari/controller/HeartbeatHandler.java",
                "changes": 199,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/controller/src/main/java/org/apache/ambari/controller/HeartbeatHandler.java?ref=af3cb35229d53803231e395df0df989a0cb9ebb9",
                "deletions": 81,
                "filename": "controller/src/main/java/org/apache/ambari/controller/HeartbeatHandler.java",
                "patch": "@@ -20,6 +20,7 @@\n import java.io.IOException;\n import java.util.ArrayList;\n import java.util.Collections;\n+import java.util.Comparator;\n import java.util.Date;\n import java.util.HashMap;\n import java.util.List;\n@@ -77,43 +78,49 @@ public ControllerResponse processHeartBeat(HeartBeat heartbeat)\n     Nodes.getInstance().checkAndUpdateNode(hostname, heartbeatTime);\n \n     List<Action> allActions = new ArrayList<Action>();\n-\n-    clusterName = Nodes.getInstance().getNode(hostname)\n-        .getNodeState().getClusterName();\n-    if (clusterName != null) {\n-      clusterRev = Clusters.getInstance().\n-          getClusterByName(clusterName).getLatestRevision(); \n-    }\n-    \n-    ComponentAndRoleStates componentStates = \n-        new ComponentAndRoleStates();\n-    //create some datastructures by looking at agent state\n-    inspectAgentState(heartbeat, componentStates);\n     \n-    //get the clusters the node belongs to\n-    Set<ClusterIdAndRev> clustersNodeBelongsTo = \n-        componentStates.getClustersNodeBelongsTo();\n     \n     //if the command-execution takes longer than one heartbeat interval\n     //the check for idleness will prevent the same node getting the same \n     //command more than once. In the future this could be improved\n     //to reflect the command execution state more accurately.\n-    if (heartbeat.getIdle()) {  \n-      for (ClusterIdAndRev clusterIdAndRev : clustersNodeBelongsTo) {\n+    if (heartbeat.getIdle()) {\n+      clusterName = Nodes.getInstance().getNode(hostname)\n+          .getNodeState().getClusterName();\n+      if (clusterName != null) {\n+        clusterRev = Clusters.getInstance().\n+            getClusterByName(clusterName).getLatestRevision(); \n+      }\n+      \n+      ComponentAndRoleStates componentStates = \n+          new ComponentAndRoleStates();\n+      //create some datastructures by looking at agent state\n+      inspectAgentState(heartbeat, componentStates);\n+      \n+      //get the clusters the node belongs to\n+      Set<ClusterNameAndRev> clustersNodeBelongsTo = \n+          componentStates.getClustersNodeBelongsTo();\n+      boolean newNode = false;\n+      //add the clusters the node *should* belong to\n+      if (clusterName != null) {\n+        newNode = checkAndAddClusterIds(clustersNodeBelongsTo, clusterName, \n+            clusterRev);\n+      }\n+      for (ClusterNameAndRev clusterIdAndRev : clustersNodeBelongsTo) {\n         //check whether this node is out-of-sync w.r.t what's running &\n         //installed, or is it compatible\n-        if (!isCompatible(clusterIdAndRev.getClusterId(), \n+        if (!isCompatible(clusterIdAndRev.getClusterName(), \n             clusterIdAndRev.getRevision(), clusterName, clusterRev)) {\n           createStopAndUninstallActions(componentStates, allActions, \n               clusterIdAndRev, true);\n           continue;\n         }\n         //get the cluster object corresponding to the clusterId\n         Cluster cluster = Clusters.getInstance()\n-            .getClusterByName(clusterIdAndRev.getClusterId());\n+            .getClusterByName(clusterIdAndRev.getClusterName());\n         //get the state machine reference to the cluster\n         ClusterFSM clusterFsm = StateMachineInvoker\n-            .getStateMachineClusterInstance(clusterIdAndRev.getClusterId());\n+            .getStateMachineClusterInstance(clusterIdAndRev.getClusterName());\n \n         //the state machine references to the services\n         List<ServiceFSM> clusterServices = clusterFsm.getServices();\n@@ -127,20 +134,25 @@ public ControllerResponse processHeartBeat(HeartBeat heartbeat)\n             boolean nodePlayingRole = \n                 nodePlayingRole(hostname, role.getRoleName());\n             if (nodePlayingRole) {\n-              boolean roleInstalled = componentStates.isInstalled(\n-                  clusterIdAndRev,\n-                  role.getAssociatedService().getServiceName(), \n-                  role.getRoleName());     \n-              boolean roleServerRunning = componentStates.isStarted(\n-                  clusterIdAndRev,\n-                  role.getAssociatedService().getServiceName(),\n-                  role.getRoleName()) \n-                  || componentStates.isStartInProgress(clusterIdAndRev,\n-                      role.getAssociatedService().getServiceName(), \n-                      role.getRoleName());\n-              boolean agentRoleStateChanged = componentStates.hasStateChanged(\n-                  clusterIdAndRev, role.getAssociatedService().getServiceName(), \n-                  role.getRoleName());\n+              boolean roleInstalled = false;\n+              boolean roleServerRunning = false;\n+              boolean agentRoleStateChanged = false;\n+              if (!newNode) {\n+                componentStates.isInstalled(\n+                    clusterIdAndRev,\n+                    role.getAssociatedService().getServiceName(), \n+                    role.getRoleName());     \n+                roleServerRunning = componentStates.isStarted(\n+                    clusterIdAndRev,\n+                    role.getAssociatedService().getServiceName(),\n+                    role.getRoleName()) \n+                    || componentStates.isStartInProgress(clusterIdAndRev,\n+                        role.getAssociatedService().getServiceName(), \n+                        role.getRoleName());\n+                agentRoleStateChanged = componentStates.hasStateChanged(\n+                    clusterIdAndRev, role.getAssociatedService().getServiceName(), \n+                    role.getRoleName());\n+              }\n               ComponentPlugin plugin = \n                   cluster.getComponentDefinition(service.getServiceName());\n               \n@@ -194,8 +206,8 @@ public ControllerResponse processHeartBeat(HeartBeat heartbeat)\n             }\n             //check/create the special component/service-level \n             //actions (like safemode check)\n-            checkAndCreateActions(cluster, clusterFsm,clusterIdAndRev, service, heartbeat, \n-                allActions, componentStates);\n+            checkAndCreateActions(cluster, clusterFsm,clusterIdAndRev, newNode,\n+                service, heartbeat, allActions, componentStates);\n           }\n         }\n         createStopAndUninstallActions(componentStates, allActions, clusterIdAndRev, false);\n@@ -208,11 +220,22 @@ public ControllerResponse processHeartBeat(HeartBeat heartbeat)\n     return r;\n   }\n   \n-  private void createInstallAction(ClusterIdAndRev clusterIdAndRev, \n+  private boolean checkAndAddClusterIds(Set<ClusterNameAndRev> clustersNodeBelongsTo, \n+      String clusterName, long clusterRev) {\n+    ClusterNameAndRev clusterNameAndRev = new ClusterNameAndRev(clusterName,\n+        clusterRev);\n+    if (!clustersNodeBelongsTo.contains(clusterNameAndRev)) {\n+      clustersNodeBelongsTo.add(clusterNameAndRev);\n+      return true;\n+    }\n+    return false;\n+  }\n+  \n+  private void createInstallAction(ClusterNameAndRev clusterIdAndRev, \n       String cluster, String component, String role, ComponentPlugin plugin, \n       List<Action> allActions) throws IOException {\n-    String clusterId = clusterIdAndRev.getClusterId();\n-    long clusterRev = clusterIdAndRev.getRevision();\n+    String clusterId = clusterIdAndRev.getClusterName();\n+    long clusterRev = clusterIdAndRev.getRevision();   \n     //action for creating dir structure\n     Action action = new Action();\n     action.setKind(Kind.CREATE_STRUCTURE_ACTION);\n@@ -232,7 +255,7 @@ private void createInstallAction(ClusterIdAndRev clusterIdAndRev,\n   }\n   \n   private void createStopAndUninstallActions(ComponentAndRoleStates componentAndRoleStates, \n-      List<Action> allActions, ClusterIdAndRev clusterIdAndRev, boolean forceUninstall) {\n+      List<Action> allActions, ClusterNameAndRev clusterIdAndRev, boolean forceUninstall) {\n     Map<String, \n         Map<String,RoleStateTracker>>\n     entrySet = componentAndRoleStates.getAllRoles(clusterIdAndRev);\n@@ -244,22 +267,22 @@ private void createStopAndUninstallActions(ComponentAndRoleStates componentAndRo\n       for (Map.Entry<String,RoleStateTracker> entryVal : roleSet) {\n         String roleName = entryVal.getKey();\n         if (forceUninstall) {\n-          addAction(getStopRoleAction(clusterIdAndRev.getClusterId(), \n+          addAction(getStopRoleAction(clusterIdAndRev.getClusterName(), \n               clusterIdAndRev.getRevision(), \n               componentName, roleName), allActions);\n-          addAction(getUninstallRoleAction(clusterIdAndRev.getClusterId(), \n+          addAction(getUninstallRoleAction(clusterIdAndRev.getClusterName(), \n               clusterIdAndRev.getRevision(), \n               componentName, roleName), allActions);\n         } else {\n           RoleStateTracker stateTracker = entryVal.getValue();\n           if (stateTracker.continueRunning) continue;\n \n-          addAction(getStopRoleAction(clusterIdAndRev.getClusterId(), \n+          addAction(getStopRoleAction(clusterIdAndRev.getClusterName(), \n               clusterIdAndRev.getRevision(), \n               componentName, roleName), allActions);\n \n           if (stateTracker.uninstall)\n-            addAction(getUninstallRoleAction(clusterIdAndRev.getClusterId(), \n+            addAction(getUninstallRoleAction(clusterIdAndRev.getClusterName(), \n                 clusterIdAndRev.getRevision(), \n                 componentName, roleName), allActions);\n         }\n@@ -268,8 +291,8 @@ private void createStopAndUninstallActions(ComponentAndRoleStates componentAndRo\n   }\n   private static class ComponentAndRoleStates {\n     //Convenience class to aid in heartbeat processing\n-    private Map<ClusterIdAndRev, Map<String, Map<String, RoleStateTracker>>>\n-    componentRoleMap = new HashMap<ClusterIdAndRev, \n+    private Map<ClusterNameAndRev, Map<String, Map<String, RoleStateTracker>>>\n+    componentRoleMap = new HashMap<ClusterNameAndRev, \n                            Map<String, Map<String, RoleStateTracker>>>();\n     \n     private Map<String, ActionResult> actionIds = \n@@ -278,17 +301,17 @@ private void createStopAndUninstallActions(ComponentAndRoleStates componentAndRo\n     private static Map<String, List<AgentRoleState>> previousStateMap =\n         new ConcurrentHashMap<String, List<AgentRoleState>>();\n     \n-    private Set<ClusterIdAndRev> clusterNodeBelongsTo = \n-        new TreeSet<ClusterIdAndRev>();\n+    private Set<ClusterNameAndRev> clusterNodeBelongsTo = \n+        new TreeSet<ClusterNameAndRev>();\n     \n     Map<String,Map<String,RoleStateTracker>> getAllRoles(\n-        ClusterIdAndRev clusterIdAndRev) {\n+        ClusterNameAndRev clusterIdAndRev) {\n       return componentRoleMap.get(clusterIdAndRev);\n     }\n     \n     void recordRoleState(String host, AgentRoleState state) {\n-      ClusterIdAndRev clusterIdAndRev = \n-          new ClusterIdAndRev(state.getClusterId(),\n+      ClusterNameAndRev clusterIdAndRev = \n+          new ClusterNameAndRev(state.getClusterId(),\n               state.getClusterDefinitionRevision());\n       clusterNodeBelongsTo.add(clusterIdAndRev);\n       \n@@ -319,7 +342,7 @@ void recordRoleState(String host, AgentRoleState state) {\n       }\n     }\n     \n-    boolean isRoleInstalled(ClusterIdAndRev clusterIdAndRev, String role) {\n+    boolean isRoleInstalled(ClusterNameAndRev clusterIdAndRev, String role) {\n       //problematic in the case where role is not unique (like 'client')\n       //TODO: no iteration please\n       Set<Map.Entry<String, Map<String, RoleStateTracker>>> entrySet = \n@@ -332,7 +355,7 @@ boolean isRoleInstalled(ClusterIdAndRev clusterIdAndRev, String role) {\n       return false;\n     }\n     \n-    boolean isStarted(ClusterIdAndRev clusterIdAndRev, String component, \n+    boolean isStarted(ClusterNameAndRev clusterIdAndRev, String component, \n         String role) {\n       Map<String,Map<String,RoleStateTracker>> componentsMap = \n           componentRoleMap.get(clusterIdAndRev);\n@@ -349,7 +372,7 @@ boolean isStarted(ClusterIdAndRev clusterIdAndRev, String component,\n       return false;\n     }\n     \n-    boolean isStartInProgress(ClusterIdAndRev clusterIdAndRev, \n+    boolean isStartInProgress(ClusterNameAndRev clusterIdAndRev, \n         String component, String role) {\n       Map<String,Map<String,RoleStateTracker>> componentsMap = \n           componentRoleMap.get(clusterIdAndRev);\n@@ -367,7 +390,7 @@ boolean isStartInProgress(ClusterIdAndRev clusterIdAndRev,\n       return false;\n     }\n     \n-    boolean isInstalled(ClusterIdAndRev clusterIdAndRev, \n+    boolean isInstalled(ClusterNameAndRev clusterIdAndRev, \n         String component, String role) {\n       Map<String,Map<String,RoleStateTracker>> componentsMap = \n           componentRoleMap.get(clusterIdAndRev);\n@@ -388,7 +411,7 @@ void recordActionId(String actionId, ActionResult actionResult) {\n     ActionResult getActionResult(String id) {\n       return actionIds.get(id);\n     }\n-    private void recordState(ClusterIdAndRev clusterIdAndRev, String component,\n+    private void recordState(ClusterNameAndRev clusterIdAndRev, String component,\n         String roleServer, AgentRoleState state) {\n       Map<String, Map<String, RoleStateTracker>> componentMap = null;\n       \n@@ -414,20 +437,20 @@ private void recordState(ClusterIdAndRev clusterIdAndRev, String component,\n               state.getClusterId(), state.getClusterDefinitionRevision()));\n       componentMap.put(component, roleMap);\n     }\n-    boolean hasStateChanged(ClusterIdAndRev clusterIdAndRev, String component,\n+    boolean hasStateChanged(ClusterNameAndRev clusterIdAndRev, String component,\n         String roleServer) {\n       return componentRoleMap.get(clusterIdAndRev).get(component)\n           .get(roleServer).stateChanged;\n     }\n-    Set<ClusterIdAndRev> getClustersNodeBelongsTo() {\n+    Set<ClusterNameAndRev> getClustersNodeBelongsTo() {\n       return clusterNodeBelongsTo;\n     }\n-    private void setStateChanged(ClusterIdAndRev clusterIdAndRev, \n+    private void setStateChanged(ClusterNameAndRev clusterIdAndRev, \n         String component, String roleServer) {\n       componentRoleMap.get(clusterIdAndRev).get(component)\n          .get(roleServer).stateChanged = true;\n     }\n-    private void continueRunning(ClusterIdAndRev clusterIdAndRev, \n+    private void continueRunning(ClusterNameAndRev clusterIdAndRev, \n         String component, String roleServer) {\n       componentRoleMap.get(clusterIdAndRev).get(component)\n          .get(roleServer).continueRunning = true;\n@@ -439,22 +462,26 @@ private void continueRunning(ClusterIdAndRev clusterIdAndRev,\n       SERVICE_AVAILABILITY_CHECK_ID, SERVICE_PRESTART_CHECK_ID\n   }  \n   \n-  private static class ClusterIdAndRev {\n-    String clusterId;\n+  private static class ClusterNameAndRev implements \n+  Comparable<ClusterNameAndRev> {\n+    String clusterName;\n     long revision;\n-    ClusterIdAndRev(String clusterId, long revision) {\n-      this.clusterId = clusterId;\n+    ClusterNameAndRev(String clusterName, long revision) {\n+      this.clusterName = clusterName;\n       this.revision = revision;\n     }\n-    String getClusterId() {\n-      return clusterId;\n+    String getClusterName() {\n+      return clusterName;\n     }\n     long getRevision() {\n       return revision;\n     }\n     @Override\n     public int hashCode() {\n-      return (clusterId + String.valueOf(revision)).hashCode();\n+      //note we only consider cluster names (one node can't have\n+      //more than one version of components of the same cluster name \n+      //installed)\n+      return clusterName.hashCode();\n     }\n     @Override\n     public boolean equals(Object obj) {\n@@ -465,8 +492,14 @@ public boolean equals(Object obj) {\n       if (obj == null || getClass() != obj.getClass()) {\n         return false;\n       }\n-      return this.clusterId.equals(((ClusterIdAndRev)obj).getClusterId()) &&\n-          this.clusterId.equals(((ClusterIdAndRev)obj).getRevision());\n+      //note we only compare cluster names (one node can't have\n+      //more than one version of components of the same cluster name \n+      //installed)\n+      return this.clusterName.equals(((ClusterNameAndRev)obj).getClusterName());\n+    }\n+    @Override\n+    public int compareTo(ClusterNameAndRev o) {\n+      return o.getClusterName().compareTo(getClusterName());\n     }\n   }\n \n@@ -498,10 +531,10 @@ private boolean isCompatible(String nodeClusterId, long nodeClusterRev,\n     }\n     return true;\n   }\n-  private static String getSpecialActionID(ClusterIdAndRev clusterIdAndRev, \n+  private static String getSpecialActionID(ClusterNameAndRev clusterIdAndRev, \n       ServiceFSM service, boolean availabilityChecker, \n       boolean prestartCheck) {\n-    String id = clusterIdAndRev.getClusterId() + clusterIdAndRev.getRevision() \n+    String id = clusterIdAndRev.getClusterName() + clusterIdAndRev.getRevision() \n         + service.getServiceName();\n     if (prestartCheck) {\n       id += SpecialServiceIDs.SERVICE_PRESTART_CHECK_ID.toString();\n@@ -549,10 +582,11 @@ private void checkActionResults(HeartBeat heartbeat,\n   }\n   \n   private void checkAndCreateActions(Cluster cluster,\n-      ClusterFSM clusterFsm, ClusterIdAndRev clusterIdAndRev, \n-      ServiceFSM service, HeartBeat heartbeat, List<Action> allActions, \n+      ClusterFSM clusterFsm, ClusterNameAndRev clusterIdAndRev, \n+      boolean newNode, ServiceFSM service, HeartBeat heartbeat, \n+      List<Action> allActions, \n       ComponentAndRoleStates installedOrStartedComponents) \n-          throws IOException {\n+          throws Exception {\n     //see whether the service is in the STARTED state, and if so,\n     //check whether there is any action-result that indicates success\n     //of the availability check (safemode, etc.)\n@@ -580,7 +614,7 @@ private void checkAndCreateActions(Cluster cluster,\n         if (installedOrStartedComponents.isRoleInstalled(clusterIdAndRev,\n             role)) {\n           Action action = plugin.checkService(cluster.getName(), role);\n-          fillActionDetails(action, clusterIdAndRev.getClusterId(),\n+          fillActionDetails(action, clusterIdAndRev.getClusterName(),\n               clusterIdAndRev.getRevision(),service.getServiceName(), role);\n           action.setId(id);\n           action.setKind(Action.Kind.RUN_ACTION);\n@@ -607,14 +641,17 @@ private void checkAndCreateActions(Cluster cluster,\n         ComponentPlugin plugin = \n             cluster.getComponentDefinition(service.getServiceName());\n         String role = plugin.runPreStartRole();\n-        if (!installedOrStartedComponents.isRoleInstalled(clusterIdAndRev,\n-            role)) {\n-          createInstallAction(clusterIdAndRev, cluster.getName(), \n-              service.getServiceName(), role, plugin,\n-              allActions);\n+        if (nodePlayingRole(heartbeat.getHostname(), role)) {\n+          if (newNode || \n+              !installedOrStartedComponents.isRoleInstalled(clusterIdAndRev,\n+              role)) {\n+            createInstallAction(clusterIdAndRev, cluster.getName(), \n+                service.getServiceName(), role, plugin,\n+                allActions);\n+          }\n         }\n         Action action = plugin.preStartAction(cluster.getName(), role);\n-        fillActionDetails(action, clusterIdAndRev.getClusterId(),\n+        fillActionDetails(action, clusterIdAndRev.getClusterName(),\n             clusterIdAndRev.getRevision(),service.getServiceName(), role);\n         action.setId(id);\n         action.setKind(Action.Kind.RUN_ACTION);",
                "raw_url": "https://github.com/apache/ambari/raw/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/java/org/apache/ambari/controller/HeartbeatHandler.java",
                "sha": "c70b45a0e03daa637fd918be5f0855c5389bfc1c",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/ambari/blob/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/java/org/apache/ambari/resource/statemachine/ClusterImpl.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/controller/src/main/java/org/apache/ambari/resource/statemachine/ClusterImpl.java?ref=af3cb35229d53803231e395df0df989a0cb9ebb9",
                "deletions": 2,
                "filename": "controller/src/main/java/org/apache/ambari/resource/statemachine/ClusterImpl.java",
                "patch": "@@ -49,7 +49,7 @@\n    *                                --START_FAILURE from any service--> FAIL\n    * ACTIVE --STOP--> STOPPING --STOP_SUCCESS from all services--> INACTIVE\n    *                             --STOP_FAILURE from any service--> UNCLEAN_STOP\n-   * FAIL --STOP--> STOPPING --STOP_SUCCESS--> INACTIVE\n+   * FAIL --STOP--> STOPPING --STOP_SUCCESS--> STOPPED\n    *                           --STOP_FAILURE--> UNCLEAN_STOP\n    * INACTIVE --RELEASE_NODES--> ATTIC\n    * ATTIC --ADD_NODES--> INACTIVE\n@@ -83,7 +83,10 @@\n   .addTransition(ClusterStateFSM.FAIL, ClusterStateFSM.STOPPING, \n       ClusterEventType.STOP)\n       \n-  .addTransition(ClusterStateFSM.STOPPING, ClusterStateFSM.INACTIVE, \n+  .addTransition(ClusterStateFSM.STOPPING, ClusterStateFSM.STOPPED, \n+      ClusterEventType.STOP_SUCCESS)\n+      \n+  .addTransition(ClusterStateFSM.STOPPED, ClusterStateFSM.STOPPED, \n       ClusterEventType.STOP_SUCCESS)\n       \n   .addTransition(ClusterStateFSM.STOPPING, ClusterStateFSM.UNCLEAN_STOP, \n@@ -120,6 +123,7 @@ public ClusterImpl(Cluster cluster, long revision,\n       serviceImpls.add(serviceImpl);\n     }\n     this.services = serviceImpls;\n+    this.clusterState = clusterState;\n   }\n   \n   public ClusterStateFSM getState() {",
                "raw_url": "https://github.com/apache/ambari/raw/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/java/org/apache/ambari/resource/statemachine/ClusterImpl.java",
                "sha": "483bf6a2053a52a32676953755f1bd2e08c3a852",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/java/org/apache/ambari/resource/statemachine/ClusterStateFSM.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/controller/src/main/java/org/apache/ambari/resource/statemachine/ClusterStateFSM.java?ref=af3cb35229d53803231e395df0df989a0cb9ebb9",
                "deletions": 1,
                "filename": "controller/src/main/java/org/apache/ambari/resource/statemachine/ClusterStateFSM.java",
                "patch": "@@ -18,5 +18,5 @@\n package org.apache.ambari.resource.statemachine;\n \n public enum ClusterStateFSM {\n-  INACTIVE, STARTING, ACTIVE, FAIL, ATTIC, STOPPING, UNCLEAN_STOP\n+  INACTIVE, STARTING, ACTIVE, FAIL, ATTIC, STOPPING, UNCLEAN_STOP, STOPPED\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/java/org/apache/ambari/resource/statemachine/ClusterStateFSM.java",
                "sha": "8afa5e5afe86b2d7fa4a4b185741c00a0f0ae82f",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/ambari/blob/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/java/org/apache/ambari/resource/statemachine/RoleImpl.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/controller/src/main/java/org/apache/ambari/resource/statemachine/RoleImpl.java?ref=af3cb35229d53803231e395df0df989a0cb9ebb9",
                "deletions": 4,
                "filename": "controller/src/main/java/org/apache/ambari/resource/statemachine/RoleImpl.java",
                "patch": "@@ -40,7 +40,7 @@\n    *                                --S_START_FAILURE--> FAIL\n    * ACTIVE --S_STOP--> STOPPING --S_STOP_SUCCESS--> INACTIVE\n    *                             --S_STOP_FAILURE--> UNCLEAN_STOP\n-   * FAIL --S_STOP--> STOPPING --S_STOP_SUCCESS--> INACTIVE\n+   * FAIL --S_STOP--> STOPPING --S_STOP_SUCCESS--> STOPPED\n    *                           --S_STOP_FAILURE--> UNCLEAN_STOP\n    */\n   \n@@ -52,6 +52,9 @@\n          .addTransition(RoleState.INACTIVE, RoleState.STARTING, \n              RoleEventType.START)\n              \n+         .addTransition(RoleState.STOPPED, RoleState.STARTING, \n+             RoleEventType.START)\n+             \n          .addTransition(RoleState.STARTING, \n              EnumSet.of(RoleState.ACTIVE, RoleState.STARTING),\n              RoleEventType.START_SUCCESS, new SuccessfulStartTransition())\n@@ -77,10 +80,10 @@\n              \n          .addTransition(RoleState.FAIL, RoleState.STOPPING, RoleEventType.STOP)\n          \n-         .addTransition(RoleState.STOPPING, RoleState.INACTIVE, \n+         .addTransition(RoleState.STOPPING, RoleState.STOPPED, \n              RoleEventType.STOP_SUCCESS)\n              \n-         .addTransition(RoleState.INACTIVE, RoleState.INACTIVE,\n+         .addTransition(RoleState.STOPPED, RoleState.STOPPED,\n              RoleEventType.STOP_SUCCESS)\n              \n          .addTransition(RoleState.STOPPING, RoleState.UNCLEAN_STOP, \n@@ -199,7 +202,7 @@ public void deactivate() {\n \n   @Override\n   public boolean shouldStop() {\n-    return myState == RoleState.STOPPING || myState == RoleState.INACTIVE;\n+    return myState == RoleState.STOPPING || myState == RoleState.STOPPED;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/ambari/raw/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/java/org/apache/ambari/resource/statemachine/RoleImpl.java",
                "sha": "010867fbc3ad742f1d642abe43b98b5175932d7d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/java/org/apache/ambari/resource/statemachine/RoleState.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/controller/src/main/java/org/apache/ambari/resource/statemachine/RoleState.java?ref=af3cb35229d53803231e395df0df989a0cb9ebb9",
                "deletions": 1,
                "filename": "controller/src/main/java/org/apache/ambari/resource/statemachine/RoleState.java",
                "patch": "@@ -18,5 +18,5 @@\n package org.apache.ambari.resource.statemachine;\n \n public enum RoleState {\n-  INACTIVE, STARTING, ACTIVE, FAIL, STOPPING, UNCLEAN_STOP\n+  INACTIVE, STARTING, ACTIVE, FAIL, STOPPING, UNCLEAN_STOP, STOPPED\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/java/org/apache/ambari/resource/statemachine/RoleState.java",
                "sha": "571aa19b0318cc4308b232e24f3c9eb1febb865f",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/ambari/blob/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/java/org/apache/ambari/resource/statemachine/ServiceImpl.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/controller/src/main/java/org/apache/ambari/resource/statemachine/ServiceImpl.java?ref=af3cb35229d53803231e395df0df989a0cb9ebb9",
                "deletions": 5,
                "filename": "controller/src/main/java/org/apache/ambari/resource/statemachine/ServiceImpl.java",
                "patch": "@@ -33,7 +33,6 @@\n \n public class ServiceImpl implements ServiceFSM, EventHandler<ServiceEvent> {\n \n-  private ServiceState myState;\n   private ClusterFSM clusterFsm;\n   private ComponentPlugin plugin;\n   \n@@ -46,7 +45,7 @@\n    * STARTED --S_UNAVAILABLE--> FAIL\n    * ACTIVE --S_STOP--> STOPPING --S_STOP_SUCCESS--> INACTIVE\n    *                             --S_STOP_FAILURE--> UNCLEAN_STOP\n-   * FAIL --S_STOP--> STOPPING --S_STOP_SUCCESS--> INACTIVE\n+   * FAIL --S_STOP--> STOPPING --S_STOP_SUCCESS--> STOPPED\n    *                           --S_STOP_FAILURE--> UNCLEAN_STOP\n    */\n   \n@@ -59,6 +58,9 @@\n          .addTransition(ServiceState.INACTIVE, ServiceState.PRESTART, \n              ServiceEventType.START)\n              \n+         .addTransition(ServiceState.STOPPED, ServiceState.PRESTART, \n+             ServiceEventType.START)\n+             \n          .addTransition(ServiceState.PRESTART, ServiceState.FAIL, \n              ServiceEventType.PRESTART_FAILURE)  \n              \n@@ -100,7 +102,10 @@\n          .addTransition(ServiceState.FAIL, ServiceState.STOPPING, \n              ServiceEventType.STOP)\n              \n-         .addTransition(ServiceState.STOPPING, ServiceState.INACTIVE, \n+         .addTransition(ServiceState.STOPPING, ServiceState.STOPPED, \n+             ServiceEventType.STOP_SUCCESS)\n+             \n+         .addTransition(ServiceState.STOPPED, ServiceState.STOPPED, \n              ServiceEventType.STOP_SUCCESS)\n              \n          .addTransition(ServiceState.STOPPING, ServiceState.UNCLEAN_STOP,\n@@ -118,7 +123,6 @@ public ServiceImpl(Cluster cluster, ClusterFSM clusterFsm, String serviceName)\n       throws IOException {\n     this.clusterFsm = clusterFsm;\n     this.serviceName = serviceName;\n-    this.myState = ServiceState.INACTIVE;\n     //load plugin and get the roles and create them\n     this.plugin = cluster.getComponentDefinition(serviceName);\n     String[] roles = this.plugin.getActiveRoles();\n@@ -293,7 +297,7 @@ public void deactivate() {\n \n   @Override\n   public boolean isActive() {\n-    return myState == ServiceState.ACTIVE;\n+    return getServiceState() == ServiceState.ACTIVE;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/ambari/raw/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/java/org/apache/ambari/resource/statemachine/ServiceImpl.java",
                "sha": "5a220e13228c22cae9168f91351a41829206c14c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/java/org/apache/ambari/resource/statemachine/ServiceState.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/controller/src/main/java/org/apache/ambari/resource/statemachine/ServiceState.java?ref=af3cb35229d53803231e395df0df989a0cb9ebb9",
                "deletions": 1,
                "filename": "controller/src/main/java/org/apache/ambari/resource/statemachine/ServiceState.java",
                "patch": "@@ -18,5 +18,5 @@\n package org.apache.ambari.resource.statemachine;\n \n public enum ServiceState {\n-  INACTIVE, PRESTART, STARTING, STARTED, ACTIVE, FAIL, STOPPING, UNCLEAN_STOP\n+  INACTIVE, PRESTART, STARTING, STARTED, ACTIVE, FAIL, STOPPING, UNCLEAN_STOP, STOPPED\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/java/org/apache/ambari/resource/statemachine/ServiceState.java",
                "sha": "b605060cfaa408005d11dceeb78815f27db1f4b5",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/java/org/apache/ambari/resource/statemachine/StateMachineInvoker.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/controller/src/main/java/org/apache/ambari/resource/statemachine/StateMachineInvoker.java?ref=af3cb35229d53803231e395df0df989a0cb9ebb9",
                "deletions": 2,
                "filename": "controller/src/main/java/org/apache/ambari/resource/statemachine/StateMachineInvoker.java",
                "patch": "@@ -21,10 +21,8 @@\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n \n-import org.apache.ambari.common.rest.entities.ClusterDefinition;\n import org.apache.ambari.common.rest.entities.ClusterState;\n import org.apache.ambari.controller.Cluster;\n-import org.apache.ambari.controller.HeartbeatHandler;\n import org.apache.ambari.event.AsyncDispatcher;\n import org.apache.ambari.event.Dispatcher;\n import org.apache.ambari.event.EventHandler;\n@@ -40,6 +38,7 @@\n     dispatcher.register(ClusterEventType.class, new ClusterEventDispatcher());\n     dispatcher.register(ServiceEventType.class, new ServiceEventDispatcher());\n     dispatcher.register(RoleEventType.class, new RoleEventDispatcher());\n+    dispatcher.start();\n   }\n   private static Log LOG = LogFactory.getLog(StateMachineInvoker.class);\n   public Dispatcher getAMBARIDispatcher() {",
                "raw_url": "https://github.com/apache/ambari/raw/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/java/org/apache/ambari/resource/statemachine/StateMachineInvoker.java",
                "sha": "3d1358b4a17bd4ddebd90eb8bd13bcd741a7eecc",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/ambari/blob/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/resources/org/apache/ambari/acd/hadoop-hdfs-0.1.0.acd",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/controller/src/main/resources/org/apache/ambari/acd/hadoop-hdfs-0.1.0.acd?ref=af3cb35229d53803231e395df0df989a0cb9ebb9",
                "deletions": 1,
                "filename": "controller/src/main/resources/org/apache/ambari/acd/hadoop-hdfs-0.1.0.acd",
                "patch": "@@ -28,11 +28,15 @@ import sys\n os.execl(\"stack/bin/hadoop\", \"stack/bin/hadoop\", role)\n ]]>\n   </start>\n-  <check run_on=\"namenode\" user=\"${hdfs.user}\">\n+  <check runOn=\"namenode\" user=\"${hdfs.user}\">\n <![CDATA[\n import os\n \n os.exec('stack/bin/hadoop', 'stack/bin/hadoop', 'dfsadmin', '-get')\n ]]>\n   </check>\n+\n+  <prestart runOn=\"namenode\" user=\"{hdfs.user}\">\n+<![CDATA[]]>\n+  </prestart>\n </component>",
                "raw_url": "https://github.com/apache/ambari/raw/af3cb35229d53803231e395df0df989a0cb9ebb9/controller/src/main/resources/org/apache/ambari/acd/hadoop-hdfs-0.1.0.acd",
                "sha": "323f6279b95927fcfa8b1e2bca377018349e0615",
                "status": "modified"
            }
        ],
        "message": "AMBARI-100. Fixes couple of NPE bugs during heartbeat processing.\n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/ambari/trunk@1195038 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/ambari/commit/3c41f89722616a2c79f55ecfa7dac9f855f0bfaa",
        "repo": "ambari",
        "unit_tests": [
            "TestHeartbeatHandler.java",
            "ClusterImplTest.java"
        ]
    },
    "ambari_af54f0f": {
        "bug_id": "ambari_af54f0f",
        "commit": "https://github.com/apache/ambari/commit/af54f0ff9aac7d054ef7de1514b780a164e178f5",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/ambari/blob/af54f0ff9aac7d054ef7de1514b780a164e178f5/ambari-server/src/main/java/org/apache/ambari/server/view/configuration/ViewConfig.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/view/configuration/ViewConfig.java?ref=af54f0ff9aac7d054ef7de1514b780a164e178f5",
                "deletions": 3,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/view/configuration/ViewConfig.java",
                "patch": "@@ -23,6 +23,7 @@\n import javax.xml.bind.annotation.XmlAccessorType;\n import javax.xml.bind.annotation.XmlElement;\n import javax.xml.bind.annotation.XmlRootElement;\n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n@@ -104,7 +105,7 @@ public String getVersion() {\n    * @return the list of parameters\n    */\n   public List<ParameterConfig> getParameters() {\n-    return parameters;\n+    return parameters == null ? Collections.<ParameterConfig>emptyList() : parameters;\n   }\n \n   /**\n@@ -113,7 +114,7 @@ public String getVersion() {\n    * @return return the list of resources\n    */\n   public List<ResourceConfig> getResources() {\n-    return resources;\n+    return resources == null ? Collections.<ResourceConfig>emptyList() : resources;\n   }\n \n   /**\n@@ -122,6 +123,6 @@ public String getVersion() {\n    * @return the list of view instances\n    */\n   public List<InstanceConfig> getInstances() {\n-    return instances;\n+    return instances == null ? Collections.<InstanceConfig>emptyList() : instances;\n   }\n }",
                "raw_url": "https://github.com/apache/ambari/raw/af54f0ff9aac7d054ef7de1514b780a164e178f5/ambari-server/src/main/java/org/apache/ambari/server/view/configuration/ViewConfig.java",
                "sha": "829e08af230765cf7cd55b6657bd4968f6e89b37",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/ambari/blob/af54f0ff9aac7d054ef7de1514b780a164e178f5/ambari-server/src/test/java/org/apache/ambari/server/view/configuration/ViewConfigTest.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/view/configuration/ViewConfigTest.java?ref=af54f0ff9aac7d054ef7de1514b780a164e178f5",
                "deletions": 10,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/view/configuration/ViewConfigTest.java",
                "patch": "@@ -73,14 +73,6 @@\n       \"        <provider-class>org.apache.ambari.server.view.configuration.ViewConfigTest$MyResourceProvider</provider-class>\\n\" +\n       \"        <service-class>org.apache.ambari.server.view.configuration.ViewConfigTest$MyResourceService</service-class>\\n\" +\n       \"    </resource>\\n\" +\n-      \"    <servlet>\\n\" +\n-      \"        <servlet-name>MyViewServlet</servlet-name>\\n\" +\n-      \"        <servlet-path>org.apache.ambari.server.view.configuration.ViewConfigTest$MyViewServlet</servlet-path>\\n\" +\n-      \"    </servlet>\\n\" +\n-      \"    <servlet-mapping>\\n\" +\n-      \"        <servlet-name>MyViewServlet</servlet-name>\\n\" +\n-      \"        <url-pattern>/ui</url-pattern>\\n\" +\n-      \"    </servlet-mapping>\\n\" +\n       \"    <instance>\\n\" +\n       \"        <name>INSTANCE1</name>\\n\" +\n       \"        <property>\\n\" +\n@@ -102,6 +94,13 @@\n       \"</view>\";\n \n \n+  private static String minimal_xml = \"<view>\\n\" +\n+      \"    <name>MY_VIEW</name>\\n\" +\n+      \"    <label>My View!</label>\\n\" +\n+      \"    <version>1.0.0</version>\\n\" +\n+      \"</view>\";\n+\n+\n   @Test\n   public void testGetName() throws Exception {\n     ViewConfig config = getConfig();\n@@ -127,6 +126,12 @@ public void testGetParameters() throws Exception {\n     Assert.assertEquals(2, parameters.size());\n     Assert.assertEquals(\"p1\", parameters.get(0).getName());\n     Assert.assertEquals(\"p2\", parameters.get(1).getName());\n+\n+    // check the case where no parameters are specified for the view...\n+    config = getConfig(minimal_xml);\n+    parameters = config.getParameters();\n+    Assert.assertNotNull(parameters);\n+    Assert.assertEquals(0, parameters.size());\n   }\n \n   @Test\n@@ -136,23 +141,35 @@ public void testGetResources() throws Exception {\n     Assert.assertEquals(2, resources.size());\n     Assert.assertEquals(\"resource\", resources.get(0).getName());\n     Assert.assertEquals(\"subresource\", resources.get(1).getName());\n+\n+    // check the case where no resources are specified for the view...\n+    config = getConfig(minimal_xml);\n+    resources = config.getResources();\n+    Assert.assertNotNull(resources);\n+    Assert.assertEquals(0, resources.size());\n   }\n \n   @Test\n   public void testGetInstances() throws Exception {\n-    ViewConfig config = getConfig();\n+    ViewConfig config = getConfig(xml);\n     List<InstanceConfig> instances = config.getInstances();\n     Assert.assertEquals(2, instances.size());\n     Assert.assertEquals(\"INSTANCE1\", instances.get(0).getName());\n     Assert.assertEquals(\"INSTANCE2\", instances.get(1).getName());\n+\n+    // check the case where no instances are specified for the view...\n+    config = getConfig(minimal_xml);\n+    instances = config.getInstances();\n+    Assert.assertNotNull(instances);\n+    Assert.assertEquals(0, instances.size());\n   }\n \n   public static  ViewConfig getConfig() throws JAXBException {\n       return getConfig(xml);\n   }\n \n   public static  ViewConfig getConfig(String xml) throws JAXBException {\n-      InputStream configStream =  new ByteArrayInputStream(xml.getBytes());\n+    InputStream configStream =  new ByteArrayInputStream(xml.getBytes());\n     JAXBContext jaxbContext = JAXBContext.newInstance(ViewConfig.class);\n     Unmarshaller unmarshaller = jaxbContext.createUnmarshaller();\n     return (ViewConfig) unmarshaller.unmarshal(configStream);",
                "raw_url": "https://github.com/apache/ambari/raw/af54f0ff9aac7d054ef7de1514b780a164e178f5/ambari-server/src/test/java/org/apache/ambari/server/view/configuration/ViewConfigTest.java",
                "sha": "42dd7bea546e5ae98def38a17e489560e4c3cb94",
                "status": "modified"
            }
        ],
        "message": "AMBARI-4679 - NPE for View with no instances / resources / parameters.",
        "parent": "https://github.com/apache/ambari/commit/ba50ff7042a0c56e0310b9455a13a5f2d4148654",
        "repo": "ambari",
        "unit_tests": [
            "ViewConfigTest.java"
        ]
    },
    "ambari_b73f84c": {
        "bug_id": "ambari_b73f84c",
        "commit": "https://github.com/apache/ambari/commit/b73f84c9c32cfca28ad012960d0049af98ce65ed",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/ambari/blob/b73f84c9c32cfca28ad012960d0049af98ce65ed/ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog270.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog270.java?ref=b73f84c9c32cfca28ad012960d0049af98ce65ed",
                "deletions": 2,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog270.java",
                "patch": "@@ -1282,8 +1282,10 @@ private boolean updateInfraKerberosDescriptor(KerberosDescriptor kerberosDescrip\n \n       for (KerberosServiceDescriptor serviceDescriptor : kerberosDescriptor.getServices().values()) {\n         updateKerberosIdentities(serviceDescriptor);\n-        for (KerberosComponentDescriptor componentDescriptor : serviceDescriptor.getComponents().values()) {\n-          updateKerberosIdentities(componentDescriptor);\n+        if (MapUtils.isNotEmpty(serviceDescriptor.getComponents())) {\n+          for (KerberosComponentDescriptor componentDescriptor : serviceDescriptor.getComponents().values()) {\n+            updateKerberosIdentities(componentDescriptor);\n+          }\n         }\n       }\n ",
                "raw_url": "https://github.com/apache/ambari/raw/b73f84c9c32cfca28ad012960d0049af98ce65ed/ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog270.java",
                "sha": "ed5c5281e973f4e0e0e10d9bc4d1aebdf8c5c948",
                "status": "modified"
            }
        ],
        "message": "AMBARI-24473. Ambari upgrade fails due to NPE when processing Ambari Infra kerberos descriptor changes (#2069)",
        "parent": "https://github.com/apache/ambari/commit/db2c3c7e497e73a5d573afc92b19d7463be2dd66",
        "repo": "ambari",
        "unit_tests": [
            "UpgradeCatalog270Test.java"
        ]
    },
    "ambari_c77e4d2": {
        "bug_id": "ambari_c77e4d2",
        "commit": "https://github.com/apache/ambari/commit/c77e4d2255c35afa002e5adc3298d6e81ccd742b",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/c77e4d2255c35afa002e5adc3298d6e81ccd742b/ambari-server/src/main/java/org/apache/ambari/server/upgrade/AbstractUpgradeCatalog.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/upgrade/AbstractUpgradeCatalog.java?ref=c77e4d2255c35afa002e5adc3298d6e81ccd742b",
                "deletions": 3,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/upgrade/AbstractUpgradeCatalog.java",
                "patch": "@@ -150,9 +150,6 @@ public AbstractUpgradeCatalog(Injector injector) {\n     registerCatalog(this);\n   }\n \n-  protected AbstractUpgradeCatalog() {\n-  }\n-\n   /**\n    * Every subclass needs to register itself\n    */",
                "raw_url": "https://github.com/apache/ambari/raw/c77e4d2255c35afa002e5adc3298d6e81ccd742b/ambari-server/src/main/java/org/apache/ambari/server/upgrade/AbstractUpgradeCatalog.java",
                "sha": "6c59784747e754dc1c80ad2094cd85ecc11f9f8b",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/c77e4d2255c35afa002e5adc3298d6e81ccd742b/ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog212.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog212.java?ref=c77e4d2255c35afa002e5adc3298d6e81ccd742b",
                "deletions": 3,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog212.java",
                "patch": "@@ -87,9 +87,6 @@ public UpgradeCatalog212(Injector injector) {\n     daoUtils = injector.getInstance(DaoUtils.class);\n   }\n \n-  protected UpgradeCatalog212() {\n-  }\n-\n   // ----- UpgradeCatalog ----------------------------------------------------\n \n   /**",
                "raw_url": "https://github.com/apache/ambari/raw/c77e4d2255c35afa002e5adc3298d6e81ccd742b/ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog212.java",
                "sha": "90854dd34603d8ac56e3b158d5b86bf0735418cb",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/ambari/blob/c77e4d2255c35afa002e5adc3298d6e81ccd742b/ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog251.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog251.java?ref=c77e4d2255c35afa002e5adc3298d6e81ccd742b",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog251.java",
                "patch": "@@ -22,6 +22,9 @@\n import org.apache.ambari.server.AmbariException;\n import org.apache.ambari.server.orm.DBAccessor.DBColumnInfo;\n \n+import com.google.inject.Inject;\n+import com.google.inject.Injector;\n+\n /**\n  * The {@link UpgradeCatalog251} upgrades Ambari from 2.5.0 to 2.5.1.\n  */\n@@ -30,6 +33,16 @@\n   static final String HOST_ROLE_COMMAND_TABLE = \"host_role_command\";\n   static final String HRC_IS_BACKGROUND_COLUMN = \"is_background\";\n \n+  /**\n+   * Constructor.\n+   *\n+   * @param injector\n+   */\n+  @Inject\n+  public UpgradeCatalog251(Injector injector) {\n+    super(injector);\n+  }\n+\n   /**\n    * {@inheritDoc}\n    */",
                "raw_url": "https://github.com/apache/ambari/raw/c77e4d2255c35afa002e5adc3298d6e81ccd742b/ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog251.java",
                "sha": "6f8f2a6d78d0b6424f78771d6c3a2aa8e8ca9813",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/ambari/blob/c77e4d2255c35afa002e5adc3298d6e81ccd742b/ambari-server/src/test/java/org/apache/ambari/server/upgrade/UpgradeCatalog212Test.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/upgrade/UpgradeCatalog212Test.java?ref=c77e4d2255c35afa002e5adc3298d6e81ccd742b",
                "deletions": 29,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/upgrade/UpgradeCatalog212Test.java",
                "patch": "@@ -52,8 +52,6 @@\n import org.apache.ambari.server.orm.DBAccessor.DBColumnInfo;\n import org.apache.ambari.server.orm.GuiceJpaInitializer;\n import org.apache.ambari.server.orm.InMemoryDefaultTestModule;\n-import org.apache.ambari.server.orm.dao.StackDAO;\n-import org.apache.ambari.server.orm.entities.StackEntity;\n import org.apache.ambari.server.state.Cluster;\n import org.apache.ambari.server.state.Clusters;\n import org.apache.ambari.server.state.Config;\n@@ -69,6 +67,7 @@\n import org.easymock.TestSubject;\n import org.junit.After;\n import org.junit.Assert;\n+import org.junit.Before;\n import org.junit.Rule;\n import org.junit.Test;\n \n@@ -114,28 +113,20 @@\n   private ResultSet resultSet;\n \n   @TestSubject\n-  private UpgradeCatalog212 testSubject = new UpgradeCatalog212();\n+  private UpgradeCatalog212 testSubject = new UpgradeCatalog212(\n+      EasyMock.createNiceMock(Injector.class));\n \n-\n-  private UpgradeCatalogHelper upgradeCatalogHelper;\n-  private StackEntity desiredStackEntity;\n-\n-\n-  // This method to be called only when an IOC is needed - typically by functional tests\n-  public void setupIoCContext() {\n+  @Before\n+  public void setUp() {\n     reset(entityManagerProvider);\n     expect(entityManagerProvider.get()).andReturn(entityManager).anyTimes();\n     replay(entityManagerProvider);\n \n     injector = Guice.createInjector(new InMemoryDefaultTestModule());\n     injector.getInstance(GuiceJpaInitializer.class);\n \n-    upgradeCatalogHelper = injector.getInstance(UpgradeCatalogHelper.class);\n     // inject AmbariMetaInfo to ensure that stacks get populated in the DB\n     injector.getInstance(AmbariMetaInfo.class);\n-    // load the stack entity\n-    StackDAO stackDAO = injector.getInstance(StackDAO.class);\n-    desiredStackEntity = stackDAO.find(\"HDP\", \"2.2.0\");\n   }\n \n   @After\n@@ -148,7 +139,6 @@ public void tearDown() throws AmbariException, SQLException {\n \n   @Test\n   public void testFinilizeTopologyDDL() throws Exception {\n-    setupIoCContext();\n     final DBAccessor dbAccessor = createNiceMock(DBAccessor.class);\n     dbAccessor.dropColumn(eq(\"topology_request\"), eq(\"cluster_name\"));\n     dbAccessor.setColumnNullable(eq(\"topology_request\"), eq(\"cluster_id\"), eq(false));\n@@ -173,7 +163,6 @@ public void configure(Binder binder) {\n \n   @Test\n   public void testExecuteDDLUpdates() throws Exception {\n-    setupIoCContext();\n     final DBAccessor dbAccessor = createNiceMock(DBAccessor.class);\n     Configuration configuration = createNiceMock(Configuration.class);\n     Connection connection = createNiceMock(Connection.class);\n@@ -211,7 +200,6 @@ public void testExecuteDDLUpdates() throws Exception {\n \n   @Test\n   public void testExecuteDMLUpdates() throws Exception {\n-    setupIoCContext();\n     Method addMissingConfigs = UpgradeCatalog212.class.getDeclaredMethod(\"addMissingConfigs\");\n     Method addNewConfigurationsFromXml = AbstractUpgradeCatalog.class.getDeclaredMethod(\"addNewConfigurationsFromXml\");\n \n@@ -235,7 +223,6 @@ public void testExecuteDMLUpdates() throws Exception {\n \n   @Test\n   public void testUpdateHBaseAdnClusterConfigs() throws Exception {\n-    setupIoCContext();\n     EasyMockSupport easyMockSupport = new EasyMockSupport();\n     final AmbariManagementController mockAmbariManagementController = easyMockSupport.createNiceMock(AmbariManagementController.class);\n     final ConfigHelper mockConfigHelper = easyMockSupport.createMock(ConfigHelper.class);\n@@ -302,7 +289,6 @@ protected void configure() {\n \n   @Test\n   public void testUpdateHBaseAdnClusterConfigsTrue() throws Exception {\n-    setupIoCContext();\n     EasyMockSupport easyMockSupport = new EasyMockSupport();\n     final AmbariManagementController mockAmbariManagementController = easyMockSupport.createNiceMock(AmbariManagementController.class);\n     final ConfigHelper mockConfigHelper = easyMockSupport.createMock(ConfigHelper.class);\n@@ -355,7 +341,6 @@ protected void configure() {\n \n   @Test\n   public void testUpdateHBaseAdnClusterConfigsNoHBaseEnv() throws Exception {\n-    setupIoCContext();\n     EasyMockSupport easyMockSupport = new EasyMockSupport();\n     final AmbariManagementController mockAmbariManagementController = easyMockSupport.createNiceMock(AmbariManagementController.class);\n     final ConfigHelper mockConfigHelper = easyMockSupport.createMock(ConfigHelper.class);\n@@ -397,7 +382,6 @@ protected void configure() {\n \n   @Test\n   public void testUpdateHBaseAdnClusterConfigsNoOverrideHBaseUID() throws Exception {\n-    setupIoCContext();\n     EasyMockSupport easyMockSupport = new EasyMockSupport();\n     final AmbariManagementController mockAmbariManagementController = easyMockSupport.createNiceMock(AmbariManagementController.class);\n     final ConfigHelper mockConfigHelper = easyMockSupport.createMock(ConfigHelper.class);\n@@ -415,9 +399,6 @@ public void testUpdateHBaseAdnClusterConfigsNoOverrideHBaseUID() throws Exceptio\n     expect(mockHbaseEnv.getProperties()).andReturn(propertiesHbaseEnv).once();\n     final Config mockClusterEnv = easyMockSupport.createNiceMock(Config.class);\n \n-    final Map<String, String> propertiesExpectedHbaseEnv = new HashMap<String, String>() {{\n-      put(\"hbase_user\", \"hbase\");\n-    }};\n     final Map<String, String> propertiesExpectedClusterEnv = new HashMap<String, String>() {{\n       put(\"override_uid\", \"false\");\n     }};\n@@ -451,17 +432,14 @@ protected void configure() {\n \n   @Test\n   public void testUpdateHiveConfigs() throws Exception {\n-    setupIoCContext();\n     EasyMockSupport easyMockSupport = new EasyMockSupport();\n     final AmbariManagementController  mockAmbariManagementController = easyMockSupport.createNiceMock(AmbariManagementController.class);\n     final ConfigHelper mockConfigHelper = easyMockSupport.createMock(ConfigHelper.class);\n \n     final Clusters mockClusters = easyMockSupport.createStrictMock(Clusters.class);\n     final Cluster mockClusterExpected = easyMockSupport.createNiceMock(Cluster.class);\n-    final Config mockHiveEnv = easyMockSupport.createNiceMock(Config.class);\n     final Config mockHiveSite = easyMockSupport.createNiceMock(Config.class);\n \n-    final Map<String, String> propertiesExpectedHiveEnv = new HashMap<>();\n     final Map<String, String> propertiesExpectedHiveSite = new HashMap<String, String>() {{\n       put(\"hive.heapsize\", \"512\");\n       put(\"hive.server2.custom.authentication.class\", \"\");\n@@ -498,7 +476,6 @@ protected void configure() {\n \n   @Test\n   public void testUpdateOozieConfigs() throws Exception {\n-    setupIoCContext();\n     EasyMockSupport easyMockSupport = new EasyMockSupport();\n     final AmbariManagementController  mockAmbariManagementController = easyMockSupport.createNiceMock(AmbariManagementController.class);\n     final ConfigHelper mockConfigHelper = easyMockSupport.createMock(ConfigHelper.class);\n@@ -539,7 +516,6 @@ protected void configure() {\n \n   @Test\n   public void testUpdateHiveEnvContent() throws Exception {\n-    setupIoCContext();\n     final Injector mockInjector = Guice.createInjector(new AbstractModule() {\n       @Override\n       protected void configure() {",
                "raw_url": "https://github.com/apache/ambari/raw/c77e4d2255c35afa002e5adc3298d6e81ccd742b/ambari-server/src/test/java/org/apache/ambari/server/upgrade/UpgradeCatalog212Test.java",
                "sha": "896602b54c8edd3b0691b39b5c4ff00db15b032d",
                "status": "modified"
            }
        ],
        "message": "AMBARI-20799 - Ambari fails to upgrade from 2.4.2.0 to 2.5.1.0 during schema upgrade with NPE (jonathanhurley)",
        "parent": "https://github.com/apache/ambari/commit/4d4e767d22b3b562c1c975bdf65f1a4b5db38fdb",
        "repo": "ambari",
        "unit_tests": [
            "AbstractUpgradeCatalogTest.java",
            "UpgradeCatalog251Test.java"
        ]
    },
    "ambari_c9936ad": {
        "bug_id": "ambari_c9936ad",
        "commit": "https://github.com/apache/ambari/commit/c9936ad349e3e703dcbf6f2a1644f8a3e45a23d0",
        "file": [
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/ambari/blob/c9936ad349e3e703dcbf6f2a1644f8a3e45a23d0/ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog260.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog260.java?ref=c9936ad349e3e703dcbf6f2a1644f8a3e45a23d0",
                "deletions": 18,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog260.java",
                "patch": "@@ -775,26 +775,28 @@ protected void updateHiveConfigs() throws AmbariException {\n           // hive-interactive-site/hive.llap.zk.sm.keytab.file and hive-interactive-site/hive.llap.task.keytab.file respectively,\n           // based on what hive-interactive-site/hive.llap.daemon.keytab.file has.\n           Config hsiSiteConfig = cluster.getDesiredConfigByType(HIVE_INTERACTIVE_SITE);\n-          Map<String, String> hsiSiteConfigProperties = hsiSiteConfig.getProperties();\n-          if (hsiSiteConfigProperties != null &&\n-                  hsiSiteConfigProperties.containsKey(HIVE_LLAP_DAEMON_KEYTAB_FILE)) {\n-            String[] identities = {HIVE_LLAP_ZK_SM_KEYTAB_FILE, HIVE_LLAP_TASK_KEYTAB_FILE};\n-            Map<String, String> newProperties = new HashMap<>();\n-            for (String identity : identities) {\n-              // Update only if we were able to modify the corresponding kerberos descriptor,\n-              // reflected in list 'getYarnKerberosDescUpdatedList'.\n-              if (getYarnKerberosDescUpdatedList().contains(identity) && hsiSiteConfigProperties.containsKey(identity)) {\n-                newProperties.put(identity, hsiSiteConfigProperties.get(HIVE_LLAP_DAEMON_KEYTAB_FILE));\n+          if (hsiSiteConfig != null) {\n+            Map<String, String> hsiSiteConfigProperties = hsiSiteConfig.getProperties();\n+            if (hsiSiteConfigProperties != null &&\n+                    hsiSiteConfigProperties.containsKey(HIVE_LLAP_DAEMON_KEYTAB_FILE)) {\n+              String[] identities = {HIVE_LLAP_ZK_SM_KEYTAB_FILE, HIVE_LLAP_TASK_KEYTAB_FILE};\n+              Map<String, String> newProperties = new HashMap<>();\n+              for (String identity : identities) {\n+                // Update only if we were able to modify the corresponding kerberos descriptor,\n+                // reflected in list 'getYarnKerberosDescUpdatedList'.\n+                if (getYarnKerberosDescUpdatedList().contains(identity) && hsiSiteConfigProperties.containsKey(identity)) {\n+                  newProperties.put(identity, hsiSiteConfigProperties.get(HIVE_LLAP_DAEMON_KEYTAB_FILE));\n+                }\n               }\n-            }\n \n-            // Update step.\n-            if (newProperties.size() > 0) {\n-              try {\n-                updateConfigurationPropertiesForCluster(cluster, HIVE_INTERACTIVE_SITE, newProperties, true, false);\n-                LOG.info(\"Updated HSI config(s) : \" + newProperties.keySet() + \" with value(s) = \" + newProperties.values()+\" respectively.\");\n-              } catch (AmbariException e) {\n-                e.printStackTrace();\n+              // Update step.\n+              if (newProperties.size() > 0) {\n+                try {\n+                  updateConfigurationPropertiesForCluster(cluster, HIVE_INTERACTIVE_SITE, newProperties, true, false);\n+                  LOG.info(\"Updated HSI config(s) : \" + newProperties.keySet() + \" with value(s) = \" + newProperties.values() + \" respectively.\");\n+                } catch (AmbariException e) {\n+                  e.printStackTrace();\n+                }\n               }\n             }\n           }",
                "raw_url": "https://github.com/apache/ambari/raw/c9936ad349e3e703dcbf6f2a1644f8a3e45a23d0/ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog260.java",
                "sha": "4d9a5dacf894aa9fad29a0c461e1b00bf6172f05",
                "status": "modified"
            }
        ],
        "message": "AMBARI-22517. NPE during Ambari schema upgrade while updating Hive configs.",
        "parent": "https://github.com/apache/ambari/commit/c5fe6cb96a6530d0fc44c4ffced2ec46db97233b",
        "repo": "ambari",
        "unit_tests": [
            "UpgradeCatalog260Test.java"
        ]
    },
    "ambari_d4a2b2b": {
        "bug_id": "ambari_d4a2b2b",
        "commit": "https://github.com/apache/ambari/commit/d4a2b2b9eb0864dd26327c896547868f16a56f6e",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/ambari/blob/d4a2b2b9eb0864dd26327c896547868f16a56f6e/contrib/ambari-log4j/src/main/java/org/apache/ambari/log4j/common/store/DatabaseStore.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/contrib/ambari-log4j/src/main/java/org/apache/ambari/log4j/common/store/DatabaseStore.java?ref=d4a2b2b9eb0864dd26327c896547868f16a56f6e",
                "deletions": 2,
                "filename": "contrib/ambari-log4j/src/main/java/org/apache/ambari/log4j/common/store/DatabaseStore.java",
                "patch": "@@ -96,10 +96,12 @@ public void persist(LoggingEvent originalEvent, Object parsedEvent)\n   @Override\n   public void close() throws IOException {\n     try {\n-      connection.close();\n+      if (this.initialized && this.connection != null) {\n+        connection.close();\n+      }\n     } catch (SQLException sqle) {\n       throw new IOException(\n-          \"Failed to close connectionto database \" + this.database, sqle);\n+          \"Failed to close connection to database \" + this.database, sqle);\n     }\n   }\n }",
                "raw_url": "https://github.com/apache/ambari/raw/d4a2b2b9eb0864dd26327c896547868f16a56f6e/contrib/ambari-log4j/src/main/java/org/apache/ambari/log4j/common/store/DatabaseStore.java",
                "sha": "55934d42dc64997a244ff0b43a7efe7c99886eee",
                "status": "modified"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/ambari/blob/d4a2b2b9eb0864dd26327c896547868f16a56f6e/contrib/ambari-log4j/src/test/java/org/apache/ambari/log4j/common/store/TestDatabaseStore.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/contrib/ambari-log4j/src/test/java/org/apache/ambari/log4j/common/store/TestDatabaseStore.java?ref=d4a2b2b9eb0864dd26327c896547868f16a56f6e",
                "deletions": 0,
                "filename": "contrib/ambari-log4j/src/test/java/org/apache/ambari/log4j/common/store/TestDatabaseStore.java",
                "patch": "@@ -0,0 +1,42 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.ambari.log4j.common.store;\n+\n+import junit.framework.TestCase;\n+import org.apache.ambari.log4j.common.LogStoreUpdateProvider;\n+import org.apache.log4j.spi.LoggingEvent;\n+\n+import java.io.IOException;\n+import java.sql.Connection;\n+\n+public class TestDatabaseStore extends TestCase {\n+\n+  class SampleLogStoreUpdateProvider implements LogStoreUpdateProvider {\n+    public void init(Connection connection) throws IOException {\n+    }\n+\n+    public void update(LoggingEvent originalEvent, Object parsedEvent)\n+        throws IOException {\n+    }\n+  }\n+\n+  public void testDatabaseStore() throws IOException {\n+    DatabaseStore store = new DatabaseStore(TestDatabaseStore.class.getName(), \"\", \"\", \"\",\n+        new SampleLogStoreUpdateProvider());\n+    store.close();\n+  }\n+}",
                "raw_url": "https://github.com/apache/ambari/raw/d4a2b2b9eb0864dd26327c896547868f16a56f6e/contrib/ambari-log4j/src/test/java/org/apache/ambari/log4j/common/store/TestDatabaseStore.java",
                "sha": "bafe16c041f5377012e8d8083630f27c8f672faa",
                "status": "added"
            }
        ],
        "message": "AMBARI-2526. NPE in the ambari log4j when the threads are shutting down. (smohanty)\n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/ambari/trunk@1498603 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/ambari/commit/2b671a163da30ae9693726c0d6e5ce1b9f880ae2",
        "repo": "ambari",
        "unit_tests": [
            "TestDatabaseStore.java"
        ]
    },
    "ambari_d5ae6ae": {
        "bug_id": "ambari_d5ae6ae",
        "commit": "https://github.com/apache/ambari/commit/d5ae6ae035d4dd634d0a42aa8066342d2aa256bc",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/ambari/blob/d5ae6ae035d4dd634d0a42aa8066342d2aa256bc/ambari-server/src/main/java/org/apache/ambari/server/state/ConfigHelper.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/state/ConfigHelper.java?ref=d5ae6ae035d4dd634d0a42aa8066342d2aa256bc",
                "deletions": 2,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/state/ConfigHelper.java",
                "patch": "@@ -686,9 +686,11 @@ public void updateConfigType(Cluster cluster,\n       } else {\n         oldConfigProperties = oldConfig.getProperties();\n         if (oldConfigProperties != null) {\n-          properties.putAll(oldConfig.getProperties());\n+          properties.putAll(oldConfigProperties);\n+        }\n+        if (oldConfig.getPropertiesAttributes() != null) {\n+          propertiesAttributes.putAll(oldConfig.getPropertiesAttributes());\n         }\n-        propertiesAttributes.putAll(oldConfig.getPropertiesAttributes());\n       }\n \n       properties.putAll(updates);",
                "raw_url": "https://github.com/apache/ambari/raw/d5ae6ae035d4dd634d0a42aa8066342d2aa256bc/ambari-server/src/main/java/org/apache/ambari/server/state/ConfigHelper.java",
                "sha": "2acd62bdf378d67763433cf9b21af1cea91db8dd",
                "status": "modified"
            },
            {
                "additions": 50,
                "blob_url": "https://github.com/apache/ambari/blob/d5ae6ae035d4dd634d0a42aa8066342d2aa256bc/ambari-server/src/test/java/org/apache/ambari/server/state/ConfigHelperTest.java",
                "changes": 51,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/state/ConfigHelperTest.java?ref=d5ae6ae035d4dd634d0a42aa8066342d2aa256bc",
                "deletions": 1,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/state/ConfigHelperTest.java",
                "patch": "@@ -164,6 +164,26 @@ public void setup() throws Exception {\n       managementController.updateClusters(new HashSet<ClusterRequest>() {{\n         add(clusterRequest3);\n       }}, null);\n+\n+      // oozie-site\n+      ConfigurationRequest cr4 = new ConfigurationRequest();\n+      cr4.setClusterName(clusterName);\n+      cr4.setType(\"oozie-site\");\n+      cr4.setVersionTag(\"version1\");\n+      cr4.setProperties(new HashMap<String, String>() {{\n+        put(\"oozie.authentication.type\", \"simple\");\n+        put(\"oozie.service.HadoopAccessorService.kerberos.enabled\", \"false\");\n+      }});\n+      cr4.setPropertiesAttributes(null);\n+\n+      final ClusterRequest clusterRequest4 =\n+        new ClusterRequest(cluster.getClusterId(), clusterName,\n+          cluster.getDesiredStackVersion().getStackVersion(), null);\n+\n+      clusterRequest4.setDesiredConfig(Collections.singletonList(cr4));\n+      managementController.updateClusters(new HashSet<ClusterRequest>() {{\n+        add(clusterRequest4);\n+      }}, null);\n     }\n \n     @After\n@@ -304,7 +324,7 @@ public void testEffectivePropertiesAttributesWithOverrides() throws Exception {\n               configHelper.getEffectiveDesiredTags(cluster, \"h1\"));\n \n       Assert.assertNotNull(effectiveAttributes);\n-      Assert.assertEquals(3, effectiveAttributes.size());\n+      Assert.assertEquals(4, effectiveAttributes.size());\n \n       Assert.assertTrue(effectiveAttributes.containsKey(\"global\"));\n       Map<String, Map<String, String>> globalAttrs = effectiveAttributes.get(\"global\");\n@@ -594,6 +614,35 @@ public void testUpdateConfigType() throws Exception {\n       Assert.assertFalse(propertiesAttributes.get(\"attribute1\").containsKey(\"ipc.client.connect.max.retries\"));\n     }\n \n+    @Test\n+    public void testUpdateConfigTypeNoPropertyAttributes() throws Exception {\n+      Config currentConfig = cluster.getDesiredConfigByType(\"oozie-site\");\n+      Map<String, String> properties = currentConfig.getProperties();\n+      // Config tag before update\n+      Assert.assertEquals(\"version1\", currentConfig.getTag());\n+      // Properties before update\n+      Assert.assertEquals(\"simple\", properties.get(\"oozie.authentication.type\"));\n+      Assert.assertEquals(\"false\", properties.get(\"oozie.service.HadoopAccessorService.kerberos.enabled\"));\n+\n+      Map<String, String> updates = new HashMap<String, String>();\n+      updates.put(\"oozie.authentication.type\", \"kerberos\");\n+      updates.put(\"oozie.service.HadoopAccessorService.kerberos.enabled\", \"true\");\n+\n+      configHelper.updateConfigType(cluster, managementController, \"oozie-site\", updates, null, \"admin\", \"Test \" +\n+        \"note\");\n+\n+      Config updatedConfig = cluster.getDesiredConfigByType(\"oozie-site\");\n+      // Config tag updated\n+      Assert.assertFalse(\"version1\".equals(updatedConfig.getTag()));\n+      // Property added\n+      properties = updatedConfig.getProperties();\n+      Assert.assertTrue(properties.containsKey(\"oozie.authentication.type\"));\n+      Assert.assertEquals(\"kerberos\", properties.get(\"oozie.authentication.type\"));\n+      // Property updated\n+      Assert.assertTrue(properties.containsKey(\"oozie.service.HadoopAccessorService.kerberos.enabled\"));\n+      Assert.assertEquals(\"true\", properties.get(\"oozie.service.HadoopAccessorService.kerberos.enabled\"));\n+    }\n+\n     @Test\n     public void testCalculateIsStaleConfigs() throws Exception {\n ",
                "raw_url": "https://github.com/apache/ambari/raw/d5ae6ae035d4dd634d0a42aa8066342d2aa256bc/ambari-server/src/test/java/org/apache/ambari/server/state/ConfigHelperTest.java",
                "sha": "06c99f46a9cfdebe9926ebcebffc05aad7da003d",
                "status": "modified"
            }
        ],
        "message": "AMBARI-13836. NPE when enabling security during Update Configurations stage (Sandor Magyari via rlevas)",
        "parent": "https://github.com/apache/ambari/commit/3083cdbab4ba03f2ae9f0b6fa02c63747dd82fa0",
        "repo": "ambari",
        "unit_tests": [
            "ConfigHelperTest.java"
        ]
    },
    "ambari_d63147f": {
        "bug_id": "ambari_d63147f",
        "commit": "https://github.com/apache/ambari/commit/d63147ff0dc8c949a060330a3c593f9f60ad5385",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/d63147ff0dc8c949a060330a3c593f9f60ad5385/ambari-server/src/main/java/org/apache/ambari/server/orm/entities/TopologyHostGroupEntity.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/orm/entities/TopologyHostGroupEntity.java?ref=d63147ff0dc8c949a060330a3c593f9f60ad5385",
                "deletions": 2,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/orm/entities/TopologyHostGroupEntity.java",
                "patch": "@@ -51,8 +51,6 @@\n   private Long id;\n \n   @Column(name = \"name\", nullable = false, updatable = false)\n-  @Basic(fetch = FetchType.LAZY)\n-  @Lob\n   private String name;\n \n   @Column(name = \"group_properties\")",
                "raw_url": "https://github.com/apache/ambari/raw/d63147ff0dc8c949a060330a3c593f9f60ad5385/ambari-server/src/main/java/org/apache/ambari/server/orm/entities/TopologyHostGroupEntity.java",
                "sha": "a795552b5ab249ef5a98c1ae4972a3e760885da3",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/d63147ff0dc8c949a060330a3c593f9f60ad5385/ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClustersImpl.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClustersImpl.java?ref=d63147ff0dc8c949a060330a3c593f9f60ad5385",
                "deletions": 1,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClustersImpl.java",
                "patch": "@@ -911,7 +911,7 @@ void deleteHostEntityRelationships(String hostname) throws AmbariException {\n           TopologyLogicalRequestEntity topologyLogicalRequestEntity = topologyRequestEntity.getTopologyLogicalRequestEntity();\n \n           for (TopologyHostRequestEntity topologyHostRequestEntity: topologyLogicalRequestEntity.getTopologyHostRequestEntities()) {\n-            if (topologyHostRequestEntity.getHostName().equals(hostname)) {\n+            if (hostname.equals(topologyHostRequestEntity.getHostName())) {\n               topologyHostRequestDAO.remove(topologyHostRequestEntity);\n             }\n           }",
                "raw_url": "https://github.com/apache/ambari/raw/d63147ff0dc8c949a060330a3c593f9f60ad5385/ambari-server/src/main/java/org/apache/ambari/server/state/cluster/ClustersImpl.java",
                "sha": "3dd3beb5c0754259f14353d98a14cd1483f1e70f",
                "status": "modified"
            },
            {
                "additions": 111,
                "blob_url": "https://github.com/apache/ambari/blob/d63147ff0dc8c949a060330a3c593f9f60ad5385/ambari-server/src/test/java/org/apache/ambari/server/state/cluster/ClustersTest.java",
                "changes": 111,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/state/cluster/ClustersTest.java?ref=d63147ff0dc8c949a060330a3c593f9f60ad5385",
                "deletions": 0,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/state/cluster/ClustersTest.java",
                "patch": "@@ -56,14 +56,20 @@\n import org.apache.ambari.server.topology.Blueprint;\n import org.apache.ambari.server.topology.Configuration;\n import org.apache.ambari.server.topology.HostGroupInfo;\n+import org.apache.ambari.server.topology.HostRequest;\n+import org.apache.ambari.server.topology.LogicalRequest;\n import org.apache.ambari.server.topology.PersistedState;\n import org.apache.ambari.server.topology.TopologyRequest;\n+import org.apache.ambari.server.topology.TopologyTask;\n import org.junit.After;\n import org.junit.Before;\n import org.junit.Test;\n \n import javax.persistence.EntityManager;\n+\n+import java.util.ArrayList;\n import java.util.Arrays;\n+import java.util.Collection;\n import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n@@ -566,4 +572,109 @@ public void testSetCurrentStackVersion() throws AmbariException {\n     Assert.assertTrue(\n         clusters.getCluster(c1).getCurrentStackVersion().getStackVersion().equals(stackId.getStackVersion()));\n   }\n+\n+\n+  @Test\n+  public void testNullHostNamesInTopologyRequests() throws AmbariException {\n+    final String hostName = \"myhost\";\n+    final String clusterName = \"mycluster\";\n+\n+    Cluster cluster = createCluster(clusterName);\n+    addHostToCluster(hostName, clusterName);\n+    addHostToCluster(hostName + \"2\", clusterName);\n+    addHostToCluster(hostName + \"3\", clusterName);\n+\n+    createTopologyRequest(cluster, hostName);\n+    clusters.deleteHost(hostName);\n+    for(Host h : cluster.getHosts()) {\n+      if(hostName.equals(h.getHostName())) {\n+        Assert.fail(\"Host is expected to be deleted\");\n+      }\n+    }\n+  }\n+\n+  private void createTopologyRequest(Cluster cluster, String hostName) {\n+    final String groupName = \"MyHostGroup\";\n+\n+    // add topology request\n+    Blueprint bp = createNiceMock(Blueprint.class);\n+    expect(bp.getName()).andReturn(\"TestBluePrint\").anyTimes();\n+\n+    Configuration clusterConfig = new Configuration(\n+      Maps.<String, Map<String, String>>newHashMap(),\n+      Maps.<String, Map<String, Map<String, String>>>newHashMap()\n+    );\n+\n+    Map<String, HostGroupInfo> hostGroups = new HashMap<>();\n+    HostGroupInfo hostGroupInfo = new HostGroupInfo(groupName);\n+    hostGroupInfo.setConfiguration(clusterConfig);\n+    hostGroupInfo.addHost(hostName);\n+    hostGroupInfo.addHost(hostName + \"2\");\n+    hostGroupInfo.addHost(hostName + \"3\");\n+    hostGroups.put(groupName, hostGroupInfo);\n+\n+    TopologyRequest topologyRequest = createNiceMock(TopologyRequest.class);\n+    expect(topologyRequest.getType()).andReturn(TopologyRequest.Type.PROVISION).anyTimes();\n+    expect(topologyRequest.getBlueprint()).andReturn(bp).anyTimes();\n+    expect(topologyRequest.getClusterId()).andReturn(cluster.getClusterId()).anyTimes();\n+    expect(topologyRequest.getConfiguration()).andReturn(clusterConfig).anyTimes();\n+    expect(topologyRequest.getDescription()).andReturn(\"Test description\").anyTimes();\n+    expect(topologyRequest.getHostGroupInfo()).andReturn(hostGroups).anyTimes();\n+\n+    replay(bp, topologyRequest);\n+\n+    persistedState.persistTopologyRequest(topologyRequest);\n+\n+    createTopologyLogicalRequest(cluster, hostName);\n+  }\n+\n+  private HostRequest createHostRequest(long hrId, String hostName) {\n+    HostRequest hr = createNiceMock(HostRequest.class);\n+    expect(hr.getId()).andReturn(hrId).anyTimes();\n+    expect(hr.getHostgroupName()).andReturn(\"MyHostGroup\").anyTimes();\n+    expect(hr.getHostName()).andReturn(hostName).anyTimes();\n+    expect(hr.getStageId()).andReturn(1L);\n+    expect(hr.getTopologyTasks()).andReturn(Collections.<TopologyTask>emptyList());\n+\n+    replay(hr);\n+    return hr;\n+  }\n+\n+  private void createTopologyLogicalRequest(Cluster cluster, String hostName) {\n+    Collection<HostRequest> hostRequests = new ArrayList<>();\n+    hostRequests.add(createHostRequest(1L, null));\n+    hostRequests.add(createHostRequest(2L, hostName));\n+    hostRequests.add(createHostRequest(3L, null));\n+    hostRequests.add(createHostRequest(4L, hostName + \"2\"));\n+    hostRequests.add(createHostRequest(5L, null));\n+    hostRequests.add(createHostRequest(6L, hostName + \"3\"));\n+\n+    Long requestId = topologyRequestDAO.findByClusterId(cluster.getClusterId()).get(0).getId();\n+    LogicalRequest logicalRequest = createNiceMock(LogicalRequest.class);\n+    expect(logicalRequest.getHostRequests()).andReturn(hostRequests).anyTimes();\n+    expect(logicalRequest.getRequestContext()).andReturn(\"Description\").anyTimes();\n+    expect(logicalRequest.getRequestId()).andReturn(1L).anyTimes();\n+    replay(logicalRequest);\n+\n+    persistedState.persistLogicalRequest(logicalRequest, requestId);\n+  }\n+\n+  private void addHostToCluster(String hostName, String clusterName) throws AmbariException {\n+    clusters.addHost(hostName);\n+\n+    Host host = clusters.getHost(hostName);\n+    setOsFamily(clusters.getHost(hostName), \"centos\", \"5.9\");\n+    host.persist();\n+\n+    Set<String> hostnames = new HashSet<>();\n+    hostnames.add(hostName);\n+    clusters.mapHostsToCluster(hostnames, clusterName);\n+  }\n+\n+  private Cluster createCluster(String clusterName) throws AmbariException {\n+    StackId stackId = new StackId(\"HDP-0.1\");\n+    clusters.addCluster(clusterName, stackId);\n+\n+    return clusters.getCluster(clusterName);\n+  }\n }",
                "raw_url": "https://github.com/apache/ambari/raw/d63147ff0dc8c949a060330a3c593f9f60ad5385/ambari-server/src/test/java/org/apache/ambari/server/state/cluster/ClustersTest.java",
                "sha": "b56326877f58a0baa1e3931e7a73ea122caa5c17",
                "status": "modified"
            }
        ],
        "message": "AMBARI-15862. NPE when deleting a host (Daniel Gergely via oleewere)",
        "parent": "https://github.com/apache/ambari/commit/32f7ec368aea96a9cd5147b341802e99e9c6a024",
        "repo": "ambari",
        "unit_tests": [
            "ClustersImplTest.java"
        ]
    },
    "ambari_d9bfcac": {
        "bug_id": "ambari_d9bfcac",
        "commit": "https://github.com/apache/ambari/commit/d9bfcac2c53fc44ffb9a45c5ea29c592153ad069",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/ambari/blob/d9bfcac2c53fc44ffb9a45c5ea29c592153ad069/ambari-server/src/main/java/org/apache/ambari/server/state/host/HostImpl.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/state/host/HostImpl.java?ref=d9bfcac2c53fc44ffb9a45c5ea29c592153ad069",
                "deletions": 4,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/state/host/HostImpl.java",
                "patch": "@@ -22,6 +22,7 @@\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n import java.util.concurrent.CopyOnWriteArrayList;\n@@ -1254,16 +1255,21 @@ public void calculateHostStatus(Long clusterId) throws AmbariException {\n       String status = scHost.getState().name();\n \n       String category = componentInfo.getCategory();\n+      if (category == null) {\n+        LOG.warn(\"In stack {}-{} service {} component {} category is null!\",\n+                stackId.getStackName(), stackId.getStackVersion(), scHost.getServiceName(), scHost.getServiceComponentName());\n+        continue;\n+      }\n \n       if (MaintenanceState.OFF == maintenanceStateHelper.getEffectiveState(scHost, this)) {\n-        if (category.equals(\"MASTER\")) {\n+        if (Objects.equals(\"MASTER\", category)) {\n           ++masterCount;\n-          if (status.equals(\"STARTED\")) {\n+          if (Objects.equals(\"STARTED\", status)) {\n             ++mastersRunning;\n           }\n-        } else if (category.equals(\"SLAVE\")) {\n+        } else if (Objects.equals(\"SLAVE\", category)) {\n           ++slaveCount;\n-          if (status.equals(\"STARTED\")) {\n+          if (Objects.equals(\"STARTED\", status)) {\n             ++slavesRunning;\n           }\n         }",
                "raw_url": "https://github.com/apache/ambari/raw/d9bfcac2c53fc44ffb9a45c5ea29c592153ad069/ambari-server/src/main/java/org/apache/ambari/server/state/host/HostImpl.java",
                "sha": "cb7af46f882460f3d23f6984d6b32cf10d1c2f92",
                "status": "modified"
            }
        ],
        "message": "AMBARI-23838. Ambari Server NPE due to Agent failure during registration (#1285)",
        "parent": "https://github.com/apache/ambari/commit/9671dd17d83d39589cb05e830ecaffcf2a025086",
        "repo": "ambari",
        "unit_tests": [
            "HostImplTest.java"
        ]
    },
    "ambari_da56057": {
        "bug_id": "ambari_da56057",
        "commit": "https://github.com/apache/ambari/commit/da5605706555d3aecca7a67fb399d9c501972ad4",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/ambari/blob/da5605706555d3aecca7a67fb399d9c501972ad4/ambari-server/src/main/java/org/apache/ambari/server/actionmanager/ActionDBAccessorImpl.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/actionmanager/ActionDBAccessorImpl.java?ref=da5605706555d3aecca7a67fb399d9c501972ad4",
                "deletions": 7,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/actionmanager/ActionDBAccessorImpl.java",
                "patch": "@@ -344,7 +344,6 @@ public void persistActions(Request request) throws AmbariException {\n       StageEntity stageEntity = stage.constructNewPersistenceEntity();\n       stageEntities.add(stageEntity);\n       stageEntity.setClusterId(clusterId);\n-      //TODO refactor to reduce merges\n       stageEntity.setRequest(requestEntity);\n       stageDAO.create(stageEntity);\n \n@@ -353,9 +352,6 @@ public void persistActions(Request request) throws AmbariException {\n       for (HostRoleCommand hostRoleCommand : orderedHostRoleCommands) {\n         HostRoleCommandEntity hostRoleCommandEntity = hostRoleCommand.constructNewPersistenceEntity();\n         hostRoleCommandEntity.setStage(stageEntity);\n-\n-        HostEntity hostEntity = null;\n-\n         hostRoleCommandDAO.create(hostRoleCommandEntity);\n \n         assert hostRoleCommandEntity.getTaskId() != null;\n@@ -365,13 +361,15 @@ public void persistActions(Request request) throws AmbariException {\n         String output = \"output-\" + hostRoleCommandEntity.getTaskId() + \".txt\";\n         String error = \"errors-\" + hostRoleCommandEntity.getTaskId() + \".txt\";\n \n+        HostEntity hostEntity = null;\n         if (null != hostRoleCommandEntity.getHostId()) {\n           hostEntity = hostDAO.findById(hostRoleCommandEntity.getHostId());\n           if (hostEntity == null) {\n             String msg = String.format(\"Host %s doesn't exist in database\", hostRoleCommandEntity.getHostName());\n             LOG.error(msg);\n             throw new AmbariException(msg);\n           }\n+\n           hostRoleCommandEntity.setHostEntity(hostEntity);\n \n           try {\n@@ -401,18 +399,20 @@ public void persistActions(Request request) throws AmbariException {\n         hostRoleCommandEntity.setExecutionCommand(executionCommandEntity);\n \n         executionCommandDAO.create(hostRoleCommandEntity.getExecutionCommand());\n-        hostRoleCommandDAO.merge(hostRoleCommandEntity);\n+        hostRoleCommandEntity = hostRoleCommandDAO.merge(hostRoleCommandEntity);\n+\n         if (null != hostEntity) {\n-          hostDAO.merge(hostEntity);\n+          hostEntity = hostDAO.merge(hostEntity);\n         }\n       }\n \n       for (RoleSuccessCriteriaEntity roleSuccessCriteriaEntity : stageEntity.getRoleSuccessCriterias()) {\n         roleSuccessCriteriaDAO.create(roleSuccessCriteriaEntity);\n       }\n \n-      stageDAO.create(stageEntity);\n+      stageEntity = stageDAO.merge(stageEntity);\n     }\n+\n     requestEntity.setStages(stageEntities);\n     requestDAO.merge(requestEntity);\n   }",
                "raw_url": "https://github.com/apache/ambari/raw/da5605706555d3aecca7a67fb399d9c501972ad4/ambari-server/src/main/java/org/apache/ambari/server/actionmanager/ActionDBAccessorImpl.java",
                "sha": "c31ca7e1066947bfdb396ab12a8c11b7eef7ec0e",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/ambari/blob/da5605706555d3aecca7a67fb399d9c501972ad4/ambari-server/src/main/java/org/apache/ambari/server/actionmanager/HostRoleCommand.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/actionmanager/HostRoleCommand.java?ref=da5605706555d3aecca7a67fb399d9c501972ad4",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/actionmanager/HostRoleCommand.java",
                "patch": "@@ -209,6 +209,22 @@ public HostRoleCommandEntity constructNewPersistenceEntity() {\n \n     hostRoleCommandEntity.setEvent(event.getEventJson());\n \n+    // set IDs if the wrapping object has them - they are most likely\n+    // non-updatable in JPA since they are retrieved from a relationship,\n+    // however the JPA cache may choose to not refresh the entity so they would\n+    // end up being null if not set before persisting the command entity\n+    if (requestId >= 0) {\n+      hostRoleCommandEntity.setRequestId(requestId);\n+    }\n+\n+    if (stageId >= 0) {\n+      hostRoleCommandEntity.setStageId(stageId);\n+    }\n+\n+    if (taskId >= 0) {\n+      hostRoleCommandEntity.setTaskId(taskId);\n+    }\n+\n     return hostRoleCommandEntity;\n   }\n ",
                "raw_url": "https://github.com/apache/ambari/raw/da5605706555d3aecca7a67fb399d9c501972ad4/ambari-server/src/main/java/org/apache/ambari/server/actionmanager/HostRoleCommand.java",
                "sha": "85c8e9fe09f230207b134cae8475fcc69ac53e1c",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/ambari/blob/da5605706555d3aecca7a67fb399d9c501972ad4/ambari-server/src/main/java/org/apache/ambari/server/orm/entities/HostRoleCommandEntity.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/orm/entities/HostRoleCommandEntity.java?ref=da5605706555d3aecca7a67fb399d9c501972ad4",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/orm/entities/HostRoleCommandEntity.java",
                "patch": "@@ -511,8 +511,27 @@ public StageEntity getStage() {\n     return stage;\n   }\n \n+  /**\n+   * Sets the associated {@link StageEntity} for this command. If the\n+   * {@link StageEntity} has been persisted, then this will also set the\n+   * commands stage and request ID fields.\n+   *\n+   * @param stage\n+   */\n   public void setStage(StageEntity stage) {\n     this.stage = stage;\n+\n+    // ensure that the IDs are also set since they may not be retrieved from JPA\n+    // when this entity is cached\n+    if (null != stage) {\n+      if (null == stageId) {\n+        stageId = stage.getStageId();\n+      }\n+\n+      if (null == requestId) {\n+        requestId = stage.getRequestId();\n+      }\n+    }\n   }\n \n   public HostEntity getHostEntity() {",
                "raw_url": "https://github.com/apache/ambari/raw/da5605706555d3aecca7a67fb399d9c501972ad4/ambari-server/src/main/java/org/apache/ambari/server/orm/entities/HostRoleCommandEntity.java",
                "sha": "74271b94b0ef86aba3a84041c5a9fda24337d87d",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/ambari/blob/da5605706555d3aecca7a67fb399d9c501972ad4/ambari-server/src/main/java/org/apache/ambari/server/topology/HostRequest.java",
                "changes": 25,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/topology/HostRequest.java?ref=da5605706555d3aecca7a67fb399d9c501972ad4",
                "deletions": 14,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/topology/HostRequest.java",
                "patch": "@@ -18,6 +18,17 @@\n \n package org.apache.ambari.server.topology;\n \n+import static org.apache.ambari.server.controller.internal.ProvisionAction.INSTALL_AND_START;\n+import static org.apache.ambari.server.controller.internal.ProvisionAction.INSTALL_ONLY;\n+import static org.apache.ambari.server.controller.internal.ProvisionAction.START_ONLY;\n+\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+\n import org.apache.ambari.server.actionmanager.HostRoleCommand;\n import org.apache.ambari.server.api.predicate.InvalidQueryException;\n import org.apache.ambari.server.api.predicate.PredicateCompiler;\n@@ -37,17 +48,6 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.util.ArrayList;\n-import java.util.Collection;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.List;\n-import java.util.Map;\n-\n-import static org.apache.ambari.server.controller.internal.ProvisionAction.INSTALL_AND_START;\n-import static org.apache.ambari.server.controller.internal.ProvisionAction.INSTALL_ONLY;\n-import static org.apache.ambari.server.controller.internal.ProvisionAction.START_ONLY;\n-\n /**\n  * Represents a set of requests to a single host such as install, start, etc.\n  */\n@@ -355,9 +355,6 @@ public HostRoleCommand getLogicalTask(long logicalTaskId) {\n     for (HostRoleCommand task : logicalTasks.values()) {\n       HostRoleCommandEntity entity = task.constructNewPersistenceEntity();\n       // the above method doesn't set all of the fields for some unknown reason\n-      entity.setRequestId(task.getRequestId());\n-      entity.setStageId(task.getStageId());\n-      entity.setTaskId(task.getTaskId());\n       entity.setOutputLog(task.getOutputLog());\n       entity.setErrorLog(task.errorLog);\n ",
                "raw_url": "https://github.com/apache/ambari/raw/da5605706555d3aecca7a67fb399d9c501972ad4/ambari-server/src/main/java/org/apache/ambari/server/topology/HostRequest.java",
                "sha": "6a65b48a50d2195c2f9ebd5e46949058052ad2c2",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/ambari/blob/da5605706555d3aecca7a67fb399d9c501972ad4/ambari-server/src/test/java/org/apache/ambari/server/actionmanager/TestActionDBAccessorImpl.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/actionmanager/TestActionDBAccessorImpl.java?ref=da5605706555d3aecca7a67fb399d9c501972ad4",
                "deletions": 0,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/actionmanager/TestActionDBAccessorImpl.java",
                "patch": "@@ -27,6 +27,7 @@\n import java.util.List;\n \n import javax.persistence.EntityManager;\n+import javax.persistence.NamedQuery;\n \n import org.apache.ambari.server.AmbariException;\n import org.apache.ambari.server.Role;\n@@ -611,6 +612,34 @@ public void testAbortRequest() throws AmbariException {\n \n   }\n \n+  /**\n+   * Tests that entities created int he {@link ActionDBAccessor} can be\n+   * retrieved with their IDs intact. EclipseLink seems to execute the\n+   * {@link NamedQuery} but then use its cached entities to fill in the data.\n+   *\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testEntitiesCreatedWithIDs() throws Exception {\n+    List<Stage> stages = new ArrayList<Stage>();\n+    Stage stage = createStubStage(hostName, requestId, stageId);\n+\n+    stages.add(stage);\n+\n+    Request request = new Request(stages, clusters);\n+\n+    // persist entities\n+    db.persistActions(request);\n+\n+    // query entities immediately to ensure IDs are populated\n+    List<HostRoleCommandEntity> commandEntities = hostRoleCommandDAO.findByRequest(requestId);\n+    Assert.assertEquals(2, commandEntities.size());\n+\n+    for (HostRoleCommandEntity entity : commandEntities) {\n+      Assert.assertEquals(Long.valueOf(requestId), entity.getRequestId());\n+      Assert.assertEquals(Long.valueOf(stageId), entity.getStageId());\n+    }\n+  }\n \n   private static class TestActionDBAccessorModule extends AbstractModule {\n     @Override",
                "raw_url": "https://github.com/apache/ambari/raw/da5605706555d3aecca7a67fb399d9c501972ad4/ambari-server/src/test/java/org/apache/ambari/server/actionmanager/TestActionDBAccessorImpl.java",
                "sha": "bf9d0db464429821dda4b3d1a0bba1e2b14014dc",
                "status": "modified"
            }
        ],
        "message": "AMBARI-18404 - Upgrade Summary Endpoint Throws NPEs Due To JPA Cached Entities With Missing IDs (jonathanhurley)",
        "parent": "https://github.com/apache/ambari/commit/1efc99c5adc536044425b9df45b72421c5a8323c",
        "repo": "ambari",
        "unit_tests": [
            "TestActionDBAccessorImpl.java",
            "HostRoleCommandEntityTest.java"
        ]
    },
    "ambari_dfb7f03": {
        "bug_id": "ambari_dfb7f03",
        "commit": "https://github.com/apache/ambari/commit/dfb7f03927aee48e81597a1dad0b270067c68563",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/ambari/blob/dfb7f03927aee48e81597a1dad0b270067c68563/AMBARI-666-CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/AMBARI-666-CHANGES.txt?ref=dfb7f03927aee48e81597a1dad0b270067c68563",
                "deletions": 0,
                "filename": "AMBARI-666-CHANGES.txt",
                "patch": "@@ -12,6 +12,9 @@ AMBARI-666 branch (unreleased changes)\n \n   NEW FEATURES\n \n+  AMBARI-801. Fix heartbeat message from the agent which is causing NPE at the\n+  server. (mahadev)\n+\n   AMBARI-778. Ensure data flows across all steps in installer wizard.\n   (Jaimin Jetly via yusaku)\n ",
                "raw_url": "https://github.com/apache/ambari/raw/dfb7f03927aee48e81597a1dad0b270067c68563/AMBARI-666-CHANGES.txt",
                "sha": "4ccfd6238b92c5c35bcdc33f8f48d5c23cce5d7d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/dfb7f03927aee48e81597a1dad0b270067c68563/ambari-agent/src/main/python/ambari_agent/Controller.py",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/Controller.py?ref=dfb7f03927aee48e81597a1dad0b270067c68563",
                "deletions": 1,
                "filename": "ambari-agent/src/main/python/ambari_agent/Controller.py",
                "patch": "@@ -107,7 +107,8 @@ def heartbeatWithServer(self):\n       try:\n         if retry==False:\n           data = json.dumps(self.heartbeat.build(id))\n-\n+          pass\n+        logger.info(\"Sending HeartBeat \" + pprint.pformat(data))\n         req = urllib2.Request(self.heartbeatUrl, data, {'Content-Type': 'application/json'})\n         \n         logger.info(data)",
                "raw_url": "https://github.com/apache/ambari/raw/dfb7f03927aee48e81597a1dad0b270067c68563/ambari-agent/src/main/python/ambari_agent/Controller.py",
                "sha": "445399cc7340282bfaf8737dc207bf23cf892e3a",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/ambari/blob/dfb7f03927aee48e81597a1dad0b270067c68563/ambari-agent/src/main/python/ambari_agent/Heartbeat.py",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-agent/src/main/python/ambari_agent/Heartbeat.py?ref=dfb7f03927aee48e81597a1dad0b270067c68563",
                "deletions": 2,
                "filename": "ambari-agent/src/main/python/ambari_agent/Heartbeat.py",
                "patch": "@@ -39,11 +39,15 @@ def build(self, id='-1'):\n     timestamp = int(time.time()*1000)\n     queueResult = self.actionQueue.result()\n     installedRoleStates = serverStatus.build()\n+    nodeStatus = { \"status\" : \"HEALTHY\",\n+                   \"cause\" : \"NONE\"}\n+    \n     heartbeat = { 'responseId'        : int(id),\n                   'timestamp'         : timestamp,\n                   'hostname'          : socket.gethostname(),\n-                  'reports'           : self.reports,\n-                  'componentStatus'   : self.componentStatus\n+                 # 'reports'           : self.reports,\n+                 # 'componentStatus'   : self.componentStatus,\n+                  'nodeStatus'        : nodeStatus\n                 }\n   \n     ",
                "raw_url": "https://github.com/apache/ambari/raw/dfb7f03927aee48e81597a1dad0b270067c68563/ambari-agent/src/main/python/ambari_agent/Heartbeat.py",
                "sha": "e9215f766fa187c951ee8b0e9937cf8debc83bf6",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/ambari/blob/dfb7f03927aee48e81597a1dad0b270067c68563/ambari-server/src/main/java/org/apache/ambari/server/actionmanager/ActionManager.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/actionmanager/ActionManager.java?ref=dfb7f03927aee48e81597a1dad0b270067c68563",
                "deletions": 1,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/actionmanager/ActionManager.java",
                "patch": "@@ -21,8 +21,13 @@\n \n import org.apache.ambari.server.agent.ActionQueue;\n import org.apache.ambari.server.agent.CommandReport;\n+import org.apache.ambari.server.agent.rest.AgentResource;\n import org.apache.ambari.server.state.live.Clusters;\n import org.apache.ambari.server.utils.StageUtils;\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n import com.google.inject.Inject;\n import com.google.inject.Singleton;\n@@ -38,7 +43,8 @@\n   private final ActionDBAccessor db;\n   private final ActionQueue actionQueue;\n   private final Clusters fsm;\n-\n+  private static Logger LOG = LoggerFactory.getLogger(ActionManager.class);\n+  \n   @Inject\n   public ActionManager(@Named(\"schedulerSleeptime\") long schedulerSleepTime,\n       @Named(\"actionTimeout\") long actionTimeout,",
                "raw_url": "https://github.com/apache/ambari/raw/dfb7f03927aee48e81597a1dad0b270067c68563/ambari-server/src/main/java/org/apache/ambari/server/actionmanager/ActionManager.java",
                "sha": "baa3df8fe89260b0fd093c6416863b726f005cad",
                "status": "modified"
            }
        ],
        "message": "AMBARI-801. Fix heartbeat message from the agent which is causing NPE at the server. (mahadev)\n\ngit-svn-id: https://svn.apache.org/repos/asf/incubator/ambari/branches/AMBARI-666@1393810 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/ambari/commit/ff66974b8912a037662212052cbd05b96c11f6e3",
        "repo": "ambari",
        "unit_tests": [
            "TestController.java",
            "TestActionManager.java"
        ]
    },
    "ambari_e3f6ec7": {
        "bug_id": "ambari_e3f6ec7",
        "commit": "https://github.com/apache/ambari/commit/e3f6ec7a1f2876007ecd6b4a30ba25c561e2729e",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/ambari/blob/e3f6ec7a1f2876007ecd6b4a30ba25c561e2729e/ambari-server/src/main/java/org/apache/ambari/server/state/PropertyInfo.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/state/PropertyInfo.java?ref=e3f6ec7a1f2876007ecd6b4a30ba25c561e2729e",
                "deletions": 1,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/state/PropertyInfo.java",
                "patch": "@@ -89,7 +89,15 @@ void afterUnmarshal(Unmarshaller unmarshaller, Object parent) {\n   }\n \n   public PropertyInfo() {\n-\n+    propertyStackUpgradeBehavior = new PropertyUpgradeBehavior();\n+    propertyStackUpgradeBehavior.setAdd(true);\n+    propertyStackUpgradeBehavior.setChange(true);\n+    propertyStackUpgradeBehavior.setDelete(false);\n+\n+    propertyAmbariUpgradeBehavior = new PropertyUpgradeBehavior();\n+    propertyAmbariUpgradeBehavior.setAdd(false);\n+    propertyAmbariUpgradeBehavior.setChange(true);\n+    propertyAmbariUpgradeBehavior.setDelete(true);\n   }\n \n   public String getName() {",
                "raw_url": "https://github.com/apache/ambari/raw/e3f6ec7a1f2876007ecd6b4a30ba25c561e2729e/ambari-server/src/main/java/org/apache/ambari/server/state/PropertyInfo.java",
                "sha": "fba2daa5a0fb8c14e2959b48d665772c36324d47",
                "status": "modified"
            },
            {
                "additions": 25,
                "blob_url": "https://github.com/apache/ambari/blob/e3f6ec7a1f2876007ecd6b4a30ba25c561e2729e/ambari-server/src/test/java/org/apache/ambari/server/state/PropertyInfoTest.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/state/PropertyInfoTest.java?ref=e3f6ec7a1f2876007ecd6b4a30ba25c561e2729e",
                "deletions": 2,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/state/PropertyInfoTest.java",
                "patch": "@@ -103,14 +103,37 @@ public void testUpgradeBehaviorTag() throws JAXBException {\n       \"<property>\\n\" +\n       \"  <name>prop_name</name>\\n\" +\n       \"  <value>prop_val</value>\\n\" +\n-      \"  <on-ambari-upgrade add=\\\"false\\\" change=\\\"true\\\" delete=\\\"true\\\"/>\\n\" +\n-      \"  <on-stack-upgrade add=\\\"true\\\" change=\\\"true\\\" delete=\\\"false\\\"/>\\n\" +\n+      \"  <on-ambari-upgrade add=\\\"true\\\" change=\\\"true\\\" delete=\\\"true\\\"/>\\n\" +\n+      \"  <on-stack-upgrade add=\\\"false\\\" change=\\\"false\\\" delete=\\\"false\\\"/>\\n\" +\n       \"</property>\";\n \n     // when\n     PropertyInfo propertyInfo = propertyInfoFrom(xml);\n \n     // then\n+    assertTrue(propertyInfo.getPropertyAmbariUpgradeBehavior().isAdd());\n+    assertTrue(propertyInfo.getPropertyAmbariUpgradeBehavior().isChange());\n+    assertTrue(propertyInfo.getPropertyAmbariUpgradeBehavior().isDelete());\n+\n+    assertFalse(propertyInfo.getPropertyStackUpgradeBehavior().isAdd());\n+    assertFalse(propertyInfo.getPropertyStackUpgradeBehavior().isChange());\n+    assertFalse(propertyInfo.getPropertyStackUpgradeBehavior().isDelete());\n+  }\n+\n+  @Test\n+  public void testBehaviorWithoutUpgradeTags() throws JAXBException {\n+    // given\n+    String xml =\n+        \"<property>\\n\" +\n+            \"  <name>prop_name</name>\\n\" +\n+            \"  <value>prop_val</value>\\n\" +\n+            \"</property>\";\n+\n+    // when\n+    PropertyInfo propertyInfo = propertyInfoFrom(xml);\n+\n+    // then\n+\n     assertFalse(propertyInfo.getPropertyAmbariUpgradeBehavior().isAdd());\n     assertTrue(propertyInfo.getPropertyAmbariUpgradeBehavior().isChange());\n     assertTrue(propertyInfo.getPropertyAmbariUpgradeBehavior().isDelete());",
                "raw_url": "https://github.com/apache/ambari/raw/e3f6ec7a1f2876007ecd6b4a30ba25c561e2729e/ambari-server/src/test/java/org/apache/ambari/server/state/PropertyInfoTest.java",
                "sha": "9a3d1952a0d342ba1fd90a340cf73036d8e68311",
                "status": "modified"
            }
        ],
        "message": "AMBARI-17026 NPE during EU at Update Target Stack step (dlysnichenko)",
        "parent": "https://github.com/apache/ambari/commit/7038957df73bcc239ad156a3041a009826bd9900",
        "repo": "ambari",
        "unit_tests": [
            "PropertyInfoTest.java"
        ]
    },
    "ambari_e73e783": {
        "bug_id": "ambari_e73e783",
        "commit": "https://github.com/apache/ambari/commit/e73e783a8b5377b809a829c362900d3cad15d69f",
        "file": [
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/ambari/blob/e73e783a8b5377b809a829c362900d3cad15d69f/ambari-server/src/main/java/org/apache/ambari/server/security/authorization/AmbariLdapAuthenticationProvider.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/security/authorization/AmbariLdapAuthenticationProvider.java?ref=e73e783a8b5377b809a829c362900d3cad15d69f",
                "deletions": 10,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/security/authorization/AmbariLdapAuthenticationProvider.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -24,7 +24,6 @@\n import org.apache.ambari.server.orm.dao.UserDAO;\n import org.apache.ambari.server.orm.entities.UserEntity;\n import org.apache.ambari.server.security.ClientSecurityType;\n-import org.apache.commons.lang.StringUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import org.springframework.dao.IncorrectResultSizeDataAccessException;\n@@ -72,17 +71,21 @@ public Authentication authenticate(Authentication authentication) throws Authent\n \n         return new AmbariAuthentication(auth, userId);\n       } catch (AuthenticationException e) {\n-        LOG.debug(\"Got exception during LDAP authentification attempt\", e);\n+        LOG.debug(\"Got exception during LDAP authentication attempt\", e);\n         // Try to help in troubleshooting\n         Throwable cause = e.getCause();\n-        if (cause != null) {\n-          // Below we check the cause of an AuthenticationException . If it is\n-          // caused by another AuthenticationException, than probably\n-          // the problem is with LDAP ManagerDN/password\n-          if ((cause != e) && (cause instanceof\n-                  org.springframework.ldap.AuthenticationException)) {\n+        if ((cause != null) && (cause != e)) {\n+          // Below we check the cause of an AuthenticationException to see what the actual cause is\n+          // and then send an appropriate message to the caller.\n+          if (cause instanceof org.springframework.ldap.CommunicationException) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.warn(\"Failed to communicate with the LDAP server: \" + cause.getMessage(), e);\n+            } else {\n+              LOG.warn(\"Failed to communicate with the LDAP server: \" + cause.getMessage());\n+            }\n+          } else if (cause instanceof org.springframework.ldap.AuthenticationException) {\n             LOG.warn(\"Looks like LDAP manager credentials (that are used for \" +\n-                    \"connecting to LDAP server) are invalid.\", e);\n+                \"connecting to LDAP server) are invalid.\", e);\n           }\n         }\n         throw new InvalidUsernamePasswordCombinationException(e);",
                "raw_url": "https://github.com/apache/ambari/raw/e73e783a8b5377b809a829c362900d3cad15d69f/ambari-server/src/main/java/org/apache/ambari/server/security/authorization/AmbariLdapAuthenticationProvider.java",
                "sha": "b5776a37f90f4652fd3620f76aa80e3d96545f94",
                "status": "modified"
            },
            {
                "additions": 205,
                "blob_url": "https://github.com/apache/ambari/blob/e73e783a8b5377b809a829c362900d3cad15d69f/ambari-server/src/main/java/org/apache/ambari/server/security/authorization/AmbariLdapBindAuthenticator.java",
                "changes": 233,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/security/authorization/AmbariLdapBindAuthenticator.java?ref=e73e783a8b5377b809a829c362900d3cad15d69f",
                "deletions": 28,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/security/authorization/AmbariLdapBindAuthenticator.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -20,26 +20,36 @@\n \n import java.util.List;\n \n+import javax.naming.NamingEnumeration;\n import javax.naming.NamingException;\n import javax.naming.directory.Attributes;\n+import javax.naming.directory.DirContext;\n \n import org.apache.ambari.server.configuration.Configuration;\n import org.apache.commons.lang.StringUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import org.springframework.ldap.core.AttributesMapper;\n+import org.springframework.ldap.core.ContextSource;\n+import org.springframework.ldap.core.DirContextAdapter;\n import org.springframework.ldap.core.DirContextOperations;\n+import org.springframework.ldap.core.DistinguishedName;\n import org.springframework.ldap.core.LdapTemplate;\n import org.springframework.ldap.core.support.BaseLdapPathContextSource;\n+import org.springframework.ldap.support.LdapUtils;\n+import org.springframework.security.authentication.BadCredentialsException;\n+import org.springframework.security.authentication.InternalAuthenticationServiceException;\n+import org.springframework.security.authentication.UsernamePasswordAuthenticationToken;\n import org.springframework.security.core.Authentication;\n-import org.springframework.security.ldap.authentication.BindAuthenticator;\n+import org.springframework.security.ldap.authentication.AbstractLdapAuthenticator;\n+import org.springframework.security.ldap.search.LdapUserSearch;\n \n \n /**\n  * An authenticator which binds as a user and checks if user should get ambari\n  * admin authorities according to LDAP group membership\n  */\n-public class AmbariLdapBindAuthenticator extends BindAuthenticator {\n+public class AmbariLdapBindAuthenticator extends AbstractLdapAuthenticator {\n   private static final Logger LOG = LoggerFactory.getLogger(AmbariLdapBindAuthenticator.class);\n \n   private Configuration configuration;\n@@ -55,19 +65,28 @@ public AmbariLdapBindAuthenticator(BaseLdapPathContextSource contextSource,\n   @Override\n   public DirContextOperations authenticate(Authentication authentication) {\n \n-    DirContextOperations user = super.authenticate(authentication);\n-    LdapServerProperties ldapServerProperties =\n-      configuration.getLdapServerProperties();\n+    if (!(authentication instanceof UsernamePasswordAuthenticationToken)) {\n+      LOG.info(\"Unexpected authentication token type encountered ({}) - failing authentication.\", authentication.getClass().getName());\n+      throw new BadCredentialsException(\"Unexpected authentication token type encountered.\");\n+    }\n+\n+    DirContextOperations user = authenticate((UsernamePasswordAuthenticationToken) authentication);\n+\n+    LdapServerProperties ldapServerProperties = configuration.getLdapServerProperties();\n     if (StringUtils.isNotEmpty(ldapServerProperties.getAdminGroupMappingRules())) {\n       setAmbariAdminAttr(user, ldapServerProperties);\n     }\n \n     // Users stored locally in ambari are matched against LDAP users by the ldap attribute configured to be used as user name.\n     // (e.g. uid, sAMAccount -> ambari user name )\n     String ldapUserName = user.getStringAttribute(ldapServerProperties.getUsernameAttribute());\n-    String loginName  = authentication.getName(); // user login name the user has logged in\n+    String loginName = authentication.getName(); // user login name the user has logged in\n \n-    if (!ldapUserName.equals(loginName)) {\n+    if (ldapUserName == null) {\n+      LOG.warn(\"The user data does not contain a value for {}.\", ldapServerProperties.getUsernameAttribute());\n+    } else if (ldapUserName.isEmpty()) {\n+      LOG.warn(\"The user data contains an empty value for {}.\", ldapServerProperties.getUsernameAttribute());\n+    } else if (!ldapUserName.equals(loginName)) {\n       // if authenticated user name is different from ldap user name than user has logged in\n       // with a login name that is different (e.g. user principal name) from the ambari user name stored in\n       // ambari db. In this case add the user login name  as login alias for ambari user name.\n@@ -76,11 +95,10 @@ public DirContextOperations authenticate(Authentication authentication) {\n       // If the ldap username needs to be processed (like converted to all lowercase characters),\n       // process it before setting it in the session via AuthorizationHelper#addLoginNameAlias\n       String processedLdapUserName;\n-      if(ldapServerProperties.isForceUsernameToLowercase()) {\n+      if (ldapServerProperties.isForceUsernameToLowercase()) {\n         processedLdapUserName = ldapUserName.toLowerCase();\n         LOG.info(\"Forcing ldap username to be lowercase characters: {} ==> {}\", ldapUserName, processedLdapUserName);\n-      }\n-      else {\n+      } else {\n         processedLdapUserName = ldapUserName;\n       }\n \n@@ -91,10 +109,169 @@ public DirContextOperations authenticate(Authentication authentication) {\n   }\n \n   /**\n-   *  Checks weather user is a member of ambari administrators group in LDAP. If\n-   *  yes, sets user's ambari_admin attribute to true\n-   * @param user\n-   * @return\n+   * Authenticates a user with a configured LDAP server using the user's username and password.\n+   * <p>\n+   * To authenticate a user:\n+   * <ol>\n+   * <li>\n+   * The LDAP server is queried for the relevant user object where the\n+   * supplied username matches the configured LDAP attribute that represents the user's username\n+   * <ul><li>Example: (&(uid=user1)(objectClass=posixAccount))</li></ul>\n+   * </li>\n+   * <li>\n+   * If found, the distinguished name (DN) of the user object is obtained from returned data and then\n+   * used, along with the supplied password to perform an LDAP bind (see {@link #bind(DirContextOperations, String)})\n+   * </li>\n+   * </ol>\n+   * <p>\n+   * Failure to authenticate will result in a {@link BadCredentialsException} to be thrown.\n+   *\n+   * @param authentication the credentials to use for authentication\n+   * @return the authenticated user details\n+   * @see #bind(DirContextOperations, String)\n+   */\n+  private DirContextOperations authenticate(UsernamePasswordAuthenticationToken authentication) {\n+    DirContextOperations user = null;\n+\n+    String username = authentication.getName();\n+    Object credentials = authentication.getCredentials();\n+    String password = (credentials instanceof String) ? (String) credentials : null;\n+\n+    if (StringUtils.isEmpty(username)) {\n+      LOG.debug(\"Empty username encountered - failing authentication.\");\n+      throw new BadCredentialsException(\"Empty username encountered.\");\n+    }\n+\n+    LOG.debug(\"Authenticating {}\", username);\n+\n+    if (StringUtils.isEmpty(password)) {\n+      LOG.debug(\"Empty password encountered - failing authentication.\");\n+      throw new BadCredentialsException(\"Empty password encountered.\");\n+    }\n+\n+    LdapUserSearch userSearch = getUserSearch();\n+    if (userSearch == null) {\n+      LOG.debug(\"The user search facility has not been set - failing authentication.\");\n+      throw new BadCredentialsException(\"The user search facility has not been set.\");\n+    } else {\n+      if (LOG.isTraceEnabled()) {\n+        LOG.trace(\"Searching for user with username {}: {}\", username, userSearch.toString());\n+      }\n+\n+      // Find the user data where the supplied username matches the value of the configured LDAP\n+      // attribute for the user's username. If a user is found, use the DN fro the returned data\n+      // and the supplied password to attempt authentication.\n+      DirContextOperations userFromSearch = userSearch.searchForUser(username);\n+\n+      if (userFromSearch == null) {\n+        LOG.debug(\"LDAP user object not found for {}\", username);\n+      } else {\n+        LOG.debug(\"Found LDAP user for {}: {}\", username, userFromSearch.getDn());\n+        user = bind(userFromSearch, password);\n+\n+        // If trace enabled, log the user's LDAP attributes.\n+        if (LOG.isTraceEnabled()) {\n+          Attributes attributes = user.getAttributes();\n+          if (attributes != null) {\n+            StringBuilder builder = new StringBuilder();\n+            NamingEnumeration<String> ids = attributes.getIDs();\n+            try {\n+              while (ids.hasMore()) {\n+                String id = ids.next();\n+                builder.append(\"\\n\\t\");\n+                builder.append(attributes.get(id));\n+              }\n+            } catch (NamingException e) {\n+              // Ignore this...\n+            }\n+            LOG.trace(\"User Attributes: {}\", builder);\n+          } else {\n+            LOG.trace(\"User Attributes: not available\");\n+          }\n+        }\n+      }\n+    }\n+\n+    // If a user was not authenticated, thrown a BadCredentialsException, else return the user data\n+    if (user == null) {\n+      LOG.debug(\"Invalid credentials for {} - failing authentication.\", username);\n+      throw new BadCredentialsException(\"Invalid credentials.\");\n+    } else {\n+      LOG.debug(\"Successfully authenticated {}\", username);\n+    }\n+\n+    return user;\n+  }\n+\n+  /**\n+   * Attempt to authenticate a user with the configured LDAP server by performing an LDAP bind.\n+   * <p>\n+   * Using the distinguished name provided in the supplied user data and the supplied password,\n+   * attempt to authenticate with the configured LDAP server. If authentication is successful, use the\n+   * attributes from the supplied user data rather than the attributes associated with the bound context\n+   * because some scenarios result in missing data within the bound context due to LDAP server implementations.\n+   * <p>\n+   * If authentication is not successful, throw a {@link BadCredentialsException}.\n+   *\n+   * @param user     the user data containing the relevant DN and associated attributes\n+   * @param password the password\n+   * @return the authenticated user details\n+   * @throws BadCredentialsException if authentication fails\n+   */\n+  private DirContextOperations bind(DirContextOperations user, String password) {\n+    ContextSource contextSource = getContextSource();\n+\n+    if (contextSource == null) {\n+      String message = \"Missing ContextSource - failing authentication.\";\n+      LOG.debug(message);\n+      throw new InternalAuthenticationServiceException(message);\n+    }\n+\n+    if (!(contextSource instanceof BaseLdapPathContextSource)) {\n+      String message = String.format(\"Unexpected ContextSource type (%s) - failing authentication.\", contextSource.getClass().getName());\n+      LOG.debug(message);\n+      throw new InternalAuthenticationServiceException(message);\n+    }\n+\n+    BaseLdapPathContextSource baseLdapPathContextSource = (BaseLdapPathContextSource) contextSource;\n+    DistinguishedName userDistinguishedName = new DistinguishedName(user.getDn());\n+    DistinguishedName fullDn = new DistinguishedName(userDistinguishedName);\n+    fullDn.prepend(baseLdapPathContextSource.getBaseLdapPath());\n+\n+    LOG.debug(\"Attempting to bind as {}\", fullDn);\n+\n+    DirContext dirContext = null;\n+\n+    try {\n+      // Perform the authentication.  The result is not used because it is expected that the supplied\n+      // user data has all of the attributes for the authenticated user. If authentication fails, it\n+      // expected that the supplied user data will be destroyed or orphaned.\n+      dirContext = baseLdapPathContextSource.getContext(fullDn.toString(), password);\n+\n+      // Build a new DirContextAdapter using the attributes from the passed in user details since it\n+      // is expected these details will be more complete of querying for them from the bound context.\n+      // Some LDAP server implementations will no return all attributes to the bound context due to\n+      // the filter being used in the query.\n+      return new DirContextAdapter(user.getAttributes(), userDistinguishedName, baseLdapPathContextSource.getBaseLdapPath());\n+    } catch (org.springframework.ldap.AuthenticationException e) {\n+      String message = String.format(\"Failed to bind as %s - %s\", user.getDn().toString(), e.getMessage());\n+      if (LOG.isTraceEnabled()) {\n+        LOG.trace(message, e);\n+      } else if (LOG.isDebugEnabled()) {\n+        LOG.debug(message);\n+      }\n+      throw new BadCredentialsException(\"The username or password is incorrect.\");\n+    } finally {\n+      LdapUtils.closeContext(dirContext);\n+    }\n+  }\n+\n+  /**\n+   * Checks weather user is a member of ambari administrators group in LDAP. If\n+   * yes, sets user's ambari_admin attribute to true\n+   *\n+   * @param user the user details\n+   * @return the updated user details\n    */\n   private DirContextOperations setAmbariAdminAttr(DirContextOperations user, LdapServerProperties ldapServerProperties) {\n     String baseDn = ldapServerProperties.getBaseDN().toLowerCase();\n@@ -105,10 +282,10 @@ private DirContextOperations setAmbariAdminAttr(DirContextOperations user, LdapS\n \n     //If groupBase is set incorrectly or isn't set - search in BaseDn\n     int indexOfBaseDn = groupBase.indexOf(baseDn);\n-    groupBase = indexOfBaseDn <= 0 ? \"\" : groupBase.substring(0,indexOfBaseDn - 1);\n+    groupBase = indexOfBaseDn <= 0 ? \"\" : groupBase.substring(0, indexOfBaseDn - 1);\n \n     String memberValue = StringUtils.isNotEmpty(adminGroupMappingMemberAttr)\n-      ? user.getStringAttribute(adminGroupMappingMemberAttr) : user.getNameInNamespace();\n+        ? user.getStringAttribute(adminGroupMappingMemberAttr) : user.getNameInNamespace();\n     LOG.debug(\"LDAP login - set '{}' as member attribute for adminGroupMappingRules\", memberValue);\n \n     String setAmbariAdminAttrFilter = resolveAmbariAdminAttrFilter(ldapServerProperties, memberValue);\n@@ -125,8 +302,8 @@ public Object mapFromAttributes(Attributes attrs)\n     ldapTemplate.setIgnorePartialResultException(true);\n     ldapTemplate.setIgnoreNameNotFoundException(true);\n \n-    List<String> ambariAdminGroups = ldapTemplate.search(\n-        groupBase, setAmbariAdminAttrFilter, attributesMapper);\n+    @SuppressWarnings(\"unchecked\")\n+    List<String> ambariAdminGroups = ldapTemplate.search(groupBase, setAmbariAdminAttrFilter, attributesMapper);\n \n     //user has admin role granted, if user is a member of at least 1 group,\n     // which matches the rules in configuration\n@@ -141,24 +318,24 @@ private String resolveAmbariAdminAttrFilter(LdapServerProperties ldapServerPrope\n     String groupMembershipAttr = ldapServerProperties.getGroupMembershipAttr();\n     String groupObjectClass = ldapServerProperties.getGroupObjectClass();\n     String adminGroupMappingRules =\n-      ldapServerProperties.getAdminGroupMappingRules();\n+        ldapServerProperties.getAdminGroupMappingRules();\n     final String groupNamingAttribute =\n-      ldapServerProperties.getGroupNamingAttr();\n+        ldapServerProperties.getGroupNamingAttr();\n     String groupSearchFilter = ldapServerProperties.getGroupSearchFilter();\n \n     String setAmbariAdminAttrFilter;\n     if (StringUtils.isEmpty(groupSearchFilter)) {\n       String adminGroupMappingRegex = createAdminGroupMappingRegex(adminGroupMappingRules, groupNamingAttribute);\n       setAmbariAdminAttrFilter = String.format(\"(&(%s=%s)(objectclass=%s)(|%s))\",\n-        groupMembershipAttr,\n-        memberValue,\n-        groupObjectClass,\n-        adminGroupMappingRegex);\n+          groupMembershipAttr,\n+          memberValue,\n+          groupObjectClass,\n+          adminGroupMappingRegex);\n     } else {\n       setAmbariAdminAttrFilter = String.format(\"(&(%s=%s)%s)\",\n-        groupMembershipAttr,\n-        memberValue,\n-        groupSearchFilter);\n+          groupMembershipAttr,\n+          memberValue,\n+          groupSearchFilter);\n     }\n     return setAmbariAdminAttrFilter;\n   }",
                "raw_url": "https://github.com/apache/ambari/raw/e73e783a8b5377b809a829c362900d3cad15d69f/ambari-server/src/main/java/org/apache/ambari/server/security/authorization/AmbariLdapBindAuthenticator.java",
                "sha": "b4ef889252080f884ae6e7aca791d511d28f2ebd",
                "status": "modified"
            },
            {
                "additions": 136,
                "blob_url": "https://github.com/apache/ambari/blob/e73e783a8b5377b809a829c362900d3cad15d69f/ambari-server/src/test/java/org/apache/ambari/server/security/authorization/AmbariLdapBindAuthenticatorTest.java",
                "changes": 226,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/security/authorization/AmbariLdapBindAuthenticatorTest.java?ref=e73e783a8b5377b809a829c362900d3cad15d69f",
                "deletions": 90,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/security/authorization/AmbariLdapBindAuthenticatorTest.java",
                "patch": "@@ -1,5 +1,4 @@\n-\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -18,132 +17,179 @@\n  */\n package org.apache.ambari.server.security.authorization;\n \n-import static org.easymock.EasyMock.eq;\n-import static org.easymock.EasyMock.expectLastCall;\n-import static org.junit.Assert.assertEquals;\n-\n+import javax.naming.NamingEnumeration;\n+import javax.naming.directory.Attribute;\n+import javax.naming.directory.Attributes;\n+import javax.naming.directory.SearchControls;\n+import javax.naming.directory.SearchResult;\n+import javax.naming.ldap.LdapName;\n import java.util.Properties;\n-\n import org.apache.ambari.server.configuration.Configuration;\n-import org.apache.directory.server.annotations.CreateLdapServer;\n-import org.apache.directory.server.annotations.CreateTransport;\n-import org.apache.directory.server.core.annotations.ApplyLdifFiles;\n-import org.apache.directory.server.core.annotations.ContextEntry;\n-import org.apache.directory.server.core.annotations.CreateDS;\n-import org.apache.directory.server.core.annotations.CreatePartition;\n-import org.apache.directory.server.core.integ.FrameworkRunner;\n-import org.easymock.EasyMockRule;\n-import org.easymock.Mock;\n-import org.easymock.MockType;\n-import org.junit.Before;\n-import org.junit.Rule;\n+import org.apache.commons.lang.StringUtils;\n+import org.easymock.EasyMockSupport;\n import org.junit.Test;\n-import org.junit.runner.RunWith;\n import org.springframework.ldap.core.DirContextOperations;\n+import org.springframework.ldap.core.DistinguishedName;\n import org.springframework.ldap.core.support.LdapContextSource;\n import org.springframework.security.authentication.UsernamePasswordAuthenticationToken;\n-import org.springframework.security.core.Authentication;\n import org.springframework.security.ldap.search.FilterBasedLdapUserSearch;\n-import org.springframework.security.ldap.search.LdapUserSearch;\n import org.springframework.web.context.request.RequestAttributes;\n import org.springframework.web.context.request.RequestContextHolder;\n import org.springframework.web.context.request.ServletRequestAttributes;\n \n+import static org.easymock.EasyMock.anyObject;\n+import static org.easymock.EasyMock.anyString;\n+import static org.easymock.EasyMock.eq;\n+import static org.easymock.EasyMock.expect;\n+import static org.easymock.EasyMock.expectLastCall;\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.fail;\n+\n+public class AmbariLdapBindAuthenticatorTest extends EasyMockSupport {\n \n-@RunWith(FrameworkRunner.class)\n-@CreateDS(allowAnonAccess = true,\n-  name = \"AmbariLdapBindAuthenticatorTest\",\n-  partitions = {\n-    @CreatePartition(name = \"Root\",\n-      suffix = \"dc=apache,dc=org\",\n-      contextEntry = @ContextEntry(\n-        entryLdif =\n-          \"dn: dc=apache,dc=org\\n\" +\n-            \"dc: apache\\n\" +\n-            \"objectClass: top\\n\" +\n-            \"objectClass: domain\\n\\n\" +\n-            \"dn: dc=ambari,dc=apache,dc=org\\n\" +\n-            \"dc: ambari\\n\" +\n-            \"objectClass: top\\n\" +\n-            \"objectClass: domain\\n\\n\"))\n-  })\n-@CreateLdapServer(allowAnonymousAccess = true,\n-  transports = {@CreateTransport(protocol = \"LDAP\")})\n-@ApplyLdifFiles(\"users.ldif\")\n-public class AmbariLdapBindAuthenticatorTest extends AmbariLdapAuthenticationProviderBaseTest {\n-\n-  @Rule\n-  public EasyMockRule mocks = new EasyMockRule(this);\n-\n-  @Mock(type = MockType.NICE)\n-  private ServletRequestAttributes servletRequestAttributes;\n-\n-  @Before\n-  public void setUp() {\n-    resetAll();\n+  @Test\n+  public void testAuthenticateWithoutLogin() throws Exception {\n+    testAuthenticate(\"username\", \"username\", false);\n+  }\n+\n+  @Test\n+  public void testAuthenticateWithNullLDAPUsername() throws Exception {\n+    testAuthenticate(\"username\", null, false);\n   }\n \n   @Test\n   public void testAuthenticateWithLoginAliasDefault() throws Exception {\n-    testAuthenticateWithLoginAlias(false);\n+    testAuthenticate(\"username\", \"ldapUsername\", false);\n   }\n \n   @Test\n   public void testAuthenticateWithLoginAliasForceToLower() throws Exception {\n-    testAuthenticateWithLoginAlias(true);\n+    testAuthenticate(\"username\", \"ldapUsername\", true);\n   }\n \n-  private void testAuthenticateWithLoginAlias(boolean forceUsernameToLower) throws Exception {\n-    // Given\n-\n-    LdapContextSource ldapCtxSource = new LdapContextSource();\n-    ldapCtxSource.setUrls(new String[] {\"ldap://localhost:\" + getLdapServer().getPort()});\n-    ldapCtxSource.setBase(\"dc=ambari,dc=apache,dc=org\");\n-    ldapCtxSource.afterPropertiesSet();\n+  @Test\n+  public void testAuthenticateBadPassword() throws Exception {\n+    String basePathString = \"dc=apache,dc=org\";\n+    String ldapUserRelativeDNString = String.format(\"uid=%s,ou=people,ou=dev\", \"ldapUsername\");\n+    LdapName ldapUserRelativeDN = new LdapName(ldapUserRelativeDNString);\n+    String ldapUserDNString = String.format(\"%s,%s\", ldapUserRelativeDNString, basePathString);\n+    DistinguishedName basePath = new DistinguishedName(basePathString);\n+\n+    LdapContextSource ldapCtxSource = createMock(LdapContextSource.class);\n+    expect(ldapCtxSource.getBaseLdapPath())\n+        .andReturn(basePath)\n+        .atLeastOnce();\n+    expect(ldapCtxSource.getContext(ldapUserDNString, \"password\"))\n+        .andThrow(new org.springframework.ldap.AuthenticationException(null))\n+        .once();\n+\n+    DirContextOperations searchedUserContext = createMock(DirContextOperations.class);\n+    expect(searchedUserContext.getDn())\n+        .andReturn(ldapUserRelativeDN)\n+        .atLeastOnce();\n+\n+    FilterBasedLdapUserSearch userSearch = createMock(FilterBasedLdapUserSearch.class);\n+    expect(userSearch.searchForUser(anyString())).andReturn(searchedUserContext).once();\n \n-    Properties properties = new Properties();\n-    properties.setProperty(Configuration.CLIENT_SECURITY.getKey(), \"ldap\");\n-    properties.setProperty(Configuration.SERVER_PERSISTENCE_TYPE.getKey(), \"in-memory\");\n-    properties.setProperty(Configuration.METADATA_DIR_PATH.getKey(),\"src/test/resources/stacks\");\n-    properties.setProperty(Configuration.SERVER_VERSION_FILE.getKey(),\"src/test/resources/version\");\n-    properties.setProperty(Configuration.OS_VERSION.getKey(),\"centos5\");\n-    properties.setProperty(Configuration.SHARED_RESOURCES_DIR.getKey(), \"src/test/resources/\");\n-    properties.setProperty(Configuration.LDAP_BASE_DN.getKey(), \"dc=ambari,dc=apache,dc=org\");\n-\n-    if(forceUsernameToLower) {\n-      properties.setProperty(Configuration.LDAP_USERNAME_FORCE_LOWERCASE.getKey(), \"true\");\n-    }\n+    replayAll();\n \n-    Configuration configuration = new Configuration(properties);\n+    Configuration configuration = new Configuration();\n \n     AmbariLdapBindAuthenticator bindAuthenticator = new AmbariLdapBindAuthenticator(ldapCtxSource, configuration);\n-\n-    LdapUserSearch userSearch = new FilterBasedLdapUserSearch(\"\", \"(&(cn={0})(objectClass=person))\", ldapCtxSource);\n     bindAuthenticator.setUserSearch(userSearch);\n \n-    // JohnSmith is a login alias for deniedUser username\n-    String loginAlias = \"JohnSmith\";\n-    String userName = \"deniedUser\";\n-\n-    Authentication authentication = new UsernamePasswordAuthenticationToken(loginAlias, \"password\");\n+    try {\n+      bindAuthenticator.authenticate(new UsernamePasswordAuthenticationToken(\"username\", \"password\"));\n+      fail(\"Expected thrown exception: org.springframework.security.authentication.BadCredentialsException\");\n+    } catch (org.springframework.security.authentication.BadCredentialsException e) {\n+      // expected\n+    } catch (Throwable t) {\n+      fail(\"Expected thrown exception: org.springframework.security.authentication.BadCredentialsException\\nEncountered thrown exception \" + t.getClass().getName());\n+    }\n \n-    RequestContextHolder.setRequestAttributes(servletRequestAttributes);\n+    verifyAll();\n+  }\n \n-    servletRequestAttributes.setAttribute(eq(loginAlias), eq(forceUsernameToLower ? userName.toLowerCase(): userName), eq(RequestAttributes.SCOPE_SESSION));\n-    expectLastCall().once();\n+  private void testAuthenticate(String ambariUsername, String ldapUsername, boolean forceUsernameToLower) throws Exception {\n+    String basePathString = \"dc=apache,dc=org\";\n+    String ldapUserRelativeDNString = String.format(\"uid=%s,ou=people,ou=dev\", ldapUsername);\n+    LdapName ldapUserRelativeDN = new LdapName(ldapUserRelativeDNString);\n+    String ldapUserDNString = String.format(\"%s,%s\", ldapUserRelativeDNString, basePathString);\n+    DistinguishedName basePath = new DistinguishedName(basePathString);\n+\n+    @SuppressWarnings(\"unchecked\")\n+    NamingEnumeration<SearchResult> adminGroups = createMock(NamingEnumeration.class);\n+    expect(adminGroups.hasMore())\n+        .andReturn(false)\n+        .atLeastOnce();\n+    adminGroups.close();\n+    expectLastCall().atLeastOnce();\n+\n+    DirContextOperations boundUserContext = createMock(DirContextOperations.class);\n+    expect(boundUserContext.search(eq(\"ou=groups\"), eq(\"(&(member=\" + ldapUserDNString + \")(objectclass=group)(|(cn=Ambari Administrators)))\"), anyObject(SearchControls.class)))\n+        .andReturn(adminGroups)\n+        .atLeastOnce();\n+    boundUserContext.close();\n+    expectLastCall().atLeastOnce();\n+\n+\n+    LdapContextSource ldapCtxSource = createMock(LdapContextSource.class);\n+    expect(ldapCtxSource.getBaseLdapPath())\n+        .andReturn(basePath)\n+        .atLeastOnce();\n+    expect(ldapCtxSource.getContext(ldapUserDNString, \"password\"))\n+        .andReturn(boundUserContext)\n+        .once();\n+    expect(ldapCtxSource.getReadOnlyContext())\n+        .andReturn(boundUserContext)\n+        .once();\n+\n+    Attribute uidAttribute = createMock(Attribute.class);\n+    expect(uidAttribute.size())\n+        .andReturn(1)\n+        .atLeastOnce();\n+    expect(uidAttribute.get()).andReturn(ldapUsername).atLeastOnce();\n+\n+    Attributes searchedAttributes = createMock(Attributes.class);\n+    expect(searchedAttributes.get(\"uid\"))\n+        .andReturn(uidAttribute)\n+        .atLeastOnce();\n+\n+    DirContextOperations searchedUserContext = createMock(DirContextOperations.class);\n+    expect(searchedUserContext.getDn())\n+        .andReturn(ldapUserRelativeDN)\n+        .atLeastOnce();\n+    expect(searchedUserContext.getAttributes())\n+        .andReturn(searchedAttributes)\n+        .atLeastOnce();\n+\n+    FilterBasedLdapUserSearch userSearch = createMock(FilterBasedLdapUserSearch.class);\n+    expect(userSearch.searchForUser(ambariUsername)).andReturn(searchedUserContext).once();\n+\n+    ServletRequestAttributes servletRequestAttributes = createMock(ServletRequestAttributes.class);\n+\n+    if (!StringUtils.isEmpty(ldapUsername) && !ambariUsername.equals(ldapUsername)) {\n+      servletRequestAttributes.setAttribute(eq(ambariUsername), eq(forceUsernameToLower ? ldapUsername.toLowerCase() : ldapUsername), eq(RequestAttributes.SCOPE_SESSION));\n+      expectLastCall().once();\n+    }\n \n     replayAll();\n \n-    // When\n+    RequestContextHolder.setRequestAttributes(servletRequestAttributes);\n \n-    DirContextOperations user = bindAuthenticator.authenticate(authentication);\n+    Properties properties = new Properties();\n+    if (forceUsernameToLower) {\n+      properties.setProperty(Configuration.LDAP_USERNAME_FORCE_LOWERCASE.getKey(), \"true\");\n+    }\n+    Configuration configuration = new Configuration(properties);\n \n-    // Then\n+    AmbariLdapBindAuthenticator bindAuthenticator = new AmbariLdapBindAuthenticator(ldapCtxSource, configuration);\n+    bindAuthenticator.setUserSearch(userSearch);\n+    DirContextOperations user = bindAuthenticator.authenticate(new UsernamePasswordAuthenticationToken(ambariUsername, \"password\"));\n \n     verifyAll();\n \n     String ldapUserNameAttribute = configuration.getLdapServerProperties().getUsernameAttribute();\n-\n-    assertEquals(userName, user.getStringAttribute(ldapUserNameAttribute));\n+    assertEquals(ldapUsername, user.getStringAttribute(ldapUserNameAttribute));\n   }\n }",
                "raw_url": "https://github.com/apache/ambari/raw/e73e783a8b5377b809a829c362900d3cad15d69f/ambari-server/src/test/java/org/apache/ambari/server/security/authorization/AmbariLdapBindAuthenticatorTest.java",
                "sha": "aed6b57f0722a8c9b18fe26c688e038218c163d4",
                "status": "modified"
            }
        ],
        "message": "AMBARI-18938.  NPE when authenticating via a Centrify LDAP proxy (rlevas)",
        "parent": "https://github.com/apache/ambari/commit/48715331207b73978803aa666209b84c26224fde",
        "repo": "ambari",
        "unit_tests": [
            "AmbariLdapAuthenticationProviderTest.java",
            "AmbariLdapBindAuthenticatorTest.java"
        ]
    },
    "ambari_eec3548": {
        "bug_id": "ambari_eec3548",
        "commit": "https://github.com/apache/ambari/commit/eec3548f10e8bceea9127f8cd641e4d77ed2474a",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/eec3548f10e8bceea9127f8cd641e4d77ed2474a/ambari-server/src/main/java/org/apache/ambari/server/view/configuration/InstanceConfig.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/view/configuration/InstanceConfig.java?ref=eec3548f10e8bceea9127f8cd641e4d77ed2474a",
                "deletions": 1,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/view/configuration/InstanceConfig.java",
                "patch": "@@ -21,6 +21,7 @@\n import javax.xml.bind.annotation.XmlAccessType;\n import javax.xml.bind.annotation.XmlAccessorType;\n import javax.xml.bind.annotation.XmlElement;\n+import java.util.Collections;\n import java.util.List;\n \n /**\n@@ -54,6 +55,6 @@ public String getName() {\n    * @return the instance properties\n    */\n   public List<PropertyConfig> getProperties() {\n-    return properties;\n+    return properties == null ? Collections.<PropertyConfig>emptyList() : properties;\n   }\n }",
                "raw_url": "https://github.com/apache/ambari/raw/eec3548f10e8bceea9127f8cd641e4d77ed2474a/ambari-server/src/main/java/org/apache/ambari/server/view/configuration/InstanceConfig.java",
                "sha": "4b0c08530e69af8eadddcd14022a2d8c62002cf4",
                "status": "modified"
            },
            {
                "additions": 32,
                "blob_url": "https://github.com/apache/ambari/blob/eec3548f10e8bceea9127f8cd641e4d77ed2474a/ambari-server/src/test/java/org/apache/ambari/server/view/configuration/InstanceConfigTest.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/view/configuration/InstanceConfigTest.java?ref=eec3548f10e8bceea9127f8cd641e4d77ed2474a",
                "deletions": 0,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/view/configuration/InstanceConfigTest.java",
                "patch": "@@ -28,6 +28,25 @@\n  * InstanceConfig tests.\n  */\n public class InstanceConfigTest {\n+\n+  private static String xml_no_properties = \"<view>\\n\" +\n+      \"    <name>MY_VIEW</name>\\n\" +\n+      \"    <label>My View!</label>\\n\" +\n+      \"    <version>1.0.0</version>\\n\" +\n+      \"    <resource>\\n\" +\n+      \"        <name>resource</name>\\n\" +\n+      \"        <plural-name>resources</plural-name>\\n\" +\n+      \"        <id-property>id</id-property>\\n\" +\n+      \"        <resource-class>org.apache.ambari.server.view.configuration.ViewConfigTest$MyResource</resource-class>\\n\" +\n+      \"        <provider-class>org.apache.ambari.server.view.configuration.ViewConfigTest$MyResourceProvider</provider-class>\\n\" +\n+      \"        <service-class>org.apache.ambari.server.view.configuration.ViewConfigTest$MyResourceService</service-class>\\n\" +\n+      \"        <sub-resource-name>subresource</sub-resource-name>\\n\" +\n+      \"    </resource>\\n\" +\n+      \"    <instance>\\n\" +\n+      \"        <name>INSTANCE1</name>\\n\" +\n+      \"    </instance>\\n\" +\n+      \"</view>\";\n+\n   @Test\n   public void testGetName() throws Exception {\n     List<InstanceConfig> instances = getInstanceConfigs();\n@@ -47,10 +66,23 @@ public void testGetProperties() throws Exception {\n \n     properties = instances.get(1).getProperties();\n     Assert.assertEquals(1, properties.size());\n+\n+    // check the case where no properties are specified for the instance...\n+    instances = getInstanceConfigs(xml_no_properties);\n+\n+    Assert.assertEquals(1, instances.size());\n+    properties = instances.get(0).getProperties();\n+    Assert.assertNotNull(properties);\n+    Assert.assertEquals(0, properties.size());\n   }\n \n   public static List<InstanceConfig> getInstanceConfigs() throws JAXBException {\n     ViewConfig config = ViewConfigTest.getConfig();\n     return config.getInstances();\n   }\n+\n+  public static List<InstanceConfig> getInstanceConfigs(String xml) throws JAXBException {\n+    ViewConfig config = ViewConfigTest.getConfig(xml);\n+    return config.getInstances();\n+  }\n }",
                "raw_url": "https://github.com/apache/ambari/raw/eec3548f10e8bceea9127f8cd641e4d77ed2474a/ambari-server/src/test/java/org/apache/ambari/server/view/configuration/InstanceConfigTest.java",
                "sha": "4f49425a6fec89ff02b4fd70224834cc50f32534",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/ambari/blob/eec3548f10e8bceea9127f8cd641e4d77ed2474a/ambari-server/src/test/java/org/apache/ambari/server/view/configuration/ViewConfigTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/view/configuration/ViewConfigTest.java?ref=eec3548f10e8bceea9127f8cd641e4d77ed2474a",
                "deletions": 1,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/view/configuration/ViewConfigTest.java",
                "patch": "@@ -184,7 +184,11 @@ public void testGetServletURLPatternMap() throws Exception {\n   }\n \n   public static  ViewConfig getConfig() throws JAXBException {\n-    InputStream configStream =  new ByteArrayInputStream(xml.getBytes());\n+      return getConfig(xml);\n+  }\n+\n+  public static  ViewConfig getConfig(String xml) throws JAXBException {\n+      InputStream configStream =  new ByteArrayInputStream(xml.getBytes());\n     JAXBContext jaxbContext = JAXBContext.newInstance(ViewConfig.class);\n     Unmarshaller unmarshaller = jaxbContext.createUnmarshaller();\n     return (ViewConfig) unmarshaller.unmarshal(configStream);",
                "raw_url": "https://github.com/apache/ambari/raw/eec3548f10e8bceea9127f8cd641e4d77ed2474a/ambari-server/src/test/java/org/apache/ambari/server/view/configuration/ViewConfigTest.java",
                "sha": "04e71a8416278d0dc7395a7d3a999c8e2e8fa850",
                "status": "modified"
            }
        ],
        "message": "AMBARI-4541 - NPE for View instance with no properties.",
        "parent": "https://github.com/apache/ambari/commit/1617c73570652bd27ed68d1d49214d17fa16c1ea",
        "repo": "ambari",
        "unit_tests": [
            "InstanceConfigTest.java"
        ]
    },
    "ambari_f012848": {
        "bug_id": "ambari_f012848",
        "commit": "https://github.com/apache/ambari/commit/f012848130e6f3ae7c8121a781ec851885185327",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/f012848130e6f3ae7c8121a781ec851885185327/ambari-server/src/main/java/org/apache/ambari/server/upgrade/AbstractUpgradeCatalog.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/upgrade/AbstractUpgradeCatalog.java?ref=f012848130e6f3ae7c8121a781ec851885185327",
                "deletions": 3,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/upgrade/AbstractUpgradeCatalog.java",
                "patch": "@@ -150,9 +150,6 @@ public AbstractUpgradeCatalog(Injector injector) {\n     registerCatalog(this);\n   }\n \n-  protected AbstractUpgradeCatalog() {\n-  }\n-\n   /**\n    * Every subclass needs to register itself\n    */",
                "raw_url": "https://github.com/apache/ambari/raw/f012848130e6f3ae7c8121a781ec851885185327/ambari-server/src/main/java/org/apache/ambari/server/upgrade/AbstractUpgradeCatalog.java",
                "sha": "6c59784747e754dc1c80ad2094cd85ecc11f9f8b",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/ambari/blob/f012848130e6f3ae7c8121a781ec851885185327/ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog212.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog212.java?ref=f012848130e6f3ae7c8121a781ec851885185327",
                "deletions": 3,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog212.java",
                "patch": "@@ -87,9 +87,6 @@ public UpgradeCatalog212(Injector injector) {\n     daoUtils = injector.getInstance(DaoUtils.class);\n   }\n \n-  protected UpgradeCatalog212() {\n-  }\n-\n   // ----- UpgradeCatalog ----------------------------------------------------\n \n   /**",
                "raw_url": "https://github.com/apache/ambari/raw/f012848130e6f3ae7c8121a781ec851885185327/ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog212.java",
                "sha": "90854dd34603d8ac56e3b158d5b86bf0735418cb",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/ambari/blob/f012848130e6f3ae7c8121a781ec851885185327/ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog251.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog251.java?ref=f012848130e6f3ae7c8121a781ec851885185327",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog251.java",
                "patch": "@@ -22,6 +22,9 @@\n import org.apache.ambari.server.AmbariException;\n import org.apache.ambari.server.orm.DBAccessor.DBColumnInfo;\n \n+import com.google.inject.Inject;\n+import com.google.inject.Injector;\n+\n /**\n  * The {@link UpgradeCatalog251} upgrades Ambari from 2.5.0 to 2.5.1.\n  */\n@@ -30,6 +33,16 @@\n   static final String HOST_ROLE_COMMAND_TABLE = \"host_role_command\";\n   static final String HRC_IS_BACKGROUND_COLUMN = \"is_background\";\n \n+  /**\n+   * Constructor.\n+   *\n+   * @param injector\n+   */\n+  @Inject\n+  public UpgradeCatalog251(Injector injector) {\n+    super(injector);\n+  }\n+\n   /**\n    * {@inheritDoc}\n    */",
                "raw_url": "https://github.com/apache/ambari/raw/f012848130e6f3ae7c8121a781ec851885185327/ambari-server/src/main/java/org/apache/ambari/server/upgrade/UpgradeCatalog251.java",
                "sha": "6f8f2a6d78d0b6424f78771d6c3a2aa8e8ca9813",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/ambari/blob/f012848130e6f3ae7c8121a781ec851885185327/ambari-server/src/test/java/org/apache/ambari/server/upgrade/UpgradeCatalog212Test.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/upgrade/UpgradeCatalog212Test.java?ref=f012848130e6f3ae7c8121a781ec851885185327",
                "deletions": 29,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/upgrade/UpgradeCatalog212Test.java",
                "patch": "@@ -52,8 +52,6 @@\n import org.apache.ambari.server.orm.DBAccessor.DBColumnInfo;\n import org.apache.ambari.server.orm.GuiceJpaInitializer;\n import org.apache.ambari.server.orm.InMemoryDefaultTestModule;\n-import org.apache.ambari.server.orm.dao.StackDAO;\n-import org.apache.ambari.server.orm.entities.StackEntity;\n import org.apache.ambari.server.state.Cluster;\n import org.apache.ambari.server.state.Clusters;\n import org.apache.ambari.server.state.Config;\n@@ -69,6 +67,7 @@\n import org.easymock.TestSubject;\n import org.junit.After;\n import org.junit.Assert;\n+import org.junit.Before;\n import org.junit.Rule;\n import org.junit.Test;\n \n@@ -114,28 +113,20 @@\n   private ResultSet resultSet;\n \n   @TestSubject\n-  private UpgradeCatalog212 testSubject = new UpgradeCatalog212();\n+  private UpgradeCatalog212 testSubject = new UpgradeCatalog212(\n+      EasyMock.createNiceMock(Injector.class));\n \n-\n-  private UpgradeCatalogHelper upgradeCatalogHelper;\n-  private StackEntity desiredStackEntity;\n-\n-\n-  // This method to be called only when an IOC is needed - typically by functional tests\n-  public void setupIoCContext() {\n+  @Before\n+  public void setUp() {\n     reset(entityManagerProvider);\n     expect(entityManagerProvider.get()).andReturn(entityManager).anyTimes();\n     replay(entityManagerProvider);\n \n     injector = Guice.createInjector(new InMemoryDefaultTestModule());\n     injector.getInstance(GuiceJpaInitializer.class);\n \n-    upgradeCatalogHelper = injector.getInstance(UpgradeCatalogHelper.class);\n     // inject AmbariMetaInfo to ensure that stacks get populated in the DB\n     injector.getInstance(AmbariMetaInfo.class);\n-    // load the stack entity\n-    StackDAO stackDAO = injector.getInstance(StackDAO.class);\n-    desiredStackEntity = stackDAO.find(\"HDP\", \"2.2.0\");\n   }\n \n   @After\n@@ -148,7 +139,6 @@ public void tearDown() throws AmbariException, SQLException {\n \n   @Test\n   public void testFinilizeTopologyDDL() throws Exception {\n-    setupIoCContext();\n     final DBAccessor dbAccessor = createNiceMock(DBAccessor.class);\n     dbAccessor.dropColumn(eq(\"topology_request\"), eq(\"cluster_name\"));\n     dbAccessor.setColumnNullable(eq(\"topology_request\"), eq(\"cluster_id\"), eq(false));\n@@ -173,7 +163,6 @@ public void configure(Binder binder) {\n \n   @Test\n   public void testExecuteDDLUpdates() throws Exception {\n-    setupIoCContext();\n     final DBAccessor dbAccessor = createNiceMock(DBAccessor.class);\n     Configuration configuration = createNiceMock(Configuration.class);\n     Connection connection = createNiceMock(Connection.class);\n@@ -211,7 +200,6 @@ public void testExecuteDDLUpdates() throws Exception {\n \n   @Test\n   public void testExecuteDMLUpdates() throws Exception {\n-    setupIoCContext();\n     Method addMissingConfigs = UpgradeCatalog212.class.getDeclaredMethod(\"addMissingConfigs\");\n     Method addNewConfigurationsFromXml = AbstractUpgradeCatalog.class.getDeclaredMethod(\"addNewConfigurationsFromXml\");\n \n@@ -235,7 +223,6 @@ public void testExecuteDMLUpdates() throws Exception {\n \n   @Test\n   public void testUpdateHBaseAdnClusterConfigs() throws Exception {\n-    setupIoCContext();\n     EasyMockSupport easyMockSupport = new EasyMockSupport();\n     final AmbariManagementController mockAmbariManagementController = easyMockSupport.createNiceMock(AmbariManagementController.class);\n     final ConfigHelper mockConfigHelper = easyMockSupport.createMock(ConfigHelper.class);\n@@ -302,7 +289,6 @@ protected void configure() {\n \n   @Test\n   public void testUpdateHBaseAdnClusterConfigsTrue() throws Exception {\n-    setupIoCContext();\n     EasyMockSupport easyMockSupport = new EasyMockSupport();\n     final AmbariManagementController mockAmbariManagementController = easyMockSupport.createNiceMock(AmbariManagementController.class);\n     final ConfigHelper mockConfigHelper = easyMockSupport.createMock(ConfigHelper.class);\n@@ -355,7 +341,6 @@ protected void configure() {\n \n   @Test\n   public void testUpdateHBaseAdnClusterConfigsNoHBaseEnv() throws Exception {\n-    setupIoCContext();\n     EasyMockSupport easyMockSupport = new EasyMockSupport();\n     final AmbariManagementController mockAmbariManagementController = easyMockSupport.createNiceMock(AmbariManagementController.class);\n     final ConfigHelper mockConfigHelper = easyMockSupport.createMock(ConfigHelper.class);\n@@ -397,7 +382,6 @@ protected void configure() {\n \n   @Test\n   public void testUpdateHBaseAdnClusterConfigsNoOverrideHBaseUID() throws Exception {\n-    setupIoCContext();\n     EasyMockSupport easyMockSupport = new EasyMockSupport();\n     final AmbariManagementController mockAmbariManagementController = easyMockSupport.createNiceMock(AmbariManagementController.class);\n     final ConfigHelper mockConfigHelper = easyMockSupport.createMock(ConfigHelper.class);\n@@ -415,9 +399,6 @@ public void testUpdateHBaseAdnClusterConfigsNoOverrideHBaseUID() throws Exceptio\n     expect(mockHbaseEnv.getProperties()).andReturn(propertiesHbaseEnv).once();\n     final Config mockClusterEnv = easyMockSupport.createNiceMock(Config.class);\n \n-    final Map<String, String> propertiesExpectedHbaseEnv = new HashMap<String, String>() {{\n-      put(\"hbase_user\", \"hbase\");\n-    }};\n     final Map<String, String> propertiesExpectedClusterEnv = new HashMap<String, String>() {{\n       put(\"override_uid\", \"false\");\n     }};\n@@ -451,17 +432,14 @@ protected void configure() {\n \n   @Test\n   public void testUpdateHiveConfigs() throws Exception {\n-    setupIoCContext();\n     EasyMockSupport easyMockSupport = new EasyMockSupport();\n     final AmbariManagementController  mockAmbariManagementController = easyMockSupport.createNiceMock(AmbariManagementController.class);\n     final ConfigHelper mockConfigHelper = easyMockSupport.createMock(ConfigHelper.class);\n \n     final Clusters mockClusters = easyMockSupport.createStrictMock(Clusters.class);\n     final Cluster mockClusterExpected = easyMockSupport.createNiceMock(Cluster.class);\n-    final Config mockHiveEnv = easyMockSupport.createNiceMock(Config.class);\n     final Config mockHiveSite = easyMockSupport.createNiceMock(Config.class);\n \n-    final Map<String, String> propertiesExpectedHiveEnv = new HashMap<>();\n     final Map<String, String> propertiesExpectedHiveSite = new HashMap<String, String>() {{\n       put(\"hive.heapsize\", \"512\");\n       put(\"hive.server2.custom.authentication.class\", \"\");\n@@ -498,7 +476,6 @@ protected void configure() {\n \n   @Test\n   public void testUpdateOozieConfigs() throws Exception {\n-    setupIoCContext();\n     EasyMockSupport easyMockSupport = new EasyMockSupport();\n     final AmbariManagementController  mockAmbariManagementController = easyMockSupport.createNiceMock(AmbariManagementController.class);\n     final ConfigHelper mockConfigHelper = easyMockSupport.createMock(ConfigHelper.class);\n@@ -539,7 +516,6 @@ protected void configure() {\n \n   @Test\n   public void testUpdateHiveEnvContent() throws Exception {\n-    setupIoCContext();\n     final Injector mockInjector = Guice.createInjector(new AbstractModule() {\n       @Override\n       protected void configure() {",
                "raw_url": "https://github.com/apache/ambari/raw/f012848130e6f3ae7c8121a781ec851885185327/ambari-server/src/test/java/org/apache/ambari/server/upgrade/UpgradeCatalog212Test.java",
                "sha": "896602b54c8edd3b0691b39b5c4ff00db15b032d",
                "status": "modified"
            }
        ],
        "message": "AMBARI-20799 - Ambari fails to upgrade from 2.4.2.0 to 2.5.1.0 during schema upgrade with NPE (jonathanhurley)",
        "parent": "https://github.com/apache/ambari/commit/f020edc85ad7cb71d47a7eaabc2bc7361dee5d1f",
        "repo": "ambari",
        "unit_tests": [
            "AbstractUpgradeCatalogTest.java",
            "UpgradeCatalog251Test.java"
        ]
    },
    "ambari_f258341": {
        "bug_id": "ambari_f258341",
        "commit": "https://github.com/apache/ambari/commit/f25834109c3096985631db8490c90a33656ecee9",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/f25834109c3096985631db8490c90a33656ecee9/ambari-server/src/main/java/org/apache/ambari/server/view/ViewClassLoader.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/view/ViewClassLoader.java?ref=f25834109c3096985631db8490c90a33656ecee9",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/view/ViewClassLoader.java",
                "patch": "@@ -81,6 +81,7 @@ private static WebAppContext getInitContext(ViewConfig viewConfig) {\n     // add as system classes to avoid conflicts and linkage errors\n     webAppContext.addSystemClass(\"org.slf4j.\");\n     webAppContext.addSystemClass(\"com.sun.jersey.\");\n+    webAppContext.addSystemClass(\"org.apache.velocity.\");\n \n     // set the class loader settings from the configuration\n     if (viewConfig != null) {",
                "raw_url": "https://github.com/apache/ambari/raw/f25834109c3096985631db8490c90a33656ecee9/ambari-server/src/main/java/org/apache/ambari/server/view/ViewClassLoader.java",
                "sha": "692f7280134f3276663d9b56a3c69ad3ac97e01c",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/ambari/blob/f25834109c3096985631db8490c90a33656ecee9/ambari-server/src/test/java/org/apache/ambari/server/view/ViewClassLoaderTest.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/view/ViewClassLoaderTest.java?ref=f25834109c3096985631db8490c90a33656ecee9",
                "deletions": 0,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/view/ViewClassLoaderTest.java",
                "patch": "@@ -80,6 +80,7 @@ public void testLoadClass() throws Exception {\n     expect(parentClassLoader.loadClass(\"com.google.inject.AbstractModule\")).andReturn(parentClass).once();\n     expect(parentClassLoader.loadClass(\"org.slf4j.LoggerFactory\")).andReturn(parentClass).once();\n     expect(parentClassLoader.loadClass(\"com.sun.jersey.api.ConflictException\")).andReturn(parentClass).once();\n+    expect(parentClassLoader.loadClass(\"org.apache.velocity.VelocityContext\")).andReturn(parentClass).once();\n \n     replay(parentClassLoader, viewConfig);\n \n@@ -126,6 +127,11 @@ public void testLoadClass() throws Exception {\n     Assert.assertNotNull(clazz);\n     Assert.assertSame(parentClass, clazz);\n \n+    clazz = classLoader.loadClass(\"org.apache.velocity.VelocityContext\");\n+\n+    Assert.assertNotNull(clazz);\n+    Assert.assertSame(parentClass, clazz);\n+\n     verify(parentClassLoader, viewConfig);\n   }\n ",
                "raw_url": "https://github.com/apache/ambari/raw/f25834109c3096985631db8490c90a33656ecee9/ambari-server/src/test/java/org/apache/ambari/server/view/ViewClassLoaderTest.java",
                "sha": "d2f0c8f22f0ec5ed7c59eaba346c29e5a652f6c9",
                "status": "modified"
            }
        ],
        "message": "AMBARI-11774 - Views : NPE accessing view that includes velocity. (tbeerbower)",
        "parent": "https://github.com/apache/ambari/commit/f2c892ad9a07fbd5f306aa4e7a8aa28857921f7a",
        "repo": "ambari",
        "unit_tests": [
            "ViewClassLoaderTest.java"
        ]
    },
    "ambari_f3aa02f": {
        "bug_id": "ambari_f3aa02f",
        "commit": "https://github.com/apache/ambari/commit/f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/pom.xml",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/pom.xml?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 1,
                "filename": "ambari-server/pom.xml",
                "patch": "@@ -1323,7 +1323,7 @@\n     <dependency>\n       <groupId>org.apache.commons</groupId>\n       <artifactId>commons-lang3</artifactId>\n-      <version>3.1</version>\n+      <version>3.8.1</version>\n     </dependency>\n     <dependency>\n       <groupId>commons-net</groupId>",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/pom.xml",
                "sha": "fd8db02fa6b5561906f27ed03e887e28cf6dd1b7",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/api/services/stackadvisor/StackAdvisorRequest.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/api/services/stackadvisor/StackAdvisorRequest.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/api/services/stackadvisor/StackAdvisorRequest.java",
                "patch": "@@ -18,6 +18,8 @@\n \n package org.apache.ambari.server.api.services.stackadvisor;\n \n+import static java.util.stream.Collectors.toMap;\n+\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collection;\n@@ -30,9 +32,12 @@\n import org.apache.ambari.server.api.services.stackadvisor.recommendations.RecommendationResponse;\n import org.apache.ambari.server.state.ChangedConfigInfo;\n import org.apache.ambari.server.state.StackId;\n+import org.apache.ambari.server.topology.Configuration;\n import org.apache.commons.lang.StringUtils;\n+import org.apache.commons.lang3.tuple.Pair;\n \n import com.google.common.base.Preconditions;\n+import com.google.common.collect.ImmutableMap;\n \n /**\n  * Stack advisor request.\n@@ -151,6 +156,22 @@ private StackAdvisorRequest(String stackName, String stackVersion) {\n     this.stackVersion = stackVersion;\n   }\n \n+  public StackAdvisorRequestBuilder builder() {\n+    return StackAdvisorRequestBuilder.forStack(stackName, stackVersion)\n+      .ofType(requestType)\n+      .forHosts(hosts)\n+      .forServices(services)\n+      .forHostComponents(hostComponents)\n+      .forHostsGroupBindings(hostGroupBindings)\n+      .withComponentHostsMap(componentHostsMap)\n+      .withConfigurations(configurations)\n+      .withChangedConfigurations(changedConfigurations)\n+      .withConfigGroups(configGroups)\n+      .withUserContext(userContext)\n+      .withGPLLicenseAccepted(gplLicenseAccepted)\n+      .withLdapConfig(ldapConfig);\n+  }\n+\n   public static class StackAdvisorRequestBuilder {\n     StackAdvisorRequest instance;\n \n@@ -204,6 +225,15 @@ public StackAdvisorRequestBuilder withConfigurations(\n       return this;\n     }\n \n+    public StackAdvisorRequestBuilder withConfigurations(Configuration configuration) {\n+      Map<String, Map<String, String>> properties = configuration.getFullProperties();\n+      this.instance.configurations = properties.entrySet().stream()\n+        .map( e -> Pair.of(e.getKey(), ImmutableMap.of(\"properties\", e.getValue())))\n+        .collect(toMap(Pair::getKey, Pair::getValue));\n+      return this;\n+    }\n+\n+\n     public StackAdvisorRequestBuilder withChangedConfigurations(\n       List<ChangedConfigInfo> changedConfigurations) {\n       this.instance.changedConfigurations = changedConfigurations;",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/api/services/stackadvisor/StackAdvisorRequest.java",
                "sha": "5fb137cf7e4ec76f5d350ed846adf0e5aea193c5",
                "status": "modified"
            },
            {
                "additions": 32,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/api/services/stackadvisor/recommendations/RecommendationResponse.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/api/services/stackadvisor/recommendations/RecommendationResponse.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/api/services/stackadvisor/recommendations/RecommendationResponse.java",
                "patch": "@@ -18,6 +18,7 @@\n \n package org.apache.ambari.server.api.services.stackadvisor.recommendations;\n \n+import static com.google.common.collect.Maps.transformValues;\n import static java.util.stream.Collectors.groupingBy;\n import static java.util.stream.Collectors.mapping;\n import static java.util.stream.Collectors.toMap;\n@@ -27,13 +28,16 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Objects;\n+import java.util.Optional;\n import java.util.Set;\n \n import org.apache.ambari.server.api.services.stackadvisor.StackAdvisorResponse;\n import org.apache.ambari.server.state.ValueAttributesInfo;\n+import org.apache.ambari.server.topology.ConfigurableHelper;\n import org.apache.commons.lang3.tuple.Pair;\n import org.codehaus.jackson.annotate.JsonIgnore;\n import org.codehaus.jackson.annotate.JsonProperty;\n+import org.codehaus.jackson.map.ObjectMapper;\n import org.codehaus.jackson.map.annotate.JsonSerialize;\n \n import com.google.common.collect.ImmutableMap;\n@@ -150,6 +154,24 @@ public void setHostGroups(Set<HostGroup> hostGroups) {\n     @JsonSerialize(include=JsonSerialize.Inclusion.NON_NULL)\n     private Map<String, ValueAttributesInfo> propertyAttributes = null;\n \n+    /**\n+     *\n+     * @param properties properties in <i>name -> value</i> format\n+     * @param attributes attributes in <i>attribute name -> property name -> value</i> format\n+     */\n+    public static BlueprintConfigurations create(Map<String, String> properties, Map<String, Map<String, String>> attributes) {\n+      BlueprintConfigurations config = new BlueprintConfigurations();\n+      config.setProperties(properties);\n+      if (attributes != null) {\n+        // transform map to property name -> attribute name -> value format\n+        Map<String, Map<String, String>> transformedAttributes = ConfigurableHelper.transformAttributesMap(attributes);\n+        ObjectMapper mapper = new ObjectMapper();\n+        config.setPropertyAttributes(\n+          new HashMap<>(transformValues(transformedAttributes, attr -> ValueAttributesInfo.fromMap(attr, Optional.of(mapper)))));\n+      }\n+      return config;\n+    }\n+\n     public BlueprintConfigurations() {\n \n     }\n@@ -176,6 +198,16 @@ public void setProperties(Map<String, String> properties) {\n       return propertyAttributes;\n     }\n \n+    /**\n+     * @return value attributes in <i>attribute name -> property name -> value</i> format\n+     */\n+    @JsonIgnore\n+    public Map<String, Map<String, String>> getPropertyAttributesAsMap() {\n+      ObjectMapper mapper = new ObjectMapper();\n+      return null == propertyAttributes ? null :\n+        ConfigurableHelper.transformAttributesMap( transformValues(propertyAttributes, vaInfo -> vaInfo.toMap(Optional.of(mapper))) );\n+    }\n+\n     public void setPropertyAttributes(Map<String, ValueAttributesInfo> propertyAttributes) {\n       this.propertyAttributes = propertyAttributes;\n     }",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/api/services/stackadvisor/recommendations/RecommendationResponse.java",
                "sha": "5d5247e90653580ebc3fcdf584f8f8bc7159051d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/controller/AddServiceRequest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/AddServiceRequest.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 1,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/AddServiceRequest.java",
                "patch": "@@ -124,7 +124,7 @@ private AddServiceRequest(\n     Configuration configuration\n   ) {\n     this.operationType = null != operationType ? operationType : OperationType.ADD_SERVICE;\n-    this.recommendationStrategy = null != recommendationStrategy ? recommendationStrategy : ConfigRecommendationStrategy.NEVER_APPLY;\n+    this.recommendationStrategy = null != recommendationStrategy ? recommendationStrategy : ConfigRecommendationStrategy.defaultForAddService();\n     this.provisionAction = null != provisionAction ? provisionAction : ProvisionAction.INSTALL_AND_START;\n     this.validationType = validationType != null ? validationType : ValidationType.DEFAULT;\n     this.stackName = stackName;",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/controller/AddServiceRequest.java",
                "sha": "e169f82ef801060bf971b916c00142ca77e43c79",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java",
                "patch": "@@ -1023,6 +1023,8 @@ private void doFilterStackDefaults(Map<String, AdvisedConfiguration> advisedConf\n     }\n   }\n \n+\n+\n   /**\n    * Update configuration properties based on advised configuration properties.\n    * @param configuration configuration being processed",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/BlueprintConfigurationProcessor.java",
                "sha": "12ef3141fd28f0623ba454d331445bd788e2f47d",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/Stack.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/Stack.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 11,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/Stack.java",
                "patch": "@@ -49,7 +49,6 @@\n import org.apache.ambari.server.state.ValueAttributesInfo;\n import org.apache.ambari.server.topology.Cardinality;\n import org.apache.ambari.server.topology.Configuration;\n-import org.apache.ambari.server.topology.validators.UnitValidatedProperty;\n \n /**\n  * Encapsulates stack information.\n@@ -657,23 +656,20 @@ public Configuration getConfiguration() {\n    * (eg. some properties need a unit to be appended)\n    */\n   public Configuration getValidDefaultConfig() {\n-    Configuration config = getConfiguration();\n-\n-    for (UnitValidatedProperty p : UnitValidatedProperty.ALL) {\n-      if (config.isPropertySet(p.getConfigType(), p.getPropertyName())) {\n-        String value = config.getPropertyValue(p.getConfigType(), p.getPropertyName());\n-        String updatedValue = UnitUpdater.updateForClusterCreate(this, p.getServiceName(), p.getConfigType(), p.getPropertyName(), value);\n-        config.setProperty(p.getConfigType(), p.getPropertyName(), updatedValue);\n-      }\n-    }\n+    Configuration config = getDefaultConfig();\n+    UnitUpdater.updateUnits(config, this);\n+    return config;\n+  }\n \n+  public Configuration getDefaultConfig() {\n+    Configuration config = getConfiguration();\n     config.getProperties().values().forEach(\n       each -> each.values().removeIf(Objects::isNull)\n     );\n-\n     return config;\n   }\n \n+\n   /**\n    * Parse components for the specified service from the stack definition.\n    *",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/Stack.java",
                "sha": "3bfdeb3f4ff979bcf695667792aa2ea6b3536992",
                "status": "modified"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/UnitUpdater.java",
                "changes": 35,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/UnitUpdater.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 1,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/UnitUpdater.java",
                "patch": "@@ -22,9 +22,12 @@\n import java.util.Collection;\n import java.util.Collections;\n import java.util.Map;\n+import java.util.Objects;\n import java.util.Optional;\n+import java.util.function.BiFunction;\n \n import org.apache.ambari.server.topology.ClusterTopology;\n+import org.apache.ambari.server.topology.Configuration;\n import org.apache.ambari.server.topology.validators.UnitValidatedProperty;\n \n /**\n@@ -65,12 +68,38 @@ public static String updateForClusterCreate(Stack stack, String serviceName, Str\n     }\n   }\n \n+  public static void updateUnits(Configuration configuration, Stack stack) {\n+    updateAllUnitValidatedProperties(configuration,\n+      (property, value) -> updateForClusterCreate(stack, property.getServiceName(), property.getConfigType(), property.getPropertyName(), value));\n+  }\n+\n+  public static void removeUnits(Configuration configuration, Stack stack) {\n+    updateAllUnitValidatedProperties(configuration,\n+      (property, value) -> removeStackUnit(stack, property.getServiceName(), property.getConfigType(), property.getPropertyName(), value));\n+  }\n+\n+  private static void updateAllUnitValidatedProperties(Configuration configuration, BiFunction<UnitValidatedProperty, String, String> valueUpdater) {\n+    for (UnitValidatedProperty p : UnitValidatedProperty.ALL) {\n+      if (configuration.isPropertySet(p.getConfigType(), p.getPropertyName())) {\n+        String value = configuration.getPropertyValue(p.getConfigType(), p.getPropertyName());\n+        String updatedValue = valueUpdater.apply(p, value);\n+        if (!Objects.equals(value, updatedValue)) {\n+          configuration.setProperty(p.getConfigType(), p.getPropertyName(), updatedValue);\n+        }\n+      }\n+    }\n+  }\n+\n   /**\n    * @return property value with removed unit\n    */\n   @Override\n   public String updateForBlueprintExport(String propertyName, String origValue, Map<String, Map<String, String>> properties, ClusterTopology topology) {\n-    PropertyUnit stackUnit = PropertyUnit.of(topology.getBlueprint().getStack(), serviceName, configType, propertyName);\n+    return removeStackUnit(topology.getBlueprint().getStack(), serviceName, configType, propertyName, origValue);\n+  }\n+\n+  static String removeStackUnit(Stack stack, String serviceName, String configType, String propertyName, String origValue) {\n+    PropertyUnit stackUnit = PropertyUnit.of(stack, serviceName, configType, propertyName);\n     PropertyValue value = PropertyValue.of(propertyName, origValue);\n     return value.withoutUnit(stackUnit);\n   }\n@@ -150,6 +179,10 @@ public boolean hasUnit(PropertyUnit unit) {\n     }\n \n     public boolean hasAnyUnit() {\n+      return hasAnyUnit(value);\n+    }\n+\n+    static boolean hasAnyUnit(String value) {\n       return !Character.isDigit(value.charAt(value.length() -1));\n     }\n ",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/UnitUpdater.java",
                "sha": "c6188b13afd04a4b97f5a7e9940e76ecd812685d",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/state/ValueAttributesInfo.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/state/ValueAttributesInfo.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/state/ValueAttributesInfo.java",
                "patch": "@@ -19,6 +19,8 @@\n package org.apache.ambari.server.state;\n \n import java.util.Collection;\n+import java.util.Map;\n+import java.util.Optional;\n \n import javax.xml.bind.annotation.XmlAccessType;\n import javax.xml.bind.annotation.XmlAccessorType;\n@@ -28,7 +30,9 @@\n \n import org.apache.ambari.server.controller.ApiModel;\n import org.codehaus.jackson.annotate.JsonProperty;\n+import org.codehaus.jackson.map.ObjectMapper;\n import org.codehaus.jackson.map.annotate.JsonSerialize;\n+import org.codehaus.jackson.type.TypeReference;\n \n import io.swagger.annotations.ApiModelProperty;\n \n@@ -396,6 +400,19 @@ public int hashCode() {\n     return result;\n   }\n \n+  public Map<String, String> toMap(Optional<ObjectMapper> mapper) {\n+    Map<String, String> map =\n+      mapper.orElseGet(ObjectMapper::new).convertValue(this, new TypeReference<Map<String, String>>(){});\n+    if ( !Boolean.parseBoolean(map.get(\"keyStore\")) ) { // keyStore is declared as a primitive value instead of Boolean -> treat false as unset\n+      map.remove(\"keyStore\");\n+    }\n+    return map;\n+  }\n+\n+  public static ValueAttributesInfo fromMap(Map<String, String> attributes, Optional<ObjectMapper> mapper) {\n+    return mapper.orElseGet(ObjectMapper::new).convertValue(attributes, ValueAttributesInfo.class);\n+  }\n+\n   @Override\n   public String toString() {\n     return \"ValueAttributesInfo{\" +",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/state/ValueAttributesInfo.java",
                "sha": "f8ddf0baa9affd7c15b7b6064ada31afe6ad146b",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/topology/ConfigRecommendationStrategy.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/topology/ConfigRecommendationStrategy.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 4,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/topology/ConfigRecommendationStrategy.java",
                "patch": "@@ -19,24 +19,46 @@\n package org.apache.ambari.server.topology;\n \n public enum ConfigRecommendationStrategy {\n+\n   /**\n    *  Configuration recommendations are always applied, overriding stack defaults and\n    *  configuration defined by the user in the Blueprint and/or Cluster Creation Template.\n    */\n-  ALWAYS_APPLY,\n+  ALWAYS_APPLY(true, true),\n   /**\n    * Configuration recommendations are ignored with this option, both for stack defaults\n    * and configuration defined by the user in the Blueprint and/or Cluster Creation Template.\n    */\n-  NEVER_APPLY,\n+  NEVER_APPLY(false, false),\n+\n   /**\n    *  Configuration recommendations are always applied for properties listed as stack defaults,\n    *  but not for configurations defined by the user in the Blueprint and/or Cluster Creation Template.\n    */\n-  ONLY_STACK_DEFAULTS_APPLY,\n+  ONLY_STACK_DEFAULTS_APPLY(true, false),\n   /**\n    *  Configuration recommendations are always applied, overriding stack defaults but they don't\n    *  override configuration defined by the user in the Blueprint and/or Cluster Creation Template.\n    */\n-  ALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES;\n+  ALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES(true, false);\n+\n+  public static ConfigRecommendationStrategy defaultForAddService() {\n+    return ALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES;\n+  }\n+\n+  private final boolean useStackAdvisor;\n+  private final boolean overrideCustomValues;\n+\n+  public boolean shouldUseStackAdvisor() {\n+    return useStackAdvisor;\n+  }\n+\n+  public boolean shouldOverrideCustomValues() {\n+    return overrideCustomValues;\n+  }\n+\n+  ConfigRecommendationStrategy(boolean useStackAdvisor, boolean overrideCustomValues) {\n+    this.useStackAdvisor = useStackAdvisor;\n+    this.overrideCustomValues = overrideCustomValues;\n+  }\n }",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/topology/ConfigRecommendationStrategy.java",
                "sha": "25d89e0423fe715ff70ae2e19b54f01415ae7cb1",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/topology/ConfigurableHelper.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/topology/ConfigurableHelper.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/topology/ConfigurableHelper.java",
                "patch": "@@ -19,6 +19,8 @@\n package org.apache.ambari.server.topology;\n \n import static com.google.common.base.Preconditions.checkArgument;\n+import static java.util.stream.Collectors.groupingBy;\n+import static java.util.stream.Collectors.toMap;\n import static org.apache.ambari.server.controller.internal.BlueprintResourceProvider.PROPERTIES_ATTRIBUTES_PROPERTY_ID;\n import static org.apache.ambari.server.controller.internal.BlueprintResourceProvider.PROPERTIES_PROPERTY_ID;\n import static org.apache.ambari.server.topology.ConfigurationFactory.isKeyInLegacyFormat;\n@@ -35,6 +37,8 @@\n \n import javax.annotation.Nullable;\n \n+import org.apache.commons.lang3.tuple.Triple;\n+\n import com.google.common.collect.ImmutableMap;\n import com.google.common.collect.ImmutableSet;\n import com.google.common.collect.Sets;\n@@ -182,4 +186,17 @@ private static String getClassName(Object object) {\n     return null != object ? object.getClass().getName() : null;\n   }\n \n+\n+  /**\n+   * Transform attibutes map from <i>attributeName -> propertyName -> value</i> to <i>propertyName -> attributeName -> value</i>\n+   * or vice versa\n+   * @param input the input map\n+   * @return the output map\n+   */\n+  public static Map<String, Map<String, String>> transformAttributesMap(Map<String, Map<String, String>> input) {\n+    return input.entrySet().stream()\n+      .flatMap(outer -> outer.getValue().entrySet().stream().map(inner -> Triple.of(outer.getKey(), inner.getKey(), inner.getValue())))\n+      .collect(groupingBy(Triple::getMiddle, toMap(Triple::getLeft, Triple::getRight)));\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/topology/ConfigurableHelper.java",
                "sha": "9e75b0e2fd91f06d84558dda47bc92ec227a0ba1",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/topology/Configuration.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/topology/Configuration.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 2,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/topology/Configuration.java",
                "patch": "@@ -29,6 +29,7 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n+\n /**\n  * Configuration for a topology entity such as a blueprint, hostgroup or cluster.\n  */\n@@ -149,8 +150,8 @@ public Configuration(Map<String, Map<String, String>> properties,\n   /**\n    * Configuration.\n    *\n-   * @param properties  properties\n-   * @param attributes  attributes\n+   * @param properties  properties in configType -> propertyName -> value format\n+   * @param attributes  attributes in configType -> attributeName -> propertyName -> attributeValue format\n    */\n   public Configuration(Map<String, Map<String, String>> properties,\n                        Map<String, Map<String, Map<String, String>>> attributes) {\n@@ -441,6 +442,17 @@ public String setAttribute(String configType, String propertyName, String attrib\n     return allTypes;\n   }\n \n+  public boolean containsConfigType(String configType) {\n+    return properties.containsKey(configType) || attributes.containsKey(configType) ||\n+      (parentConfiguration != null && parentConfiguration.containsConfigType(configType));\n+  }\n+\n+  public boolean containsConfig(String configType, String propertyName) {\n+    return (properties.containsKey(configType) && properties.get(configType).containsKey(propertyName))\n+      || (attributes.containsKey(configType) && attributes.get(configType).values().stream().filter(map -> map.containsKey(propertyName)).findAny().isPresent())\n+      || (parentConfiguration != null && parentConfiguration.containsConfig(configType, propertyName));\n+  }\n+\n   /**\n    * Get the parent configuration.\n    *",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/topology/Configuration.java",
                "sha": "e6b1d888f09db23b87ae972d2d88300aac18570e",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/AddServiceInfo.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/AddServiceInfo.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 4,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/AddServiceInfo.java",
                "patch": "@@ -41,6 +41,7 @@\n   private final Map<String, Map<String, Set<String>>> newServices;\n   private final RequestStageContainer stages;\n   private final Configuration config;\n+  private final LayoutRecommendationInfo recommendationInfo;\n \n   public AddServiceInfo(\n     AddServiceRequest request,\n@@ -49,19 +50,26 @@ public AddServiceInfo(\n     Configuration config,\n     KerberosDescriptor kerberosDescriptor,\n     RequestStageContainer stages,\n-    Map<String, Map<String, Set<String>>> newServices\n-  ) {\n+    Map<String, Map<String,\n+    Set<String>>> newServices,\n+    LayoutRecommendationInfo recommendationInfo) {\n     this.request = request;\n     this.clusterName = clusterName;\n     this.stack = stack;\n     this.kerberosDescriptor = kerberosDescriptor;\n     this.newServices = newServices;\n     this.stages = stages;\n     this.config = config;\n+    this.recommendationInfo = recommendationInfo;\n   }\n \n-  public AddServiceInfo withNewServices(Map<String, Map<String, Set<String>>> services) {\n-    return new AddServiceInfo(request, clusterName, stack, config, kerberosDescriptor, stages, services);\n+  public AddServiceInfo withLayoutRecommendation(Map<String, Map<String, Set<String>>> services,\n+                                                 LayoutRecommendationInfo recommendation) {\n+    return new AddServiceInfo(request, clusterName, stack, config, kerberosDescriptor, stages, services, recommendation);\n+  }\n+\n+  public AddServiceInfo withConfig(Configuration newConfig) {\n+    return new AddServiceInfo(request, clusterName, stack, newConfig, kerberosDescriptor, stages, newServices, recommendationInfo);\n   }\n \n   @Override\n@@ -97,6 +105,10 @@ public Configuration getConfig() {\n     return config;\n   }\n \n+  public Optional<LayoutRecommendationInfo> getRecommendationInfo() {\n+    return Optional.ofNullable(recommendationInfo);\n+  }\n+\n   public Optional<KerberosDescriptor> getKerberosDescriptor() {\n     return Optional.ofNullable(kerberosDescriptor);\n   }",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/AddServiceInfo.java",
                "sha": "25b9a3b2f238c4fc2ad6ab548207a3bf27ac6cda",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/AddServiceOrchestrator.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/AddServiceOrchestrator.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 2,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/AddServiceOrchestrator.java",
                "patch": "@@ -143,8 +143,7 @@ private AddServiceInfo recommendLayout(AddServiceInfo request) {\n    */\n   private AddServiceInfo recommendConfiguration(AddServiceInfo request) {\n     LOG.info(\"Recommending configuration for {}\", request);\n-    // TODO implement\n-    return request;\n+    return stackAdvisorAdapter.recommendConfigurations(request);\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/AddServiceOrchestrator.java",
                "sha": "4ff13019a885118d9b2d4b5dbfbf94250f0c207a",
                "status": "modified"
            },
            {
                "additions": 53,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/LayoutRecommendationInfo.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/LayoutRecommendationInfo.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 0,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/LayoutRecommendationInfo.java",
                "patch": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.ambari.server.topology.addservice;\n+\n+import static java.util.stream.Collectors.toList;\n+\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * Encapsulates information from layout recommendation that can be used or reused for config recommendation.\n+ */\n+public class LayoutRecommendationInfo {\n+  private final Map<String, Set<String>> hostGroups;\n+  private final Map<String, Map<String, Set<String>>> allServiceComponentHosts;\n+\n+  public LayoutRecommendationInfo(Map<String, Set<String>> hostGroups, Map<String, Map<String, Set<String>>> allServiceComponentHosts) {\n+    this.hostGroups = hostGroups;\n+    this.allServiceComponentHosts = allServiceComponentHosts;\n+  }\n+\n+  public Map<String, Set<String>> getHostGroups() {\n+    return hostGroups;\n+  }\n+\n+  public Map<String, Map<String, Set<String>>> getAllServiceLayouts() {\n+    return allServiceComponentHosts;\n+  }\n+\n+  public List<String> getHosts() {\n+    return getHostsFromHostGroups(hostGroups);\n+  }\n+\n+  public static List<String> getHostsFromHostGroups(Map<String, Set<String>> hostGroups) {\n+    return hostGroups.values().stream().flatMap(Set::stream).collect(toList());\n+  }\n+}",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/LayoutRecommendationInfo.java",
                "sha": "03257eef1beaf57d6cbdc128323b33862582e484",
                "status": "added"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/RequestValidator.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/RequestValidator.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 2,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/RequestValidator.java",
                "patch": "@@ -37,6 +37,7 @@\n import org.apache.ambari.server.controller.AmbariManagementController;\n import org.apache.ambari.server.controller.internal.RequestStageContainer;\n import org.apache.ambari.server.controller.internal.Stack;\n+import org.apache.ambari.server.controller.internal.UnitUpdater;\n import org.apache.ambari.server.state.Cluster;\n import org.apache.ambari.server.state.ConfigHelper;\n import org.apache.ambari.server.state.SecurityType;\n@@ -117,7 +118,7 @@ AddServiceInfo createValidServiceInfo(ActionManager actionManager, RequestFactor\n     RequestStageContainer stages = new RequestStageContainer(actionManager.getNextRequestId(), null, requestFactory, actionManager);\n     AddServiceInfo validatedRequest = new AddServiceInfo(request, cluster.getClusterName(),\n       state.getStack(), state.getConfig(), state.getKerberosDescriptor(),\n-      stages, state.getNewServices());\n+      stages, state.getNewServices(), null);\n     stages.setRequestContext(validatedRequest.describe());\n     return validatedRequest;\n   }\n@@ -250,9 +251,10 @@ void validateConfiguration() {\n     }\n \n     Configuration clusterConfig = getClusterDesiredConfigs();\n-    clusterConfig.setParentConfiguration(state.getStack().getValidDefaultConfig());\n+    clusterConfig.setParentConfiguration(state.getStack().getDefaultConfig());\n     config.setParentConfiguration(clusterConfig);\n \n+    UnitUpdater.removeUnits(config, state.getStack()); // stack advisor doesn't like units; they'll be added back after recommendation\n     state = state.with(config);\n   }\n ",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/RequestValidator.java",
                "sha": "29ac8c887ed8b8836c7429e8f801f503577139fb",
                "status": "modified"
            },
            {
                "additions": 155,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/StackAdvisorAdapter.java",
                "changes": 197,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/StackAdvisorAdapter.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 42,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/StackAdvisorAdapter.java",
                "patch": "@@ -40,10 +40,11 @@\n import org.apache.ambari.server.api.services.stackadvisor.StackAdvisorRequest;\n import org.apache.ambari.server.api.services.stackadvisor.recommendations.RecommendationResponse;\n import org.apache.ambari.server.api.services.stackadvisor.validations.ValidationResponse;\n-import org.apache.ambari.server.configuration.Configuration;\n import org.apache.ambari.server.controller.AmbariManagementController;\n+import org.apache.ambari.server.controller.internal.UnitUpdater;\n import org.apache.ambari.server.state.Cluster;\n-import org.apache.ambari.server.state.StackId;\n+import org.apache.ambari.server.topology.ConfigRecommendationStrategy;\n+import org.apache.ambari.server.topology.Configuration;\n import org.apache.commons.lang3.tuple.Pair;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -64,7 +65,7 @@\n   private StackAdvisorHelper stackAdvisorHelper;\n \n   @Inject\n-  private Configuration serverConfig;\n+  private org.apache.ambari.server.configuration.Configuration serverConfig;\n \n   @Inject\n   private Injector injector;\n@@ -79,19 +80,14 @@\n    */\n   AddServiceInfo recommendLayout(AddServiceInfo info) {\n     try {\n-      Cluster cluster = managementController.getClusters().getCluster(info.clusterName());\n-      Map<String, Map<String, Set<String>>> clusterServices = transformValues(\n-        cluster.getServices(),\n-        service -> transformValues(service.getServiceComponents(), component -> component.getServiceComponentsHosts()));\n-\n       // Requested component layout will be added to the StackAdvisor input in addition to existing\n       // component layout.\n-      Map<String, Map<String, Set<String>>> allServices = mergeDisjunctMaps(clusterServices, info.newServices());\n+      Map<String, Map<String, Set<String>>> allServices = getAllServices(info);\n \n       Map<String, Set<String>> componentsToHosts = getComponentHostMap(allServices);\n \n       Map<String, Set<String>> hostsToComponents = getHostComponentMap(componentsToHosts);\n-      List<String> hosts = ImmutableList.copyOf(cluster.getHostNames());\n+      List<String> hosts = ImmutableList.copyOf(getCluster(info).getHostNames());\n       hosts.forEach( host -> hostsToComponents.putIfAbsent(host, new HashSet<>())); // just in case there are hosts that have no components\n \n       Map<String, Set<String>> hostGroups = getHostGroupStrategy().calculateHostGroups(hostsToComponents);\n@@ -104,6 +100,7 @@ AddServiceInfo recommendLayout(AddServiceInfo info) {\n         .forHostComponents(hostsToComponents)\n         .forHostsGroupBindings(hostGroups)\n         .withComponentHostsMap(componentsToHosts)\n+        .withConfigurations(info.getConfig())\n         .withGPLLicenseAccepted(serverConfig.getGplLicenseAccepted())\n         .build();\n       RecommendationResponse response = stackAdvisorHelper.recommend(request);\n@@ -113,47 +110,162 @@ AddServiceInfo recommendLayout(AddServiceInfo info) {\n         response.getRecommendations().getBlueprint().getHostgroupComponentMap(),\n         info.getStack()::getServiceForComponent);\n \n-      Set<ValidationResponse.ValidationItem> validationItems = validateRecommendedLayout(info.getStack().getStackId(),\n-        recommendedLayout,\n-        response.getRecommendations().getBlueprintClusterBinding().getHostgroupHostMap());\n-      if (!validationItems.isEmpty()) {\n-        LOG.warn(\"Issues found during recommended topology validation:\\n{}\", Joiner.on('\\n').join(validationItems));\n-      }\n-\n-      // Keep the recommendations for new services only\n-      keepNewServicesOnly(recommendedLayout, info.newServices());\n+      // Validate layout\n+      Map<String, Set<String>> recommendedComponentHosts = getComponentHostMap(recommendedLayout);\n+      StackAdvisorRequest validationRequest = request.builder()\n+        .forHostsGroupBindings(response.getRecommendations().getBlueprintClusterBinding().getHostgroupHostMap())\n+        .withComponentHostsMap(recommendedComponentHosts)\n+        .forHostComponents(getHostComponentMap(recommendedComponentHosts)).build();\n+      validate(validationRequest);\n \n-      return info.withNewServices(recommendedLayout);\n+      Map<String,Map<String,Set<String>>> newServiceRecommendations = keepNewServicesOnly(recommendedLayout, info.newServices());\n+      LayoutRecommendationInfo recommendationInfo = new LayoutRecommendationInfo(\n+        response.getRecommendations().getBlueprintClusterBinding().getHostgroupHostMap(),\n+        recommendedLayout);\n+      return info.withLayoutRecommendation(newServiceRecommendations, recommendationInfo);\n     }\n     catch (AmbariException|StackAdvisorException ex) {\n       throw new IllegalArgumentException(\"Layout recommendation failed.\", ex);\n     }\n   }\n \n-  Set<ValidationResponse.ValidationItem> validateRecommendedLayout(StackId stackId,\n-                                                              Map<String,Map<String,Set<String>>> recommendedLayout,\n-                                                              Map<String, Set<String>> recommendedHostgroups) throws StackAdvisorException {\n-    Map<String, Set<String>> componentsToHosts = getComponentHostMap(recommendedLayout);\n-    Map<String, Set<String>> hostsToComponents = getHostComponentMap(componentsToHosts);\n-    List<String> hosts = ImmutableList.copyOf(hostsToComponents.keySet());\n-\n-    StackAdvisorRequest request = StackAdvisorRequest.StackAdvisorRequestBuilder\n-      .forStack(stackId)\n-      .ofType(StackAdvisorRequest.StackAdvisorRequestType.HOST_GROUPS)\n-      .forHosts(hosts)\n-      .forServices(recommendedLayout.keySet())\n-      .forHostComponents(hostsToComponents)\n-      .forHostsGroupBindings(recommendedHostgroups)\n-      .withComponentHostsMap(componentsToHosts)\n-      .withGPLLicenseAccepted(serverConfig.getGplLicenseAccepted())\n-      .build();\n-    ValidationResponse response = stackAdvisorHelper.validate(request);\n-\n-    return response.getItems();\n+  /**\n+   * Gets all services from the cluster together together with services from AddServiceInfo.\n+   * @param info\n+   * @return A map of <i>service name -> component name -> hosts</i> for all services in the cluster and the input\n+   *  AddServiceInfo combined.\n+   */\n+  Map<String, Map<String, Set<String>>> getAllServices(AddServiceInfo info) throws AmbariException {\n+    Cluster cluster = managementController.getClusters().getCluster(info.clusterName());\n+    Map<String, Map<String, Set<String>>> clusterServices = transformValues(\n+      cluster.getServices(),\n+      service -> transformValues(service.getServiceComponents(), component -> component.getServiceComponentsHosts()));\n+\n+   return  mergeDisjunctMaps(clusterServices, info.newServices());\n+  }\n+\n+  private Cluster getCluster(AddServiceInfo info) throws AmbariException {\n+    return managementController.getClusters().getCluster(info.clusterName());\n+  }\n+\n+  AddServiceInfo recommendConfigurations(AddServiceInfo info) {\n+    Configuration config = info.getConfig();\n+    if (info.getRequest().getRecommendationStrategy().shouldUseStackAdvisor()) {\n+      LayoutRecommendationInfo layoutInfo = getLayoutRecommendationInfo(info);\n+\n+      Map<String, Set<String>> componentHostMap = getComponentHostMap(layoutInfo.getAllServiceLayouts());\n+      Map<String, Set<String>> hostComponentMap = getHostComponentMap(componentHostMap);\n+      StackAdvisorRequest request = StackAdvisorRequest.StackAdvisorRequestBuilder\n+        .forStack(info.getStack().getStackId())\n+        .ofType(StackAdvisorRequest.StackAdvisorRequestType.CONFIGURATIONS)\n+        .forHosts(layoutInfo.getHosts())\n+        .forServices(layoutInfo.getAllServiceLayouts().keySet())\n+        .forHostComponents(hostComponentMap)\n+        .forHostsGroupBindings(layoutInfo.getHostGroups())\n+        .withComponentHostsMap(componentHostMap)\n+        .withConfigurations(config)\n+        .withGPLLicenseAccepted(serverConfig.getGplLicenseAccepted())\n+        .build();\n+      RecommendationResponse response;\n+      try {\n+        response = stackAdvisorHelper.recommend(request);\n+      }\n+      catch (StackAdvisorException|AmbariException ex) {\n+        throw new IllegalArgumentException(\"Configuration recommendation failed.\", ex);\n+      }\n+      Map<String, RecommendationResponse.BlueprintConfigurations> configRecommendations = response.getRecommendations().getBlueprint().getConfigurations();\n+\n+      // remove recommendations for existing services\n+      configRecommendations.keySet().removeIf(configType -> !info.newServices().containsKey(info.getStack().getServiceForConfigType(configType)));\n+\n+      if (info.getRequest().getRecommendationStrategy() == ConfigRecommendationStrategy.ONLY_STACK_DEFAULTS_APPLY) {\n+        removeNonStackConfigRecommendations(info.getConfig().getParentConfiguration().getParentConfiguration(), configRecommendations);\n+      }\n+\n+      Configuration recommendedConfig = toConfiguration(configRecommendations);\n+\n+      Configuration userConfig = config;\n+      Configuration clusterAndStackConfig = userConfig.getParentConfiguration();\n+\n+      if (info.getRequest().getRecommendationStrategy().shouldOverrideCustomValues()) {\n+        config = recommendedConfig;\n+        config.setParentConfiguration(userConfig);\n+      }\n+      else {\n+        config = userConfig;\n+        config.setParentConfiguration(recommendedConfig);\n+        recommendedConfig.setParentConfiguration(clusterAndStackConfig);\n+      }\n+\n+      StackAdvisorRequest validationRequest = request.builder().withConfigurations(config).build();\n+      validate(validationRequest);\n+    }\n+\n+    UnitUpdater.updateUnits(config, info.getStack());\n+    return info.withConfig(config);\n+  }\n+\n+  /**\n+   * Reuse information from layout recommendation if it happened\n+   */\n+  LayoutRecommendationInfo getLayoutRecommendationInfo(AddServiceInfo info) {\n+    if (info.getRecommendationInfo().isPresent()) {\n+      return info.getRecommendationInfo().get();\n+    }\n+    try {\n+      Map<String, Map<String, Set<String>>> allServices = getAllServices(info);\n+      Map<String, Set<String>> hostGroups =\n+        getHostGroupStrategy().calculateHostGroups(getHostComponentMap(getComponentHostMap(allServices)));\n+      return new LayoutRecommendationInfo(hostGroups, allServices);\n+    }\n+    catch (AmbariException ex) {\n+      throw new IllegalArgumentException(\"Error gathering host groups and services\", ex);\n+    }\n+  }\n+\n+  static void removeNonStackConfigRecommendations(Configuration stackConfig,  Map<String, RecommendationResponse.BlueprintConfigurations> configRecommendations) {\n+    configRecommendations.keySet().removeIf(configType -> !stackConfig.containsConfigType(configType));\n+    configRecommendations.entrySet().forEach( e -> {\n+      String cfgType = e.getKey();\n+      RecommendationResponse.BlueprintConfigurations cfg = e.getValue();\n+      cfg.getProperties().keySet().removeIf(propName -> !stackConfig.containsConfig(cfgType, propName));\n+      if (null != cfg.getPropertyAttributes()) {\n+        cfg.getPropertyAttributes().keySet().removeIf(propName -> !stackConfig.containsConfig(cfgType, propName));\n+      }\n+    });\n+    configRecommendations.values().removeIf(cfg -> cfg.getProperties().isEmpty() && cfg.getPropertyAttributes().isEmpty());\n   }\n \n-  static void keepNewServicesOnly(Map<String,Map<String,Set<String>>> recommendedLayout, Map<String,Map<String,Set<String>>> newServices) {\n-    recommendedLayout.keySet().retainAll(newServices.keySet());\n+  private void validate(StackAdvisorRequest request) {\n+    try {\n+      Set<ValidationResponse.ValidationItem> items = stackAdvisorHelper.validate(request).getItems();\n+      if (!items.isEmpty()) {\n+        LOG.warn(\"Issues found during recommended {} validation:\\n{}\", request.getRequestType(), Joiner.on('\\n').join(items));\n+      }\n+    }\n+    catch (StackAdvisorException ex) {\n+      LOG.error(request.getRequestType() + \" validation failed\", ex);\n+    }\n+  }\n+\n+  static Configuration toConfiguration(Map<String, RecommendationResponse.BlueprintConfigurations> configs) {\n+    Map<String, Map<String, String>> properties = configs.entrySet().stream()\n+      .filter( e -> e.getValue().getProperties() != null && !e.getValue().getProperties().isEmpty())\n+      .map(e -> Pair.of(e.getKey(), e.getValue().getProperties()))\n+      .collect(toMap(Pair::getKey, Pair::getValue));\n+\n+    Map<String, Map<String, Map<String, String>>> propertyAttributes = configs.entrySet().stream()\n+        .filter( e -> e.getValue().getPropertyAttributes() != null && !e.getValue().getPropertyAttributes().isEmpty())\n+        .map(e -> Pair.of(e.getKey(), e.getValue().getPropertyAttributesAsMap()))\n+        .collect(toMap(Pair::getKey, Pair::getValue));\n+\n+    return new Configuration(properties, propertyAttributes);\n+  }\n+\n+  static Map<String,Map<String,Set<String>>> keepNewServicesOnly(Map<String,Map<String,Set<String>>> recommendedLayout, Map<String,Map<String,Set<String>>> newServices) {\n+    HashMap<String, java.util.Map<String, Set<String>>> newServiceRecommendations = new HashMap<>(recommendedLayout);\n+    newServiceRecommendations.keySet().retainAll(newServices.keySet());\n+    return newServiceRecommendations;\n   }\n \n   static Map<String, Map<String, Set<String>>> getRecommendedLayout(Map<String, Set<String>> hostGroupHosts,\n@@ -170,6 +282,7 @@ static void keepNewServicesOnly(Map<String,Map<String,Set<String>>> recommendedL\n         toMap(Map.Entry::getKey, Map.Entry::getValue)));\n   }\n \n+\n   /**\n    * Transform a map of component -> hosts to a map of hosts -> components\n    * @param componentHostMap the map to transform",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/main/java/org/apache/ambari/server/topology/addservice/StackAdvisorAdapter.java",
                "sha": "7504d45d492307cd64fc655b9719849b698b1236",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/test/java/org/apache/ambari/server/controller/AddServiceRequestTest.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/controller/AddServiceRequestTest.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 4,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/controller/AddServiceRequestTest.java",
                "patch": "@@ -26,7 +26,6 @@\n import static org.apache.ambari.server.controller.internal.ProvisionAction.INSTALL_AND_START;\n import static org.apache.ambari.server.controller.internal.ProvisionAction.INSTALL_ONLY;\n import static org.apache.ambari.server.topology.ConfigRecommendationStrategy.ALWAYS_APPLY;\n-import static org.apache.ambari.server.topology.ConfigRecommendationStrategy.NEVER_APPLY;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n@@ -40,6 +39,7 @@\n \n import org.apache.ambari.server.security.encryption.CredentialStoreType;\n import org.apache.ambari.server.state.SecurityType;\n+import org.apache.ambari.server.topology.ConfigRecommendationStrategy;\n import org.apache.ambari.server.topology.Configuration;\n import org.apache.ambari.server.topology.Credential;\n import org.apache.ambari.server.topology.SecurityConfiguration;\n@@ -135,7 +135,7 @@ public void testDeserialize_defaultAndEmptyValues() throws Exception {\n \n     // default / empty values\n     assertEquals(ADD_SERVICE, request.getOperationType());\n-    assertEquals(NEVER_APPLY, request.getRecommendationStrategy());\n+    assertEquals(ConfigRecommendationStrategy.defaultForAddService(), request.getRecommendationStrategy());\n     assertEquals(INSTALL_AND_START, request.getProvisionAction());\n     assertEquals(STRICT, request.getValidationType());\n     assertNull(request.getStackName());\n@@ -159,7 +159,7 @@ public void testDeserialize_onlyServices() throws Exception {\n \n     // default / empty values\n     assertEquals(ADD_SERVICE, request.getOperationType());\n-    assertEquals(NEVER_APPLY, request.getRecommendationStrategy());\n+    assertEquals(ConfigRecommendationStrategy.defaultForAddService(), request.getRecommendationStrategy());\n     assertEquals(INSTALL_AND_START, request.getProvisionAction());\n     assertNull(request.getStackName());\n     assertNull(request.getStackVersion());\n@@ -182,7 +182,7 @@ public void testDeserialize_onlyComponents() throws Exception {\n \n     // default / empty values\n     assertEquals(ADD_SERVICE, request.getOperationType());\n-    assertEquals(NEVER_APPLY, request.getRecommendationStrategy());\n+    assertEquals(ConfigRecommendationStrategy.defaultForAddService(), request.getRecommendationStrategy());\n     assertEquals(INSTALL_AND_START, request.getProvisionAction());\n     assertNull(request.getStackName());\n     assertNull(request.getStackVersion());",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/test/java/org/apache/ambari/server/controller/AddServiceRequestTest.java",
                "sha": "4b90374d2386e8e5930f73246885b9a4593e64cf",
                "status": "modified"
            },
            {
                "additions": 55,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/UnitUpdaterTest.java",
                "changes": 61,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/UnitUpdaterTest.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 6,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/controller/internal/UnitUpdaterTest.java",
                "patch": "@@ -17,6 +17,7 @@\n  */\n package org.apache.ambari.server.controller.internal;\n \n+import static org.apache.ambari.server.testutils.TestCollectionUtils.map;\n import static org.easymock.EasyMock.expect;\n import static org.junit.Assert.assertEquals;\n \n@@ -28,7 +29,7 @@\n import org.apache.ambari.server.state.ValueAttributesInfo;\n import org.apache.ambari.server.topology.Blueprint;\n import org.apache.ambari.server.topology.ClusterTopology;\n-import org.apache.ambari.server.topology.InvalidTopologyException;\n+import org.apache.ambari.server.topology.Configuration;\n import org.easymock.EasyMockRule;\n import org.easymock.EasyMockSupport;\n import org.easymock.Mock;\n@@ -86,10 +87,54 @@ public void testRejectEmptyPropertyValue() throws Exception {\n     updateUnit(OOZIE, OOZIE_ENV, HEAPSIZE, \"\");\n   }\n \n+  @Test\n+  public void updateUnits() {\n+    stackUnitIs(HEAPSIZE, \"MB\");\n+    setUpStack(OOZIE, OOZIE_ENV);\n+\n+    Map<String, Map<String, String>> properties = map(\n+      OOZIE_ENV, map(HEAPSIZE, \"1024\"),\n+      \"core-site\", map(\"fs.trash.interval\", \"360\"));\n+    Configuration configuration = new Configuration(properties, new HashMap<>());\n+\n+    UnitUpdater.updateUnits(configuration, stack);\n+\n+    Map<String, Map<String, String>> expected = map(\n+      OOZIE_ENV, map(HEAPSIZE, \"1024m\"),\n+      \"core-site\", map(\"fs.trash.interval\", \"360\"));\n+\n+    assertEquals(expected, configuration.getProperties());\n+  }\n+\n+  @Test\n+  public void removeUnits() {\n+    stackUnitIs(HEAPSIZE, \"MB\");\n+    setUpStack(OOZIE, OOZIE_ENV);\n+\n+    Map<String, Map<String, String>> properties = map(\n+      OOZIE_ENV, map(HEAPSIZE, \"1024m\"),\n+      \"core-site\", map(\"fs.trash.interval\", \"360\"));\n+    Configuration configuration = new Configuration(properties, new HashMap<>());\n+\n+    UnitUpdater.removeUnits(configuration, stack);\n+\n+    Map<String, Map<String, String>> expected = map(\n+      OOZIE_ENV, map(HEAPSIZE, \"1024\"),\n+      \"core-site\", map(\"fs.trash.interval\", \"360\"));\n+\n+    assertEquals(expected, configuration.getProperties());\n+  }\n+\n   private void stackUnitIs(String name, String unit) {\n     ValueAttributesInfo propertyValueAttributes = new ValueAttributesInfo();\n     propertyValueAttributes.setUnit(unit);\n-    stackConfigWithMetadata.put(name, new Stack.ConfigProperty(new StackConfigurationResponse(\n+    stackConfigWithMetadata.put(name, configProperty(name, unit));\n+  }\n+\n+  public static Stack.ConfigProperty configProperty(String name, String unit) {\n+    ValueAttributesInfo propertyValueAttributes = new ValueAttributesInfo();\n+    propertyValueAttributes.setUnit(unit);\n+    return new Stack.ConfigProperty(new StackConfigurationResponse(\n       name,\n       \"any\",\n       \"any\",\n@@ -99,16 +144,20 @@ private void stackUnitIs(String name, String unit) {\n       Collections.emptySet(),\n       Collections.emptyMap(),\n       propertyValueAttributes,\n-      Collections.emptySet()\n-    )));\n+      Collections.emptySet()));\n   }\n \n-  private String updateUnit(String serviceName, String configType, String propName, String propValue) throws InvalidTopologyException, ConfigurationTopologyException {\n-    UnitUpdater updater = new UnitUpdater(serviceName, configType);\n+  private void setUpStack(String serviceName, String configType) {\n     expect(clusterTopology.getBlueprint()).andReturn(blueprint).anyTimes();\n     expect(blueprint.getStack()).andReturn(stack).anyTimes();\n     expect(stack.getConfigurationPropertiesWithMetadata(serviceName, configType)).andReturn(stackConfigWithMetadata).anyTimes();\n     replayAll();\n+  }\n+\n+  private String updateUnit(String serviceName, String configType, String propName, String propValue) {\n+    UnitUpdater updater = new UnitUpdater(serviceName, configType);\n+    setUpStack(serviceName, configType);\n     return updater.updateForClusterCreate(propName, propValue, Collections.emptyMap(), clusterTopology);\n   }\n+\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/test/java/org/apache/ambari/server/controller/internal/UnitUpdaterTest.java",
                "sha": "fbf448d437a5ef4f5f25bedd945da791ecc33859",
                "status": "modified"
            },
            {
                "additions": 50,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/test/java/org/apache/ambari/server/testutils/TestCollectionUtils.java",
                "changes": 50,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/testutils/TestCollectionUtils.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 0,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/testutils/TestCollectionUtils.java",
                "patch": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.ambari.server.testutils;\n+\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.Map;\n+\n+/**\n+ * Utilities for collections used in unit tests.\n+ */\n+public class TestCollectionUtils {\n+\n+  /**\n+   * A simple (but not production quality) way to create mutable hashmaps for unit tests\n+   * @param firstKey the first key in the map\n+   * @param firstValue the first value in the map\n+   * @param others further keys and values\n+   * @param <K> key type\n+   * @param <V> value type\n+   * @return the map\n+   */\n+  @SuppressWarnings(\"unchecked\")\n+  public static <K, V> Map<K, V> map(K firstKey, V firstValue, Object... others) {\n+    Map<K, V> map = new HashMap<>();\n+    map.put(firstKey, firstValue);\n+    Iterator iterator = Arrays.asList(others).iterator();\n+    while (iterator.hasNext()) {\n+      map.put((K)iterator.next(), (V)iterator.next());\n+    }\n+    return map;\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/test/java/org/apache/ambari/server/testutils/TestCollectionUtils.java",
                "sha": "0a7c5aca9a429fd2abbca9027af8b9c0cc051d8a",
                "status": "added"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/test/java/org/apache/ambari/server/topology/ConfigurableTest.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/topology/ConfigurableTest.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 0,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/topology/ConfigurableTest.java",
                "patch": "@@ -25,6 +25,7 @@\n import java.net.URL;\n import java.nio.charset.StandardCharsets;\n import java.util.List;\n+import java.util.Map;\n import java.util.regex.Pattern;\n \n import org.apache.commons.lang3.exception.ExceptionUtils;\n@@ -135,6 +136,21 @@ public void testParseLegacyConfigurable() throws Exception {\n       configurable.getConfiguration().getProperties());\n   }\n \n+  @Test\n+  public void testTransformAttributesMap() {\n+    Map<String, Map<String, String>> attributes = ImmutableMap.of(\n+      \"propertyName1\", ImmutableMap.of(\"minimum\", \"3000\", \"maximum\", \"4000\"),\n+      \"propertyName2\", ImmutableMap.of(\"minimum\", \"3500\", \"maximum\", \"4500\", \"hidden\", \"true\"));\n+\n+    Map<String, Map<String, String>> transformed = ImmutableMap.of(\n+      \"minimum\", ImmutableMap.of(\"propertyName1\", \"3000\", \"propertyName2\", \"3500\"),\n+      \"maximum\", ImmutableMap.of(\"propertyName1\", \"4000\", \"propertyName2\", \"4500\"),\n+      \"hidden\", ImmutableMap.of(\"propertyName2\", \"true\"));\n+\n+    assertEquals(transformed, ConfigurableHelper.transformAttributesMap(attributes));\n+    assertEquals(attributes, ConfigurableHelper.transformAttributesMap(transformed));\n+  }\n+\n   static class TestConfigurable implements Configurable {\n     Configuration configuration;\n ",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/test/java/org/apache/ambari/server/topology/ConfigurableTest.java",
                "sha": "cc4c33b353e826a9d6a408232f1039aa05faca85",
                "status": "modified"
            },
            {
                "additions": 49,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/test/java/org/apache/ambari/server/topology/ConfigurationTest.java",
                "changes": 49,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/topology/ConfigurationTest.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 0,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/topology/ConfigurationTest.java",
                "patch": "@@ -157,6 +157,55 @@ public void testGetFullProperties_withParent() {\n     assertTrue(configTypes.containsAll(Arrays.asList(\"type1\", \"type2\", \"type3\", \"type4\")));\n   }\n \n+  @Test\n+  public void containsConfigType() {\n+    Configuration configuration = createConfigurationWithParents_PropsOnly();\n+    assertTrue(configuration.containsConfigType(\"type1\"));\n+    assertTrue(configuration.containsConfigType(\"type2\"));\n+    assertTrue(configuration.containsConfigType(\"type3\"));\n+    assertTrue(configuration.containsConfigType(\"type4\"));\n+    assertFalse(configuration.containsConfigType(\"type5\"));\n+\n+    configuration = createConfigurationWithParents_AttributesOnly();\n+    assertTrue(configuration.containsConfigType(\"type1\"));\n+    assertTrue(configuration.containsConfigType(\"type2\"));\n+    assertFalse(configuration.containsConfigType(\"type3\"));\n+  }\n+\n+  @Test\n+  public void containsConfig() {\n+    Configuration configuration = createConfigurationWithParents_PropsOnly();\n+    assertTrue(configuration.containsConfig(\"type1\", \"prop1\"));\n+    assertTrue(configuration.containsConfig(\"type1\", \"prop2\"));\n+    assertTrue(configuration.containsConfig(\"type1\", \"prop3\"));\n+    assertTrue(configuration.containsConfig(\"type2\", \"prop4\"));\n+    assertTrue(configuration.containsConfig(\"type2\", \"prop5\"));\n+    assertTrue(configuration.containsConfig(\"type1\", \"prop6\"));\n+    assertTrue(configuration.containsConfig(\"type1\", \"prop9\"));\n+    assertTrue(configuration.containsConfig(\"type3\", \"prop7\"));\n+    assertTrue(configuration.containsConfig(\"type3\", \"prop8\"));\n+    assertTrue(configuration.containsConfig(\"type4\", \"prop10\"));\n+    assertTrue(configuration.containsConfig(\"type4\", \"prop11\"));\n+    assertFalse(configuration.containsConfig(\"type1\", \"prop99\"));\n+    assertFalse(configuration.containsConfig(\"core-site\", \"io.file.buffer.size\"));\n+\n+    configuration = createConfigurationWithParents_AttributesOnly();\n+    assertTrue(configuration.containsConfig(\"type1\", \"prop1\"));\n+    assertTrue(configuration.containsConfig(\"type1\", \"prop2\"));\n+    assertTrue(configuration.containsConfig(\"type1\", \"prop3\"));\n+    assertTrue(configuration.containsConfig(\"type1\", \"prop6\"));\n+    assertTrue(configuration.containsConfig(\"type1\", \"prop7\"));\n+    assertTrue(configuration.containsConfig(\"type1\", \"prop8\"));\n+    assertTrue(configuration.containsConfig(\"type1\", \"prop9\"));\n+    assertTrue(configuration.containsConfig(\"type1\", \"prop10\"));\n+    assertTrue(configuration.containsConfig(\"type1\", \"prop11\"));\n+    assertTrue(configuration.containsConfig(\"type2\", \"prop100\"));\n+    assertTrue(configuration.containsConfig(\"type2\", \"prop101\"));\n+    assertTrue(configuration.containsConfig(\"type2\", \"prop102\"));\n+    assertFalse(configuration.containsConfig(\"type1\", \"prop99\"));\n+    assertFalse(configuration.containsConfig(\"core-site\", \"io.file.buffer.size\"));\n+  }\n+\n   @Test\n   public void testGetFullProperties_withParent_specifyDepth() {\n     Configuration configuration = createConfigurationWithParents_PropsOnly();",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/test/java/org/apache/ambari/server/topology/ConfigurationTest.java",
                "sha": "51b71937d7b9450669baa23f8b2baf4953ebb6c8",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/test/java/org/apache/ambari/server/topology/addservice/RequestValidatorTest.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/topology/addservice/RequestValidatorTest.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 1,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/topology/addservice/RequestValidatorTest.java",
                "patch": "@@ -48,6 +48,7 @@\n import org.apache.ambari.server.state.kerberos.KerberosDescriptor;\n import org.apache.ambari.server.state.kerberos.KerberosDescriptorFactory;\n import org.apache.ambari.server.state.kerberos.KerberosServiceDescriptor;\n+import org.apache.ambari.server.topology.ConfigRecommendationStrategy;\n import org.apache.ambari.server.topology.Configuration;\n import org.apache.ambari.server.topology.SecurityConfiguration;\n import org.apache.ambari.server.topology.SecurityConfigurationFactory;\n@@ -467,6 +468,7 @@ public void combinesRequestConfigWithClusterAndStack() throws AmbariException {\n     requestConfig.setProperty(\"kafka-broker\", \"zookeeper.connect\", \"zookeeper.connect:request\");\n     requestConfig.setProperty(\"kafka-env\", \"custom_property\", \"custom_property:request\");\n     expect(request.getConfiguration()).andReturn(requestConfig.copy()).anyTimes();\n+    expect(request.getRecommendationStrategy()).andReturn(ConfigRecommendationStrategy.ALWAYS_APPLY).anyTimes();\n \n     Configuration clusterConfig = Configuration.newEmpty();\n     clusterConfig.setProperty(\"zookeeper-env\", \"zk_user\", \"zk_user:cluster_level\");\n@@ -477,7 +479,7 @@ public void combinesRequestConfigWithClusterAndStack() throws AmbariException {\n     stackConfig.setProperty(\"zookeeper-env\", \"zk_user\", \"zk_user:stack_default\");\n     stackConfig.setProperty(\"zookeeper-env\", \"zk_log_dir\", \"zk_log_dir:stack_default\");\n     stackConfig.setProperty(\"kafka-broker\", \"zookeeper.connect\", \"zookeeper.connect:stack_default\");\n-    expect(stack.getValidDefaultConfig()).andReturn(stackConfig).anyTimes();\n+    expect(stack.getDefaultConfig()).andReturn(stackConfig).anyTimes();\n \n     replayAll();\n ",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/test/java/org/apache/ambari/server/topology/addservice/RequestValidatorTest.java",
                "sha": "13fc281a2e114f50fc0ecea5fcd9a79e4c6f00eb",
                "status": "modified"
            },
            {
                "additions": 411,
                "blob_url": "https://github.com/apache/ambari/blob/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/test/java/org/apache/ambari/server/topology/addservice/StackAdvisorAdapterTest.java",
                "changes": 434,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/topology/addservice/StackAdvisorAdapterTest.java?ref=f3aa02f7bb4c765885af9d740c7661dd1cdbca80",
                "deletions": 23,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/topology/addservice/StackAdvisorAdapterTest.java",
                "patch": "@@ -21,31 +21,40 @@\n import static java.util.Collections.emptyMap;\n import static java.util.Collections.emptySet;\n import static java.util.stream.Collectors.toMap;\n+import static org.apache.ambari.server.api.services.stackadvisor.StackAdvisorRequest.StackAdvisorRequestType.HOST_GROUPS;\n+import static org.apache.ambari.server.controller.internal.UnitUpdaterTest.configProperty;\n+import static org.apache.ambari.server.testutils.TestCollectionUtils.map;\n import static org.easymock.EasyMock.anyObject;\n import static org.easymock.EasyMock.anyString;\n import static org.easymock.EasyMock.expect;\n import static org.easymock.EasyMock.getCurrentArguments;\n import static org.easymock.EasyMock.mock;\n import static org.easymock.EasyMock.replay;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertSame;\n \n import java.util.HashMap;\n import java.util.Map;\n import java.util.Set;\n \n import org.apache.ambari.server.api.services.stackadvisor.StackAdvisorHelper;\n+import org.apache.ambari.server.api.services.stackadvisor.StackAdvisorRequest;\n import org.apache.ambari.server.api.services.stackadvisor.recommendations.RecommendationResponse;\n import org.apache.ambari.server.api.services.stackadvisor.validations.ValidationResponse;\n-import org.apache.ambari.server.configuration.Configuration;\n+import org.apache.ambari.server.controller.AddServiceRequest;\n import org.apache.ambari.server.controller.AmbariManagementController;\n import org.apache.ambari.server.controller.internal.Stack;\n import org.apache.ambari.server.state.Cluster;\n import org.apache.ambari.server.state.Clusters;\n import org.apache.ambari.server.state.Service;\n import org.apache.ambari.server.state.ServiceComponent;\n import org.apache.ambari.server.state.StackId;\n+import org.apache.ambari.server.topology.ConfigRecommendationStrategy;\n+import org.apache.ambari.server.topology.Configuration;\n import org.apache.commons.lang3.tuple.Pair;\n import org.easymock.EasyMockRunner;\n+import org.easymock.IExpectationSetters;\n import org.easymock.Mock;\n import org.easymock.TestSubject;\n import org.junit.Before;\n@@ -66,7 +75,7 @@\n   private StackAdvisorHelper stackAdvisorHelper;\n \n   @Mock\n-  private Configuration serverConfig;\n+  private org.apache.ambari.server.configuration.Configuration serverConfig;\n \n   @Mock\n   private Injector injector;\n@@ -157,21 +166,68 @@ public void mergeDisjunctMaps_invalidInput() {\n     StackAdvisorAdapter.mergeDisjunctMaps(map1, map2);\n   }\n \n+  @Test\n+  public void getLayoutRecommendationInfo() {\n+    Map<String, Map<String, Set<String>>> newServices = ImmutableMap.of(\n+      \"KAFKA\", ImmutableMap.of(\n+        \"KAFKA_BROKER\", ImmutableSet.of(\"c7401\")),\n+      \"SPARK2\", ImmutableMap.of(\n+        \"SPARK2_JOBHISTORYSERVER\", ImmutableSet.of(\"c7402\"),\n+        \"SPARK2_CLIENT\", ImmutableSet.of(\"c7403\", \"c7404\")),\n+      \"OOZIE\", ImmutableMap.of(\n+        \"OOZIE_SERVER\", ImmutableSet.of(\"c7401\"),\n+        \"OOZIE_CLIENT\", ImmutableSet.of(\"c7403\", \"c7404\")));\n+\n+    AddServiceInfo info = new AddServiceInfo(request(ConfigRecommendationStrategy.ALWAYS_APPLY), \"c1\", stack , Configuration.newEmpty(),\n+      null, null, newServices, null); // No LayoutReommendationInfo -> needs to be calculated\n+\n+    LayoutRecommendationInfo layoutRecommendationInfo = adapter.getLayoutRecommendationInfo(info);\n+    layoutRecommendationInfo.getAllServiceLayouts();\n+\n+    assertEquals(\n+      ImmutableMap.of(\n+        \"host_group_1\", ImmutableSet.of(\"c7401\"),\n+        \"host_group_2\", ImmutableSet.of(\"c7402\"),\n+        \"host_group_3\", ImmutableSet.of(\"c7403\", \"c7404\")),\n+      layoutRecommendationInfo.getHostGroups());\n+\n+    assertEquals(\n+      ImmutableMap.<String, Map<String, Set<String>>>builder()\n+        .put(\"KAFKA\", ImmutableMap.of(\n+          \"KAFKA_BROKER\", ImmutableSet.of(\"c7401\")))\n+        .put(\"SPARK2\", ImmutableMap.of(\n+          \"SPARK2_JOBHISTORYSERVER\", ImmutableSet.of(\"c7402\"),\n+          \"SPARK2_CLIENT\", ImmutableSet.of(\"c7403\", \"c7404\")))\n+        .put(\"OOZIE\", ImmutableMap.of(\n+          \"OOZIE_SERVER\", ImmutableSet.of(\"c7401\"),\n+          \"OOZIE_CLIENT\", ImmutableSet.of(\"c7403\", \"c7404\")))\n+        .put(\"HDFS\", ImmutableMap.of(\n+          \"NAMENODE\", ImmutableSet.of(\"c7401\"),\n+          \"HDFS_CLIENT\", ImmutableSet.of(\"c7401\", \"c7402\")))\n+        .put(\"ZOOKEEPER\", ImmutableMap.of(\n+          \"ZOOKEEPER_SERVER\", ImmutableSet.of(\"c7401\"),\n+          \"ZOOKEEPER_CLIENT\", ImmutableSet.of(\"c7401\", \"c7402\")))\n+        .put(\"MAPREDUCE2\", ImmutableMap.of(\n+          \"HISTORYSERVER\", ImmutableSet.of(\"c7401\")))\n+        .build(),\n+      layoutRecommendationInfo.getAllServiceLayouts());\n+  }\n+\n   @Test\n   public void keepNewServicesOnly() {\n     Map<String, Map<String, Set<String>>> newServices = ImmutableMap.of(\n       \"KAFKA\", emptyMap(),\n       \"PIG\", emptyMap());\n \n-    Map<String, Map<String, Set<String>>> recommendationForNewServices = ImmutableMap.of(\n+    Map<String, Map<String, Set<String>>> expectedNewServiceRecommendations = ImmutableMap.of(\n       \"KAFKA\", ImmutableMap.of(\"KAFKA_BROKER\", ImmutableSet.of(\"c7405\")),\n       \"PIG\", ImmutableMap.of(\"PIG_CLIENT\", ImmutableSet.of(\"c7405\", \"c7406\")));\n \n     Map<String, Map<String, Set<String>>> recommendations = new HashMap<>(SERVICE_COMPONENT_HOST_MAP_1);\n-    recommendations.putAll(recommendationForNewServices);\n+    recommendations.putAll(expectedNewServiceRecommendations);\n \n-    StackAdvisorAdapter.keepNewServicesOnly(recommendations, newServices);\n-    assertEquals(recommendationForNewServices, recommendations);\n+    Map<String, Map<String, Set<String>>> newServiceRecommendations = StackAdvisorAdapter.keepNewServicesOnly(recommendations, newServices);\n+    assertEquals(expectedNewServiceRecommendations, newServiceRecommendations);\n   }\n \n   @Before\n@@ -182,39 +238,71 @@ public void setUp() throws Exception {\n       \"HDFS\",\n       service(\"HDFS\", ImmutableMap.of(\"NAMENODE\", ImmutableSet.of(\"c7401\"), \"HDFS_CLIENT\", ImmutableSet.of(\"c7401\", \"c7402\"))),\n       \"ZOOKEEPER\",\n-      service(\"ZOOKEEPER\", ImmutableMap.of(\"ZOOKEEPER_SERVER\", ImmutableSet.of(\"c7401\"), \"ZOOKEEPER_CLIENT\", ImmutableSet.of(\"c7401\", \"c7402\")))));\n+      service(\"ZOOKEEPER\", ImmutableMap.of(\"ZOOKEEPER_SERVER\", ImmutableSet.of(\"c7401\"), \"ZOOKEEPER_CLIENT\", ImmutableSet.of(\"c7401\", \"c7402\"))),\n+      \"MAPREDUCE2\",\n+      service(\"MAPREDUCE2\", ImmutableMap.of(\"HISTORYSERVER\", ImmutableSet.of(\"c7401\")))));\n     Clusters clusters = mock(Clusters.class);\n     expect(clusters.getCluster(anyString())).andReturn(cluster).anyTimes();\n     expect(managementController.getClusters()).andReturn(clusters).anyTimes();\n     replay(clusters, cluster, managementController);\n \n     expect(serverConfig.getGplLicenseAccepted()).andReturn(Boolean.FALSE).anyTimes();\n-    expect(serverConfig.getAddServiceHostGroupStrategyClass()).andReturn((Class)GroupByComponentsStrategy.class).anyTimes();\n+    @SuppressWarnings(\"unchecked\")\n+    IExpectationSetters iExpectationSetters = expect(serverConfig.getAddServiceHostGroupStrategyClass()).andReturn((Class) GroupByComponentsStrategy.class).anyTimes();\n     replay(serverConfig);\n \n     expect(injector.getInstance(GroupByComponentsStrategy.class)).andReturn(new GroupByComponentsStrategy()).anyTimes();\n     replay(injector);\n \n-    RecommendationResponse response = new RecommendationResponse();\n-    RecommendationResponse.Recommendation recommendation = new RecommendationResponse.Recommendation();\n-    response.setRecommendations(recommendation);\n     RecommendationResponse.BlueprintClusterBinding binding = RecommendationResponse.BlueprintClusterBinding.fromHostGroupHostMap(\n       ImmutableMap.of(\n         \"hostgroup-1\", ImmutableSet.of(\"c7401\"),\n         \"hostgroup-2\", ImmutableSet.of(\"c7402\")));\n-    recommendation.setBlueprintClusterBinding(binding);\n     RecommendationResponse.Blueprint blueprint = new RecommendationResponse.Blueprint();\n     blueprint.setHostGroups(RecommendationResponse.HostGroup.fromHostGroupComponents(\n       ImmutableMap.of(\n-        \"hostgroup-1\", ImmutableSet.of(\"NAMENODE\", \"HDFS_CLIENT\", \"ZOOKEEPER_SERVER\", \"ZOOKEEPER_CLIENT\"),\n+        \"hostgroup-1\", ImmutableSet.of(\"NAMENODE\", \"HDFS_CLIENT\", \"ZOOKEEPER_SERVER\", \"ZOOKEEPER_CLIENT\", \"HISTORYSERVER\"),\n         \"hostgroup-2\", ImmutableSet.of(\"HDFS_CLIENT\", \"ZOOKEEPER_CLIENT\", \"KAFKA_BROKER\"))\n     ));\n-    recommendation.setBlueprint(blueprint);\n-    expect(stackAdvisorHelper.recommend(anyObject())).andReturn(response);\n+    RecommendationResponse layoutResponse = createRecommendation(blueprint, binding);\n+\n+    RecommendationResponse.Blueprint configBlueprint = new RecommendationResponse.Blueprint();\n+    RecommendationResponse.BlueprintConfigurations kafkaBroker = RecommendationResponse.BlueprintConfigurations.create(\n+      ImmutableMap.of(\"log.dirs\", \"/kafka-logs\", \"offsets.topic.replication.factor\", \"1\"),\n+      ImmutableMap.of(\"maximum\", ImmutableMap.of(\"offsets.topic.replication.factor\", \"10\")));\n+    RecommendationResponse.BlueprintConfigurations spark2Defaults = RecommendationResponse.BlueprintConfigurations.create(\n+      ImmutableMap.of(\"spark.yarn.queue\", \"default\"), null);\n+    RecommendationResponse.BlueprintConfigurations mapredSite = RecommendationResponse.BlueprintConfigurations.create(\n+      ImmutableMap.of(\"mapreduce.map.memory.mb\", \"682\", \"mapreduce.reduce.memory.mb\", \"1364\"),\n+      ImmutableMap.of(\n+        \"minimum\", ImmutableMap.of(\"mapreduce.map.memory.mb\", \"682\", \"mapreduce.reduce.memory.mb\", \"682\"),\n+        \"maximum\" , ImmutableMap.of(\"mapreduce.map.memory.mb\", \"2046\", \"mapreduce.reduce.memory.mb\", \"2046\")));\n+    configBlueprint.setConfigurations(map(\n+      \"kafka-broker\", kafkaBroker,\n+      \"spark2-defaults\", spark2Defaults,\n+      \"mapred-site\", mapredSite\n+    ));\n+    RecommendationResponse configResponse = createRecommendation(configBlueprint, binding);\n+\n+    expect(stackAdvisorHelper.recommend(anyObject())).andAnswer(() -> {\n+      StackAdvisorRequest request = (StackAdvisorRequest) getCurrentArguments()[0];\n+      assertNotNull(request.getHosts());\n+      assertNotNull(request.getServices());\n+      assertNotNull(request.getStackName());\n+      assertNotNull(request.getStackVersion());\n+      assertNotNull(request.getConfigurations());\n+      assertNotNull(request.getHostComponents());\n+      assertNotNull(request.getComponentHostsMap());\n+      assertNotNull(request.getHostGroupBindings());\n+      assertNotNull(request.getLdapConfig());\n+      assertNotNull(request.getRequestType());\n+      return request.getRequestType() == HOST_GROUPS ? layoutResponse : configResponse;\n+    });\n \n     ValidationResponse validationResponse = new ValidationResponse();\n     validationResponse.setItems(emptySet());\n     expect(stackAdvisorHelper.validate(anyObject())).andReturn(validationResponse);\n+\n     replay(stackAdvisorHelper);\n \n     expect(stack.getStackId()).andReturn(new StackId(\"HDP\", \"3.0\")).anyTimes();\n@@ -224,11 +312,32 @@ public void setUp() throws Exception {\n       .put(\"HDFS_CLIENT\", \"HDFS\")\n       .put(\"ZOOKEEPER_SERVER\", \"ZOOKEEPER\")\n       .put(\"ZOOKEEPER_CLIENT\", \"ZOOKEEPER\")\n+      .put(\"HISTORYSERVER\", \"MAPREDUCE2\")\n       .build();\n     expect(stack.getServiceForComponent(anyString())).andAnswer(() -> serviceComponentMap.get(getCurrentArguments()[0])).anyTimes();\n+    ImmutableMap<String, String> configTypeServiceMap = ImmutableMap.<String, String>builder()\n+      .put(\"kafka-broker\", \"KAFKA\")\n+      .put(\"spark2-defaults\", \"SPARK2\")\n+      .put(\"mapred-site\", \"MAPREDUCE2\")\n+      .build();\n+    expect(stack.getServiceForConfigType(anyString())).andAnswer(() -> configTypeServiceMap.get(getCurrentArguments()[0])).anyTimes();\n+    expect(stack.getConfigurationPropertiesWithMetadata(\"OOZIE\", \"oozie-env\")).andReturn(\n+      ImmutableMap.of(\n+        \"mapreduce.map.memory.mb\", configProperty(\"mapreduce.map.memory.mb\", \"MB\"),\n+        \"mapreduce.reduce.memory.mb\", configProperty(\"mapreduce.reduce.memory.mb\", \"MB\"))).anyTimes();\n     replay(stack);\n   }\n \n+  private static RecommendationResponse createRecommendation(RecommendationResponse.Blueprint blueprint,\n+                                                                            RecommendationResponse.BlueprintClusterBinding binding) {\n+    RecommendationResponse response = new RecommendationResponse();\n+    RecommendationResponse.Recommendation recommendation = new RecommendationResponse.Recommendation();\n+    response.setRecommendations(recommendation);\n+    recommendation.setBlueprint(blueprint);\n+    recommendation.setBlueprintClusterBinding(binding);\n+    return response;\n+  }\n+\n   private static Service service(String name, ImmutableMap<String,ImmutableSet<String>> componentHostMap) {\n     Service service = mock(Service.class);\n     expect(service.getName()).andReturn(name).anyTimes();\n@@ -252,7 +361,7 @@ public void recommendLayout() {\n       \"KAFKA\",\n       ImmutableMap.of(\"KAFKA_BROKER\", emptySet()));\n \n-    AddServiceInfo info = new AddServiceInfo(null, \"c1\", stack, org.apache.ambari.server.topology.Configuration.newEmpty(), null, null, newServices);\n+    AddServiceInfo info = new AddServiceInfo(null, \"c1\", stack, org.apache.ambari.server.topology.Configuration.newEmpty(), null, null, newServices, null);\n     AddServiceInfo infoWithRecommendations = adapter.recommendLayout(info);\n \n     Map<String, Map<String, Set<String>>> expectedNewLayout = ImmutableMap.of(\n@@ -263,13 +372,292 @@ public void recommendLayout() {\n     assertEquals(expectedNewLayout, infoWithRecommendations.newServices());\n   }\n \n+  @Test\n+  public void recommendConfigurations_noLayoutInfo() {\n+    Map<String, Map<String, Set<String>>> newServices = ImmutableMap.of(\n+      \"KAFKA\", ImmutableMap.of(\n+        \"KAFKA_BROKER\", ImmutableSet.of(\"c7401\")),\n+      \"SPARK2\", ImmutableMap.of(\n+        \"SPARK2_JOBHISTORYSERVER\", ImmutableSet.of(\"c7402\"),\n+        \"SPARK2_CLIENT\", ImmutableSet.of(\"c7403\", \"c7404\")),\n+      \"OOZIE\", ImmutableMap.of(\n+        \"OOZIE_SERVER\", ImmutableSet.of(\"c7401\"),\n+        \"OOZIE_CLIENT\", ImmutableSet.of(\"c7403\", \"c7404\")));\n+\n+    Configuration stackConfig = Configuration.newEmpty();\n+    Configuration clusterConfig = new Configuration(\n+      map(\"oozie-env\", map(\"oozie_heapsize\", \"1024\", \"oozie_permsize\", \"256\")),\n+      emptyMap());\n+    Configuration userConfig = Configuration.newEmpty();\n+    userConfig.setParentConfiguration(clusterConfig);\n+    clusterConfig.setParentConfiguration(stackConfig);\n+\n+    AddServiceInfo info = new AddServiceInfo(request(ConfigRecommendationStrategy.ALWAYS_APPLY), \"c1\", stack , userConfig,\n+      null, null, newServices, null); // No LayoutRecommendationInfo\n+    AddServiceInfo infoWithConfig = adapter.recommendConfigurations(info);\n+\n+    Configuration recommendedConfig = infoWithConfig.getConfig();\n+    assertSame(userConfig, recommendedConfig.getParentConfiguration());\n+    assertSame(clusterConfig, userConfig.getParentConfiguration());\n+    assertSame(stackConfig, clusterConfig.getParentConfiguration());\n+\n+    // Yarn/Mapred config is excpected to be removed as it does not belong to newly added services\n+    assertEquals(\n+      ImmutableMap.of(\n+        \"kafka-broker\", ImmutableMap.of(\n+          \"log.dirs\", \"/kafka-logs\",\n+          \"offsets.topic.replication.factor\", \"1\"),\n+        \"spark2-defaults\", ImmutableMap.of(\n+          \"spark.yarn.queue\", \"default\"),\n+        \"oozie-env\", ImmutableMap.of(\n+          \"oozie_heapsize\", \"1024m\",  // unit updates should happen\n+          \"oozie_permsize\", \"256m\")),\n+      recommendedConfig.getProperties());\n+\n+    assertEquals(\n+      ImmutableMap.of(\n+        \"kafka-broker\", ImmutableMap.of(\n+          \"maximum\", ImmutableMap.of(\"offsets.topic.replication.factor\", \"10\"))),\n+      recommendedConfig.getAttributes());\n+  }\n \n-  private static Map<String, Map<String, Set<String>>> mutableCopy(Map<String, Map<String, Set<String>>> map) {\n-    Map<String, Map<String, Set<String>>> copy = new HashMap<>();\n-    map.entrySet().forEach( outer -> {\n-      Map<String, Set<String>> innerCopy = new HashMap<>(outer.getValue());\n-      copy.put(outer.getKey(), innerCopy);\n-    });\n-    return copy;\n+  @Test\n+  public void recommendConfigurations_alwaysApply() {\n+    Map<String, Map<String, Set<String>>> newServices = ImmutableMap.of(\n+      \"KAFKA\", ImmutableMap.of(\n+        \"KAFKA_BROKER\", ImmutableSet.of(\"c7401\")),\n+      \"SPARK2\", ImmutableMap.of(\n+        \"SPARK2_JOBHISTORYSERVER\", ImmutableSet.of(\"c7402\"),\n+        \"SPARK2_CLIENT\", ImmutableSet.of(\"c7403\", \"c7404\")),\n+      \"OOZIE\", ImmutableMap.of(\n+        \"OOZIE_SERVER\", ImmutableSet.of(\"c7401\"),\n+        \"OOZIE_CLIENT\", ImmutableSet.of(\"c7403\", \"c7404\")));\n+\n+    Configuration stackConfig = Configuration.newEmpty();\n+    Configuration clusterConfig = new Configuration(\n+      map(\"oozie-env\", map(\"oozie_heapsize\", \"1024\", \"oozie_permsize\", \"256\")),\n+      emptyMap());\n+    Configuration userConfig = Configuration.newEmpty();\n+    userConfig.setParentConfiguration(clusterConfig);\n+    clusterConfig.setParentConfiguration(stackConfig);\n+\n+    LayoutRecommendationInfo layoutRecommendationInfo = new LayoutRecommendationInfo(new HashMap<>(), new HashMap<>()); // contents doesn't matter for the test\n+    AddServiceInfo info = new AddServiceInfo(request(ConfigRecommendationStrategy.ALWAYS_APPLY), \"c1\", stack , userConfig,\n+      null, null, newServices, layoutRecommendationInfo);\n+    AddServiceInfo infoWithConfig = adapter.recommendConfigurations(info);\n+\n+    Configuration recommendedConfig = infoWithConfig.getConfig();\n+    assertSame(userConfig, recommendedConfig.getParentConfiguration());\n+    assertSame(clusterConfig, userConfig.getParentConfiguration());\n+    assertSame(stackConfig, clusterConfig.getParentConfiguration());\n+\n+    // Yarn/Mapred config is excpected to be removed as it does not belong to newly added services\n+    assertEquals(\n+      ImmutableMap.of(\n+        \"kafka-broker\", ImmutableMap.of(\n+          \"log.dirs\", \"/kafka-logs\",\n+          \"offsets.topic.replication.factor\", \"1\"),\n+        \"spark2-defaults\", ImmutableMap.of(\n+          \"spark.yarn.queue\", \"default\"),\n+        \"oozie-env\", ImmutableMap.of(\n+          \"oozie_heapsize\", \"1024m\",  // unit updates should happen\n+          \"oozie_permsize\", \"256m\")),\n+      recommendedConfig.getProperties());\n+\n+    assertEquals(\n+      ImmutableMap.of(\n+        \"kafka-broker\", ImmutableMap.of(\n+          \"maximum\", ImmutableMap.of(\"offsets.topic.replication.factor\", \"10\"))),\n+      recommendedConfig.getAttributes());\n+  }\n+\n+  @Test\n+  public void recommendConfigurations_alwaysDoNotOverrideCustomValues() {\n+    Map<String, Map<String, Set<String>>> newServices = ImmutableMap.of(\n+      \"KAFKA\", ImmutableMap.of(\n+        \"KAFKA_BROKER\", ImmutableSet.of(\"c7401\")),\n+      \"SPARK2\", ImmutableMap.of(\n+        \"SPARK2_JOBHISTORYSERVER\", ImmutableSet.of(\"c7402\"),\n+        \"SPARK2_CLIENT\", ImmutableSet.of(\"c7403\", \"c7404\")),\n+      \"OOZIE\", ImmutableMap.of(\n+        \"OOZIE_SERVER\", ImmutableSet.of(\"c7401\"),\n+        \"OOZIE_CLIENT\", ImmutableSet.of(\"c7403\", \"c7404\")));\n+\n+    Configuration stackConfig = Configuration.newEmpty();\n+    Configuration clusterConfig = new Configuration(\n+      map(\"oozie-env\", map(\"oozie_heapsize\", \"1024\", \"oozie_permsize\", \"256\")),\n+      emptyMap());\n+    Configuration userConfig = Configuration.newEmpty();\n+    userConfig.setParentConfiguration(clusterConfig);\n+    clusterConfig.setParentConfiguration(stackConfig);\n+\n+    LayoutRecommendationInfo layoutRecommendationInfo = new LayoutRecommendationInfo(new HashMap<>(), new HashMap<>()); // contents doesn't matter for the test\n+    AddServiceInfo info = new AddServiceInfo(request(ConfigRecommendationStrategy.ALWAYS_APPLY_DONT_OVERRIDE_CUSTOM_VALUES), \"c1\", stack , userConfig,\n+      null, null, newServices, layoutRecommendationInfo);\n+    AddServiceInfo infoWithConfig = adapter.recommendConfigurations(info);\n+\n+    assertSame(userConfig, infoWithConfig.getConfig()); // user config stays top priority\n+    Configuration recommendedConfig = userConfig.getParentConfiguration();\n+    assertSame(clusterConfig, recommendedConfig.getParentConfiguration());\n+    assertSame(stackConfig, clusterConfig.getParentConfiguration());\n+\n+    // Yarn/Mapred config is excpected to be removed as it does not belong to newly added services\n+    assertEquals(\n+      ImmutableMap.of(\n+        \"kafka-broker\", ImmutableMap.of(\n+          \"log.dirs\", \"/kafka-logs\",\n+          \"offsets.topic.replication.factor\", \"1\"),\n+        \"spark2-defaults\", ImmutableMap.of(\n+          \"spark.yarn.queue\", \"default\")),\n+      recommendedConfig.getProperties());\n+\n+    // the result of unit updates always happen in the top level config\n+    assertEquals(\n+      ImmutableMap.of(\n+        \"oozie-env\", ImmutableMap.of(\n+          \"oozie_heapsize\", \"1024m\",\n+          \"oozie_permsize\", \"256m\")),\n+      userConfig.getProperties());\n+\n+    assertEquals(\n+      ImmutableMap.of(\n+        \"kafka-broker\", ImmutableMap.of(\n+          \"maximum\", ImmutableMap.of(\"offsets.topic.replication.factor\", \"10\"))),\n+      recommendedConfig.getAttributes());\n+  }\n+\n+  @Test\n+  public void recommendConfigurations_neverApply() {\n+    Map<String, Map<String, Set<String>>> newServices = ImmutableMap.of(\n+      \"KAFKA\", ImmutableMap.of(\n+        \"KAFKA_BROKER\", ImmutableSet.of(\"c7401\")),\n+      \"SPARK2\", ImmutableMap.of(\n+        \"SPARK2_JOBHISTORYSERVER\", ImmutableSet.of(\"c7402\"),\n+        \"SPARK2_CLIENT\", ImmutableSet.of(\"c7403\", \"c7404\")),\n+      \"OOZIE\", ImmutableMap.of(\n+        \"OOZIE_SERVER\", ImmutableSet.of(\"c7401\"),\n+        \"OOZIE_CLIENT\", ImmutableSet.of(\"c7403\", \"c7404\")));\n+\n+    Configuration stackConfig = Configuration.newEmpty();\n+    Configuration clusterConfig = new Configuration(\n+      map(\"oozie-env\", map(\"oozie_heapsize\", \"1024\", \"oozie_permsize\", \"256\")),\n+      emptyMap());\n+    Configuration userConfig = Configuration.newEmpty();\n+    userConfig.setParentConfiguration(clusterConfig);\n+    clusterConfig.setParentConfiguration(stackConfig);\n+\n+    LayoutRecommendationInfo layoutRecommendationInfo = new LayoutRecommendationInfo(new HashMap<>(), new HashMap<>()); // contents doesn't matter for the test\n+    AddServiceInfo info = new AddServiceInfo(request(ConfigRecommendationStrategy.NEVER_APPLY), \"c1\", stack , userConfig,\n+      null, null, newServices, layoutRecommendationInfo);\n+    AddServiceInfo infoWithConfig = adapter.recommendConfigurations(info);\n+\n+    // No recommended config, no stack config\n+    assertSame(userConfig, infoWithConfig.getConfig());\n+    assertSame(clusterConfig, userConfig.getParentConfiguration());\n+    assertNotNull(clusterConfig.getParentConfiguration());\n+\n+    // the result of unit updates always happen in the top level config\n+    assertEquals(\n+      ImmutableMap.of(\n+        \"oozie-env\", ImmutableMap.of(\n+          \"oozie_heapsize\", \"1024m\",\n+          \"oozie_permsize\", \"256m\")),\n+      userConfig.getProperties());\n   }\n+\n+  @Test\n+  public void recommendConfigurations_onlyStackDefaultsApply() {\n+    Map<String, Map<String, Set<String>>> newServices = ImmutableMap.of(\n+      \"KAFKA\", ImmutableMap.of(\n+        \"KAFKA_BROKER\", ImmutableSet.of(\"c7401\")),\n+      \"SPARK2\", ImmutableMap.of(\n+        \"SPARK2_JOBHISTORYSERVER\", ImmutableSet.of(\"c7402\"),\n+        \"SPARK2_CLIENT\", ImmutableSet.of(\"c7403\", \"c7404\")),\n+      \"OOZIE\", ImmutableMap.of(\n+        \"OOZIE_SERVER\", ImmutableSet.of(\"c7401\"),\n+        \"OOZIE_CLIENT\", ImmutableSet.of(\"c7403\", \"c7404\")));\n+\n+    Configuration stackConfig = new Configuration(\n+      ImmutableMap.of(\"kafka-broker\", ImmutableMap.of(\"log.dirs\", \"/kafka-logs-stackdefault\")),\n+      ImmutableMap.of());\n+    Configuration clusterConfig = new Configuration(\n+      ImmutableMap.of(\"oozie-env\", ImmutableMap.of(\"oozie_heapsize\", \"1024\", \"oozie_permsize\", \"256\")),\n+      emptyMap());\n+    Configuration userConfig = Configuration.newEmpty();\n+    userConfig.setParentConfiguration(clusterConfig);\n+    clusterConfig.setParentConfiguration(stackConfig);\n+\n+    LayoutRecommendationInfo layoutRecommendationInfo = new LayoutRecommendationInfo(new HashMap<>(), new HashMap<>()); // contents doesn't matter for the test\n+    AddServiceInfo info = new AddServiceInfo(request(ConfigRecommendationStrategy.ONLY_STACK_DEFAULTS_APPLY), \"c1\", stack , userConfig,\n+      null, null, newServices, layoutRecommendationInfo);\n+    AddServiceInfo infoWithConfig = adapter.recommendConfigurations(info);\n+    Configuration recommendedConfig = infoWithConfig.getConfig().getParentConfiguration();\n+\n+    // No recommended config\n+    assertSame(userConfig, infoWithConfig.getConfig()); // user config is top level in this case\n+    assertSame(clusterConfig, recommendedConfig.getParentConfiguration());\n+    assertSame(stackConfig, clusterConfig.getParentConfiguration());\n+\n+    assertEquals(\n+      ImmutableMap.of(\n+        \"kafka-broker\", ImmutableMap.of(\n+          \"log.dirs\", \"/kafka-logs\")),\n+      recommendedConfig.getProperties());\n+\n+    // the result of unit updates always happen in the top level config\n+    assertEquals(\n+      ImmutableMap.of(\n+        \"oozie-env\", ImmutableMap.of(\n+          \"oozie_heapsize\", \"1024m\",\n+          \"oozie_permsize\", \"256m\")),\n+      userConfig.getProperties());\n+  }\n+\n+  @Test\n+  public void removeNonStackConfigRecommendations() {\n+    Map<String, Map<String, String>> stackProperties = ImmutableMap.of(\n+      \"kafka-broker\", ImmutableMap.of(\n+        \"log.dirs\", \"/kafka-logs\",\n+        \"offsets.topic.replication.factor\", \"1\"),\n+      \"spark2-defaults\", ImmutableMap.of(\n+        \"spark.yarn.queue\", \"default\"));\n+\n+    Map<String, Map<String, Map<String, String>>> stackAttributes = ImmutableMap.of(\n+      \"oozie-env\",\n+      ImmutableMap.of(\n+        \"miniumum\",\n+        ImmutableMap.of(\"oozie_heapsize\", \"1024\", \"oozie_permsize\", \"256\")));\n+\n+    Configuration stackConfig = new Configuration(stackProperties, stackAttributes);\n+\n+    Map<String, RecommendationResponse.BlueprintConfigurations> recommendedConfigs =\n+      map(\n+        \"hdfs-site\", RecommendationResponse.BlueprintConfigurations.create(\n+          map(\"dfs.namenode.name.dir\", \"/hadoop/hdfs/namenode\"),\n+          map(\"visible\", ImmutableMap.of(\"dfs.namenode.name.dir\", \"false\"))),\n+        \"oozie-env\", RecommendationResponse.BlueprintConfigurations.create(\n+          map(\"oozie_heapsize\", \"2048\"),\n+          new HashMap<>()),\n+        \"spark2-defaults\", RecommendationResponse.BlueprintConfigurations.create(\n+          map(\"spark.yarn.queue\", \"spark2\"),\n+          new HashMap<>()));\n+\n+    Map<String, RecommendationResponse.BlueprintConfigurations> recommendedConfigsForStackDefaults =\n+      ImmutableMap.of(\n+        \"oozie-env\", RecommendationResponse.BlueprintConfigurations.create(\n+          ImmutableMap.of(\"oozie_heapsize\", \"2048\"),\n+          ImmutableMap.of()),\n+        \"spark2-defaults\", RecommendationResponse.BlueprintConfigurations.create(\n+          ImmutableMap.of(\"spark.yarn.queue\", \"spark2\"),\n+          ImmutableMap.of()));\n+\n+    StackAdvisorAdapter.removeNonStackConfigRecommendations(stackConfig, recommendedConfigs);\n+\n+    assertEquals(recommendedConfigsForStackDefaults, recommendedConfigs);\n+  }\n+\n+  private AddServiceRequest request(ConfigRecommendationStrategy strategy) {\n+    return new AddServiceRequest(null, strategy, null, null, null, null, null, null, null, null, null);\n+  }\n+\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/ambari/raw/f3aa02f7bb4c765885af9d740c7661dd1cdbca80/ambari-server/src/test/java/org/apache/ambari/server/topology/addservice/StackAdvisorAdapterTest.java",
                "sha": "78f481e92ca06fb9355cc23a2e70d88c5bfb05a2",
                "status": "modified"
            }
        ],
        "message": "AMBARI-25024 Config recommendation for add service request (benyoka) (#2704)\n\n* AMBARI-25024 Config recommendation for add service request (benyoka)\r\n\r\n* AMBARI-25024 review comments (benyoka)\r\n\r\n* AMBARI-25024 fix unit test (benyoka)\r\n\r\n* AMBARI-25024 fix unit tests, work without layout recommendation\r\n\r\n* AMBARI-25024 fix merge compile issue (benyoka)\r\n\r\n* AMBARI-25024 fix NPE (benyoka)",
        "parent": "https://github.com/apache/ambari/commit/26575d3df20503f798f5be390442dd6a3201f182",
        "repo": "ambari",
        "unit_tests": [
            "RecommendationResponseTest.java",
            "AddServiceRequestTest.java",
            "BlueprintConfigurationProcessorTest.java",
            "StackTest.java",
            "UnitUpdaterTest.java",
            "ConfigurationTest.java",
            "RequestValidatorTest.java",
            "StackAdvisorAdapterTest.java"
        ]
    },
    "ambari_f6aec32": {
        "bug_id": "ambari_f6aec32",
        "commit": "https://github.com/apache/ambari/commit/f6aec3286da30d42c7faef2e84e88b968bcdf040",
        "file": [
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/ambari/blob/f6aec3286da30d42c7faef2e84e88b968bcdf040/ambari-server/src/main/java/org/apache/ambari/server/controller/ControllerModule.java",
                "changes": 41,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/ControllerModule.java?ref=f6aec3286da30d42c7faef2e84e88b968bcdf040",
                "deletions": 18,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/ControllerModule.java",
                "patch": "@@ -666,25 +666,30 @@ private void installFactories() {\n     // the dispatch factory\n     for (BeanDefinition beanDefinition : beanDefinitions) {\n       String className = beanDefinition.getBeanClassName();\n-      Class<?> clazz = ClassUtils.resolveClassName(className,\n-          ClassUtils.getDefaultClassLoader());\n-\n-      try {\n-        NotificationDispatcher dispatcher;\n-        if (clazz.equals(AmbariSNMPDispatcher.class)) {\n-          dispatcher = (NotificationDispatcher) clazz.getConstructor(Integer.class).newInstance(configuration.getAmbariSNMPUdpBindPort());\n-        } else if (clazz.equals(SNMPDispatcher.class)) {\n-          dispatcher = (NotificationDispatcher) clazz.getConstructor(Integer.class).newInstance(configuration.getSNMPUdpBindPort());\n-        } else {\n-          dispatcher = (NotificationDispatcher) clazz.newInstance();\n+      if (className != null) {\n+        Class<?> clazz = ClassUtils.resolveClassName(className,\n+                ClassUtils.getDefaultClassLoader());\n+        try {\n+          NotificationDispatcher dispatcher;\n+          if (clazz.equals(AmbariSNMPDispatcher.class)) {\n+            dispatcher = (NotificationDispatcher) clazz.getConstructor(Integer.class)\n+                    .newInstance(configuration.getAmbariSNMPUdpBindPort());\n+          } else if (clazz.equals(SNMPDispatcher.class)) {\n+            dispatcher = (NotificationDispatcher) clazz.getConstructor(Integer.class)\n+                    .newInstance(configuration.getSNMPUdpBindPort());\n+          } else {\n+            dispatcher = (NotificationDispatcher) clazz.newInstance();\n+          }\n+          dispatchFactory.register(dispatcher.getType(), dispatcher);\n+          bind((Class<NotificationDispatcher>) clazz).toInstance(dispatcher);\n+          LOG.info(\"Binding and registering notification dispatcher {}\", clazz);\n+        } catch (Exception exception) {\n+          LOG.error(\"Unable to bind and register notification dispatcher {}\",\n+                  clazz, exception);\n         }\n-        dispatchFactory.register(dispatcher.getType(), dispatcher);\n-        bind((Class<NotificationDispatcher>) clazz).toInstance(dispatcher);\n-\n-        LOG.info(\"Binding and registering notification dispatcher {}\", clazz);\n-      } catch (Exception exception) {\n-        LOG.error(\"Unable to bind and register notification dispatcher {}\",\n-            clazz, exception);\n+      } else {\n+        LOG.error(\"Binding and registering notification dispatcher is not possible for\" +\n+            \" beanDefinition: {} in the absence of className\", beanDefinition);\n       }\n     }\n ",
                "raw_url": "https://github.com/apache/ambari/raw/f6aec3286da30d42c7faef2e84e88b968bcdf040/ambari-server/src/main/java/org/apache/ambari/server/controller/ControllerModule.java",
                "sha": "f587c47b438fd98609f7b128f4db76cefcf77e62",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/ambari/blob/f6aec3286da30d42c7faef2e84e88b968bcdf040/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ClusterResourceProvider.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ClusterResourceProvider.java?ref=f6aec3286da30d42c7faef2e84e88b968bcdf040",
                "deletions": 7,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ClusterResourceProvider.java",
                "patch": "@@ -466,16 +466,26 @@ protected static ServiceConfigVersionRequest getServiceConfigVersionRequest(Stri\n       String absCategory = PropertyHelper.getPropertyCategory(entry.getKey());\n       String propName = PropertyHelper.getPropertyName(entry.getKey());\n \n-      if (absCategory.startsWith(parentCategory + \"/desired_service_config_version\")) {\n+      if (absCategory != null &&\n+              absCategory.startsWith(parentCategory + \"/desired_service_config_version\")) {\n         serviceConfigVersionRequest =\n             (serviceConfigVersionRequest ==null ) ? new ServiceConfigVersionRequest() : serviceConfigVersionRequest;\n \n-        if (propName.equals(\"service_name\"))\n-          serviceConfigVersionRequest.setServiceName(entry.getValue().toString());\n-        else if (propName.equals(\"service_config_version\"))\n-          serviceConfigVersionRequest.setVersion(Long.valueOf(entry.getValue().toString()));\n-        else if (propName.equals(\"service_config_version_note\")) {\n-          serviceConfigVersionRequest.setNote(entry.getValue().toString());\n+        if (propName != null) {\n+          switch (propName) {\n+            case \"service_name\": {\n+              serviceConfigVersionRequest.setServiceName(entry.getValue().toString());\n+              break;\n+            }\n+            case \"service_config_version\": {\n+              serviceConfigVersionRequest.setVersion(Long.valueOf(entry.getValue().toString()));\n+              break;\n+            }\n+            case \"service_config_version_note\": {\n+              serviceConfigVersionRequest.setNote(entry.getValue().toString());\n+              break;\n+            }\n+          }\n         }\n       }\n     }",
                "raw_url": "https://github.com/apache/ambari/raw/f6aec3286da30d42c7faef2e84e88b968bcdf040/ambari-server/src/main/java/org/apache/ambari/server/controller/internal/ClusterResourceProvider.java",
                "sha": "1c62f9ce2d92196c324dfdc072d970bc851b2f8c",
                "status": "modified"
            }
        ],
        "message": "AMBARI-25327 : Prevent NPE for bindNotificationDispatchers and getServiceConfigVersionRequest (#3037)\n\n* AMBARI-25327 : Prevent NPE for bindNotificationDispatchers and getServiceConfigVersionRequest\r\n\r\n* minor change\r\n\r\n* logging null classname instance",
        "parent": "https://github.com/apache/ambari/commit/b066df1d3e2683251dc588cddd94fcdcfeaa1178",
        "repo": "ambari",
        "unit_tests": [
            "ClusterResourceProviderTest.java"
        ]
    },
    "ambari_f8fe788": {
        "bug_id": "ambari_f8fe788",
        "commit": "https://github.com/apache/ambari/commit/f8fe788a2d208acff4a88fc5a1a17549cfe16100",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/ambari/blob/f8fe788a2d208acff4a88fc5a1a17549cfe16100/ambari-server/src/main/java/org/apache/ambari/server/topology/ClusterTopologyImpl.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/main/java/org/apache/ambari/server/topology/ClusterTopologyImpl.java?ref=f8fe788a2d208acff4a88fc5a1a17549cfe16100",
                "deletions": 1,
                "filename": "ambari-server/src/main/java/org/apache/ambari/server/topology/ClusterTopologyImpl.java",
                "patch": "@@ -173,7 +173,12 @@ public void addHostToTopology(String hostGroupName, String host) throws InvalidT\n     Collection<String> hosts = new ArrayList<String>();\n     Collection<String> hostGroups = getHostGroupsForComponent(component);\n     for (String group : hostGroups) {\n-      hosts.addAll(getHostGroupInfo().get(group).getHostNames());\n+      HostGroupInfo hostGroupInfo = getHostGroupInfo().get(group);\n+      if (hostGroupInfo != null) {\n+        hosts.addAll(hostGroupInfo.getHostNames());\n+      } else {\n+        LOG.warn(\"HostGroup {} not found, when checking for hosts for component {}\", group, component);\n+      }\n     }\n     return hosts;\n   }",
                "raw_url": "https://github.com/apache/ambari/raw/f8fe788a2d208acff4a88fc5a1a17549cfe16100/ambari-server/src/main/java/org/apache/ambari/server/topology/ClusterTopologyImpl.java",
                "sha": "05dc504ff1a5963193417e2bde879795ba7313d0",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/ambari/blob/f8fe788a2d208acff4a88fc5a1a17549cfe16100/ambari-server/src/test/java/org/apache/ambari/server/topology/ClusterTopologyImplTest.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/ambari/contents/ambari-server/src/test/java/org/apache/ambari/server/topology/ClusterTopologyImplTest.java?ref=f8fe788a2d208acff4a88fc5a1a17549cfe16100",
                "deletions": 1,
                "filename": "ambari-server/src/test/java/org/apache/ambari/server/topology/ClusterTopologyImplTest.java",
                "patch": "@@ -24,6 +24,7 @@\n \n import java.util.ArrayList;\n import java.util.Collection;\n+import java.util.Collections;\n import java.util.HashMap;\n import java.util.HashSet;\n import java.util.List;\n@@ -101,6 +102,11 @@ public void setUp() throws Exception {\n     Set<Component> group1Components = new HashSet<Component>();\n     group1Components.add(new Component(\"component1\"));\n     group1Components.add(new Component(\"component2\"));\n+\n+    Set<String> group1ComponentNames = new HashSet<String>();\n+    group1ComponentNames.add(\"component1\");\n+    group1ComponentNames.add(\"component2\");\n+\n     Set<Component> group2Components = new HashSet<Component>();\n     group2Components.add(new Component(\"component3\"));\n     Set<Component> group3Components = new HashSet<Component>();\n@@ -123,6 +129,11 @@ public void setUp() throws Exception {\n     expect(group2.getComponents()).andReturn(group2Components).anyTimes();\n     expect(group3.getComponents()).andReturn(group3Components).anyTimes();\n     expect(group4.getComponents()).andReturn(group4Components).anyTimes();\n+\n+    expect(group1.getComponentNames()).andReturn(group1ComponentNames).anyTimes();\n+    expect(group2.getComponentNames()).andReturn(Collections.singletonList(\"component3\")).anyTimes();\n+    expect(group3.getComponentNames()).andReturn(Collections.singletonList(\"component4\")).anyTimes();\n+    expect(group4.getComponentNames()).andReturn(Collections.singletonList(\"component5\")).anyTimes();\n   }\n \n   @After\n@@ -156,7 +167,7 @@ public void testCreate_validatorFails() throws Exception {\n   }\n \n   @Test\n-  public void testCreate_validatorSuccess() throws Exception {\n+     public void testCreate_validatorSuccess() throws Exception {\n     TestTopologyRequest request = new TestTopologyRequest(TopologyRequest.Type.PROVISION);\n \n     TopologyValidator validator = createStrictMock(TopologyValidator.class);\n@@ -182,6 +193,21 @@ public void testCreate_duplicateHosts() throws Exception {\n     new ClusterTopologyImpl(null, request);\n   }\n \n+  @Test\n+  public void test_GetHostAssigmentForComponents() throws Exception {\n+    TestTopologyRequest request = new TestTopologyRequest(TopologyRequest.Type.PROVISION);\n+\n+    TopologyValidator validator = createStrictMock(TopologyValidator.class);\n+    topologyValidators.add(validator);\n+\n+    validator.validate((ClusterTopology) notNull());\n+\n+    replayAll();\n+    replay(validator);\n+\n+    new ClusterTopologyImpl(null, request).getHostAssignmentsForComponent(\"component1\");\n+  }\n+\n   private class TestTopologyRequest implements TopologyRequest {\n     private Type type;\n ",
                "raw_url": "https://github.com/apache/ambari/raw/f8fe788a2d208acff4a88fc5a1a17549cfe16100/ambari-server/src/test/java/org/apache/ambari/server/topology/ClusterTopologyImplTest.java",
                "sha": "95e58ae7e6c104c5d667ff7c0549beef6ccd5517",
                "status": "modified"
            }
        ],
        "message": "AMBARI-14723. NPE when provisioning cluster with a Cluster template which doesn't contain all hostgroups defined in Blueprint. (Sandor Magyari via rnettleton)",
        "parent": "https://github.com/apache/ambari/commit/862336189317b6dab6e86377049532d5264f0636",
        "repo": "ambari",
        "unit_tests": [
            "ClusterTopologyImplTest.java"
        ]
    }
}