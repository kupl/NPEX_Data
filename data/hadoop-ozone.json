{
    "hadoop-ozone_6669fcd": {
        "bug_id": "hadoop-ozone_6669fcd",
        "commit": "https://github.com/apache/hadoop-ozone/commit/6669fcd56fedab2cfb89aaace38eea118c562e5b",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/6669fcd56fedab2cfb89aaace38eea118c562e5b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/BaseHttpServer.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/BaseHttpServer.java?ref=6669fcd56fedab2cfb89aaace38eea118c562e5b",
                "deletions": 7,
                "filename": "hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/BaseHttpServer.java",
                "patch": "@@ -65,14 +65,10 @@\n   public BaseHttpServer(Configuration conf, String name) throws IOException {\n     this.name = name;\n     this.conf = conf;\n+    policy = DFSUtil.getHttpPolicy(conf);\n     if (isEnabled()) {\n-      policy = DFSUtil.getHttpPolicy(conf);\n-      if (policy.isHttpEnabled()) {\n-        this.httpAddress = getHttpBindAddress();\n-      }\n-      if (policy.isHttpsEnabled()) {\n-        this.httpsAddress = getHttpsBindAddress();\n-      }\n+      this.httpAddress = getHttpBindAddress();\n+      this.httpsAddress = getHttpsBindAddress();\n       HttpServer2.Builder builder = null;\n       builder = DFSUtil.httpServerTemplateForNNAndJN(conf, this.httpAddress,\n           this.httpsAddress, name, getSpnegoPrincipal(), getKeytabFile());",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/6669fcd56fedab2cfb89aaace38eea118c562e5b/hadoop-hdds/framework/src/main/java/org/apache/hadoop/hdds/server/BaseHttpServer.java",
                "sha": "9a1d4b3c77992f481fedfa572c7a02b32860afc6",
                "status": "modified"
            }
        ],
        "message": "HDDS-1235. BaseHttpServer NPE is HTTP policy is HTTPS_ONLY. Contributed by Xiaoyu Yao. \n\nCloses #572",
        "parent": "https://github.com/apache/hadoop-ozone/commit/0492e240ae5335ccf621c09475c50d2f1ea84df8",
        "repo": "hadoop-ozone",
        "unit_tests": [
            "TestBaseHttpServer.java"
        ]
    },
    "hadoop-ozone_91e42a3": {
        "bug_id": "hadoop-ozone_91e42a3",
        "commit": "https://github.com/apache/hadoop-ozone/commit/91e42a3c1e722ce338fd7c074acd1aa65d8e4d74",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/91e42a3c1e722ce338fd7c074acd1aa65d8e4d74/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmBucketInfo.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmBucketInfo.java?ref=91e42a3c1e722ce338fd7c074acd1aa65d8e4d74",
                "deletions": 1,
                "filename": "hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmBucketInfo.java",
                "patch": "@@ -219,7 +219,9 @@ public Builder addMetadata(String key, String value) {\n     }\n \n     public Builder addAllMetadata(Map<String, String> additionalMetadata) {\n-      metadata.putAll(additionalMetadata);\n+      if (additionalMetadata != null) {\n+        metadata.putAll(additionalMetadata);\n+      }\n       return this;\n     }\n ",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/91e42a3c1e722ce338fd7c074acd1aa65d8e4d74/hadoop-ozone/common/src/main/java/org/apache/hadoop/ozone/om/helpers/OmBucketInfo.java",
                "sha": "14ed5f665b267ae704bd0826effdb3f28fe6c38d",
                "status": "modified"
            }
        ],
        "message": "HDDS-1011. Fix NPE BucketManagerImpl.setBucketProperty. Contributed by Xiaoyu Yao.",
        "parent": "https://github.com/apache/hadoop-ozone/commit/5f6d87157896a2bd228509b7091ebbe6b3dcb131",
        "repo": "hadoop-ozone",
        "unit_tests": [
            "TestOmBucketInfo.java"
        ]
    },
    "hadoop-ozone_a30e311": {
        "bug_id": "hadoop-ozone_a30e311",
        "commit": "https://github.com/apache/hadoop-ozone/commit/a30e311bf60dcae4fb0b64e19d21dab2ecfe81c5",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/a30e311bf60dcae4fb0b64e19d21dab2ecfe81c5/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java?ref=a30e311bf60dcae4fb0b64e19d21dab2ecfe81c5",
                "deletions": 2,
                "filename": "hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java",
                "patch": "@@ -486,8 +486,11 @@ private void handleUnderReplicatedContainer(final ContainerInfo container,\n         final List<DatanodeDetails> excludeList = replicas.stream()\n             .map(ContainerReplica::getDatanodeDetails)\n             .collect(Collectors.toList());\n-        inflightReplication.get(id).stream().map(r -> r.datanode)\n-            .forEach(excludeList::add);\n+        List<InflightAction> actionList = inflightReplication.get(id);\n+        if (actionList != null) {\n+          actionList.stream().map(r -> r.datanode)\n+              .forEach(excludeList::add);\n+        }\n         final List<DatanodeDetails> selectedDatanodes = containerPlacement\n             .chooseDatanodes(excludeList, null, delta,\n                 container.getUsedBytes());",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/a30e311bf60dcae4fb0b64e19d21dab2ecfe81c5/hadoop-hdds/server-scm/src/main/java/org/apache/hadoop/hdds/scm/container/ReplicationManager.java",
                "sha": "a8dff405e4386a30cbcaf1d3b94dd1b3977b8fb2",
                "status": "modified"
            }
        ],
        "message": "HDDS-1882. TestReplicationManager failed with NPE. (#1197)",
        "parent": "https://github.com/apache/hadoop-ozone/commit/1a71d601d7c2ca4da6ff8e5aa68aa97dedcf42a1",
        "repo": "hadoop-ozone",
        "unit_tests": [
            "TestReplicationManager.java"
        ]
    },
    "hadoop-ozone_b353ee9": {
        "bug_id": "hadoop-ozone_b353ee9",
        "commit": "https://github.com/apache/hadoop-ozone/commit/b353ee9fdca93792415cd5d928d0f3677a25294f",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/b353ee9fdca93792415cd5d928d0f3677a25294f/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/rpc/OzoneKMSUtil.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/rpc/OzoneKMSUtil.java?ref=b353ee9fdca93792415cd5d928d0f3677a25294f",
                "deletions": 0,
                "filename": "hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/rpc/OzoneKMSUtil.java",
                "patch": "@@ -128,6 +128,9 @@ public static URI getKeyProviderUri(UserGroupInformation ugi,\n \n   public static KeyProvider getKeyProvider(final Configuration conf,\n       final URI serverProviderUri) throws IOException{\n+    if (serverProviderUri == null) {\n+      throw new IOException(\"KMS serverProviderUri is not configured.\");\n+    }\n     return KMSUtil.createKeyProviderFromUri(conf, serverProviderUri);\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/b353ee9fdca93792415cd5d928d0f3677a25294f/hadoop-ozone/client/src/main/java/org/apache/hadoop/ozone/client/rpc/OzoneKMSUtil.java",
                "sha": "6be77709d4bc882163f3dbb53c2c46bc97fa3e4f",
                "status": "modified"
            },
            {
                "additions": 51,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/b353ee9fdca93792415cd5d928d0f3677a25294f/hadoop-ozone/client/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneKMSUtil.java",
                "changes": 51,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/client/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneKMSUtil.java?ref=b353ee9fdca93792415cd5d928d0f3677a25294f",
                "deletions": 0,
                "filename": "hadoop-ozone/client/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneKMSUtil.java",
                "patch": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.\u2002\u2002See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.\u2002\u2002The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ *  with the License.\u2002\u2002You may obtain a copy of the License at\n+ *\n+ * \u2002\u2002\u2002\u2002 http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.hadoop.ozone.client.rpc;\n+\n+import org.apache.hadoop.hdds.conf.OzoneConfiguration;\n+import org.apache.hadoop.ozone.OzoneConfigKeys;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+import java.io.IOException;\n+\n+import static org.junit.Assert.*;\n+\n+/**\n+ * Test class for {@link OzoneKMSUtil}.\n+ * */\n+public class TestOzoneKMSUtil {\n+  private OzoneConfiguration config;\n+\n+  @Before\n+  public void setUp() {\n+    config = new OzoneConfiguration();\n+    config.setBoolean(OzoneConfigKeys.OZONE_SECURITY_ENABLED_KEY, true);\n+  }\n+\n+  @Test\n+  public void getKeyProvider() {\n+    try {\n+      OzoneKMSUtil.getKeyProvider(config, null);\n+      fail(\"Expected IOException.\");\n+    } catch (IOException ioe) {\n+      assertEquals(ioe.getMessage(), \"KMS serverProviderUri is \" +\n+          \"not configured.\");\n+    }\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/b353ee9fdca93792415cd5d928d0f3677a25294f/hadoop-ozone/client/src/test/java/org/apache/hadoop/ozone/client/rpc/TestOzoneKMSUtil.java",
                "sha": "49fb5e335110f74f7204b33121110bb3b4fa88f7",
                "status": "added"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/b353ee9fdca93792415cd5d928d0f3677a25294f/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java?ref=b353ee9fdca93792415cd5d928d0f3677a25294f",
                "deletions": 1,
                "filename": "hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java",
                "patch": "@@ -59,7 +59,13 @@ public URI getKeyProviderUri() throws IOException {\n   @Override\n   public DelegationTokenIssuer[] getAdditionalTokenIssuers()\n       throws IOException {\n-    KeyProvider keyProvider = getKeyProvider();\n+    KeyProvider keyProvider;\n+    try {\n+      keyProvider = getKeyProvider();\n+    } catch (IOException ioe) {\n+      LOG.error(\"Error retrieving KeyProvider.\", ioe);\n+      return null;\n+    }\n     if (keyProvider instanceof DelegationTokenIssuer) {\n       return new DelegationTokenIssuer[]{(DelegationTokenIssuer)keyProvider};\n     }",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/b353ee9fdca93792415cd5d928d0f3677a25294f/hadoop-ozone/ozonefs/src/main/java/org/apache/hadoop/fs/ozone/OzoneFileSystem.java",
                "sha": "983c5a9d46568c4a2fdd43741224339c019af16f",
                "status": "modified"
            }
        ],
        "message": "HDDS-1430. NPE if secure ozone if KMS uri is not defined. Contributed by Ajay Kumar. (#752)",
        "parent": "https://github.com/apache/hadoop-ozone/commit/2be22f4d8bc890c3afb7cf13cd1c46f78ef91ff7",
        "repo": "hadoop-ozone",
        "unit_tests": [
            "TestOzoneFileSystem.java"
        ]
    },
    "hadoop-ozone_d3021fb": {
        "bug_id": "hadoop-ozone_d3021fb",
        "commit": "https://github.com/apache/hadoop-ozone/commit/d3021fb2bc895b0dfa25b0d562e4a2664164532a",
        "file": [
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/d3021fb2bc895b0dfa25b0d562e4a2664164532a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java?ref=d3021fb2bc895b0dfa25b0d562e4a2664164532a",
                "deletions": 15,
                "filename": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java",
                "patch": "@@ -152,8 +152,11 @@ public void connect(String encodedToken) throws Exception {\n     connectToDatanode(dn, encodedToken);\n   }\n \n-  private void connectToDatanode(DatanodeDetails dn, String encodedToken)\n-      throws IOException {\n+  private synchronized void connectToDatanode(DatanodeDetails dn,\n+      String encodedToken) throws IOException {\n+    if (isConnected(dn)){\n+      return;\n+    }\n     // read port from the data node, on failure use default configured\n     // port.\n     int port = dn.getPort(DatanodeDetails.Port.Name.STANDALONE).getValue();\n@@ -208,7 +211,7 @@ private boolean isConnected(ManagedChannel channel) {\n   }\n \n   @Override\n-  public void close() {\n+  public synchronized void close() {\n     closed = true;\n     for (ManagedChannel channel : channels.values()) {\n       channel.shutdownNow();\n@@ -397,19 +400,9 @@ public XceiverClientReply sendCommandAsync(\n \n   private XceiverClientReply sendCommandAsync(\n       ContainerCommandRequestProto request, DatanodeDetails dn)\n-      throws IOException, ExecutionException, InterruptedException {\n-    if (closed) {\n-      throw new IOException(\"This channel is not connected.\");\n-    }\n-\n+      throws IOException, InterruptedException {\n+    checkOpen(dn, request.getEncodedToken());\n     UUID dnId = dn.getUuid();\n-    ManagedChannel channel = channels.get(dnId);\n-    // If the channel doesn't exist for this specific datanode or the channel\n-    // is closed, just reconnect\n-    String token = request.getEncodedToken();\n-    if (!isConnected(channel)) {\n-      reconnect(dn, token);\n-    }\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Send command {} to datanode {}\",\n           request.getCmdType().toString(), dn.getNetworkFullPath());\n@@ -456,6 +449,21 @@ public void onCompleted() {\n     return new XceiverClientReply(replyFuture);\n   }\n \n+  private synchronized void checkOpen(DatanodeDetails dn, String encodedToken)\n+      throws IOException{\n+    if (closed) {\n+      throw new IOException(\"This channel is not connected.\");\n+    }\n+\n+    ManagedChannel channel = channels.get(dn.getUuid());\n+    // If the channel doesn't exist for this specific datanode or the channel\n+    // is closed, just reconnect\n+    if (!isConnected(channel)) {\n+      reconnect(dn, encodedToken);\n+    }\n+\n+  }\n+\n   private void reconnect(DatanodeDetails dn, String encodedToken)\n       throws IOException {\n     ManagedChannel channel;",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/d3021fb2bc895b0dfa25b0d562e4a2664164532a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientGrpc.java",
                "sha": "236370b2ab9eb24e5001529c61a70ceb105e0188",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/d3021fb2bc895b0dfa25b0d562e4a2664164532a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java?ref=d3021fb2bc895b0dfa25b0d562e4a2664164532a",
                "deletions": 1,
                "filename": "hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java",
                "patch": "@@ -231,7 +231,6 @@ public XceiverClientSpi call() throws Exception {\n             case RATIS:\n               client = XceiverClientRatis.newXceiverClientRatis(pipeline, conf,\n                   caCert);\n-              client.connect();\n               break;\n             case STAND_ALONE:\n               client = new XceiverClientGrpc(pipeline, conf, caCert);\n@@ -240,6 +239,7 @@ public XceiverClientSpi call() throws Exception {\n             default:\n               throw new IOException(\"not implemented\" + pipeline.getType());\n             }\n+            client.connect();\n             return client;\n           }\n         });",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/d3021fb2bc895b0dfa25b0d562e4a2664164532a/hadoop-hdds/client/src/main/java/org/apache/hadoop/hdds/scm/XceiverClientManager.java",
                "sha": "648d22dbb064107f82662d4dead7e6f1cb10e1c8",
                "status": "modified"
            }
        ],
        "message": "HDDS-2347 XCeiverClientGrpc's parallel use leads to NPE (#81)",
        "parent": "https://github.com/apache/hadoop-ozone/commit/27b6042f0e99ebcfdbbe21ee905084be3aadd6b1",
        "repo": "hadoop-ozone",
        "unit_tests": [
            "TestXceiverClientManager.java"
        ]
    },
    "hadoop-ozone_d8efe0a": {
        "bug_id": "hadoop-ozone_d8efe0a",
        "commit": "https://github.com/apache/hadoop-ozone/commit/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConfigKeys.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConfigKeys.java?ref=d8efe0a448e8da6d060623ac0b0eb7c006f94dbd",
                "deletions": 11,
                "filename": "hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConfigKeys.java",
                "patch": "@@ -286,17 +286,6 @@\n   public static final double\n       HDDS_DATANODE_STORAGE_UTILIZATION_CRITICAL_THRESHOLD_DEFAULT = 0.75;\n \n-  public static final String\n-      HDDS_WRITE_LOCK_REPORTING_THRESHOLD_MS_KEY =\n-      \"hdds.write.lock.reporting.threshold.ms\";\n-  public static final long\n-      HDDS_WRITE_LOCK_REPORTING_THRESHOLD_MS_DEFAULT = 5000L;\n-  public static final String\n-      HDDS_LOCK_SUPPRESS_WARNING_INTERVAL_MS_KEY =\n-      \"hdds.lock.suppress.warning.interval.ms\";\n-  public static final long\n-      HDDS_LOCK_SUPPRESS_WARNING_INTERVAL_MS_DEAFULT = 10000L;\n-\n   public static final String OZONE_CONTAINER_COPY_WORKDIR =\n       \"hdds.datanode.replication.work.dir\";\n ",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/common/src/main/java/org/apache/hadoop/ozone/OzoneConfigKeys.java",
                "sha": "e8aa22c2ac03de14cd404926f268efafe70b49c1",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/VersionEndpointTask.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/VersionEndpointTask.java?ref=d8efe0a448e8da6d060623ac0b0eb7c006f94dbd",
                "deletions": 18,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/VersionEndpointTask.java",
                "patch": "@@ -69,31 +69,39 @@ public VersionEndpointTask(EndpointStateMachine rpcEndPoint,\n       VersionResponse response = VersionResponse.getFromProtobuf(\n           versionResponse);\n       rpcEndPoint.setVersion(response);\n-      VolumeSet volumeSet = ozoneContainer.getVolumeSet();\n-      Map<String, HddsVolume> volumeMap = volumeSet.getVolumeMap();\n \n       String scmId = response.getValue(OzoneConsts.SCM_ID);\n       String clusterId = response.getValue(OzoneConsts.CLUSTER_ID);\n \n-      Preconditions.checkNotNull(scmId, \"Reply from SCM: scmId cannot be \" +\n-          \"null\");\n-      Preconditions.checkNotNull(clusterId, \"Reply from SCM: clusterId \" +\n-          \"cannot be null\");\n+      // Check volumes\n+      VolumeSet volumeSet = ozoneContainer.getVolumeSet();\n+      volumeSet.readLock();\n+      try {\n+        Map<String, HddsVolume> volumeMap = volumeSet.getVolumeMap();\n+\n+        Preconditions.checkNotNull(scmId, \"Reply from SCM: scmId cannot be \" +\n+            \"null\");\n+        Preconditions.checkNotNull(clusterId, \"Reply from SCM: clusterId \" +\n+            \"cannot be null\");\n \n-      // If version file does not exist create version file and also set scmId\n-      for (Map.Entry<String, HddsVolume> entry : volumeMap.entrySet()) {\n-        HddsVolume hddsVolume = entry.getValue();\n-        boolean result = HddsVolumeUtil.checkVolume(hddsVolume, scmId,\n-            clusterId, LOG);\n-        if (!result) {\n-          volumeSet.failVolume(hddsVolume.getHddsRootDir().getPath());\n+        // If version file does not exist create version file and also set scmId\n+        for (Map.Entry<String, HddsVolume> entry : volumeMap.entrySet()) {\n+          HddsVolume hddsVolume = entry.getValue();\n+          boolean result = HddsVolumeUtil.checkVolume(hddsVolume, scmId,\n+              clusterId, LOG);\n+          if (!result) {\n+            volumeSet.failVolume(hddsVolume.getHddsRootDir().getPath());\n+          }\n         }\n+        if (volumeSet.getVolumesList().size() == 0) {\n+          // All volumes are inconsistent state\n+          throw new DiskOutOfSpaceException(\"All configured Volumes are in \" +\n+              \"Inconsistent State\");\n+        }\n+      } finally {\n+        volumeSet.readUnlock();\n       }\n-      if (volumeSet.getVolumesList().size() == 0) {\n-        // All volumes are inconsistent state\n-        throw new DiskOutOfSpaceException(\"All configured Volumes are in \" +\n-            \"Inconsistent State\");\n-      }\n+\n       ozoneContainer.getDispatcher().setScmId(scmId);\n \n       EndpointStateMachine.EndPointStates nextState =",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/states/endpoint/VersionEndpointTask.java",
                "sha": "2d0467706ec921ee55cfea0711332ff0298e4b42",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/HddsVolumeUtil.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/HddsVolumeUtil.java?ref=d8efe0a448e8da6d060623ac0b0eb7c006f94dbd",
                "deletions": 1,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/HddsVolumeUtil.java",
                "patch": "@@ -164,7 +164,7 @@ private static String getProperty(Properties props, String propName, File\n   }\n \n   /**\n-   * Check Volume is consistent state or not.\n+   * Check Volume is in consistent state or not.\n    * @param hddsVolume\n    * @param scmId\n    * @param clusterId",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/utils/HddsVolumeUtil.java",
                "sha": "cb356dadeb236360a214d729cd511e29a3047c6c",
                "status": "modified"
            },
            {
                "additions": 97,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java",
                "changes": 178,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java?ref=d8efe0a448e8da6d060623ac0b0eb7c006f94dbd",
                "deletions": 81,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java",
                "patch": "@@ -33,15 +33,11 @@\n     .StorageContainerDatanodeProtocolProtos;\n import org.apache.hadoop.hdds.protocol.proto\n     .StorageContainerDatanodeProtocolProtos.NodeReportProto;\n-import org.apache.hadoop.ozone.OzoneConfigKeys;\n import org.apache.hadoop.ozone.common.InconsistentStorageStateException;\n import org.apache.hadoop.ozone.container.common.impl.StorageLocationReport;\n import org.apache.hadoop.ozone.container.common.utils.HddsVolumeUtil;\n import org.apache.hadoop.ozone.container.common.volume.HddsVolume.VolumeState;\n-import org.apache.hadoop.ozone.container.common.interfaces.VolumeChoosingPolicy;\n-import org.apache.hadoop.util.AutoCloseableLock;\n import org.apache.hadoop.util.DiskChecker.DiskOutOfSpaceException;\n-import org.apache.hadoop.util.InstrumentedLock;\n import org.apache.hadoop.util.ShutdownHookManager;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -53,8 +49,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.concurrent.ConcurrentHashMap;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.locks.ReentrantLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n \n /**\n  * VolumeSet to manage volumes in a DataNode.\n@@ -84,11 +79,12 @@\n   private EnumMap<StorageType, List<HddsVolume>> volumeStateMap;\n \n   /**\n-   * Lock to synchronize changes to the VolumeSet. Any update to\n-   * {@link VolumeSet#volumeMap}, {@link VolumeSet#failedVolumeMap}, or\n-   * {@link VolumeSet#volumeStateMap} should be done after acquiring this lock.\n+   * A Reentrant Read Write Lock to synchronize volume operations in VolumeSet.\n+   * Any update to {@link VolumeSet#volumeMap},\n+   * {@link VolumeSet#failedVolumeMap}, or {@link VolumeSet#volumeStateMap}\n+   * should be done after acquiring the write lock.\n    */\n-  private final AutoCloseableLock volumeSetLock;\n+  private final ReentrantReadWriteLock volumeSetRWLock;\n \n   private final String datanodeUuid;\n   private String clusterID;\n@@ -105,17 +101,7 @@ public VolumeSet(String dnUuid, String clusterID, Configuration conf)\n     this.datanodeUuid = dnUuid;\n     this.clusterID = clusterID;\n     this.conf = conf;\n-    this.volumeSetLock = new AutoCloseableLock(\n-        new InstrumentedLock(getClass().getName(), LOG,\n-            new ReentrantLock(true),\n-            conf.getTimeDuration(\n-                OzoneConfigKeys.HDDS_WRITE_LOCK_REPORTING_THRESHOLD_MS_KEY,\n-                OzoneConfigKeys.HDDS_WRITE_LOCK_REPORTING_THRESHOLD_MS_DEFAULT,\n-                TimeUnit.MILLISECONDS),\n-            conf.getTimeDuration(\n-                OzoneConfigKeys.HDDS_LOCK_SUPPRESS_WARNING_INTERVAL_MS_KEY,\n-                OzoneConfigKeys.HDDS_LOCK_SUPPRESS_WARNING_INTERVAL_MS_DEAFULT,\n-                TimeUnit.MILLISECONDS)));\n+    this.volumeSetRWLock = new ReentrantReadWriteLock();\n \n     initializeVolumeSet();\n   }\n@@ -198,14 +184,35 @@ private void checkAndSetClusterID(String idFromVersionFile)\n     }\n   }\n \n-  public void acquireLock() {\n-    volumeSetLock.acquire();\n+  /**\n+   * Acquire Volume Set Read lock.\n+   */\n+  public void readLock() {\n+    volumeSetRWLock.readLock().lock();\n+  }\n+\n+  /**\n+   * Release Volume Set Read lock.\n+   */\n+  public void readUnlock() {\n+    volumeSetRWLock.readLock().unlock();\n   }\n \n-  public void releaseLock() {\n-    volumeSetLock.release();\n+  /**\n+   * Acquire Volume Set Write lock.\n+   */\n+  public void writeLock() {\n+    volumeSetRWLock.writeLock().lock();\n+  }\n+\n+  /**\n+   * Release Volume Set Write lock.\n+   */\n+  public void writeUnlock() {\n+    volumeSetRWLock.writeLock().unlock();\n   }\n \n+\n   private HddsVolume createVolume(String locationString,\n       StorageType storageType) throws IOException {\n     HddsVolume.Builder volumeBuilder = new HddsVolume.Builder(locationString)\n@@ -227,7 +234,8 @@ public boolean addVolume(String volumeRoot, StorageType storageType) {\n     String hddsRoot = HddsVolumeUtil.getHddsRoot(volumeRoot);\n     boolean success;\n \n-    try (AutoCloseableLock lock = volumeSetLock.acquire()) {\n+    this.writeLock();\n+    try {\n       if (volumeMap.containsKey(hddsRoot)) {\n         LOG.warn(\"Volume : {} already exists in VolumeMap\", hddsRoot);\n         success = false;\n@@ -247,6 +255,8 @@ public boolean addVolume(String volumeRoot, StorageType storageType) {\n     } catch (IOException ex) {\n       LOG.error(\"Failed to add volume \" + volumeRoot + \" to VolumeSet\", ex);\n       success = false;\n+    } finally {\n+      this.writeUnlock();\n     }\n     return success;\n   }\n@@ -255,7 +265,8 @@ public boolean addVolume(String volumeRoot, StorageType storageType) {\n   public void failVolume(String dataDir) {\n     String hddsRoot = HddsVolumeUtil.getHddsRoot(dataDir);\n \n-    try (AutoCloseableLock lock = volumeSetLock.acquire()) {\n+    this.writeLock();\n+    try {\n       if (volumeMap.containsKey(hddsRoot)) {\n         HddsVolume hddsVolume = volumeMap.get(hddsRoot);\n         hddsVolume.failVolume();\n@@ -270,14 +281,17 @@ public void failVolume(String dataDir) {\n       } else {\n         LOG.warn(\"Volume : {} does not exist in VolumeSet\", hddsRoot);\n       }\n+    } finally {\n+      this.writeUnlock();\n     }\n   }\n \n   // Remove a volume from the VolumeSet completely.\n   public void removeVolume(String dataDir) throws IOException {\n     String hddsRoot = HddsVolumeUtil.getHddsRoot(dataDir);\n \n-    try (AutoCloseableLock lock = volumeSetLock.acquire()) {\n+    this.writeLock();\n+    try {\n       if (volumeMap.containsKey(hddsRoot)) {\n         HddsVolume hddsVolume = volumeMap.get(hddsRoot);\n         hddsVolume.shutdown();\n@@ -295,14 +309,11 @@ public void removeVolume(String dataDir) throws IOException {\n       } else {\n         LOG.warn(\"Volume : {} does not exist in VolumeSet\", hddsRoot);\n       }\n+    } finally {\n+      this.writeUnlock();\n     }\n   }\n \n-  public HddsVolume chooseVolume(long containerSize,\n-      VolumeChoosingPolicy choosingPolicy) throws IOException {\n-    return choosingPolicy.chooseVolume(getVolumesList(), containerSize);\n-  }\n-\n   /**\n    * This method, call shutdown on each volume to shutdown volume usage\n    * thread and write scmUsed on each volume.\n@@ -352,55 +363,60 @@ public void shutdown() {\n   public StorageContainerDatanodeProtocolProtos.NodeReportProto getNodeReport()\n       throws IOException {\n     boolean failed;\n-    StorageLocationReport[] reports = new StorageLocationReport[volumeMap\n-        .size() + failedVolumeMap.size()];\n-    int counter = 0;\n-    HddsVolume hddsVolume;\n-    for (Map.Entry<String, HddsVolume> entry : volumeMap.entrySet()) {\n-      hddsVolume = entry.getValue();\n-      VolumeInfo volumeInfo = hddsVolume.getVolumeInfo();\n-      long scmUsed = 0;\n-      long remaining = 0;\n-      failed = false;\n-      try {\n-        scmUsed = volumeInfo.getScmUsed();\n-        remaining = volumeInfo.getAvailable();\n-      } catch (IOException ex) {\n-        LOG.warn(\"Failed to get scmUsed and remaining for container \" +\n-            \"storage location {}\", volumeInfo.getRootDir());\n-        // reset scmUsed and remaining if df/du failed.\n-        scmUsed = 0;\n-        remaining = 0;\n-        failed = true;\n-      }\n+    this.readLock();\n+    try {\n+      StorageLocationReport[] reports = new StorageLocationReport[volumeMap\n+          .size() + failedVolumeMap.size()];\n+      int counter = 0;\n+      HddsVolume hddsVolume;\n+      for (Map.Entry<String, HddsVolume> entry : volumeMap.entrySet()) {\n+        hddsVolume = entry.getValue();\n+        VolumeInfo volumeInfo = hddsVolume.getVolumeInfo();\n+        long scmUsed = 0;\n+        long remaining = 0;\n+        failed = false;\n+        try {\n+          scmUsed = volumeInfo.getScmUsed();\n+          remaining = volumeInfo.getAvailable();\n+        } catch (IOException ex) {\n+          LOG.warn(\"Failed to get scmUsed and remaining for container \" +\n+              \"storage location {}\", volumeInfo.getRootDir());\n+          // reset scmUsed and remaining if df/du failed.\n+          scmUsed = 0;\n+          remaining = 0;\n+          failed = true;\n+        }\n \n-      StorageLocationReport.Builder builder =\n-          StorageLocationReport.newBuilder();\n-      builder.setStorageLocation(volumeInfo.getRootDir())\n-          .setId(hddsVolume.getStorageID())\n-          .setFailed(failed)\n-          .setCapacity(hddsVolume.getCapacity())\n-          .setRemaining(remaining)\n-          .setScmUsed(scmUsed)\n-          .setStorageType(hddsVolume.getStorageType());\n-      StorageLocationReport r = builder.build();\n-      reports[counter++] = r;\n-    }\n-    for (Map.Entry<String, HddsVolume> entry : failedVolumeMap.entrySet()) {\n-      hddsVolume = entry.getValue();\n-      StorageLocationReport.Builder builder = StorageLocationReport\n-          .newBuilder();\n-      builder.setStorageLocation(hddsVolume.getHddsRootDir()\n-          .getAbsolutePath()).setId(hddsVolume.getStorageID()).setFailed(true)\n-          .setCapacity(0).setRemaining(0).setScmUsed(0).setStorageType(\n-              hddsVolume.getStorageType());\n-      StorageLocationReport r = builder.build();\n-      reports[counter++] = r;\n-    }\n-    NodeReportProto.Builder nrb = NodeReportProto.newBuilder();\n-    for (int i = 0; i < reports.length; i++) {\n-      nrb.addStorageReport(reports[i].getProtoBufMessage());\n+        StorageLocationReport.Builder builder =\n+            StorageLocationReport.newBuilder();\n+        builder.setStorageLocation(volumeInfo.getRootDir())\n+            .setId(hddsVolume.getStorageID())\n+            .setFailed(failed)\n+            .setCapacity(hddsVolume.getCapacity())\n+            .setRemaining(remaining)\n+            .setScmUsed(scmUsed)\n+            .setStorageType(hddsVolume.getStorageType());\n+        StorageLocationReport r = builder.build();\n+        reports[counter++] = r;\n+      }\n+      for (Map.Entry<String, HddsVolume> entry : failedVolumeMap.entrySet()) {\n+        hddsVolume = entry.getValue();\n+        StorageLocationReport.Builder builder = StorageLocationReport\n+            .newBuilder();\n+        builder.setStorageLocation(hddsVolume.getHddsRootDir()\n+            .getAbsolutePath()).setId(hddsVolume.getStorageID()).setFailed(true)\n+            .setCapacity(0).setRemaining(0).setScmUsed(0).setStorageType(\n+            hddsVolume.getStorageType());\n+        StorageLocationReport r = builder.build();\n+        reports[counter++] = r;\n+      }\n+      NodeReportProto.Builder nrb = NodeReportProto.newBuilder();\n+      for (int i = 0; i < reports.length; i++) {\n+        nrb.addStorageReport(reports[i].getProtoBufMessage());\n+      }\n+      return nrb.build();\n+    } finally {\n+      this.readUnlock();\n     }\n-    return nrb.build();\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/common/volume/VolumeSet.java",
                "sha": "5b6b823c9c69ef69fb14d0735bd42c0a84a534b2",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java?ref=d8efe0a448e8da6d060623ac0b0eb7c006f94dbd",
                "deletions": 3,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java",
                "patch": "@@ -108,8 +108,8 @@ public void create(VolumeSet volumeSet, VolumeChoosingPolicy\n     Preconditions.checkNotNull(scmId, \"scmId cannot be null\");\n \n     File containerMetaDataPath = null;\n-    //acquiring volumeset lock and container lock\n-    volumeSet.acquireLock();\n+    //acquiring volumeset read lock\n+    volumeSet.readLock();\n     long maxSize = containerData.getMaxSize();\n     try {\n       HddsVolume containerVolume = volumeChoosingPolicy.chooseVolume(volumeSet\n@@ -166,7 +166,7 @@ public void create(VolumeSet volumeSet, VolumeChoosingPolicy\n       throw new StorageContainerException(\"Container creation failed.\", ex,\n           CONTAINER_INTERNAL_ERROR);\n     } finally {\n-      volumeSet.releaseLock();\n+      volumeSet.readUnlock();\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueContainer.java",
                "sha": "e5b344de483770fc5e7b201fe12220438ebe7cbe",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-ozone/blob/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-ozone/contents/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java?ref=d8efe0a448e8da6d060623ac0b0eb7c006f94dbd",
                "deletions": 2,
                "filename": "hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java",
                "patch": "@@ -271,14 +271,14 @@ ContainerCommandResponseProto handleCreateContainer(\n \n   public void populateContainerPathFields(KeyValueContainer container,\n       long maxSize) throws IOException {\n-    volumeSet.acquireLock();\n+    volumeSet.readLock();\n     try {\n       HddsVolume containerVolume = volumeChoosingPolicy.chooseVolume(volumeSet\n           .getVolumesList(), maxSize);\n       String hddsVolumeDir = containerVolume.getHddsRootDir().toString();\n       container.populatePathFields(scmID, containerVolume, hddsVolumeDir);\n     } finally {\n-      volumeSet.releaseLock();\n+      volumeSet.readUnlock();\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-ozone/raw/d8efe0a448e8da6d060623ac0b0eb7c006f94dbd/hadoop-hdds/container-service/src/main/java/org/apache/hadoop/ozone/container/keyvalue/KeyValueHandler.java",
                "sha": "922db2ad888581d88778787c2ecd7576e9f7073a",
                "status": "modified"
            }
        ],
        "message": "HDDS-354. VolumeInfo.getScmUsed throws NPE. Contributed by Hanisha Koneru.",
        "parent": "https://github.com/apache/hadoop-ozone/commit/bda2e78da6eebf1d4f2d773a9a1289709d00a5c6",
        "repo": "hadoop-ozone",
        "unit_tests": [
            "TestKeyValueHandler.java"
        ]
    }
}