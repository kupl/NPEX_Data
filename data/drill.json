{
    "drill_0abcbe3": {
        "bug_id": "drill_0abcbe3",
        "commit": "https://github.com/apache/drill/commit/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "patch": "@@ -807,6 +807,8 @@ private ExecConstants() {\n    */\n   public static final String ENABLE_ITERATOR_VALIDATION = \"drill.exec.debug.validate_iterators\";\n \n+  public static final String QUERY_ROWKEYJOIN_BATCHSIZE_KEY = \"exec.query.rowkeyjoin_batchsize\";\n+  public static final PositiveLongValidator QUERY_ROWKEYJOIN_BATCHSIZE = new PositiveLongValidator(QUERY_ROWKEYJOIN_BATCHSIZE_KEY, Long.MAX_VALUE, null);\n   /**\n    * When iterator validation is enabled, additionally validates the vectors in\n    * each batch passed to each iterator.",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "sha": "cb0fc5cf2cb3e0deec6335500a917d8a53e10099",
                "status": "modified"
            },
            {
                "additions": 95,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractDbGroupScan.java",
                "changes": 95,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractDbGroupScan.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractDbGroupScan.java",
                "patch": "@@ -0,0 +1,95 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.physical.base;\n+\n+import java.util.List;\n+\n+import org.apache.drill.common.expression.FieldReference;\n+import org.apache.drill.common.expression.SchemaPath;\n+import org.apache.drill.common.logical.StoragePluginConfig;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.drill.exec.planner.index.IndexCollection;\n+import org.apache.drill.exec.planner.cost.PluginCost;\n+import org.apache.drill.exec.planner.physical.PartitionFunction;\n+import org.apache.drill.exec.store.AbstractStoragePlugin;\n+\n+public abstract class AbstractDbGroupScan extends AbstractGroupScan implements DbGroupScan {\n+  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(AbstractDbGroupScan.class);\n+\n+  private static final String ROW_KEY = \"_id\";\n+  private static final SchemaPath ROW_KEY_PATH = SchemaPath.getSimplePath(ROW_KEY);\n+\n+  public AbstractDbGroupScan(String userName) {\n+    super(userName);\n+  }\n+\n+  public AbstractDbGroupScan(AbstractDbGroupScan that) {\n+    super(that);\n+  }\n+\n+  public abstract AbstractStoragePlugin getStoragePlugin();\n+\n+  public abstract StoragePluginConfig getStorageConfig();\n+\n+  public abstract List<SchemaPath> getColumns();\n+\n+  @Override\n+  public boolean supportsSecondaryIndex() {\n+    return false;\n+  }\n+\n+  @Override\n+  public IndexCollection getSecondaryIndexCollection(RelNode scanrel) {\n+    return null;\n+  }\n+\n+  @Override\n+  public boolean supportsRestrictedScan() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isRestrictedScan() {\n+    return false;\n+  }\n+\n+  @Override\n+  public DbGroupScan getRestrictedScan(List<SchemaPath> columns) {\n+    return null;\n+  }\n+\n+  @Override\n+  public String getRowKeyName() {\n+    return ROW_KEY;\n+  }\n+\n+  @Override\n+  public SchemaPath getRowKeyPath() {\n+    return ROW_KEY_PATH;\n+  }\n+\n+  @Override\n+  public PartitionFunction getRangePartitionFunction(List<FieldReference> refList) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+  @Override\n+  public PluginCost getPluginCostModel() {\n+    return null;\n+  }\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractDbGroupScan.java",
                "sha": "42e4bb9ff052e103f40f9bd0e90a9ebcd235cf8f",
                "status": "added"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractDbSubScan.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractDbSubScan.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractDbSubScan.java",
                "patch": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.physical.base;\n+\n+import org.apache.drill.exec.physical.impl.join.RowKeyJoin;\n+\n+public abstract class AbstractDbSubScan extends AbstractSubScan implements DbSubScan {\n+\n+  public AbstractDbSubScan(String userName) {\n+    super(userName);\n+  }\n+\n+  public boolean isRestrictedSubScan() {\n+    return false;\n+  }\n+\n+  @Override\n+  public void addJoinForRestrictedSubScan(RowKeyJoin batch) {\n+    throw new UnsupportedOperationException();\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractDbSubScan.java",
                "sha": "caa583162eaf5db0e516c850a5e7f85584fef1e3",
                "status": "added"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractPhysicalVisitor.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractPhysicalVisitor.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractPhysicalVisitor.java",
                "patch": "@@ -22,13 +22,15 @@\n import org.apache.drill.exec.physical.config.FlattenPOP;\n import org.apache.drill.exec.physical.config.HashAggregate;\n import org.apache.drill.exec.physical.config.HashPartitionSender;\n+import org.apache.drill.exec.physical.config.HashToRandomExchange;\n import org.apache.drill.exec.physical.config.IteratorValidator;\n import org.apache.drill.exec.physical.config.LateralJoinPOP;\n import org.apache.drill.exec.physical.config.Limit;\n import org.apache.drill.exec.physical.config.MergingReceiverPOP;\n import org.apache.drill.exec.physical.config.OrderedPartitionSender;\n import org.apache.drill.exec.physical.config.ProducerConsumer;\n import org.apache.drill.exec.physical.config.Project;\n+import org.apache.drill.exec.physical.config.RangePartitionSender;\n import org.apache.drill.exec.physical.config.Screen;\n import org.apache.drill.exec.physical.config.SingleSender;\n import org.apache.drill.exec.physical.config.Sort;\n@@ -156,6 +158,16 @@ public T visitMergingReceiver(MergingReceiverPOP op, X value) throws E {\n     return visitReceiver(op, value);\n   }\n \n+  @Override\n+  public T visitHashPartitionSender(HashToRandomExchange op, X value) throws E {\n+    return visitExchange(op, value);\n+  }\n+\n+  @Override\n+  public T visitRangePartitionSender(RangePartitionSender op, X value) throws E {\n+    return visitSender(op, value);\n+  }\n+\n   @Override\n   public T visitBroadcastSender(BroadcastSender op, X value) throws E {\n     return visitSender(op, value);",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/AbstractPhysicalVisitor.java",
                "sha": "ca82ca621a87822f6ad02becd82a1bbf47ac2cce",
                "status": "modified"
            },
            {
                "additions": 129,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/DbGroupScan.java",
                "changes": 129,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/DbGroupScan.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/DbGroupScan.java",
                "patch": "@@ -0,0 +1,129 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.physical.base;\n+\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.drill.common.expression.FieldReference;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.common.expression.SchemaPath;\n+import org.apache.drill.exec.planner.index.IndexCollection;\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import org.apache.drill.exec.planner.cost.PluginCost;\n+import org.apache.drill.exec.planner.physical.PartitionFunction;\n+import org.apache.drill.exec.planner.index.Statistics;\n+\n+import java.util.List;\n+\n+/**\n+ * A DbGroupScan operator represents the scan associated with a database. The underlying\n+ * database may support secondary indexes, so there are interface methods for indexes.\n+ */\n+public interface DbGroupScan extends GroupScan {\n+\n+\n+  @JsonIgnore\n+  public boolean supportsSecondaryIndex();\n+\n+  /**\n+   * Get the index collection associated with this table if any\n+   */\n+  @JsonIgnore\n+  public IndexCollection getSecondaryIndexCollection(RelNode scan);\n+\n+  /**\n+   * Set the artificial row count after applying the {@link RexNode} condition\n+   * @param condition\n+   * @param count\n+   * @param capRowCount\n+   */\n+  @JsonIgnore\n+  public void setRowCount(RexNode condition, double count, double capRowCount);\n+\n+  /**\n+   * Get the row count after applying the {@link RexNode} condition\n+   * @param condition, filter to apply\n+   * @param scanRel, the current scan rel\n+   * @return row count post filtering\n+   */\n+  @JsonIgnore\n+  public double getRowCount(RexNode condition, RelNode scanRel);\n+\n+  /**\n+   * Get the statistics for this {@link DbGroupScan}\n+   * @return the {@link Statistics} for this Scan\n+   */\n+  @JsonIgnore\n+  public Statistics getStatistics();\n+\n+  public List<SchemaPath> getColumns();\n+\n+  public void setCostFactor(double sel);\n+\n+  @JsonIgnore\n+  boolean isIndexScan();\n+\n+  /**\n+   * Whether this DbGroupScan supports creating a restricted (skip) scan\n+   * @return true if restricted scan is supported, false otherwise\n+   */\n+  @JsonIgnore\n+  boolean supportsRestrictedScan();\n+\n+  /**\n+   * Whether this DbGroupScan is itself a restricted scan\n+   * @return true if this DbGroupScan is itself a restricted scan, false otherwise\n+   */\n+  @JsonIgnore\n+  boolean isRestrictedScan();\n+\n+  /**\n+   * If this DbGroupScan supports restricted scan, create a restricted scan from this DbGroupScan.\n+   * @param columns\n+   * @return a non-null DbGroupScan if restricted scan is supported, null otherwise\n+   */\n+  @JsonIgnore\n+  DbGroupScan getRestrictedScan(List<SchemaPath> columns);\n+\n+  @JsonIgnore\n+  String getRowKeyName();\n+\n+  @JsonIgnore\n+  String getIndexHint();\n+\n+  @JsonIgnore\n+  SchemaPath getRowKeyPath();\n+\n+  /**\n+   * Get a partition function instance for range based partitioning\n+   * @param refList a list of FieldReference exprs that are participating in the range partitioning\n+   * @return instance of a partitioning function\n+   */\n+  @JsonIgnore\n+  PartitionFunction getRangePartitionFunction(List<FieldReference> refList);\n+\n+  /**\n+   * Get the format plugin cost model. The cost model will provide cost factors such as seq. scan cost,\n+   * random scan cost, block size.\n+   * @return a PluginCost cost model\n+   */\n+  @JsonIgnore\n+  PluginCost getPluginCostModel();\n+\n+  @JsonIgnore\n+  boolean isFilterPushedDown();\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/DbGroupScan.java",
                "sha": "e16fba1ff47f54956f033ab77feb275a22dd4a7f",
                "status": "added"
            },
            {
                "additions": 43,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/DbSubScan.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/DbSubScan.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/DbSubScan.java",
                "patch": "@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.physical.base;\n+\n+import org.apache.drill.exec.physical.impl.join.RowKeyJoin;\n+\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+\n+\n+public interface DbSubScan extends SubScan {\n+\n+  /**\n+   * Whether this subscan is a restricted (skip) subscan\n+   * @return true if this subscan is a restricted subscan, false otherwise\n+   */\n+  @JsonIgnore\n+  boolean isRestrictedSubScan();\n+\n+  /**\n+   * For a restricted sub-scan, this method allows associating a (hash)join instance.  A subscan within a minor\n+   * fragment must have a corresponding (hash)join batch instance from which it will retrieve its set of\n+   * rowkeys to perform the restricted scan.\n+   * @param batch\n+   */\n+  @JsonIgnore\n+  void addJoinForRestrictedSubScan(RowKeyJoin batch);\n+\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/DbSubScan.java",
                "sha": "874468d4e6851e1f46c87e41073dbcbafad494dc",
                "status": "added"
            },
            {
                "additions": 76,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/IndexGroupScan.java",
                "changes": 76,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/IndexGroupScan.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/IndexGroupScan.java",
                "patch": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.physical.base;\n+\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.common.expression.SchemaPath;\n+import org.apache.drill.exec.planner.index.Statistics;\n+\n+\n+import java.util.List;\n+\n+/**\n+ * An IndexGroupScan operator represents the scan associated with an Index.\n+ */\n+public interface IndexGroupScan extends GroupScan {\n+\n+  /**\n+   * Get the column ordinal of the rowkey column from the output schema of the IndexGroupScan\n+   * @return\n+   */\n+  @JsonIgnore\n+  public int getRowKeyOrdinal();\n+\n+  /**\n+   * Set the artificial row count after applying the {@link RexNode} condition\n+   * Mainly used for debugging\n+   * @param condition\n+   * @param count\n+   * @param capRowCount\n+   */\n+  @JsonIgnore\n+  public void setRowCount(RexNode condition, double count, double capRowCount);\n+\n+  /**\n+   * Get the row count after applying the {@link RexNode} condition\n+   * @param condition, filter to apply\n+   * @return row count post filtering\n+   */\n+  @JsonIgnore\n+  public double getRowCount(RexNode condition, RelNode scanRel);\n+\n+  /**\n+   * Set the statistics for {@link IndexGroupScan}\n+   * @param statistics\n+   */\n+  @JsonIgnore\n+  public void setStatistics(Statistics statistics);\n+\n+  @JsonIgnore\n+  public void setColumns(List<SchemaPath> columns);\n+\n+  @JsonIgnore\n+  public List<SchemaPath> getColumns();\n+\n+  @JsonIgnore\n+  public void setParallelizationWidth(int width);\n+\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/IndexGroupScan.java",
                "sha": "1047e829d0efa314e0ab8027c062cc201058fb7a",
                "status": "added"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/PhysicalVisitor.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/PhysicalVisitor.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/PhysicalVisitor.java",
                "patch": "@@ -22,13 +22,15 @@\n import org.apache.drill.exec.physical.config.FlattenPOP;\n import org.apache.drill.exec.physical.config.HashAggregate;\n import org.apache.drill.exec.physical.config.HashPartitionSender;\n+import org.apache.drill.exec.physical.config.HashToRandomExchange;\n import org.apache.drill.exec.physical.config.IteratorValidator;\n import org.apache.drill.exec.physical.config.LateralJoinPOP;\n import org.apache.drill.exec.physical.config.Limit;\n import org.apache.drill.exec.physical.config.MergingReceiverPOP;\n import org.apache.drill.exec.physical.config.OrderedPartitionSender;\n import org.apache.drill.exec.physical.config.ProducerConsumer;\n import org.apache.drill.exec.physical.config.Project;\n+import org.apache.drill.exec.physical.config.RangePartitionSender;\n import org.apache.drill.exec.physical.config.Screen;\n import org.apache.drill.exec.physical.config.SingleSender;\n import org.apache.drill.exec.physical.config.Sort;\n@@ -73,6 +75,8 @@\n   public RETURN visitOrderedPartitionSender(OrderedPartitionSender op, EXTRA value) throws EXCEP;\n   public RETURN visitUnorderedReceiver(UnorderedReceiver op, EXTRA value) throws EXCEP;\n   public RETURN visitMergingReceiver(MergingReceiverPOP op, EXTRA value) throws EXCEP;\n+  public RETURN visitHashPartitionSender(HashToRandomExchange op, EXTRA value) throws EXCEP;\n+  public RETURN visitRangePartitionSender(RangePartitionSender op, EXTRA value) throws EXCEP;\n   public RETURN visitBroadcastSender(BroadcastSender op, EXTRA value) throws EXCEP;\n   public RETURN visitScreen(Screen op, EXTRA value) throws EXCEP;\n   public RETURN visitSingleSender(SingleSender op, EXTRA value) throws EXCEP;",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/base/PhysicalVisitor.java",
                "sha": "1bb1545c487a113b3efd6ce90f3cbcbed4628fcc",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/config/RangePartitionSender.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/config/RangePartitionSender.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 26,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/config/RangePartitionSender.java",
                "patch": "@@ -17,57 +17,57 @@\n  */\n package org.apache.drill.exec.physical.config;\n \n-import java.util.Collections;\n import java.util.List;\n \n import org.apache.drill.exec.physical.MinorFragmentEndpoint;\n import org.apache.drill.exec.physical.base.AbstractSender;\n import org.apache.drill.exec.physical.base.PhysicalOperator;\n-import org.apache.drill.exec.proto.CoordinationProtos.DrillbitEndpoint;\n+import org.apache.drill.exec.physical.base.PhysicalVisitor;\n+import org.apache.drill.exec.planner.physical.PartitionFunction;\n import org.apache.drill.exec.proto.UserBitShared.CoreOperatorType;\n \n import com.fasterxml.jackson.annotation.JsonCreator;\n import com.fasterxml.jackson.annotation.JsonProperty;\n import com.fasterxml.jackson.annotation.JsonTypeName;\n \n-@JsonTypeName(\"range-sender\")\n-public class RangeSender extends AbstractSender{\n+@JsonTypeName(\"range-partition-sender\")\n+public class RangePartitionSender extends AbstractSender{\n \n-  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(RangeSender.class);\n+  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(RangePartitionSender.class);\n \n-  List<EndpointPartition> partitions;\n+  // The number of records in the outgoing batch. This is overriding the default value in Partitioner\n+  public static final int RANGE_PARTITION_OUTGOING_BATCH_SIZE = (1 << 12) - 1;\n+\n+  @JsonProperty(\"partitionFunction\")\n+  private PartitionFunction partitionFunction;\n \n   @JsonCreator\n-  public RangeSender(@JsonProperty(\"receiver-major-fragment\") int oppositeMajorFragmentId, @JsonProperty(\"child\") PhysicalOperator child, @JsonProperty(\"partitions\") List<EndpointPartition> partitions) {\n-    super(oppositeMajorFragmentId, child, Collections.<MinorFragmentEndpoint>emptyList());\n-    this.partitions = partitions;\n+  public RangePartitionSender(@JsonProperty(\"receiver-major-fragment\") int oppositeMajorFragmentId,\n+                              @JsonProperty(\"child\") PhysicalOperator child,\n+                              @JsonProperty(\"destinations\") List<MinorFragmentEndpoint> endpoints,\n+                              @JsonProperty(\"partitionFunction\") PartitionFunction partitionFunction) {\n+    super(oppositeMajorFragmentId, child, endpoints);\n+    this.partitionFunction = partitionFunction;\n   }\n \n   @Override\n   protected PhysicalOperator getNewWithChild(PhysicalOperator child) {\n-    return new RangeSender(oppositeMajorFragmentId, child, partitions);\n+    return new RangePartitionSender(oppositeMajorFragmentId, child, destinations, partitionFunction);\n   }\n \n-  public static class EndpointPartition{\n-    private final PartitionRange range;\n-    private final DrillbitEndpoint endpoint;\n+  @JsonProperty(\"partitionFunction\")\n+  public PartitionFunction getPartitionFunction() {\n+    return partitionFunction;\n+  }\n \n-    @JsonCreator\n-    public EndpointPartition(@JsonProperty(\"range\") PartitionRange range, @JsonProperty(\"endpoint\") DrillbitEndpoint endpoint) {\n-      super();\n-      this.range = range;\n-      this.endpoint = endpoint;\n-    }\n-    public PartitionRange getRange() {\n-      return range;\n-    }\n-    public DrillbitEndpoint getEndpoint() {\n-      return endpoint;\n-    }\n+  @Override\n+  public <T, X, E extends Throwable> T accept(PhysicalVisitor<T, X, E> physicalVisitor, X value) throws E {\n+    return physicalVisitor.visitRangePartitionSender(this, value);\n   }\n \n   @Override\n   public int getOperatorType() {\n-    return CoreOperatorType.RANGE_SENDER_VALUE;\n+    return CoreOperatorType.RANGE_PARTITION_SENDER_VALUE;\n   }\n+\n }",
                "previous_filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/config/RangeSender.java",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/config/RangePartitionSender.java",
                "sha": "0c0852a074f48a44c1056d67f49af818c78a5ba1",
                "status": "renamed"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScanBatch.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScanBatch.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScanBatch.java",
                "patch": "@@ -83,10 +83,14 @@\n   private final List<Map<String, String>> implicitColumnList;\n   private String currentReaderClassName;\n   private final RecordBatchStatsContext batchStatsContext;\n+\n   // Represents last outcome of next(). If an Exception is thrown\n   // during the method's execution a value IterOutcome.STOP will be assigned.\n   private IterOutcome lastOutcome;\n \n+  private List<RecordReader> readerList = null; // needed for repeatable scanners\n+  private boolean isRepeatableScan = false;     // needed for repeatable scanners\n+\n   /**\n    *\n    * @param context\n@@ -137,6 +141,15 @@ public ScanBatch(PhysicalOperator subScanConfig, FragmentContext context,\n         readers, Collections.<Map<String, String>> emptyList());\n   }\n \n+  public ScanBatch(PhysicalOperator subScanConfig, FragmentContext context,\n+                   List<RecordReader> readerList, boolean isRepeatableScan)\n+      throws ExecutionSetupException {\n+    this(context, context.newOperatorContext(subScanConfig),\n+        readerList, Collections.<Map<String, String>> emptyList());\n+    this.readerList = readerList;\n+    this.isRepeatableScan = isRepeatableScan;\n+  }\n+\n   @Override\n   public FragmentContext getContext() {\n     return context;\n@@ -255,7 +268,7 @@ private boolean getNextReaderIfHas() throws ExecutionSetupException {\n       return false;\n     }\n     currentReader = readers.next();\n-    if (readers.hasNext()) {\n+    if (!isRepeatableScan && readers.hasNext()) {\n       readers.remove();\n     }\n     implicitValues = implicitColumns.hasNext() ? implicitColumns.next() : null;",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScanBatch.java",
                "sha": "5ccf1c093207e9c5eb869de56b981eab53fe213a",
                "status": "modified"
            },
            {
                "additions": 79,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/RowKeyJoin.java",
                "changes": 79,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/RowKeyJoin.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/RowKeyJoin.java",
                "patch": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.physical.impl.join;\n+\n+import org.apache.commons.lang3.tuple.Pair;\n+import org.apache.drill.exec.record.AbstractRecordBatch.BatchState;\n+import org.apache.drill.exec.vector.ValueVector;\n+\n+/**\n+ * Interface for a row key join\n+ */\n+public interface RowKeyJoin {\n+\n+  /**\n+   * Enum for RowKeyJoin internal state.\n+   * Possible states are {INITIAL, PROCESSING, DONE}\n+   *\n+   * Initially RowKeyJoin will be at INITIAL state. Then the state will be transitioned\n+   * by the RestrictedJsonRecordReader to PROCESSING as soon as it processes the rows\n+   * related to RowKeys. Then RowKeyJoin algorithm sets to INITIAL state when leftStream has no data.\n+   * Basically RowKeyJoin calls leftStream multiple times depending upon the rightStream, hence\n+   * this transition from PROCESSING to INITIAL. If there is no data from rightStream or OutOfMemory\n+   * condition then the state is transitioned to DONE.\n+   */\n+  public enum RowKeyJoinState {\n+    INITIAL, PROCESSING, DONE;\n+  }\n+\n+  /**\n+   * Is the next batch of row keys ready to be returned\n+   * @return True if ready, false if not\n+   */\n+  public boolean hasRowKeyBatch();\n+\n+  /**\n+   * Get the next batch of row keys\n+   * @return a Pair whose left element is the ValueVector containing the row keys, right\n+   *    element is the number of row keys in this batch\n+   */\n+  public Pair<ValueVector, Integer> nextRowKeyBatch();\n+\n+\n+  /**\n+   * Get the current BatchState (this is useful when performing row key join)\n+   */\n+  public BatchState getBatchState();\n+\n+  /**\n+   * Set the BatchState (this is useful when performing row key join)\n+   * @param newState\n+   */\n+  public void setBatchState(BatchState newState);\n+\n+  /**\n+   * Set the RowKeyJoinState (this is useful for maintaining state for row key join algorithm)\n+   * @param newState\n+   */\n+  public void setRowKeyJoinState(RowKeyJoinState newState);\n+\n+  /**\n+   * Get the current RowKeyJoinState.\n+   */\n+  public RowKeyJoinState getRowKeyJoinState();\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/RowKeyJoin.java",
                "sha": "7b4dfcaaaaeddadd5a99e344fd047ea252a7fb41",
                "status": "added"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillRelOptUtil.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillRelOptUtil.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillRelOptUtil.java",
                "patch": "@@ -396,4 +396,21 @@ public RexNode go(RexNode rex) {\n       }\n     }\n   }\n+\n+  public static boolean isProjectFlatten(RelNode project) {\n+\n+    assert project instanceof Project : \"Rel is NOT an instance of project!\";\n+\n+    for (RexNode rex : project.getChildExps()) {\n+      RexNode newExpr = rex;\n+      if (rex instanceof RexCall) {\n+        RexCall function = (RexCall) rex;\n+        String functionName = function.getOperator().getName();\n+        if (functionName.equalsIgnoreCase(\"flatten\") ) {\n+          return true;\n+        }\n+      }\n+    }\n+    return false;\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillRelOptUtil.java",
                "sha": "b39328ea05fbcc7a4b3b4a43ad407d81f3564de8",
                "status": "modified"
            },
            {
                "additions": 79,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/cost/PluginCost.java",
                "changes": 79,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/cost/PluginCost.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/cost/PluginCost.java",
                "patch": "@@ -0,0 +1,79 @@\n+package org.apache.drill.exec.planner.cost;\n+\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+\n+import org.apache.drill.exec.physical.base.GroupScan;\n+\n+/**\n+ * PluginCost describes the cost factors to be used when costing for the specific storage/format plugin\n+ */\n+public interface PluginCost {\n+  org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(PluginCost.class);\n+\n+  /**\n+   * An interface to check if a parameter provided by user is valid or not.\n+   * @param <T> Type of the parameter.\n+   */\n+  interface CheckValid<T> {\n+    boolean isValid(T paramValue);\n+  }\n+\n+  /**\n+   * Class which checks whether the provided parameter value is greater than\n+   * or equals to a minimum limit.\n+   */\n+  class greaterThanEquals implements CheckValid<Integer> {\n+    private final Integer atleastEqualsTo;\n+    public greaterThanEquals(Integer atleast) {\n+      atleastEqualsTo = atleast;\n+    }\n+\n+    @Override\n+    public boolean isValid(Integer paramValue) {\n+      if (paramValue >= atleastEqualsTo &&\n+          paramValue <= Integer.MAX_VALUE) {\n+        return true;\n+      } else {\n+        logger.warn(\"Setting default value as the supplied parameter value is less than {}\", paramValue);\n+        return false;\n+      }\n+    }\n+  }\n+\n+  /**\n+   * @return the average column size in bytes\n+   */\n+  int getAverageColumnSize(GroupScan scan);\n+\n+  /**\n+   * @return the block size in bytes\n+   */\n+  int getBlockSize(GroupScan scan);\n+\n+  /**\n+   * @return the sequential block read cost\n+   */\n+  int getSequentialBlockReadCost(GroupScan scan);\n+\n+  /**\n+   * @return the random block read cost\n+   */\n+  int getRandomBlockReadCost(GroupScan scan);\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/cost/PluginCost.java",
                "sha": "d765162fb62e17a4087245242dc15922ec4e8e96",
                "status": "added"
            },
            {
                "additions": 96,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexCollection.java",
                "changes": 96,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexCollection.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexCollection.java",
                "patch": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;\n+\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.common.expression.SchemaPath;\n+\n+/**\n+ * Abstract base class for Index collection (collection of Index descriptors)\n+ *\n+ */\n+public abstract class AbstractIndexCollection implements IndexCollection, Iterable<IndexDescriptor> {\n+\n+  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(AbstractIndexCollection.class);\n+  /**\n+   * A set of indexes for a particular table\n+   */\n+  @JsonProperty\n+  protected List<IndexDescriptor> indexes;\n+\n+  public AbstractIndexCollection() {\n+    indexes = Lists.newArrayList();\n+  }\n+\n+  @Override\n+  public boolean addIndex(IndexDescriptor index) {\n+    return indexes.add(index);\n+  }\n+\n+  @Override\n+  public boolean removeIndex(IndexDescriptor index) {\n+    return indexes.remove(index);\n+  }\n+\n+  @Override\n+  public void clearAll() {\n+    indexes.clear();\n+  }\n+\n+  @Override\n+  public boolean supportsIndexSelection() {\n+    return false;\n+  }\n+\n+  @Override\n+  public double getRows(RexNode indexCondition) {\n+    throw new UnsupportedOperationException(\"getRows() not supported for this index collection.\");\n+  }\n+\n+  @Override\n+  public boolean supportsRowCountStats() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean supportsFullTextSearch() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isColumnIndexed(SchemaPath path) {\n+    for (IndexDescriptor index : indexes) {\n+      if (index.getIndexColumnOrdinal(path) >= 0) {\n+        return true;\n+      }\n+    }\n+    return false;\n+  }\n+\n+  @Override\n+  public Iterator<IndexDescriptor> iterator() {\n+    return indexes.iterator();\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexCollection.java",
                "sha": "9894b326322f6781dbc22743a5af7fbfe8b7448f",
                "status": "added"
            },
            {
                "additions": 74,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexDescriptor.java",
                "changes": 74,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexDescriptor.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexDescriptor.java",
                "patch": "@@ -0,0 +1,74 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import java.util.List;\n+\n+import org.apache.calcite.plan.RelOptCost;\n+import org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.calcite.rel.RelFieldCollation.NullDirection;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.common.expression.LogicalExpression;\n+import org.apache.drill.exec.physical.base.GroupScan;\n+import org.apache.drill.exec.physical.base.IndexGroupScan;\n+\n+/**\n+ * Abstract base class for an Index descriptor\n+ *\n+ */\n+public abstract class AbstractIndexDescriptor extends DrillIndexDefinition implements IndexDescriptor {\n+  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(AbstractIndexDescriptor .class);\n+\n+  public AbstractIndexDescriptor(List<LogicalExpression> indexCols,\n+                                 CollationContext indexCollationContext,\n+                                 List<LogicalExpression> nonIndexCols,\n+                                 List<LogicalExpression> rowKeyColumns,\n+                                 String indexName,\n+                                 String tableName,\n+                                 IndexType type,\n+                                 NullDirection nullsDirection) {\n+    super(indexCols, indexCollationContext, nonIndexCols, rowKeyColumns, indexName, tableName, type, nullsDirection);\n+  }\n+\n+  @Override\n+  public double getRows(RelNode scan, RexNode indexCondition) {\n+    throw new UnsupportedOperationException(\"getRows() not supported for this index.\");\n+  }\n+\n+  @Override\n+  public boolean supportsRowCountStats() {\n+    return false;\n+  }\n+\n+  @Override\n+  public IndexGroupScan getIndexGroupScan() {\n+    throw new UnsupportedOperationException(\"Group scan not supported for this index.\");\n+  }\n+\n+  @Override\n+  public boolean supportsFullTextSearch() {\n+    return false;\n+  }\n+\n+  @Override\n+  public RelOptCost getCost(IndexProperties indexProps, RelOptPlanner planner,\n+      int numProjectedFields, GroupScan primaryGroupScan) {\n+    throw new UnsupportedOperationException(\"getCost() not supported for this index.\");\n+  }\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexDescriptor.java",
                "sha": "f908ead4c90960be368d34b03902c0b6548c719e",
                "status": "added"
            },
            {
                "additions": 51,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexStatistics.java",
                "changes": 51,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexStatistics.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexStatistics.java",
                "patch": "@@ -0,0 +1,51 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.rel.RelCollation;\n+import org.apache.calcite.rel.RelDistribution;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.exec.planner.logical.DrillTable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.util.List;\n+\n+public abstract class AbstractIndexStatistics implements IndexStatistics {\n+\n+    protected static final Logger logger = LoggerFactory.getLogger(AbstractIndexStatistics.class);\n+    protected final RelNode input;\n+    protected final RexNode condition;\n+    protected final DrillTable table;\n+\n+    public AbstractIndexStatistics(RelNode input, RexNode condition, DrillTable table) {\n+            this.input = input;\n+            this.condition = condition;\n+            this.table = table;\n+    }\n+    public abstract double getRowCount();\n+\n+    public List<RelCollation> getCollations() {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    public RelDistribution getDistribution() {\n+        throw new UnsupportedOperationException();\n+    }\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/AbstractIndexStatistics.java",
                "sha": "dfc0897ee2887e95faf86dac5f79a44edeac4619",
                "status": "added"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/CollationContext.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/CollationContext.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/CollationContext.java",
                "patch": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.drill.common.expression.LogicalExpression;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+public class CollationContext {\n+\n+  public final Map<LogicalExpression, RelFieldCollation> collationMap;\n+  public final List<RelFieldCollation> relFieldCollations;\n+\n+  public CollationContext(Map<LogicalExpression, RelFieldCollation> collationMap,\n+      List<RelFieldCollation> relFieldCollations) {\n+    this.collationMap = collationMap;\n+    this.relFieldCollations = relFieldCollations;\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/CollationContext.java",
                "sha": "8260beea4bc2acc6b7d3b0a7ac985b7c1c66cdc9",
                "status": "added"
            },
            {
                "additions": 75,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexCollection.java",
                "changes": 75,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexCollection.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexCollection.java",
                "patch": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.exec.physical.base.IndexGroupScan;\n+import org.apache.calcite.rel.RelNode;\n+\n+import java.util.Set;\n+\n+public class DrillIndexCollection extends AbstractIndexCollection {\n+  private final RelNode scan;  // physical scan rel corresponding to the primary table\n+\n+  public DrillIndexCollection(RelNode scanRel,\n+                               Set<DrillIndexDescriptor> indexes) {\n+    this.scan = scanRel;\n+    for (IndexDescriptor index : indexes) {\n+      super.addIndex(index);\n+    }\n+  }\n+\n+  private IndexDescriptor getIndexDescriptor() {\n+\n+    //XXX need a policy to pick the indexDesc to use instead of picking the first one.\n+    return this.indexes.iterator().next();\n+  }\n+\n+  @Override\n+  public boolean supportsIndexSelection() {\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean supportsRowCountStats() {\n+    return true;\n+  }\n+\n+  @Override\n+  public boolean supportsFullTextSearch() {\n+    return true;\n+  }\n+\n+  @Override\n+  public double getRows(RexNode indexCondition) {\n+\n+    return getIndexDescriptor().getRows(scan, indexCondition);\n+  }\n+\n+  @Override\n+  public IndexGroupScan getGroupScan() {\n+    return getIndexDescriptor().getIndexGroupScan();\n+  }\n+\n+  @Override\n+  public IndexCollectionType getIndexCollectionType() {\n+    return IndexCollection.IndexCollectionType.EXTERNAL_SECONDARY_INDEX_COLLECTION;\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexCollection.java",
                "sha": "0ea3d83e7db385638a20ab624b90ca76742a93c0",
                "status": "added"
            },
            {
                "additions": 278,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexDefinition.java",
                "changes": 278,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexDefinition.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexDefinition.java",
                "patch": "@@ -0,0 +1,278 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import com.fasterxml.jackson.annotation.JsonIgnore;\n+import com.fasterxml.jackson.annotation.JsonProperty;\n+import org.apache.drill.shaded.guava.com.google.common.collect.Sets;\n+\n+import org.apache.calcite.rel.RelCollation;\n+import org.apache.calcite.rel.RelCollations;\n+import org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.calcite.rel.RelFieldCollation.NullDirection;\n+import org.apache.drill.common.expression.CastExpression;\n+import org.apache.drill.common.expression.LogicalExpression;\n+import org.apache.drill.common.expression.SchemaPath;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+\n+public class DrillIndexDefinition implements IndexDefinition {\n+  /**\n+   * The indexColumns is the list of column(s) on which this index is created. If there is more than 1 column,\n+   * the order of the columns is important: index on {a, b} is not the same as index on {b, a}\n+   * NOTE: the indexed column could be of type columnfamily.column\n+   */\n+  @JsonProperty\n+  protected final List<LogicalExpression> indexColumns;\n+\n+  /**\n+   * nonIndexColumns: the list of columns that are included in the index as 'covering'\n+   * columns but are not themselves indexed.  These are useful for covering indexes where the\n+   * query request can be satisfied directly by the index and avoid accessing the table altogether.\n+   */\n+  @JsonProperty\n+  protected final List<LogicalExpression> nonIndexColumns;\n+\n+  @JsonIgnore\n+  protected final Set<LogicalExpression> allIndexColumns;\n+\n+  @JsonProperty\n+  protected final List<LogicalExpression> rowKeyColumns;\n+\n+  @JsonProperty\n+  protected final CollationContext indexCollationContext;\n+\n+  /**\n+   * indexName: name of the index that should be unique within the scope of a table\n+   */\n+  @JsonProperty\n+  protected final String indexName;\n+\n+  protected final String tableName;\n+\n+  @JsonProperty\n+  protected final IndexDescriptor.IndexType indexType;\n+\n+  @JsonProperty\n+  protected final NullDirection nullsDirection;\n+\n+  public DrillIndexDefinition(List<LogicalExpression> indexCols,\n+                              CollationContext indexCollationContext,\n+                              List<LogicalExpression> nonIndexCols,\n+                              List<LogicalExpression> rowKeyColumns,\n+                              String indexName,\n+                              String tableName,\n+                              IndexType type,\n+                              NullDirection nullsDirection) {\n+    this.indexColumns = indexCols;\n+    this.nonIndexColumns = nonIndexCols;\n+    this.rowKeyColumns = rowKeyColumns;\n+    this.indexName = indexName;\n+    this.tableName = tableName;\n+    this.indexType = type;\n+    this.allIndexColumns = Sets.newHashSet(indexColumns);\n+    this.allIndexColumns.addAll(nonIndexColumns);\n+    this.indexCollationContext = indexCollationContext;\n+    this.nullsDirection = nullsDirection;\n+\n+  }\n+\n+  @Override\n+  public int getIndexColumnOrdinal(LogicalExpression path) {\n+    int id = indexColumns.indexOf(path);\n+    return id;\n+  }\n+\n+  @Override\n+  public boolean isCoveringIndex(List<LogicalExpression> columns) {\n+    return allIndexColumns.containsAll(columns);\n+  }\n+\n+  @Override\n+  public boolean allColumnsIndexed(Collection<LogicalExpression> columns) {\n+    return columnsInIndexFields(columns, indexColumns);\n+  }\n+\n+  @Override\n+  public boolean someColumnsIndexed(Collection<LogicalExpression> columns) {\n+    return someColumnsInIndexFields(columns, indexColumns);\n+  }\n+\n+  public boolean pathExactIn(SchemaPath path, Collection<LogicalExpression> exprs) {\n+    for (LogicalExpression expr : exprs) {\n+      if (expr instanceof SchemaPath) {\n+        if (((SchemaPath) expr).toExpr().equals(path.toExpr())) {\n+          return true;\n+        }\n+      }\n+    }\n+\n+    return false;\n+  }\n+\n+  boolean castIsCompatible(CastExpression castExpr, Collection<LogicalExpression> indexFields) {\n+    for(LogicalExpression indexExpr : indexFields) {\n+      if(indexExpr.getClass() != castExpr.getClass()) {\n+        continue;\n+      }\n+      CastExpression indexCastExpr = (CastExpression)indexExpr;\n+      //we compare input using equals because we know we are comparing SchemaPath,\n+      //if we extend to support other expression, make sure the equals of that expression\n+      //is implemented properly, otherwise it will fall to identity comparison\n+      if ( !castExpr.getInput().equals(indexCastExpr.getInput()) ) {\n+          continue;\n+      }\n+\n+      if( castExpr.getMajorType().getMinorType() != indexCastExpr.getMajorType().getMinorType()) {\n+        continue;\n+      }\n+      return true;\n+    }\n+    return false;\n+  }\n+\n+  protected boolean columnsInIndexFields(Collection<LogicalExpression> columns, Collection<LogicalExpression> indexFields) {\n+    //we need to do extra check, so we could allow the case when query condition expression is not identical with indexed fields\n+    //and they still could use the index either by implicit cast or the difference is allowed, e.g. width of varchar\n+    for (LogicalExpression col : columns) {\n+      if (col instanceof CastExpression) {\n+        if (!castIsCompatible((CastExpression) col, indexFields)) {\n+          return false;\n+        }\n+      }\n+      else {\n+        if (!pathExactIn((SchemaPath)col, indexFields)) {\n+          return false;\n+        }\n+      }\n+    }\n+    return true;//indexFields.containsAll(columns);\n+  }\n+\n+  protected boolean someColumnsInIndexFields(Collection<LogicalExpression> columns,\n+      Collection<LogicalExpression> indexFields) {\n+\n+    //we need to do extra check, so we could allow the case when query condition expression is not identical with indexed fields\n+    //and they still could use the index either by implicit cast or the difference is allowed, e.g. width of varchar\n+    for (LogicalExpression col : columns) {\n+      if (col instanceof CastExpression) {\n+        if (castIsCompatible((CastExpression) col, indexFields)) {\n+          return true;\n+        }\n+      }\n+      else {\n+        if (pathExactIn((SchemaPath)col, indexFields)) {\n+          return true;\n+        }\n+      }\n+    }\n+    return false;\n+  }\n+\n+  @Override\n+  public String toString() {\n+    String columnsDesc = \" Index columns: \" + indexColumns.toString() + \" Non-Index columns: \" + nonIndexColumns.toString();\n+    String desc = \"Table: \" + tableName + \" Index: \" + indexName + columnsDesc;\n+    return desc;\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    }\n+    if (o == null) {\n+      return false;\n+    }\n+    DrillIndexDefinition index1 = (DrillIndexDefinition) o;\n+    return tableName.equals(index1.tableName)\n+        && indexName.equals(index1.indexName)\n+        && indexType.equals(index1.indexType)\n+        && indexColumns.equals(index1.indexColumns);\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    final int prime = 31;\n+    final String fullName = tableName + indexName;\n+    int result = 1;\n+    result = prime * result + fullName.hashCode();\n+    result = prime * result + indexType.hashCode();\n+\n+    return result;\n+  }\n+\n+  @Override\n+  @JsonProperty\n+  public String getIndexName() {\n+    return indexName;\n+  }\n+\n+  @Override\n+  public String getTableName() {\n+    return tableName;\n+  }\n+\n+  @Override\n+  @JsonProperty\n+  public IndexDescriptor.IndexType getIndexType() {\n+    return indexType;\n+  }\n+\n+  @Override\n+  @JsonProperty\n+  public List<LogicalExpression> getRowKeyColumns() {\n+    return this.rowKeyColumns;\n+  }\n+\n+  @Override\n+  @JsonProperty\n+  public List<LogicalExpression> getIndexColumns() {\n+    return this.indexColumns;\n+  }\n+\n+  @Override\n+  @JsonProperty\n+  public List<LogicalExpression> getNonIndexColumns() {\n+    return this.nonIndexColumns;\n+  }\n+\n+  @Override\n+  @JsonIgnore\n+  public RelCollation getCollation() {\n+    if (indexCollationContext != null) {\n+      return RelCollations.of(indexCollationContext.relFieldCollations);\n+    }\n+    return null;\n+  }\n+\n+  @Override\n+  @JsonIgnore\n+  public Map<LogicalExpression, RelFieldCollation> getCollationMap() {\n+    return indexCollationContext.collationMap;\n+  }\n+\n+  @Override\n+  @JsonIgnore\n+  public NullDirection getNullsOrderingDirection() {\n+    return nullsDirection;\n+  }\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexDefinition.java",
                "sha": "03c2a44c68ff66a8bdb7a0ed9088780071fd232b",
                "status": "added"
            },
            {
                "additions": 110,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexDescriptor.java",
                "changes": 110,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexDescriptor.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexDescriptor.java",
                "patch": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.rel.RelFieldCollation.NullDirection;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.common.expression.LogicalExpression;\n+import org.apache.drill.exec.physical.base.GroupScan;\n+import org.apache.drill.exec.physical.base.IndexGroupScan;\n+import org.apache.drill.exec.planner.cost.PluginCost;\n+import org.apache.drill.exec.planner.logical.DrillTable;\n+\n+import java.io.IOException;\n+import java.util.List;\n+\n+public class DrillIndexDescriptor extends AbstractIndexDescriptor {\n+\n+  /**\n+   * The name of Drill's Storage Plugin on which the Index was stored\n+   */\n+  private String storage;\n+\n+  private DrillTable table;\n+\n+  public DrillIndexDescriptor(List<LogicalExpression> indexCols,\n+                              CollationContext indexCollationContext,\n+                              List<LogicalExpression> nonIndexCols,\n+                              List<LogicalExpression> rowKeyColumns,\n+                              String indexName,\n+                              String tableName,\n+                              IndexType type,\n+                              NullDirection nullsDirection) {\n+    super(indexCols, indexCollationContext, nonIndexCols, rowKeyColumns, indexName, tableName, type, nullsDirection);\n+  }\n+\n+  public DrillIndexDescriptor(DrillIndexDefinition def) {\n+    this(def.indexColumns, def.indexCollationContext, def.nonIndexColumns, def.rowKeyColumns, def.indexName,\n+        def.getTableName(), def.getIndexType(), def.nullsDirection);\n+  }\n+\n+  @Override\n+  public double getRows(RelNode scan, RexNode indexCondition) {\n+    //TODO: real implementation is to use Drill's stats implementation. for now return fake value 1.0\n+    return 1.0;\n+  }\n+\n+  @Override\n+  public IndexGroupScan getIndexGroupScan() {\n+    try {\n+      final DrillTable idxTable = getDrillTable();\n+      GroupScan scan = idxTable.getGroupScan();\n+\n+      if (!(scan instanceof IndexGroupScan)){\n+        logger.error(\"The Groupscan from table {} is not an IndexGroupScan\", idxTable.toString());\n+        return null;\n+      }\n+      return (IndexGroupScan)scan;\n+    }\n+    catch(IOException e) {\n+      logger.error(\"Error in getIndexGroupScan \", e);\n+    }\n+    return null;\n+  }\n+\n+  public void attach(String storageName, DrillTable inTable) {\n+    storage = storageName;\n+    setDrillTable(inTable);\n+  }\n+\n+  public void setStorageName(String storageName) {\n+    storage = storageName;\n+  }\n+\n+  public String getStorageName() {\n+    return storage;\n+  }\n+\n+  public void setDrillTable(DrillTable table) {\n+    this.table = table;\n+  }\n+\n+  public DrillTable getDrillTable() {\n+    return this.table;\n+  }\n+\n+  public FunctionalIndexInfo getFunctionalInfo() {\n+    return null;\n+  }\n+\n+  @Override\n+  public PluginCost getPluginCostModel() {\n+    return null;\n+  }\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/DrillIndexDescriptor.java",
                "sha": "4da62c204f6d46b4b26eb3e48d25dc9c7ae98b38",
                "status": "added"
            },
            {
                "additions": 85,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/FunctionalIndexInfo.java",
                "changes": 85,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/FunctionalIndexInfo.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/FunctionalIndexInfo.java",
                "patch": "@@ -0,0 +1,85 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.drill.common.expression.LogicalExpression;\n+import org.apache.drill.common.expression.SchemaPath;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * FunctionalIndexInfo is to collect Functional fields in IndexDescriptor, derive information needed for index plan,\n+ * e.g. convert and rewrite filter, columns, and rowtype on index scan that involve functional index.\n+ * In case different store might have different way to rename expression in index table, we allow storage plugin\n+ */\n+public interface FunctionalIndexInfo {\n+\n+  /**\n+   * @return if this index has functional indexed field, return true\n+   */\n+  boolean hasFunctional();\n+\n+  /**\n+   * @return the IndexDescriptor this IndexInfo built from\n+   */\n+  IndexDescriptor getIndexDesc();\n+\n+  /**\n+   * getNewPath: for an original path, return new rename '$N' path, notice there could be multiple renamed paths\n+   * if the there are multiple functional indexes refer original path.\n+   * @param path\n+   * @return\n+   */\n+  SchemaPath getNewPath(SchemaPath path);\n+\n+  /**\n+   * return a plain field path if the incoming index expression 'expr' is replaced to be a plain field\n+   * @param expr suppose to be an indexed expression\n+   * @return the renamed schemapath in index table for the indexed expression\n+   */\n+  SchemaPath getNewPathFromExpr(LogicalExpression expr);\n+\n+  /**\n+   * @return the map of indexed expression --> the involved schema paths in a indexed expression\n+   */\n+  Map<LogicalExpression, Set<SchemaPath>> getPathsInFunctionExpr();\n+\n+  /**\n+   * @return the map between indexed expression and to-be-converted target expression for scan in index\n+   * e.g. cast(a.b as int) -> '$0'\n+   */\n+  Map<LogicalExpression, LogicalExpression> getExprMap();\n+\n+  /**\n+   * @return the set of all new field names for indexed functions in index\n+   */\n+  Set<SchemaPath> allNewSchemaPaths();\n+\n+  /**\n+   * @return the set of all schemaPath exist in functional index fields\n+   */\n+  Set<SchemaPath> allPathsInFunction();\n+\n+  /**\n+   * Whether this implementation( may be different per storage) support rewrite rewriting varchar equality expression,\n+   * e.g. cast(a.b as varchar(2)) = 'ca'  to LIKE expression: cast(a.b as varchar(2) LIKE 'ca%'\n+   */\n+  boolean supportEqualCharConvertToLike();\n+\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/FunctionalIndexInfo.java",
                "sha": "a12dcc6c103dd576748c91031001798a8a69bf14",
                "status": "added"
            },
            {
                "additions": 76,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexCallContext.java",
                "changes": 76,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexCallContext.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexCallContext.java",
                "patch": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.plan.RelOptRuleCall;\n+import org.apache.calcite.rel.RelCollation;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rel.core.Sort;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.common.expression.LogicalExpression;\n+import org.apache.drill.common.expression.SchemaPath;\n+import org.apache.drill.exec.physical.base.DbGroupScan;\n+import org.apache.drill.exec.planner.physical.DrillDistributionTrait.DistributionField;\n+import org.apache.drill.exec.planner.common.DrillScanRelBase;\n+import org.apache.drill.exec.planner.common.DrillProjectRelBase;\n+import java.util.List;\n+import java.util.Set;\n+\n+public interface IndexCallContext {\n+  DrillScanRelBase getScan();\n+\n+  DbGroupScan getGroupScan();\n+\n+  List<RelCollation> getCollationList();\n+\n+  RelCollation getCollation();\n+\n+  boolean hasLowerProject();\n+\n+  boolean hasUpperProject();\n+\n+  RelOptRuleCall getCall();\n+\n+  Set<LogicalExpression> getLeftOutPathsInFunctions();\n+\n+  RelNode getFilter();\n+\n+  IndexableExprMarker getOrigMarker();\n+\n+  List<LogicalExpression> getSortExprs();\n+\n+  DrillProjectRelBase getLowerProject();\n+\n+  DrillProjectRelBase getUpperProject();\n+\n+  void setLeftOutPathsInFunctions(Set<LogicalExpression> exprs);\n+\n+  List<SchemaPath> getScanColumns();\n+\n+  RexNode getFilterCondition();\n+\n+  RexNode getOrigCondition();\n+\n+  Sort getSort();\n+\n+  void createSortExprs();\n+\n+  RelNode getExchange();\n+\n+  List<DistributionField> getDistributionFields();\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexCallContext.java",
                "sha": "65788cb52ae3eb932a97e2d49c45d1d96aed348c",
                "status": "added"
            },
            {
                "additions": 99,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexCollection.java",
                "changes": 99,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexCollection.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexCollection.java",
                "patch": "@@ -0,0 +1,99 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.common.expression.SchemaPath;\n+import org.apache.drill.exec.physical.base.IndexGroupScan;\n+\n+// Interface used to describe an index collection\n+public interface IndexCollection extends Iterable<IndexDescriptor> {\n+  /**\n+   * Types of an index collections: NATIVE_SECONDARY_INDEX_COLLECTION, EXTERNAL_SECONDARY_INDEX_COLLECTION\n+   */\n+  public static enum IndexCollectionType {\n+    NATIVE_SECONDARY_INDEX_COLLECTION,\n+    EXTERNAL_SECONDARY_INDEX_COLLECTION\n+  };\n+\n+  /**\n+   * Add a new index to the collection. Return True if index was successfully added; False otherwise\n+   */\n+  public boolean addIndex(IndexDescriptor index);\n+\n+  /**\n+   * Remove an index (identified by table name and index name) from the collection.\n+   * Return True if index was successfully removed; False otherwise\n+   */\n+  public boolean removeIndex(IndexDescriptor index);\n+\n+  /**\n+   * Clears all entries from this index collection\n+   */\n+  public void clearAll();\n+\n+  /**\n+   * Get the type of this index based on {@link IndexCollectionType}\n+   * @return one of the values in {@link IndexCollectionType}\n+   */\n+  public IndexCollectionType getIndexCollectionType();\n+\n+  /**\n+   * Whether or not this index collection supports index selection (selecting an\n+   * appropriate index out of multiple candidates). Typically, external index collections\n+   * such as Elasticsearch already have this capability while native secondary index collection\n+   * may not have - in such cases, Drill needs to do the index selection.\n+   */\n+  public boolean supportsIndexSelection();\n+\n+  /**\n+   * Get the estimated row count for a single index condition\n+   * @param indexCondition The index condition (e.g index_col1 < 10 AND index_col2 = 'abc')\n+   * @return The estimated row count\n+   */\n+  public double getRows(RexNode indexCondition);\n+\n+  /**\n+   * Whether or not the index supports getting row count statistics\n+   * @return True if index supports getting row count, False otherwise\n+   */\n+  public boolean supportsRowCountStats();\n+\n+  /**\n+   * Whether or not the index supports full-text search (to allow pushing down such filters)\n+   * @return True if index supports full-text search, False otherwise\n+   */\n+  public boolean supportsFullTextSearch();\n+\n+  /**\n+   * If this IndexCollection exposes a single GroupScan, return the GroupScan instance. For external indexes\n+   * such as Elasticsearch, we may have a single GroupScan representing all the indexes contained\n+   * within that collection.  On the other hand, for native indexes, each separate index would\n+   * have its own GroupScan.\n+   * @return GroupScan for this IndexCollection if available, otherwise null\n+   */\n+  public IndexGroupScan getGroupScan();\n+\n+  /**\n+   * Check if the field name is the leading key of any of the indexes in this collection\n+   * @param path\n+   * @return True if an appropriate index is found, False otherwise\n+   */\n+  public boolean isColumnIndexed(SchemaPath path);\n+\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexCollection.java",
                "sha": "9b4d170e07dc6d1153133aa6a51978d46f28ebb6",
                "status": "added"
            },
            {
                "additions": 105,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDefinition.java",
                "changes": 105,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDefinition.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDefinition.java",
                "patch": "@@ -0,0 +1,105 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.rel.RelCollation;\n+import org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.calcite.rel.RelFieldCollation.NullDirection;\n+import org.apache.drill.common.expression.LogicalExpression;\n+\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+\n+// Interface used to define an index,\n+public interface IndexDefinition {\n+  /**\n+   * Types of an index: PRIMARY_KEY_INDEX, NATIVE_SECONDARY_INDEX, EXTERNAL_SECONDARY_INDEX\n+   */\n+  static enum IndexType {\n+    PRIMARY_KEY_INDEX,\n+    NATIVE_SECONDARY_INDEX,\n+    EXTERNAL_SECONDARY_INDEX\n+  };\n+\n+  /**\n+   * Check to see if the field name is an index column and if so return the ordinal position in the index\n+   * @param path The field path you want to compare to index column names.\n+   * @return Return ordinal of the indexed column if valid, otherwise return -1\n+   */\n+  int getIndexColumnOrdinal(LogicalExpression path);\n+\n+  /**\n+   * Get the name of the index\n+   */\n+  String getIndexName();\n+\n+  /**\n+   * Check if this index 'covers' all the columns specified in the supplied list of columns\n+   * @param columns\n+   * @return True for covering index, False for non-covering\n+   */\n+  boolean isCoveringIndex(List<LogicalExpression> columns);\n+\n+  /**\n+   * Check if this index have all the columns specified in the supplied list of columns indexed\n+   * @param columns\n+   * @return True if all fields are indexed, False for some or all fields is not indexed\n+   */\n+  boolean allColumnsIndexed(Collection<LogicalExpression> columns);\n+\n+  /**\n+   * Check if this index has some columns specified in the supplied list of columns indexed\n+   * @param columns\n+   * @return True if some fields are indexed, False if none of the fields are indexed\n+   */\n+  boolean someColumnsIndexed(Collection<LogicalExpression> columns);\n+\n+  /**\n+   * Get the list of columns (typically 1 column) that constitute the row key (primary key)\n+   * @return\n+   */\n+  List<LogicalExpression> getRowKeyColumns();\n+\n+  /**\n+   * Get the name of the table this index is associated with\n+   */\n+  String getTableName();\n+\n+  /**\n+   * Get the type of this index based on {@link IndexType}\n+   * @return one of the values in {@link IndexType}\n+   */\n+  IndexType getIndexType();\n+\n+\n+  List<LogicalExpression> getIndexColumns();\n+\n+  List<LogicalExpression> getNonIndexColumns();\n+\n+  RelCollation getCollation();\n+\n+  Map<LogicalExpression, RelFieldCollation> getCollationMap();\n+\n+  /**\n+   * Get the nulls ordering of this index\n+   * @return True, if nulls first. False otherwise\n+   */\n+  NullDirection getNullsOrderingDirection();\n+\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDefinition.java",
                "sha": "995d23c5742ff12cee9b6940ddb1dfbf4be0e89d",
                "status": "added"
            },
            {
                "additions": 68,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDescriptor.java",
                "changes": 68,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDescriptor.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDescriptor.java",
                "patch": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.plan.RelOptCost;\n+import org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.exec.physical.base.GroupScan;\n+import org.apache.drill.exec.physical.base.IndexGroupScan;\n+import org.apache.drill.exec.planner.cost.PluginCost;\n+\n+\n+/**\n+ * IndexDefinition + functions to access materialized index(index table/scan, etc)\n+ */\n+\n+public interface IndexDescriptor extends IndexDefinition {\n+\n+  /**\n+   * Get the estimated row count for a single index condition\n+   * @param input The rel node corresponding to the primary table\n+   * @param indexCondition The index condition (e.g index_col1 < 10 AND index_col2 = 'abc')\n+   * @return The estimated row count\n+   */\n+  double getRows(RelNode input, RexNode indexCondition);\n+\n+  /**\n+   * Whether or not the index supports getting row count statistics\n+   * @return True if index supports getting row count, False otherwise\n+   */\n+  boolean supportsRowCountStats();\n+\n+  /**\n+   * Get an instance of the group scan associated with this index descriptor\n+   * @return An instance of group scan for this index\n+   */\n+  IndexGroupScan getIndexGroupScan();\n+\n+  /**\n+   * Whether or not the index supports full-text search (to allow pushing down such filters)\n+   * @return True if index supports full-text search, False otherwise\n+   */\n+  boolean supportsFullTextSearch();\n+\n+  FunctionalIndexInfo getFunctionalInfo();\n+\n+  public RelOptCost getCost(IndexProperties indexProps, RelOptPlanner planner,\n+      int numProjectedFields, GroupScan primaryGroupScan);\n+\n+  public PluginCost getPluginCostModel();\n+\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDescriptor.java",
                "sha": "f355285cc22b9feb88b893c88852466559530d6e",
                "status": "added"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscover.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscover.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscover.java",
                "patch": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+\n+public interface IndexDiscover {\n+    IndexCollection getTableIndex(String tableName);\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscover.java",
                "sha": "309083b1bb9edf369395ce8e79aad2ef3c3d9ff8",
                "status": "added"
            },
            {
                "additions": 110,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscoverBase.java",
                "changes": 110,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscoverBase.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscoverBase.java",
                "patch": "@@ -0,0 +1,110 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.drill.exec.physical.base.AbstractDbGroupScan;\n+import org.apache.drill.exec.planner.common.DrillScanRelBase;\n+import org.apache.drill.exec.planner.logical.DrillTable;\n+import org.apache.drill.exec.planner.physical.ScanPrel;\n+import org.apache.calcite.rel.RelNode;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+/**\n+ * IndexDiscoverBase is the layer to read index configurations of tables on storage plugins,\n+ * then based on the properties it collected, get the StoragePlugin from StoragePluginRegistry,\n+ * together with indexes information, build an IndexCollection\n+ */\n+public abstract class IndexDiscoverBase implements IndexDiscover {\n+  static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(IndexDiscoverBase.class);\n+\n+  private AbstractDbGroupScan scan; // group scan corresponding to the primary table\n+  private RelNode scanRel;   // physical scan rel corresponding to the primary table\n+\n+  public IndexDiscoverBase(AbstractDbGroupScan inScan, DrillScanRelBase inScanPrel) {\n+    scan = inScan;\n+    scanRel = inScanPrel;\n+  }\n+\n+  public IndexDiscoverBase(AbstractDbGroupScan inScan, ScanPrel inScanPrel) {\n+    scan = inScan;\n+    scanRel = inScanPrel;\n+  }\n+\n+  public AbstractDbGroupScan getOriginalScan() {\n+    return scan;\n+  }\n+\n+  public RelNode getOriginalScanRel() {\n+    return scanRel;\n+  }\n+\n+  public IndexCollection getTableIndex(String tableName, String storageName, Collection<DrillIndexDefinition>  indexDefs ) {\n+    Set<DrillIndexDescriptor> idxSet = new HashSet<>();\n+    for (DrillIndexDefinition def : indexDefs) {\n+      DrillIndexDescriptor indexDescriptor = new DrillIndexDescriptor(def);\n+      materializeIndex(storageName, indexDescriptor);\n+    }\n+    return new DrillIndexCollection(getOriginalScanRel(), idxSet);\n+  }\n+\n+  public void materializeIndex(String storageName, DrillIndexDescriptor index) {\n+    index.setStorageName(storageName);\n+    index.setDrillTable(buildDrillTable(index));\n+  }\n+\n+  /**\n+   * When there is storageName in IndexDescriptor, get a DrillTable instance based on the\n+   * StorageName and other informaiton in idxDesc that helps identifying the table.\n+   * @param idxDesc\n+   * @return\n+   */\n+  public DrillTable getExternalDrillTable(IndexDescriptor idxDesc) {\n+    //XX: get table object for this index, index storage plugin should provide interface to get the DrillTable object\n+    return null;\n+  }\n+\n+  /**\n+   * Abstract function getDrillTable will be implemented the IndexDiscover within storage plugin(e.g. HBase, MaprDB)\n+   * since the implementations of AbstractStoragePlugin, IndexDescriptor and DrillTable in that storage plugin may have\n+   * the implement details.\n+   * @param idxDesc\n+\n+   * @return\n+   */\n+  public DrillTable buildDrillTable(IndexDescriptor idxDesc) {\n+    if(idxDesc.getIndexType() == IndexDescriptor.IndexType.EXTERNAL_SECONDARY_INDEX) {\n+      return getExternalDrillTable(idxDesc);\n+    }\n+    else {\n+      return getNativeDrillTable(idxDesc);\n+    }\n+  }\n+\n+  /**\n+   * When it is native index(index provided by native storage plugin),\n+   * the actual IndexDiscover should provide the implementation to get the DrillTable object of index,\n+   * Otherwise, we call IndexDiscoverable interface exposed from external storage plugin's SchemaFactory\n+   * to get the desired DrillTable.\n+   * @param idxDesc\n+   * @return\n+   */\n+  public abstract DrillTable getNativeDrillTable(IndexDescriptor idxDesc);\n+\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscoverBase.java",
                "sha": "fde2a32d2431850609479a976c3c9b97057323fd",
                "status": "added"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscoverable.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscoverable.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscoverable.java",
                "patch": "@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.drill.exec.planner.logical.DrillTable;\n+\n+\n+/**\n+ * SchemaFactory of a storage plugin that can used to store index tables should expose this interface to allow\n+ * IndexDiscovers discovering the index table without adding dependency to the storage plugin.\n+ */\n+public interface IndexDiscoverable {\n+\n+  /**\n+   * return the found DrillTable with path (e.g. names={\"elasticsearch\", \"staffidx\", \"stjson\"})\n+   * @param discover\n+   * @param desc\n+   * @return\n+   */\n+    DrillTable findTable(IndexDiscover discover, DrillIndexDescriptor desc);\n+\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexDiscoverable.java",
                "sha": "dbf5edc82c5fcfc73da09440d546ad0387c12c9d",
                "status": "added"
            },
            {
                "additions": 63,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexGroup.java",
                "changes": 63,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexGroup.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexGroup.java",
                "patch": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;\n+\n+import java.util.List;\n+\n+/**\n+ * Encapsulates one or more IndexProperties representing (non)covering or intersecting indexes. The encapsulated\n+ * IndexProperties are used to rank the index in comparison with other IndexGroups.\n+ */\n+public class IndexGroup {\n+  private List<IndexProperties> indexProps;\n+\n+  public IndexGroup() {\n+    indexProps = Lists.newArrayList();\n+  }\n+\n+  public boolean isIntersectIndex() {\n+    if (indexProps.size() > 1) {\n+      return true;\n+    } else {\n+      return false;\n+    }\n+  }\n+\n+  public int numIndexes() {\n+    return indexProps.size();\n+  }\n+\n+  public void addIndexProp(IndexProperties prop) {\n+    indexProps.add(prop);\n+  }\n+\n+  public void addIndexProp(List<IndexProperties> prop) {\n+    indexProps.addAll(prop);\n+  }\n+\n+  public boolean removeIndexProp(IndexProperties prop) {\n+    return indexProps.remove(prop);\n+  }\n+\n+  public List<IndexProperties> getIndexProps() {\n+    return indexProps;\n+  }\n+}\n+",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexGroup.java",
                "sha": "ea34ea585b53e58946418b322ca7a7353cec00e9",
                "status": "added"
            },
            {
                "additions": 64,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexProperties.java",
                "changes": 64,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexProperties.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexProperties.java",
                "patch": "@@ -0,0 +1,64 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.plan.RelOptCost;\n+import org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.common.expression.LogicalExpression;\n+import org.apache.drill.exec.planner.common.DrillScanRelBase;\n+\n+import java.util.Map;\n+\n+/**\n+ * IndexProperties encapsulates the various metrics of a single index that are related to\n+ * the current query. These metrics are subsequently used to rank the index in comparison\n+ * with other indexes.\n+ */\n+public interface IndexProperties  {\n+\n+  void setProperties(Map<LogicalExpression, RexNode> prefixMap,\n+                            boolean satisfiesCollation,\n+                            RexNode indexColumnsRemainderFilter,\n+                            Statistics stats);\n+\n+  double getLeadingSelectivity();\n+\n+  double getRemainderSelectivity();\n+\n+  boolean isCovering();\n+\n+  double getTotalRows();\n+\n+  IndexDescriptor getIndexDesc();\n+\n+  DrillScanRelBase getPrimaryTableScan();\n+\n+  RexNode getTotalRemainderFilter();\n+\n+  boolean satisfiesCollation();\n+\n+  void setSatisfiesCollation(boolean satisfiesCollation);\n+\n+  RelOptCost getSelfCost(RelOptPlanner planner);\n+\n+  int numLeadingFilters();\n+\n+  double getAvgRowSize();\n+}\n+",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexProperties.java",
                "sha": "cfdd6d0302c0a9b89adc89251131ccecf45f3ca7",
                "status": "added"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexStatistics.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexStatistics.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexStatistics.java",
                "patch": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.rel.RelCollation;\n+import org.apache.calcite.rel.RelDistribution;\n+\n+\n+import java.util.List;\n+\n+public interface IndexStatistics {\n+    /** Returns the approximate number of rows in the table. */\n+    double getRowCount();\n+\n+    /** Returns the collections of columns on which this table is sorted. */\n+    List<RelCollation> getCollations();\n+\n+    /** Returns the distribution of the data in query result table. */\n+    RelDistribution getDistribution();\n+\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexStatistics.java",
                "sha": "e71636973c8d6bc8d52be8df6746b0d63799fdd5",
                "status": "added"
            },
            {
                "additions": 262,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexableExprMarker.java",
                "changes": 262,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexableExprMarker.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexableExprMarker.java",
                "patch": "@@ -0,0 +1,262 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+\n+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableMap;\n+import org.apache.drill.shaded.guava.com.google.common.collect.Maps;\n+import org.apache.drill.shaded.guava.com.google.common.collect.Sets;\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rex.RexCall;\n+import org.apache.calcite.rex.RexCorrelVariable;\n+import org.apache.calcite.rex.RexDynamicParam;\n+import org.apache.calcite.rex.RexFieldAccess;\n+import org.apache.calcite.rex.RexInputRef;\n+import org.apache.calcite.rex.RexLiteral;\n+import org.apache.calcite.rex.RexLocalRef;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.calcite.rex.RexOver;\n+import org.apache.calcite.rex.RexRangeRef;\n+import org.apache.calcite.rex.RexVisitorImpl;\n+import org.apache.calcite.sql.SqlKind;\n+import org.apache.calcite.sql.type.SqlTypeName;\n+import org.apache.drill.common.expression.LogicalExpression;\n+import org.apache.drill.exec.planner.logical.DrillOptiq;\n+import org.apache.drill.exec.planner.logical.DrillParseContext;\n+import org.apache.drill.exec.planner.physical.PrelUtil;\n+\n+import java.util.Map;\n+import java.util.Set;\n+\n+/**\n+ * The filter expressions that could be indexed\n+ * Other than SchemaPaths, which represent columns of a table and could be indexed,\n+ * we consider only function expressions, and specifically, CAST function.\n+ * To judge if an expression is indexable, we check these:\n+ * 1, this expression should be one operand of a comparison operator, one of SqlKind.COMPARISON:\n+ *      IN, EQUALS, NOT_EQUALS, LESS_THAN, GREATER_THAN, GREATER_THAN_OR_EQUAL, LESS_THAN_OR_EQUAL\n+ * 2, the expression tree should contain at least one inputRef (which means this expression is a\n+ *     computation on top of at least one column), and if we have more than one indexable expressions\n+ *     are found from operands of comparison operator, we should not take any expression as indexable.\n+ *\n+ * 3, (LIMIT to one level function) the expression is a function call, and no nested function call underneath, except ITEM\n+ * 4, (LIMIT to CAST), the function call is a CAST\n+ */\n+public class IndexableExprMarker extends RexVisitorImpl<Boolean> {\n+\n+  //map of rexNode->converted LogicalExpression\n+  final Map<RexNode, LogicalExpression> desiredExpressions = Maps.newHashMap();\n+\n+  //the expressions in equality comparison\n+  final Map<RexNode, LogicalExpression> equalityExpressions = Maps.newHashMap();\n+\n+  //the expression found in non-equality comparison\n+  final Map<RexNode, LogicalExpression> notInEquality = Maps.newHashMap();\n+\n+  //for =(cast(a.b as VARCHAR(len)), 'abcd'), if the 'len' is less than the max length of casted field on index table,\n+  // we want to rewrite it to LIKE(cast(a.b as VARCHAR(len)), 'abcd%')\n+  //map equalOnCastChar: key is the equal operator, value is the operand (cast(a.b as VARCHAR(10)),\n+  final Map<RexNode, LogicalExpression> equalOnCastChar = Maps.newHashMap();\n+\n+  final private RelNode inputRel;\n+\n+  //flag current recursive call state: whether we are on a direct operand of comparison operator\n+  boolean directCompareOp = false;\n+\n+  RexCall contextCall = null;\n+\n+  DrillParseContext parserContext;\n+\n+  public IndexableExprMarker(RelNode inputRel) {\n+    super(true);\n+    this.inputRel = inputRel;\n+    parserContext = new DrillParseContext(PrelUtil.getPlannerSettings(inputRel.getCluster()));\n+  }\n+\n+  public Map<RexNode, LogicalExpression> getIndexableExpression() {\n+    return ImmutableMap.copyOf(desiredExpressions);\n+  }\n+\n+  public Map<RexNode, LogicalExpression> getEqualOnCastChar() {\n+    return ImmutableMap.copyOf(equalOnCastChar);\n+  }\n+\n+  /**\n+   * return the expressions that were only in equality condition _and_ only once. ( a.b = 'value' )\n+   * @return\n+   */\n+  public Set<LogicalExpression> getExpressionsOnlyInEquality() {\n+\n+    Set<LogicalExpression> onlyInEquality = Sets.newHashSet();\n+\n+    Set<LogicalExpression> notInEqSet = Sets.newHashSet();\n+\n+    Set<LogicalExpression> inEqMoreThanOnce = Sets.newHashSet();\n+\n+    notInEqSet.addAll(notInEquality.values());\n+\n+    for (LogicalExpression expr : equalityExpressions.values()) {\n+      //only process expr that is not in any non-equality condition(!notInEqSet.contains)\n+      if (!notInEqSet.contains(expr)) {\n+\n+        //expr appear in two and more equality conditions should be ignored too\n+        if (inEqMoreThanOnce.contains(expr)) {\n+          continue;\n+        }\n+\n+        //we already have recorded this expr in equality condition, move it to inEqMoreThanOnce\n+        if (onlyInEquality.contains(expr)) {\n+          inEqMoreThanOnce.add(expr);\n+          onlyInEquality.remove(expr);\n+          continue;\n+        }\n+\n+        //finally we could take this expr\n+        onlyInEquality.add(expr);\n+      }\n+    }\n+    return onlyInEquality;\n+  }\n+\n+  @Override\n+  public Boolean visitInputRef(RexInputRef rexInputRef) {\n+    return directCompareOp;\n+  }\n+\n+  public boolean containInputRef(RexNode rex) {\n+    if (rex instanceof RexInputRef) {\n+      return true;\n+    }\n+    if ((rex instanceof RexCall) && \"ITEM\".equals(((RexCall)rex).getOperator().getName())) {\n+      return true;\n+    }\n+    //TODO: use a visitor search recursively for inputRef, if found one return true\n+    return false;\n+  }\n+\n+  public boolean operandsAreIndexable(RexCall call) {\n+    SqlKind kind = call.getKind();\n+    boolean kindIsRight = (SqlKind.COMPARISON.contains(kind) || kind==SqlKind.LIKE || kind == SqlKind.SIMILAR);\n+\n+    if (!kindIsRight) {\n+      return false;\n+    }\n+\n+    int inputReference = 0;\n+    for (RexNode operand : call.operands) {\n+      //if for this operator, there are two operands and more have inputRef, which means it is something like:\n+      // a.b = a.c, instead of a.b ='hello', so this cannot apply index\n+      if (containInputRef(operand)) {\n+        inputReference++;\n+        if(inputReference>=2) {\n+          return false;\n+        }\n+      }\n+    }\n+    return true;\n+  }\n+\n+  @Override\n+  public Boolean visitCall(RexCall call) {\n+    if (call.getKind() == SqlKind.NOT) {\n+      // Conditions under NOT are not indexable\n+      return false;\n+    }\n+    if (operandsAreIndexable(call)) {\n+      for (RexNode operand : call.operands) {\n+        directCompareOp = true;\n+        contextCall = call;\n+        boolean markIt = operand.accept(this);\n+        directCompareOp = false;\n+        contextCall = null;\n+        if (markIt) {\n+          LogicalExpression expr = DrillOptiq.toDrill(parserContext, inputRel, operand);\n+          desiredExpressions.put(operand, expr);\n+          if (call.getKind() == SqlKind.EQUALS) {\n+            equalityExpressions.put(operand, expr);\n+          }\n+          else {\n+            notInEquality.put(operand, expr);\n+          }\n+        }\n+      }\n+      return false;\n+    }\n+\n+    //now we are handling a call directly under comparison e.g. <([call], literal)\n+    if (directCompareOp) {\n+      // if it is an item, or CAST function\n+      if (\"ITEM\".equals(call.getOperator().getName())) {\n+        return directCompareOp;\n+      }\n+      else if (call.getKind() == SqlKind.CAST) {\n+        //For now, we care only direct CAST: CAST's operand is a field(schemaPath),\n+        // either ITEM call(nested name) or inputRef\n+\n+        //cast as char/varchar in equals function\n+        if(contextCall != null && contextCall.getKind() == SqlKind.EQUALS\n+            && (call.getType().getSqlTypeName()== SqlTypeName.CHAR\n+                || call.getType().getSqlTypeName()==SqlTypeName.VARCHAR)) {\n+          equalOnCastChar.put(contextCall, DrillOptiq.toDrill(parserContext, inputRel, call));\n+        }\n+\n+        RexNode castOp = call.operands.get(0);\n+        if (castOp instanceof RexInputRef) {\n+          return true;\n+        }\n+        if ((castOp instanceof RexCall) && (\"ITEM\".equals(((RexCall)castOp).getOperator().getName()))) {\n+          return true;\n+        }\n+      }\n+    }\n+\n+    for (RexNode operand : call.operands) {\n+      boolean bret = operand.accept(this);\n+    }\n+    return false;\n+  }\n+\n+  public Boolean visitLocalRef(RexLocalRef localRef) {\n+    return false;\n+  }\n+\n+  public Boolean visitLiteral(RexLiteral literal) {\n+    return false;\n+  }\n+\n+  public Boolean visitOver(RexOver over) {\n+    return false;\n+  }\n+\n+  public Boolean visitCorrelVariable(RexCorrelVariable correlVariable) {\n+    return false;\n+  }\n+\n+  public Boolean visitDynamicParam(RexDynamicParam dynamicParam) {\n+    return false;\n+  }\n+\n+  public Boolean visitRangeRef(RexRangeRef rangeRef) {\n+    return false;\n+  }\n+\n+  public Boolean visitFieldAccess(RexFieldAccess fieldAccess) {\n+    final RexNode expr = fieldAccess.getReferenceExpr();\n+    return expr.accept(this);\n+  }\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/IndexableExprMarker.java",
                "sha": "a1a6fc882a35e76cd473fc1e60ff1317865b8102",
                "status": "added"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/InvalidIndexDefinitionException.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/InvalidIndexDefinitionException.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/InvalidIndexDefinitionException.java",
                "patch": "@@ -0,0 +1,27 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.drill.common.exceptions.DrillRuntimeException;\n+\n+public class InvalidIndexDefinitionException extends DrillRuntimeException {\n+  public InvalidIndexDefinitionException(String message) {\n+    super(message);\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/InvalidIndexDefinitionException.java",
                "sha": "c17d09f000ff79a8a9c0ac56fd126781250dfd67",
                "status": "added"
            },
            {
                "additions": 66,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/Statistics.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/Statistics.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/Statistics.java",
                "patch": "@@ -0,0 +1,66 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+import org.apache.calcite.rel.RelNode;\n+import org.apache.calcite.rex.RexNode;\n+import org.apache.drill.exec.planner.common.DrillScanRelBase;\n+\n+public interface Statistics {\n+\n+  double ROWCOUNT_UNKNOWN = -1;\n+  //HUGE is same as DrillCostBase.HUGE\n+  double ROWCOUNT_HUGE = Double.MAX_VALUE;\n+  double AVG_ROWSIZE_UNKNOWN = -1;\n+  long AVG_COLUMN_SIZE = 10;\n+\n+  /** Returns whether statistics are available. Should be called prior to using the statistics\n+   */\n+  boolean isStatsAvailable();\n+\n+  /** Returns a unique index identifier\n+   *  @param idx - Index specified as a {@link IndexDescriptor}\n+   *  @return The unique index identifier\n+   */\n+  String buildUniqueIndexIdentifier(IndexDescriptor idx);\n+\n+  /** Returns the rowcount for the specified filter condition\n+   *  @param condition - Filter specified as a {@link RexNode}\n+   *  @param tabIdxName - The index name generated using {@code buildUniqueIndexIdentifier}\n+   *  @param scanRel - The current scan rel\n+   *  @return the rowcount for the filter\n+   */\n+  double getRowCount(RexNode condition, String tabIdxName, RelNode scanRel);\n+\n+  /** Returns the leading rowcount for the specified filter condition\n+   *  Leading rowcount means rowcount for filter condition only on leading index columns.\n+   *  @param condition - Filter specified as a {@link RexNode}\n+   *  @param tabIdxName - The index name generated using {@code buildUniqueIndexIdentifier}\n+   *  @param scanRel - The current scan rel\n+   *  @return the leading rowcount\n+   */\n+  double getLeadingRowCount(RexNode condition, String tabIdxName, RelNode scanRel);\n+\n+  /** Returns the average row size for the specified filter condition\n+   * @param tabIdxName - The index name generated using {@code buildUniqueIndexIdentifier}\n+   * @param isIndexScan - Whether the current rel is an index scan (false for primary table)\n+   */\n+  double getAvgRowSize(String tabIdxName, boolean isIndexScan);\n+\n+  boolean initialize(RexNode condition, DrillScanRelBase scanRel, IndexCallContext context);\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/Statistics.java",
                "sha": "2859102e21599b8b27106e7fb0cea8ba8ab88154",
                "status": "added"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/StatisticsPayload.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/StatisticsPayload.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/StatisticsPayload.java",
                "patch": "@@ -0,0 +1,24 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.index;\n+\n+public interface StatisticsPayload {\n+  double getRowCount();\n+  double getLeadingRowCount();\n+  double getAvgRowSize();\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/index/StatisticsPayload.java",
                "sha": "6894e4fdbe61156213cffe33cc819e559434bb10",
                "status": "added"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillTable.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillTable.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillTable.java",
                "patch": "@@ -91,6 +91,10 @@ public void setOptions(SessionOptionManager options) {\n     this.options = options;\n   }\n \n+  public void setGroupScan(GroupScan scan) {\n+    this.scan = scan;\n+  }\n+\n   public GroupScan getGroupScan() throws IOException{\n     if (scan == null) {\n       if (selection instanceof FileSelection && ((FileSelection) selection).isEmptyDirectory()) {",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillTable.java",
                "sha": "ed9b32fe27bd89d5d067fbac8fc240ade70f2985",
                "status": "modified"
            },
            {
                "additions": 56,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/PartitionFunction.java",
                "changes": 56,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/PartitionFunction.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/PartitionFunction.java",
                "patch": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.planner.physical;\n+\n+import java.util.List;\n+\n+import org.apache.drill.common.expression.FieldReference;\n+import org.apache.drill.exec.record.VectorWrapper;\n+\n+import com.fasterxml.jackson.annotation.JsonTypeInfo;\n+\n+@JsonTypeInfo(use=JsonTypeInfo.Id.CLASS, include=JsonTypeInfo.As.PROPERTY, property=\"@class\")\n+public interface PartitionFunction  {\n+\n+  /**\n+   * Return the list of FieldReferences that participate in the partitioning function\n+   * @return list of FieldReferences\n+   */\n+  List<FieldReference> getPartitionRefList();\n+\n+  /**\n+   * Setup method for the partitioning function\n+   * @param partitionKeys a list of partition columns on which range partitioning is needed\n+   */\n+  void setup(List<VectorWrapper<?>> partitionKeys);\n+\n+  /**\n+   * Evaluate a partitioning function for a particular row index and return the partition id\n+   * @param index the integer index into the partition keys vector for a specific 'row' of values\n+   * @param numPartitions the max number of partitions that are allowed\n+   * @return partition id, an integer value\n+   */\n+  int eval(int index, int numPartitions);\n+\n+  /**\n+   * Returns a FieldReference (LogicalExpression) for the partition function\n+   * @return FieldReference for the partition function\n+   */\n+  FieldReference getPartitionFieldRef();\n+\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/planner/physical/PartitionFunction.java",
                "sha": "754c5d753e3dad3ea8b1ef910be2a9958d6f9d5b",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java",
                "patch": "@@ -80,7 +80,7 @@ protected AbstractRecordBatch(final T popConfig, final FragmentContext context,\n     }\n   }\n \n-  protected static enum BatchState {\n+  public static enum BatchState {\n     /** Need to build schema and return. */\n     BUILD_SCHEMA,\n     /** This is still the first data batch. */",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java",
                "sha": "cb790918a47500628d124b1c7687641962c0bd60",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/store/AbstractRecordReader.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/AbstractRecordReader.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/AbstractRecordReader.java",
                "patch": "@@ -101,6 +101,11 @@ public void allocate(Map<String, ValueVector> vectorMap) throws OutOfMemoryExcep\n     }\n   }\n \n+  @Override\n+  public boolean hasNext() {\n+    return false;\n+  }\n+\n   protected List<SchemaPath> getDefaultColumnsToRead() {\n     return GroupScan.ALL_COLUMNS;\n   }",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/store/AbstractRecordReader.java",
                "sha": "1bbbe76b7c100868f925f5d260bac6fc29d7ccdb",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/store/RecordReader.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/RecordReader.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/RecordReader.java",
                "patch": "@@ -42,6 +42,14 @@\n \n   void allocate(Map<String, ValueVector> vectorMap) throws OutOfMemoryException;\n \n+  /**\n+   * Check if the reader may have potentially more data to be read in subsequent iterations. Certain types of readers\n+   * such as repeatable readers can be invoked multiple times, so this method will allow ScanBatch to check with\n+   * the reader before closing it.\n+   * @return return true if there could potentially be more reads, false otherwise\n+   */\n+  boolean hasNext();\n+\n   /**\n    * Increments this record reader forward, writing via the provided output\n    * mutator into the output batch.",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/store/RecordReader.java",
                "sha": "33b361c7b0d014c1a3fffd2cfec0733ad01d8871",
                "status": "modified"
            },
            {
                "additions": 291,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/util/EncodedSchemaPathSet.java",
                "changes": 291,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/util/EncodedSchemaPathSet.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/util/EncodedSchemaPathSet.java",
                "patch": "@@ -0,0 +1,291 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.util;\n+\n+\n+import org.apache.drill.shaded.guava.com.google.common.io.BaseEncoding;\n+import org.apache.drill.shaded.guava.com.google.common.base.Preconditions;\n+import org.apache.drill.shaded.guava.com.google.common.collect.ImmutableList;\n+import org.apache.drill.shaded.guava.com.google.common.collect.Lists;\n+import org.apache.drill.common.exceptions.DrillRuntimeException;\n+import org.apache.drill.common.expression.SchemaPath;\n+import org.apache.drill.exec.planner.physical.PlannerSettings;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.IOException;\n+import java.io.UnsupportedEncodingException;\n+import java.util.Arrays;\n+import java.util.Collection;\n+import java.util.Iterator;\n+import java.util.List;\n+\n+/**\n+ * This class provided utility methods to encode and decode a set of user specified\n+ * SchemaPaths to a set of encoded SchemaPaths with the following properties.\n+ * <ol>\n+ * <li>Valid Drill identifier as per its grammar with only one, root name segment.\n+ * <li>A single identifier can not exceed 1024 characters in length.\n+ * </ol>\n+ * <p>\n+ * Format of the encoded SchemaPath:\n+ * <blockquote><pre>$$ENC\\d\\dlt;base32 encoded input paths&gt;</pre></blockquote>\n+ * <p>\n+ * We use Base-32 over Base-64 because the later's charset includes '\\' and '+'.\n+ */\n+public class EncodedSchemaPathSet {\n+\n+  private static final int ESTIMATED_ENCODED_SIZE = 1024;\n+\n+  private static final String ENC_PREFIX = \"$$ENC\";\n+\n+  private static final String ENC_FORMAT_STRING = ENC_PREFIX + \"%02d%s\";\n+  private static final int ENC_PREFIX_SIZE = ENC_PREFIX.length() + \"00\".length();\n+  private static final int MAX_ENC_IDENTIFIER_SIZE = (PlannerSettings.DEFAULT_IDENTIFIER_MAX_LENGTH - ENC_PREFIX_SIZE);\n+  private static final int MAX_ENC_IDENTIFIER_COUNT = 100; // \"$$ENC00*...$$ENC99*\"\n+\n+  private static final BaseEncoding CODEC = BaseEncoding.base32().omitPadding(); // no-padding version\n+\n+  public static final String ENCODED_STAR_COLUMN = encode(\"*\")[0];\n+\n+  /*\n+   * Performance of various methods of encoding a Java String to UTF-8 keeps changing\n+   * between releases, hence we'll encapsulate the actual methods within these functions\n+   * and use them everywhere in Drill\n+   */\n+  private static final String UTF_8 = \"utf-8\";\n+\n+\n+  private static byte[] encodeUTF(String input) {\n+    try {\n+      return input.getBytes(UTF_8);\n+    } catch (UnsupportedEncodingException e) {\n+      throw new DrillRuntimeException(e); // should never come to this\n+    }\n+  }\n+\n+  private static String decodeUTF(byte[] input) {\n+    try {\n+      return new String(input, UTF_8);\n+    } catch (UnsupportedEncodingException e) {\n+      throw new DrillRuntimeException(e); // should never come to this\n+    }\n+  }\n+\n+  private static String decodeUTF(byte[] input, int offset, int length) {\n+    try {\n+      return new String(input, offset, length, UTF_8);\n+    } catch (UnsupportedEncodingException e) {\n+      throw new DrillRuntimeException(e); // should never come to this\n+    }\n+  }\n+\n+  /**\n+   * Returns the encoded array of SchemaPath identifiers from the input array of SchemaPath.\n+   * <p>\n+   * The returned identifiers have the following properties:\n+   * <ul>\n+   *  <li>Each SchemaPath identifier in the array has only one single root NameSegment.</li>\n+   *  <li>Maximum length of each such identifier is equal to the maximum length of Drill identifier (currently 1024).</li>\n+   * </ul>\n+   * <p>\n+   * We take advantage of the fact that Java's modified utf-8 encoding can never contain\n+   * embedded null byte.\n+   * @see <a>http://docs.oracle.com/javase/8/docs/api/java/io/DataInput.html#modified-utf-8</a>\n+   */\n+  public static String[] encode(final String... schemaPaths) {\n+    Preconditions.checkArgument(schemaPaths != null && schemaPaths.length > 0,\n+        \"At least one schema path should be provided\");\n+\n+    NoCopyByteArrayOutputStream out = new NoCopyByteArrayOutputStream(ESTIMATED_ENCODED_SIZE);\n+    int bufOffset = 1; // 1st byte is NULL\n+    for (String schemaPath : schemaPaths) {\n+      out.write(0);\n+      out.write(encodeUTF(schemaPath));\n+    }\n+    out.close();\n+\n+    final int bufLen = out.size() - 1; // not counting the first NULL byte\n+    String encodedStr = CODEC.encode(out.getBuffer(), bufOffset, bufLen);\n+    assert !encodedStr.endsWith(\"=\") : String.format(\"Encoded string '%s' ends with '='\", encodedStr);\n+    return splitIdentifiers(encodedStr);\n+  }\n+\n+  public static boolean isEncodedSchemaPath(SchemaPath schemaPath) {\n+    return schemaPath != null && isEncodedSchemaPath(schemaPath.getRootSegment().getNameSegment().getPath());\n+  }\n+\n+  public static boolean isEncodedSchemaPath(String schemaPath) {\n+    return schemaPath != null && schemaPath.startsWith(ENC_PREFIX);\n+  }\n+\n+  /**\n+   * Returns the decoded Collection of SchemaPath from the input which\n+   * may contain a mix of encoded and non-encoded SchemaPaths.\n+   * <p>\n+   * The size of returned Collection is always equal to or greater than the\n+   * input array.\n+   * <p>\n+   * The non-encoded SchemaPaths are collated in the beginning to the returned\n+   * array, in the same order as that of the input array.\n+   */\n+  public static Collection<SchemaPath> decode(final Collection<SchemaPath> encodedPaths) {\n+    String[] schemaPathStrings = new String[encodedPaths.size()];\n+    Iterator<SchemaPath> encodedPathsItr = encodedPaths.iterator();\n+    for (int i = 0; i < schemaPathStrings.length; i++) {\n+      SchemaPath schemaPath = encodedPathsItr.next();\n+      if (schemaPath.getRootSegmentPath().startsWith(ENC_PREFIX)) {\n+        // encoded schema path contains only root segment\n+        schemaPathStrings[i] = schemaPath.getRootSegmentPath();\n+      } else {\n+        schemaPathStrings[i] = schemaPath.toExpr();\n+      }\n+    }\n+    String[] decodedStrings = decode(schemaPathStrings);\n+    if (decodedStrings == schemaPathStrings) {\n+      return encodedPaths; // return the original collection as no encoded SchemaPath was found\n+    } else {\n+      ImmutableList.Builder<SchemaPath> builder = new ImmutableList.Builder<>();\n+      for (String decodedString : decodedStrings) {\n+        if (\"*\".equals(decodedString) || \"`*`\".equals(decodedString)) {\n+          builder.add(SchemaPath.STAR_COLUMN);\n+        } else {\n+          builder.add(SchemaPath.parseFromString(decodedString));\n+        }\n+      }\n+      return builder.build();\n+    }\n+  }\n+\n+  /**\n+   * Returns the decoded array of SchemaPath strings from the input which\n+   * may contain a mix of encoded and non-encoded SchemaPaths.\n+   * <p>\n+   * The size of returned array is always equal to or greater than the\n+   * input array.\n+   * <p>\n+   * The non-encoded SchemaPaths are collated in the beginning to the returned\n+   * array, in the same order as that of the input array.\n+   */\n+  public static String[] decode(final String... encodedPaths) {\n+    Preconditions.checkArgument(encodedPaths != null && encodedPaths.length > 0,\n+        \"At least one encoded path should be provided\");\n+\n+    StringBuilder sb = new StringBuilder(ESTIMATED_ENCODED_SIZE);\n+\n+    // As the encoded schema path move across components, they could get reordered.\n+    // Sorting ensures that the original order is restored before concatenating the\n+    // components back to the full encoded String.\n+    Arrays.sort(encodedPaths);\n+\n+    List<String> decodedPathList = Lists.newArrayList();\n+    for (String encodedPath : encodedPaths) {\n+      if (encodedPath.startsWith(ENC_PREFIX)) {\n+        sb.append(encodedPath, ENC_PREFIX_SIZE, encodedPath.length());\n+      } else {\n+        decodedPathList.add(encodedPath);\n+      }\n+    }\n+\n+    if (sb.length() > 0) {\n+      byte[] decodedBytes;\n+      try {\n+        decodedBytes = CODEC.decode(sb);\n+      } catch (IllegalArgumentException e) {\n+        throw new DrillRuntimeException(String.format(\n+            \"Unable to decode the input strings as encoded schema paths:\\n%s\", Arrays.asList(encodedPaths)), e);\n+      }\n+\n+      int start = 0, index = 0;\n+      for (; index < decodedBytes.length; index++) {\n+        if (decodedBytes[index] == 0 && index - start > 0) {\n+          decodedPathList.add(decodeUTF(decodedBytes, start, index-start));\n+          start = index + 1;\n+        }\n+      }\n+      if (index - start > 0) {\n+        String lastSchemaPath = decodeUTF(decodedBytes, start, index-start).trim();\n+        if (!lastSchemaPath.isEmpty()) {\n+          decodedPathList.add(lastSchemaPath);\n+        }\n+      }\n+      return decodedPathList.toArray(new String[decodedPathList.size()]);\n+    } else {\n+      // original list did not have any encoded path, return as is\n+      return encodedPaths;\n+    }\n+  }\n+\n+  /**\n+   * Splits the input string so that the length of each encoded string,\n+   * including the signature prefix is less than or equal to MAX_DRILL_IDENTIFIER_SIZE.\n+   */\n+  private static String[] splitIdentifiers(String input) {\n+    if (input.length() < MAX_ENC_IDENTIFIER_SIZE) {\n+      return new String[] { String.format(ENC_FORMAT_STRING, 0, input) };\n+    }\n+    int splitsCount = (int) Math.ceil(input.length() / (double)MAX_ENC_IDENTIFIER_SIZE);\n+    if (splitsCount > MAX_ENC_IDENTIFIER_COUNT) {\n+      throw new DrillRuntimeException(String.format(\n+          \"Encoded size of the SchemaPath identifier '%s' exceeded maximum value.\", input));\n+    }\n+    String[] result = new String[splitsCount];\n+    for (int i = 0, startIdx = 0; i < result.length; i++, startIdx += MAX_ENC_IDENTIFIER_SIZE) {\n+      // TODO: see if we can avoid memcpy due to input.substring() call\n+      result[i] = String.format(ENC_FORMAT_STRING, i, input.substring(startIdx, Math.min(input.length(), startIdx + MAX_ENC_IDENTIFIER_SIZE)));\n+    }\n+    return result;\n+  }\n+\n+  /**\n+   * Optimized version of Java's ByteArrayOutputStream which returns the underlying\n+   * byte array instead of making a copy\n+   */\n+  private static class NoCopyByteArrayOutputStream extends ByteArrayOutputStream {\n+    public NoCopyByteArrayOutputStream(int size) {\n+      super(size);\n+    }\n+\n+    public byte[] getBuffer() {\n+      return buf;\n+    }\n+\n+    public int size() {\n+      return count;\n+    }\n+\n+    @Override\n+    public void write(int b) {\n+      super.write(b);\n+    }\n+\n+    @Override\n+    public void write(byte[] b) {\n+      super.write(b, 0, b.length);\n+    }\n+\n+    @Override\n+    public void close() {\n+      try {\n+        super.close();\n+      } catch (IOException e) {\n+        throw new DrillRuntimeException(e); // should never come to this\n+      }\n+    }\n+  }\n+\n+}",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/exec/java-exec/src/main/java/org/apache/drill/exec/util/EncodedSchemaPathSet.java",
                "sha": "5f9eef80190be6ffd6d29e02096f66366f20dfb8",
                "status": "added"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/protocol/src/main/java/org/apache/drill/exec/proto/UserBitShared.java",
                "changes": 75,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/protocol/src/main/java/org/apache/drill/exec/proto/UserBitShared.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 33,
                "filename": "protocol/src/main/java/org/apache/drill/exec/proto/UserBitShared.java",
                "patch": "@@ -414,9 +414,9 @@ private FragmentState(int index, int value) {\n      */\n     UNORDERED_RECEIVER(11, 11),\n     /**\n-     * <code>RANGE_SENDER = 12;</code>\n+     * <code>RANGE_PARTITION_SENDER = 12;</code>\n      */\n-    RANGE_SENDER(12, 12),\n+    RANGE_PARTITION_SENDER(12, 12),\n     /**\n      * <code>SCREEN = 13;</code>\n      */\n@@ -593,6 +593,10 @@ private FragmentState(int index, int value) {\n      * <code>RUNTIME_FILTER = 56;</code>\n      */\n     RUNTIME_FILTER(56, 56),\n+    /**\n+     * <code>ROWKEY_JOIN = 57;</code>\n+     */\n+    ROWKEY_JOIN(57, 57),\n     ;\n \n     /**\n@@ -644,9 +648,9 @@ private FragmentState(int index, int value) {\n      */\n     public static final int UNORDERED_RECEIVER_VALUE = 11;\n     /**\n-     * <code>RANGE_SENDER = 12;</code>\n+     * <code>RANGE_PARTITION_SENDER = 12;</code>\n      */\n-    public static final int RANGE_SENDER_VALUE = 12;\n+    public static final int RANGE_PARTITION_SENDER_VALUE = 12;\n     /**\n      * <code>SCREEN = 13;</code>\n      */\n@@ -823,6 +827,10 @@ private FragmentState(int index, int value) {\n      * <code>RUNTIME_FILTER = 56;</code>\n      */\n     public static final int RUNTIME_FILTER_VALUE = 56;\n+    /**\n+     * <code>ROWKEY_JOIN = 57;</code>\n+     */\n+    public static final int ROWKEY_JOIN_VALUE = 57;\n \n \n     public final int getNumber() { return value; }\n@@ -841,7 +849,7 @@ public static CoreOperatorType valueOf(int value) {\n         case 9: return ORDERED_PARTITION_SENDER;\n         case 10: return PROJECT;\n         case 11: return UNORDERED_RECEIVER;\n-        case 12: return RANGE_SENDER;\n+        case 12: return RANGE_PARTITION_SENDER;\n         case 13: return SCREEN;\n         case 14: return SELECTION_VECTOR_REMOVER;\n         case 15: return STREAMING_AGGREGATE;\n@@ -886,6 +894,7 @@ public static CoreOperatorType valueOf(int value) {\n         case 54: return PARTITION_LIMIT;\n         case 55: return PCAPNG_SUB_SCAN;\n         case 56: return RUNTIME_FILTER;\n+        case 57: return ROWKEY_JOIN;\n         default: return null;\n       }\n     }\n@@ -24422,40 +24431,40 @@ public Builder clearStatus() {\n       \"TATEMENT\\020\\005*\\207\\001\\n\\rFragmentState\\022\\013\\n\\007SENDING\\020\" +\n       \"\\000\\022\\027\\n\\023AWAITING_ALLOCATION\\020\\001\\022\\013\\n\\007RUNNING\\020\\002\\022\" +\n       \"\\014\\n\\010FINISHED\\020\\003\\022\\r\\n\\tCANCELLED\\020\\004\\022\\n\\n\\006FAILED\\020\\005\" +\n-      \"\\022\\032\\n\\026CANCELLATION_REQUESTED\\020\\006*\\367\\010\\n\\020CoreOpe\" +\n+      \"\\022\\032\\n\\026CANCELLATION_REQUESTED\\020\\006*\\222\\t\\n\\020CoreOpe\" +\n       \"ratorType\\022\\021\\n\\rSINGLE_SENDER\\020\\000\\022\\024\\n\\020BROADCAS\" +\n       \"T_SENDER\\020\\001\\022\\n\\n\\006FILTER\\020\\002\\022\\022\\n\\016HASH_AGGREGATE\" +\n       \"\\020\\003\\022\\r\\n\\tHASH_JOIN\\020\\004\\022\\016\\n\\nMERGE_JOIN\\020\\005\\022\\031\\n\\025HAS\" +\n       \"H_PARTITION_SENDER\\020\\006\\022\\t\\n\\005LIMIT\\020\\007\\022\\024\\n\\020MERGI\" +\n       \"NG_RECEIVER\\020\\010\\022\\034\\n\\030ORDERED_PARTITION_SENDE\" +\n       \"R\\020\\t\\022\\013\\n\\007PROJECT\\020\\n\\022\\026\\n\\022UNORDERED_RECEIVER\\020\\013\",\n-      \"\\022\\020\\n\\014RANGE_SENDER\\020\\014\\022\\n\\n\\006SCREEN\\020\\r\\022\\034\\n\\030SELECT\" +\n-      \"ION_VECTOR_REMOVER\\020\\016\\022\\027\\n\\023STREAMING_AGGREG\" +\n-      \"ATE\\020\\017\\022\\016\\n\\nTOP_N_SORT\\020\\020\\022\\021\\n\\rEXTERNAL_SORT\\020\\021\" +\n-      \"\\022\\t\\n\\005TRACE\\020\\022\\022\\t\\n\\005UNION\\020\\023\\022\\014\\n\\010OLD_SORT\\020\\024\\022\\032\\n\\026\" +\n-      \"PARQUET_ROW_GROUP_SCAN\\020\\025\\022\\021\\n\\rHIVE_SUB_SCA\" +\n-      \"N\\020\\026\\022\\025\\n\\021SYSTEM_TABLE_SCAN\\020\\027\\022\\021\\n\\rMOCK_SUB_S\" +\n-      \"CAN\\020\\030\\022\\022\\n\\016PARQUET_WRITER\\020\\031\\022\\023\\n\\017DIRECT_SUB_\" +\n-      \"SCAN\\020\\032\\022\\017\\n\\013TEXT_WRITER\\020\\033\\022\\021\\n\\rTEXT_SUB_SCAN\" +\n-      \"\\020\\034\\022\\021\\n\\rJSON_SUB_SCAN\\020\\035\\022\\030\\n\\024INFO_SCHEMA_SUB\" +\n-      \"_SCAN\\020\\036\\022\\023\\n\\017COMPLEX_TO_JSON\\020\\037\\022\\025\\n\\021PRODUCER\",\n-      \"_CONSUMER\\020 \\022\\022\\n\\016HBASE_SUB_SCAN\\020!\\022\\n\\n\\006WINDO\" +\n-      \"W\\020\\\"\\022\\024\\n\\020NESTED_LOOP_JOIN\\020#\\022\\021\\n\\rAVRO_SUB_SC\" +\n-      \"AN\\020$\\022\\021\\n\\rPCAP_SUB_SCAN\\020%\\022\\022\\n\\016KAFKA_SUB_SCA\" +\n-      \"N\\020&\\022\\021\\n\\rKUDU_SUB_SCAN\\020\\'\\022\\013\\n\\007FLATTEN\\020(\\022\\020\\n\\014L\" +\n-      \"ATERAL_JOIN\\020)\\022\\n\\n\\006UNNEST\\020*\\022,\\n(HIVE_DRILL_\" +\n-      \"NATIVE_PARQUET_ROW_GROUP_SCAN\\020+\\022\\r\\n\\tJDBC_\" +\n-      \"SCAN\\020,\\022\\022\\n\\016REGEX_SUB_SCAN\\020-\\022\\023\\n\\017MAPRDB_SUB\" +\n-      \"_SCAN\\020.\\022\\022\\n\\016MONGO_SUB_SCAN\\020/\\022\\017\\n\\013KUDU_WRIT\" +\n-      \"ER\\0200\\022\\026\\n\\022OPEN_TSDB_SUB_SCAN\\0201\\022\\017\\n\\013JSON_WRI\" +\n-      \"TER\\0202\\022\\026\\n\\022HTPPD_LOG_SUB_SCAN\\0203\\022\\022\\n\\016IMAGE_S\",\n-      \"UB_SCAN\\0204\\022\\025\\n\\021SEQUENCE_SUB_SCAN\\0205\\022\\023\\n\\017PART\" +\n-      \"ITION_LIMIT\\0206\\022\\023\\n\\017PCAPNG_SUB_SCAN\\0207\\022\\022\\n\\016RU\" +\n-      \"NTIME_FILTER\\0208*g\\n\\nSaslStatus\\022\\020\\n\\014SASL_UNK\" +\n-      \"NOWN\\020\\000\\022\\016\\n\\nSASL_START\\020\\001\\022\\024\\n\\020SASL_IN_PROGRE\" +\n-      \"SS\\020\\002\\022\\020\\n\\014SASL_SUCCESS\\020\\003\\022\\017\\n\\013SASL_FAILED\\020\\004B\" +\n-      \".\\n\\033org.apache.drill.exec.protoB\\rUserBitS\" +\n-      \"haredH\\001\"\n+      \"\\022\\032\\n\\026RANGE_PARTITION_SENDER\\020\\014\\022\\n\\n\\006SCREEN\\020\\r\" +\n+      \"\\022\\034\\n\\030SELECTION_VECTOR_REMOVER\\020\\016\\022\\027\\n\\023STREAM\" +\n+      \"ING_AGGREGATE\\020\\017\\022\\016\\n\\nTOP_N_SORT\\020\\020\\022\\021\\n\\rEXTER\" +\n+      \"NAL_SORT\\020\\021\\022\\t\\n\\005TRACE\\020\\022\\022\\t\\n\\005UNION\\020\\023\\022\\014\\n\\010OLD_\" +\n+      \"SORT\\020\\024\\022\\032\\n\\026PARQUET_ROW_GROUP_SCAN\\020\\025\\022\\021\\n\\rHI\" +\n+      \"VE_SUB_SCAN\\020\\026\\022\\025\\n\\021SYSTEM_TABLE_SCAN\\020\\027\\022\\021\\n\\r\" +\n+      \"MOCK_SUB_SCAN\\020\\030\\022\\022\\n\\016PARQUET_WRITER\\020\\031\\022\\023\\n\\017D\" +\n+      \"IRECT_SUB_SCAN\\020\\032\\022\\017\\n\\013TEXT_WRITER\\020\\033\\022\\021\\n\\rTEX\" +\n+      \"T_SUB_SCAN\\020\\034\\022\\021\\n\\rJSON_SUB_SCAN\\020\\035\\022\\030\\n\\024INFO_\" +\n+      \"SCHEMA_SUB_SCAN\\020\\036\\022\\023\\n\\017COMPLEX_TO_JSON\\020\\037\\022\\025\",\n+      \"\\n\\021PRODUCER_CONSUMER\\020 \\022\\022\\n\\016HBASE_SUB_SCAN\\020\" +\n+      \"!\\022\\n\\n\\006WINDOW\\020\\\"\\022\\024\\n\\020NESTED_LOOP_JOIN\\020#\\022\\021\\n\\rA\" +\n+      \"VRO_SUB_SCAN\\020$\\022\\021\\n\\rPCAP_SUB_SCAN\\020%\\022\\022\\n\\016KAF\" +\n+      \"KA_SUB_SCAN\\020&\\022\\021\\n\\rKUDU_SUB_SCAN\\020\\'\\022\\013\\n\\007FLAT\" +\n+      \"TEN\\020(\\022\\020\\n\\014LATERAL_JOIN\\020)\\022\\n\\n\\006UNNEST\\020*\\022,\\n(H\" +\n+      \"IVE_DRILL_NATIVE_PARQUET_ROW_GROUP_SCAN\\020\" +\n+      \"+\\022\\r\\n\\tJDBC_SCAN\\020,\\022\\022\\n\\016REGEX_SUB_SCAN\\020-\\022\\023\\n\\017\" +\n+      \"MAPRDB_SUB_SCAN\\020.\\022\\022\\n\\016MONGO_SUB_SCAN\\020/\\022\\017\\n\" +\n+      \"\\013KUDU_WRITER\\0200\\022\\026\\n\\022OPEN_TSDB_SUB_SCAN\\0201\\022\\017\" +\n+      \"\\n\\013JSON_WRITER\\0202\\022\\026\\n\\022HTPPD_LOG_SUB_SCAN\\0203\\022\",\n+      \"\\022\\n\\016IMAGE_SUB_SCAN\\0204\\022\\025\\n\\021SEQUENCE_SUB_SCAN\" +\n+      \"\\0205\\022\\023\\n\\017PARTITION_LIMIT\\0206\\022\\023\\n\\017PCAPNG_SUB_SC\" +\n+      \"AN\\0207\\022\\022\\n\\016RUNTIME_FILTER\\0208\\022\\017\\n\\013ROWKEY_JOIN\\020\" +\n+      \"9*g\\n\\nSaslStatus\\022\\020\\n\\014SASL_UNKNOWN\\020\\000\\022\\016\\n\\nSAS\" +\n+      \"L_START\\020\\001\\022\\024\\n\\020SASL_IN_PROGRESS\\020\\002\\022\\020\\n\\014SASL_\" +\n+      \"SUCCESS\\020\\003\\022\\017\\n\\013SASL_FAILED\\020\\004B.\\n\\033org.apache\" +\n+      \".drill.exec.protoB\\rUserBitSharedH\\001\"\n     };\n     com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner assigner =\n       new com.google.protobuf.Descriptors.FileDescriptor.InternalDescriptorAssigner() {",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/protocol/src/main/java/org/apache/drill/exec/proto/UserBitShared.java",
                "sha": "2f5c3de214d87c151cc23d52769d691df258ed9d",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/protocol/src/main/java/org/apache/drill/exec/proto/beans/CoreOperatorType.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/protocol/src/main/java/org/apache/drill/exec/proto/beans/CoreOperatorType.java?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 3,
                "filename": "protocol/src/main/java/org/apache/drill/exec/proto/beans/CoreOperatorType.java",
                "patch": "@@ -34,7 +34,7 @@\n     ORDERED_PARTITION_SENDER(9),\n     PROJECT(10),\n     UNORDERED_RECEIVER(11),\n-    RANGE_SENDER(12),\n+    RANGE_PARTITION_SENDER(12),\n     SCREEN(13),\n     SELECTION_VECTOR_REMOVER(14),\n     STREAMING_AGGREGATE(15),\n@@ -78,7 +78,8 @@\n     SEQUENCE_SUB_SCAN(53),\n     PARTITION_LIMIT(54),\n     PCAPNG_SUB_SCAN(55),\n-    RUNTIME_FILTER(56);\n+    RUNTIME_FILTER(56),\n+    ROWKEY_JOIN(57);\n     \n     public final int number;\n     \n@@ -108,7 +109,7 @@ public static CoreOperatorType valueOf(int number)\n             case 9: return ORDERED_PARTITION_SENDER;\n             case 10: return PROJECT;\n             case 11: return UNORDERED_RECEIVER;\n-            case 12: return RANGE_SENDER;\n+            case 12: return RANGE_PARTITION_SENDER;\n             case 13: return SCREEN;\n             case 14: return SELECTION_VECTOR_REMOVER;\n             case 15: return STREAMING_AGGREGATE;\n@@ -153,6 +154,7 @@ public static CoreOperatorType valueOf(int number)\n             case 54: return PARTITION_LIMIT;\n             case 55: return PCAPNG_SUB_SCAN;\n             case 56: return RUNTIME_FILTER;\n+            case 57: return ROWKEY_JOIN;\n             default: return null;\n         }\n     }",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/protocol/src/main/java/org/apache/drill/exec/proto/beans/CoreOperatorType.java",
                "sha": "6138ad614b9f92288b6278ce10232d4d76c705e3",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/protocol/src/main/protobuf/UserBitShared.proto",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/protocol/src/main/protobuf/UserBitShared.proto?ref=0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce",
                "deletions": 1,
                "filename": "protocol/src/main/protobuf/UserBitShared.proto",
                "patch": "@@ -300,7 +300,7 @@ enum CoreOperatorType {\n   ORDERED_PARTITION_SENDER = 9;\n   PROJECT = 10;\n   UNORDERED_RECEIVER = 11;\n-  RANGE_SENDER = 12;\n+  RANGE_PARTITION_SENDER = 12;\n   SCREEN = 13;\n   SELECTION_VECTOR_REMOVER = 14;\n   STREAMING_AGGREGATE = 15;\n@@ -345,6 +345,7 @@ enum CoreOperatorType {\n   PARTITION_LIMIT = 54;\n   PCAPNG_SUB_SCAN = 55;\n   RUNTIME_FILTER = 56;\n+  ROWKEY_JOIN = 57;\n }\n \n /* Registry that contains list of jars, each jar contains its name and list of function signatures.",
                "raw_url": "https://github.com/apache/drill/raw/0abcbe3f36bf6c0a2b5fe07a778d201ead8dd2ce/protocol/src/main/protobuf/UserBitShared.proto",
                "sha": "843c6d8c385b45c247e1a1b02084cd8490a91639",
                "status": "modified"
            }
        ],
        "message": "DRILL-6381: (Part 1) Secondary Index framework\n\n\u00a0 1. Secondary Index planning interfaces and abstract classes like DBGroupScan, DbSubScan, IndexDecriptor etc.\n\u00a0 2. Statistics and Cost model interfaces/classes: PluginCost, Statistics, StatisticsPayload, AbstractIndexStatistics\n\u00a0 3. ScanBatch and RecordReader to support repeatable scan\n\u00a0 4. Secondary Index execution related interfaces: RangePartitionSender, RowKeyJoin, PartitionFunction\n  5. MD-3979: Query using cast index plan fails with NPE\n\nCo-authored-by: Aman Sinha <asinha@maprtech.com>\nCo-authored-by: chunhui-shi <cshi@maprtech.com>\nCo-authored-by: Gautam Parai <gparai@maprtech.com>\nCo-authored-by: Padma Penumarthy <ppenumar97@yahoo.com>\nCo-authored-by: Hanumath Rao Maduri <hmaduri@maprtech.com>\n\nConflicts:\n\texec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/ScanBatch.java\n\texec/java-exec/src/main/java/org/apache/drill/exec/planner/common/DrillRelOptUtil.java\n\texec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillTable.java\n\tprotocol/src/main/java/org/apache/drill/exec/proto/UserBitShared.java\n\tprotocol/src/main/java/org/apache/drill/exec/proto/beans/CoreOperatorType.java\n\tprotocol/src/main/protobuf/UserBitShared.proto",
        "parent": "https://github.com/apache/drill/commit/61e8b464063299dc1f67445157a46c4939b0cace",
        "repo": "drill",
        "unit_tests": [
            "StatisticsTest.java"
        ]
    },
    "drill_125a927": {
        "bug_id": "drill_125a927",
        "commit": "https://github.com/apache/drill/commit/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9",
        "file": [
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/drill/blob/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/README.md",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/README.md?ref=125a9271d7cf0dfb30aac8e62447507ea0a7d6c9",
                "deletions": 0,
                "filename": "contrib/format-maprdb/README.md",
                "patch": "@@ -1,2 +1,8 @@\n drill-mapr-plugin\n =================\n+By default all the tests in contrib/format-maprdb are disabled.\n+To enable and run these tests please use -Pmapr profile to\n+compile and execute the tests.\n+\n+Here is an example of the mvn command to use to run these tests.\n+mvn install -Dtests=cluster -Pmapr",
                "raw_url": "https://github.com/apache/drill/raw/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/README.md",
                "sha": "a94a7cb012bddb432f97bd276621adcf4b292642",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/drill/blob/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBFormatPluginConfig.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBFormatPluginConfig.java?ref=125a9271d7cf0dfb30aac8e62447507ea0a7d6c9",
                "deletions": 0,
                "filename": "contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBFormatPluginConfig.java",
                "patch": "@@ -32,6 +32,12 @@\n   public boolean ignoreSchemaChange = false;\n   public boolean readAllNumbersAsDouble = false;\n   public boolean disableCountOptimization = false;\n+  /* This flag is a switch to do special handling in case of\n+   * no columns in the query exists in the maprdb table. This flag\n+   * can get deprecated once it is observed that this special handling\n+   * is not regressing performance of reading maprdb table.\n+   */\n+  public boolean nonExistentFieldSupport = true;\n \n   @Override\n   public int hashCode() {\n@@ -40,6 +46,7 @@ public int hashCode() {\n     result = 31 * result + (ignoreSchemaChange ? 1231 : 1237);\n     result = 31 * result + (readAllNumbersAsDouble ? 1231 : 1237);\n     result = 31 * result + (disableCountOptimization ? 1231 : 1237);\n+    result = 31 * result + (nonExistentFieldSupport ? 1231 : 1237);\n     return result;\n   }\n \n@@ -56,6 +63,8 @@ protected boolean impEquals(Object obj) {\n       return false;\n     } else if (disableCountOptimization != other.disableCountOptimization) {\n       return false;\n+    } else if (nonExistentFieldSupport != other.nonExistentFieldSupport) {\n+      return false;\n     }\n     return true;\n   }\n@@ -76,6 +85,8 @@ public boolean isEnablePushdown() {\n     return enablePushdown;\n   }\n \n+  public boolean isNonExistentFieldSupport() { return nonExistentFieldSupport; }\n+\n   public boolean isIgnoreSchemaChange() {\n     return ignoreSchemaChange;\n   }",
                "raw_url": "https://github.com/apache/drill/raw/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/MapRDBFormatPluginConfig.java",
                "sha": "ad153fe7fa5e7f44628b084483f50b1258ad4048",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/drill/blob/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/MaprDBJsonRecordReader.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/MaprDBJsonRecordReader.java?ref=125a9271d7cf0dfb30aac8e62447507ea0a7d6c9",
                "deletions": 0,
                "filename": "contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/MaprDBJsonRecordReader.java",
                "patch": "@@ -26,6 +26,7 @@\n import java.util.List;\n import java.util.Set;\n import java.util.Stack;\n+import java.util.Collections;\n import java.util.concurrent.TimeUnit;\n \n import org.apache.drill.common.exceptions.ExecutionSetupException;\n@@ -44,6 +45,7 @@\n import org.apache.drill.exec.util.Utilities;\n import org.apache.drill.exec.vector.BaseValueVector;\n import org.apache.drill.exec.vector.complex.impl.MapOrListWriterImpl;\n+import org.apache.drill.exec.vector.complex.fn.JsonReaderUtils;\n import org.apache.drill.exec.vector.complex.impl.VectorContainerWriter;\n import org.ojai.DocumentReader;\n import org.ojai.DocumentReader.EventType;\n@@ -95,6 +97,7 @@\n   private final boolean allTextMode;\n   private final boolean ignoreSchemaChange;\n   private final boolean disableCountOptimization;\n+  private final boolean nonExistentColumnsProjection;\n \n   public MaprDBJsonRecordReader(MapRDBSubScanSpec subScanSpec,\n       MapRDBFormatPluginConfig formatPluginConfig,\n@@ -119,6 +122,7 @@ public MaprDBJsonRecordReader(MapRDBSubScanSpec subScanSpec,\n     allTextMode = formatPluginConfig.isAllTextMode();\n     ignoreSchemaChange = formatPluginConfig.isIgnoreSchemaChange();\n     disablePushdown = !formatPluginConfig.isEnablePushdown();\n+    nonExistentColumnsProjection = formatPluginConfig.isNonExistentFieldSupport();\n   }\n \n   @Override\n@@ -230,6 +234,9 @@ public int next() {\n       }\n     }\n \n+    if (nonExistentColumnsProjection && recordCount > 0) {\n+      JsonReaderUtils.ensureAtLeastOneField(vectorWriter, getColumns(), allTextMode, Collections.EMPTY_LIST);\n+    }\n     vectorWriter.setValueCount(recordCount);\n     logger.debug(\"Took {} ms to get {} records\", watch.elapsed(TimeUnit.MILLISECONDS), recordCount);\n     return recordCount;",
                "raw_url": "https://github.com/apache/drill/raw/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/src/main/java/org/apache/drill/exec/store/mapr/db/json/MaprDBJsonRecordReader.java",
                "sha": "113b3adb7d98b56762feffa397cfadbbf2c8b0f6",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/drill/blob/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/src/test/java/com/mapr/drill/maprdb/tests/json/TestSimpleJson.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/format-maprdb/src/test/java/com/mapr/drill/maprdb/tests/json/TestSimpleJson.java?ref=125a9271d7cf0dfb30aac8e62447507ea0a7d6c9",
                "deletions": 0,
                "filename": "contrib/format-maprdb/src/test/java/com/mapr/drill/maprdb/tests/json/TestSimpleJson.java",
                "patch": "@@ -57,6 +57,16 @@ public void testSelectId() throws Exception {\n     runSQLAndVerifyCount(sql, 10);\n   }\n \n+  @Test\n+  public void testSelectNonExistentColumns() throws Exception {\n+    setColumnWidths(new int[] {23});\n+    final String sql = \"SELECT\\n\"\n+            + \"  something\\n\"\n+            + \"FROM\\n\"\n+            + \"  hbase.business business limit 5\";\n+    runSQLAndVerifyCount(sql, 5);\n+  }\n+\n   @Test\n   public void testKVGen() throws Exception {\n     setColumnWidths(new int[] {21, 10, 6});",
                "raw_url": "https://github.com/apache/drill/raw/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/contrib/format-maprdb/src/test/java/com/mapr/drill/maprdb/tests/json/TestSimpleJson.java",
                "sha": "26f54b837a1ff1e0ba1a4412aa9ee2567976e08c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReader.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReader.java?ref=125a9271d7cf0dfb30aac8e62447507ea0a7d6c9",
                "deletions": 65,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReader.java",
                "patch": "@@ -104,71 +104,7 @@ public JsonReader(DrillBuf managedBuf, List<SchemaPath> columns,\n   @SuppressWarnings(\"resource\")\n   @Override\n   public void ensureAtLeastOneField(ComplexWriter writer) {\n-    List<BaseWriter.MapWriter> writerList = Lists.newArrayList();\n-    List<PathSegment> fieldPathList = Lists.newArrayList();\n-    BitSet emptyStatus = new BitSet(columns.size());\n-\n-    // first pass: collect which fields are empty\n-    for (int i = 0; i < columns.size(); i++) {\n-      SchemaPath sp = columns.get(i);\n-      PathSegment fieldPath = sp.getRootSegment();\n-      BaseWriter.MapWriter fieldWriter = writer.rootAsMap();\n-      while (fieldPath.getChild() != null && !fieldPath.getChild().isArray()) {\n-        fieldWriter = fieldWriter.map(fieldPath.getNameSegment().getPath());\n-        fieldPath = fieldPath.getChild();\n-      }\n-      writerList.add(fieldWriter);\n-      fieldPathList.add(fieldPath);\n-      if (fieldWriter.isEmptyMap()) {\n-        emptyStatus.set(i, true);\n-      }\n-      if (i == 0 && !allTextMode) {\n-        // when allTextMode is false, there is not much benefit to producing all\n-        // the empty\n-        // fields; just produce 1 field. The reason is that the type of the\n-        // fields is\n-        // unknown, so if we produce multiple Integer fields by default, a\n-        // subsequent batch\n-        // that contains non-integer fields will error out in any case. Whereas,\n-        // with\n-        // allTextMode true, we are sure that all fields are going to be treated\n-        // as varchar,\n-        // so it makes sense to produce all the fields, and in fact is necessary\n-        // in order to\n-        // avoid schema change exceptions by downstream operators.\n-        break;\n-      }\n-\n-    }\n-\n-    // second pass: create default typed vectors corresponding to empty fields\n-    // Note: this is not easily do-able in 1 pass because the same fieldWriter\n-    // may be\n-    // shared by multiple fields whereas we want to keep track of all fields\n-    // independently,\n-    // so we rely on the emptyStatus.\n-    for (int j = 0; j < fieldPathList.size(); j++) {\n-      BaseWriter.MapWriter fieldWriter = writerList.get(j);\n-      PathSegment fieldPath = fieldPathList.get(j);\n-      if (emptyStatus.get(j)) {\n-        if (allTextMode) {\n-          fieldWriter.varChar(fieldPath.getNameSegment().getPath());\n-        } else {\n-          fieldWriter.integer(fieldPath.getNameSegment().getPath());\n-        }\n-      }\n-    }\n-\n-    for (ListWriter field : emptyArrayWriters) {\n-      // checks that array has not been initialized\n-      if (field.getValueCapacity() == 0) {\n-        if (allTextMode) {\n-          field.varChar();\n-        } else {\n-          field.integer();\n-        }\n-      }\n-    }\n+    JsonReaderUtils.ensureAtLeastOneField(writer, columns, allTextMode, emptyArrayWriters);\n   }\n \n   public void setSource(int start, int end, DrillBuf buf) throws IOException {",
                "raw_url": "https://github.com/apache/drill/raw/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReader.java",
                "sha": "4ffbb26e01eb31852334203ef61bca5af592f18f",
                "status": "modified"
            },
            {
                "additions": 94,
                "blob_url": "https://github.com/apache/drill/blob/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReaderUtils.java",
                "changes": 94,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReaderUtils.java?ref=125a9271d7cf0dfb30aac8e62447507ea0a7d6c9",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReaderUtils.java",
                "patch": "@@ -0,0 +1,94 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.vector.complex.fn;\n+\n+import com.google.common.collect.Lists;\n+import org.apache.drill.common.expression.PathSegment;\n+import org.apache.drill.common.expression.SchemaPath;\n+import org.apache.drill.exec.vector.complex.writer.BaseWriter;\n+\n+import java.util.BitSet;\n+import java.util.Collection;\n+import java.util.List;\n+\n+public class JsonReaderUtils {\n+\n+  public static void ensureAtLeastOneField(BaseWriter.ComplexWriter writer,\n+                                    Collection<SchemaPath> columns,\n+                                    boolean allTextMode,\n+                                    List<BaseWriter.ListWriter> emptyArrayWriters) {\n+\n+    List<BaseWriter.MapWriter> writerList = Lists.newArrayList();\n+    List<PathSegment> fieldPathList = Lists.newArrayList();\n+    BitSet emptyStatus = new BitSet(columns.size());\n+    int i = 0;\n+\n+    // first pass: collect which fields are empty\n+    for (SchemaPath sp : columns) {\n+      PathSegment fieldPath = sp.getRootSegment();\n+      BaseWriter.MapWriter fieldWriter = writer.rootAsMap();\n+      while (fieldPath.getChild() != null && !fieldPath.getChild().isArray()) {\n+        fieldWriter = fieldWriter.map(fieldPath.getNameSegment().getPath());\n+        fieldPath = fieldPath.getChild();\n+      }\n+      writerList.add(fieldWriter);\n+      fieldPathList.add(fieldPath);\n+      if (fieldWriter.isEmptyMap()) {\n+        emptyStatus.set(i, true);\n+      }\n+      if (i == 0 && !allTextMode) {\n+        // when allTextMode is false, there is not much benefit to producing all\n+        // the empty fields; just produce 1 field. The reason is that the type of the\n+        // fields is unknown, so if we produce multiple Integer fields by default, a\n+        // subsequent batch that contains non-integer fields will error out in any case.\n+        // Whereas, with allTextMode true, we are sure that all fields are going to be\n+        // treated as varchar, so it makes sense to produce all the fields, and in fact\n+        // is necessary in order to avoid schema change exceptions by downstream operators.\n+        break;\n+      }\n+      i++;\n+    }\n+\n+    // second pass: create default typed vectors corresponding to empty fields\n+    // Note: this is not easily do-able in 1 pass because the same fieldWriter\n+    // may be shared by multiple fields whereas we want to keep track of all fields\n+    // independently, so we rely on the emptyStatus.\n+    for (int j = 0; j < fieldPathList.size(); j++) {\n+      BaseWriter.MapWriter fieldWriter = writerList.get(j);\n+      PathSegment fieldPath = fieldPathList.get(j);\n+      if (emptyStatus.get(j)) {\n+        if (allTextMode) {\n+          fieldWriter.varChar(fieldPath.getNameSegment().getPath());\n+        } else {\n+          fieldWriter.integer(fieldPath.getNameSegment().getPath());\n+        }\n+      }\n+    }\n+\n+    for (BaseWriter.ListWriter field : emptyArrayWriters) {\n+      // checks that array has not been initialized\n+      if (field.getValueCapacity() == 0) {\n+        if (allTextMode) {\n+          field.varChar();\n+        } else {\n+          field.integer();\n+        }\n+      }\n+    }\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/drill/raw/125a9271d7cf0dfb30aac8e62447507ea0a7d6c9/exec/java-exec/src/main/java/org/apache/drill/exec/vector/complex/fn/JsonReaderUtils.java",
                "sha": "775be023783401a8367debd885eda277fe3fa78e",
                "status": "added"
            }
        ],
        "message": "DRILL-5864: Selecting a non-existing field from a MapR-DB JSON table fails with NPE.",
        "parent": "https://github.com/apache/drill/commit/4a718a0bd728ae02b502ac93620d132f0f6e1b6c",
        "repo": "drill",
        "unit_tests": [
            "TestJsonReader.java"
        ]
    },
    "drill_163219c": {
        "bug_id": "drill_163219c",
        "commit": "https://github.com/apache/drill/commit/163219c2a802481cbd90171912540250d4059ea8",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/PageReadStatus.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/PageReadStatus.java?ref=163219c2a802481cbd90171912540250d4059ea8",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/PageReadStatus.java",
                "patch": "@@ -110,6 +110,7 @@\n   public boolean next() throws IOException {\n \n     currentPage = null;\n+    valuesRead = 0;\n \n     // TODO - the metatdata for total size appears to be incorrect for impala generated files, need to find cause\n     // and submit a bug report\n@@ -162,7 +163,6 @@ public boolean next() throws IOException {\n     pageDataByteArray = currentPage.getBytes().toByteArray();\n \n     readPosInBytes = 0;\n-    valuesRead = 0;\n     if (parentColumnReader.columnDescriptor.getMaxDefinitionLevel() != 0){\n       parentColumnReader.currDefLevel = -1;\n       if (!currentPage.getValueEncoding().usesDictionary()) {",
                "raw_url": "https://github.com/apache/drill/raw/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/PageReadStatus.java",
                "sha": "3ad1d6c793378cb98a90e0d7f2eb937d931d29a4",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordReader.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordReader.java?ref=163219c2a802481cbd90171912540250d4059ea8",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordReader.java",
                "patch": "@@ -228,7 +228,7 @@ public void setup(OutputMutator output) throws ExecutionSetupException {\n \n     // none of the columns in the parquet file matched the request columns from the query\n     if (columnsToScan == 0){\n-      return;\n+      throw new ExecutionSetupException(\"Error reading from parquet file. No columns requested were found in the file.\");\n     }\n     if (allFieldsFixedLength) {\n       recordsPerBatch = (int) Math.min(Math.min(batchSize / bitWidthAllFixedFields,",
                "raw_url": "https://github.com/apache/drill/raw/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRecordReader.java",
                "sha": "4c5f4bba70b1aed62d4f8085fa7fe271f2274731",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/drill/blob/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetRecordReaderTest.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetRecordReaderTest.java?ref=163219c2a802481cbd90171912540250d4059ea8",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetRecordReaderTest.java",
                "patch": "@@ -352,6 +352,15 @@ public void testMultipleRowGroupsAndReads() throws Exception {\n         \"/tmp/test.parquet\", i, props);\n   }\n \n+  @Test\n+  public void testReadError_Drill_901() throws Exception {\n+    // select cast( L_COMMENT as varchar) from  dfs.`/tmp/drilltest/employee_parquet`\n+    HashMap<String, FieldInfo> fields = new HashMap<>();\n+    ParquetTestProperties props = new ParquetTestProperties(1, 120350, DEFAULT_BYTES_PER_PAGE, fields);\n+    testParquetFullEngineEventBased(false, false, \"/parquet/par_writer_test.json\", null,\n+        \"unused, no file is generated\", 1, props, false);\n+  }\n+\n \n   @Ignore\n   @Test",
                "raw_url": "https://github.com/apache/drill/raw/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/ParquetRecordReaderTest.java",
                "sha": "ad63dc96bf4f19d6df502d9807b4094fe71cf8ec",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/drill/blob/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/test/resources/parquet/par_writer_test.json",
                "changes": 26,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/resources/parquet/par_writer_test.json?ref=163219c2a802481cbd90171912540250d4059ea8",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/resources/parquet/par_writer_test.json",
                "patch": "@@ -0,0 +1,26 @@\n+  {\n+    head : {\n+      version : 1,\n+          generator : {\n+        type : \"manual\",\n+            info : \"na\"\n+      },\n+      type : \"APACHE_DRILL_PHYSICAL\"\n+    },\n+    graph : [ {\n+    pop : \"parquet-scan\",\n+    @id : 1,\n+        entries : [ {\n+      path : \"/tpch/lineitem.parquet\"\n+    } ],\n+    storage : {\n+      type : \"file\",\n+      connection : \"classpath:///\"\n+    },\n+    columns: [ \"L_COMMENT\"]\n+  }, {\n+    pop : \"screen\",\n+    @id : 2,\n+        child : 1\n+  } ]\n+  }",
                "raw_url": "https://github.com/apache/drill/raw/163219c2a802481cbd90171912540250d4059ea8/exec/java-exec/src/test/resources/parquet/par_writer_test.json",
                "sha": "34f2ba6e7b147b578b3e7742ffd8f90588c6d58b",
                "status": "added"
            }
        ],
        "message": "DRILL-901: Fix Parquet read bug with VarBinary.\n\nAlso now throw an exception if parquet reader is not passed any columns found in the file. Previously a NPE was thrown as the setup method exited early, skipping an object initialization that manifested in the first call to the next method.",
        "parent": "https://github.com/apache/drill/commit/393adee7e441cb5b03b4489e1497282c68ffbf52",
        "repo": "drill",
        "unit_tests": [
            "ParquetRecordReaderTest.java"
        ]
    },
    "drill_4ccea48": {
        "bug_id": "drill_4ccea48",
        "commit": "https://github.com/apache/drill/commit/4ccea48905cca4a822521c83ad2ff31eb62bfa06",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/drill/blob/4ccea48905cca4a822521c83ad2ff31eb62bfa06/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java?ref=4ccea48905cca4a822521c83ad2ff31eb62bfa06",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "patch": "@@ -341,7 +341,11 @@ private void scanAndAssign (Multimap<Integer, ParquetRowGroupScan.RowGroupReadEn\n \n           endpointAssignments.put(minorFragmentId, rowGroupInfo.getRowGroupReadEntry());\n           logger.debug(\"Assigned rowGroup {} to minorFragmentId {} endpoint {}\", rowGroupInfo.getRowGroupIndex(), minorFragmentId, endpoints.get(minorFragmentId).getAddress());\n-          assignmentAffinityStats.update(bytesPerEndpoint.get(currentEndpoint) / rowGroupInfo.getLength());\n+          if (bytesPerEndpoint.get(currentEndpoint) != null) {\n+            assignmentAffinityStats.update(bytesPerEndpoint.get(currentEndpoint) / rowGroupInfo.getLength());\n+          } else {\n+            assignmentAffinityStats.update(0);\n+          }\n           iter.remove();\n           fragmentPointer = (minorFragmentId + 1) % endpoints.size();\n           break;",
                "raw_url": "https://github.com/apache/drill/raw/4ccea48905cca4a822521c83ad2ff31eb62bfa06/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "sha": "10fcdffff75976c13aaf4a28204d1f84b48d1df4",
                "status": "modified"
            }
        ],
        "message": "DRILL-244: NPE when updating assignment affinity metric",
        "parent": "https://github.com/apache/drill/commit/c93a5a1dc396fc8378da77b62a9e61314fbc98b4",
        "repo": "drill",
        "unit_tests": [
            "TestParquetGroupScan.java"
        ]
    },
    "drill_71608ca": {
        "bug_id": "drill_71608ca",
        "commit": "https://github.com/apache/drill/commit/71608ca9fb53ff0af4f1d09f32d61e7280377e7a",
        "file": [
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/drill/blob/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java?ref=71608ca9fb53ff0af4f1d09f32d61e7280377e7a",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java",
                "patch": "@@ -150,7 +150,11 @@ public boolean apply(@Nullable FileStatus status) {\n     logger.debug(\"FileSelection.minusDirectories() took {} ms, numFiles: {}\",\n         timer.elapsed(TimeUnit.MILLISECONDS), total);\n \n-    fileSel.setExpanded();\n+    // fileSel will be null if we query an empty folder\n+    if (fileSel != null) {\n+      fileSel.setExpanded();\n+    }\n+\n     return fileSel;\n   }\n ",
                "raw_url": "https://github.com/apache/drill/raw/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java",
                "sha": "5b4813ab6c4e9b852237a5552522de064f93c012",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/drill/blob/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java?ref=71608ca9fb53ff0af4f1d09f32d61e7280377e7a",
                "deletions": 2,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "patch": "@@ -27,6 +27,7 @@\n import java.util.Set;\n \n import org.apache.drill.common.exceptions.ExecutionSetupException;\n+import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.common.expression.SchemaPath;\n import org.apache.drill.common.logical.FormatPluginConfig;\n import org.apache.drill.common.logical.StoragePluginConfig;\n@@ -568,6 +569,7 @@ public long getRowCount() {\n    * @return file selection read from cache\n    *\n    * @throws IOException\n+   * @throws UserException when the updated selection is empty, this happens if the user selects an empty folder.\n    */\n   private FileSelection\n   initFromMetadataCache(FileSelection selection, Path metaFilePath) throws IOException {\n@@ -580,7 +582,8 @@ public long getRowCount() {\n     List<String> fileNames = Lists.newArrayList();\n     List<FileStatus> fileStatuses = selection.getStatuses(fs);\n \n-    if (fileStatuses.size() == 1 && fileStatuses.get(0).isDirectory()) {\n+    final Path first = fileStatuses.get(0).getPath();\n+    if (fileStatuses.size() == 1 && selection.getSelectionRoot().equals(first.toString())) {\n       // we are selecting all files from selection root. Expand the file list from the cache\n       for (Metadata.ParquetFileMetadata file : parquetTableMetadata.getFiles()) {\n         fileNames.add(file.getPath());\n@@ -590,7 +593,7 @@ public long getRowCount() {\n       // we need to expand the files from fileStatuses\n       for (FileStatus status : fileStatuses) {\n         if (status.isDirectory()) {\n-          //TODO read the metadata cache files in parallel\n+          //TODO [DRILL-4496] read the metadata cache files in parallel\n           final Path metaPath = new Path(status.getPath(), Metadata.METADATA_FILENAME);\n           final Metadata.ParquetTableMetadataBase metadata = Metadata.readBlockMeta(fs, metaPath.toString());\n           for (Metadata.ParquetFileMetadata file : metadata.getFiles()) {\n@@ -606,6 +609,11 @@ public long getRowCount() {\n       fileSet = Sets.newHashSet(fileNames);\n     }\n \n+    if (fileNames.isEmpty()) {\n+      // no files were found, most likely we tried to query some empty sub folders\n+      throw UserException.validationError().message(\"The table you tried to query is empty\").build(logger);\n+    }\n+\n     // when creating the file selection, set the selection root in the form /a/b instead of\n     // file:/a/b.  The reason is that the file names above have been created in the form\n     // /a/b/c.parquet and the format of the selection root must match that of the file names",
                "raw_url": "https://github.com/apache/drill/raw/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "sha": "47172cc8a59ace5f016a4f9d451cd2f41784d1e9",
                "status": "modified"
            },
            {
                "additions": 120,
                "blob_url": "https://github.com/apache/drill/blob/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetGroupScan.java",
                "changes": 120,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetGroupScan.java?ref=71608ca9fb53ff0af4f1d09f32d61e7280377e7a",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetGroupScan.java",
                "patch": "@@ -0,0 +1,120 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.store.parquet;\n+\n+import org.apache.drill.BaseTestQuery;\n+import org.apache.drill.common.exceptions.UserRemoteException;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+\n+public class TestParquetGroupScan extends BaseTestQuery {\n+\n+  private void prepareTables(final String tableName, boolean refreshMetadata) throws Exception {\n+    // first create some parquet subfolders\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s`      AS SELECT employee_id FROM cp.`employee.json` LIMIT 1\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/501`  AS SELECT employee_id FROM cp.`employee.json` LIMIT 2\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/502`  AS SELECT employee_id FROM cp.`employee.json` LIMIT 4\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/503`  AS SELECT employee_id FROM cp.`employee.json` LIMIT 8\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/504`  AS SELECT employee_id FROM cp.`employee.json` LIMIT 16\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/505`  AS SELECT employee_id FROM cp.`employee.json` LIMIT 32\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/60`   AS SELECT employee_id FROM cp.`employee.json` LIMIT 64\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/602`  AS SELECT employee_id FROM cp.`employee.json` LIMIT 128\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/6031` AS SELECT employee_id FROM cp.`employee.json` LIMIT 256\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/6032` AS SELECT employee_id FROM cp.`employee.json` LIMIT 512\", tableName);\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/6033` AS SELECT employee_id FROM cp.`employee.json` LIMIT 1024\", tableName);\n+\n+    // we need an empty subfolder `4376/20160401`\n+    // to do this we first create a table inside that subfolder\n+    testNoResult(\"CREATE TABLE dfs_test.tmp.`%s/6041/a` AS SELECT * FROM cp.`employee.json` LIMIT 1\", tableName);\n+    // then we delete the table, leaving the parent subfolder empty\n+    testNoResult(\"DROP TABLE   dfs_test.tmp.`%s/6041/a`\", tableName);\n+\n+    if (refreshMetadata) {\n+      // build the metadata cache file\n+      testNoResult(\"REFRESH TABLE METADATA dfs_test.tmp.`%s`\", tableName);\n+    }\n+  }\n+\n+  @Test\n+  public void testFix4376() throws Exception {\n+    prepareTables(\"4376_1\", true);\n+\n+    testBuilder()\n+      .sqlQuery(\"SELECT COUNT(*) AS `count` FROM dfs_test.tmp.`4376_1/60*`\")\n+      .ordered()\n+      .baselineColumns(\"count\").baselineValues(1984L)\n+      .go();\n+  }\n+\n+  @Test\n+  public void testWildCardEmptyWithCache() throws Exception {\n+    prepareTables(\"4376_2\", true);\n+\n+    try {\n+      runSQL(\"SELECT COUNT(*) AS `count` FROM dfs_test.tmp.`4376_2/604*`\");\n+      fail(\"Query should've failed!\");\n+    } catch (UserRemoteException uex) {\n+      final String expectedMsg = \"The table you tried to query is empty\";\n+      assertTrue(String.format(\"Error message should contain \\\"%s\\\" but was instead \\\"%s\\\"\", expectedMsg,\n+        uex.getMessage()), uex.getMessage().contains(expectedMsg));\n+    }\n+  }\n+\n+  @Test\n+  public void testWildCardEmptyNoCache() throws Exception {\n+    prepareTables(\"4376_3\", false);\n+\n+    try {\n+      runSQL(\"SELECT COUNT(*) AS `count` FROM dfs_test.tmp.`4376_3/604*`\");\n+      fail(\"Query should've failed!\");\n+    } catch (UserRemoteException uex) {\n+      final String expectedMsg = \"Table 'dfs_test.tmp.4376_3/604*' not found\";\n+      assertTrue(String.format(\"Error message should contain \\\"%s\\\" but was instead \\\"%s\\\"\", expectedMsg,\n+        uex.getMessage()), uex.getMessage().contains(expectedMsg));\n+    }\n+  }\n+\n+  @Test\n+  public void testSelectEmptyWithCache() throws Exception {\n+    prepareTables(\"4376_4\", true);\n+\n+    try {\n+      runSQL(\"SELECT COUNT(*) AS `count` FROM dfs_test.tmp.`4376_4/6041`\");\n+      fail(\"Query should've failed!\");\n+    } catch (UserRemoteException uex) {\n+      final String expectedMsg = \"The table you tried to query is empty\";\n+      assertTrue(String.format(\"Error message should contain \\\"%s\\\" but was instead \\\"%s\\\"\", expectedMsg,\n+        uex.getMessage()), uex.getMessage().contains(expectedMsg));\n+    }\n+  }\n+\n+  @Test\n+  public void testSelectEmptyNoCache() throws Exception {\n+    prepareTables(\"4376_5\", false);\n+    try {\n+      runSQL(\"SELECT COUNT(*) AS `count` FROM dfs_test.tmp.`4376_5/6041`\");\n+      fail(\"Query should've failed!\");\n+    } catch (UserRemoteException uex) {\n+      final String expectedMsg = \"Table 'dfs_test.tmp.4376_5/6041' not found\";\n+      assertTrue(String.format(\"Error message should contain \\\"%s\\\" but was instead \\\"%s\\\"\", expectedMsg,\n+        uex.getMessage()), uex.getMessage().contains(expectedMsg));\n+    }\n+  }\n+}",
                "raw_url": "https://github.com/apache/drill/raw/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetGroupScan.java",
                "sha": "21d62858f7a901a80fb4fd12266198f2559f0d83",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/drill/blob/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetMetadataCache.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetMetadataCache.java?ref=71608ca9fb53ff0af4f1d09f32d61e7280377e7a",
                "deletions": 19,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetMetadataCache.java",
                "patch": "@@ -177,25 +177,6 @@ public void testFix4449() throws Exception {\n       .go();\n   }\n \n-  @Test\n-  public void testFix4376() throws Exception {\n-    // first create some parquet subfolders\n-    runSQL(\"CREATE TABLE dfs_test.tmp.`4376`    AS SELECT * FROM cp.`employee.json` LIMIT 1\");\n-    runSQL(\"CREATE TABLE dfs_test.tmp.`4376/01` AS SELECT * FROM cp.`employee.json` LIMIT 2\");\n-    runSQL(\"CREATE TABLE dfs_test.tmp.`4376/02` AS SELECT * FROM cp.`employee.json` LIMIT 4\");\n-    runSQL(\"CREATE TABLE dfs_test.tmp.`4376/0`  AS SELECT * FROM cp.`employee.json` LIMIT 8\");\n-    runSQL(\"CREATE TABLE dfs_test.tmp.`4376/11` AS SELECT * FROM cp.`employee.json` LIMIT 16\");\n-    runSQL(\"CREATE TABLE dfs_test.tmp.`4376/12` AS SELECT * FROM cp.`employee.json` LIMIT 32\");\n-    // next, build the metadata cache file\n-    runSQL(\"REFRESH TABLE METADATA dfs_test.tmp.`4376`\");\n-\n-    testBuilder()\n-      .sqlQuery(\"SELECT COUNT(*) AS `count` FROM dfs_test.tmp.`4376/0*`\")\n-      .ordered()\n-      .baselineColumns(\"count\").baselineValues(15L)\n-      .go();\n-    }\n-\n   private void checkForMetadataFile(String table) throws Exception {\n     String tmpDir = getDfsTestTmpSchemaLocation();\n     String metaFile = Joiner.on(\"/\").join(tmpDir, table, Metadata.METADATA_FILENAME);",
                "raw_url": "https://github.com/apache/drill/raw/71608ca9fb53ff0af4f1d09f32d61e7280377e7a/exec/java-exec/src/test/java/org/apache/drill/exec/store/parquet/TestParquetMetadataCache.java",
                "sha": "4330c96471e1324572bac6283ef7f0a40b38eb37",
                "status": "modified"
            }
        ],
        "message": "DRILL-4484: NPE when querying  empty directory",
        "parent": "https://github.com/apache/drill/commit/11fe8d7cdb1df4100cd48bcce1de0b2c3c5f983a",
        "repo": "drill",
        "unit_tests": [
            "TestFileSelection.java",
            "TestParquetGroupScan.java"
        ]
    },
    "drill_7506cfb": {
        "bug_id": "drill_7506cfb",
        "commit": "https://github.com/apache/drill/commit/7506cfbb5c8522d371c12dbdc2268d48a9449a48",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroFormatPlugin.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroFormatPlugin.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 9,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroFormatPlugin.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -37,7 +37,6 @@\n import org.apache.drill.exec.store.dfs.FormatMatcher;\n import org.apache.drill.exec.store.dfs.FormatSelection;\n import org.apache.drill.exec.store.dfs.MagicString;\n-import org.apache.drill.exec.store.dfs.NamedFormatPluginConfig;\n import org.apache.drill.exec.store.dfs.easy.EasyFormatPlugin;\n import org.apache.drill.exec.store.dfs.easy.EasyWriter;\n import org.apache.drill.exec.store.dfs.easy.FileWork;\n@@ -105,13 +104,7 @@ public DrillTable isReadable(DrillFileSystem fs,\n         FileSelection selection, FileSystemPlugin fsPlugin,\n         String storageEngineName, String userName) throws IOException {\n       if (isFileReadable(fs, selection.getFirstPath(fs))) {\n-        if (plugin.getName() != null) {\n-          NamedFormatPluginConfig namedConfig = new NamedFormatPluginConfig();\n-          namedConfig.name = plugin.getName();\n-          return new AvroDrillTable(storageEngineName, fsPlugin, userName, new FormatSelection(namedConfig, selection));\n-        } else {\n-          return new AvroDrillTable(storageEngineName, fsPlugin, userName, new FormatSelection(plugin.getConfig(), selection));\n-        }\n+        return new AvroDrillTable(storageEngineName, fsPlugin, userName, new FormatSelection(plugin.getConfig(), selection));\n       }\n       return null;\n     }",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/avro/AvroFormatPlugin.java",
                "sha": "fd6e59b5f45e1b5c2562134e79a2b03edb137c68",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/BasicFormatMatcher.java",
                "changes": 8,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/BasicFormatMatcher.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 7,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/BasicFormatMatcher.java",
                "patch": "@@ -77,13 +77,7 @@ public DrillTable isReadable(DrillFileSystem fs,\n       FileSelection selection, FileSystemPlugin fsPlugin,\n       String storageEngineName, String userName) throws IOException {\n     if (isFileReadable(fs, selection.getFirstPath(fs))) {\n-      if (plugin.getName() != null) {\n-        NamedFormatPluginConfig namedConfig = new NamedFormatPluginConfig();\n-        namedConfig.name = plugin.getName();\n-        return new DynamicDrillTable(fsPlugin, storageEngineName, userName, new FormatSelection(namedConfig, selection));\n-      } else {\n-        return new DynamicDrillTable(fsPlugin, storageEngineName, userName, new FormatSelection(plugin.getConfig(), selection));\n-      }\n+      return new DynamicDrillTable(fsPlugin, storageEngineName, userName, new FormatSelection(plugin.getConfig(), selection));\n     }\n     return null;\n   }",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/BasicFormatMatcher.java",
                "sha": "65260739cac90ac91640c767e667de2ac3ca6f18",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 3,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java",
                "patch": "@@ -37,7 +37,6 @@\n  */\n public class FileSelection {\n   private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(FileSelection.class);\n-  private static final String PATH_SEPARATOR = System.getProperty(\"file.separator\");\n   private static final String WILD_CARD = \"*\";\n \n   private List<FileStatus> statuses;\n@@ -224,7 +223,7 @@ private static String commonPathForFiles(final List<String> files) {\n     int shortest = Integer.MAX_VALUE;\n     for (int i = 0; i < total; i++) {\n       final Path path = new Path(files.get(i));\n-      folders[i] = Path.getPathWithoutSchemeAndAuthority(path).toString().split(PATH_SEPARATOR);\n+      folders[i] = Path.getPathWithoutSchemeAndAuthority(path).toString().split(Path.SEPARATOR);\n       shortest = Math.min(shortest, folders[i].length);\n     }\n \n@@ -247,7 +246,7 @@ private static String commonPathForFiles(final List<String> files) {\n   private static String buildPath(final String[] path, final int folderIndex) {\n     final StringBuilder builder = new StringBuilder();\n     for (int i=0; i<folderIndex; i++) {\n-      builder.append(path[i]).append(PATH_SEPARATOR);\n+      builder.append(path[i]).append(Path.SEPARATOR);\n     }\n     builder.deleteCharAt(builder.length()-1);\n     return builder.toString();",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSelection.java",
                "sha": "6aff1dd019f9734cfe91f7b27fe372bd58cdbbda",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSystemPlugin.java",
                "changes": 27,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSystemPlugin.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 12,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSystemPlugin.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -133,15 +133,7 @@ public StoragePluginConfig getConfig() {\n   public AbstractGroupScan getPhysicalScan(String userName, JSONOptions selection, List<SchemaPath> columns)\n       throws IOException {\n     FormatSelection formatSelection = selection.getWith(lpPersistance, FormatSelection.class);\n-    FormatPlugin plugin;\n-    if (formatSelection.getFormat() instanceof NamedFormatPluginConfig) {\n-      plugin = formatCreator.getFormatPluginByName( ((NamedFormatPluginConfig) formatSelection.getFormat()).name);\n-    } else {\n-      plugin = formatPluginsByConfig.get(formatSelection.getFormat());\n-    }\n-    if (plugin == null) {\n-      plugin = formatCreator.newFormatPlugin(formatSelection.getFormat());\n-    }\n+    FormatPlugin plugin = getFormatPlugin(formatSelection.getFormat());\n     return plugin.getGroupScan(userName, formatSelection.getSelection(), columns);\n   }\n \n@@ -154,12 +146,23 @@ public FormatPlugin getFormatPlugin(String name) {\n     return formatCreator.getFormatPluginByName(name);\n   }\n \n+  /**\n+   * If format plugin configuration is for named format plugin, will return format plugin from pre-loaded list by name.\n+   * For other cases will try to find format plugin by its configuration, if not present will attempt to create one.\n+   *\n+   * @param config format plugin configuration\n+   * @return format plugin for given configuration if found, null otherwise\n+   */\n   public FormatPlugin getFormatPlugin(FormatPluginConfig config) {\n     if (config instanceof NamedFormatPluginConfig) {\n       return formatCreator.getFormatPluginByName(((NamedFormatPluginConfig) config).name);\n-    } else {\n-      return formatPluginsByConfig.get(config);\n     }\n+\n+    FormatPlugin plugin = formatPluginsByConfig.get(config);\n+    if (plugin == null) {\n+      plugin = formatCreator.newFormatPlugin(config);\n+    }\n+    return plugin;\n   }\n \n   @Override",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/FileSystemPlugin.java",
                "sha": "5d382fe376fdc410c536ef994c5bdd33d29df87a",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/WorkspaceSchemaFactory.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/WorkspaceSchemaFactory.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/WorkspaceSchemaFactory.java",
                "patch": "@@ -251,7 +251,14 @@ public boolean isOptional() {\n \n     @Override\n     public TranslatableTable apply(List<Object> arguments) {\n-      return new DrillTranslatableTable(schema.getDrillTable(new TableInstance(sig, arguments)));\n+      DrillTable drillTable = schema.getDrillTable(new TableInstance(sig, arguments));\n+      if (drillTable == null) {\n+        throw UserException\n+            .validationError()\n+            .message(\"Unable to find table [%s] in schema [%s]\", sig.name, schema.getFullSchemaName())\n+            .build(logger);\n+      }\n+      return new DrillTranslatableTable(drillTable);\n     }\n \n   }",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/WorkspaceSchemaFactory.java",
                "sha": "6629fc4e2ec8a1f7a12edaa02b9e40ac13af3372",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/easy/EasySubScan.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/easy/EasySubScan.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 9,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/easy/EasySubScan.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -26,7 +26,6 @@\n import org.apache.drill.common.logical.StoragePluginConfig;\n import org.apache.drill.exec.physical.base.AbstractSubScan;\n import org.apache.drill.exec.store.StoragePluginRegistry;\n-import org.apache.drill.exec.store.dfs.NamedFormatPluginConfig;\n import org.apache.drill.exec.store.schedule.CompleteFileWork.FileWorkImpl;\n \n import com.fasterxml.jackson.annotation.JacksonInject;\n@@ -94,13 +93,7 @@ public StoragePluginConfig getStorageConfig(){\n \n   @JsonProperty(\"format\")\n   public FormatPluginConfig getFormatConfig(){\n-    if (formatPlugin.getName() != null) {\n-      NamedFormatPluginConfig namedConfig = new NamedFormatPluginConfig();\n-      namedConfig.name = formatPlugin.getName();\n-      return namedConfig;\n-    } else {\n-      return formatPlugin.getConfig();\n-    }\n+    return formatPlugin.getConfig();\n   }\n \n   @JsonProperty(\"columns\")",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/dfs/easy/EasySubScan.java",
                "sha": "a6af1ac84894c81a74dfec9a92031864932ca228",
                "status": "modified"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java",
                "changes": 55,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 13,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java",
                "patch": "@@ -1,21 +1,26 @@\n-\n-/**\n- * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE\n- * file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the\n- * License. You may obtain a copy of the License at\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n  *\n  * http://www.apache.org/licenses/LICENSE-2.0\n  *\n- * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n- * an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n- * specific language governing permissions and limitations under the License.\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n  */\n package org.apache.drill.exec.store.httpd;\n \n import java.io.IOException;\n import java.util.List;\n \n+import com.fasterxml.jackson.annotation.JsonInclude;\n import nl.basjes.parse.core.exceptions.DissectionFailure;\n import nl.basjes.parse.core.exceptions.InvalidDissectorException;\n import nl.basjes.parse.core.exceptions.MissingDissectorsException;\n@@ -73,11 +78,11 @@ public HttpdLogFormatPlugin(final String name, final DrillbitContext context, fi\n    * This class is a POJO to hold the configuration for the HttpdLogFormat Parser. This is automatically\n    * serialized/deserialized from JSON format.\n    */\n-  @JsonTypeName(PLUGIN_EXTENSION)\n+  @JsonTypeName(PLUGIN_EXTENSION) @JsonInclude(JsonInclude.Include.NON_DEFAULT)\n   public static class HttpdLogFormatConfig implements FormatPluginConfig {\n \n-    private String logFormat;\n-    private String timestampFormat;\n+    public String logFormat;\n+    public String timestampFormat;\n \n     /**\n      * @return the logFormat\n@@ -92,6 +97,30 @@ public String getLogFormat() {\n     public String getTimestampFormat() {\n       return timestampFormat;\n     }\n+\n+    @Override\n+    public int hashCode() {\n+      int result = logFormat != null ? logFormat.hashCode() : 0;\n+      result = 31 * result + (timestampFormat != null ? timestampFormat.hashCode() : 0);\n+      return result;\n+    }\n+\n+    @Override\n+    public boolean equals(Object o) {\n+      if (this == o) {\n+        return true;\n+      }\n+      if (o == null || getClass() != o.getClass()) {\n+        return false;\n+      }\n+\n+      HttpdLogFormatConfig that = (HttpdLogFormatConfig) o;\n+\n+      if (logFormat != null ? !logFormat.equals(that.logFormat) : that.logFormat != null) {\n+        return false;\n+      }\n+      return timestampFormat != null ? timestampFormat.equals(that.timestampFormat) : that.timestampFormat == null;\n+    }\n   }\n \n   /**\n@@ -119,7 +148,7 @@ public HttpdLogRecordReader(final FragmentContext context, final DrillFileSystem\n      * The query fields passed in are formatted in a way that Drill requires. Those must be cleaned up to work with the\n      * parser.\n      *\n-     * @return Map<DrillFieldNames, ParserFieldNames>\n+     * @return Map with Drill field names as a key and Parser Field names as a value\n      */\n     private Map<String, String> makeParserFields() {\n       final Map<String, String> fieldMapping = Maps.newHashMap();",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/httpd/HttpdLogFormatPlugin.java",
                "sha": "cee9a89247ce3d8ec128c735d60154949c96567c",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 3,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "patch": "@@ -164,9 +164,6 @@\n   ) throws IOException, ExecutionSetupException {\n     super(ImpersonationUtil.resolveUserName(userName));\n     this.columns = columns;\n-    if (formatConfig == null) {\n-      formatConfig = new ParquetFormatConfig();\n-    }\n     Preconditions.checkNotNull(storageConfig);\n     Preconditions.checkNotNull(formatConfig);\n     this.formatPlugin = (ParquetFormatPlugin) engineRegistry.getFormatPlugin(storageConfig, formatConfig);\n@@ -345,6 +342,7 @@ public boolean hasFiles() {\n     return true;\n   }\n \n+  @JsonIgnore\n   @Override\n   public Collection<String> getFiles() {\n     return fileSet;",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetGroupScan.java",
                "sha": "75b18df072afcbc85b7b0f1287ccb663e8f5c2fc",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRowGroupScan.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRowGroupScan.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 12,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRowGroupScan.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -65,9 +65,12 @@ public ParquetRowGroupScan( //\n       @JsonProperty(\"selectionRoot\") String selectionRoot, //\n       @JsonProperty(\"filter\") LogicalExpression filter\n   ) throws ExecutionSetupException {\n-    this(userName, (ParquetFormatPlugin) registry.getFormatPlugin(Preconditions.checkNotNull(storageConfig),\n-            formatConfig == null ? new ParquetFormatConfig() : formatConfig),\n-        rowGroupReadEntries, columns, selectionRoot, filter);\n+    this(userName,\n+        (ParquetFormatPlugin) registry.getFormatPlugin(Preconditions.checkNotNull(storageConfig), Preconditions.checkNotNull(formatConfig)),\n+        rowGroupReadEntries,\n+        columns,\n+        selectionRoot,\n+        filter);\n   }\n \n   public ParquetRowGroupScan( //\n@@ -79,7 +82,7 @@ public ParquetRowGroupScan( //\n       LogicalExpression filter\n   ) {\n     super(userName);\n-    this.formatPlugin = Preconditions.checkNotNull(formatPlugin);\n+    this.formatPlugin = Preconditions.checkNotNull(formatPlugin, \"Could not find format config for the given configuration\");\n     this.formatConfig = formatPlugin.getConfig();\n     this.rowGroupReadEntries = rowGroupReadEntries;\n     this.columns = columns == null ? GroupScan.ALL_COLUMNS : columns;\n@@ -97,6 +100,14 @@ public StoragePluginConfig getEngineConfig() {\n     return formatPlugin.getStorageConfig();\n   }\n \n+  /**\n+   * @return Parquet plugin format config\n+   */\n+  @JsonProperty(\"format\")\n+  public ParquetFormatConfig getFormatConfig() {\n+    return formatConfig;\n+  }\n+\n   public String getSelectionRoot() {\n     return selectionRoot;\n   }\n@@ -140,11 +151,4 @@ public int getOperatorType() {\n     return CoreOperatorType.PARQUET_ROW_GROUP_SCAN_VALUE;\n   }\n \n-  /**\n-   * @return Parquet plugin format config\n-   */\n-  public ParquetFormatConfig getFormatConfig() {\n-    return formatConfig;\n-  }\n-\n }",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetRowGroupScan.java",
                "sha": "f1fb1e9207f86e23e426e214d1e8f6ec159b7007",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatConfig.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatConfig.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatConfig.java",
                "patch": "@@ -21,4 +21,14 @@\n \n @JsonTypeName(\"pcap\")\n public class PcapFormatConfig implements FormatPluginConfig {\n+\n+  @Override\n+  public int hashCode() {\n+    return 0;\n+  }\n+\n+  @Override\n+  public boolean equals(Object obj) {\n+    return obj instanceof PcapFormatConfig;\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatConfig.java",
                "sha": "89b56adadbab2232558a1aef3c4756dc1ee969c0",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatPlugin.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatPlugin.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 8,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatPlugin.java",
                "patch": "@@ -34,7 +34,6 @@\n import org.apache.drill.exec.store.dfs.FormatMatcher;\n import org.apache.drill.exec.store.dfs.FormatSelection;\n import org.apache.drill.exec.store.dfs.MagicString;\n-import org.apache.drill.exec.store.dfs.NamedFormatPluginConfig;\n import org.apache.drill.exec.store.dfs.easy.EasyFormatPlugin;\n import org.apache.drill.exec.store.dfs.easy.EasyWriter;\n import org.apache.drill.exec.store.dfs.easy.FileWork;\n@@ -99,13 +98,7 @@ public DrillTable isReadable(DrillFileSystem fs,\n                                  FileSelection selection, FileSystemPlugin fsPlugin,\n                                  String storageEngineName, String userName) throws IOException {\n       if (isFileReadable(fs, selection.getFirstPath(fs))) {\n-        if (plugin.getName() != null) {\n-          NamedFormatPluginConfig namedConfig = new NamedFormatPluginConfig();\n-          namedConfig.name = plugin.getName();\n-          return new PcapDrillTable(storageEngineName, fsPlugin, userName, new FormatSelection(namedConfig, selection));\n-        } else {\n-          return new PcapDrillTable(storageEngineName, fsPlugin, userName, new FormatSelection(plugin.getConfig(), selection));\n-        }\n+        return new PcapDrillTable(storageEngineName, fsPlugin, userName, new FormatSelection(plugin.getConfig(), selection));\n       }\n       return null;\n     }",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/main/java/org/apache/drill/exec/store/pcap/PcapFormatPlugin.java",
                "sha": "65ff2388ac30c8e93b4f453d0cb93d0962fb5020",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/PlanTestBase.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/PlanTestBase.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/PlanTestBase.java",
                "patch": "@@ -281,6 +281,19 @@ public static void testRelLogicalPlanLevExplain(String sql, String... expectedSu\n     }\n   }\n \n+\n+  /**\n+   * Creates physical plan for the given query and then executes this plan.\n+   * This method is useful for testing serialization / deserialization issues.\n+   *\n+   * @param query query string\n+   */\n+  public static void testPhysicalPlanExecutionBasedOnQuery(String query) throws Exception {\n+    query = \"EXPLAIN PLAN for \" + QueryTestUtil.normalizeQuery(query);\n+    String plan = getPlanInString(query, JSON_FORMAT);\n+    testPhysical(plan);\n+  }\n+\n   /*\n    * This will get the plan (either logical or physical) in Optiq RelNode\n    * format, based on SqlExplainLevel and Depth.",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/PlanTestBase.java",
                "sha": "22b734b2746718f39900f3522ddcced5fdc7d675",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/TestSelectWithOption.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/TestSelectWithOption.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 1,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/TestSelectWithOption.java",
                "patch": "@@ -1,4 +1,4 @@\n-/**\n+/*\n  * Licensed to the Apache Software Foundation (ASF) under one\n  * or more contributor license agreements.  See the NOTICE file\n  * distributed with this work for additional information\n@@ -19,12 +19,15 @@\n \n import static java.lang.String.format;\n import static org.apache.drill.test.TestBuilder.listOf;\n+import static org.hamcrest.CoreMatchers.containsString;\n+import static org.junit.Assert.assertThat;\n \n import java.io.File;\n import java.io.FileWriter;\n import java.io.IOException;\n \n import org.apache.drill.categories.SqlTest;\n+import org.apache.drill.common.exceptions.UserRemoteException;\n import org.apache.drill.exec.store.dfs.WorkspaceSchemaFactory;\n import org.apache.drill.test.BaseTestQuery;\n import org.apache.drill.test.TestBuilder;\n@@ -276,4 +279,16 @@ public void testUse() throws Exception {\n       test(\"use sys\");\n     }\n   }\n+\n+  @Test(expected = UserRemoteException.class)\n+  public void testAbsentTable() throws Exception {\n+    String schema = \"cp.default\";\n+    String tableName = \"absent_table\";\n+    try {\n+      test(\"select * from table(`%s`.`%s`(type=>'parquet'))\", schema, tableName);\n+    } catch (UserRemoteException e) {\n+      assertThat(e.getMessage(), containsString(String.format(\"Unable to find table [%s] in schema [%s]\", tableName, schema)));\n+      throw e;\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/TestSelectWithOption.java",
                "sha": "a6dff740bc1009de3f2c9fab65c8c245562a7ef2",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/TestTpchDistributedConcurrent.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/TestTpchDistributedConcurrent.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 1,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/TestTpchDistributedConcurrent.java",
                "patch": "@@ -177,7 +177,7 @@ public void run() {\n     }\n   }\n \n-  @Test\n+  //@Test\n   public void testConcurrentQueries() throws Exception {\n     QueryTestUtil.testRunAndPrint(client, UserBitShared.QueryType.SQL, alterSession);\n ",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/TestTpchDistributedConcurrent.java",
                "sha": "f096d558aef6c04c36f326e8567cecc208e62f9b",
                "status": "modified"
            },
            {
                "additions": 113,
                "blob_url": "https://github.com/apache/drill/blob/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java",
                "changes": 113,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java?ref=7506cfbb5c8522d371c12dbdc2268d48a9449a48",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java",
                "patch": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to you under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.store;\n+\n+import org.apache.drill.PlanTestBase;\n+import org.apache.drill.exec.store.avro.AvroTestUtil;\n+import org.junit.Test;\n+\n+import java.nio.file.Paths;\n+\n+public class FormatPluginSerDeTest extends PlanTestBase {\n+\n+  @Test\n+  public void testParquet() throws Exception {\n+    test(\"alter session set `planner.slice_target` = 1\");\n+    testPhysicalPlanSubmission(\n+        String.format(\"select * from table(cp.`%s`(type=>'parquet'))\", \"parquet/alltypes_required.parquet\"),\n+        String.format(\"select * from table(cp.`%s`(type=>'parquet', autoCorrectCorruptDates=>false))\", \"parquet/alltypes_required.parquet\"),\n+        String.format(\"select * from table(cp.`%s`(type=>'parquet', autoCorrectCorruptDates=>true))\", \"parquet/alltypes_required.parquet\")\n+    );\n+  }\n+\n+  @Test\n+  public void testAvro() throws Exception {\n+    AvroTestUtil.AvroTestRecordWriter testSetup = AvroTestUtil.generateSimplePrimitiveSchema_NoNullValues(5);\n+    String file = testSetup.getFileName();\n+    testPhysicalPlanSubmission(\n+        String.format(\"select * from dfs.`%s`\", file),\n+        String.format(\"select * from table(dfs.`%s`(type=>'avro'))\", file)\n+    );\n+  }\n+\n+  @Test\n+  public void testSequenceFile() throws Exception {\n+    String path = \"sequencefiles/simple.seq\";\n+    dirTestWatcher.copyResourceToRoot(Paths.get(path));\n+    testPhysicalPlanSubmission(\n+        String.format(\"select * from dfs.`%s`\", path),\n+        String.format(\"select * from table(dfs.`%s`(type=>'sequencefile'))\", path)\n+    );\n+  }\n+\n+  @Test\n+  public void testPcap() throws Exception {\n+    String path = \"store/pcap/tcp-1.pcap\";\n+    dirTestWatcher.copyResourceToRoot(Paths.get(path));\n+    testPhysicalPlanSubmission(\n+        String.format(\"select * from dfs.`%s`\", path),\n+        String.format(\"select * from table(dfs.`%s`(type=>'pcap'))\", path)\n+    );\n+  }\n+\n+  @Test\n+  public void testHttpd() throws Exception {\n+    String path = \"store/httpd/dfs-bootstrap.httpd\";\n+    dirTestWatcher.copyResourceToRoot(Paths.get(path));\n+    String logFormat = \"%h %t \\\"%r\\\" %>s %b \\\"%{Referer}i\\\"\";\n+    String timeStampFormat = \"dd/MMM/yyyy:HH:mm:ss ZZ\";\n+    testPhysicalPlanSubmission(\n+        String.format(\"select * from dfs.`%s`\", path),\n+        String.format(\"select * from table(dfs.`%s`(type=>'httpd', logFormat=>'%s'))\", path, logFormat),\n+        String.format(\"select * from table(dfs.`%s`(type=>'httpd', logFormat=>'%s', timestampFormat=>'%s'))\", path, logFormat, timeStampFormat)\n+    );\n+  }\n+\n+  @Test\n+  public void testJson() throws Exception {\n+    testPhysicalPlanSubmission(\n+        \"select * from cp.`donuts.json`\",\n+        \"select * from table(cp.`donuts.json`(type=>'json'))\"\n+    );\n+  }\n+\n+  @Test\n+  public void testText() throws Exception {\n+    String path = \"store/text/data/regions.csv\";\n+    dirTestWatcher.copyResourceToRoot(Paths.get(path));\n+    testPhysicalPlanSubmission(\n+        String.format(\"select * from table(dfs.`%s`(type => 'text'))\", path),\n+        String.format(\"select * from table(dfs.`%s`(type => 'text', extractHeader => false, fieldDelimiter => 'A'))\", path)\n+    );\n+  }\n+\n+  @Test\n+  public void testNamed() throws Exception {\n+    String path = \"store/text/WithQuote.tbl\";\n+    dirTestWatcher.copyResourceToRoot(Paths.get(path));\n+    String query = String.format(\"select * from table(dfs.`%s`(type=>'named', name=>'psv'))\", path);\n+    testPhysicalPlanSubmission(query);\n+  }\n+\n+  private void testPhysicalPlanSubmission(String...queries) throws Exception {\n+    for (String query : queries) {\n+      PlanTestBase.testPhysicalPlanExecutionBasedOnQuery(query);\n+    }\n+  }\n+\n+}\n+",
                "raw_url": "https://github.com/apache/drill/raw/7506cfbb5c8522d371c12dbdc2268d48a9449a48/exec/java-exec/src/test/java/org/apache/drill/exec/store/FormatPluginSerDeTest.java",
                "sha": "ca81eaa3e82e601d80c1d2f29892b54c68696e83",
                "status": "added"
            }
        ],
        "message": "DRILL-5771: Fix serDe errors for format plugins\n\n1. Fix various serde issues for format plugins described in DRILL-5771.\n2. Throw meaninful exception instead of NPE when table is not found when table function is used.\n3. Added unit tests for all format plugins for ensure serde is checked (physical plan is generated in json format and then submitted).\n4. Fix physical plan submission on Windows (DRILL-4640).\n\nThis closes #1014",
        "parent": "https://github.com/apache/drill/commit/d4c61cadbe5c7d88fd4393cc1b29648fbadfb9f1",
        "repo": "drill",
        "unit_tests": [
            "TestFileSelection.java",
            "TestParquetGroupScan.java"
        ]
    },
    "drill_79811db": {
        "bug_id": "drill_79811db",
        "commit": "https://github.com/apache/drill/commit/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "patch": "@@ -66,7 +66,6 @@\n \n   // External Sort Boot configuration\n \n-  String EXTERNAL_SORT_TARGET_BATCH_SIZE = \"drill.exec.sort.external.batch.size\";\n   String EXTERNAL_SORT_TARGET_SPILL_BATCH_SIZE = \"drill.exec.sort.external.spill.batch.size\";\n   String EXTERNAL_SORT_SPILL_GROUP_SIZE = \"drill.exec.sort.external.spill.group.size\";\n   String EXTERNAL_SORT_SPILL_THRESHOLD = \"drill.exec.sort.external.spill.threshold\";\n@@ -79,6 +78,8 @@\n   String EXTERNAL_SORT_SPILL_BATCH_SIZE = \"drill.exec.sort.external.spill.spill_batch_size\";\n   String EXTERNAL_SORT_MERGE_BATCH_SIZE = \"drill.exec.sort.external.spill.merge_batch_size\";\n   String EXTERNAL_SORT_MAX_MEMORY = \"drill.exec.sort.external.mem_limit\";\n+\n+  // Used only by the \"unmanaged\" sort.\n   String EXTERNAL_SORT_BATCH_LIMIT = \"drill.exec.sort.external.batch_limit\";\n \n   // External Sort Runtime options",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/ExecConstants.java",
                "sha": "60d6265275f4f052c763d85d6002eaf94871cc30",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortRecordBatchBuilder.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortRecordBatchBuilder.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 3,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortRecordBatchBuilder.java",
                "patch": "@@ -238,14 +238,15 @@ public void close() {\n   }\n \n   /**\n-   * For given recordcount how muchmemory does SortRecordBatchBuilder needs for its own purpose. This is used in\n+   * For given record count how much memory does SortRecordBatchBuilder needs for its own purpose. This is used in\n    * ExternalSortBatch to make decisions about whether to spill or not.\n    *\n    * @param recordCount\n    * @return\n    */\n   public static long memoryNeeded(int recordCount) {\n-    // We need 4 bytes (SV4) for each record.\n-    return recordCount * 4;\n+    // We need 4 bytes (SV4) for each record. Due to power-of-two allocations, the\n+    // backing buffer might be twice this size.\n+    return recordCount * 2 * 4;\n   }\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/sort/SortRecordBatchBuilder.java",
                "sha": "d46990f8d0878ef44e789e3bb767b2bab6476868",
                "status": "modified"
            },
            {
                "additions": 26,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/RecordBatchSizer.java",
                "changes": 97,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/RecordBatchSizer.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 71,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/RecordBatchSizer.java",
                "patch": "@@ -27,22 +27,15 @@\n import org.apache.drill.exec.record.VectorAccessible;\n import org.apache.drill.exec.record.VectorWrapper;\n import org.apache.drill.exec.record.selection.SelectionVector2;\n-import org.apache.drill.exec.vector.BaseDataValueVector;\n-import org.apache.drill.exec.vector.FixedWidthVector;\n-import org.apache.drill.exec.vector.NullableVarCharVector;\n-import org.apache.drill.exec.vector.NullableVector;\n import org.apache.drill.exec.vector.ValueVector;\n-import org.apache.drill.exec.vector.VarCharVector;\n-\n-import io.netty.buffer.DrillBuf;\n \n /**\n  * Given a record batch or vector container, determines the actual memory\n  * consumed by each column, the average row, and the entire record batch.\n  */\n \n public class RecordBatchSizer {\n-  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(RecordBatchSizer.class);\n+//  private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(RecordBatchSizer.class);\n \n   /**\n    * Column size information.\n@@ -53,23 +46,22 @@\n     /**\n      * Assumed size from Drill metadata.\n      */\n+\n     public int stdSize;\n+\n     /**\n      * Actual memory consumed by all the vectors associated with this column.\n      */\n+\n     public int totalSize;\n+\n     /**\n      * Actual average column width as determined from actual memory use. This\n      * size is larger than the actual data size since this size includes per-\n      * column overhead such as any unused vector space, etc.\n      */\n-    public int estSize;\n \n-    /**\n-     * The size of the data vector backing the column. Useful for detecting\n-     * cases of possible direct memory fragmentation.\n-     */\n-    public int dataVectorSize;\n+    public int estSize;\n     public int capacity;\n     public int density;\n     public int dataSize;\n@@ -86,53 +78,31 @@ public ColumnSize(VectorWrapper<?> vw) {\n       if (rowCount == 0) {\n         return;\n       }\n-      DrillBuf[] bufs = v.getBuffers(false);\n-      for (DrillBuf buf : bufs) {\n-        totalSize += buf.capacity();\n-      }\n+\n+      // Total size taken by all vectors (and underlying buffers)\n+      // associated with this vector.\n+\n+      totalSize = v.getAllocatedByteCount();\n \n       // Capacity is the number of values that the vector could\n       // contain. This is useful only for fixed-length vectors.\n \n       capacity = v.getValueCapacity();\n \n-      // Crude way to get the size of the buffer underlying simple (scalar) values.\n-      // Ignores maps, lists and other esoterica. Uses a crude way to subtract out\n-      // the null \"bit\" (really byte) buffer size for nullable vectors.\n+      // The amount of memory consumed by the payload: the actual\n+      // data stored in the vectors.\n \n-      if (v instanceof BaseDataValueVector) {\n-        dataVectorSize = totalSize;\n-        if (v instanceof NullableVector) {\n-          dataVectorSize -= bufs[0].getActualMemoryConsumed();\n-        }\n-      }\n+      dataSize = v.getPayloadByteCount();\n \n       // Determine \"density\" the number of rows compared to potential\n       // capacity. Low-density batches occur at block boundaries, ends\n       // of files and so on. Low-density batches throw off our estimates\n       // for Varchar columns because we don't know the actual number of\n       // bytes consumed (that information is hidden behind the Varchar\n       // implementation where we can't get at it.)\n-      //\n-      // A better solution is to have each vector do this calc rather\n-      // than trying to do it generically, but that increases the code\n-      // change footprint and slows the commit process.\n-\n-      if (v instanceof FixedWidthVector) {\n-        dataSize = stdSize * rowCount;\n-      } else if ( v instanceof VarCharVector ) {\n-        VarCharVector vv = (VarCharVector) v;\n-        dataSize = vv.getOffsetVector().getAccessor().get(rowCount);\n-      } else if ( v instanceof NullableVarCharVector ) {\n-        NullableVarCharVector vv = (NullableVarCharVector) v;\n-        dataSize = vv.getValuesVector().getOffsetVector().getAccessor().get(rowCount);\n-      } else {\n-        dataSize = 0;\n-      }\n-      if (dataSize > 0) {\n-        density = roundUp(dataSize * 100, dataVectorSize);\n-        estSize = roundUp(dataSize, rowCount);\n-      }\n+\n+      density = roundUp(dataSize * 100, totalSize);\n+      estSize = roundUp(dataSize, rowCount);\n     }\n \n     @Override\n@@ -145,8 +115,6 @@ public String toString() {\n       buf.append(estSize);\n       buf.append(\", total size: \");\n       buf.append(totalSize);\n-      buf.append(\", vector size: \");\n-      buf.append(dataVectorSize);\n       buf.append(\", data size: \");\n       buf.append(dataSize);\n       buf.append(\", row capacity: \");\n@@ -187,10 +155,12 @@ public String toString() {\n   private int sv2Size;\n   private int avgDensity;\n \n+  private int netBatchSize;\n+\n   public RecordBatchSizer(VectorAccessible va) {\n     rowCount = va.getRecordCount();\n     for (VectorWrapper<?> vw : va) {\n-      measureField(vw);\n+      measureColumn(vw);\n     }\n \n     if (rowCount > 0) {\n@@ -201,8 +171,8 @@ public RecordBatchSizer(VectorAccessible va) {\n     if (hasSv2) {\n       @SuppressWarnings(\"resource\")\n       SelectionVector2 sv2 = va.getSelectionVector2();\n-      sv2Size = sv2.getBuffer().capacity();\n-      grossRowWidth += sv2Size;\n+      sv2Size = sv2.getBuffer(false).capacity();\n+      grossRowWidth += sv2Size / rowCount;\n       netRowWidth += 2;\n     }\n \n@@ -227,12 +197,13 @@ public void applySv2() {\n     totalBatchSize += sv2Size;\n   }\n \n-  private void measureField(VectorWrapper<?> vw) {\n+  private void measureColumn(VectorWrapper<?> vw) {\n     ColumnSize colSize = new ColumnSize(vw);\n     columnSizes.add(colSize);\n \n     stdRowWidth += colSize.stdSize;\n     totalBatchSize += colSize.totalSize;\n+    netBatchSize += colSize.dataSize;\n     netRowWidth += colSize.estSize;\n   }\n \n@@ -249,27 +220,11 @@ public static int roundUp(int num, int denom) {\n   public int netRowWidth() { return netRowWidth; }\n   public int actualSize() { return totalBatchSize; }\n   public boolean hasSv2() { return hasSv2; }\n-  public int getAvgDensity() { return avgDensity; }\n+  public int avgDensity() { return avgDensity; }\n+  public int netSize() { return netBatchSize; }\n \n   public static final int MAX_VECTOR_SIZE = 16 * 1024 * 1024; // 16 MiB\n \n-  /**\n-   * Look for columns backed by vectors larger than the 16 MiB size\n-   * employed by the Netty allocator. Such large blocks can lead to\n-   * memory fragmentation and unexpected OOM errors.\n-   * @return true if any column is oversized\n-   */\n-  public boolean checkOversizeCols() {\n-    boolean hasOversize = false;\n-    for (ColumnSize colSize : columnSizes) {\n-      if ( colSize.dataVectorSize > MAX_VECTOR_SIZE) {\n-        logger.warn( \"Column is wider than 256 characters: OOM due to memory fragmentation is possible - \" + colSize.metadata.getPath() );\n-        hasOversize = true;\n-      }\n-    }\n-    return hasOversize;\n-  }\n-\n   @Override\n   public String toString() {\n     StringBuilder buf = new StringBuilder();",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/RecordBatchSizer.java",
                "sha": "22b1b0eed03115d4bf17cb6e1e00409d4b470625",
                "status": "modified"
            },
            {
                "additions": 46,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/SpillSet.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/SpillSet.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 6,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/SpillSet.java",
                "patch": "@@ -26,6 +26,7 @@\n import java.io.InputStream;\n import java.io.OutputStream;\n import java.util.Iterator;\n+import java.util.List;\n import java.util.Set;\n \n import org.apache.drill.common.config.DrillConfig;\n@@ -105,7 +106,7 @@\n \n     protected HadoopFileManager(String fsName) {\n       Configuration conf = new Configuration();\n-      conf.set(\"fs.default.name\", fsName);\n+      conf.set(FileSystem.FS_DEFAULT_NAME_KEY, fsName);\n       try {\n         fs = FileSystem.get(conf);\n       } catch (IOException e) {\n@@ -169,6 +170,12 @@ public long getReadBytes(InputStream inputStream) {\n     }\n   }\n \n+  /**\n+   * Wrapper around an input stream to collect the total bytes\n+   * read through the stream for use in reporting performance\n+   * metrics.\n+   */\n+\n   public static class CountingInputStream extends InputStream\n   {\n     private InputStream in;\n@@ -218,6 +225,12 @@ public void close() throws IOException {\n     public long getCount() { return count; }\n   }\n \n+  /**\n+   * Wrapper around an output stream to collect the total bytes\n+   * written through the stream for use in reporting performance\n+   * metrics.\n+   */\n+\n   public static class CountingOutputStream extends OutputStream {\n \n     private OutputStream out;\n@@ -333,6 +346,7 @@ public long getReadBytes(InputStream inputStream) {\n    */\n \n   private final String spillDirName;\n+  private final String spillFileName;\n \n   private int fileCount = 0;\n \n@@ -343,8 +357,30 @@ public long getReadBytes(InputStream inputStream) {\n   private long writeBytes;\n \n   public SpillSet(FragmentContext context, PhysicalOperator popConfig) {\n+    this(context, popConfig, null, \"spill\");\n+  }\n+\n+  public SpillSet(FragmentContext context, PhysicalOperator popConfig,\n+                  String opName, String fileName) {\n+    FragmentHandle handle = context.getHandle();\n     DrillConfig config = context.getConfig();\n-    dirs = Iterators.cycle(config.getStringList(ExecConstants.EXTERNAL_SORT_SPILL_DIRS));\n+    spillFileName = fileName;\n+    List<String> dirList = config.getStringList(ExecConstants.EXTERNAL_SORT_SPILL_DIRS);\n+    dirs = Iterators.cycle(dirList);\n+\n+    // If more than one directory, semi-randomly choose an offset into\n+    // the list to avoid overloading the first directory in the list.\n+\n+    if (dirList.size() > 1) {\n+      int hash = handle.getQueryId().hashCode() +\n+                 handle.getMajorFragmentId() +\n+                 handle.getMinorFragmentId() +\n+                 popConfig.getOperatorId();\n+      int offset = hash % dirList.size();\n+      for (int i = 0; i < offset; i++) {\n+        dirs.next();\n+      }\n+    }\n \n     // Use the high-performance local file system if the local file\n     // system is selected and impersonation is off. (We use that\n@@ -357,9 +393,13 @@ public SpillSet(FragmentContext context, PhysicalOperator popConfig) {\n     } else {\n       fileManager = new HadoopFileManager(spillFs);\n     }\n-    FragmentHandle handle = context.getHandle();\n-    spillDirName = String.format(\"%s_major%s_minor%s_op%s\", QueryIdHelper.getQueryId(handle.getQueryId()),\n-        handle.getMajorFragmentId(), handle.getMinorFragmentId(), popConfig.getOperatorId());\n+    spillDirName = String.format(\n+        \"%s_major%d_minor%d_op%d%s\",\n+        QueryIdHelper.getQueryId(handle.getQueryId()),\n+        handle.getMajorFragmentId(),\n+        handle.getMinorFragmentId(),\n+        popConfig.getOperatorId(),\n+        (opName == null) ? \"\" : \"_\" + opName);\n   }\n \n   public String getNextSpillFile() {\n@@ -371,7 +411,7 @@ public String getNextSpillFile() {\n     String spillDir = dirs.next();\n     String currSpillPath = Joiner.on(\"/\").join(spillDir, spillDirName);\n     currSpillDirs.add(currSpillPath);\n-    String outputFile = Joiner.on(\"/\").join(currSpillPath, \"spill\" + ++fileCount);\n+    String outputFile = Joiner.on(\"/\").join(currSpillPath, spillFileName + ++fileCount);\n     try {\n         fileManager.deleteOnExit(currSpillPath);\n     } catch (IOException e) {",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/spill/SpillSet.java",
                "sha": "74e1fb5674eced7fec3fca51915eee8c2afc6cbf",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java",
                "patch": "@@ -113,7 +113,7 @@ private VectorContainer getBatch() throws IOException {\n     if (schema != null) {\n       c = SchemaUtil.coerceContainer(c, schema, context);\n     }\n-//    logger.debug(\"Took {} us to read {} records\", watch.elapsed(TimeUnit.MICROSECONDS), c.getRecordCount());\n+    logger.trace(\"Took {} us to read {} records\", watch.elapsed(TimeUnit.MICROSECONDS), c.getRecordCount());\n     spilledBatches--;\n     currentContainer.zeroVectors();\n     Iterator<VectorWrapper<?>> wrapperIterator = c.iterator();",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/BatchGroup.java",
                "sha": "13f0dbeb59a33d43b006cba58c438167db2e88a8",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/BatchGroup.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/BatchGroup.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 15,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/BatchGroup.java",
                "patch": "@@ -75,17 +75,21 @@\n    */\n \n   public static class InputBatch extends BatchGroup {\n-    private SelectionVector2 sv2;\n+    private final SelectionVector2 sv2;\n+    private final int dataSize;\n \n-    public InputBatch(VectorContainer container, SelectionVector2 sv2, OperatorContext context, long batchSize) {\n-      super(container, context, batchSize);\n+    public InputBatch(VectorContainer container, SelectionVector2 sv2, OperatorContext context, int dataSize) {\n+      super(container, context);\n       this.sv2 = sv2;\n+      this.dataSize = dataSize;\n     }\n \n     public SelectionVector2 getSv2() {\n       return sv2;\n     }\n \n+    public int getDataSize() { return dataSize; }\n+\n     @Override\n     public int getRecordCount() {\n       if (sv2 != null) {\n@@ -148,8 +152,8 @@ public void close() throws IOException {\n     private BufferAllocator allocator;\n     private int spilledBatches = 0;\n \n-    public SpilledRun(SpillSet spillSet, String path, OperatorContext context, long batchSize) throws IOException {\n-      super(null, context, batchSize);\n+    public SpilledRun(SpillSet spillSet, String path, OperatorContext context) throws IOException {\n+      super(null, context);\n       this.spillSet = spillSet;\n       this.path = path;\n       this.allocator = context.getAllocator();\n@@ -275,25 +279,23 @@ public long closeOutputStream() throws IOException {\n       if (outputStream == null) {\n         return 0;\n       }\n-      long posn = spillSet.getPosition(outputStream);\n-      spillSet.tallyWriteBytes(posn);\n+      long writeSize = spillSet.getPosition(outputStream);\n+      spillSet.tallyWriteBytes(writeSize);\n       outputStream.close();\n       outputStream = null;\n-      logger.trace(\"Summary: Wrote {} bytes to {}\", posn, path);\n-      return posn;\n+      logger.trace(\"Summary: Wrote {} bytes to {}\", writeSize, path);\n+      return writeSize;\n     }\n   }\n \n   protected VectorContainer currentContainer;\n   protected int pointer = 0;\n-  protected OperatorContext context;\n+  protected final OperatorContext context;\n   protected BatchSchema schema;\n-  protected long dataSize;\n \n-  public BatchGroup(VectorContainer container, OperatorContext context, long dataSize) {\n+  public BatchGroup(VectorContainer container, OperatorContext context) {\n     this.currentContainer = container;\n     this.context = context;\n-    this.dataSize = dataSize;\n   }\n \n   /**\n@@ -348,8 +350,6 @@ public int getUnfilteredRecordCount() {\n     return currentContainer.getRecordCount();\n   }\n \n-  public long getDataSize() { return dataSize; }\n-\n   @Override\n   public Iterator<VectorWrapper<?>> iterator() {\n     return currentContainer.iterator();",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/BatchGroup.java",
                "sha": "7ea599c39c0e3378226c6b1f0df003b60ad161fe",
                "status": "modified"
            },
            {
                "additions": 180,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/ExternalSortBatch.java",
                "changes": 291,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/ExternalSortBatch.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 111,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/ExternalSortBatch.java",
                "patch": "@@ -200,6 +200,7 @@\n   public static final String INTERRUPTION_AFTER_SORT = \"after-sort\";\n   public static final String INTERRUPTION_AFTER_SETUP = \"after-setup\";\n   public static final String INTERRUPTION_WHILE_SPILLING = \"spilling\";\n+  public static final String INTERRUPTION_WHILE_MERGING = \"merging\";\n   public static final long DEFAULT_SPILL_BATCH_SIZE = 8L * 1024 * 1024;\n   public static final long MIN_SPILL_BATCH_SIZE = 256 * 1024;\n \n@@ -219,6 +220,11 @@\n \n   private BatchSchema schema;\n \n+  /**\n+   * Incoming batches buffered in memory prior to spilling\n+   * or an in-memory merge.\n+   */\n+\n   private LinkedList<BatchGroup.InputBatch> bufferedBatches = Lists.newLinkedList();\n   private LinkedList<BatchGroup.SpilledRun> spilledRuns = Lists.newLinkedList();\n   private SelectionVector4 sv4;\n@@ -231,6 +237,12 @@\n   private int mergeBatchRowCount;\n   private int peakNumBatches = -1;\n \n+  /**\n+   * Maximum memory this operator may use. Usually comes from the\n+   * operator definition, but may be overridden by a configuration\n+   * parameter for unit testing.\n+   */\n+\n   private long memoryLimit;\n \n   /**\n@@ -280,35 +292,79 @@\n   private long estimatedInputBatchSize;\n \n   /**\n-   * Maximum number of batches to hold in memory.\n-   * (Primarily for testing.)\n+   * Maximum number of spilled runs that can be merged in a single pass.\n    */\n \n-  private int bufferedBatchLimit;\n   private int mergeLimit;\n+\n+  /**\n+   * Target size of the first-generation spill files.\n+   */\n   private long spillFileSize;\n+\n+  /**\n+   * Tracks the minimum amount of remaining memory for use\n+   * in populating an operator metric.\n+   */\n+\n   private long minimumBufferSpace;\n \n   /**\n-   * Minimum memory level before spilling occurs. That is, we can buffer input\n-   * batches in memory until we are down to the level given by the spill point.\n+   * Maximum memory level before spilling occurs. That is, we can buffer input\n+   * batches in memory until we reach the level given by the buffer memory pool.\n+   */\n+\n+  private long bufferMemoryPool;\n+\n+  /**\n+   * Maximum memory that can hold batches during the merge\n+   * phase.\n    */\n \n-  private long spillPoint;\n   private long mergeMemoryPool;\n+\n+  /**\n+   * The target size for merge batches sent downstream.\n+   */\n+\n   private long preferredMergeBatchSize;\n-  private long bufferMemoryPool;\n-  private boolean hasOversizeCols;\n+\n+  /**\n+   * Sum of the total number of bytes read from upstream.\n+   * This is the raw memory bytes, not actual data bytes.\n+   */\n+\n   private long totalInputBytes;\n-  private Long spillBatchSize;\n+\n+  /**\n+   * The configured size for each spill batch.\n+   */\n+  private Long preferredSpillBatchSize;\n+\n+  /**\n+   * Tracks the maximum density of input batches. Density is\n+   * the amount of actual data / amount of memory consumed.\n+   * Low density batches indicate an EOF or something wrong in\n+   * an upstream operator because a low-density batch wastes\n+   * memory.\n+   */\n+\n   private int maxDensity;\n+  private int lastDensity = -1;\n \n   /**\n    * Estimated number of rows that fit into a single spill batch.\n    */\n \n   private int spillBatchRowCount;\n \n+  /**\n+   * The estimated actual spill batch size which depends on the\n+   * details of the data rows for any particular query.\n+   */\n+\n+  private int targetSpillBatchSize;\n+\n   // WARNING: The enum here is used within this class. But, the members of\n   // this enum MUST match those in the (unmanaged) ExternalSortBatch since\n   // that is the enum used in the UI to display metrics for the query profile.\n@@ -349,7 +405,7 @@ public ExternalSortBatch(ExternalSort popConfig, FragmentContext context, Record\n     allocator = oContext.getAllocator();\n     opCodeGen = new OperatorCodeGenerator(context, popConfig);\n \n-    spillSet = new SpillSet(context, popConfig);\n+    spillSet = new SpillSet(context, popConfig, \"sort\", \"run\");\n     copierHolder = new CopierHolder(context, allocator, opCodeGen);\n     configure(context.getConfig());\n   }\n@@ -368,12 +424,6 @@ private void configure(DrillConfig config) {\n       memoryLimit = Math.min(memoryLimit, configLimit);\n     }\n \n-    // Optional limit on the number of buffered in-memory batches.\n-    // 0 means no limit. Used primarily for testing. Must allow at least two\n-    // batches or no merging can occur.\n-\n-    bufferedBatchLimit = getConfigLimit(config, ExecConstants.EXTERNAL_SORT_BATCH_LIMIT, Integer.MAX_VALUE, 2);\n-\n     // Optional limit on the number of spilled runs to merge in a single\n     // pass. Limits the number of open file handles. Must allow at least\n     // two batches to merge to make progress.\n@@ -392,22 +442,31 @@ private void configure(DrillConfig config) {\n     // Set too large and the ratio between memory and input data sizes becomes\n     // small. Set too small and disk seek times dominate performance.\n \n-    spillBatchSize = config.getBytes(ExecConstants.EXTERNAL_SORT_SPILL_BATCH_SIZE);\n-    spillBatchSize = Math.max(spillBatchSize, MIN_SPILL_BATCH_SIZE);\n+    preferredSpillBatchSize = config.getBytes(ExecConstants.EXTERNAL_SORT_SPILL_BATCH_SIZE);\n+\n+    // In low memory, use no more than 1/4 of memory for each spill batch. Ensures we\n+    // can merge.\n+\n+    preferredSpillBatchSize = Math.min(preferredSpillBatchSize, memoryLimit / 4);\n+\n+    // But, the spill batch should be above some minimum size to prevent complete\n+    // thrashing.\n+\n+    preferredSpillBatchSize = Math.max(preferredSpillBatchSize, MIN_SPILL_BATCH_SIZE);\n \n     // Set the target output batch size. Use the maximum size, but only if\n     // this represents less than 10% of available memory. Otherwise, use 10%\n     // of memory, but no smaller than the minimum size. In any event, an\n     // output batch can contain no fewer than a single record.\n \n     preferredMergeBatchSize = config.getBytes(ExecConstants.EXTERNAL_SORT_MERGE_BATCH_SIZE);\n-    long maxAllowance = (long) (memoryLimit * MERGE_BATCH_ALLOWANCE);\n+    long maxAllowance = (long) (memoryLimit - 2 * preferredSpillBatchSize);\n     preferredMergeBatchSize = Math.min(maxAllowance, preferredMergeBatchSize);\n     preferredMergeBatchSize = Math.max(preferredMergeBatchSize, MIN_MERGED_BATCH_SIZE);\n \n-    logger.debug(\"Config: memory limit = {}, batch limit = {}, \" +\n-                 \"spill file size = {}, batch size = {}, merge limit = {}, merge batch size = {}\",\n-                  memoryLimit, bufferedBatchLimit, spillFileSize, spillBatchSize, mergeLimit,\n+    logger.debug(\"Config: memory limit = {}, \" +\n+                 \"spill file size = {}, spill batch size = {}, merge limit = {}, merge batch size = {}\",\n+                  memoryLimit, spillFileSize, preferredSpillBatchSize, mergeLimit,\n                   preferredMergeBatchSize);\n   }\n \n@@ -513,11 +572,21 @@ public IterOutcome innerNext() {\n \n   private IterOutcome nextOutputBatch() {\n     if (resultsIterator.next()) {\n+      injector.injectUnchecked(context.getExecutionControls(), INTERRUPTION_WHILE_MERGING);\n       return IterOutcome.OK;\n     } else {\n       logger.trace(\"Deliver phase complete: Returned {} batches, {} records\",\n                     resultsIterator.getBatchCount(), resultsIterator.getRecordCount());\n       sortState = SortState.DONE;\n+\n+      // Close the iterator here to release any remaining resources such\n+      // as spill files. This is important when a query has a join: the\n+      // first branch sort may complete before the second branch starts;\n+      // it may be quite a while after returning the last row before the\n+      // fragment executor calls this opeator's close method.\n+\n+      resultsIterator.close();\n+      resultsIterator = null;\n       return IterOutcome.NONE;\n     }\n   }\n@@ -561,11 +630,11 @@ private IterOutcome loadBatch() {\n       // out of memory and that no work as in-flight and thus abandoned.\n       // Consider removing this case once resource management is in place.\n \n-      logger.debug(\"received OUT_OF_MEMORY, trying to spill\");\n+      logger.error(\"received OUT_OF_MEMORY, trying to spill\");\n       if (bufferedBatches.size() > 2) {\n         spillFromMemory();\n       } else {\n-        logger.debug(\"not enough batches to spill, sending OUT_OF_MEMORY downstream\");\n+        logger.error(\"not enough batches to spill, sending OUT_OF_MEMORY downstream\");\n         return IterOutcome.OUT_OF_MEMORY;\n       }\n       break;\n@@ -693,9 +762,7 @@ private void setupSchema(IterOutcome upstream)  {\n     // Coerce all existing batches to the new schema.\n \n     for (BatchGroup b : bufferedBatches) {\n-//      System.out.println(\"Before: \" + allocator.getAllocatedMemory()); // Debug only\n       b.setSchema(schema);\n-//      System.out.println(\"After: \" + allocator.getAllocatedMemory()); // Debug only\n     }\n     for (BatchGroup b : spilledRuns) {\n       b.setSchema(schema);\n@@ -765,12 +832,12 @@ private void processBatch() {\n       spillFromMemory();\n     }\n \n-    // Sanity check. We should now be above the spill point.\n+    // Sanity check. We should now be below the buffer memory maximum.\n \n     long startMem = allocator.getAllocatedMemory();\n-    if (memoryLimit - startMem < spillPoint) {\n-      logger.error( \"ERROR: Failed to spill below the spill point. Spill point = {}, free memory = {}\",\n-                    spillPoint, memoryLimit - startMem);\n+    if (startMem > bufferMemoryPool) {\n+      logger.error( \"ERROR: Failed to spill above buffer limit. Buffer pool = {}, memory = {}\",\n+          bufferMemoryPool, startMem);\n     }\n \n     // Convert the incoming batch to the agreed-upon schema.\n@@ -835,7 +902,7 @@ private void processBatch() {\n     RecordBatchData rbd = new RecordBatchData(convertedBatch, allocator);\n     try {\n       rbd.setSv2(sv2);\n-      bufferedBatches.add(new BatchGroup.InputBatch(rbd.getContainer(), rbd.getSv2(), oContext, batchSize));\n+      bufferedBatches.add(new BatchGroup.InputBatch(rbd.getContainer(), rbd.getSv2(), oContext, sizer.netSize()));\n       if (peakNumBatches < bufferedBatches.size()) {\n         peakNumBatches = bufferedBatches.size();\n         stats.setLongStat(Metric.PEAK_BATCHES_IN_MEMORY, peakNumBatches);\n@@ -857,9 +924,6 @@ private void processBatch() {\n   private RecordBatchSizer analyzeIncomingBatch() {\n     RecordBatchSizer sizer = new RecordBatchSizer(incoming);\n     sizer.applySv2();\n-    if (! hasOversizeCols) {\n-      hasOversizeCols = sizer.checkOversizeCols();\n-    }\n     if (inputBatchCount == 0) {\n       logger.debug(\"{}\", sizer.toString());\n     }\n@@ -887,7 +951,7 @@ private void updateMemoryEstimates(long memoryDelta, RecordBatchSizer sizer) {\n     long actualBatchSize = sizer.actualSize();\n     int actualRecordCount = sizer.rowCount();\n \n-    if (actualBatchSize < memoryDelta) {\n+    if (actualBatchSize != memoryDelta) {\n       logger.debug(\"Memory delta: {}, actual batch size: {}, Diff: {}\",\n                    memoryDelta, actualBatchSize, memoryDelta - actualBatchSize);\n     }\n@@ -905,11 +969,12 @@ private void updateMemoryEstimates(long memoryDelta, RecordBatchSizer sizer) {\n     // We actually track the max density seen, and compare to 75% of that since\n     // Parquet produces very low density record batches.\n \n-    if (sizer.getAvgDensity() < maxDensity * 0.75) {\n-      logger.debug(\"Saw low density batch. Density: {}\", sizer.getAvgDensity());\n+    if (sizer.avgDensity() < maxDensity * 3 / 4 && sizer.avgDensity() != lastDensity) {\n+      logger.trace(\"Saw low density batch. Density: {}\", sizer.avgDensity());\n+      lastDensity = sizer.avgDensity();\n       return;\n     }\n-    maxDensity = Math.max(maxDensity, sizer.getAvgDensity());\n+    maxDensity = Math.max(maxDensity, sizer.avgDensity());\n \n     // We know the batch size and number of records. Use that to estimate\n     // the average record size. Since a typical batch has many records,\n@@ -934,6 +999,14 @@ private void updateMemoryEstimates(long memoryDelta, RecordBatchSizer sizer) {\n     long origInputBatchSize = estimatedInputBatchSize;\n     estimatedInputBatchSize = Math.max(estimatedInputBatchSize, actualBatchSize);\n \n+    // The row width may end up as zero if all fields are nulls or some\n+    // other unusual situation. In this case, assume a width of 10 just\n+    // to avoid lots of special case code.\n+\n+    if (estimatedRowWidth == 0) {\n+      estimatedRowWidth = 10;\n+    }\n+\n     // Go no further if nothing changed.\n \n     if (estimatedRowWidth == origRowEstimate && estimatedInputBatchSize == origInputBatchSize) {\n@@ -948,50 +1021,51 @@ private void updateMemoryEstimates(long memoryDelta, RecordBatchSizer sizer) {\n     // spill batches of either 64K records, or as many records as fit into the\n     // amount of memory dedicated to each spill batch, whichever is less.\n \n-    spillBatchRowCount = (int) Math.max(1, spillBatchSize / estimatedRowWidth);\n+    spillBatchRowCount = (int) Math.max(1, preferredSpillBatchSize / estimatedRowWidth / 2);\n     spillBatchRowCount = Math.min(spillBatchRowCount, Character.MAX_VALUE);\n \n+    // Compute the actual spill batch size which may be larger or smaller\n+    // than the preferred size depending on the row width. Double the estimated\n+    // memory needs to allow for power-of-two rounding.\n+\n+    targetSpillBatchSize = spillBatchRowCount * estimatedRowWidth * 2;\n+\n     // Determine the number of records per batch per merge step. The goal is to\n     // merge batches of either 64K records, or as many records as fit into the\n     // amount of memory dedicated to each merge batch, whichever is less.\n \n-    targetMergeBatchSize = preferredMergeBatchSize;\n-    mergeBatchRowCount = (int) Math.max(1, targetMergeBatchSize / estimatedRowWidth);\n+    mergeBatchRowCount = (int) Math.max(1, preferredMergeBatchSize / estimatedRowWidth / 2);\n     mergeBatchRowCount = Math.min(mergeBatchRowCount, Character.MAX_VALUE);\n+    mergeBatchRowCount = Math.max(1,  mergeBatchRowCount);\n+    targetMergeBatchSize = mergeBatchRowCount * estimatedRowWidth * 2;\n \n     // Determine the minimum memory needed for spilling. Spilling is done just\n     // before accepting a batch, so we must spill if we don't have room for a\n     // (worst case) input batch. To spill, we need room for the output batch created\n     // by merging the batches already in memory. Double this to allow for power-of-two\n     // memory allocations.\n \n-    spillPoint = estimatedInputBatchSize + 2 * spillBatchSize;\n+    long spillPoint = estimatedInputBatchSize + 2 * targetSpillBatchSize;\n \n     // The merge memory pool assumes we can spill all input batches. To make\n     // progress, we must have at least two merge batches (same size as an output\n     // batch) and one output batch. Again, double to allow for power-of-two\n     // allocation and add one for a margin of error.\n \n-    int minMergeBatches = 2 * 3 + 1;\n-    long minMergeMemory = minMergeBatches * targetMergeBatchSize;\n+    long minMergeMemory = 2 * targetSpillBatchSize + targetMergeBatchSize;\n \n     // If we are in a low-memory condition, then we might not have room for the\n     // default output batch size. In that case, pick a smaller size.\n \n-    long minMemory = Math.max(spillPoint, minMergeMemory);\n-    if (minMemory > memoryLimit) {\n+    if (minMergeMemory > memoryLimit) {\n \n-      // Figure out the minimum output batch size based on memory, but can't be\n-      // any smaller than the defined minimum.\n+      // Figure out the minimum output batch size based on memory,\n+      // must hold at least one complete row.\n \n-      targetMergeBatchSize = Math.max(MIN_MERGED_BATCH_SIZE, memoryLimit / minMergeBatches);\n-\n-      // Regardless of anything else, the batch must hold at least one\n-      // complete row.\n-\n-      targetMergeBatchSize = Math.max(estimatedRowWidth, targetMergeBatchSize);\n-      spillPoint = estimatedInputBatchSize + 2 * spillBatchSize;\n-      minMergeMemory = minMergeBatches * targetMergeBatchSize;\n+      long mergeAllowance = memoryLimit - 2 * targetSpillBatchSize;\n+      targetMergeBatchSize = Math.max(estimatedRowWidth, mergeAllowance / 2);\n+      mergeBatchRowCount = (int) (targetMergeBatchSize / estimatedRowWidth / 2);\n+      minMergeMemory = 2 * targetSpillBatchSize + targetMergeBatchSize;\n     }\n \n     // Determine the minimum total memory we would need to receive two input\n@@ -1004,7 +1078,7 @@ private void updateMemoryEstimates(long memoryDelta, RecordBatchSizer sizer) {\n     // runs when reading from disk.\n \n     bufferMemoryPool = memoryLimit - spillPoint;\n-    mergeMemoryPool = Math.max(minMergeMemory,\n+    mergeMemoryPool = Math.max(memoryLimit - minMergeMemory,\n                                (long) ((memoryLimit - 3 * targetMergeBatchSize) * 0.95));\n \n     // Sanity check: if we've been given too little memory to make progress,\n@@ -1021,14 +1095,14 @@ private void updateMemoryEstimates(long memoryDelta, RecordBatchSizer sizer) {\n     // Log the calculated values. Turn this on if things seem amiss.\n     // Message will appear only when the values change.\n \n-    logger.debug(\"Memory Estimates: record size = {} bytes; input batch = {} bytes, {} records; \" +\n-                  \"merge batch size = {} bytes, {} records; \" +\n-                  \"output batch size = {} bytes, {} records; \" +\n-                  \"Available memory: {}, spill point = {}, min. merge memory = {}\",\n-                estimatedRowWidth, estimatedInputBatchSize, actualRecordCount,\n-                spillBatchSize, spillBatchRowCount,\n-                targetMergeBatchSize, mergeBatchRowCount,\n-                memoryLimit, spillPoint, minMergeMemory);\n+    logger.debug(\"Input Batch Estimates: record size = {} bytes; input batch = {} bytes, {} records\",\n+                 estimatedRowWidth, estimatedInputBatchSize, actualRecordCount);\n+    logger.debug(\"Merge batch size = {} bytes, {} records; spill file size: {} bytes\",\n+                 targetSpillBatchSize, spillBatchRowCount, spillFileSize);\n+    logger.debug(\"Output batch size = {} bytes, {} records\",\n+                 targetMergeBatchSize, mergeBatchRowCount);\n+    logger.debug(\"Available memory: {}, buffer memory = {}, merge memory = {}\",\n+                 memoryLimit, bufferMemoryPool, mergeMemoryPool);\n   }\n \n   /**\n@@ -1050,14 +1124,7 @@ private boolean isSpillNeeded(int incomingSize) {\n     // Must spill if we are below the spill point (the amount of memory\n     // needed to do the minimal spill.)\n \n-    if (allocator.getAllocatedMemory() + incomingSize >= bufferMemoryPool) {\n-      return true; }\n-\n-    // For test purposes, configuration may have set a limit on the number of\n-    // batches in memory. Spill if we exceed this limit. (By default the number\n-    // of in-memory batches is unlimited.)\n-\n-    return bufferedBatches.size() > bufferedBatchLimit;\n+    return allocator.getAllocatedMemory() + incomingSize >= bufferMemoryPool;\n   }\n \n   /**\n@@ -1068,8 +1135,8 @@ private boolean isSpillNeeded(int incomingSize) {\n    */\n \n   private IterOutcome sortInMemory() {\n-    logger.info(\"Starting in-memory sort. Batches = {}, Records = {}, Memory = {}\",\n-                bufferedBatches.size(), inputRecordCount, allocator.getAllocatedMemory());\n+    logger.debug(\"Starting in-memory sort. Batches = {}, Records = {}, Memory = {}\",\n+                 bufferedBatches.size(), inputRecordCount, allocator.getAllocatedMemory());\n \n     // Note the difference between how we handle batches here and in the spill/merge\n     // case. In the spill/merge case, this class decides on the batch size to send\n@@ -1088,8 +1155,8 @@ private IterOutcome sortInMemory() {\n         sortState = SortState.DONE;\n         return IterOutcome.STOP;\n       } else {\n-        logger.info(\"Completed in-memory sort. Memory = {}\",\n-                allocator.getAllocatedMemory());\n+        logger.debug(\"Completed in-memory sort. Memory = {}\",\n+                     allocator.getAllocatedMemory());\n         resultsIterator = memoryMerge;\n         memoryMerge = null;\n         sortState = SortState.DELIVER;\n@@ -1111,9 +1178,9 @@ private IterOutcome sortInMemory() {\n    */\n \n   private IterOutcome mergeSpilledRuns() {\n-    logger.info(\"Starting consolidate phase. Batches = {}, Records = {}, Memory = {}, In-memory batches {}, spilled runs {}\",\n-                inputBatchCount, inputRecordCount, allocator.getAllocatedMemory(),\n-                bufferedBatches.size(), spilledRuns.size());\n+    logger.debug(\"Starting consolidate phase. Batches = {}, Records = {}, Memory = {}, In-memory batches {}, spilled runs {}\",\n+                 inputBatchCount, inputRecordCount, allocator.getAllocatedMemory(),\n+                 bufferedBatches.size(), spilledRuns.size());\n \n     // Consolidate batches to a number that can be merged in\n     // a single last pass.\n@@ -1132,7 +1199,8 @@ private IterOutcome mergeSpilledRuns() {\n     allBatches.addAll(spilledRuns);\n     spilledRuns.clear();\n \n-    logger.info(\"Starting merge phase. Runs = {}, Alloc. memory = {}\", allBatches.size(), allocator.getAllocatedMemory());\n+    logger.debug(\"Starting merge phase. Runs = {}, Alloc. memory = {}\",\n+                 allBatches.size(), allocator.getAllocatedMemory());\n \n     // Do the final merge as a results iterator.\n \n@@ -1153,9 +1221,13 @@ private boolean consolidateBatches() {\n \n     // Can't merge more than will fit into memory at one time.\n \n-    int maxMergeWidth = (int) (mergeMemoryPool / targetMergeBatchSize);\n+    int maxMergeWidth = (int) (mergeMemoryPool / targetSpillBatchSize);\n     maxMergeWidth = Math.min(mergeLimit, maxMergeWidth);\n \n+    // But, must merge at least two batches.\n+\n+    maxMergeWidth = Math.max(maxMergeWidth, 2);\n+\n     // If we can't fit all batches in memory, must spill any in-memory\n     // batches to make room for multiple spill-merge-spill cycles.\n \n@@ -1177,7 +1249,7 @@ private boolean consolidateBatches() {\n       // is available, spill some in-memory batches.\n \n       long allocated = allocator.getAllocatedMemory();\n-      long totalNeeds = spilledRunsCount * targetMergeBatchSize + allocated;\n+      long totalNeeds = spilledRunsCount * targetSpillBatchSize + allocated;\n       if (totalNeeds > mergeMemoryPool) {\n         spillFromMemory();\n         return true;\n@@ -1231,52 +1303,46 @@ private boolean consolidateBatches() {\n    * This method spills only half the accumulated batches\n    * minimizing unnecessary disk writes. The exact count must lie between\n    * the minimum and maximum spill counts.\n-    */\n+   */\n \n   private void spillFromMemory() {\n \n     // Determine the number of batches to spill to create a spill file\n     // of the desired size. The actual file size might be a bit larger\n     // or smaller than the target, which is expected.\n \n-    long estSize = 0;\n     int spillCount = 0;\n+    long spillSize = 0;\n     for (InputBatch batch : bufferedBatches) {\n-      estSize += batch.getDataSize();\n-      if (estSize > spillFileSize) {\n-        break; }\n+      long batchSize = batch.getDataSize();\n+      spillSize += batchSize;\n       spillCount++;\n+      if (spillSize + batchSize / 2 > spillFileSize) {\n+        break; }\n     }\n \n-    // Should not happen, but just to be sure...\n+    // Must always spill at least 2, even if this creates an over-size\n+    // spill file. But, if this is a final consolidation, we may have only\n+    // a single batch.\n \n-    if (spillCount == 0) {\n-      return; }\n+    spillCount = Math.max(spillCount, 2);\n+    spillCount = Math.min(spillCount, bufferedBatches.size());\n \n     // Do the actual spill.\n \n-    logger.trace(\"Starting spill from memory. Memory = {}, Buffered batch count = {}, Spill batch count = {}\",\n-                 allocator.getAllocatedMemory(), bufferedBatches.size(), spillCount);\n     mergeAndSpill(bufferedBatches, spillCount);\n   }\n \n   private void mergeAndSpill(LinkedList<? extends BatchGroup> source, int count) {\n-    if (count == 0) {\n-      return; }\n     spilledRuns.add(doMergeAndSpill(source, count));\n   }\n \n   private BatchGroup.SpilledRun doMergeAndSpill(LinkedList<? extends BatchGroup> batchGroups, int spillCount) {\n     List<BatchGroup> batchesToSpill = Lists.newArrayList();\n     spillCount = Math.min(batchGroups.size(), spillCount);\n     assert spillCount > 0 : \"Spill count to mergeAndSpill must not be zero\";\n-    long spillSize = 0;\n     for (int i = 0; i < spillCount; i++) {\n-      @SuppressWarnings(\"resource\")\n-      BatchGroup batch = batchGroups.pollFirst();\n-      assert batch != null : \"Encountered a null batch during merge and spill operation\";\n-      batchesToSpill.add(batch);\n-      spillSize += batch.getDataSize();\n+      batchesToSpill.add(batchGroups.pollFirst());\n     }\n \n     // Merge the selected set of matches and write them to the\n@@ -1288,8 +1354,11 @@ private void mergeAndSpill(LinkedList<? extends BatchGroup> source, int count) {\n     BatchGroup.SpilledRun newGroup = null;\n     try (AutoCloseable ignored = AutoCloseables.all(batchesToSpill);\n          CopierHolder.BatchMerger merger = copierHolder.startMerge(schema, batchesToSpill, spillBatchRowCount)) {\n-      logger.trace(\"Merging and spilling to {}\", outputFile);\n-      newGroup = new BatchGroup.SpilledRun(spillSet, outputFile, oContext, spillSize);\n+      logger.trace(\"Spilling {} of {} batches, {} rows, memory = {}, write to {}\",\n+                   batchesToSpill.size(), bufferedBatches.size() + batchesToSpill.size(),\n+                   spillBatchRowCount,\n+                   allocator.getAllocatedMemory(), outputFile);\n+      newGroup = new BatchGroup.SpilledRun(spillSet, outputFile, oContext);\n \n       // The copier will merge records from the buffered batches into\n       // the outputContainer up to targetRecordCount number of rows.\n@@ -1298,8 +1367,7 @@ private void mergeAndSpill(LinkedList<? extends BatchGroup> source, int count) {\n       while (merger.next()) {\n \n         // Add a new batch of records (given by merger.getOutput()) to the spill\n-        // file, opening the file if not yet open, and creating the target\n-        // directory if it does not yet exist.\n+        // file.\n         //\n         // note that addBatch also clears the merger's output container\n \n@@ -1322,7 +1390,7 @@ private void mergeAndSpill(LinkedList<? extends BatchGroup> source, int count) {\n       // It will release the memory in the close() call.\n \n       try {\n-        // Rethrow so we can organize how to handle the error.\n+        // Rethrow so we can decide how to handle the error.\n \n         throw e;\n       }\n@@ -1444,11 +1512,12 @@ public void close() {\n     } catch (RuntimeException e) {\n       ex = (ex == null) ? e : ex;\n     }\n-    try {\n-      allocator.close();\n-    } catch (RuntimeException e) {\n-      ex = (ex == null) ? e : ex;\n-    }\n+    // Note: allocator is closed by the FragmentManager\n+//    try {\n+//      allocator.close();\n+//    } catch (RuntimeException e) {\n+//      ex = (ex == null) ? e : ex;\n+//    }\n     if (ex != null) {\n       throw ex;\n     }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/xsort/managed/ExternalSortBatch.java",
                "sha": "a1162a02ecade691ba3b1054e0e1951b60d42517",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/record/SimpleVectorWrapper.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/record/SimpleVectorWrapper.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/record/SimpleVectorWrapper.java",
                "patch": "@@ -78,6 +78,7 @@ public void clear() {\n   }\n \n \n+  @SuppressWarnings(\"resource\")\n   @Override\n   public VectorWrapper<?> getChildWrapper(int[] ids) {\n     if (ids.length == 1) {\n@@ -108,4 +109,13 @@ public void transfer(VectorWrapper<?> destination) {\n     vector.makeTransferPair(((SimpleVectorWrapper<?>)destination).vector).transfer();\n   }\n \n+  @Override\n+  public String toString() {\n+    if (vector == null) {\n+      return \"null\";\n+    } else {\n+      return vector.toString();\n+    }\n+  }\n+\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/java-exec/src/main/java/org/apache/drill/exec/record/SimpleVectorWrapper.java",
                "sha": "0a9f3d6e129b5cfcef7274b8cd3c8de06b1523bf",
                "status": "modified"
            },
            {
                "additions": 76,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/FixedValueVectors.java",
                "changes": 149,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/codegen/templates/FixedValueVectors.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 73,
                "filename": "exec/vector/src/main/codegen/templates/FixedValueVectors.java",
                "patch": "@@ -69,7 +69,7 @@ public int getBufferSizeFor(final int valueCount) {\n \n   @Override\n   public int getValueCapacity(){\n-    return (int) (data.capacity() *1.0 / ${type.width});\n+    return data.capacity() / ${type.width};\n   }\n \n   @Override\n@@ -196,7 +196,7 @@ public void load(SerializedField metadata, DrillBuf buffer) {\n     data = buffer.slice(0, actualLength);\n     data.retain(1);\n     data.writerIndex(actualLength);\n-    }\n+  }\n \n   public TransferPair getTransferPair(BufferAllocator allocator){\n     return new TransferImpl(getField(), allocator);\n@@ -227,6 +227,11 @@ public void splitAndTransferTo(int startIndex, int length, ${minor.class}Vector\n     target.data.writerIndex(sliceLength);\n   }\n \n+  @Override\n+  public int getPayloadByteCount() {\n+    return getAccessor().getValueCount() * ${type.width};\n+  }\n+\n   private class TransferImpl implements TransferPair{\n     private ${minor.class}Vector to;\n \n@@ -390,7 +395,6 @@ public void get(int index, Nullable${minor.class}Holder holder){\n       return p.plusDays(days).plusMillis(millis);\n     }\n \n-\n     public StringBuilder getAsStringBuilder(int index) {\n       final int offsetIndex = index * ${type.width};\n \n@@ -539,6 +543,7 @@ public DateTime getObject(int index) {\n     public ${friendlyType} getObject(int index) {\n       return get(index);\n     }\n+\n     public ${minor.javaType!type.javaType} getPrimitiveObject(int index) {\n       return get(index);\n     }\n@@ -557,9 +562,7 @@ public void get(int index, Nullable${minor.class}Holder holder){\n       holder.isSet = 1;\n       holder.value = data.get${(minor.javaType!type.javaType)?cap_first}(index * ${type.width});\n     }\n-\n-\n-   </#if> <#-- type.width -->\n+    </#if> <#-- type.width -->\n  }\n \n  /**\n@@ -728,84 +731,84 @@ public void generateTestData(int count) {\n    }\n \n    <#else> <#-- type.width <= 8 -->\n-   public void set(int index, <#if (type.width >= 4)>${minor.javaType!type.javaType}<#else>int</#if> value) {\n-     data.set${(minor.javaType!type.javaType)?cap_first}(index * ${type.width}, value);\n-   }\n+    public void set(int index, <#if (type.width >= 4)>${minor.javaType!type.javaType}<#else>int</#if> value) {\n+      data.set${(minor.javaType!type.javaType)?cap_first}(index * ${type.width}, value);\n+    }\n \n    public void setSafe(int index, <#if (type.width >= 4)>${minor.javaType!type.javaType}<#else>int</#if> value) {\n      while(index >= getValueCapacity()) {\n-       reAlloc();\n-     }\n-     set(index, value);\n-   }\n-\n-   protected void set(int index, ${minor.class}Holder holder){\n-     data.set${(minor.javaType!type.javaType)?cap_first}(index * ${type.width}, holder.value);\n-   }\n+        reAlloc();\n+      }\n+      set(index, value);\n+    }\n \n-   public void setSafe(int index, ${minor.class}Holder holder){\n-     while(index >= getValueCapacity()) {\n-       reAlloc();\n-     }\n-     set(index, holder);\n-   }\n+    protected void set(int index, ${minor.class}Holder holder){\n+      data.set${(minor.javaType!type.javaType)?cap_first}(index * ${type.width}, holder.value);\n+    }\n \n-   protected void set(int index, Nullable${minor.class}Holder holder){\n-     data.set${(minor.javaType!type.javaType)?cap_first}(index * ${type.width}, holder.value);\n-   }\n+    public void setSafe(int index, ${minor.class}Holder holder){\n+      while(index >= getValueCapacity()) {\n+        reAlloc();\n+      }\n+      set(index, holder);\n+    }\n \n-   public void setSafe(int index, Nullable${minor.class}Holder holder){\n-     while(index >= getValueCapacity()) {\n-       reAlloc();\n-     }\n-     set(index, holder);\n-   }\n+    protected void set(int index, Nullable${minor.class}Holder holder){\n+      data.set${(minor.javaType!type.javaType)?cap_first}(index * ${type.width}, holder.value);\n+    }\n \n-   @Override\n-   public void generateTestData(int size) {\n-     setValueCount(size);\n-     boolean even = true;\n-     final int valueCount = getAccessor().getValueCount();\n-     for(int i = 0; i < valueCount; i++, even = !even) {\n-       if(even){\n-         set(i, ${minor.boxedType!type.boxedType}.MIN_VALUE);\n-       }else{\n-         set(i, ${minor.boxedType!type.boxedType}.MAX_VALUE);\n-       }\n-     }\n-   }\n+    public void setSafe(int index, Nullable${minor.class}Holder holder){\n+      while(index >= getValueCapacity()) {\n+        reAlloc();\n+      }\n+      set(index, holder);\n+    }\n \n-   public void generateTestDataAlt(int size) {\n-     setValueCount(size);\n-     boolean even = true;\n-     final int valueCount = getAccessor().getValueCount();\n-     for(int i = 0; i < valueCount; i++, even = !even) {\n-       if(even){\n-         set(i, (${(minor.javaType!type.javaType)}) 1);\n-       }else{\n-         set(i, (${(minor.javaType!type.javaType)}) 0);\n-       }\n-     }\n-   }\n+    @Override\n+    public void generateTestData(int size) {\n+      setValueCount(size);\n+      boolean even = true;\n+      final int valueCount = getAccessor().getValueCount();\n+      for(int i = 0; i < valueCount; i++, even = !even) {\n+        if(even) {\n+          set(i, ${minor.boxedType!type.boxedType}.MIN_VALUE);\n+        } else {\n+          set(i, ${minor.boxedType!type.boxedType}.MAX_VALUE);\n+        }\n+      }\n+    }\n+\n+    public void generateTestDataAlt(int size) {\n+      setValueCount(size);\n+      boolean even = true;\n+      final int valueCount = getAccessor().getValueCount();\n+      for(int i = 0; i < valueCount; i++, even = !even) {\n+        if(even) {\n+          set(i, (${(minor.javaType!type.javaType)}) 1);\n+        } else {\n+          set(i, (${(minor.javaType!type.javaType)}) 0);\n+        }\n+      }\n+    }\n \n   </#if> <#-- type.width -->\n \n-   @Override\n-   public void setValueCount(int valueCount) {\n-     final int currentValueCapacity = getValueCapacity();\n-     final int idx = (${type.width} * valueCount);\n-     while(valueCount > getValueCapacity()) {\n-       reAlloc();\n-     }\n-     if (valueCount > 0 && currentValueCapacity > valueCount * 2) {\n-       incrementAllocationMonitor();\n-     } else if (allocationMonitor > 0) {\n-       allocationMonitor = 0;\n-     }\n-     VectorTrimmer.trim(data, idx);\n-     data.writerIndex(valueCount * ${type.width});\n-   }\n- }\n+    @Override\n+    public void setValueCount(int valueCount) {\n+      final int currentValueCapacity = getValueCapacity();\n+      final int idx = (${type.width} * valueCount);\n+      while(valueCount > getValueCapacity()) {\n+        reAlloc();\n+      }\n+      if (valueCount > 0 && currentValueCapacity > valueCount * 2) {\n+        incrementAllocationMonitor();\n+      } else if (allocationMonitor > 0) {\n+        allocationMonitor = 0;\n+      }\n+      VectorTrimmer.trim(data, idx);\n+      data.writerIndex(valueCount * ${type.width});\n+    }\n+  }\n }\n \n </#if> <#-- type.major -->",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/FixedValueVectors.java",
                "sha": "b2a5dc389601dabde82592b3fd07621f352274e2",
                "status": "modified"
            },
            {
                "additions": 27,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/NullableValueVectors.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/codegen/templates/NullableValueVectors.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 3,
                "filename": "exec/vector/src/main/codegen/templates/NullableValueVectors.java",
                "patch": "@@ -45,12 +45,24 @@\n  * NB: this class is automatically generated from ${.template_name} and ValueVectorTypes.tdd using FreeMarker.\n  */\n @SuppressWarnings(\"unused\")\n-public final class ${className} extends BaseDataValueVector implements <#if type.major == \"VarLen\">VariableWidth<#else>FixedWidth</#if>Vector, NullableVector{\n+public final class ${className} extends BaseDataValueVector implements <#if type.major == \"VarLen\">VariableWidth<#else>FixedWidth</#if>Vector, NullableVector {\n   private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(${className}.class);\n \n   private final FieldReader reader = new Nullable${minor.class}ReaderImpl(Nullable${minor.class}Vector.this);\n \n   private final MaterializedField bitsField = MaterializedField.create(\"$bits$\", Types.required(MinorType.UINT1));\n+\n+  /**\n+   * Set value flag. Meaning:\n+   * <ul>\n+   * <li>0: value is not set (value is null).</li>\n+   * <li>1: value is set (value is not null).</li>\n+   * </ul>\n+   * That is, a 1 means that the values vector has a value. 0\n+   * means that the vector is null. Thus, all values start as\n+   * not set (null) and must be explicitly set (made not null).\n+   */\n+\n   private final UInt1Vector bits = new UInt1Vector(bitsField, allocator);\n   private final ${valuesName} values = new ${minor.class}Vector(field, allocator);\n \n@@ -108,8 +120,8 @@ public int getBufferSizeFor(final int valueCount) {\n       return 0;\n     }\n \n-    return values.getBufferSizeFor(valueCount)\n-        + bits.getBufferSizeFor(valueCount);\n+    return values.getBufferSizeFor(valueCount) +\n+           bits.getBufferSizeFor(valueCount);\n   }\n \n   @Override\n@@ -163,6 +175,18 @@ public boolean allocateNewSafe() {\n     return success;\n   }\n \n+  @Override\n+  public int getAllocatedByteCount() {\n+    return bits.getAllocatedByteCount() + values.getAllocatedByteCount();\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    // For nullable, we include all values, null or not, in computing\n+    // the value length.\n+    return bits.getPayloadByteCount() + values.getPayloadByteCount();\n+  }\n+\n   <#if type.major == \"VarLen\">\n   @Override\n   public void allocateNew(int totalBytes, int valueCount) {",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/NullableValueVectors.java",
                "sha": "b242728078ebfa734cd4a8a868edbeba4e665949",
                "status": "modified"
            },
            {
                "additions": 16,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/UnionVector.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/codegen/templates/UnionVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/codegen/templates/UnionVector.java",
                "patch": "@@ -201,6 +201,22 @@ public MaterializedField getField() {\n     return field;\n   }\n \n+  @Override\n+  public int getAllocatedByteCount() {\n+    // Most vectors are held inside the internal map.\n+\n+    int count = internalMap.getAllocatedByteCount();\n+    if (bit != null) {\n+      count += bit.getAllocatedByteCount();\n+    }\n+    return count;\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    return internalMap.getPayloadByteCount();\n+  }\n+\n   @Override\n   public TransferPair getTransferPair(BufferAllocator allocator) {\n     return new TransferImpl(field, allocator);",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/UnionVector.java",
                "sha": "93854e782a59a445b2e854161248a61d83920164",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/VariableLengthVectors.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/codegen/templates/VariableLengthVectors.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/codegen/templates/VariableLengthVectors.java",
                "patch": "@@ -238,6 +238,25 @@ public boolean copyFromSafe(int fromIndex, int thisIndex, ${minor.class}Vector f\n     return true;\n   }\n \n+  @Override\n+  public int getAllocatedByteCount() {\n+    return offsetVector.getAllocatedByteCount() + super.getAllocatedByteCount();\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    UInt${type.width}Vector.Accessor a = offsetVector.getAccessor();\n+    int count = a.getValueCount();\n+    if (count == 0) {\n+      return 0;\n+    } else {\n+      // If 1 or more values, then the last value is set to\n+      // the offset of the next value, which is the same as\n+      // the length of existing values.\n+      return a.get(count-1);\n+    }\n+  }\n+\n   private class TransferImpl implements TransferPair{\n     ${minor.class}Vector to;\n ",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/codegen/templates/VariableLengthVectors.java",
                "sha": "ea3c9de7c90ab37adfe7e24f8dcac2bc7f42c2f9",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/BaseDataValueVector.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/BaseDataValueVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/BaseDataValueVector.java",
                "patch": "@@ -87,4 +87,9 @@ public DrillBuf getBuffer() {\n    * the value vector. The purpose is to move the value vector to a \"mutate\" state\n    */\n   public void reset() {}\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    return data.capacity();\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/BaseDataValueVector.java",
                "sha": "4def5b83761f8a90b872e2df2315c1035d687658",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/BitVector.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/BitVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/BitVector.java",
                "patch": "@@ -449,4 +449,10 @@ public void clear() {\n     this.valueCount = 0;\n     super.clear();\n   }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    // One byte per value\n+    return valueCount;\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/BitVector.java",
                "sha": "a6c0ceafdaefc5c6245beed98b31a75d6605acfb",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/ObjectVector.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/ObjectVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/ObjectVector.java",
                "patch": "@@ -218,4 +218,16 @@ public void get(int index, ObjectHolder holder){\n       holder.obj = getObject(index);\n     }\n   }\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    // Values not stored in direct memory?\n+    return 0;\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    // Values not stored in direct memory?\n+    return 0;\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/ObjectVector.java",
                "sha": "f69dc9807165c46d6205296ce542131149122d1b",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/ValueVector.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/ValueVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/ValueVector.java",
                "patch": "@@ -175,6 +175,18 @@\n    */\n   void load(SerializedField metadata, DrillBuf buffer);\n \n+  /**\n+   * Return the total memory consumed by all buffers within this vector.\n+   */\n+\n+  int getAllocatedByteCount();\n+\n+  /**\n+   * Return the number of value bytes consumed by actual data.\n+   */\n+\n+  int getPayloadByteCount();\n+\n   /**\n    * An abstraction that is used to read from this vector instance.\n    */",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/ValueVector.java",
                "sha": "f4c793556f603e945eee08b2aebeb141d6a495b1",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/VariableWidthVector.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/VariableWidthVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 3,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/VariableWidthVector.java",
                "patch": "@@ -17,9 +17,7 @@\n  */\n package org.apache.drill.exec.vector;\n \n-import io.netty.buffer.DrillBuf;\n-\n-public interface VariableWidthVector extends ValueVector{\n+public interface VariableWidthVector extends ValueVector {\n \n   /**\n    * Allocate a new memory space for this vector.  Must be called prior to using the ValueVector.",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/VariableWidthVector.java",
                "sha": "d04234c9f762c3683a78f8a7b7fe336e6afc9c86",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/ZeroVector.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/ZeroVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/ZeroVector.java",
                "patch": "@@ -176,4 +176,14 @@ public FieldReader getReader() {\n \n   @Override\n   public void load(UserBitShared.SerializedField metadata, DrillBuf buffer) { }\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    return 0;\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    return 0;\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/ZeroVector.java",
                "sha": "9181f2042fa6a3b292327fd9129faeb415ca1b38",
                "status": "modified"
            },
            {
                "additions": 21,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/AbstractMapVector.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/AbstractMapVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 1,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/complex/AbstractMapVector.java",
                "patch": "@@ -266,7 +266,7 @@ public VectorWithOrdinal getChildVectorWithOrdinal(String name) {\n \n   @Override\n   public int getBufferSize() {\n-    int actualBufSize = 0 ;\n+    int actualBufSize = 0;\n \n     for (final ValueVector v : vectors.values()) {\n       for (final DrillBuf buf : v.getBuffers(false)) {\n@@ -275,4 +275,24 @@ public int getBufferSize() {\n     }\n     return actualBufSize;\n   }\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    int count = 0;\n+\n+    for (final ValueVector v : vectors.values()) {\n+      count += v.getAllocatedByteCount();\n+    }\n+    return count;\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    int count = 0;\n+\n+    for (final ValueVector v : vectors.values()) {\n+      count += v.getPayloadByteCount();\n+    }\n+    return count;\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/AbstractMapVector.java",
                "sha": "baba0865d8956fd835fa5e61600e69d71771afd3",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/BaseRepeatedValueVector.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/BaseRepeatedValueVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 1,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/complex/BaseRepeatedValueVector.java",
                "patch": "@@ -209,6 +209,17 @@ protected void replaceDataVector(ValueVector v) {\n     vector = v;\n   }\n \n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    return offsets.getAllocatedByteCount() + vector.getAllocatedByteCount();\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    return offsets.getPayloadByteCount() + vector.getPayloadByteCount();\n+  }\n+\n   public abstract class BaseRepeatedAccessor extends BaseValueVector.BaseAccessor implements RepeatedAccessor {\n \n     @Override\n@@ -256,5 +267,4 @@ public void setValueCount(int valueCount) {\n       vector.getMutator().setValueCount(childValueCount);\n     }\n   }\n-\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/BaseRepeatedValueVector.java",
                "sha": "1664b0a2930bc39c6643b93842658c6e09e84982",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/ListVector.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/ListVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/complex/ListVector.java",
                "patch": "@@ -317,4 +317,14 @@ public void setValueCount(int valueCount) {\n       bits.getMutator().setValueCount(valueCount);\n     }\n   }\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    return offsets.getAllocatedByteCount() + bits.getAllocatedByteCount() + super.getAllocatedByteCount();\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    return offsets.getPayloadByteCount() + bits.getPayloadByteCount() + super.getPayloadByteCount();\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/ListVector.java",
                "sha": "f71baa7e3c81a992f30cc24b49cedc77bf5ee603",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedListVector.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedListVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedListVector.java",
                "patch": "@@ -426,4 +426,14 @@ public VectorWithOrdinal getChildVectorWithOrdinal(String name) {\n   public void copyFromSafe(int fromIndex, int thisIndex, RepeatedListVector from) {\n     delegate.copyFromSafe(fromIndex, thisIndex, from.delegate);\n   }\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    return delegate.getAllocatedByteCount();\n+  }\n+\n+  @Override\n+  public int getPayloadByteCount() {\n+    return delegate.getPayloadByteCount();\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedListVector.java",
                "sha": "b5c97bf31bdba186be36560f546275adb3afda6f",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/drill/blob/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedMapVector.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedMapVector.java?ref=79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd",
                "deletions": 0,
                "filename": "exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedMapVector.java",
                "patch": "@@ -584,4 +584,9 @@ public void clear() {\n       vector.clear();\n     }\n   }\n+\n+  @Override\n+  public int getAllocatedByteCount() {\n+    return super.getAllocatedByteCount( ) + offsets.getAllocatedByteCount();\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/79811db5aa8c7f2cdbe6f74c0a40124bea9fb1fd/exec/vector/src/main/java/org/apache/drill/exec/vector/complex/RepeatedMapVector.java",
                "sha": "3707ff0d3d6060c31fa3732e59ecc3acb268566d",
                "status": "modified"
            }
        ],
        "message": "DRILL-5284: Roll-up of final fixes for managed sort\n\nSee subtasks for details.\n\n* Provide detailed, accurate estimate of size consumed by a record batch\n* Managed external sort spills too often with Parquet data\n* Managed External Sort fails with OOM\n* External sort refers to the deprecated HDFS fs.default.name param\n* Config param drill.exec.sort.external.batch.size is not used\n* NPE in managed external sort while spilling to disk\n* External Sort BatchGroup leaks memory if an OOM occurs during read\n* DRILL-5294: Under certain low-memory conditions, need to force the sort to merge\ntwo batches to make progress, even though this is a bit more than\ncomfortably fits into memory.\n\nclose #761",
        "parent": "https://github.com/apache/drill/commit/69de3a1e409bb1fb9a25e679ce1750d9f9daf238",
        "repo": "drill",
        "unit_tests": [
            "TestRecordBatchSizer.java",
            "TestValueVector.java"
        ]
    },
    "drill_826fc5b": {
        "bug_id": "drill_826fc5b",
        "commit": "https://github.com/apache/drill/commit/826fc5b9c2a6f044101967a9a2e49b20af2dae76",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveRecordReader.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveRecordReader.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76",
                "deletions": 1,
                "filename": "contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveRecordReader.java",
                "patch": "@@ -344,7 +344,10 @@ private void setValueCountAndPopulatePartitionVectors(int recordCount) {\n   @Override\n   public void cleanup() {\n     try {\n-      reader.close();\n+      if (reader != null) {\n+        reader.close();\n+        reader = null;\n+      }\n     } catch (Exception e) {\n       logger.warn(\"Failure while closing Hive Record reader.\", e);\n     }",
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/contrib/storage-hive/core/src/main/java/org/apache/drill/exec/store/hive/HiveRecordReader.java",
                "sha": "3c8b9ba419368f569cd463ee825354d859e9a073",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/text/compliant/CompliantTextRecordReader.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/text/compliant/CompliantTextRecordReader.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/text/compliant/CompliantTextRecordReader.java",
                "patch": "@@ -144,7 +144,10 @@ public int next() {\n   @Override\n   public void cleanup() {\n     try {\n-      reader.close();\n+      if (reader != null) {\n+        reader.close();\n+        reader = null;\n+      }\n     } catch (IOException e) {\n       logger.warn(\"Exception while closing stream.\", e);\n     }",
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/easy/text/compliant/CompliantTextRecordReader.java",
                "sha": "254e0d89b761020aa9a44beb8141df4921d1c59e",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/ParquetRecordReader.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/ParquetRecordReader.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76",
                "deletions": 7,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/ParquetRecordReader.java",
                "patch": "@@ -455,17 +455,22 @@ public void cleanup() {\n     // enable this for debugging when it is know that a whole file will be read\n     // limit kills upstream operators once it has enough records, so this assert will fail\n //    assert totalRecordsRead == footer.getBlocks().get(rowGroupIndex).getRowCount();\n-    for (ColumnReader column : columnStatuses) {\n-      column.clear();\n+    if (columnStatuses != null) {\n+      for (ColumnReader column : columnStatuses) {\n+        column.clear();\n+      }\n+      columnStatuses.clear();\n+      columnStatuses = null;\n     }\n-    columnStatuses.clear();\n \n     codecFactory.close();\n \n-    for (VarLengthColumn r : varLengthReader.columns) {\n-      r.clear();\n+    if (varLengthReader != null) {\n+      for (VarLengthColumn r : varLengthReader.columns) {\n+        r.clear();\n+      }\n+      varLengthReader.columns.clear();\n+      varLengthReader = null;\n     }\n-    varLengthReader.columns.clear();\n   }\n-\n }",
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/columnreaders/ParquetRecordReader.java",
                "sha": "2f07fb3527dbc4e4de25f8af07bba7c1c85d88f8",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet2/DrillParquetReader.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet2/DrillParquetReader.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet2/DrillParquetReader.java",
                "patch": "@@ -342,7 +342,10 @@ private int getPercentFilled() {\n   @Override\n   public void cleanup() {\n     try {\n-      pageReadStore.close();\n+      if (pageReadStore != null) {\n+        pageReadStore.close();\n+        pageReadStore = null;\n+      }\n     } catch (IOException e) {\n       logger.warn(\"Failure while closing PageReadStore\", e);\n     }",
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet2/DrillParquetReader.java",
                "sha": "4e7d6288f19ec48556d2532a4b81827b37754c80",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/text/DrillTextRecordReader.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/text/DrillTextRecordReader.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/text/DrillTextRecordReader.java",
                "patch": "@@ -231,7 +231,10 @@ public int find(Text text, byte delimiter, int start) {\n   @Override\n   public void cleanup() {\n     try {\n-      reader.close();\n+      if (reader != null) {\n+        reader.close();\n+        reader = null;\n+      }\n     } catch (IOException e) {\n       logger.warn(\"Exception closing reader: {}\", e);\n     }",
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/main/java/org/apache/drill/exec/store/text/DrillTextRecordReader.java",
                "sha": "e25bd74084c25188d20b0cfd91a890fa816ede1d",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/test/java/org/apache/drill/BaseTestQuery.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/BaseTestQuery.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76",
                "deletions": 1,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/BaseTestQuery.java",
                "patch": "@@ -59,6 +59,10 @@\n import com.google.common.base.Preconditions;\n import com.google.common.io.Resources;\n \n+import static org.hamcrest.core.StringContains.containsString;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertThat;\n+\n public class BaseTestQuery extends ExecTest {\n   private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(BaseTestQuery.class);\n \n@@ -319,7 +323,7 @@ protected static void testNoResult(String query, Object... args) throws Exceptio\n \n   protected static void testNoResult(int interation, String query, Object... args) throws Exception {\n     query = String.format(query, args);\n-    logger.debug(\"Running query:\\n--------------\\n\"+query);\n+    logger.debug(\"Running query:\\n--------------\\n\" + query);\n     for (int i = 0; i < interation; i++) {\n       List<QueryDataBatch> results = client.runQuery(QueryType.SQL, query);\n       for (QueryDataBatch queryDataBatch : results) {\n@@ -364,6 +368,24 @@ protected static void testSqlFromFile(String file) throws Exception{\n     test(getFile(file));\n   }\n \n+  /**\n+   * Utility method which tests given query produces a {@link UserException} and the exception message contains\n+   * the given message.\n+   * @param testSqlQuery Test query\n+   * @param expectedErrorMsg Expected error message.\n+   */\n+  protected static void errorMsgTestHelper(final String testSqlQuery, final String expectedErrorMsg) throws Exception {\n+    UserException expException = null;\n+    try {\n+      test(testSqlQuery);\n+    } catch (final UserException ex) {\n+      expException = ex;\n+    }\n+\n+    assertNotNull(\"Expected a UserException\", expException);\n+    assertThat(expException.getMessage(), containsString(expectedErrorMsg));\n+  }\n+\n   public static String getFile(String resource) throws IOException{\n     URL url = Resources.getResource(resource);\n     if (url == null) {",
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/test/java/org/apache/drill/BaseTestQuery.java",
                "sha": "db1ed348a7edc7c1ee3b92491e34dbce7047d27a",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/drill/blob/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/test/java/org/apache/drill/TestDisabledFunctionality.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/TestDisabledFunctionality.java?ref=826fc5b9c2a6f044101967a9a2e49b20af2dae76",
                "deletions": 4,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/TestDisabledFunctionality.java",
                "patch": "@@ -18,6 +18,7 @@\n package org.apache.drill;\n import org.apache.drill.common.exceptions.UserException;\n import org.apache.drill.common.util.FileUtils;\n+import org.apache.drill.exec.work.ExecErrorConstants;\n import org.apache.drill.exec.work.foreman.SqlUnsupportedException;\n import org.apache.drill.exec.work.foreman.UnsupportedDataTypeException;\n import org.apache.drill.exec.work.foreman.UnsupportedFunctionException;\n@@ -355,13 +356,15 @@ public void testFlattenWithinDistinct() throws Exception {\n     }\n   }\n \n-  @Test(expected =  UserException.class) // DRILL-2848\n+  @Test // DRILL-2848\n   public void testDisableDecimalCasts() throws Exception {\n-    test(\"select cast('1.2' as decimal(9, 2)) from cp.`employee.json` limit 1\");\n+    final String query = \"select cast('1.2' as decimal(9, 2)) from cp.`employee.json` limit 1\";\n+    errorMsgTestHelper(query, ExecErrorConstants.DECIMAL_DISABLE_ERR_MSG);\n   }\n \n-  @Test(expected = UserException.class) // DRILL-2848\n+  @Test // DRILL-2848\n   public void testDisableDecimalFromParquet() throws Exception {\n-    test(\"select * from cp.`parquet/decimal_dictionary.parquet`\");\n+    final String query = \"select * from cp.`parquet/decimal_dictionary.parquet`\";\n+    errorMsgTestHelper(query, ExecErrorConstants.DECIMAL_DISABLE_ERR_MSG);\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/drill/raw/826fc5b9c2a6f044101967a9a2e49b20af2dae76/exec/java-exec/src/test/java/org/apache/drill/TestDisabledFunctionality.java",
                "sha": "adbf653efdd5fe04489460ac3634e1352ad3bb00",
                "status": "modified"
            }
        ],
        "message": "DRILL-3017: Safeguard against NPEs in RecordReader.cleanup()s",
        "parent": "https://github.com/apache/drill/commit/6076cc6435f7decb17856991c25dac77dfe0b203",
        "repo": "drill",
        "unit_tests": [
            "ParquetRecordReaderTest.java",
            "TestDrillParquetReader.java"
        ]
    },
    "drill_894037a": {
        "bug_id": "drill_894037a",
        "commit": "https://github.com/apache/drill/commit/894037ab693dea425e88fb3ec3aff73ea5b15eb1",
        "file": [
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/drill/blob/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/ops/OperatorStats.java",
                "changes": 15,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/ops/OperatorStats.java?ref=894037ab693dea425e88fb3ec3aff73ea5b15eb1",
                "deletions": 6,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/ops/OperatorStats.java",
                "patch": "@@ -66,41 +66,44 @@ private OperatorStats(int operatorId, int operatorType, int inputCount) {\n     this.schemaCountByInput = new long[inputCount];\n   }\n \n+  private String assertionError(String msg){\n+    return String.format(\"Failure while %s for operator id %d. Currently have states of processing:%s, setup:%s, waiting:%s.\", msg, operatorId, inProcessing, inSetup, inWait);\n+  }\n   public void startSetup() {\n-    assert !inSetup  : \"Failure while starting setup.  Currently in setup.\";\n+    assert !inSetup  : assertionError(\"starting setup\");\n     stopProcessing();\n     inSetup = true;\n     setupMark = System.nanoTime();\n   }\n \n   public void stopSetup() {\n-    assert inSetup :  \"Failure while stopping setup.  Not currently in setup.\";\n+    assert inSetup :  assertionError(\"stopping setup\");\n     startProcessing();\n     setupNanos += System.nanoTime() - setupMark;\n     inSetup = false;\n   }\n \n   public void startProcessing() {\n-    assert !inProcessing : \"Failure while starting processing.  Currently in processing.\";\n+    assert !inProcessing : assertionError(\"starting processing\");\n     processingMark = System.nanoTime();\n     inProcessing = true;\n   }\n \n   public void stopProcessing() {\n-    assert inProcessing : \"Failure while stopping processing.  Not currently in processing.\";\n+    assert inProcessing : assertionError(\"stopping processing\");\n     processingNanos += System.nanoTime() - processingMark;\n     inProcessing = false;\n   }\n \n   public void startWait() {\n-    assert !inWait : \"Failure while starting waiting.  Currently in waiting.\";\n+    assert !inWait : assertionError(\"starting waiting\");\n     stopProcessing();\n     inWait = true;\n     waitMark = System.nanoTime();\n   }\n \n   public void stopWait() {\n-    assert inWait : \"Failure while stopping waiting.  Currently not in waiting.\";\n+    assert inWait : assertionError(\"stopping waiting\");\n     startProcessing();\n     waitNanos += System.nanoTime() - waitMark;\n     inWait = false;",
                "raw_url": "https://github.com/apache/drill/raw/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/ops/OperatorStats.java",
                "sha": "dcb73c82353c57f5a7e7e71d903e2b0a2a54c3f6",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java?ref=894037ab693dea425e88fb3ec3aff73ea5b15eb1",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java",
                "patch": "@@ -156,9 +156,10 @@ protected void setupNewSchema() throws Exception {\n       // update the schema in RecordWriter\n       stats.startSetup();\n       recordWriter.updateSchema(incoming.getSchema());\n-      stats.stopSetup();\n     } catch(IOException ex) {\n       throw new RuntimeException(\"Failed to update schema in RecordWriter\", ex);\n+    } finally{\n+      stats.stopSetup();\n     }\n \n     eventBasedRecordWriter = new EventBasedRecordWriter(incoming.getSchema(),",
                "raw_url": "https://github.com/apache/drill/raw/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/WriterRecordBatch.java",
                "sha": "43e0dd4c78ef3c0e5e8ea0d51f6a6952f7ef5202",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java?ref=894037ab693dea425e88fb3ec3aff73ea5b15eb1",
                "deletions": 2,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java",
                "patch": "@@ -154,14 +154,14 @@ private boolean createAggregator() {\n     try{\n       stats.startSetup();\n       this.aggregator = createAggregatorInternal();\n-      stats.stopSetup();\n       return true;\n     }catch(SchemaChangeException | ClassTransformationException | IOException ex){\n-      stats.stopSetup();\n       context.fail(ex);\n       container.clear();\n       incoming.kill();\n       return false;\n+    }finally{\n+      stats.stopSetup();\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/drill/raw/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/HashAggBatch.java",
                "sha": "dd58562df63fe5e374f963b298ce0e5b0f40235d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggBatch.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggBatch.java?ref=894037ab693dea425e88fb3ec3aff73ea5b15eb1",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggBatch.java",
                "patch": "@@ -136,13 +136,14 @@ private boolean createAggregator() {\n     try{\n       stats.startSetup();\n       this.aggregator = createAggregatorInternal();\n-      stats.stopSetup();\n       return true;\n     }catch(SchemaChangeException | ClassTransformationException | IOException ex){\n       context.fail(ex);\n       container.clear();\n       incoming.kill();\n       return false;\n+    }finally{\n+      stats.stopSetup();\n     }\n   }\n ",
                "raw_url": "https://github.com/apache/drill/raw/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/aggregate/StreamingAggBatch.java",
                "sha": "ec12de9f2714b6b3e1e7e341d7062051cb0c542f",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/drill/blob/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java?ref=894037ab693dea425e88fb3ec3aff73ea5b15eb1",
                "deletions": 6,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java",
                "patch": "@@ -134,8 +134,8 @@\n     boolean firstOutputBatch = true;\n \n     IterOutcome leftUpstream = IterOutcome.NONE;\n-    \n-    private HashTableStats htStats = new HashTableStats();\n+\n+    private final HashTableStats htStats = new HashTableStats();\n \n     @Override\n     public int getRecordCount() {\n@@ -171,9 +171,9 @@ public IterOutcome innerNext() {\n                 // Create the run time generated code needed to probe and project\n                 hashJoinProbe = setupHashJoinProbe();\n             }\n-                        \n+\n             // Store the number of records projected\n-            if (hashTable != null \n+            if (hashTable != null\n                 || joinType != JoinRelType.INNER) {\n \n                 // Allocate the memory for the vectors in the output container\n@@ -440,12 +440,13 @@ public HashJoinBatch(HashJoinPOP popConfig, FragmentContext context, RecordBatch\n     }\n \n     private void updateStats(HashTable htable) {\n+      if(htable == null) return;\n       htable.getStats(htStats);\n       this.stats.addLongStat(HashTableMetrics.HTABLE_NUM_BUCKETS, htStats.numBuckets);\n       this.stats.addLongStat(HashTableMetrics.HTABLE_NUM_ENTRIES, htStats.numEntries);\n-      this.stats.addLongStat(HashTableMetrics.HTABLE_NUM_RESIZING, htStats.numResizing);  \n+      this.stats.addLongStat(HashTableMetrics.HTABLE_NUM_RESIZING, htStats.numResizing);\n     }\n-    \n+\n     @Override\n     public void killIncoming() {\n         this.left.kill();",
                "raw_url": "https://github.com/apache/drill/raw/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/HashJoinBatch.java",
                "sha": "c43b99a784e8b97ea956a7c436e683cc8db3788d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/drill/blob/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/MergeJoinBatch.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/MergeJoinBatch.java?ref=894037ab693dea425e88fb3ec3aff73ea5b15eb1",
                "deletions": 2,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/MergeJoinBatch.java",
                "patch": "@@ -166,12 +166,12 @@ public IterOutcome innerNext() {\n           stats.startSetup();\n           this.worker = generateNewWorker();\n           first = true;\n-          stats.stopSetup();\n         } catch (ClassTransformationException | IOException | SchemaChangeException e) {\n-          stats.stopSetup();\n           context.fail(new SchemaChangeException(e));\n           kill();\n           return IterOutcome.STOP;\n+        } finally {\n+          stats.stopSetup();\n         }\n       }\n ",
                "raw_url": "https://github.com/apache/drill/raw/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/physical/impl/join/MergeJoinBatch.java",
                "sha": "e32b653a07eafc1259d4997f5a994bf8d60ea17a",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/drill/blob/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java?ref=894037ab693dea425e88fb3ec3aff73ea5b15eb1",
                "deletions": 3,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java",
                "patch": "@@ -66,9 +66,13 @@ public final IterOutcome next(RecordBatch b) {\n   }\n \n   public final IterOutcome next(int inputIndex, RecordBatch b){\n+    IterOutcome next = null;\n     stats.stopProcessing();\n-    IterOutcome next = b.next();\n-    stats.startProcessing();\n+    try{\n+      next = b.next();\n+    }finally{\n+      stats.startProcessing();\n+    }\n \n     switch(next){\n     case OK_NEW_SCHEMA:\n@@ -138,7 +142,7 @@ public WritableBatch getWritableBatch() {\n     return batch;\n \n   }\n-  \n+\n   @Override\n   public VectorContainer getOutgoingContainer() {\n     throw new UnsupportedOperationException(String.format(\" You should not call getOutgoingContainer() for class %s\", this.getClass().getCanonicalName()));",
                "raw_url": "https://github.com/apache/drill/raw/894037ab693dea425e88fb3ec3aff73ea5b15eb1/exec/java-exec/src/main/java/org/apache/drill/exec/record/AbstractRecordBatch.java",
                "sha": "4c1f82d037d7e50c3f3050b9a21e18811147d245",
                "status": "modified"
            }
        ],
        "message": "Improve OperatorStats to avoid leaking state.  Fix issue where HashJoinBatch throws NPE in stats tracking if we don't have HashTable.",
        "parent": "https://github.com/apache/drill/commit/28992889090c000104bd549230f8a7fe4ee0558c",
        "repo": "drill",
        "unit_tests": [
            "TestHashAggBatch.java"
        ]
    },
    "drill_a26fbec": {
        "bug_id": "drill_a26fbec",
        "commit": "https://github.com/apache/drill/commit/a26fbec13134f249e258be6735b82cf09ab1f406",
        "file": [
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/drill/blob/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRestServer.java",
                "changes": 37,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRestServer.java?ref=a26fbec13134f249e258be6735b82cf09ab1f406",
                "deletions": 4,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRestServer.java",
                "patch": "@@ -29,6 +29,9 @@\n import freemarker.cache.WebappTemplateLoader;\n import freemarker.core.HTMLOutputFormat;\n import freemarker.template.Configuration;\n+import io.netty.channel.ChannelPromise;\n+import io.netty.channel.DefaultChannelPromise;\n+import io.netty.util.concurrent.EventExecutor;\n import org.apache.drill.common.config.DrillConfig;\n import org.apache.drill.exec.ExecConstants;\n import org.apache.drill.exec.memory.BufferAllocator;\n@@ -108,10 +111,17 @@ public DrillRestServer(final WorkManager workManager, final ServletContext servl\n     provider.setMapper(workManager.getContext().getLpPersistence().getMapper());\n     register(provider);\n \n+    // Get an EventExecutor out of the BitServer EventLoopGroup to notify listeners for WebUserConnection. For\n+    // actual connections between Drillbits this EventLoopGroup is used to handle network related events. Though\n+    // there is no actual network connection associated with WebUserConnection but we need a CloseFuture in\n+    // WebSessionResources, so we are using EvenExecutor from network EventLoopGroup pool.\n+    final EventExecutor executor = workManager.getContext().getBitLoopGroup().next();\n+\n     register(new AbstractBinder() {\n       @Override\n       protected void configure() {\n         bind(workManager).to(WorkManager.class);\n+        bind(executor).to(EventExecutor.class);\n         bind(workManager.getContext().getLpPersistence().getMapper()).to(ObjectMapper.class);\n         bind(workManager.getContext().getStoreProvider()).to(PersistentStoreProvider.class);\n         bind(workManager.getContext().getStorage()).to(StoragePluginRegistry.class);\n@@ -159,6 +169,9 @@ private Configuration getFreemarkerConfiguration(ServletContext servletContext)\n     @Inject\n     WorkManager workManager;\n \n+    @Inject\n+    EventExecutor executor;\n+\n     @SuppressWarnings(\"resource\")\n     @Override\n     public WebUserConnection provide() {\n@@ -204,9 +217,15 @@ public WebUserConnection provide() {\n                 config.getLong(ExecConstants.HTTP_SESSION_MEMORY_RESERVATION),\n                 config.getLong(ExecConstants.HTTP_SESSION_MEMORY_MAXIMUM));\n \n+        // Create a dummy close future which is needed by Foreman only. Foreman uses this future to add a close\n+        // listener to known about channel close event from underlying layer. We use this future to notify Foreman\n+        // listeners when the Web session (not connection) between Web Client and WebServer is closed. This will help\n+        // Foreman to cancel all the running queries for this Web Client.\n+        final ChannelPromise closeFuture = new DefaultChannelPromise(null, executor);\n+\n         // Create a WebSessionResource instance which owns the lifecycle of all the session resources.\n-        // Set this instance as an attribute of HttpSession, since it will be used until session is destroyed.\n-        webSessionResources = new WebSessionResources(sessionAllocator, remoteAddress, drillUserSession);\n+        // Set this instance as an attribute of HttpSession, since it will be used until session is destroyed\n+        webSessionResources = new WebSessionResources(sessionAllocator, remoteAddress, drillUserSession, closeFuture);\n         session.setAttribute(WebSessionResources.class.getSimpleName(), webSessionResources);\n       }\n       // Create a new WebUserConnection for the request\n@@ -227,6 +246,9 @@ public void dispose(WebUserConnection instance) {\n     @Inject\n     WorkManager workManager;\n \n+    @Inject\n+    EventExecutor executor;\n+\n     @SuppressWarnings(\"resource\")\n     @Override\n     public WebUserConnection provide() {\n@@ -260,8 +282,15 @@ public WebUserConnection provide() {\n         logger.trace(\"Failed to get the remote address of the http session request\", ex);\n       }\n \n-      final WebSessionResources webSessionResources = new WebSessionResources(sessionAllocator,\n-              remoteAddress, drillUserSession);\n+      // Create a dummy close future which is needed by Foreman only. Foreman uses this future to add a close\n+      // listener to known about channel close event from underlying layer.\n+      //\n+      // The invocation of this close future is no-op as it will be triggered after query completion in unsecure case.\n+      // But we need this close future as it's expected by Foreman.\n+      final ChannelPromise closeFuture = new DefaultChannelPromise(null, executor);\n+\n+      final WebSessionResources webSessionResources = new WebSessionResources(sessionAllocator, remoteAddress,\n+          drillUserSession, closeFuture);\n \n       // Create a AnonWenUserConnection for this request\n       return new AnonWebUserConnection(webSessionResources);",
                "raw_url": "https://github.com/apache/drill/raw/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/DrillRestServer.java",
                "sha": "15458478df958d7923a2ad968cf754c2808d4ee8",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/drill/blob/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebSessionResources.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebSessionResources.java?ref=a26fbec13134f249e258be6735b82cf09ab1f406",
                "deletions": 6,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebSessionResources.java",
                "patch": "@@ -19,7 +19,6 @@\n package org.apache.drill.exec.server.rest;\n \n import io.netty.channel.ChannelPromise;\n-import io.netty.channel.DefaultChannelPromise;\n import org.apache.drill.common.AutoCloseables;\n import org.apache.drill.exec.memory.BufferAllocator;\n import org.apache.drill.exec.rpc.ChannelClosedException;\n@@ -43,11 +42,12 @@\n \n   private ChannelPromise closeFuture;\n \n-  WebSessionResources(BufferAllocator allocator, SocketAddress remoteAddress, UserSession userSession) {\n+  WebSessionResources(BufferAllocator allocator, SocketAddress remoteAddress,\n+                      UserSession userSession, ChannelPromise closeFuture) {\n     this.allocator = allocator;\n     this.remoteAddress = remoteAddress;\n     this.webUserSession = userSession;\n-    closeFuture = new DefaultChannelPromise(null);\n+    this.closeFuture = closeFuture;\n   }\n \n   public UserSession getSession() {\n@@ -68,16 +68,20 @@ public SocketAddress getRemoteAddress() {\n \n   @Override\n   public void close() {\n-\n     try {\n       AutoCloseables.close(webUserSession, allocator);\n     } catch (Exception ex) {\n       logger.error(\"Failure while closing the session resources\", ex);\n     }\n \n-    // Set the close future associated with this session.\n+    // Notify all the listeners of this closeFuture for failure events so that listeners can do cleanup related to this\n+    // WebSession. This will be called after every query execution by AnonymousWebUserConnection::cleanupSession and\n+    // for authenticated user it is called when session is invalidated.\n+    // For authenticated user it will cancel the in-flight queries based on session invalidation. Whereas for\n+    // unauthenticated user it's a no-op since there is no session associated with it. We don't have mechanism currently\n+    // to call this close future upon Http connection close.\n     if (closeFuture != null) {\n-      closeFuture.setFailure(new ChannelClosedException(\"Http Session of the user is closed.\"));\n+      closeFuture.setFailure(new ChannelClosedException(\"Http connection is closed by Web Client\"));\n       closeFuture = null;\n     }\n   }",
                "raw_url": "https://github.com/apache/drill/raw/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebSessionResources.java",
                "sha": "2ca457c02e142e6ca6c53d1d37398436690484ac",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/drill/blob/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebUserConnection.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebUserConnection.java?ref=a26fbec13134f249e258be6735b82cf09ab1f406",
                "deletions": 3,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebUserConnection.java",
                "patch": "@@ -42,9 +42,14 @@\n import java.util.Set;\n \n /**\n- * WebUserConnectionWrapper which represents the UserClientConnection for the WebUser submitting the query. It provides\n- * access to the UserSession executing the query. There is no actual physical channel corresponding to this connection\n- * wrapper.\n+ * WebUserConnectionWrapper which represents the UserClientConnection between WebServer and Foreman, for the WebUser\n+ * submitting the query. It provides access to the UserSession executing the query. There is no actual physical\n+ * channel corresponding to this connection wrapper.\n+ *\n+ * It returns a close future with no actual underlying {@link io.netty.channel.Channel} associated with it but do have an\n+ * EventExecutor out of BitServer EventLoopGroup. Since there is no actual connection established using this class,\n+ * hence the close event will never be fired by underlying layer and close future is set only when the\n+ * {@link WebSessionResources} are closed.\n  */\n \n public class WebUserConnection extends AbstractDisposableUserClientConnection implements ConnectionThrottle {",
                "raw_url": "https://github.com/apache/drill/raw/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/main/java/org/apache/drill/exec/server/rest/WebUserConnection.java",
                "sha": "f46b5e5e69b4758bb3866e84965213feefdd4648",
                "status": "modified"
            },
            {
                "additions": 168,
                "blob_url": "https://github.com/apache/drill/blob/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/test/java/org/apache/drill/exec/server/rest/WebSessionResourcesTest.java",
                "changes": 168,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/test/java/org/apache/drill/exec/server/rest/WebSessionResourcesTest.java?ref=a26fbec13134f249e258be6735b82cf09ab1f406",
                "deletions": 0,
                "filename": "exec/java-exec/src/test/java/org/apache/drill/exec/server/rest/WebSessionResourcesTest.java",
                "patch": "@@ -0,0 +1,168 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.drill.exec.server.rest;\n+\n+import io.netty.channel.ChannelPromise;\n+import io.netty.channel.DefaultChannelPromise;\n+import io.netty.util.concurrent.EventExecutor;\n+import io.netty.util.concurrent.Future;\n+import io.netty.util.concurrent.GenericFutureListener;\n+import org.apache.drill.exec.memory.BufferAllocator;\n+import org.apache.drill.exec.rpc.TransportCheck;\n+import org.apache.drill.exec.rpc.user.UserSession;\n+import org.junit.Test;\n+\n+import java.net.SocketAddress;\n+import java.util.concurrent.CountDownLatch;\n+\n+import static org.junit.Assert.assertTrue;\n+import static org.junit.Assert.fail;\n+import static org.mockito.Matchers.any;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.verify;\n+\n+/**\n+ * Validates {@link WebSessionResources} close works as expected w.r.t {@link io.netty.channel.AbstractChannel.CloseFuture}\n+ * associated with it.\n+ */\n+public class WebSessionResourcesTest {\n+  //private static final org.slf4j.Logger logger = org.slf4j.LoggerFactory.getLogger(WebSessionResourcesTest.class);\n+\n+  private WebSessionResources webSessionResources;\n+\n+  private boolean listenerComplete;\n+\n+  private CountDownLatch latch;\n+\n+  private EventExecutor executor;\n+\n+  // A close listener added in close future in one of the test to see if it's invoked correctly.\n+  private class TestClosedListener implements GenericFutureListener<Future<Void>> {\n+    @Override\n+    public void operationComplete(Future<Void> future) throws Exception {\n+      listenerComplete = true;\n+      latch.countDown();\n+    }\n+  }\n+\n+  /**\n+   * Validates {@link WebSessionResources#close()} throws NPE when closefuture passed to WebSessionResources doesn't\n+   * have a valid channel and EventExecutor associated with it.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testChannelPromiseWithNullExecutor() throws Exception {\n+    try {\n+      ChannelPromise closeFuture = new DefaultChannelPromise(null);\n+      webSessionResources = new WebSessionResources(mock(BufferAllocator.class), mock(SocketAddress.class), mock\n+          (UserSession.class), closeFuture);\n+      webSessionResources.close();\n+      fail();\n+    } catch (Exception e) {\n+      assertTrue(e instanceof NullPointerException);\n+      verify(webSessionResources.getAllocator()).close();\n+      verify(webSessionResources.getSession()).close();\n+    }\n+  }\n+\n+  /**\n+   * Validates successful {@link WebSessionResources#close()} with valid CloseFuture and other parameters.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testChannelPromiseWithValidExecutor() throws Exception {\n+    try {\n+      EventExecutor mockExecutor = mock(EventExecutor.class);\n+      ChannelPromise closeFuture = new DefaultChannelPromise(null, mockExecutor);\n+      webSessionResources = new WebSessionResources(mock(BufferAllocator.class), mock(SocketAddress.class), mock\n+          (UserSession.class), closeFuture);\n+      webSessionResources.close();\n+      verify(webSessionResources.getAllocator()).close();\n+      verify(webSessionResources.getSession()).close();\n+      verify(mockExecutor).inEventLoop();\n+      verify(mockExecutor).execute(any(Runnable.class));\n+      assertTrue(webSessionResources.getCloseFuture() == null);\n+      assertTrue(!listenerComplete);\n+    } catch (Exception e) {\n+      fail();\n+    }\n+  }\n+\n+  /**\n+   * Validates double call to {@link WebSessionResources#close()} doesn't throw any exception.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testDoubleClose() throws Exception {\n+    try {\n+      ChannelPromise closeFuture = new DefaultChannelPromise(null, mock(EventExecutor.class));\n+      webSessionResources = new WebSessionResources(mock(BufferAllocator.class), mock(SocketAddress.class), mock\n+          (UserSession.class), closeFuture);\n+      webSessionResources.close();\n+\n+      verify(webSessionResources.getAllocator()).close();\n+      verify(webSessionResources.getSession()).close();\n+      assertTrue(webSessionResources.getCloseFuture() == null);\n+\n+      webSessionResources.close();\n+    } catch (Exception e) {\n+      fail();\n+    }\n+  }\n+\n+  /**\n+   * Validates successful {@link WebSessionResources#close()} with valid CloseFuture and {@link TestClosedListener}\n+   * getting invoked which is added to the close future.\n+   * @throws Exception\n+   */\n+  @Test\n+  public void testCloseWithListener() throws Exception {\n+    try {\n+      // Assign latch, executor and closeListener for this test case\n+      GenericFutureListener<Future<Void>> closeListener = new TestClosedListener();\n+      latch = new CountDownLatch(1);\n+      executor = TransportCheck.createEventLoopGroup(1, \"Test-Thread\").next();\n+      ChannelPromise closeFuture = new DefaultChannelPromise(null, executor);\n+\n+      // create WebSessionResources with above ChannelPromise to notify listener\n+      webSessionResources = new WebSessionResources(mock(BufferAllocator.class), mock(SocketAddress.class),\n+          mock(UserSession.class), closeFuture);\n+\n+      // Add the Test Listener to close future\n+      assertTrue(!listenerComplete);\n+      closeFuture.addListener(closeListener);\n+\n+      // Close the WebSessionResources\n+      webSessionResources.close();\n+\n+      // Verify the states\n+      verify(webSessionResources.getAllocator()).close();\n+      verify(webSessionResources.getSession()).close();\n+      assertTrue(webSessionResources.getCloseFuture() == null);\n+\n+      // Since listener will be invoked so test should not wait forever\n+      latch.await();\n+      assertTrue(listenerComplete);\n+    } catch (Exception e) {\n+      fail();\n+    } finally {\n+      listenerComplete = false;\n+      executor.shutdownGracefully();\n+    }\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/drill/raw/a26fbec13134f249e258be6735b82cf09ab1f406/exec/java-exec/src/test/java/org/apache/drill/exec/server/rest/WebSessionResourcesTest.java",
                "sha": "bb990de6ef28abac7e9c862a508784b7997a9dcb",
                "status": "added"
            }
        ],
        "message": "DRILL-5874: NPE in AnonWebUserConnection.cleanupSession()\n\ncloses #993",
        "parent": "https://github.com/apache/drill/commit/8eda4d7749c129c692f9e57db4c2a755a9139052",
        "repo": "drill",
        "unit_tests": [
            "WebSessionResourcesTest.java"
        ]
    },
    "drill_a459e4d": {
        "bug_id": "drill_a459e4d",
        "commit": "https://github.com/apache/drill/commit/a459e4dbbc242fe0d06b221afe5f830780fc682e",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/a459e4dbbc242fe0d06b221afe5f830780fc682e/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetReaderUtility.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetReaderUtility.java?ref=a459e4dbbc242fe0d06b221afe5f830780fc682e",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetReaderUtility.java",
                "patch": "@@ -196,7 +196,7 @@ public static DateCorruptionStatus detectCorruptDates(ParquetMetadata footer,\n     } else {\n       // Possibly an old, un-migrated Drill file, check the column statistics to see if min/max values look corrupt\n       // only applies if there is a date column selected\n-      if (createdBy.equals(\"parquet-mr\")) {\n+      if (createdBy == null || createdBy.equals(\"parquet-mr\")) {\n         // loop through parquet column metadata to find date columns, check for corrupt values\n         return checkForCorruptDateValuesInStatistics(footer, columns, autoCorrectCorruptDates);\n       } else {",
                "raw_url": "https://github.com/apache/drill/raw/a459e4dbbc242fe0d06b221afe5f830780fc682e/exec/java-exec/src/main/java/org/apache/drill/exec/store/parquet/ParquetReaderUtility.java",
                "sha": "767c98de8480cae7bd199ae06103bebdc05e0a27",
                "status": "modified"
            }
        ],
        "message": "DRILL-5400: Fix NPE in corrupt date detection\n\nThis closes #646",
        "parent": "https://github.com/apache/drill/commit/ee3489ce3b6e5ad53e5c3a59b6e2e4e50773c630",
        "repo": "drill",
        "unit_tests": [
            "TestParquetReaderUtility.java"
        ]
    },
    "drill_c8a08c3": {
        "bug_id": "drill_c8a08c3",
        "commit": "https://github.com/apache/drill/commit/c8a08c3e793d53ae4c445f07caa639269c8b51b7",
        "file": [
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/drill/blob/c8a08c3e793d53ae4c445f07caa639269c8b51b7/common/src/main/java/org/apache/drill/common/expression/IfExpression.java",
                "changes": 21,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/common/src/main/java/org/apache/drill/common/expression/IfExpression.java?ref=c8a08c3e793d53ae4c445f07caa639269c8b51b7",
                "deletions": 1,
                "filename": "common/src/main/java/org/apache/drill/common/expression/IfExpression.java",
                "patch": "@@ -23,7 +23,11 @@\n \n import org.apache.drill.common.expression.IfExpression.IfCondition;\n import org.apache.drill.common.expression.visitors.ExprVisitor;\n+import org.apache.drill.common.types.TypeProtos;\n+import org.apache.drill.common.types.TypeProtos.DataMode;\n import org.apache.drill.common.types.TypeProtos.MajorType;\n+import org.apache.drill.common.types.TypeProtos.MinorType;\n+import org.apache.drill.common.types.Types;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -102,7 +106,22 @@ public IfExpression build(){\n \n   @Override\n   public MajorType getMajorType() {\n-    return this.elseExpression.getMajorType();\n+    // If the return type of one of the \"then\" expression or \"else\" expression is nullable, return \"if\" expression\n+    // type as nullable\n+    MajorType majorType = elseExpression.getMajorType();\n+    if (majorType.getMode() == DataMode.OPTIONAL) {\n+      return majorType;\n+    }\n+\n+    for(IfCondition condition : conditions) {\n+      if (condition.expression.getMajorType().getMode() == DataMode.OPTIONAL) {\n+        assert condition.expression.getMajorType().getMinorType() == majorType.getMinorType();\n+\n+        return condition.expression.getMajorType();\n+      }\n+    }\n+\n+    return majorType;\n   }\n \n   public static Builder newBuilder(){",
                "raw_url": "https://github.com/apache/drill/raw/c8a08c3e793d53ae4c445f07caa639269c8b51b7/common/src/main/java/org/apache/drill/common/expression/IfExpression.java",
                "sha": "d1df7f7eb68ab3cf58705dc811e889058e6dc603",
                "status": "modified"
            },
            {
                "additions": 80,
                "blob_url": "https://github.com/apache/drill/blob/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/java-exec/src/main/codegen/templates/ConvertToNullableHolder.java",
                "changes": 80,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/codegen/templates/ConvertToNullableHolder.java?ref=c8a08c3e793d53ae4c445f07caa639269c8b51b7",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/codegen/templates/ConvertToNullableHolder.java",
                "patch": "@@ -0,0 +1,80 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+<@pp.dropOutputFile />\n+<#list vv.types as type>\n+<#list type.minor as minor>\n+\n+<#assign className=\"GConvertToNullable${minor.class}Holder\" />\n+\n+<@pp.changeOutputFile name=\"/org/apache/drill/exec/expr/fn/impl/${className}.java\" />\n+\n+<#include \"/@includes/license.ftl\" />\n+\n+package org.apache.drill.exec.expr.fn.impl;\n+\n+import org.apache.drill.exec.expr.DrillSimpleFunc;\n+import org.apache.drill.exec.expr.annotations.*;\n+import org.apache.drill.exec.expr.holders.*;\n+import org.apache.drill.exec.record.RecordBatch;\n+\n+@FunctionTemplate(name = \"convertToNullable${minor.class?upper_case}\", scope = FunctionTemplate.FunctionScope.SIMPLE, nulls = FunctionTemplate.NullHandling.INTERNAL)\n+public class ${className} implements DrillSimpleFunc {\n+\n+  @Param ${minor.class}Holder input;\n+  @Output Nullable${minor.class}Holder output;\n+\n+  public void setup(RecordBatch incoming) { }\n+\n+  public void eval() {\n+    output.isSet = 1;\n+<#if type.major != \"VarLen\">\n+  <#if (minor.class == \"TimeStampTZ\")>\n+    output.value = input.value;\n+    output.index = input.index;\n+  <#elseif (minor.class == \"Interval\")>\n+    output.months = input.months;\n+    output.days = input.days;\n+    output.milliSeconds = input.milliSeconds;\n+  <#elseif (minor.class == \"IntervalDay\")>\n+    output.days = input.days;\n+    output.milliSeconds = input.milliSeconds;\n+  <#elseif minor.class.startsWith(\"Decimal\")>\n+    output.scale = input.scale;\n+    output.precision = input.precision;\n+    <#if minor.class.startsWith(\"Decimal28\") || minor.class.startsWith(\"Decimal38\")>\n+    output.sign = input.sign;\n+    output.start = input.start;\n+    output.buffer = input.buffer;\n+    <#else>\n+    output.value = input.value;\n+    </#if>\n+  <#elseif (type.width > 8)>\n+    output.start = input.start;\n+    output.buffer = input.buffer;\n+  <#else>\n+    output.value = input.value;\n+  </#if>\n+<#else>\n+    output.start = input.start;\n+    output.end = input.end;\n+    output.buffer = input.buffer;\n+</#if>\n+  }\n+}\n+</#list>\n+</#list>\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/drill/raw/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/java-exec/src/main/codegen/templates/ConvertToNullableHolder.java",
                "sha": "548d64550c0a5f8b7d2c7d26f3cdfce07d59e54b",
                "status": "added"
            },
            {
                "additions": 36,
                "blob_url": "https://github.com/apache/drill/blob/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/java-exec/src/main/java/org/apache/drill/exec/expr/ExpressionTreeMaterializer.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/expr/ExpressionTreeMaterializer.java?ref=c8a08c3e793d53ae4c445f07caa639269c8b51b7",
                "deletions": 0,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/expr/ExpressionTreeMaterializer.java",
                "patch": "@@ -284,9 +284,45 @@ public boolean apply(LogicalExpression input) {\n         }\n       }\n \n+      // If the type of the IF expression is nullable, apply a convertToNullable*Holder function for \"THEN\"/\"ELSE\"\n+      // expressions whose type is not nullable.\n+      if (IfExpression.newBuilder().setElse(newElseExpr).addConditions(conditions).build().getMajorType().getMode()\n+          == DataMode.OPTIONAL) {\n+        for (int i = 0; i < conditions.size(); ++i) {\n+          IfExpression.IfCondition condition = conditions.get(i);\n+          if (condition.expression.getMajorType().getMode() != DataMode.OPTIONAL) {\n+            conditions.set(i, new IfExpression.IfCondition(condition.condition,\n+                getConvertToNullableExpr(ImmutableList.of(condition.expression),\n+                    condition.expression.getMajorType().getMinorType(), registry)));\n+          }\n+        }\n+\n+        if (newElseExpr.getMajorType().getMode() != DataMode.OPTIONAL) {\n+          newElseExpr = getConvertToNullableExpr(ImmutableList.of(newElseExpr),\n+              newElseExpr.getMajorType().getMinorType(), registry);\n+        }\n+      }\n+\n       return validateNewExpr(IfExpression.newBuilder().setElse(newElseExpr).addConditions(conditions).build());\n     }\n \n+    private LogicalExpression getConvertToNullableExpr(List<LogicalExpression> args, MinorType minorType,\n+        FunctionImplementationRegistry registry) {\n+      String funcName = \"convertToNullable\" + minorType.toString();\n+      FunctionCall funcCall = new FunctionCall(funcName, args, ExpressionPosition.UNKNOWN);\n+      FunctionResolver resolver = FunctionResolverFactory.getResolver(funcCall);\n+\n+      DrillFuncHolder matchedConvertToNullableFuncHolder =\n+          resolver.getBestMatch(registry.getDrillRegistry().getMethods().get(funcName), funcCall);\n+\n+      if (matchedConvertToNullableFuncHolder == null) {\n+        logFunctionResolutionError(errorCollector, funcCall);\n+        return NullExpression.INSTANCE;\n+      }\n+\n+      return new DrillFuncHolderExpr(funcName, matchedConvertToNullableFuncHolder, args, ExpressionPosition.UNKNOWN);\n+    }\n+\n     private LogicalExpression rewriteNullExpression(LogicalExpression expr, MajorType type) {\n       if(expr instanceof NullExpression) {\n         return new TypedNullConstant(type);",
                "raw_url": "https://github.com/apache/drill/raw/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/java-exec/src/main/java/org/apache/drill/exec/expr/ExpressionTreeMaterializer.java",
                "sha": "fca743bb2d308aaef6af763dc9ae34b44a8a58bc",
                "status": "modified"
            },
            {
                "additions": 61,
                "blob_url": "https://github.com/apache/drill/blob/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java",
                "changes": 82,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java?ref=c8a08c3e793d53ae4c445f07caa639269c8b51b7",
                "deletions": 21,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java",
                "patch": "@@ -338,21 +338,40 @@ private LogicalExpression getDrillFunctionFromOptiqCall(RexCall call) {\n     public LogicalExpression visitLiteral(RexLiteral literal) {\n       switch(literal.getType().getSqlTypeName()){\n       case BIGINT:\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.BIGINT);\n+        }\n         long l = ((BigDecimal) literal.getValue()).longValue();\n-        return checkNullLiteral(literal, MinorType.BIGINT, ValueExpressions.getBigInt(l));\n+        return ValueExpressions.getBigInt(l);\n       case BOOLEAN:\n-        return checkNullLiteral(literal, MinorType.BIT, ValueExpressions.getBit(((Boolean) literal.getValue())));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.BIT);\n+        }\n+        return ValueExpressions.getBit(((Boolean) literal.getValue()));\n       case CHAR:\n-        return checkNullLiteral(literal, MinorType.VARCHAR, ValueExpressions.getChar(((NlsString) literal.getValue()).getValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.VARCHAR);\n+        }\n+        return ValueExpressions.getChar(((NlsString)literal.getValue()).getValue());\n       case DOUBLE:\n+        if (isLiteralNull(literal)){\n+          return createNullExpr(MinorType.FLOAT8);\n+        }\n         double d = ((BigDecimal) literal.getValue()).doubleValue();\n-        return checkNullLiteral(literal, MinorType.FLOAT8, ValueExpressions.getFloat8(d));\n+        return ValueExpressions.getFloat8(d);\n       case FLOAT:\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.FLOAT4);\n+        }\n         float f = ((BigDecimal) literal.getValue()).floatValue();\n-        return checkNullLiteral(literal, MinorType.FLOAT4, ValueExpressions.getFloat4(f));\n+        return ValueExpressions.getFloat4(f);\n       case INTEGER:\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.INT);\n+        }\n         int a = ((BigDecimal) literal.getValue()).intValue();\n-        return checkNullLiteral(literal, MinorType.INT, ValueExpressions.getInt(a));\n+        return ValueExpressions.getInt(a);\n+\n       case DECIMAL:\n         /* TODO: Enable using Decimal literals once we have more functions implemented for Decimal\n          * For now continue using Double instead of decimals\n@@ -367,24 +386,49 @@ public LogicalExpression visitLiteral(RexLiteral literal) {\n         } else if (precision <= 38) {\n             return ValueExpressions.getDecimal38((BigDecimal)literal.getValue());\n         } */\n-\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.FLOAT8);\n+        }\n         double dbl = ((BigDecimal) literal.getValue()).doubleValue();\n         logger.warn(\"Converting exact decimal into approximate decimal.  Should be fixed once decimal is implemented.\");\n-        return checkNullLiteral(literal, MinorType.FLOAT8, ValueExpressions.getFloat8(dbl));\n+        return ValueExpressions.getFloat8(dbl);\n       case VARCHAR:\n-        return checkNullLiteral(literal, MinorType.VARCHAR, ValueExpressions.getChar(((NlsString) literal.getValue()).getValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.VARCHAR);\n+        }\n+        return ValueExpressions.getChar(((NlsString)literal.getValue()).getValue());\n       case SYMBOL:\n-        return checkNullLiteral(literal, MinorType.VARCHAR, ValueExpressions.getChar(literal.getValue().toString()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.VARCHAR);\n+        }\n+        return ValueExpressions.getChar(literal.getValue().toString());\n       case DATE:\n-        return checkNullLiteral(literal, MinorType.DATE, ValueExpressions.getDate((GregorianCalendar) literal.getValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.DATE);\n+        }\n+        return (ValueExpressions.getDate((GregorianCalendar)literal.getValue()));\n       case TIME:\n-        return checkNullLiteral(literal, MinorType.TIME, ValueExpressions.getTime((GregorianCalendar) literal.getValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.TIME);\n+        }\n+        return (ValueExpressions.getTime((GregorianCalendar)literal.getValue()));\n       case TIMESTAMP:\n-        return checkNullLiteral(literal, MinorType.TIMESTAMP, ValueExpressions.getTimeStamp((GregorianCalendar) literal.getValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.TIMESTAMP);\n+        }\n+        return (ValueExpressions.getTimeStamp((GregorianCalendar) literal.getValue()));\n       case INTERVAL_YEAR_MONTH:\n-        return checkNullLiteral(literal, MinorType.INTERVALYEAR, ValueExpressions.getIntervalYear(((BigDecimal) (literal.getValue())).intValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.INTERVALYEAR);\n+        }\n+        return (ValueExpressions.getIntervalYear(((BigDecimal) (literal.getValue())).intValue()));\n       case INTERVAL_DAY_TIME:\n-        return checkNullLiteral(literal, MinorType.INTERVALDAY, ValueExpressions.getIntervalDay(((BigDecimal) (literal.getValue())).longValue()));\n+        if (isLiteralNull(literal)) {\n+          return createNullExpr(MinorType.INTERVALDAY);\n+        }\n+        return (ValueExpressions.getIntervalDay(((BigDecimal) (literal.getValue())).longValue()));\n+      case NULL:\n+        return NullExpression.INSTANCE;\n       case ANY:\n         if (isLiteralNull(literal)) {\n           return NullExpression.INSTANCE;\n@@ -395,12 +439,8 @@ public LogicalExpression visitLiteral(RexLiteral literal) {\n     }\n   }\n \n-  private static LogicalExpression checkNullLiteral(RexLiteral literal, MinorType type, LogicalExpression orExpr) {\n-    if(isLiteralNull(literal)) {\n-      return new TypedNullConstant(Types.optional(type));\n-    } else {\n-      return orExpr;\n-    }\n+  private static final TypedNullConstant createNullExpr(MinorType type) {\n+    return new TypedNullConstant(Types.optional(type));\n   }\n \n   private static boolean isLiteralNull(RexLiteral literal) {",
                "raw_url": "https://github.com/apache/drill/raw/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/java-exec/src/main/java/org/apache/drill/exec/planner/logical/DrillOptiq.java",
                "sha": "8966f18aa2cf1977909966026e7ae5e83427f544",
                "status": "modified"
            },
            {
                "additions": 48,
                "blob_url": "https://github.com/apache/drill/blob/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/TestJdbcQuery.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/TestJdbcQuery.java?ref=c8a08c3e793d53ae4c445f07caa639269c8b51b7",
                "deletions": 0,
                "filename": "exec/jdbc/src/test/java/org/apache/drill/jdbc/test/TestJdbcQuery.java",
                "patch": "@@ -947,4 +947,52 @@ public Void apply(Connection connection) {\n       }\n     });\n   }\n+\n+  @Test\n+  public void testCaseWithNoElse() throws Exception {\n+    JdbcAssert.withNoDefaultSchema()\n+        .sql(\"SELECT employee_id, CASE WHEN employee_id < 100 THEN first_name END from cp.`employee.json` \" +\n+            \"WHERE employee_id = 99 OR employee_id = 100\")\n+        .returns(\n+            \"employee_id=99; EXPR$1=Elizabeth\\n\" +\n+            \"employee_id=100; EXPR$1=null\\n\"\n+        );\n+  }\n+\n+  @Test\n+  public void testCaseWithElse() throws Exception {\n+    JdbcAssert.withNoDefaultSchema()\n+        .sql(\"SELECT employee_id, CASE WHEN employee_id < 100 THEN first_name ELSE 'Test' END from cp.`employee.json` \" +\n+            \"WHERE employee_id = 99 OR employee_id = 100\")\n+        .returns(\n+            \"employee_id=99; EXPR$1=Elizabeth\\n\" +\n+            \"employee_id=100; EXPR$1=Test\"\n+        );\n+  }\n+\n+  @Test\n+  public void testCaseWith2ThensAndNoElse() throws Exception {\n+    JdbcAssert.withNoDefaultSchema()\n+        .sql(\"SELECT employee_id, CASE WHEN employee_id < 100 THEN first_name WHEN employee_id = 100 THEN last_name END \" +\n+            \"from cp.`employee.json` \" +\n+            \"WHERE employee_id = 99 OR employee_id = 100 OR employee_id = 101\")\n+        .returns(\n+            \"employee_id=99; EXPR$1=Elizabeth\\n\" +\n+            \"employee_id=100; EXPR$1=Hunt\\n\" +\n+            \"employee_id=101; EXPR$1=null\"\n+        );\n+  }\n+\n+  @Test\n+  public void testCaseWith2ThensAndElse() throws Exception {\n+    JdbcAssert.withNoDefaultSchema()\n+        .sql(\"SELECT employee_id, CASE WHEN employee_id < 100 THEN first_name WHEN employee_id = 100 THEN last_name ELSE 'Test' END \" +\n+            \"from cp.`employee.json` \" +\n+            \"WHERE employee_id = 99 OR employee_id = 100 OR employee_id = 101\")\n+        .returns(\n+            \"employee_id=99; EXPR$1=Elizabeth\\n\" +\n+            \"employee_id=100; EXPR$1=Hunt\\n\" +\n+            \"employee_id=101; EXPR$1=Test\\n\"\n+        );\n+  }\n }",
                "raw_url": "https://github.com/apache/drill/raw/c8a08c3e793d53ae4c445f07caa639269c8b51b7/exec/jdbc/src/test/java/org/apache/drill/jdbc/test/TestJdbcQuery.java",
                "sha": "e6842286b89f7106cf32f598decc0cc511676b59",
                "status": "modified"
            }
        ],
        "message": "DRILL-665: Handle null values in case expressions (contd).\n\n1. Added functions for converting REQUIRED holder into NULLABLE holder where the minorType is same.\n2. Update in Optiq->Drill literal conversion. First check if it null type, before parsing the literal value. Parsing literal value will cause NPE if the type is NULL.\n3. Changed getReturnType of IfExpression to consider the nullable types of THEN and ELSE expressions.\n4. Added testcases.",
        "parent": "https://github.com/apache/drill/commit/2cdbd6000abc33965f1f7b960b954fd6a4f7b58f",
        "repo": "drill",
        "unit_tests": [
            "ExpressionTreeMaterializerTest.java",
            "DrillOptiqTest.java"
        ]
    },
    "drill_d44b889": {
        "bug_id": "drill_d44b889",
        "commit": "https://github.com/apache/drill/commit/d44b889ffebbb58939110776795489c052aac786",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/drill/blob/d44b889ffebbb58939110776795489c052aac786/exec/java-exec/src/main/java/org/apache/drill/exec/record/RecordIterator.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/record/RecordIterator.java?ref=d44b889ffebbb58939110776795489c052aac786",
                "deletions": 6,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/record/RecordIterator.java",
                "patch": "@@ -93,7 +93,7 @@ public void mark() {\n     // Release all batches before current batch. [0 to startBatchPosition).\n     final Map<Range<Long>,RecordBatchData> oldBatches = batches.subRangeMap(Range.closedOpen(0l, startBatchPosition)).asMapOfRanges();\n     for (Range<Long> range : oldBatches.keySet()) {\n-      oldBatches.get(range.lowerEndpoint()).clear();\n+      oldBatches.get(range).clear();\n     }\n     batches.remove(Range.closedOpen(0l, startBatchPosition));\n     markedInnerPosition = innerPosition;\n@@ -113,8 +113,9 @@ public void reset() {\n       }\n       innerPosition = markedInnerPosition;\n       outerPosition = markedOuterPosition;\n-      startBatchPosition = batches.getEntry(outerPosition).getKey().lowerEndpoint();\n-      innerRecordCount = (int)(batches.getEntry(outerPosition).getKey().upperEndpoint() - startBatchPosition);\n+      final Range<Long> markedBatchRange = batches.getEntry(outerPosition).getKey();\n+      startBatchPosition = markedBatchRange.lowerEndpoint();\n+      innerRecordCount = (int)(markedBatchRange.upperEndpoint() - startBatchPosition);\n       markedInnerPosition = -1;\n       markedOuterPosition = -1;\n     }\n@@ -133,9 +134,10 @@ public void forward(long delta) {\n     // Get vectors from new position.\n     container.transferIn(rbdNew.getContainer());\n     outerPosition = nextOuterPosition;\n-    startBatchPosition = batches.getEntry(outerPosition).getKey().lowerEndpoint();\n+    final Range<Long> markedBatchRange = batches.getEntry(outerPosition).getKey();\n+    startBatchPosition = markedBatchRange.lowerEndpoint();\n     innerPosition = (int)(outerPosition - startBatchPosition);\n-    innerRecordCount = (int)(batches.getEntry(outerPosition).getKey().upperEndpoint() - startBatchPosition);\n+    innerRecordCount = (int)(markedBatchRange.upperEndpoint() - startBatchPosition);\n   }\n \n   /**\n@@ -239,7 +241,9 @@ public long getOuterPosition() {\n \n   public int getCurrentPosition() {\n     Preconditions.checkArgument(initialized);\n-    Preconditions.checkArgument(innerPosition >= 0 && innerPosition < innerRecordCount);\n+    Preconditions.checkArgument(innerPosition >= 0 && innerPosition < innerRecordCount,\n+      String.format(\"innerPosition:%d, outerPosition:%d, innerRecordCount:%d, totalRecordCount:%d\",\n+        innerPosition, outerPosition, innerRecordCount, totalRecordCount));\n     return innerPosition;\n   }\n ",
                "raw_url": "https://github.com/apache/drill/raw/d44b889ffebbb58939110776795489c052aac786/exec/java-exec/src/main/java/org/apache/drill/exec/record/RecordIterator.java",
                "sha": "7b5ec2889760089252cce73e6a88420881bf1955",
                "status": "modified"
            }
        ],
        "message": "DRILL-4109 Fix NPE in RecordIterator.",
        "parent": "https://github.com/apache/drill/commit/46c47a24f67e53146521d50062180c4aa902f687",
        "repo": "drill",
        "unit_tests": [
            "TestRecordIterator.java"
        ]
    },
    "drill_ed51b3f": {
        "bug_id": "drill_ed51b3f",
        "commit": "https://github.com/apache/drill/commit/ed51b3f95ee4eaa3694629b1181b9e8cd13b932e",
        "file": [
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/drill/blob/ed51b3f95ee4eaa3694629b1181b9e8cd13b932e/exec/java-exec/src/main/java/org/apache/drill/exec/client/DumpCat.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/exec/java-exec/src/main/java/org/apache/drill/exec/client/DumpCat.java?ref=ed51b3f95ee4eaa3694629b1181b9e8cd13b932e",
                "deletions": 1,
                "filename": "exec/java-exec/src/main/java/org/apache/drill/exec/client/DumpCat.java",
                "patch": "@@ -54,7 +54,6 @@ public static void main(String args[]) throws Exception {\n       System.out.println(e.getMessage());\n       String[] valid = {\"-f\", \"file\"};\n       new JCommander(o, valid).usage();\n-      jc.usage();\n       System.exit(-1);\n     }\n     if (o.help) {",
                "raw_url": "https://github.com/apache/drill/raw/ed51b3f95ee4eaa3694629b1181b9e8cd13b932e/exec/java-exec/src/main/java/org/apache/drill/exec/client/DumpCat.java",
                "sha": "7e8a4a26dca4f8d621836672e24b3adc7492c967",
                "status": "modified"
            }
        ],
        "message": "DRILL-559: Fix NPE in dumpcat when arguments are not provided",
        "parent": "https://github.com/apache/drill/commit/3f92e56aeb768cac90495951befaa0f03f13372e",
        "repo": "drill",
        "unit_tests": [
            "DumpCatTest.java"
        ]
    },
    "drill_f88a73c": {
        "bug_id": "drill_f88a73c",
        "commit": "https://github.com/apache/drill/commit/f88a73c9e7a75f2d08cc54188816f591b003eff4",
        "file": [
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/drill/blob/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/CryptoFunctions.java",
                "changes": 58,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/CryptoFunctions.java?ref=f88a73c9e7a75f2d08cc54188816f591b003eff4",
                "deletions": 39,
                "filename": "contrib/udfs/src/main/java/org/apache/drill/exec/udfs/CryptoFunctions.java",
                "patch": "@@ -22,10 +22,8 @@\n import org.apache.drill.exec.expr.annotations.FunctionTemplate;\n import org.apache.drill.exec.expr.annotations.Output;\n import org.apache.drill.exec.expr.annotations.Param;\n-import org.apache.drill.exec.expr.annotations.Workspace;\n import org.apache.drill.exec.expr.holders.VarCharHolder;\n \n-import javax.crypto.Cipher;\n import javax.inject.Inject;\n \n public class CryptoFunctions {\n@@ -271,34 +269,25 @@ public void eval() {\n     @Inject\n     DrillBuf buffer;\n \n-    @Workspace\n-    Cipher cipher;\n-\n     @Override\n     public void setup() {\n-      String key = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawKey.start, rawKey.end, rawKey.buffer);\n-\n-      try {\n-        byte[] keyByteArray = key.getBytes(\"UTF-8\");\n-        java.security.MessageDigest sha = java.security.MessageDigest.getInstance(\"SHA-1\");\n-        keyByteArray = sha.digest(keyByteArray);\n-        keyByteArray = java.util.Arrays.copyOf(keyByteArray, 16);\n-        javax.crypto.spec.SecretKeySpec secretKey = new javax.crypto.spec.SecretKeySpec(keyByteArray, \"AES\");\n-\n-        cipher = Cipher.getInstance(\"AES/ECB/PKCS5Padding\");\n-        cipher.init(Cipher.ENCRYPT_MODE, secretKey);\n-      } catch (Exception e) {\n-        //Exceptions are ignored\n-      }\n     }\n \n     @Override\n     public void eval() {\n-\n+      String key = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawKey.start, rawKey.end, rawKey.buffer);\n       String input = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawInput.start, rawInput.end, rawInput.buffer);\n       String encryptedText = \"\";\n       try {\n-        encryptedText = javax.xml.bind.DatatypeConverter.printBase64Binary(cipher.doFinal(input.getBytes(\"UTF-8\")));\n+        byte[] keyByteArray = key.getBytes(java.nio.charset.StandardCharsets.UTF_8);\n+        java.security.MessageDigest sha = java.security.MessageDigest.getInstance(\"SHA-1\");\n+        keyByteArray = sha.digest(keyByteArray);\n+        keyByteArray = java.util.Arrays.copyOf(keyByteArray, 16);\n+        javax.crypto.spec.SecretKeySpec secretKey = new javax.crypto.spec.SecretKeySpec(keyByteArray, \"AES\");\n+\n+        javax.crypto.Cipher cipher = javax.crypto.Cipher.getInstance(\"AES/ECB/PKCS5Padding\");\n+        cipher.init(javax.crypto.Cipher.ENCRYPT_MODE, secretKey);\n+        encryptedText = javax.xml.bind.DatatypeConverter.printBase64Binary(cipher.doFinal(input.getBytes(java.nio.charset.StandardCharsets.UTF_8)));\n       } catch (Exception e) {\n         //Exceptions are ignored\n       }\n@@ -331,33 +320,24 @@ public void eval() {\n     @Inject\n     DrillBuf buffer;\n \n-    @Workspace\n-    Cipher cipher;\n-\n     @Override\n     public void setup() {\n-      String key = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawKey.start, rawKey.end, rawKey.buffer);\n-\n-      try {\n-        byte[] keyByteArray = key.getBytes(\"UTF-8\");\n-        java.security.MessageDigest sha = java.security.MessageDigest.getInstance(\"SHA-1\");\n-        keyByteArray = sha.digest(keyByteArray);\n-        keyByteArray = java.util.Arrays.copyOf(keyByteArray, 16);\n-        javax.crypto.spec.SecretKeySpec secretKey = new javax.crypto.spec.SecretKeySpec(keyByteArray, \"AES\");\n-\n-        cipher = Cipher.getInstance(\"AES/ECB/PKCS5Padding\");\n-        cipher.init(Cipher.DECRYPT_MODE, secretKey);\n-      } catch (Exception e) {\n-        //Exceptions are ignored\n-      }\n     }\n \n     @Override\n     public void eval() {\n-\n+      String key = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawKey.start, rawKey.end, rawKey.buffer);\n       String input = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawInput.start, rawInput.end, rawInput.buffer);\n       String decryptedText = \"\";\n       try {\n+        byte[] keyByteArray = key.getBytes(java.nio.charset.StandardCharsets.UTF_8);\n+        java.security.MessageDigest sha = java.security.MessageDigest.getInstance(\"SHA-1\");\n+        keyByteArray = sha.digest(keyByteArray);\n+        keyByteArray = java.util.Arrays.copyOf(keyByteArray, 16);\n+        javax.crypto.spec.SecretKeySpec secretKey = new javax.crypto.spec.SecretKeySpec(keyByteArray, \"AES\");\n+\n+        javax.crypto.Cipher cipher = javax.crypto.Cipher.getInstance(\"AES/ECB/PKCS5Padding\");\n+        cipher.init(javax.crypto.Cipher.DECRYPT_MODE, secretKey);\n         decryptedText = new String(cipher.doFinal(javax.xml.bind.DatatypeConverter.parseBase64Binary(input)));\n       } catch (Exception e) {\n         //Exceptions are ignored",
                "raw_url": "https://github.com/apache/drill/raw/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/CryptoFunctions.java",
                "sha": "f914fb99a371b72f88072d0a8cf121d308909e26",
                "status": "modified"
            },
            {
                "additions": 109,
                "blob_url": "https://github.com/apache/drill/blob/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/NetworkFunctions.java",
                "changes": 223,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/NetworkFunctions.java?ref=f88a73c9e7a75f2d08cc54188816f591b003eff4",
                "deletions": 114,
                "filename": "contrib/udfs/src/main/java/org/apache/drill/exec/udfs/NetworkFunctions.java",
                "patch": "@@ -24,6 +24,8 @@\n import org.apache.drill.exec.expr.annotations.Param;\n import org.apache.drill.exec.expr.holders.BigIntHolder;\n import org.apache.drill.exec.expr.holders.BitHolder;\n+import org.apache.drill.exec.expr.holders.NullableBigIntHolder;\n+import org.apache.drill.exec.expr.holders.NullableVarCharHolder;\n import org.apache.drill.exec.expr.holders.VarCharHolder;\n \n import javax.inject.Inject;\n@@ -50,18 +52,15 @@ public void setup() {\n \n \n     public void eval() {\n-\n       String ipString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputIP.start, inputIP.end, inputIP.buffer);\n       String cidrString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputCIDR.start, inputCIDR.end, inputCIDR.buffer);\n \n-      int result = 0;\n-      org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n-\n-      if (utils.getInfo().isInRange(ipString)) {\n-        result = 1;\n+      try {\n+        org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n+        out.value = utils.getInfo().isInRange(ipString) ? 1 : 0;\n+      } catch (IllegalArgumentException e) {\n+        // return false in case of invalid input\n       }\n-\n-      out.value = result;\n     }\n   }\n \n@@ -76,17 +75,20 @@ public void eval() {\n     VarCharHolder inputCIDR;\n \n     @Output\n-    BigIntHolder out;\n+    NullableBigIntHolder out;\n \n     public void setup() {\n     }\n \n     public void eval() {\n-\n       String cidrString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputCIDR.start, inputCIDR.end, inputCIDR.buffer);\n-      org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n-\n-      out.value = utils.getInfo().getAddressCountLong();\n+      try {\n+        org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n+        out.value = utils.getInfo().getAddressCountLong();\n+        out.isSet = 1;\n+      } catch (IllegalArgumentException e) {\n+        // return null in case of invalid input\n+      }\n     }\n \n   }\n@@ -101,7 +103,7 @@ public void eval() {\n     VarCharHolder inputCIDR;\n \n     @Output\n-    VarCharHolder out;\n+    NullableVarCharHolder out;\n \n     @Inject\n     DrillBuf buffer;\n@@ -110,16 +112,19 @@ public void setup() {\n     }\n \n     public void eval() {\n-\n       String cidrString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputCIDR.start, inputCIDR.end, inputCIDR.buffer);\n-      org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n-\n-      String outputValue = utils.getInfo().getBroadcastAddress();\n-\n-      out.buffer = buffer;\n-      out.start = 0;\n-      out.end = outputValue.getBytes().length;\n-      buffer.setBytes(0, outputValue.getBytes());\n+      try {\n+        org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n+        String outputValue = utils.getInfo().getBroadcastAddress();\n+\n+        out.buffer = buffer;\n+        out.start = 0;\n+        out.end = outputValue.getBytes().length;\n+        buffer.setBytes(0, outputValue.getBytes());\n+        out.isSet = 1;\n+      } catch (IllegalArgumentException e) {\n+        // return null is case of invalid input\n+      }\n     }\n \n   }\n@@ -134,7 +139,7 @@ public void eval() {\n     VarCharHolder inputCIDR;\n \n     @Output\n-    VarCharHolder out;\n+    NullableVarCharHolder out;\n \n     @Inject\n     DrillBuf buffer;\n@@ -143,16 +148,19 @@ public void setup() {\n     }\n \n     public void eval() {\n-\n       String cidrString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputCIDR.start, inputCIDR.end, inputCIDR.buffer);\n-      org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n-\n-      String outputValue = utils.getInfo().getNetmask();\n-\n-      out.buffer = buffer;\n-      out.start = 0;\n-      out.end = outputValue.getBytes().length;\n-      buffer.setBytes(0, outputValue.getBytes());\n+      try {\n+        org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n+        String outputValue = utils.getInfo().getNetmask();\n+\n+        out.buffer = buffer;\n+        out.start = 0;\n+        out.end = outputValue.getBytes().length;\n+        buffer.setBytes(0, outputValue.getBytes());\n+        out.isSet = 1;\n+      } catch (IllegalArgumentException e) {\n+        // return null is case of invalid input\n+      }\n     }\n \n   }\n@@ -167,7 +175,7 @@ public void eval() {\n     VarCharHolder inputCIDR;\n \n     @Output\n-    VarCharHolder out;\n+    NullableVarCharHolder out;\n \n     @Inject\n     DrillBuf buffer;\n@@ -176,16 +184,19 @@ public void setup() {\n     }\n \n     public void eval() {\n-\n       String cidrString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputCIDR.start, inputCIDR.end, inputCIDR.buffer);\n-      org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n-\n-      String outputValue = utils.getInfo().getLowAddress();\n-\n-      out.buffer = buffer;\n-      out.start = 0;\n-      out.end = outputValue.getBytes().length;\n-      buffer.setBytes(0, outputValue.getBytes());\n+      try {\n+        org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n+        String outputValue = utils.getInfo().getLowAddress();\n+\n+        out.buffer = buffer;\n+        out.start = 0;\n+        out.end = outputValue.getBytes().length;\n+        buffer.setBytes(0, outputValue.getBytes());\n+        out.isSet = 1;\n+      } catch (IllegalArgumentException e) {\n+        // return null is case of invalid input\n+      }\n     }\n \n   }\n@@ -200,7 +211,7 @@ public void eval() {\n     VarCharHolder inputCIDR;\n \n     @Output\n-    VarCharHolder out;\n+    NullableVarCharHolder out;\n \n     @Inject\n     DrillBuf buffer;\n@@ -209,24 +220,27 @@ public void setup() {\n     }\n \n     public void eval() {\n-\n       String cidrString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputCIDR.start, inputCIDR.end, inputCIDR.buffer);\n-      org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n-\n-      String outputValue = utils.getInfo().getHighAddress();\n-\n-      out.buffer = buffer;\n-      out.start = 0;\n-      out.end = outputValue.getBytes().length;\n-      buffer.setBytes(0, outputValue.getBytes());\n+      try {\n+        org.apache.commons.net.util.SubnetUtils utils = new org.apache.commons.net.util.SubnetUtils(cidrString);\n+        String outputValue = utils.getInfo().getHighAddress();\n+\n+        out.buffer = buffer;\n+        out.start = 0;\n+        out.end = outputValue.getBytes().length;\n+        buffer.setBytes(0, outputValue.getBytes());\n+        out.isSet = 1;\n+      } catch (IllegalArgumentException e) {\n+        // return null is case of invalid input\n+      }\n     }\n   }\n \n   /**\n    * This function encodes URL strings.\n    */\n   @FunctionTemplate(name = \"url_encode\", scope = FunctionTemplate.FunctionScope.SIMPLE, nulls = FunctionTemplate.NullHandling.NULL_IF_NULL)\n-  public static class urlencodeFunction implements DrillSimpleFunc {\n+  public static class UrlEncodeFunction implements DrillSimpleFunc {\n \n     @Param\n     VarCharHolder inputString;\n@@ -261,7 +275,7 @@ public void eval() {\n    * This function decodes URL strings.\n    */\n   @FunctionTemplate(name = \"url_decode\", scope = FunctionTemplate.FunctionScope.SIMPLE, nulls = FunctionTemplate.NullHandling.NULL_IF_NULL)\n-  public static class urldecodeFunction implements DrillSimpleFunc {\n+  public static class UrlDecodeFunction implements DrillSimpleFunc {\n \n     @Param\n     VarCharHolder inputString;\n@@ -308,27 +322,20 @@ public void eval() {\n     @Inject\n     DrillBuf buffer;\n \n-\n     public void setup() {\n     }\n \n \n     public void eval() {\n       StringBuilder result = new StringBuilder(15);\n-\n       long inputInt = in.value;\n-\n       for (int i = 0; i < 4; i++) {\n-\n         result.insert(0, Long.toString(inputInt & 0xff));\n-\n         if (i < 3) {\n           result.insert(0, '.');\n         }\n-\n         inputInt = inputInt >> 8;\n       }\n-\n       String outputValue = result.toString();\n \n       out.buffer = buffer;\n@@ -356,13 +363,26 @@ public void setup() {\n \n     public void eval() {\n       String ipString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputTextA.start, inputTextA.end, inputTextA.buffer);\n+      org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n+      if (!validator.isValidInet4Address(ipString)) {\n+        return;\n+      }\n \n       String[] ipAddressInArray = ipString.split(\"\\\\.\");\n+      if (ipAddressInArray.length < 3) {\n+        return;\n+      }\n \n-      int[] octets = new int[3];\n-\n-      for (int i = 0; i < 3; i++) {\n-        octets[i] = Integer.parseInt(ipAddressInArray[i]);\n+      // only first two octets are needed for the check\n+      int[] octets = new int[2];\n+      for (int i = 0; i < 2; i++) {\n+        try {\n+          octets[i] = Integer.parseInt(ipAddressInArray[i]);\n+        } catch (NumberFormatException e) {\n+          // should not happen since we validated the address\n+          // but if does, return false\n+          return;\n+        }\n       }\n \n       int result = 0;\n@@ -392,28 +412,34 @@ public void eval() {\n     VarCharHolder inputTextA;\n \n     @Output\n-    BigIntHolder out;\n+    NullableBigIntHolder out;\n \n     public void setup() {\n     }\n \n \n     public void eval() {\n       String ipString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputTextA.start, inputTextA.end, inputTextA.buffer);\n-      if (ipString == null || ipString.isEmpty()) {\n-        out.value = 0;\n-      } else {\n-        String[] ipAddressInArray = ipString.split(\"\\\\.\");\n-\n-        long result = 0;\n-        for (int i = 0; i < ipAddressInArray.length; i++) {\n-          int power = 3 - i;\n+      org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n+      if (!validator.isValidInet4Address(ipString)) {\n+        return;\n+      }\n+\n+      String[] ipAddressInArray = ipString.split(\"\\\\.\");\n+      long result = 0;\n+      for (int i = 0; i < ipAddressInArray.length; i++) {\n+        int power = 3 - i;\n+        try {\n           int ip = Integer.parseInt(ipAddressInArray[i]);\n           result += ip * Math.pow(256, power);\n+        } catch (NumberFormatException e) {\n+          // should not happen since we validated the address\n+          // but if does, return null\n+          return;\n         }\n-\n-        out.value = result;\n       }\n+      out.value = result;\n+      out.isSet = 1;\n     }\n   }\n \n@@ -435,18 +461,8 @@ public void setup() {\n \n     public void eval() {\n       String ipString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputIP.start, inputIP.end, inputIP.buffer);\n-      if (ipString == null || ipString.isEmpty()) {\n-        out.value = 0;\n-      } else {\n-        org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n-\n-        boolean valid = validator.isValid(ipString);\n-        if (valid) {\n-          out.value = 1;\n-        } else {\n-          out.value = 0;\n-        }\n-      }\n+      org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n+      out.value = validator.isValid(ipString) ? 1 : 0;\n     }\n   }\n \n@@ -465,21 +481,10 @@ public void eval() {\n     public void setup() {\n     }\n \n-\n     public void eval() {\n       String ipString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputIP.start, inputIP.end, inputIP.buffer);\n-      if (ipString == null || ipString.isEmpty()) {\n-        out.value = 0;\n-      } else {\n-        org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n-\n-        boolean valid = validator.isValidInet4Address(ipString);\n-        if (valid) {\n-          out.value = 1;\n-        } else {\n-          out.value = 0;\n-        }\n-      }\n+      org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n+      out.value = validator.isValidInet4Address(ipString) ? 1 : 0;\n     }\n   }\n \n@@ -500,18 +505,8 @@ public void setup() {\n \n     public void eval() {\n       String ipString = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(inputIP.start, inputIP.end, inputIP.buffer);\n-      if (ipString == null || ipString.isEmpty()) {\n-        out.value = 0;\n-      } else {\n-        org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n-\n-        boolean valid = validator.isValidInet6Address(ipString);\n-        if (valid) {\n-          out.value = 1;\n-        } else {\n-          out.value = 0;\n-        }\n-      }\n+      org.apache.commons.validator.routines.InetAddressValidator validator = org.apache.commons.validator.routines.InetAddressValidator.getInstance();\n+      out.value = validator.isValidInet6Address(ipString) ? 1 : 0;\n     }\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/drill/raw/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/NetworkFunctions.java",
                "sha": "0dbaf87a1db236afc6433d1221be74991e526925",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/drill/blob/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/PhoneticFunctions.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/PhoneticFunctions.java?ref=f88a73c9e7a75f2d08cc54188816f591b003eff4",
                "deletions": 0,
                "filename": "contrib/udfs/src/main/java/org/apache/drill/exec/udfs/PhoneticFunctions.java",
                "patch": "@@ -383,6 +383,7 @@ public void eval() {\n \n       String input = org.apache.drill.exec.expr.fn.impl.StringFunctionHelpers.toStringFromUTF8(rawInput.start, rawInput.end, rawInput.buffer);\n       String outputString = new org.apache.commons.codec.language.DoubleMetaphone().doubleMetaphone(input);\n+      outputString = outputString == null ? \"\" : outputString;\n \n       out.buffer = buffer;\n       out.start = 0;",
                "raw_url": "https://github.com/apache/drill/raw/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/main/java/org/apache/drill/exec/udfs/PhoneticFunctions.java",
                "sha": "66ab0da92aefe46b80d107f7a31d11662d9a7342",
                "status": "modified"
            },
            {
                "additions": 40,
                "blob_url": "https://github.com/apache/drill/blob/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestCryptoFunctions.java",
                "changes": 46,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestCryptoFunctions.java?ref=f88a73c9e7a75f2d08cc54188816f591b003eff4",
                "deletions": 6,
                "filename": "contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestCryptoFunctions.java",
                "patch": "@@ -17,14 +17,23 @@\n  */\n package org.apache.drill.exec.udfs;\n \n-import org.apache.drill.test.BaseTestQuery;\n import org.apache.drill.categories.SqlFunctionTest;\n import org.apache.drill.categories.UnlikelyTest;\n+import org.apache.drill.test.ClusterFixture;\n+import org.apache.drill.test.ClusterFixtureBuilder;\n+import org.apache.drill.test.ClusterTest;\n+import org.junit.BeforeClass;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n \n @Category({UnlikelyTest.class, SqlFunctionTest.class})\n-public class TestCryptoFunctions extends BaseTestQuery {\n+public class TestCryptoFunctions extends ClusterTest {\n+\n+  @BeforeClass\n+  public static void setup() throws Exception {\n+    ClusterFixtureBuilder builder = ClusterFixture.builder(dirTestWatcher);\n+    startCluster(builder);\n+  }\n \n   @Test\n   public void testMD5() throws Exception {\n@@ -74,23 +83,48 @@ public void testSHA512() throws Exception {\n \n   @Test\n   public void testAESEncrypt() throws Exception {\n-    final String query = \"select aes_encrypt('testing', 'secret_key') as encrypted FROM (VALUES(1))\";\n     testBuilder()\n-      .sqlQuery(query)\n+      .sqlQuery(\"select aes_encrypt('testing', 'secret_key') as encrypted from (values(1))\")\n       .ordered()\n       .baselineColumns(\"encrypted\")\n       .baselineValues(\"ICf+zdOrLitogB8HUDru0w==\")\n       .go();\n+\n+    testBuilder()\n+        .sqlQuery(\"select aes_encrypt(cast(null as varchar), 'secret_key') as encrypted from (values(1))\")\n+        .ordered()\n+        .baselineColumns(\"encrypted\")\n+        .baselineValues((String) null)\n+        .go();\n+    testBuilder()\n+        .sqlQuery(\"select aes_encrypt('testing', cast (null as varchar)) as encrypted from (values(1))\")\n+        .ordered()\n+        .baselineColumns(\"encrypted\")\n+        .baselineValues((String) null)\n+        .go();\n   }\n \n   @Test\n   public void testAESDecrypt() throws Exception {\n-    final String query = \"select aes_decrypt('ICf+zdOrLitogB8HUDru0w==', 'secret_key') as decrypt from (values(1))\";\n     testBuilder()\n-      .sqlQuery(query)\n+      .sqlQuery(\"select aes_decrypt('ICf+zdOrLitogB8HUDru0w==', 'secret_key') as decrypt from (values(1))\")\n       .ordered()\n       .baselineColumns(\"decrypt\")\n       .baselineValues(\"testing\")\n       .go();\n+\n+    testBuilder()\n+        .sqlQuery(\"select aes_decrypt(cast(null as varchar), 'secret_key') as decrypt from (values(1))\")\n+        .ordered()\n+        .baselineColumns(\"decrypt\")\n+        .baselineValues((String) null)\n+        .go();\n+\n+    testBuilder()\n+        .sqlQuery(\"select aes_decrypt('ICf+zdOrLitogB8HUDru0w==', cast(null as varchar)) as decrypt from (values(1))\")\n+        .ordered()\n+        .baselineColumns(\"decrypt\")\n+        .baselineValues((String) null)\n+        .go();\n   }\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/drill/raw/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestCryptoFunctions.java",
                "sha": "d382e96856186d676c21a1d9ad4e74f95610622e",
                "status": "modified"
            },
            {
                "additions": 125,
                "blob_url": "https://github.com/apache/drill/blob/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestNetworkFunctions.java",
                "changes": 139,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestNetworkFunctions.java?ref=f88a73c9e7a75f2d08cc54188816f591b003eff4",
                "deletions": 14,
                "filename": "contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestNetworkFunctions.java",
                "patch": "@@ -19,23 +19,44 @@\n \n import org.apache.drill.categories.SqlFunctionTest;\n import org.apache.drill.categories.UnlikelyTest;\n-import org.apache.drill.test.BaseTestQuery;\n+import org.apache.drill.test.ClusterFixture;\n+import org.apache.drill.test.ClusterFixtureBuilder;\n+import org.apache.drill.test.ClusterTest;\n+import org.junit.BeforeClass;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n \n @Category({UnlikelyTest.class, SqlFunctionTest.class})\n-public class TestNetworkFunctions extends BaseTestQuery {\n+public class TestNetworkFunctions extends ClusterTest {\n+\n+  @BeforeClass\n+  public static void setup() throws Exception {\n+    ClusterFixtureBuilder builder = ClusterFixture.builder(dirTestWatcher);\n+    startCluster(builder);\n+  }\n \n   @Test\n   public void testInetAton() throws Exception {\n-    final String query = \"select inet_aton('192.168.0.1') as inet from (values(1))\";\n-    testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues(Long.parseLong(\"3232235521\")).go();\n+    String query = \"select inet_aton('192.168.0.1') as inet from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues(3232235521L).go();\n+\n+    query = \"select inet_aton('192.168.0') as inet from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues((Long) null).go();\n+\n+    query = \"select inet_aton('') as inet from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues((Long) null).go();\n+\n+    query = \"select inet_aton(cast(null as varchar)) as inet from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues((Long) null).go();\n   }\n \n   @Test\n   public void testInetNtoa() throws Exception {\n-    final String query = \"select inet_ntoa(3232235521) as inet from (values(1))\";\n+    String query = \"select inet_ntoa(3232235521) as inet from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues(\"192.168.0.1\").go();\n+\n+    query = \"select inet_ntoa(cast(null as int)) as inet from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"inet\").baselineValues((String) null).go();\n   }\n \n   @Test\n@@ -46,32 +67,68 @@ public void testInNetwork() throws Exception {\n \n   @Test\n   public void testNotInNetwork() throws Exception {\n-    final String query = \"select in_network('10.10.10.10', '192.168.0.0/28') as in_net FROM (values(1))\";\n+    String query = \"select in_network('10.10.10.10', '192.168.0.0/28') as in_net from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"in_net\").baselineValues(false).go();\n+\n+    query = \"select in_network('10.10.10.10', '') as in_net from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"in_net\").baselineValues(false).go();\n+\n+    query = \"select in_network('', '192.168.0.0/28') as in_net from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"in_net\").baselineValues(false).go();\n+\n+    query = \"select in_network(cast(null as varchar), '192.168.0.0/28') as in_net from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"in_net\").baselineValues((Boolean) null).go();\n+\n+    query = \"select in_network('10.10.10.10', cast(null as varchar)) as in_net from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"in_net\").baselineValues((Boolean) null).go();\n   }\n \n   @Test\n   public void testBroadcastAddress() throws Exception {\n-    final String query = \"select broadcast_address( '192.168.0.0/28' ) AS broadcast_address FROM (values(1))\";\n+    String query = \"select broadcast_address('192.168.0.0/28') as broadcast_address from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"broadcast_address\").baselineValues(\"192.168.0.15\").go();\n+\n+    query = \"select broadcast_address('192.168.') as broadcast_address from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"broadcast_address\").baselineValues((String) null).go();\n+\n+    query = \"select broadcast_address('') as broadcast_address from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"broadcast_address\").baselineValues((String) null).go();\n   }\n \n   @Test\n   public void testNetmask() throws Exception {\n-    final String query = \"select netmask('192.168.0.0/28') AS netmask FROM (values(1))\";\n+    String query = \"select netmask('192.168.0.0/28') as netmask from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"netmask\").baselineValues(\"255.255.255.240\").go();\n+\n+    query = \"select netmask('192222') as netmask from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"netmask\").baselineValues((String) null).go();\n+\n+    query = \"select netmask('') as netmask from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"netmask\").baselineValues((String) null).go();\n   }\n \n   @Test\n   public void testLowAddress() throws Exception {\n-    final String query = \"SELECT low_address('192.168.0.0/28') AS low FROM (values(1))\";\n+    String query = \"select low_address('192.168.0.0/28') as low from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"low\").baselineValues(\"192.168.0.1\").go();\n+\n+    query = \"select low_address('192.168.0.0/') as low from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"low\").baselineValues((String) null).go();\n+\n+    query = \"select low_address('192.168.0.0/') as low from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"low\").baselineValues((String) null).go();\n   }\n \n   @Test\n   public void testHighAddress() throws Exception {\n-    final String query = \"SELECT high_address('192.168.0.0/28') AS high FROM (values(1))\";\n+    String query = \"select high_address('192.168.0.0/28') as high from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"high\").baselineValues(\"192.168.0.14\").go();\n+\n+    query = \"select high_address('192.168.0.') as high from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"high\").baselineValues((String) null).go();\n+\n+    query = \"select high_address('') as high from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"high\").baselineValues((String) null).go();\n   }\n \n   @Test\n@@ -88,8 +145,20 @@ public void testDecodeUrl() throws Exception {\n \n   @Test\n   public void testNotPrivateIP() throws Exception {\n-    final String query = \"SELECT is_private_ip('8.8.8.8') AS is_private_ip FROM (values(1))\";\n+    String query = \"select is_private_ip('8.8.8.8') as is_private_ip from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_private_ip\").baselineValues(false).go();\n+\n+    query = \"select is_private_ip('8.A.8') as is_private_ip from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_private_ip\").baselineValues(false).go();\n+\n+    query = \"select is_private_ip('192.168') as is_private_ip from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_private_ip\").baselineValues(false).go();\n+\n+    query = \"select is_private_ip('') as is_private_ip from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_private_ip\").baselineValues(false).go();\n+\n+    query = \"select is_private_ip(cast(null as varchar)) as is_private_ip from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_private_ip\").baselineValues((Boolean) null).go();\n   }\n \n   @Test\n@@ -100,8 +169,17 @@ public void testPrivateIP() throws Exception {\n \n   @Test\n   public void testNotValidIP() throws Exception {\n-    final String query = \"SELECT is_valid_IP('258.257.234.23') AS is_valid_IP FROM (values(1))\";\n+    String query = \"select is_valid_IP('258.257.234.23') as is_valid_IP from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IP('258.257.2') as is_valid_IP from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IP('') as is_valid_IP from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IP(cast(null as varchar)) as is_valid_IP from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP\").baselineValues((Boolean) null).go();\n   }\n \n   @Test\n@@ -112,8 +190,17 @@ public void testIsValidIP() throws Exception {\n \n   @Test\n   public void testNotValidIPv4() throws Exception {\n-    final String query = \"SELECT is_valid_IPv4( '192.168.0.257') AS is_valid_IP4 FROM (values(1))\";\n+    String query = \"select is_valid_IPv4('192.168.0.257') as is_valid_IP4 from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP4\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IPv4('192123') as is_valid_IP4 from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP4\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IPv4('') as is_valid_IP4 from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP4\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IPv4(cast(null as varchar)) as is_valid_IP4 from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP4\").baselineValues((Boolean) null).go();\n   }\n \n   @Test\n@@ -130,8 +217,32 @@ public void testIsValidIPv6() throws Exception {\n \n   @Test\n   public void testNotValidIPv6() throws Exception {\n-    final String query = \"SELECT is_valid_IPv6('1050:0:0:0:5:600:300c:326g') AS is_valid_IP6 FROM (values(1))\";\n+    String query = \"select is_valid_IPv6('1050:0:0:0:5:600:300c:326g') as is_valid_IP6 from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP6\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IPv6('1050:0:0:0:5:600_AAA') as is_valid_IP6 from (values(1))\";\n     testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP6\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IPv6('') as is_valid_IP6 from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP6\").baselineValues(false).go();\n+\n+    query = \"select is_valid_IPv6(cast(null as varchar)) as is_valid_IP6 from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"is_valid_IP6\").baselineValues((Boolean) null).go();\n+  }\n+\n+  @Test\n+  public void testAddressCount() throws Exception {\n+    String query = \"select address_count('192.168.0.1/30') as address_count from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"address_count\").baselineValues(2L).go();\n+\n+    query = \"select address_count('192.168') as address_count from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"address_count\").baselineValues((Long) null).go();\n+\n+    query = \"select address_count('192.168.0.1/100') as address_count from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"address_count\").baselineValues((Long) null).go();\n+\n+    query = \"select address_count('') as address_count from (values(1))\";\n+    testBuilder().sqlQuery(query).ordered().baselineColumns(\"address_count\").baselineValues((Long) null).go();\n   }\n \n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/drill/raw/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestNetworkFunctions.java",
                "sha": "349e09794b015c97a3567a58b3e35739120d283d",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/drill/blob/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestPhoneticFunctions.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/drill/contents/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestPhoneticFunctions.java?ref=f88a73c9e7a75f2d08cc54188816f591b003eff4",
                "deletions": 5,
                "filename": "contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestPhoneticFunctions.java",
                "patch": "@@ -19,12 +19,10 @@\n \n import org.apache.drill.categories.SqlFunctionTest;\n import org.apache.drill.categories.UnlikelyTest;\n-import org.apache.drill.test.BaseDirTestWatcher;\n import org.apache.drill.test.ClusterFixture;\n import org.apache.drill.test.ClusterFixtureBuilder;\n import org.apache.drill.test.ClusterTest;\n import org.junit.BeforeClass;\n-import org.junit.Rule;\n import org.junit.Test;\n import org.junit.experimental.categories.Category;\n \n@@ -33,9 +31,6 @@\n @Category({UnlikelyTest.class, SqlFunctionTest.class})\n public class TestPhoneticFunctions extends ClusterTest {\n \n-  @Rule\n-  public final BaseDirTestWatcher baseDirTestWatcher = new BaseDirTestWatcher();\n-\n   @BeforeClass\n   public static void setup() throws Exception {\n     ClusterFixtureBuilder builder = ClusterFixture.builder(dirTestWatcher);\n@@ -112,5 +107,10 @@ public void testDoubleMetaphone() throws Exception {\n         .sql(\"SELECT double_metaphone('Phoenix') AS meta FROM (VALUES(1))\")\n         .singletonString();\n     assertEquals(\"FNKS\", result);\n+\n+    result = queryBuilder()\n+        .sql(\"SELECT double_metaphone('') AS meta FROM (VALUES(1))\")\n+        .singletonString();\n+    assertEquals(\"\", result);\n   }\n }",
                "raw_url": "https://github.com/apache/drill/raw/f88a73c9e7a75f2d08cc54188816f591b003eff4/contrib/udfs/src/test/java/org/apache/drill/exec/udfs/TestPhoneticFunctions.java",
                "sha": "64fb6daa0b124dc9c0af21c208140f891b2f2703",
                "status": "modified"
            }
        ],
        "message": "DRILL-6705: Fix various failures in Crypto / Network / Phonetic functions when invalid input is given\n\n1. aes_decrypt / aes_ecrypt - moved cyper init part into eval method since it not a constant and can be different for each input\n2. double_metaphone - fixed NPE when given string is empty\n3. in_network / address_count / broadcast_address / netmask / low_address / high_address / - fixed IllegalArgumentException in case of invalid input\n4. is_private_ip / inet_aton - fixed ArrayIndexOutOfBoundsException / NumberFormatException in case of invalid input\n5. is_valid_IP / is_valid_IPv4 / is_valid_IPv6 - removed unnecessary checks\n6. Added appropriate unit tests\n\ncloses #1443",
        "parent": "https://github.com/apache/drill/commit/ea83672edc75fe379a4e19764232b10f5199d06e",
        "repo": "drill",
        "unit_tests": [
            "TestCryptoFunctions.java",
            "TestNetworkFunctions.java",
            "TestPhoneticFunctions.java"
        ]
    }
}