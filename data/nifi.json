{
    "nifi_102e3cb": {
        "bug_id": "nifi_102e3cb",
        "commit": "https://github.com/apache/nifi/commit/102e3cb093cd7380ffaf5ebff80a8fe64485c22b",
        "file": [
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/nifi/blob/102e3cb093cd7380ffaf5ebff80a8fe64485c22b/nifi/nifi-nar-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/java/org/apache/nifi/processors/standard/PostHTTP.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi/nifi-nar-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/java/org/apache/nifi/processors/standard/PostHTTP.java?ref=102e3cb093cd7380ffaf5ebff80a8fe64485c22b",
                "deletions": 12,
                "filename": "nifi/nifi-nar-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/java/org/apache/nifi/processors/standard/PostHTTP.java",
                "patch": "@@ -68,6 +68,7 @@\n import org.apache.http.conn.ManagedHttpClientConnection;\n import org.apache.http.conn.socket.ConnectionSocketFactory;\n import org.apache.http.conn.ssl.SSLConnectionSocketFactory;\n+import org.apache.http.conn.ssl.SSLContextBuilder;\n import org.apache.http.conn.ssl.SSLContexts;\n import org.apache.http.conn.ssl.TrustSelfSignedStrategy;\n import org.apache.http.entity.ContentProducer;\n@@ -352,21 +353,26 @@ private Config getConfig(final String url, final ProcessContext context) {\n     private SSLContext createSSLContext(final SSLContextService service) throws KeyStoreException, IOException, NoSuchAlgorithmException, \n         CertificateException, KeyManagementException, UnrecoverableKeyException \n     {\n-        final KeyStore truststore  = KeyStore.getInstance(service.getTrustStoreType());\n-        try (final InputStream in = new FileInputStream(new File(service.getTrustStoreFile()))) {\n-            truststore.load(in, service.getTrustStorePassword().toCharArray());\n+        SSLContextBuilder builder = SSLContexts.custom();\n+        final String trustFilename = service.getTrustStoreFile();\n+        if ( trustFilename != null ) {\n+            final KeyStore truststore  = KeyStore.getInstance(service.getTrustStoreType());\n+            try (final InputStream in = new FileInputStream(new File(service.getTrustStoreFile()))) {\n+                truststore.load(in, service.getTrustStorePassword().toCharArray());\n+            }\n+            builder = builder.loadTrustMaterial(truststore, new TrustSelfSignedStrategy());\n         }\n-        \n-        final KeyStore keystore  = KeyStore.getInstance(service.getKeyStoreType());\n-        try (final InputStream in = new FileInputStream(new File(service.getKeyStoreFile()))) {\n-            keystore.load(in, service.getKeyStorePassword().toCharArray());\n+\n+        final String keyFilename = service.getKeyStoreFile();\n+        if ( keyFilename != null ) {\n+            final KeyStore keystore  = KeyStore.getInstance(service.getKeyStoreType());\n+            try (final InputStream in = new FileInputStream(new File(service.getKeyStoreFile()))) {\n+                keystore.load(in, service.getKeyStorePassword().toCharArray());\n+            }\n+            builder = builder.loadKeyMaterial(keystore, service.getKeyStorePassword().toCharArray());\n         }\n         \n-        SSLContext sslContext = SSLContexts.custom()\n-                .loadTrustMaterial(truststore, new TrustSelfSignedStrategy())\n-                .loadKeyMaterial(keystore, service.getKeyStorePassword().toCharArray())\n-                .build();\n-        \n+        SSLContext sslContext = builder.build();\n         return sslContext;\n     }\n ",
                "raw_url": "https://github.com/apache/nifi/raw/102e3cb093cd7380ffaf5ebff80a8fe64485c22b/nifi/nifi-nar-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/java/org/apache/nifi/processors/standard/PostHTTP.java",
                "sha": "f8a33bc8e7cb58b0a2c37c251d2ec41aad33aff1",
                "status": "modified"
            }
        ],
        "message": "NIFI-310: Fixed NPE",
        "parent": "https://github.com/apache/nifi/commit/be16371b20eeb94c90cf369917325c76c3549e1c",
        "repo": "nifi",
        "unit_tests": [
            "TestPostHTTP.java"
        ]
    },
    "nifi_292dd1d": {
        "bug_id": "nifi_292dd1d",
        "commit": "https://github.com/apache/nifi/commit/292dd1d66b1726794f0d34523578727ea3a7fe08",
        "file": [
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/nifi/blob/292dd1d66b1726794f0d34523578727ea3a7fe08/nifi-commons/nifi-write-ahead-log/src/main/java/org/wali/MinimalLockingWriteAheadLog.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-commons/nifi-write-ahead-log/src/main/java/org/wali/MinimalLockingWriteAheadLog.java?ref=292dd1d66b1726794f0d34523578727ea3a7fe08",
                "deletions": 14,
                "filename": "nifi-commons/nifi-write-ahead-log/src/main/java/org/wali/MinimalLockingWriteAheadLog.java",
                "patch": "@@ -973,24 +973,28 @@ private DataInputStream getRecoveryStream() throws IOException {\n                 logger.debug(\"{} recovering from {}\", this, nextRecoveryPath);\n                 recoveryIn = createDataInputStream(nextRecoveryPath);\n                 if (hasMoreData(recoveryIn)) {\n-                    final String waliImplementationClass = recoveryIn.readUTF();\n-                    if (!MinimalLockingWriteAheadLog.class.getName().equals(waliImplementationClass)) {\n-                        continue;\n-                    }\n+                    try {\n+                        final String waliImplementationClass = recoveryIn.readUTF();\n+                        if (!MinimalLockingWriteAheadLog.class.getName().equals(waliImplementationClass)) {\n+                            continue;\n+                        }\n \n-                    final long waliVersion = recoveryIn.readInt();\n-                    if (waliVersion > writeAheadLogVersion) {\n-                        throw new IOException(\"Cannot recovery from file \" + nextRecoveryPath + \" because it was written using \"\n+                        final long waliVersion = recoveryIn.readInt();\n+                        if (waliVersion > writeAheadLogVersion) {\n+                            throw new IOException(\"Cannot recovery from file \" + nextRecoveryPath + \" because it was written using \"\n                                 + \"WALI version \" + waliVersion + \", but the version used to restore it is only \" + writeAheadLogVersion);\n-                    }\n-\n-                    final String serdeEncoding = recoveryIn.readUTF();\n-                    this.recoveryVersion = recoveryIn.readInt();\n-                    serde = serdeFactory.createSerDe(serdeEncoding);\n+                        }\n \n-                    serde.readHeader(recoveryIn);\n+                        final String serdeEncoding = recoveryIn.readUTF();\n+                        this.recoveryVersion = recoveryIn.readInt();\n+                        serde = serdeFactory.createSerDe(serdeEncoding);\n \n-                    break;\n+                        serde.readHeader(recoveryIn);\n+                        break;\n+                    } catch (final Exception e) {\n+                        logger.warn(\"Failed to recover data from Write-Ahead Log for {} because the header information could not be read properly. \"\n+                            + \"This often is the result of the file not being fully written out before the application is restarted. This file will be ignored.\", nextRecoveryPath);\n+                    }\n                 }\n             }\n ",
                "raw_url": "https://github.com/apache/nifi/raw/292dd1d66b1726794f0d34523578727ea3a7fe08/nifi-commons/nifi-write-ahead-log/src/main/java/org/wali/MinimalLockingWriteAheadLog.java",
                "sha": "894907323d66d59aab166fb90fc5853977bb016a",
                "status": "modified"
            },
            {
                "additions": 82,
                "blob_url": "https://github.com/apache/nifi/blob/292dd1d66b1726794f0d34523578727ea3a7fe08/nifi-commons/nifi-write-ahead-log/src/test/java/org/wali/TestMinimalLockingWriteAheadLog.java",
                "changes": 82,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-commons/nifi-write-ahead-log/src/test/java/org/wali/TestMinimalLockingWriteAheadLog.java?ref=292dd1d66b1726794f0d34523578727ea3a7fe08",
                "deletions": 0,
                "filename": "nifi-commons/nifi-write-ahead-log/src/test/java/org/wali/TestMinimalLockingWriteAheadLog.java",
                "patch": "@@ -23,6 +23,7 @@\n import java.io.BufferedInputStream;\n import java.io.DataInputStream;\n import java.io.DataOutputStream;\n+import java.io.EOFException;\n import java.io.File;\n import java.io.FileFilter;\n import java.io.FileInputStream;\n@@ -41,6 +42,7 @@\n import java.util.TreeSet;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n import java.util.concurrent.atomic.AtomicReference;\n \n import org.junit.Assert;\n@@ -53,6 +55,86 @@\n     private static final Logger logger = LoggerFactory.getLogger(TestMinimalLockingWriteAheadLog.class);\n \n \n+    @Test\n+    public void testTruncatedPartitionHeader() throws IOException {\n+        final int numPartitions = 4;\n+\n+        final Path path = Paths.get(\"target/testTruncatedPartitionHeader\");\n+        deleteRecursively(path.toFile());\n+        assertTrue(path.toFile().mkdirs());\n+\n+        final AtomicInteger counter = new AtomicInteger(0);\n+        final SerDe<Object> serde = new SerDe<Object>() {\n+            @Override\n+            public void readHeader(DataInputStream in) throws IOException {\n+                if (counter.getAndIncrement() == 1) {\n+                    throw new EOFException(\"Intentionally thrown for unit test\");\n+                }\n+            }\n+\n+            @Override\n+            public void serializeEdit(Object previousRecordState, Object newRecordState, DataOutputStream out) throws IOException {\n+                out.write(1);\n+            }\n+\n+            @Override\n+            public void serializeRecord(Object record, DataOutputStream out) throws IOException {\n+                out.write(1);\n+            }\n+\n+            @Override\n+            public Object deserializeEdit(DataInputStream in, Map<Object, Object> currentRecordStates, int version) throws IOException {\n+                final int val = in.read();\n+                return (val == 1) ? new Object() : null;\n+            }\n+\n+            @Override\n+            public Object deserializeRecord(DataInputStream in, int version) throws IOException {\n+                final int val = in.read();\n+                return (val == 1) ? new Object() : null;\n+            }\n+\n+            @Override\n+            public Object getRecordIdentifier(Object record) {\n+                return 1;\n+            }\n+\n+            @Override\n+            public UpdateType getUpdateType(Object record) {\n+                return UpdateType.CREATE;\n+            }\n+\n+            @Override\n+            public String getLocation(Object record) {\n+                return null;\n+            }\n+\n+            @Override\n+            public int getVersion() {\n+                return 0;\n+            }\n+        };\n+\n+        final WriteAheadRepository<Object> repo = new MinimalLockingWriteAheadLog<>(path, numPartitions, serde, (SyncListener) null);\n+        try {\n+            final Collection<Object> initialRecs = repo.recoverRecords();\n+            assertTrue(initialRecs.isEmpty());\n+\n+            repo.update(Collections.singletonList(new Object()), false);\n+            repo.update(Collections.singletonList(new Object()), false);\n+            repo.update(Collections.singletonList(new Object()), false);\n+        } finally {\n+            repo.shutdown();\n+        }\n+\n+        final WriteAheadRepository<Object> secondRepo = new MinimalLockingWriteAheadLog<>(path, numPartitions, serde, (SyncListener) null);\n+        try {\n+            secondRepo.recoverRecords();\n+        } finally {\n+            secondRepo.shutdown();\n+        }\n+    }\n+\n     @Test\n     @Ignore(\"for local testing only\")\n     public void testUpdatePerformance() throws IOException, InterruptedException {",
                "raw_url": "https://github.com/apache/nifi/raw/292dd1d66b1726794f0d34523578727ea3a7fe08/nifi-commons/nifi-write-ahead-log/src/test/java/org/wali/TestMinimalLockingWriteAheadLog.java",
                "sha": "cbca9684fe5c1c94dadd3eda7812c5f0fef6099e",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/nifi/blob/292dd1d66b1726794f0d34523578727ea3a7fe08/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/SchemaRepositoryRecordSerde.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/SchemaRepositoryRecordSerde.java?ref=292dd1d66b1726794f0d34523578727ea3a7fe08",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/SchemaRepositoryRecordSerde.java",
                "patch": "@@ -113,6 +113,10 @@ public RepositoryRecord deserializeEdit(final DataInputStream in, final Map<Obje\n     public RepositoryRecord deserializeRecord(final DataInputStream in, final int version) throws IOException {\n         final SchemaRecordReader reader = SchemaRecordReader.fromSchema(recoverySchema);\n         final Record updateRecord = reader.readRecord(in);\n+        if (updateRecord == null) {\n+            // null may be returned by reader.readRecord() if it encounters end-of-stream\n+            return null;\n+        }\n \n         // Top level is always going to be a \"Repository Record Update\" record because we need a 'Union' type record at the\n         // top level that indicates which type of record we have.",
                "raw_url": "https://github.com/apache/nifi/raw/292dd1d66b1726794f0d34523578727ea3a7fe08/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/SchemaRepositoryRecordSerde.java",
                "sha": "221f8cebe94afc06e458fa67fb459c9057006668",
                "status": "modified"
            }
        ],
        "message": "NIFI-3678: Ensure that we catch EOFException when reading header information from WAL Partition files; previously, we caught EOFExceptions when reading a 'record' from the WAL but not when reading header info\n\nNIFI-3678: If we have a transaction ID but then have no more data written to Partition file, we end up with a NPE. Added logic to avoid this and instead return null for the next record when this happens\n\nThis closes #1656.\n\nSigned-off-by: Bryan Bende <bbende@apache.org>",
        "parent": "https://github.com/apache/nifi/commit/6a75ab1740397ae5247f9bd302182cc846a8a2bd",
        "repo": "nifi",
        "unit_tests": [
            "SchemaRepositoryRecordSerdeTest.java"
        ]
    },
    "nifi_29f53c0": {
        "bug_id": "nifi_29f53c0",
        "commit": "https://github.com/apache/nifi/commit/29f53c07f5b6831d057f409b83735d8c7331ce7b",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/nifi/blob/29f53c07f5b6831d057f409b83735d8c7331ce7b/nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-pubsub-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/ConsumeKafka.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-pubsub-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/ConsumeKafka.java?ref=29f53c07f5b6831d057f409b83735d8c7331ce7b",
                "deletions": 1,
                "filename": "nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-pubsub-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/ConsumeKafka.java",
                "patch": "@@ -238,7 +238,7 @@ protected void postCommit(ProcessContext context) {\n          * broker is possible we need a mechanism to be able to disable it.\n          * 'check.connection' property will serve as such mechanism\n          */\n-        if (!kafkaProperties.getProperty(\"check.connection\").equals(\"false\")) {\n+        if (!\"false\".equals(kafkaProperties.get(\"check.connection\"))) {\n             this.checkIfInitialConnectionPossible();\n         }\n ",
                "raw_url": "https://github.com/apache/nifi/raw/29f53c07f5b6831d057f409b83735d8c7331ce7b/nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-pubsub-processors/src/main/java/org/apache/nifi/processors/kafka/pubsub/ConsumeKafka.java",
                "sha": "05fa0e3f854d4c4dcf064ea145dc85bafd68eb6f",
                "status": "modified"
            }
        ],
        "message": "NIFI-2509 This closes #805. fixed NPE condition in KafkaConsumer",
        "parent": "https://github.com/apache/nifi/commit/903b1fe46511026231ab9ca5b8bdc7aa6eeaa1d6",
        "repo": "nifi",
        "unit_tests": [
            "ConsumeKafkaTest.java"
        ]
    },
    "nifi_3a2b12d": {
        "bug_id": "nifi_3a2b12d",
        "commit": "https://github.com/apache/nifi/commit/3a2b12d7c9919c8f3be2a01125ad60ca22fecc03",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/nifi/blob/3a2b12d7c9919c8f3be2a01125ad60ca22fecc03/nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/scheduling/StandardProcessScheduler.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/scheduling/StandardProcessScheduler.java?ref=3a2b12d7c9919c8f3be2a01125ad60ca22fecc03",
                "deletions": 1,
                "filename": "nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/scheduling/StandardProcessScheduler.java",
                "patch": "@@ -297,7 +297,9 @@ public void run() {\n                         final Class<? extends ControllerService> serviceDefinition = descriptor.getControllerServiceDefinition();\n                         if ( serviceDefinition != null ) {\n                             final String serviceId = processContext.getProperty(descriptor).getValue();\n-                            serviceIds.add(serviceId);\n+                            if ( serviceId != null ) {\n+                            \tserviceIds.add(serviceId);\n+                            }\n                         }\n                     }\n                     ",
                "raw_url": "https://github.com/apache/nifi/raw/3a2b12d7c9919c8f3be2a01125ad60ca22fecc03/nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/scheduling/StandardProcessScheduler.java",
                "sha": "43e05dda1da144297d1f0d3817dd58dd1b04706e",
                "status": "modified"
            }
        ],
        "message": "NIFI-486: Fixed NPE that occurs if processor allows for controller service as optional property and no value set\n\nSigned-off-by: joewitt <joewitt@apache.org>",
        "parent": "https://github.com/apache/nifi/commit/4a16845309fce1f2cdbad3b31926563dd27b2c3c",
        "repo": "nifi",
        "unit_tests": [
            "TestStandardProcessScheduler.java"
        ]
    },
    "nifi_5098693": {
        "bug_id": "nifi_5098693",
        "commit": "https://github.com/apache/nifi/commit/50986932d6cdd0ffc7a8f0a828808bb72bb4fdf5",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/nifi/blob/50986932d6cdd0ffc7a8f0a828808bb72bb4fdf5/nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/scheduling/StandardProcessScheduler.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/scheduling/StandardProcessScheduler.java?ref=50986932d6cdd0ffc7a8f0a828808bb72bb4fdf5",
                "deletions": 1,
                "filename": "nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/scheduling/StandardProcessScheduler.java",
                "patch": "@@ -297,7 +297,9 @@ public void run() {\n                         final Class<? extends ControllerService> serviceDefinition = descriptor.getControllerServiceDefinition();\n                         if ( serviceDefinition != null ) {\n                             final String serviceId = processContext.getProperty(descriptor).getValue();\n-                            serviceIds.add(serviceId);\n+                            if ( serviceId != null ) {\n+                            \tserviceIds.add(serviceId);\n+                            }\n                         }\n                     }\n                     ",
                "raw_url": "https://github.com/apache/nifi/raw/50986932d6cdd0ffc7a8f0a828808bb72bb4fdf5/nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/scheduling/StandardProcessScheduler.java",
                "sha": "43e05dda1da144297d1f0d3817dd58dd1b04706e",
                "status": "modified"
            }
        ],
        "message": "NIFI-486: Fixed NPE that occurs if processor allows for controller service as optional property and no value set",
        "parent": "https://github.com/apache/nifi/commit/4a16845309fce1f2cdbad3b31926563dd27b2c3c",
        "repo": "nifi",
        "unit_tests": [
            "TestStandardProcessScheduler.java"
        ]
    },
    "nifi_6fa5968": {
        "bug_id": "nifi_6fa5968",
        "commit": "https://github.com/apache/nifi/commit/6fa596884bcf47774ecb5d8b6a95b30b9c3e7c7b",
        "file": [
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/nifi/blob/6fa596884bcf47774ecb5d8b6a95b30b9c3e7c7b/nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-web/nifi-web-api/src/main/java/org/apache/nifi/web/util/SnippetUtils.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-web/nifi-web-api/src/main/java/org/apache/nifi/web/util/SnippetUtils.java?ref=6fa596884bcf47774ecb5d8b6a95b30b9c3e7c7b",
                "deletions": 5,
                "filename": "nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-web/nifi-web-api/src/main/java/org/apache/nifi/web/util/SnippetUtils.java",
                "patch": "@@ -194,13 +194,21 @@ public FlowSnippetDTO populateFlowSnippet(Snippet snippet, boolean recurse) {\n     }\n     \n     private void addControllerServicesToSnippet(final FlowSnippetDTO snippetDto) {\n-        for ( final ProcessorDTO processorDto : snippetDto.getProcessors() ) {\n-            addControllerServicesToSnippet(snippetDto, processorDto);\n+        final Set<ProcessorDTO> processors = snippetDto.getProcessors();\n+        if ( processors != null ) {\n+\t    \tfor ( final ProcessorDTO processorDto : processors ) {\n+\t            addControllerServicesToSnippet(snippetDto, processorDto);\n+\t        }\n         }\n         \n-        for ( final ProcessGroupDTO processGroupDto : snippetDto.getProcessGroups() ) {\n-            final FlowSnippetDTO childGroupDto = processGroupDto.getContents();\n-            addControllerServicesToSnippet(childGroupDto);\n+        final Set<ProcessGroupDTO> childGroups = snippetDto.getProcessGroups();\n+        if ( childGroups != null ) {\n+\t        for ( final ProcessGroupDTO processGroupDto : childGroups ) {\n+\t            final FlowSnippetDTO childGroupDto = processGroupDto.getContents();\n+\t            if ( childGroupDto != null ) {\n+\t            \taddControllerServicesToSnippet(childGroupDto);\n+\t            }\n+\t        }\n         }\n     }\n     ",
                "raw_url": "https://github.com/apache/nifi/raw/6fa596884bcf47774ecb5d8b6a95b30b9c3e7c7b/nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-web/nifi-web-api/src/main/java/org/apache/nifi/web/util/SnippetUtils.java",
                "sha": "76789c606df236ab8ae9de4f927bb1739908747f",
                "status": "modified"
            }
        ],
        "message": "NIFI-535: Fixed NPE in SnippetUtils",
        "parent": "https://github.com/apache/nifi/commit/c026dff40c469f37e29a07fbfaa64a2a4575f92c",
        "repo": "nifi",
        "unit_tests": [
            "SnippetUtilsTest.java"
        ]
    },
    "nifi_83a23f9": {
        "bug_id": "nifi_83a23f9",
        "commit": "https://github.com/apache/nifi/commit/83a23f90d46201d63593f13ea4be4d88013ad765",
        "file": [
            {
                "additions": 72,
                "blob_url": "https://github.com/apache/nifi/blob/83a23f90d46201d63593f13ea4be4d88013ad765/nifi-commons/nifi-security-utils/src/main/java/org/apache/nifi/security/util/SslContextFactory.java",
                "changes": 76,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-commons/nifi-security-utils/src/main/java/org/apache/nifi/security/util/SslContextFactory.java?ref=83a23f90d46201d63593f13ea4be4d88013ad765",
                "deletions": 4,
                "filename": "nifi-commons/nifi-security-utils/src/main/java/org/apache/nifi/security/util/SslContextFactory.java",
                "patch": "@@ -48,7 +48,9 @@\n     }\n \n     /**\n-     * Creates a SSLContext instance using the given information.\n+     * Creates a SSLContext instance using the given information. The password for the key is assumed to be the same\n+     * as the password for the keystore. If this is not the case, the {@link #createSslContext(String, char[], chart[], String, String, char[], String, ClientAuth, String)}\n+     * method should be used instead\n      *\n      * @param keystore the full path to the keystore\n      * @param keystorePasswd the keystore password\n@@ -74,13 +76,48 @@ public static SSLContext createSslContext(\n             throws KeyStoreException, IOException, NoSuchAlgorithmException, CertificateException,\n             UnrecoverableKeyException, KeyManagementException {\n \n+        // Pass the keystore password as both the keystore password and the key password.\n+        return createSslContext(keystore, keystorePasswd, keystorePasswd, keystoreType, truststore, truststorePasswd, truststoreType, clientAuth, protocol);\n+    }\n+\n+    /**\n+     * Creates a SSLContext instance using the given information.\n+     *\n+     * @param keystore the full path to the keystore\n+     * @param keystorePasswd the keystore password\n+     * @param keystoreType the type of keystore (e.g., PKCS12, JKS)\n+     * @param truststore the full path to the truststore\n+     * @param truststorePasswd the truststore password\n+     * @param truststoreType the type of truststore (e.g., PKCS12, JKS)\n+     * @param clientAuth the type of client authentication\n+     * @param protocol         the protocol to use for the SSL connection\n+     *\n+     * @return a SSLContext instance\n+     * @throws java.security.KeyStoreException if any issues accessing the keystore\n+     * @throws java.io.IOException for any problems loading the keystores\n+     * @throws java.security.NoSuchAlgorithmException if an algorithm is found to be used but is unknown\n+     * @throws java.security.cert.CertificateException if there is an issue with the certificate\n+     * @throws java.security.UnrecoverableKeyException if the key is insufficient\n+     * @throws java.security.KeyManagementException if unable to manage the key\n+     */\n+    public static SSLContext createSslContext(\n+            final String keystore, final char[] keystorePasswd, final char[] keyPasswd, final String keystoreType,\n+            final String truststore, final char[] truststorePasswd, final String truststoreType,\n+            final ClientAuth clientAuth, final String protocol)\n+            throws KeyStoreException, IOException, NoSuchAlgorithmException, CertificateException,\n+            UnrecoverableKeyException, KeyManagementException {\n+\n         // prepare the keystore\n         final KeyStore keyStore = KeyStore.getInstance(keystoreType);\n         try (final InputStream keyStoreStream = new FileInputStream(keystore)) {\n             keyStore.load(keyStoreStream, keystorePasswd);\n         }\n         final KeyManagerFactory keyManagerFactory = KeyManagerFactory.getInstance(KeyManagerFactory.getDefaultAlgorithm());\n-        keyManagerFactory.init(keyStore, keystorePasswd);\n+        if (keyPasswd == null) {\n+            keyManagerFactory.init(keyStore, keystorePasswd);\n+        } else {\n+            keyManagerFactory.init(keyStore, keyPasswd);\n+        }\n \n         // prepare the truststore\n         final KeyStore trustStore = KeyStore.getInstance(truststoreType);\n@@ -105,6 +142,33 @@ public static SSLContext createSslContext(\n \n     }\n \n+    /**\n+     * Creates a SSLContext instance using the given information. This method assumes that the key password is\n+     * the same as the keystore password. If this is not the case, use the {@link #createSslContext(String, char[], char[], String, String)}\n+     * method instead.\n+     *\n+     * @param keystore the full path to the keystore\n+     * @param keystorePasswd the keystore password\n+     * @param keystoreType the type of keystore (e.g., PKCS12, JKS)\n+     * @param protocol the protocol to use for the SSL connection\n+     *\n+     * @return a SSLContext instance\n+     * @throws java.security.KeyStoreException if any issues accessing the keystore\n+     * @throws java.io.IOException for any problems loading the keystores\n+     * @throws java.security.NoSuchAlgorithmException if an algorithm is found to be used but is unknown\n+     * @throws java.security.cert.CertificateException if there is an issue with the certificate\n+     * @throws java.security.UnrecoverableKeyException if the key is insufficient\n+     * @throws java.security.KeyManagementException if unable to manage the key\n+     */\n+    public static SSLContext createSslContext(\n+        final String keystore, final char[] keystorePasswd, final String keystoreType, final String protocol)\n+        throws KeyStoreException, IOException, NoSuchAlgorithmException, CertificateException,\n+        UnrecoverableKeyException, KeyManagementException {\n+\n+        // create SSL Context passing keystore password as the key password\n+        return createSslContext(keystore, keystorePasswd, keystorePasswd, keystoreType, protocol);\n+    }\n+\n     /**\n      * Creates a SSLContext instance using the given information.\n      *\n@@ -122,7 +186,7 @@ public static SSLContext createSslContext(\n      * @throws java.security.KeyManagementException if unable to manage the key\n      */\n     public static SSLContext createSslContext(\n-            final String keystore, final char[] keystorePasswd, final String keystoreType, final String protocol)\n+        final String keystore, final char[] keystorePasswd, final char[] keyPasswd, final String keystoreType, final String protocol)\n             throws KeyStoreException, IOException, NoSuchAlgorithmException, CertificateException,\n             UnrecoverableKeyException, KeyManagementException {\n \n@@ -132,7 +196,11 @@ public static SSLContext createSslContext(\n             keyStore.load(keyStoreStream, keystorePasswd);\n         }\n         final KeyManagerFactory keyManagerFactory = KeyManagerFactory.getInstance(KeyManagerFactory.getDefaultAlgorithm());\n-        keyManagerFactory.init(keyStore, keystorePasswd);\n+        if (keyPasswd == null) {\n+            keyManagerFactory.init(keyStore, keystorePasswd);\n+        } else {\n+            keyManagerFactory.init(keyStore, keyPasswd);\n+        }\n \n         // initialize the ssl context\n         final SSLContext ctx = SSLContext.getInstance(protocol);",
                "raw_url": "https://github.com/apache/nifi/raw/83a23f90d46201d63593f13ea4be4d88013ad765/nifi-commons/nifi-security-utils/src/main/java/org/apache/nifi/security/util/SslContextFactory.java",
                "sha": "b2d14585f17a237dcbf1c0c4a7ca05193a0d5432",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/nifi/blob/83a23f90d46201d63593f13ea4be4d88013ad765/nifi-nar-bundles/nifi-standard-services/nifi-ssl-context-bundle/nifi-ssl-context-service/src/main/java/org/apache/nifi/ssl/StandardSSLContextService.java",
                "changes": 32,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-standard-services/nifi-ssl-context-bundle/nifi-ssl-context-service/src/main/java/org/apache/nifi/ssl/StandardSSLContextService.java?ref=83a23f90d46201d63593f13ea4be4d88013ad765",
                "deletions": 2,
                "filename": "nifi-nar-bundles/nifi-standard-services/nifi-ssl-context-bundle/nifi-ssl-context-service/src/main/java/org/apache/nifi/ssl/StandardSSLContextService.java",
                "patch": "@@ -21,6 +21,7 @@\n import org.apache.nifi.annotation.lifecycle.OnEnabled;\n import org.apache.nifi.components.AllowableValue;\n import org.apache.nifi.components.PropertyDescriptor;\n+import org.apache.nifi.components.PropertyValue;\n import org.apache.nifi.components.ValidationContext;\n import org.apache.nifi.components.ValidationResult;\n import org.apache.nifi.components.Validator;\n@@ -96,6 +97,15 @@\n             .addValidator(StandardValidators.NON_EMPTY_VALIDATOR)\n             .sensitive(true)\n             .build();\n+    static final PropertyDescriptor KEY_PASSWORD = new PropertyDescriptor.Builder()\n+            .name(\"key-password\")\n+            .displayName(\"Key Password\")\n+            .description(\"The password for the key. If this is not specified, but the Keystore Filename, Password, and Type are specified, \"\n+                + \"then the Keystore Password will be assumed to be the same as the Key Password.\")\n+            .addValidator(StandardValidators.NON_EMPTY_VALIDATOR)\n+            .sensitive(true)\n+            .required(false)\n+            .build();\n     public static final PropertyDescriptor SSL_ALGORITHM = new PropertyDescriptor.Builder()\n             .name(\"SSL Protocol\")\n             .defaultValue(\"TLS\")\n@@ -113,6 +123,7 @@\n         List<PropertyDescriptor> props = new ArrayList<>();\n         props.add(KEYSTORE);\n         props.add(KEYSTORE_PASSWORD);\n+        props.add(KEY_PASSWORD);\n         props.add(KEYSTORE_TYPE);\n         props.add(TRUSTSTORE);\n         props.add(TRUSTSTORE_PASSWORD);\n@@ -223,6 +234,9 @@ public ValidationResult validate(String subject, String input, ValidationContext\n     private void verifySslConfig(final ValidationContext validationContext) throws ProcessException {\n         final String protocol = validationContext.getProperty(SSL_ALGORITHM).getValue();\n         try {\n+            final PropertyValue keyPasswdProp = validationContext.getProperty(KEY_PASSWORD);\n+            final char[] keyPassword = keyPasswdProp.isSet() ? keyPasswdProp.getValue().toCharArray() : null;\n+\n             final String keystoreFile = validationContext.getProperty(KEYSTORE).getValue();\n             if (keystoreFile == null) {\n                 SslContextFactory.createTrustSslContext(\n@@ -237,6 +251,7 @@ private void verifySslConfig(final ValidationContext validationContext) throws P\n                 SslContextFactory.createSslContext(\n                         validationContext.getProperty(KEYSTORE).getValue(),\n                         validationContext.getProperty(KEYSTORE_PASSWORD).getValue().toCharArray(),\n+                    keyPassword,\n                         validationContext.getProperty(KEYSTORE_TYPE).getValue(),\n                         protocol);\n                 return;\n@@ -245,6 +260,7 @@ private void verifySslConfig(final ValidationContext validationContext) throws P\n             SslContextFactory.createSslContext(\n                     validationContext.getProperty(KEYSTORE).getValue(),\n                     validationContext.getProperty(KEYSTORE_PASSWORD).getValue().toCharArray(),\n+                keyPassword,\n                     validationContext.getProperty(KEYSTORE_TYPE).getValue(),\n                     validationContext.getProperty(TRUSTSTORE).getValue(),\n                     validationContext.getProperty(TRUSTSTORE_PASSWORD).getValue().toCharArray(),\n@@ -260,26 +276,34 @@ private void verifySslConfig(final ValidationContext validationContext) throws P\n     public SSLContext createSSLContext(final ClientAuth clientAuth) throws ProcessException {\n         final String protocol = configContext.getProperty(SSL_ALGORITHM).getValue();\n         try {\n+            final PropertyValue keyPasswdProp = configContext.getProperty(KEY_PASSWORD);\n+            final char[] keyPassword = keyPasswdProp.isSet() ? keyPasswdProp.getValue().toCharArray() : null;\n+\n             final String keystoreFile = configContext.getProperty(KEYSTORE).getValue();\n             if (keystoreFile == null) {\n+                // If keystore not specified, create SSL Context based only on trust store.\n                 return SslContextFactory.createTrustSslContext(\n                         configContext.getProperty(TRUSTSTORE).getValue(),\n                         configContext.getProperty(TRUSTSTORE_PASSWORD).getValue().toCharArray(),\n                         configContext.getProperty(TRUSTSTORE_TYPE).getValue(),\n                         protocol);\n             }\n+\n             final String truststoreFile = configContext.getProperty(TRUSTSTORE).getValue();\n             if (truststoreFile == null) {\n+                // If truststore not specified, create SSL Context based only on key store.\n                 return SslContextFactory.createSslContext(\n                         configContext.getProperty(KEYSTORE).getValue(),\n                         configContext.getProperty(KEYSTORE_PASSWORD).getValue().toCharArray(),\n+                        keyPassword,\n                         configContext.getProperty(KEYSTORE_TYPE).getValue(),\n                         protocol);\n             }\n \n             return SslContextFactory.createSslContext(\n                     configContext.getProperty(KEYSTORE).getValue(),\n                     configContext.getProperty(KEYSTORE_PASSWORD).getValue().toCharArray(),\n+                    keyPassword,\n                     configContext.getProperty(KEYSTORE_TYPE).getValue(),\n                     configContext.getProperty(TRUSTSTORE).getValue(),\n                     configContext.getProperty(TRUSTSTORE_PASSWORD).getValue().toCharArray(),\n@@ -326,6 +350,11 @@ public String getKeyStorePassword() {\n         return configContext.getProperty(KEYSTORE_PASSWORD).getValue();\n     }\n \n+    @Override\n+    public String getKeyPassword() {\n+        return configContext.getProperty(KEY_PASSWORD).getValue();\n+    }\n+\n     @Override\n     public boolean isKeyStoreConfigured() {\n         return getKeyStoreFile() != null && getKeyStorePassword() != null && getKeyStoreType() != null;\n@@ -371,8 +400,7 @@ public String getSslAlgorithm() {\n                         .build());\n             } else {\n                 try {\n-                    final boolean storeValid = CertificateUtils\n-                            .isStoreValid(file.toURI().toURL(), KeystoreType.valueOf(type), password.toCharArray());\n+                    final boolean storeValid = CertificateUtils.isStoreValid(file.toURI().toURL(), KeystoreType.valueOf(type), password.toCharArray());\n                     if (!storeValid) {\n                         results.add(new ValidationResult.Builder()\n                                 .subject(keystoreDesc + \" Properties\")",
                "raw_url": "https://github.com/apache/nifi/raw/83a23f90d46201d63593f13ea4be4d88013ad765/nifi-nar-bundles/nifi-standard-services/nifi-ssl-context-bundle/nifi-ssl-context-service/src/main/java/org/apache/nifi/ssl/StandardSSLContextService.java",
                "sha": "81be148ccfb2cc8cb5981ca962de7bd2c62af9fd",
                "status": "modified"
            },
            {
                "additions": 43,
                "blob_url": "https://github.com/apache/nifi/blob/83a23f90d46201d63593f13ea4be4d88013ad765/nifi-nar-bundles/nifi-standard-services/nifi-ssl-context-bundle/nifi-ssl-context-service/src/test/java/org/apache/nifi/ssl/SSLContextServiceTest.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-standard-services/nifi-ssl-context-bundle/nifi-ssl-context-service/src/test/java/org/apache/nifi/ssl/SSLContextServiceTest.java?ref=83a23f90d46201d63593f13ea4be4d88013ad765",
                "deletions": 1,
                "filename": "nifi-nar-bundles/nifi-standard-services/nifi-ssl-context-bundle/nifi-ssl-context-service/src/test/java/org/apache/nifi/ssl/SSLContextServiceTest.java",
                "patch": "@@ -109,7 +109,7 @@ public void testGood() throws InitializationException {\n         runner.assertValid(service);\n         service = (SSLContextService) runner.getProcessContext().getControllerServiceLookup().getControllerService(\"test-good1\");\n         Assert.assertNotNull(service);\n-        SSLContextService sslService = (SSLContextService) service;\n+        SSLContextService sslService = service;\n         sslService.createSSLContext(ClientAuth.REQUIRED);\n         sslService.createSSLContext(ClientAuth.WANT);\n         sslService.createSSLContext(ClientAuth.NONE);\n@@ -160,4 +160,46 @@ public void testGoodKeyOnly() {\n         }\n     }\n \n+    @Test\n+    public void testDifferentKeyPassword() {\n+        try {\n+            final TestRunner runner = TestRunners.newTestRunner(TestProcessor.class);\n+            final SSLContextService service = new StandardSSLContextService();\n+            final Map<String, String> properties = new HashMap<String, String>();\n+            properties.put(StandardSSLContextService.KEYSTORE.getName(), \"src/test/resources/diffpass-ks.jks\");\n+            properties.put(StandardSSLContextService.KEYSTORE_PASSWORD.getName(), \"storepassword\");\n+            properties.put(StandardSSLContextService.KEY_PASSWORD.getName(), \"keypassword\");\n+            properties.put(StandardSSLContextService.KEYSTORE_TYPE.getName(), \"JKS\");\n+            runner.addControllerService(\"test-diff-keys\", service, properties);\n+            runner.enableControllerService(service);\n+\n+            runner.setProperty(\"SSL Context Svc ID\", \"test-diff-keys\");\n+            runner.assertValid();\n+            Assert.assertNotNull(service);\n+            Assert.assertTrue(service instanceof StandardSSLContextService);\n+            SSLContextService sslService = service;\n+            sslService.createSSLContext(ClientAuth.NONE);\n+        } catch (Exception e) {\n+            System.out.println(e);\n+            Assert.fail(\"Should not have thrown a exception \" + e.getMessage());\n+        }\n+    }\n+\n+    @Test\n+    public void testDifferentKeyPasswordWithoutSpecifyingPassword() {\n+        try {\n+            final TestRunner runner = TestRunners.newTestRunner(TestProcessor.class);\n+            final SSLContextService service = new StandardSSLContextService();\n+            final Map<String, String> properties = new HashMap<String, String>();\n+            properties.put(StandardSSLContextService.KEYSTORE.getName(), \"src/test/resources/diffpass-ks.jks\");\n+            properties.put(StandardSSLContextService.KEYSTORE_PASSWORD.getName(), \"storepassword\");\n+            properties.put(StandardSSLContextService.KEYSTORE_TYPE.getName(), \"JKS\");\n+            runner.addControllerService(\"test-diff-keys\", service, properties);\n+\n+            runner.assertNotValid(service);\n+        } catch (Exception e) {\n+            System.out.println(e);\n+            Assert.fail(\"Should not have thrown a exception \" + e.getMessage());\n+        }\n+    }\n }",
                "raw_url": "https://github.com/apache/nifi/raw/83a23f90d46201d63593f13ea4be4d88013ad765/nifi-nar-bundles/nifi-standard-services/nifi-ssl-context-bundle/nifi-ssl-context-service/src/test/java/org/apache/nifi/ssl/SSLContextServiceTest.java",
                "sha": "a7719148f4c5f2b40f56c85124ae590f543b830f",
                "status": "modified"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/nifi/blob/83a23f90d46201d63593f13ea4be4d88013ad765/nifi-nar-bundles/nifi-standard-services/nifi-ssl-context-bundle/nifi-ssl-context-service/src/test/resources/diffpass-ks.jks",
                "changes": 0,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-standard-services/nifi-ssl-context-bundle/nifi-ssl-context-service/src/test/resources/diffpass-ks.jks?ref=83a23f90d46201d63593f13ea4be4d88013ad765",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-standard-services/nifi-ssl-context-bundle/nifi-ssl-context-service/src/test/resources/diffpass-ks.jks",
                "raw_url": "https://github.com/apache/nifi/raw/83a23f90d46201d63593f13ea4be4d88013ad765/nifi-nar-bundles/nifi-standard-services/nifi-ssl-context-bundle/nifi-ssl-context-service/src/test/resources/diffpass-ks.jks",
                "sha": "c4bd59c6554ba69f722bb36e7c3bcccbe4f15ea0",
                "status": "added"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/nifi/blob/83a23f90d46201d63593f13ea4be4d88013ad765/nifi-nar-bundles/nifi-standard-services/nifi-ssl-context-service-api/src/main/java/org/apache/nifi/ssl/SSLContextService.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-standard-services/nifi-ssl-context-service-api/src/main/java/org/apache/nifi/ssl/SSLContextService.java?ref=83a23f90d46201d63593f13ea4be4d88013ad765",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-standard-services/nifi-ssl-context-service-api/src/main/java/org/apache/nifi/ssl/SSLContextService.java",
                "patch": "@@ -55,6 +55,8 @@\n \n     public String getKeyStorePassword();\n \n+    public String getKeyPassword();\n+\n     public boolean isKeyStoreConfigured();\n \n     String getSslAlgorithm();",
                "raw_url": "https://github.com/apache/nifi/raw/83a23f90d46201d63593f13ea4be4d88013ad765/nifi-nar-bundles/nifi-standard-services/nifi-ssl-context-service-api/src/main/java/org/apache/nifi/ssl/SSLContextService.java",
                "sha": "94f8bda5262f7baaa35a4eb3636a3ee07e884134",
                "status": "modified"
            }
        ],
        "message": "NIFI-2466: Added option to provide separate key password to StandardSSLContextService.\n\nFixed NPE (+2 squashed commits)\nSquashed commits:\n[c5d521a] NIFI-2466: Added unit test to verify changes; fixed validation\n[aa4d418] NIFI-2446: Add option to specify key password when different than keystore password\n\nThis closes #776.\n\nSigned-off-by: Andy LoPresto <alopresto@apache.org>",
        "parent": "https://github.com/apache/nifi/commit/1511887a6825c1bc96285b8b3ac999cb9d2601cb",
        "repo": "nifi",
        "unit_tests": [
            "SSLContextServiceTest.java"
        ]
    },
    "nifi_8d960f5": {
        "bug_id": "nifi_8d960f5",
        "commit": "https://github.com/apache/nifi/commit/8d960f52436a516776d6fe775be6c6f47645078b",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/nifi/blob/8d960f52436a516776d6fe775be6c6f47645078b/nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-processors/src/main/java/org/apache/nifi/processors/kafka/PutKafka.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-processors/src/main/java/org/apache/nifi/processors/kafka/PutKafka.java?ref=8d960f52436a516776d6fe775be6c6f47645078b",
                "deletions": 1,
                "filename": "nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-processors/src/main/java/org/apache/nifi/processors/kafka/PutKafka.java",
                "patch": "@@ -408,7 +408,7 @@ private SplittableMessageContext buildMessageContext(FlowFile flowFile, ProcessC\n         String failedSegmentsString = flowFile.getAttribute(ATTR_FAILED_SEGMENTS);\n         if (flowFile.getAttribute(ATTR_PROC_ID) != null && flowFile.getAttribute(ATTR_PROC_ID).equals(this.getIdentifier()) && failedSegmentsString != null) {\n             topicName = flowFile.getAttribute(ATTR_TOPIC);\n-            key = flowFile.getAttribute(ATTR_KEY).getBytes();\n+            key = flowFile.getAttribute(ATTR_KEY) == null ? null : flowFile.getAttribute(ATTR_KEY).getBytes();\n             delimiterPattern = flowFile.getAttribute(ATTR_DELIMITER);\n         } else {\n             failedSegmentsString = null;",
                "raw_url": "https://github.com/apache/nifi/raw/8d960f52436a516776d6fe775be6c6f47645078b/nifi-nar-bundles/nifi-kafka-bundle/nifi-kafka-processors/src/main/java/org/apache/nifi/processors/kafka/PutKafka.java",
                "sha": "513f4f315061bb10bc327baacd5b04d6cd85519e",
                "status": "modified"
            }
        ],
        "message": "NIFI-1684 fixed NPE in PutKafka when retrieving key attribute bytes\n\nSigned-off-by: joewitt <joewitt@apache.org>",
        "parent": "https://github.com/apache/nifi/commit/e35c40b0fd6c707e4c2f6cbf1bda7c6ab610ce3c",
        "repo": "nifi",
        "unit_tests": [
            "PutKafkaTest.java"
        ]
    },
    "nifi_96ed405": {
        "bug_id": "nifi_96ed405",
        "commit": "https://github.com/apache/nifi/commit/96ed405d708894ee5400ebbdbf335325219faa09",
        "file": [
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-api/src/main/java/org/apache/nifi/provenance/ProvenanceEventRecord.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-api/src/main/java/org/apache/nifi/provenance/ProvenanceEventRecord.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-api/src/main/java/org/apache/nifi/provenance/ProvenanceEventRecord.java",
                "patch": "@@ -79,6 +79,16 @@\n      */\n     Map<String, String> getAttributes();\n \n+    /**\n+     * Returns the attribute with the given name\n+     *\n+     * @param attributeName the name of the attribute to get\n+     * @return the attribute with the given name or <code>null</code> if no attribute exists with the given name\n+     */\n+    default String getAttribute(String attributeName) {\n+        return getAttributes().get(attributeName);\n+    }\n+\n     /**\n      * @return all FlowFile attributes that existed on the FlowFile before this\n      * event occurred",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-api/src/main/java/org/apache/nifi/provenance/ProvenanceEventRecord.java",
                "sha": "b05bd85932a5273fa6bd23f38be3cfc26c0f49c3",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-bootstrap/src/main/java/org/apache/nifi/bootstrap/RunNiFi.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-bootstrap/src/main/java/org/apache/nifi/bootstrap/RunNiFi.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 2,
                "filename": "nifi-bootstrap/src/main/java/org/apache/nifi/bootstrap/RunNiFi.java",
                "patch": "@@ -1067,11 +1067,14 @@ public boolean accept(final File dir, final String filename) {\n         Process process = builder.start();\n         handleLogging(process);\n         Long pid = getPid(process, cmdLogger);\n-        if (pid != null) {\n+        if (pid == null) {\n+            cmdLogger.info(\"Launched Apache NiFi but could not determined the Process ID\");\n+        } else {\n             nifiPid = pid;\n             final Properties pidProperties = new Properties();\n             pidProperties.setProperty(PID_KEY, String.valueOf(nifiPid));\n             savePidProperties(pidProperties, cmdLogger);\n+            cmdLogger.info(\"Launched Apache NiFi with Process ID \" + pid);\n         }\n \n         shutdownHook = new ShutdownHook(process, this, secretKey, gracefulShutdownSeconds, loggingExecutor);\n@@ -1129,11 +1132,14 @@ public boolean accept(final File dir, final String filename) {\n                     handleLogging(process);\n \n                     pid = getPid(process, defaultLogger);\n-                    if (pid != null) {\n+                    if (pid == null) {\n+                        cmdLogger.info(\"Launched Apache NiFi but could not obtain the Process ID\");\n+                    } else {\n                         nifiPid = pid;\n                         final Properties pidProperties = new Properties();\n                         pidProperties.setProperty(PID_KEY, String.valueOf(nifiPid));\n                         savePidProperties(pidProperties, defaultLogger);\n+                        cmdLogger.info(\"Launched Apache NiFi with Process ID \" + pid);\n                     }\n \n                     shutdownHook = new ShutdownHook(process, this, secretKey, gracefulShutdownSeconds, loggingExecutor);",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-bootstrap/src/main/java/org/apache/nifi/bootstrap/RunNiFi.java",
                "sha": "10d5cde9d366192277120347f2c606c50d4f6f67",
                "status": "modified"
            },
            {
                "additions": 34,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/ProgressiveResult.java",
                "changes": 34,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/ProgressiveResult.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/ProgressiveResult.java",
                "patch": "@@ -0,0 +1,34 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance;\n+\n+import java.util.Collection;\n+\n+/**\n+ * A Provenance query result that is capable of being updated\n+ */\n+public interface ProgressiveResult {\n+\n+    void update(Collection<ProvenanceEventRecord> records, long totalHitCount);\n+\n+    void setError(String error);\n+\n+    long getTotalHitCount();\n+\n+    boolean isFinished();\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/ProgressiveResult.java",
                "sha": "e2dd33b16cd4d7b497f9ec20eb9e40ede451cf58",
                "status": "added"
            },
            {
                "additions": 45,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/StandardLineageResult.java",
                "changes": 47,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/StandardLineageResult.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 2,
                "filename": "nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/StandardLineageResult.java",
                "patch": "@@ -44,7 +44,7 @@\n /**\n  *\n  */\n-public class StandardLineageResult implements ComputeLineageResult {\n+public class StandardLineageResult implements ComputeLineageResult, ProgressiveResult {\n \n     public static final int TTL = (int) TimeUnit.MILLISECONDS.convert(30, TimeUnit.MINUTES);\n     private static final Logger logger = LoggerFactory.getLogger(StandardLineageResult.class);\n@@ -66,6 +66,7 @@\n     private int numCompletedSteps = 0;\n \n     private volatile boolean canceled = false;\n+    private final Object completionMonitor = new Object();\n \n     public StandardLineageResult(final int numSteps, final Collection<String> flowFileUuids) {\n         this.numSteps = numSteps;\n@@ -162,6 +163,7 @@ public boolean isFinished() {\n         }\n     }\n \n+    @Override\n     public void setError(final String error) {\n         writeLock.lock();\n         try {\n@@ -178,7 +180,10 @@ public void setError(final String error) {\n         }\n     }\n \n-    public void update(final Collection<ProvenanceEventRecord> records) {\n+    @Override\n+    public void update(final Collection<ProvenanceEventRecord> records, final long totalHitCount) {\n+        boolean computationComplete = false;\n+\n         writeLock.lock();\n         try {\n             relevantRecords.addAll(records);\n@@ -187,12 +192,22 @@ public void update(final Collection<ProvenanceEventRecord> records) {\n             updateExpiration();\n \n             if (numCompletedSteps >= numSteps && error == null) {\n+                computationComplete = true;\n                 computeLineage();\n                 computationNanos = System.nanoTime() - creationNanos;\n             }\n         } finally {\n             writeLock.unlock();\n         }\n+\n+        if (computationComplete) {\n+            final long computationMillis = TimeUnit.NANOSECONDS.toMillis(computationNanos);\n+            logger.info(\"Completed computation of lineage for FlowFile UUID(s) {} comprised of {} steps in {} millis\", flowFileUuids, numSteps, computationMillis);\n+\n+            synchronized (completionMonitor) {\n+                completionMonitor.notifyAll();\n+            }\n+        }\n     }\n \n     /**\n@@ -201,6 +216,7 @@ public void update(final Collection<ProvenanceEventRecord> records) {\n      * useful after all of the records have been successfully obtained\n      */\n     private void computeLineage() {\n+        logger.debug(\"Computing lineage with the following events: {}\", relevantRecords);\n         final long startNanos = System.nanoTime();\n \n         nodes.clear();\n@@ -324,4 +340,31 @@ void cancel() {\n     private void updateExpiration() {\n         expirationDate = new Date(System.currentTimeMillis() + TTL);\n     }\n+\n+    @Override\n+    public boolean awaitCompletion(final long time, final TimeUnit unit) throws InterruptedException {\n+        final long finishTime = System.currentTimeMillis() + unit.toMillis(time);\n+        synchronized (completionMonitor) {\n+            while (!isFinished()) {\n+                final long millisToWait = finishTime - System.currentTimeMillis();\n+                if (millisToWait > 0) {\n+                    completionMonitor.wait(millisToWait);\n+                } else {\n+                    return isFinished();\n+                }\n+            }\n+        }\n+\n+        return isFinished();\n+    }\n+\n+    @Override\n+    public long getTotalHitCount() {\n+        readLock.lock();\n+        try {\n+            return relevantRecords.size();\n+        } finally {\n+            readLock.unlock();\n+        }\n+    }\n }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/StandardLineageResult.java",
                "sha": "2b7b34a864bebfb77d6f3b0f39e29890032c04e6",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/StandardProvenanceEventRecord.java",
                "changes": 14,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/StandardProvenanceEventRecord.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 3,
                "filename": "nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/StandardProvenanceEventRecord.java",
                "patch": "@@ -67,7 +67,7 @@\n     private final Map<String, String> previousAttributes;\n     private final Map<String, String> updatedAttributes;\n \n-    private volatile long eventId;\n+    private volatile long eventId = -1L;\n \n     private StandardProvenanceEventRecord(final Builder builder) {\n         this.eventTime = builder.eventTime;\n@@ -369,14 +369,22 @@ private boolean different(final List<String> a, final List<String> b) {\n             return false;\n         }\n \n-        if (a == null && b != null) {\n+        if (a == null && b != null && !b.isEmpty()) {\n             return true;\n         }\n \n-        if (a != null && b == null) {\n+        if (a == null && b.isEmpty()) {\n+            return false;\n+        }\n+\n+        if (a != null && !a.isEmpty() && b == null) {\n             return true;\n         }\n \n+        if (a.isEmpty() && b == null) {\n+            return false;\n+        }\n+\n         if (a.size() != b.size()) {\n             return true;\n         }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/StandardProvenanceEventRecord.java",
                "sha": "ac60d4f68069256d677c241704539e21f65a69c5",
                "status": "modified"
            },
            {
                "additions": 66,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/StandardQueryResult.java",
                "changes": 91,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/StandardQueryResult.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 25,
                "filename": "nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/StandardQueryResult.java",
                "patch": "@@ -22,7 +22,7 @@\n import java.util.Date;\n import java.util.Iterator;\n import java.util.List;\n-import java.util.Set;\n+import java.util.SortedSet;\n import java.util.TreeSet;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.locks.Lock;\n@@ -31,8 +31,11 @@\n \n import org.apache.nifi.provenance.search.Query;\n import org.apache.nifi.provenance.search.QueryResult;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n-public class StandardQueryResult implements QueryResult {\n+public class StandardQueryResult implements QueryResult, ProgressiveResult {\n+    private static final Logger logger = LoggerFactory.getLogger(StandardQueryResult.class);\n \n     public static final int TTL = (int) TimeUnit.MILLISECONDS.convert(30, TimeUnit.MINUTES);\n     private final Query query;\n@@ -44,12 +47,12 @@\n \n     private final Lock writeLock = rwLock.writeLock();\n     // guarded by writeLock\n-    private final Set<ProvenanceEventRecord> matchingRecords = new TreeSet<>(new EventIdComparator());\n-    private long totalHitCount;\n+    private final SortedSet<ProvenanceEventRecord> matchingRecords = new TreeSet<>(new EventIdComparator());\n     private int numCompletedSteps = 0;\n     private Date expirationDate;\n     private String error;\n     private long queryTime;\n+    private final Object completionMonitor = new Object();\n \n     private volatile boolean canceled = false;\n \n@@ -65,22 +68,7 @@ public StandardQueryResult(final Query query, final int numSteps) {\n     public List<ProvenanceEventRecord> getMatchingEvents() {\n         readLock.lock();\n         try {\n-            if (matchingRecords.size() <= query.getMaxResults()) {\n-                return new ArrayList<>(matchingRecords);\n-            }\n-\n-            final List<ProvenanceEventRecord> copy = new ArrayList<>(query.getMaxResults());\n-\n-            int i = 0;\n-            final Iterator<ProvenanceEventRecord> itr = matchingRecords.iterator();\n-            while (itr.hasNext()) {\n-                copy.add(itr.next());\n-                if (++i >= query.getMaxResults()) {\n-                    break;\n-                }\n-            }\n-\n-            return copy;\n+            return new ArrayList<>(matchingRecords);\n         } finally {\n             readLock.unlock();\n         }\n@@ -137,7 +125,7 @@ public int getPercentComplete() {\n     public boolean isFinished() {\n         readLock.lock();\n         try {\n-            return numCompletedSteps >= numSteps || canceled;\n+            return numCompletedSteps >= numSteps || canceled || matchingRecords.size() >= query.getMaxResults();\n         } finally {\n             readLock.unlock();\n         }\n@@ -147,6 +135,7 @@ void cancel() {\n         this.canceled = true;\n     }\n \n+    @Override\n     public void setError(final String error) {\n         writeLock.lock();\n         try {\n@@ -163,22 +152,74 @@ public void setError(final String error) {\n         }\n     }\n \n-    public void update(final Collection<ProvenanceEventRecord> matchingRecords, final long totalHits) {\n+    @Override\n+    public void update(final Collection<ProvenanceEventRecord> newEvents, final long totalHits) {\n+        boolean queryComplete = false;\n+\n         writeLock.lock();\n         try {\n-            this.matchingRecords.addAll(matchingRecords);\n-            this.totalHitCount += totalHits;\n+            if (isFinished()) {\n+                return;\n+            }\n+\n+            this.matchingRecords.addAll(newEvents);\n+\n+            // If we've added more records than the query's max, then remove the trailing elements.\n+            // We do this, rather than avoiding the addition of the elements because we want to choose\n+            // the events with the largest ID.\n+            if (matchingRecords.size() > query.getMaxResults()) {\n+                final Iterator<ProvenanceEventRecord> itr = matchingRecords.iterator();\n+                for (int i = 0; i < query.getMaxResults(); i++) {\n+                    itr.next();\n+                }\n+\n+                while (itr.hasNext()) {\n+                    itr.next();\n+                    itr.remove();\n+                }\n+            }\n \n             numCompletedSteps++;\n             updateExpiration();\n \n-            if (numCompletedSteps >= numSteps) {\n+            if (numCompletedSteps >= numSteps || this.matchingRecords.size() >= query.getMaxResults()) {\n                 final long searchNanos = System.nanoTime() - creationNanos;\n                 queryTime = TimeUnit.MILLISECONDS.convert(searchNanos, TimeUnit.NANOSECONDS);\n+                queryComplete = true;\n+\n+                if (numCompletedSteps >= numSteps) {\n+                    logger.info(\"Completed {} comprised of {} steps in {} millis\", query, numSteps, queryTime);\n+                } else {\n+                    logger.info(\"Completed {} comprised of {} steps in {} millis (only completed {} steps because the maximum number of results was reached)\",\n+                        query, numSteps, queryTime, numCompletedSteps);\n+                }\n             }\n         } finally {\n             writeLock.unlock();\n         }\n+\n+        if (queryComplete) {\n+            synchronized (completionMonitor) {\n+                completionMonitor.notifyAll();\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public boolean awaitCompletion(final long time, final TimeUnit unit) throws InterruptedException {\n+        final long finishTime = System.currentTimeMillis() + unit.toMillis(time);\n+        synchronized (completionMonitor) {\n+            while (!isFinished()) {\n+                final long millisToWait = finishTime - System.currentTimeMillis();\n+                if (millisToWait > 0) {\n+                    completionMonitor.wait(millisToWait);\n+                } else {\n+                    return isFinished();\n+                }\n+            }\n+        }\n+\n+        return isFinished();\n     }\n \n     /**",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/StandardQueryResult.java",
                "sha": "277733970af1dcb475a7497ce94b8277b3d01c93",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/lineage/EventNode.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/lineage/EventNode.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 2,
                "filename": "nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/lineage/EventNode.java",
                "patch": "@@ -18,7 +18,6 @@\n \n import java.util.List;\n \n-import org.apache.nifi.flowfile.attributes.CoreAttributes;\n import org.apache.nifi.provenance.ProvenanceEventRecord;\n import org.apache.nifi.provenance.ProvenanceEventType;\n \n@@ -58,7 +57,7 @@ public long getEventIdentifier() {\n \n     @Override\n     public String getFlowFileUuid() {\n-        return record.getAttributes().get(CoreAttributes.UUID.key());\n+        return record.getFlowFileUuid();\n     }\n \n     @Override",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-data-provenance-utils/src/main/java/org/apache/nifi/provenance/lineage/EventNode.java",
                "sha": "4906ea39caeb2d2570170ace3de4c8c1b0bff78e",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-schema-utils/src/main/java/org/apache/nifi/repository/schema/FieldMapRecord.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-commons/nifi-schema-utils/src/main/java/org/apache/nifi/repository/schema/FieldMapRecord.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-commons/nifi-schema-utils/src/main/java/org/apache/nifi/repository/schema/FieldMapRecord.java",
                "patch": "@@ -29,6 +29,11 @@ public FieldMapRecord(final Map<RecordField, Object> values, final RecordSchema\n         this.values = convertFieldToName(values);\n     }\n \n+    public FieldMapRecord(final RecordSchema schema, final Map<String, Object> values) {\n+        this.schema = schema;\n+        this.values = new HashMap<>(values);\n+    }\n+\n     private static Map<String, Object> convertFieldToName(final Map<RecordField, Object> map) {\n         final Map<String, Object> nameMap = new HashMap<>(map.size());\n         for (final Map.Entry<RecordField, Object> entry : map.entrySet()) {",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-schema-utils/src/main/java/org/apache/nifi/repository/schema/FieldMapRecord.java",
                "sha": "acebcb9b483f489ac5da0c49fa240dac7483b5fe",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-schema-utils/src/main/java/org/apache/nifi/repository/schema/RecordSchema.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-commons/nifi-schema-utils/src/main/java/org/apache/nifi/repository/schema/RecordSchema.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 1,
                "filename": "nifi-commons/nifi-schema-utils/src/main/java/org/apache/nifi/repository/schema/RecordSchema.java",
                "patch": "@@ -120,8 +120,8 @@ public static RecordSchema readFrom(final InputStream in) throws IOException {\n \n     @SuppressWarnings(\"unchecked\")\n     private static RecordField readField(final DataInputStream dis) throws IOException {\n-        final Map<String, Object> schemaFieldMap = new HashMap<>();\n         final int numElementsToRead = dis.readInt();\n+        final Map<String, Object> schemaFieldMap = new HashMap<>(numElementsToRead);\n         for (int i = 0; i < numElementsToRead; i++) {\n             final String fieldName = dis.readUTF();\n             final String typeName = dis.readUTF();",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-schema-utils/src/main/java/org/apache/nifi/repository/schema/RecordSchema.java",
                "sha": "fe187652a06bd32b29f27694841275ec59505c10",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-schema-utils/src/main/java/org/apache/nifi/repository/schema/SchemaRecordReader.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-commons/nifi-schema-utils/src/main/java/org/apache/nifi/repository/schema/SchemaRecordReader.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 1,
                "filename": "nifi-commons/nifi-schema-utils/src/main/java/org/apache/nifi/repository/schema/SchemaRecordReader.java",
                "patch": "@@ -109,7 +109,15 @@ private Object readField(final InputStream in, final RecordField field) throws I\n             }\n         }\n \n-        return readFieldValue(in, field.getFieldType(), field.getFieldName(), field.getSubFields());\n+        try {\n+            return readFieldValue(in, field.getFieldType(), field.getFieldName(), field.getSubFields());\n+        } catch (final EOFException eof) {\n+            final EOFException exception = new EOFException(\"Failed to read field '\" + field.getFieldName() + \"'\");\n+            exception.addSuppressed(eof);\n+            throw exception;\n+        } catch (final IOException ioe) {\n+            throw new IOException(\"Failed to read field '\" + field.getFieldName() + \"'\", ioe);\n+        }\n     }\n \n ",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-schema-utils/src/main/java/org/apache/nifi/repository/schema/SchemaRecordReader.java",
                "sha": "84f353231acda7240fe35fe3b4b1da64c2fcfe64",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-schema-utils/src/main/java/org/apache/nifi/repository/schema/SchemaRecordWriter.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-commons/nifi-schema-utils/src/main/java/org/apache/nifi/repository/schema/SchemaRecordWriter.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 1,
                "filename": "nifi-commons/nifi-schema-utils/src/main/java/org/apache/nifi/repository/schema/SchemaRecordWriter.java",
                "patch": "@@ -44,8 +44,12 @@ public void writeRecord(final Record record, final OutputStream out) throws IOEx\n     }\n \n     private void writeRecordFields(final Record record, final OutputStream out) throws IOException {\n+        writeRecordFields(record, record.getSchema(), out);\n+    }\n+\n+    private void writeRecordFields(final Record record, final RecordSchema schema, final OutputStream out) throws IOException {\n         final DataOutputStream dos = out instanceof DataOutputStream ? (DataOutputStream) out : new DataOutputStream(out);\n-        for (final RecordField field : record.getSchema().getFields()) {\n+        for (final RecordField field : schema.getFields()) {\n             final Object value = record.getFieldValue(field);\n \n             try {",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-schema-utils/src/main/java/org/apache/nifi/repository/schema/SchemaRecordWriter.java",
                "sha": "5305e5bd01712c5646cf5a4ebbd31601e5079c76",
                "status": "modified"
            },
            {
                "additions": 74,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-write-ahead-log/src/main/java/org/wali/MinimalLockingWriteAheadLog.java",
                "changes": 165,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-commons/nifi-write-ahead-log/src/main/java/org/wali/MinimalLockingWriteAheadLog.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 91,
                "filename": "nifi-commons/nifi-write-ahead-log/src/main/java/org/wali/MinimalLockingWriteAheadLog.java",
                "patch": "@@ -18,6 +18,9 @@\n \n import static java.util.Objects.requireNonNull;\n \n+import java.io.BufferedInputStream;\n+import java.io.BufferedOutputStream;\n+import java.io.ByteArrayOutputStream;\n import java.io.DataInputStream;\n import java.io.DataOutputStream;\n import java.io.EOFException;\n@@ -55,12 +58,9 @@\n import java.util.concurrent.atomic.AtomicLong;\n import java.util.concurrent.locks.Lock;\n import java.util.concurrent.locks.ReadWriteLock;\n-import java.util.concurrent.locks.ReentrantLock;\n import java.util.concurrent.locks.ReentrantReadWriteLock;\n import java.util.regex.Pattern;\n \n-import org.apache.nifi.stream.io.BufferedInputStream;\n-import org.apache.nifi.stream.io.BufferedOutputStream;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -230,7 +230,7 @@ public int update(final Collection<T> records, final boolean forceSync) throws I\n                         final long transactionId = transactionIdGenerator.getAndIncrement();\n                         if (logger.isTraceEnabled()) {\n                             for (final T record : records) {\n-                                logger.trace(\"Partition {} performing Transaction {}: {}\", new Object[]{partition, transactionId, record});\n+                                logger.trace(\"Partition {} performing Transaction {}: {}\", new Object[] {partition, transactionId, record});\n                             }\n                         }\n \n@@ -670,11 +670,10 @@ public int getVersion() {\n         private final Path editDirectory;\n         private final int writeAheadLogVersion;\n \n-        private final Lock lock = new ReentrantLock();\n         private DataOutputStream dataOut = null;\n         private FileOutputStream fileOut = null;\n-        private boolean blackListed = false;\n-        private boolean closed = false;\n+        private volatile boolean blackListed = false;\n+        private volatile boolean closed = false;\n         private DataInputStream recoveryIn;\n         private int recoveryVersion;\n         private String currentJournalFilename = \"\";\n@@ -707,26 +706,15 @@ public Partition(final Path path, final SerDeFactory<S> serdeFactory, final int\n         }\n \n         public boolean tryClaim() {\n-            final boolean obtainedLock = lock.tryLock();\n-            if (!obtainedLock) {\n-                return false;\n-            }\n-\n-            // Check if the partition is blacklisted. If so, unlock it and return false. Otherwise,\n-            // leave it locked and return true, so that the caller will need to unlock.\n-            if (blackListed) {\n-                lock.unlock();\n-                return false;\n-            }\n-\n-            return true;\n+            return !blackListed;\n         }\n \n         public void releaseClaim() {\n-            lock.unlock();\n         }\n \n         public void close() {\n+            this.closed = true;\n+\n             // Note that here we are closing fileOut and NOT dataOut.\n             // This is very much intentional, not an oversight. This is done because of\n             // the way that the OutputStreams are structured. dataOut wraps a BufferedOutputStream,\n@@ -761,18 +749,12 @@ public void close() {\n                 }\n             }\n \n-            this.closed = true;\n             this.dataOut = null;\n             this.fileOut = null;\n         }\n \n         public void blackList() {\n-            lock.lock();\n-            try {\n-                blackListed = true;\n-            } finally {\n-                lock.unlock();\n-            }\n+            blackListed = true;\n             logger.debug(\"Blacklisted {}\", this);\n         }\n \n@@ -783,55 +765,50 @@ public void blackList() {\n          * @throws IOException if failure to rollover\n          */\n         public OutputStream rollover() throws IOException {\n-            lock.lock();\n-            try {\n-                // Note that here we are closing fileOut and NOT dataOut. See the note in the close()\n-                // method to understand the logic behind this.\n-                final OutputStream oldOutputStream = fileOut;\n-                dataOut = null;\n-                fileOut = null;\n+            // Note that here we are closing fileOut and NOT dataOut. See the note in the close()\n+            // method to understand the logic behind this.\n+            final OutputStream oldOutputStream = fileOut;\n+            dataOut = null;\n+            fileOut = null;\n \n-                this.serde = serdeFactory.createSerDe(null);\n-                final Path editPath = getNewEditPath();\n-                final FileOutputStream fos = new FileOutputStream(editPath.toFile());\n+            this.serde = serdeFactory.createSerDe(null);\n+            final Path editPath = getNewEditPath();\n+            final FileOutputStream fos = new FileOutputStream(editPath.toFile());\n+            try {\n+                final DataOutputStream outStream = new DataOutputStream(new BufferedOutputStream(fos));\n+                outStream.writeUTF(MinimalLockingWriteAheadLog.class.getName());\n+                outStream.writeInt(writeAheadLogVersion);\n+                outStream.writeUTF(serde.getClass().getName());\n+                outStream.writeInt(serde.getVersion());\n+                serde.writeHeader(outStream);\n+\n+                outStream.flush();\n+                dataOut = outStream;\n+                fileOut = fos;\n+            } catch (final IOException ioe) {\n                 try {\n-                    final DataOutputStream outStream = new DataOutputStream(new BufferedOutputStream(fos));\n-                    outStream.writeUTF(MinimalLockingWriteAheadLog.class.getName());\n-                    outStream.writeInt(writeAheadLogVersion);\n-                    outStream.writeUTF(serde.getClass().getName());\n-                    outStream.writeInt(serde.getVersion());\n-                    serde.writeHeader(outStream);\n-\n-                    outStream.flush();\n-                    dataOut = outStream;\n-                    fileOut = fos;\n-                } catch (final IOException ioe) {\n-                    try {\n-                        oldOutputStream.close();\n-                    } catch (final IOException ioe2) {\n-                        ioe.addSuppressed(ioe2);\n-                    }\n-\n-                    logger.error(\"Failed to create new journal for {} due to {}\", new Object[] {this, ioe.toString()}, ioe);\n-                    try {\n-                        fos.close();\n-                    } catch (final IOException innerIOE) {\n-                    }\n-\n-                    dataOut = null;\n-                    fileOut = null;\n-                    blackList();\n+                    oldOutputStream.close();\n+                } catch (final IOException ioe2) {\n+                    ioe.addSuppressed(ioe2);\n+                }\n \n-                    throw ioe;\n+                logger.error(\"Failed to create new journal for {} due to {}\", new Object[] {this, ioe.toString()}, ioe);\n+                try {\n+                    fos.close();\n+                } catch (final IOException innerIOE) {\n                 }\n \n-                currentJournalFilename = editPath.toFile().getName();\n+                dataOut = null;\n+                fileOut = null;\n+                blackList();\n \n-                blackListed = false;\n-                return oldOutputStream;\n-            } finally {\n-                lock.unlock();\n+                throw ioe;\n             }\n+\n+            currentJournalFilename = editPath.toFile().getName();\n+\n+            blackListed = false;\n+            return oldOutputStream;\n         }\n \n         private long getJournalIndex(final File file) {\n@@ -939,33 +916,39 @@ private boolean isJournalFile(final File file) {\n             return true;\n         }\n \n-        public void update(final Collection<S> records, final long transactionId, final Map<Object, S> recordMap, final boolean forceSync)\n-                throws IOException {\n-            if (this.closed) {\n-                throw new IllegalStateException(\"Partition is closed\");\n-            }\n+        public void update(final Collection<S> records, final long transactionId, final Map<Object, S> recordMap, final boolean forceSync) throws IOException {\n+            try (final ByteArrayOutputStream baos = new ByteArrayOutputStream(256);\n+                final DataOutputStream out = new DataOutputStream(baos)) {\n \n-            final DataOutputStream out = dataOut;\n-            out.writeLong(transactionId);\n+                out.writeLong(transactionId);\n+                final int numEditsToSerialize = records.size();\n+                int editsSerialized = 0;\n+                for (final S record : records) {\n+                    final Object recordId = serde.getRecordIdentifier(record);\n+                    final S previousVersion = recordMap.get(recordId);\n \n-            final int numEditsToSerialize = records.size();\n-            int editsSerialized = 0;\n-            for (final S record : records) {\n-                final Object recordId = serde.getRecordIdentifier(record);\n-                final S previousVersion = recordMap.get(recordId);\n+                    serde.serializeEdit(previousVersion, record, out);\n+                    if (++editsSerialized < numEditsToSerialize) {\n+                        out.write(TRANSACTION_CONTINUE);\n+                    } else {\n+                        out.write(TRANSACTION_COMMIT);\n+                    }\n+                }\n \n-                serde.serializeEdit(previousVersion, record, out);\n-                if (++editsSerialized < numEditsToSerialize) {\n-                    out.write(TRANSACTION_CONTINUE);\n-                } else {\n-                    out.write(TRANSACTION_COMMIT);\n+                out.flush();\n+\n+                if (this.closed) {\n+                    throw new IllegalStateException(\"Partition is closed\");\n                 }\n-            }\n \n-            out.flush();\n+                baos.writeTo(dataOut);\n+                dataOut.flush();\n \n-            if (forceSync) {\n-                fileOut.getFD().sync();\n+                if (forceSync) {\n+                    synchronized (fileOut) {\n+                        fileOut.getFD().sync();\n+                    }\n+                }\n             }\n         }\n ",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-write-ahead-log/src/main/java/org/wali/MinimalLockingWriteAheadLog.java",
                "sha": "1a9e219d66c9dba5ab065749cd0a1346b3ef7a21",
                "status": "modified"
            },
            {
                "additions": 62,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-write-ahead-log/src/test/java/org/wali/TestMinimalLockingWriteAheadLog.java",
                "changes": 77,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-commons/nifi-write-ahead-log/src/test/java/org/wali/TestMinimalLockingWriteAheadLog.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 15,
                "filename": "nifi-commons/nifi-write-ahead-log/src/test/java/org/wali/TestMinimalLockingWriteAheadLog.java",
                "patch": "@@ -53,6 +53,64 @@\n     private static final Logger logger = LoggerFactory.getLogger(TestMinimalLockingWriteAheadLog.class);\n \n \n+    @Test\n+    @Ignore(\"for local testing only\")\n+    public void testUpdatePerformance() throws IOException, InterruptedException {\n+        final int numPartitions = 4;\n+\n+        final Path path = Paths.get(\"target/minimal-locking-repo\");\n+        deleteRecursively(path.toFile());\n+        assertTrue(path.toFile().mkdirs());\n+\n+        final DummyRecordSerde serde = new DummyRecordSerde();\n+        final WriteAheadRepository<DummyRecord> repo = new MinimalLockingWriteAheadLog<>(path, numPartitions, serde, null);\n+        final Collection<DummyRecord> initialRecs = repo.recoverRecords();\n+        assertTrue(initialRecs.isEmpty());\n+\n+        final int updateCountPerThread = 1_000_000;\n+        final int numThreads = 16;\n+\n+        final Thread[] threads = new Thread[numThreads];\n+\n+        for (int j = 0; j < 2; j++) {\n+            for (int i = 0; i < numThreads; i++) {\n+                final Thread t = new Thread(new Runnable() {\n+                    @Override\n+                    public void run() {\n+                        for (int i = 0; i < updateCountPerThread; i++) {\n+                            final DummyRecord record = new DummyRecord(String.valueOf(i), UpdateType.CREATE);\n+                            try {\n+                                repo.update(Collections.singleton(record), false);\n+                            } catch (IOException e) {\n+                                e.printStackTrace();\n+                                Assert.fail(e.toString());\n+                            }\n+                        }\n+                    }\n+                });\n+\n+                threads[i] = t;\n+            }\n+\n+            final long start = System.nanoTime();\n+            for (final Thread t : threads) {\n+                t.start();\n+            }\n+            for (final Thread t : threads) {\n+                t.join();\n+            }\n+\n+            final long millis = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);\n+            if (j == 0) {\n+                System.out.println(millis + \" ms to insert \" + updateCountPerThread * numThreads + \" updates using \" + numPartitions + \" partitions and \" + numThreads + \" threads, *as a warmup!*\");\n+            } else {\n+                System.out.println(millis + \" ms to insert \" + updateCountPerThread * numThreads + \" updates using \" + numPartitions + \" partitions and \" + numThreads + \" threads\");\n+            }\n+        }\n+    }\n+\n+\n+\n     @Test\n     public void testRepoDoesntContinuallyGrowOnOutOfMemoryError() throws IOException, InterruptedException {\n         final int numPartitions = 8;\n@@ -557,21 +615,10 @@ private void verifyBlacklistedJournalContents(final File journalFile, final SerD\n                 assertEquals(2, transactionIndicator);\n             }\n \n-            long transactionId = in.readLong();\n-            assertEquals(2L, transactionId);\n-\n-            long thirdSize = in.readLong();\n-            assertEquals(8194, thirdSize);\n-\n-            // should be 8176 A's because we threw an Exception after writing 8194 of them,\n-            // but the BufferedOutputStream's buffer already had 8 bytes on it for the\n-            // transaction id and the size.\n-            for (int i = 0; i < 8176; i++) {\n-                final int c = in.read();\n-                assertEquals(\"i = \" + i, 'A', c);\n-            }\n-\n-            // Stream should now be out of data, because we threw an Exception!\n+            // In previous implementations, we would still have a partial record written out.\n+            // In the current version, however, the serde above would result in the data serialization\n+            // failing and as a result no data would be written to the stream, so the stream should\n+            // now be out of data\n             final int nextByte = in.read();\n             assertEquals(-1, nextByte);\n         }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-commons/nifi-write-ahead-log/src/test/java/org/wali/TestMinimalLockingWriteAheadLog.java",
                "sha": "5cdad8225e0672c75d1df1dfea47c8d9cc454a36",
                "status": "modified"
            },
            {
                "additions": 85,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-docs/src/main/asciidoc/administration-guide.adoc",
                "changes": 88,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-docs/src/main/asciidoc/administration-guide.adoc?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 3,
                "filename": "nifi-docs/src/main/asciidoc/administration-guide.adoc",
                "patch": "@@ -2047,8 +2047,8 @@ FlowFile Repository, if also on that disk, could become corrupt. To avoid this s\n  +\n For example, to provide two additional locations to act as part of the content repository, a user could also specify additional properties with keys of: +\n  +\n-nifi.provenance.repository.directory.content1=/repos/provenance1 +\n-nifi.provenance.repository.directory.content2=/repos/provenance2 +\n+nifi.content.repository.directory.content1=/repos/content1 +\n+nifi.content.repository.directory.content2=/repos/content2 +\n  +\n Providing three total locations, including  _nifi.content.repository.directory.default_.\n |nifi.content.repository.archive.max.retention.period|If archiving is enabled (see nifi.content.repository.archive.enabled below), then\n@@ -2073,7 +2073,25 @@ The Provenance Repository contains the information related to Data Provenance. T\n \n |====\n |*Property*|*Description*\n-|nifi.provenance.repository.implementation|The Provenance Repository implementation. The default value is org.apache.nifi.provenance.PersistentProvenanceRepository and should only be changed with caution. To store provenance events in memory instead of on disk (at the risk of data loss in the event of power/machine failure), set this property to org.apache.nifi.provenance.VolatileProvenanceRepository.\n+|nifi.provenance.repository.implementation|The Provenance Repository implementation. The default value is org.apache.nifi.provenance.PersistentProvenanceRepository.\n+Two additional repositories are available as well.\n+To store provenance events in memory instead of on disk (in which case all events will be lost on restart, and events will be evicted in a first-in-first-out order),\n+set this property to org.apache.nifi.provenance.VolatileProvenanceRepository. This leaves a configurable number of Provenance Events in the Java heap, so the number\n+of events that can be retained is very limited.\n+\n+As of Apache NiFi 1.2.0, a third option is available: org.apache.nifi.provenance.WriteAheadProvenanceRepository.\n+This implementation was created to replace the PersistentProvenanceRepository. The PersistentProvenanceRepository was originally written with the simple goal of persisting\n+Provenance Events as they are generated and providing the ability to iterate over those events sequentially. Later, it was desired to be able to compress the data so that\n+more data could be stored. After that, the ability to index and query the data was added. As requirements evolved over time, the repository kept changing without any major\n+redesigns. When used in a NiFi instance that is responsible for processing large volumes of small FlowFiles, the PersistentProvenanceRepository can quickly become a bottleneck.\n+The WriteAheadProvenanceRepository was then written to provide the same capabilities as the PersistentProvenanceRepository while providing far better performance.\n+Changing to the WriteAheadProvenanceRepository is easy to accomplish, as the two repositories support most of the same properties.\n+*Note Well*, however, the follow caveat: The WriteAheadProvenanceRepository will make use of the Provenance data stored by the PersistentProvenanceRepository. However, the\n+PersistentProvenanceRepository may not be able to read the data written by the WriteAheadProvenanceRepository. Therefore, once the Provenance Repository is changed to use\n+the WriteAheadProvenanceRepository, it cannot be changed back to the PersistentProvenanceRepository without deleting the data in the Provenance Repository. It is therefore\n+recommended that before changing the implementation, users ensure that their version of NiFi is stable, in case any issue arises that causes the user to need to roll back to\n+a previous version of NiFi that did not support the WriteAheadProvenanceRepository. It is for this reason that the default is still set to the PersistentProvenanceRepository\n+at this time.\n |====\n \n === Persistent Provenance Repository Properties\n@@ -2115,6 +2133,70 @@ Providing three total locations, including  _nifi.provenance.repository.director\n |nifi.provenance.repository.buffer.size|The Provenance Repository buffer size. The default value is 100000.\n |====\n \n+=== Write Ahead Provenance Repository Properties\n+\n+|====\n+|*Property*|*Description*\n+|nifi.provenance.repository.directory.default*|The location of the Provenance Repository. The default value is ./provenance_repository. +\n+ +\n+\t*NOTE*: Multiple provenance repositories can be specified by using the *_nifi.provenance.repository.directory._* prefix with unique suffixes and separate paths as values. +\n+ +\n+\tFor example, to provide two additional locations to act as part of the provenance repository, a user could also specify additional properties with keys of: +\n+ +\n+\tnifi.provenance.repository.directory.provenance1=/repos/provenance1 +\n+\tnifi.provenance.repository.directory.provenance2=/repos/provenance2 +\n+ +\n+\tProviding three total locations, including  _nifi.provenance.repository.directory.default_.\n+|nifi.provenance.repository.max.storage.time|The maximum amount of time to keep data provenance information. The default value is 24 hours.\n+|nifi.provenance.repository.max.storage.size|The maximum amount of data provenance information to store at a time.\n+\tThe default is 1 GB. The Data Provenance capability can consume a great deal of storage space because so much data is kept.\n+\tFor production environments, values of 1-2 TB or more is not uncommon. The repository will write to a single \"event file\" (or set of\n+\t\"event files\" if multiple storage locations are defined, as described above) for some period of time (defined by the\n+\tnifi.provenance.repository.rollover.time and  nifi.provenance.repository.rollover.size properties). Data is always aged off one file at a time,\n+\tso it is not advisable to write to a single \"event file\" for a tremendous amount of time, as it will prevent old data from aging off as smoothly.\n+|nifi.provenance.repository.rollover.time|The amount of time to wait before rolling over the \"event file\" that the repository is writing to.\n+|nifi.provenance.repository.rollover.size|The amount of data to write to a single \"event file.\" The default value is 100 MB. For production\n+\tenvironments where a very large amount of Data Provenance is generated, a value of 1 GB is also very reasonable. \n+|nifi.provenance.repository.query.threads|The number of threads to use for Provenance Repository queries. The default value is 2.\n+|nifi.provenance.repository.index.threads|The number of threads to use for indexing Provenance events so that they are searchable. The default value is 1.\n+\tFor flows that operate on a very high number of FlowFiles, the indexing of Provenance events could become a bottleneck. If this happens, increasing the\n+\tvalue of this property may increase the rate at which the Provenance Repository is able to process these records, resulting in better overall throughput.\n+\tIt is advisable to use at least 1 thread per storage location (i.e., if there are 3 storage locations, at least 3 threads should be used). For high\n+\tthroughput environments, where more CPU and disk I/O is available, it may make sense to increase this value significantly. Typically going beyond\n+\t2-4 threads per storage location is not valuable. However, this can be tuned depending on the CPU resources available compared to the I/O resources. \n+|nifi.provenance.repository.compress.on.rollover|Indicates whether to compress the provenance information when an \"event file\" is rolled over. The default value is _true_.\n+|nifi.provenance.repository.always.sync|If set to _true_, any change to the repository will be synchronized to the disk, meaning that NiFi will ask the operating system\n+\tnot to cache the information. This is very expensive and can significantly reduce NiFi performance. However, if it is _false_, there could be the potential for data\n+\tloss if either there is a sudden power loss or the operating system crashes. The default value is _false_.\n+|nifi.provenance.repository.indexed.fields|This is a comma-separated list of the fields that should be indexed and made searchable.\n+\tFields that are not indexed will not be searchable. Valid fields are: EventType, FlowFileUUID, Filename, TransitURI, ProcessorID,\n+\tAlternateIdentifierURI, Relationship, Details. The default value is: EventType, FlowFileUUID, Filename, ProcessorID.\n+|nifi.provenance.repository.indexed.attributes|This is a comma-separated list of FlowFile Attributes that should be indexed and made searchable. It is blank by default.\n+\tBut some good examples to consider are 'filename' and 'mime.type' as well as any custom attritubes you might use which are valuable for your use case.\n+|nifi.provenance.repository.index.shard.size|The repository uses Apache Lucene to performing indexing and searching capabilities. This value indicates how large a Lucene Index should\n+\tbecome before the Repository starts writing to a new Index. Large values for the shard size will result in more Java heap usage when searching the Provenance Repository but should\n+\tprovide better performance. The default value is 500 MB. However, this is due to the fact that defaults are tuned for very small environments where most users begin to use NiFi.\n+\tFor production environments, it is advisable to change this value to *4 to 8 GB*. Once all Provenance Events in the index have been aged off from the \"event files,\" the index\n+\twill be destroyed as well.\n+|nifi.provenance.repository.max.attribute.length|Indicates the maximum length that a FlowFile attribute can be when retrieving a Provenance Event from the repository.\n+\tIf the length of any attribute exceeds this value, it will be truncated when the event is retrieved. The default is 65536.\n+|nifi.provenance.repository.concurrent.merge.threads|Apache Lucene creates several \"segments\" in an Index. These segments are periodically merged together in order to provide faster\n+\tquerying. This property specifies the maximum number of threads that are allowed to be used for *each* of the storage directories. The default value is 2. For high throughput\n+\tenvironments, it is advisable to set the number of index threads larger than the number of merge threads * the number of storage locations. For example, if there are 2 storage\n+\tlocations and the number of index threads is set to 8, then the number of merge threads should likely be less than 4. While it is not critical that this be done, setting the\n+\tnumber of merge threads larger than this can result in all index threads being used to merge, which would cause the NiFi flow to periodically pause while indexing is happening,\n+\tresulting in some data being processed with much higher latency than other data.\n+|nifi.provenance.repository.warm.cache.frequency|Each time that a Provenance query is run, the query must first search the Apache Lucene indices (at least, in most cases - there are\n+\tsome queries that are run often and the results are cached to avoid searching the Lucene indices). When a Lucene index is opened for the first time, it can be very expensive and take\n+\tseveral seconds. This is compounded by having many different indices, and can result in a Provenance query taking much longer. After the index has been opened, the Operating System's\n+\tdisk cache will typically hold onto enough data to make re-opening the index much faster - at least for a period of time, until the disk cache evicts this data. If this value is set,\n+\tNiFi will periodically open each Lucene index and then close it, in order to \"warm\" the cache. This will result in far faster queries when the Provenance Repository is large. As with\n+\tall great things, though, it comes with a cost. Warming the cache does take some CPU resources, but more importantly it will evict other data from the Operating System disk cache and\n+\twill result in reading (potentially a great deal of) data from the disk. This can result in lower NiFi performance. However, if NiFi is running in an environment where CPU and disk\n+\tare not fully utilized, this feature can result in far faster Provenance queries. \n+|====\n+\n+\n === Component Status Repository\n \n The Component Status Repository contains the information for the Component Status History tool in the User Interface. These",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-docs/src/main/asciidoc/administration-guide.adoc",
                "sha": "fff0bdd990a7f0967d2e19d7a6f4534cad2db32a",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-framework-api/src/main/java/org/apache/nifi/controller/Triggerable.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-framework-api/src/main/java/org/apache/nifi/controller/Triggerable.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 1,
                "filename": "nifi-framework-api/src/main/java/org/apache/nifi/controller/Triggerable.java",
                "patch": "@@ -25,7 +25,7 @@\n \n public interface Triggerable {\n \n-    public static final long MINIMUM_SCHEDULING_NANOS = 30000L;\n+    public static final long MINIMUM_SCHEDULING_NANOS = 1L;\n \n     /**\n      * <p>",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-framework-api/src/main/java/org/apache/nifi/controller/Triggerable.java",
                "sha": "5255c0504eca1f41b75ff13684944039f8974543",
                "status": "modified"
            },
            {
                "additions": 88,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-framework-api/src/main/java/org/apache/nifi/provenance/IdentifierLookup.java",
                "changes": 88,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-framework-api/src/main/java/org/apache/nifi/provenance/IdentifierLookup.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-framework-api/src/main/java/org/apache/nifi/provenance/IdentifierLookup.java",
                "patch": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+\n+/**\n+ * Provides a mechanism for obtaining the identifiers of components, queues, etc.\n+ */\n+public interface IdentifierLookup {\n+\n+    /**\n+     * @return the identifiers of components that may generate Provenance Events\n+     */\n+    List<String> getComponentIdentifiers();\n+\n+    /**\n+     * @return a list of component types that may generate Provenance Events\n+     */\n+    List<String> getComponentTypes();\n+\n+    /**\n+     *\n+     * @return the identifiers of FlowFile Queues that are in the flow\n+     */\n+    List<String> getQueueIdentifiers();\n+\n+    default Map<String, Integer> invertQueueIdentifiers() {\n+        return invertList(getQueueIdentifiers());\n+    }\n+\n+    default Map<String, Integer> invertComponentTypes() {\n+        return invertList(getComponentTypes());\n+    }\n+\n+    default Map<String, Integer> invertComponentIdentifiers() {\n+        return invertList(getComponentIdentifiers());\n+    }\n+\n+    default Map<String, Integer> invertList(final List<String> values) {\n+        final Map<String, Integer> inverted = new HashMap<>(values.size());\n+        for (int i = 0; i < values.size(); i++) {\n+            inverted.put(values.get(i), i);\n+        }\n+        return inverted;\n+    }\n+\n+\n+    public static final IdentifierLookup EMPTY = new IdentifierLookup() {\n+        @Override\n+        public List<String> getComponentIdentifiers() {\n+            return Collections.emptyList();\n+        }\n+\n+        @Override\n+        public List<String> getComponentTypes() {\n+            return Collections.emptyList();\n+        }\n+\n+        @Override\n+        public List<String> getQueueIdentifiers() {\n+            return Collections.emptyList();\n+        }\n+\n+        @Override\n+        public Map<String, Integer> invertList(List<String> values) {\n+            return Collections.emptyMap();\n+        }\n+    };\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-framework-api/src/main/java/org/apache/nifi/provenance/IdentifierLookup.java",
                "sha": "6d548d2f4aef0058aba5c03bbf3d13e610bf6e27",
                "status": "added"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-framework-api/src/main/java/org/apache/nifi/provenance/ProvenanceRepository.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-framework-api/src/main/java/org/apache/nifi/provenance/ProvenanceRepository.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 3,
                "filename": "nifi-framework-api/src/main/java/org/apache/nifi/provenance/ProvenanceRepository.java",
                "patch": "@@ -34,12 +34,13 @@\n      * Performs any initialization needed. This should be called only by the\n      * framework.\n      *\n-     * @param eventReporter   to report to\n-     * @param authorizer      the authorizer to use for authorizing individual events\n+     * @param eventReporter to report to\n+     * @param authorizer the authorizer to use for authorizing individual events\n      * @param resourceFactory the resource factory to use for generating Provenance Resource objects for authorization purposes\n+     * @param identifierLookup a mechanism for looking up identifiers in the flow\n      * @throws java.io.IOException if unable to initialize\n      */\n-    void initialize(EventReporter eventReporter, Authorizer authorizer, ProvenanceAuthorizableFactory resourceFactory) throws IOException;\n+    void initialize(EventReporter eventReporter, Authorizer authorizer, ProvenanceAuthorizableFactory resourceFactory, IdentifierLookup identifierLookup) throws IOException;\n \n \n     ProvenanceEventRecord getEvent(long id, NiFiUser user) throws IOException;",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-framework-api/src/main/java/org/apache/nifi/provenance/ProvenanceRepository.java",
                "sha": "516a36defddaeff78c20f4f4da6cc09e075d150f",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-framework-api/src/main/java/org/apache/nifi/provenance/lineage/ComputeLineageResult.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-framework-api/src/main/java/org/apache/nifi/provenance/lineage/ComputeLineageResult.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-framework-api/src/main/java/org/apache/nifi/provenance/lineage/ComputeLineageResult.java",
                "patch": "@@ -18,6 +18,7 @@\n \n import java.util.Date;\n import java.util.List;\n+import java.util.concurrent.TimeUnit;\n \n /**\n  *\n@@ -55,4 +56,6 @@\n      * @return Indicates whether or not the lineage has finished running\n      */\n     boolean isFinished();\n+\n+    boolean awaitCompletion(long time, TimeUnit unit) throws InterruptedException;\n }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-framework-api/src/main/java/org/apache/nifi/provenance/lineage/ComputeLineageResult.java",
                "sha": "ad480be0a30e46ef8663ec52d15ad68440fbd5cd",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-framework-api/src/main/java/org/apache/nifi/provenance/search/Query.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-framework-api/src/main/java/org/apache/nifi/provenance/search/Query.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-framework-api/src/main/java/org/apache/nifi/provenance/search/Query.java",
                "patch": "@@ -93,4 +93,8 @@ public String getMaxFileSize() {\n     public String toString() {\n         return \"Query[ \" + searchTerms + \" ]\";\n     }\n+\n+    public boolean isEmpty() {\n+        return searchTerms.isEmpty() && maxFileSize == null && minFileSize == null && startDate == null && endDate == null;\n+    }\n }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-framework-api/src/main/java/org/apache/nifi/provenance/search/Query.java",
                "sha": "4db8e0ff2ca78c374d37105602523ba4a54248bf",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-framework-api/src/main/java/org/apache/nifi/provenance/search/QueryResult.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-framework-api/src/main/java/org/apache/nifi/provenance/search/QueryResult.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-framework-api/src/main/java/org/apache/nifi/provenance/search/QueryResult.java",
                "patch": "@@ -18,6 +18,7 @@\n \n import java.util.Date;\n import java.util.List;\n+import java.util.concurrent.TimeUnit;\n \n import org.apache.nifi.provenance.ProvenanceEventRecord;\n \n@@ -60,4 +61,6 @@\n      * @return Indicates whether or not the query has finished running\n      */\n     boolean isFinished();\n+\n+    boolean awaitCompletion(long time, TimeUnit unit) throws InterruptedException;\n }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-framework-api/src/main/java/org/apache/nifi/provenance/search/QueryResult.java",
                "sha": "cc84ea17a8acbe0175e9797dd12d129aaa9c0339",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-mock/src/main/java/org/apache/nifi/provenance/MockProvenanceRepository.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-mock/src/main/java/org/apache/nifi/provenance/MockProvenanceRepository.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 1,
                "filename": "nifi-mock/src/main/java/org/apache/nifi/provenance/MockProvenanceRepository.java",
                "patch": "@@ -56,7 +56,7 @@ public void registerEvent(final ProvenanceEventRecord event) {\n     }\n \n     @Override\n-    public void initialize(EventReporter eventReporter, Authorizer authorizer, ProvenanceAuthorizableFactory resourceFactory) throws IOException {\n+    public void initialize(EventReporter eventReporter, Authorizer authorizer, ProvenanceAuthorizableFactory resourceFactory, IdentifierLookup idLookup) throws IOException {\n \n     }\n ",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-mock/src/main/java/org/apache/nifi/provenance/MockProvenanceRepository.java",
                "sha": "53c3c2e62f85c28585c8a709fefcfb1049093d70",
                "status": "modified"
            },
            {
                "additions": 23,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-mock/src/main/java/org/apache/nifi/util/MockFlowFile.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-mock/src/main/java/org/apache/nifi/util/MockFlowFile.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 1,
                "filename": "nifi-mock/src/main/java/org/apache/nifi/util/MockFlowFile.java",
                "patch": "@@ -63,7 +63,29 @@ public MockFlowFile(final long id) {\n     }\n \n     public MockFlowFile(final long id, final FlowFile toCopy) {\n-        this(id);\n+        this.creationTime = System.nanoTime();\n+        this.id = id;\n+        entryDate = System.currentTimeMillis();\n+\n+        final Map<String, String> attributesToCopy = toCopy.getAttributes();\n+        String filename = attributesToCopy.get(CoreAttributes.FILENAME.key());\n+        if (filename == null) {\n+            filename = String.valueOf(System.nanoTime()) + \".mockFlowFile\";\n+        }\n+        attributes.put(CoreAttributes.FILENAME.key(), filename);\n+\n+        String path = attributesToCopy.get(CoreAttributes.PATH.key());\n+        if (path == null) {\n+            path = \"target\";\n+        }\n+        attributes.put(CoreAttributes.PATH.key(), path);\n+\n+        String uuid = attributesToCopy.get(CoreAttributes.UUID.key());\n+        if (uuid == null) {\n+            uuid = UUID.randomUUID().toString();\n+        }\n+        attributes.put(CoreAttributes.UUID.key(), uuid);\n+\n         attributes.putAll(toCopy.getAttributes());\n         final byte[] dataToCopy = ((MockFlowFile) toCopy).data;\n         this.data = new byte[dataToCopy.length];",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-mock/src/main/java/org/apache/nifi/util/MockFlowFile.java",
                "sha": "df87de59c55dcd3530c29f2be165b527d4d1a6dd",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core-api/src/main/java/org/apache/nifi/controller/AbstractPort.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core-api/src/main/java/org/apache/nifi/controller/AbstractPort.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 1,
                "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core-api/src/main/java/org/apache/nifi/controller/AbstractPort.java",
                "patch": "@@ -111,7 +111,7 @@ public AbstractPort(final String id, final String name, final ProcessGroup proce\n         yieldPeriod = new AtomicReference<>(\"1 sec\");\n         yieldExpiration = new AtomicLong(0L);\n         schedulingPeriod = new AtomicReference<>(\"0 millis\");\n-        schedulingNanos = new AtomicLong(30000);\n+        schedulingNanos = new AtomicLong(MINIMUM_SCHEDULING_NANOS);\n         scheduledState = new AtomicReference<>(ScheduledState.STOPPED);\n     }\n ",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core-api/src/main/java/org/apache/nifi/controller/AbstractPort.java",
                "sha": "1177dad1af863d4d6136a53760be586c6c1c5716",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core-api/src/main/java/org/apache/nifi/controller/StandardFunnel.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core-api/src/main/java/org/apache/nifi/controller/StandardFunnel.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 1,
                "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core-api/src/main/java/org/apache/nifi/controller/StandardFunnel.java",
                "patch": "@@ -100,7 +100,7 @@ public StandardFunnel(final String identifier, final ProcessGroup processGroup,\n         yieldPeriod = new AtomicReference<>(\"250 millis\");\n         yieldExpiration = new AtomicLong(0L);\n         schedulingPeriod = new AtomicReference<>(\"0 millis\");\n-        schedulingNanos = new AtomicLong(30000);\n+        schedulingNanos = new AtomicLong(MINIMUM_SCHEDULING_NANOS);\n         name = new AtomicReference<>(\"Funnel\");\n     }\n ",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core-api/src/main/java/org/apache/nifi/controller/StandardFunnel.java",
                "sha": "34ffbac62e12d5f28840e34d2a103586f602050f",
                "status": "modified"
            },
            {
                "additions": 38,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/FlowController.java",
                "changes": 40,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/FlowController.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 2,
                "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/FlowController.java",
                "patch": "@@ -152,6 +152,7 @@\n import org.apache.nifi.processor.SimpleProcessLogger;\n import org.apache.nifi.processor.StandardProcessorInitializationContext;\n import org.apache.nifi.processor.StandardValidationContextFactory;\n+import org.apache.nifi.provenance.IdentifierLookup;\n import org.apache.nifi.provenance.ProvenanceAuthorizableFactory;\n import org.apache.nifi.provenance.ProvenanceEventRecord;\n import org.apache.nifi.provenance.ProvenanceEventType;\n@@ -233,10 +234,12 @@\n import java.util.concurrent.atomic.AtomicReference;\n import java.util.concurrent.locks.Lock;\n import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import java.util.stream.Collectors;\n \n import static java.util.Objects.requireNonNull;\n \n-public class FlowController implements EventAccess, ControllerServiceProvider, ReportingTaskProvider, QueueProvider, Authorizable, ProvenanceAuthorizableFactory, NodeTypeProvider {\n+public class FlowController implements EventAccess, ControllerServiceProvider, ReportingTaskProvider,\n+    QueueProvider, Authorizable, ProvenanceAuthorizableFactory, NodeTypeProvider, IdentifierLookup {\n \n     // default repository implementations\n     public static final String DEFAULT_FLOWFILE_REPO_IMPLEMENTATION = \"org.apache.nifi.controller.repository.WriteAheadFlowFileRepository\";\n@@ -454,7 +457,7 @@ private FlowController(\n \n         try {\n             this.provenanceRepository = createProvenanceRepository(nifiProperties);\n-            this.provenanceRepository.initialize(createEventReporter(bulletinRepository), authorizer, this);\n+            this.provenanceRepository.initialize(createEventReporter(bulletinRepository), authorizer, this, this);\n         } catch (final Exception e) {\n             throw new RuntimeException(\"Unable to create Provenance Repository\", e);\n         }\n@@ -3886,6 +3889,39 @@ public ProvenanceEventRecord replayFlowFile(final ProvenanceEventRecord event, f\n         return replayEvent;\n     }\n \n+    @Override\n+    public List<String> getComponentIdentifiers() {\n+        final List<String> componentIds = new ArrayList<>();\n+        getGroup(getRootGroupId()).findAllProcessors().stream()\n+            .forEach(proc -> componentIds.add(proc.getIdentifier()));\n+        getGroup(getRootGroupId()).getInputPorts().stream()\n+            .forEach(port -> componentIds.add(port.getIdentifier()));\n+        getGroup(getRootGroupId()).getOutputPorts().stream()\n+            .forEach(port -> componentIds.add(port.getIdentifier()));\n+\n+        return componentIds;\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"rawtypes\")\n+    public List<String> getComponentTypes() {\n+        final Set<Class> procClasses = ExtensionManager.getExtensions(Processor.class);\n+        final List<String> componentTypes = new ArrayList<>(procClasses.size() + 2);\n+        componentTypes.add(ProvenanceEventRecord.REMOTE_INPUT_PORT_TYPE);\n+        componentTypes.add(ProvenanceEventRecord.REMOTE_OUTPUT_PORT_TYPE);\n+        procClasses.stream()\n+            .map(procClass -> procClass.getSimpleName())\n+            .forEach(componentType -> componentTypes.add(componentType));\n+        return componentTypes;\n+    }\n+\n+    @Override\n+    public List<String> getQueueIdentifiers() {\n+        return getAllQueues().stream()\n+            .map(q -> q.getIdentifier())\n+            .collect(Collectors.toList());\n+    }\n+\n     public boolean isConnected() {\n         rwLock.readLock().lock();\n         try {",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/FlowController.java",
                "sha": "191fc65fa769110d7bb67592f3301d374887d7fa",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/FileSystemRepository.java",
                "changes": 1,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/FileSystemRepository.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/FileSystemRepository.java",
                "patch": "@@ -911,6 +911,7 @@ public synchronized void write(final byte[] b, final int off, final int len) thr\n                 }\n \n                 bytesWritten += len;\n+\n                 scc.setLength(bytesWritten + initialLength);\n             }\n ",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/FileSystemRepository.java",
                "sha": "67df53952c7ae677204125c8c1983b138d274e56",
                "status": "modified"
            },
            {
                "additions": 76,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/StandardProcessSession.java",
                "changes": 114,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/StandardProcessSession.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 38,
                "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/StandardProcessSession.java",
                "patch": "@@ -16,13 +16,40 @@\n  */\n package org.apache.nifi.controller.repository;\n \n+import java.io.BufferedOutputStream;\n+import java.io.ByteArrayInputStream;\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.NoSuchElementException;\n+import java.util.Objects;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n import org.apache.commons.io.IOUtils;\n import org.apache.nifi.connectable.Connectable;\n import org.apache.nifi.connectable.Connection;\n import org.apache.nifi.controller.ProcessorNode;\n import org.apache.nifi.controller.queue.FlowFileQueue;\n import org.apache.nifi.controller.queue.QueueSize;\n import org.apache.nifi.controller.repository.claim.ContentClaim;\n+import org.apache.nifi.controller.repository.claim.ContentClaimWriteCache;\n import org.apache.nifi.controller.repository.claim.ResourceClaim;\n import org.apache.nifi.controller.repository.io.DisableOnCloseInputStream;\n import org.apache.nifi.controller.repository.io.DisableOnCloseOutputStream;\n@@ -53,32 +80,6 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.io.BufferedOutputStream;\n-import java.io.ByteArrayInputStream;\n-import java.io.EOFException;\n-import java.io.IOException;\n-import java.io.InputStream;\n-import java.io.OutputStream;\n-import java.nio.file.Files;\n-import java.nio.file.Path;\n-import java.util.ArrayList;\n-import java.util.Collection;\n-import java.util.Collections;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.Iterator;\n-import java.util.LinkedHashSet;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.NoSuchElementException;\n-import java.util.Objects;\n-import java.util.Set;\n-import java.util.UUID;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.atomic.AtomicLong;\n-import java.util.regex.Pattern;\n-import java.util.stream.Collectors;\n-\n /**\n  * <p>\n  * Provides a ProcessSession that ensures all accesses, changes and transfers\n@@ -143,6 +144,7 @@\n     private final Map<FlowFile, ProvenanceEventBuilder> forkEventBuilders = new HashMap<>();\n \n     private Checkpoint checkpoint = new Checkpoint();\n+    private final ContentClaimWriteCache claimCache;\n \n     public StandardProcessSession(final ProcessContext context) {\n         this.context = context;\n@@ -180,7 +182,7 @@ public StandardProcessSession(final ProcessContext context) {\n             context.getProvenanceRepository(), this);\n         this.sessionId = idGenerator.getAndIncrement();\n         this.connectableDescription = description;\n-\n+        this.claimCache = new ContentClaimWriteCache(context.getContentRepository());\n         LOG.trace(\"Session {} created for {}\", this, connectableDescription);\n         processingStartTime = System.nanoTime();\n     }\n@@ -312,6 +314,11 @@ private void commit(final Checkpoint checkpoint) {\n             final long commitStartNanos = System.nanoTime();\n \n             resetReadClaim();\n+            try {\n+                claimCache.flush();\n+            } finally {\n+                claimCache.reset();\n+            }\n \n             final long updateProvenanceStart = System.nanoTime();\n             updateProvenanceRepo(checkpoint);\n@@ -375,7 +382,7 @@ private void commit(final Checkpoint checkpoint) {\n             updateEventRepository(checkpoint);\n \n             final long updateEventRepositoryFinishNanos = System.nanoTime();\n-            final long updateEventRepositoryNanos = updateEventRepositoryFinishNanos - claimRemovalFinishNanos;\n+            final long updateEventRepositoryNanos = updateEventRepositoryFinishNanos - flowFileRepoUpdateFinishNanos;\n \n             // transfer the flowfiles to the connections' queues.\n             final Map<FlowFileQueue, Collection<FlowFileRecord>> recordMap = new HashMap<>();\n@@ -454,7 +461,12 @@ private void commit(final Checkpoint checkpoint) {\n             } catch (final Exception e1) {\n                 e.addSuppressed(e1);\n             }\n-            throw e;\n+\n+            if (e instanceof RuntimeException) {\n+                throw (RuntimeException) e;\n+            } else {\n+                throw new ProcessException(e);\n+            }\n         }\n     }\n \n@@ -904,6 +916,12 @@ private void rollback(final boolean penalize, final boolean rollbackCheckpoint)\n             }\n         }\n \n+        try {\n+            claimCache.reset();\n+        } catch (IOException e1) {\n+            LOG.warn(\"{} Attempted to close Output Stream for {} due to session rollback but close failed\", this, this.connectableDescription, e1);\n+        }\n+\n         final Set<StandardRepositoryRecord> recordsToHandle = new HashSet<>();\n         recordsToHandle.addAll(records.values());\n         if (rollbackCheckpoint) {\n@@ -2033,6 +2051,7 @@ private InputStream getInputStream(final FlowFile flowFile, final ContentClaim c\n                     }\n                 }\n \n+                claimCache.flush(claim);\n                 final InputStream rawInStream = context.getContentRepository().read(claim);\n \n                 if (currentReadClaimStream != null) {\n@@ -2047,6 +2066,7 @@ private InputStream getInputStream(final FlowFile flowFile, final ContentClaim c\n                 // reuse the same InputStream for the next FlowFile\n                 return new DisableOnCloseInputStream(currentReadClaimStream);\n             } else {\n+                claimCache.flush(claim);\n                 final InputStream rawInStream = context.getContentRepository().read(claim);\n                 try {\n                     StreamUtils.skip(rawInStream, offset);\n@@ -2077,6 +2097,7 @@ public void read(FlowFile source, boolean allowSessionStreamManagement, InputStr\n \n         try {\n             ensureNotAppending(record.getCurrentClaim());\n+            claimCache.flush(record.getCurrentClaim());\n         } catch (final IOException e) {\n             throw new FlowFileAccessException(\"Failed to access ContentClaim for \" + source.toString(), e);\n         }\n@@ -2241,6 +2262,7 @@ public FlowFile merge(final Collection<FlowFile> sources, final FlowFile destina\n \n             try {\n                 ensureNotAppending(record.getCurrentClaim());\n+                claimCache.flush(record.getCurrentClaim());\n             } catch (final IOException e) {\n                 throw new FlowFileAccessException(\"Unable to read from source \" + source + \" due to \" + e.toString(), e);\n             }\n@@ -2334,11 +2356,11 @@ public FlowFile write(final FlowFile source, final OutputStreamCallback writer)\n         long writtenToFlowFile = 0L;\n         ContentClaim newClaim = null;\n         try {\n-            newClaim = context.getContentRepository().create(context.getConnectable().isLossTolerant());\n+            newClaim = claimCache.getContentClaim();\n             claimLog.debug(\"Creating ContentClaim {} for 'write' for {}\", newClaim, source);\n \n             ensureNotAppending(newClaim);\n-            try (final OutputStream stream = context.getContentRepository().write(newClaim);\n+            try (final OutputStream stream = claimCache.write(newClaim);\n                 final OutputStream disableOnClose = new DisableOnCloseOutputStream(stream);\n                 final ByteCountingOutputStream countingOut = new ByteCountingOutputStream(disableOnClose)) {\n                 try {\n@@ -2373,7 +2395,7 @@ public FlowFile write(final FlowFile source, final OutputStreamCallback writer)\n         final FlowFileRecord newFile = new StandardFlowFileRecord.Builder()\n             .fromFlowFile(record.getCurrent())\n             .contentClaim(newClaim)\n-            .contentClaimOffset(0)\n+            .contentClaimOffset(Math.max(0, newClaim.getLength() - writtenToFlowFile))\n             .size(writtenToFlowFile)\n             .build();\n \n@@ -2396,6 +2418,8 @@ public FlowFile append(final FlowFile source, final OutputStreamCallback writer)\n         ContentClaim newClaim = null;\n         try {\n             if (outStream == null) {\n+                claimCache.flush(oldClaim);\n+\n                 try (final InputStream oldClaimIn = context.getContentRepository().read(oldClaim)) {\n                     newClaim = context.getContentRepository().create(context.getConnectable().isLossTolerant());\n                     claimLog.debug(\"Creating ContentClaim {} for 'append' for {}\", newClaim, source);\n@@ -2568,16 +2592,20 @@ public FlowFile write(final FlowFile source, final StreamCallback writer) {\n         long writtenToFlowFile = 0L;\n         ContentClaim newClaim = null;\n         try {\n-            newClaim = context.getContentRepository().create(context.getConnectable().isLossTolerant());\n+            newClaim = claimCache.getContentClaim();\n             claimLog.debug(\"Creating ContentClaim {} for 'write' for {}\", newClaim, source);\n \n             ensureNotAppending(newClaim);\n \n+            if (currClaim != null) {\n+                claimCache.flush(currClaim.getResourceClaim());\n+            }\n+\n             try (final InputStream is = getInputStream(source, currClaim, record.getCurrentClaimOffset(), true);\n                 final InputStream limitedIn = new LimitedInputStream(is, source.getSize());\n                 final InputStream disableOnCloseIn = new DisableOnCloseInputStream(limitedIn);\n                 final ByteCountingInputStream countingIn = new ByteCountingInputStream(disableOnCloseIn, bytesRead);\n-                final OutputStream os = context.getContentRepository().write(newClaim);\n+                final OutputStream os = claimCache.write(newClaim);\n                 final OutputStream disableOnCloseOut = new DisableOnCloseOutputStream(os);\n                 final ByteCountingOutputStream countingOut = new ByteCountingOutputStream(disableOnCloseOut)) {\n \n@@ -2626,7 +2654,7 @@ public FlowFile write(final FlowFile source, final StreamCallback writer) {\n         final FlowFileRecord newFile = new StandardFlowFileRecord.Builder()\n             .fromFlowFile(record.getCurrent())\n             .contentClaim(newClaim)\n-            .contentClaimOffset(0L)\n+            .contentClaimOffset(Math.max(0L, newClaim.getLength() - writtenToFlowFile))\n             .size(writtenToFlowFile)\n             .build();\n \n@@ -2668,8 +2696,11 @@ public FlowFile importFrom(final Path source, final boolean keepSourceFile, fina\n \n         removeTemporaryClaim(record);\n \n-        final FlowFileRecord newFile = new StandardFlowFileRecord.Builder().fromFlowFile(record.getCurrent())\n-            .contentClaim(newClaim).contentClaimOffset(claimOffset).size(newSize)\n+        final FlowFileRecord newFile = new StandardFlowFileRecord.Builder()\n+            .fromFlowFile(record.getCurrent())\n+            .contentClaim(newClaim)\n+            .contentClaimOffset(claimOffset)\n+            .size(newSize)\n             .addAttribute(CoreAttributes.FILENAME.key(), source.toFile().getName())\n             .build();\n         record.setWorking(newFile, CoreAttributes.FILENAME.key(), source.toFile().getName());\n@@ -2708,7 +2739,12 @@ public FlowFile importFrom(final InputStream source, final FlowFile destination)\n         }\n \n         removeTemporaryClaim(record);\n-        final FlowFileRecord newFile = new StandardFlowFileRecord.Builder().fromFlowFile(record.getCurrent()).contentClaim(newClaim).contentClaimOffset(claimOffset).size(newSize).build();\n+        final FlowFileRecord newFile = new StandardFlowFileRecord.Builder()\n+            .fromFlowFile(record.getCurrent())\n+            .contentClaim(newClaim)\n+            .contentClaimOffset(claimOffset)\n+            .size(newSize)\n+            .build();\n         record.setWorking(newFile);\n         return newFile;\n     }\n@@ -2720,6 +2756,7 @@ public void exportTo(final FlowFile source, final Path destination, final boolea\n         try {\n             ensureNotAppending(record.getCurrentClaim());\n \n+            claimCache.flush(record.getCurrentClaim());\n             final long copyCount = context.getContentRepository().exportTo(record.getCurrentClaim(), destination, append, record.getCurrentClaimOffset(), source.getSize());\n             bytesRead += copyCount;\n             bytesWritten += copyCount;\n@@ -2741,6 +2778,7 @@ public void exportTo(final FlowFile source, final OutputStream destination) {\n \n         try {\n             ensureNotAppending(record.getCurrentClaim());\n+            claimCache.flush(record.getCurrentClaim());\n         } catch (final IOException e) {\n             throw new FlowFileAccessException(\"Failed to access ContentClaim for \" + source.toString(), e);\n         }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/StandardProcessSession.java",
                "sha": "16a6534410bc7dca4b3edb1a620a409bace92bb6",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/WriteAheadFlowFileRepository.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/WriteAheadFlowFileRepository.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 7,
                "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/WriteAheadFlowFileRepository.java",
                "patch": "@@ -19,6 +19,7 @@\n import java.io.IOException;\n import java.nio.file.Files;\n import java.nio.file.Path;\n+import java.nio.file.Paths;\n import java.util.ArrayList;\n import java.util.Collection;\n import java.util.Collections;\n@@ -27,6 +28,8 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n import java.util.concurrent.BlockingQueue;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ConcurrentMap;\n@@ -74,6 +77,7 @@\n  * </p>\n  */\n public class WriteAheadFlowFileRepository implements FlowFileRepository, SyncListener {\n+    private static final String FLOWFILE_REPOSITORY_DIRECTORY_PREFIX = \"nifi.flowfile.repository.directory\";\n \n     private final AtomicLong flowFileSequenceGenerator = new AtomicLong(0L);\n     private final boolean alwaysSync;\n@@ -82,7 +86,7 @@\n     private volatile ScheduledFuture<?> checkpointFuture;\n \n     private final long checkpointDelayMillis;\n-    private final Path flowFileRepositoryPath;\n+    private final SortedSet<Path> flowFileRepositoryPaths = new TreeSet<>();\n     private final int numPartitions;\n     private final ScheduledExecutorService checkpointExecutor;\n \n@@ -120,7 +124,6 @@\n     public WriteAheadFlowFileRepository() {\n         alwaysSync = false;\n         checkpointDelayMillis = 0l;\n-        flowFileRepositoryPath = null;\n         numPartitions = 0;\n         checkpointExecutor = null;\n     }\n@@ -129,7 +132,13 @@ public WriteAheadFlowFileRepository(final NiFiProperties nifiProperties) {\n         alwaysSync = Boolean.parseBoolean(nifiProperties.getProperty(NiFiProperties.FLOWFILE_REPOSITORY_ALWAYS_SYNC, \"false\"));\n \n         // determine the database file path and ensure it exists\n-        flowFileRepositoryPath = nifiProperties.getFlowFileRepositoryPath();\n+        for (final String propertyName : nifiProperties.getPropertyKeys()) {\n+            if (propertyName.startsWith(FLOWFILE_REPOSITORY_DIRECTORY_PREFIX)) {\n+                final String directoryName = nifiProperties.getProperty(propertyName);\n+                flowFileRepositoryPaths.add(Paths.get(directoryName));\n+            }\n+        }\n+\n         numPartitions = nifiProperties.getFlowFileRepositoryPartitions();\n         checkpointDelayMillis = FormatUtils.getTimeDuration(nifiProperties.getFlowFileRepositoryCheckpointInterval(), TimeUnit.MILLISECONDS);\n \n@@ -140,14 +149,17 @@ public WriteAheadFlowFileRepository(final NiFiProperties nifiProperties) {\n     public void initialize(final ResourceClaimManager claimManager) throws IOException {\n         this.claimManager = claimManager;\n \n-        Files.createDirectories(flowFileRepositoryPath);\n+        for (final Path path : flowFileRepositoryPaths) {\n+            Files.createDirectories(path);\n+        }\n \n         // TODO: Should ensure that only 1 instance running and pointing at a particular path\n         // TODO: Allow for backup path that can be used if disk out of space?? Would allow a snapshot to be stored on\n         // backup and then the data deleted from the normal location; then can move backup to normal location and\n         // delete backup. On restore, if no files exist in partition's directory, would have to check backup directory\n         serdeFactory = new RepositoryRecordSerdeFactory(claimManager);\n-        wal = new MinimalLockingWriteAheadLog<>(flowFileRepositoryPath, numPartitions, serdeFactory, this);\n+        wal = new MinimalLockingWriteAheadLog<>(flowFileRepositoryPaths, numPartitions, serdeFactory, this);\n+        logger.info(\"Initialized FlowFile Repository using {} partitions\", numPartitions);\n     }\n \n     @Override\n@@ -167,12 +179,22 @@ public boolean isVolatile() {\n \n     @Override\n     public long getStorageCapacity() throws IOException {\n-        return Files.getFileStore(flowFileRepositoryPath).getTotalSpace();\n+        long capacity = 0L;\n+        for (final Path path : flowFileRepositoryPaths) {\n+            capacity += Files.getFileStore(path).getTotalSpace();\n+        }\n+\n+        return capacity;\n     }\n \n     @Override\n     public long getUsableStorageSpace() throws IOException {\n-        return Files.getFileStore(flowFileRepositoryPath).getUsableSpace();\n+        long usableSpace = 0L;\n+        for (final Path path : flowFileRepositoryPaths) {\n+            usableSpace += Files.getFileStore(path).getUsableSpace();\n+        }\n+\n+        return usableSpace;\n     }\n \n     @Override",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/WriteAheadFlowFileRepository.java",
                "sha": "9f8ff989eac92d17f696c702464214efc212fb7b",
                "status": "modified"
            },
            {
                "additions": 166,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/claim/ContentClaimWriteCache.java",
                "changes": 166,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/claim/ContentClaimWriteCache.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/claim/ContentClaimWriteCache.java",
                "patch": "@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.controller.repository.claim;\n+\n+import java.io.BufferedOutputStream;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.util.HashMap;\n+import java.util.LinkedList;\n+import java.util.Map;\n+import java.util.Queue;\n+\n+import org.apache.nifi.controller.repository.ContentRepository;\n+\n+public class ContentClaimWriteCache {\n+    private final ContentRepository contentRepo;\n+    private final Map<ResourceClaim, OutputStream> streamMap = new HashMap<>();\n+    private final Queue<ContentClaim> queue = new LinkedList<>();\n+    private final int bufferSize;\n+\n+    public ContentClaimWriteCache(final ContentRepository contentRepo) {\n+        this(contentRepo, 8192);\n+    }\n+\n+    public ContentClaimWriteCache(final ContentRepository contentRepo, final int bufferSize) {\n+        this.contentRepo = contentRepo;\n+        this.bufferSize = bufferSize;\n+    }\n+\n+    public void reset() throws IOException {\n+        try {\n+            forEachStream(OutputStream::close);\n+        } finally {\n+            streamMap.clear();\n+            queue.clear();\n+        }\n+    }\n+\n+    public ContentClaim getContentClaim() throws IOException {\n+        final ContentClaim contentClaim = queue.poll();\n+        if (contentClaim != null) {\n+            contentRepo.incrementClaimaintCount(contentClaim);\n+            return contentClaim;\n+        }\n+\n+        final ContentClaim claim = contentRepo.create(false);\n+        registerStream(claim);\n+        return claim;\n+    }\n+\n+    private OutputStream registerStream(final ContentClaim contentClaim) throws IOException {\n+        final OutputStream out = contentRepo.write(contentClaim);\n+        final OutputStream buffered = new BufferedOutputStream(out, bufferSize);\n+        streamMap.put(contentClaim.getResourceClaim(), buffered);\n+        return buffered;\n+    }\n+\n+    public OutputStream write(final ContentClaim claim) throws IOException {\n+        OutputStream out = streamMap.get(claim.getResourceClaim());\n+        if (out == null) {\n+            out = registerStream(claim);\n+        }\n+\n+        if (!(claim instanceof StandardContentClaim)) {\n+            // we know that we will only create Content Claims that are of type StandardContentClaim, so if we get anything\n+            // else, just throw an Exception because it is not valid for this Repository\n+            throw new IllegalArgumentException(\"Cannot write to \" + claim + \" because that Content Claim does belong to this Claim Cache\");\n+        }\n+\n+        final StandardContentClaim scc = (StandardContentClaim) claim;\n+        final long initialLength = Math.max(0L, scc.getLength());\n+\n+        final OutputStream bcos = out;\n+        return new OutputStream() {\n+            private long bytesWritten = 0L;\n+\n+            @Override\n+            public void write(final int b) throws IOException {\n+                bcos.write(b);\n+                bytesWritten++;\n+                scc.setLength(initialLength + bytesWritten);\n+            }\n+\n+            @Override\n+            public void write(byte[] b, int off, int len) throws IOException {\n+                bcos.write(b, off, len);\n+                bytesWritten += len;\n+                scc.setLength(initialLength + bytesWritten);\n+            }\n+\n+            @Override\n+            public void write(final byte[] b) throws IOException {\n+                write(b, 0, b.length);\n+            }\n+\n+            @Override\n+            public void flush() throws IOException {\n+                // do nothing - do not flush underlying stream.\n+            }\n+\n+            @Override\n+            public void close() throws IOException {\n+                queue.offer(claim);\n+            }\n+        };\n+    }\n+\n+    public void flush(final ContentClaim contentClaim) throws IOException {\n+        if (contentClaim == null) {\n+            return;\n+        }\n+\n+        flush(contentClaim.getResourceClaim());\n+    }\n+\n+    public void flush(final ResourceClaim claim) throws IOException {\n+        final OutputStream out = streamMap.get(claim);\n+        if (out != null) {\n+            out.flush();\n+        }\n+    }\n+\n+    public void flush() throws IOException {\n+        forEachStream(OutputStream::flush);\n+    }\n+\n+    private void forEachStream(final StreamProcessor proc) throws IOException {\n+        IOException exception = null;\n+\n+        for (final OutputStream out : streamMap.values()) {\n+            try {\n+                proc.process(out);\n+            } catch (final IOException ioe) {\n+                if (exception == null) {\n+                    exception = ioe;\n+                } else {\n+                    ioe.addSuppressed(exception);\n+                    exception = ioe;\n+                }\n+            }\n+        }\n+\n+        if (exception != null) {\n+            throw exception;\n+        }\n+    }\n+\n+    private interface StreamProcessor {\n+        void process(final OutputStream out) throws IOException;\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/claim/ContentClaimWriteCache.java",
                "sha": "6b608acc2197e367ec4fbf3967a0e46d90fce34e",
                "status": "added"
            },
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/schema/ContentClaimFieldMap.java",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/schema/ContentClaimFieldMap.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/schema/ContentClaimFieldMap.java",
                "patch": "@@ -64,6 +64,39 @@ public RecordSchema getSchema() {\n         return schema;\n     }\n \n+    @Override\n+    public int hashCode() {\n+        return (int) (31 + contentClaimOffset + 21 * resourceClaimFieldMap.hashCode());\n+    }\n+\n+    @Override\n+    public boolean equals(final Object obj) {\n+        if (this == obj) {\n+            return true;\n+        }\n+        if (obj == null) {\n+            return false;\n+        }\n+        if (getClass() != obj.getClass()) {\n+            return false;\n+        }\n+\n+        ContentClaimFieldMap other = (ContentClaimFieldMap) obj;\n+        if (contentClaimOffset != other.contentClaimOffset) {\n+            return false;\n+        }\n+\n+        if (resourceClaimFieldMap == null) {\n+            if (other.resourceClaimFieldMap != null) {\n+                return false;\n+            }\n+        } else if (!resourceClaimFieldMap.equals(other.resourceClaimFieldMap)) {\n+            return false;\n+        }\n+\n+        return true;\n+    }\n+\n     @Override\n     public String toString() {\n         return \"ContentClaimFieldMap[\" + contentClaim + \"]\";",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/schema/ContentClaimFieldMap.java",
                "sha": "b218ee60fe24e6d5c7b094a50dc0dc89e8540199",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/schema/RepositoryRecordUpdate.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/schema/RepositoryRecordUpdate.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 1,
                "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/schema/RepositoryRecordUpdate.java",
                "patch": "@@ -17,6 +17,7 @@\n \n package org.apache.nifi.controller.repository.schema;\n \n+import org.apache.nifi.controller.repository.RepositoryRecordType;\n import org.apache.nifi.repository.schema.NamedValue;\n import org.apache.nifi.repository.schema.Record;\n import org.apache.nifi.repository.schema.RecordSchema;\n@@ -39,7 +40,10 @@ public RecordSchema getSchema() {\n     @Override\n     public Object getFieldValue(final String fieldName) {\n         if (RepositoryRecordSchema.REPOSITORY_RECORD_UPDATE_V2.equals(fieldName)) {\n-            final String actionType = (String) fieldMap.getFieldValue(RepositoryRecordSchema.ACTION_TYPE);\n+            String actionType = (String) fieldMap.getFieldValue(RepositoryRecordSchema.ACTION_TYPE);\n+            if (RepositoryRecordType.CONTENTMISSING.name().equals(actionType)) {\n+                actionType = RepositoryRecordType.DELETE.name();\n+            }\n             final UpdateType updateType = UpdateType.valueOf(actionType);\n \n             final String actionName;",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/repository/schema/RepositoryRecordUpdate.java",
                "sha": "93fa4e41339c27a8fcee0f03a615a173c18832dc",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/scheduling/EventDrivenSchedulingAgent.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/scheduling/EventDrivenSchedulingAgent.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 2,
                "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/scheduling/EventDrivenSchedulingAgent.java",
                "patch": "@@ -83,7 +83,7 @@ public EventDrivenSchedulingAgent(final FlowEngine flowEngine, final ControllerS\n \n         for (int i = 0; i < maxThreadCount; i++) {\n             final Runnable eventDrivenTask = new EventDrivenTask(workerQueue);\n-            flowEngine.scheduleWithFixedDelay(eventDrivenTask, 0L, 30000, TimeUnit.NANOSECONDS);\n+            flowEngine.scheduleWithFixedDelay(eventDrivenTask, 0L, 1L, TimeUnit.NANOSECONDS);\n         }\n     }\n \n@@ -132,7 +132,7 @@ public void setMaxThreadCount(final int maxThreadCount) {\n             final int tasksToAdd = maxThreadCount - oldMax;\n             for (int i = 0; i < tasksToAdd; i++) {\n                 final Runnable eventDrivenTask = new EventDrivenTask(workerQueue);\n-                flowEngine.scheduleWithFixedDelay(eventDrivenTask, 0L, 30000, TimeUnit.NANOSECONDS);\n+                flowEngine.scheduleWithFixedDelay(eventDrivenTask, 0L, 1L, TimeUnit.NANOSECONDS);\n             }\n         }\n     }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/scheduling/EventDrivenSchedulingAgent.java",
                "sha": "22684a6d8828ca889e8af4b38ecdac199db81352",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/status/history/ProcessorStatusDescriptor.java",
                "changes": 48,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/status/history/ProcessorStatusDescriptor.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 11,
                "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/status/history/ProcessorStatusDescriptor.java",
                "patch": "@@ -87,6 +87,13 @@\n         Formatter.DURATION,\n         s -> TimeUnit.MILLISECONDS.convert(s.getProcessingNanos(), TimeUnit.NANOSECONDS))),\n \n+    TASK_NANOS(new StandardMetricDescriptor<ProcessorStatus>(\n+        \"taskNanos\",\n+        \"Total Task Time (nanos)\",\n+        \"The total number of thread-nanoseconds that the Processor has used to complete its tasks in the past 5 minutes\",\n+        Formatter.COUNT,\n+        ProcessorStatus::getProcessingNanos), false),\n+\n     FLOWFILES_REMOVED(new StandardMetricDescriptor<ProcessorStatus>(\n         \"flowFilesRemoved\",\n         \"FlowFiles Removed (5 mins)\",\n@@ -122,35 +129,50 @@ public Long reduce(final List<StatusSnapshot> values) {\n         }\n     )),\n \n-    AVERAGE_TASK_MILLIS(new StandardMetricDescriptor<ProcessorStatus>(\n-        \"averageTaskMillis\",\n-        \"Average Task Duration\",\n-        \"The average duration it took this Processor to complete a task, as averaged over the past 5 minutes\",\n-        Formatter.DURATION,\n-        s -> s.getInvocations() == 0 ? 0 : TimeUnit.MILLISECONDS.convert(s.getProcessingNanos(), TimeUnit.NANOSECONDS) / s.getInvocations(),\n+    AVERAGE_TASK_NANOS(new StandardMetricDescriptor<ProcessorStatus>(\n+        \"averageTaskNanos\",\n+        \"Average Task Duration (nanoseconds)\",\n+        \"The average number of nanoseconds it took this Processor to complete a task, over the past 5 minutes\",\n+        Formatter.COUNT,\n+        s -> s.getInvocations() == 0 ? 0 : s.getProcessingNanos() / s.getInvocations(),\n         new ValueReducer<StatusSnapshot, Long>() {\n             @Override\n             public Long reduce(final List<StatusSnapshot> values) {\n-                long procMillis = 0L;\n+                long procNanos = 0L;\n                 int invocations = 0;\n \n                 for (final StatusSnapshot snapshot : values) {\n-                    procMillis += snapshot.getStatusMetrics().get(TASK_MILLIS.getDescriptor()).longValue();\n-                    invocations += snapshot.getStatusMetrics().get(TASK_COUNT.getDescriptor()).intValue();\n+                    final Long taskNanos = snapshot.getStatusMetrics().get(TASK_NANOS.getDescriptor());\n+                    if (taskNanos != null) {\n+                        procNanos += taskNanos.longValue();\n+                    }\n+\n+                    final Long taskInvocations = snapshot.getStatusMetrics().get(TASK_COUNT.getDescriptor());\n+                    if (taskInvocations != null) {\n+                        invocations += taskInvocations.intValue();\n+                    }\n                 }\n \n                 if (invocations == 0) {\n                     return 0L;\n                 }\n \n-                return procMillis / invocations;\n+                return procNanos / invocations;\n             }\n         }));\n \n-    private MetricDescriptor<ProcessorStatus> descriptor;\n+\n+\n+    private final MetricDescriptor<ProcessorStatus> descriptor;\n+    private final boolean visible;\n \n     private ProcessorStatusDescriptor(final MetricDescriptor<ProcessorStatus> descriptor) {\n+        this(descriptor, true);\n+    }\n+\n+    private ProcessorStatusDescriptor(final MetricDescriptor<ProcessorStatus> descriptor, final boolean visible) {\n         this.descriptor = descriptor;\n+        this.visible = visible;\n     }\n \n     public String getField() {\n@@ -160,4 +182,8 @@ public String getField() {\n     public MetricDescriptor<ProcessorStatus> getDescriptor() {\n         return descriptor;\n     }\n+\n+    public boolean isVisible() {\n+        return visible;\n+    }\n }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/status/history/ProcessorStatusDescriptor.java",
                "sha": "59a4b1b377ab688d195fea64d1dbaaa32ea4eed9",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/status/history/VolatileComponentStatusRepository.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/status/history/VolatileComponentStatusRepository.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 1,
                "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/status/history/VolatileComponentStatusRepository.java",
                "patch": "@@ -93,7 +93,9 @@ public boolean evaluate(final Capture capture) {\n                 snapshot.setTimestamp(capture.getCaptureDate());\n \n                 for (final ProcessorStatusDescriptor descriptor : ProcessorStatusDescriptor.values()) {\n-                    snapshot.addStatusMetric(descriptor.getDescriptor(), descriptor.getDescriptor().getValueFunction().getValue(status));\n+                    if (descriptor.isVisible()) {\n+                        snapshot.addStatusMetric(descriptor.getDescriptor(), descriptor.getDescriptor().getValueFunction().getValue(status));\n+                    }\n                 }\n \n                 history.addStatusSnapshot(snapshot);",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/status/history/VolatileComponentStatusRepository.java",
                "sha": "320397267b9c08bffe103c89683ab689747061bb",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/tasks/ContinuallyRunProcessorTask.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/tasks/ContinuallyRunProcessorTask.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 1,
                "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/tasks/ContinuallyRunProcessorTask.java",
                "patch": "@@ -82,6 +82,13 @@ static boolean isWorkToDo(final ProcessorNode procNode) {\n         return procNode.isTriggerWhenEmpty() || !procNode.hasIncomingConnection() || !Connectables.hasNonLoopConnection(procNode) || Connectables.flowFilesQueued(procNode);\n     }\n \n+    private boolean isBackPressureEngaged() {\n+        return procNode.getIncomingConnections().stream()\n+            .filter(con -> con.getSource() == procNode)\n+            .map(con -> con.getFlowFileQueue())\n+            .anyMatch(queue -> queue.isFull());\n+    }\n+\n     @Override\n     public Boolean call() {\n         // make sure processor is not yielded\n@@ -127,6 +134,7 @@ public Boolean call() {\n         scheduleState.incrementActiveThreadCount();\n \n         final long startNanos = System.nanoTime();\n+        final long finishIfBackpressureEngaged = startNanos + (batchNanos / 25L);\n         final long finishNanos = startNanos + batchNanos;\n         int invocationCount = 0;\n         try {\n@@ -140,10 +148,16 @@ public Boolean call() {\n                         return false;\n                     }\n \n-                    if (System.nanoTime() > finishNanos) {\n+                    final long nanoTime = System.nanoTime();\n+                    if (nanoTime > finishNanos) {\n+                        return false;\n+                    }\n+\n+                    if (nanoTime > finishIfBackpressureEngaged && isBackPressureEngaged()) {\n                         return false;\n                     }\n \n+\n                     if (!isWorkToDo(procNode)) {\n                         break;\n                     }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/tasks/ContinuallyRunProcessorTask.java",
                "sha": "01f3c8cb0a851f9575172caf3304c0fdd50e4601",
                "status": "modified"
            },
            {
                "additions": 37,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/test/java/org/apache/nifi/controller/repository/TestFileSystemRepository.java",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/test/java/org/apache/nifi/controller/repository/TestFileSystemRepository.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 2,
                "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/test/java/org/apache/nifi/controller/repository/TestFileSystemRepository.java",
                "patch": "@@ -34,10 +34,15 @@\n import java.nio.file.Path;\n import java.nio.file.StandardCopyOption;\n import java.nio.file.StandardOpenOption;\n+import java.text.NumberFormat;\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collections;\n+import java.util.HashMap;\n import java.util.List;\n+import java.util.Locale;\n+import java.util.Map;\n+import java.util.Random;\n import java.util.concurrent.TimeUnit;\n \n import org.apache.nifi.controller.repository.claim.ContentClaim;\n@@ -56,8 +61,6 @@\n import ch.qos.logback.classic.Logger;\n import ch.qos.logback.classic.spi.ILoggingEvent;\n import ch.qos.logback.core.read.ListAppender;\n-import java.util.HashMap;\n-import java.util.Map;\n \n public class TestFileSystemRepository {\n \n@@ -88,6 +91,38 @@ public void shutdown() throws IOException {\n         repository.shutdown();\n     }\n \n+    @Test\n+    public void testWritePerformance() throws IOException {\n+        final long bytesToWrite = 1_000_000_000L;\n+        final int contentSize = 100;\n+\n+        final int iterations = (int) (bytesToWrite / contentSize);\n+        final byte[] content = new byte[contentSize];\n+        final Random random = new Random();\n+        random.nextBytes(content);\n+\n+        //        final ContentClaimWriteCache cache = new ContentClaimWriteCache(repository);\n+\n+        final long start = System.nanoTime();\n+        for (int i = 0; i < iterations; i++) {\n+            final ContentClaim claim = repository.create(false);\n+            try (final OutputStream out = repository.write(claim)) {\n+                out.write(content);\n+            }\n+            //            final ContentClaim claim = cache.getContentClaim();\n+            //            try (final OutputStream out = cache.write(claim)) {\n+            //                out.write(content);\n+            //            }\n+        }\n+        final long millis = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);\n+\n+        final long mb = bytesToWrite / (1024 * 1024);\n+        final long seconds = millis / 1000L;\n+        final double mbps = (double) mb / (double) seconds;\n+        System.out.println(\"Took \" + millis + \" millis to write \" + contentSize + \" bytes \" + iterations + \" times (total of \"\n+            + NumberFormat.getNumberInstance(Locale.US).format(bytesToWrite) + \" bytes) for a write rate of \" + mbps + \" MB/s\");\n+    }\n+\n     @Test\n     public void testMinimalArchiveCleanupIntervalHonoredAndLogged() throws Exception {\n         // We are going to construct our own repository using different properties, so",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/test/java/org/apache/nifi/controller/repository/TestFileSystemRepository.java",
                "sha": "845ca560078720bb1761a922bd0e59223a63a3ea",
                "status": "modified"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/test/java/org/apache/nifi/controller/repository/TestStandardProcessSession.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/test/java/org/apache/nifi/controller/repository/TestStandardProcessSession.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 2,
                "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/test/java/org/apache/nifi/controller/repository/TestStandardProcessSession.java",
                "patch": "@@ -86,6 +86,7 @@\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Before;\n+import org.junit.Ignore;\n import org.junit.Test;\n import org.mockito.Mockito;\n import org.mockito.invocation.InvocationOnMock;\n@@ -987,6 +988,7 @@ public void testExpireDecrementsClaimsOnce() throws IOException {\n     }\n \n     @Test\n+    @Ignore\n     public void testManyFilesOpened() throws IOException {\n \n         StandardProcessSession[] standardProcessSessions = new StandardProcessSession[100000];\n@@ -1672,9 +1674,9 @@ public ContentClaim create(boolean lossTolerant) throws IOException {\n \n         @Override\n         public int incrementClaimaintCount(ContentClaim claim) {\n-            final AtomicInteger count = claimantCounts.get(claim);\n+            AtomicInteger count = claimantCounts.get(claim);\n             if (count == null) {\n-                throw new IllegalArgumentException(\"Unknown Claim: \" + claim);\n+                count = new AtomicInteger(0);\n             }\n             return count.incrementAndGet();\n         }\n@@ -1835,6 +1837,11 @@ public void write(byte[] b) throws IOException {\n                     fos.write(b);\n                     ((StandardContentClaim) claim).setLength(claim.getLength() + b.length);\n                 }\n+\n+                @Override\n+                public void close() throws IOException {\n+                    super.close();\n+                }\n             };\n         }\n ",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/test/java/org/apache/nifi/controller/repository/TestStandardProcessSession.java",
                "sha": "e0c9ffee5ed39d65a7cafced7ed5b5486e72f0cf",
                "status": "modified"
            },
            {
                "additions": 261,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/test/java/org/apache/nifi/controller/repository/TestWriteAheadFlowFileRepository.java",
                "changes": 261,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/test/java/org/apache/nifi/controller/repository/TestWriteAheadFlowFileRepository.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/test/java/org/apache/nifi/controller/repository/TestWriteAheadFlowFileRepository.java",
                "patch": "@@ -18,6 +18,7 @@\n \n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n import static org.mockito.Matchers.any;\n import static org.mockito.Mockito.doAnswer;\n import static org.mockito.Mockito.when;\n@@ -33,12 +34,17 @@\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n import java.util.UUID;\n+import java.util.concurrent.TimeUnit;\n \n import org.apache.nifi.connectable.Connectable;\n import org.apache.nifi.connectable.Connection;\n import org.apache.nifi.controller.StandardFlowFileQueue;\n+import org.apache.nifi.controller.queue.DropFlowFileStatus;\n import org.apache.nifi.controller.queue.FlowFileQueue;\n+import org.apache.nifi.controller.queue.ListFlowFileStatus;\n import org.apache.nifi.controller.queue.QueueSize;\n import org.apache.nifi.controller.repository.claim.ContentClaim;\n import org.apache.nifi.controller.repository.claim.ResourceClaim;\n@@ -47,15 +53,22 @@\n import org.apache.nifi.controller.repository.claim.StandardResourceClaimManager;\n import org.apache.nifi.controller.swap.StandardSwapContents;\n import org.apache.nifi.controller.swap.StandardSwapSummary;\n+import org.apache.nifi.flowfile.FlowFilePrioritizer;\n+import org.apache.nifi.processor.FlowFileFilter;\n+import org.apache.nifi.util.MockFlowFile;\n import org.apache.nifi.util.NiFiProperties;\n import org.apache.nifi.util.file.FileUtils;\n import org.junit.After;\n+import org.junit.Assert;\n import org.junit.Before;\n import org.junit.BeforeClass;\n+import org.junit.Ignore;\n import org.junit.Test;\n import org.mockito.Mockito;\n import org.mockito.invocation.InvocationOnMock;\n import org.mockito.stubbing.Answer;\n+import org.wali.MinimalLockingWriteAheadLog;\n+import org.wali.WriteAheadRepository;\n \n public class TestWriteAheadFlowFileRepository {\n \n@@ -74,6 +87,254 @@ public void clearRepo() throws IOException {\n         }\n     }\n \n+\n+    @Test\n+    @Ignore(\"Intended only for local performance testing before/after making changes\")\n+    public void testUpdatePerformance() throws IOException, InterruptedException {\n+        final FlowFileQueue queue = new FlowFileQueue() {\n+\n+            @Override\n+            public String getIdentifier() {\n+                return \"4444\";\n+            }\n+\n+            @Override\n+            public List<FlowFilePrioritizer> getPriorities() {\n+                return null;\n+            }\n+\n+            @Override\n+            public SwapSummary recoverSwappedFlowFiles() {\n+                return null;\n+            }\n+\n+            @Override\n+            public void purgeSwapFiles() {\n+            }\n+\n+            @Override\n+            public void setPriorities(List<FlowFilePrioritizer> newPriorities) {\n+            }\n+\n+            @Override\n+            public void setBackPressureObjectThreshold(long maxQueueSize) {\n+            }\n+\n+            @Override\n+            public long getBackPressureObjectThreshold() {\n+                return 0;\n+            }\n+\n+            @Override\n+            public void setBackPressureDataSizeThreshold(String maxDataSize) {\n+            }\n+\n+            @Override\n+            public String getBackPressureDataSizeThreshold() {\n+                return null;\n+            }\n+\n+            @Override\n+            public QueueSize size() {\n+                return null;\n+            }\n+\n+            @Override\n+            public boolean isEmpty() {\n+                return false;\n+            }\n+\n+            @Override\n+            public boolean isActiveQueueEmpty() {\n+                return false;\n+            }\n+\n+            @Override\n+            public QueueSize getUnacknowledgedQueueSize() {\n+                return null;\n+            }\n+\n+            @Override\n+            public void acknowledge(FlowFileRecord flowFile) {\n+            }\n+\n+            @Override\n+            public void acknowledge(Collection<FlowFileRecord> flowFiles) {\n+            }\n+\n+            @Override\n+            public boolean isFull() {\n+                return false;\n+            }\n+\n+            @Override\n+            public void put(FlowFileRecord file) {\n+            }\n+\n+            @Override\n+            public void putAll(Collection<FlowFileRecord> files) {\n+            }\n+\n+            @Override\n+            public FlowFileRecord poll(Set<FlowFileRecord> expiredRecords) {\n+                return null;\n+            }\n+\n+            @Override\n+            public List<FlowFileRecord> poll(int maxResults, Set<FlowFileRecord> expiredRecords) {\n+                return null;\n+            }\n+\n+            @Override\n+            public long drainQueue(Queue<FlowFileRecord> sourceQueue, List<FlowFileRecord> destination, int maxResults, Set<FlowFileRecord> expiredRecords) {\n+                return 0;\n+            }\n+\n+            @Override\n+            public List<FlowFileRecord> poll(FlowFileFilter filter, Set<FlowFileRecord> expiredRecords) {\n+                return null;\n+            }\n+\n+            @Override\n+            public String getFlowFileExpiration() {\n+                return null;\n+            }\n+\n+            @Override\n+            public int getFlowFileExpiration(TimeUnit timeUnit) {\n+                return 0;\n+            }\n+\n+            @Override\n+            public void setFlowFileExpiration(String flowExpirationPeriod) {\n+            }\n+\n+            @Override\n+            public DropFlowFileStatus dropFlowFiles(String requestIdentifier, String requestor) {\n+                return null;\n+            }\n+\n+            @Override\n+            public DropFlowFileStatus getDropFlowFileStatus(String requestIdentifier) {\n+                return null;\n+            }\n+\n+            @Override\n+            public DropFlowFileStatus cancelDropFlowFileRequest(String requestIdentifier) {\n+                return null;\n+            }\n+\n+            @Override\n+            public ListFlowFileStatus listFlowFiles(String requestIdentifier, int maxResults) {\n+                return null;\n+            }\n+\n+            @Override\n+            public ListFlowFileStatus getListFlowFileStatus(String requestIdentifier) {\n+                return null;\n+            }\n+\n+            @Override\n+            public ListFlowFileStatus cancelListFlowFileRequest(String requestIdentifier) {\n+                return null;\n+            }\n+\n+            @Override\n+            public FlowFileRecord getFlowFile(String flowFileUuid) throws IOException {\n+                return null;\n+            }\n+\n+            @Override\n+            public void verifyCanList() throws IllegalStateException {\n+            }\n+        };\n+\n+\n+        final int numPartitions = 16;\n+        final int numThreads = 8;\n+        final int totalUpdates = 160_000_000;\n+        final int batchSize = 10;\n+\n+        final Path path = Paths.get(\"target/minimal-locking-repo\");\n+        deleteRecursively(path.toFile());\n+        assertTrue(path.toFile().mkdirs());\n+\n+        final ResourceClaimManager claimManager = new StandardResourceClaimManager();\n+        final RepositoryRecordSerdeFactory serdeFactory = new RepositoryRecordSerdeFactory(claimManager);\n+        final WriteAheadRepository<RepositoryRecord> repo = new MinimalLockingWriteAheadLog<>(path, numPartitions, serdeFactory, null);\n+        final Collection<RepositoryRecord> initialRecs = repo.recoverRecords();\n+        assertTrue(initialRecs.isEmpty());\n+\n+        final int updateCountPerThread = totalUpdates / numThreads;\n+\n+        final Thread[] threads = new Thread[numThreads];\n+        for (int j = 0; j < 2; j++) {\n+            for (int i = 0; i < numThreads; i++) {\n+                final Thread t = new Thread(new Runnable() {\n+                    @Override\n+                    public void run() {\n+                        final List<RepositoryRecord> records = new ArrayList<>();\n+                        final int numBatches = updateCountPerThread / batchSize;\n+                        final MockFlowFile baseFlowFile = new MockFlowFile(0L);\n+\n+                        for (int i = 0; i < numBatches; i++) {\n+                            records.clear();\n+                            for (int k = 0; k < batchSize; k++) {\n+                                final FlowFileRecord flowFile = new MockFlowFile(i % 100_000, baseFlowFile);\n+                                final String uuid = flowFile.getAttribute(\"uuid\");\n+\n+                                final StandardRepositoryRecord record = new StandardRepositoryRecord(null, flowFile);\n+                                record.setDestination(queue);\n+                                final Map<String, String> updatedAttrs = Collections.singletonMap(\"uuid\", uuid);\n+                                record.setWorking(flowFile, updatedAttrs);\n+\n+                                records.add(record);\n+                            }\n+\n+                            try {\n+                                repo.update(records, false);\n+                            } catch (IOException e) {\n+                                e.printStackTrace();\n+                                Assert.fail(e.toString());\n+                            }\n+                        }\n+                    }\n+                });\n+\n+                t.setDaemon(true);\n+                threads[i] = t;\n+            }\n+\n+            final long start = System.nanoTime();\n+            for (final Thread t : threads) {\n+                t.start();\n+            }\n+            for (final Thread t : threads) {\n+                t.join();\n+            }\n+\n+            final long millis = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);\n+            if (j == 0) {\n+                System.out.println(millis + \" ms to insert \" + updateCountPerThread * numThreads + \" updates using \" + numPartitions + \" partitions and \" + numThreads + \" threads, *as a warmup!*\");\n+            } else {\n+                System.out.println(millis + \" ms to insert \" + updateCountPerThread * numThreads + \" updates using \" + numPartitions + \" partitions and \" + numThreads + \" threads\");\n+            }\n+        }\n+    }\n+\n+    private void deleteRecursively(final File file) {\n+        final File[] children = file.listFiles();\n+        if (children != null) {\n+            for (final File child : children) {\n+                deleteRecursively(child);\n+            }\n+        }\n+\n+        file.delete();\n+    }\n+\n+\n+\n     @Test\n     public void testResourceClaimsIncremented() throws IOException {\n         final ResourceClaimManager claimManager = new StandardResourceClaimManager();",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/test/java/org/apache/nifi/controller/repository/TestWriteAheadFlowFileRepository.java",
                "sha": "329b26819e04c8a89862356a85d6029d5055ab76",
                "status": "modified"
            },
            {
                "additions": 98,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/test/java/org/apache/nifi/controller/repository/claim/TestContentClaimWriteCache.java",
                "changes": 98,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/test/java/org/apache/nifi/controller/repository/claim/TestContentClaimWriteCache.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/test/java/org/apache/nifi/controller/repository/claim/TestContentClaimWriteCache.java",
                "patch": "@@ -0,0 +1,98 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.controller.repository.claim;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+\n+import org.apache.nifi.controller.repository.FileSystemRepository;\n+import org.apache.nifi.controller.repository.TestFileSystemRepository;\n+import org.apache.nifi.controller.repository.util.DiskUtils;\n+import org.apache.nifi.stream.io.StreamUtils;\n+import org.apache.nifi.util.NiFiProperties;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Test;\n+\n+public class TestContentClaimWriteCache {\n+\n+    private FileSystemRepository repository = null;\n+    private StandardResourceClaimManager claimManager = null;\n+    private final File rootFile = new File(\"target/testContentClaimWriteCache\");\n+    private NiFiProperties nifiProperties;\n+\n+    @Before\n+    public void setup() throws IOException {\n+        System.setProperty(NiFiProperties.PROPERTIES_FILE_PATH, TestFileSystemRepository.class.getResource(\"/conf/nifi.properties\").getFile());\n+        nifiProperties = NiFiProperties.createBasicNiFiProperties(null, null);\n+        if (rootFile.exists()) {\n+            DiskUtils.deleteRecursively(rootFile);\n+        }\n+        repository = new FileSystemRepository(nifiProperties);\n+        claimManager = new StandardResourceClaimManager();\n+        repository.initialize(claimManager);\n+        repository.purge();\n+    }\n+\n+    @After\n+    public void shutdown() throws IOException {\n+        repository.shutdown();\n+    }\n+\n+    @Test\n+    public void testFlushWriteCorrectData() throws IOException {\n+        final ContentClaimWriteCache cache = new ContentClaimWriteCache(repository, 4);\n+\n+        final ContentClaim claim1 = cache.getContentClaim();\n+        assertNotNull(claim1);\n+\n+        final OutputStream out = cache.write(claim1);\n+        assertNotNull(out);\n+        out.write(\"hello\".getBytes());\n+        out.write(\"good-bye\".getBytes());\n+\n+        cache.flush();\n+\n+        assertEquals(13L, claim1.getLength());\n+        final InputStream in = repository.read(claim1);\n+        final byte[] buff = new byte[(int) claim1.getLength()];\n+        StreamUtils.fillBuffer(in, buff);\n+        Assert.assertArrayEquals(\"hellogood-bye\".getBytes(), buff);\n+\n+        final ContentClaim claim2 = cache.getContentClaim();\n+        final OutputStream out2 = cache.write(claim2);\n+        assertNotNull(out2);\n+        out2.write(\"good-day\".getBytes());\n+        out2.write(\"hello\".getBytes());\n+\n+        cache.flush();\n+\n+        assertEquals(13L, claim2.getLength());\n+        final InputStream in2 = repository.read(claim2);\n+        final byte[] buff2 = new byte[(int) claim2.getLength()];\n+        StreamUtils.fillBuffer(in2, buff2);\n+        Assert.assertArrayEquals(\"good-dayhello\".getBytes(), buff2);\n+    }\n+\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/test/java/org/apache/nifi/controller/repository/claim/TestContentClaimWriteCache.java",
                "sha": "fc08f55c028ed72cf267c5804ecf0e0a8f17acb5",
                "status": "added"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-resources/pom.xml",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-resources/pom.xml?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 1,
                "filename": "nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-resources/pom.xml",
                "patch": "@@ -98,14 +98,16 @@\n         <nifi.provenance.repository.rollover.time>30 secs</nifi.provenance.repository.rollover.time>\n         <nifi.provenance.repository.rollover.size>100 MB</nifi.provenance.repository.rollover.size>\n         <nifi.provenance.repository.query.threads>2</nifi.provenance.repository.query.threads>\n-        <nifi.provenance.repository.index.threads>1</nifi.provenance.repository.index.threads>\n+        <nifi.provenance.repository.index.threads>2</nifi.provenance.repository.index.threads>\n         <nifi.provenance.repository.compress.on.rollover>true</nifi.provenance.repository.compress.on.rollover>\n         <nifi.provenance.repository.indexed.fields>EventType, FlowFileUUID, Filename, ProcessorID, Relationship</nifi.provenance.repository.indexed.fields>\n         <nifi.provenance.repository.indexed.attributes />\n         <nifi.provenance.repository.index.shard.size>500 MB</nifi.provenance.repository.index.shard.size>\n         <nifi.provenance.repository.always.sync>false</nifi.provenance.repository.always.sync>\n         <nifi.provenance.repository.journal.count>16</nifi.provenance.repository.journal.count>\n         <nifi.provenance.repository.max.attribute.length>65536</nifi.provenance.repository.max.attribute.length>\n+        <nifi.provenance.repository.concurrent.merge.threads>2</nifi.provenance.repository.concurrent.merge.threads>\n+        <nifi.provenance.repository.warm.cache.frequency>1 hour</nifi.provenance.repository.warm.cache.frequency>\n \n         <!-- volatile provenance repository properties -->\n         <nifi.provenance.repository.buffer.size>100000</nifi.provenance.repository.buffer.size>",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-resources/pom.xml",
                "sha": "cecfabf4aeebadc82dd8d1104534aa00bb0fd734",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/AbstractRecordWriter.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/AbstractRecordWriter.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 7,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/AbstractRecordWriter.java",
                "patch": "@@ -32,26 +32,34 @@\n     private static final Logger logger = LoggerFactory.getLogger(AbstractRecordWriter.class);\n \n     private final File file;\n+    private final String storageLocation;\n     private final TocWriter tocWriter;\n     private final Lock lock = new ReentrantLock();\n \n     private volatile boolean dirty = false;\n     private volatile boolean closed = false;\n \n-    private int recordsWritten = 0;\n-\n     public AbstractRecordWriter(final File file, final TocWriter writer) throws IOException {\n         logger.trace(\"Creating Record Writer for {}\", file);\n \n         this.file = file;\n+        this.storageLocation = file.getName();\n+        this.tocWriter = writer;\n+    }\n+\n+    public AbstractRecordWriter(final String storageLocation, final TocWriter writer) throws IOException {\n+        logger.trace(\"Creating Record Writer for {}\", storageLocation);\n+\n+        this.file = null;\n+        this.storageLocation = storageLocation;\n         this.tocWriter = writer;\n     }\n \n     @Override\n     public synchronized void close() throws IOException {\n         closed = true;\n \n-        logger.trace(\"Closing Record Writer for {}\", file == null ? null : file.getName());\n+        logger.trace(\"Closing Record Writer for {}\", getStorageLocation());\n \n         lock();\n         try {\n@@ -94,9 +102,8 @@ public synchronized void close() throws IOException {\n         }\n     }\n \n-    @Override\n-    public int getRecordsWritten() {\n-        return recordsWritten;\n+    protected String getStorageLocation() {\n+        return storageLocation;\n     }\n \n     @Override\n@@ -133,6 +140,7 @@ public void markDirty() {\n         this.dirty = true;\n     }\n \n+    @Override\n     public boolean isDirty() {\n         return dirty;\n     }\n@@ -142,7 +150,7 @@ protected void resetDirtyFlag() {\n     }\n \n     @Override\n-    public void sync() throws IOException {\n+    public synchronized void sync() throws IOException {\n         try {\n             if (tocWriter != null) {\n                 tocWriter.sync();",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/AbstractRecordWriter.java",
                "sha": "fcc481b9ad387a700b6f73483888663693f5f4ef",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/ByteArraySchemaRecordWriter.java",
                "changes": 17,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/ByteArraySchemaRecordWriter.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 7,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/ByteArraySchemaRecordWriter.java",
                "patch": "@@ -18,34 +18,37 @@\n package org.apache.nifi.provenance;\n \n import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n import java.io.File;\n import java.io.IOException;\n import java.io.OutputStream;\n+import java.util.concurrent.atomic.AtomicLong;\n \n+import org.apache.nifi.provenance.schema.EventFieldNames;\n import org.apache.nifi.provenance.schema.EventRecord;\n-import org.apache.nifi.provenance.schema.EventRecordFields;\n import org.apache.nifi.provenance.schema.ProvenanceEventSchema;\n import org.apache.nifi.provenance.serialization.CompressableRecordWriter;\n import org.apache.nifi.provenance.toc.TocWriter;\n import org.apache.nifi.repository.schema.Record;\n import org.apache.nifi.repository.schema.RecordSchema;\n import org.apache.nifi.repository.schema.SchemaRecordWriter;\n-import org.apache.nifi.stream.io.DataOutputStream;\n \n public class ByteArraySchemaRecordWriter extends CompressableRecordWriter {\n     private static final RecordSchema eventSchema = ProvenanceEventSchema.PROVENANCE_EVENT_SCHEMA_V1;\n-    private static final RecordSchema contentClaimSchema = new RecordSchema(eventSchema.getField(EventRecordFields.Names.CONTENT_CLAIM).getSubFields());\n+    private static final RecordSchema contentClaimSchema = new RecordSchema(eventSchema.getField(EventFieldNames.CONTENT_CLAIM).getSubFields());\n     public static final int SERIALIZATION_VERSION = 1;\n     public static final String SERIALIZATION_NAME = \"ByteArraySchemaRecordWriter\";\n \n     private final SchemaRecordWriter recordWriter = new SchemaRecordWriter();\n \n-    public ByteArraySchemaRecordWriter(final File file, final TocWriter tocWriter, final boolean compressed, final int uncompressedBlockSize) throws IOException {\n-        super(file, tocWriter, compressed, uncompressedBlockSize);\n+    public ByteArraySchemaRecordWriter(final File file, final AtomicLong idGenerator, final TocWriter tocWriter, final boolean compressed,\n+        final int uncompressedBlockSize) throws IOException {\n+        super(file, idGenerator, tocWriter, compressed, uncompressedBlockSize);\n     }\n \n-    public ByteArraySchemaRecordWriter(final OutputStream out, final TocWriter tocWriter, final boolean compressed, final int uncompressedBlockSize) throws IOException {\n-        super(out, tocWriter, compressed, uncompressedBlockSize);\n+    public ByteArraySchemaRecordWriter(final OutputStream out, final String storageLocation, final AtomicLong idGenerator, final TocWriter tocWriter, final boolean compressed,\n+        final int uncompressedBlockSize) throws IOException {\n+        super(out, storageLocation, idGenerator, tocWriter, compressed, uncompressedBlockSize);\n     }\n \n     @Override",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/ByteArraySchemaRecordWriter.java",
                "sha": "2a42501530d5690cb861b2d1706977d63d6d35b3",
                "status": "modified"
            },
            {
                "additions": 145,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/EventIdFirstSchemaRecordReader.java",
                "changes": 145,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/EventIdFirstSchemaRecordReader.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/EventIdFirstSchemaRecordReader.java",
                "patch": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.DataInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import org.apache.nifi.provenance.schema.EventIdFirstHeaderSchema;\n+import org.apache.nifi.provenance.schema.LookupTableEventRecord;\n+import org.apache.nifi.provenance.serialization.CompressableRecordReader;\n+import org.apache.nifi.provenance.toc.TocReader;\n+import org.apache.nifi.repository.schema.Record;\n+import org.apache.nifi.repository.schema.RecordSchema;\n+import org.apache.nifi.repository.schema.SchemaRecordReader;\n+import org.apache.nifi.stream.io.LimitingInputStream;\n+import org.apache.nifi.stream.io.StreamUtils;\n+\n+public class EventIdFirstSchemaRecordReader extends CompressableRecordReader {\n+    private RecordSchema schema; // effectively final\n+    private SchemaRecordReader recordReader;  // effectively final\n+\n+    private List<String> componentIds;\n+    private List<String> componentTypes;\n+    private List<String> queueIds;\n+    private List<String> eventTypes;\n+    private long firstEventId;\n+    private long systemTimeOffset;\n+\n+    public EventIdFirstSchemaRecordReader(final InputStream in, final String filename, final TocReader tocReader, final int maxAttributeChars) throws IOException {\n+        super(in, filename, tocReader, maxAttributeChars);\n+    }\n+\n+    private void verifySerializationVersion(final int serializationVersion) {\n+        if (serializationVersion > EventIdFirstSchemaRecordWriter.SERIALIZATION_VERSION) {\n+            throw new IllegalArgumentException(\"Unable to deserialize record because the version is \" + serializationVersion\n+                + \" and supported versions are 1-\" + EventIdFirstSchemaRecordWriter.SERIALIZATION_VERSION);\n+        }\n+    }\n+\n+    @Override\n+    @SuppressWarnings(\"unchecked\")\n+    protected synchronized void readHeader(final DataInputStream in, final int serializationVersion) throws IOException {\n+        verifySerializationVersion(serializationVersion);\n+        final int eventSchemaLength = in.readInt();\n+        final byte[] buffer = new byte[eventSchemaLength];\n+        StreamUtils.fillBuffer(in, buffer);\n+\n+        try (final ByteArrayInputStream bais = new ByteArrayInputStream(buffer)) {\n+            schema = RecordSchema.readFrom(bais);\n+        }\n+\n+        recordReader = SchemaRecordReader.fromSchema(schema);\n+\n+        final int headerSchemaLength = in.readInt();\n+        final byte[] headerSchemaBuffer = new byte[headerSchemaLength];\n+        StreamUtils.fillBuffer(in, headerSchemaBuffer);\n+\n+        final RecordSchema headerSchema;\n+        try (final ByteArrayInputStream bais = new ByteArrayInputStream(headerSchemaBuffer)) {\n+            headerSchema = RecordSchema.readFrom(bais);\n+        }\n+\n+        final SchemaRecordReader headerReader = SchemaRecordReader.fromSchema(headerSchema);\n+        final Record headerRecord = headerReader.readRecord(in);\n+        componentIds = (List<String>) headerRecord.getFieldValue(EventIdFirstHeaderSchema.FieldNames.COMPONENT_IDS);\n+        componentTypes = (List<String>) headerRecord.getFieldValue(EventIdFirstHeaderSchema.FieldNames.COMPONENT_TYPES);\n+        queueIds = (List<String>) headerRecord.getFieldValue(EventIdFirstHeaderSchema.FieldNames.QUEUE_IDS);\n+        eventTypes = (List<String>) headerRecord.getFieldValue(EventIdFirstHeaderSchema.FieldNames.EVENT_TYPES);\n+        firstEventId = (Long) headerRecord.getFieldValue(EventIdFirstHeaderSchema.FieldNames.FIRST_EVENT_ID);\n+        systemTimeOffset = (Long) headerRecord.getFieldValue(EventIdFirstHeaderSchema.FieldNames.TIMESTAMP_OFFSET);\n+    }\n+\n+    @Override\n+    protected StandardProvenanceEventRecord nextRecord(final DataInputStream in, final int serializationVersion) throws IOException {\n+        verifySerializationVersion(serializationVersion);\n+\n+        final long byteOffset = getBytesConsumed();\n+        final long eventId = in.readInt() + firstEventId;\n+        final int recordLength = in.readInt();\n+\n+        return readRecord(in, eventId, byteOffset, recordLength);\n+    }\n+\n+    private StandardProvenanceEventRecord readRecord(final DataInputStream in, final long eventId, final long startOffset, final int recordLength) throws IOException {\n+        final InputStream limitedIn = new LimitingInputStream(in, recordLength);\n+\n+        final Record eventRecord = recordReader.readRecord(limitedIn);\n+        if (eventRecord == null) {\n+            return null;\n+        }\n+\n+        final StandardProvenanceEventRecord deserializedEvent = LookupTableEventRecord.getEvent(eventRecord, getFilename(), startOffset, getMaxAttributeLength(),\n+            firstEventId, systemTimeOffset, componentIds, componentTypes, queueIds, eventTypes);\n+        deserializedEvent.setEventId(eventId);\n+        return deserializedEvent;\n+    }\n+\n+    private boolean isData(final InputStream in) throws IOException {\n+        in.mark(1);\n+        final int nextByte = in.read();\n+        in.reset();\n+\n+        return nextByte > -1;\n+    }\n+\n+    @Override\n+    protected Optional<StandardProvenanceEventRecord> readToEvent(final long eventId, final DataInputStream dis, final int serializationVersion) throws IOException {\n+        verifySerializationVersion(serializationVersion);\n+\n+        while (isData(dis)) {\n+            final long startOffset = getBytesConsumed();\n+            final long id = dis.readInt() + firstEventId;\n+            final int recordLength = dis.readInt();\n+\n+            if (id >= eventId) {\n+                final StandardProvenanceEventRecord event = readRecord(dis, id, startOffset, recordLength);\n+                return Optional.ofNullable(event);\n+            } else {\n+                // This is not the record we want. Skip over it instead of deserializing it.\n+                StreamUtils.skip(dis, recordLength);\n+            }\n+        }\n+\n+        return Optional.empty();\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/EventIdFirstSchemaRecordReader.java",
                "sha": "612b6c8c3730d5136726b47d0124a175d2e0aa06",
                "status": "added"
            },
            {
                "additions": 241,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/EventIdFirstSchemaRecordWriter.java",
                "changes": 241,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/EventIdFirstSchemaRecordWriter.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/EventIdFirstSchemaRecordWriter.java",
                "patch": "@@ -0,0 +1,241 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance;\n+\n+import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n+import java.io.File;\n+import java.io.IOException;\n+import java.io.OutputStream;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import org.apache.nifi.provenance.schema.EventFieldNames;\n+import org.apache.nifi.provenance.schema.EventIdFirstHeaderSchema;\n+import org.apache.nifi.provenance.schema.LookupTableEventRecord;\n+import org.apache.nifi.provenance.schema.LookupTableEventSchema;\n+import org.apache.nifi.provenance.serialization.CompressableRecordWriter;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n+import org.apache.nifi.provenance.toc.TocWriter;\n+import org.apache.nifi.repository.schema.FieldMapRecord;\n+import org.apache.nifi.repository.schema.Record;\n+import org.apache.nifi.repository.schema.RecordSchema;\n+import org.apache.nifi.repository.schema.SchemaRecordWriter;\n+import org.apache.nifi.util.timebuffer.LongEntityAccess;\n+import org.apache.nifi.util.timebuffer.TimedBuffer;\n+import org.apache.nifi.util.timebuffer.TimestampedLong;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class EventIdFirstSchemaRecordWriter extends CompressableRecordWriter {\n+    private static final Logger logger = LoggerFactory.getLogger(EventIdFirstSchemaRecordWriter.class);\n+\n+    private static final RecordSchema eventSchema = LookupTableEventSchema.EVENT_SCHEMA;\n+    private static final RecordSchema contentClaimSchema = new RecordSchema(eventSchema.getField(EventFieldNames.CONTENT_CLAIM).getSubFields());\n+    private static final RecordSchema previousContentClaimSchema = new RecordSchema(eventSchema.getField(EventFieldNames.PREVIOUS_CONTENT_CLAIM).getSubFields());\n+    private static final RecordSchema headerSchema = EventIdFirstHeaderSchema.SCHEMA;\n+\n+    public static final int SERIALIZATION_VERSION = 1;\n+    public static final String SERIALIZATION_NAME = \"EventIdFirstSchemaRecordWriter\";\n+    private final IdentifierLookup idLookup;\n+\n+    private final SchemaRecordWriter schemaRecordWriter = new SchemaRecordWriter();\n+    private final AtomicInteger recordCount = new AtomicInteger(0);\n+\n+    private final Map<String, Integer> componentIdMap;\n+    private final Map<String, Integer> componentTypeMap;\n+    private final Map<String, Integer> queueIdMap;\n+    private static final Map<String, Integer> eventTypeMap;\n+    private static final List<String> eventTypeNames;\n+\n+    private static final TimedBuffer<TimestampedLong> serializeTimes = new TimedBuffer<>(TimeUnit.SECONDS, 60, new LongEntityAccess());\n+    private static final TimedBuffer<TimestampedLong> lockTimes = new TimedBuffer<>(TimeUnit.SECONDS, 60, new LongEntityAccess());\n+    private static final TimedBuffer<TimestampedLong> writeTimes = new TimedBuffer<>(TimeUnit.SECONDS, 60, new LongEntityAccess());\n+    private static final TimedBuffer<TimestampedLong> bytesWritten = new TimedBuffer<>(TimeUnit.SECONDS, 60, new LongEntityAccess());\n+    private static final AtomicLong totalRecordCount = new AtomicLong(0L);\n+\n+    private long firstEventId;\n+    private long systemTimeOffset;\n+\n+    static {\n+        eventTypeMap = new HashMap<>();\n+        eventTypeNames = new ArrayList<>();\n+\n+        int count = 0;\n+        for (final ProvenanceEventType eventType : ProvenanceEventType.values()) {\n+            eventTypeMap.put(eventType.name(), count++);\n+            eventTypeNames.add(eventType.name());\n+        }\n+    }\n+\n+    public EventIdFirstSchemaRecordWriter(final File file, final AtomicLong idGenerator, final TocWriter writer, final boolean compressed,\n+        final int uncompressedBlockSize, final IdentifierLookup idLookup) throws IOException {\n+        super(file, idGenerator, writer, compressed, uncompressedBlockSize);\n+\n+        this.idLookup = idLookup;\n+        componentIdMap = idLookup.invertComponentIdentifiers();\n+        componentTypeMap = idLookup.invertComponentTypes();\n+        queueIdMap = idLookup.invertQueueIdentifiers();\n+    }\n+\n+    public EventIdFirstSchemaRecordWriter(final OutputStream out, final String storageLocation, final AtomicLong idGenerator, final TocWriter tocWriter, final boolean compressed,\n+        final int uncompressedBlockSize, final IdentifierLookup idLookup) throws IOException {\n+        super(out, storageLocation, idGenerator, tocWriter, compressed, uncompressedBlockSize);\n+\n+        this.idLookup = idLookup;\n+        componentIdMap = idLookup.invertComponentIdentifiers();\n+        componentTypeMap = idLookup.invertComponentTypes();\n+        queueIdMap = idLookup.invertQueueIdentifiers();\n+    }\n+\n+    @Override\n+    public StorageSummary writeRecord(final ProvenanceEventRecord record) throws IOException {\n+        if (isDirty()) {\n+            throw new IOException(\"Cannot update Provenance Repository because this Record Writer has already failed to write to the Repository\");\n+        }\n+\n+        final long serializeStart = System.nanoTime();\n+        final byte[] serialized;\n+        try (final ByteArrayOutputStream baos = new ByteArrayOutputStream(256);\n+            final DataOutputStream dos = new DataOutputStream(baos)) {\n+            writeRecord(record, 0L, dos);\n+            serialized = baos.toByteArray();\n+        }\n+\n+        final long lockStart = System.nanoTime();\n+        final long writeStart;\n+        final long startBytes;\n+        final long endBytes;\n+        final long recordIdentifier;\n+        synchronized (this) {\n+            writeStart = System.nanoTime();\n+            try {\n+                recordIdentifier = record.getEventId() == -1L ? getIdGenerator().getAndIncrement() : record.getEventId();\n+                startBytes = getBytesWritten();\n+\n+                ensureStreamState(recordIdentifier, startBytes);\n+\n+                final DataOutputStream out = getBufferedOutputStream();\n+                final int recordIdOffset = (int) (recordIdentifier - firstEventId);\n+                out.writeInt(recordIdOffset);\n+                out.writeInt(serialized.length);\n+                out.write(serialized);\n+\n+                recordCount.incrementAndGet();\n+                endBytes = getBytesWritten();\n+            } catch (final IOException ioe) {\n+                markDirty();\n+                throw ioe;\n+            }\n+        }\n+\n+        if (logger.isDebugEnabled()) {\n+            // Collect stats and periodically dump them if log level is set to at least info.\n+            final long writeNanos = System.nanoTime() - writeStart;\n+            writeTimes.add(new TimestampedLong(writeNanos));\n+\n+            final long serializeNanos = lockStart - serializeStart;\n+            serializeTimes.add(new TimestampedLong(serializeNanos));\n+\n+            final long lockNanos = writeStart - lockStart;\n+            lockTimes.add(new TimestampedLong(lockNanos));\n+            bytesWritten.add(new TimestampedLong(endBytes - startBytes));\n+\n+            final long recordCount = totalRecordCount.incrementAndGet();\n+            if (recordCount % 1_000_000 == 0) {\n+                final long sixtySecondsAgo = System.currentTimeMillis() - 60000L;\n+                final Long writeNanosLast60 = writeTimes.getAggregateValue(sixtySecondsAgo).getValue();\n+                final Long lockNanosLast60 = lockTimes.getAggregateValue(sixtySecondsAgo).getValue();\n+                final Long serializeNanosLast60 = serializeTimes.getAggregateValue(sixtySecondsAgo).getValue();\n+                final Long bytesWrittenLast60 = bytesWritten.getAggregateValue(sixtySecondsAgo).getValue();\n+                logger.debug(\"In the last 60 seconds, have spent {} millis writing to file ({} MB), {} millis waiting on synchronize block, {} millis serializing events\",\n+                    TimeUnit.NANOSECONDS.toMillis(writeNanosLast60),\n+                    bytesWrittenLast60 / 1024 / 1024,\n+                    TimeUnit.NANOSECONDS.toMillis(lockNanosLast60),\n+                    TimeUnit.NANOSECONDS.toMillis(serializeNanosLast60));\n+            }\n+        }\n+\n+        final long serializedLength = endBytes - startBytes;\n+        final TocWriter tocWriter = getTocWriter();\n+        final Integer blockIndex = tocWriter == null ? null : tocWriter.getCurrentBlockIndex();\n+        final File file = getFile();\n+        final String storageLocation = file.getParentFile().getName() + \"/\" + file.getName();\n+        return new StorageSummary(recordIdentifier, storageLocation, blockIndex, serializedLength, endBytes);\n+    }\n+\n+    @Override\n+    public int getRecordsWritten() {\n+        return recordCount.get();\n+    }\n+\n+    protected Record createRecord(final ProvenanceEventRecord event, final long eventId) {\n+        return new LookupTableEventRecord(event, eventId, eventSchema, contentClaimSchema, previousContentClaimSchema, firstEventId, systemTimeOffset,\n+            componentIdMap, componentTypeMap, queueIdMap, eventTypeMap);\n+    }\n+\n+    @Override\n+    protected void writeRecord(final ProvenanceEventRecord event, final long eventId, final DataOutputStream out) throws IOException {\n+        final Record eventRecord = createRecord(event, eventId);\n+        schemaRecordWriter.writeRecord(eventRecord, out);\n+    }\n+\n+    @Override\n+    protected synchronized void writeHeader(final long firstEventId, final DataOutputStream out) throws IOException {\n+        final ByteArrayOutputStream baos = new ByteArrayOutputStream();\n+        eventSchema.writeTo(baos);\n+\n+        out.writeInt(baos.size());\n+        baos.writeTo(out);\n+\n+        baos.reset();\n+        headerSchema.writeTo(baos);\n+        out.writeInt(baos.size());\n+        baos.writeTo(out);\n+\n+        this.firstEventId = firstEventId;\n+        this.systemTimeOffset = System.currentTimeMillis();\n+\n+        final Map<String, Object> headerValues = new HashMap<>();\n+        headerValues.put(EventIdFirstHeaderSchema.FieldNames.FIRST_EVENT_ID, firstEventId);\n+        headerValues.put(EventIdFirstHeaderSchema.FieldNames.TIMESTAMP_OFFSET, systemTimeOffset);\n+        headerValues.put(EventIdFirstHeaderSchema.FieldNames.COMPONENT_IDS, idLookup.getComponentIdentifiers());\n+        headerValues.put(EventIdFirstHeaderSchema.FieldNames.COMPONENT_TYPES, idLookup.getComponentTypes());\n+        headerValues.put(EventIdFirstHeaderSchema.FieldNames.QUEUE_IDS, idLookup.getQueueIdentifiers());\n+        headerValues.put(EventIdFirstHeaderSchema.FieldNames.EVENT_TYPES, eventTypeNames);\n+        final FieldMapRecord headerInfo = new FieldMapRecord(headerSchema, headerValues);\n+\n+        schemaRecordWriter.writeRecord(headerInfo, out);\n+    }\n+\n+    @Override\n+    protected int getSerializationVersion() {\n+        return SERIALIZATION_VERSION;\n+    }\n+\n+    @Override\n+    protected String getSerializationName() {\n+        return SERIALIZATION_NAME;\n+    }\n+\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/EventIdFirstSchemaRecordWriter.java",
                "sha": "bb8d52fef3ac44c120baccbcf6cdc6aeea76c2e9",
                "status": "added"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/IndexConfiguration.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/IndexConfiguration.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 2,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/IndexConfiguration.java",
                "patch": "@@ -63,7 +63,7 @@ public IndexConfiguration(final RepositoryConfiguration repoConfig) {\n     private Map<File, List<File>> recoverIndexDirectories() {\n         final Map<File, List<File>> indexDirectoryMap = new HashMap<>();\n \n-        for (final File storageDirectory : repoConfig.getStorageDirectories()) {\n+        for (final File storageDirectory : repoConfig.getStorageDirectories().values()) {\n             final List<File> indexDirectories = new ArrayList<>();\n             final File[] matching = storageDirectory.listFiles(new FileFilter() {\n                 @Override\n@@ -85,6 +85,10 @@ public boolean accept(final File pathname) {\n     }\n \n     private Long getFirstEntryTime(final File provenanceLogFile) {\n+        if (provenanceLogFile == null) {\n+            return null;\n+        }\n+\n         try (final RecordReader reader = RecordReaders.newRecordReader(provenanceLogFile, null, Integer.MAX_VALUE)) {\n             final StandardProvenanceEventRecord firstRecord = reader.nextRecord();\n             if (firstRecord == null) {\n@@ -121,10 +125,14 @@ public void removeIndexDirectory(final File indexDirectory) {\n         }\n     }\n \n+\n     public File getWritableIndexDirectory(final File provenanceLogFile, final long newIndexTimestamp) {\n+        return getWritableIndexDirectoryForStorageDirectory(provenanceLogFile.getParentFile(), provenanceLogFile, newIndexTimestamp);\n+    }\n+\n+    public File getWritableIndexDirectoryForStorageDirectory(final File storageDirectory, final File provenanceLogFile, final long newIndexTimestamp) {\n         lock.lock();\n         try {\n-            final File storageDirectory = provenanceLogFile.getParentFile();\n             List<File> indexDirectories = this.indexDirectoryMap.get(storageDirectory);\n             if (indexDirectories == null) {\n                 final File newDir = addNewIndex(storageDirectory, provenanceLogFile, newIndexTimestamp);",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/IndexConfiguration.java",
                "sha": "a28d15018f000630b48aea6f4a3c7edff0864d22",
                "status": "modified"
            },
            {
                "additions": 92,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/PersistentProvenanceRepository.java",
                "changes": 260,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/PersistentProvenanceRepository.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 168,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/PersistentProvenanceRepository.java",
                "patch": "@@ -16,10 +16,54 @@\n  */\n package org.apache.nifi.provenance;\n \n+import java.io.EOFException;\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.io.FileNotFoundException;\n+import java.io.FilenameFilter;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.Date;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.LinkedHashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.concurrent.locks.Lock;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n import org.apache.lucene.document.Document;\n import org.apache.lucene.index.DirectoryReader;\n import org.apache.lucene.index.IndexNotFoundException;\n-import org.apache.lucene.index.IndexWriter;\n+import org.apache.lucene.index.IndexReader;\n import org.apache.lucene.search.IndexSearcher;\n import org.apache.lucene.search.ScoreDoc;\n import org.apache.lucene.search.TopDocs;\n@@ -32,14 +76,17 @@\n import org.apache.nifi.authorization.resource.Authorizable;\n import org.apache.nifi.authorization.user.NiFiUser;\n import org.apache.nifi.events.EventReporter;\n-import org.apache.nifi.processor.DataUnit;\n+import org.apache.nifi.provenance.authorization.EventAuthorizer;\n import org.apache.nifi.provenance.expiration.ExpirationAction;\n import org.apache.nifi.provenance.expiration.FileRemovalAction;\n+import org.apache.nifi.provenance.index.EventIndexWriter;\n import org.apache.nifi.provenance.lineage.ComputeLineageSubmission;\n import org.apache.nifi.provenance.lineage.FlowFileLineage;\n import org.apache.nifi.provenance.lineage.Lineage;\n import org.apache.nifi.provenance.lineage.LineageComputationType;\n import org.apache.nifi.provenance.lucene.DeleteIndexAction;\n+import org.apache.nifi.provenance.lucene.DocsReader;\n+import org.apache.nifi.provenance.lucene.DocumentToEventConverter;\n import org.apache.nifi.provenance.lucene.FieldNames;\n import org.apache.nifi.provenance.lucene.IndexManager;\n import org.apache.nifi.provenance.lucene.IndexSearch;\n@@ -56,8 +103,10 @@\n import org.apache.nifi.provenance.serialization.RecordReaders;\n import org.apache.nifi.provenance.serialization.RecordWriter;\n import org.apache.nifi.provenance.serialization.RecordWriters;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n import org.apache.nifi.provenance.toc.TocReader;\n import org.apache.nifi.provenance.toc.TocUtil;\n+import org.apache.nifi.provenance.util.NamedThreadFactory;\n import org.apache.nifi.reporting.Severity;\n import org.apache.nifi.util.FormatUtils;\n import org.apache.nifi.util.NiFiProperties;\n@@ -74,51 +123,6 @@\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import java.io.EOFException;\n-import java.io.File;\n-import java.io.FileFilter;\n-import java.io.FileNotFoundException;\n-import java.io.FilenameFilter;\n-import java.io.IOException;\n-import java.nio.file.Files;\n-import java.nio.file.Path;\n-import java.nio.file.Paths;\n-import java.util.ArrayList;\n-import java.util.Collection;\n-import java.util.Collections;\n-import java.util.Comparator;\n-import java.util.Date;\n-import java.util.HashMap;\n-import java.util.HashSet;\n-import java.util.Iterator;\n-import java.util.LinkedHashSet;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.Set;\n-import java.util.SortedMap;\n-import java.util.TreeMap;\n-import java.util.concurrent.BlockingQueue;\n-import java.util.concurrent.Callable;\n-import java.util.concurrent.ConcurrentHashMap;\n-import java.util.concurrent.ConcurrentMap;\n-import java.util.concurrent.ExecutionException;\n-import java.util.concurrent.ExecutorService;\n-import java.util.concurrent.Executors;\n-import java.util.concurrent.Future;\n-import java.util.concurrent.LinkedBlockingQueue;\n-import java.util.concurrent.ScheduledExecutorService;\n-import java.util.concurrent.ThreadFactory;\n-import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.atomic.AtomicBoolean;\n-import java.util.concurrent.atomic.AtomicInteger;\n-import java.util.concurrent.atomic.AtomicLong;\n-import java.util.concurrent.atomic.AtomicReference;\n-import java.util.concurrent.locks.Lock;\n-import java.util.concurrent.locks.ReadWriteLock;\n-import java.util.concurrent.locks.ReentrantReadWriteLock;\n-import java.util.regex.Pattern;\n-import java.util.stream.Collectors;\n-\n public class PersistentProvenanceRepository implements ProvenanceRepository {\n \n     public static final String EVENT_CATEGORY = \"Provenance Repository\";\n@@ -209,7 +213,7 @@ public PersistentProvenanceRepository() {\n     }\n \n     public PersistentProvenanceRepository(final NiFiProperties nifiProperties) throws IOException {\n-        this(createRepositoryConfiguration(nifiProperties), 10000);\n+        this(RepositoryConfiguration.create(nifiProperties), 10000);\n     }\n \n     public PersistentProvenanceRepository(final RepositoryConfiguration configuration, final int rolloverCheckMillis) throws IOException {\n@@ -220,7 +224,7 @@ public PersistentProvenanceRepository(final RepositoryConfiguration configuratio\n         this.configuration = configuration;\n         this.maxAttributeChars = configuration.getMaxAttributeChars();\n \n-        for (final File file : configuration.getStorageDirectories()) {\n+        for (final File file : configuration.getStorageDirectories().values()) {\n             final Path storageDirectory = file.toPath();\n             final Path journalDirectory = storageDirectory.resolve(\"journals\");\n \n@@ -234,7 +238,7 @@ public PersistentProvenanceRepository(final RepositoryConfiguration configuratio\n         this.maxPartitionMillis = configuration.getMaxEventFileLife(TimeUnit.MILLISECONDS);\n         this.maxPartitionBytes = configuration.getMaxEventFileCapacity();\n         this.indexConfig = new IndexConfiguration(configuration);\n-        this.indexManager = new SimpleIndexManager();\n+        this.indexManager = new SimpleIndexManager(configuration);\n         this.alwaysSync = configuration.isAlwaysSync();\n         this.rolloverCheckMillis = rolloverCheckMillis;\n \n@@ -254,7 +258,8 @@ protected IndexManager getIndexManager() {\n     }\n \n     @Override\n-    public void initialize(final EventReporter eventReporter, final Authorizer authorizer, final ProvenanceAuthorizableFactory resourceFactory) throws IOException {\n+    public void initialize(final EventReporter eventReporter, final Authorizer authorizer, final ProvenanceAuthorizableFactory resourceFactory,\n+        final IdentifierLookup idLookup) throws IOException {\n         writeLock.lock();\n         try {\n             if (initialized.getAndSet(true)) {\n@@ -325,90 +330,18 @@ public void run() {\n         }\n     }\n \n-    private static RepositoryConfiguration createRepositoryConfiguration(final NiFiProperties nifiProperties) throws IOException {\n-        final Map<String, Path> storageDirectories = nifiProperties.getProvenanceRepositoryPaths();\n-        if (storageDirectories.isEmpty()) {\n-            storageDirectories.put(\"provenance_repository\", Paths.get(\"provenance_repository\"));\n-        }\n-        final String storageTime = nifiProperties.getProperty(NiFiProperties.PROVENANCE_MAX_STORAGE_TIME, \"24 hours\");\n-        final String storageSize = nifiProperties.getProperty(NiFiProperties.PROVENANCE_MAX_STORAGE_SIZE, \"1 GB\");\n-        final String rolloverTime = nifiProperties.getProperty(NiFiProperties.PROVENANCE_ROLLOVER_TIME, \"5 mins\");\n-        final String rolloverSize = nifiProperties.getProperty(NiFiProperties.PROVENANCE_ROLLOVER_SIZE, \"100 MB\");\n-        final String shardSize = nifiProperties.getProperty(NiFiProperties.PROVENANCE_INDEX_SHARD_SIZE, \"500 MB\");\n-        final int queryThreads = nifiProperties.getIntegerProperty(NiFiProperties.PROVENANCE_QUERY_THREAD_POOL_SIZE, 2);\n-        final int indexThreads = nifiProperties.getIntegerProperty(NiFiProperties.PROVENANCE_INDEX_THREAD_POOL_SIZE, 1);\n-        final int journalCount = nifiProperties.getIntegerProperty(NiFiProperties.PROVENANCE_JOURNAL_COUNT, 16);\n-\n-        final long storageMillis = FormatUtils.getTimeDuration(storageTime, TimeUnit.MILLISECONDS);\n-        final long maxStorageBytes = DataUnit.parseDataSize(storageSize, DataUnit.B).longValue();\n-        final long rolloverMillis = FormatUtils.getTimeDuration(rolloverTime, TimeUnit.MILLISECONDS);\n-        final long rolloverBytes = DataUnit.parseDataSize(rolloverSize, DataUnit.B).longValue();\n-\n-        final boolean compressOnRollover = Boolean.parseBoolean(nifiProperties.getProperty(NiFiProperties.PROVENANCE_COMPRESS_ON_ROLLOVER));\n-        final String indexedFieldString = nifiProperties.getProperty(NiFiProperties.PROVENANCE_INDEXED_FIELDS);\n-        final String indexedAttrString = nifiProperties.getProperty(NiFiProperties.PROVENANCE_INDEXED_ATTRIBUTES);\n-\n-        final Boolean alwaysSync = Boolean.parseBoolean(nifiProperties.getProperty(\"nifi.provenance.repository.always.sync\", \"false\"));\n-\n-        final int defaultMaxAttrChars = 65536;\n-        final String maxAttrLength = nifiProperties.getProperty(\"nifi.provenance.repository.max.attribute.length\", String.valueOf(defaultMaxAttrChars));\n-        int maxAttrChars;\n-        try {\n-            maxAttrChars = Integer.parseInt(maxAttrLength);\n-            // must be at least 36 characters because that's the length of the uuid attribute,\n-            // which must be kept intact\n-            if (maxAttrChars < 36) {\n-                maxAttrChars = 36;\n-                logger.warn(\"Found max attribute length property set to \" + maxAttrLength + \" but minimum length is 36; using 36 instead\");\n-            }\n-        } catch (final Exception e) {\n-            maxAttrChars = defaultMaxAttrChars;\n-        }\n-\n-        final List<SearchableField> searchableFields = SearchableFieldParser.extractSearchableFields(indexedFieldString, true);\n-        final List<SearchableField> searchableAttributes = SearchableFieldParser.extractSearchableFields(indexedAttrString, false);\n-\n-        // We always want to index the Event Time.\n-        if (!searchableFields.contains(SearchableFields.EventTime)) {\n-            searchableFields.add(SearchableFields.EventTime);\n-        }\n-\n-        final RepositoryConfiguration config = new RepositoryConfiguration();\n-        for (final Path path : storageDirectories.values()) {\n-            config.addStorageDirectory(path.toFile());\n-        }\n-        config.setCompressOnRollover(compressOnRollover);\n-        config.setSearchableFields(searchableFields);\n-        config.setSearchableAttributes(searchableAttributes);\n-        config.setMaxEventFileCapacity(rolloverBytes);\n-        config.setMaxEventFileLife(rolloverMillis, TimeUnit.MILLISECONDS);\n-        config.setMaxRecordLife(storageMillis, TimeUnit.MILLISECONDS);\n-        config.setMaxStorageCapacity(maxStorageBytes);\n-        config.setQueryThreadPoolSize(queryThreads);\n-        config.setIndexThreadPoolSize(indexThreads);\n-        config.setJournalCount(journalCount);\n-        config.setMaxAttributeChars(maxAttrChars);\n-\n-        if (shardSize != null) {\n-            config.setDesiredIndexSize(DataUnit.parseDataSize(shardSize, DataUnit.B).longValue());\n-        }\n-\n-        config.setAlwaysSync(alwaysSync);\n-\n-        return config;\n-    }\n \n     // protected in order to override for unit tests\n     protected RecordWriter[] createWriters(final RepositoryConfiguration config, final long initialRecordId) throws IOException {\n-        final List<File> storageDirectories = config.getStorageDirectories();\n+        final List<File> storageDirectories = new ArrayList<>(config.getStorageDirectories().values());\n \n         final RecordWriter[] writers = new RecordWriter[config.getJournalCount()];\n         for (int i = 0; i < config.getJournalCount(); i++) {\n             final File storageDirectory = storageDirectories.get(i % storageDirectories.size());\n             final File journalDirectory = new File(storageDirectory, \"journals\");\n             final File journalFile = new File(journalDirectory, String.valueOf(initialRecordId) + \".journal.\" + i);\n \n-            writers[i] = RecordWriters.newSchemaRecordWriter(journalFile, false, false);\n+            writers[i] = RecordWriters.newSchemaRecordWriter(journalFile, idGenerator, false, false);\n             writers[i].writeHeader(initialRecordId);\n         }\n \n@@ -460,7 +393,7 @@ public boolean isAuthorized(final ProvenanceEventRecord event, final NiFiUser us\n         return Result.Approved.equals(result.getResult());\n     }\n \n-    protected void authorize(final ProvenanceEventRecord event, final NiFiUser user) {\n+    public void authorize(final ProvenanceEventRecord event, final NiFiUser user) {\n         if (authorizer == null) {\n             return;\n         }\n@@ -474,11 +407,11 @@ protected void authorize(final ProvenanceEventRecord event, final NiFiUser user)\n         eventAuthorizable.authorize(authorizer, RequestAction.READ, user, event.getAttributes());\n     }\n \n-    private List<ProvenanceEventRecord> filterUnauthorizedEvents(final List<ProvenanceEventRecord> events, final NiFiUser user) {\n+    public List<ProvenanceEventRecord> filterUnauthorizedEvents(final List<ProvenanceEventRecord> events, final NiFiUser user) {\n         return events.stream().filter(event -> isAuthorized(event, user)).collect(Collectors.<ProvenanceEventRecord>toList());\n     }\n \n-    private Set<ProvenanceEventRecord> replaceUnauthorizedWithPlaceholders(final Set<ProvenanceEventRecord> events, final NiFiUser user) {\n+    public Set<ProvenanceEventRecord> replaceUnauthorizedWithPlaceholders(final Set<ProvenanceEventRecord> events, final NiFiUser user) {\n         return events.stream().map(event -> isAuthorized(event, user) ? event : new PlaceholderProvenanceEvent(event)).collect(Collectors.toSet());\n     }\n \n@@ -594,7 +527,7 @@ private void recover() throws IOException {\n         long minIndexedId = Long.MAX_VALUE;\n \n         final List<File> filesToRecover = new ArrayList<>();\n-        for (final File file : configuration.getStorageDirectories()) {\n+        for (final File file : configuration.getStorageDirectories().values()) {\n             final File[] matchingFiles = file.listFiles(new FileFilter() {\n                 @Override\n                 public boolean accept(final File pathname) {\n@@ -780,10 +713,10 @@ private void persistRecord(final Iterable<ProvenanceEventRecord> records) {\n                 try {\n                     long recordsWritten = 0L;\n                     for (final ProvenanceEventRecord nextRecord : records) {\n-                        final long eventId = idGenerator.getAndIncrement();\n-                        bytesWritten += writer.writeRecord(nextRecord, eventId);\n+                        final StorageSummary persistedEvent = writer.writeRecord(nextRecord);\n+                        bytesWritten += persistedEvent.getSerializedLength();\n                         recordsWritten++;\n-                        logger.trace(\"Wrote record with ID {} to {}\", eventId, writer);\n+                        logger.trace(\"Wrote record with ID {} to {}\", persistedEvent.getEventId(), writer);\n                     }\n \n                     writer.flush();\n@@ -1175,7 +1108,7 @@ private void deleteDirectory(final File dir) {\n      */\n     private List<File> getAllIndexDirectories() {\n         final List<File> allIndexDirs = new ArrayList<>();\n-        for (final File storageDir : configuration.getStorageDirectories()) {\n+        for (final File storageDir : configuration.getStorageDirectories().values()) {\n             final File[] indexDirs = storageDir.listFiles(new FilenameFilter() {\n                 @Override\n                 public boolean accept(final File dir, final String name) {\n@@ -1237,7 +1170,7 @@ public void waitForRollover() {\n     protected int getJournalCount() {\n         // determine how many 'journals' we have in the journals directories\n         int journalFileCount = 0;\n-        for (final File storageDir : configuration.getStorageDirectories()) {\n+        for (final File storageDir : configuration.getStorageDirectories().values()) {\n             final File journalsDir = new File(storageDir, \"journals\");\n             final File[] journalFiles = journalsDir.listFiles();\n             if (journalFiles != null) {\n@@ -1313,7 +1246,7 @@ private void rollover(final boolean force) throws IOException {\n \n             // Choose a storage directory to store the merged file in.\n             final long storageDirIdx = storageDirectoryIndex.getAndIncrement();\n-            final List<File> storageDirs = configuration.getStorageDirectories();\n+            final List<File> storageDirs = new ArrayList<>(configuration.getStorageDirectories().values());\n             final File storageDir = storageDirs.get((int) (storageDirIdx % storageDirs.size()));\n \n             Future<?> future = null;\n@@ -1479,8 +1412,8 @@ public void run() {\n         final Map<String, List<File>> journalMap = new HashMap<>();\n \n         // Map journals' basenames to the files with that basename.\n-        final List<File> storageDirs = configuration.getStorageDirectories();\n-        for (final File storageDir : configuration.getStorageDirectories()) {\n+        final List<File> storageDirs = new ArrayList<>(configuration.getStorageDirectories().values());\n+        for (final File storageDir : storageDirs) {\n             final File journalDir = new File(storageDir, \"journals\");\n             if (!journalDir.exists()) {\n                 continue;\n@@ -1729,7 +1662,7 @@ public int compare(final StandardProvenanceEventRecord o1, final StandardProvena\n \n             // loop over each entry in the map, persisting the records to the merged file in order, and populating the map\n             // with the next entry from the journal file from which the previous record was written.\n-            try (final RecordWriter writer = RecordWriters.newSchemaRecordWriter(writerFile, configuration.isCompressOnRollover(), true)) {\n+            try (final RecordWriter writer = RecordWriters.newSchemaRecordWriter(writerFile, idGenerator, configuration.isCompressOnRollover(), true)) {\n                 writer.writeHeader(minEventId);\n \n                 final IndexingAction indexingAction = createIndexingAction();\n@@ -1741,7 +1674,7 @@ public int compare(final StandardProvenanceEventRecord o1, final StandardProvena\n                 final AtomicBoolean finishedAdding = new AtomicBoolean(false);\n                 final List<Future<?>> futures = new ArrayList<>();\n \n-                final IndexWriter indexWriter = getIndexManager().borrowIndexWriter(indexingDirectory);\n+                final EventIndexWriter indexWriter = getIndexManager().borrowIndexWriter(indexingDirectory);\n                 try {\n                     final ExecutorService exec = Executors.newFixedThreadPool(configuration.getIndexThreadPoolSize(), new ThreadFactory() {\n                         @Override\n@@ -1772,7 +1705,7 @@ public Object call() throws IOException {\n                                                 continue;\n                                             }\n \n-                                            indexingAction.index(tuple.getKey(), indexWriter, tuple.getValue());\n+                                            indexingAction.index(tuple.getKey(), indexWriter.getIndexWriter(), tuple.getValue());\n                                         } catch (final Throwable t) {\n                                             logger.error(\"Failed to index Provenance Event for \" + writerFile + \" to \" + indexingDirectory, t);\n                                             if (indexingFailureCount.incrementAndGet() >= MAX_INDEXING_FAILURE_COUNT) {\n@@ -1795,7 +1728,7 @@ public Object call() throws IOException {\n                             final StandardProvenanceEventRecord record = entry.getKey();\n                             final RecordReader reader = entry.getValue();\n \n-                            writer.writeRecord(record, record.getEventId());\n+                            writer.writeRecord(record);\n                             final int blockIndex = writer.getTocWriter().getCurrentBlockIndex();\n \n                             boolean accepted = false;\n@@ -1879,7 +1812,7 @@ public Object call() throws IOException {\n                         }\n                     }\n                 } finally {\n-                    getIndexManager().returnIndexWriter(indexingDirectory, indexWriter);\n+                    getIndexManager().returnIndexWriter(indexWriter);\n                 }\n \n                 indexConfig.setMaxIdIndexed(maxId);\n@@ -1945,7 +1878,7 @@ public boolean evaluate(final ProvenanceEventRecord event) {\n      * events indexed, etc.\n      */\n     protected IndexingAction createIndexingAction() {\n-        return new IndexingAction(this);\n+        return new IndexingAction(configuration.getSearchableFields(), configuration.getSearchableAttributes());\n     }\n \n     private StandardProvenanceEventRecord truncateAttributes(final StandardProvenanceEventRecord original) {\n@@ -2322,7 +2255,7 @@ public AsyncLineageSubmission submitExpandChildren(final long eventId, final NiF\n             if (event == null) {\n                 final AsyncLineageSubmission submission = new AsyncLineageSubmission(LineageComputationType.EXPAND_CHILDREN, eventId, Collections.<String>emptyList(), 1, userId);\n                 lineageSubmissionMap.put(submission.getLineageIdentifier(), submission);\n-                submission.getResult().update(Collections.<ProvenanceEventRecord>emptyList());\n+                submission.getResult().update(Collections.emptyList(), 0L);\n                 return submission;\n             }\n \n@@ -2359,9 +2292,9 @@ public AsyncLineageSubmission submitExpandParents(final long eventId, final NiFi\n         try {\n             final ProvenanceEventRecord event = getEvent(eventId);\n             if (event == null) {\n-                final AsyncLineageSubmission submission = new AsyncLineageSubmission(LineageComputationType.EXPAND_CHILDREN, eventId, Collections.<String>emptyList(), 1, userId);\n+                final AsyncLineageSubmission submission = new AsyncLineageSubmission(LineageComputationType.EXPAND_PARENTS, eventId, Collections.emptyList(), 1, userId);\n                 lineageSubmissionMap.put(submission.getLineageIdentifier(), submission);\n-                submission.getResult().update(Collections.<ProvenanceEventRecord>emptyList());\n+                submission.getResult().update(Collections.emptyList(), 0L);\n                 return submission;\n             }\n \n@@ -2642,11 +2575,21 @@ public void run() {\n             }\n \n             try {\n-                final Set<ProvenanceEventRecord> matchingRecords = LineageQuery.computeLineageForFlowFiles(PersistentProvenanceRepository.this,\n-                        getIndexManager(), indexDir, null, flowFileUuids, maxAttributeChars);\n+                final DocumentToEventConverter converter = new DocumentToEventConverter() {\n+                    @Override\n+                    public Set<ProvenanceEventRecord> convert(TopDocs topDocs, IndexReader indexReader) throws IOException {\n+                        // Always authorized. We do this because we need to pull back the event, regardless of whether or not\n+                        // the user is truly authorized, because instead of ignoring unauthorized events, we want to replace them.\n+                        final EventAuthorizer authorizer = EventAuthorizer.GRANT_ALL;\n+                        final DocsReader docsReader = new DocsReader();\n+                        return docsReader.read(topDocs, authorizer, indexReader, getAllLogFiles(), new AtomicInteger(0), Integer.MAX_VALUE, maxAttributeChars);\n+                    }\n+                };\n+\n+                final Set<ProvenanceEventRecord> matchingRecords = LineageQuery.computeLineageForFlowFiles(getIndexManager(), indexDir, null, flowFileUuids, converter);\n \n                 final StandardLineageResult result = submission.getResult();\n-                result.update(replaceUnauthorizedWithPlaceholders(matchingRecords, user));\n+                result.update(replaceUnauthorizedWithPlaceholders(matchingRecords, user), matchingRecords.size());\n \n                 logger.info(\"Successfully created Lineage for FlowFiles with UUIDs {} in {} milliseconds; Lineage contains {} nodes and {} edges\",\n                         flowFileUuids, result.getComputationTime(TimeUnit.MILLISECONDS), result.getNodes().size(), result.getEdges().size());\n@@ -2666,7 +2609,6 @@ public void run() {\n     }\n \n     private class RemoveExpiredQueryResults implements Runnable {\n-\n         @Override\n         public void run() {\n             try {\n@@ -2697,22 +2639,4 @@ public void run() {\n             }\n         }\n     }\n-\n-    private static class NamedThreadFactory implements ThreadFactory {\n-\n-        private final AtomicInteger counter = new AtomicInteger(0);\n-        private final ThreadFactory defaultThreadFactory = Executors.defaultThreadFactory();\n-        private final String namePrefix;\n-\n-        public NamedThreadFactory(final String namePrefix) {\n-            this.namePrefix = namePrefix;\n-        }\n-\n-        @Override\n-        public Thread newThread(final Runnable r) {\n-            final Thread thread = defaultThreadFactory.newThread(r);\n-            thread.setName(namePrefix + \"-\" + counter.incrementAndGet());\n-            return thread;\n-        }\n-    }\n }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/PersistentProvenanceRepository.java",
                "sha": "ed183f9c5bda3ed8c5237bd1b8daa3a379117ca8",
                "status": "modified"
            },
            {
                "additions": 134,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/RepositoryConfiguration.java",
                "changes": 140,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/RepositoryConfiguration.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 6,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/RepositoryConfiguration.java",
                "patch": "@@ -17,20 +17,35 @@\n package org.apache.nifi.provenance;\n \n import java.io.File;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n import java.util.ArrayList;\n import java.util.Collections;\n+import java.util.LinkedHashMap;\n import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n import java.util.concurrent.TimeUnit;\n \n+import org.apache.nifi.processor.DataUnit;\n import org.apache.nifi.provenance.search.SearchableField;\n+import org.apache.nifi.util.FormatUtils;\n+import org.apache.nifi.util.NiFiProperties;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n public class RepositoryConfiguration {\n+    private static final Logger logger = LoggerFactory.getLogger(RepositoryConfiguration.class);\n \n-    private final List<File> storageDirectories = new ArrayList<>();\n+    public static final String CONCURRENT_MERGE_THREADS = \"nifi.provenance.repository.concurrent.merge.threads\";\n+    public static final String WARM_CACHE_FREQUENCY = \"nifi.provenance.repository.warm.cache.frequency\";\n+\n+    private final Map<String, File> storageDirectories = new LinkedHashMap<>();\n     private long recordLifeMillis = TimeUnit.MILLISECONDS.convert(24, TimeUnit.HOURS);\n     private long storageCapacity = 1024L * 1024L * 1024L;   // 1 GB\n     private long eventFileMillis = TimeUnit.MILLISECONDS.convert(5, TimeUnit.MINUTES);\n     private long eventFileBytes = 1024L * 1024L * 5L;   // 5 MB\n+    private int maxFileEvents = Integer.MAX_VALUE;\n     private long desiredIndexBytes = 1024L * 1024L * 500L; // 500 MB\n     private int journalCount = 16;\n     private int compressionBlockBytes = 1024 * 1024;\n@@ -43,6 +58,8 @@\n     private int queryThreadPoolSize = 2;\n     private int indexThreadPoolSize = 1;\n     private boolean allowRollover = true;\n+    private int concurrentMergeThreads = 4;\n+    private Integer warmCacheFrequencyMinutes = null;\n \n     public void setAllowRollover(final boolean allow) {\n         this.allowRollover = allow;\n@@ -52,7 +69,6 @@ public boolean isAllowRollover() {\n         return allowRollover;\n     }\n \n-\n     public int getCompressionBlockBytes() {\n         return compressionBlockBytes;\n     }\n@@ -66,17 +82,21 @@ public void setCompressionBlockBytes(int compressionBlockBytes) {\n      *\n      * @return the directories where provenance files will be stored\n      */\n-    public List<File> getStorageDirectories() {\n-        return Collections.unmodifiableList(storageDirectories);\n+    public Map<String, File> getStorageDirectories() {\n+        return Collections.unmodifiableMap(storageDirectories);\n     }\n \n     /**\n      * Specifies where the repository should store data\n      *\n      * @param storageDirectory the directory to store provenance files\n      */\n-    public void addStorageDirectory(final File storageDirectory) {\n-        this.storageDirectories.add(storageDirectory);\n+    public void addStorageDirectory(final String partitionName, final File storageDirectory) {\n+        this.storageDirectories.put(partitionName, storageDirectory);\n+    }\n+\n+    public void addStorageDirectories(final Map<String, File> storageDirectories) {\n+        this.storageDirectories.putAll(storageDirectories);\n     }\n \n     /**\n@@ -147,6 +167,20 @@ public void setMaxEventFileCapacity(final long maxEventFileBytes) {\n         this.eventFileBytes = maxEventFileBytes;\n     }\n \n+    /**\n+     * @return the maximum number of events that should be written to a single event file before the file is rolled over\n+     */\n+    public int getMaxEventFileCount() {\n+        return maxFileEvents;\n+    }\n+\n+    /**\n+     * @param maxCount the maximum number of events that should be written to a single event file before the file is rolled over\n+     */\n+    public void setMaxEventFileCount(final int maxCount) {\n+        this.maxFileEvents = maxCount;\n+    }\n+\n     /**\n      * @return the fields that should be indexed\n      */\n@@ -218,6 +252,14 @@ public void setIndexThreadPoolSize(final int indexThreadPoolSize) {\n         this.indexThreadPoolSize = indexThreadPoolSize;\n     }\n \n+    public void setConcurrentMergeThreads(final int mergeThreads) {\n+        this.concurrentMergeThreads = mergeThreads;\n+    }\n+\n+    public int getConcurrentMergeThreads() {\n+        return concurrentMergeThreads;\n+    }\n+\n     /**\n      * <p>\n      * Specifies the desired size of each Provenance Event index shard, in\n@@ -310,4 +352,90 @@ public void setMaxAttributeChars(int maxAttributeChars) {\n         this.maxAttributeChars = maxAttributeChars;\n     }\n \n+    public void setWarmCacheFrequencyMinutes(Integer frequencyMinutes) {\n+        this.warmCacheFrequencyMinutes = frequencyMinutes;\n+    }\n+\n+    public Optional<Integer> getWarmCacheFrequencyMinutes() {\n+        return Optional.ofNullable(warmCacheFrequencyMinutes);\n+    }\n+\n+    public static RepositoryConfiguration create(final NiFiProperties nifiProperties) {\n+        final Map<String, Path> storageDirectories = nifiProperties.getProvenanceRepositoryPaths();\n+        if (storageDirectories.isEmpty()) {\n+            storageDirectories.put(\"provenance_repository\", Paths.get(\"provenance_repository\"));\n+        }\n+        final String storageTime = nifiProperties.getProperty(NiFiProperties.PROVENANCE_MAX_STORAGE_TIME, \"24 hours\");\n+        final String storageSize = nifiProperties.getProperty(NiFiProperties.PROVENANCE_MAX_STORAGE_SIZE, \"1 GB\");\n+        final String rolloverTime = nifiProperties.getProperty(NiFiProperties.PROVENANCE_ROLLOVER_TIME, \"5 mins\");\n+        final String rolloverSize = nifiProperties.getProperty(NiFiProperties.PROVENANCE_ROLLOVER_SIZE, \"100 MB\");\n+        final String shardSize = nifiProperties.getProperty(NiFiProperties.PROVENANCE_INDEX_SHARD_SIZE, \"500 MB\");\n+        final int queryThreads = nifiProperties.getIntegerProperty(NiFiProperties.PROVENANCE_QUERY_THREAD_POOL_SIZE, 2);\n+        final int indexThreads = nifiProperties.getIntegerProperty(NiFiProperties.PROVENANCE_INDEX_THREAD_POOL_SIZE, 2);\n+        final int journalCount = nifiProperties.getIntegerProperty(NiFiProperties.PROVENANCE_JOURNAL_COUNT, 16);\n+        final int concurrentMergeThreads = nifiProperties.getIntegerProperty(CONCURRENT_MERGE_THREADS, 2);\n+        final String warmCacheFrequency = nifiProperties.getProperty(WARM_CACHE_FREQUENCY);\n+\n+        final long storageMillis = FormatUtils.getTimeDuration(storageTime, TimeUnit.MILLISECONDS);\n+        final long maxStorageBytes = DataUnit.parseDataSize(storageSize, DataUnit.B).longValue();\n+        final long rolloverMillis = FormatUtils.getTimeDuration(rolloverTime, TimeUnit.MILLISECONDS);\n+        final long rolloverBytes = DataUnit.parseDataSize(rolloverSize, DataUnit.B).longValue();\n+\n+        final boolean compressOnRollover = Boolean.parseBoolean(nifiProperties.getProperty(NiFiProperties.PROVENANCE_COMPRESS_ON_ROLLOVER));\n+        final String indexedFieldString = nifiProperties.getProperty(NiFiProperties.PROVENANCE_INDEXED_FIELDS);\n+        final String indexedAttrString = nifiProperties.getProperty(NiFiProperties.PROVENANCE_INDEXED_ATTRIBUTES);\n+\n+        final Boolean alwaysSync = Boolean.parseBoolean(nifiProperties.getProperty(\"nifi.provenance.repository.always.sync\", \"false\"));\n+\n+        final int defaultMaxAttrChars = 65536;\n+        final String maxAttrLength = nifiProperties.getProperty(\"nifi.provenance.repository.max.attribute.length\", String.valueOf(defaultMaxAttrChars));\n+        int maxAttrChars;\n+        try {\n+            maxAttrChars = Integer.parseInt(maxAttrLength);\n+            // must be at least 36 characters because that's the length of the uuid attribute,\n+            // which must be kept intact\n+            if (maxAttrChars < 36) {\n+                maxAttrChars = 36;\n+                logger.warn(\"Found max attribute length property set to \" + maxAttrLength + \" but minimum length is 36; using 36 instead\");\n+            }\n+        } catch (final Exception e) {\n+            maxAttrChars = defaultMaxAttrChars;\n+        }\n+\n+        final List<SearchableField> searchableFields = SearchableFieldParser.extractSearchableFields(indexedFieldString, true);\n+        final List<SearchableField> searchableAttributes = SearchableFieldParser.extractSearchableFields(indexedAttrString, false);\n+\n+        // We always want to index the Event Time.\n+        if (!searchableFields.contains(SearchableFields.EventTime)) {\n+            searchableFields.add(SearchableFields.EventTime);\n+        }\n+\n+        final RepositoryConfiguration config = new RepositoryConfiguration();\n+        for (final Map.Entry<String, Path> entry : storageDirectories.entrySet()) {\n+            config.addStorageDirectory(entry.getKey(), entry.getValue().toFile());\n+        }\n+        config.setCompressOnRollover(compressOnRollover);\n+        config.setSearchableFields(searchableFields);\n+        config.setSearchableAttributes(searchableAttributes);\n+        config.setMaxEventFileCapacity(rolloverBytes);\n+        config.setMaxEventFileLife(rolloverMillis, TimeUnit.MILLISECONDS);\n+        config.setMaxRecordLife(storageMillis, TimeUnit.MILLISECONDS);\n+        config.setMaxStorageCapacity(maxStorageBytes);\n+        config.setQueryThreadPoolSize(queryThreads);\n+        config.setIndexThreadPoolSize(indexThreads);\n+        config.setJournalCount(journalCount);\n+        config.setMaxAttributeChars(maxAttrChars);\n+        config.setConcurrentMergeThreads(concurrentMergeThreads);\n+\n+        if (warmCacheFrequency != null && !warmCacheFrequency.trim().equals(\"\")) {\n+            config.setWarmCacheFrequencyMinutes((int) FormatUtils.getTimeDuration(warmCacheFrequency, TimeUnit.MINUTES));\n+        }\n+        if (shardSize != null) {\n+            config.setDesiredIndexSize(DataUnit.parseDataSize(shardSize, DataUnit.B).longValue());\n+        }\n+\n+        config.setAlwaysSync(alwaysSync);\n+\n+        return config;\n+    }\n }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/RepositoryConfiguration.java",
                "sha": "7a2f57e37a94847a3e87cf67c6ee2e18c78dec7a",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/StandardRecordReader.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/StandardRecordReader.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 2,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/StandardRecordReader.java",
                "patch": "@@ -32,6 +32,9 @@\n import org.slf4j.LoggerFactory;\n \n public class StandardRecordReader extends CompressableRecordReader {\n+    public static final int SERIALIZATION_VERISON = 9;\n+    public static final String SERIALIZATION_NAME = \"org.apache.nifi.provenance.PersistentProvenanceRepository\";\n+\n     private static final Logger logger = LoggerFactory.getLogger(StandardRecordReader.class);\n     private static final Pattern UUID_PATTERN = Pattern.compile(\"[a-fA-F0-9]{8}\\\\-([a-fA-F0-9]{4}\\\\-){3}[a-fA-F0-9]{12}\");\n \n@@ -121,9 +124,9 @@ private StandardProvenanceEventRecord readPreVersion6Record(final DataInputStrea\n \n     @Override\n     public StandardProvenanceEventRecord nextRecord(final DataInputStream dis, final int serializationVersion) throws IOException {\n-        if (serializationVersion > StandardRecordWriter.SERIALIZATION_VERISON) {\n+        if (serializationVersion > SERIALIZATION_VERISON) {\n             throw new IllegalArgumentException(\"Unable to deserialize record because the version is \"\n-                + serializationVersion + \" and supported versions are 1-\" + StandardRecordWriter.SERIALIZATION_VERISON);\n+                + serializationVersion + \" and supported versions are 1-\" + SERIALIZATION_VERISON);\n         }\n \n         // Schema changed drastically in version 6 so we created a new method to handle old records",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/StandardRecordReader.java",
                "sha": "ce875d61964404064be2fd6d7e20ed5b3a688d9e",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/StandardRecordWriter.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/StandardRecordWriter.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 5,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/StandardRecordWriter.java",
                "patch": "@@ -16,17 +16,18 @@\n  */\n package org.apache.nifi.provenance;\n \n+import java.io.DataOutputStream;\n import java.io.File;\n import java.io.IOException;\n import java.io.OutputStream;\n import java.io.UTFDataFormatException;\n import java.util.Collection;\n import java.util.Map;\n+import java.util.concurrent.atomic.AtomicLong;\n \n import org.apache.nifi.provenance.serialization.CompressableRecordWriter;\n import org.apache.nifi.provenance.serialization.RecordWriter;\n import org.apache.nifi.provenance.toc.TocWriter;\n-import org.apache.nifi.stream.io.DataOutputStream;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -45,15 +46,16 @@\n     private final File file;\n \n \n-    public StandardRecordWriter(final File file, final TocWriter writer, final boolean compressed, final int uncompressedBlockSize) throws IOException {\n-        super(file, writer, compressed, uncompressedBlockSize);\n+    public StandardRecordWriter(final File file, final AtomicLong idGenerator, final TocWriter writer, final boolean compressed, final int uncompressedBlockSize) throws IOException {\n+        super(file, idGenerator, writer, compressed, uncompressedBlockSize);\n         logger.trace(\"Creating Record Writer for {}\", file.getName());\n \n         this.file = file;\n     }\n \n-    public StandardRecordWriter(final OutputStream out, final TocWriter tocWriter, final boolean compressed, final int uncompressedBlockSize) throws IOException {\n-        super(out, tocWriter, compressed, uncompressedBlockSize);\n+    public StandardRecordWriter(final OutputStream out, final String storageLocation, final AtomicLong idGenerator, final TocWriter tocWriter,\n+        final boolean compressed, final int uncompressedBlockSize) throws IOException {\n+        super(out, storageLocation, idGenerator, tocWriter, compressed, uncompressedBlockSize);\n         this.file = null;\n     }\n ",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/StandardRecordWriter.java",
                "sha": "0a749adb2edd32053255a1e0efebcea98f2485f6",
                "status": "modified"
            },
            {
                "additions": 280,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/WriteAheadProvenanceRepository.java",
                "changes": 280,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/WriteAheadProvenanceRepository.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/WriteAheadProvenanceRepository.java",
                "patch": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance;\n+\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+\n+import org.apache.nifi.authorization.Authorizer;\n+import org.apache.nifi.authorization.RequestAction;\n+import org.apache.nifi.authorization.resource.Authorizable;\n+import org.apache.nifi.authorization.user.NiFiUser;\n+import org.apache.nifi.events.EventReporter;\n+import org.apache.nifi.provenance.authorization.EventAuthorizer;\n+import org.apache.nifi.provenance.authorization.UserEventAuthorizer;\n+import org.apache.nifi.provenance.index.EventIndex;\n+import org.apache.nifi.provenance.index.lucene.LuceneEventIndex;\n+import org.apache.nifi.provenance.lineage.ComputeLineageSubmission;\n+import org.apache.nifi.provenance.lucene.IndexManager;\n+import org.apache.nifi.provenance.lucene.SimpleIndexManager;\n+import org.apache.nifi.provenance.search.Query;\n+import org.apache.nifi.provenance.search.QuerySubmission;\n+import org.apache.nifi.provenance.search.SearchableField;\n+import org.apache.nifi.provenance.serialization.RecordReaders;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n+import org.apache.nifi.provenance.store.EventFileManager;\n+import org.apache.nifi.provenance.store.EventStore;\n+import org.apache.nifi.provenance.store.PartitionedWriteAheadEventStore;\n+import org.apache.nifi.provenance.store.RecordReaderFactory;\n+import org.apache.nifi.provenance.store.RecordWriterFactory;\n+import org.apache.nifi.provenance.store.StorageResult;\n+import org.apache.nifi.provenance.toc.StandardTocWriter;\n+import org.apache.nifi.provenance.toc.TocUtil;\n+import org.apache.nifi.provenance.toc.TocWriter;\n+import org.apache.nifi.provenance.util.CloseableUtil;\n+import org.apache.nifi.reporting.Severity;\n+import org.apache.nifi.util.NiFiProperties;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+/**\n+ * <p>\n+ * A Provenance Repository that is made up of two distinct concepts: An {@link EventStore Event Store} that is responsible\n+ * for storing and accessing the events (this repository makes use of an Event Store that uses a backing Write-Ahead Log, hence the name\n+ * WriteAheadProvenanceRepository) and an {@link EventIndex Event Index} that is responsible for indexing and searching those\n+ * events.\n+ * </p>\n+ *\n+ * <p>\n+ * When a Provenance Event is added to the repository, it is first stored in the Event Store. The Event Store reports the location (namely, the\n+ * Event Identifier) that it used to store the event. The stored event is then given to the Event Index along with its storage location. The index\n+ * is then responsible for indexing the event in real-time. Once this has completed, the method returns.\n+ * </p>\n+ *\n+ * <p>\n+ * The Event Index that is used by this implementation currently is the {@link LuceneEventIndex}, which is powered by Apache Lucene. This index provides\n+ * very high throughput. However, this high throughput is gained by avoiding continual 'commits' of the Index Writer. As a result, on restart, this Repository\n+ * may take a minute or two to re-index some of the Provenance Events, as some of the Events may have been added to the index without committing the Index Writer.\n+ * Given the substantial performance improvement gained by committing the Index Writer only periodically, this trade-off is generally well accepted.\n+ * </p>\n+ *\n+ * <p>\n+ * This Repositories supports the notion of 'partitions'. The repository can be configured to store data to one or more partitions. Each partition is typically\n+ * stored on a separate physical partition on disk. As a result, this allows striping of data across multiple partitions in order to achieve linear scalability\n+ * across disks for far greater performance.\n+ * </p>\n+ */\n+public class WriteAheadProvenanceRepository implements ProvenanceRepository {\n+    private static final Logger logger = LoggerFactory.getLogger(WriteAheadProvenanceRepository.class);\n+    private static final int BLOCK_SIZE = 1024 * 32;\n+    public static final String EVENT_CATEGORY = \"Provenance Repository\";\n+\n+    private final RepositoryConfiguration config;\n+\n+    // effectively final\n+    private EventStore eventStore;\n+    private EventIndex eventIndex;\n+    private EventReporter eventReporter;\n+    private Authorizer authorizer;\n+    private ProvenanceAuthorizableFactory resourceFactory;\n+\n+    /**\n+     * This constructor exists solely for the use of the Java Service Loader mechanism and should not be used.\n+     */\n+    public WriteAheadProvenanceRepository() {\n+        config = null;\n+    }\n+\n+    public WriteAheadProvenanceRepository(final NiFiProperties nifiProperties) {\n+        this(RepositoryConfiguration.create(nifiProperties));\n+    }\n+\n+    public WriteAheadProvenanceRepository(final RepositoryConfiguration config) {\n+        this.config = config;\n+    }\n+\n+    @Override\n+    public synchronized void initialize(final EventReporter eventReporter, final Authorizer authorizer, final ProvenanceAuthorizableFactory resourceFactory,\n+        final IdentifierLookup idLookup) throws IOException {\n+        final RecordWriterFactory recordWriterFactory = (file, idGenerator, compressed, createToc) -> {\n+            final TocWriter tocWriter = createToc ? new StandardTocWriter(TocUtil.getTocFile(file), false, false) : null;\n+            return new EventIdFirstSchemaRecordWriter(file, idGenerator, tocWriter, compressed, BLOCK_SIZE, idLookup);\n+        };\n+\n+        final EventFileManager fileManager = new EventFileManager();\n+        final RecordReaderFactory recordReaderFactory = (file, logs, maxChars) -> {\n+            fileManager.obtainReadLock(file);\n+            try {\n+                return RecordReaders.newRecordReader(file, logs, maxChars);\n+            } finally {\n+                fileManager.releaseReadLock(file);\n+            }\n+        };\n+\n+        eventStore = new PartitionedWriteAheadEventStore(config, recordWriterFactory, recordReaderFactory, eventReporter, fileManager);\n+\n+        final IndexManager indexManager = new SimpleIndexManager(config);\n+        eventIndex = new LuceneEventIndex(config, indexManager, eventReporter);\n+\n+        this.eventReporter = eventReporter;\n+        this.authorizer = authorizer;\n+        this.resourceFactory = resourceFactory;\n+\n+        eventStore.initialize();\n+        eventIndex.initialize(eventStore);\n+\n+        eventStore.reindexLatestEvents(eventIndex);\n+    }\n+\n+    @Override\n+    public ProvenanceEventBuilder eventBuilder() {\n+        return new StandardProvenanceEventRecord.Builder();\n+    }\n+\n+    @Override\n+    public void registerEvent(final ProvenanceEventRecord event) {\n+        registerEvents(Collections.singleton(event));\n+    }\n+\n+    @Override\n+    public void registerEvents(final Iterable<ProvenanceEventRecord> events) {\n+        final StorageResult storageResult;\n+\n+        try {\n+            storageResult = eventStore.addEvents(events);\n+        } catch (final IOException e) {\n+            logger.error(\"Failed to write events to the Event Store\", e);\n+            eventReporter.reportEvent(Severity.ERROR, EVENT_CATEGORY, \"Failed to write Provenance Events to the repository. See logs for more details.\");\n+            return;\n+        }\n+\n+        final Map<ProvenanceEventRecord, StorageSummary> locationMap = storageResult.getStorageLocations();\n+        if (!locationMap.isEmpty()) {\n+            eventIndex.addEvents(locationMap);\n+        }\n+    }\n+\n+    @Override\n+    public List<ProvenanceEventRecord> getEvents(final long firstRecordId, final int maxRecords) throws IOException {\n+        return eventStore.getEvents(firstRecordId, maxRecords);\n+    }\n+\n+    @Override\n+    public ProvenanceEventRecord getEvent(final long id) throws IOException {\n+        return eventStore.getEvent(id).orElse(null);\n+    }\n+\n+    @Override\n+    public Long getMaxEventId() {\n+        return eventStore.getMaxEventId();\n+    }\n+\n+    @Override\n+    public void close() {\n+        CloseableUtil.closeQuietly(eventStore, eventIndex);\n+    }\n+\n+    @Override\n+    public ProvenanceEventRecord getEvent(final long id, final NiFiUser user) throws IOException {\n+        final ProvenanceEventRecord event = getEvent(id);\n+        if (event == null) {\n+            return null;\n+        }\n+\n+        authorize(event, user);\n+        return event;\n+    }\n+\n+    private void authorize(final ProvenanceEventRecord event, final NiFiUser user) {\n+        if (authorizer == null) {\n+            return;\n+        }\n+\n+        final Authorizable eventAuthorizable;\n+        if (event.isRemotePortType()) {\n+            eventAuthorizable = resourceFactory.createRemoteDataAuthorizable(event.getComponentId());\n+        } else {\n+            eventAuthorizable = resourceFactory.createLocalDataAuthorizable(event.getComponentId());\n+        }\n+        eventAuthorizable.authorize(authorizer, RequestAction.READ, user, event.getAttributes());\n+    }\n+\n+\n+    @Override\n+    public List<ProvenanceEventRecord> getEvents(final long firstRecordId, final int maxRecords, final NiFiUser user) throws IOException {\n+        final List<ProvenanceEventRecord> events = getEvents(firstRecordId, maxRecords);\n+        return createEventAuthorizer(user).filterUnauthorizedEvents(events);\n+    }\n+\n+    private EventAuthorizer createEventAuthorizer(final NiFiUser user) {\n+        return new UserEventAuthorizer(authorizer, resourceFactory, user);\n+    }\n+\n+    @Override\n+    public ProvenanceEventRepository getProvenanceEventRepository() {\n+        return this;\n+    }\n+\n+    @Override\n+    public QuerySubmission submitQuery(final Query query, final NiFiUser user) {\n+        return eventIndex.submitQuery(query, createEventAuthorizer(user), user.getIdentity());\n+    }\n+\n+    @Override\n+    public QuerySubmission retrieveQuerySubmission(final String queryIdentifier, final NiFiUser user) {\n+        return eventIndex.retrieveQuerySubmission(queryIdentifier, user);\n+    }\n+\n+    @Override\n+    public ComputeLineageSubmission submitLineageComputation(final String flowFileUuid, final NiFiUser user) {\n+        return eventIndex.submitLineageComputation(flowFileUuid, user, createEventAuthorizer(user));\n+    }\n+\n+    @Override\n+    public ComputeLineageSubmission submitLineageComputation(final long eventId, final NiFiUser user) {\n+        return eventIndex.submitLineageComputation(eventId, user, createEventAuthorizer(user));\n+    }\n+\n+    @Override\n+    public ComputeLineageSubmission retrieveLineageSubmission(final String lineageIdentifier, final NiFiUser user) {\n+        return eventIndex.retrieveLineageSubmission(lineageIdentifier, user);\n+    }\n+\n+    @Override\n+    public ComputeLineageSubmission submitExpandParents(final long eventId, final NiFiUser user) {\n+        return eventIndex.submitExpandParents(eventId, user, createEventAuthorizer(user));\n+    }\n+\n+    @Override\n+    public ComputeLineageSubmission submitExpandChildren(final long eventId, final NiFiUser user) {\n+        return eventIndex.submitExpandChildren(eventId, user, createEventAuthorizer(user));\n+    }\n+\n+    @Override\n+    public List<SearchableField> getSearchableFields() {\n+        return Collections.unmodifiableList(config.getSearchableFields());\n+    }\n+\n+    @Override\n+    public List<SearchableField> getSearchableAttributes() {\n+        return Collections.unmodifiableList(config.getSearchableAttributes());\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/WriteAheadProvenanceRepository.java",
                "sha": "229a96d5d93f22948e9fe74f8945918c69891b8e",
                "status": "added"
            },
            {
                "additions": 119,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/authorization/EventAuthorizer.java",
                "changes": 119,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/authorization/EventAuthorizer.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/authorization/EventAuthorizer.java",
                "patch": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.authorization;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.stream.Collectors;\n+\n+import org.apache.nifi.authorization.AccessDeniedException;\n+import org.apache.nifi.provenance.PlaceholderProvenanceEvent;\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+\n+public interface EventAuthorizer {\n+\n+    /**\n+     * Determines whether or not the has access to the given Provenance Event.\n+     * This method does not imply the user is directly attempting to access the specified resource. If the user is\n+     * attempting a direct access use Authorizable.authorize().\n+     *\n+     * @param event the event to authorize\n+     * @return is authorized\n+     */\n+    boolean isAuthorized(ProvenanceEventRecord event);\n+\n+    /**\n+     * Authorizes the current user for the specified action on the specified resource. This method does\n+     * imply the user is directly accessing the specified resource.\n+     *\n+     * @param event the event to authorize\n+     * @throws AccessDeniedException if the user is not authorized\n+     */\n+    void authorize(ProvenanceEventRecord event) throws AccessDeniedException;\n+\n+    /**\n+     * Filters out any events that the user is not authorized to access\n+     *\n+     * @param events the events to filtered\n+     * @return a List that contains only events from the original, for which the user has access\n+     */\n+    default List<ProvenanceEventRecord> filterUnauthorizedEvents(List<ProvenanceEventRecord> events) {\n+        return events.stream()\n+            .filter(event -> isAuthorized(event))\n+            .collect(Collectors.toList());\n+    }\n+\n+    /**\n+     * Returns a Set of provenance events for which any of the given events that the user does not\n+     * have access to has been replaced by a placeholder event\n+     *\n+     * @param events the events to filter\n+     * @return a Set containing only provenance events that the user has access to\n+     */\n+    default Set<ProvenanceEventRecord> replaceUnauthorizedWithPlaceholders(Set<ProvenanceEventRecord> events) {\n+        return events.stream()\n+            .map(event -> isAuthorized(event) ? event : new PlaceholderProvenanceEvent(event))\n+            .collect(Collectors.toSet());\n+    }\n+\n+    public static final EventAuthorizer GRANT_ALL = new EventAuthorizer() {\n+        @Override\n+        public boolean isAuthorized(ProvenanceEventRecord event) {\n+            return true;\n+        }\n+\n+        @Override\n+        public void authorize(ProvenanceEventRecord event) throws AccessDeniedException {\n+        }\n+\n+        @Override\n+        public List<ProvenanceEventRecord> filterUnauthorizedEvents(List<ProvenanceEventRecord> events) {\n+            return events;\n+        }\n+\n+        @Override\n+        public Set<ProvenanceEventRecord> replaceUnauthorizedWithPlaceholders(Set<ProvenanceEventRecord> events) {\n+            return events;\n+        }\n+    };\n+\n+    public static final EventAuthorizer DENY_ALL = new EventAuthorizer() {\n+        @Override\n+        public boolean isAuthorized(ProvenanceEventRecord event) {\n+            return false;\n+        }\n+\n+        @Override\n+        public void authorize(ProvenanceEventRecord event) throws AccessDeniedException {\n+            throw new AccessDeniedException();\n+        }\n+\n+        @Override\n+        public List<ProvenanceEventRecord> filterUnauthorizedEvents(List<ProvenanceEventRecord> events) {\n+            return Collections.emptyList();\n+        }\n+\n+        @Override\n+        public Set<ProvenanceEventRecord> replaceUnauthorizedWithPlaceholders(Set<ProvenanceEventRecord> events) {\n+            return events.stream()\n+                .map(event -> new PlaceholderProvenanceEvent(event))\n+                .collect(Collectors.toSet());\n+        }\n+    };\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/authorization/EventAuthorizer.java",
                "sha": "ab193e489cc6703a08de4bfdb864dcbbddea23d8",
                "status": "added"
            },
            {
                "additions": 42,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/authorization/EventTransformer.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/authorization/EventTransformer.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/authorization/EventTransformer.java",
                "patch": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.authorization;\n+\n+import java.util.Optional;\n+\n+import org.apache.nifi.provenance.PlaceholderProvenanceEvent;\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+\n+/**\n+ * An interface for transforming a Provenance Event for which the user is not authorized to access\n+ */\n+public interface EventTransformer {\n+\n+    Optional<ProvenanceEventRecord> transform(ProvenanceEventRecord unauthorizedEvent);\n+\n+    /**\n+     * An EventTransformer that transforms any event into an Empty Optional\n+     */\n+    public static final EventTransformer EMPTY_TRANSFORMER = event -> Optional.empty();\n+\n+    /**\n+     * An EventTransformer that transforms any event into a Placeholder event\n+     */\n+    public static final EventTransformer PLACEHOLDER_TRANSFORMER = event -> Optional.of(new PlaceholderProvenanceEvent(event));\n+\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/authorization/EventTransformer.java",
                "sha": "1c48aad367e0973854d0588e1232c9b30e9701dc",
                "status": "added"
            },
            {
                "additions": 76,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/authorization/UserEventAuthorizer.java",
                "changes": 76,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/authorization/UserEventAuthorizer.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/authorization/UserEventAuthorizer.java",
                "patch": "@@ -0,0 +1,76 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.authorization;\n+\n+import org.apache.nifi.authorization.AuthorizationResult;\n+import org.apache.nifi.authorization.AuthorizationResult.Result;\n+import org.apache.nifi.authorization.Authorizer;\n+import org.apache.nifi.authorization.RequestAction;\n+import org.apache.nifi.authorization.resource.Authorizable;\n+import org.apache.nifi.authorization.user.NiFiUser;\n+import org.apache.nifi.provenance.ProvenanceAuthorizableFactory;\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.web.ResourceNotFoundException;\n+\n+public class UserEventAuthorizer implements EventAuthorizer {\n+    private final Authorizer authorizer;\n+    private final ProvenanceAuthorizableFactory resourceFactory;\n+    private final NiFiUser user;\n+\n+    public UserEventAuthorizer(final Authorizer authorizer, final ProvenanceAuthorizableFactory authorizableFactory, final NiFiUser user) {\n+        this.authorizer = authorizer;\n+        this.resourceFactory = authorizableFactory;\n+        this.user = user;\n+    }\n+\n+    @Override\n+    public boolean isAuthorized(final ProvenanceEventRecord event) {\n+        if (authorizer == null || user == null) {\n+            return true;\n+        }\n+\n+        final Authorizable eventAuthorizable;\n+        try {\n+            if (event.isRemotePortType()) {\n+                eventAuthorizable = resourceFactory.createRemoteDataAuthorizable(event.getComponentId());\n+            } else {\n+                eventAuthorizable = resourceFactory.createLocalDataAuthorizable(event.getComponentId());\n+            }\n+        } catch (final ResourceNotFoundException rnfe) {\n+            return false;\n+        }\n+\n+        final AuthorizationResult result = eventAuthorizable.checkAuthorization(authorizer, RequestAction.READ, user, event.getAttributes());\n+        return Result.Approved.equals(result.getResult());\n+    }\n+\n+    @Override\n+    public void authorize(final ProvenanceEventRecord event) {\n+        if (authorizer == null) {\n+            return;\n+        }\n+\n+        final Authorizable eventAuthorizable;\n+        if (event.isRemotePortType()) {\n+            eventAuthorizable = resourceFactory.createRemoteDataAuthorizable(event.getComponentId());\n+        } else {\n+            eventAuthorizable = resourceFactory.createLocalDataAuthorizable(event.getComponentId());\n+        }\n+        eventAuthorizable.authorize(authorizer, RequestAction.READ, user, event.getAttributes());\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/authorization/UserEventAuthorizer.java",
                "sha": "5126b7e678eaf4eb9fae29d2252c13c5d9d77628",
                "status": "added"
            },
            {
                "additions": 160,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/EventIndex.java",
                "changes": 160,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/EventIndex.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/EventIndex.java",
                "patch": "@@ -0,0 +1,160 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.index;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Map;\n+\n+import org.apache.nifi.authorization.user.NiFiUser;\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.authorization.EventAuthorizer;\n+import org.apache.nifi.provenance.lineage.ComputeLineageSubmission;\n+import org.apache.nifi.provenance.search.Query;\n+import org.apache.nifi.provenance.search.QuerySubmission;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n+import org.apache.nifi.provenance.store.EventStore;\n+\n+/**\n+ * An Event Index is responsible for indexing Provenance Events in such a way that the index can be quickly\n+ * searched to in order to retrieve events of interest.\n+ */\n+public interface EventIndex extends Closeable {\n+\n+    /**\n+     * Initializes the Event Index, providing it access to the Event Store, in case it is necessary for performing\n+     * initialization tasks\n+     *\n+     * @param eventStore the EventStore that holds the events that have been given to the repository.\n+     */\n+    void initialize(EventStore eventStore);\n+\n+    /**\n+     * Adds the given events to the index so that they can be queried later.\n+     *\n+     * @param events the events to index along with their associated Storage Summaries\n+     */\n+    void addEvents(Map<ProvenanceEventRecord, StorageSummary> events);\n+\n+    /**\n+     * Replaces the entries in the appropriate index with the given events\n+     *\n+     * @param events the events to add or replace along with their associated Storage Summaries\n+     */\n+    void reindexEvents(Map<ProvenanceEventRecord, StorageSummary> events);\n+\n+    /**\n+     * @return the number of bytes that are utilized by the Event Index\n+     */\n+    long getSize();\n+\n+    /**\n+     * Submits a Query asynchronously and returns a QuerySubmission that can be used to obtain the results\n+     *\n+     * @param query the query to perform\n+     * @param authorizer the authorizer to use in order to determine whether or not a particular event should be included in the result\n+     * @param userId the ID of the user on whose behalf the query is being submitted\n+     *\n+     * @return a QuerySubmission that can be used to retrieve the results later\n+     */\n+    QuerySubmission submitQuery(Query query, EventAuthorizer authorizer, String userId);\n+\n+    /**\n+     * Asynchronously computes the lineage for the FlowFile that is identified by the Provenance Event with the given ID.\n+     *\n+     * @param eventId the ID of the Provenance Event for which the lineage should be calculated\n+     * @param user the NiFi user on whose behalf the computing is being performed\n+     * @param authorizer the authorizer to use in order to determine whether or not a particular event should be included in the result\n+     *\n+     * @return a ComputeLineageSubmission that can be used to retrieve the results later\n+     */\n+    ComputeLineageSubmission submitLineageComputation(long eventId, NiFiUser user, EventAuthorizer authorizer);\n+\n+    /**\n+     * Asynchronously computes the lineage for the FlowFile that has the given FlowFile UUID.\n+     *\n+     * @param flowFileUuid the UUID of the FlowFile for which the lineage should be computed\n+     * @param user the NiFi user on whose behalf the computing is being performed\n+     * @param authorizer the authorizer to use in order to determine whether or not a particular event should be included in the result\n+     *\n+     * @return a ComputeLineageSubmission that can be used to retrieve the results later\n+     */\n+    ComputeLineageSubmission submitLineageComputation(String flowFileUuid, NiFiUser user, EventAuthorizer authorizer);\n+\n+    /**\n+     * Asynchronously computes the lineage that makes up the 'child flowfiles' generated by the event with the given ID. This method is\n+     * valid only for Events that produce 'child flowfiles' such as FORK, CLONE, REPLAY, etc.\n+     *\n+     * @param eventId the ID of the Provenance Event for which the lineage should be calculated\n+     * @param user the NiFi user on whose behalf the computing is being performed\n+     * @param authorizer the authorizer to use in order to determine whether or not a particular event should be included in the result\n+     *\n+     * @return a ComputeLineageSubmission that can be used to retrieve the results later\n+     */\n+    ComputeLineageSubmission submitExpandChildren(long eventId, NiFiUser user, EventAuthorizer authorizer);\n+\n+    /**\n+     * Asynchronously computes the lineage that makes up the 'parent flowfiles' that were involved in the event with the given ID. This method\n+     * is valid only for Events that have 'parent flowfiles' such as FORK, JOIN, etc.\n+     *\n+     * @param eventId the ID of the Provenance Event for which the lineage should be calculated\n+     * @param user the NiFi user on whose behalf the computing is being performed\n+     * @param authorizer the authorizer to use in order to determine whether or not a particular event should be included in the result\n+     *\n+     * @return a ComputeLineageSubmission that can be used to retrieve the results later\n+     */\n+    ComputeLineageSubmission submitExpandParents(long eventId, NiFiUser user, EventAuthorizer authorizer);\n+\n+    /**\n+     * Retrieves the ComputeLineageSubmission that was returned by the 'submitLineageComputation' methods\n+     *\n+     * @param lineageIdentifier the identifier of the linage\n+     * @param user the NiFi user on whose behalf the retrieval is being performed\n+     * @return the ComputeLineageSubmission that represents the asynchronous lineage computation that is being performed under the given\n+     *         identifier, or <code>null</code> if the identifier cannot be found.\n+     */\n+    ComputeLineageSubmission retrieveLineageSubmission(String lineageIdentifier, NiFiUser user);\n+\n+    /**\n+     * Retrieves the QuerySubmission that was returned by the 'submitQuery' method\n+     *\n+     * @param queryIdentifier the identifier of the query\n+     * @param user the NiFi user on whose behalf the retrieval is being performed\n+     * @return the QuerySubmission that represents the asynchronous query that is being performed under the given\n+     *         identifier, or <code>null</code> if the identifier cannot be found.\n+     */\n+    QuerySubmission retrieveQuerySubmission(String queryIdentifier, NiFiUser user);\n+\n+    /**\n+     * Upon restart of NiFi, it is possible that the Event Index will have lost some events due to frequency of committing the index.\n+     * In such as case, this method is responsible for returning the minimum Provenance Event ID that it knows is safely indexed. If\n+     * any Provenance Event exists in the Event Store with an ID greater than the value returned, that Event should be re-indexed.\n+     *\n+     * @param partitionName the name of the Partition for which the minimum Event ID is desired\n+     * @return the minimum Provenance Event ID that the Index knows is safely indexed for the given partition\n+     */\n+    long getMinimumEventIdToReindex(String partitionName);\n+\n+    /**\n+     * Instructs the Event Index to commit any changes that have been made to the partition with the given name\n+     *\n+     * @param partitionName the name of the partition to commit changes\n+     * @throws IOException if unable to commit the changes\n+     */\n+    void commitChanges(String partitionName) throws IOException;\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/EventIndex.java",
                "sha": "051cd1f930a4c53e6c6ffd8ff1213e3b6411105c",
                "status": "added"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/EventIndexSearcher.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/EventIndexSearcher.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/EventIndexSearcher.java",
                "patch": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.index;\n+\n+import java.io.Closeable;\n+import java.io.File;\n+\n+import org.apache.lucene.search.IndexSearcher;\n+\n+public interface EventIndexSearcher extends Closeable {\n+    IndexSearcher getIndexSearcher();\n+\n+    File getIndexDirectory();\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/EventIndexSearcher.java",
                "sha": "83894089da26aab91ed3f74a22017239ae3c0939",
                "status": "added"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/EventIndexWriter.java",
                "changes": 44,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/EventIndexWriter.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/EventIndexWriter.java",
                "patch": "@@ -0,0 +1,44 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.index;\n+\n+import java.io.Closeable;\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.List;\n+\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.index.IndexWriter;\n+\n+public interface EventIndexWriter extends Closeable {\n+\n+    boolean index(Document document, int commitThreshold) throws IOException;\n+\n+    boolean index(List<Document> documents, int commitThreshold) throws IOException;\n+\n+    File getDirectory();\n+\n+    long commit() throws IOException;\n+\n+    int getEventsIndexedSinceCommit();\n+\n+    long getEventsIndexed();\n+\n+    IndexWriter getIndexWriter();\n+\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/EventIndexWriter.java",
                "sha": "f0af7dcc198742052c8aa74b71e0bce616433371",
                "status": "added"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/SearchFailedException.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/SearchFailedException.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/SearchFailedException.java",
                "patch": "@@ -0,0 +1,28 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.index;\n+\n+public class SearchFailedException extends RuntimeException {\n+    public SearchFailedException(final String message, final Throwable cause) {\n+        super(message, cause);\n+    }\n+\n+    public SearchFailedException(final Throwable cause) {\n+        super(cause);\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/SearchFailedException.java",
                "sha": "ce1bedb1cfe2df83ca724fb61dd13d7264d5c8cf",
                "status": "added"
            },
            {
                "additions": 33,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/CachedQuery.java",
                "changes": 33,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/CachedQuery.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/CachedQuery.java",
                "patch": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.index.lucene;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.search.Query;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n+\n+public interface CachedQuery {\n+\n+    void update(ProvenanceEventRecord event, StorageSummary storageSummary);\n+\n+    Optional<List<Long>> evaluate(Query query);\n+\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/CachedQuery.java",
                "sha": "770c4552bf3789528503028953fa7b7ab938a55f",
                "status": "added"
            },
            {
                "additions": 24,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/CommitPreference.java",
                "changes": 24,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/CommitPreference.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/CommitPreference.java",
                "patch": "@@ -0,0 +1,24 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.index.lucene;\n+\n+public enum CommitPreference {\n+    FORCE_COMMIT,\n+    PREVENT_COMMIT,\n+    NO_PREFERENCE;\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/CommitPreference.java",
                "sha": "2208917162fb899e83c945286c2357b55fab9dd6",
                "status": "added"
            },
            {
                "additions": 143,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/ConvertEventToLuceneDocument.java",
                "changes": 143,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/ConvertEventToLuceneDocument.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/ConvertEventToLuceneDocument.java",
                "patch": "@@ -0,0 +1,143 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.nifi.provenance.index.lucene;\n+\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Set;\n+\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field;\n+import org.apache.lucene.document.Field.Store;\n+import org.apache.lucene.document.FieldType;\n+import org.apache.lucene.document.LongField;\n+import org.apache.lucene.document.StringField;\n+import org.apache.lucene.index.FieldInfo.IndexOptions;\n+import org.apache.nifi.flowfile.attributes.CoreAttributes;\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.ProvenanceEventType;\n+import org.apache.nifi.provenance.SearchableFields;\n+import org.apache.nifi.provenance.lucene.LuceneUtil;\n+import org.apache.nifi.provenance.search.SearchableField;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n+\n+public class ConvertEventToLuceneDocument {\n+    private final Set<SearchableField> searchableEventFields;\n+    private final Set<SearchableField> searchableAttributeFields;\n+\n+    public ConvertEventToLuceneDocument(final List<SearchableField> searchableEventFields, final List<SearchableField> searchableAttributes) {\n+        this.searchableEventFields = Collections.unmodifiableSet(new HashSet<>(searchableEventFields));\n+        this.searchableAttributeFields = Collections.unmodifiableSet(new HashSet<>(searchableAttributes));\n+    }\n+\n+    private void addField(final Document doc, final SearchableField field, final String value) {\n+        if (value == null || (!field.isAttribute() && !searchableEventFields.contains(field))) {\n+            return;\n+        }\n+\n+        doc.add(new StringField(field.getSearchableFieldName(), value.toLowerCase(), Store.NO));\n+    }\n+\n+\n+    public Document convert(final ProvenanceEventRecord record, final StorageSummary persistedEvent) {\n+        final Document doc = new Document();\n+        addField(doc, SearchableFields.FlowFileUUID, record.getFlowFileUuid());\n+        addField(doc, SearchableFields.Filename, record.getAttribute(CoreAttributes.FILENAME.key()));\n+        addField(doc, SearchableFields.ComponentID, record.getComponentId());\n+        addField(doc, SearchableFields.AlternateIdentifierURI, record.getAlternateIdentifierUri());\n+        addField(doc, SearchableFields.EventType, record.getEventType().name());\n+        addField(doc, SearchableFields.Relationship, record.getRelationship());\n+        addField(doc, SearchableFields.Details, record.getDetails());\n+        addField(doc, SearchableFields.ContentClaimSection, record.getContentClaimSection());\n+        addField(doc, SearchableFields.ContentClaimContainer, record.getContentClaimContainer());\n+        addField(doc, SearchableFields.ContentClaimIdentifier, record.getContentClaimIdentifier());\n+        addField(doc, SearchableFields.SourceQueueIdentifier, record.getSourceQueueIdentifier());\n+        addField(doc, SearchableFields.TransitURI, record.getTransitUri());\n+\n+        for (final SearchableField searchableField : searchableAttributeFields) {\n+            addField(doc, searchableField, LuceneUtil.truncateIndexField(record.getAttribute(searchableField.getSearchableFieldName())));\n+        }\n+\n+        // Index the fields that we always index (unless there's nothing else to index at all)\n+        if (!doc.getFields().isEmpty()) {\n+            // Always include Lineage Start Date because it allows us to make our Lineage queries more efficient.\n+            doc.add(new LongField(SearchableFields.LineageStartDate.getSearchableFieldName(), record.getLineageStartDate(), Store.NO));\n+            // Always include Event Time because most queries are bound by a start and end time.\n+            doc.add(new LongField(SearchableFields.EventTime.getSearchableFieldName(), record.getEventTime(), Store.NO));\n+            // We always include File Size because the UI wants to always render the controls for specifying this. This idea could be revisited.\n+            doc.add(new LongField(SearchableFields.FileSize.getSearchableFieldName(), record.getFileSize(), Store.NO));\n+            // We always store the event Event ID in the Document but do not index it. It doesn't make sense to query based on Event ID because\n+            // if we want a particular Event ID, we can just obtain it directly from the EventStore. But when we obtain a Document, this info must\n+            // be stored so that we know how to lookup the event in the store.\n+            doc.add(new UnIndexedLongField(SearchableFields.Identifier.getSearchableFieldName(), persistedEvent.getEventId()));\n+\n+            // If it's event is a FORK, or JOIN, add the FlowFileUUID for all child/parent UUIDs.\n+            final ProvenanceEventType eventType = record.getEventType();\n+            if (eventType == ProvenanceEventType.FORK || eventType == ProvenanceEventType.CLONE || eventType == ProvenanceEventType.REPLAY) {\n+                for (final String uuid : record.getChildUuids()) {\n+                    if (!uuid.equals(record.getFlowFileUuid())) {\n+                        addField(doc, SearchableFields.FlowFileUUID, uuid);\n+                    }\n+                }\n+            } else if (eventType == ProvenanceEventType.JOIN) {\n+                for (final String uuid : record.getParentUuids()) {\n+                    if (!uuid.equals(record.getFlowFileUuid())) {\n+                        addField(doc, SearchableFields.FlowFileUUID, uuid);\n+                    }\n+                }\n+            } else if (eventType == ProvenanceEventType.RECEIVE && record.getSourceSystemFlowFileIdentifier() != null) {\n+                // If we get a receive with a Source System FlowFile Identifier, we add another Document that shows the UUID\n+                // that the Source System uses to refer to the data.\n+                final String sourceIdentifier = record.getSourceSystemFlowFileIdentifier();\n+                final String sourceFlowFileUUID;\n+                final int lastColon = sourceIdentifier.lastIndexOf(\":\");\n+                if (lastColon > -1 && lastColon < sourceIdentifier.length() - 2) {\n+                    sourceFlowFileUUID = sourceIdentifier.substring(lastColon + 1);\n+                } else {\n+                    sourceFlowFileUUID = null;\n+                }\n+\n+                if (sourceFlowFileUUID != null) {\n+                    addField(doc, SearchableFields.FlowFileUUID, sourceFlowFileUUID);\n+                }\n+            }\n+\n+            return doc;\n+        }\n+\n+        return null;\n+    }\n+\n+    private static class UnIndexedLongField extends Field {\n+        static final FieldType TYPE = new FieldType();\n+        static {\n+            TYPE.setIndexed(false);\n+            TYPE.setTokenized(true);\n+            TYPE.setOmitNorms(true);\n+            TYPE.setIndexOptions(IndexOptions.DOCS_ONLY);\n+            TYPE.setNumericType(FieldType.NumericType.LONG);\n+            TYPE.setStored(true);\n+            TYPE.freeze();\n+        }\n+\n+        public UnIndexedLongField(String name, long value) {\n+            super(name, TYPE);\n+            fieldsData = Long.valueOf(value);\n+        }\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/ConvertEventToLuceneDocument.java",
                "sha": "765b81f367ba1de24f48341f947e7c815e93a467",
                "status": "added"
            },
            {
                "additions": 244,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/EventIndexTask.java",
                "changes": 244,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/EventIndexTask.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/EventIndexTask.java",
                "patch": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.index.lucene;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.search.NumericRangeQuery;\n+import org.apache.nifi.events.EventReporter;\n+import org.apache.nifi.provenance.RepositoryConfiguration;\n+import org.apache.nifi.provenance.SearchableFields;\n+import org.apache.nifi.provenance.index.EventIndexWriter;\n+import org.apache.nifi.provenance.lucene.IndexManager;\n+import org.apache.nifi.reporting.Severity;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class EventIndexTask implements Runnable {\n+    private static final Logger logger = LoggerFactory.getLogger(EventIndexTask.class);\n+    private static final String EVENT_CATEGORY = \"Provenance Repository\";\n+    public static final int MAX_DOCUMENTS_PER_THREAD = 100;\n+    public static final int DEFAULT_MAX_EVENTS_PER_COMMIT = 1_000_000;\n+\n+    private final BlockingQueue<StoredDocument> documentQueue;\n+    private final IndexManager indexManager;\n+    private volatile boolean shutdown = false;\n+\n+    private final IndexDirectoryManager directoryManager;\n+    private final EventReporter eventReporter;\n+    private final int commitThreshold;\n+\n+    public EventIndexTask(final BlockingQueue<StoredDocument> documentQueue, final RepositoryConfiguration repoConfig, final IndexManager indexManager,\n+        final IndexDirectoryManager directoryManager, final int maxEventsPerCommit, final EventReporter eventReporter) {\n+        this.documentQueue = documentQueue;\n+        this.indexManager = indexManager;\n+        this.directoryManager = directoryManager;\n+        this.commitThreshold = maxEventsPerCommit;\n+        this.eventReporter = eventReporter;\n+    }\n+\n+    public void shutdown() {\n+        this.shutdown = true;\n+    }\n+\n+    private void fetchDocuments(final List<StoredDocument> destination) throws InterruptedException {\n+        // We want to fetch up to INDEX_BUFFER_SIZE documents at a time. However, we don't want to continually\n+        // call #drainTo on the queue. So we call poll, blocking for up to 1 second. If we get any event, then\n+        // we will call drainTo to gather the rest. If we get no events, then we just return, having gathered\n+        // no events.\n+        StoredDocument firstDoc = documentQueue.poll(1, TimeUnit.SECONDS);\n+        if (firstDoc == null) {\n+            return;\n+        }\n+\n+        destination.add(firstDoc);\n+        documentQueue.drainTo(destination, MAX_DOCUMENTS_PER_THREAD - 1);\n+    }\n+\n+    @Override\n+    public void run() {\n+        final List<StoredDocument> toIndex = new ArrayList<>(MAX_DOCUMENTS_PER_THREAD);\n+\n+        while (!shutdown) {\n+            try {\n+                // Get the Documents that we want to index.\n+                toIndex.clear();\n+                fetchDocuments(toIndex);\n+\n+                if (toIndex.isEmpty()) {\n+                    continue;\n+                }\n+\n+                // Write documents to the currently active index.\n+                final Map<String, List<StoredDocument>> docsByPartition = toIndex.stream()\n+                    .collect(Collectors.groupingBy(doc -> doc.getStorageSummary().getPartitionName().get()));\n+\n+                for (final Map.Entry<String, List<StoredDocument>> entry : docsByPartition.entrySet()) {\n+                    final String partitionName = entry.getKey();\n+                    final List<StoredDocument> docs = entry.getValue();\n+\n+                    index(docs, partitionName);\n+                }\n+            } catch (final Exception e) {\n+                logger.error(\"Failed to index Provenance Events\", e);\n+                eventReporter.reportEvent(Severity.ERROR, EVENT_CATEGORY, \"Failed to index Provenance Events. See logs for more information.\");\n+            }\n+        }\n+    }\n+\n+\n+    /**\n+     * Re-indexes the documents given. The IndexableDocument's provided are required to have the IndexDirectory provided.\n+     */\n+    void reIndex(final List<IndexableDocument> toIndex, final CommitPreference commitPreference) throws IOException {\n+        if (toIndex.isEmpty()) {\n+            return;\n+        }\n+\n+        final Map<File, List<IndexableDocument>> docsByIndexDir = toIndex.stream().collect(Collectors.groupingBy(doc -> doc.getIndexDirectory()));\n+        for (final Map.Entry<File, List<IndexableDocument>> entry : docsByIndexDir.entrySet()) {\n+            final File indexDirectory = entry.getKey();\n+            final List<IndexableDocument> documentsForIndex = entry.getValue();\n+\n+            final EventIndexWriter indexWriter = indexManager.borrowIndexWriter(indexDirectory);\n+            try {\n+                // Remove any documents that already exist in this index that are overlapping.\n+                long minId = Long.MAX_VALUE;\n+                long maxId = Long.MIN_VALUE;\n+\n+                for (final IndexableDocument doc : toIndex) {\n+                    final long eventId = doc.getDocument().getField(SearchableFields.Identifier.getSearchableFieldName()).numericValue().longValue();\n+                    if (eventId < minId) {\n+                        minId = eventId;\n+                    }\n+                    if (eventId > maxId) {\n+                        maxId = eventId;\n+                    }\n+                }\n+\n+                final NumericRangeQuery<Long> query = NumericRangeQuery.newLongRange(\n+                    SearchableFields.Identifier.getSearchableFieldName(), minId, maxId, true, true);\n+                indexWriter.getIndexWriter().deleteDocuments(query);\n+\n+                final List<Document> documents = documentsForIndex.stream()\n+                    .map(doc -> doc.getDocument())\n+                    .collect(Collectors.toList());\n+\n+                indexWriter.index(documents, commitThreshold);\n+            } finally {\n+                indexManager.returnIndexWriter(indexWriter, CommitPreference.FORCE_COMMIT.equals(commitPreference), false);\n+            }\n+        }\n+    }\n+\n+\n+    private void index(final List<StoredDocument> toIndex, final String partitionName) throws IOException {\n+        if (toIndex.isEmpty()) {\n+            return;\n+        }\n+\n+        // Convert the IndexableDocument list into a List of Documents so that we can pass them to the Index Writer.\n+        final List<Document> documents = toIndex.stream()\n+            .map(doc -> doc.getDocument())\n+            .collect(Collectors.toList());\n+\n+        boolean requestClose = false;\n+        boolean requestCommit = false;\n+\n+        final long minEventTime = toIndex.stream()\n+            .mapToLong(doc -> doc.getDocument().getField(SearchableFields.EventTime.getSearchableFieldName()).numericValue().longValue())\n+            .min()\n+            .getAsLong();\n+\n+        // Synchronize on the directory manager because we don't want the active directory to change\n+        // while we are obtaining an index writer for it. I.e., determining the active directory\n+        // and obtaining an Index Writer for it need to be done atomically.\n+        final EventIndexWriter indexWriter;\n+        final File indexDirectory;\n+        synchronized (directoryManager) {\n+            indexDirectory = directoryManager.getWritableIndexingDirectory(minEventTime, partitionName);\n+            indexWriter = indexManager.borrowIndexWriter(indexDirectory);\n+        }\n+\n+        try {\n+            // Perform the actual indexing.\n+            boolean writerIndicatesCommit = indexWriter.index(documents, commitThreshold);\n+\n+            // If we don't need to commit index based on what index writer tells us, we will still want\n+            // to commit the index if it's assigned to a partition and this is no longer the active index\n+            // for that partition. This prevents the following case:\n+            //\n+            // Thread T1: pulls events from queue\n+            //            Maps events to Index Directory D1\n+            // Thread T2: pulls events from queue\n+            //            Maps events to Index Directory D1, the active index for Partition P1.\n+            //            Writes events to D1.\n+            //            Commits Index Writer for D1.\n+            //            Closes Index Writer for D1.\n+            // Thread T1: Writes events to D1.\n+            //            Determines that Index Writer for D1 does not need to be committed or closed.\n+            //\n+            // In the case outlined above, we would potentially lose those events from the index! To avoid this,\n+            // we simply decide to commit the index if this writer is no longer the active writer for the index.\n+            // However, if we have 10 threads, we don't want all 10 threads trying to commit the index after each\n+            // update. We want to commit when they've all finished. This is what the IndexManager will do if we request\n+            // that it commit the index. It will also close the index if requested, once all writers have finished.\n+            // So when this is the case, we will request that the Index Manager both commit and close the writer.\n+\n+            final Optional<File> activeIndexDirOption = directoryManager.getActiveIndexDirectory(partitionName);\n+            if (!activeIndexDirOption.isPresent() || !activeIndexDirOption.get().equals(indexDirectory)) {\n+                requestCommit = true;\n+                requestClose = true;\n+            }\n+\n+            if (writerIndicatesCommit) {\n+                commit(indexWriter);\n+                requestCommit = false; // we've already committed the index writer so no need to request that the index manager do so also.\n+                final boolean directoryManagerIndicatesClose = directoryManager.onIndexCommitted(indexDirectory);\n+                requestClose = requestClose || directoryManagerIndicatesClose;\n+\n+                if (logger.isDebugEnabled()) {\n+                    final long maxId = documents.stream()\n+                        .mapToLong(doc -> doc.getField(SearchableFields.Identifier.getSearchableFieldName()).numericValue().longValue())\n+                        .max()\n+                        .orElse(-1L);\n+                    logger.debug(\"Committed index {} after writing a max Event ID of {}\", indexDirectory, maxId);\n+                }\n+            }\n+        } finally {\n+            indexManager.returnIndexWriter(indexWriter, requestCommit, requestClose);\n+        }\n+    }\n+\n+\n+    protected void commit(final EventIndexWriter indexWriter) throws IOException {\n+        final long start = System.nanoTime();\n+        final long approximateCommitCount = indexWriter.commit();\n+        final long millis = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);\n+        logger.debug(\"Successfully committed approximately {} Events to {} in {} millis\", approximateCommitCount, indexWriter, millis);\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/EventIndexTask.java",
                "sha": "f8bbd3be22fc4fd4eebb0d7442d00017c992e8df",
                "status": "added"
            },
            {
                "additions": 358,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/IndexDirectoryManager.java",
                "changes": 358,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/IndexDirectoryManager.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/IndexDirectoryManager.java",
                "patch": "@@ -0,0 +1,358 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.index.lucene;\n+\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Date;\n+import java.util.HashMap;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import java.util.stream.Collectors;\n+\n+import org.apache.nifi.provenance.RepositoryConfiguration;\n+import org.apache.nifi.provenance.util.DirectoryUtils;\n+import org.apache.nifi.util.Tuple;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class IndexDirectoryManager {\n+    private static final Logger logger = LoggerFactory.getLogger(IndexDirectoryManager.class);\n+    private static final FileFilter INDEX_DIRECTORY_FILTER = f -> f.getName().startsWith(\"index-\");\n+    private static final Pattern INDEX_FILENAME_PATTERN = Pattern.compile(\"index-(\\\\d+)\");\n+\n+    private final RepositoryConfiguration repoConfig;\n+\n+    // guarded by synchronizing on 'this'\n+    private final SortedMap<Long, List<IndexLocation>> indexLocationByTimestamp = new TreeMap<>();\n+    private final Map<String, IndexLocation> activeIndices = new HashMap<>();\n+\n+    public IndexDirectoryManager(final RepositoryConfiguration repoConfig) {\n+        this.repoConfig = repoConfig;\n+    }\n+\n+    public synchronized void initialize() {\n+        final Map<File, Tuple<Long, IndexLocation>> latestIndexByStorageDir = new HashMap<>();\n+\n+        for (final Map.Entry<String, File> entry : repoConfig.getStorageDirectories().entrySet()) {\n+            final String partitionName = entry.getKey();\n+            final File storageDir = entry.getValue();\n+\n+            final File[] indexDirs = storageDir.listFiles(INDEX_DIRECTORY_FILTER);\n+            if (indexDirs == null) {\n+                logger.warn(\"Unable to access Provenance Repository storage directory {}\", storageDir);\n+                continue;\n+            }\n+\n+            for (final File indexDir : indexDirs) {\n+                final Matcher matcher = INDEX_FILENAME_PATTERN.matcher(indexDir.getName());\n+                if (!matcher.matches()) {\n+                    continue;\n+                }\n+\n+                final long startTime = DirectoryUtils.getIndexTimestamp(indexDir);\n+                final List<IndexLocation> dirsForTimestamp = indexLocationByTimestamp.computeIfAbsent(startTime, t -> new ArrayList<>());\n+                final IndexLocation indexLoc = new IndexLocation(indexDir, startTime, partitionName, repoConfig.getDesiredIndexSize());\n+                dirsForTimestamp.add(indexLoc);\n+\n+                final Tuple<Long, IndexLocation> tuple = latestIndexByStorageDir.get(storageDir);\n+                if (tuple == null || startTime > tuple.getKey()) {\n+                    latestIndexByStorageDir.put(storageDir, new Tuple<>(startTime, indexLoc));\n+                }\n+            }\n+        }\n+\n+        // Restore the activeIndices to point at the newest index in each storage location.\n+        for (final Tuple<Long, IndexLocation> tuple : latestIndexByStorageDir.values()) {\n+            final IndexLocation indexLoc = tuple.getValue();\n+            activeIndices.put(indexLoc.getPartitionName(), indexLoc);\n+        }\n+    }\n+\n+\n+    public synchronized void deleteDirectory(final File directory) {\n+        final Iterator<Map.Entry<Long, List<IndexLocation>>> itr = indexLocationByTimestamp.entrySet().iterator();\n+        while (itr.hasNext()) {\n+            final Map.Entry<Long, List<IndexLocation>> entry = itr.next();\n+            final List<IndexLocation> locations = entry.getValue();\n+\n+            final IndexLocation locToRemove = new IndexLocation(directory, DirectoryUtils.getIndexTimestamp(directory),\n+                directory.getName(), repoConfig.getDesiredIndexSize());\n+            locations.remove(locToRemove);\n+            if (locations.isEmpty()) {\n+                itr.remove();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Returns a List of all indexes where the latest event in the index has an event time before the given timestamp\n+     *\n+     * @param timestamp the cutoff\n+     * @return all Files that belong to an index, where the index has no events later than the given time\n+     */\n+    public synchronized List<File> getDirectoriesBefore(final long timestamp) {\n+        final List<File> selected = new ArrayList<>();\n+\n+        // An index cannot be expired if it is the latest index in the storage directory. As a result, we need to\n+        // separate the indexes by Storage Directory so that we can easily determine if this is the case.\n+        final Map<String, List<IndexLocation>> startTimeWithFileByStorageDirectory = flattenDirectoriesByTimestamp().stream()\n+            .collect(Collectors.groupingBy(indexLoc -> indexLoc.getPartitionName()));\n+\n+        // Scan through the index directories and the associated index event start time.\n+        // If looking at index N, we can determine the index end time by assuming that it is the same as the\n+        // start time of index N+1. So we determine the time range of each index and select an index only if\n+        // its start time is before the given timestamp and its end time is <= the given timestamp.\n+        for (final List<IndexLocation> startTimeWithFile : startTimeWithFileByStorageDirectory.values()) {\n+            for (int i = 0; i < startTimeWithFile.size(); i++) {\n+                final IndexLocation indexLoc = startTimeWithFile.get(i);\n+\n+                final String partition = indexLoc.getPartitionName();\n+                final IndexLocation activeLocation = activeIndices.get(partition);\n+                if (indexLoc.equals(activeLocation)) {\n+                    continue;\n+                }\n+\n+                final Long indexStartTime = indexLoc.getIndexStartTimestamp();\n+                if (indexStartTime > timestamp) {\n+                    // If the first timestamp in the index is later than the desired timestamp,\n+                    // then we are done. We can do this because the list is ordered by monotonically\n+                    // increasing timestamp as the Tuple key.\n+                    break;\n+                }\n+\n+                if (i < startTimeWithFile.size() - 1) {\n+                    final IndexLocation nextLocation = startTimeWithFile.get(i + 1);\n+                    final Long indexEndTime = nextLocation.getIndexStartTimestamp();\n+                    if (indexEndTime <= timestamp) {\n+                        logger.debug(\"Considering Index Location {} older than {} ({}) because its events have an EventTime \"\n+                            + \"ranging from {} ({}) to {} ({}) based on the following IndexLocations: {}\", nextLocation, timestamp, new Date(timestamp),\n+                            indexStartTime, new Date(indexStartTime), indexEndTime, new Date(indexEndTime), startTimeWithFile);\n+\n+                        selected.add(nextLocation.getIndexDirectory());\n+                    }\n+                }\n+            }\n+        }\n+\n+        logger.debug(\"Returning the following list of index locations because they were finished being written to before {}: {}\", timestamp, selected);\n+        return selected;\n+    }\n+\n+    /**\n+     * Convert directoriesByTimestamp to a List of IndexLocations.\n+     * This allows us to easily get the 'next' value when iterating over the elements.\n+     * This is useful because we know that the 'next' value will have a timestamp that is when that\n+     * file started being written to - which is the same as when this index stopped being written to.\n+     *\n+     * @return a List of all IndexLocations known\n+     */\n+    private List<IndexLocation> flattenDirectoriesByTimestamp() {\n+        final List<IndexLocation> startTimeWithFile = new ArrayList<>();\n+        for (final Map.Entry<Long, List<IndexLocation>> entry : indexLocationByTimestamp.entrySet()) {\n+            for (final IndexLocation indexLoc : entry.getValue()) {\n+                startTimeWithFile.add(indexLoc);\n+            }\n+        }\n+\n+        return startTimeWithFile;\n+    }\n+\n+    public synchronized List<File> getDirectories(final Long startTime, final Long endTime) {\n+        final List<File> selected = new ArrayList<>();\n+\n+        // An index cannot be expired if it is the latest index in the partition. As a result, we need to\n+        // separate the indexes by partition so that we can easily determine if this is the case.\n+        final Map<String, List<IndexLocation>> startTimeWithFileByStorageDirectory = flattenDirectoriesByTimestamp().stream()\n+            .collect(Collectors.groupingBy(indexLoc -> indexLoc.getPartitionName()));\n+\n+        for (final List<IndexLocation> locationList : startTimeWithFileByStorageDirectory.values()) {\n+            selected.addAll(getDirectories(startTime, endTime, locationList));\n+        }\n+\n+        return selected;\n+    }\n+\n+    public synchronized List<File> getDirectories(final Long startTime, final Long endTime, final String partitionName) {\n+        // An index cannot be expired if it is the latest index in the partition. As a result, we need to\n+        // separate the indexes by partition so that we can easily determine if this is the case.\n+        final Map<String, List<IndexLocation>> startTimeWithFileByStorageDirectory = flattenDirectoriesByTimestamp().stream()\n+            .collect(Collectors.groupingBy(indexLoc -> indexLoc.getPartitionName()));\n+\n+        final List<IndexLocation> indexLocations = startTimeWithFileByStorageDirectory.get(partitionName);\n+        if (indexLocations == null) {\n+            return Collections.emptyList();\n+        }\n+\n+        return getDirectories(startTime, endTime, indexLocations);\n+    }\n+\n+    protected static List<File> getDirectories(final Long startTime, final Long endTime, final List<IndexLocation> locations) {\n+        final List<File> selected = new ArrayList<>();\n+\n+        int overlapCount = 0;\n+        for (int i = 0; i < locations.size(); i++) {\n+            final IndexLocation indexLoc = locations.get(i);\n+            final Long indexStartTimestamp = indexLoc.getIndexStartTimestamp();\n+            if (endTime != null && indexStartTimestamp > endTime) {\n+                if (overlapCount == 0) {\n+                    // Because of how we handle index timestamps and the multi-threading, it is possible\n+                    // the we could have some overlap where Thread T1 gets an Event with start time 1,000\n+                    // for instance. Then T2 gets and Event with start time 1,002 and ends up creating a\n+                    // new index directory with a start time of 1,002. Then T1 could end up writing events\n+                    // with timestamp 1,000 to an index with a 'start time' of 1,002. Because of this,\n+                    // the index start times are approximate. To address this, we include one extra Index\n+                    // Directory based on start time, so that if we want index directories for Time Range\n+                    // 1,000 - 1,001 and have indexes 999 and 1,002 we will include the 999 and the 'overlapping'\n+                    // directory of 1,002 since it could potentially have an event with overlapping timestamp.\n+                    overlapCount++;\n+                } else {\n+                    continue;\n+                }\n+            }\n+\n+            if (startTime != null) {\n+                final Long indexEndTimestamp;\n+                if (i < locations.size() - 1) {\n+                    final IndexLocation nextIndexLoc = locations.get(i + 1);\n+                    indexEndTimestamp = nextIndexLoc.getIndexStartTimestamp();\n+                    if (indexEndTimestamp < startTime) {\n+                        continue;\n+                    }\n+                }\n+            }\n+\n+            selected.add(indexLoc.getIndexDirectory());\n+        }\n+\n+        return selected;\n+    }\n+\n+    /**\n+     * Notifies the Index Directory Manager that an Index Writer has been committed for the\n+     * given index directory. This allows the Directory Manager to know that it needs to check\n+     * the size of the index directory and not return this directory as a writable directory\n+     * any more if the size has reached the configured threshold.\n+     *\n+     * @param indexDir the directory that was written to\n+     * @return <code>true</code> if the index directory has reached its max threshold and should no\n+     *         longer be written to, <code>false</code> if the index directory is not full.\n+     */\n+    public boolean onIndexCommitted(final File indexDir) {\n+        final long indexSize = getSize(indexDir);\n+        synchronized (this) {\n+            String partitionName = null;\n+            for (final Map.Entry<String, IndexLocation> entry : activeIndices.entrySet()) {\n+                if (indexDir.equals(entry.getValue().getIndexDirectory())) {\n+                    partitionName = entry.getKey();\n+                    break;\n+                }\n+            }\n+\n+            // If the index is not the active index directory, it should no longer be written to.\n+            if (partitionName == null) {\n+                logger.debug(\"Size of Provenance Index at {} is now {}. However, was unable to find the appropriate Active Index to roll over.\", indexDir, indexSize);\n+                return true;\n+            }\n+\n+            // If the index size >= desired index size, it should no longer be written to.\n+            if (indexSize >= repoConfig.getDesiredIndexSize()) {\n+                logger.info(\"Size of Provenance Index at {} is now {}. Will close this index and roll over to a new one.\", indexDir, indexSize);\n+                activeIndices.remove(partitionName);\n+\n+                return true;\n+            }\n+\n+            // Index directory is the active index directory and has not yet exceeded the desired size.\n+            return false;\n+        }\n+    }\n+\n+    public synchronized Optional<File> getActiveIndexDirectory(final String partitionName) {\n+        final IndexLocation indexLocation = activeIndices.get(partitionName);\n+        if (indexLocation == null) {\n+            return Optional.empty();\n+        }\n+\n+        return Optional.of(indexLocation.getIndexDirectory());\n+    }\n+\n+    private long getSize(final File indexDir) {\n+        if (!indexDir.exists()) {\n+            return 0L;\n+        }\n+        if (!indexDir.isDirectory()) {\n+            throw new IllegalArgumentException(\"Must specify a directory but specified \" + indexDir);\n+        }\n+\n+        // List all files in the Index Directory.\n+        final File[] files = indexDir.listFiles();\n+        if (files == null) {\n+            return 0L;\n+        }\n+\n+        long sum = 0L;\n+        for (final File file : files) {\n+            sum += file.length();\n+        }\n+\n+        return sum;\n+    }\n+\n+    /**\n+     * Provides the File that is the directory for the index that should be written to. If there is no index yet\n+     * to be written to, or if the index has reached its max size, a new one will be created. The given {@code earliestTimestamp}\n+     * should represent the event time of the first event that will go into the index. This is used for file naming purposes so\n+     * that the appropriate directories can be looked up quickly later.\n+     *\n+     * @param earliestTimestamp the event time of the first event that will go into a new index, if a new index is created by this call.\n+     * @param partitionName the name of the partition to write to\n+     * @return the directory that should be written to\n+     */\n+    public synchronized File getWritableIndexingDirectory(final long earliestTimestamp, final String partitionName) {\n+        IndexLocation indexLoc = activeIndices.get(partitionName);\n+        if (indexLoc == null || indexLoc.isIndexFull()) {\n+            indexLoc = new IndexLocation(createIndex(earliestTimestamp, partitionName), earliestTimestamp, partitionName, repoConfig.getDesiredIndexSize());\n+            logger.debug(\"Created new Index Directory {}\", indexLoc);\n+\n+            indexLocationByTimestamp.computeIfAbsent(earliestTimestamp, t -> new ArrayList<>()).add(indexLoc);\n+            activeIndices.put(partitionName, indexLoc);\n+        }\n+\n+        return indexLoc.getIndexDirectory();\n+    }\n+\n+    private File createIndex(final long earliestTimestamp, final String partitionName) {\n+        final File storageDir = repoConfig.getStorageDirectories().entrySet().stream()\n+            .filter(e -> e.getKey().equals(partitionName))\n+            .map(Map.Entry::getValue)\n+            .findFirst()\n+            .orElseThrow(() -> new IllegalArgumentException(\"Invalid Partition: \" + partitionName));\n+        final File indexDir = new File(storageDir, \"index-\" + earliestTimestamp);\n+\n+        return indexDir;\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/IndexDirectoryManager.java",
                "sha": "09878ff9abe4c346ecd498b87a6d45988d0b1626",
                "status": "added"
            },
            {
                "additions": 90,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/IndexLocation.java",
                "changes": 90,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/IndexLocation.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/IndexLocation.java",
                "patch": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.index.lucene;\n+\n+import java.io.File;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.nifi.provenance.util.DirectoryUtils;\n+\n+public class IndexLocation {\n+    private static final long SIZE_CHECK_MILLIS = TimeUnit.SECONDS.toMillis(30L);\n+\n+    private final File indexDirectory;\n+    private final long indexStartTimestamp;\n+    private final String partitionName;\n+    private final long desiredIndexSize;\n+    private volatile long lastSizeCheckTime = System.currentTimeMillis();\n+\n+    public IndexLocation(final File indexDirectory, final long indexStartTimestamp, final String partitionName, final long desiredIndexSize) {\n+        this.indexDirectory = indexDirectory;\n+        this.indexStartTimestamp = indexStartTimestamp;\n+        this.partitionName = partitionName;\n+        this.desiredIndexSize = desiredIndexSize;\n+    }\n+\n+    public File getIndexDirectory() {\n+        return indexDirectory;\n+    }\n+\n+    public long getIndexStartTimestamp() {\n+        return indexStartTimestamp;\n+    }\n+\n+    public String getPartitionName() {\n+        return partitionName;\n+    }\n+\n+    public boolean isIndexFull() {\n+        final long now = System.currentTimeMillis();\n+        final long millisSinceLastSizeCheck = now - lastSizeCheckTime;\n+        if (millisSinceLastSizeCheck < SIZE_CHECK_MILLIS) {\n+            return false;\n+        }\n+\n+        lastSizeCheckTime = now;\n+        return DirectoryUtils.getSize(indexDirectory) >= desiredIndexSize;\n+    }\n+\n+    @Override\n+    public int hashCode() {\n+        return 31 + 41 * indexDirectory.hashCode();\n+    }\n+\n+    @Override\n+    public boolean equals(final Object obj) {\n+        if (obj == null) {\n+            return false;\n+        }\n+        if (obj == this) {\n+            return true;\n+        }\n+\n+        if (!(obj instanceof IndexLocation)) {\n+            return false;\n+        }\n+\n+        final IndexLocation other = (IndexLocation) obj;\n+        return indexDirectory.equals(other.getIndexDirectory());\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"IndexLocation[directory=\" + indexDirectory + \"]\";\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/IndexLocation.java",
                "sha": "33867c6d6a36328ab4d2143142db6c2c4b7ef24a",
                "status": "added"
            },
            {
                "additions": 47,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/IndexableDocument.java",
                "changes": 47,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/IndexableDocument.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/IndexableDocument.java",
                "patch": "@@ -0,0 +1,47 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.index.lucene;\n+\n+import java.io.File;\n+\n+import org.apache.lucene.document.Document;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n+\n+public class IndexableDocument {\n+    private final Document document;\n+    private final StorageSummary persistenceLocation;\n+    private final File indexDirectory;\n+\n+    public IndexableDocument(final Document document, final StorageSummary location, final File indexDirectory) {\n+        this.document = document;\n+        this.persistenceLocation = location;\n+        this.indexDirectory = indexDirectory;\n+    }\n+\n+    public Document getDocument() {\n+        return document;\n+    }\n+\n+    public StorageSummary getPersistenceLocation() {\n+        return persistenceLocation;\n+    }\n+\n+    public File getIndexDirectory() {\n+        return indexDirectory;\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/IndexableDocument.java",
                "sha": "1fc163f2ea889fe0b0239a98f447d7850aaebe53",
                "status": "added"
            },
            {
                "additions": 77,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/LatestEventsPerProcessorQuery.java",
                "changes": 77,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/LatestEventsPerProcessorQuery.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/LatestEventsPerProcessorQuery.java",
                "patch": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.index.lucene;\n+\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.SearchableFields;\n+import org.apache.nifi.provenance.search.Query;\n+import org.apache.nifi.provenance.search.SearchTerm;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n+import org.apache.nifi.util.RingBuffer;\n+\n+public class LatestEventsPerProcessorQuery implements CachedQuery {\n+    private static final String COMPONENT_ID_FIELD_NAME = SearchableFields.ComponentID.getSearchableFieldName();\n+    private final ConcurrentMap<String, RingBuffer<Long>> latestRecords = new ConcurrentHashMap<>();\n+\n+    @Override\n+    public void update(final ProvenanceEventRecord event, final StorageSummary storageSummary) {\n+        final String componentId = event.getComponentId();\n+        final RingBuffer<Long> ringBuffer = latestRecords.computeIfAbsent(componentId, id -> new RingBuffer<>(1000));\n+        ringBuffer.add(storageSummary.getEventId());\n+    }\n+\n+    @Override\n+    public Optional<List<Long>> evaluate(final Query query) {\n+        if (query.getMaxResults() > 1000) {\n+            // If query max results > 1000 then we know we don't have enough results. So just return empty.\n+            return Optional.empty();\n+        }\n+\n+        final List<SearchTerm> terms = query.getSearchTerms();\n+        if (terms.size() != 1) {\n+            return Optional.empty();\n+        }\n+\n+        final SearchTerm term = terms.get(0);\n+        if (!COMPONENT_ID_FIELD_NAME.equals(term.getSearchableField().getSearchableFieldName())) {\n+            return Optional.empty();\n+        }\n+\n+        if (query.getEndDate() != null || query.getStartDate() != null) {\n+            return Optional.empty();\n+        }\n+\n+        final RingBuffer<Long> ringBuffer = latestRecords.get(term.getValue());\n+        if (ringBuffer == null || ringBuffer.getSize() < query.getMaxResults()) {\n+            return Optional.empty();\n+        }\n+\n+        List<Long> eventIds = ringBuffer.asList();\n+        if (eventIds.size() > query.getMaxResults()) {\n+            eventIds = eventIds.subList(0, query.getMaxResults());\n+        }\n+\n+        return Optional.of(eventIds);\n+    }\n+\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/LatestEventsPerProcessorQuery.java",
                "sha": "73b0a14b3a1a485f30e2e001ab1ddc3a4aa152ee",
                "status": "added"
            },
            {
                "additions": 55,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/LatestEventsQuery.java",
                "changes": 55,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/LatestEventsQuery.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/LatestEventsQuery.java",
                "patch": "@@ -0,0 +1,55 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.index.lucene;\n+\n+import java.util.List;\n+import java.util.Optional;\n+\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.search.Query;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n+import org.apache.nifi.util.RingBuffer;\n+\n+public class LatestEventsQuery implements CachedQuery {\n+\n+    final RingBuffer<Long> latestRecords = new RingBuffer<>(1000);\n+\n+    @Override\n+    public void update(final ProvenanceEventRecord event, final StorageSummary storageSummary) {\n+        latestRecords.add(storageSummary.getEventId());\n+    }\n+\n+    @Override\n+    public Optional<List<Long>> evaluate(final Query query) {\n+        if (latestRecords.getSize() < query.getMaxResults()) {\n+            return Optional.empty();\n+        }\n+\n+        if (query.getSearchTerms().isEmpty() && query.getStartDate() == null && query.getEndDate() == null) {\n+            final List<Long> eventList = latestRecords.asList();\n+            if (eventList.size() > query.getMaxResults()) {\n+                return Optional.of(eventList.subList(0, query.getMaxResults()));\n+            } else {\n+                return Optional.of(eventList);\n+            }\n+        } else {\n+            return Optional.empty();\n+        }\n+    }\n+\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/LatestEventsQuery.java",
                "sha": "94cd013285befa150cf37ef3730a988d070d3878",
                "status": "added"
            },
            {
                "additions": 67,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/LuceneCacheWarmer.java",
                "changes": 67,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/LuceneCacheWarmer.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/LuceneCacheWarmer.java",
                "patch": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.index.lucene;\n+\n+import java.io.File;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.nifi.provenance.index.EventIndexSearcher;\n+import org.apache.nifi.provenance.lucene.IndexManager;\n+import org.apache.nifi.provenance.util.DirectoryUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class LuceneCacheWarmer implements Runnable {\n+    private static final Logger logger = LoggerFactory.getLogger(LuceneCacheWarmer.class);\n+\n+    private final File storageDir;\n+    private final IndexManager indexManager;\n+\n+    public LuceneCacheWarmer(final File storageDir, final IndexManager indexManager) {\n+        this.storageDir = storageDir;\n+        this.indexManager = indexManager;\n+    }\n+\n+    @Override\n+    public void run() {\n+        try {\n+            final File[] indexDirs = storageDir.listFiles(DirectoryUtils.INDEX_FILE_FILTER);\n+            if (indexDirs == null) {\n+                logger.info(\"Cannot warm Lucene Index Cache for \" + storageDir + \" because the directory could not be read\");\n+                return;\n+            }\n+\n+            logger.info(\"Beginning warming of Lucene Index Cache for \" + storageDir);\n+            final long startNanos = System.nanoTime();\n+            for (final File indexDir : indexDirs) {\n+                final long indexStartNanos = System.nanoTime();\n+\n+                final EventIndexSearcher eventSearcher = indexManager.borrowIndexSearcher(indexDir);\n+                indexManager.returnIndexSearcher(eventSearcher);\n+\n+                final long indexWarmMillis = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - indexStartNanos);\n+                logger.debug(\"Took {} ms to warm Lucene Index {}\", indexWarmMillis, indexDir);\n+            }\n+\n+            final long warmSecs = TimeUnit.NANOSECONDS.toSeconds(System.nanoTime() - startNanos);\n+            logger.info(\"Finished warming all Lucene Indexes for {} in {} seconds\", storageDir, warmSecs);\n+        } catch (final Exception e) {\n+            logger.error(\"Failed to warm Lucene Index Cache for \" + storageDir, e);\n+        }\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/LuceneCacheWarmer.java",
                "sha": "15b11b4e65e17f63ead4a12899efda3c14da7172",
                "status": "added"
            },
            {
                "additions": 737,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/LuceneEventIndex.java",
                "changes": 737,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/LuceneEventIndex.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/LuceneEventIndex.java",
                "patch": "@@ -0,0 +1,737 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.index.lucene;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.Date;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.Term;\n+import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.lucene.search.BooleanQuery;\n+import org.apache.lucene.search.TermQuery;\n+import org.apache.nifi.authorization.AccessDeniedException;\n+import org.apache.nifi.authorization.user.NiFiUser;\n+import org.apache.nifi.events.EventReporter;\n+import org.apache.nifi.provenance.AsyncLineageSubmission;\n+import org.apache.nifi.provenance.AsyncQuerySubmission;\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.RepositoryConfiguration;\n+import org.apache.nifi.provenance.SearchableFields;\n+import org.apache.nifi.provenance.StandardLineageResult;\n+import org.apache.nifi.provenance.StandardQueryResult;\n+import org.apache.nifi.provenance.authorization.EventAuthorizer;\n+import org.apache.nifi.provenance.authorization.EventTransformer;\n+import org.apache.nifi.provenance.index.EventIndex;\n+import org.apache.nifi.provenance.index.EventIndexSearcher;\n+import org.apache.nifi.provenance.index.EventIndexWriter;\n+import org.apache.nifi.provenance.lineage.ComputeLineageSubmission;\n+import org.apache.nifi.provenance.lineage.LineageComputationType;\n+import org.apache.nifi.provenance.lucene.IndexManager;\n+import org.apache.nifi.provenance.lucene.LuceneUtil;\n+import org.apache.nifi.provenance.search.Query;\n+import org.apache.nifi.provenance.search.QuerySubmission;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n+import org.apache.nifi.provenance.store.EventStore;\n+import org.apache.nifi.provenance.util.DirectoryUtils;\n+import org.apache.nifi.provenance.util.NamedThreadFactory;\n+import org.apache.nifi.reporting.Severity;\n+import org.apache.nifi.util.file.FileUtils;\n+import org.apache.nifi.util.timebuffer.LongEntityAccess;\n+import org.apache.nifi.util.timebuffer.TimedBuffer;\n+import org.apache.nifi.util.timebuffer.TimestampedLong;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+\n+public class LuceneEventIndex implements EventIndex {\n+    private static final Logger logger = LoggerFactory.getLogger(LuceneEventIndex.class);\n+    private static final String EVENT_CATEGORY = \"Provenance Repository\";\n+\n+    public static final int MAX_UNDELETED_QUERY_RESULTS = 10;\n+    public static final int MAX_DELETE_INDEX_WAIT_SECONDS = 30;\n+    public static final int MAX_LINEAGE_NODES = 1000;\n+    public static final int MAX_INDEX_THREADS = 100;\n+\n+    private final ConcurrentMap<String, AsyncQuerySubmission> querySubmissionMap = new ConcurrentHashMap<>();\n+    private final ConcurrentMap<String, AsyncLineageSubmission> lineageSubmissionMap = new ConcurrentHashMap<>();\n+    private final BlockingQueue<StoredDocument> documentQueue = new LinkedBlockingQueue<>(1000);\n+    private final List<EventIndexTask> indexTasks = Collections.synchronizedList(new ArrayList<>());\n+    private final ExecutorService queryExecutor;\n+    private final ExecutorService indexExecutor;\n+    private final RepositoryConfiguration config;\n+    private final IndexManager indexManager;\n+    private final ConvertEventToLuceneDocument eventConverter;\n+    private final IndexDirectoryManager directoryManager;\n+    private volatile boolean closed = false;\n+\n+    private final TimedBuffer<TimestampedLong> queuePauseNanos = new TimedBuffer<>(TimeUnit.SECONDS, 300, new LongEntityAccess());\n+    private final TimedBuffer<TimestampedLong> eventsIndexed = new TimedBuffer<>(TimeUnit.SECONDS, 300, new LongEntityAccess());\n+    private final AtomicLong eventCount = new AtomicLong(0L);\n+    private final EventReporter eventReporter;\n+\n+    private final List<CachedQuery> cachedQueries = new ArrayList<>();\n+\n+    private ScheduledExecutorService maintenanceExecutor; // effectively final\n+    private ScheduledExecutorService cacheWarmerExecutor;\n+    private EventStore eventStore;\n+\n+    public LuceneEventIndex(final RepositoryConfiguration config, final IndexManager indexManager, final EventReporter eventReporter) {\n+        this(config, indexManager, EventIndexTask.DEFAULT_MAX_EVENTS_PER_COMMIT, eventReporter);\n+    }\n+\n+    public LuceneEventIndex(final RepositoryConfiguration config, final IndexManager indexManager, final int maxEventsPerCommit, final EventReporter eventReporter) {\n+        this.eventReporter = eventReporter;\n+        queryExecutor = Executors.newFixedThreadPool(config.getQueryThreadPoolSize(), new NamedThreadFactory(\"Provenance Query\"));\n+        indexExecutor = Executors.newFixedThreadPool(config.getIndexThreadPoolSize(), new NamedThreadFactory(\"Index Provenance Events\"));\n+        cacheWarmerExecutor = Executors.newScheduledThreadPool(config.getStorageDirectories().size(), new NamedThreadFactory(\"Warm Lucene Index\", true));\n+        directoryManager = new IndexDirectoryManager(config);\n+\n+        // Limit number of indexing threads to 100. When we restore the repository on restart,\n+        // we have to re-index up to MAX_THREADS * MAX_DOCUMENTS_PER_THREADS events prior to\n+        // the last event that the index holds. This is done because we could have that many\n+        // events 'in flight', waiting to be indexed when the last index writer was committed,\n+        // so even though the index says the largest event ID is 1,000,000 for instance, Event\n+        // with ID 999,999 may still not have been indexed because another thread was in the\n+        // process of writing the event to the index.\n+        final int configuredIndexPoolSize = config.getIndexThreadPoolSize();\n+        final int numIndexThreads;\n+        if (configuredIndexPoolSize > MAX_INDEX_THREADS) {\n+            logger.warn(\"The Provenance Repository is configured to perform indexing of events using {} threads. This number exceeds the maximum allowable number of threads, which is {}. \"\n+                + \"Will proceed using {} threads. This value is limited because the performance of indexing will decrease and startup times will increase when setting this value too high.\",\n+                configuredIndexPoolSize, MAX_INDEX_THREADS, MAX_INDEX_THREADS);\n+            numIndexThreads = MAX_INDEX_THREADS;\n+        } else {\n+            numIndexThreads = configuredIndexPoolSize;\n+        }\n+\n+        for (int i = 0; i < numIndexThreads; i++) {\n+            final EventIndexTask task = new EventIndexTask(documentQueue, config, indexManager, directoryManager, maxEventsPerCommit, eventReporter);\n+            indexTasks.add(task);\n+            indexExecutor.submit(task);\n+        }\n+        this.config = config;\n+        this.indexManager = indexManager;\n+        this.eventConverter = new ConvertEventToLuceneDocument(config.getSearchableFields(), config.getSearchableAttributes());\n+    }\n+\n+    @Override\n+    public void initialize(final EventStore eventStore) {\n+        this.eventStore = eventStore;\n+        directoryManager.initialize();\n+\n+        maintenanceExecutor = Executors.newScheduledThreadPool(1, new NamedThreadFactory(\"Provenance Repository Maintenance\"));\n+        maintenanceExecutor.scheduleWithFixedDelay(() -> performMaintenance(), 1, 1, TimeUnit.MINUTES);\n+        maintenanceExecutor.scheduleWithFixedDelay(new RemoveExpiredQueryResults(), 30, 30, TimeUnit.SECONDS);\n+\n+        cachedQueries.add(new LatestEventsQuery());\n+        cachedQueries.add(new LatestEventsPerProcessorQuery());\n+\n+        final Optional<Integer> warmCacheMinutesOption = config.getWarmCacheFrequencyMinutes();\n+        if (warmCacheMinutesOption.isPresent() && warmCacheMinutesOption.get() > 0) {\n+            for (final File storageDir : config.getStorageDirectories().values()) {\n+                final int minutes = warmCacheMinutesOption.get();\n+                cacheWarmerExecutor.scheduleWithFixedDelay(new LuceneCacheWarmer(storageDir, indexManager), 1, minutes, TimeUnit.MINUTES);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public long getMinimumEventIdToReindex(final String partitionName) {\n+        return Math.max(0, getMaxEventId(partitionName) - EventIndexTask.MAX_DOCUMENTS_PER_THREAD * LuceneEventIndex.MAX_INDEX_THREADS);\n+    }\n+\n+    protected IndexDirectoryManager getDirectoryManager() {\n+        return directoryManager;\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        closed = true;\n+        queryExecutor.shutdownNow();\n+        indexExecutor.shutdown();\n+        cacheWarmerExecutor.shutdown();\n+\n+        if (maintenanceExecutor != null) {\n+            maintenanceExecutor.shutdown();\n+        }\n+\n+        for (final EventIndexTask task : indexTasks) {\n+            task.shutdown();\n+        }\n+    }\n+\n+    long getMaxEventId(final String partitionName) {\n+        final List<File> allDirectories = getDirectoryManager().getDirectories(0L, Long.MAX_VALUE, partitionName);\n+        if (allDirectories.isEmpty()) {\n+            return -1L;\n+        }\n+\n+        Collections.sort(allDirectories, DirectoryUtils.NEWEST_INDEX_FIRST);\n+\n+        for (final File directory : allDirectories) {\n+            final EventIndexSearcher searcher;\n+            try {\n+                searcher = indexManager.borrowIndexSearcher(directory);\n+            } catch (final IOException ioe) {\n+                logger.warn(\"Unable to read from Index Directory {}. Will assume that the index is incomplete and not consider this index when determining max event ID\", directory);\n+                continue;\n+            }\n+\n+            try {\n+                final IndexReader reader = searcher.getIndexSearcher().getIndexReader();\n+                final int maxDocId = reader.maxDoc() - 1;\n+                final Document document = reader.document(maxDocId);\n+                final long eventId = document.getField(SearchableFields.Identifier.getSearchableFieldName()).numericValue().longValue();\n+                logger.info(\"Determined that Max Event ID indexed for Partition {} is approximately {} based on index {}\", partitionName, eventId, directory);\n+                return eventId;\n+            } catch (final IOException ioe) {\n+                logger.warn(\"Unable to search Index Directory {}. Will assume that the index is incomplete and not consider this index when determining max event ID\", directory, ioe);\n+            } finally {\n+                indexManager.returnIndexSearcher(searcher);\n+            }\n+        }\n+\n+        return -1L;\n+    }\n+\n+    @Override\n+    public void reindexEvents(final Map<ProvenanceEventRecord, StorageSummary> events) {\n+        final EventIndexTask indexTask = new EventIndexTask(documentQueue, config, indexManager, directoryManager, EventIndexTask.DEFAULT_MAX_EVENTS_PER_COMMIT, eventReporter);\n+\n+        File lastIndexDir = null;\n+        long lastEventTime = -2L;\n+\n+        final List<IndexableDocument> indexableDocs = new ArrayList<>(events.size());\n+        for (final Map.Entry<ProvenanceEventRecord, StorageSummary> entry : events.entrySet()) {\n+            final ProvenanceEventRecord event = entry.getKey();\n+            final StorageSummary summary = entry.getValue();\n+\n+            for (final CachedQuery cachedQuery : cachedQueries) {\n+                cachedQuery.update(event, summary);\n+            }\n+\n+            final Document document = eventConverter.convert(event, summary);\n+            if (document == null) {\n+                logger.debug(\"Received Provenance Event {} to index but it contained no information that should be indexed, so skipping it\", event);\n+            } else {\n+                final File indexDir;\n+                if (event.getEventTime() == lastEventTime) {\n+                    indexDir = lastIndexDir;\n+                } else {\n+                    final List<File> files = getDirectoryManager().getDirectories(event.getEventTime(), null);\n+                    indexDir = files.isEmpty() ? null : files.get(0);\n+                    lastIndexDir = indexDir;\n+                }\n+\n+                final IndexableDocument doc = new IndexableDocument(document, summary, indexDir);\n+                indexableDocs.add(doc);\n+            }\n+        }\n+\n+        try {\n+            indexTask.reIndex(indexableDocs, CommitPreference.PREVENT_COMMIT);\n+        } catch (final IOException ioe) {\n+            logger.error(\"Failed to reindex some Provenance Events\", ioe);\n+            eventReporter.reportEvent(Severity.ERROR, EVENT_CATEGORY, \"Failed to re-index some Provenance Events. \"\n+                + \"Some Provenance Events may not be available for querying. See logs for more information.\");\n+        }\n+    }\n+\n+    @Override\n+    public void commitChanges(final String partitionName) throws IOException {\n+        final Optional<File> indexDir = directoryManager.getActiveIndexDirectory(partitionName);\n+        if (indexDir.isPresent()) {\n+            final EventIndexWriter eventIndexWriter = indexManager.borrowIndexWriter(indexDir.get());\n+            try {\n+                eventIndexWriter.commit();\n+            } finally {\n+                indexManager.returnIndexWriter(eventIndexWriter, false, false);\n+            }\n+        }\n+    }\n+\n+    protected void addEvent(final ProvenanceEventRecord event, final StorageSummary location) {\n+        for (final CachedQuery cachedQuery : cachedQueries) {\n+            cachedQuery.update(event, location);\n+        }\n+\n+        final Document document = eventConverter.convert(event, location);\n+        if (document == null) {\n+            logger.debug(\"Received Provenance Event {} to index but it contained no information that should be indexed, so skipping it\", event);\n+        } else {\n+            final StoredDocument doc = new StoredDocument(document, location);\n+            boolean added = false;\n+            while (!added && !closed) {\n+\n+                added = documentQueue.offer(doc);\n+                if (!added) {\n+                    final long start = System.nanoTime();\n+                    try {\n+                        added = documentQueue.offer(doc, 1, TimeUnit.SECONDS);\n+                    } catch (final InterruptedException e) {\n+                        Thread.currentThread().interrupt();\n+                        logger.warn(\"Interrupted while attempting to enqueue Provenance Event for indexing; this event will not be indexed\");\n+                        return;\n+                    }\n+                    final long nanos = System.nanoTime() - start;\n+                    queuePauseNanos.add(new TimestampedLong(nanos));\n+                }\n+\n+                if (added) {\n+                    final long totalEventCount = eventCount.incrementAndGet();\n+                    if (totalEventCount % 1_000_000 == 0 && logger.isDebugEnabled()) {\n+                        incrementAndReportStats();\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+    private void incrementAndReportStats() {\n+        final long fiveMinutesAgo = System.currentTimeMillis() - TimeUnit.MINUTES.toMillis(5);\n+        final TimestampedLong nanosLastFive = queuePauseNanos.getAggregateValue(fiveMinutesAgo);\n+        if (nanosLastFive == null) {\n+            return;\n+        }\n+\n+        final TimestampedLong eventsLast5 = eventsIndexed.getAggregateValue(fiveMinutesAgo);\n+        if (eventsLast5 == null) {\n+            return;\n+        }\n+\n+        final long numEventsLast5 = eventsLast5.getValue();\n+\n+        final long millis = TimeUnit.NANOSECONDS.toMillis(nanosLastFive.getValue());\n+        logger.debug(\"In the last 5 minutes, have spent {} CPU-millis waiting to enqueue events for indexing and have indexed {} events ({} since NiFi started)\",\n+            millis, numEventsLast5, eventCount.get());\n+    }\n+\n+    @Override\n+    public void addEvents(final Map<ProvenanceEventRecord, StorageSummary> events) {\n+        eventsIndexed.add(new TimestampedLong((long) events.size()));\n+\n+        for (final Map.Entry<ProvenanceEventRecord, StorageSummary> entry : events.entrySet()) {\n+            addEvent(entry.getKey(), entry.getValue());\n+        }\n+    }\n+\n+\n+    @Override\n+    public ComputeLineageSubmission submitLineageComputation(final long eventId, final NiFiUser user, final EventAuthorizer eventAuthorizer) {\n+        final Optional<ProvenanceEventRecord> eventOption;\n+        try {\n+            eventOption = eventStore.getEvent(eventId);\n+        } catch (final Exception e) {\n+            logger.error(\"Failed to retrieve Provenance Event with ID \" + eventId + \" to calculate data lineage due to: \" + e, e);\n+            final AsyncLineageSubmission result = new AsyncLineageSubmission(LineageComputationType.FLOWFILE_LINEAGE, eventId, Collections.<String> emptySet(), 1, user.getIdentity());\n+            result.getResult().setError(\"Failed to retrieve Provenance Event with ID \" + eventId + \". See logs for more information.\");\n+            return result;\n+        }\n+\n+        if (!eventOption.isPresent()) {\n+            final AsyncLineageSubmission result = new AsyncLineageSubmission(LineageComputationType.FLOWFILE_LINEAGE, eventId, Collections.<String> emptySet(), 1, user.getIdentity());\n+            result.getResult().setError(\"Could not find Provenance Event with ID \" + eventId);\n+            lineageSubmissionMap.put(result.getLineageIdentifier(), result);\n+            return result;\n+        }\n+\n+        final ProvenanceEventRecord event = eventOption.get();\n+        return submitLineageComputation(Collections.singleton(event.getFlowFileUuid()), user, eventAuthorizer, LineageComputationType.FLOWFILE_LINEAGE,\n+            eventId, event.getLineageStartDate(), Long.MAX_VALUE);\n+    }\n+\n+\n+    private ComputeLineageSubmission submitLineageComputation(final Collection<String> flowFileUuids, final NiFiUser user, final EventAuthorizer eventAuthorizer,\n+        final LineageComputationType computationType, final Long eventId, final long startTimestamp, final long endTimestamp) {\n+\n+        final List<File> indexDirs = directoryManager.getDirectories(startTimestamp, endTimestamp);\n+        final AsyncLineageSubmission submission = new AsyncLineageSubmission(computationType, eventId, flowFileUuids, indexDirs.size(), user.getIdentity());\n+        lineageSubmissionMap.put(submission.getLineageIdentifier(), submission);\n+\n+        final BooleanQuery lineageQuery = buildLineageQuery(flowFileUuids);\n+        final List<File> indexDirectories = directoryManager.getDirectories(startTimestamp, endTimestamp);\n+        if (indexDirectories.isEmpty()) {\n+            submission.getResult().update(Collections.emptyList(), 0L);\n+        } else {\n+            Collections.sort(indexDirectories, DirectoryUtils.OLDEST_INDEX_FIRST);\n+\n+            for (final File indexDir : indexDirectories) {\n+                queryExecutor.submit(new QueryTask(lineageQuery, submission.getResult(), MAX_LINEAGE_NODES, indexManager, indexDir,\n+                    eventStore, eventAuthorizer, EventTransformer.PLACEHOLDER_TRANSFORMER));\n+            }\n+        }\n+\n+        // Some computations will complete very quickly. In this case, we don't want to wait\n+        // for the client to submit a second query to obtain the result. Instead, we want to just\n+        // wait some short period of time for the computation to complete before returning the submission.\n+        try {\n+            submission.getResult().awaitCompletion(500, TimeUnit.MILLISECONDS);\n+        } catch (final InterruptedException ie) {\n+            Thread.currentThread().interrupt();\n+        }\n+\n+        return submission;\n+    }\n+\n+    private BooleanQuery buildLineageQuery(final Collection<String> flowFileUuids) {\n+        // Create a query for all Events related to the FlowFiles of interest. We do this by adding all ID's as\n+        // \"SHOULD\" clauses and then setting the minimum required to 1.\n+        final BooleanQuery lineageQuery;\n+        if (flowFileUuids == null || flowFileUuids.isEmpty()) {\n+            lineageQuery = null;\n+        } else {\n+            lineageQuery = new BooleanQuery();\n+            for (final String flowFileUuid : flowFileUuids) {\n+                lineageQuery.add(new TermQuery(new Term(SearchableFields.FlowFileUUID.getSearchableFieldName(), flowFileUuid)), Occur.SHOULD);\n+            }\n+            lineageQuery.setMinimumNumberShouldMatch(1);\n+        }\n+\n+        return lineageQuery;\n+    }\n+\n+    @Override\n+    public QuerySubmission submitQuery(final Query query, final EventAuthorizer authorizer, final String userId) {\n+        validate(query);\n+\n+        // Check if we have any cached queries first that can give us the answer\n+        for (final CachedQuery cachedQuery : cachedQueries) {\n+            final Optional<List<Long>> eventIdListOption = cachedQuery.evaluate(query);\n+            if (eventIdListOption.isPresent()) {\n+                final AsyncQuerySubmission submission = new AsyncQuerySubmission(query, 1, userId);\n+                querySubmissionMap.put(query.getIdentifier(), submission);\n+\n+                final List<Long> eventIds = eventIdListOption.get();\n+\n+                queryExecutor.submit(() -> {\n+                    List<ProvenanceEventRecord> events;\n+                    try {\n+                        events = eventStore.getEvents(eventIds, authorizer, EventTransformer.EMPTY_TRANSFORMER);\n+                        submission.getResult().update(events, eventIds.size());\n+                    } catch (final Exception e) {\n+                        submission.getResult().setError(\"Failed to retrieve Provenance Events from store; see logs for more details\");\n+                        logger.error(\"Failed to retrieve Provenance Events from store\", e);\n+                    }\n+                });\n+\n+                // There are some queries that are optimized and will complete very quickly. As a result,\n+                // we don't want to wait for the client to issue a second request, so we will give the query\n+                // up to 500 milliseconds to complete before running.\n+                try {\n+                    submission.getResult().awaitCompletion(500, TimeUnit.MILLISECONDS);\n+                } catch (final InterruptedException e) {\n+                    Thread.currentThread().interrupt();\n+                }\n+\n+                return submission;\n+            }\n+        }\n+\n+        final List<File> indexDirectories = directoryManager.getDirectories(\n+            query.getStartDate() == null ? null : query.getStartDate().getTime(),\n+            query.getEndDate() == null ? null : query.getEndDate().getTime());\n+\n+        final AsyncQuerySubmission submission = new AsyncQuerySubmission(query, indexDirectories.size(), userId);\n+        querySubmissionMap.put(query.getIdentifier(), submission);\n+\n+        final org.apache.lucene.search.Query luceneQuery = LuceneUtil.convertQuery(query);\n+        logger.debug(\"Submitting query {} with identifier {} against index directories {}\", luceneQuery, query.getIdentifier(), indexDirectories);\n+\n+        if (indexDirectories.isEmpty()) {\n+            submission.getResult().update(Collections.emptyList(), 0L);\n+        } else {\n+            Collections.sort(indexDirectories, DirectoryUtils.NEWEST_INDEX_FIRST);\n+\n+            for (final File indexDir : indexDirectories) {\n+                queryExecutor.submit(new QueryTask(luceneQuery, submission.getResult(), query.getMaxResults(), indexManager, indexDir,\n+                    eventStore, authorizer, EventTransformer.EMPTY_TRANSFORMER));\n+            }\n+        }\n+\n+        // There are some queries that are optimized and will complete very quickly. As a result,\n+        // we don't want to wait for the client to issue a second request, so we will give the query\n+        // up to 500 milliseconds to complete before running.\n+        try {\n+            submission.getResult().awaitCompletion(500, TimeUnit.MILLISECONDS);\n+        } catch (final InterruptedException e) {\n+            Thread.currentThread().interrupt();\n+        }\n+\n+        return submission;\n+    }\n+\n+\n+    @Override\n+    public ComputeLineageSubmission submitLineageComputation(final String flowFileUuid, final NiFiUser user, final EventAuthorizer eventAuthorizer) {\n+        return submitLineageComputation(Collections.singleton(flowFileUuid), user, eventAuthorizer, LineageComputationType.FLOWFILE_LINEAGE, null, 0L, Long.MAX_VALUE);\n+    }\n+\n+    @Override\n+    public ComputeLineageSubmission submitExpandChildren(final long eventId, final NiFiUser user, final EventAuthorizer authorizer) {\n+        final String userId = user.getIdentity();\n+\n+        try {\n+            final Optional<ProvenanceEventRecord> eventOption = eventStore.getEvent(eventId);\n+            if (!eventOption.isPresent()) {\n+                final AsyncLineageSubmission submission = new AsyncLineageSubmission(LineageComputationType.EXPAND_CHILDREN, eventId, Collections.emptyList(), 1, userId);\n+                lineageSubmissionMap.put(submission.getLineageIdentifier(), submission);\n+                submission.getResult().update(Collections.emptyList(), 0L);\n+                return submission;\n+            }\n+\n+            final ProvenanceEventRecord event = eventOption.get();\n+            switch (event.getEventType()) {\n+                case CLONE:\n+                case FORK:\n+                case JOIN:\n+                case REPLAY: {\n+                    return submitLineageComputation(event.getChildUuids(), user, authorizer, LineageComputationType.EXPAND_CHILDREN,\n+                        eventId, event.getEventTime(), Long.MAX_VALUE);\n+                }\n+                default: {\n+                    final AsyncLineageSubmission submission = new AsyncLineageSubmission(LineageComputationType.EXPAND_CHILDREN,\n+                        eventId, Collections.<String> emptyList(), 1, userId);\n+\n+                    lineageSubmissionMap.put(submission.getLineageIdentifier(), submission);\n+                    submission.getResult().setError(\"Event ID \" + eventId + \" indicates an event of type \" + event.getEventType() + \" so its children cannot be expanded\");\n+                    return submission;\n+                }\n+            }\n+        } catch (final Exception e) {\n+            final AsyncLineageSubmission submission = new AsyncLineageSubmission(LineageComputationType.EXPAND_CHILDREN,\n+                eventId, Collections.<String> emptyList(), 1, userId);\n+            lineageSubmissionMap.put(submission.getLineageIdentifier(), submission);\n+            submission.getResult().setError(\"Failed to expand children for lineage of event with ID \" + eventId + \" due to: \" + e);\n+            return submission;\n+        }\n+    }\n+\n+    @Override\n+    public ComputeLineageSubmission submitExpandParents(final long eventId, final NiFiUser user, final EventAuthorizer authorizer) {\n+        final String userId = user.getIdentity();\n+\n+        try {\n+            final Optional<ProvenanceEventRecord> eventOption = eventStore.getEvent(eventId);\n+            if (!eventOption.isPresent()) {\n+                final AsyncLineageSubmission submission = new AsyncLineageSubmission(LineageComputationType.EXPAND_PARENTS, eventId, Collections.emptyList(), 1, userId);\n+                lineageSubmissionMap.put(submission.getLineageIdentifier(), submission);\n+                submission.getResult().update(Collections.emptyList(), 0L);\n+                return submission;\n+            }\n+\n+            final ProvenanceEventRecord event = eventOption.get();\n+            switch (event.getEventType()) {\n+                case JOIN:\n+                case FORK:\n+                case CLONE:\n+                case REPLAY: {\n+                    return submitLineageComputation(event.getParentUuids(), user, authorizer, LineageComputationType.EXPAND_PARENTS,\n+                        eventId, event.getLineageStartDate(), event.getEventTime());\n+                }\n+                default: {\n+                    final AsyncLineageSubmission submission = new AsyncLineageSubmission(LineageComputationType.EXPAND_PARENTS,\n+                        eventId, Collections.<String> emptyList(), 1, userId);\n+\n+                    lineageSubmissionMap.put(submission.getLineageIdentifier(), submission);\n+                    submission.getResult().setError(\"Event ID \" + eventId + \" indicates an event of type \" + event.getEventType() + \" so its parents cannot be expanded\");\n+                    return submission;\n+                }\n+            }\n+        } catch (final Exception e) {\n+            final AsyncLineageSubmission submission = new AsyncLineageSubmission(LineageComputationType.EXPAND_PARENTS,\n+                eventId, Collections.<String> emptyList(), 1, userId);\n+            lineageSubmissionMap.put(submission.getLineageIdentifier(), submission);\n+\n+            submission.getResult().setError(\"Failed to expand parents for lineage of event with ID \" + eventId + \" due to: \" + e);\n+            return submission;\n+        }\n+    }\n+\n+    @Override\n+    public AsyncLineageSubmission retrieveLineageSubmission(final String lineageIdentifier, final NiFiUser user) {\n+        final AsyncLineageSubmission submission = lineageSubmissionMap.get(lineageIdentifier);\n+        final String userId = submission.getSubmitterIdentity();\n+\n+        if (user == null && userId == null) {\n+            return submission;\n+        }\n+\n+        if (user == null) {\n+            throw new AccessDeniedException(\"Cannot retrieve Provenance Lineage Submission because no user id was provided\");\n+        }\n+\n+        if (userId == null || userId.equals(user.getIdentity())) {\n+            return submission;\n+        }\n+\n+        throw new AccessDeniedException(\"Cannot retrieve Provenance Lineage Submission because \" + user.getIdentity() + \" is not the user who submitted the request\");\n+    }\n+\n+    @Override\n+    public QuerySubmission retrieveQuerySubmission(final String queryIdentifier, final NiFiUser user) {\n+        final QuerySubmission submission = querySubmissionMap.get(queryIdentifier);\n+\n+        final String userId = submission.getSubmitterIdentity();\n+\n+        if (user == null && userId == null) {\n+            return submission;\n+        }\n+\n+        if (user == null) {\n+            throw new AccessDeniedException(\"Cannot retrieve Provenance Query Submission because no user id was provided\");\n+        }\n+\n+        if (userId == null || userId.equals(user.getIdentity())) {\n+            return submission;\n+        }\n+\n+        throw new AccessDeniedException(\"Cannot retrieve Provenance Query Submission because \" + user.getIdentity() + \" is not the user who submitted the request\");\n+    }\n+\n+    @Override\n+    public long getSize() {\n+        long total = 0;\n+        for (final File file : directoryManager.getDirectories(null, null)) {\n+            total += DirectoryUtils.getSize(file);\n+        }\n+        return total;\n+    }\n+\n+    private void validate(final Query query) {\n+        final int numQueries = querySubmissionMap.size();\n+        if (numQueries > MAX_UNDELETED_QUERY_RESULTS) {\n+            throw new IllegalStateException(\"Cannot process query because there are currently \" + numQueries + \" queries whose results have not \"\n+                + \"been deleted due to poorly behaving clients not issuing DELETE requests. Please try again later.\");\n+        }\n+\n+        if (query.getEndDate() != null && query.getStartDate() != null && query.getStartDate().getTime() > query.getEndDate().getTime()) {\n+            throw new IllegalArgumentException(\"Query End Time cannot be before Query Start Time\");\n+        }\n+    }\n+\n+    void performMaintenance() {\n+        try {\n+            final List<ProvenanceEventRecord> firstEvents = eventStore.getEvents(0, 1);\n+            if (firstEvents.isEmpty()) {\n+                return;\n+            }\n+\n+            final ProvenanceEventRecord firstEvent = firstEvents.get(0);\n+            final long earliestEventTime = firstEvent.getEventTime();\n+            logger.debug(\"First Event Time is {} ({}) with Event ID {}; will delete any Lucene Index that is older than this\",\n+                earliestEventTime, new Date(earliestEventTime), firstEvent.getEventId());\n+            final List<File> indicesBeforeEarliestEvent = directoryManager.getDirectoriesBefore(earliestEventTime);\n+\n+            for (final File index : indicesBeforeEarliestEvent) {\n+                logger.debug(\"Index directory {} is now expired. Attempting to remove index\", index);\n+                tryDeleteIndex(index);\n+            }\n+        } catch (final Exception e) {\n+            logger.error(\"Failed to perform background maintenance procedures\", e);\n+            eventReporter.reportEvent(Severity.ERROR, EVENT_CATEGORY, \"Failed to perform maintenance of Provenance Repository. See logs for more information.\");\n+        }\n+    }\n+\n+    protected boolean tryDeleteIndex(final File indexDirectory) {\n+        final long startNanos = System.nanoTime();\n+        boolean removed = false;\n+        while (!removed && System.nanoTime() - startNanos < TimeUnit.SECONDS.toNanos(MAX_DELETE_INDEX_WAIT_SECONDS)) {\n+            removed = indexManager.removeIndex(indexDirectory);\n+\n+            if (!removed) {\n+                try {\n+                    Thread.sleep(5000L);\n+                } catch (final InterruptedException ie) {\n+                    logger.debug(\"Interrupted when trying to remove index {} from IndexManager; will not remove index\", indexDirectory);\n+                    Thread.currentThread().interrupt();\n+                    return false;\n+                }\n+            }\n+        }\n+\n+        if (removed) {\n+            try {\n+                FileUtils.deleteFile(indexDirectory, true);\n+                logger.debug(\"Successfully deleted directory {}\", indexDirectory);\n+            } catch (final IOException e) {\n+                logger.warn(\"The Lucene Index located at \" + indexDirectory + \" has expired and contains no Provenance Events that still exist in the respository. \"\n+                    + \"However, the directory could not be deleted.\", e);\n+            }\n+\n+            directoryManager.deleteDirectory(indexDirectory);\n+            logger.info(\"Successfully removed expired Lucene Index {}\", indexDirectory);\n+        } else {\n+            logger.warn(\"The Lucene Index located at {} has expired and contains no Provenance Events that still exist in the respository. \"\n+                + \"However, the directory could not be deleted because it is still actively being used. Will continue to try to delete \"\n+                + \"in a subsequent maintenance cycle.\", indexDirectory);\n+        }\n+\n+        return removed;\n+    }\n+\n+    private class RemoveExpiredQueryResults implements Runnable {\n+        @Override\n+        public void run() {\n+            try {\n+                final Date now = new Date();\n+\n+                final Iterator<Map.Entry<String, AsyncQuerySubmission>> queryIterator = querySubmissionMap.entrySet().iterator();\n+                while (queryIterator.hasNext()) {\n+                    final Map.Entry<String, AsyncQuerySubmission> entry = queryIterator.next();\n+\n+                    final StandardQueryResult result = entry.getValue().getResult();\n+                    if (entry.getValue().isCanceled() || result.isFinished() && result.getExpiration().before(now)) {\n+                        queryIterator.remove();\n+                    }\n+                }\n+\n+                final Iterator<Map.Entry<String, AsyncLineageSubmission>> lineageIterator = lineageSubmissionMap.entrySet().iterator();\n+                while (lineageIterator.hasNext()) {\n+                    final Map.Entry<String, AsyncLineageSubmission> entry = lineageIterator.next();\n+\n+                    final StandardLineageResult result = entry.getValue().getResult();\n+                    if (entry.getValue().isCanceled() || result.isFinished() && result.getExpiration().before(now)) {\n+                        lineageIterator.remove();\n+                    }\n+                }\n+            } catch (final Exception e) {\n+                logger.error(\"Failed to expire Provenance Query Results due to {}\", e.toString());\n+                logger.error(\"\", e);\n+            }\n+        }\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/LuceneEventIndex.java",
                "sha": "a58340343b898908b246b5cce031f1b0589013df",
                "status": "added"
            },
            {
                "additions": 208,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/QueryTask.java",
                "changes": 208,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/QueryTask.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/QueryTask.java",
                "patch": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.index.lucene;\n+\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.search.Query;\n+import org.apache.lucene.search.TopDocs;\n+import org.apache.nifi.provenance.ProgressiveResult;\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.SearchableFields;\n+import org.apache.nifi.provenance.authorization.EventAuthorizer;\n+import org.apache.nifi.provenance.authorization.EventTransformer;\n+import org.apache.nifi.provenance.index.EventIndexSearcher;\n+import org.apache.nifi.provenance.index.SearchFailedException;\n+import org.apache.nifi.provenance.lucene.IndexManager;\n+import org.apache.nifi.provenance.store.EventStore;\n+import org.apache.nifi.util.Tuple;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class QueryTask implements Runnable {\n+    private static final Logger logger = LoggerFactory.getLogger(QueryTask.class);\n+    private static final Set<String> LUCENE_FIELDS_TO_LOAD = Collections.singleton(SearchableFields.Identifier.getSearchableFieldName());\n+\n+    private final Query query;\n+    private final ProgressiveResult queryResult;\n+    private final int maxResults;\n+    private final IndexManager indexManager;\n+    private final File indexDir;\n+    private final EventStore eventStore;\n+    private final EventAuthorizer authorizer;\n+    private final EventTransformer transformer;\n+\n+    public QueryTask(final Query query, final ProgressiveResult result, final int maxResults, final IndexManager indexManager,\n+        final File indexDir, final EventStore eventStore, final EventAuthorizer authorizer,\n+        final EventTransformer unauthorizedTransformer) {\n+        this.query = query;\n+        this.queryResult = result;\n+        this.maxResults = maxResults;\n+        this.indexManager = indexManager;\n+        this.indexDir = indexDir;\n+        this.eventStore = eventStore;\n+        this.authorizer = authorizer;\n+        this.transformer = unauthorizedTransformer;\n+    }\n+\n+    @Override\n+    public void run() {\n+        if (queryResult.getTotalHitCount() >= maxResults) {\n+            logger.debug(\"Will not query lucene index {} because maximum results have already been obtained\", indexDir);\n+            queryResult.update(Collections.emptyList(), 0L);\n+            return;\n+        }\n+\n+        if (queryResult.isFinished()) {\n+            logger.debug(\"Will not query lucene index {} because the query is already finished\", indexDir);\n+            return;\n+        }\n+\n+\n+        final long borrowStart = System.nanoTime();\n+        final EventIndexSearcher searcher;\n+        try {\n+            searcher = indexManager.borrowIndexSearcher(indexDir);\n+        } catch (final FileNotFoundException fnfe) {\n+            // We do not consider this an error because it may well just be the case that the event index has aged off and\n+            // been deleted or that we've just created the index and haven't yet committed the writer. So instead, we just\n+            // update the result ot indicate that this index search is complete with no results.\n+            queryResult.update(Collections.emptyList(), 0);\n+\n+            // nothing has been indexed yet, or the data has already aged off\n+            logger.info(\"Attempted to search Provenance Index {} but could not find the directory or the directory did not contain a valid Lucene index. \"\n+                + \"This usually indicates that either the index was just created and hasn't fully been initialized, or that the index was recently aged off.\", indexDir);\n+            return;\n+        } catch (final IOException ioe) {\n+            queryResult.setError(\"Failed to query index \" + indexDir + \"; see logs for more details\");\n+            logger.error(\"Failed to query index \" + indexDir, ioe);\n+            return;\n+        }\n+\n+        try {\n+            final long borrowMillis = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - borrowStart);\n+            logger.debug(\"Borrowing index searcher for {} took {} ms\", indexDir, borrowMillis);\n+            final long startNanos = System.nanoTime();\n+\n+            // If max number of results are retrieved, do not bother querying lucene\n+            if (queryResult.getTotalHitCount() >= maxResults) {\n+                logger.debug(\"Will not query lucene index {} because maximum results have already been obtained\", indexDir);\n+                queryResult.update(Collections.emptyList(), 0L);\n+                return;\n+            }\n+\n+            if (queryResult.isFinished()) {\n+                logger.debug(\"Will not query lucene index {} because the query is already finished\", indexDir);\n+                return;\n+            }\n+\n+            // Query lucene\n+            final IndexReader indexReader = searcher.getIndexSearcher().getIndexReader();\n+            final TopDocs topDocs;\n+            try {\n+                topDocs = searcher.getIndexSearcher().search(query, maxResults);\n+            } catch (final Exception e) {\n+                logger.error(\"Failed to query Lucene for index \" + indexDir, e);\n+                queryResult.setError(\"Failed to query Lucene for index \" + indexDir + \" due to \" + e);\n+                return;\n+            } finally {\n+                final long ms = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - startNanos);\n+                logger.debug(\"Querying Lucene for index {} took {} ms\", indexDir, ms);\n+            }\n+\n+            // If max number of results are retrieved, do not bother reading docs\n+            if (queryResult.getTotalHitCount() >= maxResults) {\n+                logger.debug(\"Will not read events from store for {} because maximum results have already been obtained\", indexDir);\n+                queryResult.update(Collections.emptyList(), 0L);\n+                return;\n+            }\n+\n+            if (queryResult.isFinished()) {\n+                logger.debug(\"Will not read events from store for {} because the query has already finished\", indexDir);\n+                return;\n+            }\n+\n+            final Tuple<List<ProvenanceEventRecord>, Integer> eventsAndTotalHits = readDocuments(topDocs, indexReader);\n+\n+            if (eventsAndTotalHits == null) {\n+                queryResult.update(Collections.emptyList(), 0L);\n+                logger.info(\"Will not update query results for queried index {} for query {} because the maximum number of results have been reached already\",\n+                    indexDir, query);\n+            } else {\n+                queryResult.update(eventsAndTotalHits.getKey(), eventsAndTotalHits.getValue());\n+\n+                final long searchNanos = System.nanoTime() - startNanos;\n+                final long millis = TimeUnit.NANOSECONDS.toMillis(searchNanos);\n+                logger.info(\"Successfully queried index {} for query {}; retrieved {} events with a total of {} hits in {} millis\",\n+                    indexDir, query, eventsAndTotalHits.getKey().size(), eventsAndTotalHits.getValue(), millis);\n+            }\n+        } catch (final Exception e) {\n+            logger.error(\"Failed to query events against index \" + indexDir, e);\n+            queryResult.setError(\"Failed to complete query due to \" + e);\n+        } finally {\n+            indexManager.returnIndexSearcher(searcher);\n+        }\n+    }\n+\n+    private Tuple<List<ProvenanceEventRecord>, Integer> readDocuments(final TopDocs topDocs, final IndexReader indexReader) {\n+        // If no topDocs is supplied, just provide a Tuple that has no records and a hit count of 0.\n+        if (topDocs == null || topDocs.totalHits == 0) {\n+            return new Tuple<>(Collections.<ProvenanceEventRecord> emptyList(), 0);\n+        }\n+\n+        final long start = System.nanoTime();\n+        final List<Long> eventIds = Arrays.stream(topDocs.scoreDocs)\n+            .mapToInt(scoreDoc -> scoreDoc.doc)\n+            .mapToObj(docId -> {\n+                try {\n+                    return indexReader.document(docId, LUCENE_FIELDS_TO_LOAD);\n+                } catch (final Exception e) {\n+                    throw new SearchFailedException(\"Failed to read Provenance Events from Event File\", e);\n+                }\n+            })\n+            .map(doc -> doc.getField(SearchableFields.Identifier.getSearchableFieldName()).numericValue().longValue())\n+            .collect(Collectors.toList());\n+\n+        final long endConvert = System.nanoTime();\n+        final long ms = TimeUnit.NANOSECONDS.toMillis(endConvert - start);\n+        logger.debug(\"Converting documents took {} ms\", ms);\n+\n+        List<ProvenanceEventRecord> events;\n+        try {\n+            events = eventStore.getEvents(eventIds, authorizer, transformer);\n+        } catch (IOException e) {\n+            throw new SearchFailedException(\"Unable to retrieve events from the Provenance Store\", e);\n+        }\n+\n+        final long fetchEventNanos = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - endConvert);\n+        logger.debug(\"Fetching {} events from Event Store took {} ms ({} events actually fetched)\", eventIds.size(), fetchEventNanos, events.size());\n+\n+        final int totalHits = topDocs.totalHits;\n+        return new Tuple<>(events, totalHits);\n+    }\n+\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/QueryTask.java",
                "sha": "38d3f618f2466195e1a1c10ad38410d02ccec366",
                "status": "added"
            },
            {
                "additions": 39,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/StoredDocument.java",
                "changes": 39,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/StoredDocument.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/StoredDocument.java",
                "patch": "@@ -0,0 +1,39 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.index.lucene;\n+\n+import org.apache.lucene.document.Document;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n+\n+public class StoredDocument {\n+    private final Document document;\n+    private final StorageSummary storageSummary;\n+\n+    public StoredDocument(final Document document, final StorageSummary summary) {\n+        this.document = document;\n+        this.storageSummary = summary;\n+    }\n+\n+    public Document getDocument() {\n+        return document;\n+    }\n+\n+    public StorageSummary getStorageSummary() {\n+        return storageSummary;\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/index/lucene/StoredDocument.java",
                "sha": "207ba9f98342bee1a593c6548db162e4f0cae781",
                "status": "added"
            },
            {
                "additions": 107,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/CachingIndexManager.java",
                "changes": 176,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/CachingIndexManager.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 69,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/CachingIndexManager.java",
                "patch": "@@ -36,6 +36,8 @@\n import org.apache.lucene.search.IndexSearcher;\n import org.apache.lucene.store.Directory;\n import org.apache.lucene.store.FSDirectory;\n+import org.apache.nifi.provenance.index.EventIndexSearcher;\n+import org.apache.nifi.provenance.index.EventIndexWriter;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -47,53 +49,61 @@\n     private final Map<File, List<ActiveIndexSearcher>> activeSearchers = new HashMap<>();\n \n \n-    public void removeIndex(final File indexDirectory) {\n+    @Override\n+    public boolean removeIndex(final File indexDirectory) {\n         final File absoluteFile = indexDirectory.getAbsoluteFile();\n         logger.info(\"Removing index {}\", indexDirectory);\n \n         lock.lock();\n         try {\n             final IndexWriterCount count = writerCounts.remove(absoluteFile);\n-            if ( count != null ) {\n+            if (count != null) {\n                 try {\n                     count.close();\n                 } catch (final IOException ioe) {\n                     logger.warn(\"Failed to close Index Writer {} for {}\", count.getWriter(), absoluteFile);\n-                    if ( logger.isDebugEnabled() ) {\n+                    if (logger.isDebugEnabled()) {\n                         logger.warn(\"\", ioe);\n                     }\n+\n+                    return false;\n                 }\n             }\n \n             final List<ActiveIndexSearcher> searcherList = activeSearchers.remove(absoluteFile);\n             if (searcherList != null) {\n-                for ( final ActiveIndexSearcher searcher : searcherList ) {\n+                for (final ActiveIndexSearcher searcher : searcherList) {\n                     try {\n                         searcher.close();\n                     } catch (final IOException ioe) {\n                         logger.warn(\"Failed to close Index Searcher {} for {} due to {}\",\n-                                searcher.getSearcher(), absoluteFile, ioe);\n-                        if ( logger.isDebugEnabled() ) {\n+                            searcher.getSearcher(), absoluteFile, ioe);\n+                        if (logger.isDebugEnabled()) {\n                             logger.warn(\"\", ioe);\n                         }\n+\n+                        return false;\n                     }\n                 }\n             }\n         } finally {\n             lock.unlock();\n         }\n+\n+        return true;\n     }\n \n-    public IndexWriter borrowIndexWriter(final File indexingDirectory) throws IOException {\n-        final File absoluteFile = indexingDirectory.getAbsoluteFile();\n-        logger.trace(\"Borrowing index writer for {}\", indexingDirectory);\n+    @Override\n+    public EventIndexWriter borrowIndexWriter(final File indexDirectory) throws IOException {\n+        final File absoluteFile = indexDirectory.getAbsoluteFile();\n+        logger.trace(\"Borrowing index writer for {}\", indexDirectory);\n \n         lock.lock();\n         try {\n             IndexWriterCount writerCount = writerCounts.remove(absoluteFile);\n-            if ( writerCount == null ) {\n+            if (writerCount == null) {\n                 final List<Closeable> closeables = new ArrayList<>();\n-                final Directory directory = FSDirectory.open(indexingDirectory);\n+                final Directory directory = FSDirectory.open(indexDirectory);\n                 closeables.add(directory);\n \n                 try {\n@@ -104,10 +114,11 @@ public IndexWriter borrowIndexWriter(final File indexingDirectory) throws IOExce\n                     config.setWriteLockTimeout(300000L);\n \n                     final IndexWriter indexWriter = new IndexWriter(directory, config);\n-                    writerCount = new IndexWriterCount(indexWriter, analyzer, directory, 1);\n-                    logger.debug(\"Providing new index writer for {}\", indexingDirectory);\n+                    final EventIndexWriter eventIndexWriter = new LuceneEventIndexWriter(indexWriter, indexDirectory);\n+                    writerCount = new IndexWriterCount(eventIndexWriter, analyzer, directory, 1);\n+                    logger.debug(\"Providing new index writer for {}\", indexDirectory);\n                 } catch (final IOException ioe) {\n-                    for ( final Closeable closeable : closeables ) {\n+                    for (final Closeable closeable : closeables) {\n                         try {\n                             closeable.close();\n                         } catch (final IOException ioe2) {\n@@ -122,16 +133,16 @@ public IndexWriter borrowIndexWriter(final File indexingDirectory) throws IOExce\n \n                 // Mark any active searchers as poisoned because we are updating the index\n                 final List<ActiveIndexSearcher> searchers = activeSearchers.get(absoluteFile);\n-                if ( searchers != null ) {\n+                if (searchers != null) {\n                     for (final ActiveIndexSearcher activeSearcher : searchers) {\n-                        logger.debug(\"Poisoning {} because it is searching {}, which is getting updated\", activeSearcher, indexingDirectory);\n+                        logger.debug(\"Poisoning {} because it is searching {}, which is getting updated\", activeSearcher, indexDirectory);\n                         activeSearcher.poison();\n                     }\n                 }\n             } else {\n-                logger.debug(\"Providing existing index writer for {} and incrementing count to {}\", indexingDirectory, writerCount.getCount() + 1);\n+                logger.debug(\"Providing existing index writer for {} and incrementing count to {}\", indexDirectory, writerCount.getCount() + 1);\n                 writerCounts.put(absoluteFile, new IndexWriterCount(writerCount.getWriter(),\n-                        writerCount.getAnalyzer(), writerCount.getDirectory(), writerCount.getCount() + 1));\n+                    writerCount.getAnalyzer(), writerCount.getDirectory(), writerCount.getCount() + 1));\n             }\n \n             return writerCount.getWriter();\n@@ -140,31 +151,53 @@ public IndexWriter borrowIndexWriter(final File indexingDirectory) throws IOExce\n         }\n     }\n \n-    public void returnIndexWriter(final File indexingDirectory, final IndexWriter writer) {\n-        final File absoluteFile = indexingDirectory.getAbsoluteFile();\n-        logger.trace(\"Returning Index Writer for {} to IndexManager\", indexingDirectory);\n+\n+    @Override\n+    public void returnIndexWriter(final EventIndexWriter writer) {\n+        returnIndexWriter(writer, true, true);\n+    }\n+\n+    @Override\n+    public void returnIndexWriter(final EventIndexWriter writer, final boolean commit, final boolean isCloseable) {\n+        final File indexDirectory = writer.getDirectory();\n+        final File absoluteFile = indexDirectory.getAbsoluteFile();\n+        logger.trace(\"Returning Index Writer for {} to IndexManager\", indexDirectory);\n \n         lock.lock();\n         try {\n-            final IndexWriterCount count = writerCounts.remove(absoluteFile);\n+            final IndexWriterCount count = writerCounts.get(absoluteFile);\n \n             try {\n-                if ( count == null ) {\n+                if (count == null) {\n                     logger.warn(\"Index Writer {} was returned to IndexManager for {}, but this writer is not known. \"\n-                            + \"This could potentially lead to a resource leak\", writer, indexingDirectory);\n+                        + \"This could potentially lead to a resource leak\", writer, indexDirectory);\n                     writer.close();\n-                } else if ( count.getCount() <= 1 ) {\n+                } else if (count.getCount() <= 1) {\n                     // we are finished with this writer.\n-                    logger.debug(\"Decrementing count for Index Writer for {} to {}; Closing writer\", indexingDirectory, count.getCount() - 1);\n-                    count.close();\n+                    logger.info(\"Decrementing count for Index Writer for {} to {}. Now finished writing to this Index Directory\",\n+                        indexDirectory, count.getCount() - 1);\n+\n+                    try {\n+                        if (commit) {\n+                            writer.commit();\n+                        }\n+                    } finally {\n+                        if (isCloseable) {\n+                            try {\n+                                count.close();\n+                            } finally {\n+                                writerCounts.remove(absoluteFile);\n+                            }\n+                        }\n+                    }\n                 } else {\n                     // decrement the count.\n-                    logger.debug(\"Decrementing count for Index Writer for {} to {}\", indexingDirectory, count.getCount() - 1);\n+                    logger.debug(\"Decrementing count for Index Writer for {} to {}\", indexDirectory, count.getCount() - 1);\n                     writerCounts.put(absoluteFile, new IndexWriterCount(count.getWriter(), count.getAnalyzer(), count.getDirectory(), count.getCount() - 1));\n                 }\n             } catch (final IOException ioe) {\n                 logger.warn(\"Failed to close Index Writer {} due to {}\", writer, ioe);\n-                if ( logger.isDebugEnabled() ) {\n+                if (logger.isDebugEnabled()) {\n                     logger.warn(\"\", ioe);\n                 }\n             }\n@@ -174,15 +207,16 @@ public void returnIndexWriter(final File indexingDirectory, final IndexWriter wr\n     }\n \n \n-    public IndexSearcher borrowIndexSearcher(final File indexDir) throws IOException {\n+    @Override\n+    public EventIndexSearcher borrowIndexSearcher(final File indexDir) throws IOException {\n         final File absoluteFile = indexDir.getAbsoluteFile();\n         logger.trace(\"Borrowing index searcher for {}\", indexDir);\n \n         lock.lock();\n         try {\n             // check if we already have a reader cached.\n             List<ActiveIndexSearcher> currentlyCached = activeSearchers.get(absoluteFile);\n-            if ( currentlyCached == null ) {\n+            if (currentlyCached == null) {\n                 currentlyCached = new ArrayList<>();\n                 activeSearchers.put(absoluteFile, currentlyCached);\n             } else {\n@@ -197,7 +231,7 @@ public IndexSearcher borrowIndexSearcher(final File indexDir) throws IOException\n \n                         // if there are no references to the reader, it will have been closed. Since there is no\n                         // isClosed() method, this is how we determine whether it's been closed or not.\n-                        final int refCount = searcher.getSearcher().getIndexReader().getRefCount();\n+                        final int refCount = searcher.getSearcher().getIndexSearcher().getIndexReader().getRefCount();\n                         if (refCount <= 0) {\n                             // if refCount == 0, then the reader has been closed, so we cannot use the searcher\n                             logger.debug(\"Reference count for cached Index Searcher for {} is currently {}; \"\n@@ -216,16 +250,17 @@ public IndexSearcher borrowIndexSearcher(final File indexDir) throws IOException\n             // if we have an Index Writer, and if so create a Reader based on the Index Writer.\n             // This will provide us a 'near real time' index reader.\n             final IndexWriterCount writerCount = writerCounts.remove(absoluteFile);\n-            if ( writerCount == null ) {\n+            if (writerCount == null) {\n                 final Directory directory = FSDirectory.open(absoluteFile);\n                 logger.debug(\"No Index Writer currently exists for {}; creating a cachable reader\", indexDir);\n \n                 try {\n                     final DirectoryReader directoryReader = DirectoryReader.open(directory);\n                     final IndexSearcher searcher = new IndexSearcher(directoryReader);\n+                    final EventIndexSearcher eventIndexSearcher = new LuceneEventIndexSearcher(searcher, indexDir, directory, directoryReader);\n \n                     // we want to cache the searcher that we create, since it's just a reader.\n-                    final ActiveIndexSearcher cached = new ActiveIndexSearcher(searcher, absoluteFile, directoryReader, directory, true);\n+                    final ActiveIndexSearcher cached = new ActiveIndexSearcher(eventIndexSearcher, absoluteFile, directoryReader, directory, true);\n                     currentlyCached.add(cached);\n \n                     return cached.getSearcher();\n@@ -243,22 +278,23 @@ public IndexSearcher borrowIndexSearcher(final File indexDir) throws IOException\n                 }\n             } else {\n                 logger.debug(\"Index Writer currently exists for {}; creating a non-cachable reader and incrementing \"\n-                        + \"counter to {}\", indexDir, writerCount.getCount() + 1);\n+                    + \"counter to {}\", indexDir, writerCount.getCount() + 1);\n \n                 // increment the writer count to ensure that it's kept open.\n                 writerCounts.put(absoluteFile, new IndexWriterCount(writerCount.getWriter(),\n-                        writerCount.getAnalyzer(), writerCount.getDirectory(), writerCount.getCount() + 1));\n+                    writerCount.getAnalyzer(), writerCount.getDirectory(), writerCount.getCount() + 1));\n \n                 // create a new Index Searcher from the writer so that we don't have an issue with trying\n                 // to read from a directory that's locked. If we get the \"no segments* file found\" with\n                 // Lucene, this indicates that an IndexWriter already has the directory open.\n-                final IndexWriter writer = writerCount.getWriter();\n-                final DirectoryReader directoryReader = DirectoryReader.open(writer, false);\n+                final EventIndexWriter writer = writerCount.getWriter();\n+                final DirectoryReader directoryReader = DirectoryReader.open(writer.getIndexWriter(), false);\n                 final IndexSearcher searcher = new IndexSearcher(directoryReader);\n+                final EventIndexSearcher eventIndexSearcher = new LuceneEventIndexSearcher(searcher, indexDir, null, directoryReader);\n \n                 // we don't want to cache this searcher because it's based on a writer, so we want to get\n                 // new values the next time that we search.\n-                final ActiveIndexSearcher activeSearcher = new ActiveIndexSearcher(searcher, absoluteFile, directoryReader, null, false);\n+                final ActiveIndexSearcher activeSearcher = new ActiveIndexSearcher(eventIndexSearcher, absoluteFile, directoryReader, null, false);\n \n                 currentlyCached.add(activeSearcher);\n                 return activeSearcher.getSearcher();\n@@ -269,17 +305,19 @@ public IndexSearcher borrowIndexSearcher(final File indexDir) throws IOException\n     }\n \n \n-    public void returnIndexSearcher(final File indexDirectory, final IndexSearcher searcher) {\n+    @Override\n+    public void returnIndexSearcher(final EventIndexSearcher searcher) {\n+        final File indexDirectory = searcher.getIndexDirectory();\n         final File absoluteFile = indexDirectory.getAbsoluteFile();\n         logger.trace(\"Returning index searcher for {} to IndexManager\", indexDirectory);\n \n         lock.lock();\n         try {\n             // check if we already have a reader cached.\n             final List<ActiveIndexSearcher> currentlyCached = activeSearchers.get(absoluteFile);\n-            if ( currentlyCached == null ) {\n+            if (currentlyCached == null) {\n                 logger.warn(\"Received Index Searcher for {} but no searcher was provided for that directory; this could \"\n-                        + \"result in a resource leak\", indexDirectory);\n+                    + \"result in a resource leak\", indexDirectory);\n                 return;\n             }\n \n@@ -289,20 +327,20 @@ public void returnIndexSearcher(final File indexDirectory, final IndexSearcher s\n             boolean activeSearcherFound = false;\n             while (itr.hasNext()) {\n                 final ActiveIndexSearcher activeSearcher = itr.next();\n-                if ( activeSearcher.getSearcher().equals(searcher) ) {\n+                if (activeSearcher.getSearcher().equals(searcher)) {\n                     activeSearcherFound = true;\n-                    if ( activeSearcher.isCache() ) {\n+                    if (activeSearcher.isCache()) {\n                         // if the searcher is poisoned, close it and remove from \"pool\". Otherwise,\n                         // just decrement the count. Note here that when we call close() it won't actually close\n                         // the underlying directory reader unless there are no more references to it\n-                        if ( activeSearcher.isPoisoned() ) {\n+                        if (activeSearcher.isPoisoned()) {\n                             itr.remove();\n \n                             try {\n                                 activeSearcher.close();\n                             } catch (final IOException ioe) {\n                                 logger.warn(\"Failed to close Index Searcher for {} due to {}\", absoluteFile, ioe);\n-                                if ( logger.isDebugEnabled() ) {\n+                                if (logger.isDebugEnabled()) {\n                                     logger.warn(\"\", ioe);\n                                 }\n                             }\n@@ -322,26 +360,26 @@ public void returnIndexSearcher(final File indexDirectory, final IndexSearcher s\n \n                         // decrement the writer count because we incremented it when creating the searcher\n                         final IndexWriterCount writerCount = writerCounts.remove(absoluteFile);\n-                        if ( writerCount != null ) {\n-                            if ( writerCount.getCount() <= 1 ) {\n+                        if (writerCount != null) {\n+                            if (writerCount.getCount() <= 1) {\n                                 try {\n                                     logger.debug(\"Index searcher for {} is not cached. Writer count is \"\n-                                            + \"decremented to {}; closing writer\", indexDirectory, writerCount.getCount() - 1);\n+                                        + \"decremented to {}; closing writer\", indexDirectory, writerCount.getCount() - 1);\n \n                                     writerCount.close();\n                                 } catch (final IOException ioe) {\n                                     logger.warn(\"Failed to close Index Writer for {} due to {}\", absoluteFile, ioe);\n-                                    if ( logger.isDebugEnabled() ) {\n+                                    if (logger.isDebugEnabled()) {\n                                         logger.warn(\"\", ioe);\n                                     }\n                                 }\n                             } else {\n                                 logger.debug(\"Index searcher for {} is not cached. Writer count is decremented \"\n-                                        + \"to {}; leaving writer open\", indexDirectory, writerCount.getCount() - 1);\n+                                    + \"to {}; leaving writer open\", indexDirectory, writerCount.getCount() - 1);\n \n                                 writerCounts.put(absoluteFile, new IndexWriterCount(writerCount.getWriter(),\n-                                        writerCount.getAnalyzer(), writerCount.getDirectory(),\n-                                        writerCount.getCount() - 1));\n+                                    writerCount.getAnalyzer(), writerCount.getDirectory(),\n+                                    writerCount.getCount() - 1));\n                             }\n                         }\n \n@@ -353,7 +391,7 @@ public void returnIndexSearcher(final File indexDirectory, final IndexSearcher s\n                             }\n                         } catch (final IOException ioe) {\n                             logger.warn(\"Failed to close Index Searcher for {} due to {}\", absoluteFile, ioe);\n-                            if ( logger.isDebugEnabled() ) {\n+                            if (logger.isDebugEnabled()) {\n                                 logger.warn(\"\", ioe);\n                             }\n                         }\n@@ -378,11 +416,11 @@ public void close() throws IOException {\n         try {\n             IOException ioe = null;\n \n-            for ( final IndexWriterCount count : writerCounts.values() ) {\n+            for (final IndexWriterCount count : writerCounts.values()) {\n                 try {\n                     count.close();\n                 } catch (final IOException e) {\n-                    if ( ioe == null ) {\n+                    if (ioe == null) {\n                         ioe = e;\n                     } else {\n                         ioe.addSuppressed(e);\n@@ -395,7 +433,7 @@ public void close() throws IOException {\n                     try {\n                         searcher.close();\n                     } catch (final IOException e) {\n-                        if ( ioe == null ) {\n+                        if (ioe == null) {\n                             ioe = e;\n                         } else {\n                             ioe.addSuppressed(e);\n@@ -404,7 +442,7 @@ public void close() throws IOException {\n                 }\n             }\n \n-            if ( ioe != null ) {\n+            if (ioe != null) {\n                 throw ioe;\n             }\n         } finally {\n@@ -415,39 +453,39 @@ public void close() throws IOException {\n \n     private static void close(final Closeable... closeables) throws IOException {\n         IOException ioe = null;\n-        for ( final Closeable closeable : closeables ) {\n-            if ( closeable == null ) {\n+        for (final Closeable closeable : closeables) {\n+            if (closeable == null) {\n                 continue;\n             }\n \n             try {\n                 closeable.close();\n             } catch (final IOException e) {\n-                if ( ioe == null ) {\n+                if (ioe == null) {\n                     ioe = e;\n                 } else {\n                     ioe.addSuppressed(e);\n                 }\n             }\n         }\n \n-        if ( ioe != null ) {\n+        if (ioe != null) {\n             throw ioe;\n         }\n     }\n \n \n     private static class ActiveIndexSearcher {\n-        private final IndexSearcher searcher;\n+        private final EventIndexSearcher searcher;\n         private final DirectoryReader directoryReader;\n         private final File indexDirectory;\n         private final Directory directory;\n         private final boolean cache;\n         private final AtomicInteger referenceCount = new AtomicInteger(1);\n         private volatile boolean poisoned = false;\n \n-        public ActiveIndexSearcher(final IndexSearcher searcher, final File indexDirectory, final DirectoryReader directoryReader,\n-                final Directory directory, final boolean cache) {\n+        public ActiveIndexSearcher(final EventIndexSearcher searcher, final File indexDirectory, final DirectoryReader directoryReader,\n+            final Directory directory, final boolean cache) {\n             this.searcher = searcher;\n             this.directoryReader = directoryReader;\n             this.indexDirectory = indexDirectory;\n@@ -459,7 +497,7 @@ public boolean isCache() {\n             return cache;\n         }\n \n-        public IndexSearcher getSearcher() {\n+        public EventIndexSearcher getSearcher() {\n             return searcher;\n         }\n \n@@ -499,12 +537,12 @@ public String toString() {\n \n \n     private static class IndexWriterCount implements Closeable {\n-        private final IndexWriter writer;\n+        private final EventIndexWriter writer;\n         private final Analyzer analyzer;\n         private final Directory directory;\n         private final int count;\n \n-        public IndexWriterCount(final IndexWriter writer, final Analyzer analyzer, final Directory directory, final int count) {\n+        public IndexWriterCount(final EventIndexWriter writer, final Analyzer analyzer, final Directory directory, final int count) {\n             this.writer = writer;\n             this.analyzer = analyzer;\n             this.directory = directory;\n@@ -519,7 +557,7 @@ public Directory getDirectory() {\n             return directory;\n         }\n \n-        public IndexWriter getWriter() {\n+        public EventIndexWriter getWriter() {\n             return writer;\n         }\n ",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/CachingIndexManager.java",
                "sha": "eefcecb3baecf36a43fe3de036756af0f54a60ff",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/DeleteIndexAction.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/DeleteIndexAction.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 5,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/DeleteIndexAction.java",
                "patch": "@@ -25,6 +25,7 @@\n import org.apache.nifi.provenance.IndexConfiguration;\n import org.apache.nifi.provenance.PersistentProvenanceRepository;\n import org.apache.nifi.provenance.expiration.ExpirationAction;\n+import org.apache.nifi.provenance.index.EventIndexWriter;\n import org.apache.nifi.provenance.serialization.RecordReader;\n import org.apache.nifi.provenance.serialization.RecordReaders;\n import org.slf4j.Logger;\n@@ -60,15 +61,16 @@ public File execute(final File expiredFile) throws IOException {\n             final Term term = new Term(FieldNames.STORAGE_FILENAME, LuceneUtil.substringBefore(expiredFile.getName(), \".\"));\n \n             boolean deleteDir = false;\n-            final IndexWriter writer = indexManager.borrowIndexWriter(indexingDirectory);\n+            final EventIndexWriter writer = indexManager.borrowIndexWriter(indexingDirectory);\n             try {\n-                writer.deleteDocuments(term);\n-                writer.commit();\n-                final int docsLeft = writer.numDocs();\n+                final IndexWriter indexWriter = writer.getIndexWriter();\n+                indexWriter.deleteDocuments(term);\n+                indexWriter.commit();\n+                final int docsLeft = indexWriter.numDocs();\n                 deleteDir = docsLeft <= 0;\n                 logger.debug(\"After expiring {}, there are {} docs left for index {}\", expiredFile, docsLeft, indexingDirectory);\n             } finally {\n-                indexManager.returnIndexWriter(indexingDirectory, writer);\n+                indexManager.returnIndexWriter(writer);\n             }\n \n             // we've confirmed that all documents have been removed. Delete the index directory.",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/DeleteIndexAction.java",
                "sha": "f372a2d3a50aa4d754e382d505c248b96cc41bd9",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/DocsReader.java",
                "changes": 65,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/DocsReader.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 35,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/DocsReader.java",
                "patch": "@@ -30,25 +30,25 @@\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicInteger;\n \n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.index.IndexableField;\n+import org.apache.lucene.search.ScoreDoc;\n+import org.apache.lucene.search.TopDocs;\n import org.apache.nifi.provenance.ProvenanceEventRecord;\n import org.apache.nifi.provenance.SearchableFields;\n import org.apache.nifi.provenance.StandardProvenanceEventRecord;\n-import org.apache.nifi.provenance.authorization.AuthorizationCheck;\n+import org.apache.nifi.provenance.authorization.EventAuthorizer;\n import org.apache.nifi.provenance.serialization.RecordReader;\n import org.apache.nifi.provenance.serialization.RecordReaders;\n import org.apache.nifi.provenance.toc.TocReader;\n-import org.apache.lucene.document.Document;\n-import org.apache.lucene.index.IndexReader;\n-import org.apache.lucene.index.IndexableField;\n-import org.apache.lucene.search.ScoreDoc;\n-import org.apache.lucene.search.TopDocs;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-class DocsReader {\n+public class DocsReader {\n     private final Logger logger = LoggerFactory.getLogger(DocsReader.class);\n \n-    public Set<ProvenanceEventRecord> read(final TopDocs topDocs, final AuthorizationCheck authCheck, final IndexReader indexReader, final Collection<Path> allProvenanceLogFiles,\n+    public Set<ProvenanceEventRecord> read(final TopDocs topDocs, final EventAuthorizer authorizer, final IndexReader indexReader, final Collection<Path> allProvenanceLogFiles,\n             final AtomicInteger retrievalCount, final int maxResults, final int maxAttributeChars) throws IOException {\n         if (retrievalCount.get() >= maxResults) {\n             return Collections.emptySet();\n@@ -67,7 +67,7 @@\n \n         final long readDocuments = System.nanoTime() - start;\n         logger.debug(\"Reading {} Lucene Documents took {} millis\", docs.size(), TimeUnit.NANOSECONDS.toMillis(readDocuments));\n-        return read(docs, authCheck, allProvenanceLogFiles, retrievalCount, maxResults, maxAttributeChars);\n+        return read(docs, authorizer, allProvenanceLogFiles, retrievalCount, maxResults, maxAttributeChars);\n     }\n \n \n@@ -106,46 +106,41 @@ private ProvenanceEventRecord getRecord(final Document d, final RecordReader rea\n         return record;\n     }\n \n-    public Set<ProvenanceEventRecord> read(final List<Document> docs, final AuthorizationCheck authCheck, final Collection<Path> allProvenanceLogFiles,\n+    public Set<ProvenanceEventRecord> read(final List<Document> docs, final EventAuthorizer authorizer, final Collection<Path> allProvenanceLogFiles,\n             final AtomicInteger retrievalCount, final int maxResults, final int maxAttributeChars) throws IOException {\n \n         if (retrievalCount.get() >= maxResults) {\n             return Collections.emptySet();\n         }\n \n         final long start = System.nanoTime();\n-\n-        Set<ProvenanceEventRecord> matchingRecords = new LinkedHashSet<>();\n-\n-        Map<String, List<Document>> byStorageNameDocGroups = LuceneUtil.groupDocsByStorageFileName(docs);\n+        final Set<ProvenanceEventRecord> matchingRecords = new LinkedHashSet<>();\n+        final Map<String, List<Document>> byStorageNameDocGroups = LuceneUtil.groupDocsByStorageFileName(docs);\n \n         int eventsReadThisFile = 0;\n         int logFileCount = 0;\n \n         for (String storageFileName : byStorageNameDocGroups.keySet()) {\n-            File provenanceEventFile = LuceneUtil.getProvenanceLogFile(storageFileName, allProvenanceLogFiles);\n-            if (provenanceEventFile != null) {\n-                try (RecordReader reader = RecordReaders.newRecordReader(provenanceEventFile, allProvenanceLogFiles,\n-                        maxAttributeChars)) {\n-\n-                    Iterator<Document> docIter = byStorageNameDocGroups.get(storageFileName).iterator();\n-                    while (docIter.hasNext() && retrievalCount.getAndIncrement() < maxResults) {\n-                        ProvenanceEventRecord event = this.getRecord(docIter.next(), reader);\n-                        if (event != null && authCheck.isAuthorized(event)) {\n-                            matchingRecords.add(event);\n-                            eventsReadThisFile++;\n-                        }\n-                    }\n+            final File provenanceEventFile = LuceneUtil.getProvenanceLogFile(storageFileName, allProvenanceLogFiles);\n+            if (provenanceEventFile == null) {\n+                logger.warn(\"Could not find Provenance Log File with \"\n+                    + \"basename {} in the Provenance Repository; assuming \"\n+                    + \"file has expired and continuing without it\", storageFileName);\n+                continue;\n+            }\n \n-                } catch (Exception e) {\n-                    logger.warn(\"Failed while trying to read Provenance Events. The event file '\"\n-                            + provenanceEventFile.getAbsolutePath() +\n-                            \"' may be missing or corrupted.\", e);\n+            try (final RecordReader reader = RecordReaders.newRecordReader(provenanceEventFile, allProvenanceLogFiles, maxAttributeChars)) {\n+                final Iterator<Document> docIter = byStorageNameDocGroups.get(storageFileName).iterator();\n+                while (docIter.hasNext() && retrievalCount.getAndIncrement() < maxResults) {\n+                    final ProvenanceEventRecord event = getRecord(docIter.next(), reader);\n+                    if (event != null && authorizer.isAuthorized(event)) {\n+                        matchingRecords.add(event);\n+                        eventsReadThisFile++;\n+                    }\n                 }\n-            } else {\n-                logger.warn(\"Could not find Provenance Log File with \"\n-                        + \"basename {} in the Provenance Repository; assuming \"\n-                        + \"file has expired and continuing without it\", storageFileName);\n+            } catch (final Exception e) {\n+                logger.warn(\"Failed to read Provenance Events. The event file '\"\n+                    + provenanceEventFile.getAbsolutePath() + \"' may be missing or corrupt.\", e);\n             }\n         }\n ",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/DocsReader.java",
                "sha": "0e96b6248fea54632f46ac3e46748e25d0e31795",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/DocumentToEventConverter.java",
                "changes": 30,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/DocumentToEventConverter.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/DocumentToEventConverter.java",
                "patch": "@@ -0,0 +1,30 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.lucene;\n+\n+import java.io.IOException;\n+import java.util.Set;\n+\n+import org.apache.lucene.index.IndexReader;\n+import org.apache.lucene.search.TopDocs;\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+\n+public interface DocumentToEventConverter {\n+\n+    Set<ProvenanceEventRecord> convert(TopDocs topDocs, IndexReader indexReader) throws IOException;\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/DocumentToEventConverter.java",
                "sha": "18d38604ee65d9c4bd5b1d02b6cd6c8c0a1f3e2d",
                "status": "added"
            },
            {
                "additions": 9,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/IndexManager.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/IndexManager.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 7,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/IndexManager.java",
                "patch": "@@ -21,17 +21,19 @@\n import java.io.File;\n import java.io.IOException;\n \n-import org.apache.lucene.index.IndexWriter;\n-import org.apache.lucene.search.IndexSearcher;\n+import org.apache.nifi.provenance.index.EventIndexSearcher;\n+import org.apache.nifi.provenance.index.EventIndexWriter;\n \n public interface IndexManager extends Closeable {\n-    IndexSearcher borrowIndexSearcher(File indexDir) throws IOException;\n+    EventIndexSearcher borrowIndexSearcher(File indexDir) throws IOException;\n \n-    IndexWriter borrowIndexWriter(File indexingDirectory) throws IOException;\n+    EventIndexWriter borrowIndexWriter(File indexDirectory) throws IOException;\n \n-    void removeIndex(final File indexDirectory);\n+    boolean removeIndex(final File indexDirectory);\n \n-    void returnIndexSearcher(File indexDirectory, IndexSearcher searcher);\n+    void returnIndexSearcher(EventIndexSearcher searcher);\n \n-    void returnIndexWriter(File indexingDirectory, IndexWriter writer);\n+    void returnIndexWriter(EventIndexWriter writer, boolean commit, boolean isCloseable);\n+\n+    void returnIndexWriter(EventIndexWriter writer);\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/IndexManager.java",
                "sha": "331d1410ae3760d317c26bbac9f170195a0f0bbb",
                "status": "modified"
            },
            {
                "additions": 30,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/IndexSearch.java",
                "changes": 38,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/IndexSearch.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 8,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/IndexSearch.java",
                "patch": "@@ -21,18 +21,20 @@\n import java.io.IOException;\n import java.util.Collections;\n import java.util.Date;\n+import java.util.List;\n import java.util.Set;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicInteger;\n \n-import org.apache.lucene.search.IndexSearcher;\n import org.apache.lucene.search.Query;\n import org.apache.lucene.search.TopDocs;\n+import org.apache.nifi.authorization.AccessDeniedException;\n import org.apache.nifi.authorization.user.NiFiUser;\n import org.apache.nifi.provenance.PersistentProvenanceRepository;\n import org.apache.nifi.provenance.ProvenanceEventRecord;\n import org.apache.nifi.provenance.StandardQueryResult;\n-import org.apache.nifi.provenance.authorization.AuthorizationCheck;\n+import org.apache.nifi.provenance.authorization.EventAuthorizer;\n+import org.apache.nifi.provenance.index.EventIndexSearcher;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -86,14 +88,14 @@ public StandardQueryResult search(final org.apache.nifi.provenance.search.Query\n         final Query luceneQuery = LuceneUtil.convertQuery(provenanceQuery);\n \n         final long start = System.nanoTime();\n-        IndexSearcher searcher = null;\n+        EventIndexSearcher searcher = null;\n         try {\n             searcher = indexManager.borrowIndexSearcher(indexDirectory);\n             final long searchStartNanos = System.nanoTime();\n             final long openSearcherNanos = searchStartNanos - start;\n \n             logger.debug(\"Searching {} for {}\", this, provenanceQuery);\n-            final TopDocs topDocs = searcher.search(luceneQuery, provenanceQuery.getMaxResults());\n+            final TopDocs topDocs = searcher.getIndexSearcher().search(luceneQuery, provenanceQuery.getMaxResults());\n             final long finishSearch = System.nanoTime();\n             final long searchNanos = finishSearch - searchStartNanos;\n \n@@ -107,9 +109,29 @@ public StandardQueryResult search(final org.apache.nifi.provenance.search.Query\n \n             final DocsReader docsReader = new DocsReader();\n \n-            final AuthorizationCheck authCheck = event -> repository.isAuthorized(event, user);\n-\n-            matchingRecords = docsReader.read(topDocs, authCheck, searcher.getIndexReader(), repository.getAllLogFiles(), retrievedCount,\n+            final EventAuthorizer authorizer = new EventAuthorizer() {\n+                @Override\n+                public boolean isAuthorized(ProvenanceEventRecord event) {\n+                    return repository.isAuthorized(event, user);\n+                }\n+\n+                @Override\n+                public void authorize(ProvenanceEventRecord event) throws AccessDeniedException {\n+                    repository.authorize(event, user);\n+                }\n+\n+                @Override\n+                public List<ProvenanceEventRecord> filterUnauthorizedEvents(List<ProvenanceEventRecord> events) {\n+                    return repository.filterUnauthorizedEvents(events, user);\n+                }\n+\n+                @Override\n+                public Set<ProvenanceEventRecord> replaceUnauthorizedWithPlaceholders(Set<ProvenanceEventRecord> events) {\n+                    return repository.replaceUnauthorizedWithPlaceholders(events, user);\n+                }\n+            };\n+\n+            matchingRecords = docsReader.read(topDocs, authorizer, searcher.getIndexSearcher().getIndexReader(), repository.getAllLogFiles(), retrievedCount,\n                 provenanceQuery.getMaxResults(), maxAttributeChars);\n \n             final long readRecordsNanos = System.nanoTime() - finishSearch;\n@@ -133,7 +155,7 @@ public StandardQueryResult search(final org.apache.nifi.provenance.search.Query\n             return sqr;\n         } finally {\n             if ( searcher != null ) {\n-                indexManager.returnIndexSearcher(indexDirectory, searcher);\n+                indexManager.returnIndexSearcher(searcher);\n             }\n         }\n     }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/IndexSearch.java",
                "sha": "514af38c7a5eb5ec22541a4cb7a4c258d087b67b",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/IndexingAction.java",
                "changes": 16,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/IndexingAction.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 8,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/IndexingAction.java",
                "patch": "@@ -19,6 +19,7 @@\n import java.io.IOException;\n import java.util.Collections;\n import java.util.HashSet;\n+import java.util.List;\n import java.util.Set;\n \n import org.apache.lucene.document.Document;\n@@ -28,23 +29,22 @@\n import org.apache.lucene.document.StringField;\n import org.apache.lucene.index.IndexWriter;\n import org.apache.nifi.flowfile.attributes.CoreAttributes;\n-import org.apache.nifi.provenance.PersistentProvenanceRepository;\n import org.apache.nifi.provenance.ProvenanceEventType;\n import org.apache.nifi.provenance.SearchableFields;\n import org.apache.nifi.provenance.StandardProvenanceEventRecord;\n import org.apache.nifi.provenance.search.SearchableField;\n \n public class IndexingAction {\n-    private final Set<SearchableField> nonAttributeSearchableFields;\n-    private final Set<SearchableField> attributeSearchableFields;\n+    private final Set<SearchableField> searchableEventFields;\n+    private final Set<SearchableField> searchableAttributeFields;\n \n-    public IndexingAction(final PersistentProvenanceRepository repo) {\n-        attributeSearchableFields = Collections.unmodifiableSet(new HashSet<>(repo.getConfiguration().getSearchableAttributes()));\n-        nonAttributeSearchableFields = Collections.unmodifiableSet(new HashSet<>(repo.getConfiguration().getSearchableFields()));\n+    public IndexingAction(final List<SearchableField> searchableEventFields, final List<SearchableField> searchableAttributes) {\n+        this.searchableEventFields = Collections.unmodifiableSet(new HashSet<>(searchableEventFields));\n+        this.searchableAttributeFields = Collections.unmodifiableSet(new HashSet<>(searchableAttributes));\n     }\n \n     private void addField(final Document doc, final SearchableField field, final String value, final Store store) {\n-        if (value == null || (!field.isAttribute() && !nonAttributeSearchableFields.contains(field))) {\n+        if (value == null || (!field.isAttribute() && !searchableEventFields.contains(field))) {\n             return;\n         }\n \n@@ -67,7 +67,7 @@ public void index(final StandardProvenanceEventRecord record, final IndexWriter\n         addField(doc, SearchableFields.SourceQueueIdentifier, record.getSourceQueueIdentifier(), Store.NO);\n         addField(doc, SearchableFields.TransitURI, record.getTransitUri(), Store.NO);\n \n-        for (final SearchableField searchableField : attributeSearchableFields) {\n+        for (final SearchableField searchableField : searchableAttributeFields) {\n             addField(doc, searchableField, LuceneUtil.truncateIndexField(record.getAttribute(searchableField.getSearchableFieldName())), Store.NO);\n         }\n ",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/IndexingAction.java",
                "sha": "a0be319ddc5f6655f0d0d018f87e3ad6bea9c60b",
                "status": "modified"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/LineageQuery.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/LineageQuery.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 16,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/LineageQuery.java",
                "patch": "@@ -25,18 +25,15 @@\n import java.util.Collections;\n import java.util.Set;\n import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.atomic.AtomicInteger;\n \n import org.apache.lucene.index.Term;\n import org.apache.lucene.search.BooleanClause.Occur;\n import org.apache.lucene.search.BooleanQuery;\n-import org.apache.lucene.search.IndexSearcher;\n import org.apache.lucene.search.TermQuery;\n import org.apache.lucene.search.TopDocs;\n-import org.apache.nifi.provenance.PersistentProvenanceRepository;\n import org.apache.nifi.provenance.ProvenanceEventRecord;\n import org.apache.nifi.provenance.SearchableFields;\n-import org.apache.nifi.provenance.authorization.AuthorizationCheck;\n+import org.apache.nifi.provenance.index.EventIndexSearcher;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -46,8 +43,8 @@\n     public static final int MAX_LINEAGE_UUIDS = 100;\n     private static final Logger logger = LoggerFactory.getLogger(LineageQuery.class);\n \n-    public static Set<ProvenanceEventRecord> computeLineageForFlowFiles(final PersistentProvenanceRepository repo, final IndexManager indexManager, final File indexDirectory,\n-            final String lineageIdentifier, final Collection<String> flowFileUuids, final int maxAttributeChars) throws IOException {\n+    public static Set<ProvenanceEventRecord> computeLineageForFlowFiles(final IndexManager indexManager, final File indexDirectory,\n+        final String lineageIdentifier, final Collection<String> flowFileUuids, final DocumentToEventConverter docsToEventConverter) throws IOException {\n         if (requireNonNull(flowFileUuids).size() > MAX_LINEAGE_UUIDS) {\n             throw new IllegalArgumentException(String.format(\"Cannot compute lineage for more than %s FlowFiles. This lineage contains %s.\", MAX_LINEAGE_UUIDS, flowFileUuids.size()));\n         }\n@@ -56,7 +53,7 @@\n             throw new IllegalArgumentException(\"Must specify either Lineage Identifier or FlowFile UUIDs to compute lineage\");\n         }\n \n-        final IndexSearcher searcher;\n+        final EventIndexSearcher searcher;\n         try {\n             searcher = indexManager.borrowIndexSearcher(indexDirectory);\n             try {\n@@ -75,24 +72,18 @@\n \n                 final long searchStart = System.nanoTime();\n                 logger.debug(\"Searching {} for {}\", indexDirectory, flowFileIdQuery);\n-                final TopDocs uuidQueryTopDocs = searcher.search(flowFileIdQuery, MAX_QUERY_RESULTS);\n+                final TopDocs uuidQueryTopDocs = searcher.getIndexSearcher().search(flowFileIdQuery, MAX_QUERY_RESULTS);\n                 final long searchEnd = System.nanoTime();\n \n-                // Always authorized. We do this because we need to pull back the event, regardless of whether or not\n-                // the user is truly authorized, because instead of ignoring unauthorized events, we want to replace them.\n-                final AuthorizationCheck authCheck = event -> true;\n-\n-                final DocsReader docsReader = new DocsReader();\n-                final Set<ProvenanceEventRecord> recs = docsReader.read(uuidQueryTopDocs, authCheck, searcher.getIndexReader(), repo.getAllLogFiles(),\n-                        new AtomicInteger(0), Integer.MAX_VALUE, maxAttributeChars);\n+                final Set<ProvenanceEventRecord> recs = docsToEventConverter.convert(uuidQueryTopDocs, searcher.getIndexSearcher().getIndexReader());\n \n                 final long readDocsEnd = System.nanoTime();\n                 logger.debug(\"Finished Lineage Query against {}; Lucene search took {} millis, reading records took {} millis\",\n                         indexDirectory, TimeUnit.NANOSECONDS.toMillis(searchEnd - searchStart), TimeUnit.NANOSECONDS.toMillis(readDocsEnd - searchEnd));\n \n                 return recs;\n             } finally {\n-                indexManager.returnIndexSearcher(indexDirectory, searcher);\n+                indexManager.returnIndexSearcher(searcher);\n             }\n         } catch (final FileNotFoundException fnfe) {\n             // nothing has been indexed yet, or the data has already aged off",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/LineageQuery.java",
                "sha": "2388483580b377f52ca2035b472c05651cea175c",
                "status": "modified"
            },
            {
                "additions": 92,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/LuceneEventIndexSearcher.java",
                "changes": 92,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/LuceneEventIndexSearcher.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/LuceneEventIndexSearcher.java",
                "patch": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.lucene;\n+\n+import java.io.Closeable;\n+import java.io.File;\n+\n+import org.apache.lucene.index.DirectoryReader;\n+import org.apache.lucene.search.IndexSearcher;\n+import org.apache.lucene.store.Directory;\n+import org.apache.nifi.provenance.index.EventIndexSearcher;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class LuceneEventIndexSearcher implements EventIndexSearcher {\n+    private static final Logger logger = LoggerFactory.getLogger(LuceneEventIndexSearcher.class);\n+\n+    private final IndexSearcher indexSearcher;\n+    private final File indexDirectory;\n+    private final Directory directory;\n+    private final DirectoryReader directoryReader;\n+\n+    // guarded by synchronizing on 'this'\n+    private int usageCounter = 0;\n+    private boolean closed = false;\n+\n+    public LuceneEventIndexSearcher(final IndexSearcher indexSearcher, final File indexDirectory, final Directory directory, final DirectoryReader directoryReader) {\n+        this.indexSearcher = indexSearcher;\n+        this.indexDirectory = indexDirectory;\n+        this.directory = directory;\n+        this.directoryReader = directoryReader;\n+    }\n+\n+    @Override\n+    public IndexSearcher getIndexSearcher() {\n+        return indexSearcher;\n+    }\n+\n+    @Override\n+    public File getIndexDirectory() {\n+        return indexDirectory;\n+    }\n+\n+    @Override\n+    public synchronized void close() {\n+        closed = true;\n+        if (usageCounter == 0) {\n+            closeQuietly(directoryReader);\n+            closeQuietly(directory);\n+        }\n+    }\n+\n+    public synchronized void incrementUsageCounter() {\n+        usageCounter++;\n+    }\n+\n+    public synchronized void decrementUsageCounter() {\n+        usageCounter--;\n+        if (usageCounter == 0 && closed) {\n+            closeQuietly(directoryReader);\n+            closeQuietly(directory);\n+        }\n+    }\n+\n+    private void closeQuietly(final Closeable closeable) {\n+        if (closeable == null) {\n+            return;\n+        }\n+\n+        try {\n+            closeable.close();\n+        } catch (final Exception e) {\n+            logger.warn(\"Failed to close {} due to {}\", closeable, e);\n+        }\n+    }\n+\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/LuceneEventIndexSearcher.java",
                "sha": "07b9167ccafcfc39f01bf8788b9aa3790dc313a0",
                "status": "added"
            },
            {
                "additions": 144,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/LuceneEventIndexWriter.java",
                "changes": 144,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/LuceneEventIndexWriter.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/LuceneEventIndexWriter.java",
                "patch": "@@ -0,0 +1,144 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.lucene;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.nifi.provenance.index.EventIndexWriter;\n+\n+public class LuceneEventIndexWriter implements EventIndexWriter {\n+    private final IndexWriter indexWriter;\n+    private final File directory;\n+    private final long maxCommitNanos;\n+\n+    private final AtomicReference<CommitStats> commitStats = new AtomicReference<>();\n+    private final AtomicLong totalIndexed = new AtomicLong(0L);\n+    private final AtomicLong lastCommitTotalIndexed = new AtomicLong(0L);\n+\n+    public LuceneEventIndexWriter(final IndexWriter indexWriter, final File directory) {\n+        this(indexWriter, directory, TimeUnit.SECONDS.toNanos(30L));\n+    }\n+\n+    public LuceneEventIndexWriter(final IndexWriter indexWriter, final File directory, final long maxCommitNanos) {\n+        this.indexWriter = indexWriter;\n+        this.directory = directory;\n+        this.maxCommitNanos = maxCommitNanos;\n+\n+        commitStats.set(new CommitStats(0, System.nanoTime() + maxCommitNanos));\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        indexWriter.close();\n+    }\n+\n+    @Override\n+    public boolean index(final Document document, final int commitThreshold) throws IOException {\n+        return index(Collections.singletonList(document), commitThreshold);\n+    }\n+\n+    @Override\n+    public boolean index(List<Document> documents, final int commitThreshold) throws IOException {\n+        if (documents.isEmpty()) {\n+            return false;\n+        }\n+\n+        final int numDocs = documents.size();\n+        indexWriter.addDocuments(documents);\n+        totalIndexed.addAndGet(numDocs);\n+\n+        boolean updated = false;\n+        while (!updated) {\n+            final CommitStats stats = commitStats.get();\n+            CommitStats updatedStats = new CommitStats(stats.getIndexedSinceCommit() + numDocs, stats.getNextCommitTimestamp());\n+\n+            if (updatedStats.getIndexedSinceCommit() >= commitThreshold || System.nanoTime() >= updatedStats.getNextCommitTimestamp()) {\n+                updatedStats = new CommitStats(0, System.nanoTime() + maxCommitNanos);\n+                updated = commitStats.compareAndSet(stats, updatedStats);\n+                if (updated) {\n+                    return true;\n+                }\n+            } else {\n+                updated = commitStats.compareAndSet(stats, updatedStats);\n+            }\n+        }\n+\n+        return false;\n+    }\n+\n+    @Override\n+    public File getDirectory() {\n+        return directory;\n+    }\n+\n+    @Override\n+    public long commit() throws IOException {\n+        final long lastCommitCount = lastCommitTotalIndexed.get();\n+        final long currentCommitCount = totalIndexed.get();\n+        indexWriter.commit();\n+        commitStats.set(new CommitStats(0, System.nanoTime() + maxCommitNanos));\n+        lastCommitTotalIndexed.set(currentCommitCount);\n+        return currentCommitCount - lastCommitCount;\n+    }\n+\n+    @Override\n+    public int getEventsIndexedSinceCommit() {\n+        return commitStats.get().getIndexedSinceCommit();\n+    }\n+\n+    @Override\n+    public long getEventsIndexed() {\n+        return totalIndexed.get();\n+    }\n+\n+    @Override\n+    public IndexWriter getIndexWriter() {\n+        return indexWriter;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"LuceneEventIndexWriter[dir=\" + directory + \"]\";\n+    }\n+\n+    private static class CommitStats {\n+        private final long nextCommitTimestamp;\n+        private final int indexedSinceCommit;\n+\n+        public CommitStats(final int indexedCount, final long nextCommitTime) {\n+            this.nextCommitTimestamp = nextCommitTime;\n+            this.indexedSinceCommit = indexedCount;\n+        }\n+\n+        public long getNextCommitTimestamp() {\n+            return nextCommitTimestamp;\n+        }\n+\n+        public int getIndexedSinceCommit() {\n+            return indexedSinceCommit;\n+        }\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/LuceneEventIndexWriter.java",
                "sha": "db8c5287d780145e291ccf5ec6538f016fad841d",
                "status": "added"
            },
            {
                "additions": 249,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/SimpleIndexManager.java",
                "changes": 336,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/SimpleIndexManager.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 87,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/SimpleIndexManager.java",
                "patch": "@@ -24,155 +24,306 @@\n import java.util.HashMap;\n import java.util.List;\n import java.util.Map;\n-import java.util.concurrent.ConcurrentHashMap;\n-import java.util.concurrent.ConcurrentMap;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n import java.util.concurrent.TimeUnit;\n \n import org.apache.lucene.analysis.Analyzer;\n import org.apache.lucene.analysis.standard.StandardAnalyzer;\n+import org.apache.lucene.index.ConcurrentMergeScheduler;\n import org.apache.lucene.index.DirectoryReader;\n import org.apache.lucene.index.IndexWriter;\n import org.apache.lucene.index.IndexWriterConfig;\n import org.apache.lucene.search.IndexSearcher;\n import org.apache.lucene.store.Directory;\n import org.apache.lucene.store.FSDirectory;\n+import org.apache.nifi.provenance.RepositoryConfiguration;\n+import org.apache.nifi.provenance.index.EventIndexSearcher;\n+import org.apache.nifi.provenance.index.EventIndexWriter;\n+import org.apache.nifi.provenance.util.NamedThreadFactory;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n public class SimpleIndexManager implements IndexManager {\n     private static final Logger logger = LoggerFactory.getLogger(SimpleIndexManager.class);\n \n-    private final ConcurrentMap<Object, List<Closeable>> closeables = new ConcurrentHashMap<>();\n-    private final Map<File, IndexWriterCount> writerCounts = new HashMap<>();\n-\n-    private final ExecutorService searchExecutor = Executors.newCachedThreadPool();\n+    private final Map<File, IndexWriterCount> writerCounts = new HashMap<>(); // guarded by synchronizing on map itself\n+    private final ExecutorService searchExecutor;\n+    private final RepositoryConfiguration repoConfig;\n \n+    public SimpleIndexManager(final RepositoryConfiguration repoConfig) {\n+        this.repoConfig = repoConfig;\n+        this.searchExecutor = Executors.newFixedThreadPool(repoConfig.getQueryThreadPoolSize(), new NamedThreadFactory(\"Search Lucene Index\"));\n+    }\n \n     @Override\n     public void close() throws IOException {\n         logger.debug(\"Shutting down SimpleIndexManager search executor\");\n-        this.searchExecutor.shutdown();\n+\n+        searchExecutor.shutdown();\n         try {\n-            if (!this.searchExecutor.awaitTermination(5, TimeUnit.SECONDS)) {\n-                this.searchExecutor.shutdownNow();\n+            if (!searchExecutor.awaitTermination(5, TimeUnit.SECONDS)) {\n+                searchExecutor.shutdownNow();\n             }\n         } catch (InterruptedException e) {\n             Thread.currentThread().interrupt();\n-            this.searchExecutor.shutdownNow();\n+            searchExecutor.shutdownNow();\n         }\n     }\n \n     @Override\n-    public IndexSearcher borrowIndexSearcher(final File indexDir) throws IOException {\n-        logger.debug(\"Creating index searcher for {}\", indexDir);\n-        final Directory directory = FSDirectory.open(indexDir);\n-        final DirectoryReader directoryReader = DirectoryReader.open(directory);\n-        final IndexSearcher searcher = new IndexSearcher(directoryReader, this.searchExecutor);\n+    public EventIndexSearcher borrowIndexSearcher(final File indexDir) throws IOException {\n+        final File absoluteFile = indexDir.getAbsoluteFile();\n+\n+        final IndexWriterCount writerCount;\n+        synchronized (writerCounts) {\n+            writerCount = writerCounts.remove(absoluteFile);\n \n-        final List<Closeable> closeableList = new ArrayList<>(2);\n-        closeableList.add(directoryReader);\n-        closeableList.add(directory);\n-        closeables.put(searcher, closeableList);\n-        logger.debug(\"Created index searcher {} for {}\", searcher, indexDir);\n+            if (writerCount != null) {\n+                // Increment writer count and create an Index Searcher based on the writer\n+                writerCounts.put(absoluteFile, new IndexWriterCount(writerCount.getWriter(), writerCount.getAnalyzer(),\n+                    writerCount.getDirectory(), writerCount.getCount() + 1, writerCount.isCloseableWhenUnused()));\n+            }\n+        }\n+\n+        final DirectoryReader directoryReader;\n+        if (writerCount == null) {\n+            logger.trace(\"Creating index searcher for {}\", indexDir);\n+            final Directory directory = FSDirectory.open(indexDir);\n+            directoryReader = DirectoryReader.open(directory);\n+        } else {\n+            final EventIndexWriter eventIndexWriter = writerCount.getWriter();\n+            directoryReader = DirectoryReader.open(eventIndexWriter.getIndexWriter(), false);\n+        }\n \n-        return searcher;\n+        final IndexSearcher searcher = new IndexSearcher(directoryReader, this.searchExecutor);\n+\n+        logger.trace(\"Created index searcher {} for {}\", searcher, indexDir);\n+        return new LuceneEventIndexSearcher(searcher, indexDir, null, directoryReader);\n     }\n \n     @Override\n-    public void returnIndexSearcher(final File indexDirectory, final IndexSearcher searcher) {\n+    public void returnIndexSearcher(final EventIndexSearcher searcher) {\n+        final File indexDirectory = searcher.getIndexDirectory();\n         logger.debug(\"Closing index searcher {} for {}\", searcher, indexDirectory);\n+        closeQuietly(searcher);\n+        logger.debug(\"Closed index searcher {}\", searcher);\n+\n+        final IndexWriterCount count;\n+        boolean closeWriter = false;\n+        synchronized (writerCounts) {\n+            final File absoluteFile = searcher.getIndexDirectory().getAbsoluteFile();\n+            count = writerCounts.get(absoluteFile);\n+            if (count == null) {\n+                logger.debug(\"Returning EventIndexSearcher for {}; there is no active writer for this searcher so will not decrement writerCounts\", absoluteFile);\n+                return;\n+            }\n \n-        final List<Closeable> closeableList = closeables.get(searcher);\n-        if (closeableList != null) {\n-            for (final Closeable closeable : closeableList) {\n-                closeQuietly(closeable);\n+            if (count.getCount() <= 1) {\n+                // we are finished with this writer.\n+                final boolean close = count.isCloseableWhenUnused();\n+                logger.debug(\"Decrementing count for Index Writer for {} to {}{}\", indexDirectory, count.getCount() - 1, close ? \"; closing writer\" : \"\");\n+\n+                if (close) {\n+                    writerCounts.remove(absoluteFile);\n+                    closeWriter = true;\n+                } else {\n+                    writerCounts.put(absoluteFile, new IndexWriterCount(count.getWriter(), count.getAnalyzer(), count.getDirectory(),\n+                        count.getCount() - 1, count.isCloseableWhenUnused()));\n+                }\n+            } else {\n+                writerCounts.put(absoluteFile, new IndexWriterCount(count.getWriter(), count.getAnalyzer(), count.getDirectory(),\n+                    count.getCount() - 1, count.isCloseableWhenUnused()));\n             }\n         }\n \n-        logger.debug(\"Closed index searcher {}\", searcher);\n+        if (closeWriter) {\n+            try {\n+                close(count);\n+            } catch (final Exception e) {\n+                logger.warn(\"Failed to close Index Writer {} due to {}\", count.getWriter(), e.toString(), e);\n+            }\n+        }\n     }\n \n     @Override\n-    public void removeIndex(final File indexDirectory) {\n+    public boolean removeIndex(final File indexDirectory) {\n+        final File absoluteFile = indexDirectory.getAbsoluteFile();\n+        logger.debug(\"Attempting to remove index {} from SimpleIndexManager\", absoluteFile);\n+\n+        IndexWriterCount writerCount;\n+        synchronized (writerCounts) {\n+            writerCount = writerCounts.remove(absoluteFile);\n+            if (writerCount == null) {\n+                logger.debug(\"Allowing removal of index {} because there is no IndexWriterCount for this directory\", absoluteFile);\n+                return true; // return true since directory has no writers\n+            }\n+\n+            if (writerCount.getCount() > 0) {\n+                logger.debug(\"Not allowing removal of index {} because the active writer count for this directory is {}\", absoluteFile, writerCount.getCount());\n+                writerCounts.put(absoluteFile, writerCount);\n+                return false;\n+            }\n+        }\n+\n+        try {\n+            logger.debug(\"Removing index {} from SimpleIndexManager and closing the writer\", absoluteFile);\n+\n+            close(writerCount);\n+        } catch (final Exception e) {\n+            logger.error(\"Failed to close Index Writer for {} while removing Index from the repository;\"\n+                + \"this directory may need to be cleaned up manually.\", e);\n+        }\n+\n+        return true;\n     }\n \n \n-    @Override\n-    public synchronized IndexWriter borrowIndexWriter(final File indexingDirectory) throws IOException {\n-        final File absoluteFile = indexingDirectory.getAbsoluteFile();\n-        logger.trace(\"Borrowing index writer for {}\", indexingDirectory);\n+    private IndexWriterCount createWriter(final File indexDirectory) throws IOException {\n+        final List<Closeable> closeables = new ArrayList<>();\n+        final Directory directory = FSDirectory.open(indexDirectory);\n+        closeables.add(directory);\n \n-        IndexWriterCount writerCount = writerCounts.remove(absoluteFile);\n-        if (writerCount == null) {\n-            final List<Closeable> closeables = new ArrayList<>();\n-            final Directory directory = FSDirectory.open(indexingDirectory);\n-            closeables.add(directory);\n+        try {\n+            final Analyzer analyzer = new StandardAnalyzer();\n+            closeables.add(analyzer);\n \n-            try {\n-                final Analyzer analyzer = new StandardAnalyzer();\n-                closeables.add(analyzer);\n-\n-                final IndexWriterConfig config = new IndexWriterConfig(LuceneUtil.LUCENE_VERSION, analyzer);\n-                config.setWriteLockTimeout(300000L);\n-\n-                final IndexWriter indexWriter = new IndexWriter(directory, config);\n-                writerCount = new IndexWriterCount(indexWriter, analyzer, directory, 1);\n-                logger.debug(\"Providing new index writer for {}\", indexingDirectory);\n-            } catch (final IOException ioe) {\n-                for (final Closeable closeable : closeables) {\n-                    try {\n-                        closeable.close();\n-                    } catch (final IOException ioe2) {\n-                        ioe.addSuppressed(ioe2);\n-                    }\n+            final IndexWriterConfig config = new IndexWriterConfig(LuceneUtil.LUCENE_VERSION, analyzer);\n+\n+            final ConcurrentMergeScheduler mergeScheduler = new ConcurrentMergeScheduler();\n+            final int mergeThreads = repoConfig.getConcurrentMergeThreads();\n+            mergeScheduler.setMaxMergesAndThreads(mergeThreads, mergeThreads);\n+            config.setMergeScheduler(mergeScheduler);\n+\n+            final IndexWriter indexWriter = new IndexWriter(directory, config);\n+            final EventIndexWriter eventIndexWriter = new LuceneEventIndexWriter(indexWriter, indexDirectory);\n+\n+            final IndexWriterCount writerCount = new IndexWriterCount(eventIndexWriter, analyzer, directory, 1, false);\n+            logger.debug(\"Providing new index writer for {}\", indexDirectory);\n+            return writerCount;\n+        } catch (final IOException ioe) {\n+            for (final Closeable closeable : closeables) {\n+                try {\n+                    closeable.close();\n+                } catch (final IOException ioe2) {\n+                    ioe.addSuppressed(ioe2);\n                 }\n+            }\n+\n+            throw ioe;\n+        }\n+    }\n \n-                throw ioe;\n+    @Override\n+    public EventIndexWriter borrowIndexWriter(final File indexDirectory) throws IOException {\n+        final File absoluteFile = indexDirectory.getAbsoluteFile();\n+        logger.trace(\"Borrowing index writer for {}\", indexDirectory);\n+\n+        IndexWriterCount writerCount = null;\n+        synchronized (writerCounts) {\n+            writerCount = writerCounts.get(absoluteFile);\n+\n+            if (writerCount == null) {\n+                writerCount = createWriter(indexDirectory);\n+                writerCounts.put(absoluteFile, writerCount);\n+            } else {\n+                logger.trace(\"Providing existing index writer for {} and incrementing count to {}\", indexDirectory, writerCount.getCount() + 1);\n+                writerCounts.put(absoluteFile, new IndexWriterCount(writerCount.getWriter(),\n+                    writerCount.getAnalyzer(), writerCount.getDirectory(), writerCount.getCount() + 1, writerCount.isCloseableWhenUnused()));\n             }\n \n-            writerCounts.put(absoluteFile, writerCount);\n-        } else {\n-            logger.debug(\"Providing existing index writer for {} and incrementing count to {}\", indexingDirectory, writerCount.getCount() + 1);\n-            writerCounts.put(absoluteFile, new IndexWriterCount(writerCount.getWriter(),\n-                writerCount.getAnalyzer(), writerCount.getDirectory(), writerCount.getCount() + 1));\n+            if (writerCounts.size() > repoConfig.getStorageDirectories().size() * 2) {\n+                logger.debug(\"Index Writer returned; writer count map now has size {}; writerCount = {}; full writerCounts map = {}\",\n+                    writerCounts.size(), writerCount, writerCounts);\n+            }\n         }\n \n         return writerCount.getWriter();\n     }\n \n+    @Override\n+    public void returnIndexWriter(final EventIndexWriter writer) {\n+        returnIndexWriter(writer, true, true);\n+    }\n \n     @Override\n-    public synchronized void returnIndexWriter(final File indexingDirectory, final IndexWriter writer) {\n-        final File absoluteFile = indexingDirectory.getAbsoluteFile();\n-        logger.trace(\"Returning Index Writer for {} to IndexManager\", indexingDirectory);\n+    public void returnIndexWriter(final EventIndexWriter writer, final boolean commit, final boolean isCloseable) {\n+        final File indexDirectory = writer.getDirectory();\n+        final File absoluteFile = indexDirectory.getAbsoluteFile();\n+        logger.trace(\"Returning Index Writer for {} to IndexManager\", indexDirectory);\n+\n+        boolean unused = false;\n+        IndexWriterCount count = null;\n+        boolean close = isCloseable;\n+        try {\n+            synchronized (writerCounts) {\n+                count = writerCounts.get(absoluteFile);\n+                if (count != null && count.isCloseableWhenUnused()) {\n+                    close = true;\n+                }\n \n-        final IndexWriterCount count = writerCounts.remove(absoluteFile);\n+                if (count == null) {\n+                    logger.warn(\"Index Writer {} was returned to IndexManager for {}, but this writer is not known. \"\n+                        + \"This could potentially lead to a resource leak\", writer, indexDirectory);\n+                    writer.close();\n+                } else if (count.getCount() <= 1) {\n+                    // we are finished with this writer.\n+                    unused = true;\n+                    if (close) {\n+                        logger.debug(\"Decrementing count for Index Writer for {} to {}; closing writer\", indexDirectory, count.getCount() - 1);\n+                        writerCounts.remove(absoluteFile);\n+                    } else {\n+                        logger.trace(\"Decrementing count for Index Writer for {} to {}\", indexDirectory, count.getCount() - 1);\n+\n+                        // If writer is not closeable, then we need to decrement its count.\n+                        writerCounts.put(absoluteFile, new IndexWriterCount(count.getWriter(), count.getAnalyzer(), count.getDirectory(),\n+                            count.getCount() - 1, close));\n+                    }\n+                } else {\n+                    // decrement the count.\n+                    if (close) {\n+                        logger.debug(\"Decrementing count for Index Writer for {} to {} and marking as closeable when no longer in use\", indexDirectory, count.getCount() - 1);\n+                    } else {\n+                        logger.trace(\"Decrementing count for Index Writer for {} to {}\", indexDirectory, count.getCount() - 1);\n+                    }\n \n-        try {\n-            if (count == null) {\n-                logger.warn(\"Index Writer {} was returned to IndexManager for {}, but this writer is not known. \"\n-                    + \"This could potentially lead to a resource leak\", writer, indexingDirectory);\n-                writer.close();\n-            } else if (count.getCount() <= 1) {\n-                // we are finished with this writer.\n-                logger.debug(\"Decrementing count for Index Writer for {} to {}; Closing writer\", indexingDirectory, count.getCount() - 1);\n+                    writerCounts.put(absoluteFile, new IndexWriterCount(count.getWriter(), count.getAnalyzer(),\n+                        count.getDirectory(), count.getCount() - 1, close));\n+                }\n+\n+                if (writerCounts.size() > repoConfig.getStorageDirectories().size() * 2) {\n+                    logger.debug(\"Index Writer returned; writer count map now has size {}; writer = {}, commit = {}, isCloseable = {}, writerCount = {}; full writerCounts Map = {}\",\n+                        writerCounts.size(), writer, commit, isCloseable, count, writerCounts);\n+                }\n+            }\n+\n+            // Committing and closing are very expensive, so we want to do those outside of the synchronized block.\n+            // So we use an 'unused' variable to tell us whether or not we should actually do so.\n+            if (unused) {\n                 try {\n-                    writer.commit();\n+                    if (commit) {\n+                        writer.commit();\n+                    }\n                 } finally {\n-                    count.close();\n+                    if (close) {\n+                        logger.info(\"Index Writer for {} has been returned to Index Manager and is no longer in use. Closing Index Writer\", indexDirectory);\n+                        close(count);\n+                    }\n                 }\n-            } else {\n-                // decrement the count.\n-                logger.debug(\"Decrementing count for Index Writer for {} to {}\", indexingDirectory, count.getCount() - 1);\n-                writerCounts.put(absoluteFile, new IndexWriterCount(count.getWriter(), count.getAnalyzer(), count.getDirectory(), count.getCount() - 1));\n-            }\n-        } catch (final IOException ioe) {\n-            logger.warn(\"Failed to close Index Writer {} due to {}\", writer, ioe);\n-            if (logger.isDebugEnabled()) {\n-                logger.warn(\"\", ioe);\n             }\n+        } catch (final Exception e) {\n+            logger.warn(\"Failed to close Index Writer {} due to {}\", writer, e.toString(), e);\n+        }\n+    }\n+\n+    // This method exists solely for unit testing purposes.\n+    protected void close(final IndexWriterCount count) throws IOException {\n+        count.close();\n+    }\n+\n+    protected int getWriterCount() {\n+        synchronized (writerCounts) {\n+            return writerCounts.size();\n         }\n     }\n \n@@ -191,17 +342,23 @@ private static void closeQuietly(final Closeable... closeables) {\n     }\n \n \n-    private static class IndexWriterCount implements Closeable {\n-        private final IndexWriter writer;\n+    protected static class IndexWriterCount implements Closeable {\n+        private final EventIndexWriter writer;\n         private final Analyzer analyzer;\n         private final Directory directory;\n         private final int count;\n+        private final boolean closeableWhenUnused;\n \n-        public IndexWriterCount(final IndexWriter writer, final Analyzer analyzer, final Directory directory, final int count) {\n+        public IndexWriterCount(final EventIndexWriter writer, final Analyzer analyzer, final Directory directory, final int count, final boolean closeableWhenUnused) {\n             this.writer = writer;\n             this.analyzer = analyzer;\n             this.directory = directory;\n             this.count = count;\n+            this.closeableWhenUnused = closeableWhenUnused;\n+        }\n+\n+        public boolean isCloseableWhenUnused() {\n+            return closeableWhenUnused;\n         }\n \n         public Analyzer getAnalyzer() {\n@@ -212,7 +369,7 @@ public Directory getDirectory() {\n             return directory;\n         }\n \n-        public IndexWriter getWriter() {\n+        public EventIndexWriter getWriter() {\n             return writer;\n         }\n \n@@ -224,5 +381,10 @@ public int getCount() {\n         public void close() throws IOException {\n             closeQuietly(writer, analyzer, directory);\n         }\n+\n+        @Override\n+        public String toString() {\n+            return \"IndexWriterCount[count=\" + count + \", writer=\" + writer + \", closeableWhenUnused=\" + closeableWhenUnused + \"]\";\n+        }\n     }\n }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/lucene/SimpleIndexManager.java",
                "sha": "b0b01e592baf8d443a108736811ecfd3d3463728",
                "status": "modified"
            },
            {
                "additions": 59,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/EventFieldNames.java",
                "changes": 59,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/EventFieldNames.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/EventFieldNames.java",
                "patch": "@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.schema;\n+\n+public class EventFieldNames {\n+    public static final String EVENT_IDENTIFIER = \"Event ID\";\n+    public static final String EVENT_TYPE = \"Event Type\";\n+    public static final String EVENT_TIME = \"Event Time\";\n+    public static final String FLOWFILE_ENTRY_DATE = \"FlowFile Entry Date\";\n+    public static final String EVENT_DURATION = \"Event Duration\";\n+    public static final String LINEAGE_START_DATE = \"Lineage Start Date\";\n+    public static final String COMPONENT_ID = \"Component ID\";\n+    public static final String COMPONENT_TYPE = \"Component Type\";\n+    public static final String FLOWFILE_UUID = \"FlowFile UUID\";\n+    public static final String EVENT_DETAILS = \"Event Details\";\n+    public static final String SOURCE_QUEUE_IDENTIFIER = \"Source Queue Identifier\";\n+    public static final String CONTENT_CLAIM = \"Content Claim\";\n+    public static final String PREVIOUS_CONTENT_CLAIM = \"Previous Content Claim\";\n+    public static final String EXPLICIT_CURRENT_CONTENT_CLAIM = \"Full Current Content Claim\";\n+    public static final String PARENT_UUIDS = \"Parent UUIDs\";\n+    public static final String CHILD_UUIDS = \"Child UUIDs\";\n+\n+    public static final String ATTRIBUTE_NAME = \"Attribute Name\";\n+    public static final String ATTRIBUTE_VALUE = \"Attribute Value\";\n+    public static final String PREVIOUS_ATTRIBUTES = \"Previous Attributes\";\n+    public static final String UPDATED_ATTRIBUTES = \"Updated Attributes\";\n+\n+    public static final String CONTENT_CLAIM_CONTAINER = \"Content Claim Container\";\n+    public static final String CONTENT_CLAIM_SECTION = \"Content Claim Section\";\n+    public static final String CONTENT_CLAIM_IDENTIFIER = \"Content Claim Identifier\";\n+    public static final String CONTENT_CLAIM_OFFSET = \"Content Claim Offset\";\n+    public static final String CONTENT_CLAIM_SIZE = \"Content Claim Size\";\n+\n+    public static final String TRANSIT_URI = \"Transit URI\";\n+    public static final String SOURCE_SYSTEM_FLOWFILE_IDENTIFIER = \"Source System FlowFile Identifier\";\n+    public static final String ALTERNATE_IDENTIFIER = \"Alternate Identifier\";\n+    public static final String RELATIONSHIP = \"Relationship\";\n+\n+    // For Lookup Tables\n+    public static final String NO_VALUE = \"No Value\";\n+    public static final String EXPLICIT_VALUE = \"Explicit Value\";\n+    public static final String LOOKUP_VALUE = \"Lookup Value\";\n+    public static final String UNCHANGED_VALUE = \"Unchanged\";\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/EventFieldNames.java",
                "sha": "d6f50dda2e4ebc6fb3b88f4f980a890dc8afc8ac",
                "status": "added"
            },
            {
                "additions": 52,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/EventIdFirstHeaderSchema.java",
                "changes": 52,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/EventIdFirstHeaderSchema.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/EventIdFirstHeaderSchema.java",
                "patch": "@@ -0,0 +1,52 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.schema;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+\n+import org.apache.nifi.repository.schema.FieldType;\n+import org.apache.nifi.repository.schema.RecordField;\n+import org.apache.nifi.repository.schema.RecordSchema;\n+import org.apache.nifi.repository.schema.Repetition;\n+import org.apache.nifi.repository.schema.SimpleRecordField;\n+\n+public class EventIdFirstHeaderSchema {\n+\n+    public static RecordSchema SCHEMA = buildSchema();\n+\n+    public static final class FieldNames {\n+        public static final String FIRST_EVENT_ID = \"First Event ID\";\n+        public static final String TIMESTAMP_OFFSET = \"Timestamp Offset\";\n+        public static final String COMPONENT_IDS = \"Component Identifiers\";\n+        public static final String COMPONENT_TYPES = \"Component Types\";\n+        public static final String QUEUE_IDS = \"Queue Identifiers\";\n+        public static final String EVENT_TYPES = \"Event Types\";\n+    }\n+\n+    private static RecordSchema buildSchema() {\n+        final List<RecordField> fields = new ArrayList<>();\n+        fields.add(new SimpleRecordField(FieldNames.FIRST_EVENT_ID, FieldType.LONG, Repetition.EXACTLY_ONE));\n+        fields.add(new SimpleRecordField(FieldNames.TIMESTAMP_OFFSET, FieldType.LONG, Repetition.EXACTLY_ONE));\n+        fields.add(new SimpleRecordField(FieldNames.COMPONENT_IDS, FieldType.STRING, Repetition.ZERO_OR_MORE));\n+        fields.add(new SimpleRecordField(FieldNames.COMPONENT_TYPES, FieldType.STRING, Repetition.ZERO_OR_MORE));\n+        fields.add(new SimpleRecordField(FieldNames.QUEUE_IDS, FieldType.STRING, Repetition.ZERO_OR_MORE));\n+        fields.add(new SimpleRecordField(FieldNames.EVENT_TYPES, FieldType.STRING, Repetition.ZERO_OR_MORE));\n+        return new RecordSchema(fields);\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/EventIdFirstHeaderSchema.java",
                "sha": "1c35c5aaaa1de9a879b7ac50022b61ed96ff7329",
                "status": "added"
            },
            {
                "additions": 56,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/EventRecord.java",
                "changes": 108,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/EventRecord.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 52,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/EventRecord.java",
                "patch": "@@ -69,47 +69,47 @@ private static Record createContentClaimRecord(final RecordSchema contentClaimSc\n     @Override\n     public Object getFieldValue(final String fieldName) {\n         switch (fieldName) {\n-            case EventRecordFields.Names.EVENT_IDENTIFIER:\n+            case EventFieldNames.EVENT_IDENTIFIER:\n                 return eventId;\n-            case EventRecordFields.Names.ALTERNATE_IDENTIFIER:\n+            case EventFieldNames.ALTERNATE_IDENTIFIER:\n                 return event.getAlternateIdentifierUri();\n-            case EventRecordFields.Names.CHILD_UUIDS:\n+            case EventFieldNames.CHILD_UUIDS:\n                 return event.getChildUuids();\n-            case EventRecordFields.Names.COMPONENT_ID:\n+            case EventFieldNames.COMPONENT_ID:\n                 return event.getComponentId();\n-            case EventRecordFields.Names.COMPONENT_TYPE:\n+            case EventFieldNames.COMPONENT_TYPE:\n                 return event.getComponentType();\n-            case EventRecordFields.Names.CONTENT_CLAIM:\n+            case EventFieldNames.CONTENT_CLAIM:\n                 return contentClaimRecord;\n-            case EventRecordFields.Names.EVENT_DETAILS:\n+            case EventFieldNames.EVENT_DETAILS:\n                 return event.getDetails();\n-            case EventRecordFields.Names.EVENT_DURATION:\n+            case EventFieldNames.EVENT_DURATION:\n                 return event.getEventDuration();\n-            case EventRecordFields.Names.EVENT_TIME:\n+            case EventFieldNames.EVENT_TIME:\n                 return event.getEventTime();\n-            case EventRecordFields.Names.EVENT_TYPE:\n+            case EventFieldNames.EVENT_TYPE:\n                 return event.getEventType().name();\n-            case EventRecordFields.Names.FLOWFILE_ENTRY_DATE:\n+            case EventFieldNames.FLOWFILE_ENTRY_DATE:\n                 return event.getFlowFileEntryDate();\n-            case EventRecordFields.Names.FLOWFILE_UUID:\n+            case EventFieldNames.FLOWFILE_UUID:\n                 return event.getFlowFileUuid();\n-            case EventRecordFields.Names.LINEAGE_START_DATE:\n+            case EventFieldNames.LINEAGE_START_DATE:\n                 return event.getLineageStartDate();\n-            case EventRecordFields.Names.PARENT_UUIDS:\n+            case EventFieldNames.PARENT_UUIDS:\n                 return event.getParentUuids();\n-            case EventRecordFields.Names.PREVIOUS_ATTRIBUTES:\n+            case EventFieldNames.PREVIOUS_ATTRIBUTES:\n                 return event.getPreviousAttributes();\n-            case EventRecordFields.Names.PREVIOUS_CONTENT_CLAIM:\n+            case EventFieldNames.PREVIOUS_CONTENT_CLAIM:\n                 return previousClaimRecord;\n-            case EventRecordFields.Names.RELATIONSHIP:\n+            case EventFieldNames.RELATIONSHIP:\n                 return event.getRelationship();\n-            case EventRecordFields.Names.SOURCE_QUEUE_IDENTIFIER:\n+            case EventFieldNames.SOURCE_QUEUE_IDENTIFIER:\n                 return event.getSourceQueueIdentifier();\n-            case EventRecordFields.Names.SOURCE_SYSTEM_FLOWFILE_IDENTIFIER:\n+            case EventFieldNames.SOURCE_SYSTEM_FLOWFILE_IDENTIFIER:\n                 return event.getSourceSystemFlowFileIdentifier();\n-            case EventRecordFields.Names.TRANSIT_URI:\n+            case EventFieldNames.TRANSIT_URI:\n                 return event.getTransitUri();\n-            case EventRecordFields.Names.UPDATED_ATTRIBUTES:\n+            case EventFieldNames.UPDATED_ATTRIBUTES:\n                 return event.getUpdatedAttributes();\n         }\n \n@@ -119,48 +119,52 @@ public Object getFieldValue(final String fieldName) {\n     @SuppressWarnings(\"unchecked\")\n     public static StandardProvenanceEventRecord getEvent(final Record record, final String storageFilename, final long storageByteOffset, final int maxAttributeLength) {\n         final StandardProvenanceEventRecord.Builder builder = new StandardProvenanceEventRecord.Builder();\n-        builder.setAlternateIdentifierUri((String) record.getFieldValue(EventRecordFields.Names.ALTERNATE_IDENTIFIER));\n-        builder.setChildUuids((List<String>) record.getFieldValue(EventRecordFields.Names.CHILD_UUIDS));\n-        builder.setComponentId((String) record.getFieldValue(EventRecordFields.Names.COMPONENT_ID));\n-        builder.setComponentType((String) record.getFieldValue(EventRecordFields.Names.COMPONENT_TYPE));\n-        builder.setDetails((String) record.getFieldValue(EventRecordFields.Names.EVENT_DETAILS));\n-        builder.setEventDuration((Long) record.getFieldValue(EventRecordFields.Names.EVENT_DURATION));\n-        builder.setEventTime((Long) record.getFieldValue(EventRecordFields.Names.EVENT_TIME));\n-        builder.setEventType(ProvenanceEventType.valueOf((String) record.getFieldValue(EventRecordFields.Names.EVENT_TYPE)));\n-        builder.setFlowFileEntryDate((Long) record.getFieldValue(EventRecordFields.Names.FLOWFILE_ENTRY_DATE));\n-        builder.setFlowFileUUID((String) record.getFieldValue(EventRecordFields.Names.FLOWFILE_UUID));\n-        builder.setLineageStartDate((Long) record.getFieldValue(EventRecordFields.Names.LINEAGE_START_DATE));\n-        builder.setParentUuids((List<String>) record.getFieldValue(EventRecordFields.Names.PARENT_UUIDS));\n-        builder.setPreviousAttributes(truncateAttributes((Map<String, String>) record.getFieldValue(EventRecordFields.Names.PREVIOUS_ATTRIBUTES), maxAttributeLength));\n-        builder.setEventId((Long) record.getFieldValue(EventRecordFields.Names.EVENT_IDENTIFIER));\n-        builder.setRelationship((String) record.getFieldValue(EventRecordFields.Names.RELATIONSHIP));\n-        builder.setSourceQueueIdentifier((String) record.getFieldValue(EventRecordFields.Names.SOURCE_QUEUE_IDENTIFIER));\n-        builder.setSourceSystemFlowFileIdentifier((String) record.getFieldValue(EventRecordFields.Names.SOURCE_SYSTEM_FLOWFILE_IDENTIFIER));\n-        builder.setTransitUri((String) record.getFieldValue(EventRecordFields.Names.TRANSIT_URI));\n-        builder.setUpdatedAttributes(truncateAttributes((Map<String, String>) record.getFieldValue(EventRecordFields.Names.UPDATED_ATTRIBUTES), maxAttributeLength));\n+        builder.setAlternateIdentifierUri((String) record.getFieldValue(EventFieldNames.ALTERNATE_IDENTIFIER));\n+        builder.setChildUuids((List<String>) record.getFieldValue(EventFieldNames.CHILD_UUIDS));\n+        builder.setComponentId((String) record.getFieldValue(EventFieldNames.COMPONENT_ID));\n+        builder.setComponentType((String) record.getFieldValue(EventFieldNames.COMPONENT_TYPE));\n+        builder.setDetails((String) record.getFieldValue(EventFieldNames.EVENT_DETAILS));\n+        builder.setEventDuration((Long) record.getFieldValue(EventFieldNames.EVENT_DURATION));\n+        builder.setEventTime((Long) record.getFieldValue(EventFieldNames.EVENT_TIME));\n+        builder.setEventType(ProvenanceEventType.valueOf((String) record.getFieldValue(EventFieldNames.EVENT_TYPE)));\n+        builder.setFlowFileEntryDate((Long) record.getFieldValue(EventFieldNames.FLOWFILE_ENTRY_DATE));\n+        builder.setFlowFileUUID((String) record.getFieldValue(EventFieldNames.FLOWFILE_UUID));\n+        builder.setLineageStartDate((Long) record.getFieldValue(EventFieldNames.LINEAGE_START_DATE));\n+        builder.setParentUuids((List<String>) record.getFieldValue(EventFieldNames.PARENT_UUIDS));\n+        builder.setPreviousAttributes(truncateAttributes((Map<String, String>) record.getFieldValue(EventFieldNames.PREVIOUS_ATTRIBUTES), maxAttributeLength));\n+        builder.setRelationship((String) record.getFieldValue(EventFieldNames.RELATIONSHIP));\n+        builder.setSourceQueueIdentifier((String) record.getFieldValue(EventFieldNames.SOURCE_QUEUE_IDENTIFIER));\n+        builder.setSourceSystemFlowFileIdentifier((String) record.getFieldValue(EventFieldNames.SOURCE_SYSTEM_FLOWFILE_IDENTIFIER));\n+        builder.setTransitUri((String) record.getFieldValue(EventFieldNames.TRANSIT_URI));\n+        builder.setUpdatedAttributes(truncateAttributes((Map<String, String>) record.getFieldValue(EventFieldNames.UPDATED_ATTRIBUTES), maxAttributeLength));\n+\n+        final Long eventId = (Long) record.getFieldValue(EventFieldNames.EVENT_IDENTIFIER);\n+        if (eventId != null) {\n+            builder.setEventId(eventId);\n+        }\n \n         builder.setStorageLocation(storageFilename, storageByteOffset);\n \n-        final Record currentClaimRecord = (Record) record.getFieldValue(EventRecordFields.Names.CONTENT_CLAIM);\n+        final Record currentClaimRecord = (Record) record.getFieldValue(EventFieldNames.CONTENT_CLAIM);\n         if (currentClaimRecord == null) {\n             builder.setCurrentContentClaim(null, null, null, null, 0L);\n         } else {\n             builder.setCurrentContentClaim(\n-                (String) currentClaimRecord.getFieldValue(EventRecordFields.Names.CONTENT_CLAIM_CONTAINER),\n-                (String) currentClaimRecord.getFieldValue(EventRecordFields.Names.CONTENT_CLAIM_SECTION),\n-                (String) currentClaimRecord.getFieldValue(EventRecordFields.Names.CONTENT_CLAIM_IDENTIFIER),\n-                (Long) currentClaimRecord.getFieldValue(EventRecordFields.Names.CONTENT_CLAIM_OFFSET),\n-                (Long) currentClaimRecord.getFieldValue(EventRecordFields.Names.CONTENT_CLAIM_SIZE));\n+                (String) currentClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_CONTAINER),\n+                (String) currentClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_SECTION),\n+                (String) currentClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_IDENTIFIER),\n+                (Long) currentClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_OFFSET),\n+                (Long) currentClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_SIZE));\n         }\n \n-        final Record previousClaimRecord = (Record) record.getFieldValue(EventRecordFields.Names.PREVIOUS_CONTENT_CLAIM);\n+        final Record previousClaimRecord = (Record) record.getFieldValue(EventFieldNames.PREVIOUS_CONTENT_CLAIM);\n         if (previousClaimRecord != null) {\n             builder.setPreviousContentClaim(\n-                (String) previousClaimRecord.getFieldValue(EventRecordFields.Names.CONTENT_CLAIM_CONTAINER),\n-                (String) previousClaimRecord.getFieldValue(EventRecordFields.Names.CONTENT_CLAIM_SECTION),\n-                (String) previousClaimRecord.getFieldValue(EventRecordFields.Names.CONTENT_CLAIM_IDENTIFIER),\n-                (Long) previousClaimRecord.getFieldValue(EventRecordFields.Names.CONTENT_CLAIM_OFFSET),\n-                (Long) previousClaimRecord.getFieldValue(EventRecordFields.Names.CONTENT_CLAIM_SIZE));\n+                (String) previousClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_CONTAINER),\n+                (String) previousClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_SECTION),\n+                (String) previousClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_IDENTIFIER),\n+                (Long) previousClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_OFFSET),\n+                (Long) previousClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_SIZE));\n         }\n \n         return builder.build();",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/EventRecord.java",
                "sha": "8c82b110fdec87036ddf7b4f7aff325f21be6df1",
                "status": "modified"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/EventRecordFields.java",
                "changes": 92,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/EventRecordFields.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 63,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/EventRecordFields.java",
                "patch": "@@ -29,82 +29,48 @@\n \n public class EventRecordFields {\n \n-    public static class Names {\n-        public static final String EVENT_IDENTIFIER = \"Event ID\";\n-        public static final String EVENT_TYPE = \"Event Type\";\n-        public static final String EVENT_TIME = \"Event Time\";\n-        public static final String FLOWFILE_ENTRY_DATE = \"FlowFile Entry Date\";\n-        public static final String EVENT_DURATION = \"Event Duration\";\n-        public static final String LINEAGE_START_DATE = \"Lineage Start Date\";\n-        public static final String COMPONENT_ID = \"Component ID\";\n-        public static final String COMPONENT_TYPE = \"Component Type\";\n-        public static final String FLOWFILE_UUID = \"FlowFile UUID\";\n-        public static final String EVENT_DETAILS = \"Event Details\";\n-        public static final String SOURCE_QUEUE_IDENTIFIER = \"Source Queue Identifier\";\n-        public static final String CONTENT_CLAIM = \"Content Claim\";\n-        public static final String PREVIOUS_CONTENT_CLAIM = \"Previous Content Claim\";\n-        public static final String PARENT_UUIDS = \"Parent UUIDs\";\n-        public static final String CHILD_UUIDS = \"Child UUIDs\";\n-\n-        public static final String ATTRIBUTE_NAME = \"Attribute Name\";\n-        public static final String ATTRIBUTE_VALUE = \"Attribute Value\";\n-        public static final String PREVIOUS_ATTRIBUTES = \"Previous Attributes\";\n-        public static final String UPDATED_ATTRIBUTES = \"Updated Attributes\";\n-\n-        public static final String CONTENT_CLAIM_CONTAINER = \"Content Claim Container\";\n-        public static final String CONTENT_CLAIM_SECTION = \"Content Claim Section\";\n-        public static final String CONTENT_CLAIM_IDENTIFIER = \"Content Claim Identifier\";\n-        public static final String CONTENT_CLAIM_OFFSET = \"Content Claim Offset\";\n-        public static final String CONTENT_CLAIM_SIZE = \"Content Claim Size\";\n-\n-        public static final String TRANSIT_URI = \"Transit URI\";\n-        public static final String SOURCE_SYSTEM_FLOWFILE_IDENTIFIER = \"Source System FlowFile Identifier\";\n-        public static final String ALTERNATE_IDENTIFIER = \"Alternate Identifier\";\n-        public static final String RELATIONSHIP = \"Relationship\";\n-    }\n-\n     // General Event fields.\n-    public static final RecordField RECORD_IDENTIFIER = new SimpleRecordField(Names.EVENT_IDENTIFIER, FieldType.LONG, EXACTLY_ONE);\n-    public static final RecordField EVENT_TYPE = new SimpleRecordField(Names.EVENT_TYPE, FieldType.STRING, EXACTLY_ONE);\n-    public static final RecordField EVENT_TIME = new SimpleRecordField(Names.EVENT_TIME, FieldType.LONG, EXACTLY_ONE);\n-    public static final RecordField FLOWFILE_ENTRY_DATE = new SimpleRecordField(Names.FLOWFILE_ENTRY_DATE, FieldType.LONG, EXACTLY_ONE);\n-    public static final RecordField EVENT_DURATION = new SimpleRecordField(Names.EVENT_DURATION, FieldType.LONG, EXACTLY_ONE);\n-    public static final RecordField LINEAGE_START_DATE = new SimpleRecordField(Names.LINEAGE_START_DATE, FieldType.LONG, EXACTLY_ONE);\n-    public static final RecordField COMPONENT_ID = new SimpleRecordField(Names.COMPONENT_ID, FieldType.STRING, ZERO_OR_ONE);\n-    public static final RecordField COMPONENT_TYPE = new SimpleRecordField(Names.COMPONENT_TYPE, FieldType.STRING, ZERO_OR_ONE);\n-    public static final RecordField FLOWFILE_UUID = new SimpleRecordField(Names.FLOWFILE_UUID, FieldType.STRING, EXACTLY_ONE);\n-    public static final RecordField EVENT_DETAILS = new SimpleRecordField(Names.EVENT_DETAILS, FieldType.STRING, ZERO_OR_ONE);\n-    public static final RecordField SOURCE_QUEUE_IDENTIFIER = new SimpleRecordField(Names.SOURCE_QUEUE_IDENTIFIER, FieldType.STRING, ZERO_OR_ONE);\n+    public static final RecordField RECORD_IDENTIFIER = new SimpleRecordField(EventFieldNames.EVENT_IDENTIFIER, FieldType.LONG, EXACTLY_ONE);\n+    public static final RecordField EVENT_TYPE = new SimpleRecordField(EventFieldNames.EVENT_TYPE, FieldType.STRING, EXACTLY_ONE);\n+    public static final RecordField EVENT_TIME = new SimpleRecordField(EventFieldNames.EVENT_TIME, FieldType.LONG, EXACTLY_ONE);\n+    public static final RecordField FLOWFILE_ENTRY_DATE = new SimpleRecordField(EventFieldNames.FLOWFILE_ENTRY_DATE, FieldType.LONG, EXACTLY_ONE);\n+    public static final RecordField EVENT_DURATION = new SimpleRecordField(EventFieldNames.EVENT_DURATION, FieldType.LONG, EXACTLY_ONE);\n+    public static final RecordField LINEAGE_START_DATE = new SimpleRecordField(EventFieldNames.LINEAGE_START_DATE, FieldType.LONG, EXACTLY_ONE);\n+    public static final RecordField COMPONENT_ID = new SimpleRecordField(EventFieldNames.COMPONENT_ID, FieldType.STRING, ZERO_OR_ONE);\n+    public static final RecordField COMPONENT_TYPE = new SimpleRecordField(EventFieldNames.COMPONENT_TYPE, FieldType.STRING, ZERO_OR_ONE);\n+    public static final RecordField FLOWFILE_UUID = new SimpleRecordField(EventFieldNames.FLOWFILE_UUID, FieldType.STRING, EXACTLY_ONE);\n+    public static final RecordField EVENT_DETAILS = new SimpleRecordField(EventFieldNames.EVENT_DETAILS, FieldType.STRING, ZERO_OR_ONE);\n+    public static final RecordField SOURCE_QUEUE_IDENTIFIER = new SimpleRecordField(EventFieldNames.SOURCE_QUEUE_IDENTIFIER, FieldType.STRING, ZERO_OR_ONE);\n \n     // Attributes\n-    public static final RecordField ATTRIBUTE_NAME = new SimpleRecordField(Names.ATTRIBUTE_NAME, FieldType.LONG_STRING, EXACTLY_ONE);\n-    public static final RecordField ATTRIBUTE_VALUE_REQUIRED = new SimpleRecordField(Names.ATTRIBUTE_VALUE, FieldType.LONG_STRING, EXACTLY_ONE);\n-    public static final RecordField ATTRIBUTE_VALUE_OPTIONAL = new SimpleRecordField(Names.ATTRIBUTE_VALUE, FieldType.LONG_STRING, ZERO_OR_ONE);\n+    public static final RecordField ATTRIBUTE_NAME = new SimpleRecordField(EventFieldNames.ATTRIBUTE_NAME, FieldType.LONG_STRING, EXACTLY_ONE);\n+    public static final RecordField ATTRIBUTE_VALUE_REQUIRED = new SimpleRecordField(EventFieldNames.ATTRIBUTE_VALUE, FieldType.LONG_STRING, EXACTLY_ONE);\n+    public static final RecordField ATTRIBUTE_VALUE_OPTIONAL = new SimpleRecordField(EventFieldNames.ATTRIBUTE_VALUE, FieldType.LONG_STRING, ZERO_OR_ONE);\n \n-    public static final RecordField PREVIOUS_ATTRIBUTES = new MapRecordField(Names.PREVIOUS_ATTRIBUTES, ATTRIBUTE_NAME, ATTRIBUTE_VALUE_REQUIRED, EXACTLY_ONE);\n-    public static final RecordField UPDATED_ATTRIBUTES = new MapRecordField(Names.UPDATED_ATTRIBUTES, ATTRIBUTE_NAME, ATTRIBUTE_VALUE_OPTIONAL, EXACTLY_ONE);\n+    public static final RecordField PREVIOUS_ATTRIBUTES = new MapRecordField(EventFieldNames.PREVIOUS_ATTRIBUTES, ATTRIBUTE_NAME, ATTRIBUTE_VALUE_REQUIRED, EXACTLY_ONE);\n+    public static final RecordField UPDATED_ATTRIBUTES = new MapRecordField(EventFieldNames.UPDATED_ATTRIBUTES, ATTRIBUTE_NAME, ATTRIBUTE_VALUE_OPTIONAL, EXACTLY_ONE);\n \n     // Content Claims\n-    public static final RecordField CONTENT_CLAIM_CONTAINER = new SimpleRecordField(Names.CONTENT_CLAIM_CONTAINER, FieldType.STRING, EXACTLY_ONE);\n-    public static final RecordField CONTENT_CLAIM_SECTION = new SimpleRecordField(Names.CONTENT_CLAIM_SECTION, FieldType.STRING, EXACTLY_ONE);\n-    public static final RecordField CONTENT_CLAIM_IDENTIFIER = new SimpleRecordField(Names.CONTENT_CLAIM_IDENTIFIER, FieldType.STRING, EXACTLY_ONE);\n-    public static final RecordField CONTENT_CLAIM_OFFSET = new SimpleRecordField(Names.CONTENT_CLAIM_OFFSET, FieldType.LONG, EXACTLY_ONE);\n-    public static final RecordField CONTENT_CLAIM_SIZE = new SimpleRecordField(Names.CONTENT_CLAIM_SIZE, FieldType.LONG, EXACTLY_ONE);\n-    public static final RecordField CURRENT_CONTENT_CLAIM = new ComplexRecordField(Names.CONTENT_CLAIM, ZERO_OR_ONE,\n+    public static final RecordField CONTENT_CLAIM_CONTAINER = new SimpleRecordField(EventFieldNames.CONTENT_CLAIM_CONTAINER, FieldType.STRING, EXACTLY_ONE);\n+    public static final RecordField CONTENT_CLAIM_SECTION = new SimpleRecordField(EventFieldNames.CONTENT_CLAIM_SECTION, FieldType.STRING, EXACTLY_ONE);\n+    public static final RecordField CONTENT_CLAIM_IDENTIFIER = new SimpleRecordField(EventFieldNames.CONTENT_CLAIM_IDENTIFIER, FieldType.STRING, EXACTLY_ONE);\n+    public static final RecordField CONTENT_CLAIM_OFFSET = new SimpleRecordField(EventFieldNames.CONTENT_CLAIM_OFFSET, FieldType.LONG, EXACTLY_ONE);\n+    public static final RecordField CONTENT_CLAIM_SIZE = new SimpleRecordField(EventFieldNames.CONTENT_CLAIM_SIZE, FieldType.LONG, EXACTLY_ONE);\n+    public static final RecordField CURRENT_CONTENT_CLAIM = new ComplexRecordField(EventFieldNames.CONTENT_CLAIM, ZERO_OR_ONE,\n         CONTENT_CLAIM_CONTAINER, CONTENT_CLAIM_SECTION, CONTENT_CLAIM_IDENTIFIER, CONTENT_CLAIM_OFFSET, CONTENT_CLAIM_SIZE);\n-    public static final RecordField PREVIOUS_CONTENT_CLAIM = new ComplexRecordField(Names.PREVIOUS_CONTENT_CLAIM, ZERO_OR_ONE,\n+    public static final RecordField PREVIOUS_CONTENT_CLAIM = new ComplexRecordField(EventFieldNames.PREVIOUS_CONTENT_CLAIM, ZERO_OR_ONE,\n         CONTENT_CLAIM_CONTAINER, CONTENT_CLAIM_SECTION, CONTENT_CLAIM_IDENTIFIER, CONTENT_CLAIM_OFFSET, CONTENT_CLAIM_SIZE);\n \n     // EventType-Specific fields\n     // for FORK, JOIN, CLONE, REPLAY\n-    public static final RecordField PARENT_UUIDS = new SimpleRecordField(Names.PARENT_UUIDS, FieldType.STRING, ZERO_OR_MORE);\n-    public static final RecordField CHILD_UUIDS = new SimpleRecordField(Names.CHILD_UUIDS, FieldType.STRING, ZERO_OR_MORE);\n+    public static final RecordField PARENT_UUIDS = new SimpleRecordField(EventFieldNames.PARENT_UUIDS, FieldType.STRING, ZERO_OR_MORE);\n+    public static final RecordField CHILD_UUIDS = new SimpleRecordField(EventFieldNames.CHILD_UUIDS, FieldType.STRING, ZERO_OR_MORE);\n \n     // for SEND/RECEIVE/FETCH\n-    public static final RecordField TRANSIT_URI = new SimpleRecordField(Names.TRANSIT_URI, FieldType.STRING, ZERO_OR_ONE);\n-    public static final RecordField SOURCE_SYSTEM_FLOWFILE_IDENTIFIER = new SimpleRecordField(Names.SOURCE_QUEUE_IDENTIFIER, FieldType.STRING, ZERO_OR_ONE);\n+    public static final RecordField TRANSIT_URI = new SimpleRecordField(EventFieldNames.TRANSIT_URI, FieldType.STRING, ZERO_OR_ONE);\n+    public static final RecordField SOURCE_SYSTEM_FLOWFILE_IDENTIFIER = new SimpleRecordField(EventFieldNames.SOURCE_SYSTEM_FLOWFILE_IDENTIFIER, FieldType.STRING, ZERO_OR_ONE);\n \n     // for ADD_INFO\n-    public static final RecordField ALTERNATE_IDENTIFIER = new SimpleRecordField(Names.ALTERNATE_IDENTIFIER, FieldType.STRING, ZERO_OR_ONE);\n-    public static final RecordField RELATIONSHIP = new SimpleRecordField(Names.RELATIONSHIP, FieldType.STRING, ZERO_OR_ONE);\n+    public static final RecordField ALTERNATE_IDENTIFIER = new SimpleRecordField(EventFieldNames.ALTERNATE_IDENTIFIER, FieldType.STRING, ZERO_OR_ONE);\n+    public static final RecordField RELATIONSHIP = new SimpleRecordField(EventFieldNames.RELATIONSHIP, FieldType.STRING, ZERO_OR_ONE);\n }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/EventRecordFields.java",
                "sha": "3d79ab4f5619065b214b1baeec7efdbafc406d79",
                "status": "modified"
            },
            {
                "additions": 363,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/LookupTableEventRecord.java",
                "changes": 363,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/LookupTableEventRecord.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/LookupTableEventRecord.java",
                "patch": "@@ -0,0 +1,363 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.schema;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Supplier;\n+\n+import org.apache.nifi.flowfile.attributes.CoreAttributes;\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.ProvenanceEventType;\n+import org.apache.nifi.provenance.StandardProvenanceEventRecord;\n+import org.apache.nifi.repository.schema.FieldMapRecord;\n+import org.apache.nifi.repository.schema.NamedValue;\n+import org.apache.nifi.repository.schema.Record;\n+import org.apache.nifi.repository.schema.RecordField;\n+import org.apache.nifi.repository.schema.RecordSchema;\n+\n+public class LookupTableEventRecord implements Record {\n+    private final RecordSchema schema;\n+    private final ProvenanceEventRecord event;\n+    private final long eventId;\n+    private final Record contentClaimRecord;\n+    private final Record previousClaimRecord;\n+\n+    private final long eventIdStartOffset;\n+    private final long startTimeOffset;\n+    private final Map<String, Integer> componentIdMap;\n+    private final Map<String, Integer> componentTypeMap;\n+    private final Map<String, Integer> queueIdMap;\n+    private final Map<String, Integer> eventTypeMap;\n+\n+    public LookupTableEventRecord(final ProvenanceEventRecord event, final long eventId, final RecordSchema schema, final RecordSchema contentClaimSchema,\n+        final RecordSchema previousContentClaimSchema, final long eventIdStartOffset, final long startTimeOffset, final Map<String, Integer> componentIdMap,\n+        final Map<String, Integer> componentTypeMap, final Map<String, Integer> queueIdMap, final Map<String, Integer> eventTypeMap) {\n+        this.schema = schema;\n+        this.event = event;\n+        this.eventId = eventId;\n+        this.previousClaimRecord = createPreviousContentClaimRecord(previousContentClaimSchema, event.getPreviousContentClaimContainer(), event.getPreviousContentClaimSection(),\n+            event.getPreviousContentClaimIdentifier(), event.getPreviousContentClaimOffset(), event.getPreviousFileSize());\n+        this.contentClaimRecord = createContentClaimRecord(contentClaimSchema, event.getContentClaimContainer(), event.getContentClaimSection(),\n+            event.getContentClaimIdentifier(), event.getContentClaimOffset(), event.getFileSize());\n+\n+        this.eventIdStartOffset = eventIdStartOffset;\n+        this.startTimeOffset = startTimeOffset;\n+        this.componentIdMap = componentIdMap;\n+        this.componentTypeMap = componentTypeMap;\n+        this.queueIdMap = queueIdMap;\n+        this.eventTypeMap = eventTypeMap;\n+    }\n+\n+    @Override\n+    public RecordSchema getSchema() {\n+        return schema;\n+    }\n+\n+\n+    private static Record createPreviousContentClaimRecord(final RecordSchema contentClaimSchema, final String container, final String section,\n+        final String identifier, final Long offset, final Long size) {\n+\n+        if (container == null || section == null || identifier == null) {\n+            return null;\n+        }\n+\n+        final Map<RecordField, Object> fieldValues = new HashMap<>();\n+        fieldValues.put(EventRecordFields.CONTENT_CLAIM_CONTAINER, container);\n+        fieldValues.put(EventRecordFields.CONTENT_CLAIM_SECTION, section);\n+        fieldValues.put(EventRecordFields.CONTENT_CLAIM_IDENTIFIER, identifier);\n+        fieldValues.put(EventRecordFields.CONTENT_CLAIM_OFFSET, offset);\n+        fieldValues.put(EventRecordFields.CONTENT_CLAIM_SIZE, size);\n+        return new FieldMapRecord(fieldValues, contentClaimSchema);\n+\n+    }\n+\n+    private static Record createContentClaimRecord(final RecordSchema contentClaimSchema, final String container, final String section,\n+            final String identifier, final Long offset, final Long size) {\n+\n+        if (container == null || section == null || identifier == null) {\n+            final Map<RecordField, Object> lookupValues = Collections.singletonMap(LookupTableEventRecordFields.NO_VALUE, EventFieldNames.NO_VALUE);\n+            final List<RecordField> noValueFields = Collections.singletonList(contentClaimSchema.getField(EventFieldNames.NO_VALUE));\n+            return new FieldMapRecord(lookupValues, new RecordSchema(noValueFields));\n+        }\n+\n+        final Map<RecordField, Object> fieldValues = new HashMap<>();\n+        fieldValues.put(EventRecordFields.CONTENT_CLAIM_CONTAINER, container);\n+        fieldValues.put(EventRecordFields.CONTENT_CLAIM_SECTION, section);\n+        fieldValues.put(EventRecordFields.CONTENT_CLAIM_IDENTIFIER, identifier);\n+        fieldValues.put(EventRecordFields.CONTENT_CLAIM_OFFSET, offset);\n+        fieldValues.put(EventRecordFields.CONTENT_CLAIM_SIZE, size);\n+\n+        final List<RecordField> explicitClaimFields = contentClaimSchema.getField(EventFieldNames.EXPLICIT_VALUE).getSubFields();\n+        final Record explicitClaimRecord = new FieldMapRecord(fieldValues, new RecordSchema(explicitClaimFields));\n+        return explicitClaimRecord;\n+    }\n+\n+    private static String readLookupValue(final Object recordValue, final List<String> lookup) {\n+        if (recordValue == null) {\n+            return null;\n+        }\n+\n+        // NO_VALUE type\n+        if (recordValue instanceof Boolean) {\n+            return null;\n+        }\n+\n+        // LOOKUP type\n+        if (recordValue instanceof Integer) {\n+            final Integer indexValue = (Integer) recordValue;\n+            final int index = indexValue.intValue();\n+            if (index > lookup.size() - 1) {\n+                return null;\n+            }\n+\n+            return lookup.get(index);\n+        }\n+\n+        // EXPLICIT_VALUE type\n+        if (recordValue instanceof String) {\n+            return (String) recordValue;\n+        }\n+\n+        return null;\n+    }\n+\n+    private NamedValue createLookupValue(final String literalValue, final Map<String, Integer> lookup) {\n+        if (literalValue == null) {\n+            final Map<RecordField, Object> lookupValues = Collections.singletonMap(LookupTableEventRecordFields.NO_VALUE, EventFieldNames.NO_VALUE);\n+            final Record record = new FieldMapRecord(lookupValues, LookupTableEventSchema.NO_VALUE_SCHEMA);\n+            final NamedValue namedValue = new NamedValue(EventFieldNames.NO_VALUE, record);\n+            return namedValue;\n+        }\n+\n+        final Integer index = lookup.get(literalValue);\n+        if (index == null) {\n+            final Map<RecordField, Object> lookupValues = Collections.singletonMap(LookupTableEventRecordFields.EXPLICIT_STRING, literalValue);\n+            final Record record = new FieldMapRecord(lookupValues, LookupTableEventSchema.EXPLICIT_STRING_SCHEMA);\n+            final NamedValue namedValue = new NamedValue(EventFieldNames.EXPLICIT_VALUE, record);\n+            return namedValue;\n+        } else {\n+            final Map<RecordField, Object> lookupValues = Collections.singletonMap(LookupTableEventRecordFields.LOOKUP_VALUE, index);\n+            final Record record = new FieldMapRecord(lookupValues, LookupTableEventSchema.LOOKUP_VALUE_SCHEMA);\n+            final NamedValue namedValue = new NamedValue(EventFieldNames.LOOKUP_VALUE, record);\n+            return namedValue;\n+        }\n+    }\n+\n+    private NamedValue createExplicitSameOrNoneValue(final Record newValue, final Record oldValue, final Supplier<Record> recordSupplier) {\n+        if (newValue == null || EventFieldNames.NO_VALUE.equals(newValue.getSchema().getFields().get(0).getFieldName())) {\n+            final Map<RecordField, Object> lookupValues = Collections.singletonMap(LookupTableEventRecordFields.NO_VALUE, EventFieldNames.NO_VALUE);\n+            final Record record = new FieldMapRecord(lookupValues, LookupTableEventSchema.NO_VALUE_SCHEMA);\n+            final NamedValue namedValue = new NamedValue(EventFieldNames.NO_VALUE, record);\n+            return namedValue;\n+        } else if (newValue.equals(oldValue)) {\n+            final Map<RecordField, Object> lookupValues = Collections.singletonMap(LookupTableEventRecordFields.UNCHANGED_VALUE, EventFieldNames.UNCHANGED_VALUE);\n+            final Record record = new FieldMapRecord(lookupValues, LookupTableEventSchema.UNCHANGED_VALUE_SCHEMA);\n+            final NamedValue namedValue = new NamedValue(EventFieldNames.UNCHANGED_VALUE, record);\n+            return namedValue;\n+        }\n+\n+        final Record record = recordSupplier.get();\n+        final NamedValue namedValue = new NamedValue(EventFieldNames.EXPLICIT_VALUE, record);\n+        return namedValue;\n+    }\n+\n+    @Override\n+    public Object getFieldValue(final String fieldName) {\n+        switch (fieldName) {\n+            case EventFieldNames.EVENT_IDENTIFIER:\n+                return (int) (eventId - eventIdStartOffset);\n+            case EventFieldNames.ALTERNATE_IDENTIFIER:\n+                return event.getAlternateIdentifierUri();\n+            case EventFieldNames.CHILD_UUIDS:\n+                return event.getChildUuids();\n+            case EventFieldNames.COMPONENT_ID:\n+                return createLookupValue(event.getComponentId(), componentIdMap);\n+            case EventFieldNames.COMPONENT_TYPE:\n+                return createLookupValue(event.getComponentType(), componentTypeMap);\n+            case EventFieldNames.CONTENT_CLAIM:\n+                return createExplicitSameOrNoneValue(contentClaimRecord, previousClaimRecord, () -> contentClaimRecord);\n+            case EventFieldNames.EVENT_DETAILS:\n+                return event.getDetails();\n+            case EventFieldNames.EVENT_DURATION:\n+                return (int) event.getEventDuration();\n+            case EventFieldNames.EVENT_TIME:\n+                return (int) (event.getEventTime() - startTimeOffset);\n+            case EventFieldNames.EVENT_TYPE:\n+                return eventTypeMap.get(event.getEventType().name());\n+            case EventFieldNames.FLOWFILE_ENTRY_DATE:\n+                return (int) (event.getFlowFileEntryDate() - startTimeOffset);\n+            case EventFieldNames.LINEAGE_START_DATE:\n+                return (int) (event.getLineageStartDate() - startTimeOffset);\n+            case EventFieldNames.PARENT_UUIDS:\n+                return event.getParentUuids();\n+            case EventFieldNames.PREVIOUS_ATTRIBUTES:\n+                return event.getPreviousAttributes();\n+            case EventFieldNames.PREVIOUS_CONTENT_CLAIM:\n+                return previousClaimRecord;\n+            case EventFieldNames.RELATIONSHIP:\n+                return event.getRelationship();\n+            case EventFieldNames.SOURCE_QUEUE_IDENTIFIER:\n+                return createLookupValue(event.getSourceQueueIdentifier(), queueIdMap);\n+            case EventFieldNames.SOURCE_SYSTEM_FLOWFILE_IDENTIFIER:\n+                return event.getSourceSystemFlowFileIdentifier();\n+            case EventFieldNames.TRANSIT_URI:\n+                return event.getTransitUri();\n+            case EventFieldNames.UPDATED_ATTRIBUTES:\n+                return event.getUpdatedAttributes();\n+            case EventFieldNames.FLOWFILE_UUID:\n+                return event.getAttribute(CoreAttributes.UUID.key());\n+        }\n+\n+        return null;\n+    }\n+\n+    private static Long addLong(final Integer optionalValue, final long requiredValue) {\n+        if (optionalValue == null) {\n+            return null;\n+        }\n+\n+        return optionalValue.longValue() + requiredValue;\n+    }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public static StandardProvenanceEventRecord getEvent(final Record record, final String storageFilename, final long storageByteOffset, final int maxAttributeLength,\n+        final long eventIdStartOffset, final long startTimeOffset, final List<String> componentIds, final List<String> componentTypes,\n+        final List<String> queueIds, final List<String> eventTypes) {\n+\n+        final Map<String, String> previousAttributes = truncateAttributes((Map<String, String>) record.getFieldValue(EventFieldNames.PREVIOUS_ATTRIBUTES), maxAttributeLength);\n+        final Map<String, String> updatedAttributes = truncateAttributes((Map<String, String>) record.getFieldValue(EventFieldNames.UPDATED_ATTRIBUTES), maxAttributeLength);\n+\n+        final StandardProvenanceEventRecord.Builder builder = new StandardProvenanceEventRecord.Builder();\n+        builder.setAlternateIdentifierUri((String) record.getFieldValue(EventFieldNames.ALTERNATE_IDENTIFIER));\n+        builder.setChildUuids((List<String>) record.getFieldValue(EventFieldNames.CHILD_UUIDS));\n+        builder.setDetails((String) record.getFieldValue(EventFieldNames.EVENT_DETAILS));\n+        builder.setParentUuids((List<String>) record.getFieldValue(EventFieldNames.PARENT_UUIDS));\n+        builder.setPreviousAttributes(previousAttributes);\n+        builder.setRelationship((String) record.getFieldValue(EventFieldNames.RELATIONSHIP));\n+        builder.setSourceSystemFlowFileIdentifier((String) record.getFieldValue(EventFieldNames.SOURCE_SYSTEM_FLOWFILE_IDENTIFIER));\n+        builder.setTransitUri((String) record.getFieldValue(EventFieldNames.TRANSIT_URI));\n+        builder.setUpdatedAttributes(updatedAttributes);\n+\n+\n+        builder.setComponentId(readLookupValue(record.getFieldValue(EventFieldNames.COMPONENT_ID), componentIds));\n+        builder.setComponentType(readLookupValue(record.getFieldValue(EventFieldNames.COMPONENT_TYPE), componentTypes));\n+        builder.setSourceQueueIdentifier(readLookupValue(record.getFieldValue(EventFieldNames.SOURCE_QUEUE_IDENTIFIER), queueIds));\n+\n+        // Determine the event type\n+        final Integer eventTypeOrdinal = (Integer) record.getFieldValue(EventFieldNames.EVENT_TYPE);\n+        if (eventTypeOrdinal == null || eventTypeOrdinal > eventTypes.size() || eventTypeOrdinal < 0) {\n+            builder.setEventType(ProvenanceEventType.UNKNOWN);\n+        } else {\n+            try {\n+                builder.setEventType(ProvenanceEventType.valueOf(eventTypes.get(eventTypeOrdinal)));\n+            } catch (final Exception e) {\n+                builder.setEventType(ProvenanceEventType.UNKNOWN);\n+            }\n+        }\n+\n+        String uuid = updatedAttributes == null ? null : updatedAttributes.get(CoreAttributes.UUID.key());\n+        if (uuid == null) {\n+            uuid = previousAttributes == null ? null : previousAttributes.get(CoreAttributes.UUID.key());\n+        }\n+        builder.setFlowFileUUID(uuid);\n+\n+        builder.setEventDuration((Integer) record.getFieldValue(EventFieldNames.EVENT_DURATION));\n+        builder.setEventTime(addLong((Integer) record.getFieldValue(EventFieldNames.EVENT_TIME), startTimeOffset));\n+        builder.setFlowFileEntryDate(addLong((Integer) record.getFieldValue(EventFieldNames.FLOWFILE_ENTRY_DATE), startTimeOffset));\n+        builder.setLineageStartDate(addLong((Integer) record.getFieldValue(EventFieldNames.LINEAGE_START_DATE), startTimeOffset));\n+\n+        final Integer eventId = (Integer) record.getFieldValue(EventFieldNames.EVENT_IDENTIFIER);\n+        if (eventId != null) {\n+            builder.setEventId(eventId.longValue() + eventIdStartOffset);\n+        }\n+\n+        builder.setStorageLocation(storageFilename, storageByteOffset);\n+\n+        final Record previousClaimRecord = (Record) record.getFieldValue(EventFieldNames.PREVIOUS_CONTENT_CLAIM);\n+        if (previousClaimRecord != null) {\n+            builder.setPreviousContentClaim(\n+                (String) previousClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_CONTAINER),\n+                (String) previousClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_SECTION),\n+                (String) previousClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_IDENTIFIER),\n+                (Long) previousClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_OFFSET),\n+                (Long) previousClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_SIZE));\n+        }\n+\n+        final Object contentClaimObject = record.getFieldValue(EventFieldNames.CONTENT_CLAIM);\n+\n+        // NO_VALUE type\n+        builder.setCurrentContentClaim(null, null, null, null, 0L);\n+        if (contentClaimObject != null) {\n+            if (contentClaimObject instanceof String) {\n+                final String contentClaimDescription = (String) contentClaimObject;\n+                switch (contentClaimDescription) {\n+                    case EventFieldNames.UNCHANGED_VALUE:\n+                        builder.setCurrentContentClaim((String) previousClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_CONTAINER),\n+                            (String) previousClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_SECTION),\n+                            (String) previousClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_IDENTIFIER),\n+                            (Long) previousClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_OFFSET),\n+                            (Long) previousClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_SIZE));\n+                        break;\n+                }\n+            } else if (contentClaimObject instanceof Record) {\n+                final Record currentClaimRecord = (Record) contentClaimObject;\n+                builder.setCurrentContentClaim(\n+                    (String) currentClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_CONTAINER),\n+                    (String) currentClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_SECTION),\n+                    (String) currentClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_IDENTIFIER),\n+                    (Long) currentClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_OFFSET),\n+                    (Long) currentClaimRecord.getFieldValue(EventFieldNames.CONTENT_CLAIM_SIZE));\n+            }\n+        }\n+\n+        return builder.build();\n+    }\n+\n+    private static Map<String, String> truncateAttributes(final Map<String, String> attributes, final int maxAttributeLength) {\n+        if (attributes == null) {\n+            return null;\n+        }\n+\n+        // Check if any attribute value exceeds the attribute length\n+        final boolean anyExceedsLength = attributes.values().stream()\n+            .filter(value -> value != null)\n+            .anyMatch(value -> value.length() > maxAttributeLength);\n+\n+        if (!anyExceedsLength) {\n+            return attributes;\n+        }\n+\n+        final Map<String, String> truncated = new HashMap<>();\n+        for (final Map.Entry<String, String> entry : attributes.entrySet()) {\n+            final String key = entry.getKey();\n+            final String value = entry.getValue();\n+\n+            if (value == null || value.length() <= maxAttributeLength) {\n+                truncated.put(key, value);\n+                continue;\n+            }\n+\n+            truncated.put(key, value.substring(0, maxAttributeLength));\n+        }\n+\n+        return truncated;\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/LookupTableEventRecord.java",
                "sha": "eccff2aefba12a07c9f92c67c17f19f946c78b2d",
                "status": "added"
            },
            {
                "additions": 89,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/LookupTableEventRecordFields.java",
                "changes": 89,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/LookupTableEventRecordFields.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/LookupTableEventRecordFields.java",
                "patch": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.schema;\n+\n+import static org.apache.nifi.repository.schema.Repetition.EXACTLY_ONE;\n+import static org.apache.nifi.repository.schema.Repetition.ZERO_OR_MORE;\n+import static org.apache.nifi.repository.schema.Repetition.ZERO_OR_ONE;\n+\n+import org.apache.nifi.repository.schema.ComplexRecordField;\n+import org.apache.nifi.repository.schema.FieldType;\n+import org.apache.nifi.repository.schema.MapRecordField;\n+import org.apache.nifi.repository.schema.RecordField;\n+import org.apache.nifi.repository.schema.Repetition;\n+import org.apache.nifi.repository.schema.SimpleRecordField;\n+import org.apache.nifi.repository.schema.UnionRecordField;\n+\n+public class LookupTableEventRecordFields {\n+\n+    // General Event fields.\n+    public static final RecordField RECORD_IDENTIFIER_OFFSET = new SimpleRecordField(EventFieldNames.EVENT_IDENTIFIER, FieldType.INT, EXACTLY_ONE);\n+    public static final RecordField EVENT_TYPE_ORDINAL = new SimpleRecordField(EventFieldNames.EVENT_TYPE, FieldType.INT, EXACTLY_ONE);\n+    public static final RecordField EVENT_TIME_OFFSET = new SimpleRecordField(EventFieldNames.EVENT_TIME, FieldType.INT, EXACTLY_ONE);\n+    public static final RecordField FLOWFILE_ENTRY_DATE_OFFSET = new SimpleRecordField(EventFieldNames.FLOWFILE_ENTRY_DATE, FieldType.INT, EXACTLY_ONE);\n+    public static final RecordField EVENT_DURATION = new SimpleRecordField(EventFieldNames.EVENT_DURATION, FieldType.INT, EXACTLY_ONE);\n+    public static final RecordField LINEAGE_START_DATE_OFFSET = new SimpleRecordField(EventFieldNames.LINEAGE_START_DATE, FieldType.INT, EXACTLY_ONE);\n+    public static final RecordField EVENT_DETAILS = new SimpleRecordField(EventFieldNames.EVENT_DETAILS, FieldType.STRING, ZERO_OR_ONE);\n+\n+    // Make lookup id or a string, depending on whether or not available in header.\n+    public static final RecordField NO_VALUE = new SimpleRecordField(EventFieldNames.NO_VALUE, FieldType.STRING, Repetition.EXACTLY_ONE);\n+    public static final RecordField EXPLICIT_STRING = new SimpleRecordField(EventFieldNames.EXPLICIT_VALUE, FieldType.STRING, Repetition.EXACTLY_ONE);\n+    public static final RecordField LOOKUP_VALUE = new SimpleRecordField(EventFieldNames.LOOKUP_VALUE, FieldType.INT, Repetition.EXACTLY_ONE);\n+    public static final RecordField UNCHANGED_VALUE = new SimpleRecordField(EventFieldNames.UNCHANGED_VALUE, FieldType.STRING, Repetition.EXACTLY_ONE);\n+\n+    public static final RecordField COMPONENT_ID = new UnionRecordField(EventFieldNames.COMPONENT_ID, Repetition.EXACTLY_ONE, NO_VALUE, EXPLICIT_STRING, LOOKUP_VALUE);\n+    public static final RecordField SOURCE_QUEUE_ID = new UnionRecordField(EventFieldNames.SOURCE_QUEUE_IDENTIFIER, Repetition.EXACTLY_ONE, NO_VALUE, EXPLICIT_STRING, LOOKUP_VALUE);\n+    public static final RecordField COMPONENT_TYPE = new UnionRecordField(EventFieldNames.COMPONENT_TYPE, Repetition.EXACTLY_ONE, EXPLICIT_STRING, LOOKUP_VALUE);\n+\n+    // Attributes\n+    public static final RecordField ATTRIBUTE_NAME = new SimpleRecordField(EventFieldNames.ATTRIBUTE_NAME, FieldType.LONG_STRING, EXACTLY_ONE);\n+    public static final RecordField ATTRIBUTE_VALUE_REQUIRED = new SimpleRecordField(EventFieldNames.ATTRIBUTE_VALUE, FieldType.LONG_STRING, EXACTLY_ONE);\n+    public static final RecordField ATTRIBUTE_VALUE_OPTIONAL = new SimpleRecordField(EventFieldNames.ATTRIBUTE_VALUE, FieldType.LONG_STRING, ZERO_OR_ONE);\n+\n+    public static final RecordField PREVIOUS_ATTRIBUTES = new MapRecordField(EventFieldNames.PREVIOUS_ATTRIBUTES, ATTRIBUTE_NAME, ATTRIBUTE_VALUE_REQUIRED, EXACTLY_ONE);\n+    public static final RecordField UPDATED_ATTRIBUTES = new MapRecordField(EventFieldNames.UPDATED_ATTRIBUTES, ATTRIBUTE_NAME, ATTRIBUTE_VALUE_OPTIONAL, EXACTLY_ONE);\n+\n+    // Content Claims\n+    public static final RecordField CONTENT_CLAIM_CONTAINER = new SimpleRecordField(EventFieldNames.CONTENT_CLAIM_CONTAINER, FieldType.STRING, EXACTLY_ONE);\n+    public static final RecordField CONTENT_CLAIM_SECTION = new SimpleRecordField(EventFieldNames.CONTENT_CLAIM_SECTION, FieldType.STRING, EXACTLY_ONE);\n+    public static final RecordField CONTENT_CLAIM_IDENTIFIER = new SimpleRecordField(EventFieldNames.CONTENT_CLAIM_IDENTIFIER, FieldType.STRING, EXACTLY_ONE);\n+    public static final RecordField CONTENT_CLAIM_OFFSET = new SimpleRecordField(EventFieldNames.CONTENT_CLAIM_OFFSET, FieldType.LONG, EXACTLY_ONE);\n+    public static final RecordField CONTENT_CLAIM_SIZE = new SimpleRecordField(EventFieldNames.CONTENT_CLAIM_SIZE, FieldType.LONG, EXACTLY_ONE);\n+\n+    public static final RecordField PREVIOUS_CONTENT_CLAIM = new ComplexRecordField(EventFieldNames.PREVIOUS_CONTENT_CLAIM, ZERO_OR_ONE,\n+        CONTENT_CLAIM_CONTAINER, CONTENT_CLAIM_SECTION, CONTENT_CLAIM_IDENTIFIER, CONTENT_CLAIM_OFFSET, CONTENT_CLAIM_SIZE);\n+\n+    public static final RecordField CURRENT_CONTENT_CLAIM_EXPLICIT = new ComplexRecordField(EventFieldNames.EXPLICIT_VALUE, EXACTLY_ONE,\n+        CONTENT_CLAIM_CONTAINER, CONTENT_CLAIM_SECTION, CONTENT_CLAIM_IDENTIFIER, CONTENT_CLAIM_OFFSET, CONTENT_CLAIM_SIZE);\n+    public static final RecordField CURRENT_CONTENT_CLAIM = new UnionRecordField(EventFieldNames.CONTENT_CLAIM,\n+        Repetition.EXACTLY_ONE, NO_VALUE, UNCHANGED_VALUE, CURRENT_CONTENT_CLAIM_EXPLICIT);\n+\n+\n+    // EventType-Specific fields\n+    // for FORK, JOIN, CLONE, REPLAY\n+    public static final RecordField PARENT_UUIDS = new SimpleRecordField(EventFieldNames.PARENT_UUIDS, FieldType.STRING, ZERO_OR_MORE);\n+    public static final RecordField CHILD_UUIDS = new SimpleRecordField(EventFieldNames.CHILD_UUIDS, FieldType.STRING, ZERO_OR_MORE);\n+\n+    // for SEND/RECEIVE/FETCH\n+    public static final RecordField TRANSIT_URI = new SimpleRecordField(EventFieldNames.TRANSIT_URI, FieldType.STRING, ZERO_OR_ONE);\n+    public static final RecordField SOURCE_SYSTEM_FLOWFILE_IDENTIFIER = new SimpleRecordField(EventFieldNames.SOURCE_SYSTEM_FLOWFILE_IDENTIFIER, FieldType.STRING, ZERO_OR_ONE);\n+\n+    // for ADD_INFO\n+    public static final RecordField ALTERNATE_IDENTIFIER = new SimpleRecordField(EventFieldNames.ALTERNATE_IDENTIFIER, FieldType.STRING, ZERO_OR_ONE);\n+    public static final RecordField RELATIONSHIP = new SimpleRecordField(EventFieldNames.RELATIONSHIP, FieldType.STRING, ZERO_OR_ONE);\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/LookupTableEventRecordFields.java",
                "sha": "7b33ded0952c4c79b60c1a3321176a82f5773273",
                "status": "added"
            },
            {
                "additions": 94,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/LookupTableEventSchema.java",
                "changes": 94,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/LookupTableEventSchema.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/LookupTableEventSchema.java",
                "patch": "@@ -0,0 +1,94 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.schema;\n+\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.ALTERNATE_IDENTIFIER;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.CHILD_UUIDS;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.COMPONENT_ID;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.COMPONENT_TYPE;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.CURRENT_CONTENT_CLAIM;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.EVENT_DETAILS;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.EVENT_DURATION;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.EVENT_TIME_OFFSET;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.EVENT_TYPE_ORDINAL;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.EXPLICIT_STRING;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.FLOWFILE_ENTRY_DATE_OFFSET;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.LINEAGE_START_DATE_OFFSET;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.LOOKUP_VALUE;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.NO_VALUE;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.PARENT_UUIDS;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.PREVIOUS_ATTRIBUTES;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.PREVIOUS_CONTENT_CLAIM;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.RECORD_IDENTIFIER_OFFSET;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.RELATIONSHIP;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.SOURCE_QUEUE_ID;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.SOURCE_SYSTEM_FLOWFILE_IDENTIFIER;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.TRANSIT_URI;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.UNCHANGED_VALUE;\n+import static org.apache.nifi.provenance.schema.LookupTableEventRecordFields.UPDATED_ATTRIBUTES;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+\n+import org.apache.nifi.repository.schema.RecordField;\n+import org.apache.nifi.repository.schema.RecordSchema;\n+\n+public class LookupTableEventSchema {\n+    public static final RecordSchema EVENT_SCHEMA = buildSchemaV1(false);\n+\n+    public static final RecordSchema NO_VALUE_SCHEMA = new RecordSchema(Collections.singletonList(NO_VALUE));\n+    public static final RecordSchema EXPLICIT_STRING_SCHEMA = new RecordSchema(Collections.singletonList(EXPLICIT_STRING));\n+    public static final RecordSchema UNCHANGED_VALUE_SCHEMA = new RecordSchema(Collections.singletonList(UNCHANGED_VALUE));\n+    public static final RecordSchema LOOKUP_VALUE_SCHEMA = new RecordSchema(Collections.singletonList(LOOKUP_VALUE));\n+\n+    public static final RecordSchema CONTENT_CLAIM_SCHEMA = new RecordSchema(Collections.singletonList(CURRENT_CONTENT_CLAIM));\n+\n+    private static RecordSchema buildSchemaV1(final boolean includeEventId) {\n+        final List<RecordField> fields = new ArrayList<>();\n+        if (includeEventId) {\n+            fields.add(RECORD_IDENTIFIER_OFFSET);\n+        }\n+\n+        fields.add(EVENT_TYPE_ORDINAL);\n+        fields.add(EVENT_TIME_OFFSET);\n+        fields.add(FLOWFILE_ENTRY_DATE_OFFSET);\n+        fields.add(EVENT_DURATION);\n+        fields.add(LINEAGE_START_DATE_OFFSET);\n+        fields.add(COMPONENT_ID);\n+        fields.add(COMPONENT_TYPE);\n+        fields.add(EVENT_DETAILS);\n+        fields.add(PREVIOUS_ATTRIBUTES);\n+        fields.add(UPDATED_ATTRIBUTES);\n+        fields.add(CURRENT_CONTENT_CLAIM);\n+        fields.add(PREVIOUS_CONTENT_CLAIM);\n+        fields.add(SOURCE_QUEUE_ID);\n+\n+        // EventType-Specific fields\n+        fields.add(PARENT_UUIDS);  // for FORK, JOIN, CLONE, REPLAY events\n+        fields.add(CHILD_UUIDS); // for FORK, JOIN, CLONE, REPLAY events\n+        fields.add(TRANSIT_URI); // for SEND/RECEIVE/FETCH events\n+        fields.add(SOURCE_SYSTEM_FLOWFILE_IDENTIFIER); // for SEND/RECEIVE events\n+        fields.add(ALTERNATE_IDENTIFIER); // for ADD_INFO events\n+        fields.add(RELATIONSHIP); // for ROUTE events\n+\n+        final RecordSchema schema = new RecordSchema(fields);\n+        return schema;\n+    }\n+\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/LookupTableEventSchema.java",
                "sha": "71103364221ff5dad61038ed75e710bc336bd8a4",
                "status": "added"
            },
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/ProvenanceEventSchema.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/ProvenanceEventSchema.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 3,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/ProvenanceEventSchema.java",
                "patch": "@@ -46,11 +46,15 @@\n import org.apache.nifi.repository.schema.RecordSchema;\n \n public class ProvenanceEventSchema {\n-    public static final RecordSchema PROVENANCE_EVENT_SCHEMA_V1 = buildSchemaV1();\n+    public static final RecordSchema PROVENANCE_EVENT_SCHEMA_V1 = buildSchemaV1(true);\n+    public static final RecordSchema PROVENANCE_EVENT_SCHEMA_V1_WITHOUT_EVENT_ID = buildSchemaV1(false);\n \n-    private static RecordSchema buildSchemaV1() {\n+    private static RecordSchema buildSchemaV1(final boolean includeEventId) {\n         final List<RecordField> fields = new ArrayList<>();\n-        fields.add(RECORD_IDENTIFIER);\n+        if (includeEventId) {\n+            fields.add(RECORD_IDENTIFIER);\n+        }\n+\n         fields.add(EVENT_TYPE);\n         fields.add(EVENT_TIME);\n         fields.add(FLOWFILE_ENTRY_DATE);",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/schema/ProvenanceEventSchema.java",
                "sha": "46556130654733f00d3d0c581a10276d2ebbd4b4",
                "status": "modified"
            },
            {
                "additions": 90,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/CompressableRecordReader.java",
                "changes": 107,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/CompressableRecordReader.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 17,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/CompressableRecordReader.java",
                "patch": "@@ -17,26 +17,26 @@\n \n package org.apache.nifi.provenance.serialization;\n \n+import java.io.BufferedInputStream;\n import java.io.DataInputStream;\n import java.io.EOFException;\n import java.io.IOException;\n import java.io.InputStream;\n import java.nio.charset.StandardCharsets;\n+import java.util.Optional;\n import java.util.zip.GZIPInputStream;\n \n import org.apache.nifi.provenance.ProvenanceEventRecord;\n import org.apache.nifi.provenance.StandardProvenanceEventRecord;\n-import org.apache.nifi.provenance.StandardRecordReader;\n import org.apache.nifi.provenance.toc.TocReader;\n-import org.apache.nifi.stream.io.BufferedInputStream;\n import org.apache.nifi.stream.io.ByteCountingInputStream;\n import org.apache.nifi.stream.io.LimitingInputStream;\n import org.apache.nifi.stream.io.StreamUtils;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n public abstract class CompressableRecordReader implements RecordReader {\n-    private static final Logger logger = LoggerFactory.getLogger(StandardRecordReader.class);\n+    private static final Logger logger = LoggerFactory.getLogger(CompressableRecordReader.class);\n \n     private final ByteCountingInputStream rawInputStream;\n     private final String filename;\n@@ -48,6 +48,7 @@\n \n     private DataInputStream dis;\n     private ByteCountingInputStream byteCountingIn;\n+    private StandardProvenanceEventRecord pushbackEvent = null;\n \n     public CompressableRecordReader(final InputStream in, final String filename, final int maxAttributeChars) throws IOException {\n         this(in, filename, null, maxAttributeChars);\n@@ -120,6 +121,8 @@ public void skipToBlock(final int blockIndex) throws IOException {\n             try {\n                 StreamUtils.skip(rawInputStream, bytesToSkip);\n                 logger.debug(\"Skipped stream from offset {} to {} ({} bytes skipped)\", curOffset, offset, bytesToSkip);\n+            } catch (final EOFException eof) {\n+                throw new EOFException(\"Attempted to skip to byte offset \" + offset + \" for \" + filename + \" but file does not have that many bytes (TOC Reader=\" + getTocReader() + \")\");\n             } catch (final IOException e) {\n                 throw new IOException(\"Failed to skip to offset \" + offset + \" for block \" + blockIndex + \" of Provenance Log \" + filename, e);\n             }\n@@ -177,24 +180,29 @@ public long getBytesConsumed() {\n         return byteCountingIn.getBytesConsumed();\n     }\n \n-    private boolean isData() throws IOException {\n-        byteCountingIn.mark(1);\n-        int nextByte = byteCountingIn.read();\n-        byteCountingIn.reset();\n+    @Override\n+    public boolean isData() {\n+        try {\n+            byteCountingIn.mark(1);\n+            int nextByte = byteCountingIn.read();\n+            byteCountingIn.reset();\n+\n+            if (nextByte < 0) {\n+                try {\n+                    resetStreamForNextBlock();\n+                } catch (final EOFException eof) {\n+                    return false;\n+                }\n \n-        if (nextByte < 0) {\n-            try {\n-                resetStreamForNextBlock();\n-            } catch (final EOFException eof) {\n-                return false;\n+                byteCountingIn.mark(1);\n+                nextByte = byteCountingIn.read();\n+                byteCountingIn.reset();\n             }\n \n-            byteCountingIn.mark(1);\n-            nextByte = byteCountingIn.read();\n-            byteCountingIn.reset();\n+            return nextByte >= 0;\n+        } catch (final IOException ioe) {\n+            return false;\n         }\n-\n-        return nextByte >= 0;\n     }\n \n     @Override\n@@ -268,13 +276,78 @@ protected int getMaxAttributeLength() {\n \n     @Override\n     public StandardProvenanceEventRecord nextRecord() throws IOException {\n+        if (pushbackEvent != null) {\n+            final StandardProvenanceEventRecord toReturn = pushbackEvent;\n+            pushbackEvent = null;\n+            return toReturn;\n+        }\n+\n         if (isData()) {\n             return nextRecord(dis, serializationVersion);\n         } else {\n             return null;\n         }\n     }\n \n+    protected Optional<Integer> getBlockIndex(final long eventId) {\n+        final TocReader tocReader = getTocReader();\n+        if (tocReader == null) {\n+            return Optional.empty();\n+        } else {\n+            final Integer blockIndex = tocReader.getBlockIndexForEventId(eventId);\n+            return Optional.ofNullable(blockIndex);\n+        }\n+    }\n+\n+    @Override\n+    public Optional<ProvenanceEventRecord> skipToEvent(final long eventId) throws IOException {\n+        if (pushbackEvent != null) {\n+            final StandardProvenanceEventRecord previousPushBack = pushbackEvent;\n+            if (previousPushBack.getEventId() >= eventId) {\n+                return Optional.of(previousPushBack);\n+            } else {\n+                pushbackEvent = null;\n+            }\n+        }\n+\n+        final Optional<Integer> blockIndex = getBlockIndex(eventId);\n+        if (blockIndex.isPresent()) {\n+            // Skip to the appropriate block index and then read until we've found an Event\n+            // that has an ID >= the event id.\n+            skipToBlock(blockIndex.get());\n+        }\n+\n+        try {\n+            boolean read = true;\n+            while (read) {\n+                final Optional<StandardProvenanceEventRecord> eventOptional = readToEvent(eventId, dis, serializationVersion);\n+                if (eventOptional.isPresent()) {\n+                    pushbackEvent = eventOptional.get();\n+                    return Optional.of(pushbackEvent);\n+                } else {\n+                    read = isData();\n+                }\n+            }\n+\n+            return Optional.empty();\n+        } catch (final EOFException eof) {\n+            // This can occur if we run out of data and attempt to read the next event ID.\n+            logger.error(\"Unexpectedly reached end of File when looking for Provenance Event with ID {} in {}\", eventId, filename);\n+            return Optional.empty();\n+        }\n+    }\n+\n+    protected Optional<StandardProvenanceEventRecord> readToEvent(final long eventId, final DataInputStream dis, final int serializationVerison) throws IOException {\n+        StandardProvenanceEventRecord event;\n+        while ((event = nextRecord()) != null) {\n+            if (event.getEventId() >= eventId) {\n+                return Optional.of(event);\n+            }\n+        }\n+\n+        return Optional.empty();\n+    }\n+\n     protected abstract StandardProvenanceEventRecord nextRecord(DataInputStream in, int serializationVersion) throws IOException;\n \n     protected void readHeader(DataInputStream in, int serializationVersion) throws IOException {",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/CompressableRecordReader.java",
                "sha": "1a6c3c55dc45e3d95023eecc161a2627acafb27d",
                "status": "modified"
            },
            {
                "additions": 50,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/CompressableRecordWriter.java",
                "changes": 80,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/CompressableRecordWriter.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 30,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/CompressableRecordWriter.java",
                "patch": "@@ -17,17 +17,18 @@\n \n package org.apache.nifi.provenance.serialization;\n \n+import java.io.BufferedOutputStream;\n+import java.io.DataOutputStream;\n import java.io.File;\n import java.io.FileOutputStream;\n import java.io.IOException;\n import java.io.OutputStream;\n+import java.util.concurrent.atomic.AtomicLong;\n \n import org.apache.nifi.provenance.AbstractRecordWriter;\n import org.apache.nifi.provenance.ProvenanceEventRecord;\n import org.apache.nifi.provenance.toc.TocWriter;\n-import org.apache.nifi.stream.io.BufferedOutputStream;\n import org.apache.nifi.stream.io.ByteCountingOutputStream;\n-import org.apache.nifi.stream.io.DataOutputStream;\n import org.apache.nifi.stream.io.GZIPOutputStream;\n import org.apache.nifi.stream.io.NonCloseableOutputStream;\n import org.slf4j.Logger;\n@@ -40,47 +41,56 @@\n     private final ByteCountingOutputStream rawOutStream;\n     private final boolean compressed;\n     private final int uncompressedBlockSize;\n+    private final AtomicLong idGenerator;\n \n     private DataOutputStream out;\n     private ByteCountingOutputStream byteCountingOut;\n-    private long lastBlockOffset = 0L;\n+    private long blockStartOffset = 0L;\n     private int recordCount = 0;\n \n \n-    public CompressableRecordWriter(final File file, final TocWriter writer, final boolean compressed, final int uncompressedBlockSize) throws IOException {\n+    public CompressableRecordWriter(final File file, final AtomicLong idGenerator, final TocWriter writer, final boolean compressed,\n+        final int uncompressedBlockSize) throws IOException {\n         super(file, writer);\n         logger.trace(\"Creating Record Writer for {}\", file.getName());\n \n         this.compressed = compressed;\n         this.fos = new FileOutputStream(file);\n         rawOutStream = new ByteCountingOutputStream(fos);\n         this.uncompressedBlockSize = uncompressedBlockSize;\n+        this.idGenerator = idGenerator;\n     }\n \n-    public CompressableRecordWriter(final OutputStream out, final TocWriter tocWriter, final boolean compressed, final int uncompressedBlockSize) throws IOException {\n-        super(null, tocWriter);\n+    public CompressableRecordWriter(final OutputStream out, final String storageLocation, final AtomicLong idGenerator, final TocWriter tocWriter, final boolean compressed,\n+        final int uncompressedBlockSize) throws IOException {\n+        super(storageLocation, tocWriter);\n         this.fos = null;\n \n         this.compressed = compressed;\n         this.uncompressedBlockSize = uncompressedBlockSize;\n         this.rawOutStream = new ByteCountingOutputStream(out);\n+        this.idGenerator = idGenerator;\n     }\n \n \n+    protected AtomicLong getIdGenerator() {\n+        return idGenerator;\n+    }\n+\n     @Override\n     public synchronized void writeHeader(final long firstEventId) throws IOException {\n         if (isDirty()) {\n             throw new IOException(\"Cannot update Provenance Repository because this Record Writer has already failed to write to the Repository\");\n         }\n \n         try {\n-            lastBlockOffset = rawOutStream.getBytesWritten();\n+            blockStartOffset = rawOutStream.getBytesWritten();\n             resetWriteStream(firstEventId);\n             out.writeUTF(getSerializationName());\n             out.writeInt(getSerializationVersion());\n             writeHeader(firstEventId, out);\n             out.flush();\n-            lastBlockOffset = rawOutStream.getBytesWritten();\n+            blockStartOffset = getBytesWritten();\n         } catch (final IOException ioe) {\n             markDirty();\n             throw ioe;\n@@ -95,7 +105,7 @@ public synchronized void writeHeader(final long firstEventId) throws IOException\n      * @param eventId the first id that will be written to the new block\n      * @throws IOException if unable to flush/close the current streams properly\n      */\n-    private void resetWriteStream(final long eventId) throws IOException {\n+    protected void resetWriteStream(final Long eventId) throws IOException {\n         try {\n             if (out != null) {\n                 out.flush();\n@@ -114,13 +124,13 @@ private void resetWriteStream(final long eventId) throws IOException {\n                     out.close();\n                 }\n \n-                if (tocWriter != null) {\n+                if (tocWriter != null && eventId != null) {\n                     tocWriter.addBlockOffset(rawOutStream.getBytesWritten(), eventId);\n                 }\n \n                 writableStream = new BufferedOutputStream(new GZIPOutputStream(new NonCloseableOutputStream(rawOutStream), 1), 65536);\n             } else {\n-                if (tocWriter != null) {\n+                if (tocWriter != null && eventId != null) {\n                     tocWriter.addBlockOffset(rawOutStream.getBytesWritten(), eventId);\n                 }\n \n@@ -136,41 +146,47 @@ private void resetWriteStream(final long eventId) throws IOException {\n         }\n     }\n \n-\n+    protected synchronized void ensureStreamState(final long recordIdentifier, final long startBytes) throws IOException {\n+        // add a new block to the TOC if needed.\n+        if (getTocWriter() != null && (startBytes - blockStartOffset >= uncompressedBlockSize)) {\n+            blockStartOffset = startBytes;\n+            resetWriteStream(recordIdentifier);\n+        }\n+    }\n \n     @Override\n-    public long writeRecord(final ProvenanceEventRecord record, final long recordIdentifier) throws IOException {\n+    public synchronized StorageSummary writeRecord(final ProvenanceEventRecord record) throws IOException {\n         if (isDirty()) {\n             throw new IOException(\"Cannot update Provenance Repository because this Record Writer has already failed to write to the Repository\");\n         }\n \n         try {\n+            final long recordIdentifier = record.getEventId() == -1L ? idGenerator.getAndIncrement() : record.getEventId();\n             final long startBytes = byteCountingOut.getBytesWritten();\n \n-            // add a new block to the TOC if needed.\n-            if (getTocWriter() != null && (startBytes - lastBlockOffset >= uncompressedBlockSize)) {\n-                lastBlockOffset = startBytes;\n-\n-                if (compressed) {\n-                    // because of the way that GZIPOutputStream works, we need to call close() on it in order for it\n-                    // to write its trailing bytes. But we don't want to close the underlying OutputStream, so we wrap\n-                    // the underlying OutputStream in a NonCloseableOutputStream\n-                    resetWriteStream(recordIdentifier);\n-                }\n-            }\n-\n+            ensureStreamState(recordIdentifier, startBytes);\n             writeRecord(record, recordIdentifier, out);\n \n             recordCount++;\n-            return byteCountingOut.getBytesWritten() - startBytes;\n+            final long bytesWritten = byteCountingOut.getBytesWritten();\n+            final long serializedLength = bytesWritten - startBytes;\n+            final TocWriter tocWriter = getTocWriter();\n+            final Integer blockIndex = tocWriter == null ? null : tocWriter.getCurrentBlockIndex();\n+            final String storageLocation = getStorageLocation();\n+            return new StorageSummary(recordIdentifier, storageLocation, blockIndex, serializedLength, bytesWritten);\n         } catch (final IOException ioe) {\n             markDirty();\n             throw ioe;\n         }\n     }\n \n     @Override\n-    public void flush() throws IOException {\n+    public synchronized long getBytesWritten() {\n+        return byteCountingOut == null ? 0L : byteCountingOut.getBytesWritten();\n+    }\n+\n+    @Override\n+    public synchronized void flush() throws IOException {\n         out.flush();\n     }\n \n@@ -180,22 +196,26 @@ public synchronized int getRecordsWritten() {\n     }\n \n     @Override\n-    protected OutputStream getBufferedOutputStream() {\n+    protected synchronized DataOutputStream getBufferedOutputStream() {\n         return out;\n     }\n \n     @Override\n-    protected OutputStream getUnderlyingOutputStream() {\n+    protected synchronized OutputStream getUnderlyingOutputStream() {\n         return fos;\n     }\n \n     @Override\n-    protected void syncUnderlyingOutputStream() throws IOException {\n+    protected synchronized void syncUnderlyingOutputStream() throws IOException {\n         if (fos != null) {\n             fos.getFD().sync();\n         }\n     }\n \n+    protected boolean isCompressed() {\n+        return compressed;\n+    }\n+\n     protected abstract void writeRecord(final ProvenanceEventRecord event, final long eventId, final DataOutputStream out) throws IOException;\n \n     protected abstract void writeHeader(final long firstEventId, final DataOutputStream out) throws IOException;",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/CompressableRecordWriter.java",
                "sha": "b5646009b025c5f84151edecee17326a0d6b8715",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/EmptyRecordReader.java",
                "changes": 12,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/EmptyRecordReader.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/EmptyRecordReader.java",
                "patch": "@@ -18,7 +18,9 @@\n package org.apache.nifi.provenance.serialization;\n \n import java.io.IOException;\n+import java.util.Optional;\n \n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n import org.apache.nifi.provenance.StandardProvenanceEventRecord;\n import org.apache.nifi.provenance.toc.TocReader;\n \n@@ -69,4 +71,14 @@ public long getBytesConsumed() {\n     public long getMaxEventId() throws IOException {\n         return 0;\n     }\n+\n+    @Override\n+    public Optional<ProvenanceEventRecord> skipToEvent(long eventId) throws IOException {\n+        return Optional.empty();\n+    }\n+\n+    @Override\n+    public boolean isData() {\n+        return false;\n+    }\n }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/EmptyRecordReader.java",
                "sha": "d4487ab2e5321a69faf909b9e1db7bd7c67112ab",
                "status": "modified"
            },
            {
                "additions": 188,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/EventFileCompressor.java",
                "changes": 188,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/EventFileCompressor.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/EventFileCompressor.java",
                "patch": "@@ -0,0 +1,188 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.serialization;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.FileOutputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.OutputStream;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.nifi.provenance.store.EventFileManager;\n+import org.apache.nifi.provenance.toc.StandardTocReader;\n+import org.apache.nifi.provenance.toc.StandardTocWriter;\n+import org.apache.nifi.provenance.toc.TocReader;\n+import org.apache.nifi.provenance.toc.TocUtil;\n+import org.apache.nifi.provenance.toc.TocWriter;\n+import org.apache.nifi.provenance.util.CloseableUtil;\n+import org.apache.nifi.stream.io.ByteCountingOutputStream;\n+import org.apache.nifi.stream.io.GZIPOutputStream;\n+import org.apache.nifi.stream.io.NonCloseableOutputStream;\n+import org.apache.nifi.stream.io.StreamUtils;\n+import org.apache.nifi.util.FormatUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * <p>\n+ * This class is responsible for compressing Event Files as a background task. This is done as a background task instead of being\n+ * done inline because if compression is performed inline, whenever NiFi is restarted (especially if done so abruptly), it is very\n+ * possible that the GZIP stream will be corrupt. As a result, we would stand to lose some Provenance Events when NiFi is restarted.\n+ * In order to avoid that, we write data in an uncompressed format and then compress the data in the background. Once the data has\n+ * been compressed, this task will then remove the original, uncompressed file. If the file is being read by another thread, this\n+ * task will wait for the other thread to finish reading the data before deleting the file. This synchronization of the File is handled\n+ * via the {@link EventFileManager Event File Manager}.\n+ * </p>\n+ */\n+public class EventFileCompressor implements Runnable {\n+    private static final Logger logger = LoggerFactory.getLogger(EventFileCompressor.class);\n+    private final BlockingQueue<File> filesToCompress;\n+    private final EventFileManager eventFileManager;\n+    private volatile boolean shutdown = false;\n+\n+    public EventFileCompressor(final BlockingQueue<File> filesToCompress, final EventFileManager eventFileManager) {\n+        this.filesToCompress = filesToCompress;\n+        this.eventFileManager = eventFileManager;\n+    }\n+\n+    public void shutdown() {\n+        shutdown = true;\n+    }\n+\n+    @Override\n+    public void run() {\n+        while (!shutdown) {\n+            File uncompressedEventFile = null;\n+\n+            try {\n+                final long start = System.nanoTime();\n+                uncompressedEventFile = filesToCompress.poll(1, TimeUnit.SECONDS);\n+                if (uncompressedEventFile == null || shutdown) {\n+                    continue;\n+                }\n+\n+                File outputFile = null;\n+                long bytesBefore = 0L;\n+                StandardTocReader tocReader = null;\n+\n+                File tmpTocFile = null;\n+                eventFileManager.obtainReadLock(uncompressedEventFile);\n+                try {\n+                    StandardTocWriter tocWriter = null;\n+\n+                    final File tocFile = TocUtil.getTocFile(uncompressedEventFile);\n+                    try {\n+                        tocReader = new StandardTocReader(tocFile);\n+                    } catch (final IOException e) {\n+                        logger.error(\"Failed to read TOC File {}\", tocFile, e);\n+                        continue;\n+                    }\n+\n+                    bytesBefore = uncompressedEventFile.length();\n+\n+                    try {\n+                        outputFile = new File(uncompressedEventFile.getParentFile(), uncompressedEventFile.getName() + \".gz\");\n+                        try {\n+                            tmpTocFile = new File(tocFile.getParentFile(), tocFile.getName() + \".tmp\");\n+                            tocWriter = new StandardTocWriter(tmpTocFile, true, false);\n+                            compress(uncompressedEventFile, tocReader, outputFile, tocWriter);\n+                            tocWriter.close();\n+                        } catch (final IOException ioe) {\n+                            logger.error(\"Failed to compress {} on rollover\", uncompressedEventFile, ioe);\n+                        }\n+                    } finally {\n+                        CloseableUtil.closeQuietly(tocReader, tocWriter);\n+                    }\n+                } finally {\n+                    eventFileManager.releaseReadLock(uncompressedEventFile);\n+                }\n+\n+                eventFileManager.obtainWriteLock(uncompressedEventFile);\n+                try {\n+                    // Attempt to delete the input file and associated toc file\n+                    if (uncompressedEventFile.delete()) {\n+                        if (tocReader != null) {\n+                            final File tocFile = tocReader.getFile();\n+                            if (!tocFile.delete()) {\n+                                logger.warn(\"Failed to delete {}; this file should be cleaned up manually\", tocFile);\n+                            }\n+\n+                            if (tmpTocFile != null) {\n+                                tmpTocFile.renameTo(tocFile);\n+                            }\n+                        }\n+                    } else {\n+                        logger.warn(\"Failed to delete {}; this file should be cleaned up manually\", uncompressedEventFile);\n+                    }\n+                } finally {\n+                    eventFileManager.releaseWriteLock(uncompressedEventFile);\n+                }\n+\n+                final long millis = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);\n+                final long bytesAfter = outputFile.length();\n+                final double reduction = 100 * (1 - (double) bytesAfter / (double) bytesBefore);\n+                final String reductionTwoDecimals = String.format(\"%.2f\", reduction);\n+                logger.debug(\"Successfully compressed Provenance Event File {} in {} millis from {} to {}, a reduction of {}%\",\n+                    uncompressedEventFile, millis, FormatUtils.formatDataSize(bytesBefore), FormatUtils.formatDataSize(bytesAfter), reductionTwoDecimals);\n+            } catch (final InterruptedException e) {\n+                Thread.currentThread().interrupt();\n+                return;\n+            } catch (final Exception e) {\n+                logger.error(\"Failed to compress {}\", uncompressedEventFile, e);\n+            }\n+        }\n+    }\n+\n+    public static void compress(final File input, final TocReader tocReader, final File output, final TocWriter tocWriter) throws IOException {\n+        try (final InputStream fis = new FileInputStream(input);\n+            final OutputStream fos = new FileOutputStream(output);\n+            final ByteCountingOutputStream byteCountingOut = new ByteCountingOutputStream(fos)) {\n+\n+            int blockIndex = 0;\n+            while (true) {\n+                // Determine the min and max byte ranges for the current block.\n+                final long blockStart = tocReader.getBlockOffset(blockIndex);\n+                if (blockStart == -1) {\n+                    break;\n+                }\n+\n+                long blockEnd = tocReader.getBlockOffset(blockIndex + 1);\n+                if (blockEnd < 0) {\n+                    blockEnd = input.length();\n+                }\n+\n+                final long firstEventId = tocReader.getFirstEventIdForBlock(blockIndex);\n+                final long blockStartOffset = byteCountingOut.getBytesWritten();\n+\n+                try (final OutputStream ncos = new NonCloseableOutputStream(byteCountingOut);\n+                    final OutputStream gzipOut = new GZIPOutputStream(ncos, 1)) {\n+                    StreamUtils.copy(fis, gzipOut, blockEnd - blockStart);\n+                }\n+\n+                tocWriter.addBlockOffset(blockStartOffset, firstEventId);\n+                blockIndex++;\n+            }\n+        }\n+\n+        // Close the TOC Reader and TOC Writer\n+        CloseableUtil.closeQuietly(tocReader, tocWriter);\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/EventFileCompressor.java",
                "sha": "4814c95d7b5b605dc1e56bef58e5d69091dba977",
                "status": "added"
            },
            {
                "additions": 22,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/RecordReader.java",
                "changes": 23,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/RecordReader.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 1,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/RecordReader.java",
                "patch": "@@ -18,7 +18,9 @@\n \n import java.io.Closeable;\n import java.io.IOException;\n+import java.util.Optional;\n \n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n import org.apache.nifi.provenance.StandardProvenanceEventRecord;\n import org.apache.nifi.provenance.toc.TocReader;\n \n@@ -50,13 +52,25 @@\n     /**\n      * Skips to the specified compression block\n      *\n-     * @param blockIndex the byte index to skip to\n+     * @param blockIndex the block index to skip to\n      * @throws IOException if the underlying stream throws IOException, or if the reader has already\n      * read passed the specified compression block index\n      * @throws IllegalStateException if the RecordReader does not have a TableOfContents associated with it\n      */\n     void skipToBlock(int blockIndex) throws IOException;\n \n+    /**\n+     * Skips to the first event in the stream with an Event ID >= the given ID. If no event is found with an\n+     * ID >= the given ID an empty Optional is returned. Otherwise, an Optional containing the first event in the stream with an\n+     * ID >= the given ID is returned. Unlike {@link #nextRecord()}, this method does not consume the returned event from the stream.\n+     * I.e., if a record is returned, that same record will be returned again the next time that {@link #nextRecord()} is called.\n+     *\n+     * @param eventId the ID of the event to retrieve\n+     * @return the first event in the stream with an Event ID >= the given ID or an empty Optional if no such event can be found\n+     * @throws IOException if the underlying stream throws IOException\n+     */\n+    Optional<ProvenanceEventRecord> skipToEvent(long eventId) throws IOException;\n+\n     /**\n      * Returns the block index that the Reader is currently reading from.\n      * Note that the block index is incremented at the beginning of the {@link #nextRecord()}\n@@ -100,4 +114,11 @@\n      * @throws IOException if unable to get id of the last event\n      */\n     long getMaxEventId() throws IOException;\n+\n+    /**\n+     * Returns <code>true</code> if there is more data for hte Record Reader to read, <code>false</code> otherwise.\n+     *\n+     * @return <code>true</code> if there is more data for hte Record Reader to read, <code>false</code> otherwise.\n+     */\n+    boolean isData();\n }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/RecordReader.java",
                "sha": "9377f2c785d01c674ab03c95c6aff0b2600b4254",
                "status": "modified"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/RecordReaders.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/RecordReaders.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 5,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/RecordReaders.java",
                "patch": "@@ -30,8 +30,9 @@\n \n import org.apache.nifi.provenance.ByteArraySchemaRecordReader;\n import org.apache.nifi.provenance.ByteArraySchemaRecordWriter;\n+import org.apache.nifi.provenance.EventIdFirstSchemaRecordReader;\n+import org.apache.nifi.provenance.EventIdFirstSchemaRecordWriter;\n import org.apache.nifi.provenance.StandardRecordReader;\n-import org.apache.nifi.provenance.StandardRecordWriter;\n import org.apache.nifi.provenance.lucene.LuceneUtil;\n import org.apache.nifi.provenance.toc.StandardTocReader;\n import org.apache.nifi.provenance.toc.TocReader;\n@@ -78,10 +79,10 @@ public static RecordReader newRecordReader(File file, final Collection<Path> pro\n             String filename = file.getName();\n             openStream: while ( fis == null ) {\n                 final File dir = file.getParentFile();\n-                final String baseName = LuceneUtil.substringBefore(file.getName(), \".\");\n+                final String baseName = LuceneUtil.substringBefore(file.getName(), \".prov\");\n \n-                // depending on which rollover actions have occurred, we could have 3 possibilities for the\n-                // filename that we need. The majority of the time, we will use the extension \".prov.indexed.gz\"\n+                // depending on which rollover actions have occurred, we could have 2 possibilities for the\n+                // filename that we need. The majority of the time, we will use the extension \".prov.gz\"\n                 // because most often we are compressing on rollover and most often we have already finished\n                 // compressing by the time that we are querying the data.\n                 for ( final String extension : new String[] {\".prov.gz\", \".prov\"} ) {\n@@ -123,7 +124,7 @@ public static RecordReader newRecordReader(File file, final Collection<Path> pro\n             }\n \n             switch (serializationName) {\n-                case StandardRecordWriter.SERIALIZATION_NAME: {\n+                case StandardRecordReader.SERIALIZATION_NAME: {\n                     if (tocFile.exists()) {\n                         final TocReader tocReader = new StandardTocReader(tocFile);\n                         return new StandardRecordReader(bufferedInStream, filename, tocReader, maxAttributeChars);\n@@ -139,6 +140,14 @@ public static RecordReader newRecordReader(File file, final Collection<Path> pro\n                         return new ByteArraySchemaRecordReader(bufferedInStream, filename, maxAttributeChars);\n                     }\n                 }\n+                case EventIdFirstSchemaRecordWriter.SERIALIZATION_NAME: {\n+                    if (!tocFile.exists()) {\n+                        throw new FileNotFoundException(\"Cannot create TOC Reader because the file \" + tocFile + \" does not exist\");\n+                    }\n+\n+                    final TocReader tocReader = new StandardTocReader(tocFile);\n+                    return new EventIdFirstSchemaRecordReader(bufferedInStream, filename, tocReader, maxAttributeChars);\n+                }\n                 default: {\n                     throw new IOException(\"Unable to read data from file \" + file + \" because the file was written using an unknown Serializer: \" + serializationName);\n                 }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/RecordReaders.java",
                "sha": "8e79ddd7fc0e639e00faea70cb48d502fbf7caa3",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/RecordWriter.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/RecordWriter.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 2,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/RecordWriter.java",
                "patch": "@@ -37,11 +37,10 @@\n      * Writes the given record out to the underlying stream\n      *\n      * @param record the record to write\n-     * @param recordIdentifier the new identifier of the record\n      * @return the number of bytes written for the given records\n      * @throws IOException if unable to write the record to the stream\n      */\n-    long writeRecord(ProvenanceEventRecord record, long recordIdentifier) throws IOException;\n+    StorageSummary writeRecord(ProvenanceEventRecord record) throws IOException;\n \n     /**\n      * Flushes any data that is held in a buffer to the underlying storage mechanism\n@@ -55,6 +54,11 @@\n      */\n     int getRecordsWritten();\n \n+    /**\n+     * @return the number of bytes written to this writer\n+     */\n+    long getBytesWritten();\n+\n     /**\n      * @return the file that this RecordWriter is writing to\n      */\n@@ -88,6 +92,11 @@\n      */\n     void markDirty();\n \n+    /**\n+     * @return <code>true</code> if {@link #markDirty()} has been called, <code>false</code> otherwise\n+     */\n+    boolean isDirty();\n+\n     /**\n      * Syncs the content written to this writer to disk.\n      *",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/RecordWriter.java",
                "sha": "c9d2a22adf012bb5f280cfcc8c3a3fdee3a71091",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/RecordWriters.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/RecordWriters.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 4,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/RecordWriters.java",
                "patch": "@@ -18,6 +18,7 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicLong;\n \n import org.apache.nifi.provenance.ByteArraySchemaRecordWriter;\n import org.apache.nifi.provenance.toc.StandardTocWriter;\n@@ -27,13 +28,14 @@\n public class RecordWriters {\n     private static final int DEFAULT_COMPRESSION_BLOCK_SIZE = 1024 * 1024; // 1 MB\n \n-    public static RecordWriter newSchemaRecordWriter(final File file, final boolean compressed, final boolean createToc) throws IOException {\n-        return newSchemaRecordWriter(file, compressed, createToc, DEFAULT_COMPRESSION_BLOCK_SIZE);\n+    public static RecordWriter newSchemaRecordWriter(final File file, final AtomicLong idGenerator, final boolean compressed, final boolean createToc) throws IOException {\n+        return newSchemaRecordWriter(file, idGenerator, compressed, createToc, DEFAULT_COMPRESSION_BLOCK_SIZE);\n     }\n \n-    public static RecordWriter newSchemaRecordWriter(final File file, final boolean compressed, final boolean createToc, final int compressionBlockBytes) throws IOException {\n+    public static RecordWriter newSchemaRecordWriter(final File file, final AtomicLong idGenerator, final boolean compressed, final boolean createToc,\n+        final int compressionBlockBytes) throws IOException {\n         final TocWriter tocWriter = createToc ? new StandardTocWriter(TocUtil.getTocFile(file), false, false) : null;\n-        return new ByteArraySchemaRecordWriter(file, tocWriter, compressed, compressionBlockBytes);\n+        return new ByteArraySchemaRecordWriter(file, idGenerator, tocWriter, compressed, compressionBlockBytes);\n     }\n \n }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/RecordWriters.java",
                "sha": "cacaebdff01efa8836aa1c8733fb1499bf09d3bf",
                "status": "modified"
            },
            {
                "additions": 72,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/StorageSummary.java",
                "changes": 72,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/StorageSummary.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/StorageSummary.java",
                "patch": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.serialization;\n+\n+import java.util.Optional;\n+\n+public class StorageSummary {\n+    private final long eventId;\n+    private final String storageLocation;\n+    private final String partitionName;\n+    private final Integer blockIndex;\n+    private final long serializedLength;\n+    private final long bytesWritten;\n+\n+    public StorageSummary(final long eventId, final String storageLocation, final Integer blockIndex, final long serializedLength, final long bytesWritten) {\n+        this(eventId, storageLocation, null, blockIndex, serializedLength, bytesWritten);\n+    }\n+\n+    public StorageSummary(final long eventId, final String storageLocation, final String partitionName,\n+        final Integer blockIndex, final long serializedLength, final long bytesWritten) {\n+        this.eventId = eventId;\n+        this.storageLocation = storageLocation;\n+        this.partitionName = partitionName;\n+        this.blockIndex = blockIndex;\n+        this.serializedLength = serializedLength;\n+        this.bytesWritten = bytesWritten;\n+    }\n+\n+    public long getEventId() {\n+        return eventId;\n+    }\n+\n+    public String getStorageLocation() {\n+        return storageLocation;\n+    }\n+\n+    public Optional<String> getPartitionName() {\n+        return Optional.ofNullable(partitionName);\n+    }\n+\n+    public Integer getBlockIndex() {\n+        return blockIndex;\n+    }\n+\n+    public long getSerializedLength() {\n+        return serializedLength;\n+    }\n+\n+    public long getBytesWritten() {\n+        return bytesWritten;\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"StorageSummary[eventId=\" + getEventId() + \", partition=\" + getPartitionName().orElse(null) + \", location=\" + getStorageLocation() + \"]\";\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/serialization/StorageSummary.java",
                "sha": "dffcd6d8eda3cf91fa3ff46b138b2b2df246b881",
                "status": "added"
            },
            {
                "additions": 109,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/EventFileManager.java",
                "changes": 109,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/EventFileManager.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/EventFileManager.java",
                "patch": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.store;\n+\n+import java.io.File;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n+import java.util.concurrent.locks.ReadWriteLock;\n+import java.util.concurrent.locks.ReentrantReadWriteLock;\n+import java.util.function.Function;\n+\n+import org.apache.nifi.provenance.lucene.LuceneUtil;\n+import org.apache.nifi.util.Tuple;\n+\n+/**\n+ * The EventFileManager is responsible for maintaining locks on Event Files so that we can ensure that no thread deletes\n+ * an Event File while it is still being read. Without this manager, this could happen, for instance, if the Compression Thread\n+ * were to compress an Event File, and then delete the original/uncompressed version while a Provenance Query was reading the\n+ * uncompressed version of the file.\n+ */\n+public class EventFileManager {\n+\n+    private final ConcurrentMap<String, Tuple<ReadWriteLock, Integer>> lockMap = new ConcurrentHashMap<>();\n+\n+    private String getMapKey(final File file) {\n+        return LuceneUtil.substringBefore(file.getName(), \".prov\");\n+    }\n+\n+    private ReadWriteLock updateCount(final File file, final Function<Integer, Integer> update) {\n+        final String key = getMapKey(file);\n+        boolean updated = false;\n+\n+        Tuple<ReadWriteLock, Integer> updatedTuple = null;\n+        while (!updated) {\n+            final Tuple<ReadWriteLock, Integer> tuple = lockMap.computeIfAbsent(key, k -> new Tuple<>(new ReentrantReadWriteLock(), 0));\n+            final Integer updatedCount = update.apply(tuple.getValue());\n+            updatedTuple = new Tuple<>(tuple.getKey(), updatedCount);\n+            updated = lockMap.replace(key, tuple, updatedTuple);\n+        }\n+\n+        return updatedTuple.getKey();\n+    }\n+\n+    private ReadWriteLock incrementCount(final File file) {\n+        return updateCount(file, val -> val + 1);\n+    }\n+\n+    private ReadWriteLock decrementCount(final File file) {\n+        return updateCount(file, val -> val - 1);\n+    }\n+\n+\n+    public void obtainReadLock(final File file) {\n+        final ReadWriteLock rwLock = incrementCount(file);\n+        rwLock.readLock().lock();\n+    }\n+\n+    public void releaseReadLock(final File file) {\n+        final ReadWriteLock rwLock = decrementCount(file);\n+        rwLock.readLock().unlock();\n+    }\n+\n+    public void obtainWriteLock(final File file) {\n+        final ReadWriteLock rwLock = incrementCount(file);\n+        rwLock.writeLock().lock();\n+    }\n+\n+    public void releaseWriteLock(final File file) {\n+        final String key = getMapKey(file);\n+\n+        boolean updated = false;\n+        while (!updated) {\n+            final Tuple<ReadWriteLock, Integer> tuple = lockMap.get(key);\n+            if (tuple == null) {\n+                throw new IllegalMonitorStateException(\"Lock is not owned\");\n+            }\n+\n+            // If this is the only reference to the lock, remove it from the map and then unlock.\n+            if (tuple.getValue() <= 1) {\n+                updated = lockMap.remove(key, tuple);\n+                if (updated) {\n+                    tuple.getKey().writeLock().unlock();\n+                }\n+            } else {\n+                final Tuple<ReadWriteLock, Integer> updatedTuple = new Tuple<>(tuple.getKey(), tuple.getValue() - 1);\n+                updated = lockMap.replace(key, tuple, updatedTuple);\n+                if (updated) {\n+                    tuple.getKey().writeLock().unlock();\n+                }\n+            }\n+        }\n+    }\n+\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/EventFileManager.java",
                "sha": "3754113a0badc8481e5c3be4f3e816c5c5d15e3d",
                "status": "added"
            },
            {
                "additions": 123,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/EventStore.java",
                "changes": 123,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/EventStore.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/EventStore.java",
                "patch": "@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.store;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.authorization.EventAuthorizer;\n+import org.apache.nifi.provenance.authorization.EventTransformer;\n+import org.apache.nifi.provenance.index.EventIndex;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n+\n+/**\n+ * <p>\n+ * An Event Store is responsible for storing Provenance Events and retrieving them at a later time.\n+ * </p>\n+ */\n+public interface EventStore extends Closeable {\n+\n+    /**\n+     * Performs any initialization routines that need to happen before the store is used\n+     *\n+     * @throws IOException if unable to perform initialization\n+     */\n+    void initialize() throws IOException;\n+\n+    /**\n+     * Adds the given events to the store. All events will be written to the same Storage Location.\n+     * I.e., all of the {@link StorageSummary} objects that are provided when calling the {@link StorageResult#getStorageLocations()}\n+     * method will return the same value for the {@link StorageSummary#getStorageLocation()}. Each one, however, will\n+     * have a different Event ID and potentially a different Block Index.\n+     *\n+     * @param events the events to add\n+     * @return a mapping of event to the location where it was stored\n+     * @throws IOException if unable to add the events\n+     */\n+    StorageResult addEvents(Iterable<ProvenanceEventRecord> events) throws IOException;\n+\n+    /**\n+     * @return the number of bytes occupied by the events in the store\n+     * @throws IOException if unable to determine the size of the store\n+     */\n+    long getSize() throws IOException;\n+\n+    /**\n+     * @return the largest Event ID that has been written to this store, or -1 if no events have yet been stored.\n+     */\n+    long getMaxEventId();\n+\n+    /**\n+     * Retrieves the event with the given ID\n+     *\n+     * @param id the ID of the event to retrieve\n+     * @return an Optional containing the Event with the given ID, or an empty optional if the event cannot be found\n+     * @throws IOException if unable to read the event from storage\n+     */\n+    Optional<ProvenanceEventRecord> getEvent(long id) throws IOException;\n+\n+    /**\n+     * Retrieves up to maxRecords events from the store, starting with the event whose ID is equal to firstRecordId. If that\n+     * event cannot be found, then the first event will be the oldest event in the store whose ID is greater than firstRecordId.\n+     * All events will be returned in the order that they were written to the store. I.e., all events will have monotonically\n+     * increasing Event ID's. No events will be filtered out, since there is no EventAuthorizer provided.\n+     *\n+     * @param firstRecordId the ID of the first event to retrieve\n+     * @param maxRecords the maximum number of records to retrieve. The actual number of results returned may be less than this.\n+     * @return a List of ProvenanceEventRecord's\n+     * @throws IOException if unable to retrieve records from the store\n+     */\n+    List<ProvenanceEventRecord> getEvents(long firstRecordId, int maxRecords) throws IOException;\n+\n+    /**\n+     * Retrieves up to maxRecords events from the store, starting with the event whose ID is equal to firstRecordId. If that\n+     * event cannot be found, then the first event will be the oldest event in the store whose ID is greater than firstRecordId.\n+     * All events will be returned in the order that they were written to the store. I.e., all events will have monotonically\n+     * increasing Event ID's.\n+     *\n+     * @param firstRecordId the ID of the first event to retrieve\n+     * @param maxRecords the maximum number of records to retrieve. The actual number of results returned may be less than this.\n+     * @param authorizer the authorizer that should be used to filter out any events that the user doesn't have access to\n+     * @param unauthorizedTransformer the transformer to apply to unauthorized events\n+     * @return a List of ProvenanceEventRecord's\n+     * @throws IOException if unable to retrieve records from the store\n+     */\n+    List<ProvenanceEventRecord> getEvents(long firstRecordId, int maxRecords, EventAuthorizer authorizer, EventTransformer unauthorizedTransformer) throws IOException;\n+\n+    /**\n+     * Given a List of Event ID's, returns a List of Provenance Events that contain the events that have those corresponding\n+     * Event ID's. If any events cannot be found, a warning will be logged but no Exception will be thrown.\n+     *\n+     * @param eventIds a Stream of Event ID's\n+     * @param authorizer the authorizer that should be used to filter out any events that the user doesn't have access to\n+     * @param unauthorizedTransformer the transformer to apply to unauthorized events\n+     * @return a List of events that correspond to the given ID's\n+     * @throws IOException if unable to retrieve records from the store\n+     */\n+    List<ProvenanceEventRecord> getEvents(List<Long> eventIds, EventAuthorizer authorizer, EventTransformer unauthorizedTransformer) throws IOException;\n+\n+    /**\n+     * Causes the latest events in this store to be re-indexed by the given Event Index\n+     *\n+     * @param eventIndex the EventIndex to use for indexing events\n+     */\n+    void reindexLatestEvents(EventIndex eventIndex);\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/EventStore.java",
                "sha": "ba4acea97440c2cc4a41a7d46d6928c2c47c7fbc",
                "status": "added"
            },
            {
                "additions": 113,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/EventStorePartition.java",
                "changes": 113,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/EventStorePartition.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/EventStorePartition.java",
                "patch": "@@ -0,0 +1,113 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.store;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.authorization.EventAuthorizer;\n+import org.apache.nifi.provenance.store.iterator.EventIterator;\n+\n+public interface EventStorePartition extends Closeable {\n+    /**\n+     * Performs any initialization routines that need to happen before the store is used\n+     *\n+     * @throws IOException if unable to perform initialization\n+     */\n+    void initialize() throws IOException;\n+\n+    /**\n+     * Adds the given events to the store\n+     *\n+     * @param events the events to add\n+     * @return a mapping of event to the location where it was stored\n+     * @throws IOException if unable to add the events\n+     */\n+    StorageResult addEvents(Iterable<ProvenanceEventRecord> events) throws IOException;\n+\n+    /**\n+     * @return the number of bytes occupied by the events in the store\n+     */\n+    long getSize();\n+\n+    /**\n+     * @return the largest Event ID that has been written to this store, or -1 if no events have yet been stored.\n+     */\n+    long getMaxEventId();\n+\n+    /**\n+     * Retrieves the event with the given ID\n+     *\n+     * @param id the ID of the event to retrieve\n+     * @return the Event with the given ID, or <code>null</code> if the event cannot be found\n+     * @throws IOException if unable to read the event from storage\n+     */\n+    Optional<ProvenanceEventRecord> getEvent(long id) throws IOException;\n+\n+    /**\n+     * Retrieves up to maxRecords events from the store, starting with the event whose ID is equal to firstRecordId. If that\n+     * event cannot be found, then the first event will be the oldest event in the store whose ID is greater than firstRecordId.\n+     * All events will be returned in the order that they were written to the store. I.e., all events will have monotonically\n+     * increasing Event ID's.\n+     *\n+     * @param firstRecordId the ID of the first event to retrieve\n+     * @param maxEvents the maximum number of events to retrieve. The actual number of results returned may be less than this.\n+     * @param authorizer the authorizer that should be used to filter out any events that the user doesn't have access to\n+     * @return a List of ProvenanceEventRecord's\n+     * @throws IOException if unable to retrieve records from the store\n+     */\n+    List<ProvenanceEventRecord> getEvents(long firstRecordId, int maxEvents, EventAuthorizer authorizer) throws IOException;\n+\n+    /**\n+     * Returns an {@link EventIterator} that is capable of iterating over the events in the store beginning with the given\n+     * record id. The events returned by the EventIterator will be provided in the order in which they were stored in the\n+     * partition. All events retrieved from this EventIterator will have monotonically increasing Event ID's.\n+     *\n+     * @param minimumEventId the minimum value of any Event ID that should be returned\n+     * @return an EventIterator that is capable of iterating over events in the store\n+     */\n+    EventIterator createEventIterator(long minimumEventId);\n+\n+    /**\n+     * Returns an {@link EventIterator} that iterates over the given event ID's and returns one ProvenanceEventRecord for\n+     * each given, if the ID given can be found. If a given ID cannot be found, it will be skipped and no error will be reported.\n+     *\n+     * @param eventIds the ID's of the events to retrieve\n+     * @return an EventIterator that iterates over the given event ID's\n+     */\n+    EventIterator createEventIterator(List<Long> eventIds);\n+\n+    /**\n+     * Purges any events from the partition that are older than the given amount of time\n+     *\n+     * @param olderThan the amount of time for which any event older than this should be removed\n+     * @param timeUnit the unit of time that applies to the first argument\n+     */\n+    void purgeOldEvents(long olderThan, TimeUnit timeUnit);\n+\n+    /**\n+     * Purges some number of events from the partition. The oldest events will be purged.\n+     *\n+     * @return the number of bytes purged from the partition\n+     */\n+    long purgeOldestEvents();\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/EventStorePartition.java",
                "sha": "ccb94f821b0336791dbbe01bddc1d3fbac41bb50",
                "status": "added"
            },
            {
                "additions": 284,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/PartitionedEventStore.java",
                "changes": 284,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/PartitionedEventStore.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/PartitionedEventStore.java",
                "patch": "@@ -0,0 +1,284 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.store;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.function.Function;\n+\n+import org.apache.lucene.util.NamedThreadFactory;\n+import org.apache.nifi.events.EventReporter;\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.RepositoryConfiguration;\n+import org.apache.nifi.provenance.authorization.EventAuthorizer;\n+import org.apache.nifi.provenance.authorization.EventTransformer;\n+import org.apache.nifi.provenance.store.iterator.AuthorizingEventIterator;\n+import org.apache.nifi.provenance.store.iterator.EventIterator;\n+import org.apache.nifi.provenance.util.DirectoryUtils;\n+import org.apache.nifi.reporting.Severity;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public abstract class PartitionedEventStore implements EventStore {\n+    private static final Logger logger = LoggerFactory.getLogger(PartitionedEventStore.class);\n+    private static final String EVENT_CATEGORY = \"Provenance Repository\";\n+\n+    private final AtomicLong partitionIndex = new AtomicLong(0L);\n+    private final RepositoryConfiguration repoConfig;\n+    private final EventReporter eventReporter;\n+    private ScheduledExecutorService maintenanceExecutor;\n+\n+    public PartitionedEventStore(final RepositoryConfiguration config, final EventReporter eventReporter) {\n+        this.repoConfig = config;\n+        this.eventReporter = eventReporter;\n+    }\n+\n+\n+    @Override\n+    public void initialize() throws IOException {\n+        maintenanceExecutor = Executors.newScheduledThreadPool(1, new NamedThreadFactory(\"Provenance Repository Maintenance\"));\n+        maintenanceExecutor.scheduleWithFixedDelay(() -> performMaintenance(), 1, 1, TimeUnit.MINUTES);\n+\n+        for (final EventStorePartition partition : getPartitions()) {\n+            partition.initialize();\n+        }\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        if (maintenanceExecutor != null) {\n+            maintenanceExecutor.shutdownNow();\n+        }\n+\n+        IOException thrown = null;\n+\n+        for (final EventStorePartition partition : getPartitions()) {\n+            try {\n+               partition.close();\n+            } catch (final IOException ioe) {\n+                if (thrown == null) {\n+                    thrown = ioe;\n+                } else {\n+                    thrown.addSuppressed(ioe);\n+                }\n+            }\n+        }\n+\n+        if (thrown != null) {\n+            throw thrown;\n+        }\n+    }\n+\n+\n+    @Override\n+    public StorageResult addEvents(final Iterable<ProvenanceEventRecord> events) throws IOException {\n+        final List<? extends EventStorePartition> partitions = getPartitions();\n+        final int index = (int) (partitionIndex.getAndIncrement() % partitions.size());\n+        final EventStorePartition partition = partitions.get(index);\n+        return partition.addEvents(events);\n+    }\n+\n+    @Override\n+    public long getSize() {\n+        long size = 0;\n+        for (final EventStorePartition partition : getPartitions()) {\n+            size += partition.getSize();\n+        }\n+\n+        return size;\n+    }\n+\n+    private long getRepoSize() {\n+        long total = 0L;\n+\n+        for (final File storageDir : repoConfig.getStorageDirectories().values()) {\n+            total += DirectoryUtils.getSize(storageDir);\n+        }\n+\n+        return total;\n+    }\n+\n+    @Override\n+    public long getMaxEventId() {\n+        return getPartitions().stream()\n+            .mapToLong(part -> part.getMaxEventId())\n+            .max()\n+            .orElse(-1L);\n+    }\n+\n+    @Override\n+    public Optional<ProvenanceEventRecord> getEvent(final long id) throws IOException {\n+        for (final EventStorePartition partition : getPartitions()) {\n+            final Optional<ProvenanceEventRecord> option = partition.getEvent(id);\n+            if (option.isPresent()) {\n+                return option;\n+            }\n+        }\n+\n+        return Optional.empty();\n+    }\n+\n+    @Override\n+    public List<ProvenanceEventRecord> getEvents(final long firstRecordId, final int maxRecords) throws IOException {\n+        return getEvents(firstRecordId, maxRecords, EventAuthorizer.GRANT_ALL, EventTransformer.EMPTY_TRANSFORMER);\n+    }\n+\n+    @Override\n+    public List<ProvenanceEventRecord> getEvents(final long firstRecordId, final int maxRecords, final EventAuthorizer authorizer,\n+        final EventTransformer transformer) throws IOException {\n+        if (firstRecordId + maxRecords < 1 || maxRecords < 1 || firstRecordId > getMaxEventId()) {\n+            return Collections.emptyList();\n+        }\n+\n+        return getEvents(maxRecords, authorizer, part -> part.createEventIterator(firstRecordId), transformer);\n+    }\n+\n+    @Override\n+    public List<ProvenanceEventRecord> getEvents(final List<Long> eventIds, final EventAuthorizer authorizer, final EventTransformer transformer) throws IOException {\n+        if (eventIds == null || eventIds.isEmpty()) {\n+            return Collections.emptyList();\n+        }\n+\n+        return getEvents(eventIds.size(), authorizer, part -> part.createEventIterator(eventIds), transformer);\n+    }\n+\n+    private List<ProvenanceEventRecord> getEvents(final int maxRecords, final EventAuthorizer authorizer,\n+        final Function<EventStorePartition, EventIterator> eventIteratorFactory, final EventTransformer transformer) throws IOException {\n+\n+        if (maxRecords < 1) {\n+            return Collections.emptyList();\n+        }\n+\n+        final List<ProvenanceEventRecord> selectedEvents = new ArrayList<>();\n+\n+        // Create a Map so that the key is the next record available from a partition and the value is the EventIterator from which\n+        // the record came. This sorted map is then used so that we are able to always get the first entry, which is the next\n+        // lowest record id among all partitions.\n+        final SortedMap<ProvenanceEventRecord, EventIterator> recordToIteratorMap = new TreeMap<>(\n+            (o1, o2) -> Long.compare(o1.getEventId(), o2.getEventId()));\n+\n+        try {\n+            // Seed our map with the first event in each Partition.\n+            for (final EventStorePartition partition : getPartitions()) {\n+                final EventAuthorizer nonNullAuthorizer = authorizer == null ? EventAuthorizer.GRANT_ALL : authorizer;\n+                final EventIterator partitionIterator = eventIteratorFactory.apply(partition);\n+                final EventIterator iterator = new AuthorizingEventIterator(partitionIterator, nonNullAuthorizer, transformer);\n+\n+                final Optional<ProvenanceEventRecord> option = iterator.nextEvent();\n+                if (option.isPresent()) {\n+                    recordToIteratorMap.put(option.get(), iterator);\n+                }\n+            }\n+\n+            // If no records found, just return the empty list.\n+            if (recordToIteratorMap.isEmpty()) {\n+                return selectedEvents;\n+            }\n+\n+            // Get the event with the next-lowest ID. Add it to the list of selected events,\n+            // then read the next event from the same EventIterator that this event came from.\n+            // This ensures that our map is always populated with the next event for each\n+            // EventIterator, which also ensures that the first key in our map is the event\n+            // with the lowest ID (since all events from a given EventIterator have monotonically\n+            // increasing Event ID's).\n+            ProvenanceEventRecord nextEvent = recordToIteratorMap.firstKey();\n+            while (nextEvent != null && selectedEvents.size() < maxRecords) {\n+                selectedEvents.add(nextEvent);\n+\n+                final EventIterator iterator = recordToIteratorMap.remove(nextEvent);\n+                final Optional<ProvenanceEventRecord> nextRecordFromIterator = iterator.nextEvent();\n+                if (nextRecordFromIterator.isPresent()) {\n+                    recordToIteratorMap.put(nextRecordFromIterator.get(), iterator);\n+                }\n+\n+                nextEvent = recordToIteratorMap.isEmpty() ? null : recordToIteratorMap.firstKey();\n+            }\n+\n+            return selectedEvents;\n+        } finally {\n+            // Ensure that we close all record readers that have been created\n+            for (final EventIterator iterator : recordToIteratorMap.values()) {\n+                try {\n+                    iterator.close();\n+                } catch (final Exception e) {\n+                    if (logger.isDebugEnabled()) {\n+                        logger.warn(\"Failed to close Record Reader {}\", iterator, e);\n+                    } else {\n+                        logger.warn(\"Failed to close Record Reader {}\", iterator);\n+                    }\n+                }\n+            }\n+        }\n+    }\n+\n+\n+    void performMaintenance() {\n+        try {\n+            final long maxFileLife = repoConfig.getMaxRecordLife(TimeUnit.MILLISECONDS);\n+            for (final EventStorePartition partition : getPartitions()) {\n+                try {\n+                    partition.purgeOldEvents(maxFileLife, TimeUnit.MILLISECONDS);\n+                } catch (final Exception e) {\n+                    logger.error(\"Failed to purge expired events from \" + partition, e);\n+                    eventReporter.reportEvent(Severity.WARNING, EVENT_CATEGORY,\n+                        \"Failed to purge expired events from Provenance Repository. See logs for more information.\");\n+                }\n+            }\n+\n+            final long maxStorageCapacity = repoConfig.getMaxStorageCapacity();\n+            long currentSize;\n+            try {\n+                currentSize = getRepoSize();\n+            } catch (final Exception e) {\n+                logger.error(\"Could not determine size of Provenance Repository. Will not expire any data due to storage limits\", e);\n+                eventReporter.reportEvent(Severity.WARNING, EVENT_CATEGORY, \"Failed to determine size of Provenance Repository. \"\n+                    + \"No data will be expired due to storage limits at this time. See logs for more information.\");\n+                return;\n+            }\n+\n+            while (currentSize > maxStorageCapacity) {\n+                for (final EventStorePartition partition : getPartitions()) {\n+                    try {\n+                        final long removed = partition.purgeOldestEvents();\n+                        currentSize -= removed;\n+                    } catch (final Exception e) {\n+                        logger.error(\"Failed to purge oldest events from \" + partition, e);\n+                        eventReporter.reportEvent(Severity.WARNING, EVENT_CATEGORY,\n+                            \"Failed to purge oldest events from Provenance Repository. See logs for more information.\");\n+                    }\n+                }\n+            }\n+        } catch (final Exception e) {\n+            logger.error(\"Failed to perform periodic maintenance\", e);\n+            eventReporter.reportEvent(Severity.ERROR, EVENT_CATEGORY,\n+                \"Failed to perform periodic maintenace for Provenance Repository. See logs for more information.\");\n+        }\n+    }\n+\n+    protected abstract List<? extends EventStorePartition> getPartitions();\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/PartitionedEventStore.java",
                "sha": "5f922dd43b632580cf4dc3f73fba455dcdde5992",
                "status": "added"
            },
            {
                "additions": 142,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/PartitionedWriteAheadEventStore.java",
                "changes": 142,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/PartitionedWriteAheadEventStore.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/PartitionedWriteAheadEventStore.java",
                "patch": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.store;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import org.apache.lucene.util.NamedThreadFactory;\n+import org.apache.nifi.events.EventReporter;\n+import org.apache.nifi.provenance.RepositoryConfiguration;\n+import org.apache.nifi.provenance.index.EventIndex;\n+import org.apache.nifi.provenance.serialization.EventFileCompressor;\n+\n+public class PartitionedWriteAheadEventStore extends PartitionedEventStore {\n+    private final BlockingQueue<File> filesToCompress;\n+    private final List<WriteAheadStorePartition> partitions;\n+    private final RepositoryConfiguration repoConfig;\n+\n+    private final ExecutorService compressionExecutor;\n+    private final List<EventFileCompressor> fileCompressors = Collections.synchronizedList(new ArrayList<>());\n+    private final EventReporter eventReporter;\n+    private final EventFileManager fileManager;\n+\n+    public PartitionedWriteAheadEventStore(final RepositoryConfiguration repoConfig, final RecordWriterFactory recordWriterFactory,\n+        final RecordReaderFactory recordReaderFactory, final EventReporter eventReporter, final EventFileManager fileManager) {\n+        super(repoConfig, eventReporter);\n+        this.repoConfig = repoConfig;\n+        this.eventReporter = eventReporter;\n+        this.filesToCompress = new LinkedBlockingQueue<>(100);\n+        final AtomicLong idGenerator = new AtomicLong(0L);\n+        this.partitions = createPartitions(repoConfig, recordWriterFactory, recordReaderFactory, idGenerator);\n+        this.fileManager = fileManager;\n+\n+        // Creates tasks to compress data on rollover\n+        if (repoConfig.isCompressOnRollover()) {\n+            compressionExecutor = Executors.newFixedThreadPool(repoConfig.getIndexThreadPoolSize(), new NamedThreadFactory(\"Compress Provenance Logs\"));\n+        } else {\n+            compressionExecutor = null;\n+        }\n+    }\n+\n+    private List<WriteAheadStorePartition> createPartitions(final RepositoryConfiguration repoConfig, final RecordWriterFactory recordWriterFactory,\n+        final RecordReaderFactory recordReaderFactory, final AtomicLong idGenerator) {\n+        final Map<String, File> storageDirectories = repoConfig.getStorageDirectories();\n+        final List<WriteAheadStorePartition> partitions = new ArrayList<>(storageDirectories.size());\n+\n+        for (final Map.Entry<String, File> entry : storageDirectories.entrySet()) {\n+            // Need to ensure that the same partition directory always gets the same partition index.\n+            // If we don't, then we will end up re-indexing the events from 1 index into another index, and\n+            // this will result in a lot of duplicates (up to a million per index per restart). This is the reason\n+            // that we use a partition name here based on the properties file.\n+            final String partitionName = entry.getKey();\n+            final File storageDirectory = entry.getValue();\n+            partitions.add(new WriteAheadStorePartition(storageDirectory, partitionName, repoConfig,\n+                recordWriterFactory, recordReaderFactory, filesToCompress, idGenerator, eventReporter));\n+        }\n+\n+        return partitions;\n+    }\n+\n+    @Override\n+    public void initialize() throws IOException {\n+        if (repoConfig.isCompressOnRollover()) {\n+            for (int i = 0; i < repoConfig.getIndexThreadPoolSize(); i++) {\n+                final EventFileCompressor compressor = new EventFileCompressor(filesToCompress, fileManager);\n+                compressionExecutor.submit(compressor);\n+                fileCompressors.add(compressor);\n+            }\n+        }\n+\n+        super.initialize();\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        super.close();\n+\n+        for (final EventFileCompressor compressor : fileCompressors) {\n+            compressor.shutdown();\n+        }\n+\n+        if (compressionExecutor != null) {\n+            compressionExecutor.shutdown();\n+        }\n+    }\n+\n+    @Override\n+    public void reindexLatestEvents(final EventIndex eventIndex) {\n+        final List<WriteAheadStorePartition> partitions = getPartitions();\n+        final int numPartitions = partitions.size();\n+\n+        final List<Future<?>> futures = new ArrayList<>(numPartitions);\n+        final ExecutorService executor = Executors.newFixedThreadPool(numPartitions);\n+\n+        for (final WriteAheadStorePartition partition : partitions) {\n+            futures.add(executor.submit(() -> partition.reindexLatestEvents(eventIndex)));\n+        }\n+\n+        executor.shutdown();\n+        for (final Future<?> future : futures) {\n+            try {\n+                future.get();\n+            } catch (InterruptedException e) {\n+                Thread.currentThread().interrupt();\n+                throw new RuntimeException(\"Failed to re-index events because Thread was interrupted\", e);\n+            } catch (ExecutionException e) {\n+                throw new RuntimeException(\"Failed to re-index events\", e);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected List<WriteAheadStorePartition> getPartitions() {\n+        return partitions;\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/PartitionedWriteAheadEventStore.java",
                "sha": "14de80eae0654cf754b127d505bbde9b8740b0e7",
                "status": "added"
            },
            {
                "additions": 29,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/RecordReaderFactory.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/RecordReaderFactory.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/RecordReaderFactory.java",
                "patch": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.store;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Path;\n+import java.util.Collection;\n+\n+import org.apache.nifi.provenance.serialization.RecordReader;\n+\n+public interface RecordReaderFactory {\n+    RecordReader newRecordReader(File file, Collection<Path> provenanceLogFiles, int maxAttributeChars) throws IOException;\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/RecordReaderFactory.java",
                "sha": "ddb8165f8fdacbe8944b3068e1c1733fe7d0b412",
                "status": "added"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/RecordWriterFactory.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/RecordWriterFactory.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/RecordWriterFactory.java",
                "patch": "@@ -0,0 +1,28 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.store;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import org.apache.nifi.provenance.serialization.RecordWriter;\n+\n+public interface RecordWriterFactory {\n+    RecordWriter createWriter(final File file, final AtomicLong idGenerator, final boolean compressed, final boolean createToc) throws IOException;\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/RecordWriterFactory.java",
                "sha": "89da603516226f72ddf85f50c8bac605896a49c6",
                "status": "added"
            },
            {
                "additions": 93,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/RecordWriterLease.java",
                "changes": 93,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/RecordWriterLease.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/RecordWriterLease.java",
                "patch": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.store;\n+\n+import org.apache.nifi.provenance.serialization.RecordWriter;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class RecordWriterLease {\n+    private final Logger logger = LoggerFactory.getLogger(RecordWriterLease.class);\n+\n+    private final RecordWriter writer;\n+    private final long maxBytes;\n+    private final int maxEvents;\n+    private long usageCounter;\n+    private boolean markedRollable = false;\n+    private boolean closed = false;\n+\n+    public RecordWriterLease(final RecordWriter writer, final long maxBytes) {\n+        this(writer, maxBytes, Integer.MAX_VALUE);\n+    }\n+\n+    public RecordWriterLease(final RecordWriter writer, final long maxBytes, final int maxEvents) {\n+        this.writer = writer;\n+        this.maxBytes = maxBytes;\n+        this.maxEvents = maxEvents;\n+    }\n+\n+    public RecordWriter getWriter() {\n+        return writer;\n+    }\n+\n+    public synchronized boolean tryClaim() {\n+        if (markedRollable || writer.isClosed() || writer.isDirty() || writer.getBytesWritten() >= maxBytes || writer.getRecordsWritten() >= maxEvents) {\n+            return false;\n+        }\n+\n+        usageCounter++;\n+        return true;\n+    }\n+\n+    public synchronized void relinquishClaim() {\n+        usageCounter--;\n+\n+        if (closed && usageCounter < 1) {\n+            try {\n+                writer.close();\n+            } catch (final Exception e) {\n+                logger.warn(\"Failed to close \" + writer, e);\n+            }\n+        }\n+    }\n+\n+    public synchronized boolean shouldRoll() {\n+        if (markedRollable) {\n+            return true;\n+        }\n+\n+        if (usageCounter < 1 && (writer.isClosed() || writer.isDirty() || writer.getBytesWritten() >= maxBytes || writer.getRecordsWritten() >= maxEvents)) {\n+            markedRollable = true;\n+            return true;\n+        }\n+\n+        return false;\n+    }\n+\n+    public synchronized void close() {\n+        closed = true;\n+\n+        if (usageCounter < 1) {\n+            try {\n+                writer.close();\n+            } catch (final Exception e) {\n+                logger.warn(\"Failed to close \" + writer, e);\n+            }\n+        }\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/RecordWriterLease.java",
                "sha": "8543d2b4db7c4330a5b63b7aa907a4cf118c24d5",
                "status": "added"
            },
            {
                "additions": 67,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/StorageResult.java",
                "changes": 67,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/StorageResult.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/StorageResult.java",
                "patch": "@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.store;\n+\n+import java.util.Collections;\n+import java.util.Map;\n+\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n+\n+public interface StorageResult {\n+    /**\n+     * @return a map of each Provenance Event Record to the location where it was stored\n+     */\n+    Map<ProvenanceEventRecord, StorageSummary> getStorageLocations();\n+\n+    /**\n+     * Indicates whether or not the storage of events triggered the store to roll over\n+     * the storage location that it is storing data to\n+     *\n+     * @return <code>true</code> if the store rolled over to a new storage location, <code>false</code> otherwise\n+     */\n+    boolean triggeredRollover();\n+\n+    /**\n+     * @return the number of events that were stored in the storage location that was rolled over, or\n+     *         <code>null</code> if no storage locations were rolled over.\n+     */\n+    Integer getEventsRolledOver();\n+\n+    public static StorageResult EMPTY = new StorageResult() {\n+        @Override\n+        public Map<ProvenanceEventRecord, StorageSummary> getStorageLocations() {\n+            return Collections.emptyMap();\n+        }\n+\n+        @Override\n+        public boolean triggeredRollover() {\n+            return false;\n+        }\n+\n+        @Override\n+        public Integer getEventsRolledOver() {\n+            return null;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return \"StorageResult.EMPTY\";\n+        }\n+    };\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/StorageResult.java",
                "sha": "94b1ece8d2f077536c4bcbf86cca23c3b8c54aeb",
                "status": "added"
            },
            {
                "additions": 637,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/WriteAheadStorePartition.java",
                "changes": 637,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/WriteAheadStorePartition.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/WriteAheadStorePartition.java",
                "patch": "@@ -0,0 +1,637 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.store;\n+\n+import java.io.EOFException;\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Objects;\n+import java.util.Optional;\n+import java.util.SortedMap;\n+import java.util.TreeMap;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import org.apache.nifi.events.EventReporter;\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.RepositoryConfiguration;\n+import org.apache.nifi.provenance.StandardProvenanceEventRecord;\n+import org.apache.nifi.provenance.authorization.EventAuthorizer;\n+import org.apache.nifi.provenance.index.EventIndex;\n+import org.apache.nifi.provenance.serialization.RecordReader;\n+import org.apache.nifi.provenance.serialization.RecordWriter;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n+import org.apache.nifi.provenance.store.iterator.EventIterator;\n+import org.apache.nifi.provenance.store.iterator.SelectiveRecordReaderEventIterator;\n+import org.apache.nifi.provenance.store.iterator.SequentialRecordReaderEventIterator;\n+import org.apache.nifi.provenance.toc.TocUtil;\n+import org.apache.nifi.provenance.util.DirectoryUtils;\n+import org.apache.nifi.provenance.util.NamedThreadFactory;\n+import org.apache.nifi.util.FormatUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class WriteAheadStorePartition implements EventStorePartition {\n+    private static final Logger logger = LoggerFactory.getLogger(WriteAheadStorePartition.class);\n+\n+\n+    private final RepositoryConfiguration config;\n+    private final File partitionDirectory;\n+    private final String partitionName;\n+    private final RecordWriterFactory recordWriterFactory;\n+    private final RecordReaderFactory recordReaderFactory;\n+    private final BlockingQueue<File> filesToCompress;\n+    private final AtomicLong idGenerator;\n+    private final AtomicLong maxEventId = new AtomicLong(-1L);\n+    private volatile boolean closed = false;\n+\n+    private AtomicReference<RecordWriterLease> eventWriterLeaseRef = new AtomicReference<>();\n+\n+    private final SortedMap<Long, File> minEventIdToPathMap = new TreeMap<>();  // guarded by synchronizing on object\n+\n+    public WriteAheadStorePartition(final File storageDirectory, final String partitionName, final RepositoryConfiguration repoConfig, final RecordWriterFactory recordWriterFactory,\n+        final RecordReaderFactory recordReaderFactory, final BlockingQueue<File> filesToCompress, final AtomicLong idGenerator, final EventReporter eventReporter) {\n+\n+        this.partitionName = partitionName;\n+        this.config = repoConfig;\n+        this.idGenerator = idGenerator;\n+        this.partitionDirectory = storageDirectory;\n+        this.recordWriterFactory = recordWriterFactory;\n+        this.recordReaderFactory = recordReaderFactory;\n+        this.filesToCompress = filesToCompress;\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        closed = true;\n+\n+        final RecordWriterLease lease = eventWriterLeaseRef.get();\n+        if (lease != null) {\n+            lease.close();\n+        }\n+    }\n+\n+    @Override\n+    public synchronized void initialize() throws IOException {\n+        if (!partitionDirectory.exists()) {\n+            Files.createDirectories(partitionDirectory.toPath());\n+        }\n+\n+        final File[] files = partitionDirectory.listFiles(DirectoryUtils.EVENT_FILE_FILTER);\n+        if (files == null) {\n+            throw new IOException(\"Could not access files in the \" + partitionDirectory + \" directory\");\n+        }\n+\n+        // We need to determine what the largest Event ID is in this partition. To do this, we\n+        // iterate over all files starting with the file that has the greatest ID, and try to find\n+        // the largest Event ID in that file. Once we successfully determine the greatest Event ID\n+        // in any one of the files, we are done, since we are iterating over the files in order of\n+        // the Largest Event ID to the smallest.\n+        long maxEventId = -1L;\n+        final List<File> fileList = Arrays.asList(files);\n+        Collections.sort(fileList, DirectoryUtils.LARGEST_ID_FIRST);\n+        for (final File file : fileList) {\n+            try {\n+                final RecordReader reader = recordReaderFactory.newRecordReader(file, Collections.emptyList(), Integer.MAX_VALUE);\n+                final long eventId = reader.getMaxEventId();\n+                if (eventId > maxEventId) {\n+                    maxEventId = eventId;\n+                    break;\n+                }\n+            } catch (final IOException ioe) {\n+                logger.warn(\"Could not read file {}; if this file contains Provenance Events, new events may be created with the same event identifiers\", file, ioe);\n+            }\n+        }\n+\n+        synchronized (minEventIdToPathMap) {\n+            for (final File file : fileList) {\n+                final long minEventId = DirectoryUtils.getMinId(file);\n+                minEventIdToPathMap.put(minEventId, file);\n+            }\n+        }\n+\n+        this.maxEventId.set(maxEventId);\n+\n+        // If configured to compress, compress any files that are not yet compressed.\n+        if (config.isCompressOnRollover()) {\n+            final File[] uncompressedFiles = partitionDirectory.listFiles(f -> f.getName().endsWith(\".prov\"));\n+            if (uncompressedFiles != null) {\n+                for (final File file : uncompressedFiles) {\n+                    // If we have both a compressed file and an uncompressed file for the same .prov file, then\n+                    // we must have been in the process of compressing it when NiFi was restarted. Delete the partial\n+                    // .gz file and we will start compressing it again.\n+                    final File compressed = new File(file.getParentFile(), file.getName() + \".gz\");\n+                    if (compressed.exists()) {\n+                        compressed.delete();\n+                    }\n+                }\n+            }\n+        }\n+\n+        // Update the ID Generator to the max of the ID Generator or maxEventId\n+        final long nextPartitionId = maxEventId + 1;\n+        final long updatedId = idGenerator.updateAndGet(curVal -> Math.max(curVal, nextPartitionId));\n+        logger.info(\"After recovering {}, next Event ID to be generated will be {}\", partitionDirectory, updatedId);\n+    }\n+\n+\n+    @Override\n+    public StorageResult addEvents(final Iterable<ProvenanceEventRecord> events) throws IOException {\n+        if (closed) {\n+            throw new IOException(this + \" is closed\");\n+        }\n+\n+        // Claim a Record Writer Lease so that we have a writer to persist the events to\n+        boolean claimed = false;\n+        RecordWriterLease lease = null;\n+        while (!claimed) {\n+            lease = getLease();\n+            claimed = lease.tryClaim();\n+\n+            if (claimed) {\n+                break;\n+            }\n+\n+            if (lease.shouldRoll()) {\n+                tryRollover(lease);\n+            }\n+        }\n+\n+        // Add the events to the writer and ensure that we always\n+        // relinquish the claim that we've obtained on the writer\n+        Map<ProvenanceEventRecord, StorageSummary> storageMap;\n+        final RecordWriter writer = lease.getWriter();\n+        try {\n+            storageMap = addEvents(events, writer);\n+        } finally {\n+            lease.relinquishClaim();\n+        }\n+\n+        // Roll over the writer if necessary\n+        Integer eventsRolledOver = null;\n+        final boolean shouldRoll = lease.shouldRoll();\n+        try {\n+            if (shouldRoll && tryRollover(lease)) {\n+                eventsRolledOver = writer.getRecordsWritten();\n+            }\n+        } catch (final IOException ioe) {\n+            logger.error(\"Updated {} but failed to rollover to a new Event File\", this, ioe);\n+        }\n+\n+        final Integer rolloverCount = eventsRolledOver;\n+        return new StorageResult() {\n+            @Override\n+            public Map<ProvenanceEventRecord, StorageSummary> getStorageLocations() {\n+                return storageMap;\n+            }\n+\n+            @Override\n+            public boolean triggeredRollover() {\n+                return rolloverCount != null;\n+            }\n+\n+            @Override\n+            public Integer getEventsRolledOver() {\n+                return rolloverCount;\n+            }\n+\n+            @Override\n+            public String toString() {\n+                return getStorageLocations().toString();\n+            }\n+        };\n+    }\n+\n+    private RecordWriterLease getLease() throws IOException {\n+        while (true) {\n+            final RecordWriterLease lease = eventWriterLeaseRef.get();\n+            if (lease != null) {\n+                return lease;\n+            }\n+\n+            if (tryRollover(null)) {\n+                return eventWriterLeaseRef.get();\n+            }\n+        }\n+    }\n+\n+    private synchronized boolean tryRollover(final RecordWriterLease lease) throws IOException {\n+        if (!Objects.equals(lease, eventWriterLeaseRef.get())) {\n+            return false;\n+        }\n+\n+        final long nextEventId = idGenerator.get();\n+        final File updatedEventFile = new File(partitionDirectory, nextEventId + \".prov\");\n+        final RecordWriter updatedWriter = recordWriterFactory.createWriter(updatedEventFile, idGenerator, false, true);\n+        final RecordWriterLease updatedLease = new RecordWriterLease(updatedWriter, config.getMaxEventFileCapacity(), config.getMaxEventFileCount());\n+        final boolean updated = eventWriterLeaseRef.compareAndSet(lease, updatedLease);\n+\n+        if (updated) {\n+            updatedWriter.writeHeader(nextEventId);\n+\n+            synchronized (minEventIdToPathMap) {\n+                minEventIdToPathMap.put(nextEventId, updatedEventFile);\n+            }\n+\n+            if (config.isCompressOnRollover() && lease != null && lease.getWriter() != null) {\n+                boolean offered = false;\n+                while (!offered && !closed) {\n+                    try {\n+                        offered = filesToCompress.offer(lease.getWriter().getFile(), 1, TimeUnit.SECONDS);\n+                    } catch (final InterruptedException ie) {\n+                        Thread.currentThread().interrupt();\n+                        throw new IOException(\"Interrupted while waiting to enqueue \" + lease.getWriter().getFile() + \" for compression\");\n+                    }\n+                }\n+            }\n+\n+            return true;\n+        } else {\n+            try {\n+                updatedWriter.close();\n+            } catch (final Exception e) {\n+                logger.warn(\"Failed to close Record Writer {}; some resources may not be cleaned up properly.\", updatedWriter, e);\n+            }\n+\n+            updatedEventFile.delete();\n+            return false;\n+        }\n+    }\n+\n+    private Map<ProvenanceEventRecord, StorageSummary> addEvents(final Iterable<ProvenanceEventRecord> events, final RecordWriter writer) throws IOException {\n+        final Map<ProvenanceEventRecord, StorageSummary> locationMap = new HashMap<>();\n+\n+        try {\n+            long maxId = -1L;\n+            int numEvents = 0;\n+            for (final ProvenanceEventRecord nextEvent : events) {\n+                final StorageSummary writerSummary = writer.writeRecord(nextEvent);\n+                final StorageSummary summaryWithIndex = new StorageSummary(writerSummary.getEventId(), writerSummary.getStorageLocation(), this.partitionName,\n+                    writerSummary.getBlockIndex(), writerSummary.getSerializedLength(), writerSummary.getBytesWritten());\n+                locationMap.put(nextEvent, summaryWithIndex);\n+                maxId = summaryWithIndex.getEventId();\n+                numEvents++;\n+            }\n+\n+            if (numEvents == 0) {\n+                return locationMap;\n+            }\n+\n+            writer.flush();\n+\n+            // Update max event id to be equal to be the greater of the current value or the\n+            // max value just written.\n+            final long maxIdWritten = maxId;\n+            this.maxEventId.getAndUpdate(cur -> maxIdWritten > cur ? maxIdWritten : cur);\n+\n+            if (config.isAlwaysSync()) {\n+                writer.sync();\n+            }\n+        } catch (final Exception e) {\n+            // We need to set the repoDirty flag before we release the lock for this journal.\n+            // Otherwise, another thread may write to this journal -- this is a problem because\n+            // the journal contains part of our record but not all of it. Writing to the end of this\n+            // journal will result in corruption!\n+            writer.markDirty();\n+            throw e;\n+        }\n+\n+        return locationMap;\n+    }\n+\n+\n+    @Override\n+    public long getSize() {\n+        return getEventFilesFromDisk()\n+            .collect(Collectors.summarizingLong(file -> file.length()))\n+            .getSum();\n+    }\n+\n+    private Stream<File> getEventFilesFromDisk() {\n+        final File[] files = partitionDirectory.listFiles(DirectoryUtils.EVENT_FILE_FILTER);\n+        return files == null ? Stream.empty() : Arrays.stream(files);\n+    }\n+\n+    @Override\n+    public long getMaxEventId() {\n+        return maxEventId.get();\n+    }\n+\n+    @Override\n+    public Optional<ProvenanceEventRecord> getEvent(final long id) throws IOException {\n+        final Optional<File> option = getPathForEventId(id);\n+        if (!option.isPresent()) {\n+            return Optional.empty();\n+        }\n+\n+        try (final RecordReader reader = recordReaderFactory.newRecordReader(option.get(), Collections.emptyList(), config.getMaxAttributeChars())) {\n+            final Optional<ProvenanceEventRecord> eventOption = reader.skipToEvent(id);\n+            if (!eventOption.isPresent()) {\n+                return eventOption;\n+            }\n+\n+            // If an event is returned, the event may be the one we want, or it may be an event with a\n+            // higher event ID, if the desired event is not in the record reader. So we need to get the\n+            // event and check the Event ID to know whether to return the empty optional or the Optional\n+            // that was returned.\n+            final ProvenanceEventRecord event = eventOption.get();\n+            if (event.getEventId() == id) {\n+                return eventOption;\n+            } else {\n+                return Optional.empty();\n+            }\n+        }\n+    }\n+\n+    @Override\n+    public List<ProvenanceEventRecord> getEvents(final long firstRecordId, final int maxEvents, final EventAuthorizer authorizer) throws IOException {\n+        final List<ProvenanceEventRecord> events = new ArrayList<>(Math.min(maxEvents, 1000));\n+        try (final EventIterator iterator = createEventIterator(firstRecordId)) {\n+            Optional<ProvenanceEventRecord> eventOption;\n+            while ((eventOption = iterator.nextEvent()).isPresent() && events.size() < maxEvents) {\n+                final ProvenanceEventRecord event = eventOption.get();\n+                if (authorizer.isAuthorized(event)) {\n+                    events.add(event);\n+                }\n+            }\n+        }\n+\n+        return events;\n+    }\n+\n+    @Override\n+    public EventIterator createEventIterator(final long minDesiredId) {\n+        final List<File> filesOfInterest = new ArrayList<>();\n+        synchronized (minEventIdToPathMap) {\n+            File lastFile = null;\n+\n+            for (final Map.Entry<Long, File> entry : minEventIdToPathMap.entrySet()) {\n+                final long minFileId = entry.getKey();\n+\n+                // If the minimum ID for the file is greater than the minDesiredId, then\n+                // that means that we will want to iterate over this file.\n+                if (minFileId > minDesiredId) {\n+                    // The minimum ID for this file is greater than the desired ID, so\n+                    // that means that the last file we saw may have the minimum desired\n+                    // ID and any number of more events before we get to this file. So\n+                    // if we've not already added the lastFile, add it now.\n+                    if (filesOfInterest.isEmpty() && lastFile != null) {\n+                        filesOfInterest.add(lastFile);\n+                    }\n+\n+                    filesOfInterest.add(entry.getValue());\n+                }\n+\n+                lastFile = entry.getValue();\n+            }\n+\n+            // We don't know the max ID of the last file, so we always want to include it, since it may contain\n+            // an event with an ID greater than minDesiredId.\n+            if (lastFile != null && !filesOfInterest.contains(lastFile)) {\n+                filesOfInterest.add(lastFile);\n+            }\n+        }\n+\n+        if (filesOfInterest.isEmpty()) {\n+            return EventIterator.EMPTY;\n+        }\n+\n+        return new SequentialRecordReaderEventIterator(filesOfInterest, recordReaderFactory, minDesiredId, config.getMaxAttributeChars());\n+    }\n+\n+\n+    @Override\n+    public EventIterator createEventIterator(final List<Long> eventIds) {\n+        final List<File> allFiles;\n+        synchronized (minEventIdToPathMap) {\n+            allFiles = new ArrayList<>(minEventIdToPathMap.values());\n+        }\n+\n+        if (allFiles.isEmpty()) {\n+            return EventIterator.EMPTY;\n+        }\n+\n+        return new SelectiveRecordReaderEventIterator(allFiles, recordReaderFactory, eventIds, config.getMaxAttributeChars());\n+    }\n+\n+    private Optional<File> getPathForEventId(final long id) {\n+        File lastFile = null;\n+\n+        synchronized (minEventIdToPathMap) {\n+            for (final Map.Entry<Long, File> entry : minEventIdToPathMap.entrySet()) {\n+                final long minId = entry.getKey();\n+                if (minId > id) {\n+                    break;\n+                }\n+\n+                lastFile = entry.getValue();\n+            }\n+        }\n+\n+        return Optional.ofNullable(lastFile);\n+    }\n+\n+\n+    @Override\n+    public void purgeOldEvents(final long olderThan, final TimeUnit unit) {\n+        final long timeCutoff = System.currentTimeMillis() - unit.toMillis(olderThan);\n+\n+        getEventFilesFromDisk().filter(file -> file.lastModified() < timeCutoff)\n+            .sorted(DirectoryUtils.SMALLEST_ID_FIRST)\n+            .forEach(file -> delete(file));\n+    }\n+\n+\n+    @Override\n+    public long purgeOldestEvents() {\n+        final List<File> eventFiles = getEventFilesFromDisk().sorted(DirectoryUtils.SMALLEST_ID_FIRST).collect(Collectors.toList());\n+        if (eventFiles.isEmpty()) {\n+            return 0L;\n+        }\n+\n+        for (final File eventFile : eventFiles) {\n+            final long fileSize = eventFile.length();\n+\n+            if (delete(eventFile)) {\n+                logger.debug(\"{} Deleted {} event file ({}) due to storage limits\", this, eventFile, FormatUtils.formatDataSize(fileSize));\n+                return fileSize;\n+            } else {\n+                logger.warn(\"{} Failed to delete oldest event file {}. This file should be cleaned up manually.\", this, eventFile);\n+                continue;\n+            }\n+        }\n+\n+        return 0L;\n+    }\n+\n+    private boolean delete(final File file) {\n+        final long firstEventId = DirectoryUtils.getMinId(file);\n+        synchronized (minEventIdToPathMap) {\n+            minEventIdToPathMap.remove(firstEventId);\n+        }\n+\n+        if (!file.delete()) {\n+            logger.warn(\"Failed to remove Provenance Event file {}; this file should be cleaned up manually\", file);\n+            return false;\n+        }\n+\n+        final File tocFile = TocUtil.getTocFile(file);\n+        if (tocFile.exists() && !tocFile.delete()) {\n+            logger.warn(\"Failed to remove Provenance Table-of-Contents file {}; this file should be cleaned up manually\", tocFile);\n+        }\n+\n+        return true;\n+    }\n+\n+    void reindexLatestEvents(final EventIndex eventIndex) {\n+        final List<File> eventFiles = getEventFilesFromDisk().sorted(DirectoryUtils.SMALLEST_ID_FIRST).collect(Collectors.toList());\n+        if (eventFiles.isEmpty()) {\n+            return;\n+        }\n+\n+        final long minEventIdToReindex = eventIndex.getMinimumEventIdToReindex(partitionName);\n+        final long maxEventId = getMaxEventId();\n+        final long eventsToReindex = maxEventId - minEventIdToReindex;\n+\n+        logger.info(\"The last Provenance Event indexed for partition {} is {}, but the last event written to partition has ID {}. \"\n+            + \"Re-indexing up to the last {} events to ensure that the Event Index is accurate and up-to-date\",\n+            partitionName, minEventIdToReindex, maxEventId, eventsToReindex, partitionDirectory);\n+\n+        // Find the first event file that we care about.\n+        int firstEventFileIndex = 0;\n+        for (int i = eventFiles.size() - 1; i >= 0; i--) {\n+            final File eventFile = eventFiles.get(i);\n+            final long minIdInFile = DirectoryUtils.getMinId(eventFile);\n+            if (minIdInFile <= minEventIdToReindex) {\n+                firstEventFileIndex = i;\n+                break;\n+            }\n+        }\n+\n+        // Create a subList that contains the files of interest\n+        final List<File> eventFilesToReindex = eventFiles.subList(firstEventFileIndex, eventFiles.size());\n+\n+        final ExecutorService executor = Executors.newFixedThreadPool(Math.min(4, eventFilesToReindex.size()), new NamedThreadFactory(\"Re-Index Provenance Events\", true));\n+        final List<Future<?>> futures = new ArrayList<>(eventFilesToReindex.size());\n+        final AtomicLong reindexedCount = new AtomicLong(0L);\n+\n+        // Re-Index the last bunch of events.\n+        // We don't use an Event Iterator here because it's possible that one of the event files could be corrupt (for example, if NiFi does while\n+        // writing to the file, a record may be incomplete). We don't want to prevent us from moving on and continuing to index the rest of the\n+        // un-indexed events. So we just use a List of files and create a reader for each one.\n+        final long start = System.nanoTime();\n+        int fileCount = 0;\n+        for (final File eventFile : eventFilesToReindex) {\n+            final boolean skipToEvent;\n+            if (fileCount++ == 0) {\n+                skipToEvent = true;\n+            } else {\n+                skipToEvent = false;\n+            }\n+\n+            final Runnable reindexTask = new Runnable() {\n+                @Override\n+                public void run() {\n+                    final Map<ProvenanceEventRecord, StorageSummary> storageMap = new HashMap<>(1000);\n+\n+                    try (final RecordReader recordReader = recordReaderFactory.newRecordReader(eventFile, Collections.emptyList(), Integer.MAX_VALUE)) {\n+                        if (skipToEvent) {\n+                            final Optional<ProvenanceEventRecord> eventOption = recordReader.skipToEvent(minEventIdToReindex);\n+                            if (!eventOption.isPresent()) {\n+                                return;\n+                            }\n+                        }\n+\n+                        StandardProvenanceEventRecord event = null;\n+                        while (true) {\n+                            final long startBytesConsumed = recordReader.getBytesConsumed();\n+\n+                            event = recordReader.nextRecord();\n+                            if (event == null) {\n+                                eventIndex.reindexEvents(storageMap);\n+                                reindexedCount.addAndGet(storageMap.size());\n+                                storageMap.clear();\n+                                break; // stop reading from this file\n+                            } else {\n+                                final long eventSize = recordReader.getBytesConsumed() - startBytesConsumed;\n+                                storageMap.put(event, new StorageSummary(event.getEventId(), eventFile.getName(), partitionName, recordReader.getBlockIndex(), eventSize, 0L));\n+\n+                                if (storageMap.size() == 1000) {\n+                                    eventIndex.reindexEvents(storageMap);\n+                                    reindexedCount.addAndGet(storageMap.size());\n+                                    storageMap.clear();\n+                                }\n+                            }\n+                        }\n+                    } catch (final EOFException eof) {\n+                        // Ran out of data. Continue on.\n+                        logger.warn(\"Failed to find event with ID {} in Event File {} due to {}\", minEventIdToReindex, eventFile, eof.toString());\n+                    } catch (final Exception e) {\n+                        logger.error(\"Failed to index Provenance Events found in {}\", eventFile, e);\n+                    }\n+                }\n+            };\n+\n+            futures.add(executor.submit(reindexTask));\n+        }\n+\n+        for (final Future<?> future : futures) {\n+            try {\n+                future.get();\n+            } catch (final ExecutionException ee) {\n+                logger.error(\"Failed to re-index some Provenance events. These events may not be query-able via the Provenance interface\", ee.getCause());\n+            } catch (final InterruptedException e) {\n+                Thread.currentThread().interrupt();\n+                logger.error(\"Interrupted while waiting for Provenance events to be re-indexed\", e);\n+                break;\n+            }\n+        }\n+\n+        try {\n+            eventIndex.commitChanges(partitionName);\n+        } catch (final IOException e) {\n+            logger.error(\"Failed to re-index Provenance Events for partition \" + partitionName, e);\n+        }\n+\n+        final long millis = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);\n+        final long seconds = millis / 1000L;\n+        final long millisRemainder = millis % 1000L;\n+        logger.info(\"Finished re-indexing {} events across {} files for {} in {}.{} seconds\",\n+            reindexedCount.get(), eventFilesToReindex.size(), partitionDirectory, seconds, millisRemainder);\n+    }\n+\n+    @Override\n+    public String toString() {\n+        return \"Provenance Event Store Partition[directory=\" + partitionDirectory + \"]\";\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/WriteAheadStorePartition.java",
                "sha": "a25043a50fea0fd5dd660ea25cf640ebd098c708",
                "status": "added"
            },
            {
                "additions": 63,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/iterator/AuthorizingEventIterator.java",
                "changes": 63,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/iterator/AuthorizingEventIterator.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/iterator/AuthorizingEventIterator.java",
                "patch": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.store.iterator;\n+\n+import java.io.IOException;\n+import java.util.Optional;\n+\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.authorization.EventAuthorizer;\n+import org.apache.nifi.provenance.authorization.EventTransformer;\n+\n+public class AuthorizingEventIterator implements EventIterator {\n+    private final EventIterator iterator;\n+    private final EventAuthorizer authorizer;\n+    private final EventTransformer transformer;\n+\n+    public AuthorizingEventIterator(final EventIterator iterator, final EventAuthorizer authorizer,\n+        final EventTransformer unauthorizedTransformer) {\n+        this.iterator = iterator;\n+        this.authorizer = authorizer;\n+        this.transformer = unauthorizedTransformer;\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        iterator.close();\n+    }\n+\n+    @Override\n+    public Optional<ProvenanceEventRecord> nextEvent() throws IOException {\n+        while (true) {\n+            final Optional<ProvenanceEventRecord> next = iterator.nextEvent();\n+            if (!next.isPresent()) {\n+                return next;\n+            }\n+\n+            if (authorizer.isAuthorized(next.get())) {\n+                return next;\n+            }\n+\n+            final Optional<ProvenanceEventRecord> eventOption = transformer.transform(next.get());\n+            if (eventOption.isPresent()) {\n+                return eventOption;\n+            }\n+        }\n+    }\n+\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/iterator/AuthorizingEventIterator.java",
                "sha": "7ff2be79c82c10a9aa3da4fa39c67481738835f7",
                "status": "added"
            },
            {
                "additions": 56,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/iterator/EventIterator.java",
                "changes": 56,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/iterator/EventIterator.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/iterator/EventIterator.java",
                "patch": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.store.iterator;\n+\n+import java.io.Closeable;\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Iterator;\n+import java.util.Optional;\n+\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+\n+public interface EventIterator extends Closeable {\n+\n+    Optional<ProvenanceEventRecord> nextEvent() throws IOException;\n+\n+    public static EventIterator EMPTY = new EventIterator() {\n+        @Override\n+        public void close() throws IOException {\n+        }\n+\n+        @Override\n+        public Optional<ProvenanceEventRecord> nextEvent() {\n+            return Optional.empty();\n+        }\n+    };\n+\n+    public static EventIterator of(final ProvenanceEventRecord... events) {\n+        final Iterator<ProvenanceEventRecord> itr = Arrays.asList(events).iterator();\n+        return new EventIterator() {\n+            @Override\n+            public void close() throws IOException {\n+            }\n+\n+            @Override\n+            public Optional<ProvenanceEventRecord> nextEvent() {\n+                return itr.hasNext() ? Optional.empty() : Optional.of(itr.next());\n+            }\n+        };\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/iterator/EventIterator.java",
                "sha": "79acda3d1bae9763a2c47b2dea5fd7594b6ab200",
                "status": "added"
            },
            {
                "additions": 174,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/iterator/SelectiveRecordReaderEventIterator.java",
                "changes": 174,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/iterator/SelectiveRecordReaderEventIterator.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/iterator/SelectiveRecordReaderEventIterator.java",
                "patch": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.store.iterator;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.serialization.RecordReader;\n+import org.apache.nifi.provenance.store.RecordReaderFactory;\n+import org.apache.nifi.provenance.util.DirectoryUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class SelectiveRecordReaderEventIterator implements EventIterator {\n+    private static final Logger logger = LoggerFactory.getLogger(SelectiveRecordReaderEventIterator.class);\n+    private final List<File> files;\n+    private final RecordReaderFactory readerFactory;\n+    private final List<Long> eventIds;\n+    private final Iterator<Long> idIterator;\n+    private final int maxAttributeChars;\n+\n+    private boolean closed = false;\n+    private RecordReader reader;\n+    private File currentFile;\n+\n+    public SelectiveRecordReaderEventIterator(final List<File> filesToRead, final RecordReaderFactory readerFactory, final List<Long> eventIds, final int maxAttributeChars) {\n+        this.readerFactory = readerFactory;\n+\n+        this.eventIds = new ArrayList<>(eventIds);\n+        Collections.sort(this.eventIds);\n+        idIterator = this.eventIds.iterator();\n+\n+        // Make a copy of the list of files and prune out any Files that are not relevant to the Event ID's that we were given.\n+        if (eventIds.isEmpty() || filesToRead.isEmpty()) {\n+            this.files = Collections.emptyList();\n+        } else {\n+            this.files = filterUnneededFiles(filesToRead, this.eventIds);\n+        }\n+\n+        this.maxAttributeChars = maxAttributeChars;\n+    }\n+\n+    protected static List<File> filterUnneededFiles(final List<File> filesToRead, final List<Long> eventIds) {\n+        final List<File> files = new ArrayList<>();\n+        final Long firstEventId = eventIds.get(0);\n+        final Long lastEventId = eventIds.get(eventIds.size() - 1);\n+\n+        final List<File> sortedFileList = new ArrayList<>(filesToRead);\n+        Collections.sort(sortedFileList, DirectoryUtils.SMALLEST_ID_FIRST);\n+\n+        File lastFile = null;\n+        for (final File file : filesToRead) {\n+            final long firstIdInFile = DirectoryUtils.getMinId(file);\n+            if (firstIdInFile > lastEventId) {\n+                continue;\n+            }\n+\n+            if (firstIdInFile > firstEventId) {\n+                if (files.isEmpty() && lastFile != null) {\n+                    files.add(lastFile);\n+                }\n+\n+                files.add(file);\n+            }\n+\n+            lastFile = file;\n+        }\n+\n+        if (files.isEmpty() && lastFile != null) {\n+            files.add(lastFile);\n+        }\n+\n+        return files;\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        closed = true;\n+\n+        if (reader != null) {\n+            reader.close();\n+        }\n+    }\n+\n+\n+    @Override\n+    public Optional<ProvenanceEventRecord> nextEvent() throws IOException {\n+        if (closed) {\n+            throw new IOException(\"EventIterator is already closed\");\n+        }\n+\n+        final long start = System.nanoTime();\n+        try {\n+            while (idIterator.hasNext()) {\n+                // Determine the next event ID to fetch\n+                final long eventId = idIterator.next();\n+\n+                // Determine which file the event should be in.\n+                final File fileForEvent = getFileForEventId(eventId);\n+                if (fileForEvent == null) {\n+                    continue;\n+                }\n+\n+                // If we determined which file the event should be in, and that's not the file that\n+                // we are currently reading from, rotate the reader to the appropriate one.\n+                if (!fileForEvent.equals(currentFile)) {\n+                    if (reader != null) {\n+                        try {\n+                            reader.close();\n+                        } catch (final Exception e) {\n+                            logger.warn(\"Failed to close {}; some resources may not be cleaned up appropriately\", reader);\n+                        }\n+                    }\n+\n+                    reader = readerFactory.newRecordReader(fileForEvent, Collections.emptyList(), maxAttributeChars);\n+                    this.currentFile = fileForEvent;\n+                }\n+\n+                final Optional<ProvenanceEventRecord> eventOption = reader.skipToEvent(eventId);\n+                if (eventOption.isPresent() && eventOption.get().getEventId() == eventId) {\n+                    reader.nextRecord();    // consume the event from the stream.\n+                    return eventOption;\n+                }\n+\n+                continue;\n+            }\n+\n+            return Optional.empty();\n+        } finally {\n+            final long ms = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);\n+            logger.trace(\"Took {} ms to read next event\", ms);\n+        }\n+    }\n+\n+    private File getFileForEventId(final long eventId) {\n+        File lastFile = null;\n+        for (final File file : files) {\n+            final long firstEventId = DirectoryUtils.getMinId(file);\n+            if (firstEventId == eventId) {\n+                return file;\n+            }\n+\n+            if (firstEventId > eventId) {\n+                return lastFile;\n+            }\n+\n+            lastFile = file;\n+        }\n+\n+        return lastFile;\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/iterator/SelectiveRecordReaderEventIterator.java",
                "sha": "c4a130ba0158bf30cdcc6b6664f7f9e35b47f1ed",
                "status": "added"
            },
            {
                "additions": 115,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/iterator/SequentialRecordReaderEventIterator.java",
                "changes": 115,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/iterator/SequentialRecordReaderEventIterator.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/iterator/SequentialRecordReaderEventIterator.java",
                "patch": "@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.store.iterator;\n+\n+import java.io.EOFException;\n+import java.io.File;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Optional;\n+\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.serialization.RecordReader;\n+import org.apache.nifi.provenance.store.RecordReaderFactory;\n+\n+public class SequentialRecordReaderEventIterator implements EventIterator {\n+    private final Iterator<File> fileIterator;\n+    private final RecordReaderFactory readerFactory;\n+    private final long minimumEventId;\n+    private final int maxAttributeChars;\n+\n+    private boolean closed = false;\n+    private RecordReader reader;\n+\n+    public SequentialRecordReaderEventIterator(final List<File> filesToRead, final RecordReaderFactory readerFactory, final long minimumEventId, final int maxAttributeChars) {\n+        this.fileIterator = filesToRead.iterator();\n+        this.readerFactory = readerFactory;\n+        this.minimumEventId = minimumEventId;\n+        this.maxAttributeChars = maxAttributeChars;\n+    }\n+\n+    @Override\n+    public void close() throws IOException {\n+        closed = true;\n+\n+        if (reader != null) {\n+            reader.close();\n+        }\n+    }\n+\n+    @Override\n+    public Optional<ProvenanceEventRecord> nextEvent() throws IOException {\n+        if (closed) {\n+            throw new IOException(\"EventIterator is already closed\");\n+        }\n+\n+        if (reader == null) {\n+            if (!rotateReader()) {\n+                return Optional.empty();\n+            }\n+        }\n+\n+        while (true) {\n+            final ProvenanceEventRecord event = reader.nextRecord();\n+            if (event == null) {\n+                if (rotateReader()) {\n+                    continue;\n+                } else {\n+                    return Optional.empty();\n+                }\n+            } else {\n+                return Optional.of(event);\n+            }\n+        }\n+    }\n+\n+    private boolean rotateReader() throws IOException {\n+        final boolean readerExists = (reader != null);\n+        if (readerExists) {\n+            reader.close();\n+        }\n+\n+        boolean multipleReadersOpened = false;\n+        while (true) {\n+            if (!fileIterator.hasNext()) {\n+                return false;\n+            }\n+\n+            final File eventFile = fileIterator.next();\n+            try {\n+                reader = readerFactory.newRecordReader(eventFile, Collections.emptyList(), maxAttributeChars);\n+                break;\n+            } catch (final FileNotFoundException | EOFException e) {\n+                multipleReadersOpened = true;\n+                // File may have aged off or was not fully written. Move to next file\n+                continue;\n+            }\n+        }\n+\n+        // If this is the first file in our list, the event of interest may not be the first event,\n+        // so skip to the event that we want.\n+        if (!readerExists && !multipleReadersOpened) {\n+            reader.skipToEvent(minimumEventId);\n+        }\n+\n+        return true;\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/store/iterator/SequentialRecordReaderEventIterator.java",
                "sha": "869febfb6b682dd53516a636a1cbd3e951f4a585",
                "status": "added"
            },
            {
                "additions": 52,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/toc/StandardTocReader.java",
                "changes": 68,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/toc/StandardTocReader.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 16,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/toc/StandardTocReader.java",
                "patch": "@@ -16,12 +16,13 @@\n  */\n package org.apache.nifi.provenance.toc;\n \n-import java.io.DataInputStream;\n import java.io.EOFException;\n import java.io.File;\n import java.io.FileInputStream;\n import java.io.IOException;\n \n+import org.apache.nifi.stream.io.StreamUtils;\n+\n /**\n  * Standard implementation of TocReader.\n  *\n@@ -38,27 +39,29 @@\n     private final boolean compressed;\n     private final long[] offsets;\n     private final long[] firstEventIds;\n+    private final File file;\n \n     public StandardTocReader(final File file) throws IOException {\n-        try (final FileInputStream fis = new FileInputStream(file);\n-                final DataInputStream dis = new DataInputStream(fis)) {\n+        this.file = file;\n+        final long fileLength = file.length();\n+        if (fileLength < 2) {\n+            throw new EOFException();\n+        }\n \n-            final int version = dis.read();\n-            if ( version < 0 ) {\n-                throw new EOFException();\n-            }\n+        try (final FileInputStream fis = new FileInputStream(file)) {\n+            final byte[] buffer = new byte[(int) fileLength];\n+            StreamUtils.fillBuffer(fis, buffer);\n \n-            final int compressionFlag = dis.read();\n-            if ( compressionFlag < 0 ) {\n-                throw new EOFException();\n-            }\n+            final int version = buffer[0];\n+            final int compressionFlag = buffer[1];\n \n             if ( compressionFlag == 0 ) {\n                 compressed = false;\n             } else if ( compressionFlag == 1 ) {\n                 compressed = true;\n             } else {\n-                throw new IOException(\"Table of Contents appears to be corrupt: could not read 'compression flag' from header; expected value of 0 or 1 but got \" + compressionFlag);\n+                throw new IOException(\"Table of Contents file \" + file + \" appears to be corrupt: could not read 'compression flag' from header; \"\n+                    + \"expected value of 0 or 1 but got \" + compressionFlag);\n             }\n \n             final int blockInfoBytes;\n@@ -72,7 +75,7 @@ public StandardTocReader(final File file) throws IOException {\n                     break;\n             }\n \n-            final int numBlocks = (int) ((file.length() - 2) / blockInfoBytes);\n+            final int numBlocks = (buffer.length - 2) / blockInfoBytes;\n             offsets = new long[numBlocks];\n \n             if ( version > 1 ) {\n@@ -81,21 +84,40 @@ public StandardTocReader(final File file) throws IOException {\n                 firstEventIds = new long[0];\n             }\n \n+            int index = 2;\n             for (int i=0; i < numBlocks; i++) {\n-                offsets[i] = dis.readLong();\n+                offsets[i] = readLong(buffer, index);\n+                index += 8;\n \n                 if ( version > 1 ) {\n-                    firstEventIds[i] = dis.readLong();\n+                    firstEventIds[i] = readLong(buffer, index);\n+                    index += 8;\n                 }\n             }\n         }\n     }\n \n+    private long readLong(final byte[] buffer, final int offset) {\n+        return ((long) buffer[offset] << 56) +\n+            ((long) (buffer[offset + 1] & 0xFF) << 48) +\n+            ((long) (buffer[offset + 2] & 0xFF) << 40) +\n+            ((long) (buffer[offset + 3] & 0xFF) << 32) +\n+            ((long) (buffer[offset + 4] & 0xFF) << 24) +\n+            ((long) (buffer[offset + 5] & 0xFF) << 16) +\n+            ((long) (buffer[offset + 6] & 0xFF) << 8) +\n+            (buffer[offset + 7] & 0xFF);\n+    }\n+\n     @Override\n     public boolean isCompressed() {\n         return compressed;\n     }\n \n+    @Override\n+    public File getFile() {\n+        return file;\n+    }\n+\n     @Override\n     public long getBlockOffset(final int blockIndex) {\n         if ( blockIndex >= offsets.length ) {\n@@ -104,6 +126,15 @@ public long getBlockOffset(final int blockIndex) {\n         return offsets[blockIndex];\n     }\n \n+    @Override\n+    public long getFirstEventIdForBlock(final int blockIndex) {\n+        if (blockIndex >= firstEventIds.length) {\n+            return -1L;\n+        }\n+\n+        return firstEventIds[blockIndex];\n+    }\n+\n     @Override\n     public long getLastBlockOffset() {\n         if ( offsets.length == 0 ) {\n@@ -113,7 +144,7 @@ public long getLastBlockOffset() {\n     }\n \n     @Override\n-    public void close() throws IOException {\n+    public void close() {\n     }\n \n     @Override\n@@ -152,4 +183,9 @@ public Integer getBlockIndexForEventId(final long eventId) {\n         // Therefore, if the event is present, it must be in the last block.\n         return firstEventIds.length - 1;\n     }\n+\n+    @Override\n+    public String toString() {\n+        return \"StandardTocReader[file=\" + file + \", compressed=\" + compressed + \"]\";\n+    }\n }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/toc/StandardTocReader.java",
                "sha": "a9c0f20e5ec4bd4338da065326bde1fc1f46e179",
                "status": "modified"
            },
            {
                "additions": 17,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/toc/TocReader.java",
                "changes": 18,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/toc/TocReader.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 1,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/toc/TocReader.java",
                "patch": "@@ -17,6 +17,7 @@\n package org.apache.nifi.provenance.toc;\n \n import java.io.Closeable;\n+import java.io.File;\n \n /**\n  * <p>\n@@ -36,15 +37,30 @@\n      */\n     boolean isCompressed();\n \n+    /**\n+     * @return the file that holds the TOC information\n+     */\n+    File getFile();\n+\n     /**\n      * Returns the byte offset into the Journal File for the Block with the given index.\n      *\n      * @param blockIndex the block index to get the byte offset for\n      * @return the byte offset for the given block index, or <code>-1</code> if the given block index\n-     * does not exist\n+     *         does not exist\n      */\n     long getBlockOffset(int blockIndex);\n \n+    /**\n+     * Returns the ID of the first event that is found in the block with the given index, or -1 if\n+     * the given block index does not exist.\n+     *\n+     * @param blockIndex the block index to get the first event id for\n+     * @return the ID of the first event that is found in the block with the given index, or -1 if\n+     *         the given block index does not exist\n+     */\n+    long getFirstEventIdForBlock(int blockIndex);\n+\n     /**\n      * Returns the byte offset into the Journal File of the last Block in the given index\n      * @return the byte offset into the Journal File of the last Block in the given index",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/toc/TocReader.java",
                "sha": "0bb630eea773bcd95e105e759fdb0dc46ae3af12",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/toc/TocUtil.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/toc/TocUtil.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 1,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/toc/TocUtil.java",
                "patch": "@@ -32,7 +32,7 @@\n      */\n     public static File getTocFile(final File journalFile) {\n         final File tocDir = new File(journalFile.getParentFile(), \"toc\");\n-        final String basename = LuceneUtil.substringBefore(journalFile.getName(), \".\");\n+        final String basename = LuceneUtil.substringBefore(journalFile.getName(), \".prov\");\n         final File tocFile = new File(tocDir, basename + \".toc\");\n         return tocFile;\n     }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/toc/TocUtil.java",
                "sha": "91f70db6536335ec3e5715d7ceae358d1bc4d5a6",
                "status": "modified"
            },
            {
                "additions": 45,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/CloseableUtil.java",
                "changes": 45,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/CloseableUtil.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/CloseableUtil.java",
                "patch": "@@ -0,0 +1,45 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.util;\n+\n+import java.io.Closeable;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class CloseableUtil {\n+    private static final Logger logger = LoggerFactory.getLogger(CloseableUtil.class);\n+\n+    public static void closeQuietly(final Closeable... closeables) {\n+        for (final Closeable closeable : closeables) {\n+            if (closeable == null) {\n+                continue;\n+            }\n+\n+            try {\n+                closeable.close();\n+            } catch (final Exception e) {\n+                if (logger.isDebugEnabled()) {\n+                    logger.warn(\"Failed to close {}; sources resources may not be cleaned up appropriately.\", closeable, e);\n+                } else {\n+                    logger.warn(\"Failed to close {}; sources resources may not be cleaned up appropriately.\", closeable);\n+                }\n+            }\n+        }\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/CloseableUtil.java",
                "sha": "26caa10ce38a706ea882c8e91d06adae1c9c4f8e",
                "status": "added"
            },
            {
                "additions": 96,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/DirectoryUtils.java",
                "changes": 96,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/DirectoryUtils.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/DirectoryUtils.java",
                "patch": "@@ -0,0 +1,96 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.util;\n+\n+import java.io.File;\n+import java.io.FileFilter;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n+\n+import org.apache.nifi.provenance.RepositoryConfiguration;\n+\n+public class DirectoryUtils {\n+\n+    public static final FileFilter EVENT_FILE_FILTER = f -> f.getName().endsWith(\".prov\") || f.getName().endsWith(\".prov.gz\");\n+    public static final FileFilter INDEX_FILE_FILTER = f -> f.getName().startsWith(\"index-\");\n+    public static final Comparator<File> SMALLEST_ID_FIRST = (a, b) -> Long.compare(getMinId(a), getMinId(b));\n+    public static final Comparator<File> LARGEST_ID_FIRST = SMALLEST_ID_FIRST.reversed();\n+    public static final Comparator<File> OLDEST_INDEX_FIRST = (a, b) -> Long.compare(getIndexTimestamp(a), getIndexTimestamp(b));\n+    public static final Comparator<File> NEWEST_INDEX_FIRST = OLDEST_INDEX_FIRST.reversed();\n+\n+    public static List<Path> getProvenanceEventFiles(final RepositoryConfiguration repoConfig) {\n+        return repoConfig.getStorageDirectories().values().stream()\n+            .flatMap(f -> {\n+                final File[] eventFiles = f.listFiles(EVENT_FILE_FILTER);\n+                return eventFiles == null ? Stream.empty() : Arrays.stream(eventFiles);\n+            })\n+            .map(f -> f.toPath())\n+            .collect(Collectors.toList());\n+    }\n+\n+    public static long getMinId(final File file) {\n+        final String filename = file.getName();\n+        final int firstDotIndex = filename.indexOf(\".\");\n+        if (firstDotIndex < 1) {\n+            return -1L;\n+        }\n+\n+        final String firstEventId = filename.substring(0, firstDotIndex);\n+        try {\n+            return Long.parseLong(firstEventId);\n+        } catch (final NumberFormatException nfe) {\n+            return -1L;\n+        }\n+    }\n+\n+    public static long getIndexTimestamp(final File file) {\n+        final String filename = file.getName();\n+        if (!filename.startsWith(\"index-\") && filename.length() > 6) {\n+            return -1L;\n+        }\n+\n+        final String suffix = filename.substring(6);\n+        try {\n+            return Long.parseLong(suffix);\n+        } catch (final NumberFormatException nfe) {\n+            return -1L;\n+        }\n+    }\n+\n+    public static long getSize(final File file) {\n+        if (file.isFile()) {\n+            return file.length();\n+        }\n+\n+        final File[] children = file.listFiles();\n+        if (children == null || children.length == 0) {\n+            return 0L;\n+        }\n+\n+        long total = 0L;\n+        for (final File child : children) {\n+            total += getSize(child);\n+        }\n+\n+        return total;\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/DirectoryUtils.java",
                "sha": "a90500d90dd58b5a20145b9bb8adb7c1df474e0f",
                "status": "added"
            },
            {
                "additions": 79,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/DumpEventFile.java",
                "changes": 79,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/DumpEventFile.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/DumpEventFile.java",
                "patch": "@@ -0,0 +1,79 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.util;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.Date;\n+\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.StandardProvenanceEventRecord;\n+import org.apache.nifi.provenance.serialization.RecordReader;\n+import org.apache.nifi.provenance.serialization.RecordReaders;\n+\n+public class DumpEventFile {\n+\n+    private static void printUsage() {\n+        System.out.println(\"Usage:\");\n+        System.out.println();\n+        System.out.println(\"java \" + DumpEventFile.class.getName() + \" <Event File to Dump>\");\n+        System.out.println();\n+    }\n+\n+    public static void main(final String[] args) throws IOException {\n+        if (args.length != 1) {\n+            printUsage();\n+            return;\n+        }\n+\n+        final File file = new File(args[0]);\n+        if (!file.exists()) {\n+            System.out.println(\"Cannot find file \" + file.getAbsolutePath());\n+            return;\n+        }\n+\n+        try (final RecordReader reader = RecordReaders.newRecordReader(file, Collections.emptyList(), 65535)) {\n+            StandardProvenanceEventRecord event;\n+            int index = 0;\n+            while ((event = reader.nextRecord()) != null) {\n+                final long byteOffset = reader.getBytesConsumed();\n+                final String string = stringify(event, index++, byteOffset);\n+                System.out.println(string);\n+            }\n+        }\n+    }\n+\n+    private static String stringify(final ProvenanceEventRecord event, final int index, final long byteOffset) {\n+        final StringBuilder sb = new StringBuilder();\n+        sb.append(\"Event Index in File = \").append(index).append(\", Byte Offset = \").append(byteOffset);\n+        sb.append(\"\\n\\t\").append(\"Event ID = \").append(event.getEventId());\n+        sb.append(\"\\n\\t\").append(\"Event Type = \").append(event.getEventType());\n+        sb.append(\"\\n\\t\").append(\"Event Time = \").append(new Date(event.getEventTime()));\n+        sb.append(\"\\n\\t\").append(\"Event UUID = \").append(event.getFlowFileUuid());\n+        sb.append(\"\\n\\t\").append(\"Component ID = \").append(event.getComponentId());\n+        sb.append(\"\\n\\t\").append(\"Event ID = \").append(event.getComponentType());\n+        sb.append(\"\\n\\t\").append(\"Transit URI = \").append(event.getTransitUri());\n+        sb.append(\"\\n\\t\").append(\"Parent IDs = \").append(event.getParentUuids());\n+        sb.append(\"\\n\\t\").append(\"Child IDs = \").append(event.getChildUuids());\n+        sb.append(\"\\n\\t\").append(\"Previous Attributes = \").append(event.getPreviousAttributes());\n+        sb.append(\"\\n\\t\").append(\"Updated Attributes = \").append(event.getUpdatedAttributes());\n+\n+        return sb.toString();\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/DumpEventFile.java",
                "sha": "df16356828fef4b84015e915ebf4c0b03039d673",
                "status": "added"
            },
            {
                "additions": 47,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/NamedThreadFactory.java",
                "changes": 47,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/NamedThreadFactory.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/NamedThreadFactory.java",
                "patch": "@@ -0,0 +1,47 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.util;\n+\n+import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadFactory;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+public class NamedThreadFactory implements ThreadFactory {\n+\n+    private final AtomicInteger counter = new AtomicInteger(0);\n+    private final ThreadFactory defaultThreadFactory = Executors.defaultThreadFactory();\n+    private final String namePrefix;\n+    private final boolean daemon;\n+\n+    public NamedThreadFactory(final String namePrefix) {\n+        this(namePrefix, false);\n+    }\n+\n+    public NamedThreadFactory(final String namePrefix, final boolean daemon) {\n+        this.namePrefix = namePrefix;\n+        this.daemon = daemon;\n+    }\n+\n+    @Override\n+    public Thread newThread(final Runnable r) {\n+        final Thread thread = defaultThreadFactory.newThread(r);\n+        thread.setName(namePrefix + \"-\" + counter.incrementAndGet());\n+        thread.setDaemon(daemon);\n+        return thread;\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/NamedThreadFactory.java",
                "sha": "2ee6ed6fa2d4f1bccd33499f0b58e84b9989f32d",
                "status": "added"
            },
            {
                "additions": 185,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/StorageSummaryEvent.java",
                "changes": 185,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/StorageSummaryEvent.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/StorageSummaryEvent.java",
                "patch": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.util;\n+\n+import java.util.List;\n+import java.util.Map;\n+\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.ProvenanceEventType;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n+\n+public class StorageSummaryEvent implements ProvenanceEventRecord {\n+    private final ProvenanceEventRecord event;\n+    private final StorageSummary storageSummary;\n+\n+    public StorageSummaryEvent(final ProvenanceEventRecord event, final StorageSummary storageSummary) {\n+        this.event = event;\n+        this.storageSummary = storageSummary;\n+    }\n+\n+    @Override\n+    public long getEventId() {\n+        return storageSummary.getEventId();\n+    }\n+\n+    @Override\n+    public long getEventTime() {\n+        return event.getEventTime();\n+    }\n+\n+    @Override\n+    public long getFlowFileEntryDate() {\n+        return event.getFlowFileEntryDate();\n+    }\n+\n+    @Override\n+    public long getLineageStartDate() {\n+        return event.getLineageStartDate();\n+    }\n+\n+    @Override\n+    public long getFileSize() {\n+        return event.getFileSize();\n+    }\n+\n+    @Override\n+    public Long getPreviousFileSize() {\n+        return event.getPreviousFileSize();\n+    }\n+\n+    @Override\n+    public long getEventDuration() {\n+        return event.getEventDuration();\n+    }\n+\n+    @Override\n+    public ProvenanceEventType getEventType() {\n+        return event.getEventType();\n+    }\n+\n+    @Override\n+    public Map<String, String> getAttributes() {\n+        return event.getAttributes();\n+    }\n+\n+    @Override\n+    public Map<String, String> getPreviousAttributes() {\n+        return event.getPreviousAttributes();\n+    }\n+\n+    @Override\n+    public Map<String, String> getUpdatedAttributes() {\n+        return event.getUpdatedAttributes();\n+    }\n+\n+    @Override\n+    public String getComponentId() {\n+        return event.getComponentId();\n+    }\n+\n+    @Override\n+    public String getComponentType() {\n+        return event.getComponentType();\n+    }\n+\n+    @Override\n+    public String getTransitUri() {\n+        return event.getTransitUri();\n+    }\n+\n+    @Override\n+    public String getSourceSystemFlowFileIdentifier() {\n+        return event.getSourceSystemFlowFileIdentifier();\n+    }\n+\n+    @Override\n+    public String getFlowFileUuid() {\n+        return event.getFlowFileUuid();\n+    }\n+\n+    @Override\n+    public List<String> getParentUuids() {\n+        return event.getParentUuids();\n+    }\n+\n+    @Override\n+    public List<String> getChildUuids() {\n+        return event.getChildUuids();\n+    }\n+\n+    @Override\n+    public String getAlternateIdentifierUri() {\n+        return event.getAlternateIdentifierUri();\n+    }\n+\n+    @Override\n+    public String getDetails() {\n+        return event.getDetails();\n+    }\n+\n+    @Override\n+    public String getRelationship() {\n+        return event.getRelationship();\n+    }\n+\n+    @Override\n+    public String getSourceQueueIdentifier() {\n+        return event.getSourceQueueIdentifier();\n+    }\n+\n+    @Override\n+    public String getContentClaimSection() {\n+        return event.getContentClaimSection();\n+    }\n+\n+    @Override\n+    public String getPreviousContentClaimSection() {\n+        return event.getPreviousContentClaimSection();\n+    }\n+\n+    @Override\n+    public String getContentClaimContainer() {\n+        return event.getContentClaimContainer();\n+    }\n+\n+    @Override\n+    public String getPreviousContentClaimContainer() {\n+        return event.getPreviousContentClaimContainer();\n+    }\n+\n+    @Override\n+    public String getContentClaimIdentifier() {\n+        return event.getContentClaimIdentifier();\n+    }\n+\n+    @Override\n+    public String getPreviousContentClaimIdentifier() {\n+        return event.getPreviousContentClaimIdentifier();\n+    }\n+\n+    @Override\n+    public Long getContentClaimOffset() {\n+        return event.getContentClaimOffset();\n+    }\n+\n+    @Override\n+    public Long getPreviousContentClaimOffset() {\n+        return event.getPreviousContentClaimOffset();\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/java/org/apache/nifi/provenance/util/StorageSummaryEvent.java",
                "sha": "41d5ade7b6677ef84b60cf0a6d63a39d00c90a22",
                "status": "added"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/resources/META-INF/services/org.apache.nifi.provenance.ProvenanceRepository",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/resources/META-INF/services/org.apache.nifi.provenance.ProvenanceRepository?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 1,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/resources/META-INF/services/org.apache.nifi.provenance.ProvenanceRepository",
                "patch": "@@ -12,4 +12,5 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-org.apache.nifi.provenance.PersistentProvenanceRepository\n\\ No newline at end of file\n+org.apache.nifi.provenance.PersistentProvenanceRepository\n+org.apache.nifi.provenance.WriteAheadProvenanceRepository\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/main/resources/META-INF/services/org.apache.nifi.provenance.ProvenanceRepository",
                "sha": "6a353d2af85b79955ebe765526888c99559e6348",
                "status": "modified"
            },
            {
                "additions": 60,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/AbstractTestRecordReaderWriter.java",
                "changes": 80,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/AbstractTestRecordReaderWriter.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 20,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/AbstractTestRecordReaderWriter.java",
                "patch": "@@ -17,8 +17,8 @@\n \n package org.apache.nifi.provenance;\n \n-import static org.apache.nifi.provenance.TestUtil.createFlowFile;\n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n import static org.junit.Assert.assertNotNull;\n import static org.junit.Assert.assertNull;\n import static org.junit.Assert.assertTrue;\n@@ -27,8 +27,10 @@\n import java.io.FileInputStream;\n import java.io.IOException;\n import java.io.InputStream;\n-import java.util.HashMap;\n+import java.util.ArrayList;\n+import java.util.List;\n import java.util.Map;\n+import java.util.Optional;\n import java.util.UUID;\n \n import org.apache.nifi.provenance.serialization.RecordReader;\n@@ -42,27 +44,15 @@\n import org.junit.BeforeClass;\n import org.junit.Test;\n \n+\n public abstract class AbstractTestRecordReaderWriter {\n     @BeforeClass\n     public static void setLogLevel() {\n         System.setProperty(\"org.slf4j.simpleLogger.log.org.apache.nifi.provenance\", \"INFO\");\n     }\n \n     protected ProvenanceEventRecord createEvent() {\n-        final Map<String, String> attributes = new HashMap<>();\n-        attributes.put(\"filename\", \"1.txt\");\n-        attributes.put(\"uuid\", UUID.randomUUID().toString());\n-\n-        final ProvenanceEventBuilder builder = new StandardProvenanceEventRecord.Builder();\n-        builder.setEventTime(System.currentTimeMillis());\n-        builder.setEventType(ProvenanceEventType.RECEIVE);\n-        builder.setTransitUri(\"nifi://unit-test\");\n-        builder.fromFlowFile(createFlowFile(3L, 3000L, attributes));\n-        builder.setComponentId(\"1234\");\n-        builder.setComponentType(\"dummy processor\");\n-        final ProvenanceEventRecord record = builder.build();\n-\n-        return record;\n+        return TestUtil.createEvent();\n     }\n \n     @Test\n@@ -73,7 +63,7 @@ public void testSimpleWriteWithToc() throws IOException {\n         final RecordWriter writer = createWriter(journalFile, tocWriter, false, 1024 * 1024);\n \n         writer.writeHeader(1L);\n-        writer.writeRecord(createEvent(), 1L);\n+        writer.writeRecord(createEvent());\n         writer.close();\n \n         final TocReader tocReader = new StandardTocReader(tocFile);\n@@ -101,7 +91,7 @@ public void testSingleRecordCompressed() throws IOException {\n         final RecordWriter writer = createWriter(journalFile, tocWriter, true, 8192);\n \n         writer.writeHeader(1L);\n-        writer.writeRecord(createEvent(), 1L);\n+        writer.writeRecord(createEvent());\n         writer.close();\n \n         final TocReader tocReader = new StandardTocReader(tocFile);\n@@ -131,7 +121,7 @@ public void testMultipleRecordsSameBlockCompressed() throws IOException {\n \n         writer.writeHeader(1L);\n         for (int i = 0; i < 10; i++) {\n-            writer.writeRecord(createEvent(), i);\n+            writer.writeRecord(createEvent());\n         }\n         writer.close();\n \n@@ -170,7 +160,7 @@ public void testMultipleRecordsMultipleBlocksCompressed() throws IOException {\n \n         writer.writeHeader(1L);\n         for (int i = 0; i < 10; i++) {\n-            writer.writeRecord(createEvent(), i);\n+            writer.writeRecord(createEvent());\n         }\n         writer.close();\n \n@@ -198,6 +188,56 @@ public void testMultipleRecordsMultipleBlocksCompressed() throws IOException {\n         FileUtils.deleteFile(journalFile.getParentFile(), true);\n     }\n \n+    @Test\n+    public void testSkipToEvent() throws IOException {\n+        final File journalFile = new File(\"target/storage/\" + UUID.randomUUID().toString() + \"/testSimpleWrite.gz\");\n+        final File tocFile = TocUtil.getTocFile(journalFile);\n+        final TocWriter tocWriter = new StandardTocWriter(tocFile, false, false);\n+        // new block each 10 bytes\n+        final RecordWriter writer = createWriter(journalFile, tocWriter, true, 100);\n+\n+        writer.writeHeader(0L);\n+        final int numEvents = 10;\n+        final List<ProvenanceEventRecord> events = new ArrayList<>();\n+        for (int i = 0; i < numEvents; i++) {\n+            final ProvenanceEventRecord event = createEvent();\n+            events.add(event);\n+            writer.writeRecord(event);\n+        }\n+        writer.close();\n+\n+        final TocReader tocReader = new StandardTocReader(tocFile);\n+\n+        try (final FileInputStream fis = new FileInputStream(journalFile);\n+            final RecordReader reader = createReader(fis, journalFile.getName(), tocReader, 2048)) {\n+\n+            for (int i = 0; i < numEvents; i++) {\n+                final Optional<ProvenanceEventRecord> eventOption = reader.skipToEvent(i);\n+                assertTrue(eventOption.isPresent());\n+                assertEquals(i, eventOption.get().getEventId());\n+                assertEquals(events.get(i), eventOption.get());\n+\n+                final StandardProvenanceEventRecord consumedEvent = reader.nextRecord();\n+                assertEquals(eventOption.get(), consumedEvent);\n+            }\n+\n+            assertFalse(reader.skipToEvent(numEvents + 1).isPresent());\n+        }\n+\n+        try (final FileInputStream fis = new FileInputStream(journalFile);\n+            final RecordReader reader = createReader(fis, journalFile.getName(), tocReader, 2048)) {\n+\n+            for (int i = 0; i < 3; i++) {\n+                final Optional<ProvenanceEventRecord> eventOption = reader.skipToEvent(8);\n+                assertTrue(eventOption.isPresent());\n+                assertEquals(events.get(8), eventOption.get());\n+            }\n+\n+            final StandardProvenanceEventRecord consumedEvent = reader.nextRecord();\n+            assertEquals(events.get(8), consumedEvent);\n+        }\n+    }\n+\n     protected abstract RecordWriter createWriter(File file, TocWriter tocWriter, boolean compressed, int uncompressedBlockSize) throws IOException;\n \n     protected abstract RecordReader createReader(InputStream in, String journalFilename, TocReader tocReader, int maxAttributeSize) throws IOException;",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/AbstractTestRecordReaderWriter.java",
                "sha": "36397c4744a5c2ccaa52aef06e9137fd0bd38f65",
                "status": "modified"
            },
            {
                "additions": 477,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestEventIdFirstSchemaRecordReaderWriter.java",
                "changes": 477,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestEventIdFirstSchemaRecordReaderWriter.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestEventIdFirstSchemaRecordReaderWriter.java",
                "patch": "@@ -0,0 +1,477 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n+\n+import java.io.File;\n+import java.io.FileInputStream;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import org.apache.nifi.provenance.serialization.RecordReader;\n+import org.apache.nifi.provenance.serialization.RecordWriter;\n+import org.apache.nifi.provenance.toc.StandardTocReader;\n+import org.apache.nifi.provenance.toc.StandardTocWriter;\n+import org.apache.nifi.provenance.toc.TocReader;\n+import org.apache.nifi.provenance.toc.TocUtil;\n+import org.apache.nifi.provenance.toc.TocWriter;\n+import org.apache.nifi.util.file.FileUtils;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.BeforeClass;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+public class TestEventIdFirstSchemaRecordReaderWriter extends AbstractTestRecordReaderWriter {\n+    private final AtomicLong idGenerator = new AtomicLong(0L);\n+    private File journalFile;\n+    private File tocFile;\n+\n+    @BeforeClass\n+    public static void setupLogger() {\n+        System.setProperty(\"org.slf4j.simpleLogger.log.org.apache.nifi\", \"DEBUG\");\n+    }\n+\n+    @Before\n+    public void setup() {\n+        journalFile = new File(\"target/storage/\" + UUID.randomUUID().toString() + \"/testEventIdFirstSchemaRecordReaderWriter\");\n+        tocFile = TocUtil.getTocFile(journalFile);\n+        idGenerator.set(0L);\n+    };\n+\n+    @Test\n+    public void testContentClaimUnchanged() throws IOException {\n+        final File journalFile = new File(\"target/storage/\" + UUID.randomUUID().toString() + \"/testSimpleWrite.gz\");\n+        final File tocFile = TocUtil.getTocFile(journalFile);\n+        final TocWriter tocWriter = new StandardTocWriter(tocFile, false, false);\n+        final RecordWriter writer = createWriter(journalFile, tocWriter, true, 8192);\n+\n+        final Map<String, String> attributes = new HashMap<>();\n+        attributes.put(\"filename\", \"1.txt\");\n+        attributes.put(\"uuid\", UUID.randomUUID().toString());\n+\n+        final ProvenanceEventBuilder builder = new StandardProvenanceEventRecord.Builder();\n+        builder.setEventTime(System.currentTimeMillis());\n+        builder.setEventType(ProvenanceEventType.RECEIVE);\n+        builder.setTransitUri(\"nifi://unit-test\");\n+        builder.fromFlowFile(TestUtil.createFlowFile(3L, 3000L, attributes));\n+        builder.setComponentId(\"1234\");\n+        builder.setComponentType(\"dummy processor\");\n+        builder.setPreviousContentClaim(\"container-1\", \"section-1\", \"identifier-1\", 1L, 1L);\n+        builder.setCurrentContentClaim(\"container-1\", \"section-1\", \"identifier-1\", 1L, 1L);\n+        final ProvenanceEventRecord record = builder.build();\n+\n+        writer.writeHeader(1L);\n+        writer.writeRecord(record);\n+        writer.close();\n+\n+        final TocReader tocReader = new StandardTocReader(tocFile);\n+\n+        try (final FileInputStream fis = new FileInputStream(journalFile);\n+            final RecordReader reader = createReader(fis, journalFile.getName(), tocReader, 2048)) {\n+            assertEquals(0, reader.getBlockIndex());\n+            reader.skipToBlock(0);\n+            final StandardProvenanceEventRecord recovered = reader.nextRecord();\n+            assertNotNull(recovered);\n+\n+            assertEquals(\"nifi://unit-test\", recovered.getTransitUri());\n+\n+            assertEquals(\"container-1\", recovered.getPreviousContentClaimContainer());\n+            assertEquals(\"container-1\", recovered.getContentClaimContainer());\n+\n+            assertEquals(\"section-1\", recovered.getPreviousContentClaimSection());\n+            assertEquals(\"section-1\", recovered.getContentClaimSection());\n+\n+            assertEquals(\"identifier-1\", recovered.getPreviousContentClaimIdentifier());\n+            assertEquals(\"identifier-1\", recovered.getContentClaimIdentifier());\n+\n+            assertEquals(1L, recovered.getPreviousContentClaimOffset().longValue());\n+            assertEquals(1L, recovered.getContentClaimOffset().longValue());\n+\n+            assertEquals(1L, recovered.getPreviousFileSize().longValue());\n+            assertEquals(1L, recovered.getContentClaimOffset().longValue());\n+\n+            assertNull(reader.nextRecord());\n+        }\n+\n+        FileUtils.deleteFile(journalFile.getParentFile(), true);\n+    }\n+\n+    @Test\n+    public void testContentClaimRemoved() throws IOException {\n+        final File journalFile = new File(\"target/storage/\" + UUID.randomUUID().toString() + \"/testSimpleWrite.gz\");\n+        final File tocFile = TocUtil.getTocFile(journalFile);\n+        final TocWriter tocWriter = new StandardTocWriter(tocFile, false, false);\n+        final RecordWriter writer = createWriter(journalFile, tocWriter, true, 8192);\n+\n+        final Map<String, String> attributes = new HashMap<>();\n+        attributes.put(\"filename\", \"1.txt\");\n+        attributes.put(\"uuid\", UUID.randomUUID().toString());\n+\n+        final ProvenanceEventBuilder builder = new StandardProvenanceEventRecord.Builder();\n+        builder.setEventTime(System.currentTimeMillis());\n+        builder.setEventType(ProvenanceEventType.RECEIVE);\n+        builder.setTransitUri(\"nifi://unit-test\");\n+        builder.fromFlowFile(TestUtil.createFlowFile(3L, 3000L, attributes));\n+        builder.setComponentId(\"1234\");\n+        builder.setComponentType(\"dummy processor\");\n+        builder.setPreviousContentClaim(\"container-1\", \"section-1\", \"identifier-1\", 1L, 1L);\n+        builder.setCurrentContentClaim(null, null, null, 0L, 0L);\n+        final ProvenanceEventRecord record = builder.build();\n+\n+        writer.writeHeader(1L);\n+        writer.writeRecord(record);\n+        writer.close();\n+\n+        final TocReader tocReader = new StandardTocReader(tocFile);\n+\n+        try (final FileInputStream fis = new FileInputStream(journalFile);\n+            final RecordReader reader = createReader(fis, journalFile.getName(), tocReader, 2048)) {\n+            assertEquals(0, reader.getBlockIndex());\n+            reader.skipToBlock(0);\n+            final StandardProvenanceEventRecord recovered = reader.nextRecord();\n+            assertNotNull(recovered);\n+\n+            assertEquals(\"nifi://unit-test\", recovered.getTransitUri());\n+\n+            assertEquals(\"container-1\", recovered.getPreviousContentClaimContainer());\n+            assertNull(recovered.getContentClaimContainer());\n+\n+            assertEquals(\"section-1\", recovered.getPreviousContentClaimSection());\n+            assertNull(recovered.getContentClaimSection());\n+\n+            assertEquals(\"identifier-1\", recovered.getPreviousContentClaimIdentifier());\n+            assertNull(recovered.getContentClaimIdentifier());\n+\n+            assertEquals(1L, recovered.getPreviousContentClaimOffset().longValue());\n+            assertNull(recovered.getContentClaimOffset());\n+\n+            assertEquals(1L, recovered.getPreviousFileSize().longValue());\n+            assertEquals(0L, recovered.getFileSize());\n+\n+            assertNull(reader.nextRecord());\n+        }\n+\n+        FileUtils.deleteFile(journalFile.getParentFile(), true);\n+    }\n+\n+    @Test\n+    public void testContentClaimAdded() throws IOException {\n+        final File journalFile = new File(\"target/storage/\" + UUID.randomUUID().toString() + \"/testSimpleWrite.gz\");\n+        final File tocFile = TocUtil.getTocFile(journalFile);\n+        final TocWriter tocWriter = new StandardTocWriter(tocFile, false, false);\n+        final RecordWriter writer = createWriter(journalFile, tocWriter, true, 8192);\n+\n+        final Map<String, String> attributes = new HashMap<>();\n+        attributes.put(\"filename\", \"1.txt\");\n+        attributes.put(\"uuid\", UUID.randomUUID().toString());\n+\n+        final ProvenanceEventBuilder builder = new StandardProvenanceEventRecord.Builder();\n+        builder.setEventTime(System.currentTimeMillis());\n+        builder.setEventType(ProvenanceEventType.RECEIVE);\n+        builder.setTransitUri(\"nifi://unit-test\");\n+        builder.fromFlowFile(TestUtil.createFlowFile(3L, 3000L, attributes));\n+        builder.setComponentId(\"1234\");\n+        builder.setComponentType(\"dummy processor\");\n+        builder.setCurrentContentClaim(\"container-1\", \"section-1\", \"identifier-1\", 1L, 1L);\n+        final ProvenanceEventRecord record = builder.build();\n+\n+        writer.writeHeader(1L);\n+        writer.writeRecord(record);\n+        writer.close();\n+\n+        final TocReader tocReader = new StandardTocReader(tocFile);\n+\n+        try (final FileInputStream fis = new FileInputStream(journalFile);\n+            final RecordReader reader = createReader(fis, journalFile.getName(), tocReader, 2048)) {\n+            assertEquals(0, reader.getBlockIndex());\n+            reader.skipToBlock(0);\n+            final StandardProvenanceEventRecord recovered = reader.nextRecord();\n+            assertNotNull(recovered);\n+\n+            assertEquals(\"nifi://unit-test\", recovered.getTransitUri());\n+\n+            assertEquals(\"container-1\", recovered.getContentClaimContainer());\n+            assertNull(recovered.getPreviousContentClaimContainer());\n+\n+            assertEquals(\"section-1\", recovered.getContentClaimSection());\n+            assertNull(recovered.getPreviousContentClaimSection());\n+\n+            assertEquals(\"identifier-1\", recovered.getContentClaimIdentifier());\n+            assertNull(recovered.getPreviousContentClaimIdentifier());\n+\n+            assertEquals(1L, recovered.getContentClaimOffset().longValue());\n+            assertNull(recovered.getPreviousContentClaimOffset());\n+\n+            assertEquals(1L, recovered.getFileSize());\n+            assertNull(recovered.getPreviousContentClaimOffset());\n+\n+            assertNull(reader.nextRecord());\n+        }\n+\n+        FileUtils.deleteFile(journalFile.getParentFile(), true);\n+    }\n+\n+    @Test\n+    public void testContentClaimChanged() throws IOException {\n+        final File journalFile = new File(\"target/storage/\" + UUID.randomUUID().toString() + \"/testSimpleWrite.gz\");\n+        final File tocFile = TocUtil.getTocFile(journalFile);\n+        final TocWriter tocWriter = new StandardTocWriter(tocFile, false, false);\n+        final RecordWriter writer = createWriter(journalFile, tocWriter, true, 8192);\n+\n+        final Map<String, String> attributes = new HashMap<>();\n+        attributes.put(\"filename\", \"1.txt\");\n+        attributes.put(\"uuid\", UUID.randomUUID().toString());\n+\n+        final ProvenanceEventBuilder builder = new StandardProvenanceEventRecord.Builder();\n+        builder.setEventTime(System.currentTimeMillis());\n+        builder.setEventType(ProvenanceEventType.RECEIVE);\n+        builder.setTransitUri(\"nifi://unit-test\");\n+        builder.fromFlowFile(TestUtil.createFlowFile(3L, 3000L, attributes));\n+        builder.setComponentId(\"1234\");\n+        builder.setComponentType(\"dummy processor\");\n+        builder.setPreviousContentClaim(\"container-1\", \"section-1\", \"identifier-1\", 1L, 1L);\n+        builder.setCurrentContentClaim(\"container-2\", \"section-2\", \"identifier-2\", 2L, 2L);\n+        final ProvenanceEventRecord record = builder.build();\n+\n+        writer.writeHeader(1L);\n+        writer.writeRecord(record);\n+        writer.close();\n+\n+        final TocReader tocReader = new StandardTocReader(tocFile);\n+\n+        try (final FileInputStream fis = new FileInputStream(journalFile);\n+            final RecordReader reader = createReader(fis, journalFile.getName(), tocReader, 2048)) {\n+            assertEquals(0, reader.getBlockIndex());\n+            reader.skipToBlock(0);\n+            final StandardProvenanceEventRecord recovered = reader.nextRecord();\n+            assertNotNull(recovered);\n+\n+            assertEquals(\"nifi://unit-test\", recovered.getTransitUri());\n+\n+            assertEquals(\"container-1\", recovered.getPreviousContentClaimContainer());\n+            assertEquals(\"container-2\", recovered.getContentClaimContainer());\n+\n+            assertEquals(\"section-1\", recovered.getPreviousContentClaimSection());\n+            assertEquals(\"section-2\", recovered.getContentClaimSection());\n+\n+            assertEquals(\"identifier-1\", recovered.getPreviousContentClaimIdentifier());\n+            assertEquals(\"identifier-2\", recovered.getContentClaimIdentifier());\n+\n+            assertEquals(1L, recovered.getPreviousContentClaimOffset().longValue());\n+            assertEquals(2L, recovered.getContentClaimOffset().longValue());\n+\n+            assertEquals(1L, recovered.getPreviousFileSize().longValue());\n+            assertEquals(2L, recovered.getContentClaimOffset().longValue());\n+\n+            assertNull(reader.nextRecord());\n+        }\n+\n+        FileUtils.deleteFile(journalFile.getParentFile(), true);\n+    }\n+\n+    @Test\n+    public void testEventIdAndTimestampCorrect() throws IOException {\n+        final File journalFile = new File(\"target/storage/\" + UUID.randomUUID().toString() + \"/testSimpleWrite.gz\");\n+        final File tocFile = TocUtil.getTocFile(journalFile);\n+        final TocWriter tocWriter = new StandardTocWriter(tocFile, false, false);\n+        final RecordWriter writer = createWriter(journalFile, tocWriter, true, 8192);\n+\n+        final Map<String, String> attributes = new HashMap<>();\n+        attributes.put(\"filename\", \"1.txt\");\n+        attributes.put(\"uuid\", UUID.randomUUID().toString());\n+\n+        final long timestamp = System.currentTimeMillis() - 10000L;\n+\n+        final StandardProvenanceEventRecord.Builder builder = new StandardProvenanceEventRecord.Builder();\n+        builder.setEventId(1_000_000);\n+        builder.setEventTime(timestamp);\n+        builder.setEventType(ProvenanceEventType.RECEIVE);\n+        builder.setTransitUri(\"nifi://unit-test\");\n+        builder.fromFlowFile(TestUtil.createFlowFile(3L, 3000L, attributes));\n+        builder.setComponentId(\"1234\");\n+        builder.setComponentType(\"dummy processor\");\n+        builder.setPreviousContentClaim(\"container-1\", \"section-1\", \"identifier-1\", 1L, 1L);\n+        builder.setCurrentContentClaim(\"container-2\", \"section-2\", \"identifier-2\", 2L, 2L);\n+        final ProvenanceEventRecord record = builder.build();\n+\n+        writer.writeHeader(500_000L);\n+        writer.writeRecord(record);\n+        writer.close();\n+\n+        final TocReader tocReader = new StandardTocReader(tocFile);\n+\n+        try (final FileInputStream fis = new FileInputStream(journalFile);\n+            final RecordReader reader = createReader(fis, journalFile.getName(), tocReader, 2048)) {\n+\n+            final ProvenanceEventRecord event = reader.nextRecord();\n+            assertNotNull(event);\n+            assertEquals(1_000_000L, event.getEventId());\n+            assertEquals(timestamp, event.getEventTime());\n+            assertNull(reader.nextRecord());\n+        }\n+\n+        FileUtils.deleteFile(journalFile.getParentFile(), true);\n+    }\n+\n+\n+    @Test\n+    public void testComponentIdInlineAndLookup() throws IOException {\n+        final File journalFile = new File(\"target/storage/\" + UUID.randomUUID().toString() + \"/testSimpleWrite.prov\");\n+        final File tocFile = TocUtil.getTocFile(journalFile);\n+        final TocWriter tocWriter = new StandardTocWriter(tocFile, false, false);\n+\n+        final IdentifierLookup lookup = new IdentifierLookup() {\n+            @Override\n+            public List<String> getQueueIdentifiers() {\n+                return Collections.emptyList();\n+            }\n+\n+            @Override\n+            public List<String> getComponentTypes() {\n+                return Collections.singletonList(\"unit-test-component-1\");\n+            }\n+\n+            @Override\n+            public List<String> getComponentIdentifiers() {\n+                return Collections.singletonList(\"1234\");\n+            }\n+        };\n+\n+        final RecordWriter writer = new EventIdFirstSchemaRecordWriter(journalFile, idGenerator, tocWriter, false, 1024 * 32, lookup);\n+\n+        final Map<String, String> attributes = new HashMap<>();\n+        attributes.put(\"filename\", \"1.txt\");\n+        attributes.put(\"uuid\", UUID.randomUUID().toString());\n+\n+        final StandardProvenanceEventRecord.Builder builder = new StandardProvenanceEventRecord.Builder();\n+        builder.setEventId(1_000_000);\n+        builder.setEventTime(System.currentTimeMillis());\n+        builder.setEventType(ProvenanceEventType.RECEIVE);\n+        builder.setTransitUri(\"nifi://unit-test\");\n+        builder.fromFlowFile(TestUtil.createFlowFile(3L, 3000L, attributes));\n+        builder.setComponentId(\"1234\");\n+        builder.setComponentType(\"unit-test-component-2\");\n+        builder.setPreviousContentClaim(\"container-1\", \"section-1\", \"identifier-1\", 1L, 1L);\n+        builder.setCurrentContentClaim(\"container-2\", \"section-2\", \"identifier-2\", 2L, 2L);\n+\n+        writer.writeHeader(500_000L);\n+        writer.writeRecord(builder.build());\n+\n+        builder.setEventId(1_000_001L);\n+        builder.setComponentId(\"4444\");\n+        builder.setComponentType(\"unit-test-component-1\");\n+        writer.writeRecord(builder.build());\n+\n+        writer.close();\n+\n+        final TocReader tocReader = new StandardTocReader(tocFile);\n+\n+        try (final FileInputStream fis = new FileInputStream(journalFile);\n+            final RecordReader reader = createReader(fis, journalFile.getName(), tocReader, 2048)) {\n+\n+            ProvenanceEventRecord event = reader.nextRecord();\n+            assertNotNull(event);\n+            assertEquals(1_000_000L, event.getEventId());\n+            assertEquals(\"1234\", event.getComponentId());\n+            assertEquals(\"unit-test-component-2\", event.getComponentType());\n+\n+            event = reader.nextRecord();\n+            assertNotNull(event);\n+            assertEquals(1_000_001L, event.getEventId());\n+            assertEquals(\"4444\", event.getComponentId());\n+            assertEquals(\"unit-test-component-1\", event.getComponentType());\n+\n+            assertNull(reader.nextRecord());\n+        }\n+\n+        FileUtils.deleteFile(journalFile.getParentFile(), true);\n+    }\n+\n+    @Override\n+    protected RecordWriter createWriter(final File file, final TocWriter tocWriter, final boolean compressed, final int uncompressedBlockSize) throws IOException {\n+        return new EventIdFirstSchemaRecordWriter(file, idGenerator, tocWriter, compressed, uncompressedBlockSize, IdentifierLookup.EMPTY);\n+    }\n+\n+    @Override\n+    protected RecordReader createReader(final InputStream in, final String journalFilename, final TocReader tocReader, final int maxAttributeSize) throws IOException {\n+        return new EventIdFirstSchemaRecordReader(in, journalFilename, tocReader, maxAttributeSize);\n+    }\n+\n+    @Test\n+    @Ignore\n+    public void testPerformanceOfRandomAccessReads() throws Exception {\n+        journalFile = new File(\"target/storage/\" + UUID.randomUUID().toString() + \"/testPerformanceOfRandomAccessReads.gz\");\n+        tocFile = TocUtil.getTocFile(journalFile);\n+\n+        final int blockSize = 1024 * 32;\n+        try (final RecordWriter writer = createWriter(journalFile, new StandardTocWriter(tocFile, true, false), true, blockSize)) {\n+            writer.writeHeader(0L);\n+\n+            for (int i = 0; i < 100_000; i++) {\n+                writer.writeRecord(createEvent());\n+            }\n+        }\n+\n+        final long[] eventIds = new long[] {\n+            4, 80, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 40_000, 80_000, 99_000\n+        };\n+\n+        boolean loopForever = true;\n+        while (loopForever) {\n+            final long start = System.nanoTime();\n+            for (int i = 0; i < 1000; i++) {\n+                try (final InputStream in = new FileInputStream(journalFile);\n+                    final RecordReader reader = createReader(in, journalFile.getName(), new StandardTocReader(tocFile), 32 * 1024)) {\n+\n+                    for (final long id : eventIds) {\n+                        time(() -> {\n+                            reader.skipToEvent(id);\n+                            return reader.nextRecord();\n+                        }, id);\n+                    }\n+                }\n+            }\n+\n+            final long ms = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);\n+            System.out.println(ms + \" ms total\");\n+        }\n+    }\n+\n+    private void time(final Callable<StandardProvenanceEventRecord> task, final long id) throws Exception {\n+        final long start = System.nanoTime();\n+        final StandardProvenanceEventRecord event = task.call();\n+        Assert.assertNotNull(event);\n+        Assert.assertEquals(id, event.getEventId());\n+        //        System.out.println(event);\n+        final long nanos = System.nanoTime() - start;\n+        final long millis = TimeUnit.NANOSECONDS.toMillis(nanos);\n+        //        System.out.println(\"Took \" + millis + \" ms to \" + taskDescription);\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestEventIdFirstSchemaRecordReaderWriter.java",
                "sha": "9c89ab30e2131402c7a9ec75889125c7d542ceee",
                "status": "added"
            },
            {
                "additions": 105,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestPersistentProvenanceRepository.java",
                "changes": 193,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestPersistentProvenanceRepository.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 88,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestPersistentProvenanceRepository.java",
                "patch": "@@ -23,6 +23,7 @@\n import static org.junit.Assert.assertTrue;\n import static org.mockito.Mockito.mock;\n \n+import java.io.DataOutputStream;\n import java.io.File;\n import java.io.FileFilter;\n import java.io.FileInputStream;\n@@ -65,6 +66,8 @@\n import org.apache.nifi.authorization.user.NiFiUser;\n import org.apache.nifi.events.EventReporter;\n import org.apache.nifi.flowfile.FlowFile;\n+import org.apache.nifi.provenance.index.EventIndexSearcher;\n+import org.apache.nifi.provenance.index.EventIndexWriter;\n import org.apache.nifi.provenance.lineage.EventNode;\n import org.apache.nifi.provenance.lineage.Lineage;\n import org.apache.nifi.provenance.lineage.LineageEdge;\n@@ -83,7 +86,6 @@\n import org.apache.nifi.provenance.serialization.RecordWriter;\n import org.apache.nifi.provenance.serialization.RecordWriters;\n import org.apache.nifi.reporting.Severity;\n-import org.apache.nifi.stream.io.DataOutputStream;\n import org.apache.nifi.util.NiFiProperties;\n import org.apache.nifi.util.file.FileUtils;\n import org.junit.After;\n@@ -120,7 +122,7 @@\n \n     private static RepositoryConfiguration createConfiguration() {\n         config = new RepositoryConfiguration();\n-        config.addStorageDirectory(new File(\"target/storage/\" + UUID.randomUUID().toString()));\n+        config.addStorageDirectory(\"1\", new File(\"target/storage/\" + UUID.randomUUID().toString()));\n         config.setCompressOnRollover(true);\n         config.setMaxEventFileLife(2000L, TimeUnit.SECONDS);\n         config.setCompressionBlockBytes(100);\n@@ -152,14 +154,15 @@ public static void findJournalSizes() throws IOException {\n         final File tempRecordFile = tempFolder.newFile(\"record.tmp\");\n         System.out.println(\"findJournalSizes position 0 = \" + tempRecordFile.length());\n \n-        final RecordWriter writer = RecordWriters.newSchemaRecordWriter(tempRecordFile, false, false);\n+        final AtomicLong idGenerator = new AtomicLong(0L);\n+        final RecordWriter writer = RecordWriters.newSchemaRecordWriter(tempRecordFile, idGenerator, false, false);\n         writer.writeHeader(12345L);\n         writer.flush();\n         headerSize = Long.valueOf(tempRecordFile.length()).intValue();\n-        writer.writeRecord(record, 12345L);\n+        writer.writeRecord(record);\n         writer.flush();\n         recordSize = Long.valueOf(tempRecordFile.length()).intValue() - headerSize;\n-        writer.writeRecord(record2, 23456L);\n+        writer.writeRecord(record2);\n         writer.flush();\n         recordSize2 = Long.valueOf(tempRecordFile.length()).intValue() - headerSize - recordSize;\n         writer.close();\n@@ -187,34 +190,45 @@ public void reportEvent(Severity severity, String category, String message) {\n \n     @After\n     public void closeRepo() throws IOException {\n-        if (repo != null) {\n-            try {\n-                repo.close();\n-            } catch (final IOException ioe) {\n-            }\n+        if (repo == null) {\n+            return;\n         }\n \n+        try {\n+            repo.close();\n+        } catch (final IOException ioe) {\n+        }\n+\n+        // Delete all of the storage files. We do this in order to clean up the tons of files that\n+        // we create but also to ensure that we have closed all of the file handles. If we leave any\n+        // streams open, for instance, this will throw an IOException, causing our unit test to fail.\n         if (config != null) {\n-            // Delete all of the storage files. We do this in order to clean up the tons of files that\n-            // we create but also to ensure that we have closed all of the file handles. If we leave any\n-            // streams open, for instance, this will throw an IOException, causing our unit test to fail.\n-            for (final File storageDir : config.getStorageDirectories()) {\n-                if (storageDir.exists()) {\n-                    int i;\n-                    for (i = 0; i < 3; i++) {\n-                        try {\n-                            System.out.println(\"file: \" + storageDir.toString() + \" exists=\" + storageDir.exists());\n-                            FileUtils.deleteFile(storageDir, true);\n-                            break;\n-                        } catch (final IOException ioe) {\n-                            // if there is a virus scanner, etc. running in the background we may not be able to\n-                            // delete the file. Wait a sec and try again.\n-                            if (i == 2) {\n-                                throw ioe;\n-                            } else {\n-                                try {\n-                                    Thread.sleep(1000L);\n-                                } catch (final InterruptedException ie) {\n+            for (final File storageDir : config.getStorageDirectories().values()) {\n+                int i;\n+                for (i = 0; i < 3; i++) {\n+                    try {\n+                        FileUtils.deleteFile(storageDir, true);\n+                        break;\n+                    } catch (final IOException ioe) {\n+                        // if there is a virus scanner, etc. running in the background we may not be able to\n+                        // delete the file. Wait a sec and try again.\n+                        if (i == 2) {\n+                            throw ioe;\n+                        } else {\n+                            try {\n+                                System.out.println(\"file: \" + storageDir.toString() + \" exists=\" + storageDir.exists());\n+                                FileUtils.deleteFile(storageDir, true);\n+                                break;\n+                            } catch (final IOException ioe2) {\n+                                // if there is a virus scanner, etc. running in the background we may not be able to\n+                                // delete the file. Wait a sec and try again.\n+                                if (i == 2) {\n+                                    throw ioe2;\n+                                } else {\n+                                    try {\n+                                        Thread.sleep(1000L);\n+                                    } catch (final InterruptedException ie) {\n+                                    }\n                                 }\n                             }\n                         }\n@@ -240,7 +254,7 @@ public void testPerformance() throws IOException, InterruptedException {\n         config.setJournalCount(10);\n         config.setQueryThreadPoolSize(10);\n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final Map<String, String> attributes = new HashMap<>();\n         attributes.put(\"abc\", \"xyz\");\n@@ -288,7 +302,7 @@ public void run() {\n         System.out.println(\"Closing and re-initializing\");\n         repo.close();\n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n         System.out.println(\"Re-initialized\");\n \n         final long fetchStart = System.nanoTime();\n@@ -311,7 +325,7 @@ public String getProperty(String key) {\n                 return \"2000 millis\";\n             } else if (key.equals(NiFiProperties.PROVENANCE_REPO_DIRECTORY_PREFIX + \".default\")) {\n                 createConfiguration();\n-                return config.getStorageDirectories().get(0).getAbsolutePath();\n+                return config.getStorageDirectories().values().iterator().next().getAbsolutePath();\n             } else {\n                 return null;\n             }\n@@ -340,8 +354,8 @@ public void constructorNiFiProperties() throws IOException {\n \n     @Test\n     public void constructorConfig() throws IOException {\n-        RepositoryConfiguration configuration = createTestableRepositoryConfiguration(properties);\n-        TestablePersistentProvenanceRepository tppr = new TestablePersistentProvenanceRepository(configuration, 20000);\n+        RepositoryConfiguration configuration = RepositoryConfiguration.create(properties);\n+        new TestablePersistentProvenanceRepository(configuration, 20000);\n     }\n \n     @Test\n@@ -350,7 +364,7 @@ public void testAddAndRecover() throws IOException, InterruptedException {\n         config.setMaxEventFileCapacity(1L);\n         config.setMaxEventFileLife(1, TimeUnit.SECONDS);\n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final Map<String, String> attributes = new HashMap<>();\n         attributes.put(\"abc\", \"xyz\");\n@@ -376,7 +390,7 @@ public void testAddAndRecover() throws IOException, InterruptedException {\n         Thread.sleep(500L); // Give the repo time to shutdown (i.e., close all file handles, etc.)\n \n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n         final List<ProvenanceEventRecord> recoveredRecords = repo.getEvents(0L, 12);\n \n         assertEquals(10, recoveredRecords.size());\n@@ -399,7 +413,7 @@ public void testAddToMultipleLogsAndRecover() throws IOException, InterruptedExc\n         config.setMaxEventFileLife(2, TimeUnit.SECONDS);\n         config.setSearchableFields(searchableFields);\n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final Map<String, String> attributes = new HashMap<>();\n         attributes.put(\"abc\", \"xyz\");\n@@ -454,7 +468,7 @@ public void testIndexOnRolloverWithImmenseAttribute() throws IOException {\n         config.setSearchableFields(new ArrayList<>(SearchableFields.getStandardFields()));\n         config.setSearchableAttributes(SearchableFieldParser.extractSearchableFields(\"immense\", false));\n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         int immenseAttrSize = 33000; // must be greater than 32766 for a meaningful test\n         StringBuilder immenseBldr = new StringBuilder(immenseAttrSize);\n@@ -498,7 +512,7 @@ public void testIndexOnRolloverAndSubsequentSearch() throws IOException, Interru\n         config.setMaxEventFileLife(500, TimeUnit.MILLISECONDS);\n         config.setSearchableFields(new ArrayList<>(SearchableFields.getStandardFields()));\n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final String uuid = \"00000000-0000-0000-0000-000000000000\";\n         final Map<String, String> attributes = new HashMap<>();\n@@ -542,7 +556,7 @@ public void testCompressOnRollover() throws IOException, InterruptedException, P\n         config.setMaxEventFileLife(500, TimeUnit.MILLISECONDS);\n         config.setCompressOnRollover(true);\n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final String uuid = \"00000000-0000-0000-0000-000000000000\";\n         final Map<String, String> attributes = new HashMap<>();\n@@ -565,7 +579,7 @@ public void testCompressOnRollover() throws IOException, InterruptedException, P\n         }\n \n         repo.waitForRollover();\n-        final File storageDir = config.getStorageDirectories().get(0);\n+        final File storageDir = config.getStorageDirectories().values().iterator().next();\n         final File compressedLogFile = new File(storageDir, \"0.prov.gz\");\n         assertTrue(compressedLogFile.exists());\n     }\n@@ -580,7 +594,7 @@ public void testIndexAndCompressOnRolloverAndSubsequentSearch() throws IOExcepti\n         config.setSearchableFields(new ArrayList<>(SearchableFields.getStandardFields()));\n \n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final String uuid = \"10000000-0000-0000-0000-000000000000\";\n         final Map<String, String> attributes = new HashMap<>();\n@@ -653,8 +667,8 @@ protected synchronized CachingIndexManager getIndexManager() {\n                         final AtomicInteger indexSearcherCount = new AtomicInteger(0);\n \n                         @Override\n-                        public IndexSearcher borrowIndexSearcher(File indexDir) throws IOException {\n-                            final IndexSearcher searcher = mgr.borrowIndexSearcher(indexDir);\n+                        public EventIndexSearcher borrowIndexSearcher(File indexDir) throws IOException {\n+                            final EventIndexSearcher searcher = mgr.borrowIndexSearcher(indexDir);\n                             final int idx = indexSearcherCount.incrementAndGet();\n                             obtainIndexSearcherLatch.countDown();\n \n@@ -677,7 +691,7 @@ public IndexSearcher borrowIndexSearcher(File indexDir) throws IOException {\n                         }\n \n                         @Override\n-                        public IndexWriter borrowIndexWriter(File indexingDirectory) throws IOException {\n+                        public EventIndexWriter borrowIndexWriter(File indexingDirectory) throws IOException {\n                             return mgr.borrowIndexWriter(indexingDirectory);\n                         }\n \n@@ -687,18 +701,19 @@ public void close() throws IOException {\n                         }\n \n                         @Override\n-                        public void removeIndex(File indexDirectory) {\n+                        public boolean removeIndex(File indexDirectory) {\n                             mgr.removeIndex(indexDirectory);\n+                            return true;\n                         }\n \n                         @Override\n-                        public void returnIndexSearcher(File indexDirectory, IndexSearcher searcher) {\n-                            mgr.returnIndexSearcher(indexDirectory, searcher);\n+                        public void returnIndexSearcher(EventIndexSearcher searcher) {\n+                            mgr.returnIndexSearcher(searcher);\n                         }\n \n                         @Override\n-                        public void returnIndexWriter(File indexingDirectory, IndexWriter writer) {\n-                            mgr.returnIndexWriter(indexingDirectory, writer);\n+                        public void returnIndexWriter(EventIndexWriter writer) {\n+                            mgr.returnIndexWriter(writer);\n                         }\n                     };\n                 }\n@@ -707,7 +722,7 @@ public void returnIndexWriter(File indexingDirectory, IndexWriter writer) {\n             }\n         };\n \n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final String uuid = \"10000000-0000-0000-0000-000000000000\";\n         final Map<String, String> attributes = new HashMap<>();\n@@ -790,15 +805,15 @@ public void run() {\n     @Test\n     public void testIndexAndCompressOnRolloverAndSubsequentSearchMultipleStorageDirs() throws IOException, InterruptedException, ParseException {\n         final RepositoryConfiguration config = createConfiguration();\n-        config.addStorageDirectory(new File(\"target/storage/\" + UUID.randomUUID().toString()));\n+        config.addStorageDirectory(\"2\", new File(\"target/storage/\" + UUID.randomUUID().toString()));\n         config.setMaxRecordLife(30, TimeUnit.SECONDS);\n         config.setMaxStorageCapacity(1024L * 1024L);\n         config.setMaxEventFileLife(1, TimeUnit.SECONDS);\n         config.setMaxEventFileCapacity(1024L * 1024L);\n         config.setSearchableFields(new ArrayList<>(SearchableFields.getStandardFields()));\n \n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final String uuid = \"00000000-0000-0000-0000-000000000000\";\n         final Map<String, String> attributes = new HashMap<>();\n@@ -885,7 +900,7 @@ public void testIndexAndCompressOnRolloverAndSubsequentEmptySearch() throws IOEx\n         config.setSearchableFields(new ArrayList<>(SearchableFields.getStandardFields()));\n \n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final String uuid = \"00000000-0000-0000-0000-000000000000\";\n         final Map<String, String> attributes = new HashMap<>();\n@@ -941,7 +956,7 @@ public void testLineageReceiveDrop() throws IOException, InterruptedException, P\n         config.setSearchableFields(new ArrayList<>(SearchableFields.getStandardFields()));\n \n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final String uuid = \"00000000-0000-0000-0000-000000000001\";\n         final Map<String, String> attributes = new HashMap<>();\n@@ -996,7 +1011,7 @@ public void testLineageReceiveDropAsync() throws IOException, InterruptedExcepti\n         config.setSearchableFields(new ArrayList<>(SearchableFields.getStandardFields()));\n \n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final String uuid = \"00000000-0000-0000-0000-000000000001\";\n         final Map<String, String> attributes = new HashMap<>();\n@@ -1055,7 +1070,7 @@ public void testLineageManyToOneSpawn() throws IOException, InterruptedException\n         config.setSearchableFields(new ArrayList<>(SearchableFields.getStandardFields()));\n \n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final String childId = \"00000000-0000-0000-0000-000000000000\";\n \n@@ -1105,7 +1120,7 @@ public void testLineageManyToOneSpawnAsync() throws IOException, InterruptedExce\n         config.setSearchableFields(new ArrayList<>(SearchableFields.getStandardFields()));\n \n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final String childId = \"00000000-0000-0000-0000-000000000000\";\n \n@@ -1152,7 +1167,7 @@ public void testCorrectProvenanceEventIdOnRestore() throws IOException {\n         final RepositoryConfiguration config = createConfiguration();\n         config.setMaxEventFileLife(1, TimeUnit.SECONDS);\n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final String uuid = \"00000000-0000-0000-0000-000000000000\";\n         final Map<String, String> attributes = new HashMap<>();\n@@ -1178,7 +1193,7 @@ public void testCorrectProvenanceEventIdOnRestore() throws IOException {\n         repo.close();\n \n         final PersistentProvenanceRepository secondRepo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        secondRepo.initialize(getEventReporter(), null, null);\n+        secondRepo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         try {\n             final ProvenanceEventRecord event11 = builder.build();\n@@ -1239,7 +1254,7 @@ private File prepCorruptedEventFileTests() throws Exception {\n         config.setDesiredIndexSize(10);\n \n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         String uuid = UUID.randomUUID().toString();\n         for (int i = 0; i < 20; i++) {\n@@ -1253,7 +1268,7 @@ private File prepCorruptedEventFileTests() throws Exception {\n             }\n         }\n         repo.waitForRollover();\n-        File eventFile = new File(config.getStorageDirectories().get(0), \"10.prov.gz\");\n+        File eventFile = new File(config.getStorageDirectories().values().iterator().next(), \"10.prov.gz\");\n         assertTrue(eventFile.delete());\n         return eventFile;\n     }\n@@ -1270,7 +1285,7 @@ public void testIndexDirectoryRemoved() throws InterruptedException, IOException\n         config.setDesiredIndexSize(10); // force new index to be created for each rollover\n \n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final String uuid = \"00000000-0000-0000-0000-000000000000\";\n         final Map<String, String> attributes = new HashMap<>();\n@@ -1298,7 +1313,7 @@ public void testIndexDirectoryRemoved() throws InterruptedException, IOException\n         Thread.sleep(2000L);\n \n         final FileFilter indexFileFilter = file -> file.getName().startsWith(\"index\");\n-        final int numIndexDirs = config.getStorageDirectories().get(0).listFiles(indexFileFilter).length;\n+        final int numIndexDirs = config.getStorageDirectories().values().iterator().next().listFiles(indexFileFilter).length;\n         assertEquals(1, numIndexDirs);\n \n         // add more records so that we will create a new index\n@@ -1324,7 +1339,7 @@ public void testIndexDirectoryRemoved() throws InterruptedException, IOException\n         assertEquals(20, result.getMatchingEvents().size());\n \n         // Ensure index directories exists\n-        File[] indexDirs = config.getStorageDirectories().get(0).listFiles(indexFileFilter);\n+        File[] indexDirs = config.getStorageDirectories().values().iterator().next().listFiles(indexFileFilter);\n         assertEquals(2, indexDirs.length);\n \n         // expire old events and indexes\n@@ -1337,7 +1352,7 @@ public void testIndexDirectoryRemoved() throws InterruptedException, IOException\n         assertEquals(10, newRecordSet.getMatchingEvents().size());\n \n         // Ensure that one index directory is gone\n-        indexDirs = config.getStorageDirectories().get(0).listFiles(indexFileFilter);\n+        indexDirs = config.getStorageDirectories().values().iterator().next().listFiles(indexFileFilter);\n         assertEquals(1, indexDirs.length);\n     }\n \n@@ -1354,12 +1369,12 @@ public void testNotAuthorizedGetSpecificEvent() throws IOException {\n         final AccessDeniedException expectedException = new AccessDeniedException(\"Unit Test - Intentionally Thrown\");\n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS) {\n             @Override\n-            protected void authorize(ProvenanceEventRecord event, NiFiUser user) {\n+            public void authorize(ProvenanceEventRecord event, NiFiUser user) {\n                 throw expectedException;\n             }\n         };\n \n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final String uuid = \"00000000-0000-0000-0000-000000000000\";\n         final Map<String, String> attributes = new HashMap<>();\n@@ -1409,7 +1424,7 @@ public boolean isAuthorized(ProvenanceEventRecord event, NiFiUser user) {\n             }\n         };\n \n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final String uuid = \"00000000-0000-0000-0000-000000000000\";\n         final Map<String, String> attributes = new HashMap<>();\n@@ -1461,7 +1476,7 @@ public boolean isAuthorized(ProvenanceEventRecord event, NiFiUser user) {\n             }\n         };\n \n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final String uuid = \"00000000-0000-0000-0000-000000000000\";\n         final Map<String, String> attributes = new HashMap<>();\n@@ -1522,7 +1537,7 @@ public boolean isAuthorized(ProvenanceEventRecord event, NiFiUser user) {\n             }\n         };\n \n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final String uuid = \"00000000-0000-0000-0000-000000000000\";\n         final Map<String, String> attributes = new HashMap<>();\n@@ -1641,7 +1656,7 @@ protected int getJournalCount() {\n                 return journalCountRef.get();\n             }\n         };\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final Map<String, String> attributes = new HashMap<>();\n         final ProvenanceEventBuilder builder = new StandardProvenanceEventRecord.Builder();\n@@ -1697,7 +1712,7 @@ public void testTextualQuery() throws InterruptedException, IOException, ParseEx\n         config.setMaxEventFileLife(500, TimeUnit.MILLISECONDS);\n         config.setSearchableFields(new ArrayList<>(SearchableFields.getStandardFields()));\n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final String uuid = \"00000000-0000-0000-0000-000000000000\";\n         final Map<String, String> attributes = new HashMap<>();\n@@ -1732,7 +1747,7 @@ public void testTextualQuery() throws InterruptedException, IOException, ParseEx\n         final List<File> indexDirs = indexConfig.getIndexDirectories();\n \n         final String query = \"uuid:00000000-0000-0000-0000-0000000000* AND NOT filename:file-?\";\n-        final List<Document> results = runQuery(indexDirs.get(0), config.getStorageDirectories(), query);\n+        final List<Document> results = runQuery(indexDirs.get(0), new ArrayList<>(config.getStorageDirectories().values()), query);\n \n         assertEquals(6, results.size());\n     }\n@@ -1786,7 +1801,7 @@ public void testMergeJournals() throws IOException, InterruptedException {\n         final RepositoryConfiguration config = createConfiguration();\n         config.setMaxEventFileLife(3, TimeUnit.SECONDS);\n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final Map<String, String> attributes = new HashMap<>();\n \n@@ -1813,7 +1828,7 @@ public void run() {\n \n         repo.waitForRollover();\n \n-        final File storageDir = config.getStorageDirectories().get(0);\n+        final File storageDir = config.getStorageDirectories().values().iterator().next();\n         long counter = 0;\n         for (final File file : storageDir.listFiles()) {\n             if (file.isFile()) {\n@@ -1853,7 +1868,7 @@ public void testMergeJournalsBadFirstRecord() throws IOException, InterruptedExc\n         final RepositoryConfiguration config = createConfiguration();\n         config.setMaxEventFileLife(3, TimeUnit.SECONDS);\n         TestablePersistentProvenanceRepository testRepo = new TestablePersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        testRepo.initialize(getEventReporter(), null, null);\n+        testRepo.initialize(getEventReporter(), null, null, null);\n \n         final Map<String, String> attributes = new HashMap<>();\n \n@@ -1897,7 +1912,7 @@ public void run() {\n                         + \"that the record wasn't completely written to the file. This journal will be skipped.\",\n                 reportedEvents.get(reportedEvents.size() - 1).getMessage());\n \n-        final File storageDir = config.getStorageDirectories().get(0);\n+        final File storageDir = config.getStorageDirectories().values().iterator().next();\n         assertTrue(checkJournalRecords(storageDir, false) < 10000);\n     }\n \n@@ -1906,7 +1921,7 @@ public void testMergeJournalsBadRecordAfterFirst() throws IOException, Interrupt\n         final RepositoryConfiguration config = createConfiguration();\n         config.setMaxEventFileLife(3, TimeUnit.SECONDS);\n         TestablePersistentProvenanceRepository testRepo = new TestablePersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        testRepo.initialize(getEventReporter(), null, null);\n+        testRepo.initialize(getEventReporter(), null, null, null);\n \n         final Map<String, String> attributes = new HashMap<>();\n \n@@ -1951,7 +1966,7 @@ public void run() {\n                         + \"be skipped.\",\n                 reportedEvents.get(reportedEvents.size() - 1).getMessage());\n \n-        final File storageDir = config.getStorageDirectories().get(0);\n+        final File storageDir = config.getStorageDirectories().values().iterator().next();\n         assertTrue(checkJournalRecords(storageDir, false) < 10000);\n     }\n \n@@ -1960,7 +1975,7 @@ public void testMergeJournalsEmptyJournal() throws IOException, InterruptedExcep\n         final RepositoryConfiguration config = createConfiguration();\n         config.setMaxEventFileLife(3, TimeUnit.SECONDS);\n         TestablePersistentProvenanceRepository testRepo = new TestablePersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        testRepo.initialize(getEventReporter(), null, null);\n+        testRepo.initialize(getEventReporter(), null, null, null);\n \n         final Map<String, String> attributes = new HashMap<>();\n \n@@ -1997,7 +2012,7 @@ public void run() {\n \n         assertEquals(\"mergeJournals() should not error on empty journal\", 0, reportedEvents.size());\n \n-        final File storageDir = config.getStorageDirectories().get(0);\n+        final File storageDir = config.getStorageDirectories().values().iterator().next();\n         assertEquals(config.getJournalCount() - 1, checkJournalRecords(storageDir, true));\n     }\n \n@@ -2025,7 +2040,7 @@ protected long getRolloverRetryMillis() {\n                 return 10L; // retry quickly.\n             }\n         };\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final Map<String, String> attributes = new HashMap<>();\n \n@@ -2062,7 +2077,7 @@ public void testTruncateAttributes() throws IOException, InterruptedException {\n         config.setMaxAttributeChars(50);\n         config.setMaxEventFileLife(3, TimeUnit.SECONDS);\n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS);\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final String maxLengthChars = \"12345678901234567890123456789012345678901234567890\";\n         final Map<String, String> attributes = new HashMap<>();\n@@ -2108,7 +2123,7 @@ public void testExceptionOnIndex() throws IOException {\n         repo = new PersistentProvenanceRepository(config, DEFAULT_ROLLOVER_MILLIS) {\n             @Override\n             protected synchronized IndexingAction createIndexingAction() {\n-                return new IndexingAction(repo) {\n+                return new IndexingAction(config.getSearchableFields(), config.getSearchableAttributes()) {\n                     @Override\n                     public void index(StandardProvenanceEventRecord record, IndexWriter indexWriter, Integer blockIndex) throws IOException {\n                         final int count = indexedEventCount.incrementAndGet();\n@@ -2121,7 +2136,7 @@ public void index(StandardProvenanceEventRecord record, IndexWriter indexWriter,\n                 };\n             }\n         };\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         final Map<String, String> attributes = new HashMap<>();\n         attributes.put(\"uuid\", \"12345678-0000-0000-0000-012345678912\");\n@@ -2169,7 +2184,7 @@ public void testFailureToCreateWriterDoesNotPreventSubsequentRollover() throws I\n         };\n \n         // initialize with our event reporter\n-        repo.initialize(getEventReporter(), null, null);\n+        repo.initialize(getEventReporter(), null, null, IdentifierLookup.EMPTY);\n \n         // create some events in the journal files.\n         final Map<String, String> attributes = new HashMap<>();\n@@ -2219,10 +2234,12 @@ public ReportedEvent(final Severity severity, final String category, final Strin\n             this.message = message;\n         }\n \n+        @SuppressWarnings(\"unused\")\n         public String getCategory() {\n             return category;\n         }\n \n+        @SuppressWarnings(\"unused\")\n         public String getMessage() {\n             return message;\n         }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestPersistentProvenanceRepository.java",
                "sha": "48d8e09aff60d8ac1670569410fb71c29f7f2896",
                "status": "modified"
            },
            {
                "additions": 75,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestSchemaRecordReaderWriter.java",
                "changes": 105,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestSchemaRecordReaderWriter.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 30,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestSchemaRecordReaderWriter.java",
                "patch": "@@ -24,6 +24,7 @@\n import static org.junit.Assert.assertTrue;\n \n import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n import java.io.File;\n import java.io.FileInputStream;\n import java.io.IOException;\n@@ -34,11 +35,13 @@\n import java.util.List;\n import java.util.Map;\n import java.util.UUID;\n+import java.util.concurrent.Callable;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n import java.util.function.Consumer;\n \n+import org.apache.nifi.provenance.schema.EventFieldNames;\n import org.apache.nifi.provenance.schema.EventRecord;\n-import org.apache.nifi.provenance.schema.EventRecordFields;\n import org.apache.nifi.provenance.schema.ProvenanceEventSchema;\n import org.apache.nifi.provenance.serialization.RecordReader;\n import org.apache.nifi.provenance.serialization.RecordWriter;\n@@ -55,21 +58,73 @@\n import org.apache.nifi.repository.schema.RecordSchema;\n import org.apache.nifi.repository.schema.Repetition;\n import org.apache.nifi.repository.schema.SimpleRecordField;\n-import org.apache.nifi.stream.io.DataOutputStream;\n import org.apache.nifi.stream.io.NullOutputStream;\n+import org.junit.Assert;\n import org.junit.Before;\n import org.junit.Ignore;\n import org.junit.Test;\n \n public class TestSchemaRecordReaderWriter extends AbstractTestRecordReaderWriter {\n-\n+    private final AtomicLong idGenerator = new AtomicLong(0L);\n     private File journalFile;\n     private File tocFile;\n \n     @Before\n     public void setup() {\n         journalFile = new File(\"target/storage/\" + UUID.randomUUID().toString() + \"/testFieldAddedToSchema\");\n         tocFile = TocUtil.getTocFile(journalFile);\n+        idGenerator.set(0L);\n+    }\n+\n+\n+    @Test\n+    @Ignore(\"runs forever for performance analysis/profiling\")\n+    public void testPerformanceOfRandomAccessReads() throws Exception {\n+        journalFile = new File(\"target/storage/\" + UUID.randomUUID().toString() + \"/testPerformanceOfRandomAccessReads.gz\");\n+        tocFile = TocUtil.getTocFile(journalFile);\n+\n+        try (final RecordWriter writer = createWriter(journalFile, new StandardTocWriter(tocFile, true, false), true, 1024 * 32)) {\n+            writer.writeHeader(0L);\n+\n+            for (int i = 0; i < 100_000; i++) {\n+                writer.writeRecord(createEvent());\n+            }\n+        }\n+\n+        final long[] eventIds = new long[] {\n+            4, 80, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 40_000, 80_000, 99_000\n+        };\n+\n+        boolean loopForever = true;\n+        while (loopForever) {\n+            final long start = System.nanoTime();\n+            for (int i = 0; i < 1000; i++) {\n+                try (final InputStream in = new FileInputStream(journalFile);\n+                    final RecordReader reader = createReader(in, journalFile.getName(), new StandardTocReader(tocFile), 32 * 1024)) {\n+\n+                    for (final long id : eventIds) {\n+                        time(() -> {\n+                            reader.skipToEvent(id);\n+                            return reader.nextRecord();\n+                        }, id);\n+                    }\n+                }\n+            }\n+\n+            final long ms = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);\n+            System.out.println(ms + \" ms total\");\n+        }\n+    }\n+\n+    private void time(final Callable<StandardProvenanceEventRecord> task, final long id) throws Exception {\n+        final long start = System.nanoTime();\n+        final StandardProvenanceEventRecord event = task.call();\n+        Assert.assertNotNull(event);\n+        Assert.assertEquals(id, event.getEventId());\n+        //        System.out.println(event);\n+        final long nanos = System.nanoTime() - start;\n+        final long millis = TimeUnit.NANOSECONDS.toMillis(nanos);\n+        //        System.out.println(\"Took \" + millis + \" ms to \" + taskDescription);\n     }\n \n \n@@ -83,8 +138,8 @@ public void testFieldAddedToSchema() throws IOException {\n \n         try (final ByteArraySchemaRecordWriter writer = createSchemaWriter(schemaModifier, toAdd)) {\n             writer.writeHeader(1L);\n-            writer.writeRecord(createEvent(), 3L);\n-            writer.writeRecord(createEvent(), 3L);\n+            writer.writeRecord(createEvent());\n+            writer.writeRecord(createEvent());\n         }\n \n         try (final InputStream in = new FileInputStream(journalFile);\n@@ -94,7 +149,6 @@ public void testFieldAddedToSchema() throws IOException {\n             for (int i = 0; i < 2; i++) {\n                 final StandardProvenanceEventRecord event = reader.nextRecord();\n                 assertNotNull(event);\n-                assertEquals(3L, event.getEventId());\n                 assertEquals(\"1234\", event.getComponentId());\n                 assertEquals(ProvenanceEventType.RECEIVE, event.getEventType());\n \n@@ -111,14 +165,14 @@ public void testFieldRemovedFromSchema() throws IOException {\n             // Create a schema that has the fields modified\n             final RecordSchema schemaV1 = ProvenanceEventSchema.PROVENANCE_EVENT_SCHEMA_V1;\n             final List<RecordField> fields = new ArrayList<>(schemaV1.getFields());\n-            fields.remove(new SimpleRecordField(EventRecordFields.Names.UPDATED_ATTRIBUTES, FieldType.STRING, Repetition.EXACTLY_ONE));\n-            fields.remove(new SimpleRecordField(EventRecordFields.Names.PREVIOUS_ATTRIBUTES, FieldType.STRING, Repetition.EXACTLY_ONE));\n+            fields.remove(new SimpleRecordField(EventFieldNames.UPDATED_ATTRIBUTES, FieldType.STRING, Repetition.EXACTLY_ONE));\n+            fields.remove(new SimpleRecordField(EventFieldNames.PREVIOUS_ATTRIBUTES, FieldType.STRING, Repetition.EXACTLY_ONE));\n             final RecordSchema recordSchema = new RecordSchema(fields);\n \n             // Create a record writer whose schema does not contain updated attributes or previous attributes.\n             // This means that we must also override the method that writes out attributes so that we are able\n             // to avoid actually writing them out.\n-            final ByteArraySchemaRecordWriter writer = new ByteArraySchemaRecordWriter(journalFile, tocWriter, false, 0) {\n+            final ByteArraySchemaRecordWriter writer = new ByteArraySchemaRecordWriter(journalFile, idGenerator, tocWriter, false, 0) {\n                 @Override\n                 public void writeHeader(long firstEventId, DataOutputStream out) throws IOException {\n                     final ByteArrayOutputStream baos = new ByteArrayOutputStream();\n@@ -130,15 +184,15 @@ public void writeHeader(long firstEventId, DataOutputStream out) throws IOExcept\n \n                 @Override\n                 protected Record createRecord(final ProvenanceEventRecord event, final long eventId) {\n-                    final RecordSchema contentClaimSchema = new RecordSchema(recordSchema.getField(EventRecordFields.Names.CONTENT_CLAIM).getSubFields());\n+                    final RecordSchema contentClaimSchema = new RecordSchema(recordSchema.getField(EventFieldNames.CONTENT_CLAIM).getSubFields());\n                     return new EventRecord(event, eventId, recordSchema, contentClaimSchema);\n                 }\n             };\n \n             try {\n                 writer.writeHeader(1L);\n-                writer.writeRecord(createEvent(), 3L);\n-                writer.writeRecord(createEvent(), 3L);\n+                writer.writeRecord(createEvent());\n+                writer.writeRecord(createEvent());\n             } finally {\n                 writer.close();\n             }\n@@ -154,7 +208,6 @@ protected Record createRecord(final ProvenanceEventRecord event, final long even\n             for (int i = 0; i < 2; i++) {\n                 final StandardProvenanceEventRecord event = reader.nextRecord();\n                 assertNotNull(event);\n-                assertEquals(3L, event.getEventId());\n                 assertEquals(ProvenanceEventType.RECEIVE, event.getEventType());\n \n                 // We will still have a Map<String, String> for updated attributes because the\n@@ -175,7 +228,7 @@ public void testAddOneRecordReadTwice() throws IOException {\n \n         try (final ByteArraySchemaRecordWriter writer = createSchemaWriter(schemaModifier, toAdd)) {\n             writer.writeHeader(1L);\n-            writer.writeRecord(createEvent(), 3L);\n+            writer.writeRecord(createEvent());\n         }\n \n         try (final InputStream in = new FileInputStream(journalFile);\n@@ -207,9 +260,9 @@ private ByteArraySchemaRecordWriter createSchemaWriter(final Consumer<List<Recor\n         fieldModifier.accept(fields);\n \n         final RecordSchema recordSchema = new RecordSchema(fields);\n-        final RecordSchema contentClaimSchema = new RecordSchema(recordSchema.getField(EventRecordFields.Names.CONTENT_CLAIM).getSubFields());\n+        final RecordSchema contentClaimSchema = new RecordSchema(recordSchema.getField(EventFieldNames.CONTENT_CLAIM).getSubFields());\n \n-        final ByteArraySchemaRecordWriter writer = new ByteArraySchemaRecordWriter(journalFile, tocWriter, false, 0) {\n+        final ByteArraySchemaRecordWriter writer = new ByteArraySchemaRecordWriter(journalFile, idGenerator, tocWriter, false, 0) {\n             @Override\n             public void writeHeader(long firstEventId, DataOutputStream out) throws IOException {\n                 final ByteArrayOutputStream baos = new ByteArrayOutputStream();\n@@ -250,14 +303,13 @@ public void testWritePerformance() throws IOException {\n         final int numEvents = 10_000_000;\n         final long startNanos = System.nanoTime();\n         try (final OutputStream nullOut = new NullOutputStream();\n-            final RecordWriter writer = new ByteArraySchemaRecordWriter(nullOut, tocWriter, false, 0)) {\n+            final RecordWriter writer = new ByteArraySchemaRecordWriter(nullOut, \"out\", idGenerator, tocWriter, false, 0)) {\n \n             writer.writeHeader(0L);\n \n             for (int i = 0; i < numEvents; i++) {\n-                writer.writeRecord(event, i);\n+                writer.writeRecord(event);\n             }\n-\n         }\n \n         final long nanos = System.nanoTime() - startNanos;\n@@ -280,20 +332,20 @@ public void testReadPerformance() throws IOException, InterruptedException {\n         try (final ByteArrayOutputStream headerOut = new ByteArrayOutputStream();\n             final DataOutputStream out = new DataOutputStream(headerOut)) {\n \n-            final RecordWriter schemaWriter = new ByteArraySchemaRecordWriter(out, null, false, 0);\n+            final RecordWriter schemaWriter = new ByteArraySchemaRecordWriter(out, \"out\", idGenerator, null, false, 0);\n             schemaWriter.writeHeader(1L);\n \n             header = headerOut.toByteArray();\n         }\n \n         final byte[] serializedRecord;\n         try (final ByteArrayOutputStream headerOut = new ByteArrayOutputStream();\n-            final RecordWriter writer = new ByteArraySchemaRecordWriter(headerOut, null, false, 0)) {\n+            final RecordWriter writer = new ByteArraySchemaRecordWriter(headerOut, \"out\", idGenerator, null, false, 0)) {\n \n             writer.writeHeader(1L);\n             headerOut.reset();\n \n-            writer.writeRecord(event, 1L);\n+            writer.writeRecord(event);\n             writer.flush();\n             serializedRecord = headerOut.toByteArray();\n         }\n@@ -322,7 +374,7 @@ public void testReadPerformance() throws IOException, InterruptedException {\n \n     @Override\n     protected RecordWriter createWriter(File file, TocWriter tocWriter, boolean compressed, int uncompressedBlockSize) throws IOException {\n-        return new ByteArraySchemaRecordWriter(file, tocWriter, compressed, uncompressedBlockSize);\n+        return new ByteArraySchemaRecordWriter(file, idGenerator, tocWriter, compressed, uncompressedBlockSize);\n     }\n \n \n@@ -331,11 +383,4 @@ protected RecordReader createReader(InputStream in, String journalFilename, TocR\n         final ByteArraySchemaRecordReader reader = new ByteArraySchemaRecordReader(in, journalFilename, tocReader, maxAttributeSize);\n         return reader;\n     }\n-\n-    private static interface WriteRecordInterceptor {\n-        void writeRawRecord(ProvenanceEventRecord event, long recordIdentifier, DataOutputStream out) throws IOException;\n-    }\n-\n-    private static WriteRecordInterceptor NOP_INTERCEPTOR = (event, id, out) -> {};\n-    private static WriteRecordInterceptor WRITE_DUMMY_STRING_INTERCEPTOR = (event, id, out) -> out.writeUTF(\"hello\");\n }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestSchemaRecordReaderWriter.java",
                "sha": "2eb353e0ea8524d838e80c20c0e4d239a2b4fe18",
                "status": "modified"
            },
            {
                "additions": 18,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestStandardRecordReaderWriter.java",
                "changes": 29,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestStandardRecordReaderWriter.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 11,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestStandardRecordReaderWriter.java",
                "patch": "@@ -16,7 +16,11 @@\n  */\n package org.apache.nifi.provenance;\n \n+import static org.apache.nifi.provenance.TestUtil.createFlowFile;\n+import static org.junit.Assert.assertTrue;\n+\n import java.io.ByteArrayOutputStream;\n+import java.io.DataOutputStream;\n import java.io.File;\n import java.io.IOException;\n import java.io.InputStream;\n@@ -25,23 +29,26 @@\n import java.util.Map;\n import java.util.UUID;\n import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n \n import org.apache.commons.lang3.StringUtils;\n import org.apache.nifi.provenance.serialization.RecordReader;\n import org.apache.nifi.provenance.serialization.RecordWriter;\n import org.apache.nifi.provenance.toc.NopTocWriter;\n import org.apache.nifi.provenance.toc.TocReader;\n import org.apache.nifi.provenance.toc.TocWriter;\n-import org.apache.nifi.stream.io.DataOutputStream;\n import org.apache.nifi.stream.io.NullOutputStream;\n+import org.junit.Before;\n import org.junit.Ignore;\n import org.junit.Test;\n \n-import static org.apache.nifi.provenance.TestUtil.createFlowFile;\n-import static org.junit.Assert.assertTrue;\n-\n public class TestStandardRecordReaderWriter extends AbstractTestRecordReaderWriter {\n+    private AtomicLong idGenerator = new AtomicLong(0L);\n \n+    @Before\n+    public void resetIds() {\n+        idGenerator.set(0L);\n+    }\n \n     @Test\n     @Ignore(\"For local testing only\")\n@@ -56,12 +63,12 @@ public void testWritePerformance() throws IOException {\n         final int numEvents = 10_000_000;\n         final long startNanos = System.nanoTime();\n         try (final OutputStream nullOut = new NullOutputStream();\n-            final RecordWriter writer = new StandardRecordWriter(nullOut, tocWriter, false, 100000)) {\n+            final RecordWriter writer = new StandardRecordWriter(nullOut, \"devnull\", idGenerator, tocWriter, false, 100000)) {\n \n             writer.writeHeader(0L);\n \n             for (int i = 0; i < numEvents; i++) {\n-                writer.writeRecord(event, i);\n+                writer.writeRecord(event);\n             }\n         }\n \n@@ -90,12 +97,12 @@ public void testReadPerformance() throws IOException {\n \n         final byte[] serializedRecord;\n         try (final ByteArrayOutputStream headerOut = new ByteArrayOutputStream();\n-            final StandardRecordWriter writer = new StandardRecordWriter(headerOut, null, false, 0)) {\n+            final StandardRecordWriter writer = new StandardRecordWriter(headerOut, \"devnull\", idGenerator, null, false, 0)) {\n \n             writer.writeHeader(1L);\n             headerOut.reset();\n \n-            writer.writeRecord(event, 1L);\n+            writer.writeRecord(event);\n             writer.flush();\n             serializedRecord = headerOut.toByteArray();\n         }\n@@ -142,18 +149,18 @@ public void testWriteUtfLargerThan64k() throws IOException, InterruptedException\n         }\n \n         try (final ByteArrayOutputStream recordOut = new ByteArrayOutputStream();\n-            final StandardRecordWriter writer = new StandardRecordWriter(recordOut, null, false, 0)) {\n+            final StandardRecordWriter writer = new StandardRecordWriter(recordOut, \"devnull\", idGenerator, null, false, 0)) {\n \n             writer.writeHeader(1L);\n             recordOut.reset();\n \n-            writer.writeRecord(record, 1L);\n+            writer.writeRecord(record);\n         }\n     }\n \n     @Override\n     protected RecordWriter createWriter(File file, TocWriter tocWriter, boolean compressed, int uncompressedBlockSize) throws IOException {\n-        return new StandardRecordWriter(file, tocWriter, compressed, uncompressedBlockSize);\n+        return new StandardRecordWriter(file, idGenerator, tocWriter, compressed, uncompressedBlockSize);\n     }\n \n     @Override",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestStandardRecordReaderWriter.java",
                "sha": "27002c87407cc583a75d08971a2fe68a610abf7d",
                "status": "modified"
            },
            {
                "additions": 19,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestUtil.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestUtil.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestUtil.java",
                "patch": "@@ -18,6 +18,7 @@\n \n import java.util.HashMap;\n import java.util.Map;\n+import java.util.UUID;\n \n import org.apache.nifi.flowfile.FlowFile;\n \n@@ -82,4 +83,22 @@ public long getQueueDateIndex() {\n             }\n         };\n     }\n+\n+    public static ProvenanceEventRecord createEvent() {\n+        final Map<String, String> attributes = new HashMap<>();\n+        attributes.put(\"filename\", \"1.txt\");\n+        attributes.put(\"uuid\", UUID.randomUUID().toString());\n+\n+        final ProvenanceEventBuilder builder = new StandardProvenanceEventRecord.Builder();\n+        builder.setEventTime(System.currentTimeMillis());\n+        builder.setEventType(ProvenanceEventType.RECEIVE);\n+        builder.setTransitUri(\"nifi://unit-test\");\n+        builder.fromFlowFile(createFlowFile(3L, 3000L, attributes));\n+        builder.setComponentId(\"1234\");\n+        builder.setComponentType(\"dummy processor\");\n+        final ProvenanceEventRecord record = builder.build();\n+\n+        return record;\n+    }\n+\n }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/TestUtil.java",
                "sha": "224ee713f18e21cc9cd2c69ac30d07b771ed0458",
                "status": "modified"
            },
            {
                "additions": 142,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/index/lucene/TestEventIndexTask.java",
                "changes": 142,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/index/lucene/TestEventIndexTask.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/index/lucene/TestEventIndexTask.java",
                "patch": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.index.lucene;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.concurrent.BlockingQueue;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import org.apache.lucene.document.Document;\n+import org.apache.lucene.document.Field.Store;\n+import org.apache.lucene.document.LongField;\n+import org.apache.lucene.index.IndexWriter;\n+import org.apache.nifi.events.EventReporter;\n+import org.apache.nifi.provenance.RepositoryConfiguration;\n+import org.apache.nifi.provenance.SearchableFields;\n+import org.apache.nifi.provenance.index.EventIndexWriter;\n+import org.apache.nifi.provenance.lucene.IndexManager;\n+import org.apache.nifi.provenance.lucene.LuceneEventIndexWriter;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n+import org.junit.BeforeClass;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+\n+public class TestEventIndexTask {\n+\n+    @BeforeClass\n+    public static void setupClass() {\n+        System.setProperty(\"org.slf4j.simpleLogger.log.org.apache.nifi\", \"DEBUG\");\n+    }\n+\n+    @Test(timeout = 5000)\n+    public void testIndexWriterCommittedWhenAppropriate() throws IOException, InterruptedException {\n+        final BlockingQueue<StoredDocument> docQueue = new LinkedBlockingQueue<>();\n+        final RepositoryConfiguration repoConfig = new RepositoryConfiguration();\n+        final File storageDir = new File(\"target/storage/TestEventIndexTask/1\");\n+        repoConfig.addStorageDirectory(\"1\", storageDir);\n+\n+        final AtomicInteger commitCount = new AtomicInteger(0);\n+\n+        // Mock out an IndexWriter and keep track of the number of events that are indexed.\n+        final IndexWriter indexWriter = Mockito.mock(IndexWriter.class);\n+        final EventIndexWriter eventIndexWriter = new LuceneEventIndexWriter(indexWriter, storageDir);\n+\n+        final IndexManager indexManager = Mockito.mock(IndexManager.class);\n+        Mockito.when(indexManager.borrowIndexWriter(Mockito.any(File.class))).thenReturn(eventIndexWriter);\n+\n+        final IndexDirectoryManager directoryManager = new IndexDirectoryManager(repoConfig);\n+\n+        // Create an EventIndexTask and override the commit(IndexWriter) method so that we can keep track of how\n+        // many times the index writer gets committed.\n+        final EventIndexTask task = new EventIndexTask(docQueue, repoConfig, indexManager, directoryManager, 201, EventReporter.NO_OP) {\n+            @Override\n+            protected void commit(EventIndexWriter indexWriter) throws IOException {\n+                commitCount.incrementAndGet();\n+            }\n+        };\n+\n+        // Create 4 threads, each one a daemon thread running the EventIndexTask\n+        for (int i = 0; i < 4; i++) {\n+            final Thread t = new Thread(task);\n+            t.setDaemon(true);\n+            t.start();\n+        }\n+\n+        assertEquals(0, commitCount.get());\n+\n+        // Index 100 documents with a storage filename of \"0.0.prov\"\n+        for (int i = 0; i < 100; i++) {\n+            final Document document = new Document();\n+            document.add(new LongField(SearchableFields.EventTime.getSearchableFieldName(), System.currentTimeMillis(), Store.NO));\n+\n+            final StorageSummary location = new StorageSummary(1L, \"0.0.prov\", \"1\", 0, 1000L, 1000L);\n+            final StoredDocument storedDoc = new StoredDocument(document, location);\n+            docQueue.add(storedDoc);\n+        }\n+        assertEquals(0, commitCount.get());\n+\n+        // Index 100 documents\n+        for (int i = 0; i < 100; i++) {\n+            final Document document = new Document();\n+            document.add(new LongField(SearchableFields.EventTime.getSearchableFieldName(), System.currentTimeMillis(), Store.NO));\n+\n+            final StorageSummary location = new StorageSummary(1L, \"0.0.prov\", \"1\", 0, 1000L, 1000L);\n+            final StoredDocument storedDoc = new StoredDocument(document, location);\n+            docQueue.add(storedDoc);\n+        }\n+\n+        // Wait until we've indexed all 200 events\n+        while (eventIndexWriter.getEventsIndexed() < 200) {\n+            Thread.sleep(10L);\n+        }\n+\n+        // Wait a bit and make sure that we still haven't committed the index writer.\n+        Thread.sleep(100L);\n+        assertEquals(0, commitCount.get());\n+\n+        // Add another document.\n+        final Document document = new Document();\n+        document.add(new LongField(SearchableFields.EventTime.getSearchableFieldName(), System.currentTimeMillis(), Store.NO));\n+        final StorageSummary location = new StorageSummary(1L, \"0.0.prov\", \"1\", 0, 1000L, 1000L);\n+\n+        StoredDocument storedDoc = new StoredDocument(document, location);\n+        docQueue.add(storedDoc);\n+\n+        // Wait until index writer is committed.\n+        while (commitCount.get() == 0) {\n+            Thread.sleep(10L);\n+        }\n+        assertEquals(1, commitCount.get());\n+\n+        // Add a new IndexableDocument with a count of 1 to ensure that the writer is committed again.\n+        storedDoc = new StoredDocument(document, location);\n+        docQueue.add(storedDoc);\n+        Thread.sleep(100L);\n+        assertEquals(1, commitCount.get());\n+\n+        // Add a new IndexableDocument with a count of 3. Index writer should not be committed again.\n+        storedDoc = new StoredDocument(document, location);\n+        docQueue.add(storedDoc);\n+        Thread.sleep(100L);\n+        assertEquals(1, commitCount.get());\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/index/lucene/TestEventIndexTask.java",
                "sha": "4c58b1326937e9db48a21668e1d45a12e81a9a82",
                "status": "added"
            },
            {
                "additions": 100,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/index/lucene/TestIndexDirectoryManager.java",
                "changes": 100,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/index/lucene/TestIndexDirectoryManager.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/index/lucene/TestIndexDirectoryManager.java",
                "patch": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.index.lucene;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.io.File;\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.UUID;\n+\n+import org.apache.nifi.provenance.RepositoryConfiguration;\n+import org.junit.Test;\n+\n+public class TestIndexDirectoryManager {\n+\n+    @Test\n+    public void testGetDirectoriesIncludesMatchingTimestampPlusOne() {\n+        final List<IndexLocation> locations = new ArrayList<>();\n+        locations.add(createLocation(999L));\n+        locations.add(createLocation(1002L));\n+        locations.add(createLocation(1005L));\n+\n+        final List<File> directories = IndexDirectoryManager.getDirectories(1000L, 1001L, locations);\n+        assertEquals(2, directories.size());\n+        assertTrue(directories.contains(new File(\"index-999\")));\n+        assertTrue(directories.contains(new File(\"index-1002\")));\n+    }\n+\n+    @Test\n+    public void testGetDirectoriesOnlyObtainsDirectoriesForDesiredPartition() {\n+        final RepositoryConfiguration config = createConfig(2);\n+\n+        final File storageDir1 = config.getStorageDirectories().get(\"1\");\n+        final File storageDir2 = config.getStorageDirectories().get(\"2\");\n+\n+        final File index1 = new File(storageDir1, \"index-1\");\n+        final File index2 = new File(storageDir1, \"index-2\");\n+        final File index3 = new File(storageDir2, \"index-3\");\n+        final File index4 = new File(storageDir2, \"index-4\");\n+\n+        final File[] allIndices = new File[] {index1, index2, index3, index4};\n+        for (final File file : allIndices) {\n+            assertTrue(file.mkdirs() || file.exists());\n+        }\n+\n+        try {\n+            final IndexDirectoryManager mgr = new IndexDirectoryManager(config);\n+            mgr.initialize();\n+\n+            final List<File> indexes1 = mgr.getDirectories(0L, Long.MAX_VALUE, \"1\");\n+            final List<File> indexes2 = mgr.getDirectories(0L, Long.MAX_VALUE, \"2\");\n+\n+            assertEquals(2, indexes1.size());\n+            assertTrue(indexes1.contains(index1));\n+            assertTrue(indexes1.contains(index2));\n+\n+            assertEquals(2, indexes2.size());\n+            assertTrue(indexes2.contains(index3));\n+            assertTrue(indexes2.contains(index4));\n+        } finally {\n+            for (final File file : allIndices) {\n+                file.delete();\n+            }\n+        }\n+    }\n+\n+\n+    private IndexLocation createLocation(final long timestamp) {\n+        return createLocation(timestamp, \"1\");\n+    }\n+\n+    private IndexLocation createLocation(final long timestamp, final String partitionName) {\n+        return new IndexLocation(new File(\"index-\" + timestamp), timestamp, partitionName, 1024 * 1024L);\n+    }\n+\n+    private RepositoryConfiguration createConfig(final int partitions) {\n+        final RepositoryConfiguration repoConfig = new RepositoryConfiguration();\n+        for (int i = 1; i <= partitions; i++) {\n+            repoConfig.addStorageDirectory(String.valueOf(i), new File(\"target/storage/testIndexDirectoryManager/\" + UUID.randomUUID() + \"/\" + i));\n+        }\n+        return repoConfig;\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/index/lucene/TestIndexDirectoryManager.java",
                "sha": "3f3c42296c2224cdcaa9d110ba3753b95cd5e0cc",
                "status": "added"
            },
            {
                "additions": 538,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/index/lucene/TestLuceneEventIndex.java",
                "changes": 538,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/index/lucene/TestLuceneEventIndex.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/index/lucene/TestLuceneEventIndex.java",
                "patch": "@@ -0,0 +1,538 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.index.lucene;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertNull;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+import org.apache.nifi.authorization.AccessDeniedException;\n+import org.apache.nifi.authorization.user.NiFiUser;\n+import org.apache.nifi.events.EventReporter;\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.ProvenanceEventType;\n+import org.apache.nifi.provenance.RepositoryConfiguration;\n+import org.apache.nifi.provenance.SearchableFields;\n+import org.apache.nifi.provenance.StandardProvenanceEventRecord;\n+import org.apache.nifi.provenance.authorization.EventAuthorizer;\n+import org.apache.nifi.provenance.lineage.ComputeLineageSubmission;\n+import org.apache.nifi.provenance.lineage.LineageNode;\n+import org.apache.nifi.provenance.lineage.LineageNodeType;\n+import org.apache.nifi.provenance.lineage.ProvenanceEventLineageNode;\n+import org.apache.nifi.provenance.lucene.IndexManager;\n+import org.apache.nifi.provenance.lucene.SimpleIndexManager;\n+import org.apache.nifi.provenance.search.Query;\n+import org.apache.nifi.provenance.search.QueryResult;\n+import org.apache.nifi.provenance.search.QuerySubmission;\n+import org.apache.nifi.provenance.search.SearchTerms;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n+import org.apache.nifi.provenance.store.ArrayListEventStore;\n+import org.apache.nifi.provenance.store.EventStore;\n+import org.apache.nifi.provenance.store.StorageResult;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TestName;\n+import org.mockito.Mockito;\n+import org.mockito.invocation.InvocationOnMock;\n+import org.mockito.stubbing.Answer;\n+\n+public class TestLuceneEventIndex {\n+\n+    private final AtomicLong idGenerator = new AtomicLong(0L);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @BeforeClass\n+    public static void setLogger() {\n+        System.setProperty(\"org.slf4j.simpleLogger.log.org.apache.nifi\", \"DEBUG\");\n+    }\n+\n+\n+    @Test(timeout = 5000)\n+    public void testGetMinimumIdToReindex() throws InterruptedException {\n+        final RepositoryConfiguration repoConfig = createConfig(1);\n+        repoConfig.setDesiredIndexSize(1L);\n+        final IndexManager indexManager = new SimpleIndexManager(repoConfig);\n+\n+        final ArrayListEventStore eventStore = new ArrayListEventStore();\n+        final LuceneEventIndex index = new LuceneEventIndex(repoConfig, indexManager, 20_000, EventReporter.NO_OP);\n+        index.initialize(eventStore);\n+\n+        for (int i = 0; i < 50_000; i++) {\n+            final ProvenanceEventRecord event = createEvent(\"1234\");\n+            final StorageResult storageResult = eventStore.addEvent(event);\n+            index.addEvents(storageResult.getStorageLocations());\n+        }\n+\n+        while (index.getMaxEventId(\"1\") < 40_000L) {\n+            Thread.sleep(25);\n+        }\n+\n+        final long id = index.getMinimumEventIdToReindex(\"1\");\n+        assertTrue(id >= 30000L);\n+    }\n+\n+    @Test(timeout = 5000)\n+    public void testUnauthorizedEventsGetPlaceholdersForLineage() throws InterruptedException {\n+        final RepositoryConfiguration repoConfig = createConfig(1);\n+        repoConfig.setDesiredIndexSize(1L);\n+        final IndexManager indexManager = new SimpleIndexManager(repoConfig);\n+\n+        final ArrayListEventStore eventStore = new ArrayListEventStore();\n+        final LuceneEventIndex index = new LuceneEventIndex(repoConfig, indexManager, 3, EventReporter.NO_OP);\n+        index.initialize(eventStore);\n+\n+        for (int i = 0; i < 3; i++) {\n+            final ProvenanceEventRecord event = createEvent(\"1234\");\n+            final StorageResult storageResult = eventStore.addEvent(event);\n+            index.addEvents(storageResult.getStorageLocations());\n+        }\n+\n+        final NiFiUser user = createUser();\n+\n+        List<LineageNode> nodes = Collections.emptyList();\n+        while (nodes.size() < 3) {\n+            final ComputeLineageSubmission submission = index.submitLineageComputation(1L, user, EventAuthorizer.DENY_ALL);\n+            assertTrue(submission.getResult().awaitCompletion(5, TimeUnit.SECONDS));\n+\n+            nodes = submission.getResult().getNodes();\n+            Thread.sleep(25L);\n+        }\n+\n+        assertEquals(3, nodes.size());\n+\n+        for (final LineageNode node : nodes) {\n+            assertEquals(LineageNodeType.PROVENANCE_EVENT_NODE, node.getNodeType());\n+            final ProvenanceEventLineageNode eventNode = (ProvenanceEventLineageNode) node;\n+            assertEquals(ProvenanceEventType.UNKNOWN, eventNode.getEventType());\n+        }\n+    }\n+\n+    @Test(timeout = 5000)\n+    public void testUnauthorizedEventsGetPlaceholdersForExpandChildren() throws InterruptedException {\n+        final RepositoryConfiguration repoConfig = createConfig(1);\n+        repoConfig.setDesiredIndexSize(1L);\n+        final IndexManager indexManager = new SimpleIndexManager(repoConfig);\n+\n+        final ArrayListEventStore eventStore = new ArrayListEventStore();\n+        final LuceneEventIndex index = new LuceneEventIndex(repoConfig, indexManager, 3, EventReporter.NO_OP);\n+        index.initialize(eventStore);\n+\n+        final ProvenanceEventRecord firstEvent = createEvent(\"4444\");\n+\n+        final Map<String, String> previousAttributes = new HashMap<>();\n+        previousAttributes.put(\"uuid\", \"4444\");\n+        final Map<String, String> updatedAttributes = new HashMap<>();\n+        updatedAttributes.put(\"updated\", \"true\");\n+        final ProvenanceEventRecord fork = new StandardProvenanceEventRecord.Builder()\n+            .setEventType(ProvenanceEventType.FORK)\n+            .setAttributes(previousAttributes, updatedAttributes)\n+            .addChildFlowFile(\"1234\")\n+            .setComponentId(\"component-1\")\n+            .setComponentType(\"unit test\")\n+            .setEventId(idGenerator.getAndIncrement())\n+            .setEventTime(System.currentTimeMillis())\n+            .setFlowFileEntryDate(System.currentTimeMillis())\n+            .setFlowFileUUID(\"4444\")\n+            .setLineageStartDate(System.currentTimeMillis())\n+            .setCurrentContentClaim(\"container\", \"section\", \"unit-test-id\", 0L, 1024L)\n+            .build();\n+\n+        index.addEvents(eventStore.addEvent(firstEvent).getStorageLocations());\n+        index.addEvents(eventStore.addEvent(fork).getStorageLocations());\n+\n+        for (int i = 0; i < 3; i++) {\n+            final ProvenanceEventRecord event = createEvent(\"1234\");\n+            final StorageResult storageResult = eventStore.addEvent(event);\n+            index.addEvents(storageResult.getStorageLocations());\n+        }\n+\n+        final NiFiUser user = createUser();\n+\n+        final EventAuthorizer allowForkEvents = new EventAuthorizer() {\n+            @Override\n+            public boolean isAuthorized(ProvenanceEventRecord event) {\n+                return event.getEventType() == ProvenanceEventType.FORK;\n+            }\n+\n+            @Override\n+            public void authorize(ProvenanceEventRecord event) throws AccessDeniedException {\n+            }\n+        };\n+\n+        List<LineageNode> nodes = Collections.emptyList();\n+        while (nodes.size() < 5) {\n+            final ComputeLineageSubmission submission = index.submitExpandChildren(1L, user, allowForkEvents);\n+            assertTrue(submission.getResult().awaitCompletion(5, TimeUnit.SECONDS));\n+\n+            nodes = submission.getResult().getNodes();\n+            Thread.sleep(25L);\n+        }\n+\n+        assertEquals(5, nodes.size());\n+\n+        assertEquals(1L, nodes.stream().filter(n -> n.getNodeType() == LineageNodeType.FLOWFILE_NODE).count());\n+        assertEquals(4L, nodes.stream().filter(n -> n.getNodeType() == LineageNodeType.PROVENANCE_EVENT_NODE).count());\n+\n+        final Map<ProvenanceEventType, List<LineageNode>> eventMap = nodes.stream()\n+            .filter(n -> n.getNodeType() == LineageNodeType.PROVENANCE_EVENT_NODE)\n+            .collect(Collectors.groupingBy(n -> ((ProvenanceEventLineageNode) n).getEventType()));\n+\n+        assertEquals(2, eventMap.size());\n+        assertEquals(1, eventMap.get(ProvenanceEventType.FORK).size());\n+        assertEquals(3, eventMap.get(ProvenanceEventType.UNKNOWN).size());\n+    }\n+\n+    @Test(timeout = 5000)\n+    public void testUnauthorizedEventsGetPlaceholdersForFindParents() throws InterruptedException {\n+        final RepositoryConfiguration repoConfig = createConfig(1);\n+        repoConfig.setDesiredIndexSize(1L);\n+        final IndexManager indexManager = new SimpleIndexManager(repoConfig);\n+\n+        final ArrayListEventStore eventStore = new ArrayListEventStore();\n+        final LuceneEventIndex index = new LuceneEventIndex(repoConfig, indexManager, 3, EventReporter.NO_OP);\n+        index.initialize(eventStore);\n+\n+        final ProvenanceEventRecord firstEvent = createEvent(\"4444\");\n+\n+        final Map<String, String> previousAttributes = new HashMap<>();\n+        previousAttributes.put(\"uuid\", \"4444\");\n+        final Map<String, String> updatedAttributes = new HashMap<>();\n+        updatedAttributes.put(\"updated\", \"true\");\n+        final ProvenanceEventRecord join = new StandardProvenanceEventRecord.Builder()\n+            .setEventType(ProvenanceEventType.JOIN)\n+            .setAttributes(previousAttributes, updatedAttributes)\n+            .addParentUuid(\"4444\")\n+            .addChildFlowFile(\"1234\")\n+            .setComponentId(\"component-1\")\n+            .setComponentType(\"unit test\")\n+            .setEventId(idGenerator.getAndIncrement())\n+            .setEventTime(System.currentTimeMillis())\n+            .setFlowFileEntryDate(System.currentTimeMillis())\n+            .setFlowFileUUID(\"1234\")\n+            .setLineageStartDate(System.currentTimeMillis())\n+            .setCurrentContentClaim(\"container\", \"section\", \"unit-test-id\", 0L, 1024L)\n+            .build();\n+\n+        index.addEvents(eventStore.addEvent(firstEvent).getStorageLocations());\n+        index.addEvents(eventStore.addEvent(join).getStorageLocations());\n+\n+        for (int i = 0; i < 3; i++) {\n+            final ProvenanceEventRecord event = createEvent(\"1234\");\n+            final StorageResult storageResult = eventStore.addEvent(event);\n+            index.addEvents(storageResult.getStorageLocations());\n+        }\n+\n+        final NiFiUser user = createUser();\n+\n+        final EventAuthorizer allowJoinEvents = new EventAuthorizer() {\n+            @Override\n+            public boolean isAuthorized(ProvenanceEventRecord event) {\n+                return event.getEventType() == ProvenanceEventType.JOIN;\n+            }\n+\n+            @Override\n+            public void authorize(ProvenanceEventRecord event) throws AccessDeniedException {\n+            }\n+        };\n+\n+        List<LineageNode> nodes = Collections.emptyList();\n+        while (nodes.size() < 2) {\n+            final ComputeLineageSubmission submission = index.submitExpandParents(1L, user, allowJoinEvents);\n+            assertTrue(submission.getResult().awaitCompletion(5, TimeUnit.SECONDS));\n+\n+            nodes = submission.getResult().getNodes();\n+            Thread.sleep(25L);\n+        }\n+\n+        assertEquals(2, nodes.size());\n+\n+        final Map<ProvenanceEventType, List<LineageNode>> eventMap = nodes.stream()\n+            .filter(n -> n.getNodeType() == LineageNodeType.PROVENANCE_EVENT_NODE)\n+            .collect(Collectors.groupingBy(n -> ((ProvenanceEventLineageNode) n).getEventType()));\n+\n+        assertEquals(2, eventMap.size());\n+        assertEquals(1, eventMap.get(ProvenanceEventType.JOIN).size());\n+        assertEquals(1, eventMap.get(ProvenanceEventType.UNKNOWN).size());\n+\n+        assertEquals(\"4444\", eventMap.get(ProvenanceEventType.UNKNOWN).get(0).getFlowFileUuid());\n+    }\n+\n+    @Test(timeout = 5000)\n+    public void testUnauthorizedEventsGetFilteredForQuery() throws InterruptedException {\n+        final RepositoryConfiguration repoConfig = createConfig(1);\n+        repoConfig.setDesiredIndexSize(1L);\n+        final IndexManager indexManager = new SimpleIndexManager(repoConfig);\n+\n+        final ArrayListEventStore eventStore = new ArrayListEventStore();\n+        final LuceneEventIndex index = new LuceneEventIndex(repoConfig, indexManager, 3, EventReporter.NO_OP);\n+        index.initialize(eventStore);\n+\n+        for (int i = 0; i < 3; i++) {\n+            final ProvenanceEventRecord event = createEvent(\"1234\");\n+            final StorageResult storageResult = eventStore.addEvent(event);\n+            index.addEvents(storageResult.getStorageLocations());\n+        }\n+\n+        final Query query = new Query(UUID.randomUUID().toString());\n+        final EventAuthorizer authorizer = new EventAuthorizer() {\n+            @Override\n+            public boolean isAuthorized(ProvenanceEventRecord event) {\n+                return event.getEventId() % 2 == 0;\n+            }\n+\n+            @Override\n+            public void authorize(ProvenanceEventRecord event) throws AccessDeniedException {\n+                throw new AccessDeniedException();\n+            }\n+        };\n+\n+        List<ProvenanceEventRecord> events = Collections.emptyList();\n+        while (events.size() < 2) {\n+            final QuerySubmission submission = index.submitQuery(query, authorizer, \"unit test\");\n+            assertTrue(submission.getResult().awaitCompletion(5, TimeUnit.SECONDS));\n+            events = submission.getResult().getMatchingEvents();\n+            Thread.sleep(25L);\n+        }\n+\n+        assertEquals(2, events.size());\n+    }\n+\n+\n+    private NiFiUser createUser() {\n+        return new NiFiUser() {\n+            @Override\n+            public String getIdentity() {\n+                return \"unit test\";\n+            }\n+\n+            @Override\n+            public NiFiUser getChain() {\n+                return null;\n+            }\n+\n+            @Override\n+            public boolean isAnonymous() {\n+                return false;\n+            }\n+\n+            @Override\n+            public String getClientAddress() {\n+                return \"127.0.0.1\";\n+            }\n+        };\n+    }\n+\n+\n+    @Test(timeout = 5000)\n+    public void testExpiration() throws InterruptedException, IOException {\n+        final RepositoryConfiguration repoConfig = createConfig(1);\n+        repoConfig.setDesiredIndexSize(1L);\n+        final IndexManager indexManager = new SimpleIndexManager(repoConfig);\n+\n+        final LuceneEventIndex index = new LuceneEventIndex(repoConfig, indexManager, 1, EventReporter.NO_OP);\n+\n+        final List<ProvenanceEventRecord> events = new ArrayList<>();\n+        events.add(createEvent(500000L));\n+        events.add(createEvent());\n+\n+        final EventStore eventStore = Mockito.mock(EventStore.class);\n+        Mockito.doAnswer(new Answer<List<ProvenanceEventRecord>>() {\n+            @Override\n+            public List<ProvenanceEventRecord> answer(final InvocationOnMock invocation) throws Throwable {\n+                final Long eventId = invocation.getArgumentAt(0, Long.class);\n+                assertEquals(0, eventId.longValue());\n+                assertEquals(1, invocation.getArgumentAt(1, Integer.class).intValue());\n+                return Collections.singletonList(events.get(0));\n+            }\n+        }).when(eventStore).getEvents(Mockito.anyLong(), Mockito.anyInt());\n+\n+        index.initialize(eventStore);\n+        index.addEvent(events.get(0), createStorageSummary(events.get(0).getEventId()));\n+\n+        // Add the first event to the index and wait for it to be indexed, since indexing is asynchronous.\n+        List<File> allDirectories = Collections.emptyList();\n+        while (allDirectories.isEmpty()) {\n+            allDirectories = index.getDirectoryManager().getDirectories(null, null);\n+        }\n+\n+        events.remove(0); // Remove the first event from the store\n+        index.performMaintenance();\n+        assertEquals(1, index.getDirectoryManager().getDirectories(null, null).size());\n+    }\n+\n+    private StorageSummary createStorageSummary(final long eventId) {\n+        return new StorageSummary(eventId, \"1.prov\", \"1\", 1, 2L, 2L);\n+    }\n+\n+\n+    @Test(timeout = 5000)\n+    public void addThenQueryWithEmptyQuery() throws InterruptedException {\n+        final RepositoryConfiguration repoConfig = createConfig();\n+        final IndexManager indexManager = new SimpleIndexManager(repoConfig);\n+\n+        final LuceneEventIndex index = new LuceneEventIndex(repoConfig, indexManager, 1, EventReporter.NO_OP);\n+\n+        final ProvenanceEventRecord event = createEvent();\n+\n+        index.addEvent(event, new StorageSummary(event.getEventId(), \"1.prov\", \"1\", 1, 2L, 2L));\n+\n+        final Query query = new Query(UUID.randomUUID().toString());\n+\n+        final ArrayListEventStore eventStore = new ArrayListEventStore();\n+        eventStore.addEvent(event);\n+        index.initialize(eventStore);\n+\n+        // We don't know how long it will take for the event to be indexed, so keep querying until\n+        // we get a result. The test will timeout after 5 seconds if we've still not succeeded.\n+        List<ProvenanceEventRecord> matchingEvents = Collections.emptyList();\n+        while (matchingEvents.isEmpty()) {\n+            final QuerySubmission submission = index.submitQuery(query, EventAuthorizer.GRANT_ALL, \"unit test user\");\n+            assertNotNull(submission);\n+\n+            final QueryResult result = submission.getResult();\n+            assertNotNull(result);\n+            result.awaitCompletion(100, TimeUnit.MILLISECONDS);\n+\n+            assertTrue(result.isFinished());\n+            assertNull(result.getError());\n+\n+            matchingEvents = result.getMatchingEvents();\n+            assertNotNull(matchingEvents);\n+            Thread.sleep(100L); // avoid crushing the CPU\n+        }\n+\n+        assertEquals(1, matchingEvents.size());\n+        assertEquals(event, matchingEvents.get(0));\n+    }\n+\n+    @Test(timeout = 50000)\n+    public void testQuerySpecificField() throws InterruptedException {\n+        final RepositoryConfiguration repoConfig = createConfig();\n+        final IndexManager indexManager = new SimpleIndexManager(repoConfig);\n+\n+        final LuceneEventIndex index = new LuceneEventIndex(repoConfig, indexManager, 2, EventReporter.NO_OP);\n+\n+        // add 2 events, one of which we will query for.\n+        final ProvenanceEventRecord event = createEvent();\n+        index.addEvent(event, new StorageSummary(event.getEventId(), \"1.prov\", \"1\", 1, 2L, 2L));\n+        index.addEvent(createEvent(), new StorageSummary(2L, \"1.prov\", \"1\", 1, 2L, 2L));\n+\n+        // Create a query that searches for the event with the FlowFile UUID equal to the first event's.\n+        final Query query = new Query(UUID.randomUUID().toString());\n+        query.addSearchTerm(SearchTerms.newSearchTerm(SearchableFields.FlowFileUUID, event.getFlowFileUuid()));\n+\n+        final ArrayListEventStore eventStore = new ArrayListEventStore();\n+        eventStore.addEvent(event);\n+        index.initialize(eventStore);\n+\n+        // We don't know how long it will take for the event to be indexed, so keep querying until\n+        // we get a result. The test will timeout after 5 seconds if we've still not succeeded.\n+        List<ProvenanceEventRecord> matchingEvents = Collections.emptyList();\n+        while (matchingEvents.isEmpty()) {\n+            final QuerySubmission submission = index.submitQuery(query, EventAuthorizer.GRANT_ALL, \"unit test user\");\n+            assertNotNull(submission);\n+\n+            final QueryResult result = submission.getResult();\n+            assertNotNull(result);\n+            result.awaitCompletion(100, TimeUnit.MILLISECONDS);\n+\n+            assertTrue(result.isFinished());\n+            assertNull(result.getError());\n+\n+            matchingEvents = result.getMatchingEvents();\n+            assertNotNull(matchingEvents);\n+            Thread.sleep(100L); // avoid crushing the CPU\n+        }\n+\n+        assertEquals(1, matchingEvents.size());\n+        assertEquals(event, matchingEvents.get(0));\n+    }\n+\n+    private RepositoryConfiguration createConfig() {\n+        return createConfig(1);\n+    }\n+\n+    private RepositoryConfiguration createConfig(final int storageDirectoryCount) {\n+        final RepositoryConfiguration config = new RepositoryConfiguration();\n+        final String unitTestName = testName.getMethodName();\n+        final File storageDir = new File(\"target/storage/\" + unitTestName + \"/\" + UUID.randomUUID().toString());\n+\n+        for (int i = 0; i < storageDirectoryCount; i++) {\n+            config.addStorageDirectory(String.valueOf(i + 1), new File(storageDir, String.valueOf(i)));\n+        }\n+\n+        config.setSearchableFields(Collections.singletonList(SearchableFields.FlowFileUUID));\n+        config.setSearchableAttributes(Collections.singletonList(SearchableFields.newSearchableAttribute(\"updated\")));\n+\n+        for (final File file : config.getStorageDirectories().values()) {\n+            assertTrue(file.exists() || file.mkdirs());\n+        }\n+\n+        return config;\n+    }\n+\n+    private ProvenanceEventRecord createEvent() {\n+        return createEvent(System.currentTimeMillis());\n+    }\n+\n+    private ProvenanceEventRecord createEvent(final String uuid) {\n+        return createEvent(System.currentTimeMillis(), uuid);\n+    }\n+\n+    private ProvenanceEventRecord createEvent(final long timestamp) {\n+        return createEvent(timestamp, UUID.randomUUID().toString());\n+    }\n+\n+    private ProvenanceEventRecord createEvent(final long timestamp, final String uuid) {\n+        final Map<String, String> previousAttributes = new HashMap<>();\n+        previousAttributes.put(\"uuid\", uuid);\n+        final Map<String, String> updatedAttributes = new HashMap<>();\n+        updatedAttributes.put(\"updated\", \"true\");\n+\n+        final ProvenanceEventRecord event = new StandardProvenanceEventRecord.Builder()\n+            .setEventType(ProvenanceEventType.CONTENT_MODIFIED)\n+            .setAttributes(previousAttributes, updatedAttributes)\n+            .setComponentId(\"component-1\")\n+            .setComponentType(\"unit test\")\n+            .setEventId(idGenerator.getAndIncrement())\n+            .setEventTime(timestamp)\n+            .setFlowFileEntryDate(timestamp)\n+            .setFlowFileUUID(uuid)\n+            .setLineageStartDate(timestamp)\n+            .setCurrentContentClaim(\"container\", \"section\", \"unit-test-id\", 0L, 1024L)\n+            .build();\n+\n+        return event;\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/index/lucene/TestLuceneEventIndex.java",
                "sha": "c89237636753f1b3742376c9368e3d565c9b6a2f",
                "status": "added"
            },
            {
                "additions": 14,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/lucene/TestCachingIndexManager.java",
                "changes": 28,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/lucene/TestCachingIndexManager.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 14,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/lucene/TestCachingIndexManager.java",
                "patch": "@@ -29,14 +29,14 @@\n import org.apache.lucene.document.Document;\n import org.apache.lucene.document.Field.Store;\n import org.apache.lucene.document.StringField;\n-import org.apache.lucene.index.IndexWriter;\n import org.apache.lucene.index.Term;\n import org.apache.lucene.search.BooleanClause;\n+import org.apache.lucene.search.BooleanClause.Occur;\n import org.apache.lucene.search.BooleanQuery;\n-import org.apache.lucene.search.IndexSearcher;\n import org.apache.lucene.search.TermQuery;\n import org.apache.lucene.search.TopDocs;\n-import org.apache.lucene.search.BooleanClause.Occur;\n+import org.apache.nifi.provenance.index.EventIndexSearcher;\n+import org.apache.nifi.provenance.index.EventIndexWriter;\n import org.apache.nifi.util.file.FileUtils;\n import org.junit.After;\n import org.junit.Before;\n@@ -67,47 +67,47 @@ public void cleanup() throws IOException {\n     public void test() throws IOException {\n         // Create and IndexWriter and add a document to the index, then close the writer.\n         // This gives us something that we can query.\n-        final IndexWriter writer = manager.borrowIndexWriter(indexDir);\n+        final EventIndexWriter writer = manager.borrowIndexWriter(indexDir);\n         final Document doc = new Document();\n         doc.add(new StringField(\"unit test\", \"true\", Store.YES));\n-        writer.addDocument(doc);\n-        manager.returnIndexWriter(indexDir, writer);\n+        writer.index(doc, 1000);\n+        manager.returnIndexWriter(writer);\n \n         // Get an Index Searcher that we can use to query the index.\n-        final IndexSearcher cachedSearcher = manager.borrowIndexSearcher(indexDir);\n+        final EventIndexSearcher cachedSearcher = manager.borrowIndexSearcher(indexDir);\n \n         // Ensure that we get the expected results.\n         assertCount(cachedSearcher, 1);\n \n         // While we already have an Index Searcher, get a writer for the same index.\n         // This will cause the Index Searcher to be marked as poisoned.\n-        final IndexWriter writer2 = manager.borrowIndexWriter(indexDir);\n+        final EventIndexWriter writer2 = manager.borrowIndexWriter(indexDir);\n \n         // Obtain a new Index Searcher with the writer open. This Index Searcher should *NOT*\n         // be the same as the previous searcher because the new one will be a Near-Real-Time Index Searcher\n         // while the other is not.\n-        final IndexSearcher nrtSearcher = manager.borrowIndexSearcher(indexDir);\n+        final EventIndexSearcher nrtSearcher = manager.borrowIndexSearcher(indexDir);\n         assertNotSame(cachedSearcher, nrtSearcher);\n \n         // Ensure that we get the expected query results.\n         assertCount(nrtSearcher, 1);\n \n         // Return the writer, so that there is no longer an active writer for the index.\n-        manager.returnIndexWriter(indexDir, writer2);\n+        manager.returnIndexWriter(writer2);\n \n         // Ensure that we still get the same result.\n         assertCount(cachedSearcher, 1);\n-        manager.returnIndexSearcher(indexDir, cachedSearcher);\n+        manager.returnIndexSearcher(cachedSearcher);\n \n         // Ensure that our near-real-time index searcher still gets the same result.\n         assertCount(nrtSearcher, 1);\n-        manager.returnIndexSearcher(indexDir, nrtSearcher);\n+        manager.returnIndexSearcher(nrtSearcher);\n     }\n \n-    private void assertCount(final IndexSearcher searcher, final int count) throws IOException {\n+    private void assertCount(final EventIndexSearcher searcher, final int count) throws IOException {\n         final BooleanQuery query = new BooleanQuery();\n         query.add(new BooleanClause(new TermQuery(new Term(\"unit test\", \"true\")), Occur.MUST));\n-        final TopDocs topDocs = searcher.search(query, count * 10);\n+        final TopDocs topDocs = searcher.getIndexSearcher().search(query, count * 10);\n         assertNotNull(topDocs);\n         assertEquals(1, topDocs.totalHits);\n     }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/lucene/TestCachingIndexManager.java",
                "sha": "a42b73a9f639d15ae15f7123c0f0e047cbfc92d0",
                "status": "modified"
            },
            {
                "additions": 91,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/lucene/TestSimpleIndexManager.java",
                "changes": 104,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/lucene/TestSimpleIndexManager.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 13,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/lucene/TestSimpleIndexManager.java",
                "patch": "@@ -18,18 +18,21 @@\n package org.apache.nifi.provenance.lucene;\n \n import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertTrue;\n \n import java.io.File;\n import java.io.IOException;\n import java.util.UUID;\n+import java.util.concurrent.atomic.AtomicInteger;\n \n import org.apache.lucene.document.Document;\n import org.apache.lucene.document.Field.Store;\n import org.apache.lucene.document.StringField;\n-import org.apache.lucene.index.IndexWriter;\n-import org.apache.lucene.search.IndexSearcher;\n import org.apache.lucene.search.MatchAllDocsQuery;\n import org.apache.lucene.search.TopDocs;\n+import org.apache.nifi.provenance.RepositoryConfiguration;\n+import org.apache.nifi.provenance.index.EventIndexSearcher;\n+import org.apache.nifi.provenance.index.EventIndexWriter;\n import org.apache.nifi.util.file.FileUtils;\n import org.junit.BeforeClass;\n import org.junit.Test;\n@@ -40,33 +43,108 @@ public static void setLogLevel() {\n         System.setProperty(\"org.slf4j.simpleLogger.log.org.apache.nifi.provenance\", \"DEBUG\");\n     }\n \n-\n     @Test\n     public void testMultipleWritersSimultaneouslySameIndex() throws IOException {\n-        final SimpleIndexManager mgr = new SimpleIndexManager();\n+        final SimpleIndexManager mgr = new SimpleIndexManager(new RepositoryConfiguration());\n         final File dir = new File(\"target/\" + UUID.randomUUID().toString());\n         try {\n-            final IndexWriter writer1 = mgr.borrowIndexWriter(dir);\n-            final IndexWriter writer2 = mgr.borrowIndexWriter(dir);\n+            final EventIndexWriter writer1 = mgr.borrowIndexWriter(dir);\n+            final EventIndexWriter writer2 = mgr.borrowIndexWriter(dir);\n \n             final Document doc1 = new Document();\n             doc1.add(new StringField(\"id\", \"1\", Store.YES));\n \n             final Document doc2 = new Document();\n             doc2.add(new StringField(\"id\", \"2\", Store.YES));\n \n-            writer1.addDocument(doc1);\n-            writer2.addDocument(doc2);\n-            mgr.returnIndexWriter(dir, writer2);\n-            mgr.returnIndexWriter(dir, writer1);\n+            writer1.index(doc1, 1000);\n+            writer2.index(doc2, 1000);\n+            mgr.returnIndexWriter(writer2);\n+            mgr.returnIndexWriter(writer1);\n \n-            final IndexSearcher searcher = mgr.borrowIndexSearcher(dir);\n-            final TopDocs topDocs = searcher.search(new MatchAllDocsQuery(), 2);\n+            final EventIndexSearcher searcher = mgr.borrowIndexSearcher(dir);\n+            final TopDocs topDocs = searcher.getIndexSearcher().search(new MatchAllDocsQuery(), 2);\n             assertEquals(2, topDocs.totalHits);\n-            mgr.returnIndexSearcher(dir, searcher);\n+            mgr.returnIndexSearcher(searcher);\n         } finally {\n             FileUtils.deleteFile(dir, true);\n         }\n     }\n \n+    @Test\n+    public void testWriterCloseIfPreviouslyMarkedCloseable() throws IOException {\n+        final AtomicInteger closeCount = new AtomicInteger(0);\n+\n+        final SimpleIndexManager mgr = new SimpleIndexManager(new RepositoryConfiguration()) {\n+            @Override\n+            protected void close(IndexWriterCount count) throws IOException {\n+                closeCount.incrementAndGet();\n+            }\n+        };\n+\n+        final File dir = new File(\"target/\" + UUID.randomUUID().toString());\n+\n+        final EventIndexWriter writer1 = mgr.borrowIndexWriter(dir);\n+        final EventIndexWriter writer2 = mgr.borrowIndexWriter(dir);\n+        assertTrue(writer1 == writer2);\n+\n+        mgr.returnIndexWriter(writer1, true, true);\n+        assertEquals(0, closeCount.get());\n+\n+        final EventIndexWriter[] writers = new EventIndexWriter[10];\n+        for (int i = 0; i < writers.length; i++) {\n+            writers[i] = mgr.borrowIndexWriter(dir);\n+            assertTrue(writers[i] == writer1);\n+        }\n+\n+        for (int i = 0; i < writers.length; i++) {\n+            mgr.returnIndexWriter(writers[i], true, false);\n+            assertEquals(0, closeCount.get());\n+            assertEquals(1, mgr.getWriterCount());\n+        }\n+\n+        // this should close the index writer even though 'false' is passed in\n+        // because the previous call marked the writer as closeable and this is\n+        // the last reference to the writer.\n+        mgr.returnIndexWriter(writer2, false, false);\n+        assertEquals(1, closeCount.get());\n+        assertEquals(0, mgr.getWriterCount());\n+    }\n+\n+    @Test\n+    public void testWriterCloseIfOnlyUser() throws IOException {\n+        final AtomicInteger closeCount = new AtomicInteger(0);\n+\n+        final SimpleIndexManager mgr = new SimpleIndexManager(new RepositoryConfiguration()) {\n+            @Override\n+            protected void close(IndexWriterCount count) throws IOException {\n+                closeCount.incrementAndGet();\n+            }\n+        };\n+\n+        final File dir = new File(\"target/\" + UUID.randomUUID().toString());\n+\n+        final EventIndexWriter writer = mgr.borrowIndexWriter(dir);\n+        mgr.returnIndexWriter(writer, true, true);\n+        assertEquals(1, closeCount.get());\n+    }\n+\n+    @Test\n+    public void testWriterLeftOpenIfNotCloseable() throws IOException {\n+        final AtomicInteger closeCount = new AtomicInteger(0);\n+\n+        final SimpleIndexManager mgr = new SimpleIndexManager(new RepositoryConfiguration()) {\n+            @Override\n+            protected void close(IndexWriterCount count) throws IOException {\n+                closeCount.incrementAndGet();\n+            }\n+        };\n+\n+        final File dir = new File(\"target/\" + UUID.randomUUID().toString());\n+\n+        final EventIndexWriter writer = mgr.borrowIndexWriter(dir);\n+        mgr.returnIndexWriter(writer, true, false);\n+        assertEquals(0, closeCount.get());\n+    }\n+\n }",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/lucene/TestSimpleIndexManager.java",
                "sha": "05369ca39b005a3518e10fc4b1c046498299c18f",
                "status": "modified"
            },
            {
                "additions": 155,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/ArrayListEventStore.java",
                "changes": 155,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/ArrayListEventStore.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/ArrayListEventStore.java",
                "patch": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.store;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.authorization.EventAuthorizer;\n+import org.apache.nifi.provenance.authorization.EventTransformer;\n+import org.apache.nifi.provenance.index.EventIndex;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+public class ArrayListEventStore implements EventStore {\n+    private static final Logger logger = LoggerFactory.getLogger(ArrayListEventStore.class);\n+\n+    private final List<ProvenanceEventRecord> events = new ArrayList<>();\n+    private final AtomicLong idGenerator = new AtomicLong(0L);\n+\n+    @Override\n+    public void close() throws IOException {\n+    }\n+\n+    @Override\n+    public void initialize() throws IOException {\n+    }\n+\n+    public StorageResult addEvent(final ProvenanceEventRecord event) {\n+        return addEvents(Collections.singleton(event));\n+    }\n+\n+    @Override\n+    public synchronized StorageResult addEvents(Iterable<ProvenanceEventRecord> events) {\n+        final Map<ProvenanceEventRecord, StorageSummary> storageLocations = new HashMap<>();\n+\n+        for (final ProvenanceEventRecord event : events) {\n+            this.events.add(event);\n+\n+            final StorageSummary storageSummary = new StorageSummary(idGenerator.getAndIncrement(), \"location\", \"1\", 1, 0L, 0L);\n+            storageLocations.put(event, storageSummary);\n+        }\n+\n+        return new StorageResult() {\n+            @Override\n+            public Map<ProvenanceEventRecord, StorageSummary> getStorageLocations() {\n+                return storageLocations;\n+            }\n+\n+            @Override\n+            public boolean triggeredRollover() {\n+                return false;\n+            }\n+\n+            @Override\n+            public Integer getEventsRolledOver() {\n+                return null;\n+            }\n+        };\n+    }\n+\n+    @Override\n+    public long getSize() throws IOException {\n+        return 0;\n+    }\n+\n+    @Override\n+    public long getMaxEventId() {\n+        return idGenerator.get() - 1;\n+    }\n+\n+    @Override\n+    public synchronized Optional<ProvenanceEventRecord> getEvent(long id) throws IOException {\n+        if (events.size() <= id) {\n+            return Optional.empty();\n+        }\n+\n+        return Optional.ofNullable(events.get((int) id));\n+    }\n+\n+    @Override\n+    public List<ProvenanceEventRecord> getEvents(long firstRecordId, int maxResults) throws IOException {\n+        return getEvents(firstRecordId, maxResults, EventAuthorizer.GRANT_ALL, EventTransformer.EMPTY_TRANSFORMER);\n+    }\n+\n+    @Override\n+    public List<ProvenanceEventRecord> getEvents(long firstRecordId, int maxResults, EventAuthorizer authorizer, EventTransformer transformer) throws IOException {\n+        final List<ProvenanceEventRecord> events = new ArrayList<>();\n+        for (int i = 0; i < maxResults; i++) {\n+            final Optional<ProvenanceEventRecord> eventOption = getEvent(firstRecordId + i);\n+            if (!eventOption.isPresent()) {\n+                break;\n+            }\n+\n+            events.add(eventOption.get());\n+        }\n+\n+        return events;\n+    }\n+\n+    @Override\n+    public List<ProvenanceEventRecord> getEvents(final List<Long> eventIds, final EventAuthorizer authorizer, final EventTransformer transformer) {\n+        final List<ProvenanceEventRecord> events = new ArrayList<>();\n+        for (final Long eventId : eventIds) {\n+            final Optional<ProvenanceEventRecord> eventOption;\n+            try {\n+                eventOption = getEvent(eventId);\n+            } catch (final Exception e) {\n+                logger.warn(\"Failed to retrieve event with ID \" + eventId, e);\n+                continue;\n+            }\n+\n+            if (!eventOption.isPresent()) {\n+                continue;\n+            }\n+\n+            if (authorizer.isAuthorized(eventOption.get())) {\n+                events.add(eventOption.get());\n+            } else {\n+                final Optional<ProvenanceEventRecord> transformedOption = transformer.transform(eventOption.get());\n+                if (transformedOption.isPresent()) {\n+                    events.add(transformedOption.get());\n+                }\n+            }\n+        }\n+\n+        return events;\n+    }\n+\n+    @Override\n+    public void reindexLatestEvents(EventIndex eventIndex) {\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/ArrayListEventStore.java",
                "sha": "94a36996168652759dcb33588a8d8dc2c7dd7120",
                "status": "added"
            },
            {
                "additions": 240,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/TestEventFileManager.java",
                "changes": 240,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/TestEventFileManager.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/TestEventFileManager.java",
                "patch": "@@ -0,0 +1,240 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.store;\n+\n+import static org.junit.Assert.assertTrue;\n+\n+import java.io.File;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import org.junit.Test;\n+\n+public class TestEventFileManager {\n+\n+    @Test(timeout = 5000)\n+    public void testTwoWriteLocks() throws InterruptedException {\n+        final EventFileManager fileManager = new EventFileManager();\n+        final File f1 = new File(\"1.prov\");\n+        final File gz = new File(\"1.prov.gz\");\n+\n+        final AtomicBoolean obtained = new AtomicBoolean(false);\n+\n+        final Thread t1 = new Thread(new Runnable() {\n+            @Override\n+            public void run() {\n+                fileManager.obtainWriteLock(f1);\n+\n+                synchronized (obtained) {\n+                    obtained.set(true);\n+                    obtained.notify();\n+                }\n+\n+                try {\n+                    Thread.sleep(500L);\n+                } catch (InterruptedException e) {\n+                }\n+                fileManager.releaseWriteLock(f1);\n+            }\n+        });\n+\n+        t1.start();\n+\n+        final Thread t2 = new Thread(new Runnable() {\n+            @Override\n+            public void run() {\n+                synchronized (obtained) {\n+                    while (!obtained.get()) {\n+                        try {\n+                            obtained.wait();\n+                        } catch (InterruptedException e) {\n+                        }\n+                    }\n+                }\n+\n+                fileManager.obtainWriteLock(gz);\n+                fileManager.releaseWriteLock(gz);\n+            }\n+        });\n+\n+        final long start = System.nanoTime();\n+        t2.start();\n+        t2.join();\n+        final long nanos = System.nanoTime() - start;\n+        assertTrue(nanos > TimeUnit.MILLISECONDS.toNanos(300L));\n+    }\n+\n+\n+    @Test(timeout = 5000)\n+    public void testTwoReadLocks() throws InterruptedException {\n+        final EventFileManager fileManager = new EventFileManager();\n+        final File f1 = new File(\"1.prov\");\n+        final File gz = new File(\"1.prov.gz\");\n+\n+        final AtomicBoolean obtained = new AtomicBoolean(false);\n+\n+        final Thread t1 = new Thread(new Runnable() {\n+            @Override\n+            public void run() {\n+                fileManager.obtainReadLock(f1);\n+\n+                synchronized (obtained) {\n+                    obtained.set(true);\n+                    obtained.notify();\n+                }\n+\n+                try {\n+                    Thread.sleep(100000L);\n+                } catch (InterruptedException e) {\n+                }\n+                fileManager.releaseReadLock(f1);\n+            }\n+        });\n+\n+        t1.start();\n+\n+        final Thread t2 = new Thread(new Runnable() {\n+            @Override\n+            public void run() {\n+                synchronized (obtained) {\n+                    while (!obtained.get()) {\n+                        try {\n+                            obtained.wait();\n+                        } catch (InterruptedException e) {\n+                        }\n+                    }\n+                }\n+\n+                fileManager.obtainReadLock(gz);\n+                fileManager.releaseReadLock(gz);\n+            }\n+        });\n+\n+        final long start = System.nanoTime();\n+        t2.start();\n+        t2.join();\n+        final long nanos = System.nanoTime() - start;\n+        assertTrue(nanos < TimeUnit.MILLISECONDS.toNanos(500L));\n+    }\n+\n+\n+    @Test(timeout = 5000)\n+    public void testWriteThenRead() throws InterruptedException {\n+        final EventFileManager fileManager = new EventFileManager();\n+        final File f1 = new File(\"1.prov\");\n+        final File gz = new File(\"1.prov.gz\");\n+\n+        final AtomicBoolean obtained = new AtomicBoolean(false);\n+\n+        final Thread t1 = new Thread(new Runnable() {\n+            @Override\n+            public void run() {\n+                fileManager.obtainWriteLock(f1);\n+\n+                synchronized (obtained) {\n+                    obtained.set(true);\n+                    obtained.notify();\n+                }\n+\n+                try {\n+                    Thread.sleep(500L);\n+                } catch (InterruptedException e) {\n+                }\n+                fileManager.releaseWriteLock(f1);\n+            }\n+        });\n+\n+        t1.start();\n+\n+        final Thread t2 = new Thread(new Runnable() {\n+            @Override\n+            public void run() {\n+                synchronized (obtained) {\n+                    while (!obtained.get()) {\n+                        try {\n+                            obtained.wait();\n+                        } catch (InterruptedException e) {\n+                        }\n+                    }\n+                }\n+\n+                fileManager.obtainReadLock(gz);\n+                fileManager.releaseReadLock(gz);\n+            }\n+        });\n+\n+        final long start = System.nanoTime();\n+        t2.start();\n+        t2.join();\n+        final long nanos = System.nanoTime() - start;\n+        assertTrue(nanos > TimeUnit.MILLISECONDS.toNanos(300L));\n+    }\n+\n+\n+    @Test(timeout = 5000)\n+    public void testReadThenWrite() throws InterruptedException {\n+        final EventFileManager fileManager = new EventFileManager();\n+        final File f1 = new File(\"1.prov\");\n+        final File gz = new File(\"1.prov.gz\");\n+\n+        final AtomicBoolean obtained = new AtomicBoolean(false);\n+\n+        final Thread t1 = new Thread(new Runnable() {\n+            @Override\n+            public void run() {\n+                fileManager.obtainReadLock(f1);\n+\n+                synchronized (obtained) {\n+                    obtained.set(true);\n+                    obtained.notify();\n+                }\n+\n+                try {\n+                    Thread.sleep(500L);\n+                } catch (InterruptedException e) {\n+                }\n+                fileManager.releaseReadLock(f1);\n+            }\n+        });\n+\n+        t1.start();\n+\n+        final Thread t2 = new Thread(new Runnable() {\n+            @Override\n+            public void run() {\n+                synchronized (obtained) {\n+                    while (!obtained.get()) {\n+                        try {\n+                            obtained.wait();\n+                        } catch (InterruptedException e) {\n+                        }\n+                    }\n+                }\n+\n+                fileManager.obtainWriteLock(gz);\n+                fileManager.releaseWriteLock(gz);\n+            }\n+        });\n+\n+        final long start = System.nanoTime();\n+        t2.start();\n+        t2.join();\n+        final long nanos = System.nanoTime() - start;\n+        assertTrue(nanos > TimeUnit.MILLISECONDS.toNanos(300L));\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/TestEventFileManager.java",
                "sha": "42b8be2f0b729c13b600ef277354a2b938b04e67",
                "status": "added"
            },
            {
                "additions": 468,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/TestPartitionedWriteAheadEventStore.java",
                "changes": 468,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/TestPartitionedWriteAheadEventStore.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/TestPartitionedWriteAheadEventStore.java",
                "patch": "@@ -0,0 +1,468 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.store;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.assertFalse;\n+import static org.junit.Assert.assertNotNull;\n+import static org.junit.Assert.assertTrue;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import org.apache.nifi.authorization.AccessDeniedException;\n+import org.apache.nifi.events.EventReporter;\n+import org.apache.nifi.provenance.EventIdFirstSchemaRecordWriter;\n+import org.apache.nifi.provenance.IdentifierLookup;\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.ProvenanceEventType;\n+import org.apache.nifi.provenance.RepositoryConfiguration;\n+import org.apache.nifi.provenance.StandardProvenanceEventRecord;\n+import org.apache.nifi.provenance.authorization.EventAuthorizer;\n+import org.apache.nifi.provenance.authorization.EventTransformer;\n+import org.apache.nifi.provenance.serialization.RecordReaders;\n+import org.apache.nifi.provenance.serialization.RecordWriters;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n+import org.apache.nifi.provenance.toc.StandardTocWriter;\n+import org.apache.nifi.provenance.toc.TocUtil;\n+import org.apache.nifi.provenance.toc.TocWriter;\n+import org.junit.Before;\n+import org.junit.Ignore;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.TestName;\n+\n+public class TestPartitionedWriteAheadEventStore {\n+    private static final RecordWriterFactory writerFactory = (file, idGen, compress, createToc) -> RecordWriters.newSchemaRecordWriter(file, idGen, compress, createToc);\n+    private static final RecordReaderFactory readerFactory = (file, logs, maxChars) -> RecordReaders.newRecordReader(file, logs, maxChars);\n+\n+    private final AtomicLong idGenerator = new AtomicLong(0L);\n+\n+    @Rule\n+    public TestName testName = new TestName();\n+\n+    @Before\n+    public void resetIds() {\n+        idGenerator.set(0L);\n+    }\n+\n+\n+    @Test\n+    @Ignore\n+    public void testPerformanceOfAccessingEvents() throws Exception {\n+        final RecordWriterFactory recordWriterFactory = (file, idGenerator, compressed, createToc) -> {\n+            final TocWriter tocWriter = createToc ? new StandardTocWriter(TocUtil.getTocFile(file), false, false) : null;\n+            return new EventIdFirstSchemaRecordWriter(file, idGenerator, tocWriter, compressed, 1024 * 1024, IdentifierLookup.EMPTY);\n+        };\n+\n+        final RecordReaderFactory recordReaderFactory = (file, logs, maxChars) -> RecordReaders.newRecordReader(file, logs, maxChars);\n+        final PartitionedWriteAheadEventStore store = new PartitionedWriteAheadEventStore(createConfig(),\n+            recordWriterFactory, recordReaderFactory, EventReporter.NO_OP, new EventFileManager());\n+        store.initialize();\n+\n+        assertEquals(-1, store.getMaxEventId());\n+        for (int i = 0; i < 100_000; i++) {\n+            final ProvenanceEventRecord event1 = createEvent();\n+            store.addEvents(Collections.singleton(event1));\n+        }\n+\n+        final List<Long> eventIdList = Arrays.asList(4L, 80L, 1024L, 40_000L, 80_000L, 99_000L);\n+\n+        while (true) {\n+            for (int i = 0; i < 100; i++) {\n+                time(() -> store.getEvents(eventIdList, EventAuthorizer.GRANT_ALL, EventTransformer.EMPTY_TRANSFORMER), \"Fetch Events\");\n+            }\n+\n+            Thread.sleep(1000L);\n+        }\n+    }\n+\n+    private void time(final Callable<?> task, final String taskDescription) throws Exception {\n+        final long start = System.nanoTime();\n+        task.call();\n+        final long nanos = System.nanoTime() - start;\n+        final long millis = TimeUnit.NANOSECONDS.toMillis(nanos);\n+        System.out.println(\"Took \" + millis + \" ms to \" + taskDescription);\n+    }\n+\n+    @Test\n+    public void testSingleWriteThenRead() throws IOException {\n+        final PartitionedWriteAheadEventStore store = new PartitionedWriteAheadEventStore(createConfig(), writerFactory, readerFactory, EventReporter.NO_OP, new EventFileManager());\n+        store.initialize();\n+\n+        assertEquals(-1, store.getMaxEventId());\n+        final ProvenanceEventRecord event1 = createEvent();\n+        final StorageResult result = store.addEvents(Collections.singleton(event1));\n+\n+        final StorageSummary summary = result.getStorageLocations().values().iterator().next();\n+        final long eventId = summary.getEventId();\n+        final ProvenanceEventRecord eventWithId = addId(event1, eventId);\n+\n+        assertEquals(0, store.getMaxEventId());\n+\n+        final ProvenanceEventRecord read = store.getEvent(eventId).get();\n+        assertEquals(eventWithId, read);\n+    }\n+\n+    @Test\n+    public void testMultipleWritesThenReads() throws IOException {\n+        final PartitionedWriteAheadEventStore store = new PartitionedWriteAheadEventStore(createConfig(), writerFactory, readerFactory, EventReporter.NO_OP, new EventFileManager());\n+        store.initialize();\n+        assertEquals(-1, store.getMaxEventId());\n+\n+        final int numEvents = 20;\n+        final List<ProvenanceEventRecord> events = new ArrayList<>(numEvents);\n+        for (int i = 0; i < numEvents; i++) {\n+            final ProvenanceEventRecord event = createEvent();\n+            store.addEvents(Collections.singleton(event));\n+            assertEquals(i, store.getMaxEventId());\n+\n+            events.add(event);\n+        }\n+\n+        for (int i = 0; i < numEvents; i++) {\n+            final ProvenanceEventRecord read = store.getEvent(i).get();\n+            assertEquals(events.get(i), read);\n+        }\n+    }\n+\n+\n+    @Test()\n+    public void testMultipleWritesThenGetAllInSingleRead() throws IOException {\n+        final PartitionedWriteAheadEventStore store = new PartitionedWriteAheadEventStore(createConfig(), writerFactory, readerFactory, EventReporter.NO_OP, new EventFileManager());\n+        store.initialize();\n+        assertEquals(-1, store.getMaxEventId());\n+\n+        final int numEvents = 20;\n+        final List<ProvenanceEventRecord> events = new ArrayList<>(numEvents);\n+        for (int i = 0; i < numEvents; i++) {\n+            final ProvenanceEventRecord event = createEvent();\n+            store.addEvents(Collections.singleton(event));\n+            assertEquals(i, store.getMaxEventId());\n+\n+            events.add(event);\n+        }\n+\n+        List<ProvenanceEventRecord> eventsRead = store.getEvents(0L, numEvents, null, EventTransformer.EMPTY_TRANSFORMER);\n+        assertNotNull(eventsRead);\n+\n+        assertEquals(numEvents, eventsRead.size());\n+        for (int i = 0; i < numEvents; i++) {\n+            final ProvenanceEventRecord read = eventsRead.get(i);\n+            assertEquals(events.get(i), read);\n+        }\n+\n+        eventsRead = store.getEvents(-1000, 1000, null, EventTransformer.EMPTY_TRANSFORMER);\n+        assertNotNull(eventsRead);\n+        assertTrue(eventsRead.isEmpty());\n+\n+        eventsRead = store.getEvents(10, 0, null, EventTransformer.EMPTY_TRANSFORMER);\n+        assertNotNull(eventsRead);\n+        assertTrue(eventsRead.isEmpty());\n+\n+        eventsRead = store.getEvents(10, 1, null, EventTransformer.EMPTY_TRANSFORMER);\n+        assertNotNull(eventsRead);\n+        assertFalse(eventsRead.isEmpty());\n+        assertEquals(1, eventsRead.size());\n+        assertEquals(events.get(10), eventsRead.get(0));\n+\n+        eventsRead = store.getEvents(20, 1000, null, EventTransformer.EMPTY_TRANSFORMER);\n+        assertNotNull(eventsRead);\n+        assertTrue(eventsRead.isEmpty());\n+    }\n+\n+    @Test\n+    public void testGetSize() throws IOException {\n+        final PartitionedWriteAheadEventStore store = new PartitionedWriteAheadEventStore(createConfig(), writerFactory, readerFactory, EventReporter.NO_OP, new EventFileManager());\n+        store.initialize();\n+\n+        long storeSize = 0L;\n+        final int numEvents = 20;\n+        for (int i = 0; i < numEvents; i++) {\n+            final ProvenanceEventRecord event = createEvent();\n+            store.addEvents(Collections.singleton(event));\n+            final long newSize = store.getSize();\n+            assertTrue(newSize > storeSize);\n+            storeSize = newSize;\n+        }\n+    }\n+\n+    @Test\n+    public void testMaxEventIdRestored() throws IOException {\n+        final RepositoryConfiguration config = createConfig();\n+        final PartitionedWriteAheadEventStore store = new PartitionedWriteAheadEventStore(config, writerFactory, readerFactory, EventReporter.NO_OP, new EventFileManager());\n+        store.initialize();\n+\n+        final int numEvents = 20;\n+        for (int i = 0; i < numEvents; i++) {\n+            final ProvenanceEventRecord event = createEvent();\n+            store.addEvents(Collections.singleton(event));\n+        }\n+\n+        assertEquals(19, store.getMaxEventId());\n+        store.close();\n+\n+        final PartitionedWriteAheadEventStore recoveredStore = new PartitionedWriteAheadEventStore(config, writerFactory, readerFactory, EventReporter.NO_OP, new EventFileManager());\n+        recoveredStore.initialize();\n+        assertEquals(19, recoveredStore.getMaxEventId());\n+    }\n+\n+    @Test\n+    public void testGetEvent() throws IOException {\n+        final RepositoryConfiguration config = createConfig();\n+        final PartitionedWriteAheadEventStore store = new PartitionedWriteAheadEventStore(config, writerFactory, readerFactory, EventReporter.NO_OP, new EventFileManager());\n+        store.initialize();\n+\n+        final int numEvents = 20;\n+        final List<ProvenanceEventRecord> events = new ArrayList<>(numEvents);\n+        for (int i = 0; i < numEvents; i++) {\n+            final ProvenanceEventRecord event = createEvent();\n+            store.addEvents(Collections.singleton(event));\n+            events.add(event);\n+        }\n+\n+        // Ensure that each event is retrieved successfully.\n+        for (int i = 0; i < numEvents; i++) {\n+            final ProvenanceEventRecord event = store.getEvent(i).get();\n+            assertEquals(events.get(i), event);\n+        }\n+\n+        assertFalse(store.getEvent(-1L).isPresent());\n+        assertFalse(store.getEvent(20L).isPresent());\n+    }\n+\n+    @Test\n+    public void testGetEventsWithMinIdAndCount() throws IOException {\n+        final RepositoryConfiguration config = createConfig();\n+        config.setMaxEventFileCount(100);\n+        final PartitionedWriteAheadEventStore store = new PartitionedWriteAheadEventStore(config, writerFactory, readerFactory, EventReporter.NO_OP, new EventFileManager());\n+        store.initialize();\n+\n+        final int numEvents = 50_000;\n+        final List<ProvenanceEventRecord> events = new ArrayList<>(numEvents);\n+        for (int i = 0; i < numEvents; i++) {\n+            final ProvenanceEventRecord event = createEvent();\n+            store.addEvents(Collections.singleton(event));\n+            if (i < 1000) {\n+                events.add(event);\n+            }\n+        }\n+\n+        assertTrue(store.getEvents(-1000L, 1000).isEmpty());\n+        assertEquals(events, store.getEvents(0, events.size()));\n+        assertEquals(events, store.getEvents(-30, events.size()));\n+        assertEquals(events.subList(10, events.size()), store.getEvents(10L, events.size() - 10));\n+        assertTrue(store.getEvents(numEvents, 100).isEmpty());\n+    }\n+\n+    @Test\n+    public void testGetEventsWithMinIdAndCountWithAuthorizer() throws IOException {\n+        final RepositoryConfiguration config = createConfig();\n+        final PartitionedWriteAheadEventStore store = new PartitionedWriteAheadEventStore(config, writerFactory, readerFactory, EventReporter.NO_OP, new EventFileManager());\n+        store.initialize();\n+\n+        final int numEvents = 20;\n+        final List<ProvenanceEventRecord> events = new ArrayList<>(numEvents);\n+        for (int i = 0; i < numEvents; i++) {\n+            final ProvenanceEventRecord event = createEvent();\n+            store.addEvents(Collections.singleton(event));\n+            events.add(event);\n+        }\n+\n+        final EventAuthorizer allowEventNumberedEventIds = new EventAuthorizer() {\n+            @Override\n+            public boolean isAuthorized(final ProvenanceEventRecord event) {\n+                return event.getEventId() % 2 == 0L;\n+            }\n+\n+            @Override\n+            public void authorize(ProvenanceEventRecord event) throws AccessDeniedException {\n+                if (!isAuthorized(event)) {\n+                    throw new AccessDeniedException();\n+                }\n+            }\n+        };\n+\n+        final List<ProvenanceEventRecord> storedEvents = store.getEvents(0, 20, allowEventNumberedEventIds, EventTransformer.EMPTY_TRANSFORMER);\n+        assertEquals(numEvents / 2, storedEvents.size());\n+        for (int i = 0; i < storedEvents.size(); i++) {\n+            assertEquals(events.get(i * 2), storedEvents.get(i));\n+        }\n+    }\n+\n+\n+    @Test\n+    public void testGetEventsWithStartOffsetAndCountWithNothingAuthorized() throws IOException {\n+        final RepositoryConfiguration config = createConfig();\n+        final PartitionedWriteAheadEventStore store = new PartitionedWriteAheadEventStore(config, writerFactory, readerFactory, EventReporter.NO_OP, new EventFileManager());\n+        store.initialize();\n+\n+        final int numEvents = 20;\n+        final List<ProvenanceEventRecord> events = new ArrayList<>(numEvents);\n+        for (int i = 0; i < numEvents; i++) {\n+            final ProvenanceEventRecord event = createEvent();\n+            store.addEvents(Collections.singleton(event));\n+            events.add(event);\n+        }\n+\n+        final EventAuthorizer allowEventNumberedEventIds = EventAuthorizer.DENY_ALL;\n+        final List<ProvenanceEventRecord> storedEvents = store.getEvents(0, 20, allowEventNumberedEventIds, EventTransformer.EMPTY_TRANSFORMER);\n+        assertTrue(storedEvents.isEmpty());\n+    }\n+\n+    @Test\n+    public void testGetSpecificEventIds() throws IOException {\n+        final RepositoryConfiguration config = createConfig();\n+        final PartitionedWriteAheadEventStore store = new PartitionedWriteAheadEventStore(config, writerFactory, readerFactory, EventReporter.NO_OP, new EventFileManager());\n+        store.initialize();\n+\n+        final int numEvents = 20;\n+        final List<ProvenanceEventRecord> events = new ArrayList<>(numEvents);\n+        for (int i = 0; i < numEvents; i++) {\n+            final ProvenanceEventRecord event = createEvent();\n+            store.addEvents(Collections.singleton(event));\n+            events.add(event);\n+        }\n+\n+        final EventAuthorizer allowEvenNumberedEventIds = new EventAuthorizer() {\n+            @Override\n+            public boolean isAuthorized(final ProvenanceEventRecord event) {\n+                return event.getEventId() % 2 == 0L;\n+            }\n+\n+            @Override\n+            public void authorize(ProvenanceEventRecord event) throws AccessDeniedException {\n+                if (!isAuthorized(event)) {\n+                    throw new AccessDeniedException();\n+                }\n+            }\n+        };\n+\n+        final List<Long> evenEventIds = new ArrayList<>();\n+        final List<Long> oddEventIds = new ArrayList<>();\n+        final List<Long> allEventIds = new ArrayList<>();\n+        for (int i = 0; i < 20; i++) {\n+            final Long id = Long.valueOf(i);\n+            allEventIds.add(id);\n+\n+            if (i % 2 == 0) {\n+                evenEventIds.add(id);\n+            } else {\n+                oddEventIds.add(id);\n+            }\n+        }\n+\n+        final List<ProvenanceEventRecord> storedEvents = store.getEvents(evenEventIds, allowEvenNumberedEventIds, EventTransformer.EMPTY_TRANSFORMER);\n+        assertEquals(numEvents / 2, storedEvents.size());\n+        for (int i = 0; i < storedEvents.size(); i++) {\n+            assertEquals(events.get(i * 2), storedEvents.get(i));\n+        }\n+\n+        assertTrue(store.getEvents(oddEventIds, allowEvenNumberedEventIds, EventTransformer.EMPTY_TRANSFORMER).isEmpty());\n+\n+        final List<ProvenanceEventRecord> allStoredEvents = store.getEvents(allEventIds, EventAuthorizer.GRANT_ALL, EventTransformer.EMPTY_TRANSFORMER);\n+        assertEquals(events, allStoredEvents);\n+    }\n+\n+\n+    @Test\n+    public void testWriteAfterRecoveringRepo() throws IOException {\n+        final RepositoryConfiguration config = createConfig();\n+        final PartitionedWriteAheadEventStore store = new PartitionedWriteAheadEventStore(config, writerFactory, readerFactory, EventReporter.NO_OP, new EventFileManager());\n+        store.initialize();\n+\n+        for (int i = 0; i < 4; i++) {\n+            store.addEvents(Collections.singleton(createEvent()));\n+        }\n+\n+        store.close();\n+\n+        final PartitionedWriteAheadEventStore recoveredStore = new PartitionedWriteAheadEventStore(config, writerFactory, readerFactory, EventReporter.NO_OP, new EventFileManager());\n+        recoveredStore.initialize();\n+\n+        List<ProvenanceEventRecord> recoveredEvents = recoveredStore.getEvents(0, 10);\n+        assertEquals(4, recoveredEvents.size());\n+\n+        // ensure that we can still write to the store\n+        for (int i = 0; i < 4; i++) {\n+            recoveredStore.addEvents(Collections.singleton(createEvent()));\n+        }\n+\n+        recoveredEvents = recoveredStore.getEvents(0, 10);\n+        assertEquals(8, recoveredEvents.size());\n+\n+        for (int i = 0; i < 8; i++) {\n+            assertEquals(i, recoveredEvents.get(i).getEventId());\n+        }\n+    }\n+\n+\n+    private RepositoryConfiguration createConfig() {\n+        return createConfig(2);\n+    }\n+\n+    private RepositoryConfiguration createConfig(final int numStorageDirs) {\n+        final RepositoryConfiguration config = new RepositoryConfiguration();\n+        final String unitTestName = testName.getMethodName();\n+        final File storageDir = new File(\"target/storage/\" + unitTestName + \"/\" + UUID.randomUUID().toString());\n+\n+        for (int i = 1; i <= numStorageDirs; i++) {\n+            config.addStorageDirectory(String.valueOf(i), new File(storageDir, String.valueOf(i)));\n+        }\n+\n+        return config;\n+    }\n+\n+    private ProvenanceEventRecord addId(final ProvenanceEventRecord event, final long eventId) {\n+        return new StandardProvenanceEventRecord.Builder()\n+            .fromEvent(event)\n+            .setEventId(eventId)\n+            .build();\n+    }\n+\n+\n+    private ProvenanceEventRecord createEvent() {\n+        final String uuid = UUID.randomUUID().toString();\n+        final Map<String, String> previousAttributes = new HashMap<>();\n+        previousAttributes.put(\"uuid\", uuid);\n+        final Map<String, String> updatedAttributes = new HashMap<>();\n+        updatedAttributes.put(\"updated\", \"true\");\n+\n+        return new StandardProvenanceEventRecord.Builder()\n+            .setEventType(ProvenanceEventType.CONTENT_MODIFIED)\n+            .setAttributes(previousAttributes, updatedAttributes)\n+            .setComponentId(\"component-1\")\n+            .setComponentType(\"unit test\")\n+            .setEventTime(System.currentTimeMillis())\n+            .setFlowFileEntryDate(System.currentTimeMillis())\n+            .setFlowFileUUID(uuid)\n+            .setLineageStartDate(System.currentTimeMillis())\n+            .setCurrentContentClaim(\"container\", \"section\", \"unit-test-id\", 0L, 1024L)\n+            .build();\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/TestPartitionedWriteAheadEventStore.java",
                "sha": "7c5e43b564367ac0d26ea1a157907efb9afd9ea2",
                "status": "added"
            },
            {
                "additions": 111,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/TestWriteAheadStorePartition.java",
                "changes": 111,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/TestWriteAheadStorePartition.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/TestWriteAheadStorePartition.java",
                "patch": "@@ -0,0 +1,111 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.store;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.concurrent.atomic.AtomicLong;\n+import java.util.stream.Collectors;\n+\n+import org.apache.nifi.events.EventReporter;\n+import org.apache.nifi.provenance.EventIdFirstSchemaRecordWriter;\n+import org.apache.nifi.provenance.IdentifierLookup;\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.RepositoryConfiguration;\n+import org.apache.nifi.provenance.TestUtil;\n+import org.apache.nifi.provenance.index.EventIndex;\n+import org.apache.nifi.provenance.serialization.RecordReaders;\n+import org.apache.nifi.provenance.serialization.StorageSummary;\n+import org.apache.nifi.provenance.toc.StandardTocWriter;\n+import org.apache.nifi.provenance.toc.TocUtil;\n+import org.apache.nifi.provenance.toc.TocWriter;\n+import org.junit.Test;\n+import org.mockito.Mockito;\n+import org.mockito.invocation.InvocationOnMock;\n+import org.mockito.stubbing.Answer;\n+\n+public class TestWriteAheadStorePartition {\n+\n+    @Test\n+    @SuppressWarnings(\"unchecked\")\n+    public void testReindex() throws IOException {\n+        final RepositoryConfiguration repoConfig = createConfig(1, \"testReindex\");\n+        repoConfig.setMaxEventFileCount(5);\n+\n+        final String partitionName = repoConfig.getStorageDirectories().keySet().iterator().next();\n+        final File storageDirectory = repoConfig.getStorageDirectories().values().iterator().next();\n+\n+        final RecordWriterFactory recordWriterFactory = (file, idGenerator, compressed, createToc) -> {\n+            final TocWriter tocWriter = createToc ? new StandardTocWriter(TocUtil.getTocFile(file), false, false) : null;\n+            return new EventIdFirstSchemaRecordWriter(file, idGenerator, tocWriter, compressed, 32 * 1024, IdentifierLookup.EMPTY);\n+        };\n+\n+        final RecordReaderFactory recordReaderFactory = (file, logs, maxChars) -> RecordReaders.newRecordReader(file, logs, maxChars);\n+\n+        final WriteAheadStorePartition partition = new WriteAheadStorePartition(storageDirectory, partitionName, repoConfig, recordWriterFactory,\n+            recordReaderFactory, new LinkedBlockingQueue<>(), new AtomicLong(0L), EventReporter.NO_OP);\n+\n+        for (int i = 0; i < 100; i++) {\n+            partition.addEvents(Collections.singleton(TestUtil.createEvent()));\n+        }\n+\n+        final Map<ProvenanceEventRecord, StorageSummary> reindexedEvents = new HashMap<>();\n+        final EventIndex eventIndex = Mockito.mock(EventIndex.class);\n+        Mockito.doAnswer(new Answer<Object>() {\n+            @Override\n+            public Object answer(final InvocationOnMock invocation) throws Throwable {\n+                final Map<ProvenanceEventRecord, StorageSummary> events = invocation.getArgumentAt(0, Map.class);\n+                reindexedEvents.putAll(events);\n+                return null;\n+            }\n+        }).when(eventIndex).reindexEvents(Mockito.anyMap());\n+\n+        Mockito.doReturn(18L).when(eventIndex).getMinimumEventIdToReindex(\"1\");\n+        partition.reindexLatestEvents(eventIndex);\n+\n+        final List<Long> eventIdsReindexed = reindexedEvents.values().stream()\n+            .map(StorageSummary::getEventId)\n+            .sorted()\n+            .collect(Collectors.toList());\n+\n+        assertEquals(82, eventIdsReindexed.size());\n+        for (int i = 0; i < eventIdsReindexed.size(); i++) {\n+            assertEquals(18 + i, eventIdsReindexed.get(i).intValue());\n+        }\n+    }\n+\n+    private RepositoryConfiguration createConfig(final int numStorageDirs, final String testName) {\n+        final RepositoryConfiguration config = new RepositoryConfiguration();\n+        final File storageDir = new File(\"target/storage/\" + testName + \"/\" + UUID.randomUUID().toString());\n+\n+        for (int i = 1; i <= numStorageDirs; i++) {\n+            config.addStorageDirectory(String.valueOf(1), new File(storageDir, String.valueOf(i)));\n+        }\n+\n+        config.setJournalCount(4);\n+        return config;\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/TestWriteAheadStorePartition.java",
                "sha": "3879411e1a32fc7560c5ddfa4227982b8b2db918",
                "status": "added"
            },
            {
                "additions": 146,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/iterator/TestSelectiveRecordReaderEventIterator.java",
                "changes": 146,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/iterator/TestSelectiveRecordReaderEventIterator.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 0,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/iterator/TestSelectiveRecordReaderEventIterator.java",
                "patch": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.nifi.provenance.store.iterator;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.atomic.AtomicLong;\n+\n+import org.apache.nifi.provenance.EventIdFirstSchemaRecordWriter;\n+import org.apache.nifi.provenance.IdentifierLookup;\n+import org.apache.nifi.provenance.ProvenanceEventRecord;\n+import org.apache.nifi.provenance.TestUtil;\n+import org.apache.nifi.provenance.serialization.RecordReaders;\n+import org.apache.nifi.provenance.serialization.RecordWriter;\n+import org.apache.nifi.provenance.store.RecordReaderFactory;\n+import org.apache.nifi.provenance.toc.StandardTocWriter;\n+import org.apache.nifi.provenance.toc.TocUtil;\n+import org.apache.nifi.provenance.toc.TocWriter;\n+import org.junit.Assert;\n+import org.junit.Ignore;\n+import org.junit.Test;\n+\n+\n+public class TestSelectiveRecordReaderEventIterator {\n+\n+\n+    private RecordWriter createWriter(final File file, final TocWriter tocWriter, final boolean compressed, final int uncompressedBlockSize) throws IOException {\n+        return new EventIdFirstSchemaRecordWriter(file, new AtomicLong(0L), tocWriter, compressed, uncompressedBlockSize, IdentifierLookup.EMPTY);\n+    }\n+\n+    @Test\n+    public void testFilterUnneededFiles() {\n+        final File file1 = new File(\"1.prov\");\n+        final File file1000 = new File(\"1000.prov\");\n+        final File file2000 = new File(\"2000.prov\");\n+        final File file3000 = new File(\"3000.prov\");\n+\n+        // Filter out the first file.\n+        final List<File> files = new ArrayList<>();\n+        files.add(file1);\n+        files.add(file1000);\n+        files.add(file2000);\n+        files.add(file3000);\n+\n+        List<Long> eventIds = new ArrayList<>();\n+        eventIds.add(1048L);\n+        eventIds.add(2048L);\n+        eventIds.add(3048L);\n+\n+        List<File> filteredFiles = SelectiveRecordReaderEventIterator.filterUnneededFiles(files, eventIds);\n+        assertEquals(Arrays.asList(new File[] {file1000, file2000, file3000}), filteredFiles);\n+\n+        // Filter out file at end\n+        eventIds.clear();\n+        eventIds.add(1L);\n+        eventIds.add(1048L);\n+\n+        filteredFiles = SelectiveRecordReaderEventIterator.filterUnneededFiles(files, eventIds);\n+        assertEquals(Arrays.asList(new File[] {file1, file1000}), filteredFiles);\n+    }\n+\n+    @Test\n+    @Ignore(\"For local testing only. Runs indefinitely\")\n+    public void testPerformanceOfRandomAccessReads() throws Exception {\n+        final File dir = new File(\"target/storage/\" + UUID.randomUUID().toString());\n+        final File journalFile = new File(dir, \"/4.prov.gz\");\n+        final File tocFile = TocUtil.getTocFile(journalFile);\n+\n+        final int blockSize = 1024 * 32;\n+        try (final RecordWriter writer = createWriter(journalFile, new StandardTocWriter(tocFile, true, false), true, blockSize)) {\n+            writer.writeHeader(0L);\n+\n+            for (int i = 0; i < 100_000; i++) {\n+                writer.writeRecord(TestUtil.createEvent());\n+            }\n+        }\n+\n+        final Long[] eventIds = new Long[] {\n+            4L, 80L, 1024L, 1025L, 1026L, 1027L, 1028L, 1029L, 1030L, 40_000L, 80_000L, 99_000L\n+        };\n+\n+        final RecordReaderFactory readerFactory = (file, logs, maxChars) -> RecordReaders.newRecordReader(file, logs, maxChars);\n+\n+        final List<File> files = new ArrayList<>();\n+        files.add(new File(dir, \"0.prov\"));\n+        files.add(new File(dir, \"0.prov\"));\n+        files.add(new File(dir, \"1.prov\"));\n+        files.add(new File(dir, \"2.prov\"));\n+        files.add(new File(dir, \"3.prov\"));\n+        files.add(journalFile);\n+        files.add(new File(dir, \"100000000.prov\"));\n+\n+        boolean loopForever = true;\n+        while (loopForever) {\n+            final long start = System.nanoTime();\n+            for (int i = 0; i < 1000; i++) {\n+                final SelectiveRecordReaderEventIterator iterator = new SelectiveRecordReaderEventIterator(\n+                    Collections.singletonList(journalFile), readerFactory, Arrays.asList(eventIds), 32 * 1024);\n+\n+                for (final long id : eventIds) {\n+                    time(() -> {\n+                        return iterator.nextEvent().orElse(null);\n+                    }, id);\n+                }\n+            }\n+\n+            final long ms = TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start);\n+            System.out.println(ms + \" ms total\");\n+        }\n+    }\n+\n+    private void time(final Callable<ProvenanceEventRecord> task, final long id) throws Exception {\n+        final long start = System.nanoTime();\n+        final ProvenanceEventRecord event = task.call();\n+        Assert.assertNotNull(event);\n+        Assert.assertEquals(id, event.getEventId());\n+        //        System.out.println(event);\n+        final long nanos = System.nanoTime() - start;\n+        final long millis = TimeUnit.NANOSECONDS.toMillis(nanos);\n+        //        System.out.println(\"Took \" + millis + \" ms to \" + taskDescription);\n+    }\n+}",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-persistent-provenance-repository/src/test/java/org/apache/nifi/provenance/store/iterator/TestSelectiveRecordReaderEventIterator.java",
                "sha": "0089f6191d6e37c09cfe1d6c273d6e9c3d6e3aef",
                "status": "added"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/nifi/blob/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-volatile-provenance-repository/src/main/java/org/apache/nifi/provenance/VolatileProvenanceRepository.java",
                "changes": 9,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-volatile-provenance-repository/src/main/java/org/apache/nifi/provenance/VolatileProvenanceRepository.java?ref=96ed405d708894ee5400ebbdbf335325219faa09",
                "deletions": 4,
                "filename": "nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-volatile-provenance-repository/src/main/java/org/apache/nifi/provenance/VolatileProvenanceRepository.java",
                "patch": "@@ -124,7 +124,8 @@ public Thread newThread(final Runnable r) {\n     }\n \n     @Override\n-    public void initialize(final EventReporter eventReporter, final Authorizer authorizer, final ProvenanceAuthorizableFactory resourceFactory) throws IOException {\n+    public void initialize(final EventReporter eventReporter, final Authorizer authorizer, final ProvenanceAuthorizableFactory resourceFactory,\n+        final IdentifierLookup idLookup) throws IOException {\n         if (initialized.getAndSet(true)) {\n             return;\n         }\n@@ -542,7 +543,7 @@ public ComputeLineageSubmission submitExpandParents(final long eventId, final Ni\n         if (event == null) {\n             final AsyncLineageSubmission submission = new AsyncLineageSubmission(LineageComputationType.EXPAND_PARENTS, eventId, Collections.<String>emptyList(), 1, userId);\n             lineageSubmissionMap.put(submission.getLineageIdentifier(), submission);\n-            submission.getResult().update(Collections.<ProvenanceEventRecord>emptyList());\n+            submission.getResult().update(Collections.<ProvenanceEventRecord> emptyList(), 0L);\n             return submission;\n         }\n \n@@ -573,7 +574,7 @@ public ComputeLineageSubmission submitExpandChildren(final long eventId, final N\n         if (event == null) {\n             final AsyncLineageSubmission submission = new AsyncLineageSubmission(LineageComputationType.EXPAND_CHILDREN, eventId, Collections.<String>emptyList(), 1, userId);\n             lineageSubmissionMap.put(submission.getLineageIdentifier(), submission);\n-            submission.getResult().update(Collections.<ProvenanceEventRecord>emptyList());\n+            submission.getResult().update(Collections.<ProvenanceEventRecord> emptyList(), 0L);\n             return submission;\n         }\n \n@@ -681,7 +682,7 @@ public ComputeLineageRunnable(final RingBuffer<ProvenanceEventRecord> ringBuffer\n         @Override\n         public void run() {\n             final List<ProvenanceEventRecord> records = ringBuffer.getSelectedElements(filter);\n-            submission.getResult().update(records);\n+            submission.getResult().update(records, records.size());\n         }\n     }\n ",
                "raw_url": "https://github.com/apache/nifi/raw/96ed405d708894ee5400ebbdbf335325219faa09/nifi-nar-bundles/nifi-provenance-repository-bundle/nifi-volatile-provenance-repository/src/main/java/org/apache/nifi/provenance/VolatileProvenanceRepository.java",
                "sha": "f08fed45593ff80332b9fbe1100b47358dcef990",
                "status": "modified"
            }
        ],
        "message": "NIFI-3356: Initial implementation of writeahead provenance repository\n- The idea behind NIFI-3356 was to improve the efficiency and throughput of the Provenance Repository, as it is often the bottleneck. While testing the newly designed repository,\n  a handful of other, fairly minor, changes were made to improve efficiency as well, as these came to light when testing the new repository:\n\n- Use a BufferedOutputStream within StandardProcessSession (via a ClaimCache abstraction) in order to avoid continually writing to FileOutputStream when writing many small FlowFiles\n- Updated threading model of MinimalLockingWriteAheadLog - now performs serialization outside of lock and writes to a 'synchronized' OutputStream\n- Change minimum scheduling period for components from 30 microseconds to 1 nanosecond. ScheduledExecutor is very inconsistent with timing of task scheduling. With the bored.yield.duration\n  now present, this value doesn't need to be set to 30 microseconds. This was originally done to avoid processors that had no work from dominating the CPU. However, now that we will yield\n  when processors have no work, this results in slowing down processors that are able to perform work.\n- Allow nifi.properties to specify multiple directories for FlowFile Repository\n- If backpressure is engaged while running a batch of sessions, then stop batch processing earlier. This helps FlowFiles to move through the system much more smoothly instead of the\n  herky-jerky queuing that we previously saw at very high rates of FlowFiles.\n- Added NiFi PID to log message when starting nifi. This was simply an update to the log message that provides helpful information.\n\nNIFI-3356: Fixed bug in ContentClaimWriteCache that resulted in data corruption and fixed bug in RepositoryConfiguration that threw exception if cache warm duration was set to empty string\n\nNIFI-3356: Fixed NPE\n\nNIFI-3356: Added debug-level performance monitoring\n\nNIFI-3356: Updates to unit tests that failed after rebasing against master\n\nNIFI-3356: Incorporated PR review feedback\n\nNIFI-3356: Fixed bug where we would delete index directories that are still in use; also added additional debug logging and a simple util class that can be used to textualize provenance event files - useful in debugging\n\nThis closes #1493",
        "parent": "https://github.com/apache/nifi/commit/8d467f3d1fe9b9e96e67f2979aa5676465afedc1",
        "repo": "nifi",
        "unit_tests": [
            "TestVolatileProvenanceRepository.java"
        ]
    },
    "nifi_ba96e43": {
        "bug_id": "nifi_ba96e43",
        "commit": "https://github.com/apache/nifi/commit/ba96e43a8e0fa682e9c803228d292969ffd0c686",
        "file": [
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/nifi/blob/ba96e43a8e0fa682e9c803228d292969ffd0c686/nifi/nifi-nar-bundles/nifi-geo-bundle/nifi-geo-processors/src/main/java/org/apache/nifi/processors/GeoEnrichIP.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi/nifi-nar-bundles/nifi-geo-bundle/nifi-geo-processors/src/main/java/org/apache/nifi/processors/GeoEnrichIP.java?ref=ba96e43a8e0fa682e9c803228d292969ffd0c686",
                "deletions": 2,
                "filename": "nifi/nifi-nar-bundles/nifi-geo-bundle/nifi-geo-processors/src/main/java/org/apache/nifi/processors/GeoEnrichIP.java",
                "patch": "@@ -189,8 +189,17 @@ public void onTrigger(final ProcessContext context, final ProcessSession session\n         final Map<String, String> attrs = new HashMap<>();\n         attrs.put(new StringBuilder(ipAttributeName).append(\".geo.lookup.micros\").toString(), String.valueOf(stopWatch.getDuration(TimeUnit.MICROSECONDS)));\n         attrs.put(new StringBuilder(ipAttributeName).append(\".geo.city\").toString(), response.getCity().getName());\n-        attrs.put(new StringBuilder(ipAttributeName).append(\".geo.latitude\").toString(), response.getLocation().getLatitude().toString());\n-        attrs.put(new StringBuilder(ipAttributeName).append(\".geo.longitude\").toString(), response.getLocation().getLongitude().toString());\n+        \n+        final Double latitude = response.getLocation().getLatitude();\n+        if ( latitude != null ) {\n+        \tattrs.put(new StringBuilder(ipAttributeName).append(\".geo.latitude\").toString(), latitude.toString());\n+        }\n+        \n+        final Double longitude = response.getLocation().getLongitude();\n+        if ( longitude != null ) {\n+        \tattrs.put(new StringBuilder(ipAttributeName).append(\".geo.longitude\").toString(), longitude.toString());\n+        }\n+        \n         int i = 0;\n         for (final Subdivision subd : response.getSubdivisions()) {\n             attrs.put(new StringBuilder(ipAttributeName).append(\".geo.subdivision.\").append(i).toString(), subd.getName());",
                "raw_url": "https://github.com/apache/nifi/raw/ba96e43a8e0fa682e9c803228d292969ffd0c686/nifi/nifi-nar-bundles/nifi-geo-bundle/nifi-geo-processors/src/main/java/org/apache/nifi/processors/GeoEnrichIP.java",
                "sha": "1ecb221daa1b7575522098b0e2bb9fce4ec445c7",
                "status": "modified"
            }
        ],
        "message": "NIFI-549: Fixed NPE",
        "parent": "https://github.com/apache/nifi/commit/0759660897a8ca9e91d02157ec434abd5e2a7d7e",
        "repo": "nifi",
        "unit_tests": [
            "TestGeoEnrichIP.java"
        ]
    },
    "nifi_c974ea9": {
        "bug_id": "nifi_c974ea9",
        "commit": "https://github.com/apache/nifi/commit/c974ea90f89186bfd5f12e34f2cec3728d237b89",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/nifi/blob/c974ea90f89186bfd5f12e34f2cec3728d237b89/nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/scheduling/StandardProcessScheduler.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/scheduling/StandardProcessScheduler.java?ref=c974ea90f89186bfd5f12e34f2cec3728d237b89",
                "deletions": 2,
                "filename": "nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/scheduling/StandardProcessScheduler.java",
                "patch": "@@ -339,8 +339,8 @@ public void run() {\n                             final ProcessorLog procLog = new SimpleProcessLogger(procNode.getIdentifier(), procNode.getProcessor());\n \n                             procLog.error(\"{} failed to invoke @OnScheduled method due to {}; processor will not be scheduled to run for {}\",\n-                                    new Object[]{procNode.getProcessor(), cause.getCause(), administrativeYieldDuration}, cause.getCause());\n-                            LOG.error(\"Failed to invoke @OnScheduled method due to {}\", cause.getCause().toString(), cause.getCause());\n+                                    new Object[]{procNode.getProcessor(), cause, administrativeYieldDuration}, cause);\n+                            LOG.error(\"Failed to invoke @OnScheduled method due to {}\", cause.toString(), cause);\n \n                             ReflectionUtils.quietlyInvokeMethodsWithAnnotation(OnUnscheduled.class, procNode.getProcessor(), processContext);\n                             ReflectionUtils.quietlyInvokeMethodsWithAnnotation(OnStopped.class, procNode.getProcessor(), processContext);",
                "raw_url": "https://github.com/apache/nifi/raw/c974ea90f89186bfd5f12e34f2cec3728d237b89/nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/controller/scheduling/StandardProcessScheduler.java",
                "sha": "772582358d6fde8e647553b4adf08be83c35823e",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/nifi/blob/c974ea90f89186bfd5f12e34f2cec3728d237b89/nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/processor/SimpleProcessLogger.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/processor/SimpleProcessLogger.java?ref=c974ea90f89186bfd5f12e34f2cec3728d237b89",
                "deletions": 1,
                "filename": "nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/processor/SimpleProcessLogger.java",
                "patch": "@@ -243,7 +243,7 @@ public void error(String msg) {\n         for (int i = 0; i < os.length; i++) {\n             modifiedArgs[i + 1] = os[i];\n         }\n-        modifiedArgs[modifiedArgs.length - 1] = t.toString();\n+        modifiedArgs[modifiedArgs.length - 1] = (t == null) ? \"\" : t.toString();\n \n         return modifiedArgs;\n     }",
                "raw_url": "https://github.com/apache/nifi/raw/c974ea90f89186bfd5f12e34f2cec3728d237b89/nifi/nifi-nar-bundles/nifi-framework-bundle/nifi-framework/nifi-framework-core/src/main/java/org/apache/nifi/processor/SimpleProcessLogger.java",
                "sha": "0a345a095f1548abc8d4e1db83f23cebf7192d5b",
                "status": "modified"
            }
        ],
        "message": "NIFI-491: Fixed bug that caused InvocationTargetException.getCause().getCause() instead of InvocationTargetException.getCause(); also if null passed into logger, avoid the NPE that results",
        "parent": "https://github.com/apache/nifi/commit/a7862a19ba56f4ce8350d34b38d7d4edc6002a66",
        "repo": "nifi",
        "unit_tests": [
            "TestSimpleProcessLogger.java"
        ]
    },
    "nifi_fdbcf34": {
        "bug_id": "nifi_fdbcf34",
        "commit": "https://github.com/apache/nifi/commit/fdbcf34281bf6ee71f4f5ba6b9445254bc874752",
        "file": [
            {
                "additions": 7,
                "blob_url": "https://github.com/apache/nifi/blob/fdbcf34281bf6ee71f4f5ba6b9445254bc874752/nifi-nar-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/java/org/apache/nifi/processors/standard/InvokeHTTP.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/nifi/contents/nifi-nar-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/java/org/apache/nifi/processors/standard/InvokeHTTP.java?ref=fdbcf34281bf6ee71f4f5ba6b9445254bc874752",
                "deletions": 3,
                "filename": "nifi-nar-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/java/org/apache/nifi/processors/standard/InvokeHTTP.java",
                "patch": "@@ -21,6 +21,7 @@\n import com.burgstaller.okhttp.digest.CachingAuthenticator;\n import com.burgstaller.okhttp.digest.DigestAuthenticator;\n import com.google.common.io.Files;\n+import java.security.Principal;\n import okhttp3.Cache;\n import okhttp3.Credentials;\n import okhttp3.MediaType;\n@@ -132,7 +133,6 @@\n                     description = \"Send request header with a key matching the Dynamic Property Key and a value created by evaluating \"\n                             + \"the Attribute Expression Language set in the value of the Dynamic Property.\")\n public final class InvokeHTTP extends AbstractProcessor {\n-\n     // flowfile attribute keys returned after reading the response\n     public final static String STATUS_CODE = \"invokehttp.status.code\";\n     public final static String STATUS_MESSAGE = \"invokehttp.status.message\";\n@@ -1182,8 +1182,12 @@ private String csv(Collection<String> values) {\n                 map.put(key, value);\n         });\n \n-        if (\"HTTPS\".equals(url.getProtocol().toUpperCase())) {\n-            map.put(REMOTE_DN, responseHttp.handshake().peerPrincipal().getName());\n+        if (responseHttp.request().isHttps()) {\n+            Principal principal = responseHttp.handshake().peerPrincipal();\n+\n+            if (principal != null) {\n+                map.put(REMOTE_DN, principal.getName());\n+            }\n         }\n \n         return map;",
                "raw_url": "https://github.com/apache/nifi/raw/fdbcf34281bf6ee71f4f5ba6b9445254bc874752/nifi-nar-bundles/nifi-standard-bundle/nifi-standard-processors/src/main/java/org/apache/nifi/processors/standard/InvokeHTTP.java",
                "sha": "8926ba2d25094bb76a2ebe9818c28e3a0e0de45c",
                "status": "modified"
            }
        ],
        "message": "NIFI-5747 fix NPE when redirecting from HTTPS to HTTP for InvokeHTTP\n\nSigned-off-by: Koji Kawamura <ijokarumawak@apache.org>",
        "parent": "https://github.com/apache/nifi/commit/234ddb0fe1a36ad947c340114058d82c777d791f",
        "repo": "nifi",
        "unit_tests": [
            "TestInvokeHTTP.java"
        ]
    }
}