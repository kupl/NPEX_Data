{
    "crunch_0b25d1e": {
        "bug_id": "crunch_0b25d1e",
        "commit": "https://github.com/apache/crunch/commit/0b25d1e6ec10b14efa1ecbe562f97d1ce8fcabd5",
        "file": [
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/crunch/blob/0b25d1e6ec10b14efa1ecbe562f97d1ce8fcabd5/src/main/java/com/cloudera/crunch/io/SourceTargetHelper.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/src/main/java/com/cloudera/crunch/io/SourceTargetHelper.java?ref=0b25d1e6ec10b14efa1ecbe562f97d1ce8fcabd5",
                "deletions": 22,
                "filename": "src/main/java/com/cloudera/crunch/io/SourceTargetHelper.java",
                "patch": "@@ -17,35 +17,44 @@\n import java.io.FileNotFoundException;\n import java.io.IOException;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n \n /**\n  * Functions for configuring the inputs/outputs of MapReduce jobs.\n- *\n+ * \n  */\n public class SourceTargetHelper {\n-  public static long getPathSize(Configuration conf, Path path) throws IOException {\n-\treturn getPathSize(FileSystem.get(conf), path);\n-  }\n-  \n-  public static long getPathSize(FileSystem fs, Path path) throws IOException {\n-    FileStatus[] stati = null;\n-    try {\n-      stati = fs.listStatus(path);\n-    } catch (FileNotFoundException e) {\n-      return 0L;\n-    }\n-    \n-    if (stati.length == 0) {\n-      return 0L;\n-    }\n-    long size = 0;\n-    for (FileStatus status : stati) {\n-      size += status.getLen();\n-    }\n-    return size;\n-  }\n+\n+\tprivate static final Log LOG = LogFactory.getLog(SourceTargetHelper.class);\n+\n+\tpublic static long getPathSize(Configuration conf, Path path) throws IOException {\n+\t\treturn getPathSize(FileSystem.get(conf), path);\n+\t}\n+\n+\tpublic static long getPathSize(FileSystem fs, Path path) throws IOException {\n+\t\tFileStatus[] stati = null;\n+\t\ttry {\n+\t\t\tstati = fs.listStatus(path);\n+\t\t\tif (stati == null) {\n+\t\t\t\tthrow new FileNotFoundException(path + \" doesn't exist\");\n+\t\t\t}\n+\t\t} catch (FileNotFoundException e) {\n+\t\t\tLOG.warn(\"Returning 0 for getPathSize on non-existant path '\" + path + \"'\");\n+\t\t\treturn 0L;\n+\t\t}\n+\n+\t\tif (stati.length == 0) {\n+\t\t\treturn 0L;\n+\t\t}\n+\t\tlong size = 0;\n+\t\tfor (FileStatus status : stati) {\n+\t\t\tsize += status.getLen();\n+\t\t}\n+\t\treturn size;\n+\t}\n }",
                "raw_url": "https://github.com/apache/crunch/raw/0b25d1e6ec10b14efa1ecbe562f97d1ce8fcabd5/src/main/java/com/cloudera/crunch/io/SourceTargetHelper.java",
                "sha": "60b60501f808efad80e10239242057e9a43e64ec",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/crunch/blob/0b25d1e6ec10b14efa1ecbe562f97d1ce8fcabd5/src/test/java/com/cloudera/crunch/io/SourceTargetHelperTest.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/src/test/java/com/cloudera/crunch/io/SourceTargetHelperTest.java?ref=0b25d1e6ec10b14efa1ecbe562f97d1ce8fcabd5",
                "deletions": 8,
                "filename": "src/test/java/com/cloudera/crunch/io/SourceTargetHelperTest.java",
                "patch": "@@ -3,19 +3,39 @@\n import static org.junit.Assert.assertEquals;\n \n import java.io.File;\n+import java.io.IOException;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n import org.apache.hadoop.fs.Path;\n import org.junit.Test;\n \n public class SourceTargetHelperTest {\n-  @Test\n-  public void testGetNonexistentPathSize() throws Exception {\n-\tFile tmp = File.createTempFile(\"pathsize\", \"\");\n-\tPath tmpPath = new Path(tmp.getAbsolutePath());\n-\ttmp.delete();\n-\tFileSystem fs = FileSystem.getLocal(new Configuration());\n-\tassertEquals(0L, SourceTargetHelper.getPathSize(fs, tmpPath));\n-  }\n+\t@Test\n+\tpublic void testGetNonexistentPathSize() throws Exception {\n+\t\tFile tmp = File.createTempFile(\"pathsize\", \"\");\n+\t\tPath tmpPath = new Path(tmp.getAbsolutePath());\n+\t\ttmp.delete();\n+\t\tFileSystem fs = FileSystem.getLocal(new Configuration());\n+\t\tassertEquals(0L, SourceTargetHelper.getPathSize(fs, tmpPath));\n+\t}\n+\n+\t@Test\n+\tpublic void testGetNonExistentPathSize_NonExistantPath() throws IOException {\n+\t\tFileSystem mockFs = new MockFileSystem();\n+\t\tassertEquals(0L, SourceTargetHelper.getPathSize(mockFs, new Path(\"does/not/exist\")));\n+\t}\n+\n+\t/**\n+\t * Mock FileSystem that returns null for {@link FileSystem#listStatus(Path)}.\n+\t */\n+\tstatic class MockFileSystem extends LocalFileSystem {\n+\n+\t\t@Override\n+\t\tpublic FileStatus[] listStatus(Path f) throws IOException {\n+\t\t\treturn null;\n+\t\t}\n+\t}\n }",
                "raw_url": "https://github.com/apache/crunch/raw/0b25d1e6ec10b14efa1ecbe562f97d1ce8fcabd5/src/test/java/com/cloudera/crunch/io/SourceTargetHelperTest.java",
                "sha": "fd3c6ff88e65b8397e17116bdb902f1b612842f8",
                "status": "modified"
            }
        ],
        "message": "Merge branch 'sourcetarget-npe'",
        "parent": "https://github.com/apache/crunch/commit/6bf055f106586feab0df077c6e7b9e25e3548da4",
        "patched_files": [
            "SourceTargetHelper.java"
        ],
        "repo": "crunch",
        "unit_tests": [
            "SourceTargetHelperTest.java"
        ]
    },
    "crunch_0bde57f": {
        "bug_id": "crunch_0bde57f",
        "commit": "https://github.com/apache/crunch/commit/0bde57f964ce2f6839de823767ca826df8c3b9fa",
        "file": [
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/crunch/blob/0bde57f964ce2f6839de823767ca826df8c3b9fa/src/main/java/com/cloudera/crunch/io/SourceTargetHelper.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/src/main/java/com/cloudera/crunch/io/SourceTargetHelper.java?ref=0bde57f964ce2f6839de823767ca826df8c3b9fa",
                "deletions": 22,
                "filename": "src/main/java/com/cloudera/crunch/io/SourceTargetHelper.java",
                "patch": "@@ -17,35 +17,44 @@\n import java.io.FileNotFoundException;\n import java.io.IOException;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n \n /**\n  * Functions for configuring the inputs/outputs of MapReduce jobs.\n- *\n+ * \n  */\n public class SourceTargetHelper {\n-  public static long getPathSize(Configuration conf, Path path) throws IOException {\n-\treturn getPathSize(FileSystem.get(conf), path);\n-  }\n-  \n-  public static long getPathSize(FileSystem fs, Path path) throws IOException {\n-    FileStatus[] stati = null;\n-    try {\n-      stati = fs.listStatus(path);\n-    } catch (FileNotFoundException e) {\n-      return 0L;\n-    }\n-    \n-    if (stati.length == 0) {\n-      return 0L;\n-    }\n-    long size = 0;\n-    for (FileStatus status : stati) {\n-      size += status.getLen();\n-    }\n-    return size;\n-  }\n+\n+\tprivate static final Log LOG = LogFactory.getLog(SourceTargetHelper.class);\n+\n+\tpublic static long getPathSize(Configuration conf, Path path) throws IOException {\n+\t\treturn getPathSize(FileSystem.get(conf), path);\n+\t}\n+\n+\tpublic static long getPathSize(FileSystem fs, Path path) throws IOException {\n+\t\tFileStatus[] stati = null;\n+\t\ttry {\n+\t\t\tstati = fs.listStatus(path);\n+\t\t\tif (stati == null) {\n+\t\t\t\tthrow new FileNotFoundException(path + \" doesn't exist\");\n+\t\t\t}\n+\t\t} catch (FileNotFoundException e) {\n+\t\t\tLOG.warn(\"Returning 0 for getPathSize on non-existant path '\" + path + \"'\");\n+\t\t\treturn 0L;\n+\t\t}\n+\n+\t\tif (stati.length == 0) {\n+\t\t\treturn 0L;\n+\t\t}\n+\t\tlong size = 0;\n+\t\tfor (FileStatus status : stati) {\n+\t\t\tsize += status.getLen();\n+\t\t}\n+\t\treturn size;\n+\t}\n }",
                "raw_url": "https://github.com/apache/crunch/raw/0bde57f964ce2f6839de823767ca826df8c3b9fa/src/main/java/com/cloudera/crunch/io/SourceTargetHelper.java",
                "sha": "60b60501f808efad80e10239242057e9a43e64ec",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/crunch/blob/0bde57f964ce2f6839de823767ca826df8c3b9fa/src/test/java/com/cloudera/crunch/io/SourceTargetHelperTest.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/src/test/java/com/cloudera/crunch/io/SourceTargetHelperTest.java?ref=0bde57f964ce2f6839de823767ca826df8c3b9fa",
                "deletions": 8,
                "filename": "src/test/java/com/cloudera/crunch/io/SourceTargetHelperTest.java",
                "patch": "@@ -3,19 +3,39 @@\n import static org.junit.Assert.assertEquals;\n \n import java.io.File;\n+import java.io.IOException;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n import org.apache.hadoop.fs.Path;\n import org.junit.Test;\n \n public class SourceTargetHelperTest {\n-  @Test\n-  public void testGetNonexistentPathSize() throws Exception {\n-\tFile tmp = File.createTempFile(\"pathsize\", \"\");\n-\tPath tmpPath = new Path(tmp.getAbsolutePath());\n-\ttmp.delete();\n-\tFileSystem fs = FileSystem.getLocal(new Configuration());\n-\tassertEquals(0L, SourceTargetHelper.getPathSize(fs, tmpPath));\n-  }\n+\t@Test\n+\tpublic void testGetNonexistentPathSize() throws Exception {\n+\t\tFile tmp = File.createTempFile(\"pathsize\", \"\");\n+\t\tPath tmpPath = new Path(tmp.getAbsolutePath());\n+\t\ttmp.delete();\n+\t\tFileSystem fs = FileSystem.getLocal(new Configuration());\n+\t\tassertEquals(0L, SourceTargetHelper.getPathSize(fs, tmpPath));\n+\t}\n+\n+\t@Test\n+\tpublic void testGetNonExistentPathSize_NonExistantPath() throws IOException {\n+\t\tFileSystem mockFs = new MockFileSystem();\n+\t\tassertEquals(0L, SourceTargetHelper.getPathSize(mockFs, new Path(\"does/not/exist\")));\n+\t}\n+\n+\t/**\n+\t * Mock FileSystem that returns null for {@link FileSystem#listStatus(Path)}.\n+\t */\n+\tstatic class MockFileSystem extends LocalFileSystem {\n+\n+\t\t@Override\n+\t\tpublic FileStatus[] listStatus(Path f) throws IOException {\n+\t\t\treturn null;\n+\t\t}\n+\t}\n }",
                "raw_url": "https://github.com/apache/crunch/raw/0bde57f964ce2f6839de823767ca826df8c3b9fa/src/test/java/com/cloudera/crunch/io/SourceTargetHelperTest.java",
                "sha": "fd3c6ff88e65b8397e17116bdb902f1b612842f8",
                "status": "modified"
            }
        ],
        "message": "Prevent NPE on getPathSize for non-existant file\n\nSome FileSystem implementations (i.e. DistributedFileSystem) return\nnull (instead of throwing a FileNotFoundException) when listStatus\nis called with a non-existant path. The null situation is now handled\ninstead of allowing an NPE to be thrown, and a warning is logged.",
        "parent": "https://github.com/apache/crunch/commit/400b63706b2382bba72b064f55ae80ed22f9ba22",
        "patched_files": [
            "SourceTargetHelper.java"
        ],
        "repo": "crunch",
        "unit_tests": [
            "SourceTargetHelperTest.java"
        ]
    },
    "crunch_5c52323": {
        "bug_id": "crunch_5c52323",
        "commit": "https://github.com/apache/crunch/commit/5c523231f6e5d664197f14b8977f5fb1620162a1",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/crunch/blob/5c523231f6e5d664197f14b8977f5fb1620162a1/src/main/java/com/cloudera/crunch/impl/mem/collect/MemCollection.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/src/main/java/com/cloudera/crunch/impl/mem/collect/MemCollection.java?ref=5c523231f6e5d664197f14b8977f5fb1620162a1",
                "deletions": 2,
                "filename": "src/main/java/com/cloudera/crunch/impl/mem/collect/MemCollection.java",
                "patch": "@@ -63,13 +63,13 @@ public Pipeline getPipeline() {\n \n   @Override\n   public PCollection<S> union(PCollection<S>... collections) {\n-    Collection<S> output = Lists.newArrayList();\n-    output.addAll(collect);\n+    Collection<S> output = Lists.newArrayList();    \n     for (PCollection<S> pcollect : collections) {\n       for (S s : pcollect.materialize()) {\n         output.add(s);\n       }\n     }\n+    output.addAll(collect);\n     return new MemCollection<S>(output, collections[0].getPType());\n   }\n ",
                "raw_url": "https://github.com/apache/crunch/raw/5c523231f6e5d664197f14b8977f5fb1620162a1/src/main/java/com/cloudera/crunch/impl/mem/collect/MemCollection.java",
                "sha": "3a1a4f7429907e48db9d5d4ed552da0ab27a2bfa",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/crunch/blob/5c523231f6e5d664197f14b8977f5fb1620162a1/src/main/java/com/cloudera/crunch/impl/mr/MRPipeline.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/src/main/java/com/cloudera/crunch/impl/mr/MRPipeline.java?ref=5c523231f6e5d664197f14b8977f5fb1620162a1",
                "deletions": 0,
                "filename": "src/main/java/com/cloudera/crunch/impl/mr/MRPipeline.java",
                "patch": "@@ -26,6 +26,9 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n \n+import com.cloudera.crunch.DoFn;\n+import com.cloudera.crunch.Emitter;\n+import com.cloudera.crunch.MapFn;\n import com.cloudera.crunch.PCollection;\n import com.cloudera.crunch.PTable;\n import com.cloudera.crunch.Pipeline;\n@@ -38,6 +41,7 @@\n import com.cloudera.crunch.impl.mr.collect.InputTable;\n import com.cloudera.crunch.impl.mr.collect.PCollectionImpl;\n import com.cloudera.crunch.impl.mr.collect.PGroupedTableImpl;\n+import com.cloudera.crunch.impl.mr.collect.UnionCollection;\n import com.cloudera.crunch.impl.mr.plan.MSCRPlanner;\n import com.cloudera.crunch.impl.mr.run.RuntimeParameters;\n import com.cloudera.crunch.io.At;\n@@ -135,9 +139,13 @@ public void done() {\n     return read(At.textFile(pathName));\n   }\n \n+  @SuppressWarnings(\"unchecked\")\n   public void write(PCollection<?> pcollection, Target target) {\n     if (pcollection instanceof PGroupedTableImpl) {\n       pcollection = ((PGroupedTableImpl) pcollection).ungroup();\n+    } else if (pcollection instanceof UnionCollection) {\n+      pcollection = pcollection.parallelDo(\"UnionCollectionWrapper\",  \n+    \t\t  (MapFn)IdentityFn.<Object>getInstance(), pcollection.getPType());\t \n     }\n     addOutput((PCollectionImpl) pcollection, target);\n   }\n@@ -151,6 +159,11 @@ private void addOutput(PCollectionImpl impl, Target target) {\n   \n   @Override\n   public <T> Iterable<T> materialize(PCollection<T> pcollection) {\n+\t  \n+    if (pcollection instanceof UnionCollection) {\n+    \tpcollection = pcollection.parallelDo(\"UnionCollectionWrapper\",  \n+\t        (MapFn)IdentityFn.<Object>getInstance(), pcollection.getPType());\t \n+\t}  \n     PCollectionImpl impl = (PCollectionImpl) pcollection;\n     SourceTarget<T> matTarget = impl.getMaterializedAt();\n     if (matTarget != null && matTarget instanceof ReadableSourceTarget) {",
                "raw_url": "https://github.com/apache/crunch/raw/5c523231f6e5d664197f14b8977f5fb1620162a1/src/main/java/com/cloudera/crunch/impl/mr/MRPipeline.java",
                "sha": "9d2628f53aaf1e1aad40d86438172983d9935b97",
                "status": "modified"
            },
            {
                "additions": 146,
                "blob_url": "https://github.com/apache/crunch/blob/5c523231f6e5d664197f14b8977f5fb1620162a1/src/test/java/com/cloudera/crunch/impl/mr/collect/UnionCollectionTest.java",
                "changes": 146,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/src/test/java/com/cloudera/crunch/impl/mr/collect/UnionCollectionTest.java?ref=5c523231f6e5d664197f14b8977f5fb1620162a1",
                "deletions": 0,
                "filename": "src/test/java/com/cloudera/crunch/impl/mr/collect/UnionCollectionTest.java",
                "patch": "@@ -0,0 +1,146 @@\n+package com.cloudera.crunch.impl.mr.collect;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Parameterized.Parameters;\n+\n+import com.cloudera.crunch.PCollection;\n+import com.cloudera.crunch.PTableKeyValueTest;\n+import com.cloudera.crunch.Pipeline;\n+import com.cloudera.crunch.impl.mem.MemPipeline;\n+import com.cloudera.crunch.impl.mr.MRPipeline;\n+import com.cloudera.crunch.io.At;\n+import com.cloudera.crunch.io.To;\n+import com.cloudera.crunch.test.FileHelper;\n+import com.cloudera.crunch.type.PTypeFamily;\n+import com.cloudera.crunch.type.avro.AvroTypeFamily;\n+import com.cloudera.crunch.type.avro.Avros;\n+import com.cloudera.crunch.type.writable.WritableTypeFamily;\n+import com.google.common.collect.Lists;\n+\n+@RunWith(value = Parameterized.class)\n+public class UnionCollectionTest {\n+\n+\tprivate static final Log LOG = LogFactory.getLog(UnionCollectionTest.class);\n+\n+\tprivate PTypeFamily typeFamily;\n+\tprivate Pipeline pipeline;\n+\tprivate PCollection<String> union;\n+\n+\tprivate ArrayList<String> EXPECTED = Lists.newArrayList(\"b\", \"c\", \"a\", \"e\",\n+\t\t\t\"c\", \"d\", \"a\");\n+\n+\t@Before\n+\t@SuppressWarnings(\"unchecked\")\n+\tpublic void setUp() throws IOException {\n+\t\tString inputFile1 = FileHelper.createTempCopyOf(\"set1.txt\");\n+\t\tString inputFile2 = FileHelper.createTempCopyOf(\"set2.txt\");\n+\n+\t\tPCollection<String> firstCollection = pipeline.read(At.textFile(\n+\t\t\t\tinputFile1, typeFamily.strings()));\n+\t\tPCollection<String> secondCollection = pipeline.read(At.textFile(\n+\t\t\t\tinputFile2, typeFamily.strings()));\n+\n+\t\tLOG.info(\"Test fixture: [\" + pipeline.getClass().getSimpleName()\n+\t\t\t\t+ \" : \" + typeFamily.getClass().getSimpleName() + \"]  First: \"\n+\t\t\t\t+ Lists.newArrayList(firstCollection.materialize().iterator())\n+\t\t\t\t+ \", Second: \"\n+\t\t\t\t+ Lists.newArrayList(secondCollection.materialize().iterator()));\n+\n+\t\tunion = secondCollection.union(firstCollection);\n+\t}\n+\n+\t@After\n+\tpublic void tearDown() {\n+\t\tpipeline.done();\n+\t}\n+\n+\t@Parameters\n+\tpublic static Collection<Object[]> data() throws IOException {\n+\t\tObject[][] data = new Object[][] {\n+\t\t\t\t{ WritableTypeFamily.getInstance(),\n+\t\t\t\t\t\tnew MRPipeline(PTableKeyValueTest.class) },\n+\t\t\t\t{ WritableTypeFamily.getInstance(), MemPipeline.getInstance() },\n+\t\t\t\t{ AvroTypeFamily.getInstance(),\n+\t\t\t\t\t\tnew MRPipeline(PTableKeyValueTest.class) },\n+\t\t\t\t{ AvroTypeFamily.getInstance(), MemPipeline.getInstance() } };\n+\t\treturn Arrays.asList(data);\n+\t}\n+\n+\tpublic UnionCollectionTest(PTypeFamily typeFamily, Pipeline pipeline) {\n+\t\tthis.typeFamily = typeFamily;\n+\t\tthis.pipeline = pipeline;\n+\t}\n+\n+\t@Test\n+\tpublic void unionMaterializeShouldNotThrowNPE() {\n+\t\tcheckMaterialized(union.materialize());\n+\t\tcheckMaterialized(pipeline.materialize(union));\n+\t}\n+\n+\tprivate void checkMaterialized(Iterable<String> materialized) {\n+\n+\t\tArrayList<String> list = Lists.newArrayList(materialized.iterator());\n+\t\tLOG.info(\"Materialized union: \" + list);\n+\n+\t\tassertEquals(EXPECTED, list);\n+\t}\n+\n+\t@Test\n+\tpublic void unionWriteShouldNotThrowNPE() throws IOException {\n+\n+\t\tFile outputPath1 = FileHelper.createOutputPath();\n+\t\tFile outputPath2 = FileHelper.createOutputPath();\n+\t\tFile outputPath3 = FileHelper.createOutputPath();\n+\n+\t\tif (typeFamily == AvroTypeFamily.getInstance()) {\n+\t\t\tunion.write(To.avroFile(outputPath1.getAbsolutePath()));\n+\t\t\tpipeline.write(union, To.avroFile(outputPath2.getAbsolutePath()));\n+\n+\t\t\tpipeline.run();\n+\n+\t\t\tcheckFileContents(outputPath1.getAbsolutePath());\n+\t\t\tcheckFileContents(outputPath2.getAbsolutePath());\n+\n+\t\t} else {\n+\t\t\t\n+\t\t\tunion.write(To.textFile(outputPath1.getAbsolutePath()));\n+\t\t\tpipeline.write(union, To.textFile(outputPath2.getAbsolutePath()));\n+\t\t\tpipeline.writeTextFile(union, outputPath3.getAbsolutePath());\n+\n+\t\t\tpipeline.run();\n+\n+\t\t\tcheckFileContents(outputPath1.getAbsolutePath());\n+\t\t\tcheckFileContents(outputPath2.getAbsolutePath());\n+\t\t\tcheckFileContents(outputPath3.getAbsolutePath());\n+\t\t}\n+\n+\t}\n+\n+\tprivate void checkFileContents(String filePath) throws IOException {\n+\n+\t\tArrayList<String> result = (typeFamily != AvroTypeFamily.getInstance() || !(pipeline instanceof MRPipeline)) ? Lists\n+\t\t\t\t.newArrayList(pipeline\n+\t\t\t\t\t\t.read(At.textFile(filePath, typeFamily.strings()))\n+\t\t\t\t\t\t.materialize().iterator()) : Lists\n+\t\t\t\t.newArrayList(pipeline\n+\t\t\t\t\t\t.read(At.avroFile(filePath, Avros.strings()))\n+\t\t\t\t\t\t.materialize().iterator());\n+\n+\t\tLOG.info(\"Saved Union: \" + result);\n+\t\tassertEquals(EXPECTED, result);\n+\t}\n+}",
                "raw_url": "https://github.com/apache/crunch/raw/5c523231f6e5d664197f14b8977f5fb1620162a1/src/test/java/com/cloudera/crunch/impl/mr/collect/UnionCollectionTest.java",
                "sha": "4691e606c355817b6f771a4bebdd5587e8d48600",
                "status": "added"
            }
        ],
        "message": "Merge pull request #25 from tzolov/master\n\nFix NPE thrown on write or materialize of Union collections",
        "parent": "https://github.com/apache/crunch/commit/db5054f6f8835e499a512353e69881c067e68fc2",
        "patched_files": [
            "MRPipeline.java",
            "MemCollection.java",
            "UnionCollection.java"
        ],
        "repo": "crunch",
        "unit_tests": [
            "UnionCollectionTest.java"
        ]
    },
    "crunch_7061fa7": {
        "bug_id": "crunch_7061fa7",
        "commit": "https://github.com/apache/crunch/commit/7061fa75d318f93dd66506ee51ea1643a444bdba",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/crunch/blob/7061fa75d318f93dd66506ee51ea1643a444bdba/src/main/java/com/cloudera/crunch/impl/mr/collect/InputCollection.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/src/main/java/com/cloudera/crunch/impl/mr/collect/InputCollection.java?ref=7061fa75d318f93dd66506ee51ea1643a444bdba",
                "deletions": 2,
                "filename": "src/main/java/com/cloudera/crunch/impl/mr/collect/InputCollection.java",
                "patch": "@@ -46,8 +46,8 @@ public InputCollection(Source<S> source, MRPipeline pipeline) {\n   @Override\n   protected long getSizeInternal() {\n     long sz = source.getSize(pipeline.getConfiguration());\n-    if (sz == 0) {\n-      throw new IllegalStateException(\"Input source \" + source + \" is empty\");\n+    if (sz < 0) {\n+      throw new IllegalStateException(\"Input source \" + source + \" does not exist!\");\n     }\n     return sz;\n   }",
                "raw_url": "https://github.com/apache/crunch/raw/7061fa75d318f93dd66506ee51ea1643a444bdba/src/main/java/com/cloudera/crunch/impl/mr/collect/InputCollection.java",
                "sha": "a0328b4709d0d03eaa1a43625961cc5985705efe",
                "status": "modified"
            },
            {
                "additions": 20,
                "blob_url": "https://github.com/apache/crunch/blob/7061fa75d318f93dd66506ee51ea1643a444bdba/src/main/java/com/cloudera/crunch/io/CompositePathIterable.java",
                "changes": 22,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/src/main/java/com/cloudera/crunch/io/CompositePathIterable.java?ref=7061fa75d318f93dd66506ee51ea1643a444bdba",
                "deletions": 2,
                "filename": "src/main/java/com/cloudera/crunch/io/CompositePathIterable.java",
                "patch": "@@ -17,6 +17,7 @@\n import java.io.FileNotFoundException;\n import java.io.IOException;\n import java.util.Iterator;\n+import java.util.NoSuchElementException;\n \n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n@@ -31,6 +32,18 @@\n   private final FileSystem fs;\n   private final FileReaderFactory<T> readerFactory;\n \n+  private final UnmodifiableIterator<T> emptyIterator = new UnmodifiableIterator<T>() {\n+\t  @Override\n+\t  public boolean hasNext() {\n+\t\t  return false;\t  \n+\t  }\n+\n+\t  @Override\n+\t  public T next() {\n+\t\tthrow new NoSuchElementException();\n+\t  }\n+  };\n+\n   private static final PathFilter FILTER = new PathFilter() {\n \t@Override\n \tpublic boolean accept(Path path) {\n@@ -45,7 +58,7 @@ public boolean accept(Path path) {\n \t} catch (FileNotFoundException e) {\n \t  stati = null;\n \t}\n-\tif (stati == null || stati.length == 0) {\n+\tif (stati == null /*|| stati.length == 0*/) {\n \t  throw new IOException(\"No files found to materialize at: \" + path);\n \t}\n \treturn new CompositePathIterable<S>(stati, fs, readerFactory);\n@@ -56,9 +69,14 @@ private CompositePathIterable(FileStatus[] stati, FileSystem fs, FileReaderFacto\n \tthis.fs = fs;\n \tthis.readerFactory = readerFactory;\n   }\n-  \n+    \n   @Override\n   public Iterator<T> iterator() {\n+\t\n+\tif (stati.length == 0) {\n+      return emptyIterator;\n+\t}\n+\t\n \treturn new UnmodifiableIterator<T>() {\n \t  private int index = 0;\n \t  private Iterator<T> iter = readerFactory.read(fs, stati[index++].getPath());",
                "raw_url": "https://github.com/apache/crunch/raw/7061fa75d318f93dd66506ee51ea1643a444bdba/src/main/java/com/cloudera/crunch/io/CompositePathIterable.java",
                "sha": "e88fc39eea24d0c24ff45014caa2356b3789091b",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/crunch/blob/7061fa75d318f93dd66506ee51ea1643a444bdba/src/main/java/com/cloudera/crunch/io/SourceTargetHelper.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/src/main/java/com/cloudera/crunch/io/SourceTargetHelper.java?ref=7061fa75d318f93dd66506ee51ea1643a444bdba",
                "deletions": 0,
                "filename": "src/main/java/com/cloudera/crunch/io/SourceTargetHelper.java",
                "patch": "@@ -32,6 +32,9 @@ public static long getPathSize(Configuration conf, Path path) throws IOException\n   }\n   \n   public static long getPathSize(FileSystem fs, Path path) throws IOException {\n+\tif (!fs.exists(path)) {\n+       return -1L;\n+    }\n     FileStatus[] stati = null;\n     try {\n       stati = fs.listStatus(path);",
                "raw_url": "https://github.com/apache/crunch/raw/7061fa75d318f93dd66506ee51ea1643a444bdba/src/main/java/com/cloudera/crunch/io/SourceTargetHelper.java",
                "sha": "9ba200350e892e18de16634d476d80823ccdcaa0",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/crunch/blob/7061fa75d318f93dd66506ee51ea1643a444bdba/src/main/java/com/cloudera/crunch/io/impl/FileSourceImpl.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/src/main/java/com/cloudera/crunch/io/impl/FileSourceImpl.java?ref=7061fa75d318f93dd66506ee51ea1643a444bdba",
                "deletions": 1,
                "filename": "src/main/java/com/cloudera/crunch/io/impl/FileSourceImpl.java",
                "patch": "@@ -72,8 +72,9 @@ public long getSize(Configuration configuration) {\n \t  return SourceTargetHelper.getPathSize(configuration, path);\n \t} catch (IOException e) {\n \t  LOG.warn(String.format(\"Exception thrown looking up size of: %s\", path), e);\n+\t  throw new IllegalStateException(\"Failed to get the file size of:\"+ path, e);\n \t}\n-\treturn 1L;\n+\t//return 1L;\n   }\n \n ",
                "raw_url": "https://github.com/apache/crunch/raw/7061fa75d318f93dd66506ee51ea1643a444bdba/src/main/java/com/cloudera/crunch/io/impl/FileSourceImpl.java",
                "sha": "5d9936ec5073fb6b44a75ac93e82b6b1995a4f3d",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/crunch/blob/7061fa75d318f93dd66506ee51ea1643a444bdba/src/test/java/com/cloudera/crunch/io/SourceTargetHelperTest.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/src/test/java/com/cloudera/crunch/io/SourceTargetHelperTest.java?ref=7061fa75d318f93dd66506ee51ea1643a444bdba",
                "deletions": 1,
                "filename": "src/test/java/com/cloudera/crunch/io/SourceTargetHelperTest.java",
                "patch": "@@ -16,6 +16,6 @@ public void testGetNonexistentPathSize() throws Exception {\n \tPath tmpPath = new Path(tmp.getAbsolutePath());\n \ttmp.delete();\n \tFileSystem fs = FileSystem.getLocal(new Configuration());\n-\tassertEquals(0L, SourceTargetHelper.getPathSize(fs, tmpPath));\n+\tassertEquals(-1L, SourceTargetHelper.getPathSize(fs, tmpPath));\n   }\n }",
                "raw_url": "https://github.com/apache/crunch/raw/7061fa75d318f93dd66506ee51ea1643a444bdba/src/test/java/com/cloudera/crunch/io/SourceTargetHelperTest.java",
                "sha": "dad8a25ed65bb922edf7c2e73cbef53eb504c6ef",
                "status": "modified"
            }
        ],
        "message": "fix NPE on InputCollection getSize()",
        "parent": "https://github.com/apache/crunch/commit/4b6a08f511c1ce6a3075881072c3eb491b93591c",
        "patched_files": [
            "FileSourceImpl.java",
            "CompositePathIterable.java",
            "InputCollection.java",
            "SourceTargetHelper.java"
        ],
        "repo": "crunch",
        "unit_tests": [
            "SourceTargetHelperTest.java"
        ]
    },
    "crunch_9397eda": {
        "bug_id": "crunch_9397eda",
        "commit": "https://github.com/apache/crunch/commit/9397edad2dce5b055a2eb539b933aa16759e2a55",
        "file": [
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/crunch/blob/9397edad2dce5b055a2eb539b933aa16759e2a55/src/main/java/com/cloudera/crunch/io/SourceTargetHelper.java",
                "changes": 53,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/src/main/java/com/cloudera/crunch/io/SourceTargetHelper.java?ref=9397edad2dce5b055a2eb539b933aa16759e2a55",
                "deletions": 22,
                "filename": "src/main/java/com/cloudera/crunch/io/SourceTargetHelper.java",
                "patch": "@@ -17,35 +17,44 @@\n import java.io.FileNotFoundException;\n import java.io.IOException;\n \n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n \n /**\n  * Functions for configuring the inputs/outputs of MapReduce jobs.\n- *\n+ * \n  */\n public class SourceTargetHelper {\n-  public static long getPathSize(Configuration conf, Path path) throws IOException {\n-\treturn getPathSize(FileSystem.get(conf), path);\n-  }\n-  \n-  public static long getPathSize(FileSystem fs, Path path) throws IOException {\n-    FileStatus[] stati = null;\n-    try {\n-      stati = fs.listStatus(path);\n-    } catch (FileNotFoundException e) {\n-      return 0L;\n-    }\n-    \n-    if (stati.length == 0) {\n-      return 0L;\n-    }\n-    long size = 0;\n-    for (FileStatus status : stati) {\n-      size += status.getLen();\n-    }\n-    return size;\n-  }\n+\n+\tprivate static final Log LOG = LogFactory.getLog(SourceTargetHelper.class);\n+\n+\tpublic static long getPathSize(Configuration conf, Path path) throws IOException {\n+\t\treturn getPathSize(FileSystem.get(conf), path);\n+\t}\n+\n+\tpublic static long getPathSize(FileSystem fs, Path path) throws IOException {\n+\t\tFileStatus[] stati = null;\n+\t\ttry {\n+\t\t\tstati = fs.listStatus(path);\n+\t\t\tif (stati == null) {\n+\t\t\t\tthrow new FileNotFoundException(path + \" doesn't exist\");\n+\t\t\t}\n+\t\t} catch (FileNotFoundException e) {\n+\t\t\tLOG.warn(\"Returning 0 for getPathSize on non-existant path '\" + path + \"'\");\n+\t\t\treturn 0L;\n+\t\t}\n+\n+\t\tif (stati.length == 0) {\n+\t\t\treturn 0L;\n+\t\t}\n+\t\tlong size = 0;\n+\t\tfor (FileStatus status : stati) {\n+\t\t\tsize += status.getLen();\n+\t\t}\n+\t\treturn size;\n+\t}\n }",
                "raw_url": "https://github.com/apache/crunch/raw/9397edad2dce5b055a2eb539b933aa16759e2a55/src/main/java/com/cloudera/crunch/io/SourceTargetHelper.java",
                "sha": "60b60501f808efad80e10239242057e9a43e64ec",
                "status": "modified"
            },
            {
                "additions": 28,
                "blob_url": "https://github.com/apache/crunch/blob/9397edad2dce5b055a2eb539b933aa16759e2a55/src/test/java/com/cloudera/crunch/io/SourceTargetHelperTest.java",
                "changes": 36,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/src/test/java/com/cloudera/crunch/io/SourceTargetHelperTest.java?ref=9397edad2dce5b055a2eb539b933aa16759e2a55",
                "deletions": 8,
                "filename": "src/test/java/com/cloudera/crunch/io/SourceTargetHelperTest.java",
                "patch": "@@ -3,19 +3,39 @@\n import static org.junit.Assert.assertEquals;\n \n import java.io.File;\n+import java.io.IOException;\n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.LocalFileSystem;\n import org.apache.hadoop.fs.Path;\n import org.junit.Test;\n \n public class SourceTargetHelperTest {\n-  @Test\n-  public void testGetNonexistentPathSize() throws Exception {\n-\tFile tmp = File.createTempFile(\"pathsize\", \"\");\n-\tPath tmpPath = new Path(tmp.getAbsolutePath());\n-\ttmp.delete();\n-\tFileSystem fs = FileSystem.getLocal(new Configuration());\n-\tassertEquals(0L, SourceTargetHelper.getPathSize(fs, tmpPath));\n-  }\n+\t@Test\n+\tpublic void testGetNonexistentPathSize() throws Exception {\n+\t\tFile tmp = File.createTempFile(\"pathsize\", \"\");\n+\t\tPath tmpPath = new Path(tmp.getAbsolutePath());\n+\t\ttmp.delete();\n+\t\tFileSystem fs = FileSystem.getLocal(new Configuration());\n+\t\tassertEquals(0L, SourceTargetHelper.getPathSize(fs, tmpPath));\n+\t}\n+\n+\t@Test\n+\tpublic void testGetNonExistentPathSize_NonExistantPath() throws IOException {\n+\t\tFileSystem mockFs = new MockFileSystem();\n+\t\tassertEquals(0L, SourceTargetHelper.getPathSize(mockFs, new Path(\"does/not/exist\")));\n+\t}\n+\n+\t/**\n+\t * Mock FileSystem that returns null for {@link FileSystem#listStatus(Path)}.\n+\t */\n+\tstatic class MockFileSystem extends LocalFileSystem {\n+\n+\t\t@Override\n+\t\tpublic FileStatus[] listStatus(Path f) throws IOException {\n+\t\t\treturn null;\n+\t\t}\n+\t}\n }",
                "raw_url": "https://github.com/apache/crunch/raw/9397edad2dce5b055a2eb539b933aa16759e2a55/src/test/java/com/cloudera/crunch/io/SourceTargetHelperTest.java",
                "sha": "fd3c6ff88e65b8397e17116bdb902f1b612842f8",
                "status": "modified"
            }
        ],
        "message": "Merge pull request #29 from gabrielreid/sourcetarget-npe\n\nPrevent NPE on getPathSize for non-existant file",
        "parent": "https://github.com/apache/crunch/commit/400b63706b2382bba72b064f55ae80ed22f9ba22",
        "patched_files": [
            "SourceTargetHelper.java"
        ],
        "repo": "crunch",
        "unit_tests": [
            "SourceTargetHelperTest.java"
        ]
    },
    "crunch_c69a354": {
        "bug_id": "crunch_c69a354",
        "commit": "https://github.com/apache/crunch/commit/c69a354f760f7318dc020c9825d79814ca5aa5b4",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/crunch/blob/c69a354f760f7318dc020c9825d79814ca5aa5b4/src/main/java/com/cloudera/crunch/impl/mem/collect/MemCollection.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/src/main/java/com/cloudera/crunch/impl/mem/collect/MemCollection.java?ref=c69a354f760f7318dc020c9825d79814ca5aa5b4",
                "deletions": 2,
                "filename": "src/main/java/com/cloudera/crunch/impl/mem/collect/MemCollection.java",
                "patch": "@@ -63,13 +63,13 @@ public Pipeline getPipeline() {\n \n   @Override\n   public PCollection<S> union(PCollection<S>... collections) {\n-    Collection<S> output = Lists.newArrayList();\n-    output.addAll(collect);\n+    Collection<S> output = Lists.newArrayList();    \n     for (PCollection<S> pcollect : collections) {\n       for (S s : pcollect.materialize()) {\n         output.add(s);\n       }\n     }\n+    output.addAll(collect);\n     return new MemCollection<S>(output, collections[0].getPType());\n   }\n ",
                "raw_url": "https://github.com/apache/crunch/raw/c69a354f760f7318dc020c9825d79814ca5aa5b4/src/main/java/com/cloudera/crunch/impl/mem/collect/MemCollection.java",
                "sha": "3a1a4f7429907e48db9d5d4ed552da0ab27a2bfa",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/crunch/blob/c69a354f760f7318dc020c9825d79814ca5aa5b4/src/main/java/com/cloudera/crunch/impl/mr/MRPipeline.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/src/main/java/com/cloudera/crunch/impl/mr/MRPipeline.java?ref=c69a354f760f7318dc020c9825d79814ca5aa5b4",
                "deletions": 0,
                "filename": "src/main/java/com/cloudera/crunch/impl/mr/MRPipeline.java",
                "patch": "@@ -26,6 +26,9 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n \n+import com.cloudera.crunch.DoFn;\n+import com.cloudera.crunch.Emitter;\n+import com.cloudera.crunch.MapFn;\n import com.cloudera.crunch.PCollection;\n import com.cloudera.crunch.PTable;\n import com.cloudera.crunch.Pipeline;\n@@ -38,6 +41,7 @@\n import com.cloudera.crunch.impl.mr.collect.InputTable;\n import com.cloudera.crunch.impl.mr.collect.PCollectionImpl;\n import com.cloudera.crunch.impl.mr.collect.PGroupedTableImpl;\n+import com.cloudera.crunch.impl.mr.collect.UnionCollection;\n import com.cloudera.crunch.impl.mr.plan.MSCRPlanner;\n import com.cloudera.crunch.impl.mr.run.RuntimeParameters;\n import com.cloudera.crunch.io.At;\n@@ -135,9 +139,13 @@ public void done() {\n     return read(At.textFile(pathName));\n   }\n \n+  @SuppressWarnings(\"unchecked\")\n   public void write(PCollection<?> pcollection, Target target) {\n     if (pcollection instanceof PGroupedTableImpl) {\n       pcollection = ((PGroupedTableImpl) pcollection).ungroup();\n+    } else if (pcollection instanceof UnionCollection) {\n+      pcollection = pcollection.parallelDo(\"UnionCollectionWrapper\",  \n+    \t\t  (MapFn)IdentityFn.<Object>getInstance(), pcollection.getPType());\t \n     }\n     addOutput((PCollectionImpl) pcollection, target);\n   }\n@@ -151,6 +159,11 @@ private void addOutput(PCollectionImpl impl, Target target) {\n   \n   @Override\n   public <T> Iterable<T> materialize(PCollection<T> pcollection) {\n+\t  \n+    if (pcollection instanceof UnionCollection) {\n+    \tpcollection = pcollection.parallelDo(\"UnionCollectionWrapper\",  \n+\t        (MapFn)IdentityFn.<Object>getInstance(), pcollection.getPType());\t \n+\t}  \n     PCollectionImpl impl = (PCollectionImpl) pcollection;\n     SourceTarget<T> matTarget = impl.getMaterializedAt();\n     if (matTarget != null && matTarget instanceof ReadableSourceTarget) {",
                "raw_url": "https://github.com/apache/crunch/raw/c69a354f760f7318dc020c9825d79814ca5aa5b4/src/main/java/com/cloudera/crunch/impl/mr/MRPipeline.java",
                "sha": "9d2628f53aaf1e1aad40d86438172983d9935b97",
                "status": "modified"
            },
            {
                "additions": 146,
                "blob_url": "https://github.com/apache/crunch/blob/c69a354f760f7318dc020c9825d79814ca5aa5b4/src/test/java/com/cloudera/crunch/impl/mr/collect/UnionCollectionTest.java",
                "changes": 146,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/src/test/java/com/cloudera/crunch/impl/mr/collect/UnionCollectionTest.java?ref=c69a354f760f7318dc020c9825d79814ca5aa5b4",
                "deletions": 0,
                "filename": "src/test/java/com/cloudera/crunch/impl/mr/collect/UnionCollectionTest.java",
                "patch": "@@ -0,0 +1,146 @@\n+package com.cloudera.crunch.impl.mr.collect;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.Collection;\n+\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.Parameterized;\n+import org.junit.runners.Parameterized.Parameters;\n+\n+import com.cloudera.crunch.PCollection;\n+import com.cloudera.crunch.PTableKeyValueTest;\n+import com.cloudera.crunch.Pipeline;\n+import com.cloudera.crunch.impl.mem.MemPipeline;\n+import com.cloudera.crunch.impl.mr.MRPipeline;\n+import com.cloudera.crunch.io.At;\n+import com.cloudera.crunch.io.To;\n+import com.cloudera.crunch.test.FileHelper;\n+import com.cloudera.crunch.type.PTypeFamily;\n+import com.cloudera.crunch.type.avro.AvroTypeFamily;\n+import com.cloudera.crunch.type.avro.Avros;\n+import com.cloudera.crunch.type.writable.WritableTypeFamily;\n+import com.google.common.collect.Lists;\n+\n+@RunWith(value = Parameterized.class)\n+public class UnionCollectionTest {\n+\n+\tprivate static final Log LOG = LogFactory.getLog(UnionCollectionTest.class);\n+\n+\tprivate PTypeFamily typeFamily;\n+\tprivate Pipeline pipeline;\n+\tprivate PCollection<String> union;\n+\n+\tprivate ArrayList<String> EXPECTED = Lists.newArrayList(\"b\", \"c\", \"a\", \"e\",\n+\t\t\t\"c\", \"d\", \"a\");\n+\n+\t@Before\n+\t@SuppressWarnings(\"unchecked\")\n+\tpublic void setUp() throws IOException {\n+\t\tString inputFile1 = FileHelper.createTempCopyOf(\"set1.txt\");\n+\t\tString inputFile2 = FileHelper.createTempCopyOf(\"set2.txt\");\n+\n+\t\tPCollection<String> firstCollection = pipeline.read(At.textFile(\n+\t\t\t\tinputFile1, typeFamily.strings()));\n+\t\tPCollection<String> secondCollection = pipeline.read(At.textFile(\n+\t\t\t\tinputFile2, typeFamily.strings()));\n+\n+\t\tLOG.info(\"Test fixture: [\" + pipeline.getClass().getSimpleName()\n+\t\t\t\t+ \" : \" + typeFamily.getClass().getSimpleName() + \"]  First: \"\n+\t\t\t\t+ Lists.newArrayList(firstCollection.materialize().iterator())\n+\t\t\t\t+ \", Second: \"\n+\t\t\t\t+ Lists.newArrayList(secondCollection.materialize().iterator()));\n+\n+\t\tunion = secondCollection.union(firstCollection);\n+\t}\n+\n+\t@After\n+\tpublic void tearDown() {\n+\t\tpipeline.done();\n+\t}\n+\n+\t@Parameters\n+\tpublic static Collection<Object[]> data() throws IOException {\n+\t\tObject[][] data = new Object[][] {\n+\t\t\t\t{ WritableTypeFamily.getInstance(),\n+\t\t\t\t\t\tnew MRPipeline(PTableKeyValueTest.class) },\n+\t\t\t\t{ WritableTypeFamily.getInstance(), MemPipeline.getInstance() },\n+\t\t\t\t{ AvroTypeFamily.getInstance(),\n+\t\t\t\t\t\tnew MRPipeline(PTableKeyValueTest.class) },\n+\t\t\t\t{ AvroTypeFamily.getInstance(), MemPipeline.getInstance() } };\n+\t\treturn Arrays.asList(data);\n+\t}\n+\n+\tpublic UnionCollectionTest(PTypeFamily typeFamily, Pipeline pipeline) {\n+\t\tthis.typeFamily = typeFamily;\n+\t\tthis.pipeline = pipeline;\n+\t}\n+\n+\t@Test\n+\tpublic void unionMaterializeShouldNotThrowNPE() {\n+\t\tcheckMaterialized(union.materialize());\n+\t\tcheckMaterialized(pipeline.materialize(union));\n+\t}\n+\n+\tprivate void checkMaterialized(Iterable<String> materialized) {\n+\n+\t\tArrayList<String> list = Lists.newArrayList(materialized.iterator());\n+\t\tLOG.info(\"Materialized union: \" + list);\n+\n+\t\tassertEquals(EXPECTED, list);\n+\t}\n+\n+\t@Test\n+\tpublic void unionWriteShouldNotThrowNPE() throws IOException {\n+\n+\t\tFile outputPath1 = FileHelper.createOutputPath();\n+\t\tFile outputPath2 = FileHelper.createOutputPath();\n+\t\tFile outputPath3 = FileHelper.createOutputPath();\n+\n+\t\tif (typeFamily == AvroTypeFamily.getInstance()) {\n+\t\t\tunion.write(To.avroFile(outputPath1.getAbsolutePath()));\n+\t\t\tpipeline.write(union, To.avroFile(outputPath2.getAbsolutePath()));\n+\n+\t\t\tpipeline.run();\n+\n+\t\t\tcheckFileContents(outputPath1.getAbsolutePath());\n+\t\t\tcheckFileContents(outputPath2.getAbsolutePath());\n+\n+\t\t} else {\n+\t\t\t\n+\t\t\tunion.write(To.textFile(outputPath1.getAbsolutePath()));\n+\t\t\tpipeline.write(union, To.textFile(outputPath2.getAbsolutePath()));\n+\t\t\tpipeline.writeTextFile(union, outputPath3.getAbsolutePath());\n+\n+\t\t\tpipeline.run();\n+\n+\t\t\tcheckFileContents(outputPath1.getAbsolutePath());\n+\t\t\tcheckFileContents(outputPath2.getAbsolutePath());\n+\t\t\tcheckFileContents(outputPath3.getAbsolutePath());\n+\t\t}\n+\n+\t}\n+\n+\tprivate void checkFileContents(String filePath) throws IOException {\n+\n+\t\tArrayList<String> result = (typeFamily != AvroTypeFamily.getInstance() || !(pipeline instanceof MRPipeline)) ? Lists\n+\t\t\t\t.newArrayList(pipeline\n+\t\t\t\t\t\t.read(At.textFile(filePath, typeFamily.strings()))\n+\t\t\t\t\t\t.materialize().iterator()) : Lists\n+\t\t\t\t.newArrayList(pipeline\n+\t\t\t\t\t\t.read(At.avroFile(filePath, Avros.strings()))\n+\t\t\t\t\t\t.materialize().iterator());\n+\n+\t\tLOG.info(\"Saved Union: \" + result);\n+\t\tassertEquals(EXPECTED, result);\n+\t}\n+}",
                "raw_url": "https://github.com/apache/crunch/raw/c69a354f760f7318dc020c9825d79814ca5aa5b4/src/test/java/com/cloudera/crunch/impl/mr/collect/UnionCollectionTest.java",
                "sha": "4691e606c355817b6f771a4bebdd5587e8d48600",
                "status": "added"
            }
        ],
        "message": "Fix NPE thrown on UnionCollection write() or materialize() method calls",
        "parent": "https://github.com/apache/crunch/commit/db5054f6f8835e499a512353e69881c067e68fc2",
        "patched_files": [
            "MRPipeline.java",
            "MemCollection.java",
            "UnionCollection.java"
        ],
        "repo": "crunch",
        "unit_tests": [
            "UnionCollectionTest.java"
        ]
    },
    "crunch_d743ce7": {
        "bug_id": "crunch_d743ce7",
        "commit": "https://github.com/apache/crunch/commit/d743ce7c8aa9107c3c73bac51bdb5ec3c761f094",
        "file": [
            {
                "additions": 101,
                "blob_url": "https://github.com/apache/crunch/blob/d743ce7c8aa9107c3c73bac51bdb5ec3c761f094/crunch/src/it/java/org/apache/crunch/PTableUnionTest.java",
                "changes": 101,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/crunch/src/it/java/org/apache/crunch/PTableUnionTest.java?ref=d743ce7c8aa9107c3c73bac51bdb5ec3c761f094",
                "deletions": 0,
                "filename": "crunch/src/it/java/org/apache/crunch/PTableUnionTest.java",
                "patch": "@@ -0,0 +1,101 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.crunch;\n+\n+import static org.junit.Assert.assertNotNull;\n+\n+import java.io.IOException;\n+\n+import org.apache.crunch.PCollection;\n+import org.apache.crunch.PTable;\n+import org.apache.crunch.fn.IdentityFn;\n+import org.apache.crunch.impl.mr.MRPipeline;\n+import org.apache.crunch.test.TemporaryPath;\n+import org.apache.crunch.test.TemporaryPaths;\n+import org.apache.crunch.types.avro.Avros;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+\n+\n+public class PTableUnionTest {\n+\n+  public static class FirstLetterKeyFn extends DoFn<String, Pair<String, String>> {\n+\n+    private static final long serialVersionUID = 5517897875971194220L;\n+\n+    @Override\n+    public void process(String input, Emitter<Pair<String, String>> emitter) {\n+      if (input.length() > 0) {\n+        emitter.emit(Pair.of(input.substring(0, 1), input));\n+      }\n+    }\n+  }\n+  \n+  @Rule\n+  public TemporaryPath tmpDir = TemporaryPaths.create();\n+  \n+  protected MRPipeline pipeline;\n+\n+  @Before\n+  public void setUp() {\n+    pipeline = new MRPipeline(this.getClass(), tmpDir.getDefaultConfiguration());\n+  }\n+\n+  @After\n+  public void tearDown() {\n+    pipeline.done();\n+  }\n+\n+  @Test\n+  public void tableUnionMaterializeNPE() throws Exception {\n+    PCollection<String> words = pipeline.readTextFile(tmpDir.copyResourceFileName(\"shakes.txt\"));\n+    PCollection<String> lorum = pipeline.readTextFile(tmpDir.copyResourceFileName(\"maugham.txt\"));\n+    lorum.materialize();\n+\n+    PTable<String, String> wordsByFirstLetter =\n+        words.parallelDo(\"byFirstLetter\", new FirstLetterKeyFn(), Avros.tableOf(Avros.strings(), Avros.strings()));\n+    PTable<String, String> lorumByFirstLetter =\n+        lorum.parallelDo(\"byFirstLetter\", new FirstLetterKeyFn(), Avros.tableOf(Avros.strings(), Avros.strings()));\n+\n+    @SuppressWarnings(\"unchecked\")\n+    PTable<String, String> union = wordsByFirstLetter.union(lorumByFirstLetter);\n+\n+    assertNotNull(union.materialize().iterator().next());\n+  }\n+\n+  @Test\n+  public void collectionUnionMaterializeNPE() throws Exception {\n+    PCollection<String> words = pipeline.readTextFile(tmpDir.copyResourceFileName(\"shakes.txt\"));\n+    PCollection<String> lorum = pipeline.readTextFile(tmpDir.copyResourceFileName(\"maugham.txt\"));\n+    lorum.materialize();\n+\n+    IdentityFn<String> identity = IdentityFn.getInstance();\n+    words = words.parallelDo(identity, Avros.strings());\n+    lorum = lorum.parallelDo(identity, Avros.strings());\n+\n+    @SuppressWarnings(\"unchecked\")\n+    PCollection<String> union = words.union(lorum);\n+\n+    union.materialize().iterator();\n+    \n+    assertNotNull(union.materialize().iterator().next());\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/crunch/raw/d743ce7c8aa9107c3c73bac51bdb5ec3c761f094/crunch/src/it/java/org/apache/crunch/PTableUnionTest.java",
                "sha": "1d31096ee3b9694aa05dbbfa051455fd04131eb6",
                "status": "added"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/crunch/blob/d743ce7c8aa9107c3c73bac51bdb5ec3c761f094/crunch/src/main/java/org/apache/crunch/impl/mr/MRPipeline.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/crunch/src/main/java/org/apache/crunch/impl/mr/MRPipeline.java?ref=d743ce7c8aa9107c3c73bac51bdb5ec3c761f094",
                "deletions": 1,
                "filename": "crunch/src/main/java/org/apache/crunch/impl/mr/MRPipeline.java",
                "patch": "@@ -285,7 +285,7 @@ private void addOutput(PCollectionImpl<?> impl, Target target) {\n    */\n   private <T> PCollectionImpl<T> toPcollectionImpl(PCollection<T> pcollection) {\n     PCollectionImpl<T> pcollectionImpl = null;\n-    if (pcollection instanceof UnionCollection) {\n+    if (pcollection instanceof UnionCollection || pcollection instanceof UnionTable) {\n       pcollectionImpl = (PCollectionImpl<T>) pcollection.parallelDo(\"UnionCollectionWrapper\",\n           (MapFn) IdentityFn.<Object> getInstance(), pcollection.getPType());\n     } else {",
                "raw_url": "https://github.com/apache/crunch/raw/d743ce7c8aa9107c3c73bac51bdb5ec3c761f094/crunch/src/main/java/org/apache/crunch/impl/mr/MRPipeline.java",
                "sha": "6ef7491c0804b0549ee09d530beb34163f5d42f9",
                "status": "modified"
            }
        ],
        "message": "CRUNCH-154: Fix NPE on materialized union of two PTables",
        "parent": "https://github.com/apache/crunch/commit/035b1b91d60c1ed5029135d73706ffd54b184a8c",
        "patched_files": [
            "MRPipeline.java"
        ],
        "repo": "crunch",
        "unit_tests": [
            "PTableUnionTest.java",
            "MRPipelineTest.java"
        ]
    },
    "crunch_f57c8fc": {
        "bug_id": "crunch_f57c8fc",
        "commit": "https://github.com/apache/crunch/commit/f57c8fc0fc110e9effb95a622aa54c3817c81869",
        "file": [
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/crunch/blob/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-core/src/it/java/org/apache/crunch/ExternalFilesystemIT.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/crunch-core/src/it/java/org/apache/crunch/ExternalFilesystemIT.java?ref=f57c8fc0fc110e9effb95a622aa54c3817c81869",
                "deletions": 1,
                "filename": "crunch-core/src/it/java/org/apache/crunch/ExternalFilesystemIT.java",
                "patch": "@@ -101,7 +101,7 @@ public void testReadWrite() throws Exception {\n         pipeline.run();\n \n         // assert the output was written correctly\n-        try (FSDataInputStream inputStream = dfsCluster2.open(new Path(\"hdfs://cluster2/output/out0-m-00000\"))) {\n+        try (FSDataInputStream inputStream = dfsCluster2.open(new Path(\"hdfs://cluster2/output/part-m-00000\"))) {\n             String readValue = IOUtils.toString(inputStream).trim();\n             Assert.assertEquals(testString, readValue);\n         }",
                "raw_url": "https://github.com/apache/crunch/raw/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-core/src/it/java/org/apache/crunch/ExternalFilesystemIT.java",
                "sha": "a4efc8b58435625af0ef7f3834b068724f97966d",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/crunch/blob/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-core/src/main/java/org/apache/crunch/impl/mr/run/RuntimeParameters.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/crunch-core/src/main/java/org/apache/crunch/impl/mr/run/RuntimeParameters.java?ref=f57c8fc0fc110e9effb95a622aa54c3817c81869",
                "deletions": 0,
                "filename": "crunch-core/src/main/java/org/apache/crunch/impl/mr/run/RuntimeParameters.java",
                "patch": "@@ -51,6 +51,8 @@\n \n   public static final String FILE_TARGET_MAX_DISTCP_TASKS = \"crunch.file.target.max.distcp.tasks\";\n \n+  public static final String FILE_TARGET_MAX_DISTCP_TASK_BANDWIDTH_MB = \"crunch.file.target.max.distcp.task.bandwidth.mb\";\n+\n   // Not instantiated\n   private RuntimeParameters() {\n   }",
                "raw_url": "https://github.com/apache/crunch/raw/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-core/src/main/java/org/apache/crunch/impl/mr/run/RuntimeParameters.java",
                "sha": "bc15169088c8263081ba6ae794e7830c1a613c5d",
                "status": "modified"
            },
            {
                "additions": 44,
                "blob_url": "https://github.com/apache/crunch/blob/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-core/src/main/java/org/apache/crunch/io/impl/FileTargetImpl.java",
                "changes": 66,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/crunch-core/src/main/java/org/apache/crunch/io/impl/FileTargetImpl.java?ref=f57c8fc0fc110e9effb95a622aa54c3817c81869",
                "deletions": 22,
                "filename": "crunch-core/src/main/java/org/apache/crunch/io/impl/FileTargetImpl.java",
                "patch": "@@ -51,13 +51,16 @@\n import org.apache.crunch.io.SourceTargetHelper;\n import org.apache.crunch.types.Converter;\n import org.apache.crunch.types.PType;\n+import org.apache.crunch.util.CrunchRenameCopyListing;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.mapreduce.Job;\n import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n+import org.apache.hadoop.tools.CopyListing;\n import org.apache.hadoop.tools.DistCp;\n+import org.apache.hadoop.tools.DistCpConstants;\n import org.apache.hadoop.tools.DistCpOptions;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n@@ -194,14 +197,17 @@ public void handleOutputs(Configuration conf, Path workingPath, int index) throw\n     Path srcPattern = getSourcePattern(workingPath, index);\n     boolean sameFs = isCompatible(srcFs, path);\n     boolean useDistributedCopy = conf.getBoolean(RuntimeParameters.FILE_TARGET_USE_DISTCP, true);\n-    int maxDistributedCopyTasks = conf.getInt(RuntimeParameters.FILE_TARGET_MAX_DISTCP_TASKS, 1000);\n+    int maxDistributedCopyTasks = conf.getInt(RuntimeParameters.FILE_TARGET_MAX_DISTCP_TASKS, 100);\n+    int maxDistributedCopyTaskBandwidthMB = conf.getInt(RuntimeParameters.FILE_TARGET_MAX_DISTCP_TASK_BANDWIDTH_MB,\n+        DistCpConstants.DEFAULT_BANDWIDTH_MB);\n     int maxThreads = conf.getInt(RuntimeParameters.FILE_TARGET_MAX_THREADS, 1);\n \n     if (!sameFs) {\n       if (useDistributedCopy) {\n         LOG.info(\"Source and destination are in different file systems, performing distributed copy from {} to {}\", srcPattern,\n             path);\n-        handleOutputsDistributedCopy(dstFsConf, srcPattern, srcFs, dstFs, maxDistributedCopyTasks);\n+        handleOutputsDistributedCopy(conf, srcPattern, srcFs, dstFs, maxDistributedCopyTasks,\n+            maxDistributedCopyTaskBandwidthMB);\n       } else {\n         LOG.info(\"Source and destination are in different file systems, performing asynch copies from {} to {}\", srcPattern, path);\n         handleOutputsAsynchronously(conf, srcPattern, srcFs, dstFs, sameFs, maxThreads);\n@@ -210,18 +216,17 @@ public void handleOutputs(Configuration conf, Path workingPath, int index) throw\n       LOG.info(\"Source and destination are in the same file system, performing asynch renames from {} to {}\", srcPattern, path);\n       handleOutputsAsynchronously(conf, srcPattern, srcFs, dstFs, sameFs, maxThreads);\n     }\n-\n   }\n \n   private void handleOutputsAsynchronously(Configuration conf, Path srcPattern, FileSystem srcFs, FileSystem dstFs,\n           boolean sameFs, int maxThreads) throws IOException {\n+    Configuration dstFsConf = getEffectiveBundleConfig(conf);\n     Path[] srcs = FileUtil.stat2Paths(srcFs.globStatus(srcPattern), srcPattern);\n     List<ListenableFuture<Boolean>> renameFutures = Lists.newArrayList();\n     ListeningExecutorService executorService =\n         MoreExecutors.listeningDecorator(\n             Executors.newFixedThreadPool(\n                 maxThreads));\n-    Configuration dstFsConf = getEffectiveBundleConfig(conf);\n     for (Path s : srcs) {\n       Path d = getDestFile(dstFsConf, s, path, s.getName().contains(\"-m-\"));\n       renameFutures.add(\n@@ -255,26 +260,12 @@ private void handleOutputsAsynchronously(Configuration conf, Path srcPattern, Fi\n   }\n \n   private void handleOutputsDistributedCopy(Configuration conf, Path srcPattern, FileSystem srcFs, FileSystem dstFs,\n-          int maxDistributedCopyTasks) throws IOException {\n+          int maxTasks, int maxBandwidthMB) throws IOException {\n+    Configuration dstFsConf = getEffectiveBundleConfig(conf);\n     Path[] srcs = FileUtil.stat2Paths(srcFs.globStatus(srcPattern), srcPattern);\n     if (srcs.length > 0) {\n-      LOG.info(\"Distributed copying {} files using at most {} tasks\", srcs.length, maxDistributedCopyTasks);\n-      // Once https://issues.apache.org/jira/browse/HADOOP-15281 is available, we can use the direct write\n-      // distcp optimization if the target path is in S3\n-      DistCpOptions options = new DistCpOptions(Arrays.asList(srcs), path);\n-      options.setMaxMaps(maxDistributedCopyTasks);\n-      options.setOverwrite(true);\n-      options.setBlocking(true);\n-\n-      Configuration distCpConf = new Configuration(conf);\n-      // Remove unnecessary and problematic properties from the DistCp configuration. This is necessary since\n-      // files referenced by these properties may have already been deleted when the DistCp is being started.\n-      distCpConf.unset(\"mapreduce.job.cache.files\");\n-      distCpConf.unset(\"mapreduce.job.classpath.files\");\n-      distCpConf.unset(\"tmpjars\");\n-\n       try {\n-        DistCp distCp = new DistCp(distCpConf, options);\n+        DistCp distCp = createDistCp(srcs, maxTasks, maxBandwidthMB, dstFsConf);\n         if (!distCp.execute().isSuccessful()) {\n           throw new CrunchRuntimeException(\"Distributed copy failed from \" + srcPattern + \" to \" + path);\n         }\n@@ -329,7 +320,38 @@ protected Path getDestFile(Configuration conf, Path src, Path dir, boolean mapOn\n     }\n     return new Path(dir, outputFilename);\n   }\n-  \n+\n+  protected DistCp createDistCp(Path[] srcs, int maxTasks, int maxBandwidthMB, Configuration conf) throws Exception {\n+    LOG.info(\"Distributed copying {} files using at most {} tasks and bandwidth limit of {} MB/s per task\",\n+        new Object[]{srcs.length, maxTasks, maxBandwidthMB});\n+\n+    Configuration distCpConf = new Configuration(conf);\n+\n+    // Remove unnecessary and problematic properties from the DistCp configuration. This is necessary since\n+    // files referenced by these properties may have already been deleted when the DistCp is being started.\n+    distCpConf.unset(\"mapreduce.job.cache.files\");\n+    distCpConf.unset(\"mapreduce.job.classpath.files\");\n+    distCpConf.unset(\"tmpjars\");\n+\n+    // Setup renaming for part files\n+    List<String> renames = Lists.newArrayList();\n+    for (Path s : srcs) {\n+      Path d = getDestFile(conf, s, path, s.getName().contains(\"-m-\"));\n+      renames.add(s.getName() + \":\" + d.getName());\n+    }\n+    distCpConf.setStrings(CrunchRenameCopyListing.DISTCP_PATH_RENAMES, renames.toArray(new String[renames.size()]));\n+    distCpConf.setClass(DistCpConstants.CONF_LABEL_COPY_LISTING_CLASS, CrunchRenameCopyListing.class, CopyListing.class);\n+\n+    // Once https://issues.apache.org/jira/browse/HADOOP-15281 is available, we can use the direct write\n+    // distcp optimization if the target path is in S3\n+    DistCpOptions options = new DistCpOptions(Arrays.asList(srcs), path);\n+    options.setMaxMaps(maxTasks);\n+    options.setMapBandwidth(maxBandwidthMB);\n+    options.setBlocking(true);\n+\n+    return new DistCp(distCpConf, options);\n+  }\n+\n   /**\n    * Extract the partition number from a raw reducer output filename.\n    *",
                "raw_url": "https://github.com/apache/crunch/raw/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-core/src/main/java/org/apache/crunch/io/impl/FileTargetImpl.java",
                "sha": "ce47bcccc281dcd7a70ff9f6098956e08cc2ac59",
                "status": "modified"
            },
            {
                "additions": 272,
                "blob_url": "https://github.com/apache/crunch/blob/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-core/src/main/java/org/apache/crunch/util/CrunchRenameCopyListing.java",
                "changes": 272,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/crunch-core/src/main/java/org/apache/crunch/util/CrunchRenameCopyListing.java?ref=f57c8fc0fc110e9effb95a622aa54c3817c81869",
                "deletions": 0,
                "filename": "crunch-core/src/main/java/org/apache/crunch/util/CrunchRenameCopyListing.java",
                "patch": "@@ -0,0 +1,272 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information regarding copyright ownership.  The ASF licenses this file to you under the\n+ * Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License.  You may obtain a\n+ * copy of the License at\n+ * <p>\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ * <p>\n+ * Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\"\n+ * BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language\n+ * governing permissions and limitations under the License.\n+ */\n+package org.apache.crunch.util;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.io.SequenceFile;\n+import org.apache.hadoop.io.Text;\n+import org.apache.hadoop.security.Credentials;\n+import org.apache.hadoop.tools.CopyListing;\n+import org.apache.hadoop.tools.CopyListingFileStatus;\n+import org.apache.hadoop.tools.DistCpOptions;\n+import org.apache.hadoop.tools.DistCpOptions.FileAttribute;\n+import org.apache.hadoop.tools.SimpleCopyListing;\n+import org.apache.hadoop.tools.util.DistCpUtils;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+import java.io.IOException;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Stack;\n+\n+/**\n+ * A custom {@link CopyListing} implementation capable of dynamically renaming\n+ * the target paths according to a {@link #DISTCP_PATH_RENAMES configured set of values}.\n+ * <p>\n+ * Once https://issues.apache.org/jira/browse/HADOOP-16147 is available, this\n+ * class can be significantly simplified.\n+ * </p>\n+ */\n+public class CrunchRenameCopyListing extends SimpleCopyListing {\n+  /**\n+   * Comma-separated list of original-file:renamed-file path rename pairs.\n+   */\n+  public static final String DISTCP_PATH_RENAMES = \"crunch.distcp.path.renames\";\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(CrunchRenameCopyListing.class);\n+  private final Map<String, String> pathRenames;\n+\n+  private long totalPaths = 0;\n+  private long totalBytesToCopy = 0;\n+\n+  /**\n+   * Constructor, to initialize configuration.\n+   *\n+   * @param configuration The input configuration, with which the source/target FileSystems may be accessed.\n+   * @param credentials - Credentials object on which the FS delegation tokens are cached. If null\n+   * delegation token caching is skipped\n+   */\n+  public CrunchRenameCopyListing(Configuration configuration, Credentials credentials) {\n+    super(configuration, credentials);\n+\n+    pathRenames = new HashMap<>();\n+\n+    String[] pathRenameConf = configuration.getStrings(DISTCP_PATH_RENAMES);\n+    if (pathRenameConf == null) {\n+      throw new IllegalArgumentException(\"Missing required configuration: \" + DISTCP_PATH_RENAMES);\n+    }\n+    for (String pathRename : pathRenameConf) {\n+      String[] pathRenameParts = pathRename.split(\":\");\n+      if (pathRenameParts.length != 2) {\n+        throw new IllegalArgumentException(\"Invalid path rename format: \" + pathRename);\n+      }\n+      if (pathRenames.put(pathRenameParts[0], pathRenameParts[1]) != null) {\n+        throw new IllegalArgumentException(\"Invalid duplicate path rename: \" + pathRenameParts[0]);\n+      }\n+    }\n+    LOG.info(\"Loaded {} path rename entries\", pathRenames.size());\n+\n+    // Clear out the rename configuration property, as it is no longer needed\n+    configuration.unset(DISTCP_PATH_RENAMES);\n+  }\n+\n+  @Override\n+  public void doBuildListing(SequenceFile.Writer fileListWriter, DistCpOptions options) throws IOException {\n+    try {\n+      for (Path path : options.getSourcePaths()) {\n+        FileSystem sourceFS = path.getFileSystem(getConf());\n+        final boolean preserveAcls = options.shouldPreserve(FileAttribute.ACL);\n+        final boolean preserveXAttrs = options.shouldPreserve(FileAttribute.XATTR);\n+        final boolean preserveRawXAttrs = options.shouldPreserveRawXattrs();\n+        path = makeQualified(path);\n+\n+        FileStatus rootStatus = sourceFS.getFileStatus(path);\n+        Path sourcePathRoot = computeSourceRootPath(rootStatus, options);\n+\n+        FileStatus[] sourceFiles = sourceFS.listStatus(path);\n+        boolean explore = (sourceFiles != null && sourceFiles.length > 0);\n+        if (!explore || rootStatus.isDirectory()) {\n+          CopyListingFileStatus rootCopyListingStatus = DistCpUtils.toCopyListingFileStatus(sourceFS, rootStatus, preserveAcls,\n+              preserveXAttrs, preserveRawXAttrs);\n+          writeToFileListingRoot(fileListWriter, rootCopyListingStatus, sourcePathRoot, options);\n+        }\n+        if (explore) {\n+          for (FileStatus sourceStatus : sourceFiles) {\n+            if (LOG.isDebugEnabled()) {\n+              LOG.debug(\"Recording source-path: {} for copy.\", sourceStatus.getPath());\n+            }\n+            CopyListingFileStatus sourceCopyListingStatus = DistCpUtils.toCopyListingFileStatus(sourceFS, sourceStatus,\n+                preserveAcls && sourceStatus.isDirectory(), preserveXAttrs && sourceStatus.isDirectory(),\n+                preserveRawXAttrs && sourceStatus.isDirectory());\n+            writeToFileListing(fileListWriter, sourceCopyListingStatus, sourcePathRoot, options);\n+\n+            if (isDirectoryAndNotEmpty(sourceFS, sourceStatus)) {\n+              if (LOG.isDebugEnabled()) {\n+                LOG.debug(\"Traversing non-empty source dir: {}\", sourceStatus.getPath());\n+              }\n+              traverseNonEmptyDirectory(fileListWriter, sourceStatus, sourcePathRoot, options);\n+            }\n+          }\n+        }\n+      }\n+      fileListWriter.close();\n+      fileListWriter = null;\n+    } finally {\n+      if (fileListWriter != null) {\n+        try {\n+          fileListWriter.close();\n+        } catch(IOException e) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Exception in closing {}\", fileListWriter, e);\n+          }\n+        }\n+      }\n+    }\n+  }\n+\n+  private Path computeSourceRootPath(FileStatus sourceStatus, DistCpOptions options) throws IOException {\n+    Path target = options.getTargetPath();\n+    FileSystem targetFS = target.getFileSystem(getConf());\n+    final boolean targetPathExists = options.getTargetPathExists();\n+\n+    boolean solitaryFile = options.getSourcePaths().size() == 1 && !sourceStatus.isDirectory();\n+\n+    if (solitaryFile) {\n+      if (targetFS.isFile(target) || !targetPathExists) {\n+        return sourceStatus.getPath();\n+      } else {\n+        return sourceStatus.getPath().getParent();\n+      }\n+    } else {\n+      boolean specialHandling =\n+          (options.getSourcePaths().size() == 1 && !targetPathExists) || options.shouldSyncFolder() || options.shouldOverwrite();\n+\n+      return specialHandling && sourceStatus.isDirectory() ? sourceStatus.getPath() : sourceStatus.getPath().getParent();\n+    }\n+  }\n+\n+  private Path makeQualified(Path path) throws IOException {\n+    final FileSystem fs = path.getFileSystem(getConf());\n+    return path.makeQualified(fs.getUri(), fs.getWorkingDirectory());\n+  }\n+\n+  private static boolean isDirectoryAndNotEmpty(FileSystem fileSystem, FileStatus fileStatus) throws IOException {\n+    return fileStatus.isDirectory() && getChildren(fileSystem, fileStatus).length > 0;\n+  }\n+\n+  private static FileStatus[] getChildren(FileSystem fileSystem, FileStatus parent) throws IOException {\n+    return fileSystem.listStatus(parent.getPath());\n+  }\n+\n+  private void traverseNonEmptyDirectory(SequenceFile.Writer fileListWriter, FileStatus sourceStatus, Path sourcePathRoot,\n+      DistCpOptions options) throws IOException {\n+    FileSystem sourceFS = sourcePathRoot.getFileSystem(getConf());\n+    final boolean preserveAcls = options.shouldPreserve(FileAttribute.ACL);\n+    final boolean preserveXAttrs = options.shouldPreserve(FileAttribute.XATTR);\n+    final boolean preserveRawXattrs = options.shouldPreserveRawXattrs();\n+    Stack<FileStatus> pathStack = new Stack<>();\n+    pathStack.push(sourceStatus);\n+\n+    while (!pathStack.isEmpty()) {\n+      for (FileStatus child : getChildren(sourceFS, pathStack.pop())) {\n+        if (LOG.isDebugEnabled()) {\n+          LOG.debug(\"Recording source-path: {} for copy.\", sourceStatus.getPath());\n+        }\n+        CopyListingFileStatus childCopyListingStatus = DistCpUtils.toCopyListingFileStatus(sourceFS, child,\n+            preserveAcls && child.isDirectory(), preserveXAttrs && child.isDirectory(), preserveRawXattrs && child.isDirectory());\n+        writeToFileListing(fileListWriter, childCopyListingStatus, sourcePathRoot, options);\n+        if (isDirectoryAndNotEmpty(sourceFS, child)) {\n+          if (LOG.isDebugEnabled()) {\n+            LOG.debug(\"Traversing non-empty source dir: {}\", sourceStatus.getPath());\n+          }\n+          pathStack.push(child);\n+        }\n+      }\n+    }\n+  }\n+\n+  private void writeToFileListingRoot(SequenceFile.Writer fileListWriter, CopyListingFileStatus fileStatus, Path sourcePathRoot,\n+      DistCpOptions options) throws IOException {\n+    boolean syncOrOverwrite = options.shouldSyncFolder() || options.shouldOverwrite();\n+    if (fileStatus.getPath().equals(sourcePathRoot) && fileStatus.isDirectory() && syncOrOverwrite) {\n+      // Skip the root-paths when syncOrOverwrite\n+      if (LOG.isDebugEnabled()) {\n+        LOG.debug(\"Skip {}\", fileStatus.getPath());\n+      }\n+      return;\n+    }\n+    writeToFileListing(fileListWriter, fileStatus, sourcePathRoot, options);\n+  }\n+\n+  private void writeToFileListing(SequenceFile.Writer fileListWriter, CopyListingFileStatus fileStatus, Path sourcePathRoot,\n+      DistCpOptions options) throws IOException {\n+    if (LOG.isDebugEnabled()) {\n+      LOG.debug(\"REL PATH: {}, FULL PATH: {}\",\n+          DistCpUtils.getRelativePath(sourcePathRoot, fileStatus.getPath()), fileStatus.getPath());\n+    }\n+\n+    if (!shouldCopy(fileStatus.getPath(), options)) {\n+      return;\n+    }\n+\n+    fileListWriter.append(getFileListingKey(sourcePathRoot, fileStatus),\n+        getFileListingValue(fileStatus));\n+    fileListWriter.sync();\n+\n+    if (!fileStatus.isDirectory()) {\n+      totalBytesToCopy += fileStatus.getLen();\n+    }\n+    totalPaths++;\n+  }\n+\n+  /**\n+   * Returns the key for an entry in the copy listing sequence file\n+   * @param sourcePathRoot the root source path for determining the relative target path\n+   * @param fileStatus the copy listing file status\n+   * @return the key for the sequence file entry\n+   */\n+  protected Text getFileListingKey(Path sourcePathRoot, CopyListingFileStatus fileStatus) {\n+    Path fileStatusPath = fileStatus.getPath();\n+    String pathName = fileStatusPath.getName();\n+    String renamedPathName = pathRenames.get(pathName);\n+\n+    if (renamedPathName != null && !pathName.equals(renamedPathName)) {\n+      LOG.info(\"Applying dynamic rename of {} to {}\", pathName, renamedPathName);\n+      fileStatusPath = new Path(fileStatusPath.getParent(), renamedPathName);\n+    }\n+    return new Text(DistCpUtils.getRelativePath(sourcePathRoot, fileStatusPath));\n+  }\n+\n+  /**\n+   * Returns the value for an entry in the copy listing sequence file\n+   * @param fileStatus the copy listing file status\n+   * @return the value for the sequence file entry\n+   */\n+  protected CopyListingFileStatus getFileListingValue(CopyListingFileStatus fileStatus) {\n+    return fileStatus;\n+  }\n+\n+  @Override\n+  protected long getBytesToCopy() {\n+    return totalBytesToCopy;\n+  }\n+\n+  @Override\n+  protected long getNumberOfPaths() {\n+    return totalPaths;\n+  }\n+}\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/crunch/raw/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-core/src/main/java/org/apache/crunch/util/CrunchRenameCopyListing.java",
                "sha": "b930bebc2baccda250a3c5844ad71c1772bd75ae",
                "status": "added"
            },
            {
                "additions": 0,
                "blob_url": "https://github.com/apache/crunch/blob/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-hbase/src/main/java/org/apache/crunch/io/hbase/HFileTarget.java",
                "changes": 71,
                "contents_url": "https://api.github.com/repos/apache/crunch/contents/crunch-hbase/src/main/java/org/apache/crunch/io/hbase/HFileTarget.java?ref=f57c8fc0fc110e9effb95a622aa54c3817c81869",
                "deletions": 71,
                "filename": "crunch-hbase/src/main/java/org/apache/crunch/io/hbase/HFileTarget.java",
                "patch": "@@ -17,35 +17,22 @@\n  */\n package org.apache.crunch.io.hbase;\n \n-import org.apache.crunch.CrunchRuntimeException;\n-import org.apache.crunch.impl.mr.run.RuntimeParameters;\n import org.apache.crunch.io.SequentialFileNamingScheme;\n import org.apache.crunch.io.impl.FileTargetImpl;\n import org.apache.crunch.types.Converter;\n import org.apache.crunch.types.PTableType;\n import org.apache.crunch.types.PType;\n import org.apache.hadoop.conf.Configuration;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.FileUtil;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.hbase.Cell;\n import org.apache.hadoop.hbase.HBaseConfiguration;\n import org.apache.hadoop.hbase.HColumnDescriptor;\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable;\n import org.apache.hadoop.hbase.mapreduce.KeyValueSerialization;\n import org.apache.hadoop.mapreduce.Job;\n-import org.apache.hadoop.tools.DistCp;\n-import org.apache.hadoop.tools.DistCpOptions;\n-import org.slf4j.Logger;\n-import org.slf4j.LoggerFactory;\n-\n-import java.io.IOException;\n-import java.util.Arrays;\n \n public class HFileTarget extends FileTargetImpl {\n \n-  private static final Logger LOG = LoggerFactory.getLogger(HFileTarget.class);\n-\n   public HFileTarget(String path) {\n     this(new Path(path));\n   }\n@@ -90,64 +77,6 @@ public void configureForMapReduce(Job job, PType<?> ptype, Path outputPath, Stri\n     return new HBaseValueConverter<Cell>(Cell.class);\n   }\n \n-  @Override\n-  public void handleOutputs(Configuration conf, Path workingPath, int index) throws IOException {\n-    FileSystem srcFs = workingPath.getFileSystem(conf);\n-    Path src = getSourcePattern(workingPath, index);\n-    Path[] srcs = FileUtil.stat2Paths(srcFs.globStatus(src), src);\n-    FileSystem dstFs = path.getFileSystem(conf);\n-    if (!dstFs.exists(path)) {\n-      dstFs.mkdirs(path);\n-    }\n-    boolean sameFs = isCompatible(srcFs, path);\n-\n-    if (!sameFs) {\n-      if (srcs.length > 0) {\n-        int maxDistributedCopyTasks = conf.getInt(RuntimeParameters.FILE_TARGET_MAX_DISTCP_TASKS, 1000);\n-        LOG.info(\n-                \"Source and destination are in different file systems, performing distcp of {} files from [{}] to [{}] \"\n-                        + \"using at most {} tasks\",\n-                new Object[] { srcs.length, src, path, maxDistributedCopyTasks });\n-        // Once https://issues.apache.org/jira/browse/HADOOP-15281 is available, we can use the direct write\n-        // distcp optimization if the target path is in S3\n-        DistCpOptions options = new DistCpOptions(Arrays.asList(srcs), path);\n-        options.setMaxMaps(maxDistributedCopyTasks);\n-        options.setOverwrite(true);\n-        options.setBlocking(true);\n-\n-        Configuration distCpConf = new Configuration(conf);\n-        // Remove unnecessary and problematic properties from the DistCp configuration. This is necessary since\n-        // files referenced by these properties may have already been deleted when the DistCp is being started.\n-        distCpConf.unset(\"mapreduce.job.cache.files\");\n-        distCpConf.unset(\"mapreduce.job.classpath.files\");\n-        distCpConf.unset(\"tmpjars\");\n-\n-        try {\n-          DistCp distCp = new DistCp(distCpConf, options);\n-          if (!distCp.execute().isSuccessful()) {\n-            throw new CrunchRuntimeException(\"Unable to move files through distcp from \" + src + \" to \" + path);\n-          }\n-          LOG.info(\"Distributed copy completed for {} files\", srcs.length);\n-        } catch (Exception e) {\n-          throw new CrunchRuntimeException(\"Unable to move files through distcp from \" + src + \" to \" + path, e);\n-        }\n-      } else {\n-        LOG.info(\"No files found at [{}], not attempting to copy HFiles\", src);\n-      }\n-    } else {\n-      LOG.info(\n-              \"Source and destination are in the same file system, performing rename of {} files from [{}] to [{}]\",\n-              new Object[] { srcs.length, src, path });\n-\n-      for (Path s : srcs) {\n-        Path d = getDestFile(conf, s, path, s.getName().contains(\"-m-\"));\n-        srcFs.rename(s, d);\n-      }\n-    }\n-    dstFs.create(getSuccessIndicator(), true).close();\n-    LOG.info(\"Created success indicator file\");\n-  }\n-\n   @Override\n   public String toString() {\n     return \"HFile(\" + path + \")\";",
                "raw_url": "https://github.com/apache/crunch/raw/f57c8fc0fc110e9effb95a622aa54c3817c81869/crunch-hbase/src/main/java/org/apache/crunch/io/hbase/HFileTarget.java",
                "sha": "b1ce5ba34dcf7ef9d8ec5d472f7195ff750c7436",
                "status": "modified"
            }
        ],
        "message": "CRUNCH-679: Improvements for usage of DistCp (#20)\n\n* CRUNCH-679: Improvements for usage of DistCp\r\n\r\n* CRUNCH-679: Fix NPE bug by preserving IOUtils.cleanup logic\r\n\r\n* CRUNCH-679: CrunchRenameCopyListing's constructor needs to be public\r\n\r\n* CRUNCH-679: Unset rename configuration after loading into copy listing\r\n\r\n* CRUNCH-679: Reduce default max distcp map tasks from 1000 to 100\r\n\r\n* CRUNCH-679: Update log message formatting",
        "parent": "https://github.com/apache/crunch/commit/587e2c9eaaec8ae9a26e6b1b5c99be7f4d521951",
        "patched_files": [
            "HFileTarget.java",
            "FileTargetImpl.java"
        ],
        "repo": "crunch",
        "unit_tests": [
            "HFileTargetTest.java",
            "FileTargetImplTest.java"
        ]
    }
}