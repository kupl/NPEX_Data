[{"commit": "https://github.com/apache/storm/commit/55f346cb093d4f219ecb43b1e6b1a7c2ea5d9a5f", "parent": "https://github.com/apache/storm/commit/7b1a98fc10fad516ef9ed0b3afc53a1d7be8a169", "message": "STORM-3211: Fix NPE in WindowedBoltExecutor on getComponentConfiguration", "bug_id": "storm_1", "file": [{"additions": 2, "raw_url": "https://github.com/apache/storm/raw/55f346cb093d4f219ecb43b1e6b1a7c2ea5d9a5f/storm-client/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java", "blob_url": "https://github.com/apache/storm/blob/55f346cb093d4f219ecb43b1e6b1a7c2ea5d9a5f/storm-client/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java", "sha": "4713e95d39b7ca69904893abd3bc156a16f125de", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java?ref=55f346cb093d4f219ecb43b1e6b1a7c2ea5d9a5f", "patch": "@@ -16,6 +16,7 @@\n import static org.apache.storm.topology.base.BaseWindowedBolt.Duration;\n \n import java.util.Collection;\n+import java.util.Collections;\n import java.util.HashSet;\n import java.util.Iterator;\n import java.util.List;\n@@ -345,7 +346,7 @@ public void declareOutputFields(OutputFieldsDeclarer declarer) {\n \n     @Override\n     public Map<String, Object> getComponentConfiguration() {\n-        return bolt.getComponentConfiguration();\n+        return bolt.getComponentConfiguration() != null ? bolt.getComponentConfiguration() : Collections.emptyMap();\n     }\n \n     protected WindowLifecycleListener<Tuple> newWindowLifecycleListener() {", "filename": "storm-client/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java"}, {"additions": 9, "raw_url": "https://github.com/apache/storm/raw/55f346cb093d4f219ecb43b1e6b1a7c2ea5d9a5f/storm-client/test/jvm/org/apache/storm/topology/WindowedBoltExecutorTest.java", "blob_url": "https://github.com/apache/storm/blob/55f346cb093d4f219ecb43b1e6b1a7c2ea5d9a5f/storm-client/test/jvm/org/apache/storm/topology/WindowedBoltExecutorTest.java", "sha": "0e388388ae2716d7ee9c14add836b841f694aad9", "changes": 9, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/test/jvm/org/apache/storm/topology/WindowedBoltExecutorTest.java?ref=55f346cb093d4f219ecb43b1e6b1a7c2ea5d9a5f", "patch": "@@ -39,6 +39,7 @@\n import static org.junit.Assert.assertArrayEquals;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertThat;\n+import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n \n /**\n@@ -214,6 +215,14 @@ public void testExecuteWithLateTupleStream() throws Exception {\n         Mockito.verify(outputCollector).emit(\"$late\", Arrays.asList(tuple), new Values(tuple));\n     }\n \n+    @Test\n+    public void testEmptyConfigOnWrappedBolt() {\n+        IWindowedBolt wrappedBolt = Mockito.mock(IWindowedBolt.class);\n+        Mockito.when(wrappedBolt.getComponentConfiguration()).thenReturn(null);\n+        executor = new WindowedBoltExecutor(wrappedBolt);\n+        assertTrue(\"Configuration is not empty\", executor.getComponentConfiguration().isEmpty());\n+    }\n+\n     private static class TestWindowedBolt extends BaseWindowedBolt {\n         List<TupleWindow> tupleWindows = new ArrayList<>();\n ", "filename": "storm-client/test/jvm/org/apache/storm/topology/WindowedBoltExecutorTest.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/9165a783590a65e935e5ee229abbad2942b12333", "parent": "https://github.com/apache/storm/commit/dfb9b55e10b60e8755c8552eee4563ec3c86aa77", "message": "Merge pull request #3156 from efgpinto/STORM-3211\n\nSTORM-3211: Fix NPE in WindowedBoltExecutor on getComponentConfiguration", "bug_id": "storm_2", "file": [{"additions": 2, "raw_url": "https://github.com/apache/storm/raw/9165a783590a65e935e5ee229abbad2942b12333/storm-client/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java", "blob_url": "https://github.com/apache/storm/blob/9165a783590a65e935e5ee229abbad2942b12333/storm-client/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java", "sha": "4713e95d39b7ca69904893abd3bc156a16f125de", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java?ref=9165a783590a65e935e5ee229abbad2942b12333", "patch": "@@ -16,6 +16,7 @@\n import static org.apache.storm.topology.base.BaseWindowedBolt.Duration;\n \n import java.util.Collection;\n+import java.util.Collections;\n import java.util.HashSet;\n import java.util.Iterator;\n import java.util.List;\n@@ -345,7 +346,7 @@ public void declareOutputFields(OutputFieldsDeclarer declarer) {\n \n     @Override\n     public Map<String, Object> getComponentConfiguration() {\n-        return bolt.getComponentConfiguration();\n+        return bolt.getComponentConfiguration() != null ? bolt.getComponentConfiguration() : Collections.emptyMap();\n     }\n \n     protected WindowLifecycleListener<Tuple> newWindowLifecycleListener() {", "filename": "storm-client/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java"}, {"additions": 9, "raw_url": "https://github.com/apache/storm/raw/9165a783590a65e935e5ee229abbad2942b12333/storm-client/test/jvm/org/apache/storm/topology/WindowedBoltExecutorTest.java", "blob_url": "https://github.com/apache/storm/blob/9165a783590a65e935e5ee229abbad2942b12333/storm-client/test/jvm/org/apache/storm/topology/WindowedBoltExecutorTest.java", "sha": "0e388388ae2716d7ee9c14add836b841f694aad9", "changes": 9, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/test/jvm/org/apache/storm/topology/WindowedBoltExecutorTest.java?ref=9165a783590a65e935e5ee229abbad2942b12333", "patch": "@@ -39,6 +39,7 @@\n import static org.junit.Assert.assertArrayEquals;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertThat;\n+import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n \n /**\n@@ -214,6 +215,14 @@ public void testExecuteWithLateTupleStream() throws Exception {\n         Mockito.verify(outputCollector).emit(\"$late\", Arrays.asList(tuple), new Values(tuple));\n     }\n \n+    @Test\n+    public void testEmptyConfigOnWrappedBolt() {\n+        IWindowedBolt wrappedBolt = Mockito.mock(IWindowedBolt.class);\n+        Mockito.when(wrappedBolt.getComponentConfiguration()).thenReturn(null);\n+        executor = new WindowedBoltExecutor(wrappedBolt);\n+        assertTrue(\"Configuration is not empty\", executor.getComponentConfiguration().isEmpty());\n+    }\n+\n     private static class TestWindowedBolt extends BaseWindowedBolt {\n         List<TupleWindow> tupleWindows = new ArrayList<>();\n ", "filename": "storm-client/test/jvm/org/apache/storm/topology/WindowedBoltExecutorTest.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/d1912ae98afe9f470a05e57835c41f056cebb311", "parent": "https://github.com/apache/storm/commit/9f10f8a14482e4eb1c1323ac86d3c373634539c2", "message": "STORM-3372: Fix NPE when shutting down HdfsBolt, fix storm-hdfs tests not running", "bug_id": "storm_3", "file": [{"additions": 5, "raw_url": "https://github.com/apache/storm/raw/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs-blobstore/pom.xml", "blob_url": "https://github.com/apache/storm/blob/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs-blobstore/pom.xml", "sha": "ccc69a63ed0bde5d19dd2ec10c590cc9a85a30f8", "changes": 5, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs-blobstore/pom.xml?ref=d1912ae98afe9f470a05e57835c41f056cebb311", "patch": "@@ -208,6 +208,11 @@\n             <artifactId>guava</artifactId>\n             <version>${guava.version}</version>\n         </dependency>\n+        <dependency>\n+            <groupId>org.junit.jupiter</groupId>\n+            <artifactId>junit-jupiter-params</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n     </dependencies>\n     <build>\n         <plugins>", "filename": "external/storm-hdfs-blobstore/pom.xml"}, {"additions": 127, "raw_url": "https://github.com/apache/storm/raw/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs-blobstore/src/test/java/org/apache/storm/hdfs/blobstore/BlobStoreTest.java", "blob_url": "https://github.com/apache/storm/blob/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs-blobstore/src/test/java/org/apache/storm/hdfs/blobstore/BlobStoreTest.java", "sha": "53cca758f6173b3df8f863f7716ba5ff24e8136d", "changes": 266, "status": "modified", "deletions": 139, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs-blobstore/src/test/java/org/apache/storm/hdfs/blobstore/BlobStoreTest.java?ref=d1912ae98afe9f470a05e57835c41f056cebb311", "patch": "@@ -18,6 +18,7 @@\n  */\n package org.apache.storm.hdfs.blobstore;\n \n+import org.apache.storm.hdfs.testing.MiniDFSClusterExtension;\n import org.apache.commons.io.FileUtils;\n import org.apache.storm.Config;\n import org.apache.storm.blobstore.AtomicOutputStream;\n@@ -28,14 +29,9 @@\n import org.apache.storm.generated.AuthorizationException;\n import org.apache.storm.generated.KeyNotFoundException;\n import org.apache.storm.generated.SettableBlobMeta;\n-import org.apache.storm.hdfs.testing.MiniDFSClusterRule;\n import org.apache.storm.security.auth.FixedGroupsMapping;\n import org.apache.storm.security.auth.NimbusPrincipal;\n import org.apache.storm.security.auth.SingleUserPrincipal;\n-import org.junit.After;\n-import org.junit.Before;\n-import org.junit.ClassRule;\n-import org.junit.Test;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -55,30 +51,34 @@\n \n import static org.junit.Assert.*;\n \n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.RegisterExtension;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.EnumSource;\n+import org.junit.jupiter.params.provider.ValueSource;\n+\n public class BlobStoreTest {\n \n-    @ClassRule\n-    public static final MiniDFSClusterRule DFS_CLUSTER_RULE = new MiniDFSClusterRule();\n+    @RegisterExtension\n+    public static final MiniDFSClusterExtension DFS_CLUSTER_EXTENSION = new MiniDFSClusterExtension();\n \n     private static final Logger LOG = LoggerFactory.getLogger(BlobStoreTest.class);\n     URI base;\n-    File baseFile;\n     private static final Map<String, Object> CONF = new HashMap<>();\n     public static final int READ = 0x01;\n     public static final int WRITE = 0x02;\n     public static final int ADMIN = 0x04;\n \n-    @Before\n+    @BeforeEach\n     public void init() {\n         initializeConfigs();\n-        baseFile = new File(\"/tmp/blob-store-test-\" + UUID.randomUUID());\n-        base = baseFile.toURI();\n     }\n \n-    @After\n+    @AfterEach\n     public void cleanup()\n         throws IOException {\n-        FileUtils.deleteDirectory(baseFile);\n     }\n \n     // Method which initializes nimbus admin\n@@ -160,7 +160,7 @@ private AutoCloseableBlobStoreContainer initHdfs(String dirName)\n         conf.put(Config.STORM_PRINCIPAL_TO_LOCAL_PLUGIN, \"org.apache.storm.security.auth.DefaultPrincipalToLocal\");\n         conf.put(Config.STORM_BLOBSTORE_REPLICATION_FACTOR, 3);\n         HdfsBlobStore store = new HdfsBlobStore();\n-        store.prepareInternal(conf, null, DFS_CLUSTER_RULE.getDfscluster().getConfiguration(0));\n+        store.prepareInternal(conf, null, DFS_CLUSTER_EXTENSION.getDfscluster().getConfiguration(0));\n         return new AutoCloseableBlobStoreContainer(store);\n     }\n \n@@ -204,15 +204,6 @@ public void testMultipleHdfs()\n         }\n     }\n \n-    @Test\n-    public void testHdfsWithAuth()\n-        throws Exception {\n-        // use different blobstore dir so it doesn't conflict with other tests\n-        try (AutoCloseableBlobStoreContainer container = initHdfs(\"/storm/blobstore3\")) {\n-            testWithAuthentication(container.blobStore);\n-        }\n-    }\n-\n     // Test for replication.\n     public void testReplication(String path, BlobStore store)\n         throws Exception {\n@@ -289,133 +280,130 @@ public void testReplication(String path, BlobStore store)\n         store.deleteBlob(\"test\", getSubject(createSubject));\n     }\n \n-    public Subject getSubject(String name) {\n+    public static Subject getSubject(String name) {\n         Subject subject = new Subject();\n         SingleUserPrincipal user = new SingleUserPrincipal(name);\n         subject.getPrincipals().add(user);\n         return subject;\n     }\n-\n-    // Check for Blobstore with authentication\n-    public void testWithAuthentication(BlobStore store)\n-        throws Exception {\n-        //Test for Nimbus Admin\n-        Subject admin = getSubject(\"admin\");\n-        assertStoreHasExactly(store);\n-        SettableBlobMeta metadata = new SettableBlobMeta(BlobStoreAclHandler.DEFAULT);\n-        try (AtomicOutputStream out = store.createBlob(\"test\", metadata, admin)) {\n-            assertStoreHasExactly(store, \"test\");\n-            out.write(1);\n+    \n+    static enum AuthenticationTestSubject {\n+        //Nimbus Admin\n+        ADMIN(getSubject(\"admin\")),\n+        //Nimbus groups admin\n+        ADMIN_GROUPS_USER(getSubject(\"adminGroupsUser\")),\n+        //Supervisor admin\n+        SUPERVISOR(getSubject(\"supervisor\")),\n+        //Nimbus itself\n+        NIMBUS(getNimbusSubject());\n+        \n+        private Subject subject;\n+\n+        private AuthenticationTestSubject(Subject subject) {\n+            this.subject = subject;\n         }\n-        store.deleteBlob(\"test\", admin);\n-\n-        //Test for Nimbus Groups Admin\n-        Subject adminsGroupsUser = getSubject(\"adminsGroupsUser\");\n-        assertStoreHasExactly(store);\n-        metadata = new SettableBlobMeta(BlobStoreAclHandler.DEFAULT);\n-        try (AtomicOutputStream out = store.createBlob(\"test\", metadata, adminsGroupsUser)) {\n-            assertStoreHasExactly(store, \"test\");\n-            out.write(1);\n+    }\n+    \n+    @ParameterizedTest\n+    @EnumSource(value = AuthenticationTestSubject.class)\n+    void testWithAuthentication(AuthenticationTestSubject testSubject) throws Exception {\n+        try (AutoCloseableBlobStoreContainer container = initHdfs(\"/storm/blobstore-auth-\" + testSubject.name())) {\n+            BlobStore store = container.blobStore;\n+            assertStoreHasExactly(store);\n+            SettableBlobMeta metadata = new SettableBlobMeta(BlobStoreAclHandler.DEFAULT);\n+            try (AtomicOutputStream out = store.createBlob(\"test\", metadata, testSubject.subject)) {\n+                assertStoreHasExactly(store, \"test\");\n+                out.write(1);\n+            }\n+            store.deleteBlob(\"test\", testSubject.subject);\n         }\n-        store.deleteBlob(\"test\", adminsGroupsUser);\n-\n-        //Test for Supervisor Admin\n-        Subject supervisor = getSubject(\"supervisor\");\n-        assertStoreHasExactly(store);\n-        metadata = new SettableBlobMeta(BlobStoreAclHandler.DEFAULT);\n-        try (AtomicOutputStream out = store.createBlob(\"test\", metadata, supervisor)) {\n+    }\n+    \n+    @ParameterizedTest\n+    @ValueSource(booleans = {true, false})\n+    void testWithAuthenticationDummy(boolean securityEnabled) throws Exception {\n+        try (AutoCloseableBlobStoreContainer container = initHdfs(\"/storm/blobstore-auth-dummy-sec-\" + securityEnabled)) {\n+            BlobStore store = container.blobStore;\n+            Subject who = getSubject(\"test_subject\");\n+            assertStoreHasExactly(store);\n+\n+            // Tests for case when subject != null (security turned on) and\n+            // acls for the blob are set to WORLD_EVERYTHING\n+            SettableBlobMeta metadata = new SettableBlobMeta(securityEnabled ? BlobStoreAclHandler.DEFAULT : BlobStoreAclHandler.WORLD_EVERYTHING);\n+            try (AtomicOutputStream out = store.createBlob(\"test\", metadata, who)) {\n+                out.write(1);\n+            }\n             assertStoreHasExactly(store, \"test\");\n-            out.write(1);\n+            if (securityEnabled) {\n+                // Testing whether acls are set to WORLD_EVERYTHING. Here the acl should not contain WORLD_EVERYTHING because\n+                // the subject is neither null nor empty. The ACL should however contain USER_EVERYTHING as user needs to have\n+                // complete access to the blob\n+                assertTrue(\"ACL contains WORLD_EVERYTHING\", !metadata.toString().contains(\"AccessControl(type:OTHER, access:7)\"));\n+            } else {\n+                // Testing whether acls are set to WORLD_EVERYTHING\n+                assertTrue(\"ACL does not contain WORLD_EVERYTHING\", metadata.toString().contains(\"AccessControl(type:OTHER, access:7)\"));\n+            }\n+            \n+            readAssertEqualsWithAuth(store, who, \"test\", 1);\n+\n+            LOG.info(\"Deleting test\");\n+            store.deleteBlob(\"test\", who);\n+            assertStoreHasExactly(store);\n         }\n-        store.deleteBlob(\"test\", supervisor);\n-\n-        //Test for Nimbus itself as a user\n-        Subject nimbus = getNimbusSubject();\n-        assertStoreHasExactly(store);\n-        metadata = new SettableBlobMeta(BlobStoreAclHandler.DEFAULT);\n-        try (AtomicOutputStream out = store.createBlob(\"test\", metadata, nimbus)) {\n+    }\n+    \n+    @Test\n+    void testWithAuthenticationUpdate() throws Exception {\n+        try (AutoCloseableBlobStoreContainer container = initHdfs(\"/storm/blobstore-auth-update\")) {\n+            BlobStore store = container.blobStore;\n+            Subject who = getSubject(\"test_subject\");\n+            assertStoreHasExactly(store);\n+\n+            SettableBlobMeta metadata = new SettableBlobMeta(BlobStoreAclHandler.DEFAULT);\n+            try (AtomicOutputStream out = store.createBlob(\"test\", metadata, who)) {\n+                out.write(1);\n+            }\n             assertStoreHasExactly(store, \"test\");\n-            out.write(1);\n-        }\n-        store.deleteBlob(\"test\", nimbus);\n-\n-        // Test with a dummy test_subject for cases where subject !=null (security turned on)\n-        Subject who = getSubject(\"test_subject\");\n-        assertStoreHasExactly(store);\n-\n-        // Tests for case when subject != null (security turned on) and\n-        // acls for the blob are set to WORLD_EVERYTHING\n-        metadata = new SettableBlobMeta(BlobStoreAclHandler.WORLD_EVERYTHING);\n-        try (AtomicOutputStream out = store.createBlob(\"test\", metadata, who)) {\n-            out.write(1);\n-        }\n-        assertStoreHasExactly(store, \"test\");\n-        // Testing whether acls are set to WORLD_EVERYTHING\n-        assertTrue(\"ACL does not contain WORLD_EVERYTHING\", metadata.toString().contains(\"AccessControl(type:OTHER, access:7)\"));\n-        readAssertEqualsWithAuth(store, who, \"test\", 1);\n-\n-        LOG.info(\"Deleting test\");\n-        store.deleteBlob(\"test\", who);\n-        assertStoreHasExactly(store);\n-\n-        // Tests for case when subject != null (security turned on) and\n-        // acls are not set for the blob (DEFAULT)\n-        LOG.info(\"Creating test again\");\n-        metadata = new SettableBlobMeta(BlobStoreAclHandler.DEFAULT);\n-        try (AtomicOutputStream out = store.createBlob(\"test\", metadata, who)) {\n-            out.write(2);\n-        }\n-        assertStoreHasExactly(store, \"test\");\n-        // Testing whether acls are set to WORLD_EVERYTHING. Here the acl should not contain WORLD_EVERYTHING because\n-        // the subject is neither null nor empty. The ACL should however contain USER_EVERYTHING as user needs to have\n-        // complete access to the blob\n-        assertTrue(\"ACL does not contain WORLD_EVERYTHING\", !metadata.toString().contains(\"AccessControl(type:OTHER, access:7)\"));\n-        readAssertEqualsWithAuth(store, who, \"test\", 2);\n-\n-        LOG.info(\"Updating test\");\n-        try (AtomicOutputStream out = store.updateBlob(\"test\", who)) {\n-            out.write(3);\n-        }\n-        assertStoreHasExactly(store, \"test\");\n-        readAssertEqualsWithAuth(store, who, \"test\", 3);\n-\n-        LOG.info(\"Updating test again\");\n-        try (AtomicOutputStream out = store.updateBlob(\"test\", who)) {\n-            out.write(4);\n-        }\n-        LOG.info(\"SLEEPING\");\n-        Thread.sleep(2);\n-        assertStoreHasExactly(store, \"test\");\n-        readAssertEqualsWithAuth(store, who, \"test\", 3);\n+            readAssertEqualsWithAuth(store, who, \"test\", 1);\n+            \n+            try (AtomicOutputStream out = store.updateBlob(\"test\", who)) {\n+                out.write(2);\n+            }\n+            assertStoreHasExactly(store, \"test\");\n+            readAssertEqualsWithAuth(store, who, \"test\", 2);\n+            \n+            try (AtomicOutputStream out = store.updateBlob(\"test\", who)) {\n+                out.write(3);\n+            }\n+            assertStoreHasExactly(store, \"test\");\n+            readAssertEqualsWithAuth(store, who, \"test\", 3);\n \n-        //Test for subject with no principals and acls set to WORLD_EVERYTHING\n-        who = new Subject();\n-        metadata = new SettableBlobMeta(BlobStoreAclHandler.WORLD_EVERYTHING);\n-        LOG.info(\"Creating test\");\n-        try (AtomicOutputStream out = store.createBlob(\"test-empty-subject-WE\", metadata, who)) {\n-            out.write(2);\n+            LOG.info(\"Deleting test\");\n+            store.deleteBlob(\"test\", who);\n+            assertStoreHasExactly(store);\n         }\n-        assertStoreHasExactly(store, \"test-empty-subject-WE\", \"test\");\n-        // Testing whether acls are set to WORLD_EVERYTHING\n-        assertTrue(\"ACL does not contain WORLD_EVERYTHING\", metadata.toString().contains(\"AccessControl(type:OTHER, access:7)\"));\n-        readAssertEqualsWithAuth(store, who, \"test-empty-subject-WE\", 2);\n-\n-        //Test for subject with no principals and acls set to DEFAULT\n-        who = new Subject();\n-        metadata = new SettableBlobMeta(BlobStoreAclHandler.DEFAULT);\n-        LOG.info(\"Creating other\");\n-        try (AtomicOutputStream out = store.createBlob(\"test-empty-subject-DEF\", metadata, who)) {\n-            out.write(2);\n-        }\n-        assertStoreHasExactly(store, \"test-empty-subject-DEF\", \"test\", \"test-empty-subject-WE\");\n-        // Testing whether acls are set to WORLD_EVERYTHING\n-        assertTrue(\"ACL does not contain WORLD_EVERYTHING\", metadata.toString().contains(\"AccessControl(type:OTHER, access:7)\"));\n-        readAssertEqualsWithAuth(store, who, \"test-empty-subject-DEF\", 2);\n-\n-        if (store instanceof HdfsBlobStore) {\n-            ((HdfsBlobStore) store).fullCleanup(1);\n-        } else {\n-            fail(\"Error the blobstore is of unknowntype\");\n+    }\n+    \n+    @ParameterizedTest\n+    @ValueSource(booleans = {true, false})\n+    void testWithAuthenticationNoPrincipal(boolean securityEnabled) throws Exception {\n+        try (AutoCloseableBlobStoreContainer container = initHdfs(\"/storm/blobstore-auth-no-principal-sec-\" + securityEnabled)) {\n+            BlobStore store = container.blobStore;\n+            //Test for subject with no principals\n+            Subject who = new Subject();\n+            assertStoreHasExactly(store);\n+\n+            // Tests for case when subject != null (security turned on) and\n+            // acls for the blob are set to WORLD_EVERYTHING\n+            SettableBlobMeta metadata = new SettableBlobMeta(securityEnabled ? BlobStoreAclHandler.DEFAULT : BlobStoreAclHandler.WORLD_EVERYTHING);\n+            try (AtomicOutputStream out = store.createBlob(\"test\", metadata, who)) {\n+                out.write(1);\n+            }\n+            assertStoreHasExactly(store, \"test\");\n+            // With no principals in the subject ACL should always be set to WORLD_EVERYTHING\n+            assertTrue(\"ACL does not contain WORLD_EVERYTHING\", metadata.toString().contains(\"AccessControl(type:OTHER, access:7)\"));\n+            \n+            readAssertEqualsWithAuth(store, who, \"test\", 1);\n         }\n     }\n \n@@ -535,6 +523,6 @@ public void testMultiple(BlobStore store)\n             fail(\"Error the blobstore is of unknowntype\");\n         }\n         assertStoreHasExactly(store, \"test\");\n-        readAssertEquals(store, \"test\", 3);\n+        readAssertEquals(store, \"test\", 4);\n     }\n }", "filename": "external/storm-hdfs-blobstore/src/test/java/org/apache/storm/hdfs/blobstore/BlobStoreTest.java"}, {"additions": 1, "raw_url": "https://github.com/apache/storm/raw/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/pom.xml", "blob_url": "https://github.com/apache/storm/blob/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/pom.xml", "sha": "a19d821e623954b08142c75b96c9111c85e3564c", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/pom.xml?ref=d1912ae98afe9f470a05e57835c41f056cebb311", "patch": "@@ -234,7 +234,7 @@\n             <plugin>\n                 <groupId>org.apache.maven.plugins</groupId>\n                 <artifactId>maven-surefire-plugin</artifactId>\n-\t\t<configuration>\n+                <configuration>\n                     <reuseForks>false</reuseForks>\n                     <forkCount>1</forkCount>\n                 </configuration>", "filename": "external/storm-hdfs/pom.xml"}, {"additions": 3, "raw_url": "https://github.com/apache/storm/raw/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java", "blob_url": "https://github.com/apache/storm/blob/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java", "sha": "a145274b187dfb121e8c6b04689a103c550832c9", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java?ref=d1912ae98afe9f470a05e57835c41f056cebb311", "patch": "@@ -247,7 +247,9 @@ public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {\n     @Override\n     public void cleanup() {\n         doRotationAndRemoveAllWriters();\n-        this.rotationTimer.cancel();\n+        if (this.rotationTimer != null) {\n+            this.rotationTimer.cancel();\n+        }\n     }\n \n     private void doRotationAndRemoveAllWriters() {", "filename": "external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java"}, {"additions": 12, "raw_url": "https://github.com/apache/storm/raw/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/TestHdfsBolt.java", "blob_url": "https://github.com/apache/storm/blob/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/TestHdfsBolt.java", "sha": "7f63cc0969aa80c401c2fa3c8bdde8c97c4d1ab9", "changes": 12, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/TestHdfsBolt.java?ref=d1912ae98afe9f470a05e57835c41f056cebb311", "patch": "@@ -56,6 +56,7 @@\n import static org.mockito.Mockito.verify;\n import static org.mockito.Mockito.verifyZeroInteractions;\n \n+\n @RunWith(MockitoJUnitRunner.class)\n public class TestHdfsBolt {\n \n@@ -203,6 +204,17 @@ public void testTickTuples() throws IOException {\n         //Tick should have flushed it\n         Assert.assertEquals(1, countNonZeroLengthFiles(testRoot));\n     }\n+    \n+    @Test\n+    public void testCleanupDoesNotThrowExceptionWhenRotationPolicyIsNotTimed() {\n+        //STORM-3372: Rotation policy other than TimedRotationPolicy causes NPE on cleanup\n+        FileRotationPolicy fieldsRotationPolicy =\n+            new FileSizeRotationPolicy(10_000, FileSizeRotationPolicy.Units.MB);\n+        HdfsBolt bolt = makeHdfsBolt(hdfsURI, 10, 10000f)\n+            .withRotationPolicy(fieldsRotationPolicy);\n+        bolt.prepare(new Config(), topologyContext, collector);\n+        bolt.cleanup();\n+    }\n \n     public void createBaseDirectory(FileSystem passedFs, String path) throws IOException {\n         Path p = new Path(path);", "filename": "external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/TestHdfsBolt.java"}, {"additions": 1, "raw_url": "https://github.com/apache/storm/raw/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/format/TestSimpleFileNameFormat.java", "blob_url": "https://github.com/apache/storm/blob/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/format/TestSimpleFileNameFormat.java", "sha": "f8e1e5e30208236b5cf307385b40048e8871c046", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/format/TestSimpleFileNameFormat.java?ref=d1912ae98afe9f470a05e57835c41f056cebb311", "patch": "@@ -69,7 +69,7 @@ public void testTimeFormat() {\n     }\n \n     private TopologyContext createTopologyContext(Map<String, Object> topoConf) {\n-        Map<Integer, String> taskToComponent = new HashMap<Integer, String>();\n+        Map<Integer, String> taskToComponent = new HashMap<>();\n         taskToComponent.put(7, \"Xcom\");\n         return new TopologyContext(null, topoConf, taskToComponent, null, null, null, null, null, null, 7, 6703, null, null, null, null,\n                                    null, null, null);", "filename": "external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/format/TestSimpleFileNameFormat.java"}, {"additions": 3, "raw_url": "https://github.com/apache/storm/raw/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSemantics.java", "blob_url": "https://github.com/apache/storm/blob/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSemantics.java", "sha": "3528a3df3d4184ed1e7164786e3e9ad69dcd1931", "changes": 5, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSemantics.java?ref=d1912ae98afe9f470a05e57835c41f056cebb311", "patch": "@@ -12,6 +12,8 @@\n \n package org.apache.storm.hdfs.spout;\n \n+import static org.hamcrest.core.IsNull.notNullValue;\n+\n import java.io.IOException;\n import org.apache.hadoop.fs.CommonConfigurationKeys;\n import org.apache.hadoop.fs.FSDataOutputStream;\n@@ -30,7 +32,6 @@\n \n import static org.junit.Assert.assertThat;\n import static org.junit.Assert.fail;\n-import static org.mockito.ArgumentMatchers.notNull;\n \n public class TestHdfsSemantics {\n \n@@ -124,7 +125,7 @@ public void testAppendSemantics() throws Exception {\n \n         //2 try to append to a closed file\n         try (FSDataOutputStream os2 = fs.append(file1)) {\n-            assertThat(os2, notNull());\n+            assertThat(os2, notNullValue());\n         }\n     }\n ", "filename": "external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSemantics.java"}, {"additions": 13, "raw_url": "https://github.com/apache/storm/raw/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSpout.java", "blob_url": "https://github.com/apache/storm/blob/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSpout.java", "sha": "133de5d7a08aef3180c1dff50a7dbaf4cab41236", "changes": 25, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSpout.java?ref=d1912ae98afe9f470a05e57835c41f056cebb311", "patch": "@@ -12,6 +12,9 @@\n \n package org.apache.storm.hdfs.spout;\n \n+import static org.hamcrest.core.Is.is;\n+import static org.junit.Assert.assertThat;\n+\n import java.io.BufferedReader;\n import java.io.File;\n import java.io.IOException;\n@@ -192,6 +195,9 @@ public void testEmptySimpleText_ACK() throws Exception {\n         Path file1 = new Path(source.toString() + \"/file_empty.txt\");\n         createTextFile(file1, 0);\n \n+        //Ensure the second file has a later modified timestamp, as the spout should pick the first file first.\n+        Thread.sleep(2);\n+\n         Path file2 = new Path(source.toString() + \"/file.txt\");\n         createTextFile(file2, 5);\n \n@@ -203,15 +209,13 @@ public void testEmptySimpleText_ACK() throws Exception {\n             conf.put(Config.TOPOLOGY_ACKER_EXECUTORS, \"1\"); // enable ACKing\n             openSpout(spout, 0, conf);\n \n-            // consume empty file\n-            runSpout(spout, \"r1\");\n-            Path arc1 = new Path(archive.toString() + \"/file_empty.txt\");\n-            checkCollectorOutput_txt((MockCollector) spout.getCollector(), arc1);\n-\n-            // consume file 2\n-            runSpout(spout, \"r5\", \"a0\", \"a1\", \"a2\", \"a3\", \"a4\");\n+            // Read once. Since the first file is empty, the spout should continue with file 2\n+            runSpout(spout, \"r6\", \"a0\", \"a1\", \"a2\", \"a3\", \"a4\");\n+            //File 1 should be moved to archive\n+            assertThat(fs.isFile(new Path(archive.toString() + \"/file_empty.txt\")), is(true));\n+            //File 2 should be read\n             Path arc2 = new Path(archive.toString() + \"/file.txt\");\n-            checkCollectorOutput_txt((MockCollector) spout.getCollector(), arc1, arc2);\n+            checkCollectorOutput_txt((MockCollector) spout.getCollector(), arc2);\n         }\n     }\n \n@@ -681,11 +685,8 @@ private void openSpout(HdfsSpout spout, int spoutId, Map<String, Object> topoCon\n \n     private void createTextFile(Path file, int lineCount) throws IOException {\n         FSDataOutputStream os = fs.create(file);\n-        int size = 0;\n         for (int i = 0; i < lineCount; i++) {\n             os.writeBytes(\"line \" + i + System.lineSeparator());\n-            String msg = \"line \" + i + System.lineSeparator();\n-            size += msg.getBytes().length;\n         }\n         os.close();\n     }\n@@ -772,7 +773,7 @@ public MockTextFailingReader(FileSystem fs, Path file, Map<String, Object> conf)\n         private final int componentId;\n \n         public MockTopologyContext(int componentId, Map<String, Object> topoConf) {\n-            super(null, topoConf, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null);\n+            super(null, topoConf, null, null, null, null, null, null, null, 0, 0, null, null, null, null, null, null, null);\n             this.componentId = componentId;\n         }\n ", "filename": "external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSpout.java"}, {"additions": 64, "raw_url": "https://github.com/apache/storm/raw/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/testing/MiniDFSClusterExtension.java", "blob_url": "https://github.com/apache/storm/blob/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/testing/MiniDFSClusterExtension.java", "sha": "f88fef5838697a3e9df3b64914f431ce927617f2", "changes": 64, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/testing/MiniDFSClusterExtension.java?ref=d1912ae98afe9f470a05e57835c41f056cebb311", "patch": "@@ -0,0 +1,64 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.storm.hdfs.testing;\n+\n+import java.util.function.Supplier;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.junit.jupiter.api.extension.AfterEachCallback;\n+import org.junit.jupiter.api.extension.BeforeEachCallback;\n+import org.junit.jupiter.api.extension.ExtensionContext;\n+\n+public class MiniDFSClusterExtension implements BeforeEachCallback, AfterEachCallback {\n+\n+    private static final String TEST_BUILD_DATA = \"test.build.data\";\n+\n+    private final Supplier<Configuration> hadoopConfSupplier;\n+    private Configuration hadoopConf;\n+    private MiniDFSCluster dfscluster;\n+\n+    public MiniDFSClusterExtension() {\n+        this(() -> new Configuration());\n+    }\n+\n+    public MiniDFSClusterExtension(Supplier<Configuration> hadoopConfSupplier) {\n+        this.hadoopConfSupplier = hadoopConfSupplier;\n+    }\n+\n+    public Configuration getHadoopConf() {\n+        return hadoopConf;\n+    }\n+\n+    public MiniDFSCluster getDfscluster() {\n+        return dfscluster;\n+    }\n+\n+    @Override\n+    public void beforeEach(ExtensionContext arg0) throws Exception {\n+        System.setProperty(TEST_BUILD_DATA, \"target/test/data\");\n+        hadoopConf = hadoopConfSupplier.get();\n+        dfscluster = new MiniDFSCluster.Builder(hadoopConf).numDataNodes(3).build();\n+        dfscluster.waitActive();\n+    }\n+\n+    @Override\n+    public void afterEach(ExtensionContext arg0) throws Exception {\n+        dfscluster.shutdown();\n+        System.clearProperty(TEST_BUILD_DATA);\n+    }\n+}", "filename": "external/storm-hdfs/src/test/java/org/apache/storm/hdfs/testing/MiniDFSClusterExtension.java"}, {"additions": 5, "raw_url": "https://github.com/apache/storm/raw/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/testing/MiniDFSClusterRule.java", "blob_url": "https://github.com/apache/storm/blob/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/testing/MiniDFSClusterRule.java", "sha": "6265a52ee9556112500d34aeef3e0a649c3bfe95", "changes": 5, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/testing/MiniDFSClusterRule.java?ref=d1912ae98afe9f470a05e57835c41f056cebb311", "patch": "@@ -23,6 +23,10 @@\n import org.junit.runner.Description;\n import org.junit.runners.model.Statement;\n \n+/**\n+ * @deprecated Use {@link MiniDFSClusterExtension} instead, along with JUnit 5 for new tests.\n+ */\n+@Deprecated\n public class MiniDFSClusterRule implements TestRule {\n \n     private static final String TEST_BUILD_DATA = \"test.build.data\";\n@@ -57,6 +61,7 @@ public void evaluate() throws Throwable {\n                     hadoopConf = hadoopConfSupplier.get();\n                     dfscluster = new MiniDFSCluster.Builder(hadoopConf).numDataNodes(3).build();\n                     dfscluster.waitActive();\n+                    base.evaluate();\n                 } finally {\n                     if (dfscluster != null) {\n                         dfscluster.shutdown();", "filename": "external/storm-hdfs/src/test/java/org/apache/storm/hdfs/testing/MiniDFSClusterRule.java"}, {"additions": 1, "raw_url": "https://github.com/apache/storm/raw/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/src/test/resources/log4j.properties", "blob_url": "https://github.com/apache/storm/blob/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/src/test/resources/log4j.properties", "sha": "c952abd128f4ccff65eff0682c2f6420cd4621e1", "changes": 5, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/src/test/resources/log4j.properties?ref=d1912ae98afe9f470a05e57835c41f056cebb311", "patch": "@@ -20,7 +20,4 @@ log4j.rootLogger = WARN, out\n \n log4j.appender.out = org.apache.log4j.ConsoleAppender\n log4j.appender.out.layout = org.apache.log4j.PatternLayout\n-log4j.appender.out.layout.ConversionPattern = %d (%t) [%p - %l] %m%n\n-\n-log4j.logger.org.apache.storm.hdfs = INFO\n-\n+log4j.appender.out.layout.ConversionPattern = %d (%t) [%p - %l] %m%n\n\\ No newline at end of file", "filename": "external/storm-hdfs/src/test/resources/log4j.properties"}, {"additions": 32, "raw_url": "https://github.com/apache/storm/raw/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/src/test/resources/log4j2.xml", "blob_url": "https://github.com/apache/storm/blob/d1912ae98afe9f470a05e57835c41f056cebb311/external/storm-hdfs/src/test/resources/log4j2.xml", "sha": "546b1b380865c6dd04e7006c5dcab07888a8dab9", "changes": 32, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/src/test/resources/log4j2.xml?ref=d1912ae98afe9f470a05e57835c41f056cebb311", "patch": "@@ -0,0 +1,32 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+ Licensed to the Apache Software Foundation (ASF) under one or more\n+ contributor license agreements.  See the NOTICE file distributed with\n+ this work for additional information regarding copyright ownership.\n+ The ASF licenses this file to You under the Apache License, Version 2.0\n+ (the \"License\"); you may not use this file except in compliance with\n+ the License.  You may obtain a copy of the License at\n+\n+     http://www.apache.org/licenses/LICENSE-2.0\n+\n+ Unless required by applicable law or agreed to in writing, software\n+ distributed under the License is distributed on an \"AS IS\" BASIS,\n+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ See the License for the specific language governing permissions and\n+ limitations under the License.\n+-->\n+<Configuration status=\"WARN\">\n+    <Appenders>\n+        <Console name=\"Console\" target=\"SYSTEM_OUT\">\n+            <PatternLayout pattern=\"%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\" charset=\"UTF-8\"/>\n+        </Console>\n+    </Appenders>\n+    <Loggers>\n+        <Root level=\"WARN\">\n+            <AppenderRef ref=\"Console\"/>\n+        </Root>\n+        <Logger name=\"org.apache.storm\" level=\"INFO\" additivity=\"false\">\n+            <AppenderRef ref=\"Console\"/>\n+        </Logger>\n+    </Loggers>\n+</Configuration>\n\\ No newline at end of file", "filename": "external/storm-hdfs/src/test/resources/log4j2.xml"}, {"additions": 1, "raw_url": "https://github.com/apache/storm/raw/d1912ae98afe9f470a05e57835c41f056cebb311/pom.xml", "blob_url": "https://github.com/apache/storm/blob/d1912ae98afe9f470a05e57835c41f056cebb311/pom.xml", "sha": "3b587a7ff62e1c72d2acc15aa4b25e71b50253ce", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/pom.xml?ref=d1912ae98afe9f470a05e57835c41f056cebb311", "patch": "@@ -295,7 +295,7 @@\n         <servlet.version>3.1.0</servlet.version>\n         <joda-time.version>2.3</joda-time.version>\n         <thrift.version>0.12.0</thrift.version>\n-        <junit.jupiter.version>5.3.2</junit.jupiter.version>\n+        <junit.jupiter.version>5.5.0-M1</junit.jupiter.version>\n         <surefire.version>2.22.1</surefire.version>\n         <awaitility.version>3.1.0</awaitility.version>\n         <hdrhistogram.version>2.1.10</hdrhistogram.version>", "filename": "pom.xml"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/2576e38dfd89149ba47f1af4e74f2ee5f512d66f", "parent": "https://github.com/apache/storm/commit/0114323fc7f434d65b474798f5cfd35b85d8c3df", "message": "Merge pull request #3088 from nescohen/fix/minor-ssasl-npe\n\nSTORM-3470: fix null dereference in SimpleSaslServer authentication", "bug_id": "storm_4", "file": [{"additions": 3, "raw_url": "https://github.com/apache/storm/raw/2576e38dfd89149ba47f1af4e74f2ee5f512d66f/storm-client/src/jvm/org/apache/storm/security/auth/sasl/SimpleSaslServerCallbackHandler.java", "blob_url": "https://github.com/apache/storm/blob/2576e38dfd89149ba47f1af4e74f2ee5f512d66f/storm-client/src/jvm/org/apache/storm/security/auth/sasl/SimpleSaslServerCallbackHandler.java", "sha": "5962edaf7511261c291b751bff2cb6b0459643aa", "changes": 5, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/security/auth/sasl/SimpleSaslServerCallbackHandler.java?ref=2576e38dfd89149ba47f1af4e74f2ee5f512d66f", "patch": "@@ -16,6 +16,7 @@\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.List;\n+import java.util.Objects;\n import java.util.Optional;\n import javax.security.auth.callback.Callback;\n import javax.security.auth.callback.CallbackHandler;\n@@ -178,9 +179,9 @@ public void handle(Callback[] callbacks) throws UnsupportedCallbackException, IO\n                 ac.setAuthorizedID(zid);\n             }\n \n-            //When zid and zid are not equal, nid is attempting to impersonate zid, We\n+            //When nid and zid are not equal, nid is attempting to impersonate zid, We\n             //add the nid as the real user in reqContext's subject which will be used during authorization.\n-            if (!nid.equals(zid)) {\n+            if (!Objects.equals(nid, zid)) {\n                 LOG.info(\"Impersonation attempt  authenticationID = {} authorizationID = {}\",\n                          nid, zid);\n                 if (!allowImpersonation) {", "filename": "storm-client/src/jvm/org/apache/storm/security/auth/sasl/SimpleSaslServerCallbackHandler.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/f015130b0e4b33191f9fa84302f46f9a5e10c014", "parent": "https://github.com/apache/storm/commit/94e5ad1fba57f991257e461b512114f5555f011b", "message": "Merge pull request #2990 from srdo/STORM-3372\n\nSTORM-3372: Fix NPE when shutting down HdfsBolt, fix storm-hdfs tests not running", "bug_id": "storm_5", "file": [{"additions": 5, "raw_url": "https://github.com/apache/storm/raw/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs-blobstore/pom.xml", "blob_url": "https://github.com/apache/storm/blob/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs-blobstore/pom.xml", "sha": "ccc69a63ed0bde5d19dd2ec10c590cc9a85a30f8", "changes": 5, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs-blobstore/pom.xml?ref=f015130b0e4b33191f9fa84302f46f9a5e10c014", "patch": "@@ -208,6 +208,11 @@\n             <artifactId>guava</artifactId>\n             <version>${guava.version}</version>\n         </dependency>\n+        <dependency>\n+            <groupId>org.junit.jupiter</groupId>\n+            <artifactId>junit-jupiter-params</artifactId>\n+            <scope>test</scope>\n+        </dependency>\n     </dependencies>\n     <build>\n         <plugins>", "filename": "external/storm-hdfs-blobstore/pom.xml"}, {"additions": 127, "raw_url": "https://github.com/apache/storm/raw/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs-blobstore/src/test/java/org/apache/storm/hdfs/blobstore/BlobStoreTest.java", "blob_url": "https://github.com/apache/storm/blob/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs-blobstore/src/test/java/org/apache/storm/hdfs/blobstore/BlobStoreTest.java", "sha": "53cca758f6173b3df8f863f7716ba5ff24e8136d", "changes": 266, "status": "modified", "deletions": 139, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs-blobstore/src/test/java/org/apache/storm/hdfs/blobstore/BlobStoreTest.java?ref=f015130b0e4b33191f9fa84302f46f9a5e10c014", "patch": "@@ -18,6 +18,7 @@\n  */\n package org.apache.storm.hdfs.blobstore;\n \n+import org.apache.storm.hdfs.testing.MiniDFSClusterExtension;\n import org.apache.commons.io.FileUtils;\n import org.apache.storm.Config;\n import org.apache.storm.blobstore.AtomicOutputStream;\n@@ -28,14 +29,9 @@\n import org.apache.storm.generated.AuthorizationException;\n import org.apache.storm.generated.KeyNotFoundException;\n import org.apache.storm.generated.SettableBlobMeta;\n-import org.apache.storm.hdfs.testing.MiniDFSClusterRule;\n import org.apache.storm.security.auth.FixedGroupsMapping;\n import org.apache.storm.security.auth.NimbusPrincipal;\n import org.apache.storm.security.auth.SingleUserPrincipal;\n-import org.junit.After;\n-import org.junit.Before;\n-import org.junit.ClassRule;\n-import org.junit.Test;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -55,30 +51,34 @@\n \n import static org.junit.Assert.*;\n \n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.extension.RegisterExtension;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.EnumSource;\n+import org.junit.jupiter.params.provider.ValueSource;\n+\n public class BlobStoreTest {\n \n-    @ClassRule\n-    public static final MiniDFSClusterRule DFS_CLUSTER_RULE = new MiniDFSClusterRule();\n+    @RegisterExtension\n+    public static final MiniDFSClusterExtension DFS_CLUSTER_EXTENSION = new MiniDFSClusterExtension();\n \n     private static final Logger LOG = LoggerFactory.getLogger(BlobStoreTest.class);\n     URI base;\n-    File baseFile;\n     private static final Map<String, Object> CONF = new HashMap<>();\n     public static final int READ = 0x01;\n     public static final int WRITE = 0x02;\n     public static final int ADMIN = 0x04;\n \n-    @Before\n+    @BeforeEach\n     public void init() {\n         initializeConfigs();\n-        baseFile = new File(\"/tmp/blob-store-test-\" + UUID.randomUUID());\n-        base = baseFile.toURI();\n     }\n \n-    @After\n+    @AfterEach\n     public void cleanup()\n         throws IOException {\n-        FileUtils.deleteDirectory(baseFile);\n     }\n \n     // Method which initializes nimbus admin\n@@ -160,7 +160,7 @@ private AutoCloseableBlobStoreContainer initHdfs(String dirName)\n         conf.put(Config.STORM_PRINCIPAL_TO_LOCAL_PLUGIN, \"org.apache.storm.security.auth.DefaultPrincipalToLocal\");\n         conf.put(Config.STORM_BLOBSTORE_REPLICATION_FACTOR, 3);\n         HdfsBlobStore store = new HdfsBlobStore();\n-        store.prepareInternal(conf, null, DFS_CLUSTER_RULE.getDfscluster().getConfiguration(0));\n+        store.prepareInternal(conf, null, DFS_CLUSTER_EXTENSION.getDfscluster().getConfiguration(0));\n         return new AutoCloseableBlobStoreContainer(store);\n     }\n \n@@ -204,15 +204,6 @@ public void testMultipleHdfs()\n         }\n     }\n \n-    @Test\n-    public void testHdfsWithAuth()\n-        throws Exception {\n-        // use different blobstore dir so it doesn't conflict with other tests\n-        try (AutoCloseableBlobStoreContainer container = initHdfs(\"/storm/blobstore3\")) {\n-            testWithAuthentication(container.blobStore);\n-        }\n-    }\n-\n     // Test for replication.\n     public void testReplication(String path, BlobStore store)\n         throws Exception {\n@@ -289,133 +280,130 @@ public void testReplication(String path, BlobStore store)\n         store.deleteBlob(\"test\", getSubject(createSubject));\n     }\n \n-    public Subject getSubject(String name) {\n+    public static Subject getSubject(String name) {\n         Subject subject = new Subject();\n         SingleUserPrincipal user = new SingleUserPrincipal(name);\n         subject.getPrincipals().add(user);\n         return subject;\n     }\n-\n-    // Check for Blobstore with authentication\n-    public void testWithAuthentication(BlobStore store)\n-        throws Exception {\n-        //Test for Nimbus Admin\n-        Subject admin = getSubject(\"admin\");\n-        assertStoreHasExactly(store);\n-        SettableBlobMeta metadata = new SettableBlobMeta(BlobStoreAclHandler.DEFAULT);\n-        try (AtomicOutputStream out = store.createBlob(\"test\", metadata, admin)) {\n-            assertStoreHasExactly(store, \"test\");\n-            out.write(1);\n+    \n+    static enum AuthenticationTestSubject {\n+        //Nimbus Admin\n+        ADMIN(getSubject(\"admin\")),\n+        //Nimbus groups admin\n+        ADMIN_GROUPS_USER(getSubject(\"adminGroupsUser\")),\n+        //Supervisor admin\n+        SUPERVISOR(getSubject(\"supervisor\")),\n+        //Nimbus itself\n+        NIMBUS(getNimbusSubject());\n+        \n+        private Subject subject;\n+\n+        private AuthenticationTestSubject(Subject subject) {\n+            this.subject = subject;\n         }\n-        store.deleteBlob(\"test\", admin);\n-\n-        //Test for Nimbus Groups Admin\n-        Subject adminsGroupsUser = getSubject(\"adminsGroupsUser\");\n-        assertStoreHasExactly(store);\n-        metadata = new SettableBlobMeta(BlobStoreAclHandler.DEFAULT);\n-        try (AtomicOutputStream out = store.createBlob(\"test\", metadata, adminsGroupsUser)) {\n-            assertStoreHasExactly(store, \"test\");\n-            out.write(1);\n+    }\n+    \n+    @ParameterizedTest\n+    @EnumSource(value = AuthenticationTestSubject.class)\n+    void testWithAuthentication(AuthenticationTestSubject testSubject) throws Exception {\n+        try (AutoCloseableBlobStoreContainer container = initHdfs(\"/storm/blobstore-auth-\" + testSubject.name())) {\n+            BlobStore store = container.blobStore;\n+            assertStoreHasExactly(store);\n+            SettableBlobMeta metadata = new SettableBlobMeta(BlobStoreAclHandler.DEFAULT);\n+            try (AtomicOutputStream out = store.createBlob(\"test\", metadata, testSubject.subject)) {\n+                assertStoreHasExactly(store, \"test\");\n+                out.write(1);\n+            }\n+            store.deleteBlob(\"test\", testSubject.subject);\n         }\n-        store.deleteBlob(\"test\", adminsGroupsUser);\n-\n-        //Test for Supervisor Admin\n-        Subject supervisor = getSubject(\"supervisor\");\n-        assertStoreHasExactly(store);\n-        metadata = new SettableBlobMeta(BlobStoreAclHandler.DEFAULT);\n-        try (AtomicOutputStream out = store.createBlob(\"test\", metadata, supervisor)) {\n+    }\n+    \n+    @ParameterizedTest\n+    @ValueSource(booleans = {true, false})\n+    void testWithAuthenticationDummy(boolean securityEnabled) throws Exception {\n+        try (AutoCloseableBlobStoreContainer container = initHdfs(\"/storm/blobstore-auth-dummy-sec-\" + securityEnabled)) {\n+            BlobStore store = container.blobStore;\n+            Subject who = getSubject(\"test_subject\");\n+            assertStoreHasExactly(store);\n+\n+            // Tests for case when subject != null (security turned on) and\n+            // acls for the blob are set to WORLD_EVERYTHING\n+            SettableBlobMeta metadata = new SettableBlobMeta(securityEnabled ? BlobStoreAclHandler.DEFAULT : BlobStoreAclHandler.WORLD_EVERYTHING);\n+            try (AtomicOutputStream out = store.createBlob(\"test\", metadata, who)) {\n+                out.write(1);\n+            }\n             assertStoreHasExactly(store, \"test\");\n-            out.write(1);\n+            if (securityEnabled) {\n+                // Testing whether acls are set to WORLD_EVERYTHING. Here the acl should not contain WORLD_EVERYTHING because\n+                // the subject is neither null nor empty. The ACL should however contain USER_EVERYTHING as user needs to have\n+                // complete access to the blob\n+                assertTrue(\"ACL contains WORLD_EVERYTHING\", !metadata.toString().contains(\"AccessControl(type:OTHER, access:7)\"));\n+            } else {\n+                // Testing whether acls are set to WORLD_EVERYTHING\n+                assertTrue(\"ACL does not contain WORLD_EVERYTHING\", metadata.toString().contains(\"AccessControl(type:OTHER, access:7)\"));\n+            }\n+            \n+            readAssertEqualsWithAuth(store, who, \"test\", 1);\n+\n+            LOG.info(\"Deleting test\");\n+            store.deleteBlob(\"test\", who);\n+            assertStoreHasExactly(store);\n         }\n-        store.deleteBlob(\"test\", supervisor);\n-\n-        //Test for Nimbus itself as a user\n-        Subject nimbus = getNimbusSubject();\n-        assertStoreHasExactly(store);\n-        metadata = new SettableBlobMeta(BlobStoreAclHandler.DEFAULT);\n-        try (AtomicOutputStream out = store.createBlob(\"test\", metadata, nimbus)) {\n+    }\n+    \n+    @Test\n+    void testWithAuthenticationUpdate() throws Exception {\n+        try (AutoCloseableBlobStoreContainer container = initHdfs(\"/storm/blobstore-auth-update\")) {\n+            BlobStore store = container.blobStore;\n+            Subject who = getSubject(\"test_subject\");\n+            assertStoreHasExactly(store);\n+\n+            SettableBlobMeta metadata = new SettableBlobMeta(BlobStoreAclHandler.DEFAULT);\n+            try (AtomicOutputStream out = store.createBlob(\"test\", metadata, who)) {\n+                out.write(1);\n+            }\n             assertStoreHasExactly(store, \"test\");\n-            out.write(1);\n-        }\n-        store.deleteBlob(\"test\", nimbus);\n-\n-        // Test with a dummy test_subject for cases where subject !=null (security turned on)\n-        Subject who = getSubject(\"test_subject\");\n-        assertStoreHasExactly(store);\n-\n-        // Tests for case when subject != null (security turned on) and\n-        // acls for the blob are set to WORLD_EVERYTHING\n-        metadata = new SettableBlobMeta(BlobStoreAclHandler.WORLD_EVERYTHING);\n-        try (AtomicOutputStream out = store.createBlob(\"test\", metadata, who)) {\n-            out.write(1);\n-        }\n-        assertStoreHasExactly(store, \"test\");\n-        // Testing whether acls are set to WORLD_EVERYTHING\n-        assertTrue(\"ACL does not contain WORLD_EVERYTHING\", metadata.toString().contains(\"AccessControl(type:OTHER, access:7)\"));\n-        readAssertEqualsWithAuth(store, who, \"test\", 1);\n-\n-        LOG.info(\"Deleting test\");\n-        store.deleteBlob(\"test\", who);\n-        assertStoreHasExactly(store);\n-\n-        // Tests for case when subject != null (security turned on) and\n-        // acls are not set for the blob (DEFAULT)\n-        LOG.info(\"Creating test again\");\n-        metadata = new SettableBlobMeta(BlobStoreAclHandler.DEFAULT);\n-        try (AtomicOutputStream out = store.createBlob(\"test\", metadata, who)) {\n-            out.write(2);\n-        }\n-        assertStoreHasExactly(store, \"test\");\n-        // Testing whether acls are set to WORLD_EVERYTHING. Here the acl should not contain WORLD_EVERYTHING because\n-        // the subject is neither null nor empty. The ACL should however contain USER_EVERYTHING as user needs to have\n-        // complete access to the blob\n-        assertTrue(\"ACL does not contain WORLD_EVERYTHING\", !metadata.toString().contains(\"AccessControl(type:OTHER, access:7)\"));\n-        readAssertEqualsWithAuth(store, who, \"test\", 2);\n-\n-        LOG.info(\"Updating test\");\n-        try (AtomicOutputStream out = store.updateBlob(\"test\", who)) {\n-            out.write(3);\n-        }\n-        assertStoreHasExactly(store, \"test\");\n-        readAssertEqualsWithAuth(store, who, \"test\", 3);\n-\n-        LOG.info(\"Updating test again\");\n-        try (AtomicOutputStream out = store.updateBlob(\"test\", who)) {\n-            out.write(4);\n-        }\n-        LOG.info(\"SLEEPING\");\n-        Thread.sleep(2);\n-        assertStoreHasExactly(store, \"test\");\n-        readAssertEqualsWithAuth(store, who, \"test\", 3);\n+            readAssertEqualsWithAuth(store, who, \"test\", 1);\n+            \n+            try (AtomicOutputStream out = store.updateBlob(\"test\", who)) {\n+                out.write(2);\n+            }\n+            assertStoreHasExactly(store, \"test\");\n+            readAssertEqualsWithAuth(store, who, \"test\", 2);\n+            \n+            try (AtomicOutputStream out = store.updateBlob(\"test\", who)) {\n+                out.write(3);\n+            }\n+            assertStoreHasExactly(store, \"test\");\n+            readAssertEqualsWithAuth(store, who, \"test\", 3);\n \n-        //Test for subject with no principals and acls set to WORLD_EVERYTHING\n-        who = new Subject();\n-        metadata = new SettableBlobMeta(BlobStoreAclHandler.WORLD_EVERYTHING);\n-        LOG.info(\"Creating test\");\n-        try (AtomicOutputStream out = store.createBlob(\"test-empty-subject-WE\", metadata, who)) {\n-            out.write(2);\n+            LOG.info(\"Deleting test\");\n+            store.deleteBlob(\"test\", who);\n+            assertStoreHasExactly(store);\n         }\n-        assertStoreHasExactly(store, \"test-empty-subject-WE\", \"test\");\n-        // Testing whether acls are set to WORLD_EVERYTHING\n-        assertTrue(\"ACL does not contain WORLD_EVERYTHING\", metadata.toString().contains(\"AccessControl(type:OTHER, access:7)\"));\n-        readAssertEqualsWithAuth(store, who, \"test-empty-subject-WE\", 2);\n-\n-        //Test for subject with no principals and acls set to DEFAULT\n-        who = new Subject();\n-        metadata = new SettableBlobMeta(BlobStoreAclHandler.DEFAULT);\n-        LOG.info(\"Creating other\");\n-        try (AtomicOutputStream out = store.createBlob(\"test-empty-subject-DEF\", metadata, who)) {\n-            out.write(2);\n-        }\n-        assertStoreHasExactly(store, \"test-empty-subject-DEF\", \"test\", \"test-empty-subject-WE\");\n-        // Testing whether acls are set to WORLD_EVERYTHING\n-        assertTrue(\"ACL does not contain WORLD_EVERYTHING\", metadata.toString().contains(\"AccessControl(type:OTHER, access:7)\"));\n-        readAssertEqualsWithAuth(store, who, \"test-empty-subject-DEF\", 2);\n-\n-        if (store instanceof HdfsBlobStore) {\n-            ((HdfsBlobStore) store).fullCleanup(1);\n-        } else {\n-            fail(\"Error the blobstore is of unknowntype\");\n+    }\n+    \n+    @ParameterizedTest\n+    @ValueSource(booleans = {true, false})\n+    void testWithAuthenticationNoPrincipal(boolean securityEnabled) throws Exception {\n+        try (AutoCloseableBlobStoreContainer container = initHdfs(\"/storm/blobstore-auth-no-principal-sec-\" + securityEnabled)) {\n+            BlobStore store = container.blobStore;\n+            //Test for subject with no principals\n+            Subject who = new Subject();\n+            assertStoreHasExactly(store);\n+\n+            // Tests for case when subject != null (security turned on) and\n+            // acls for the blob are set to WORLD_EVERYTHING\n+            SettableBlobMeta metadata = new SettableBlobMeta(securityEnabled ? BlobStoreAclHandler.DEFAULT : BlobStoreAclHandler.WORLD_EVERYTHING);\n+            try (AtomicOutputStream out = store.createBlob(\"test\", metadata, who)) {\n+                out.write(1);\n+            }\n+            assertStoreHasExactly(store, \"test\");\n+            // With no principals in the subject ACL should always be set to WORLD_EVERYTHING\n+            assertTrue(\"ACL does not contain WORLD_EVERYTHING\", metadata.toString().contains(\"AccessControl(type:OTHER, access:7)\"));\n+            \n+            readAssertEqualsWithAuth(store, who, \"test\", 1);\n         }\n     }\n \n@@ -535,6 +523,6 @@ public void testMultiple(BlobStore store)\n             fail(\"Error the blobstore is of unknowntype\");\n         }\n         assertStoreHasExactly(store, \"test\");\n-        readAssertEquals(store, \"test\", 3);\n+        readAssertEquals(store, \"test\", 4);\n     }\n }", "filename": "external/storm-hdfs-blobstore/src/test/java/org/apache/storm/hdfs/blobstore/BlobStoreTest.java"}, {"additions": 1, "raw_url": "https://github.com/apache/storm/raw/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/pom.xml", "blob_url": "https://github.com/apache/storm/blob/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/pom.xml", "sha": "a19d821e623954b08142c75b96c9111c85e3564c", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/pom.xml?ref=f015130b0e4b33191f9fa84302f46f9a5e10c014", "patch": "@@ -234,7 +234,7 @@\n             <plugin>\n                 <groupId>org.apache.maven.plugins</groupId>\n                 <artifactId>maven-surefire-plugin</artifactId>\n-\t\t<configuration>\n+                <configuration>\n                     <reuseForks>false</reuseForks>\n                     <forkCount>1</forkCount>\n                 </configuration>", "filename": "external/storm-hdfs/pom.xml"}, {"additions": 3, "raw_url": "https://github.com/apache/storm/raw/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java", "blob_url": "https://github.com/apache/storm/blob/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java", "sha": "a145274b187dfb121e8c6b04689a103c550832c9", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java?ref=f015130b0e4b33191f9fa84302f46f9a5e10c014", "patch": "@@ -247,7 +247,9 @@ public void declareOutputFields(OutputFieldsDeclarer outputFieldsDeclarer) {\n     @Override\n     public void cleanup() {\n         doRotationAndRemoveAllWriters();\n-        this.rotationTimer.cancel();\n+        if (this.rotationTimer != null) {\n+            this.rotationTimer.cancel();\n+        }\n     }\n \n     private void doRotationAndRemoveAllWriters() {", "filename": "external/storm-hdfs/src/main/java/org/apache/storm/hdfs/bolt/AbstractHdfsBolt.java"}, {"additions": 12, "raw_url": "https://github.com/apache/storm/raw/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/TestHdfsBolt.java", "blob_url": "https://github.com/apache/storm/blob/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/TestHdfsBolt.java", "sha": "7f63cc0969aa80c401c2fa3c8bdde8c97c4d1ab9", "changes": 12, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/TestHdfsBolt.java?ref=f015130b0e4b33191f9fa84302f46f9a5e10c014", "patch": "@@ -56,6 +56,7 @@\n import static org.mockito.Mockito.verify;\n import static org.mockito.Mockito.verifyZeroInteractions;\n \n+\n @RunWith(MockitoJUnitRunner.class)\n public class TestHdfsBolt {\n \n@@ -203,6 +204,17 @@ public void testTickTuples() throws IOException {\n         //Tick should have flushed it\n         Assert.assertEquals(1, countNonZeroLengthFiles(testRoot));\n     }\n+    \n+    @Test\n+    public void testCleanupDoesNotThrowExceptionWhenRotationPolicyIsNotTimed() {\n+        //STORM-3372: Rotation policy other than TimedRotationPolicy causes NPE on cleanup\n+        FileRotationPolicy fieldsRotationPolicy =\n+            new FileSizeRotationPolicy(10_000, FileSizeRotationPolicy.Units.MB);\n+        HdfsBolt bolt = makeHdfsBolt(hdfsURI, 10, 10000f)\n+            .withRotationPolicy(fieldsRotationPolicy);\n+        bolt.prepare(new Config(), topologyContext, collector);\n+        bolt.cleanup();\n+    }\n \n     public void createBaseDirectory(FileSystem passedFs, String path) throws IOException {\n         Path p = new Path(path);", "filename": "external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/TestHdfsBolt.java"}, {"additions": 1, "raw_url": "https://github.com/apache/storm/raw/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/format/TestSimpleFileNameFormat.java", "blob_url": "https://github.com/apache/storm/blob/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/format/TestSimpleFileNameFormat.java", "sha": "f8e1e5e30208236b5cf307385b40048e8871c046", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/format/TestSimpleFileNameFormat.java?ref=f015130b0e4b33191f9fa84302f46f9a5e10c014", "patch": "@@ -69,7 +69,7 @@ public void testTimeFormat() {\n     }\n \n     private TopologyContext createTopologyContext(Map<String, Object> topoConf) {\n-        Map<Integer, String> taskToComponent = new HashMap<Integer, String>();\n+        Map<Integer, String> taskToComponent = new HashMap<>();\n         taskToComponent.put(7, \"Xcom\");\n         return new TopologyContext(null, topoConf, taskToComponent, null, null, null, null, null, null, 7, 6703, null, null, null, null,\n                                    null, null, null);", "filename": "external/storm-hdfs/src/test/java/org/apache/storm/hdfs/bolt/format/TestSimpleFileNameFormat.java"}, {"additions": 3, "raw_url": "https://github.com/apache/storm/raw/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSemantics.java", "blob_url": "https://github.com/apache/storm/blob/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSemantics.java", "sha": "3528a3df3d4184ed1e7164786e3e9ad69dcd1931", "changes": 5, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSemantics.java?ref=f015130b0e4b33191f9fa84302f46f9a5e10c014", "patch": "@@ -12,6 +12,8 @@\n \n package org.apache.storm.hdfs.spout;\n \n+import static org.hamcrest.core.IsNull.notNullValue;\n+\n import java.io.IOException;\n import org.apache.hadoop.fs.CommonConfigurationKeys;\n import org.apache.hadoop.fs.FSDataOutputStream;\n@@ -30,7 +32,6 @@\n \n import static org.junit.Assert.assertThat;\n import static org.junit.Assert.fail;\n-import static org.mockito.ArgumentMatchers.notNull;\n \n public class TestHdfsSemantics {\n \n@@ -124,7 +125,7 @@ public void testAppendSemantics() throws Exception {\n \n         //2 try to append to a closed file\n         try (FSDataOutputStream os2 = fs.append(file1)) {\n-            assertThat(os2, notNull());\n+            assertThat(os2, notNullValue());\n         }\n     }\n ", "filename": "external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSemantics.java"}, {"additions": 13, "raw_url": "https://github.com/apache/storm/raw/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSpout.java", "blob_url": "https://github.com/apache/storm/blob/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSpout.java", "sha": "133de5d7a08aef3180c1dff50a7dbaf4cab41236", "changes": 25, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSpout.java?ref=f015130b0e4b33191f9fa84302f46f9a5e10c014", "patch": "@@ -12,6 +12,9 @@\n \n package org.apache.storm.hdfs.spout;\n \n+import static org.hamcrest.core.Is.is;\n+import static org.junit.Assert.assertThat;\n+\n import java.io.BufferedReader;\n import java.io.File;\n import java.io.IOException;\n@@ -192,6 +195,9 @@ public void testEmptySimpleText_ACK() throws Exception {\n         Path file1 = new Path(source.toString() + \"/file_empty.txt\");\n         createTextFile(file1, 0);\n \n+        //Ensure the second file has a later modified timestamp, as the spout should pick the first file first.\n+        Thread.sleep(2);\n+\n         Path file2 = new Path(source.toString() + \"/file.txt\");\n         createTextFile(file2, 5);\n \n@@ -203,15 +209,13 @@ public void testEmptySimpleText_ACK() throws Exception {\n             conf.put(Config.TOPOLOGY_ACKER_EXECUTORS, \"1\"); // enable ACKing\n             openSpout(spout, 0, conf);\n \n-            // consume empty file\n-            runSpout(spout, \"r1\");\n-            Path arc1 = new Path(archive.toString() + \"/file_empty.txt\");\n-            checkCollectorOutput_txt((MockCollector) spout.getCollector(), arc1);\n-\n-            // consume file 2\n-            runSpout(spout, \"r5\", \"a0\", \"a1\", \"a2\", \"a3\", \"a4\");\n+            // Read once. Since the first file is empty, the spout should continue with file 2\n+            runSpout(spout, \"r6\", \"a0\", \"a1\", \"a2\", \"a3\", \"a4\");\n+            //File 1 should be moved to archive\n+            assertThat(fs.isFile(new Path(archive.toString() + \"/file_empty.txt\")), is(true));\n+            //File 2 should be read\n             Path arc2 = new Path(archive.toString() + \"/file.txt\");\n-            checkCollectorOutput_txt((MockCollector) spout.getCollector(), arc1, arc2);\n+            checkCollectorOutput_txt((MockCollector) spout.getCollector(), arc2);\n         }\n     }\n \n@@ -681,11 +685,8 @@ private void openSpout(HdfsSpout spout, int spoutId, Map<String, Object> topoCon\n \n     private void createTextFile(Path file, int lineCount) throws IOException {\n         FSDataOutputStream os = fs.create(file);\n-        int size = 0;\n         for (int i = 0; i < lineCount; i++) {\n             os.writeBytes(\"line \" + i + System.lineSeparator());\n-            String msg = \"line \" + i + System.lineSeparator();\n-            size += msg.getBytes().length;\n         }\n         os.close();\n     }\n@@ -772,7 +773,7 @@ public MockTextFailingReader(FileSystem fs, Path file, Map<String, Object> conf)\n         private final int componentId;\n \n         public MockTopologyContext(int componentId, Map<String, Object> topoConf) {\n-            super(null, topoConf, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null, null);\n+            super(null, topoConf, null, null, null, null, null, null, null, 0, 0, null, null, null, null, null, null, null);\n             this.componentId = componentId;\n         }\n ", "filename": "external/storm-hdfs/src/test/java/org/apache/storm/hdfs/spout/TestHdfsSpout.java"}, {"additions": 64, "raw_url": "https://github.com/apache/storm/raw/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/testing/MiniDFSClusterExtension.java", "blob_url": "https://github.com/apache/storm/blob/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/testing/MiniDFSClusterExtension.java", "sha": "f88fef5838697a3e9df3b64914f431ce927617f2", "changes": 64, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/testing/MiniDFSClusterExtension.java?ref=f015130b0e4b33191f9fa84302f46f9a5e10c014", "patch": "@@ -0,0 +1,64 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.storm.hdfs.testing;\n+\n+import java.util.function.Supplier;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.junit.jupiter.api.extension.AfterEachCallback;\n+import org.junit.jupiter.api.extension.BeforeEachCallback;\n+import org.junit.jupiter.api.extension.ExtensionContext;\n+\n+public class MiniDFSClusterExtension implements BeforeEachCallback, AfterEachCallback {\n+\n+    private static final String TEST_BUILD_DATA = \"test.build.data\";\n+\n+    private final Supplier<Configuration> hadoopConfSupplier;\n+    private Configuration hadoopConf;\n+    private MiniDFSCluster dfscluster;\n+\n+    public MiniDFSClusterExtension() {\n+        this(() -> new Configuration());\n+    }\n+\n+    public MiniDFSClusterExtension(Supplier<Configuration> hadoopConfSupplier) {\n+        this.hadoopConfSupplier = hadoopConfSupplier;\n+    }\n+\n+    public Configuration getHadoopConf() {\n+        return hadoopConf;\n+    }\n+\n+    public MiniDFSCluster getDfscluster() {\n+        return dfscluster;\n+    }\n+\n+    @Override\n+    public void beforeEach(ExtensionContext arg0) throws Exception {\n+        System.setProperty(TEST_BUILD_DATA, \"target/test/data\");\n+        hadoopConf = hadoopConfSupplier.get();\n+        dfscluster = new MiniDFSCluster.Builder(hadoopConf).numDataNodes(3).build();\n+        dfscluster.waitActive();\n+    }\n+\n+    @Override\n+    public void afterEach(ExtensionContext arg0) throws Exception {\n+        dfscluster.shutdown();\n+        System.clearProperty(TEST_BUILD_DATA);\n+    }\n+}", "filename": "external/storm-hdfs/src/test/java/org/apache/storm/hdfs/testing/MiniDFSClusterExtension.java"}, {"additions": 5, "raw_url": "https://github.com/apache/storm/raw/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/testing/MiniDFSClusterRule.java", "blob_url": "https://github.com/apache/storm/blob/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/testing/MiniDFSClusterRule.java", "sha": "6265a52ee9556112500d34aeef3e0a649c3bfe95", "changes": 5, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/src/test/java/org/apache/storm/hdfs/testing/MiniDFSClusterRule.java?ref=f015130b0e4b33191f9fa84302f46f9a5e10c014", "patch": "@@ -23,6 +23,10 @@\n import org.junit.runner.Description;\n import org.junit.runners.model.Statement;\n \n+/**\n+ * @deprecated Use {@link MiniDFSClusterExtension} instead, along with JUnit 5 for new tests.\n+ */\n+@Deprecated\n public class MiniDFSClusterRule implements TestRule {\n \n     private static final String TEST_BUILD_DATA = \"test.build.data\";\n@@ -57,6 +61,7 @@ public void evaluate() throws Throwable {\n                     hadoopConf = hadoopConfSupplier.get();\n                     dfscluster = new MiniDFSCluster.Builder(hadoopConf).numDataNodes(3).build();\n                     dfscluster.waitActive();\n+                    base.evaluate();\n                 } finally {\n                     if (dfscluster != null) {\n                         dfscluster.shutdown();", "filename": "external/storm-hdfs/src/test/java/org/apache/storm/hdfs/testing/MiniDFSClusterRule.java"}, {"additions": 1, "raw_url": "https://github.com/apache/storm/raw/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/src/test/resources/log4j.properties", "blob_url": "https://github.com/apache/storm/blob/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/src/test/resources/log4j.properties", "sha": "c952abd128f4ccff65eff0682c2f6420cd4621e1", "changes": 5, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/src/test/resources/log4j.properties?ref=f015130b0e4b33191f9fa84302f46f9a5e10c014", "patch": "@@ -20,7 +20,4 @@ log4j.rootLogger = WARN, out\n \n log4j.appender.out = org.apache.log4j.ConsoleAppender\n log4j.appender.out.layout = org.apache.log4j.PatternLayout\n-log4j.appender.out.layout.ConversionPattern = %d (%t) [%p - %l] %m%n\n-\n-log4j.logger.org.apache.storm.hdfs = INFO\n-\n+log4j.appender.out.layout.ConversionPattern = %d (%t) [%p - %l] %m%n\n\\ No newline at end of file", "filename": "external/storm-hdfs/src/test/resources/log4j.properties"}, {"additions": 32, "raw_url": "https://github.com/apache/storm/raw/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/src/test/resources/log4j2.xml", "blob_url": "https://github.com/apache/storm/blob/f015130b0e4b33191f9fa84302f46f9a5e10c014/external/storm-hdfs/src/test/resources/log4j2.xml", "sha": "546b1b380865c6dd04e7006c5dcab07888a8dab9", "changes": 32, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-hdfs/src/test/resources/log4j2.xml?ref=f015130b0e4b33191f9fa84302f46f9a5e10c014", "patch": "@@ -0,0 +1,32 @@\n+<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<!--\n+ Licensed to the Apache Software Foundation (ASF) under one or more\n+ contributor license agreements.  See the NOTICE file distributed with\n+ this work for additional information regarding copyright ownership.\n+ The ASF licenses this file to You under the Apache License, Version 2.0\n+ (the \"License\"); you may not use this file except in compliance with\n+ the License.  You may obtain a copy of the License at\n+\n+     http://www.apache.org/licenses/LICENSE-2.0\n+\n+ Unless required by applicable law or agreed to in writing, software\n+ distributed under the License is distributed on an \"AS IS\" BASIS,\n+ WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ See the License for the specific language governing permissions and\n+ limitations under the License.\n+-->\n+<Configuration status=\"WARN\">\n+    <Appenders>\n+        <Console name=\"Console\" target=\"SYSTEM_OUT\">\n+            <PatternLayout pattern=\"%d{HH:mm:ss.SSS} [%t] %-5level %logger{36} - %msg%n\" charset=\"UTF-8\"/>\n+        </Console>\n+    </Appenders>\n+    <Loggers>\n+        <Root level=\"WARN\">\n+            <AppenderRef ref=\"Console\"/>\n+        </Root>\n+        <Logger name=\"org.apache.storm\" level=\"INFO\" additivity=\"false\">\n+            <AppenderRef ref=\"Console\"/>\n+        </Logger>\n+    </Loggers>\n+</Configuration>\n\\ No newline at end of file", "filename": "external/storm-hdfs/src/test/resources/log4j2.xml"}, {"additions": 1, "raw_url": "https://github.com/apache/storm/raw/f015130b0e4b33191f9fa84302f46f9a5e10c014/pom.xml", "blob_url": "https://github.com/apache/storm/blob/f015130b0e4b33191f9fa84302f46f9a5e10c014/pom.xml", "sha": "3b587a7ff62e1c72d2acc15aa4b25e71b50253ce", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/pom.xml?ref=f015130b0e4b33191f9fa84302f46f9a5e10c014", "patch": "@@ -295,7 +295,7 @@\n         <servlet.version>3.1.0</servlet.version>\n         <joda-time.version>2.3</joda-time.version>\n         <thrift.version>0.12.0</thrift.version>\n-        <junit.jupiter.version>5.3.2</junit.jupiter.version>\n+        <junit.jupiter.version>5.5.0-M1</junit.jupiter.version>\n         <surefire.version>2.22.1</surefire.version>\n         <awaitility.version>3.1.0</awaitility.version>\n         <hdrhistogram.version>2.1.10</hdrhistogram.version>", "filename": "pom.xml"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/c985695e0728eb3af171ca6d346c51cc4e0b083a", "parent": "https://github.com/apache/storm/commit/aaf1113360ce151d86948860c2f290befa79abb8", "message": "STORM-3379: Fix intermittent NPE during worker boot in local mode", "bug_id": "storm_6", "file": [{"additions": 20, "raw_url": "https://github.com/apache/storm/raw/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-client/src/jvm/org/apache/storm/daemon/worker/Worker.java", "blob_url": "https://github.com/apache/storm/blob/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-client/src/jvm/org/apache/storm/daemon/worker/Worker.java", "sha": "175a91adee5a8f305f64e7a8dfaa26aa8c1d0ade", "changes": 26, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/daemon/worker/Worker.java?ref=c985695e0728eb3af171ca6d346c51cc4e0b083a", "patch": "@@ -14,6 +14,7 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.net.UnknownHostException;\n import java.nio.charset.Charset;\n import java.security.PrivilegedExceptionAction;\n import java.util.ArrayList;\n@@ -22,6 +23,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Supplier;\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n import java.util.stream.Collectors;\n@@ -45,6 +47,7 @@\n import org.apache.storm.generated.ExecutorStats;\n import org.apache.storm.generated.LSWorkerHeartbeat;\n import org.apache.storm.generated.LogConfig;\n+import org.apache.storm.generated.Supervisor;\n import org.apache.storm.generated.SupervisorWorkerHeartbeat;\n import org.apache.storm.messaging.IConnection;\n import org.apache.storm.messaging.IContext;\n@@ -61,6 +64,7 @@\n import org.apache.storm.utils.NimbusClient;\n import org.apache.storm.utils.ObjectReader;\n import org.apache.storm.utils.SupervisorClient;\n+import org.apache.storm.utils.SupervisorIfaceFactory;\n import org.apache.storm.utils.Time;\n import org.apache.storm.utils.Utils;\n import org.slf4j.Logger;\n@@ -87,6 +91,7 @@\n     private AtomicReference<Credentials> credentialsAtom;\n     private Subject subject;\n     private Collection<IAutoCredentials> autoCreds;\n+    private final Supplier<SupervisorIfaceFactory> supervisorIfaceSupplier;\n \n \n     /**\n@@ -103,7 +108,7 @@\n      */\n \n     public Worker(Map<String, Object> conf, IContext context, String topologyId, String assignmentId,\n-                  int supervisorPort, int port, String workerId) {\n+                  int supervisorPort, int port, String workerId, Supplier<SupervisorIfaceFactory> supervisorIfaceSupplier) {\n         this.conf = conf;\n         this.context = context;\n         this.topologyId = topologyId;\n@@ -113,6 +118,7 @@ public Worker(Map<String, Object> conf, IContext context, String topologyId, Str\n         this.workerId = workerId;\n         this.logConfigManager = new LogConfigManager();\n         this.metricRegistry = new StormMetricRegistry();\n+        this.supervisorIfaceSupplier = supervisorIfaceSupplier;\n     }\n \n     public static void main(String[] args) throws Exception {\n@@ -125,8 +131,16 @@ public static void main(String[] args) throws Exception {\n         Map<String, Object> conf = ConfigUtils.readStormConfig();\n         Utils.setupDefaultUncaughtExceptionHandler();\n         StormCommon.validateDistributedMode(conf);\n-        Worker worker = new Worker(conf, null, stormId, assignmentId, Integer.parseInt(supervisorPort),\n-                                   Integer.parseInt(portStr), workerId);\n+        int supervisorPortInt = Integer.parseInt(supervisorPort);\n+        Supplier<SupervisorIfaceFactory> supervisorIfaceSuppler = () -> {\n+            try {\n+                return SupervisorClient.getConfiguredClient(conf, Utils.hostname(), supervisorPortInt);\n+            } catch (UnknownHostException e) {\n+                throw Utils.wrapInRuntime(e);\n+            }\n+        };\n+        Worker worker = new Worker(conf, null, stormId, assignmentId, supervisorPortInt,\n+                                   Integer.parseInt(portStr), workerId, supervisorIfaceSuppler);\n         worker.start();\n         int workerShutdownSleepSecs = ObjectReader.getInt(conf.get(Config.SUPERVISOR_WORKER_SHUTDOWN_SLEEP_SECS));\n         LOG.info(\"Adding shutdown hook with kill in {} secs\", workerShutdownSleepSecs);\n@@ -172,7 +186,7 @@ public void start() throws Exception {\n     private Object loadWorker(Map<String, Object> topologyConf, IStateStorage stateStorage, IStormClusterState stormClusterState,\n                               Map<String, String> initCreds, Credentials initialCredentials)\n         throws Exception {\n-        workerState = new WorkerState(conf, context, topologyId, assignmentId, supervisorPort, port, workerId,\n+        workerState = new WorkerState(conf, context, topologyId, assignmentId, supervisorIfaceSupplier, port, workerId,\n                                       topologyConf, stateStorage, stormClusterState, autoCreds, metricRegistry);\n \n         // Heartbeat here so that worker process dies if this fails\n@@ -425,8 +439,8 @@ private void heartbeatToMasterIfLocalbeatFail(LSWorkerHeartbeat lsWorkerHeartbea\n         SupervisorWorkerHeartbeat workerHeartbeat = new SupervisorWorkerHeartbeat(lsWorkerHeartbeat.get_topology_id(),\n                                                                                   lsWorkerHeartbeat.get_executors(),\n                                                                                   lsWorkerHeartbeat.get_time_secs());\n-        try (SupervisorClient client = SupervisorClient.getConfiguredClient(conf, Utils.hostname(), supervisorPort)) {\n-            client.getClient().sendSupervisorWorkerHeartbeat(workerHeartbeat);\n+        try (SupervisorIfaceFactory fac = supervisorIfaceSupplier.get()) {\n+            fac.getIface().sendSupervisorWorkerHeartbeat(workerHeartbeat);\n         } catch (Exception tr1) {\n             //If any error/exception thrown, report directly to nimbus.\n             LOG.warn(\"Exception when send heartbeat to local supervisor\", tr1.getMessage());", "filename": "storm-client/src/jvm/org/apache/storm/daemon/worker/Worker.java"}, {"additions": 27, "raw_url": "https://github.com/apache/storm/raw/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java", "blob_url": "https://github.com/apache/storm/blob/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java", "sha": "913261d24088438698ceb97d5b86afaeb90de622", "changes": 45, "status": "modified", "deletions": 18, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java?ref=c985695e0728eb3af171ca6d346c51cc4e0b083a", "patch": "@@ -69,13 +69,15 @@\n import org.apache.storm.serialization.KryoTupleSerializer;\n import org.apache.storm.shade.com.google.common.collect.ImmutableMap;\n import org.apache.storm.shade.com.google.common.collect.Sets;\n+import org.apache.storm.shade.org.apache.commons.lang.Validate;\n import org.apache.storm.task.WorkerTopologyContext;\n import org.apache.storm.tuple.AddressedTuple;\n import org.apache.storm.tuple.Fields;\n import org.apache.storm.utils.ConfigUtils;\n import org.apache.storm.utils.JCQueue;\n import org.apache.storm.utils.ObjectReader;\n import org.apache.storm.utils.SupervisorClient;\n+import org.apache.storm.utils.SupervisorIfaceFactory;\n import org.apache.storm.utils.ThriftTopologyUtils;\n import org.apache.storm.utils.Utils;\n import org.apache.storm.utils.Utils.SmartThread;\n@@ -92,7 +94,7 @@\n     final IConnection receiver;\n     final String topologyId;\n     final String assignmentId;\n-    final int supervisorPort;\n+    private final Supplier<SupervisorIfaceFactory> supervisorIfaceSupplier;\n     final int port;\n     final String workerId;\n     final IStateStorage stateStorage;\n@@ -151,18 +153,18 @@\n     private final StormMetricRegistry metricRegistry;\n \n     public WorkerState(Map<String, Object> conf, IContext mqContext, String topologyId, String assignmentId,\n-                       int supervisorPort, int port, String workerId, Map<String, Object> topologyConf, IStateStorage stateStorage,\n+                       Supplier<SupervisorIfaceFactory> supervisorIfaceSupplier, int port, String workerId, Map<String, Object> topologyConf, IStateStorage stateStorage,\n                        IStormClusterState stormClusterState, Collection<IAutoCredentials> autoCredentials,\n                        StormMetricRegistry metricRegistry) throws IOException,\n         InvalidTopologyException {\n         this.metricRegistry = metricRegistry;\n         this.autoCredentials = autoCredentials;\n         this.conf = conf;\n+        this.supervisorIfaceSupplier = supervisorIfaceSupplier;\n         this.localExecutors = new HashSet<>(readWorkerExecutors(stormClusterState, topologyId, assignmentId, port));\n         this.mqContext = (null != mqContext) ? mqContext : TransportFactory.makeContext(topologyConf);\n         this.topologyId = topologyId;\n         this.assignmentId = assignmentId;\n-        this.supervisorPort = supervisorPort;\n         this.port = port;\n         this.workerId = workerId;\n         this.stateStorage = stateStorage;\n@@ -370,9 +372,14 @@ public StormTimer getUserTimer() {\n     public SmartThread makeTransferThread() {\n         return workerTransfer.makeTransferThread();\n     }\n-\n+    \n     public void refreshConnections() {\n-        Assignment assignment = getLocalAssignment(conf, stormClusterState, topologyId);\n+        Assignment assignment = null;\n+        try {\n+            assignment = getLocalAssignment(stormClusterState, topologyId);\n+        } catch (Exception e) {\n+            LOG.warn(\"Failed to read assignment. This should only happen when topology is shutting down.\", e);\n+        }\n \n         Set<NodeInfo> neededConnections = new HashSet<>();\n         Map<Integer, NodeInfo> newTaskToNodePort = new HashMap<>();\n@@ -393,14 +400,16 @@ public void refreshConnections() {\n         Set<NodeInfo> newConnections = Sets.difference(neededConnections, currentConnections);\n         Set<NodeInfo> removeConnections = Sets.difference(currentConnections, neededConnections);\n \n+        Map<String, String> nodeHost = assignment != null ? assignment.get_node_host() : null;\n         // Add new connections atomically\n         cachedNodeToPortSocket.getAndUpdate(prev -> {\n             Map<NodeInfo, IConnection> next = new HashMap<>(prev);\n             for (NodeInfo nodeInfo : newConnections) {\n                 next.put(nodeInfo,\n                          mqContext.connect(\n                              topologyId,\n-                             assignment.get_node_host().get(nodeInfo.get_node()),    // Host\n+                             //nodeHost is not null here, as newConnections is only non-empty if assignment was not null above.\n+                             nodeHost.get(nodeInfo.get_node()),    // Host\n                              nodeInfo.get_port().iterator().next().intValue(),       // Port\n                              workerTransfer.getRemoteBackPressureStatus()));\n             }\n@@ -625,7 +634,8 @@ public boolean areAllConnectionsReady() {\n         LOG.info(\"Reading assignments\");\n         List<List<Long>> executorsAssignedToThisWorker = new ArrayList<>();\n         executorsAssignedToThisWorker.add(Constants.SYSTEM_EXECUTOR_ID);\n-        Map<List<Long>, NodeInfo> executorToNodePort = getLocalAssignment(conf, stormClusterState, topologyId).get_executor_node_port();\n+        Map<List<Long>, NodeInfo> executorToNodePort = \n+            getLocalAssignment(stormClusterState, topologyId).get_executor_node_port();\n         for (Map.Entry<List<Long>, NodeInfo> entry : executorToNodePort.entrySet()) {\n             NodeInfo nodeInfo = entry.getValue();\n             if (nodeInfo.get_node().equals(assignmentId) && nodeInfo.get_port().iterator().next() == port) {\n@@ -635,18 +645,17 @@ public boolean areAllConnectionsReady() {\n         return executorsAssignedToThisWorker;\n     }\n \n-    private Assignment getLocalAssignment(Map<String, Object> conf, IStormClusterState stormClusterState, String topologyId) {\n-        if (!ConfigUtils.isLocalMode(conf)) {\n-            try (SupervisorClient supervisorClient = SupervisorClient.getConfiguredClient(conf, Utils.hostname(),\n-                                                                                          supervisorPort)) {\n-                Assignment assignment = supervisorClient.getClient().getLocalAssignmentForStorm(topologyId);\n-                return assignment;\n-            } catch (Throwable tr1) {\n+    private Assignment getLocalAssignment(IStormClusterState stormClusterState, String topologyId) {\n+        try (SupervisorIfaceFactory fac = supervisorIfaceSupplier.get()) {\n+            return fac.getIface().getLocalAssignmentForStorm(topologyId);\n+        } catch (Throwable e) {\n                 //if any error/exception thrown, fetch it from zookeeper\n-                return stormClusterState.remoteAssignmentInfo(topologyId, null);\n+            Assignment assignment = stormClusterState.remoteAssignmentInfo(topologyId, null);\n+            if (assignment == null) {\n+                throw new RuntimeException(\"Failed to read worker assignment.\"\n+                    + \" Supervisor client threw exception, and assignment in Zookeeper was null\", e);\n             }\n-        } else {\n-            return stormClusterState.remoteAssignmentInfo(topologyId, null);\n+            return assignment;\n         }\n     }\n \n@@ -666,7 +675,7 @@ private Assignment getLocalAssignment(Map<String, Object> conf, IStormClusterSta\n         for (List<Long> executor : executors) {\n             int port = this.getPort();\n             receiveQueueMap.put(executor, new JCQueue(\"receive-queue\" + executor.toString(),\n-                recvQueueSize, overflowLimit, recvBatchSize, backPressureWaitStrategy,\n+                                                      recvQueueSize, overflowLimit, recvBatchSize, backPressureWaitStrategy,\n                 this.getTopologyId(), Constants.SYSTEM_COMPONENT_ID, -1, this.getPort(), metricRegistry));\n \n         }", "filename": "storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java"}, {"additions": 1, "raw_url": "https://github.com/apache/storm/raw/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-client/src/jvm/org/apache/storm/security/auth/workertoken/WorkerTokenAuthorizer.java", "blob_url": "https://github.com/apache/storm/blob/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-client/src/jvm/org/apache/storm/security/auth/workertoken/WorkerTokenAuthorizer.java", "sha": "c225e2742f6564db0abb966f125860c5a3521491", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/security/auth/workertoken/WorkerTokenAuthorizer.java?ref=c985695e0728eb3af171ca6d346c51cc4e0b083a", "patch": "@@ -152,4 +152,4 @@ public void close() {\n             state.disconnect();\n         }\n     }\n-}\n\\ No newline at end of file\n+}", "filename": "storm-client/src/jvm/org/apache/storm/security/auth/workertoken/WorkerTokenAuthorizer.java"}, {"additions": 3, "raw_url": "https://github.com/apache/storm/raw/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-client/src/jvm/org/apache/storm/utils/SupervisorClient.java", "blob_url": "https://github.com/apache/storm/blob/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-client/src/jvm/org/apache/storm/utils/SupervisorClient.java", "sha": "64d5aceffa0873f93f2afb03730a1e738bd71701", "changes": 5, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/utils/SupervisorClient.java?ref=c985695e0728eb3af171ca6d346c51cc4e0b083a", "patch": "@@ -28,7 +28,7 @@\n  * <li>nimbus -> supervisor: assign assignments for a node.</li>\n  * </ul>\n  */\n-public class SupervisorClient extends ThriftClient {\n+public class SupervisorClient extends ThriftClient implements SupervisorIfaceFactory {\n     private static final Logger LOG = LoggerFactory.getLogger(SupervisorClient.class);\n     private Supervisor.Client client;\n \n@@ -76,7 +76,8 @@ public static SupervisorClient getConfiguredClientAs(Map conf, String host, int\n         }\n     }\n \n-    public Supervisor.Client getClient() {\n+    @Override\n+    public Supervisor.Client getIface() {\n         return client;\n     }\n }", "filename": "storm-client/src/jvm/org/apache/storm/utils/SupervisorClient.java"}, {"additions": 30, "raw_url": "https://github.com/apache/storm/raw/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-client/src/jvm/org/apache/storm/utils/SupervisorIfaceFactory.java", "blob_url": "https://github.com/apache/storm/blob/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-client/src/jvm/org/apache/storm/utils/SupervisorIfaceFactory.java", "sha": "eddfa7332e56f8a82e1892ce4219d5215fd57865", "changes": 30, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/utils/SupervisorIfaceFactory.java?ref=c985695e0728eb3af171ca6d346c51cc4e0b083a", "patch": "@@ -0,0 +1,30 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.utils;\n+\n+import java.io.Closeable;\n+\n+public interface SupervisorIfaceFactory extends Closeable {\n+\n+    org.apache.storm.generated.Supervisor.Iface getIface();\n+\n+    @Override\n+    default void close() {\n+    }\n+}", "filename": "storm-client/src/jvm/org/apache/storm/utils/SupervisorIfaceFactory.java"}, {"additions": 2, "raw_url": "https://github.com/apache/storm/raw/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Container.java", "blob_url": "https://github.com/apache/storm/blob/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Container.java", "sha": "8b58483477967b80769321358a55419004740df2", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Container.java?ref=c985695e0728eb3af171ca6d346c51cc4e0b083a", "patch": "@@ -309,7 +309,7 @@ private boolean isPosixProcessAlive(long pid, String user) throws IOException {\n         }\n         return ret;\n     }\n-\n+    \n     @Override\n     public boolean areAllProcessesDead() throws IOException {\n         Set<Long> pids = getAllPids();\n@@ -325,7 +325,7 @@ public boolean areAllProcessesDead() throws IOException {\n                 break;\n             }\n         }\n-\n+        \n         if (allDead && shutdownTimer != null) {\n             shutdownTimer.stop();\n             shutdownTimer = null;", "filename": "storm-server/src/main/java/org/apache/storm/daemon/supervisor/Container.java"}, {"additions": 5, "raw_url": "https://github.com/apache/storm/raw/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/ContainerLauncher.java", "blob_url": "https://github.com/apache/storm/blob/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/ContainerLauncher.java", "sha": "b3100180a46ca6db1730db35d7a90f01b76bbb15", "changes": 7, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/main/java/org/apache/storm/daemon/supervisor/ContainerLauncher.java?ref=c985695e0728eb3af171ca6d346c51cc4e0b083a", "patch": "@@ -46,14 +46,17 @@ protected ContainerLauncher() {\n      * @param sharedContext Used in local mode to let workers talk together without netty\n      * @param metricsRegistry The metrics registry.\n      * @param containerMemoryTracker The shared memory tracker for the supervisor's containers\n+     * @param localSupervisor The local supervisor Thrift interface. Only used for local clusters, distributed clusters use Thrift directly.\n      * @return the proper container launcher\n      * @throws IOException on any error\n      */\n     public static ContainerLauncher make(Map<String, Object> conf, String supervisorId, int supervisorPort,\n                                          IContext sharedContext, StormMetricsRegistry metricsRegistry, \n-                                         ContainerMemoryTracker containerMemoryTracker) throws IOException {\n+                                         ContainerMemoryTracker containerMemoryTracker,\n+                                         org.apache.storm.generated.Supervisor.Iface localSupervisor) throws IOException {\n         if (ConfigUtils.isLocalMode(conf)) {\n-            return new LocalContainerLauncher(conf, supervisorId, supervisorPort, sharedContext, metricsRegistry, containerMemoryTracker);\n+            return new LocalContainerLauncher(conf, supervisorId, supervisorPort, sharedContext, metricsRegistry, containerMemoryTracker,\n+                localSupervisor);\n         }\n \n         ResourceIsolationInterface resourceIsolationManager = null;", "filename": "storm-server/src/main/java/org/apache/storm/daemon/supervisor/ContainerLauncher.java"}, {"additions": 8, "raw_url": "https://github.com/apache/storm/raw/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/LocalContainer.java", "blob_url": "https://github.com/apache/storm/blob/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/LocalContainer.java", "sha": "228da8434d863fcc70342c8aa0fd3f83dc29c3f8", "changes": 10, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/main/java/org/apache/storm/daemon/supervisor/LocalContainer.java?ref=c985695e0728eb3af171ca6d346c51cc4e0b083a", "patch": "@@ -27,15 +27,18 @@\n public class LocalContainer extends Container {\n     private static final Logger LOG = LoggerFactory.getLogger(LocalContainer.class);\n     private final IContext _sharedContext;\n+    private final org.apache.storm.generated.Supervisor.Iface localSupervisor;\n     private volatile boolean _isAlive = false;\n \n     public LocalContainer(Map<String, Object> conf, String supervisorId, int supervisorPort, int port,\n                           LocalAssignment assignment, IContext sharedContext, StormMetricsRegistry metricsRegistry,\n-                          ContainerMemoryTracker containerMemoryTracker) throws IOException {\n+                          ContainerMemoryTracker containerMemoryTracker,\n+                          org.apache.storm.generated.Supervisor.Iface localSupervisor) throws IOException {\n         super(ContainerType.LAUNCH, conf, supervisorId, supervisorPort, port, assignment, null, null, null, null, metricsRegistry, \n             containerMemoryTracker);\n         _sharedContext = sharedContext;\n         _workerId = Utils.uuid();\n+        this.localSupervisor = localSupervisor;\n     }\n \n     @Override\n@@ -50,7 +53,10 @@ protected void createBlobstoreLinks() {\n \n     @Override\n     public void launch() throws IOException {\n-        Worker worker = new Worker(_conf, _sharedContext, _topologyId, _supervisorId, _supervisorPort, _port, _workerId);\n+        Worker worker = new Worker(_conf, _sharedContext, _topologyId, _supervisorId, _supervisorPort, _port, _workerId,\n+            () -> {\n+                return () -> localSupervisor;\n+            });\n         try {\n             worker.start();\n         } catch (Exception e) {", "filename": "storm-server/src/main/java/org/apache/storm/daemon/supervisor/LocalContainer.java"}, {"additions": 5, "raw_url": "https://github.com/apache/storm/raw/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/LocalContainerLauncher.java", "blob_url": "https://github.com/apache/storm/blob/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/LocalContainerLauncher.java", "sha": "d9f3f8d8957b93baff08f3814bad2da5d82fd183", "changes": 7, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/main/java/org/apache/storm/daemon/supervisor/LocalContainerLauncher.java?ref=c985695e0728eb3af171ca6d346c51cc4e0b083a", "patch": "@@ -29,22 +29,25 @@\n     private final IContext _sharedContext;\n     private final StormMetricsRegistry metricsRegistry;\n     private final ContainerMemoryTracker containerMemoryTracker;\n+    private final org.apache.storm.generated.Supervisor.Iface localSupervisor;\n \n     public LocalContainerLauncher(Map<String, Object> conf, String supervisorId, int supervisorPort,\n                                   IContext sharedContext, StormMetricsRegistry metricsRegistry, \n-                                  ContainerMemoryTracker containerMemoryTracker) {\n+                                  ContainerMemoryTracker containerMemoryTracker,\n+                                  org.apache.storm.generated.Supervisor.Iface localSupervisor) {\n         _conf = conf;\n         _supervisorId = supervisorId;\n         _supervisorPort = supervisorPort;\n         _sharedContext = sharedContext;\n         this.metricsRegistry = metricsRegistry;\n         this.containerMemoryTracker = containerMemoryTracker;\n+        this.localSupervisor = localSupervisor;\n     }\n \n     @Override\n     public Container launchContainer(int port, LocalAssignment assignment, LocalState state) throws IOException {\n         LocalContainer ret = new LocalContainer(_conf, _supervisorId, _supervisorPort,\n-            port, assignment, _sharedContext, metricsRegistry, containerMemoryTracker);\n+            port, assignment, _sharedContext, metricsRegistry, containerMemoryTracker, localSupervisor);\n         ret.setup();\n         ret.launch();\n         return ret;", "filename": "storm-server/src/main/java/org/apache/storm/daemon/supervisor/LocalContainerLauncher.java"}, {"additions": 2, "raw_url": "https://github.com/apache/storm/raw/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java", "blob_url": "https://github.com/apache/storm/blob/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java", "sha": "6b18bbea7a1943a216d6cca3c63a1c040a5ec77d", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java?ref=c985695e0728eb3af171ca6d346c51cc4e0b083a", "patch": "@@ -86,7 +86,8 @@ public ReadClusterState(Supervisor supervisor) throws Exception {\n         this.slotMetrics = supervisor.getSlotMetrics();\n \n         this.launcher = ContainerLauncher.make(superConf, assignmentId, supervisorPort,\n-            supervisor.getSharedContext(), supervisor.getMetricsRegistry(), supervisor.getContainerMemoryTracker());\n+            supervisor.getSharedContext(), supervisor.getMetricsRegistry(), supervisor.getContainerMemoryTracker(),\n+            supervisor.getSupervisorThriftInterface());\n \n         this.metricsProcessor = null;\n         try {", "filename": "storm-server/src/main/java/org/apache/storm/daemon/supervisor/ReadClusterState.java"}, {"additions": 60, "raw_url": "https://github.com/apache/storm/raw/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java", "blob_url": "https://github.com/apache/storm/blob/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java", "sha": "9bc4f7f5bca05ce2c56d51f32ccc9808d0b4280b", "changes": 108, "status": "modified", "deletions": 48, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java?ref=c985695e0728eb3af171ca6d346c51cc4e0b083a", "patch": "@@ -112,6 +112,8 @@\n     private ThriftServer thriftServer;\n     //used for local cluster heartbeating\n     private Nimbus.Iface localNimbus;\n+    //Passed to workers in local clusters, exposed by thrift server in distributed mode\n+    private org.apache.storm.generated.Supervisor.Iface supervisorThriftInterface;\n \n     private Supervisor(ISupervisor iSupervisor, StormMetricsRegistry metricsRegistry)\n         throws IOException, IllegalAccessException, InstantiationException, ClassNotFoundException {\n@@ -178,6 +180,8 @@ public Supervisor(Map<String, Object> conf, IContext sharedContext, ISupervisor\n         this.workerHeartbeatTimer = new StormTimer(null, new DefaultUncaughtExceptionHandler());\n \n         this.eventTimer = new StormTimer(null, new DefaultUncaughtExceptionHandler());\n+        \n+        this.supervisorThriftInterface = createSupervisorIface();\n     }\n \n     /**\n@@ -393,6 +397,59 @@ public void checkAuthorization(String topoName, Map<String, Object> topoConf, St\n         }\n     }\n \n+    private org.apache.storm.generated.Supervisor.Iface createSupervisorIface() {\n+        return new org.apache.storm.generated.Supervisor.Iface() {\n+            @Override\n+            public void sendSupervisorAssignments(SupervisorAssignments assignments)\n+                throws AuthorizationException, TException {\n+                checkAuthorization(\"sendSupervisorAssignments\");\n+                LOG.info(\"Got an assignments from master, will start to sync with assignments: {}\", assignments);\n+                SynchronizeAssignments syn = new SynchronizeAssignments(getSupervisor(), assignments,\n+                    getReadClusterState());\n+                getEventManger().add(syn);\n+            }\n+\n+            @Override\n+            public Assignment getLocalAssignmentForStorm(String id)\n+                throws NotAliveException, AuthorizationException, TException {\n+                Map<String, Object> topoConf = null;\n+                try {\n+                    topoConf = ConfigUtils.readSupervisorStormConf(conf, id);\n+                } catch (IOException e) {\n+                    LOG.warn(\"Topology config is not localized yet...\");\n+                }\n+                checkAuthorization(id, topoConf, \"getLocalAssignmentForStorm\");\n+                Assignment assignment = getStormClusterState().assignmentInfo(id, null);\n+                if (null == assignment) {\n+                    throw new WrappedNotAliveException(\"No local assignment assigned for storm: \"\n+                        + id\n+                        + \" for node: \"\n+                        + getHostName());\n+                }\n+                return assignment;\n+            }\n+\n+            @Override\n+            public void sendSupervisorWorkerHeartbeat(SupervisorWorkerHeartbeat heartbeat)\n+                throws AuthorizationException, NotAliveException, TException {\n+                // do nothing except validate heartbeat for now.\n+                String id = heartbeat.get_storm_id();\n+                Map<String, Object> topoConf = null;\n+                try {\n+                    topoConf = ConfigUtils.readSupervisorStormConf(conf, id);\n+                } catch (IOException e) {\n+                    LOG.warn(\"Topology config is not localized yet...\");\n+                    throw new WrappedNotAliveException(id + \" does not appear to be alive, you should probably exit\");\n+                }\n+                checkAuthorization(id, topoConf, \"sendSupervisorWorkerHeartbeat\");\n+            }\n+        };\n+    }\n+\n+    public org.apache.storm.generated.Supervisor.Iface getSupervisorThriftInterface() {\n+        return supervisorThriftInterface;\n+    }\n+    \n     private void launchSupervisorThriftServer(Map<String, Object> conf) throws IOException {\n         // validate port\n         int port = getThriftServerPort();\n@@ -404,53 +461,7 @@ private void launchSupervisorThriftServer(Map<String, Object> conf) throws IOExc\n             throw new RuntimeException(e);\n         }\n \n-        TProcessor processor = new org.apache.storm.generated.Supervisor.Processor(\n-            new org.apache.storm.generated.Supervisor.Iface() {\n-                @Override\n-                public void sendSupervisorAssignments(SupervisorAssignments assignments)\n-                    throws AuthorizationException, TException {\n-                    checkAuthorization(\"sendSupervisorAssignments\");\n-                    LOG.info(\"Got an assignments from master, will start to sync with assignments: {}\", assignments);\n-                    SynchronizeAssignments syn = new SynchronizeAssignments(getSupervisor(), assignments,\n-                                                                            getReadClusterState());\n-                    getEventManger().add(syn);\n-                }\n-\n-                @Override\n-                public Assignment getLocalAssignmentForStorm(String id)\n-                    throws NotAliveException, AuthorizationException, TException {\n-                    Map<String, Object> topoConf = null;\n-                    try {\n-                        topoConf = ConfigUtils.readSupervisorStormConf(conf, id);\n-                    } catch (IOException e) {\n-                        LOG.warn(\"Topology config is not localized yet...\");\n-                    }\n-                    checkAuthorization(id, topoConf, \"getLocalAssignmentForStorm\");\n-                    Assignment assignment = getStormClusterState().assignmentInfo(id, null);\n-                    if (null == assignment) {\n-                        throw new WrappedNotAliveException(\"No local assignment assigned for storm: \"\n-                                                    + id\n-                                                    + \" for node: \"\n-                                                    + getHostName());\n-                    }\n-                    return assignment;\n-                }\n-\n-                @Override\n-                public void sendSupervisorWorkerHeartbeat(SupervisorWorkerHeartbeat heartbeat)\n-                    throws AuthorizationException, NotAliveException, TException {\n-                    // do nothing except validate heartbeat for now.\n-                    String id = heartbeat.get_storm_id();\n-                    Map<String, Object> topoConf = null;\n-                    try {\n-                        topoConf = ConfigUtils.readSupervisorStormConf(conf, id);\n-                    } catch (IOException e) {\n-                        LOG.warn(\"Topology config is not localized yet...\");\n-                        throw new WrappedNotAliveException(id + \" does not appear to be alive, you should probably exit\");\n-                    }\n-                    checkAuthorization(id, topoConf, \"sendSupervisorWorkerHeartbeat\");\n-                }\n-            });\n+        TProcessor processor = new org.apache.storm.generated.Supervisor.Processor<>(supervisorThriftInterface);\n         this.thriftServer = new ThriftServer(conf, processor, ThriftConnectionType.SUPERVISOR);\n         this.thriftServer.serve();\n     }\n@@ -536,7 +547,8 @@ public void shutdownAllWorkers(BiConsumer<Slot, Long> onWarnTimeout, UniFunc<Slo\n         } else {\n             try {\n                 ContainerLauncher launcher = ContainerLauncher.make(getConf(), getId(), getThriftServerPort(),\n-                                                                    getSharedContext(), getMetricsRegistry(), getContainerMemoryTracker());\n+                                                                    getSharedContext(), getMetricsRegistry(), getContainerMemoryTracker(),\n+                                                                    supervisorThriftInterface);\n                 killWorkers(SupervisorUtils.supervisorWorkerIds(conf), launcher);\n             } catch (Exception e) {\n                 throw Utils.wrapInRuntime(e);", "filename": "storm-server/src/main/java/org/apache/storm/daemon/supervisor/Supervisor.java"}, {"additions": 1, "raw_url": "https://github.com/apache/storm/raw/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-server/src/main/java/org/apache/storm/nimbus/AssignmentDistributionService.java", "blob_url": "https://github.com/apache/storm/blob/c985695e0728eb3af171ca6d346c51cc4e0b083a/storm-server/src/main/java/org/apache/storm/nimbus/AssignmentDistributionService.java", "sha": "05cb1dfa42228616e9833d192c2f9eb3a194368d", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/main/java/org/apache/storm/nimbus/AssignmentDistributionService.java?ref=c985695e0728eb3af171ca6d346c51cc4e0b083a", "patch": "@@ -288,7 +288,7 @@ private void sendAssignmentsToNode(NodeAssignments assignments) {\n                 try (SupervisorClient client = SupervisorClient.getConfiguredClient(service.getConf(),\n                                                                                     assignments.getHost(), assignments.getServerPort())) {\n                     try {\n-                        client.getClient().sendSupervisorAssignments(assignments.getAssignments());\n+                        client.getIface().sendSupervisorAssignments(assignments.getAssignments());\n                     } catch (Exception e) {\n                         //just ignore the exception.\n                         LOG.error(\"Exception when trying to send assignments to node {}: {}\", assignments.getNode(), e.getMessage());", "filename": "storm-server/src/main/java/org/apache/storm/nimbus/AssignmentDistributionService.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/df84dec0a55c7dba4c5f400329481eee8d20492a", "parent": "https://github.com/apache/storm/commit/94cd157c0f12ceb6538169ce4b4f6193f7f77eed", "message": "STORM-3300: Fix NPE in Acker that could occur if sending reset timeout tuples", "bug_id": "storm_7", "file": [{"additions": 4, "raw_url": "https://github.com/apache/storm/raw/df84dec0a55c7dba4c5f400329481eee8d20492a/storm-client/src/jvm/org/apache/storm/daemon/Acker.java", "blob_url": "https://github.com/apache/storm/blob/df84dec0a55c7dba4c5f400329481eee8d20492a/storm-client/src/jvm/org/apache/storm/daemon/Acker.java", "sha": "336957af2bb712f75b41fd3d6cba8474dc6becd9", "changes": 7, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/daemon/Acker.java?ref=df84dec0a55c7dba4c5f400329481eee8d20492a", "patch": "@@ -78,9 +78,10 @@ public void execute(Tuple input) {\n             pending.put(id, curr);\n         } else if (ACKER_RESET_TIMEOUT_STREAM_ID.equals(streamId)) {\n             resetTimeout = true;\n-            if (curr != null) {\n-                pending.put(id, curr);\n-            } //else if it has not been added yet, there is no reason time it out later on\n+            if (curr == null) {\n+                curr = new AckObject();\n+            }\n+            pending.put(id, curr);\n         } else if (Constants.SYSTEM_FLUSH_STREAM_ID.equals(streamId)) {\n             collector.flush();\n             return;", "filename": "storm-client/src/jvm/org/apache/storm/daemon/Acker.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/6d58e63eb085ef21adf9170fbc3e9eb93db0c621", "parent": "https://github.com/apache/storm/commit/f3c14378e4ef48e1516242acd2fd24d287702e98", "message": "STORM-3223: Fix NPE on blacklisted rack", "bug_id": "storm_8", "file": [{"additions": 1, "raw_url": "https://github.com/apache/storm/raw/6d58e63eb085ef21adf9170fbc3e9eb93db0c621/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java", "blob_url": "https://github.com/apache/storm/blob/6d58e63eb085ef21adf9170fbc3e9eb93db0c621/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java", "sha": "59b0f31a1b19a43af3fe861b81d6e96ef97fb6f9", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java?ref=6d58e63eb085ef21adf9170fbc3e9eb93db0c621", "patch": "@@ -332,7 +332,7 @@ public LazyNodeSorting(TopologyDetails td, ExecutorDetails exec,\n \n         private TreeSet<ObjectResources> getSortedNodesFor(String rackId) {\n             return cachedNodes.computeIfAbsent(rackId,\n-                (rid) -> sortNodes(rackIdToNodes.get(rid), exec, td, rid, perNodeScheduledCount));\n+                (rid) -> sortNodes(rackIdToNodes.getOrDefault(rid, Collections.emptyList()), exec, td, rid, perNodeScheduledCount));\n         }\n \n         @Override", "filename": "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/91458a32cded87fb64dbb1b22db8059edd6af99d", "parent": "https://github.com/apache/storm/commit/aa6bc4d2cc49a36f65dcf9c4792c426f17fff069", "message": "STORM-3208 fix worker kill NPE", "bug_id": "storm_9", "file": [{"additions": 6, "raw_url": "https://github.com/apache/storm/raw/91458a32cded87fb64dbb1b22db8059edd6af99d/storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java", "blob_url": "https://github.com/apache/storm/blob/91458a32cded87fb64dbb1b22db8059edd6af99d/storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java", "sha": "082fc3b2bd68b0bcd0ae03b42627173a1d0707db", "changes": 10, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java?ref=91458a32cded87fb64dbb1b22db8059edd6af99d", "patch": "@@ -213,10 +213,12 @@ public void cleanUpForRestart() throws IOException {\n         super.cleanUpForRestart();\n         synchronized (_localState) {\n             Map<String, Integer> workersToPort = _localState.getApprovedWorkers();\n-            workersToPort.remove(origWorkerId);\n-            removeWorkersOn(workersToPort, _port);\n-            _localState.setApprovedWorkers(workersToPort);\n-            LOG.info(\"Removed Worker ID {}\", origWorkerId);\n+            if (workersToPort != null) {\n+                workersToPort.remove(origWorkerId);\n+                removeWorkersOn(workersToPort, _port);\n+                _localState.setApprovedWorkers(workersToPort);\n+                LOG.info(\"Removed Worker ID {}\", origWorkerId);\n+            }\n         }\n     }\n ", "filename": "storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/917be55a5bd6be8baa4811c6684897da949c3bb1", "parent": "https://github.com/apache/storm/commit/14b0b4fc5e0945456769fd58a3595188e3dea234", "message": "STORM-3075 fix NPE", "bug_id": "storm_10", "file": [{"additions": 6, "raw_url": "https://github.com/apache/storm/raw/917be55a5bd6be8baa4811c6684897da949c3bb1/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java", "blob_url": "https://github.com/apache/storm/blob/917be55a5bd6be8baa4811c6684897da949c3bb1/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java", "sha": "48e8c210a2e1dd26730b02da683f898a736ce83e", "changes": 10, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java?ref=917be55a5bd6be8baa4811c6684897da949c3bb1", "patch": "@@ -487,14 +487,16 @@ public Nimbus(Map<String, Object> conf, INimbus inimbus, IStormClusterState stor\n             blobStore = ServerUtils.getNimbusBlobStore(conf, this.nimbusHostPortInfo, null);\n         }\n         this.blobStore = blobStore;\n+\n+        if (topoCache == null) {\n+            topoCache = new TopoCache(blobStore, conf);\n+        }\n         if (leaderElector == null) {\n             leaderElector = Zookeeper.zkLeaderElector(conf, zkClient, blobStore, topoCache, stormClusterState, getNimbusAcls(conf));\n         }\n         this.leaderElector = leaderElector;\n         this.blobStore.setLeaderElector(this.leaderElector);\n-        if (topoCache == null) {\n-            topoCache = new TopoCache(blobStore, conf);\n-        }\n+\n         this.topoCache = topoCache;\n         this.assignmentsDistributer = AssignmentDistributionService.getInstance(conf);\n         this.idToSchedStatus = new AtomicReference<>(new HashMap<>());\n@@ -2136,7 +2138,7 @@ private void mkAssignments(String scratchTopoId) throws Exception {\n                 LOG.info(\"Fragmentation after scheduling is: {} MB, {} PCore CPUs\", fragmentedMemory(), fragmentedCpu());\n                 nodeIdToResources.get().forEach((id, node) ->\n                                                     LOG.info(\n-                                                        \"Node Id: {} Total Mem: {}, Used Mem: {}, Avialble Mem: {}, Total CPU: {}, Used \" +\n+                                                        \"Node Id: {} Total Mem: {}, Used Mem: {}, Available Mem: {}, Total CPU: {}, Used \" +\n                                                         \"CPU: {}, Available CPU: {}, fragmented: {}\",\n                                                         id, node.getTotalMem(), node.getUsedMem(), node.getAvailableMem(),\n                                                         node.getTotalCpu(), node.getUsedCpu(), node.getAvailableCpu(), isFragmented(node)));", "filename": "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/fe388df9b3e9649fbdf9f2e57206ee655212bcc6", "parent": "https://github.com/apache/storm/commit/4cd851bd457a8544cf9987c4b0f8741ef7c8dc93", "message": "STORM-3084 fix Nimbus NPE", "bug_id": "storm_11", "file": [{"additions": 9, "raw_url": "https://github.com/apache/storm/raw/fe388df9b3e9649fbdf9f2e57206ee655212bcc6/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java", "blob_url": "https://github.com/apache/storm/blob/fe388df9b3e9649fbdf9f2e57206ee655212bcc6/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java", "sha": "c586dd1a2556efddd002be12fcf8eacd8841ecb3", "changes": 15, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java?ref=fe388df9b3e9649fbdf9f2e57206ee655212bcc6", "patch": "@@ -61,7 +61,7 @@\n     Map<String, Assignment> assignmentsInfo();\n \n     /**\n-     * Sync the remote state store assignments to local backend, used when master gains leadership, see {@link LeaderListenerCallback}\n+     * Sync the remote state store assignments to local backend, used when master gains leadership, see {@link LeaderListenerCallback}.\n      *\n      * @param remote assigned assignments for a specific {@link IStormClusterState} instance, usually a supervisor/node.\n      */\n@@ -75,7 +75,7 @@\n     boolean isAssignmentsBackendSynchronized();\n \n     /**\n-     * Mark the assignments as synced successfully, see {@link #isAssignmentsBackendSynchronized()}\n+     * Mark the assignments as synced successfully, see {@link #isAssignmentsBackendSynchronized()}.\n      */\n     void setAssignmentsBackendSynchronized();\n \n@@ -92,7 +92,7 @@\n     List<String> activeStorms();\n \n     /**\n-     * Get a storm base for a topology\n+     * Get a storm base for a topology.\n      *\n      * @param stormId  the id of the topology\n      * @param callback something to call if the data changes (best effort)\n@@ -297,13 +297,16 @@\n     default Map<String, SupervisorInfo> allSupervisorInfo(Runnable callback) {\n         Map<String, SupervisorInfo> ret = new HashMap<>();\n         for (String id : supervisors(callback)) {\n-            ret.put(id, supervisorInfo(id));\n+            SupervisorInfo supervisorInfo = supervisorInfo(id);\n+            if (supervisorInfo != null) {\n+                ret.put(id, supervisorInfo);\n+            }\n         }\n         return ret;\n     }\n \n     /**\n-     * Get a topology ID from the name of a topology\n+     * Get a topology ID from the name of a topology.\n      *\n      * @param topologyName the name of the topology to look for\n      * @return the id of the topology or null if it is not alive.\n@@ -316,7 +319,7 @@\n         Map<String, StormBase> stormBases = new HashMap<>();\n         for (String topologyId : activeStorms()) {\n             StormBase base = stormBase(topologyId, null);\n-            if (base != null) { //rece condition with delete\n+            if (base != null) { //race condition with delete\n                 stormBases.put(topologyId, base);\n             }\n         }", "filename": "storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/1613206802ffa00f1b6b9ab56a741e86aeff6a5e", "parent": "https://github.com/apache/storm/commit/26d2f955251c0fa56520f6fe08cb35ef5171f321", "message": "STORM-3141: Fix NPE in WorkerState.transferLocalBatch, and refactor BackpressureTracker to get rid of placeholder JCQueue", "bug_id": "storm_12", "file": [{"additions": 39, "raw_url": "https://github.com/apache/storm/raw/1613206802ffa00f1b6b9ab56a741e86aeff6a5e/storm-client/src/jvm/org/apache/storm/daemon/worker/BackPressureTracker.java", "blob_url": "https://github.com/apache/storm/blob/1613206802ffa00f1b6b9ab56a741e86aeff6a5e/storm-client/src/jvm/org/apache/storm/daemon/worker/BackPressureTracker.java", "sha": "8d96447b8e53ed424e44479c4eb6fd36aca59ad3", "changes": 63, "status": "modified", "deletions": 24, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/daemon/worker/BackPressureTracker.java?ref=1613206802ffa00f1b6b9ab56a741e86aeff6a5e", "patch": "@@ -22,56 +22,53 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Map.Entry;\n-import java.util.concurrent.ConcurrentHashMap;\n-import org.apache.storm.Constants;\n import org.apache.storm.messaging.netty.BackPressureStatus;\n import org.apache.storm.utils.JCQueue;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import static org.apache.storm.Constants.SYSTEM_TASK_ID;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import java.util.stream.Collectors;\n+import org.apache.storm.shade.org.apache.commons.lang.builder.ToStringBuilder;\n+import org.apache.storm.shade.org.apache.commons.lang.builder.ToStringStyle;\n \n /***\n- *   Tracks the BackPressure status using a Map<TaskId, JCQueue>.\n- *   Special value NONE, is used to indicate that the task is not under BackPressure\n- *   ConcurrentHashMap does not allow storing null values, so we use the special value NONE instead.\n+ *   Tracks the BackPressure status.\n  */\n public class BackPressureTracker {\n     static final Logger LOG = LoggerFactory.getLogger(BackPressureTracker.class);\n-    private static final JCQueue NONE = new JCQueue(\"NoneQ\", 2, 0, 1, null,\n-                                                    \"none\", Constants.SYSTEM_COMPONENT_ID, -1, 0) {\n-    };\n-    private final Map<Integer, JCQueue> tasks = new ConcurrentHashMap<>(); // updates are more frequent than iteration\n+    private final Map<Integer, BackpressureState> tasks;\n     private final String workerId;\n \n-    public BackPressureTracker(String workerId, List<Integer> allLocalTasks) {\n+    public BackPressureTracker(String workerId, Map<Integer, JCQueue> localTasksToQueues) {\n         this.workerId = workerId;\n-        for (Integer taskId : allLocalTasks) {\n-            if (taskId != SYSTEM_TASK_ID) {\n-                tasks.put(taskId, NONE);  // all tasks are considered to be not under BP initially\n-            }\n-        }\n+        this.tasks = localTasksToQueues.entrySet().stream()\n+            .collect(Collectors.toMap(\n+                entry -> entry.getKey(),\n+                entry -> new BackpressureState(entry.getValue())));\n     }\n \n     private void recordNoBackPressure(Integer taskId) {\n-        tasks.put(taskId, NONE);\n+        tasks.get(taskId).backpressure.set(false);\n     }\n \n     /***\n      * Record BP for a task.\n      * This is called by transferLocalBatch() on NettyWorker thread\n      * @return true if an update was recorded, false if taskId is already under BP\n      */\n-    public boolean recordBackPressure(Integer taskId, JCQueue recvQ) {\n-        return tasks.put(taskId, recvQ) == NONE;\n+    public boolean recordBackPressure(Integer taskId) {\n+        return tasks.get(taskId).backpressure.getAndSet(true);\n     }\n \n     // returns true if there was a change in the BP situation\n     public boolean refreshBpTaskList() {\n         boolean changed = false;\n         LOG.debug(\"Running Back Pressure status change check\");\n-        for (Entry<Integer, JCQueue> entry : tasks.entrySet()) {\n-            if (entry.getValue() != NONE && entry.getValue().isEmptyOverflow()) {\n+        for (Entry<Integer, BackpressureState> entry : tasks.entrySet()) {\n+            BackpressureState state = entry.getValue();\n+            if (state.backpressure.get() && state.queue.isEmptyOverflow()) {\n                 recordNoBackPressure(entry.getKey());\n                 changed = true;\n             }\n@@ -83,14 +80,32 @@ public BackPressureStatus getCurrStatus() {\n         ArrayList<Integer> bpTasks = new ArrayList<>(tasks.size());\n         ArrayList<Integer> nonBpTasks = new ArrayList<>(tasks.size());\n \n-        for (Entry<Integer, JCQueue> entry : tasks.entrySet()) {\n-            JCQueue q = entry.getValue();\n-            if (q != NONE) {\n+        for (Entry<Integer, BackpressureState> entry : tasks.entrySet()) {\n+            boolean backpressure = entry.getValue().backpressure.get();\n+            if (backpressure) {\n                 bpTasks.add(entry.getKey());\n             } else {\n                 nonBpTasks.add(entry.getKey());\n             }\n         }\n         return new BackPressureStatus(workerId, bpTasks, nonBpTasks);\n     }\n+    \n+    private static class BackpressureState {\n+        private final JCQueue queue;\n+        //No task is under backpressure initially\n+        private final AtomicBoolean backpressure = new AtomicBoolean(false);\n+\n+        public BackpressureState(JCQueue queue) {\n+            this.queue = queue;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+                .append(queue)\n+                .append(backpressure)\n+                .toString();\n+        }\n+    }\n }", "filename": "storm-client/src/jvm/org/apache/storm/daemon/worker/BackPressureTracker.java"}, {"additions": 11, "raw_url": "https://github.com/apache/storm/raw/1613206802ffa00f1b6b9ab56a741e86aeff6a5e/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java", "blob_url": "https://github.com/apache/storm/blob/1613206802ffa00f1b6b9ab56a741e86aeff6a5e/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java", "sha": "9510bc0e0a422f452c6f41ed25b62079313c8288", "changes": 31, "status": "modified", "deletions": 20, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java?ref=1613206802ffa00f1b6b9ab56a741e86aeff6a5e", "patch": "@@ -114,11 +114,9 @@\n     final ReentrantReadWriteLock endpointSocketLock;\n     final AtomicReference<Map<Integer, NodeInfo>> cachedTaskToNodePort;\n     final AtomicReference<Map<NodeInfo, IConnection>> cachedNodeToPortSocket;\n-    final Map<List<Long>, JCQueue> executorReceiveQueueMap;\n     // executor id is in form [start_task_id end_task_id]\n-    // short executor id is start_task_id\n-    final Map<Integer, JCQueue> shortExecutorReceiveQueueMap;\n-    final Map<Integer, Integer> taskToShortExecutor;\n+    final Map<List<Long>, JCQueue> executorReceiveQueueMap;\n+    final Map<Integer, JCQueue> taskToExecutorQueue;\n     final Runnable suicideCallback;\n     final Utils.UptimeComputer uptime;\n     final Map<String, Object> defaultSharedResources;\n@@ -166,12 +164,15 @@ public WorkerState(Map<String, Object> conf, IContext mqContext, String topology\n         this.isTopologyActive = new AtomicBoolean(false);\n         this.stormComponentToDebug = new AtomicReference<>();\n         this.executorReceiveQueueMap = mkReceiveQueueMap(topologyConf, localExecutors);\n-        this.shortExecutorReceiveQueueMap = new HashMap<>();\n         this.localTaskIds = new ArrayList<>();\n+        this.taskToExecutorQueue = new HashMap<>();\n         this.blobToLastKnownVersion = new ConcurrentHashMap<>();\n         for (Map.Entry<List<Long>, JCQueue> entry : executorReceiveQueueMap.entrySet()) {\n-            this.shortExecutorReceiveQueueMap.put(entry.getKey().get(0).intValue(), entry.getValue());\n-            this.localTaskIds.addAll(StormCommon.executorIdToTasks(entry.getKey()));\n+            List<Integer> taskIds = StormCommon.executorIdToTasks(entry.getKey());\n+            for (Integer taskId : taskIds) {\n+                this.taskToExecutorQueue.put(taskId, entry.getValue());\n+            }\n+            this.localTaskIds.addAll(taskIds);\n         }\n         Collections.sort(localTaskIds);\n         this.topologyConf = topologyConf;\n@@ -192,12 +193,6 @@ public WorkerState(Map<String, Object> conf, IContext mqContext, String topology\n         this.endpointSocketLock = new ReentrantReadWriteLock();\n         this.cachedNodeToPortSocket = new AtomicReference<>(new HashMap<>());\n         this.cachedTaskToNodePort = new AtomicReference<>(new HashMap<>());\n-        this.taskToShortExecutor = new HashMap<>();\n-        for (List<Long> executor : this.localExecutors) {\n-            for (Integer task : StormCommon.executorIdToTasks(executor)) {\n-                taskToShortExecutor.put(task, executor.get(0).intValue());\n-            }\n-        }\n         this.suicideCallback = Utils.mkSuicideFn();\n         this.uptime = Utils.makeUptimeComputer();\n         this.defaultSharedResources = makeDefaultResources();\n@@ -212,7 +207,7 @@ public WorkerState(Map<String, Object> conf, IContext mqContext, String topology\n         }\n         int maxTaskId = getMaxTaskId(componentToSortedTasks);\n         this.workerTransfer = new WorkerTransfer(this, topologyConf, maxTaskId);\n-        this.bpTracker = new BackPressureTracker(workerId, localTaskIds);\n+        this.bpTracker = new BackPressureTracker(workerId, taskToExecutorQueue);\n         this.deserializedWorkerHooks = deserializeWorkerHooks();\n     }\n \n@@ -323,10 +318,6 @@ public StormTopology getSystemTopology() {\n         return executorReceiveQueueMap;\n     }\n \n-    public Map<Integer, JCQueue> getShortExecutorReceiveQueueMap() {\n-        return shortExecutorReceiveQueueMap;\n-    }\n-\n     public Runnable getSuicideCallback() {\n         return suicideCallback;\n     }\n@@ -531,7 +522,7 @@ private void transferLocalBatch(ArrayList<AddressedTuple> tupleBatch) {\n \n         for (int i = 0; i < tupleBatch.size(); i++) {\n             AddressedTuple tuple = tupleBatch.get(i);\n-            JCQueue queue = shortExecutorReceiveQueueMap.get(tuple.dest);\n+            JCQueue queue = taskToExecutorQueue.get(tuple.dest);\n \n             // 1- try adding to main queue if its overflow is not empty\n             if (queue.isEmptyOverflow()) {\n@@ -542,7 +533,7 @@ private void transferLocalBatch(ArrayList<AddressedTuple> tupleBatch) {\n \n             // 2- BP detected (i.e MainQ is full). So try adding to overflow\n             int currOverflowCount = queue.getOverflowCount();\n-            if (bpTracker.recordBackPressure(tuple.dest, queue)) {\n+            if (bpTracker.recordBackPressure(tuple.dest)) {\n                 receiver.sendBackPressureStatus(bpTracker.getCurrStatus());\n                 lastOverflowCount = currOverflowCount;\n             } else {", "filename": "storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java"}, {"additions": 34, "raw_url": "https://github.com/apache/storm/raw/1613206802ffa00f1b6b9ab56a741e86aeff6a5e/storm-server/src/test/java/org/apache/storm/MessagingTest.java", "blob_url": "https://github.com/apache/storm/blob/1613206802ffa00f1b6b9ab56a741e86aeff6a5e/storm-server/src/test/java/org/apache/storm/MessagingTest.java", "sha": "74b73883c96aea07cbc9de8804dca679a390a1bd", "changes": 34, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/test/java/org/apache/storm/MessagingTest.java?ref=1613206802ffa00f1b6b9ab56a741e86aeff6a5e", "patch": "@@ -58,4 +58,38 @@ public void testLocalTransport() throws Exception {\n             Assert.assertEquals(6 * 4, Testing.readTuples(results, \"2\").size());\n         }\n     }\n+    \n+    @Test\n+    public void testRemoteTransportWithManyTasksInReceivingExecutor() throws Exception {\n+        //STORM-3141 regression test\n+        //Verify that remote worker can handle many tasks in one executor\n+        Config topoConf = new Config();\n+        topoConf.put(Config.TOPOLOGY_WORKERS, 2);\n+        topoConf.put(Config.STORM_MESSAGING_TRANSPORT, \"org.apache.storm.messaging.netty.Context\");\n+\n+        try (ILocalCluster cluster = new LocalCluster.Builder().withSimulatedTime()\n+                                                               .withSupervisors(1).withPortsPerSupervisor(2)\n+                                                               .withDaemonConf(topoConf).build()) {\n+\n+            TopologyBuilder builder = new TopologyBuilder();\n+            builder.setSpout(\"1\", new TestWordSpout(true), 1);\n+            builder.setBolt(\"2\", new TestGlobalCount(), 1)\n+                .setNumTasks(10)\n+                .shuffleGrouping(\"1\");\n+            StormTopology stormTopology = builder.createTopology();\n+\n+            List<FixedTuple> fixedTuples = new ArrayList<>();\n+            for (int i = 0; i < 12; i++) {\n+                fixedTuples.add(new FixedTuple(Collections.singletonList(\"a\")));\n+                fixedTuples.add(new FixedTuple(Collections.singletonList(\"b\")));\n+            }\n+            Map<String, List<FixedTuple>> data = new HashMap<>();\n+            data.put(\"1\", fixedTuples);\n+            MockedSources mockedSources = new MockedSources(data);\n+            CompleteTopologyParam completeTopologyParam = new CompleteTopologyParam();\n+            completeTopologyParam.setMockedSources(mockedSources);\n+            Map<String, List<FixedTuple>> results = Testing.completeTopology(cluster, stormTopology, completeTopologyParam);\n+            Assert.assertEquals(6 * 4, Testing.readTuples(results, \"2\").size());\n+        }\n+    }\n }", "filename": "storm-server/src/test/java/org/apache/storm/MessagingTest.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/4605ae0a34858309171e726e1924b9a37695c977", "parent": "https://github.com/apache/storm/commit/0f89cd8ab29f7db4ceed9afee4a329ff756ce3d4", "message": "Merge branch 'STORM-3223' of https://github.com/revans2/incubator-storm into STORM-3223\n\nSTORM-3223: Fix NPE on blacklisted rack\n\nThis closes #2830", "bug_id": "storm_13", "file": [{"additions": 1, "raw_url": "https://github.com/apache/storm/raw/4605ae0a34858309171e726e1924b9a37695c977/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java", "blob_url": "https://github.com/apache/storm/blob/4605ae0a34858309171e726e1924b9a37695c977/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java", "sha": "59b0f31a1b19a43af3fe861b81d6e96ef97fb6f9", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java?ref=4605ae0a34858309171e726e1924b9a37695c977", "patch": "@@ -332,7 +332,7 @@ public LazyNodeSorting(TopologyDetails td, ExecutorDetails exec,\n \n         private TreeSet<ObjectResources> getSortedNodesFor(String rackId) {\n             return cachedNodes.computeIfAbsent(rackId,\n-                (rid) -> sortNodes(rackIdToNodes.get(rid), exec, td, rid, perNodeScheduledCount));\n+                (rid) -> sortNodes(rackIdToNodes.getOrDefault(rid, Collections.emptyList()), exec, td, rid, perNodeScheduledCount));\n         }\n \n         @Override", "filename": "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/BaseResourceAwareStrategy.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/88fc18b2f2d031e9915b852e16cd406fbc21a91e", "parent": "https://github.com/apache/storm/commit/8ea2679d47dd14443f44944fed98b9e7b6c6c516", "message": "Merge branch 'agresch_storm-3208' of https://github.com/agresch/storm into STORM-3208\n\nSTORM-3208 fix worker kill NPE\n\nThis closes #2816", "bug_id": "storm_14", "file": [{"additions": 8, "raw_url": "https://github.com/apache/storm/raw/88fc18b2f2d031e9915b852e16cd406fbc21a91e/storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java", "blob_url": "https://github.com/apache/storm/blob/88fc18b2f2d031e9915b852e16cd406fbc21a91e/storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java", "sha": "9a6e5264e93ac9e80efb2c12e1cc95260ca3ba53", "changes": 12, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java?ref=88fc18b2f2d031e9915b852e16cd406fbc21a91e", "patch": "@@ -213,10 +213,14 @@ public void cleanUpForRestart() throws IOException {\n         super.cleanUpForRestart();\n         synchronized (_localState) {\n             Map<String, Integer> workersToPort = _localState.getApprovedWorkers();\n-            workersToPort.remove(origWorkerId);\n-            removeWorkersOn(workersToPort, _port);\n-            _localState.setApprovedWorkers(workersToPort);\n-            LOG.info(\"Removed Worker ID {}\", origWorkerId);\n+            if (workersToPort != null) {\n+                workersToPort.remove(origWorkerId);\n+                removeWorkersOn(workersToPort, _port);\n+                _localState.setApprovedWorkers(workersToPort);\n+                LOG.info(\"Removed Worker ID {}\", origWorkerId);\n+            } else {\n+                LOG.warn(\"No approved workers exists\");\n+            }\n         }\n     }\n ", "filename": "storm-server/src/main/java/org/apache/storm/daemon/supervisor/BasicContainer.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/d541e60bc41f9cbe3ba1469966bd9031ebaec207", "parent": "https://github.com/apache/storm/commit/290458c121181b6213e3efe71cfcd7b5025738c1", "message": "STORM-3065: Reduce storm-server test fork count to 1, fix some test-specific NPEs", "bug_id": "storm_15", "file": [{"additions": 4, "raw_url": "https://github.com/apache/storm/raw/d541e60bc41f9cbe3ba1469966bd9031ebaec207/storm-client/src/jvm/org/apache/storm/zookeeper/ClientZookeeper.java", "blob_url": "https://github.com/apache/storm/blob/d541e60bc41f9cbe3ba1469966bd9031ebaec207/storm-client/src/jvm/org/apache/storm/zookeeper/ClientZookeeper.java", "sha": "d443984939ff20ca42999c94da213323681e6bdf", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/zookeeper/ClientZookeeper.java?ref=d541e60bc41f9cbe3ba1469966bd9031ebaec207", "patch": "@@ -77,14 +77,14 @@ public static CuratorFramework mkClient(Map<String, Object> conf, List<String> s\n     // Deletes the state inside the zookeeper for a key, for which the\n     // contents of the key starts with nimbus host port information\n     public static void deleteNodeBlobstore(CuratorFramework zk, String parentPath, String hostPortInfo) {\n-        String normalizedPatentPath = normalizePath(parentPath);\n+        String normalizedParentPath = normalizePath(parentPath);\n         List<String> childPathList = null;\n-        if (existsNode(zk, normalizedPatentPath, false)) {\n-            childPathList = getChildren(zk, normalizedPatentPath, false);\n+        if (existsNode(zk, normalizedParentPath, false)) {\n+            childPathList = getChildren(zk, normalizedParentPath, false);\n             for (String child : childPathList) {\n                 if (child.startsWith(hostPortInfo)) {\n                     LOG.debug(\"deleteNode child {}\", child);\n-                    deleteNode(zk, normalizedPatentPath + \"/\" + child);\n+                    deleteNode(zk, normalizedParentPath + \"/\" + child);\n                 }\n             }\n         }", "filename": "storm-client/src/jvm/org/apache/storm/zookeeper/ClientZookeeper.java"}, {"additions": 7, "raw_url": "https://github.com/apache/storm/raw/d541e60bc41f9cbe3ba1469966bd9031ebaec207/storm-server/pom.xml", "blob_url": "https://github.com/apache/storm/blob/d541e60bc41f9cbe3ba1469966bd9031ebaec207/storm-server/pom.xml", "sha": "d1b855cb627fb1041f5871a941a35d99a0e1d8f0", "changes": 7, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/pom.xml?ref=d541e60bc41f9cbe3ba1469966bd9031ebaec207", "patch": "@@ -117,6 +117,13 @@\n             </testResource>\n         </testResources>\n         <plugins>\n+            <plugin>\n+                <groupId>org.apache.maven.plugins</groupId>\n+                <artifactId>maven-surefire-plugin</artifactId>\n+                <configuration>\n+                    <forkCount>1</forkCount>\n+                </configuration>\n+            </plugin>\n             <plugin>\n                 <groupId>org.apache.maven.plugins</groupId>\n                 <artifactId>maven-surefire-report-plugin</artifactId>", "filename": "storm-server/pom.xml"}, {"additions": 2, "raw_url": "https://github.com/apache/storm/raw/d541e60bc41f9cbe3ba1469966bd9031ebaec207/storm-server/src/main/java/org/apache/storm/testing/InProcessZookeeper.java", "blob_url": "https://github.com/apache/storm/blob/d541e60bc41f9cbe3ba1469966bd9031ebaec207/storm-server/src/main/java/org/apache/storm/testing/InProcessZookeeper.java", "sha": "c18edccb36186434cb89b8c433af6ecb43556ae6", "changes": 8, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/main/java/org/apache/storm/testing/InProcessZookeeper.java?ref=d541e60bc41f9cbe3ba1469966bd9031ebaec207", "patch": "@@ -26,21 +26,17 @@\n \n     private final TmpPath zkTmp;\n     private final NIOServerCnxnFactory zookeeper;\n-    private final long zkPort;\n \n     public InProcessZookeeper() throws Exception {\n         zkTmp = new TmpPath();\n-        @SuppressWarnings(\"unchecked\")\n-        List<Object> portAndHandle = Zookeeper.mkInprocessZookeeper(zkTmp.getPath(), null);\n-        zkPort = (Long) portAndHandle.get(0);\n-        zookeeper = (NIOServerCnxnFactory) portAndHandle.get(1);\n+        zookeeper = Zookeeper.mkInprocessZookeeper(zkTmp.getPath(), null);\n     }\n \n     /**\n      * @return the port ZK is listening on (localhost)\n      */\n     public long getPort() {\n-        return zkPort;\n+        return zookeeper.getLocalPort();\n     }\n \n     @Override", "filename": "storm-server/src/main/java/org/apache/storm/testing/InProcessZookeeper.java"}, {"additions": 2, "raw_url": "https://github.com/apache/storm/raw/d541e60bc41f9cbe3ba1469966bd9031ebaec207/storm-server/src/main/java/org/apache/storm/zookeeper/Zookeeper.java", "blob_url": "https://github.com/apache/storm/blob/d541e60bc41f9cbe3ba1469966bd9031ebaec207/storm-server/src/main/java/org/apache/storm/zookeeper/Zookeeper.java", "sha": "7a060f8c1e91804766e089604a826e75f8cddc72", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/main/java/org/apache/storm/zookeeper/Zookeeper.java?ref=d541e60bc41f9cbe3ba1469966bd9031ebaec207", "patch": "@@ -72,7 +72,7 @@ public static void resetInstance() {\n         _instance = INSTANCE;\n     }\n \n-    public static List mkInprocessZookeeper(String localdir, Integer port) throws Exception {\n+    public static NIOServerCnxnFactory mkInprocessZookeeper(String localdir, Integer port) throws Exception {\n         File localfile = new File(localdir);\n         ZooKeeperServer zk = new ZooKeeperServer(localfile, localfile, 2000);\n         NIOServerCnxnFactory factory = null;\n@@ -96,7 +96,7 @@ public static List mkInprocessZookeeper(String localdir, Integer port) throws Ex\n         }\n         LOG.info(\"Starting inprocess zookeeper at port {} and dir {}\", report, localdir);\n         factory.startup(zk);\n-        return Arrays.asList((Object) new Long(report), (Object) factory);\n+        return factory;\n     }\n \n     public static void shutdownInprocessZookeeper(NIOServerCnxnFactory handle) {", "filename": "storm-server/src/main/java/org/apache/storm/zookeeper/Zookeeper.java"}, {"additions": 4, "raw_url": "https://github.com/apache/storm/raw/d541e60bc41f9cbe3ba1469966bd9031ebaec207/storm-server/src/test/java/org/apache/storm/blobstore/LocalFsBlobStoreTest.java", "blob_url": "https://github.com/apache/storm/blob/d541e60bc41f9cbe3ba1469966bd9031ebaec207/storm-server/src/test/java/org/apache/storm/blobstore/LocalFsBlobStoreTest.java", "sha": "b2045bf1d685ef81b7336308616a5f05d47bb104", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/test/java/org/apache/storm/blobstore/LocalFsBlobStoreTest.java?ref=d541e60bc41f9cbe3ba1469966bd9031ebaec207", "patch": "@@ -46,7 +46,6 @@\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.assertTrue;\n import static org.junit.Assert.fail;\n-import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.spy;\n \n public class LocalFsBlobStoreTest {\n@@ -67,7 +66,7 @@ public void init() {\n     try {\n       zk = new InProcessZookeeper();\n     } catch (Exception e) {\n-      e.printStackTrace();\n+      throw new RuntimeException(e);\n     }\n   }\n \n@@ -77,7 +76,7 @@ public void cleanup() throws IOException {\n     try {\n       zk.close();\n     } catch (Exception e) {\n-      e.printStackTrace();\n+      throw new RuntimeException(e);\n     }\n   }\n \n@@ -100,7 +99,8 @@ private LocalFsBlobStore initLocalFs() {\n     conf.put(Config.STORM_ZOOKEEPER_PORT, zk.getPort());\n     conf.put(Config.STORM_LOCAL_DIR, baseFile.getAbsolutePath());\n     conf.put(Config.STORM_PRINCIPAL_TO_LOCAL_PLUGIN,\"org.apache.storm.security.auth.DefaultPrincipalToLocal\");\n-    spy.prepare(conf, null, mock(NimbusInfo.class), null);\n+    NimbusInfo nimbusInfo = new NimbusInfo(\"localhost\", 0, false);\n+    spy.prepare(conf, null, nimbusInfo, null);\n     return spy;\n   }\n ", "filename": "storm-server/src/test/java/org/apache/storm/blobstore/LocalFsBlobStoreTest.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/3ad1937ab019eff9cb5c74fa6b51785dffd46529", "parent": "https://github.com/apache/storm/commit/41f977a9e1e10a53633fabd77b30852c052dba3f", "message": "STORM-3048 Fix a Potential NPE\n\n* Commits squashed by Jungtaek Lim <kabhwan@gmail.com>\n\nThis closes #2657", "bug_id": "storm_16", "file": [{"additions": 3, "raw_url": "https://github.com/apache/storm/raw/3ad1937ab019eff9cb5c74fa6b51785dffd46529/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/ConstraintSolverStrategy.java", "blob_url": "https://github.com/apache/storm/blob/3ad1937ab019eff9cb5c74fa6b51785dffd46529/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/ConstraintSolverStrategy.java", "sha": "bf527560175cc75fdbffd7a18c674a066828667c", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/ConstraintSolverStrategy.java?ref=3ad1937ab019eff9cb5c74fa6b51785dffd46529", "patch": "@@ -99,6 +99,7 @@ public static boolean validateSolution(Cluster cluster, TopologyDetails td) {\n      */\n     private static boolean checkConstraintsSatisfied(Cluster cluster, TopologyDetails topo) {\n         LOG.info(\"Checking constraints...\");\n+        assert (cluster.getAssignmentById(topo.getId()) != null);\n         Map<ExecutorDetails, WorkerSlot> result = cluster.getAssignmentById(topo.getId()).getExecutorToSlot();\n         Map<ExecutorDetails, String> execToComp = topo.getExecutorToComponent();\n         //get topology constraints\n@@ -136,6 +137,7 @@ private static boolean checkConstraintsSatisfied(Cluster cluster, TopologyDetail\n \n     private static boolean checkSpreadSchedulingValid(Cluster cluster, TopologyDetails topo) {\n         LOG.info(\"Checking for a valid scheduling...\");\n+        assert (cluster.getAssignmentById(topo.getId()) != null);\n         Map<ExecutorDetails, WorkerSlot> result = cluster.getAssignmentById(topo.getId()).getExecutorToSlot();\n         Map<ExecutorDetails, String> execToComp = topo.getExecutorToComponent();\n         Map<WorkerSlot, HashSet<ExecutorDetails>> workerExecMap = new HashMap<>();\n@@ -174,6 +176,7 @@ private static boolean checkSpreadSchedulingValid(Cluster cluster, TopologyDetai\n      */\n     private static boolean checkResourcesCorrect(Cluster cluster, TopologyDetails topo) {\n         LOG.info(\"Checking Resources...\");\n+        assert (cluster.getAssignmentById(topo.getId()) != null);\n         Map<ExecutorDetails, WorkerSlot> result = cluster.getAssignmentById(topo.getId()).getExecutorToSlot();\n         Map<RAS_Node, Collection<ExecutorDetails>> nodeToExecs = new HashMap<>();\n         Map<ExecutorDetails, WorkerSlot> mergedExecToWorker = new HashMap<>();", "filename": "storm-server/src/main/java/org/apache/storm/scheduler/resource/strategies/scheduling/ConstraintSolverStrategy.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/0267d549a96c1e7aefa3cb5d23e1cca559dd8e74", "parent": "https://github.com/apache/storm/commit/09e01231cc427004bab475c9c70f21fa79cfedef", "message": "Refactor addResource and addResources Method and avoid NPEs", "bug_id": "storm_17", "file": [{"additions": 3, "raw_url": "https://github.com/apache/storm/raw/0267d549a96c1e7aefa3cb5d23e1cca559dd8e74/storm-client/src/jvm/org/apache/storm/coordination/BatchSubtopologyBuilder.java", "blob_url": "https://github.com/apache/storm/blob/0267d549a96c1e7aefa3cb5d23e1cca559dd8e74/storm-client/src/jvm/org/apache/storm/coordination/BatchSubtopologyBuilder.java", "sha": "c1efb46053eb6a57e0a8e8e0dcb8ee9e32d085d3", "changes": 21, "status": "modified", "deletions": 18, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/coordination/BatchSubtopologyBuilder.java?ref=0267d549a96c1e7aefa3cb5d23e1cca559dd8e74", "patch": "@@ -450,17 +450,7 @@ private void addDeclaration(InputDeclaration declaration) {\n         @Override\n         public BoltDeclarer addConfigurations(Map<String, Object> conf) {\n             if (conf != null) {\n-                component.componentConf.putAll(conf);\n-            }\n-            return this;\n-        }\n-\n-        @Override\n-        public BoltDeclarer addResources(Map<String, Double> resources) {\n-            if (resources != null) {\n-                Map<String, Double> currentResources = (Map<String, Double>) component.componentConf.computeIfAbsent(\n-                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());\n-                currentResources.putAll(resources);\n+                getComponentConfiguration().putAll(conf);\n             }\n             return this;\n         }\n@@ -471,14 +461,9 @@ public BoltDeclarer addSharedMemory(SharedMemory request) {\n             return this;\n         }\n \n-        @SuppressWarnings(\"unchecked\")\n         @Override\n-        public BoltDeclarer addResource(String resourceName, Number resourceValue) {\n-            Map<String, Double> resourcesMap = (Map<String, Double>) component.componentConf.computeIfAbsent(\n-                Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());\n-\n-            resourcesMap.put(resourceName, resourceValue.doubleValue());\n-            return this;\n+        public Map<String, Object> getComponentConfiguration() {\n+            return component.componentConf;\n         }\n     }\n }", "filename": "storm-client/src/jvm/org/apache/storm/coordination/BatchSubtopologyBuilder.java"}, {"additions": 7, "raw_url": "https://github.com/apache/storm/raw/0267d549a96c1e7aefa3cb5d23e1cca559dd8e74/storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java", "blob_url": "https://github.com/apache/storm/blob/0267d549a96c1e7aefa3cb5d23e1cca559dd8e74/storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java", "sha": "7d6a6be1c7a804d030f8445f9c281251d95e31f5", "changes": 24, "status": "modified", "deletions": 17, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java?ref=0267d549a96c1e7aefa3cb5d23e1cca559dd8e74", "patch": "@@ -407,24 +407,14 @@ public LinearDRPCInputDeclarer addConfigurations(Map<String, Object> conf) {\n             return this;\n         }\n \n+        /**\n+         * return the current component configuration.\n+         *\n+         * @return the current configuration.\n+         */\n         @Override\n-        public LinearDRPCInputDeclarer addResources(Map<String, Double> resources) {\n-            if (resources != null) {\n-                Map<String, Double> currentResources = (Map<String, Double>) component.componentConf.computeIfAbsent(\n-                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());\n-                currentResources.putAll(resources);\n-            }\n-            return this;\n-        }\n-\n-        @SuppressWarnings(\"unchecked\")\n-        @Override\n-        public LinearDRPCInputDeclarer addResource(String resourceName, Number resourceValue) {\n-            Map<String, Double> resourcesMap = (Map<String, Double>) component.componentConf.computeIfAbsent(\n-                Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());\n-\n-            resourcesMap.put(resourceName, resourceValue.doubleValue());\n-            return this;\n+        public Map<String, Object> getComponentConfiguration() {\n+            return component.componentConf;\n         }\n \n         @Override", "filename": "storm-client/src/jvm/org/apache/storm/drpc/LinearDRPCTopologyBuilder.java"}, {"additions": 26, "raw_url": "https://github.com/apache/storm/raw/0267d549a96c1e7aefa3cb5d23e1cca559dd8e74/storm-client/src/jvm/org/apache/storm/topology/BaseConfigurationDeclarer.java", "blob_url": "https://github.com/apache/storm/blob/0267d549a96c1e7aefa3cb5d23e1cca559dd8e74/storm-client/src/jvm/org/apache/storm/topology/BaseConfigurationDeclarer.java", "sha": "ea78e2a78adb5f7ec84788d23014115652a3cdaa", "changes": 26, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/topology/BaseConfigurationDeclarer.java?ref=0267d549a96c1e7aefa3cb5d23e1cca559dd8e74", "patch": "@@ -86,4 +86,30 @@ public T setCPULoad(Number amount) {\n         }\n         return (T) this;\n     }\n+\n+    @SuppressWarnings(\"unchecked\")\n+    public T addResource(String resourceName, Number resourceValue) {\n+        Map<String, Double> resourcesMap = (Map<String, Double>) getComponentConfiguration().computeIfAbsent(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP,\n+            (x) -> new HashMap<String, Double>());\n+        resourcesMap.put(resourceName, resourceValue.doubleValue());\n+\n+        return (T) this;\n+    }\n+\n+    /**\n+     * Add generic resources for this component.\n+     *\n+     * @param resources\n+     */\n+    @Override\n+    public T addResources(Map<String, Double> resources) {\n+        if (resources != null) {\n+            Map<String, Double> currentResources = (Map<String, Double>) getComponentConfiguration().computeIfAbsent(\n+                Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());\n+            currentResources.putAll(resources);\n+        }\n+        return (T) this;\n+    }\n+\n+\n }", "filename": "storm-client/src/jvm/org/apache/storm/topology/BaseConfigurationDeclarer.java"}, {"additions": 7, "raw_url": "https://github.com/apache/storm/raw/0267d549a96c1e7aefa3cb5d23e1cca559dd8e74/storm-client/src/jvm/org/apache/storm/topology/ComponentConfigurationDeclarer.java", "blob_url": "https://github.com/apache/storm/blob/0267d549a96c1e7aefa3cb5d23e1cca559dd8e74/storm-client/src/jvm/org/apache/storm/topology/ComponentConfigurationDeclarer.java", "sha": "28c44186765ae137430a98fdd727a083e2b3d46f", "changes": 7, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/topology/ComponentConfigurationDeclarer.java?ref=0267d549a96c1e7aefa3cb5d23e1cca559dd8e74", "patch": "@@ -28,6 +28,13 @@\n      */\n     T addConfigurations(Map<String, Object> conf);\n \n+    /**\n+     * return the current component configuration.\n+     *\n+     * @return the current configuration.\n+     */\n+    Map<String, Object> getComponentConfiguration();\n+\n     /**\n      * Add in a single config.\n      * @param config the key for the config", "filename": "storm-client/src/jvm/org/apache/storm/topology/ComponentConfigurationDeclarer.java"}, {"additions": 10, "raw_url": "https://github.com/apache/storm/raw/0267d549a96c1e7aefa3cb5d23e1cca559dd8e74/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java", "blob_url": "https://github.com/apache/storm/blob/0267d549a96c1e7aefa3cb5d23e1cca559dd8e74/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java", "sha": "60a7eb351a514ceee33a72270703801c4e94e75e", "changes": 10, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java?ref=0267d549a96c1e7aefa3cb5d23e1cca559dd8e74", "patch": "@@ -585,6 +585,16 @@ public T addConfigurations(Map<String, Object> conf) {\n             return (T) this;\n         }\n \n+        /**\n+         * return the current component configuration.\n+         *\n+         * @return the current configuration.\n+         */\n+        @Override\n+        public Map<String, Object> getComponentConfiguration() {\n+            return parseJson(commons.get(id).get_json_conf());\n+        }\n+\n         @Override\n         public T addResources(Map<String, Double> resources) {\n             if (resources != null && !resources.isEmpty()) {", "filename": "storm-client/src/jvm/org/apache/storm/topology/TopologyBuilder.java"}, {"additions": 16, "raw_url": "https://github.com/apache/storm/raw/0267d549a96c1e7aefa3cb5d23e1cca559dd8e74/storm-client/src/jvm/org/apache/storm/transactional/TransactionalTopologyBuilder.java", "blob_url": "https://github.com/apache/storm/blob/0267d549a96c1e7aefa3cb5d23e1cca559dd8e74/storm-client/src/jvm/org/apache/storm/transactional/TransactionalTopologyBuilder.java", "sha": "a6602b1ba2c7c2027db91ab7fc6769ad66d4e35a", "changes": 51, "status": "modified", "deletions": 35, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/transactional/TransactionalTopologyBuilder.java?ref=0267d549a96c1e7aefa3cb5d23e1cca559dd8e74", "patch": "@@ -230,31 +230,21 @@ public SpoutDeclarer addConfigurations(Map<String, Object> conf) {\n             return this;\n         }\n \n+        /**\n+         * return the current component configuration.\n+         *\n+         * @return the current configuration.\n+         */\n         @Override\n-        public SpoutDeclarerImpl addResources(Map<String, Double> resources) {\n-            if (resources != null) {\n-                Map<String, Double> currentResources = (Map<String, Double>) spoutConf.computeIfAbsent(\n-                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());\n-                currentResources.putAll(resources);\n-            }\n-            return this;\n+        public Map<String, Object> getComponentConfiguration() {\n+            return spoutConf;\n         }\n \n         @Override\n         public SpoutDeclarer addSharedMemory(SharedMemory request) {\n             spoutSharedMemory.add(request);\n             return this;\n         }\n-\n-        @SuppressWarnings(\"unchecked\")\n-        @Override\n-        public SpoutDeclarer addResource(String resourceName, Number resourceValue) {\n-            Map<String, Double> resourcesMap = (Map<String, Double>) spoutConf.computeIfAbsent(Config.TOPOLOGY_COMPONENT_RESOURCES_MAP,\n-                (k) -> new HashMap<>());\n-\n-            resourcesMap.put(resourceName, resourceValue.doubleValue());\n-            return this;\n-        }\n     }\n     \n     private static class BoltDeclarerImpl extends BaseConfigurationDeclarer<BoltDeclarer> implements BoltDeclarer {\n@@ -553,34 +543,25 @@ private void addDeclaration(InputDeclaration declaration) {\n         @Override\n         public BoltDeclarer addConfigurations(Map<String, Object> conf) {\n             if (conf != null) {\n-                component.componentConf.putAll(conf);\n+                getComponentConfiguration().putAll(conf);\n             }\n             return this;\n         }\n \n+        /**\n+         * return the current component configuration.\n+         *\n+         * @return the current configuration.\n+         */\n         @Override\n-        public BoltDeclarer addResources(Map<String, Double> resources) {\n-            if (resources != null) {\n-                Map<String, Double> currentResources = (Map<String, Double>) component.componentConf.computeIfAbsent(\n-                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());\n-                currentResources.putAll(resources);\n-            }\n-            return this;\n+        public Map<String, Object> getComponentConfiguration() {\n+            return component.componentConf;\n         }\n+\n         @Override\n         public BoltDeclarer addSharedMemory(SharedMemory request) {\n             component.sharedMemory.add(request);\n             return this;\n         }\n-\n-        @SuppressWarnings(\"unchecked\")\n-        @Override\n-        public BoltDeclarer addResource(String resourceName, Number resourceValue) {\n-            Map<String, Double> resourcesMap = (Map<String, Double>) component.componentConf.computeIfAbsent(\n-                Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());\n-\n-            resourcesMap.put(resourceName, resourceValue.doubleValue());\n-            return this;\n-        }\n     }\n }", "filename": "storm-client/src/jvm/org/apache/storm/transactional/TransactionalTopologyBuilder.java"}, {"additions": 15, "raw_url": "https://github.com/apache/storm/raw/0267d549a96c1e7aefa3cb5d23e1cca559dd8e74/storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java", "blob_url": "https://github.com/apache/storm/blob/0267d549a96c1e7aefa3cb5d23e1cca559dd8e74/storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java", "sha": "fcf8174deeb7de3abc043911f5e36235954f744f", "changes": 50, "status": "modified", "deletions": 35, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java?ref=0267d549a96c1e7aefa3cb5d23e1cca559dd8e74", "patch": "@@ -366,24 +366,14 @@ public SpoutDeclarer addConfigurations(Map<String, Object> conf) {\n             return this;\n         }\n \n+        /**\n+         * return the current component configuration.\n+         *\n+         * @return the current configuration.\n+         */\n         @Override\n-        public SpoutDeclarer addResources(Map<String, Double> resources) {\n-            if (resources != null) {\n-                Map<String, Double> currentResources = (Map<String, Double>) component.componentConf.computeIfAbsent(\n-                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());\n-                currentResources.putAll(resources);\n-            }\n-            return this;\n-        }\n-\n-        @SuppressWarnings(\"unchecked\")\n-        @Override\n-        public SpoutDeclarer addResource(String resourceName, Number resourceValue) {\n-            Map<String, Double> resourcesMap = (Map<String, Double>) component.componentConf.computeIfAbsent(\n-                Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());\n-\n-            resourcesMap.put(resourceName, resourceValue.doubleValue());\n-            return this;\n+        public Map<String, Object> getComponentConfiguration() {\n+            return component.componentConf;\n         }\n \n         @Override\n@@ -779,30 +769,20 @@ public BoltDeclarer addConfigurations(Map<String, Object> conf) {\n             return this;\n         }\n \n+        /**\n+         * return the current component configuration.\n+         *\n+         * @return the current configuration.\n+         */\n         @Override\n-        public BoltDeclarer addResources(Map<String, Double> resources) {\n-            if (resources != null) {\n-                Map<String, Double> currentResources = (Map<String, Double>) component.componentConf.computeIfAbsent(\n-                    Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());\n-                currentResources.putAll(resources);\n-            }\n-            return this;\n+        public Map<String, Object> getComponentConfiguration() {\n+            return component.componentConf;\n         }\n \n-        @Override\n+       @Override\n         public BoltDeclarer addSharedMemory(SharedMemory request) {\n             component.sharedMemory.add(request);\n             return this;\n         }\n-\n-        @SuppressWarnings(\"unchecked\")\n-        @Override\n-        public BoltDeclarer addResource(String resourceName, Number resourceValue) {\n-            Map<String, Double> resourcesMap = (Map<String, Double>) component.componentConf.computeIfAbsent(\n-                Config.TOPOLOGY_COMPONENT_RESOURCES_MAP, (k) -> new HashMap<>());\n-\n-            resourcesMap.put(resourceName, resourceValue.doubleValue());\n-            return this;\n-        }\n     }    \n }", "filename": "storm-client/src/jvm/org/apache/storm/trident/topology/TridentTopologyBuilder.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/07c795af5dd398832049262d25858ecc846f4ac5", "parent": "https://github.com/apache/storm/commit/2d7c7d31679b16a8797f39ff3f1c8df2b63952d7", "message": "Merge branch 'STORM-3141' of https://github.com/srdo/storm into STORM-3141\n\nSTORM-3141: Fix NPE in WorkerState.transferLocalBatch\n\nThis closes #2750", "bug_id": "storm_18", "file": [{"additions": 39, "raw_url": "https://github.com/apache/storm/raw/07c795af5dd398832049262d25858ecc846f4ac5/storm-client/src/jvm/org/apache/storm/daemon/worker/BackPressureTracker.java", "blob_url": "https://github.com/apache/storm/blob/07c795af5dd398832049262d25858ecc846f4ac5/storm-client/src/jvm/org/apache/storm/daemon/worker/BackPressureTracker.java", "sha": "a4e87bae623f64d483ae9f5e54863d40d5513cbb", "changes": 63, "status": "modified", "deletions": 24, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/daemon/worker/BackPressureTracker.java?ref=07c795af5dd398832049262d25858ecc846f4ac5", "patch": "@@ -22,56 +22,53 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Map.Entry;\n-import java.util.concurrent.ConcurrentHashMap;\n-import org.apache.storm.Constants;\n import org.apache.storm.messaging.netty.BackPressureStatus;\n import org.apache.storm.utils.JCQueue;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n-import static org.apache.storm.Constants.SYSTEM_TASK_ID;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import java.util.stream.Collectors;\n+import org.apache.storm.shade.org.apache.commons.lang.builder.ToStringBuilder;\n+import org.apache.storm.shade.org.apache.commons.lang.builder.ToStringStyle;\n \n /***\n- *   Tracks the BackPressure status using a Map<TaskId, JCQueue>.\n- *   Special value NONE, is used to indicate that the task is not under BackPressure\n- *   ConcurrentHashMap does not allow storing null values, so we use the special value NONE instead.\n+ *   Tracks the BackPressure status.\n  */\n public class BackPressureTracker {\n     static final Logger LOG = LoggerFactory.getLogger(BackPressureTracker.class);\n-    private static final JCQueue NONE = new JCQueue(\"NoneQ\", 2, 0, 1, null,\n-                                                    \"none\", Constants.SYSTEM_COMPONENT_ID, -1, 0) {\n-    };\n-    private final Map<Integer, JCQueue> tasks = new ConcurrentHashMap<>(); // updates are more frequent than iteration\n+    private final Map<Integer, BackpressureState> tasks;\n     private final String workerId;\n \n-    public BackPressureTracker(String workerId, List<Integer> allLocalTasks) {\n+    public BackPressureTracker(String workerId, Map<Integer, JCQueue> localTasksToQueues) {\n         this.workerId = workerId;\n-        for (Integer taskId : allLocalTasks) {\n-            if (taskId != SYSTEM_TASK_ID) {\n-                tasks.put(taskId, NONE);  // all tasks are considered to be not under BP initially\n-            }\n-        }\n+        this.tasks = localTasksToQueues.entrySet().stream()\n+            .collect(Collectors.toMap(\n+                entry -> entry.getKey(),\n+                entry -> new BackpressureState(entry.getValue())));\n     }\n \n     private void recordNoBackPressure(Integer taskId) {\n-        tasks.put(taskId, NONE);\n+        tasks.get(taskId).backpressure.set(false);\n     }\n \n     /***\n      * Record BP for a task.\n      * This is called by transferLocalBatch() on NettyWorker thread\n      * @return true if an update was recorded, false if taskId is already under BP\n      */\n-    public boolean recordBackPressure(Integer taskId, JCQueue recvQ) {\n-        return tasks.put(taskId, recvQ) == NONE;\n+    public boolean recordBackPressure(Integer taskId) {\n+        return tasks.get(taskId).backpressure.getAndSet(true) == false;\n     }\n \n     // returns true if there was a change in the BP situation\n     public boolean refreshBpTaskList() {\n         boolean changed = false;\n         LOG.debug(\"Running Back Pressure status change check\");\n-        for (Entry<Integer, JCQueue> entry : tasks.entrySet()) {\n-            if (entry.getValue() != NONE && entry.getValue().isEmptyOverflow()) {\n+        for (Entry<Integer, BackpressureState> entry : tasks.entrySet()) {\n+            BackpressureState state = entry.getValue();\n+            if (state.backpressure.get() && state.queue.isEmptyOverflow()) {\n                 recordNoBackPressure(entry.getKey());\n                 changed = true;\n             }\n@@ -83,14 +80,32 @@ public BackPressureStatus getCurrStatus() {\n         ArrayList<Integer> bpTasks = new ArrayList<>(tasks.size());\n         ArrayList<Integer> nonBpTasks = new ArrayList<>(tasks.size());\n \n-        for (Entry<Integer, JCQueue> entry : tasks.entrySet()) {\n-            JCQueue q = entry.getValue();\n-            if (q != NONE) {\n+        for (Entry<Integer, BackpressureState> entry : tasks.entrySet()) {\n+            boolean backpressure = entry.getValue().backpressure.get();\n+            if (backpressure) {\n                 bpTasks.add(entry.getKey());\n             } else {\n                 nonBpTasks.add(entry.getKey());\n             }\n         }\n         return new BackPressureStatus(workerId, bpTasks, nonBpTasks);\n     }\n+    \n+    private static class BackpressureState {\n+        private final JCQueue queue;\n+        //No task is under backpressure initially\n+        private final AtomicBoolean backpressure = new AtomicBoolean(false);\n+\n+        public BackpressureState(JCQueue queue) {\n+            this.queue = queue;\n+        }\n+\n+        @Override\n+        public String toString() {\n+            return new ToStringBuilder(this, ToStringStyle.SHORT_PREFIX_STYLE)\n+                .append(queue)\n+                .append(backpressure)\n+                .toString();\n+        }\n+    }\n }", "filename": "storm-client/src/jvm/org/apache/storm/daemon/worker/BackPressureTracker.java"}, {"additions": 11, "raw_url": "https://github.com/apache/storm/raw/07c795af5dd398832049262d25858ecc846f4ac5/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java", "blob_url": "https://github.com/apache/storm/blob/07c795af5dd398832049262d25858ecc846f4ac5/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java", "sha": "9510bc0e0a422f452c6f41ed25b62079313c8288", "changes": 31, "status": "modified", "deletions": 20, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java?ref=07c795af5dd398832049262d25858ecc846f4ac5", "patch": "@@ -114,11 +114,9 @@\n     final ReentrantReadWriteLock endpointSocketLock;\n     final AtomicReference<Map<Integer, NodeInfo>> cachedTaskToNodePort;\n     final AtomicReference<Map<NodeInfo, IConnection>> cachedNodeToPortSocket;\n-    final Map<List<Long>, JCQueue> executorReceiveQueueMap;\n     // executor id is in form [start_task_id end_task_id]\n-    // short executor id is start_task_id\n-    final Map<Integer, JCQueue> shortExecutorReceiveQueueMap;\n-    final Map<Integer, Integer> taskToShortExecutor;\n+    final Map<List<Long>, JCQueue> executorReceiveQueueMap;\n+    final Map<Integer, JCQueue> taskToExecutorQueue;\n     final Runnable suicideCallback;\n     final Utils.UptimeComputer uptime;\n     final Map<String, Object> defaultSharedResources;\n@@ -166,12 +164,15 @@ public WorkerState(Map<String, Object> conf, IContext mqContext, String topology\n         this.isTopologyActive = new AtomicBoolean(false);\n         this.stormComponentToDebug = new AtomicReference<>();\n         this.executorReceiveQueueMap = mkReceiveQueueMap(topologyConf, localExecutors);\n-        this.shortExecutorReceiveQueueMap = new HashMap<>();\n         this.localTaskIds = new ArrayList<>();\n+        this.taskToExecutorQueue = new HashMap<>();\n         this.blobToLastKnownVersion = new ConcurrentHashMap<>();\n         for (Map.Entry<List<Long>, JCQueue> entry : executorReceiveQueueMap.entrySet()) {\n-            this.shortExecutorReceiveQueueMap.put(entry.getKey().get(0).intValue(), entry.getValue());\n-            this.localTaskIds.addAll(StormCommon.executorIdToTasks(entry.getKey()));\n+            List<Integer> taskIds = StormCommon.executorIdToTasks(entry.getKey());\n+            for (Integer taskId : taskIds) {\n+                this.taskToExecutorQueue.put(taskId, entry.getValue());\n+            }\n+            this.localTaskIds.addAll(taskIds);\n         }\n         Collections.sort(localTaskIds);\n         this.topologyConf = topologyConf;\n@@ -192,12 +193,6 @@ public WorkerState(Map<String, Object> conf, IContext mqContext, String topology\n         this.endpointSocketLock = new ReentrantReadWriteLock();\n         this.cachedNodeToPortSocket = new AtomicReference<>(new HashMap<>());\n         this.cachedTaskToNodePort = new AtomicReference<>(new HashMap<>());\n-        this.taskToShortExecutor = new HashMap<>();\n-        for (List<Long> executor : this.localExecutors) {\n-            for (Integer task : StormCommon.executorIdToTasks(executor)) {\n-                taskToShortExecutor.put(task, executor.get(0).intValue());\n-            }\n-        }\n         this.suicideCallback = Utils.mkSuicideFn();\n         this.uptime = Utils.makeUptimeComputer();\n         this.defaultSharedResources = makeDefaultResources();\n@@ -212,7 +207,7 @@ public WorkerState(Map<String, Object> conf, IContext mqContext, String topology\n         }\n         int maxTaskId = getMaxTaskId(componentToSortedTasks);\n         this.workerTransfer = new WorkerTransfer(this, topologyConf, maxTaskId);\n-        this.bpTracker = new BackPressureTracker(workerId, localTaskIds);\n+        this.bpTracker = new BackPressureTracker(workerId, taskToExecutorQueue);\n         this.deserializedWorkerHooks = deserializeWorkerHooks();\n     }\n \n@@ -323,10 +318,6 @@ public StormTopology getSystemTopology() {\n         return executorReceiveQueueMap;\n     }\n \n-    public Map<Integer, JCQueue> getShortExecutorReceiveQueueMap() {\n-        return shortExecutorReceiveQueueMap;\n-    }\n-\n     public Runnable getSuicideCallback() {\n         return suicideCallback;\n     }\n@@ -531,7 +522,7 @@ private void transferLocalBatch(ArrayList<AddressedTuple> tupleBatch) {\n \n         for (int i = 0; i < tupleBatch.size(); i++) {\n             AddressedTuple tuple = tupleBatch.get(i);\n-            JCQueue queue = shortExecutorReceiveQueueMap.get(tuple.dest);\n+            JCQueue queue = taskToExecutorQueue.get(tuple.dest);\n \n             // 1- try adding to main queue if its overflow is not empty\n             if (queue.isEmptyOverflow()) {\n@@ -542,7 +533,7 @@ private void transferLocalBatch(ArrayList<AddressedTuple> tupleBatch) {\n \n             // 2- BP detected (i.e MainQ is full). So try adding to overflow\n             int currOverflowCount = queue.getOverflowCount();\n-            if (bpTracker.recordBackPressure(tuple.dest, queue)) {\n+            if (bpTracker.recordBackPressure(tuple.dest)) {\n                 receiver.sendBackPressureStatus(bpTracker.getCurrStatus());\n                 lastOverflowCount = currOverflowCount;\n             } else {", "filename": "storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java"}, {"additions": 119, "raw_url": "https://github.com/apache/storm/raw/07c795af5dd398832049262d25858ecc846f4ac5/storm-client/test/jvm/org/apache/storm/daemon/worker/BackPressureTrackerTest.java", "blob_url": "https://github.com/apache/storm/blob/07c795af5dd398832049262d25858ecc846f4ac5/storm-client/test/jvm/org/apache/storm/daemon/worker/BackPressureTrackerTest.java", "sha": "7e891b5c48ca0ad784f99674184b0c4ecc1328f7", "changes": 119, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/test/jvm/org/apache/storm/daemon/worker/BackPressureTrackerTest.java?ref=07c795af5dd398832049262d25858ecc846f4ac5", "patch": "@@ -0,0 +1,119 @@\n+/*\n+ * Copyright 2018 The Apache Software Foundation.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.daemon.worker;\n+\n+import static org.hamcrest.Matchers.contains;\n+import static org.hamcrest.Matchers.empty;\n+import static org.hamcrest.Matchers.is;\n+import static org.junit.Assert.assertThat;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+import java.util.Collections;\n+import org.apache.storm.messaging.netty.BackPressureStatus;\n+import org.apache.storm.shade.org.apache.curator.shaded.com.google.common.collect.ImmutableMap;\n+import org.apache.storm.utils.JCQueue;\n+import org.junit.Test;\n+\n+public class BackPressureTrackerTest {\n+\n+    private static final String WORKER_ID = \"worker\";\n+\n+    @Test\n+    public void testGetBackpressure() {\n+        int taskIdNoBackPressure = 1;\n+        JCQueue noBackPressureQueue = mock(JCQueue.class);\n+        BackPressureTracker tracker = new BackPressureTracker(WORKER_ID,\n+            Collections.singletonMap(taskIdNoBackPressure, noBackPressureQueue));\n+\n+        BackPressureStatus status = tracker.getCurrStatus();\n+\n+        assertThat(status.workerId, is(WORKER_ID));\n+        assertThat(status.nonBpTasks, contains(taskIdNoBackPressure));\n+        assertThat(status.bpTasks, is(empty()));\n+    }\n+\n+    @Test\n+    public void testSetBackpressure() {\n+        int taskIdNoBackPressure = 1;\n+        JCQueue noBackPressureQueue = mock(JCQueue.class);\n+        int taskIdBackPressure = 2;\n+        JCQueue backPressureQueue = mock(JCQueue.class);\n+        BackPressureTracker tracker = new BackPressureTracker(WORKER_ID, ImmutableMap.of(\n+            taskIdNoBackPressure, noBackPressureQueue,\n+            taskIdBackPressure, backPressureQueue));\n+\n+        boolean backpressureChanged = tracker.recordBackPressure(taskIdBackPressure);\n+        BackPressureStatus status = tracker.getCurrStatus();\n+\n+        assertThat(backpressureChanged, is(true));\n+        assertThat(status.workerId, is(WORKER_ID));\n+        assertThat(status.nonBpTasks, contains(taskIdNoBackPressure));\n+        assertThat(status.bpTasks, contains(taskIdBackPressure));\n+    }\n+\n+    @Test\n+    public void testSetBackpressureWithExistingBackpressure() {\n+        int taskId = 1;\n+        JCQueue queue = mock(JCQueue.class);\n+        BackPressureTracker tracker = new BackPressureTracker(WORKER_ID, ImmutableMap.of(\n+            taskId, queue));\n+        tracker.recordBackPressure(taskId);\n+\n+        boolean backpressureChanged = tracker.recordBackPressure(taskId);\n+        BackPressureStatus status = tracker.getCurrStatus();\n+\n+        assertThat(backpressureChanged, is(false));\n+        assertThat(status.workerId, is(WORKER_ID));\n+        assertThat(status.bpTasks, contains(taskId));\n+    }\n+\n+    @Test\n+    public void testRefreshBackpressureWithEmptyOverflow() {\n+        int taskId = 1;\n+        JCQueue queue = mock(JCQueue.class);\n+        when(queue.isEmptyOverflow()).thenReturn(true);\n+        BackPressureTracker tracker = new BackPressureTracker(WORKER_ID, ImmutableMap.of(\n+            taskId, queue));\n+        tracker.recordBackPressure(taskId);\n+\n+        boolean backpressureChanged = tracker.refreshBpTaskList();\n+        BackPressureStatus status = tracker.getCurrStatus();\n+\n+        assertThat(backpressureChanged, is(true));\n+        assertThat(status.workerId, is(WORKER_ID));\n+        assertThat(status.nonBpTasks, contains(taskId));\n+    }\n+\n+    @Test\n+    public void testRefreshBackPressureWithNonEmptyOverflow() {\n+        int taskId = 1;\n+        JCQueue queue = mock(JCQueue.class);\n+        when(queue.isEmptyOverflow()).thenReturn(false);\n+        BackPressureTracker tracker = new BackPressureTracker(WORKER_ID, ImmutableMap.of(\n+            taskId, queue));\n+        tracker.recordBackPressure(taskId);\n+\n+        boolean backpressureChanged = tracker.refreshBpTaskList();\n+        BackPressureStatus status = tracker.getCurrStatus();\n+\n+        assertThat(backpressureChanged, is(false));\n+        assertThat(status.workerId, is(WORKER_ID));\n+        assertThat(status.bpTasks, contains(taskId));\n+    }\n+\n+}", "filename": "storm-client/test/jvm/org/apache/storm/daemon/worker/BackPressureTrackerTest.java"}, {"additions": 34, "raw_url": "https://github.com/apache/storm/raw/07c795af5dd398832049262d25858ecc846f4ac5/storm-server/src/test/java/org/apache/storm/MessagingTest.java", "blob_url": "https://github.com/apache/storm/blob/07c795af5dd398832049262d25858ecc846f4ac5/storm-server/src/test/java/org/apache/storm/MessagingTest.java", "sha": "74b73883c96aea07cbc9de8804dca679a390a1bd", "changes": 34, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/test/java/org/apache/storm/MessagingTest.java?ref=07c795af5dd398832049262d25858ecc846f4ac5", "patch": "@@ -58,4 +58,38 @@ public void testLocalTransport() throws Exception {\n             Assert.assertEquals(6 * 4, Testing.readTuples(results, \"2\").size());\n         }\n     }\n+    \n+    @Test\n+    public void testRemoteTransportWithManyTasksInReceivingExecutor() throws Exception {\n+        //STORM-3141 regression test\n+        //Verify that remote worker can handle many tasks in one executor\n+        Config topoConf = new Config();\n+        topoConf.put(Config.TOPOLOGY_WORKERS, 2);\n+        topoConf.put(Config.STORM_MESSAGING_TRANSPORT, \"org.apache.storm.messaging.netty.Context\");\n+\n+        try (ILocalCluster cluster = new LocalCluster.Builder().withSimulatedTime()\n+                                                               .withSupervisors(1).withPortsPerSupervisor(2)\n+                                                               .withDaemonConf(topoConf).build()) {\n+\n+            TopologyBuilder builder = new TopologyBuilder();\n+            builder.setSpout(\"1\", new TestWordSpout(true), 1);\n+            builder.setBolt(\"2\", new TestGlobalCount(), 1)\n+                .setNumTasks(10)\n+                .shuffleGrouping(\"1\");\n+            StormTopology stormTopology = builder.createTopology();\n+\n+            List<FixedTuple> fixedTuples = new ArrayList<>();\n+            for (int i = 0; i < 12; i++) {\n+                fixedTuples.add(new FixedTuple(Collections.singletonList(\"a\")));\n+                fixedTuples.add(new FixedTuple(Collections.singletonList(\"b\")));\n+            }\n+            Map<String, List<FixedTuple>> data = new HashMap<>();\n+            data.put(\"1\", fixedTuples);\n+            MockedSources mockedSources = new MockedSources(data);\n+            CompleteTopologyParam completeTopologyParam = new CompleteTopologyParam();\n+            completeTopologyParam.setMockedSources(mockedSources);\n+            Map<String, List<FixedTuple>> results = Testing.completeTopology(cluster, stormTopology, completeTopologyParam);\n+            Assert.assertEquals(6 * 4, Testing.readTuples(results, \"2\").size());\n+        }\n+    }\n }", "filename": "storm-server/src/test/java/org/apache/storm/MessagingTest.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/b25d580780d31598f568bbe8dde800fe4b0d6f61", "parent": "https://github.com/apache/storm/commit/ee1be2b73fffff05e32cfd15857561c24655d90c", "message": "STORM-2808: Fix for NPE in UI", "bug_id": "storm_19", "file": [{"additions": 5, "raw_url": "https://github.com/apache/storm/raw/b25d580780d31598f568bbe8dde800fe4b0d6f61/storm-core/src/clj/org/apache/storm/ui/core.clj", "blob_url": "https://github.com/apache/storm/blob/b25d580780d31598f568bbe8dde800fe4b0d6f61/storm-core/src/clj/org/apache/storm/ui/core.clj", "sha": "d12ff749832753013b22d7edab56cf5815d00d99", "changes": 10, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/clj/org/apache/storm/ui/core.clj?ref=b25d580780d31598f568bbe8dde800fe4b0d6f61", "patch": "@@ -46,7 +46,7 @@\n   (:import [org.apache.storm.generated AuthorizationException ProfileRequest ProfileAction NodeInfo])\n   (:import [org.apache.storm.security.auth AuthUtils])\n   (:import [org.apache.storm.utils VersionInfo ConfigUtils Utils WebAppUtils])\n-  (:import [org.apache.storm Config])\n+  (:import [org.apache.storm Config Constants])\n   (:import [java.io File])\n   (:import [java.net URLEncoder URLDecoder])\n   (:import [org.json.simple JSONValue])\n@@ -425,8 +425,8 @@\n            resourceSummary (if (> (.size sups) 0)\n                              (reduce #(map + %1 %2)\n                                (for [^SupervisorSummary s sups\n-                                     :let [sup-total-mem (get (.get_total_resources s) Config/SUPERVISOR_MEMORY_CAPACITY_MB)\n-                                           sup-total-cpu (get (.get_total_resources s) Config/SUPERVISOR_CPU_CAPACITY)\n+                                     :let [sup-total-mem (get (.get_total_resources s) Constants/COMMON_TOTAL_MEMORY_RESOURCE_NAME)\n+                                           sup-total-cpu (get (.get_total_resources s) Constants/COMMON_CPU_RESOURCE_NAME)\n                                            sup-avail-mem (max (- sup-total-mem (.get_used_mem s)) 0.0)\n                                            sup-avail-cpu (max (- sup-total-cpu (.get_used_cpu s)) 0.0)\n                                            sup-fragmented-cpu (.get_fragmented_cpu s)\n@@ -518,8 +518,8 @@\n   (let [slotsTotal (.get_num_workers summary)\n         slotsUsed (.get_num_used_workers summary)\n         slotsFree (max (- slotsTotal slotsUsed) 0)\n-        totalMem (get (.get_total_resources summary) Config/SUPERVISOR_MEMORY_CAPACITY_MB)\n-        totalCpu (get (.get_total_resources summary) Config/SUPERVISOR_CPU_CAPACITY)\n+        totalMem (get (.get_total_resources summary) Constants/COMMON_TOTAL_MEMORY_RESOURCE_NAME)\n+        totalCpu (get (.get_total_resources summary) Constants/COMMON_CPU_RESOURCE_NAME)\n         usedMem (.get_used_mem summary)\n         usedCpu (.get_used_cpu summary)\n         availMem (max (- totalMem usedMem) 0)", "filename": "storm-core/src/clj/org/apache/storm/ui/core.clj"}, {"additions": 2, "raw_url": "https://github.com/apache/storm/raw/b25d580780d31598f568bbe8dde800fe4b0d6f61/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java", "blob_url": "https://github.com/apache/storm/blob/b25d580780d31598f568bbe8dde800fe4b0d6f61/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java", "sha": "f878cf052dd45b5de4a6b89dc830d840a19bfd86", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java?ref=b25d580780d31598f568bbe8dde800fe4b0d6f61", "patch": "@@ -958,8 +958,8 @@ private static void setLoggerTimeouts(LogLevel level) {\n             List<DataPoint> metrics = new ArrayList<>();\n             metrics.add(new DataPoint(\"slotsTotal\", sup.get_num_workers()));\n             metrics.add(new DataPoint(\"slotsUsed\", sup.get_num_used_workers()));\n-            metrics.add(new DataPoint(\"totalMem\", sup.get_total_resources().get(Config.SUPERVISOR_MEMORY_CAPACITY_MB)));\n-            metrics.add(new DataPoint(\"totalCpu\", sup.get_total_resources().get(Config.SUPERVISOR_CPU_CAPACITY)));\n+            metrics.add(new DataPoint(\"totalMem\", sup.get_total_resources().get(Constants.COMMON_TOTAL_MEMORY_RESOURCE_NAME)));\n+            metrics.add(new DataPoint(\"totalCpu\", sup.get_total_resources().get(Constants.COMMON_CPU_RESOURCE_NAME)));\n             metrics.add(new DataPoint(\"usedMem\", sup.get_used_mem()));\n             metrics.add(new DataPoint(\"usedCpu\", sup.get_used_cpu()));\n             ret.put(info, metrics);", "filename": "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/3370b2d6495be770989c048a35cd3998e899bd60", "parent": "https://github.com/apache/storm/commit/02639f4f07140fdc3248a9ef869d62654d378d24", "message": "STORM-3059: Fix NPE when the processing guarantee is not AT_LEAST_ONCE and the spout filters out a null tuple", "bug_id": "storm_20", "file": [{"additions": 5, "raw_url": "https://github.com/apache/storm/raw/3370b2d6495be770989c048a35cd3998e899bd60/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java", "blob_url": "https://github.com/apache/storm/blob/3370b2d6495be770989c048a35cd3998e899bd60/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java", "sha": "4c2a6ffa391ec9dc5b66102b171e7963e1185474", "changes": 8, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java?ref=3370b2d6495be770989c048a35cd3998e899bd60", "patch": "@@ -486,9 +486,11 @@ private boolean emitOrRetryTuple(ConsumerRecord<K, V> record) {\n                 /*if a null tuple is not configured to be emitted, it should be marked as emitted and acked immediately\n                 * to allow its offset to be commited to Kafka*/\n                 LOG.debug(\"Not emitting null tuple for record [{}] as defined in configuration.\", record);\n-                msgId.setNullTuple(true);\n-                offsetManagers.get(tp).addToEmitMsgs(msgId.offset());\n-                ack(msgId);\n+                if (isAtLeastOnceProcessing()) {\n+                    msgId.setNullTuple(true);\n+                    offsetManagers.get(tp).addToEmitMsgs(msgId.offset());\n+                    ack(msgId);\n+                }\n             }\n         }\n         return false;", "filename": "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java"}, {"additions": 37, "raw_url": "https://github.com/apache/storm/raw/3370b2d6495be770989c048a35cd3998e899bd60/external/storm-kafka-client/src/test/java/org/apache/storm/kafka/NullRecordTranslator.java", "blob_url": "https://github.com/apache/storm/blob/3370b2d6495be770989c048a35cd3998e899bd60/external/storm-kafka-client/src/test/java/org/apache/storm/kafka/NullRecordTranslator.java", "sha": "065244e1c54b18aee6d4df33e53c4d32dd10581b", "changes": 37, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-kafka-client/src/test/java/org/apache/storm/kafka/NullRecordTranslator.java?ref=3370b2d6495be770989c048a35cd3998e899bd60", "patch": "@@ -0,0 +1,37 @@\n+/*\n+ * Copyright 2018 The Apache Software Foundation.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *      http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.storm.kafka;\n+\n+import java.util.Collections;\n+import java.util.List;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.storm.kafka.spout.RecordTranslator;\n+import org.apache.storm.tuple.Fields;\n+\n+public class NullRecordTranslator<K, V> implements RecordTranslator<K, V> {\n+\n+    @Override\n+    public List<Object> apply(ConsumerRecord<K, V> record) {\n+        return null;\n+\n+    }\n+\n+    @Override\n+    public Fields getFieldsFor(String stream) {\n+        return new Fields(\"topic\", \"key\", \"value\");\n+    }\n+}", "filename": "external/storm-kafka-client/src/test/java/org/apache/storm/kafka/NullRecordTranslator.java"}, {"additions": 44, "raw_url": "https://github.com/apache/storm/raw/3370b2d6495be770989c048a35cd3998e899bd60/external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/KafkaSpoutMessagingGuaranteeTest.java", "blob_url": "https://github.com/apache/storm/blob/3370b2d6495be770989c048a35cd3998e899bd60/external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/KafkaSpoutMessagingGuaranteeTest.java", "sha": "1d2c1718aefb5239e62fc133a7269a55dde1129a", "changes": 58, "status": "modified", "deletions": 14, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/KafkaSpoutMessagingGuaranteeTest.java?ref=3370b2d6495be770989c048a35cd3998e899bd60", "patch": "@@ -21,6 +21,7 @@\n import static org.hamcrest.CoreMatchers.not;\n import static org.hamcrest.CoreMatchers.nullValue;\n import static org.junit.Assert.assertThat;\n+import static org.mockito.ArgumentMatchers.any;\n import static org.mockito.ArgumentMatchers.anyList;\n import static org.mockito.ArgumentMatchers.anyLong;\n import static org.mockito.ArgumentMatchers.argThat;\n@@ -42,6 +43,7 @@\n import org.apache.kafka.clients.consumer.KafkaConsumer;\n import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n import org.apache.kafka.common.TopicPartition;\n+import org.apache.storm.kafka.NullRecordTranslator;\n import org.apache.storm.kafka.spout.config.builder.SingleTopicKafkaSpoutConfiguration;\n import org.apache.storm.kafka.spout.internal.CommitMetadataManager;\n import org.apache.storm.kafka.spout.subscription.ManualPartitioner;\n@@ -63,7 +65,7 @@\n \n     @Captor\n     private ArgumentCaptor<Map<TopicPartition, OffsetAndMetadata>> commitCapture;\n-    \n+\n     private final TopologyContext contextMock = mock(TopologyContext.class);\n     private final SpoutOutputCollector collectorMock = mock(SpoutOutputCollector.class);\n     private final Map<String, Object> conf = new HashMap<>();\n@@ -95,7 +97,7 @@ public void testAtMostOnceModeCommitsBeforeEmit() throws Exception {\n         inOrder.verify(consumerMock).poll(anyLong());\n         inOrder.verify(consumerMock).commitSync(commitCapture.capture());\n         inOrder.verify(collectorMock).emit(eq(SingleTopicKafkaSpoutConfiguration.STREAM), anyList());\n-        \n+\n         CommitMetadataManager metadataManager = new CommitMetadataManager(contextMock, KafkaSpoutConfig.ProcessingGuarantee.AT_MOST_ONCE);\n         Map<TopicPartition, OffsetAndMetadata> committedOffsets = commitCapture.getValue();\n         assertThat(committedOffsets.get(partition).offset(), is(0L));\n@@ -190,7 +192,7 @@ public void testAtMostOnceModeDoesNotCommitAckedTuples() throws Exception {\n             .setTupleTrackingEnforced(true)\n             .build();\n         try (SimulatedTime time = new SimulatedTime()) {\n-            KafkaSpout<String, String> spout = SpoutWithMockedConsumerSetupHelper.setupSpout(spoutConfig, conf, contextMock, collectorMock, consumerMock,partition);\n+            KafkaSpout<String, String> spout = SpoutWithMockedConsumerSetupHelper.setupSpout(spoutConfig, conf, contextMock, collectorMock, consumerMock, partition);\n \n             when(consumerMock.poll(anyLong())).thenReturn(new ConsumerRecords<>(Collections.singletonMap(partition,\n                 SpoutWithMockedConsumerSetupHelper.createRecords(partition, 0, 1))));\n@@ -203,13 +205,13 @@ public void testAtMostOnceModeDoesNotCommitAckedTuples() throws Exception {\n             assertThat(\"Should have captured a message id\", msgIdCaptor.getValue(), not(nullValue()));\n \n             spout.ack(msgIdCaptor.getValue());\n-            \n+\n             Time.advanceTime(KafkaSpout.TIMER_DELAY_MS + spoutConfig.getOffsetsCommitPeriodMs());\n-            \n+\n             when(consumerMock.poll(anyLong())).thenReturn(new ConsumerRecords<>(Collections.emptyMap()));\n-            \n+\n             spout.nextTuple();\n-            \n+\n             verify(consumerMock, never()).commitSync(argThat(arg -> {\n                 return !arg.containsKey(partition);\n             }));\n@@ -223,32 +225,60 @@ public void testNoGuaranteeModeCommitsPolledTuples() throws Exception {\n             .setProcessingGuarantee(KafkaSpoutConfig.ProcessingGuarantee.NO_GUARANTEE)\n             .setTupleTrackingEnforced(true)\n             .build();\n-        \n+\n         try (SimulatedTime time = new SimulatedTime()) {\n-            KafkaSpout<String, String> spout = SpoutWithMockedConsumerSetupHelper.setupSpout(spoutConfig, conf, contextMock, collectorMock, consumerMock,partition);\n+            KafkaSpout<String, String> spout = SpoutWithMockedConsumerSetupHelper.setupSpout(spoutConfig, conf, contextMock, collectorMock, consumerMock, partition);\n \n             when(consumerMock.poll(anyLong())).thenReturn(new ConsumerRecords<>(Collections.singletonMap(partition,\n                 SpoutWithMockedConsumerSetupHelper.createRecords(partition, 0, 1))));\n \n             spout.nextTuple();\n-            \n+\n             when(consumerMock.position(partition)).thenReturn(1L);\n \n             ArgumentCaptor<KafkaSpoutMessageId> msgIdCaptor = ArgumentCaptor.forClass(KafkaSpoutMessageId.class);\n             verify(collectorMock).emit(eq(SingleTopicKafkaSpoutConfiguration.STREAM), anyList(), msgIdCaptor.capture());\n             assertThat(\"Should have captured a message id\", msgIdCaptor.getValue(), not(nullValue()));\n-            \n+\n             Time.advanceTime(KafkaSpout.TIMER_DELAY_MS + spoutConfig.getOffsetsCommitPeriodMs());\n-            \n+\n             spout.nextTuple();\n-            \n+\n             verify(consumerMock).commitAsync(commitCapture.capture(), isNull());\n-            \n+\n             CommitMetadataManager metadataManager = new CommitMetadataManager(contextMock, KafkaSpoutConfig.ProcessingGuarantee.NO_GUARANTEE);\n             Map<TopicPartition, OffsetAndMetadata> committedOffsets = commitCapture.getValue();\n             assertThat(committedOffsets.get(partition).offset(), is(1L));\n             assertThat(committedOffsets.get(partition).metadata(), is(metadataManager.getCommitMetadata()));\n         }\n     }\n \n+    private void doFilterNullTupleTest(KafkaSpoutConfig.ProcessingGuarantee processingGuaranteee) {\n+        //STORM-3059\n+        KafkaSpoutConfig<String, String> spoutConfig = createKafkaSpoutConfigBuilder(mock(TopicFilter.class), mock(ManualPartitioner.class), -1)\n+            .setProcessingGuarantee(processingGuaranteee)\n+            .setTupleTrackingEnforced(true)\n+            .setRecordTranslator(new NullRecordTranslator<>())\n+            .build();\n+        \n+        KafkaSpout<String, String> spout = SpoutWithMockedConsumerSetupHelper.setupSpout(spoutConfig, conf, contextMock, collectorMock, consumerMock, partition);\n+\n+        when(consumerMock.poll(anyLong())).thenReturn(new ConsumerRecords<>(Collections.singletonMap(partition,\n+            SpoutWithMockedConsumerSetupHelper.createRecords(partition, 0, 1))));\n+\n+        spout.nextTuple();\n+        \n+        verify(collectorMock, never()).emit(any(), any(), any());\n+    }\n+    \n+    @Test\n+    public void testAtMostOnceModeCanFilterNullTuples() {\n+        doFilterNullTupleTest(KafkaSpoutConfig.ProcessingGuarantee.AT_MOST_ONCE);\n+    }\n+    \n+    @Test\n+    public void testNoGuaranteeModeCanFilterNullTuples() {\n+        doFilterNullTupleTest(KafkaSpoutConfig.ProcessingGuarantee.NO_GUARANTEE);\n+    }\n+\n }", "filename": "external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/KafkaSpoutMessagingGuaranteeTest.java"}, {"additions": 3, "raw_url": "https://github.com/apache/storm/raw/3370b2d6495be770989c048a35cd3998e899bd60/external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/KafkaSpoutNullTupleTest.java", "blob_url": "https://github.com/apache/storm/blob/3370b2d6495be770989c048a35cd3998e899bd60/external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/KafkaSpoutNullTupleTest.java", "sha": "ce93ea2ef58c1bb437de55596376e952d9f42007", "changes": 28, "status": "modified", "deletions": 25, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/KafkaSpoutNullTupleTest.java?ref=3370b2d6495be770989c048a35cd3998e899bd60", "patch": "@@ -18,19 +18,18 @@\n package org.apache.storm.kafka.spout;\n \n \n-import org.apache.kafka.clients.consumer.ConsumerRecord;\n import org.apache.storm.kafka.spout.config.builder.SingleTopicKafkaSpoutConfiguration;\n-import org.apache.storm.tuple.Fields;\n import org.apache.storm.utils.Time;\n import org.junit.Test;\n \n-import java.util.List;\n import java.util.regex.Pattern;\n \n import static org.mockito.ArgumentMatchers.*;\n import static org.mockito.Mockito.never;\n import static org.mockito.Mockito.verify;\n \n+import org.apache.storm.kafka.NullRecordTranslator;\n+\n public class KafkaSpoutNullTupleTest extends KafkaSpoutAbstractTest {\n \n     public KafkaSpoutNullTupleTest() {\n@@ -40,11 +39,10 @@ public KafkaSpoutNullTupleTest() {\n \n     @Override\n     KafkaSpoutConfig<String, String> createSpoutConfig() {\n-\n         return KafkaSpoutConfig.builder(\"127.0.0.1:\" + kafkaUnitRule.getKafkaUnit().getKafkaPort(),\n                 Pattern.compile(SingleTopicKafkaSpoutConfiguration.TOPIC))\n                 .setOffsetCommitPeriodMs(commitOffsetPeriodMs)\n-                .setRecordTranslator(new NullRecordExtractor())\n+                .setRecordTranslator(new NullRecordTranslator<>())\n                 .build();\n     }\n \n@@ -70,24 +68,4 @@ public void testShouldCommitAllMessagesIfNotSetToEmitNullTuples() throws Excepti\n         verifyAllMessagesCommitted(messageCount);\n     }\n \n-    private class NullRecordExtractor implements RecordTranslator {\n-\n-        @Override\n-        public List<Object> apply(ConsumerRecord record) {\n-            return null;\n-\n-        }\n-\n-        @Override\n-        public Fields getFieldsFor(String stream) {\n-            return new Fields(\"topic\", \"key\", \"value\");\n-        }\n-\n-        @Override\n-        public Object apply(Object record) {\n-            return null;\n-        }\n-    }\n-\n-\n }", "filename": "external/storm-kafka-client/src/test/java/org/apache/storm/kafka/spout/KafkaSpoutNullTupleTest.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/e7e1177992ba3a151d3162b8c6de041bf6708005", "parent": "https://github.com/apache/storm/commit/9bf202277bf02cee8fbe8c627aa1834d00f556ef", "message": "[STORM-2729] Fix NPE in WorkerState runWorkerStartHooks and runWorkerShutdownHooks methods", "bug_id": "storm_21", "file": [{"additions": 12, "raw_url": "https://github.com/apache/storm/raw/e7e1177992ba3a151d3162b8c6de041bf6708005/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java", "blob_url": "https://github.com/apache/storm/blob/e7e1177992ba3a151d3162b8c6de041bf6708005/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java", "sha": "d2444916e795b9d9fbf9ba8ed58a5995a34e9673", "changes": 22, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java?ref=e7e1177992ba3a151d3162b8c6de041bf6708005", "patch": "@@ -583,20 +583,22 @@ public WorkerTopologyContext getWorkerTopologyContext() {\n \n     public void runWorkerStartHooks() {\n         WorkerTopologyContext workerContext = getWorkerTopologyContext();\n-        for (ByteBuffer hook : topology.get_worker_hooks()) {\n-            byte[] hookBytes = Utils.toByteArray(hook);\n-            BaseWorkerHook hookObject = Utils.javaDeserialize(hookBytes, BaseWorkerHook.class);\n-            hookObject.start(topologyConf, workerContext);\n-\n+        if (topology.is_set_worker_hooks()) {\n+            for (ByteBuffer hook : topology.get_worker_hooks()) {\n+                byte[] hookBytes = Utils.toByteArray(hook);\n+                BaseWorkerHook hookObject = Utils.javaDeserialize(hookBytes, BaseWorkerHook.class);\n+                hookObject.start(topologyConf, workerContext);\n+            }\n         }\n     }\n \n     public void runWorkerShutdownHooks() {\n-        for (ByteBuffer hook : topology.get_worker_hooks()) {\n-            byte[] hookBytes = Utils.toByteArray(hook);\n-            BaseWorkerHook hookObject = Utils.javaDeserialize(hookBytes, BaseWorkerHook.class);\n-            hookObject.shutdown();\n-\n+        if (topology.is_set_worker_hooks()) {\n+            for (ByteBuffer hook : topology.get_worker_hooks()) {\n+                byte[] hookBytes = Utils.toByteArray(hook);\n+                BaseWorkerHook hookObject = Utils.javaDeserialize(hookBytes, BaseWorkerHook.class);\n+                hookObject.shutdown();\n+            }\n         }\n     }\n ", "filename": "storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/e69fbc51ada2fd542d717bbd7623c39f4f6a1cb8", "parent": "https://github.com/apache/storm/commit/c8947c2fede62036c20472c9e0335ef90a06b536", "message": "STORM-2779 NPE on shutting down WindowedBoltExecutor\n\n* waterMarkEventGenerator could be null when timestamp field is not specified", "bug_id": "storm_22", "file": [{"additions": 3, "raw_url": "https://github.com/apache/storm/raw/e69fbc51ada2fd542d717bbd7623c39f4f6a1cb8/storm-client/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java", "blob_url": "https://github.com/apache/storm/blob/e69fbc51ada2fd542d717bbd7623c39f4f6a1cb8/storm-client/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java", "sha": "5089f64f807b5f6c6600d49709132b3abeed4f86", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java?ref=e69fbc51ada2fd542d717bbd7623c39f4f6a1cb8", "patch": "@@ -327,7 +327,9 @@ public void execute(Tuple input) {\n \n     @Override\n     public void cleanup() {\n-        waterMarkEventGenerator.shutdown();\n+        if (waterMarkEventGenerator != null) {\n+            waterMarkEventGenerator.shutdown();\n+        }\n         windowManager.shutdown();\n         bolt.cleanup();\n     }", "filename": "storm-client/src/jvm/org/apache/storm/topology/WindowedBoltExecutor.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/d5762c8a5baac269fcb8134f9947b1dc91c3e5ac", "parent": "https://github.com/apache/storm/commit/1b73dc74ad7b1129a7e242be152ad25399ba0252", "message": "fixed the NPE", "bug_id": "storm_23", "file": [{"additions": 3, "raw_url": "https://github.com/apache/storm/raw/d5762c8a5baac269fcb8134f9947b1dc91c3e5ac/storm-core/src/jvm/org/apache/storm/command/AdminCommands.java", "blob_url": "https://github.com/apache/storm/blob/d5762c8a5baac269fcb8134f9947b1dc91c3e5ac/storm-core/src/jvm/org/apache/storm/command/AdminCommands.java", "sha": "f09ad5fa241393692f9e88d4328217326cefb1c7", "changes": 7, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/org/apache/storm/command/AdminCommands.java?ref=d5762c8a5baac269fcb8134f9947b1dc91c3e5ac", "patch": "@@ -46,7 +46,6 @@\n     private static final Logger LOG = LoggerFactory.getLogger(AdminCommands.class);\n     private static BlobStore nimbusBlobStore;\n     private static IStormClusterState stormClusterState;\n-    private static CuratorFramework zk;\n     private static Map conf;\n \n     public static void main(String [] args) throws Exception {\n@@ -69,16 +68,16 @@ public static void main(String [] args) throws Exception {\n     }\n \n     private static void initialize() {\n-        Map conf = ConfigUtils.readStormConfig();\n-        BlobStore nimbusBlobStore = Utils.getNimbusBlobStore (conf, NimbusInfo.fromConf(conf));\n+        conf = ConfigUtils.readStormConfig();\n+        nimbusBlobStore = Utils.getNimbusBlobStore (conf, NimbusInfo.fromConf(conf));\n         List<String> servers = (List<String>) conf.get(Config.STORM_ZOOKEEPER_SERVERS);\n         Object port = conf.get(Config.STORM_ZOOKEEPER_PORT);\n         List<ACL> acls = null;\n         if (Utils.isZkAuthenticationConfiguredStormServer(conf)) {\n             acls = adminZkAcls();\n         }\n         try {\n-            IStormClusterState stormClusterState = ClusterUtils.mkStormClusterState(conf, acls, new ClusterStateContext(DaemonType.NIMBUS));\n+            stormClusterState = ClusterUtils.mkStormClusterState(conf, acls, new ClusterStateContext(DaemonType.NIMBUS));\n         } catch (Exception e) {\n             LOG.error(\"admin can't create stormClusterState\");\n             new RuntimeException(e);", "filename": "storm-core/src/jvm/org/apache/storm/command/AdminCommands.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/d568408e4d284199cf53baed7dd1471ab9628ca4", "parent": "https://github.com/apache/storm/commit/2657c25f232f9cf6f7d6580cd27c2d76a7b6c4c9", "message": "STORM-2811: Fix NPE in Nimbus when killing the same topology multiple times, fix integration test killing the same topology multiple times", "bug_id": "storm_24", "file": [{"additions": 0, "raw_url": "https://github.com/apache/storm/raw/d568408e4d284199cf53baed7dd1471ab9628ca4/integration-test/src/main/java/org/apache/storm/st/utils/StringDecorator.java", "blob_url": "https://github.com/apache/storm/blob/d568408e4d284199cf53baed7dd1471ab9628ca4/integration-test/src/main/java/org/apache/storm/st/utils/StringDecorator.java", "sha": "34c2b65592aa537bafaea2d8446c94e69fd5288e", "changes": 1, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/integration-test/src/main/java/org/apache/storm/st/utils/StringDecorator.java?ref=d568408e4d284199cf53baed7dd1471ab9628ca4", "patch": "@@ -17,7 +17,6 @@\n \n package org.apache.storm.st.utils;\n \n-import java.nio.charset.StandardCharsets;\n import org.apache.commons.lang.StringUtils;\n \n public class StringDecorator {", "filename": "integration-test/src/main/java/org/apache/storm/st/utils/StringDecorator.java"}, {"additions": 1, "raw_url": "https://github.com/apache/storm/raw/d568408e4d284199cf53baed7dd1471ab9628ca4/integration-test/src/test/java/org/apache/storm/st/DemoTest.java", "blob_url": "https://github.com/apache/storm/blob/d568408e4d284199cf53baed7dd1471ab9628ca4/integration-test/src/test/java/org/apache/storm/st/DemoTest.java", "sha": "133014bfe27a95642efdedc5f018172481c87da5", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/integration-test/src/test/java/org/apache/storm/st/DemoTest.java?ref=d568408e4d284199cf53baed7dd1471ab9628ca4", "patch": "@@ -80,6 +80,7 @@ public void testExclamationTopology() throws Exception {\n     public void cleanup() throws Exception {\n         if (topo != null) {\n             topo.killOrThrow();\n+            topo = null;\n         }\n     }\n }", "filename": "integration-test/src/test/java/org/apache/storm/st/DemoTest.java"}, {"additions": 1, "raw_url": "https://github.com/apache/storm/raw/d568408e4d284199cf53baed7dd1471ab9628ca4/integration-test/src/test/java/org/apache/storm/st/tests/window/SlidingWindowTest.java", "blob_url": "https://github.com/apache/storm/blob/d568408e4d284199cf53baed7dd1471ab9628ca4/integration-test/src/test/java/org/apache/storm/st/tests/window/SlidingWindowTest.java", "sha": "83b981e3614462137b690c19ea4ccd225b25bb82", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/integration-test/src/test/java/org/apache/storm/st/tests/window/SlidingWindowTest.java?ref=d568408e4d284199cf53baed7dd1471ab9628ca4", "patch": "@@ -188,6 +188,7 @@ static void runAndVerifyTime(int windowSec, int slideSec, TestableTopology testa\n     public void cleanup() throws Exception {\n         if (topo != null) {\n             topo.killOrThrow();\n+            topo = null;\n         }\n     }\n }", "filename": "integration-test/src/test/java/org/apache/storm/st/tests/window/SlidingWindowTest.java"}, {"additions": 2, "raw_url": "https://github.com/apache/storm/raw/d568408e4d284199cf53baed7dd1471ab9628ca4/integration-test/src/test/java/org/apache/storm/st/tests/window/TumblingWindowTest.java", "blob_url": "https://github.com/apache/storm/blob/d568408e4d284199cf53baed7dd1471ab9628ca4/integration-test/src/test/java/org/apache/storm/st/tests/window/TumblingWindowTest.java", "sha": "28b3969ed6813a9b229b9d2a02e9e94bcf092018", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/integration-test/src/test/java/org/apache/storm/st/tests/window/TumblingWindowTest.java?ref=d568408e4d284199cf53baed7dd1471ab9628ca4", "patch": "@@ -30,7 +30,7 @@\n \n public final class TumblingWindowTest extends AbstractTest {\n     private static Logger log = LoggerFactory.getLogger(TumblingWindowTest.class);\n-    TopoWrap topo;\n+    private TopoWrap topo;\n \n     @DataProvider\n     public static Object[][] generateWindows() {\n@@ -94,6 +94,7 @@ public void testTumbleTime(int tumbleSec) throws Exception {\n     public void cleanup() throws Exception {\n         if (topo != null) {\n             topo.killOrThrow();\n+            topo = null;\n         }\n     }\n }", "filename": "integration-test/src/test/java/org/apache/storm/st/tests/window/TumblingWindowTest.java"}, {"additions": 1, "raw_url": "https://github.com/apache/storm/raw/d568408e4d284199cf53baed7dd1471ab9628ca4/integration-test/src/test/java/org/apache/storm/st/wrapper/StormCluster.java", "blob_url": "https://github.com/apache/storm/blob/d568408e4d284199cf53baed7dd1471ab9628ca4/integration-test/src/test/java/org/apache/storm/st/wrapper/StormCluster.java", "sha": "4521b36bd34b092d9b6f2c7a82aad42dc5f7cbe5", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/integration-test/src/test/java/org/apache/storm/st/wrapper/StormCluster.java?ref=d568408e4d284199cf53baed7dd1471ab9628ca4", "patch": "@@ -93,7 +93,7 @@ public void killOrThrow(String topologyName) throws Exception {\n                 client.killTopologyWithOpts(topologyName, killOptions);\n                 log.info(\"Topology killed: \" + topologyName);\n                 return;\n-            } catch (Throwable e) {\n+            } catch (TException e) {\n                 log.warn(\"Couldn't kill topology: \" + topologyName + \", going to retry soon. Exception: \" + ExceptionUtils.getFullStackTrace(e));\n                 Thread.sleep(TimeUnit.SECONDS.toMillis(2));\n             }", "filename": "integration-test/src/test/java/org/apache/storm/st/wrapper/StormCluster.java"}, {"additions": 2, "raw_url": "https://github.com/apache/storm/raw/d568408e4d284199cf53baed7dd1471ab9628ca4/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java", "blob_url": "https://github.com/apache/storm/blob/d568408e4d284199cf53baed7dd1471ab9628ca4/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java", "sha": "051ccd537fba7df23eb1c0a8ac94f2669fb1b90e", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java?ref=d568408e4d284199cf53baed7dd1471ab9628ca4", "patch": "@@ -168,8 +168,8 @@\n     default Optional<String> getTopoId(final String topologyName) {\n         String ret = null;\n         for (String topoId: activeStorms()) {\n-            String name = stormBase(topoId, null).get_name();\n-            if (topologyName.equals(name)) {\n+            StormBase base = stormBase(topoId, null);\n+            if(base != null && topologyName.equals(base.get_name())) {\n                 ret = topoId;\n                 break;\n             }", "filename": "storm-client/src/jvm/org/apache/storm/cluster/IStormClusterState.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/7cf7647e546201d3f175860526888cd92b3927b3", "parent": "https://github.com/apache/storm/commit/da5c3acd7e5123876566db6ddf2d8de2b4a3506e", "message": "fixed SpoutExecutor NPE", "bug_id": "storm_25", "file": [{"additions": 1, "raw_url": "https://github.com/apache/storm/raw/7cf7647e546201d3f175860526888cd92b3927b3/storm-core/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java", "blob_url": "https://github.com/apache/storm/blob/7cf7647e546201d3f175860526888cd92b3927b3/storm-core/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java", "sha": "7a5066b590b11cb668679ded4f224e09a8e9800a", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java?ref=7cf7647e546201d3f175860526888cd92b3927b3", "patch": "@@ -202,7 +202,7 @@ public void tupleActionFn(int taskId, TupleImpl tuple) throws Exception {\n             Long id = (Long) tuple.getValue(0);\n             Long timeDeltaMs = (Long) tuple.getValue(1);\n             TupleInfo tupleInfo = (TupleInfo) pending.remove(id);\n-            if (tupleInfo.getMessageId() != null) {\n+            if (tupleInfo != null && tupleInfo.getMessageId() != null) {\n                 if (taskId != tupleInfo.getTaskId()) {\n                     throw new RuntimeException(\"Fatal error, mismatched task ids: \" + taskId + \" \" + tupleInfo.getTaskId());\n                 }", "filename": "storm-core/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/ed6dca0246e053c443e6ecdbea1e08073ebbaff2", "parent": "https://github.com/apache/storm/commit/bf0f3a2fb0806d44ba79867ad911a2df19d8c360", "message": "Merge branch 'STORM-2729' of https://github.com/Ethanlm/storm into STORM-2729\n\nSTORM-2729: Fix NPE in WorkerState runWorkerStartHooks and runWorkerShutdownHooks methods\n\nThis closes #2311", "bug_id": "storm_26", "file": [{"additions": 12, "raw_url": "https://github.com/apache/storm/raw/ed6dca0246e053c443e6ecdbea1e08073ebbaff2/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java", "blob_url": "https://github.com/apache/storm/blob/ed6dca0246e053c443e6ecdbea1e08073ebbaff2/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java", "sha": "d2444916e795b9d9fbf9ba8ed58a5995a34e9673", "changes": 22, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java?ref=ed6dca0246e053c443e6ecdbea1e08073ebbaff2", "patch": "@@ -583,20 +583,22 @@ public WorkerTopologyContext getWorkerTopologyContext() {\n \n     public void runWorkerStartHooks() {\n         WorkerTopologyContext workerContext = getWorkerTopologyContext();\n-        for (ByteBuffer hook : topology.get_worker_hooks()) {\n-            byte[] hookBytes = Utils.toByteArray(hook);\n-            BaseWorkerHook hookObject = Utils.javaDeserialize(hookBytes, BaseWorkerHook.class);\n-            hookObject.start(topologyConf, workerContext);\n-\n+        if (topology.is_set_worker_hooks()) {\n+            for (ByteBuffer hook : topology.get_worker_hooks()) {\n+                byte[] hookBytes = Utils.toByteArray(hook);\n+                BaseWorkerHook hookObject = Utils.javaDeserialize(hookBytes, BaseWorkerHook.class);\n+                hookObject.start(topologyConf, workerContext);\n+            }\n         }\n     }\n \n     public void runWorkerShutdownHooks() {\n-        for (ByteBuffer hook : topology.get_worker_hooks()) {\n-            byte[] hookBytes = Utils.toByteArray(hook);\n-            BaseWorkerHook hookObject = Utils.javaDeserialize(hookBytes, BaseWorkerHook.class);\n-            hookObject.shutdown();\n-\n+        if (topology.is_set_worker_hooks()) {\n+            for (ByteBuffer hook : topology.get_worker_hooks()) {\n+                byte[] hookBytes = Utils.toByteArray(hook);\n+                BaseWorkerHook hookObject = Utils.javaDeserialize(hookBytes, BaseWorkerHook.class);\n+                hookObject.shutdown();\n+            }\n         }\n     }\n ", "filename": "storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/9833f5449e598b7f257a229e64ed2f479c4d07ac", "parent": "https://github.com/apache/storm/commit/e1dd247ce97ad560c72554ed612e1ec8e69e9777", "message": "STORM-2635 Deep log search doesn\u2019t work when there\u2019s no topology in topology history\n\n* just fix the NPE issue", "bug_id": "storm_27", "file": [{"additions": 4, "raw_url": "https://github.com/apache/storm/raw/9833f5449e598b7f257a229e64ed2f479c4d07ac/storm-core/test/clj/org/apache/storm/nimbus_test.clj", "blob_url": "https://github.com/apache/storm/blob/9833f5449e598b7f257a229e64ed2f479c4d07ac/storm-core/test/clj/org/apache/storm/nimbus_test.clj", "sha": "c4f3fadc0228ceab8011fa26f1278290229f590f", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/test/clj/org/apache/storm/nimbus_test.clj?ref=9833f5449e598b7f257a229e64ed2f479c4d07ac", "patch": "@@ -537,6 +537,10 @@\n                                 (TestPlannerSpout. true) (Integer. 4))}\n                          {}))\n         (bind state (.getClusterState cluster))\n+        ; get topology history when there's no topology history\n+        (let [hist-topo-ids (vec (sort (.get_topo_ids (.getTopologyHistory (.getNimbus cluster) (System/getProperty \"user.name\")))))]\n+             (log-message \"Checking user \" (System/getProperty \"user.name\") \" \" hist-topo-ids)\n+             (is (= 0 (count hist-topo-ids))))\n         (.submitTopology cluster \"test\" {TOPOLOGY-MESSAGE-TIMEOUT-SECS 20, LOGS-USERS [\"alice\", (System/getProperty \"user.name\")]} topology)\n         (bind storm-id (StormCommon/getStormId state \"test\"))\n         (.advanceClusterTime cluster 5)", "filename": "storm-core/test/clj/org/apache/storm/nimbus_test.clj"}, {"additions": 6, "raw_url": "https://github.com/apache/storm/raw/9833f5449e598b7f257a229e64ed2f479c4d07ac/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java", "blob_url": "https://github.com/apache/storm/blob/9833f5449e598b7f257a229e64ed2f479c4d07ac/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java", "sha": "5fcf1ec3c9b3889be8f195ead6b5b9f305e7a7ad", "changes": 8, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java?ref=9833f5449e598b7f257a229e64ed2f479c4d07ac", "patch": "@@ -2116,9 +2116,13 @@ private boolean isUserPartOf(String user, Collection<String> groupsToCheck) thro\n \n     private List<String> readTopologyHistory(String user, Collection<String> adminUsers) throws IOException {\n         LocalState state = topologyHistoryState;\n+        List<LSTopoHistory> topoHistoryList = state.getTopoHistoryList();\n+        if (topoHistoryList == null || topoHistoryList.isEmpty()) {\n+            return Collections.emptyList();\n+        }\n+\n         List<String> ret = new ArrayList<>();\n-        for (LSTopoHistory history: state.getTopoHistoryList()) {\n-            \n+        for (LSTopoHistory history: topoHistoryList) {\n             if (user == null || //Security off\n                     adminUsers.contains(user) || //is admin\n                     isUserPartOf(user, history.get_groups()) || //is in allowed group", "filename": "storm-server/src/main/java/org/apache/storm/daemon/nimbus/Nimbus.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/faaacaee046bfa4f458c19cade678515a021d836", "parent": "https://github.com/apache/storm/commit/abe9b676c0f15fa47809ae4a094001e345521de6", "message": "fix possible NPE & ClassCastException", "bug_id": "storm_28", "file": [{"additions": 17, "raw_url": "https://github.com/apache/storm/raw/faaacaee046bfa4f458c19cade678515a021d836/storm-core/src/jvm/org/apache/storm/stats/StatsUtil.java", "blob_url": "https://github.com/apache/storm/blob/faaacaee046bfa4f458c19cade678515a021d836/storm-core/src/jvm/org/apache/storm/stats/StatsUtil.java", "sha": "0ed2af96de4ff8a306c213844863dba9efa4a831", "changes": 23, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/org/apache/storm/stats/StatsUtil.java?ref=faaacaee046bfa4f458c19cade678515a021d836", "patch": "@@ -578,7 +578,7 @@ public static Map postAggregateTopoStats(\n         Map ret = new HashMap();\n         putRawKV(ret, NUM_TASKS, task2comp.size());\n         putRawKV(ret, NUM_WORKERS, ((Set) getByKeyword(accData, WORKERS_SET)).size());\n-        putRawKV(ret, NUM_EXECUTORS, exec2nodePort.size());\n+        putRawKV(ret, NUM_EXECUTORS, exec2nodePort != null ? exec2nodePort.size() : 0);\n \n         Map bolt2stats = getMapByKeyword(accData, BOLT_TO_STATS);\n         Map aggBolt2stats = new HashMap();\n@@ -1339,11 +1339,18 @@ private static Map mergeMaps(Map m1, Map m2) {\n      */\n     private static Map filterSysStreams(Map stats, boolean includeSys) {\n         if (!includeSys) {\n-            for (Object win : stats.keySet()) {\n-                Map stream2stat = (Map) stats.get(win);\n-                for (Iterator itr = stream2stat.keySet().iterator(); itr.hasNext(); ) {\n-                    Object key = itr.next();\n-                    if (key instanceof String && Utils.isSystemId((String) key)) {\n+            for (Iterator itr = stats.keySet().iterator(); itr.hasNext(); ) {\n+                Object winOrStream = itr.next();\n+                if (isWindow(winOrStream)) {\n+                    Map stream2stat = (Map) stats.get(winOrStream);\n+                    for (Iterator subItr = stream2stat.keySet().iterator(); subItr.hasNext(); ) {\n+                        Object key = subItr.next();\n+                        if (key instanceof String && Utils.isSystemId((String) key)) {\n+                            subItr.remove();\n+                        }\n+                    }\n+                } else {\n+                    if (winOrStream instanceof String && Utils.isSystemId((String) winOrStream)) {\n                         itr.remove();\n                     }\n                 }\n@@ -1352,6 +1359,10 @@ private static Map filterSysStreams(Map stats, boolean includeSys) {\n         return stats;\n     }\n \n+    private static boolean isWindow(Object key) {\n+        return key.equals(\"600\") || key.equals(\"10800\") || key.equals(\"86400\") || key.equals(\":all-time\");\n+    }\n+\n     /**\n      * equals to clojure's: (merge-with (partial merge-with sum-or-0) acc-out spout-out)\n      */", "filename": "storm-core/src/jvm/org/apache/storm/stats/StatsUtil.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/3fc80c4b0bfc83d2534fab160c72894af044dbc3", "parent": "https://github.com/apache/storm/commit/e5564c0f888e40af2726a645d24cfad0aaeed26a", "message": "fixed a potential NPE", "bug_id": "storm_29", "file": [{"additions": 3, "raw_url": "https://github.com/apache/storm/raw/3fc80c4b0bfc83d2534fab160c72894af044dbc3/storm-core/src/jvm/org/apache/storm/stats/StatsUtil.java", "blob_url": "https://github.com/apache/storm/blob/3fc80c4b0bfc83d2534fab160c72894af044dbc3/storm-core/src/jvm/org/apache/storm/stats/StatsUtil.java", "sha": "75ec2925c68d51a197b83217358bd25591e8d768", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/org/apache/storm/stats/StatsUtil.java?ref=3fc80c4b0bfc83d2534fab160c72894af044dbc3", "patch": "@@ -1156,6 +1156,9 @@ public static List extractDataFromHb(Map executor2hostPort, Map task2component,\n     public static List extractDataFromHb(Map executor2hostPort, Map task2component, Map beats,\n                                          boolean includeSys, StormTopology topology, String compId) {\n         List ret = new ArrayList();\n+        if (executor2hostPort == null) {\n+            return ret;\n+        }\n         for (Object o : executor2hostPort.entrySet()) {\n             Map.Entry entry = (Map.Entry) o;\n             List key = (List) entry.getKey();", "filename": "storm-core/src/jvm/org/apache/storm/stats/StatsUtil.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/b0944100e339f6d28c6c7bb936bef8ebb1948d89", "parent": "https://github.com/apache/storm/commit/cd5c9e8f904205a6ca6eee9222ca954ca8b37ec3", "message": "STORM-2101: fixes npe in compute-executors in nimbus", "bug_id": "storm_30", "file": [{"additions": 11, "raw_url": "https://github.com/apache/storm/raw/b0944100e339f6d28c6c7bb936bef8ebb1948d89/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj", "blob_url": "https://github.com/apache/storm/blob/b0944100e339f6d28c6c7bb936bef8ebb1948d89/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj", "sha": "393899030cc69abbab632fff72aac20684d7650c", "changes": 21, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj?ref=b0944100e339f6d28c6c7bb936bef8ebb1948d89", "patch": "@@ -637,16 +637,17 @@\n         storm-conf (read-storm-conf-as-nimbus storm-id blob-store)\n         topology (read-storm-topology-as-nimbus storm-id blob-store)\n         task->component (get-clojurified-task-info topology storm-conf)]\n-    (->> (StormCommon/stormTaskInfo topology storm-conf)\n-         (Utils/reverseMap)\n-         clojurify-structure\n-         (map-val sort)\n-         ((fn [ & maps ] (Utils/joinMaps (into-array Map (into [component->executors] maps)))))\n-         (clojurify-structure)\n-         (map-val (partial apply (fn part-fixed [a b] (Utils/partitionFixed a b))))\n-         (mapcat second)\n-         (map to-executor-id)\n-         )))\n+    (if (nil? component->executors)\n+      []\n+      (->> (StormCommon/stormTaskInfo topology storm-conf)\n+           (Utils/reverseMap)\n+           clojurify-structure\n+           (map-val sort)\n+           ((fn [ & maps ] (Utils/joinMaps (into-array Map (into [component->executors] maps)))))\n+           (clojurify-structure)\n+           (map-val (partial apply (fn part-fixed [a b] (Utils/partitionFixed a b))))\n+           (mapcat second)\n+           (map to-executor-id)))))\n \n (defn- compute-executor->component [nimbus storm-id]\n   (let [conf (:conf nimbus)", "filename": "storm-core/src/clj/org/apache/storm/daemon/nimbus.clj"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/164fd9c6e1f6dd0925858ba3fce40859e547938d", "parent": "https://github.com/apache/storm/commit/f3e4dd3ffa818d575f531edde6045cdf9c21decb", "message": "Merge branch 'STORM-2126_fix_npe_due_to_race_in_compute_new_sched_assignments' of https://github.com/abellina/storm into STORM-2126\n\nSTORM-2126: fix NPE due to race condition in compute-new-sched-assign\u2026", "bug_id": "storm_31", "file": [{"additions": 15, "raw_url": "https://github.com/apache/storm/raw/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/AbstractChannelHandler.java", "blob_url": "https://github.com/apache/storm/blob/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/AbstractChannelHandler.java", "sha": "64be39d37e99f1272d5f2ed442d7ee456b86060e", "changes": 32, "status": "modified", "deletions": 17, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/AbstractChannelHandler.java?ref=164fd9c6e1f6dd0925858ba3fce40859e547938d", "patch": "@@ -1,21 +1,19 @@\n-/*\n- * *\n- *  * Licensed to the Apache Software Foundation (ASF) under one\n- *  * or more contributor license agreements.  See the NOTICE file\n- *  * distributed with this work for additional information\n- *  * regarding copyright ownership.  The ASF licenses this file\n- *  * to you under the Apache License, Version 2.0 (the\n- *  * \"License\"); you may not use this file except in compliance\n- *  * with the License.  You may obtain a copy of the License at\n- *  * <p>\n- *  * http://www.apache.org/licenses/LICENSE-2.0\n- *  * <p>\n- *  * Unless required by applicable law or agreed to in writing, software\n- *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n- *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- *  * See the License for the specific language governing permissions and\n- *  * limitations under the License.\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n  *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n  */\n package org.apache.storm.sql.runtime;\n ", "filename": "external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/AbstractChannelHandler.java"}, {"additions": 15, "raw_url": "https://github.com/apache/storm/raw/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/AbstractValuesProcessor.java", "blob_url": "https://github.com/apache/storm/blob/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/AbstractValuesProcessor.java", "sha": "5b18b5bb5bc592a8f46f70a21a171d61d3317604", "changes": 33, "status": "modified", "deletions": 18, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/AbstractValuesProcessor.java?ref=164fd9c6e1f6dd0925858ba3fce40859e547938d", "patch": "@@ -1,23 +1,20 @@\n-/*\n- * *\n- *  * Licensed to the Apache Software Foundation (ASF) under one\n- *  * or more contributor license agreements.  See the NOTICE file\n- *  * distributed with this work for additional information\n- *  * regarding copyright ownership.  The ASF licenses this file\n- *  * to you under the Apache License, Version 2.0 (the\n- *  * \"License\"); you may not use this file except in compliance\n- *  * with the License.  You may obtain a copy of the License at\n- *  * <p>\n- *  * http://www.apache.org/licenses/LICENSE-2.0\n- *  * <p>\n- *  * Unless required by applicable law or agreed to in writing, software\n- *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n- *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- *  * See the License for the specific language governing permissions and\n- *  * limitations under the License.\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n  *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n  */\n-\n package org.apache.storm.sql.runtime;\n \n import org.apache.storm.tuple.Values;", "filename": "external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/AbstractValuesProcessor.java"}, {"additions": 15, "raw_url": "https://github.com/apache/storm/raw/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/ChannelContext.java", "blob_url": "https://github.com/apache/storm/blob/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/ChannelContext.java", "sha": "65ad01c625dd8cecf015d15511e9ac290c95ac21", "changes": 32, "status": "modified", "deletions": 17, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/ChannelContext.java?ref=164fd9c6e1f6dd0925858ba3fce40859e547938d", "patch": "@@ -1,21 +1,19 @@\n-/*\n- * *\n- *  * Licensed to the Apache Software Foundation (ASF) under one\n- *  * or more contributor license agreements.  See the NOTICE file\n- *  * distributed with this work for additional information\n- *  * regarding copyright ownership.  The ASF licenses this file\n- *  * to you under the Apache License, Version 2.0 (the\n- *  * \"License\"); you may not use this file except in compliance\n- *  * with the License.  You may obtain a copy of the License at\n- *  * <p>\n- *  * http://www.apache.org/licenses/LICENSE-2.0\n- *  * <p>\n- *  * Unless required by applicable law or agreed to in writing, software\n- *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n- *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- *  * See the License for the specific language governing permissions and\n- *  * limitations under the License.\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n  *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n  */\n package org.apache.storm.sql.runtime;\n ", "filename": "external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/ChannelContext.java"}, {"additions": 15, "raw_url": "https://github.com/apache/storm/raw/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/ChannelHandler.java", "blob_url": "https://github.com/apache/storm/blob/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/ChannelHandler.java", "sha": "af02b7e6befeb353865d84a8f10a38f4288b2889", "changes": 32, "status": "modified", "deletions": 17, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/ChannelHandler.java?ref=164fd9c6e1f6dd0925858ba3fce40859e547938d", "patch": "@@ -1,21 +1,19 @@\n-/*\n- * *\n- *  * Licensed to the Apache Software Foundation (ASF) under one\n- *  * or more contributor license agreements.  See the NOTICE file\n- *  * distributed with this work for additional information\n- *  * regarding copyright ownership.  The ASF licenses this file\n- *  * to you under the Apache License, Version 2.0 (the\n- *  * \"License\"); you may not use this file except in compliance\n- *  * with the License.  You may obtain a copy of the License at\n- *  * <p>\n- *  * http://www.apache.org/licenses/LICENSE-2.0\n- *  * <p>\n- *  * Unless required by applicable law or agreed to in writing, software\n- *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n- *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- *  * See the License for the specific language governing permissions and\n- *  * limitations under the License.\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n  *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n  */\n package org.apache.storm.sql.runtime;\n ", "filename": "external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/ChannelHandler.java"}, {"additions": 15, "raw_url": "https://github.com/apache/storm/raw/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/Channels.java", "blob_url": "https://github.com/apache/storm/blob/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/Channels.java", "sha": "3b5eeddabeafad27fcc24b8ba42a8a41043b39fc", "changes": 32, "status": "modified", "deletions": 17, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/Channels.java?ref=164fd9c6e1f6dd0925858ba3fce40859e547938d", "patch": "@@ -1,21 +1,19 @@\n-/*\n- * *\n- *  * Licensed to the Apache Software Foundation (ASF) under one\n- *  * or more contributor license agreements.  See the NOTICE file\n- *  * distributed with this work for additional information\n- *  * regarding copyright ownership.  The ASF licenses this file\n- *  * to you under the Apache License, Version 2.0 (the\n- *  * \"License\"); you may not use this file except in compliance\n- *  * with the License.  You may obtain a copy of the License at\n- *  * <p>\n- *  * http://www.apache.org/licenses/LICENSE-2.0\n- *  * <p>\n- *  * Unless required by applicable law or agreed to in writing, software\n- *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n- *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- *  * See the License for the specific language governing permissions and\n- *  * limitations under the License.\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n  *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n  */\n package org.apache.storm.sql.runtime;\n ", "filename": "external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/Channels.java"}, {"additions": 15, "raw_url": "https://github.com/apache/storm/raw/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSource.java", "blob_url": "https://github.com/apache/storm/blob/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSource.java", "sha": "352af732884f856272ed005d74fdc31b85b33ed1", "changes": 32, "status": "modified", "deletions": 17, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSource.java?ref=164fd9c6e1f6dd0925858ba3fce40859e547938d", "patch": "@@ -1,21 +1,19 @@\n-/*\n- * *\n- *  * Licensed to the Apache Software Foundation (ASF) under one\n- *  * or more contributor license agreements.  See the NOTICE file\n- *  * distributed with this work for additional information\n- *  * regarding copyright ownership.  The ASF licenses this file\n- *  * to you under the Apache License, Version 2.0 (the\n- *  * \"License\"); you may not use this file except in compliance\n- *  * with the License.  You may obtain a copy of the License at\n- *  * <p>\n- *  * http://www.apache.org/licenses/LICENSE-2.0\n- *  * <p>\n- *  * Unless required by applicable law or agreed to in writing, software\n- *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n- *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- *  * See the License for the specific language governing permissions and\n- *  * limitations under the License.\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n  *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n  */\n package org.apache.storm.sql.runtime;\n ", "filename": "external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSource.java"}, {"additions": 15, "raw_url": "https://github.com/apache/storm/raw/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSourcesProvider.java", "blob_url": "https://github.com/apache/storm/blob/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSourcesProvider.java", "sha": "85e20d90475d4797e161697fae40d8dd49b5f1f0", "changes": 33, "status": "modified", "deletions": 18, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSourcesProvider.java?ref=164fd9c6e1f6dd0925858ba3fce40859e547938d", "patch": "@@ -1,23 +1,20 @@\n-/*\n- * *\n- *  * Licensed to the Apache Software Foundation (ASF) under one\n- *  * or more contributor license agreements.  See the NOTICE file\n- *  * distributed with this work for additional information\n- *  * regarding copyright ownership.  The ASF licenses this file\n- *  * to you under the Apache License, Version 2.0 (the\n- *  * \"License\"); you may not use this file except in compliance\n- *  * with the License.  You may obtain a copy of the License at\n- *  * <p>\n- *  * http://www.apache.org/licenses/LICENSE-2.0\n- *  * <p>\n- *  * Unless required by applicable law or agreed to in writing, software\n- *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n- *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- *  * See the License for the specific language governing permissions and\n- *  * limitations under the License.\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n  *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n  */\n-\n package org.apache.storm.sql.runtime;\n \n import java.net.URI;", "filename": "external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSourcesProvider.java"}, {"additions": 15, "raw_url": "https://github.com/apache/storm/raw/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSourcesRegistry.java", "blob_url": "https://github.com/apache/storm/blob/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSourcesRegistry.java", "sha": "b2221ff3cc25ab17cc9e4511b344887518884c63", "changes": 33, "status": "modified", "deletions": 18, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSourcesRegistry.java?ref=164fd9c6e1f6dd0925858ba3fce40859e547938d", "patch": "@@ -1,23 +1,20 @@\n-/*\n- * *\n- *  * Licensed to the Apache Software Foundation (ASF) under one\n- *  * or more contributor license agreements.  See the NOTICE file\n- *  * distributed with this work for additional information\n- *  * regarding copyright ownership.  The ASF licenses this file\n- *  * to you under the Apache License, Version 2.0 (the\n- *  * \"License\"); you may not use this file except in compliance\n- *  * with the License.  You may obtain a copy of the License at\n- *  * <p>\n- *  * http://www.apache.org/licenses/LICENSE-2.0\n- *  * <p>\n- *  * Unless required by applicable law or agreed to in writing, software\n- *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n- *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- *  * See the License for the specific language governing permissions and\n- *  * limitations under the License.\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n  *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n  */\n-\n package org.apache.storm.sql.runtime;\n \n import org.slf4j.Logger;", "filename": "external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/DataSourcesRegistry.java"}, {"additions": 17, "raw_url": "https://github.com/apache/storm/raw/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/SimpleSqlTridentConsumer.java", "blob_url": "https://github.com/apache/storm/blob/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/SimpleSqlTridentConsumer.java", "sha": "c9abd168f5eb60990813807c3e0609d96a76ee0c", "changes": 17, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/SimpleSqlTridentConsumer.java?ref=164fd9c6e1f6dd0925858ba3fce40859e547938d", "patch": "@@ -1,3 +1,20 @@\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n package org.apache.storm.sql.runtime;\n \n import org.apache.storm.trident.state.StateFactory;", "filename": "external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/SimpleSqlTridentConsumer.java"}, {"additions": 15, "raw_url": "https://github.com/apache/storm/raw/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/StormSqlFunctions.java", "blob_url": "https://github.com/apache/storm/blob/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/StormSqlFunctions.java", "sha": "a373483fff9ff14424a903c7378ca5600d1688c7", "changes": 32, "status": "modified", "deletions": 17, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/StormSqlFunctions.java?ref=164fd9c6e1f6dd0925858ba3fce40859e547938d", "patch": "@@ -1,21 +1,19 @@\n-/*\n- * *\n- *  * Licensed to the Apache Software Foundation (ASF) under one\n- *  * or more contributor license agreements.  See the NOTICE file\n- *  * distributed with this work for additional information\n- *  * regarding copyright ownership.  The ASF licenses this file\n- *  * to you under the Apache License, Version 2.0 (the\n- *  * \"License\"); you may not use this file except in compliance\n- *  * with the License.  You may obtain a copy of the License at\n- *  * <p>\n- *  * http://www.apache.org/licenses/LICENSE-2.0\n- *  * <p>\n- *  * Unless required by applicable law or agreed to in writing, software\n- *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n- *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- *  * See the License for the specific language governing permissions and\n- *  * limitations under the License.\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n  *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n  */\n package org.apache.storm.sql.runtime;\n ", "filename": "external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/StormSqlFunctions.java"}, {"additions": 15, "raw_url": "https://github.com/apache/storm/raw/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/trident/AbstractTridentProcessor.java", "blob_url": "https://github.com/apache/storm/blob/164fd9c6e1f6dd0925858ba3fce40859e547938d/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/trident/AbstractTridentProcessor.java", "sha": "0f9f9bdef57b3ac310694fe0d9e72efd20204e38", "changes": 33, "status": "modified", "deletions": 18, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/trident/AbstractTridentProcessor.java?ref=164fd9c6e1f6dd0925858ba3fce40859e547938d", "patch": "@@ -1,23 +1,20 @@\n-/*\n- * *\n- *  * Licensed to the Apache Software Foundation (ASF) under one\n- *  * or more contributor license agreements.  See the NOTICE file\n- *  * distributed with this work for additional information\n- *  * regarding copyright ownership.  The ASF licenses this file\n- *  * to you under the Apache License, Version 2.0 (the\n- *  * \"License\"); you may not use this file except in compliance\n- *  * with the License.  You may obtain a copy of the License at\n- *  * <p>\n- *  * http://www.apache.org/licenses/LICENSE-2.0\n- *  * <p>\n- *  * Unless required by applicable law or agreed to in writing, software\n- *  * distributed under the License is distributed on an \"AS IS\" BASIS,\n- *  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n- *  * See the License for the specific language governing permissions and\n- *  * limitations under the License.\n+/**\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n  *\n+ * http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n  */\n-\n package org.apache.storm.sql.runtime.trident;\n \n import org.apache.storm.sql.runtime.ISqlTridentDataSource;", "filename": "external/sql/storm-sql-runtime/src/jvm/org/apache/storm/sql/runtime/trident/AbstractTridentProcessor.java"}, {"additions": 2, "raw_url": "https://github.com/apache/storm/raw/164fd9c6e1f6dd0925858ba3fce40859e547938d/pom.xml", "blob_url": "https://github.com/apache/storm/blob/164fd9c6e1f6dd0925858ba3fce40859e547938d/pom.xml", "sha": "64b9a97786c221117e1fbcc77a4d00daa980e584", "changes": 3, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/pom.xml?ref=164fd9c6e1f6dd0925858ba3fce40859e547938d", "patch": "@@ -353,7 +353,7 @@\n                     <plugin>\n                         <groupId>org.apache.rat</groupId>\n                         <artifactId>apache-rat-plugin</artifactId>\n-                        <version>0.11</version>\n+                        <version>0.12</version>\n                         <executions>\n                             <execution>\n                                 <phase>test</phase>\n@@ -364,6 +364,7 @@\n                         </executions>\n                         <configuration>\n                             <excludeSubProjects>false</excludeSubProjects>\n+                            <consoleOutput>true</consoleOutput>\n                             <excludes>\n                                 <!-- exclude maven artifacts -->\n                                 <exclude>**/target/**</exclude>", "filename": "pom.xml"}, {"additions": 30, "raw_url": "https://github.com/apache/storm/raw/164fd9c6e1f6dd0925858ba3fce40859e547938d/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj", "blob_url": "https://github.com/apache/storm/blob/164fd9c6e1f6dd0925858ba3fce40859e547938d/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj", "sha": "6878ac6256eaa920cf8eba732bc908269324c88a", "changes": 72, "status": "modified", "deletions": 42, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj?ref=164fd9c6e1f6dd0925858ba3fce40859e547938d", "patch": "@@ -448,26 +448,6 @@\n                   [[id info]]))\n               supervisor-ids)))))\n \n-(defn- all-scheduling-slots\n-  [nimbus topologies missing-assignment-topologies]\n-  (let [storm-cluster-state (:storm-cluster-state nimbus)\n-        ^INimbus inimbus (:inimbus nimbus)\n-\n-        supervisor-infos (all-supervisor-info storm-cluster-state nil)\n-\n-        supervisor-details (dofor [[id info] supervisor-infos]\n-                             (SupervisorDetails. id (:meta info) (:resources-map info)))\n-\n-        ret (.allSlotsAvailableForScheduling inimbus\n-                     supervisor-details\n-                     topologies\n-                     (set missing-assignment-topologies)\n-                     )\n-        ]\n-    (dofor [^WorkerSlot slot ret]\n-      [(.getNodeId slot) (.getPort slot)]\n-      )))\n-\n (defn- get-version-for-key [key nimbus-host-port-info conf]\n   (let [version (KeySequenceNumber. key nimbus-host-port-info)]\n     (.getKeySequenceNumber version conf)))\n@@ -712,27 +692,35 @@\n                                                    {})))]]\n              {tid (SchedulerAssignmentImpl. tid executor->slot)})))\n \n-(defn- read-all-supervisor-details [nimbus all-scheduling-slots supervisor->dead-ports]\n+(defn- read-all-supervisor-details\n+  [nimbus supervisor->dead-ports topologies missing-assignment-topologies]\n   \"return a map: {supervisor-id SupervisorDetails}\"\n   (let [storm-cluster-state (:storm-cluster-state nimbus)\n         supervisor-infos (all-supervisor-info storm-cluster-state)\n-        nonexistent-supervisor-slots (apply dissoc all-scheduling-slots (keys supervisor-infos))\n-        all-supervisor-details (into {} (for [[sid supervisor-info] supervisor-infos\n-                                              :let [hostname (:hostname supervisor-info)\n-                                                    scheduler-meta (:scheduler-meta supervisor-info)\n-                                                    dead-ports (supervisor->dead-ports sid)\n-                                                    ;; hide the dead-ports from the all-ports\n-                                                    ;; these dead-ports can be reused in next round of assignments\n-                                                    all-ports (-> (get all-scheduling-slots sid)\n-                                                                  (set/difference dead-ports)\n-                                                                  ((fn [ports] (map int ports))))\n-                                                    supervisor-details (SupervisorDetails. sid hostname scheduler-meta all-ports (:resources-map supervisor-info))]]\n-                                          {sid supervisor-details}))]\n-    (merge all-supervisor-details\n-           (into {}\n-              (for [[sid ports] nonexistent-supervisor-slots]\n-                [sid (SupervisorDetails. sid nil ports)]))\n-           )))\n+        supervisor-details (for [[id info] supervisor-infos]\n+                             (SupervisorDetails. id (:meta info) (:resources-map info)))\n+        ;; Note that allSlotsAvailableForScheduling\n+        ;; only uses the supervisor-details. The rest of the arguments\n+        ;; are there to satisfy the INimbus interface.\n+        all-scheduling-slots (->> (.allSlotsAvailableForScheduling\n+                                    (:inimbus nimbus)\n+                                    supervisor-details\n+                                    topologies\n+                                    (set missing-assignment-topologies))\n+                                  (map (fn [s] {(.getNodeId s) #{(.getPort s)}}))\n+                                  (apply merge-with set/union))]\n+    (into {} (for [[sid supervisor-info] supervisor-infos\n+                   :let [hostname (:hostname supervisor-info)\n+                         scheduler-meta (:scheduler-meta supervisor-info)\n+                         dead-ports (supervisor->dead-ports sid)\n+                         ;; hide the dead-ports from the all-ports\n+                         ;; these dead-ports can be reused in next round of assignments\n+                         all-ports (-> (get all-scheduling-slots sid)\n+                                       (set/difference dead-ports)\n+                                       (as-> ports (map int ports)))\n+                         supervisor-details (SupervisorDetails. sid hostname scheduler-meta all-ports (:resources-map supervisor-info))]]\n+               {sid supervisor-details}))))\n+\n \n ;TODO: when translating this function, you should replace the map-val with a proper for loop HERE\n (defn- compute-topology->executor->node+port [scheduler-assignments]\n@@ -842,11 +830,11 @@\n                                                                   (get t)\n                                                                   num-used-workers )\n                                                               (-> topologies (.getById t) .getNumWorkers)))))))\n-        all-scheduling-slots (->> (all-scheduling-slots nimbus topologies missing-assignment-topologies)\n-                                  (map (fn [[node-id port]] {node-id #{port}}))\n-                                  (apply merge-with set/union))\n \n-        supervisors (read-all-supervisor-details nimbus all-scheduling-slots supervisor->dead-ports)\n+        supervisors (read-all-supervisor-details nimbus \n+                                                 supervisor->dead-ports\n+                                                 topologies\n+                                                 missing-assignment-topologies)\n         cluster (Cluster. (:inimbus nimbus) supervisors topology->scheduler-assignment conf)]\n \n     ;; set the status map with existing topology statuses", "filename": "storm-core/src/clj/org/apache/storm/daemon/nimbus.clj"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/cbc506c0b4e56ef70e640c268b5b57f845c0e1fe", "parent": "https://github.com/apache/storm/commit/07629c1f898ebb0cedcc19e15e4813692b6a9345", "message": "Avoid NPE while prining Metrics", "bug_id": "storm_32", "file": [{"additions": 1, "raw_url": "https://github.com/apache/storm/raw/cbc506c0b4e56ef70e640c268b5b57f845c0e1fe/examples/storm-starter/src/jvm/org/apache/storm/starter/ThroughputVsLatency.java", "blob_url": "https://github.com/apache/storm/blob/cbc506c0b4e56ef70e640c268b5b57f845c0e1fe/examples/storm-starter/src/jvm/org/apache/storm/starter/ThroughputVsLatency.java", "sha": "8ecfb3a1b032d5569f927895cad97971654ae450", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/examples/storm-starter/src/jvm/org/apache/storm/starter/ThroughputVsLatency.java?ref=cbc506c0b4e56ef70e640c268b5b57f845c0e1fe", "patch": "@@ -273,7 +273,7 @@ public static void printMetrics(C client, String name) throws Exception {\n     long acked = 0;\n     long failed = 0;\n     for (ExecutorSummary exec: info.get_executors()) {\n-      if (\"spout\".equals(exec.get_component_id())) {\n+      if (\"spout\".equals(exec.get_component_id()) && exec.get_stats() != null && exec.get_stats().get_specific() != null) {\n         SpoutStats stats = exec.get_stats().get_specific().get_spout();\n         Map<String, Long> failedMap = stats.get_failed().get(\":all-time\");\n         Map<String, Long> ackedMap = stats.get_acked().get(\":all-time\");", "filename": "examples/storm-starter/src/jvm/org/apache/storm/starter/ThroughputVsLatency.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/e61e7738328913ec429742cd2b3cb22a39eb263f", "parent": "https://github.com/apache/storm/commit/bcadf0461f1819d54cd3e8bde321c8f8a7775789", "message": "STORM-2074: fix storm-kafka-monitor NPE bug", "bug_id": "storm_33", "file": [{"additions": 11, "raw_url": "https://github.com/apache/storm/raw/e61e7738328913ec429742cd2b3cb22a39eb263f/external/storm-kafka-monitor/src/main/java/org/apache/storm/kafka/monitor/KafkaOffsetLagUtil.java", "blob_url": "https://github.com/apache/storm/blob/e61e7738328913ec429742cd2b3cb22a39eb263f/external/storm-kafka-monitor/src/main/java/org/apache/storm/kafka/monitor/KafkaOffsetLagUtil.java", "sha": "8608bef6d97a80bd8ddb5262b9aedb20a8599e8f", "changes": 14, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-kafka-monitor/src/main/java/org/apache/storm/kafka/monitor/KafkaOffsetLagUtil.java?ref=e61e7738328913ec429742cd2b3cb22a39eb263f", "patch": "@@ -89,6 +89,10 @@ public static void main (String args[]) {\n                     printUsageAndExit(options, OPTION_ZK_SERVERS_LONG + \" and \" + OPTION_ZK_COMMITTED_NODE_LONG + \" are required  with \" +\n                             OPTION_OLD_CONSUMER_LONG);\n                 }\n+                String zkNode = commandLine.getOptionValue(OPTION_ZK_COMMITTED_NODE_LONG);\n+                if (zkNode == null || zkNode.length() <= 1) {\n+                \t printUsageAndExit(options, OPTION_ZK_COMMITTED_NODE_LONG+\" '\"+zkNode+\"' is invalid.\");\n+                }\n                 String[] topics = commandLine.getOptionValue(OPTION_TOPIC_LONG).split(\",\");\n                 if (topics != null && topics.length > 1) {\n                     printUsageAndExit(options, \"Multiple topics not supported with option \" + OPTION_OLD_CONSUMER_LONG + \". Either a single topic or a \" +\n@@ -373,16 +377,20 @@ private static Options buildOptions () {\n         curatorFramework.start();\n         String partitionPrefix = \"partition_\";\n         String zkPath = oldKafkaSpoutOffsetQuery.getZkPath();\n-        if (!zkPath.endsWith(\"/\")) {\n-            zkPath += \"/\";\n+         if (zkPath.endsWith(\"/\")) {\n+            zkPath = zkPath.substring(0, zkPath.length()-1);\n+        }\n+        if (curatorFramework.checkExists().forPath(zkPath) == null) {\n+        \tSystem.out.printf(\"--zk-node '%s' is not exists.%n\", zkPath);\n+        \tSystem.exit(1);\n         }\n         byte[] zkData;\n         try {\n             if (topicPartitions != null) {\n                 for (Map.Entry<String, List<Integer>> topicEntry: topicPartitions.entrySet()) {\n                     Map<Integer, Long> partitionOffsets = new HashMap<>();\n                     for (Integer partition: topicEntry.getValue()) {\n-                        String path = zkPath + (oldKafkaSpoutOffsetQuery.isWildCardTopic() ? topicEntry.getKey() + \"/\" : \"\") + partitionPrefix + partition;\n+                        String path = zkPath + \"/\" + (oldKafkaSpoutOffsetQuery.isWildCardTopic() ? topicEntry.getKey() + \"/\" : \"\") + partitionPrefix + partition;\n                         if (curatorFramework.checkExists().forPath(path) != null) {\n                             zkData = curatorFramework.getData().forPath(path);\n                             Map<Object, Object> offsetData = (Map<Object, Object>) JSONValue.parse(new String(zkData, \"UTF-8\"));", "filename": "external/storm-kafka-monitor/src/main/java/org/apache/storm/kafka/monitor/KafkaOffsetLagUtil.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/a37ddbaa7a4c8b94a68efda94a74910333333820", "parent": "https://github.com/apache/storm/commit/dd00bc0a2a5d105cabc71b8eeaa132f93746de2a", "message": "[STORM-1575] fix TwitterSampleSpout NPE on close", "bug_id": "storm_34", "file": [{"additions": 6, "raw_url": "https://github.com/apache/storm/raw/a37ddbaa7a4c8b94a68efda94a74910333333820/examples/storm-starter/src/jvm/org/apache/storm/starter/spout/TwitterSampleSpout.java", "blob_url": "https://github.com/apache/storm/blob/a37ddbaa7a4c8b94a68efda94a74910333333820/examples/storm-starter/src/jvm/org/apache/storm/starter/spout/TwitterSampleSpout.java", "sha": "e8a2c05ce6a20093a6456972cec29e747d11c099", "changes": 12, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/storm/contents/examples/storm-starter/src/jvm/org/apache/storm/starter/spout/TwitterSampleSpout.java?ref=a37ddbaa7a4c8b94a68efda94a74910333333820", "patch": "@@ -103,24 +103,24 @@ public void onStallWarning(StallWarning arg0) {\n \n \t\t};\n \n-\t\tTwitterStream twitterStream = new TwitterStreamFactory(\n+\t\t_twitterStream = new TwitterStreamFactory(\n \t\t\t\tnew ConfigurationBuilder().setJSONStoreEnabled(true).build())\n \t\t\t\t.getInstance();\n \n-\t\ttwitterStream.addListener(listener);\n-\t\ttwitterStream.setOAuthConsumer(consumerKey, consumerSecret);\n+\t\t_twitterStream.addListener(listener);\n+\t\t_twitterStream.setOAuthConsumer(consumerKey, consumerSecret);\n \t\tAccessToken token = new AccessToken(accessToken, accessTokenSecret);\n-\t\ttwitterStream.setOAuthAccessToken(token);\n+\t\t_twitterStream.setOAuthAccessToken(token);\n \t\t\n \t\tif (keyWords.length == 0) {\n \n-\t\t\ttwitterStream.sample();\n+\t\t\t_twitterStream.sample();\n \t\t}\n \n \t\telse {\n \n \t\t\tFilterQuery query = new FilterQuery().track(keyWords);\n-\t\t\ttwitterStream.filter(query);\n+\t\t\t_twitterStream.filter(query);\n \t\t}\n \n \t}", "filename": "examples/storm-starter/src/jvm/org/apache/storm/starter/spout/TwitterSampleSpout.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/5b86f4712f92d34c32134e933b8f2c7fc3c866f0", "parent": "https://github.com/apache/storm/commit/f0049e6fadb015be2744dde30a6e2fa46e168697", "message": "STORM-1108: Fix NPE in simulated time.", "bug_id": "storm_35", "file": [{"additions": 11, "raw_url": "https://github.com/apache/storm/raw/5b86f4712f92d34c32134e933b8f2c7fc3c866f0/storm-core/src/jvm/backtype/storm/utils/Time.java", "blob_url": "https://github.com/apache/storm/blob/5b86f4712f92d34c32134e933b8f2c7fc3c866f0/storm-core/src/jvm/backtype/storm/utils/Time.java", "sha": "6af7185ac350f2875e80493bb88630b80ff985d3", "changes": 12, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/backtype/storm/utils/Time.java?ref=5b86f4712f92d34c32134e933b8f2c7fc3c866f0", "patch": "@@ -58,14 +58,24 @@ public static void sleepUntil(long targetTimeMs) throws InterruptedException {\n         if(simulating.get()) {\n             try {\n                 synchronized(sleepTimesLock) {\n+                    if (threadSleepTimes == null) {\n+                        LOG.debug(\"{} is still sleeping after simulated time disabled.\", Thread.currentThread(), new RuntimeException(\"STACK TRACE\"));\n+                        throw new InterruptedException();\n+                    }\n                     threadSleepTimes.put(Thread.currentThread(), new AtomicLong(targetTimeMs));\n                 }\n                 while(simulatedCurrTimeMs.get() < targetTimeMs) {\n+                    synchronized(sleepTimesLock) {\n+                        if (threadSleepTimes == null) {\n+                            LOG.debug(\"{} is still sleeping after simulated time disabled.\", Thread.currentThread(), new RuntimeException(\"STACK TRACE\"));\n+                            throw new InterruptedException();\n+                        }\n+                    }\n                     Thread.sleep(10);\n                 }\n             } finally {\n                 synchronized(sleepTimesLock) {\n-                    if (simulating.get()) {\n+                    if (simulating.get() && threadSleepTimes != null) {\n                         threadSleepTimes.remove(Thread.currentThread());\n                     }\n                 }", "filename": "storm-core/src/jvm/backtype/storm/utils/Time.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/8b845392e920d753eb24fcffead0263416bc6227", "parent": "https://github.com/apache/storm/commit/4523e54eba2ca7cf4b95aa664daa175fa0ae8185", "message": "[STORM-1051] Fix for flushMessagse NPE", "bug_id": "storm_36", "file": [{"additions": 1, "raw_url": "https://github.com/apache/storm/raw/8b845392e920d753eb24fcffead0263416bc6227/storm-core/src/jvm/backtype/storm/messaging/netty/Client.java", "blob_url": "https://github.com/apache/storm/blob/8b845392e920d753eb24fcffead0263416bc6227/storm-core/src/jvm/backtype/storm/messaging/netty/Client.java", "sha": "8e661938a198b1f48876d96e70d0e3ba4f6805eb", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/backtype/storm/messaging/netty/Client.java?ref=8b845392e920d753eb24fcffead0263416bc6227", "patch": "@@ -317,7 +317,7 @@ private int iteratorSize(Iterator<TaskMessage> msgs) {\n      * If the write operation fails, then we will close the channel and trigger a reconnect.\n      */\n     private void flushMessages(Channel channel, final MessageBatch batch) {\n-        if(batch.isEmpty()){\n+        if(null == batch || batch.isEmpty()){\n             return;\n         }\n ", "filename": "storm-core/src/jvm/backtype/storm/messaging/netty/Client.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/a507899d5b2706ecca16420e082aa351864603ca", "parent": "https://github.com/apache/storm/commit/09bfca3379d35598a46bae7e495a3ad1a597349a", "message": "Merge branch 'executor-npe' of https://github.com/unsleepy22/storm into STORM-2045", "bug_id": "storm_37", "file": [{"additions": 1, "raw_url": "https://github.com/apache/storm/raw/a507899d5b2706ecca16420e082aa351864603ca/storm-core/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java", "blob_url": "https://github.com/apache/storm/blob/a507899d5b2706ecca16420e082aa351864603ca/storm-core/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java", "sha": "7a5066b590b11cb668679ded4f224e09a8e9800a", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java?ref=a507899d5b2706ecca16420e082aa351864603ca", "patch": "@@ -202,7 +202,7 @@ public void tupleActionFn(int taskId, TupleImpl tuple) throws Exception {\n             Long id = (Long) tuple.getValue(0);\n             Long timeDeltaMs = (Long) tuple.getValue(1);\n             TupleInfo tupleInfo = (TupleInfo) pending.remove(id);\n-            if (tupleInfo.getMessageId() != null) {\n+            if (tupleInfo != null && tupleInfo.getMessageId() != null) {\n                 if (taskId != tupleInfo.getTaskId()) {\n                     throw new RuntimeException(\"Fatal error, mismatched task ids: \" + taskId + \" \" + tupleInfo.getTaskId());\n                 }", "filename": "storm-core/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/3857644b4c459cbe42b78ca8a13bd55ef704fd30", "parent": "https://github.com/apache/storm/commit/5a79ba5f9b509d575e374e7dce58e286c1387430", "message": "Guard against NPE, and avoid using NaN values", "bug_id": "storm_38", "file": [{"additions": 42, "raw_url": "https://github.com/apache/storm/raw/3857644b4c459cbe42b78ca8a13bd55ef704fd30/storm-core/src/clj/backtype/storm/stats.clj", "blob_url": "https://github.com/apache/storm/blob/3857644b4c459cbe42b78ca8a13bd55ef704fd30/storm-core/src/clj/backtype/storm/stats.clj", "sha": "cb77bb717d30a5efc630e7a436e934c0bf90d260", "changes": 87, "status": "modified", "deletions": 45, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/clj/backtype/storm/stats.clj?ref=3857644b4c459cbe42b78ca8a13bd55ef704fd30", "patch": "@@ -287,29 +287,51 @@\n       specific-stats\n       rate)))\n \n+(defn valid-number?\n+  \"Returns true if x is a number that is not NaN, false otherwise\"\n+  [x]\n+  (and (number? x)\n+       (not (Double/isNaN x))))\n+\n+(defn apply-default\n+  [f defaulting-fn & args]\n+  (apply f (map defaulting-fn args)))\n+\n+(defn apply-or-0\n+  [f & args]\n+  (apply apply-default\n+         f\n+         #(if (valid-number? %) % 0)\n+         args))\n+\n+(defn sum-or-0\n+  [& args]\n+  (apply apply-or-0 + args))\n+\n+(defn product-or-0\n+  [& args]\n+  (apply apply-or-0 * args))\n+\n+(defn max-or-0\n+  [& args]\n+  (apply apply-or-0 max args))\n+\n (defn- agg-bolt-lat-and-count\n   \"Aggregates number executed, process latency, and execute latency across all\n   streams.\"\n   [idk->exec-avg idk->proc-avg idk->num-executed]\n-  {:pre (apply = (map #(set (keys %))\n-                      [idk->exec-avg\n-                       idk->proc-avg\n-                       idk->num-executed]))}\n-  (letfn [(weight-avg [[id avg]] (let [num-e (get idk->num-executed id)]\n-                                   (if (and avg num-e)\n-                                     (* avg num-e)\n-                                     0)))]\n+  (letfn [(weight-avg [[id avg]]\n+            (let [num-e (get idk->num-executed id)]\n+              (product-or-0 avg num-e)))]\n     {:executeLatencyTotal (sum (map weight-avg idk->exec-avg))\n      :processLatencyTotal (sum (map weight-avg idk->proc-avg))\n      :executed (sum (vals idk->num-executed))}))\n \n (defn- agg-spout-lat-and-count\n   \"Aggregates number acked and complete latencies across all streams.\"\n   [sid->comp-avg sid->num-acked]\n-  {:pre (apply = (map #(set (keys %))\n-                      [sid->comp-avg\n-                       sid->num-acked]))}\n-  (letfn [(weight-avg [[id avg]] (* avg (get sid->num-acked id)))]\n+  (letfn [(weight-avg [[id avg]]\n+            (product-or-0 avg (get sid->num-acked id)))]\n     {:completeLatencyTotal (sum (map weight-avg sid->comp-avg))\n      :acked (sum (vals sid->num-acked))}))\n \n@@ -335,30 +357,21 @@\n (defn- agg-bolt-streams-lat-and-count\n   \"Aggregates number executed and process & execute latencies.\"\n   [idk->exec-avg idk->proc-avg idk->executed]\n-  {:pre (apply = (map #(set (keys %))\n-                      [idk->exec-avg\n-                       idk->proc-avg\n-                       idk->executed]))}\n-  (letfn [(weight-avg [id avg] (let [num-e (idk->executed id)]\n-                                   (if (and avg num-e)\n-                                     (* avg num-e)\n-                                     0)))]\n+  (letfn [(weight-avg [id avg]\n+            (let [num-e (idk->executed id)]\n+              (product-or-0 avg num-e)))]\n     (into {}\n       (for [k (keys idk->exec-avg)]\n-        [k {:executeLatencyTotal (weight-avg k (idk->exec-avg k))\n-            :processLatencyTotal (weight-avg k (idk->proc-avg k))\n+        [k {:executeLatencyTotal (weight-avg k (get idk->exec-avg k))\n+            :processLatencyTotal (weight-avg k (get idk->proc-avg k))\n             :executed (idk->executed k)}]))))\n \n (defn- agg-spout-streams-lat-and-count\n   \"Aggregates number acked and complete latencies.\"\n   [idk->comp-avg idk->acked]\n-  {:pre (apply = (map #(set (keys %))\n-                      [idk->comp-avg\n-                       idk->acked]))}\n-  (letfn [(weight-avg [id avg] (let [num-e (get idk->acked id)]\n-                                   (if (and avg num-e)\n-                                     (* avg num-e)\n-                                     0)))]\n+  (letfn [(weight-avg [id avg]\n+            (let [num-e (get idk->acked id)]\n+              (product-or-0 avg num-e)))]\n     (into {}\n       (for [k (keys idk->comp-avg)]\n         [k {:completeLatencyTotal (weight-avg k (get idk->comp-avg k))\n@@ -596,22 +609,6 @@\n                     vals\n                     sum)})}))\n \n-(defn apply-default\n-  [f defaulting-fn & args]\n-  (apply f (map defaulting-fn args)))\n-\n-(defn apply-or-0\n-  [f & args]\n-  (apply apply-default f #(or % 0) args))\n-\n-(defn sum-or-0\n-  [& args]\n-  (apply apply-or-0 + args))\n-\n-(defn max-or-0\n-  [& args]\n-  (apply apply-or-0 max args))\n-\n (defn merge-agg-comp-stats-comp-page-bolt\n   [{acc-in :cid+sid->input-stats\n     acc-out :sid->output-stats", "filename": "storm-core/src/clj/backtype/storm/stats.clj"}, {"additions": 9, "raw_url": "https://github.com/apache/storm/raw/3857644b4c459cbe42b78ca8a13bd55ef704fd30/storm-core/src/jvm/backtype/storm/metric/internal/LatencyStatAndMetric.java", "blob_url": "https://github.com/apache/storm/blob/3857644b4c459cbe42b78ca8a13bd55ef704fd30/storm-core/src/jvm/backtype/storm/metric/internal/LatencyStatAndMetric.java", "sha": "8c472bd7cc3761f5a6da97618e12ac866d203d39", "changes": 12, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/backtype/storm/metric/internal/LatencyStatAndMetric.java?ref=3857644b4c459cbe42b78ca8a13bd55ef704fd30", "patch": "@@ -145,7 +145,10 @@ synchronized Object getValueAndReset(long now) {\n         }\n \n         long timeSpent = now - _bucketStart;\n-        double ret = ((double)(lat + _exactExtraLat))/(count + _exactExtraCount);\n+        long exactExtraCountSum = count + _exactExtraCount;\n+        double ret = exactExtraCountSum > 0 ?\n+                ((double)(lat + _exactExtraLat))/exactExtraCountSum :\n+                0.0;\n         _bucketStart = now;\n         _exactExtraLat = 0;\n         _exactExtraCount = 0;\n@@ -227,7 +230,10 @@ private synchronized void rotate(long lat, long count, long timeSpent, long targ\n         ret.put(\"600\", readApproximateLatAvg(lat, count, timeSpent, _tmTime, _tmLatBuckets, _tmCountBuckets, 600 * 1000));\n         ret.put(\"10800\", readApproximateLatAvg(lat, count, timeSpent, _thTime, _thLatBuckets, _thCountBuckets, 10800 * 1000));\n         ret.put(\"86400\", readApproximateLatAvg(lat, count, timeSpent, _odTime, _odLatBuckets, _odCountBuckets, 86400 * 1000));\n-        ret.put(\":all-time\", ((double)lat + _allTimeLat)/(count + _allTimeCount));\n+        long allTimeCountSum = count + _allTimeCount;\n+        ret.put(\":all-time\", allTimeCountSum > 0 ?\n+                ((double)lat + _allTimeLat)/allTimeCountSum :\n+                0.0);\n         return ret;\n     }\n \n@@ -242,7 +248,7 @@ private synchronized void rotate(long lat, long count, long timeSpent, long targ\n             totalCount += countBuckets[i];\n             timeNeeded -= bucketTime[i];\n         }\n-        return ((double)totalLat)/totalCount;\n+        return totalCount > 0 ? ((double)totalLat)/totalCount : 0.0;\n     }\n \n     public void close() {", "filename": "storm-core/src/jvm/backtype/storm/metric/internal/LatencyStatAndMetric.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/c0c72eab7004e1e8e46e6c24f2abb7e4b24c863a", "parent": "https://github.com/apache/storm/commit/35a73151a3abf868e58002a1685c7fa2f98030d3", "message": "STORM-2126: fix NPE due to race condition in compute-new-sched-assignments/read-all-supervisor-details", "bug_id": "storm_39", "file": [{"additions": 30, "raw_url": "https://github.com/apache/storm/raw/c0c72eab7004e1e8e46e6c24f2abb7e4b24c863a/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj", "blob_url": "https://github.com/apache/storm/blob/c0c72eab7004e1e8e46e6c24f2abb7e4b24c863a/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj", "sha": "6878ac6256eaa920cf8eba732bc908269324c88a", "changes": 72, "status": "modified", "deletions": 42, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj?ref=c0c72eab7004e1e8e46e6c24f2abb7e4b24c863a", "patch": "@@ -448,26 +448,6 @@\n                   [[id info]]))\n               supervisor-ids)))))\n \n-(defn- all-scheduling-slots\n-  [nimbus topologies missing-assignment-topologies]\n-  (let [storm-cluster-state (:storm-cluster-state nimbus)\n-        ^INimbus inimbus (:inimbus nimbus)\n-\n-        supervisor-infos (all-supervisor-info storm-cluster-state nil)\n-\n-        supervisor-details (dofor [[id info] supervisor-infos]\n-                             (SupervisorDetails. id (:meta info) (:resources-map info)))\n-\n-        ret (.allSlotsAvailableForScheduling inimbus\n-                     supervisor-details\n-                     topologies\n-                     (set missing-assignment-topologies)\n-                     )\n-        ]\n-    (dofor [^WorkerSlot slot ret]\n-      [(.getNodeId slot) (.getPort slot)]\n-      )))\n-\n (defn- get-version-for-key [key nimbus-host-port-info conf]\n   (let [version (KeySequenceNumber. key nimbus-host-port-info)]\n     (.getKeySequenceNumber version conf)))\n@@ -712,27 +692,35 @@\n                                                    {})))]]\n              {tid (SchedulerAssignmentImpl. tid executor->slot)})))\n \n-(defn- read-all-supervisor-details [nimbus all-scheduling-slots supervisor->dead-ports]\n+(defn- read-all-supervisor-details\n+  [nimbus supervisor->dead-ports topologies missing-assignment-topologies]\n   \"return a map: {supervisor-id SupervisorDetails}\"\n   (let [storm-cluster-state (:storm-cluster-state nimbus)\n         supervisor-infos (all-supervisor-info storm-cluster-state)\n-        nonexistent-supervisor-slots (apply dissoc all-scheduling-slots (keys supervisor-infos))\n-        all-supervisor-details (into {} (for [[sid supervisor-info] supervisor-infos\n-                                              :let [hostname (:hostname supervisor-info)\n-                                                    scheduler-meta (:scheduler-meta supervisor-info)\n-                                                    dead-ports (supervisor->dead-ports sid)\n-                                                    ;; hide the dead-ports from the all-ports\n-                                                    ;; these dead-ports can be reused in next round of assignments\n-                                                    all-ports (-> (get all-scheduling-slots sid)\n-                                                                  (set/difference dead-ports)\n-                                                                  ((fn [ports] (map int ports))))\n-                                                    supervisor-details (SupervisorDetails. sid hostname scheduler-meta all-ports (:resources-map supervisor-info))]]\n-                                          {sid supervisor-details}))]\n-    (merge all-supervisor-details\n-           (into {}\n-              (for [[sid ports] nonexistent-supervisor-slots]\n-                [sid (SupervisorDetails. sid nil ports)]))\n-           )))\n+        supervisor-details (for [[id info] supervisor-infos]\n+                             (SupervisorDetails. id (:meta info) (:resources-map info)))\n+        ;; Note that allSlotsAvailableForScheduling\n+        ;; only uses the supervisor-details. The rest of the arguments\n+        ;; are there to satisfy the INimbus interface.\n+        all-scheduling-slots (->> (.allSlotsAvailableForScheduling\n+                                    (:inimbus nimbus)\n+                                    supervisor-details\n+                                    topologies\n+                                    (set missing-assignment-topologies))\n+                                  (map (fn [s] {(.getNodeId s) #{(.getPort s)}}))\n+                                  (apply merge-with set/union))]\n+    (into {} (for [[sid supervisor-info] supervisor-infos\n+                   :let [hostname (:hostname supervisor-info)\n+                         scheduler-meta (:scheduler-meta supervisor-info)\n+                         dead-ports (supervisor->dead-ports sid)\n+                         ;; hide the dead-ports from the all-ports\n+                         ;; these dead-ports can be reused in next round of assignments\n+                         all-ports (-> (get all-scheduling-slots sid)\n+                                       (set/difference dead-ports)\n+                                       (as-> ports (map int ports)))\n+                         supervisor-details (SupervisorDetails. sid hostname scheduler-meta all-ports (:resources-map supervisor-info))]]\n+               {sid supervisor-details}))))\n+\n \n ;TODO: when translating this function, you should replace the map-val with a proper for loop HERE\n (defn- compute-topology->executor->node+port [scheduler-assignments]\n@@ -842,11 +830,11 @@\n                                                                   (get t)\n                                                                   num-used-workers )\n                                                               (-> topologies (.getById t) .getNumWorkers)))))))\n-        all-scheduling-slots (->> (all-scheduling-slots nimbus topologies missing-assignment-topologies)\n-                                  (map (fn [[node-id port]] {node-id #{port}}))\n-                                  (apply merge-with set/union))\n \n-        supervisors (read-all-supervisor-details nimbus all-scheduling-slots supervisor->dead-ports)\n+        supervisors (read-all-supervisor-details nimbus \n+                                                 supervisor->dead-ports\n+                                                 topologies\n+                                                 missing-assignment-topologies)\n         cluster (Cluster. (:inimbus nimbus) supervisors topology->scheduler-assignment conf)]\n \n     ;; set the status map with existing topology statuses", "filename": "storm-core/src/clj/org/apache/storm/daemon/nimbus.clj"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/0352361d2649f1d0536f7acebb4175d6b9285e8d", "parent": "https://github.com/apache/storm/commit/73640f0c7185385199fefaad233d2c3fe572f266", "message": "Merge branch 'STORM-2101_compute_executors_npe_in_nimbus' of https://github.com/abellina/storm into STORM-2101", "bug_id": "storm_40", "file": [{"additions": 11, "raw_url": "https://github.com/apache/storm/raw/0352361d2649f1d0536f7acebb4175d6b9285e8d/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj", "blob_url": "https://github.com/apache/storm/blob/0352361d2649f1d0536f7acebb4175d6b9285e8d/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj", "sha": "f3cfca4551eb663c76e10db29e42f9232f892b46", "changes": 21, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/clj/org/apache/storm/daemon/nimbus.clj?ref=0352361d2649f1d0536f7acebb4175d6b9285e8d", "patch": "@@ -637,16 +637,17 @@\n         storm-conf (read-storm-conf-as-nimbus storm-id blob-store)\n         topology (read-storm-topology-as-nimbus storm-id blob-store)\n         task->component (get-clojurified-task-info topology storm-conf)]\n-    (->> (StormCommon/stormTaskInfo topology storm-conf)\n-         (Utils/reverseMap)\n-         clojurify-structure\n-         (map-val sort)\n-         ((fn [ & maps ] (Utils/joinMaps (into-array Map (into [component->executors] maps)))))\n-         (clojurify-structure)\n-         (map-val (partial apply (fn part-fixed [a b] (Utils/partitionFixed a b))))\n-         (mapcat second)\n-         (map to-executor-id)\n-         )))\n+    (if (nil? component->executors)\n+      []\n+      (->> (StormCommon/stormTaskInfo topology storm-conf)\n+           (Utils/reverseMap)\n+           clojurify-structure\n+           (map-val sort)\n+           ((fn [ & maps ] (Utils/joinMaps (into-array Map (into [component->executors] maps)))))\n+           (clojurify-structure)\n+           (map-val (partial apply (fn part-fixed [a b] (Utils/partitionFixed a b))))\n+           (mapcat second)\n+           (map to-executor-id)))))\n \n (defn- compute-executor->component [nimbus storm-id]\n   (let [conf (:conf nimbus)", "filename": "storm-core/src/clj/org/apache/storm/daemon/nimbus.clj"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/cfc87d2b8628eb4576eb26acb3c786c368a419e2", "parent": "https://github.com/apache/storm/commit/7db6aa84076823939a7493218a4c310c226cc983", "message": "Fix TransactionalTopologyBuilder NPE", "bug_id": "storm_41", "file": [{"additions": 1, "raw_url": "https://github.com/apache/storm/raw/cfc87d2b8628eb4576eb26acb3c786c368a419e2/src/jvm/backtype/storm/transactional/TransactionalTopologyBuilder.java", "blob_url": "https://github.com/apache/storm/blob/cfc87d2b8628eb4576eb26acb3c786c368a419e2/src/jvm/backtype/storm/transactional/TransactionalTopologyBuilder.java", "sha": "721a7d47e00c6234d97fab02859a4d1232155e3f", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/src/jvm/backtype/storm/transactional/TransactionalTopologyBuilder.java?ref=cfc87d2b8628eb4576eb26acb3c786c368a419e2", "patch": "@@ -52,7 +52,7 @@ public TransactionalTopologyBuilder(String id, String spoutId, ITransactionalSpo\n         _id = id;\n         _spoutId = spoutId;\n         _spout = spout;\n-        _spoutParallelism = spoutParallelism.intValue();\n+        _spoutParallelism = (spoutParallelism == null) ? null : spoutParallelism.intValue();\n     }\n     \n     public TransactionalTopologyBuilder(String id, String spoutId, ITransactionalSpout spout) {", "filename": "src/jvm/backtype/storm/transactional/TransactionalTopologyBuilder.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/bc79b4a8d757a3191a85815877345d38710c73e2", "parent": "https://github.com/apache/storm/commit/d36be51a39abb03ac47e01eb2e1fda31f9f9110b", "message": "Add a missing space, fix potential NPE, add comment to javadoc about reset timeout being expensive", "bug_id": "storm_42", "file": [{"additions": 4, "raw_url": "https://github.com/apache/storm/raw/bc79b4a8d757a3191a85815877345d38710c73e2/storm-core/src/jvm/org/apache/storm/daemon/Acker.java", "blob_url": "https://github.com/apache/storm/blob/bc79b4a8d757a3191a85815877345d38710c73e2/storm-core/src/jvm/org/apache/storm/daemon/Acker.java", "sha": "d7b9a2ec13f6ee246cd14634a777c91626b5adf0", "changes": 5, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/org/apache/storm/daemon/Acker.java?ref=bc79b4a8d757a3191a85815877345d38710c73e2", "patch": "@@ -101,7 +101,10 @@ public void execute(Tuple input) {\n             }\n             curr.failed = true;\n             pending.put(id, curr);\n-        } else if(ACKER_RESET_TIMEOUT_STREAM_ID.equals(streamId)) {\n+        } else if (ACKER_RESET_TIMEOUT_STREAM_ID.equals(streamId)) {\n+            if (curr == null) {\n+                curr = new AckObject();\n+            }\n             pending.put(id, curr);\n         } else {\n             LOG.warn(\"Unknown source stream {} from task-{}\", streamId, input.getSourceTask());", "filename": "storm-core/src/jvm/org/apache/storm/daemon/Acker.java"}, {"additions": 1, "raw_url": "https://github.com/apache/storm/raw/bc79b4a8d757a3191a85815877345d38710c73e2/storm-core/src/jvm/org/apache/storm/task/OutputCollector.java", "blob_url": "https://github.com/apache/storm/blob/bc79b4a8d757a3191a85815877345d38710c73e2/storm-core/src/jvm/org/apache/storm/task/OutputCollector.java", "sha": "4db87f0d3f5b4bd052179eadcf0e2e4be28bfa37", "changes": 1, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/org/apache/storm/task/OutputCollector.java?ref=bc79b4a8d757a3191a85815877345d38710c73e2", "patch": "@@ -221,6 +221,7 @@ public void fail(Tuple input) {\n     /**\n     * Resets the message timeout for any tuple trees to which the given tuple belongs.\n     * The timeout is reset to Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS.\n+    * Note that this is an expensive operation, and should be used sparingly.\n     * @param input the tuple to reset timeout for\n     */\n     @Override", "filename": "storm-core/src/jvm/org/apache/storm/task/OutputCollector.java"}, {"additions": 6, "raw_url": "https://github.com/apache/storm/raw/bc79b4a8d757a3191a85815877345d38710c73e2/storm-core/src/jvm/org/apache/storm/topology/BasicOutputCollector.java", "blob_url": "https://github.com/apache/storm/blob/bc79b4a8d757a3191a85815877345d38710c73e2/storm-core/src/jvm/org/apache/storm/topology/BasicOutputCollector.java", "sha": "1d1e5ffff2f15bbcd35b5dee1a402fa86b5f5350", "changes": 6, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/org/apache/storm/topology/BasicOutputCollector.java?ref=bc79b4a8d757a3191a85815877345d38710c73e2", "patch": "@@ -52,6 +52,12 @@ public void emitDirect(int taskId, List<Object> tuple) {\n         emitDirect(taskId, Utils.DEFAULT_STREAM_ID, tuple);\n     }\n \n+    /**\n+    * Resets the message timeout for any tuple trees to which the given tuple belongs.\n+    * The timeout is reset to Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS.\n+    * Note that this is an expensive operation, and should be used sparingly.\n+    * @param input the tuple to reset timeout for\n+    */\n     public void resetTimeout(Tuple tuple){\n         out.resetTimeout(tuple);\n     }", "filename": "storm-core/src/jvm/org/apache/storm/topology/BasicOutputCollector.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/08938c222e0f81f40c3d9bd3e5231bb86ae6e815", "parent": "https://github.com/apache/storm/commit/883a76b4d3b4d12b1a468597d5e358495cacf632", "message": "STORM-1594 org.apache.storm.tuple.Fields can throw NPE if given invalid field in selector\n\n* Closes #1522", "bug_id": "storm_43", "file": [{"additions": 2, "raw_url": "https://github.com/apache/storm/raw/08938c222e0f81f40c3d9bd3e5231bb86ae6e815/storm-core/src/jvm/org/apache/storm/tuple/Fields.java", "blob_url": "https://github.com/apache/storm/blob/08938c222e0f81f40c3d9bd3e5231bb86ae6e815/storm-core/src/jvm/org/apache/storm/tuple/Fields.java", "sha": "840b2d32e2e47996e8982ac418ecd43b3d20e496", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/org/apache/storm/tuple/Fields.java?ref=08938c222e0f81f40c3d9bd3e5231bb86ae6e815", "patch": "@@ -59,8 +59,8 @@ public Fields(List<String> fields) {\n      */\n     public List<Object> select(Fields selector, List<Object> tuple) {\n         List<Object> ret = new ArrayList<>(selector.size());\n-        for(String s: selector) {\n-            ret.add(tuple.get(_index.get(s)));\n+        for (String s : selector) {\n+            ret.add(tuple.get(fieldIndex(s))); \n         }\n         return ret;\n     }", "filename": "storm-core/src/jvm/org/apache/storm/tuple/Fields.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/da387c1e3fb8d7a704c479569d7e84ce52d58e7c", "parent": "https://github.com/apache/storm/commit/d1280c2b965f8920647ea26b00bbd8c2262f0464", "message": "STORM-1945 Fix NPE bugs on topology spout lag for storm-kafka-monitor\n\n* Fix several spots which are throwing NPE on TopologySpoutLag\n  * Use a trick to get classname of Spout without requiring storm-kafka and storm-kafka-client\n* Move storm-kafka-monitor jar to toollib directory\n  * since jars in extlib are added to worker classpath which we don't want for this jar", "bug_id": "storm_44", "file": [{"additions": 1, "raw_url": "https://github.com/apache/storm/raw/da387c1e3fb8d7a704c479569d7e84ce52d58e7c/bin/storm-kafka-monitor", "blob_url": "https://github.com/apache/storm/blob/da387c1e3fb8d7a704c479569d7e84ce52d58e7c/bin/storm-kafka-monitor", "sha": "bfe50b757ac8f70ae7d8dbfcbb231a4043c8885f", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/bin/storm-kafka-monitor?ref=da387c1e3fb8d7a704c479569d7e84ce52d58e7c", "patch": "@@ -40,4 +40,4 @@ else\n   JAVA=\"$JAVA_HOME/bin/java\"\n fi\n \n-exec \"$JAVA\" -cp \"$STORM_BASE_DIR/extlib/*\" org.apache.storm.kafka.monitor.KafkaOffsetLagUtil \"$@\"\n+exec \"$JAVA\" -cp \"$STORM_BASE_DIR/toollib/*\" org.apache.storm.kafka.monitor.KafkaOffsetLagUtil \"$@\"", "filename": "bin/storm-kafka-monitor"}, {"additions": 33, "raw_url": "https://github.com/apache/storm/raw/da387c1e3fb8d7a704c479569d7e84ce52d58e7c/storm-core/src/jvm/org/apache/storm/utils/TopologySpoutLag.java", "blob_url": "https://github.com/apache/storm/blob/da387c1e3fb8d7a704c479569d7e84ce52d58e7c/storm-core/src/jvm/org/apache/storm/utils/TopologySpoutLag.java", "sha": "0d3d2f1ceb175abf92de52ce9360da5ed45b1c30", "changes": 40, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/org/apache/storm/utils/TopologySpoutLag.java?ref=da387c1e3fb8d7a704c479569d7e84ce52d58e7c", "patch": "@@ -43,26 +43,44 @@\n     public static List<Map<String, Object>> lag (StormTopology stormTopology, Map topologyConf) {\n         List<Map<String, Object>> result = new ArrayList<>();\n         Map<String, SpoutSpec> spouts = stormTopology.get_spouts();\n-        Object object = null;\n+        String className = null;\n         for (Map.Entry<String, SpoutSpec> spout: spouts.entrySet()) {\n             try {\n                 SpoutSpec spoutSpec = spout.getValue();\n                 ComponentObject componentObject = spoutSpec.get_spout_object();\n-                object = Utils.getSetComponentObject(componentObject);\n-                if (object.getClass().getCanonicalName().endsWith(\"storm.kafka.spout.KafkaSpout\")) {\n+                // FIXME: yes it's a trick so we might be better to find alternative way...\n+                className = getClassNameFromComponentObject(componentObject);\n+                logger.debug(\"spout classname: {}\", className);\n+                if (className.endsWith(\"storm.kafka.spout.KafkaSpout\")) {\n                     result.add(getLagResultForNewKafkaSpout(spout.getKey(), spoutSpec, topologyConf));\n-                } else if (object.getClass().getCanonicalName().endsWith(\"storm.kafka.KafkaSpout\")) {\n+                } else if (className.endsWith(\"storm.kafka.KafkaSpout\")) {\n                     result.add(getLagResultForOldKafkaSpout(spout.getKey(), spoutSpec, topologyConf));\n                 }\n             } catch (Exception e) {\n-                logger.warn(\"Exception thrown while getting lag for spout id: \" + spout.getKey() + \" and spout class: \" + object.getClass().getCanonicalName());\n-                logger.warn(\"Exception message:\" + e.getMessage());\n+                logger.warn(\"Exception thrown while getting lag for spout id: \" + spout.getKey() + \" and spout class: \" + className);\n+                logger.warn(\"Exception message:\" + e.getMessage(), e);\n             }\n         }\n         return result;\n     }\n \n+    private static String getClassNameFromComponentObject(ComponentObject componentObject) {\n+        try {\n+            Object object = Utils.getSetComponentObject(componentObject);\n+            return object.getClass().getCanonicalName();\n+        } catch (RuntimeException e) {\n+\n+            if (e.getCause() instanceof ClassNotFoundException) {\n+                return e.getCause().getMessage().trim();\n+            }\n+\n+            throw e;\n+        }\n+    }\n+\n     private static List<String> getCommandLineOptionsForNewKafkaSpout (Map<String, Object> jsonConf) {\n+        logger.debug(\"json configuration: {}\", jsonConf);\n+\n         List<String> commands = new ArrayList<>();\n         String configKeyPrefix = \"config.\";\n         commands.add(\"-t\");\n@@ -75,6 +93,8 @@\n     }\n \n     private static List<String> getCommandLineOptionsForOldKafkaSpout (Map<String, Object> jsonConf, Map topologyConf) {\n+        logger.debug(\"json configuration: {}\", jsonConf);\n+\n         List<String> commands = new ArrayList<>();\n         String configKeyPrefix = \"config.\";\n         commands.add(\"-o\");\n@@ -122,7 +142,13 @@\n             commands.add(stormHomeDir != null ? stormHomeDir + \"bin\" + File.separator + \"storm-kafka-monitor\" : \"storm-kafka-monitor\");\n             Map<String, Object> jsonMap = (Map<String, Object>) JSONValue.parse(json);\n             commands.addAll(old ? getCommandLineOptionsForOldKafkaSpout(jsonMap, topologyConf) : getCommandLineOptionsForNewKafkaSpout(jsonMap));\n-            result = ShellUtils.execCommand(commands.toArray(new String[0]));\n+\n+            logger.debug(\"Command to run: {}\", commands);\n+\n+            // if commands contains one or more null value, spout is compiled with lower version of storm-kafka / storm-kafka-client\n+            if (!commands.contains(null)) {\n+                result = ShellUtils.execCommand(commands.toArray(new String[0]));\n+            }\n         }\n         Map<String, Object> kafkaSpoutLagInfo = new HashMap<>();\n         kafkaSpoutLagInfo.put(SPOUT_ID, spoutId);", "filename": "storm-core/src/jvm/org/apache/storm/utils/TopologySpoutLag.java"}, {"additions": 10, "raw_url": "https://github.com/apache/storm/raw/da387c1e3fb8d7a704c479569d7e84ce52d58e7c/storm-dist/binary/src/main/assembly/binary.xml", "blob_url": "https://github.com/apache/storm/blob/da387c1e3fb8d7a704c479569d7e84ce52d58e7c/storm-dist/binary/src/main/assembly/binary.xml", "sha": "6760bc856c2abfb906ce25c4dbd8bb09e1201fff", "changes": 12, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-dist/binary/src/main/assembly/binary.xml?ref=da387c1e3fb8d7a704c479569d7e84ce52d58e7c", "patch": "@@ -340,10 +340,10 @@\n         </fileSet>\n         <!-- $STORM_HOME/extlib -->\n         <fileSet>\n-            <directory>${project.basedir}/../../external/storm-kafka-monitor/target</directory>\n+            <directory></directory>\n             <outputDirectory>extlib</outputDirectory>\n             <includes>\n-                <include>storm*jar</include>\n+                <include></include>\n             </includes>\n         </fileSet>\n         <!-- $STORM_HOME/extlib-daemon, for daemons only -->\n@@ -354,6 +354,14 @@\n                 <include></include>\n             </includes>\n         </fileSet>\n+        <!-- $STORM_HOME/toollib -->\n+        <fileSet>\n+            <directory>${project.basedir}/../../external/storm-kafka-monitor/target</directory>\n+            <outputDirectory>toollib</outputDirectory>\n+            <includes>\n+                <include>storm*jar</include>\n+            </includes>\n+        </fileSet>\n \n     </fileSets>\n ", "filename": "storm-dist/binary/src/main/assembly/binary.xml"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/1308f89dc7316ea8b1483136cd5ca1209790ef81", "parent": "https://github.com/apache/storm/commit/1afa5a27cb7ab5191905d0cfafdb46f542a7c039", "message": "Merge branch 'storm-1208-ui-npe-nan' of https://github.com/d2r/storm", "bug_id": "storm_45", "file": [{"additions": 43, "raw_url": "https://github.com/apache/storm/raw/1308f89dc7316ea8b1483136cd5ca1209790ef81/storm-core/src/clj/backtype/storm/stats.clj", "blob_url": "https://github.com/apache/storm/blob/1308f89dc7316ea8b1483136cd5ca1209790ef81/storm-core/src/clj/backtype/storm/stats.clj", "sha": "ea4efe41310b5f473d8f7af5cb01d637756db149", "changes": 88, "status": "modified", "deletions": 45, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/clj/backtype/storm/stats.clj?ref=1308f89dc7316ea8b1483136cd5ca1209790ef81", "patch": "@@ -287,29 +287,52 @@\n       specific-stats\n       rate)))\n \n+(defn valid-number?\n+  \"Returns true if x is a number that is not NaN or Infinity, false otherwise\"\n+  [x]\n+  (and (number? x)\n+       (not (Double/isNaN x))\n+       (not (Double/isInfinite x))))\n+\n+(defn apply-default\n+  [f defaulting-fn & args]\n+  (apply f (map defaulting-fn args)))\n+\n+(defn apply-or-0\n+  [f & args]\n+  (apply apply-default\n+         f\n+         #(if (valid-number? %) % 0)\n+         args))\n+\n+(defn sum-or-0\n+  [& args]\n+  (apply apply-or-0 + args))\n+\n+(defn product-or-0\n+  [& args]\n+  (apply apply-or-0 * args))\n+\n+(defn max-or-0\n+  [& args]\n+  (apply apply-or-0 max args))\n+\n (defn- agg-bolt-lat-and-count\n   \"Aggregates number executed, process latency, and execute latency across all\n   streams.\"\n   [idk->exec-avg idk->proc-avg idk->num-executed]\n-  {:pre (apply = (map #(set (keys %))\n-                      [idk->exec-avg\n-                       idk->proc-avg\n-                       idk->num-executed]))}\n-  (letfn [(weight-avg [[id avg]] (let [num-e (get idk->num-executed id)]\n-                                   (if (and avg num-e)\n-                                     (* avg num-e)\n-                                     0)))]\n+  (letfn [(weight-avg [[id avg]]\n+            (let [num-e (get idk->num-executed id)]\n+              (product-or-0 avg num-e)))]\n     {:executeLatencyTotal (sum (map weight-avg idk->exec-avg))\n      :processLatencyTotal (sum (map weight-avg idk->proc-avg))\n      :executed (sum (vals idk->num-executed))}))\n \n (defn- agg-spout-lat-and-count\n   \"Aggregates number acked and complete latencies across all streams.\"\n   [sid->comp-avg sid->num-acked]\n-  {:pre (apply = (map #(set (keys %))\n-                      [sid->comp-avg\n-                       sid->num-acked]))}\n-  (letfn [(weight-avg [[id avg]] (* avg (get sid->num-acked id)))]\n+  (letfn [(weight-avg [[id avg]]\n+            (product-or-0 avg (get sid->num-acked id)))]\n     {:completeLatencyTotal (sum (map weight-avg sid->comp-avg))\n      :acked (sum (vals sid->num-acked))}))\n \n@@ -335,30 +358,21 @@\n (defn- agg-bolt-streams-lat-and-count\n   \"Aggregates number executed and process & execute latencies.\"\n   [idk->exec-avg idk->proc-avg idk->executed]\n-  {:pre (apply = (map #(set (keys %))\n-                      [idk->exec-avg\n-                       idk->proc-avg\n-                       idk->executed]))}\n-  (letfn [(weight-avg [id avg] (let [num-e (idk->executed id)]\n-                                   (if (and avg num-e)\n-                                     (* avg num-e)\n-                                     0)))]\n+  (letfn [(weight-avg [id avg]\n+            (let [num-e (idk->executed id)]\n+              (product-or-0 avg num-e)))]\n     (into {}\n       (for [k (keys idk->exec-avg)]\n-        [k {:executeLatencyTotal (weight-avg k (idk->exec-avg k))\n-            :processLatencyTotal (weight-avg k (idk->proc-avg k))\n+        [k {:executeLatencyTotal (weight-avg k (get idk->exec-avg k))\n+            :processLatencyTotal (weight-avg k (get idk->proc-avg k))\n             :executed (idk->executed k)}]))))\n \n (defn- agg-spout-streams-lat-and-count\n   \"Aggregates number acked and complete latencies.\"\n   [idk->comp-avg idk->acked]\n-  {:pre (apply = (map #(set (keys %))\n-                      [idk->comp-avg\n-                       idk->acked]))}\n-  (letfn [(weight-avg [id avg] (let [num-e (get idk->acked id)]\n-                                   (if (and avg num-e)\n-                                     (* avg num-e)\n-                                     0)))]\n+  (letfn [(weight-avg [id avg]\n+            (let [num-e (get idk->acked id)]\n+              (product-or-0 avg num-e)))]\n     (into {}\n       (for [k (keys idk->comp-avg)]\n         [k {:completeLatencyTotal (weight-avg k (get idk->comp-avg k))\n@@ -596,22 +610,6 @@\n                     vals\n                     sum)})}))\n \n-(defn apply-default\n-  [f defaulting-fn & args]\n-  (apply f (map defaulting-fn args)))\n-\n-(defn apply-or-0\n-  [f & args]\n-  (apply apply-default f #(or % 0) args))\n-\n-(defn sum-or-0\n-  [& args]\n-  (apply apply-or-0 + args))\n-\n-(defn max-or-0\n-  [& args]\n-  (apply apply-or-0 max args))\n-\n (defn merge-agg-comp-stats-comp-page-bolt\n   [{acc-in :cid+sid->input-stats\n     acc-out :sid->output-stats", "filename": "storm-core/src/clj/backtype/storm/stats.clj"}, {"additions": 8, "raw_url": "https://github.com/apache/storm/raw/1308f89dc7316ea8b1483136cd5ca1209790ef81/storm-core/src/jvm/backtype/storm/metric/internal/LatencyStatAndMetric.java", "blob_url": "https://github.com/apache/storm/blob/1308f89dc7316ea8b1483136cd5ca1209790ef81/storm-core/src/jvm/backtype/storm/metric/internal/LatencyStatAndMetric.java", "sha": "614f95ef24d11f25573b8c379fdeabb228cdb961", "changes": 13, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/backtype/storm/metric/internal/LatencyStatAndMetric.java?ref=1308f89dc7316ea8b1483136cd5ca1209790ef81", "patch": "@@ -19,11 +19,10 @@\n \n import java.util.Map;\n import java.util.HashMap;\n-import java.util.Timer;\n import java.util.TimerTask;\n-import java.util.concurrent.atomic.AtomicLong;\n \n import backtype.storm.metric.api.IMetric;\n+import backtype.storm.utils.Utils;\n \n /**\n  * Acts as a Latency Metric, but also keeps track of approximate latency\n@@ -145,7 +144,9 @@ synchronized Object getValueAndReset(long now) {\n         }\n \n         long timeSpent = now - _bucketStart;\n-        double ret = ((double)(lat + _exactExtraLat))/(count + _exactExtraCount);\n+        long exactExtraCountSum = count + _exactExtraCount;\n+        double ret = Utils.zeroIfNaNOrInf(\n+                ((double) (lat + _exactExtraLat)) / exactExtraCountSum);\n         _bucketStart = now;\n         _exactExtraLat = 0;\n         _exactExtraCount = 0;\n@@ -227,7 +228,9 @@ private synchronized void rotate(long lat, long count, long timeSpent, long targ\n         ret.put(\"600\", readApproximateLatAvg(lat, count, timeSpent, _tmTime, _tmLatBuckets, _tmCountBuckets, 600 * 1000));\n         ret.put(\"10800\", readApproximateLatAvg(lat, count, timeSpent, _thTime, _thLatBuckets, _thCountBuckets, 10800 * 1000));\n         ret.put(\"86400\", readApproximateLatAvg(lat, count, timeSpent, _odTime, _odLatBuckets, _odCountBuckets, 86400 * 1000));\n-        ret.put(\":all-time\", ((double)lat + _allTimeLat)/(count + _allTimeCount));\n+        long allTimeCountSum = count + _allTimeCount;\n+        ret.put(\":all-time\", Utils.zeroIfNaNOrInf(\n+                (double) lat + _allTimeLat)/allTimeCountSum);\n         return ret;\n     }\n \n@@ -242,7 +245,7 @@ private synchronized void rotate(long lat, long count, long timeSpent, long targ\n             totalCount += countBuckets[i];\n             timeNeeded -= bucketTime[i];\n         }\n-        return ((double)totalLat)/totalCount;\n+        return Utils.zeroIfNaNOrInf(((double) totalLat) / totalCount);\n     }\n \n     public void close() {", "filename": "storm-core/src/jvm/backtype/storm/metric/internal/LatencyStatAndMetric.java"}, {"additions": 4, "raw_url": "https://github.com/apache/storm/raw/1308f89dc7316ea8b1483136cd5ca1209790ef81/storm-core/src/jvm/backtype/storm/utils/Utils.java", "blob_url": "https://github.com/apache/storm/blob/1308f89dc7316ea8b1483136cd5ca1209790ef81/storm-core/src/jvm/backtype/storm/utils/Utils.java", "sha": "00af36736fae1b905e3ae94cc6e2adee538eff38", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/backtype/storm/utils/Utils.java?ref=1308f89dc7316ea8b1483136cd5ca1209790ef81", "patch": "@@ -760,5 +760,9 @@ public static long zipFileSize(File myFile) throws IOException{\n         raf.close();\n         return val;\n     }\n+\n+    public static double zeroIfNaNOrInf(double x) {\n+        return (Double.isNaN(x) || Double.isInfinite(x)) ? 0.0 : x;\n+    }\n }\n ", "filename": "storm-core/src/jvm/backtype/storm/utils/Utils.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/01f7a02bd0971eb40322d4ab63fd8bc948192544", "parent": "https://github.com/apache/storm/commit/97608671d59c8d8267822c62481e4a40963834c8", "message": "external/storm-kafka: avoid NPE on null message payloads", "bug_id": "storm_46", "file": [{"additions": 3, "raw_url": "https://github.com/apache/storm/raw/01f7a02bd0971eb40322d4ab63fd8bc948192544/external/storm-kafka/src/jvm/storm/kafka/KafkaUtils.java", "blob_url": "https://github.com/apache/storm/blob/01f7a02bd0971eb40322d4ab63fd8bc948192544/external/storm-kafka/src/jvm/storm/kafka/KafkaUtils.java", "sha": "4b9d11336c5672a652281549a8bdfd41b37e7a3e", "changes": 3, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-kafka/src/jvm/storm/kafka/KafkaUtils.java?ref=01f7a02bd0971eb40322d4ab63fd8bc948192544", "patch": "@@ -202,6 +202,9 @@ public static ByteBufferMessageSet fetchMessages(KafkaConfig config, SimpleConsu\n     public static Iterable<List<Object>> generateTuples(KafkaConfig kafkaConfig, Message msg) {\n         Iterable<List<Object>> tups;\n         ByteBuffer payload = msg.payload();\n+        if (payload == null) {\n+            return null;\n+        }\n         ByteBuffer key = msg.key();\n         if (key != null && kafkaConfig.scheme instanceof KeyValueSchemeAsMultiScheme) {\n             tups = ((KeyValueSchemeAsMultiScheme) kafkaConfig.scheme).deserializeKeyAndValue(Utils.toByteArray(key), Utils.toByteArray(payload));", "filename": "external/storm-kafka/src/jvm/storm/kafka/KafkaUtils.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/6a0e86d6092ac32a842b7ac676d4e584f615584f", "parent": "https://github.com/apache/storm/commit/8fc0b92b168a95624a7d43eee4225192e65110bb", "message": "Merge branch 'storm1587' of https://github.com/kishorvpatil/incubator-storm into STORM-1587\n\nSTORM-1587: Avoid NPE while prining Metrics", "bug_id": "storm_47", "file": [{"additions": 1, "raw_url": "https://github.com/apache/storm/raw/6a0e86d6092ac32a842b7ac676d4e584f615584f/examples/storm-starter/src/jvm/org/apache/storm/starter/ThroughputVsLatency.java", "blob_url": "https://github.com/apache/storm/blob/6a0e86d6092ac32a842b7ac676d4e584f615584f/examples/storm-starter/src/jvm/org/apache/storm/starter/ThroughputVsLatency.java", "sha": "8ecfb3a1b032d5569f927895cad97971654ae450", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/examples/storm-starter/src/jvm/org/apache/storm/starter/ThroughputVsLatency.java?ref=6a0e86d6092ac32a842b7ac676d4e584f615584f", "patch": "@@ -273,7 +273,7 @@ public static void printMetrics(C client, String name) throws Exception {\n     long acked = 0;\n     long failed = 0;\n     for (ExecutorSummary exec: info.get_executors()) {\n-      if (\"spout\".equals(exec.get_component_id())) {\n+      if (\"spout\".equals(exec.get_component_id()) && exec.get_stats() != null && exec.get_stats().get_specific() != null) {\n         SpoutStats stats = exec.get_stats().get_specific().get_spout();\n         Map<String, Long> failedMap = stats.get_failed().get(\":all-time\");\n         Map<String, Long> ackedMap = stats.get_acked().get(\":all-time\");", "filename": "examples/storm-starter/src/jvm/org/apache/storm/starter/ThroughputVsLatency.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/2e44c9e6a2fb4382950da463fad9f392f8991468", "parent": "https://github.com/apache/storm/commit/11b01518f0935dedfd71d09b697d7189a29b91b5", "message": "when a resource can't be found, log an error and exit rather than throw NPE", "bug_id": "storm_48", "file": [{"additions": 4, "raw_url": "https://github.com/apache/storm/raw/2e44c9e6a2fb4382950da463fad9f392f8991468/flux-core/src/main/java/org/apache/storm/flux/parser/FluxParser.java", "blob_url": "https://github.com/apache/storm/blob/2e44c9e6a2fb4382950da463fad9f392f8991468/flux-core/src/main/java/org/apache/storm/flux/parser/FluxParser.java", "sha": "72f8a8e6cdbe9c2b224841fbf20feb0b6ca7fae4", "changes": 4, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/flux-core/src/main/java/org/apache/storm/flux/parser/FluxParser.java?ref=2e44c9e6a2fb4382950da463fad9f392f8991468", "patch": "@@ -63,6 +63,10 @@ public static TopologyDef parseResource(String resource, boolean dumpYaml, boole\n                                             String propertiesFile, boolean envSub) throws IOException {\n         Yaml yaml = yaml();\n         InputStream in = FluxParser.class.getResourceAsStream(resource);\n+        if(in == null){\n+            LOG.error(\"Unable to load classpath resource: \" + resource);\n+            System.exit(1);\n+        }\n         TopologyDef topology = loadYaml(yaml, in, propertiesFile, envSub);\n         in.close();\n         if(dumpYaml){", "filename": "flux-core/src/main/java/org/apache/storm/flux/parser/FluxParser.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/b1360fd7cb58c24199439d50ae54bba9b3c393d3", "parent": "https://github.com/apache/storm/commit/54cf35c0b387e21445e0e860b1304f5af178fb50", "message": "fix NPE when no input fields given for aggregator", "bug_id": "storm_49", "file": [{"additions": 2, "raw_url": "https://github.com/apache/storm/raw/b1360fd7cb58c24199439d50ae54bba9b3c393d3/CHANGELOG.md", "blob_url": "https://github.com/apache/storm/blob/b1360fd7cb58c24199439d50ae54bba9b3c393d3/CHANGELOG.md", "sha": "4c94948d2ed7f1658441f64e257e563e55bfa42e", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/CHANGELOG.md?ref=b1360fd7cb58c24199439d50ae54bba9b3c393d3", "patch": "@@ -3,9 +3,11 @@\n  * Changed debug level of \"Failed message\" logging to DEBUG\n  * Deprecated LinearDRPCTopologyBuilder, TimeCacheMap, and transactional topologies\n  * During \"storm jar\", whether topology is already running or not is checked before submitting jar to save time (thanks jasonjckn)\n+ * Added BaseMultiReducer class to Trident that provides empty implementations of prepare and cleanup\n  * Bug fix: When an item is consumed off an internal buffer, the entry on the buffer is nulled to allow GC to happen on that data\n  * Bug fix: Helper class for Trident MapStates now clear their read cache when a new commit happens, preventing updates from spilling over from a failed batch attempt to the next attempt\n  * Bug fix: Fix NonTransactionalMap to take in an IBackingMap for regular values rather than TransactionalValue (thanks sjoerdmulder)\n+ * Bug fix: Fix NPE when no input fields given for regular Aggregator\n \n ## 0.8.0\n ", "filename": "CHANGELOG.md"}, {"additions": 9, "raw_url": "https://github.com/apache/storm/raw/b1360fd7cb58c24199439d50ae54bba9b3c393d3/src/jvm/storm/trident/fluent/ChainedAggregatorDeclarer.java", "blob_url": "https://github.com/apache/storm/blob/b1360fd7cb58c24199439d50ae54bba9b3c393d3/src/jvm/storm/trident/fluent/ChainedAggregatorDeclarer.java", "sha": "de8fe9c0109cd008e259964bdc19df79626db915", "changes": 13, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/storm/contents/src/jvm/storm/trident/fluent/ChainedAggregatorDeclarer.java?ref=b1360fd7cb58c24199439d50ae54bba9b3c393d3", "patch": "@@ -60,11 +60,16 @@ public Stream chainEnd() {\n         Set<String> allInFields = new HashSet<String>();\n         for(int i=0; i<_aggs.size(); i++) {\n             AggSpec spec = _aggs.get(i);\n-            inputFields[i] = spec.inFields;\n+            Fields infields = spec.inFields;\n+            if(infields==null) infields = new Fields();\n+            Fields outfields = spec.outFields;\n+            if(outfields==null) outfields = new Fields();\n+\n+            inputFields[i] = infields;\n             aggs[i] = spec.agg;\n-            outSizes[i] = spec.outFields.size();  \n-            allOutFields.addAll(spec.outFields.toList());\n-            allInFields.addAll(spec.inFields.toList());\n+            outSizes[i] = outfields.size();  \n+            allOutFields.addAll(outfields.toList());\n+            allInFields.addAll(infields.toList());\n         }\n         if(new HashSet(allOutFields).size() != allOutFields.size()) {\n             throw new IllegalArgumentException(\"Output fields for chained aggregators must be distinct: \" + allOutFields.toString());", "filename": "src/jvm/storm/trident/fluent/ChainedAggregatorDeclarer.java"}, {"additions": 30, "raw_url": "https://github.com/apache/storm/raw/b1360fd7cb58c24199439d50ae54bba9b3c393d3/src/jvm/storm/trident/testing/CountAsAggregator.java", "blob_url": "https://github.com/apache/storm/blob/b1360fd7cb58c24199439d50ae54bba9b3c393d3/src/jvm/storm/trident/testing/CountAsAggregator.java", "sha": "52f482f0c16e47ebce83b29a2171d63e7140eeb7", "changes": 30, "status": "added", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/src/jvm/storm/trident/testing/CountAsAggregator.java?ref=b1360fd7cb58c24199439d50ae54bba9b3c393d3", "patch": "@@ -0,0 +1,30 @@\n+package storm.trident.testing;\n+\n+import backtype.storm.tuple.Values;\n+import storm.trident.operation.BaseAggregator;\n+import storm.trident.operation.TridentCollector;\n+import storm.trident.tuple.TridentTuple;\n+\n+\n+public class CountAsAggregator extends BaseAggregator<CountAsAggregator.State> {\n+\n+    static class State {\n+        long count = 0;\n+    }\n+    \n+    @Override\n+    public State init(Object batchId, TridentCollector collector) {\n+        return new State();\n+    }\n+\n+    @Override\n+    public void aggregate(State state, TridentTuple tuple, TridentCollector collector) {\n+        state.count++;\n+    }\n+\n+    @Override\n+    public void complete(State state, TridentCollector collector) {\n+        collector.emit(new Values(state.count));\n+    }\n+    \n+}", "filename": "src/jvm/storm/trident/testing/CountAsAggregator.java"}, {"additions": 3, "raw_url": "https://github.com/apache/storm/raw/b1360fd7cb58c24199439d50ae54bba9b3c393d3/src/jvm/storm/trident/testing/Split.java", "blob_url": "https://github.com/apache/storm/blob/b1360fd7cb58c24199439d50ae54bba9b3c393d3/src/jvm/storm/trident/testing/Split.java", "sha": "65cdb8bfb965bb75227122ed6cfec030b912976f", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/src/jvm/storm/trident/testing/Split.java?ref=b1360fd7cb58c24199439d50ae54bba9b3c393d3", "patch": "@@ -10,7 +10,9 @@\n     @Override\n     public void execute(TridentTuple tuple, TridentCollector collector) {\n         for(String word: tuple.getString(0).split(\" \")) {\n-            collector.emit(new Values(word));\n+            if(word.length() > 0) {\n+                collector.emit(new Values(word));\n+            }\n         }\n     }\n     ", "filename": "src/jvm/storm/trident/testing/Split.java"}, {"additions": 18, "raw_url": "https://github.com/apache/storm/raw/b1360fd7cb58c24199439d50ae54bba9b3c393d3/test/clj/storm/trident/integration_test.clj", "blob_url": "https://github.com/apache/storm/blob/b1360fd7cb58c24199439d50ae54bba9b3c393d3/test/clj/storm/trident/integration_test.clj", "sha": "2e63d83257ebffb38ccf13519eef17502379fbdb", "changes": 19, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/test/clj/storm/trident/integration_test.clj?ref=b1360fd7cb58c24199439d50ae54bba9b3c393d3", "patch": "@@ -1,7 +1,7 @@\n (ns storm.trident.integration-test\n   (:use [clojure test])\n   (:require [backtype.storm [testing :as t]])\n-  (:import [storm.trident.testing Split])\n+  (:import [storm.trident.testing Split CountAsAggregator])\n   (:use [storm.trident testing])\n   (:use [backtype.storm util]))\n   \n@@ -37,3 +37,20 @@\n           (is (= [[2]] (exec-drpc drpc \"words\" \"man\")))\n           (is (= [[8]] (exec-drpc drpc \"words\" \"man where you the\")))\n           )))))\n+\n+\n+(deftest test-count-agg\n+  (t/with-local-cluster [cluster]\n+    (with-drpc [drpc]\n+      (letlocals\n+        (bind topo (TridentTopology.))\n+        (-> topo\n+            (.newDRPCStream \"numwords\" drpc)\n+            (.each (fields \"args\") (Split.) (fields \"word\"))\n+            (.aggregate (CountAsAggregator.) (fields \"count\"))\n+            (.project (fields \"count\")))\n+        (with-topology [cluster topo]\n+          (is (= [[1]] (exec-drpc drpc \"numwords\" \"the\")))\n+          (is (= [[0]] (exec-drpc drpc \"numwords\" \"\")))\n+          (is (= [[8]] (exec-drpc drpc \"numwords\" \"1 2 3 4 5 6 7 8\")))\n+          )))))\n\\ No newline at end of file", "filename": "test/clj/storm/trident/integration_test.clj"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/ef059451029d0a155451c5fea7cca7dbaf102140", "parent": "https://github.com/apache/storm/commit/7db6aa84076823939a7493218a4c310c226cc983", "message": "Merge pull request #234 from xumingming/issue233\n\nFix TransactionalTopologyBuilder NPE", "bug_id": "storm_50", "file": [{"additions": 1, "raw_url": "https://github.com/apache/storm/raw/ef059451029d0a155451c5fea7cca7dbaf102140/src/jvm/backtype/storm/transactional/TransactionalTopologyBuilder.java", "blob_url": "https://github.com/apache/storm/blob/ef059451029d0a155451c5fea7cca7dbaf102140/src/jvm/backtype/storm/transactional/TransactionalTopologyBuilder.java", "sha": "721a7d47e00c6234d97fab02859a4d1232155e3f", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/src/jvm/backtype/storm/transactional/TransactionalTopologyBuilder.java?ref=ef059451029d0a155451c5fea7cca7dbaf102140", "patch": "@@ -52,7 +52,7 @@ public TransactionalTopologyBuilder(String id, String spoutId, ITransactionalSpo\n         _id = id;\n         _spoutId = spoutId;\n         _spout = spout;\n-        _spoutParallelism = spoutParallelism.intValue();\n+        _spoutParallelism = (spoutParallelism == null) ? null : spoutParallelism.intValue();\n     }\n     \n     public TransactionalTopologyBuilder(String id, String spoutId, ITransactionalSpout spout) {", "filename": "src/jvm/backtype/storm/transactional/TransactionalTopologyBuilder.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/9d6761efb72f372d44e36088b0ddc51a38a00dfa", "parent": "https://github.com/apache/storm/commit/b10ebde2a514491df60b746234061fbc04e742b2", "message": "fix npe when component configuration for spout is null", "bug_id": "storm_51", "file": [{"additions": 1, "raw_url": "https://github.com/apache/storm/raw/9d6761efb72f372d44e36088b0ddc51a38a00dfa/src/jvm/backtype/storm/drpc/DRPCSpout.java", "blob_url": "https://github.com/apache/storm/blob/9d6761efb72f372d44e36088b0ddc51a38a00dfa/src/jvm/backtype/storm/drpc/DRPCSpout.java", "sha": "d8aa512c17e771001bf15df16ed51efea1585e06", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/src/jvm/backtype/storm/drpc/DRPCSpout.java?ref=9d6761efb72f372d44e36088b0ddc51a38a00dfa", "patch": "@@ -149,6 +149,6 @@ public void declareOutputFields(OutputFieldsDeclarer declarer) {\n \n     @Override\n     public Map<String, Object> getComponentConfiguration() {\n-        return new HashMap<String, Object>();\n+        return null;\n     }\n }", "filename": "src/jvm/backtype/storm/drpc/DRPCSpout.java"}, {"additions": 7, "raw_url": "https://github.com/apache/storm/raw/9d6761efb72f372d44e36088b0ddc51a38a00dfa/src/jvm/backtype/storm/task/TopologyContext.java", "blob_url": "https://github.com/apache/storm/blob/9d6761efb72f372d44e36088b0ddc51a38a00dfa/src/jvm/backtype/storm/task/TopologyContext.java", "sha": "667d36eeba6be5dfd86d3714e42fdb7e26c28433", "changes": 11, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/storm/contents/src/jvm/backtype/storm/task/TopologyContext.java?ref=9d6761efb72f372d44e36088b0ddc51a38a00dfa", "patch": "@@ -332,10 +332,13 @@ public int maxTopologyMessageTimeout(Map<String, Object> topologyConfig) {\n         Integer max = Utils.getInt(topologyConfig.get(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS));\n         for(String spout: getRawTopology().get_spouts().keySet()) {\n             ComponentCommon common = getComponentCommon(spout);\n-            Map conf = (Map) JSONValue.parse(common.get_json_conf());\n-            Object comp = conf.get(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS);\n-            if(comp!=null) {\n-                max = Math.max(Utils.getInt(comp), max);\n+            String jsonConf = common.get_json_conf();\n+            if(jsonConf!=null) {\n+                Map conf = (Map) JSONValue.parse(jsonConf);\n+                Object comp = conf.get(Config.TOPOLOGY_MESSAGE_TIMEOUT_SECS);\n+                if(comp!=null) {\n+                    max = Math.max(Utils.getInt(comp), max);\n+                }\n             }\n         }\n         return max;", "filename": "src/jvm/backtype/storm/task/TopologyContext.java"}, {"additions": 4, "raw_url": "https://github.com/apache/storm/raw/9d6761efb72f372d44e36088b0ddc51a38a00dfa/src/jvm/backtype/storm/testing/TestWordSpout.java", "blob_url": "https://github.com/apache/storm/blob/9d6761efb72f372d44e36088b0ddc51a38a00dfa/src/jvm/backtype/storm/testing/TestWordSpout.java", "sha": "68560a1cc8e5f1f45367bc71ad0933ee5234cbd4", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/storm/contents/src/jvm/backtype/storm/testing/TestWordSpout.java?ref=9d6761efb72f372d44e36088b0ddc51a38a00dfa", "patch": "@@ -57,10 +57,12 @@ public void declareOutputFields(OutputFieldsDeclarer declarer) {\n \n     @Override\n     public Map<String, Object> getComponentConfiguration() {\n-        Map<String, Object> ret = new HashMap<String, Object>();\n         if(!_isDistributed) {\n+            Map<String, Object> ret = new HashMap<String, Object>();\n             ret.put(Config.TOPOLOGY_MAX_TASK_PARALLELISM, 1);\n+            return ret;\n+        } else {\n+            return null;\n         }\n-        return ret;\n     }    \n }\n\\ No newline at end of file", "filename": "src/jvm/backtype/storm/testing/TestWordSpout.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/4d2804aacb0a0abb60b5760a2f8bb3eb657ab67b", "parent": "https://github.com/apache/storm/commit/401ebebdf8b201b87ff944cc3c2217cc30acd1fc", "message": "Resolve NPE that can occur if there is no SourceComponent in a Tuple", "bug_id": "storm_52", "file": [{"additions": 2, "raw_url": "https://github.com/apache/storm/raw/4d2804aacb0a0abb60b5760a2f8bb3eb657ab67b/storm-core/src/jvm/backtype/storm/tuple/TupleImpl.java", "blob_url": "https://github.com/apache/storm/blob/4d2804aacb0a0abb60b5760a2f8bb3eb657ab67b/storm-core/src/jvm/backtype/storm/tuple/TupleImpl.java", "sha": "40ad11c3c78413361d83079d0d92860b441c7370", "changes": 4, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/backtype/storm/tuple/TupleImpl.java?ref=4d2804aacb0a0abb60b5760a2f8bb3eb657ab67b", "patch": "@@ -215,8 +215,8 @@ public String getSourceStreamId() {\n     }\n \n     public boolean isTick() {\n-        return this.getSourceComponent().equals(Constants.SYSTEM_COMPONENT_ID) &&\n-               this.getSourceStreamId().equals(Constants.SYSTEM_TICK_STREAM_ID);\n+        return Constants.SYSTEM_COMPONENT_ID.equals(this.getSourceComponent()) &&\n+               Constants.SYSTEM_TICK_STREAM_ID.equals(this.getSourceStreamId());\n     }\n \n     public MessageId getMessageId() {", "filename": "storm-core/src/jvm/backtype/storm/tuple/TupleImpl.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/14477f4412f4adfaf42367e8a697e1a17b7dfb8e", "parent": "https://github.com/apache/storm/commit/a7c83108a77a2d04dfc0e43ade235b0db2921a29", "message": "Log \"task is null\" instead of let worker died\n\n* when task is null in transfer-fn, creating TaskMessage leads NPE", "bug_id": "storm_53", "file": [{"additions": 4, "raw_url": "https://github.com/apache/storm/raw/14477f4412f4adfaf42367e8a697e1a17b7dfb8e/storm-core/src/clj/backtype/storm/daemon/worker.clj", "blob_url": "https://github.com/apache/storm/blob/14477f4412f4adfaf42367e8a697e1a17b7dfb8e/storm-core/src/clj/backtype/storm/daemon/worker.clj", "sha": "c4afcd94e05d4f34a3b41d7817883222d91cbba2", "changes": 6, "status": "modified", "deletions": 2, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/clj/backtype/storm/daemon/worker.clj?ref=14477f4412f4adfaf42367e8a697e1a17b7dfb8e", "patch": "@@ -133,8 +133,10 @@\n                     (when (not (.get remoteMap node+port))\n                       (.put remoteMap node+port (ArrayList.)))\n                     (let [remote (.get remoteMap node+port)]\n-                      (.add remote (TaskMessage. task (.serialize serializer tuple)))\n-                     )))) \n+                      (if (not-nil? task)\n+                        (.add remote (TaskMessage. task (.serialize serializer tuple)))\n+                        (log-warn \"Can't transfer tuple - task value is null. tuple information: \" tuple))\n+                     ))))\n                 (local-transfer local)\n                 (disruptor/publish transfer-queue remoteMap)\n               ))]", "filename": "storm-core/src/clj/backtype/storm/daemon/worker.clj"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/1a83267f5cacdff7db3934e3b0f00123357e2a4d", "parent": "https://github.com/apache/storm/commit/71d615b7cc9a96b6667b976a25dc86ef54a66169", "message": "Merge branch '0223' of https://github.com/hustfxj/storm into STORM-1572\n\nSTORM-1572: throw NPE when parsing the command line arguments by CLI", "bug_id": "storm_54", "file": [{"additions": 6, "raw_url": "https://github.com/apache/storm/raw/1a83267f5cacdff7db3934e3b0f00123357e2a4d/storm-core/src/jvm/org/apache/storm/command/CLI.java", "blob_url": "https://github.com/apache/storm/blob/1a83267f5cacdff7db3934e3b0f00123357e2a4d/storm-core/src/jvm/org/apache/storm/command/CLI.java", "sha": "2bad836afafe14c41dd48aef2841ea9ae8e6b0c8", "changes": 9, "status": "modified", "deletions": 3, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/org/apache/storm/command/CLI.java?ref=1a83267f5cacdff7db3934e3b0f00123357e2a4d", "patch": "@@ -238,10 +238,13 @@ public CLIBuilder arg(String name, Parse parse, Assoc assoc) {\n             DefaultParser parser = new DefaultParser();\n             CommandLine cl = parser.parse(options, rawArgs);\n             HashMap<String, Object> ret = new HashMap<>();\n-            for (Opt opt: opts) {\n+            for (Opt opt : opts) {\n                 Object current = null;\n-                for (String val: cl.getOptionValues(opt.shortName)) {\n-                    current = opt.process(current, val);\n+                String[] strings = cl.getOptionValues(opt.shortName);\n+                if (strings != null) {\n+                    for (String val : cl.getOptionValues(opt.shortName)) {\n+                        current = opt.process(current, val);\n+                    }\n                 }\n                 if (current == null) {\n                     current = opt.defaultValue;", "filename": "storm-core/src/jvm/org/apache/storm/command/CLI.java"}, {"additions": 3, "raw_url": "https://github.com/apache/storm/raw/1a83267f5cacdff7db3934e3b0f00123357e2a4d/storm-core/test/jvm/org/apache/storm/command/TestCLI.java", "blob_url": "https://github.com/apache/storm/blob/1a83267f5cacdff7db3934e3b0f00123357e2a4d/storm-core/test/jvm/org/apache/storm/command/TestCLI.java", "sha": "5b2f220d0bedd67833a9c28858bd4ecf35844c58", "changes": 4, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/test/jvm/org/apache/storm/command/TestCLI.java?ref=1a83267f5cacdff7db3934e3b0f00123357e2a4d", "patch": "@@ -32,13 +32,15 @@ public void testSimple() throws Exception {\n            .opt(\"b\", \"bb\", 1, CLI.AS_INT)\n            .opt(\"c\", \"cc\", 1, CLI.AS_INT, CLI.FIRST_WINS)\n            .opt(\"d\", \"dd\", null, CLI.AS_STRING, CLI.INTO_LIST)\n+           .opt(\"e\", \"ee\", null, CLI.AS_INT)\n            .arg(\"A\")\n            .arg(\"B\", CLI.AS_INT)\n            .parse(\"-a100\", \"--aa\", \"200\", \"-c2\", \"-b\", \"50\", \"--cc\", \"100\", \"A-VALUE\", \"1\", \"2\", \"3\", \"-b40\", \"-d1\", \"-d2\", \"-d3\");\n-        assertEquals(6, values.size());\n+        assertEquals(7, values.size());\n         assertEquals(\"200\", (String)values.get(\"a\"));\n         assertEquals((Integer)40, (Integer)values.get(\"b\"));\n         assertEquals((Integer)2, (Integer)values.get(\"c\"));\n+        assertEquals(null, values.get(\"e\"));\n \n         List<String> d = (List<String>)values.get(\"d\");\n         assertEquals(3, d.size());", "filename": "storm-core/test/jvm/org/apache/storm/command/TestCLI.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/47ce0ae97830380eb85db7ae97a7f7f3adbff820", "parent": "https://github.com/apache/storm/commit/4bf2de58f0972b731600269cd419c1a3d1869458", "message": "fixed NPE in MaxMetric. bumped version 0.9.0-wip16a-scala292", "bug_id": "storm_55", "file": [{"additions": 1, "raw_url": "https://github.com/apache/storm/raw/47ce0ae97830380eb85db7ae97a7f7f3adbff820/project.clj", "blob_url": "https://github.com/apache/storm/blob/47ce0ae97830380eb85db7ae97a7f7f3adbff820/project.clj", "sha": "e5ac888baa804bfcb32e1030029e9a61001e11b0", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/project.clj?ref=47ce0ae97830380eb85db7ae97a7f7f3adbff820", "patch": "@@ -1,4 +1,4 @@\n-(defproject storm/storm-kafka \"0.9.0-wip16-scala292\"\n+(defproject storm/storm-kafka \"0.9.0-wip16a-scala292\"\n   :java-source-paths [\"src/jvm\"]\n   :repositories {\"scala-tools\" \"http://scala-tools.org/repo-releases\"\n                   \"conjars\" \"http://conjars.org/repo/\"}", "filename": "project.clj"}, {"additions": 2, "raw_url": "https://github.com/apache/storm/raw/47ce0ae97830380eb85db7ae97a7f7f3adbff820/src/jvm/storm/kafka/trident/MaxMetric.java", "blob_url": "https://github.com/apache/storm/blob/47ce0ae97830380eb85db7ae97a7f7f3adbff820/src/jvm/storm/kafka/trident/MaxMetric.java", "sha": "9c5656af173231a8a5f7acf49b33de505a768065", "changes": 2, "status": "modified", "deletions": 0, "contents_url": "https://api.github.com/repos/apache/storm/contents/src/jvm/storm/kafka/trident/MaxMetric.java?ref=47ce0ae97830380eb85db7ae97a7f7f3adbff820", "patch": "@@ -12,6 +12,8 @@ public Long identity() {\n \n     @Override\n     public Long combine(Long l1, Long l2) {\n+        if(l1 == null) return l2;\n+        if(l2 == null) return l1;\n         return Math.max(l1, l2);\n     }\n ", "filename": "src/jvm/storm/kafka/trident/MaxMetric.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/6584476f7d26dfe11498a56e99d1488641d1d482", "parent": "https://github.com/apache/storm/commit/b21fd1447ebe02114cbef5d325b607cb5798d6a1", "message": "fix the issue: \"Timeouts in CoordinatedBolt can cause NPE later on\"", "bug_id": "storm_56", "file": [{"additions": 13, "raw_url": "https://github.com/apache/storm/raw/6584476f7d26dfe11498a56e99d1488641d1d482/src/jvm/backtype/storm/coordination/CoordinatedBolt.java", "blob_url": "https://github.com/apache/storm/blob/6584476f7d26dfe11498a56e99d1488641d1d482/src/jvm/backtype/storm/coordination/CoordinatedBolt.java", "sha": "3a8cc7de9dccd6f3692dc84dc5fac0fbc7a27eee", "changes": 19, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/storm/contents/src/jvm/backtype/storm/coordination/CoordinatedBolt.java?ref=6584476f7d26dfe11498a56e99d1488641d1d482", "patch": "@@ -85,7 +85,9 @@ public void emitDirect(int task, String stream, Collection<Tuple> anchors, List<\n         public void ack(Tuple tuple) {\n             Object id = tuple.getValue(0);\n             synchronized(_tracked) {\n-                _tracked.get(id).receivedTuples++;\n+                TrackingInfo track = _tracked.get(id);\n+                if (track != null)\n+                    track.receivedTuples++;\n             }\n             boolean failed = checkFinishId(tuple);\n             if(failed) {\n@@ -98,7 +100,9 @@ public void ack(Tuple tuple) {\n         public void fail(Tuple tuple) {\n             Object id = tuple.getValue(0);\n             synchronized(_tracked) {\n-                _tracked.get(id).failed = true;\n+                TrackingInfo track = _tracked.get(id);\n+                if (track != null)\n+                    track.failed = true;\n             }\n             _delegate.fail(tuple);\n         }\n@@ -110,10 +114,13 @@ public void reportError(Throwable error) {\n \n         private void updateTaskCounts(Object id, List<Integer> tasks) {\n             synchronized(_tracked) {\n-                Map<Integer, Integer> taskEmittedTuples = _tracked.get(id).taskEmittedTuples;\n-                for(Integer task: tasks) {\n-                    int newCount = get(taskEmittedTuples, task, 0) + 1;\n-                    taskEmittedTuples.put(task, newCount);\n+                TrackingInfo track = _tracked.get(id);\n+                if (track != null) {\n+                    Map<Integer, Integer> taskEmittedTuples = track.taskEmittedTuples;\n+                    for(Integer task: tasks) {\n+                        int newCount = get(taskEmittedTuples, task, 0) + 1;\n+                        taskEmittedTuples.put(task, newCount);\n+                    }\n                 }\n             }\n         }", "filename": "src/jvm/backtype/storm/coordination/CoordinatedBolt.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/6408063df31a758a70bf06e67dda7cf7ec744cb0", "parent": "https://github.com/apache/storm/commit/415654dcc5c6301d05df95ba9ab15d9bb03f95ea", "message": "Merge branch 'STORM-1108' of https://github.com/revans2/incubator-storm into STORM-1108\n\nSTORM-1108: Fix NPE in simulated time", "bug_id": "storm_57", "file": [{"additions": 11, "raw_url": "https://github.com/apache/storm/raw/6408063df31a758a70bf06e67dda7cf7ec744cb0/storm-core/src/jvm/backtype/storm/utils/Time.java", "blob_url": "https://github.com/apache/storm/blob/6408063df31a758a70bf06e67dda7cf7ec744cb0/storm-core/src/jvm/backtype/storm/utils/Time.java", "sha": "6af7185ac350f2875e80493bb88630b80ff985d3", "changes": 12, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/backtype/storm/utils/Time.java?ref=6408063df31a758a70bf06e67dda7cf7ec744cb0", "patch": "@@ -58,14 +58,24 @@ public static void sleepUntil(long targetTimeMs) throws InterruptedException {\n         if(simulating.get()) {\n             try {\n                 synchronized(sleepTimesLock) {\n+                    if (threadSleepTimes == null) {\n+                        LOG.debug(\"{} is still sleeping after simulated time disabled.\", Thread.currentThread(), new RuntimeException(\"STACK TRACE\"));\n+                        throw new InterruptedException();\n+                    }\n                     threadSleepTimes.put(Thread.currentThread(), new AtomicLong(targetTimeMs));\n                 }\n                 while(simulatedCurrTimeMs.get() < targetTimeMs) {\n+                    synchronized(sleepTimesLock) {\n+                        if (threadSleepTimes == null) {\n+                            LOG.debug(\"{} is still sleeping after simulated time disabled.\", Thread.currentThread(), new RuntimeException(\"STACK TRACE\"));\n+                            throw new InterruptedException();\n+                        }\n+                    }\n                     Thread.sleep(10);\n                 }\n             } finally {\n                 synchronized(sleepTimesLock) {\n-                    if (simulating.get()) {\n+                    if (simulating.get() && threadSleepTimes != null) {\n                         threadSleepTimes.remove(Thread.currentThread());\n                     }\n                 }", "filename": "storm-core/src/jvm/backtype/storm/utils/Time.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/e55f67a58210461abbbdb1318c3b2a73b448a645", "parent": "https://github.com/apache/storm/commit/c38d7950a347e906a8ebe7b2889c65c6e8e8c936", "message": "STORM-2481 Upgrade Aether version to resolve Aether bug BUG-451566\n\n* this resolves NPE issue on Aether and helps showing proper error message\n* this also fixes the bug which dependencies root directorys does not exist\n  * after the patch it will create the directory if not found", "bug_id": "storm_58", "file": [{"additions": 1, "raw_url": "https://github.com/apache/storm/raw/e55f67a58210461abbbdb1318c3b2a73b448a645/pom.xml", "blob_url": "https://github.com/apache/storm/blob/e55f67a58210461abbbdb1318c3b2a73b448a645/pom.xml", "sha": "e1a43549e9e6a40570d4b4988ee8963096105bb6", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/pom.xml?ref=e55f67a58210461abbbdb1318c3b2a73b448a645", "patch": "@@ -302,7 +302,7 @@\n         <!-- by default the clojure test set are all clojure tests that are not integration tests. This property is overridden in the profiles -->\n         <clojure.test.set>!integration.*</clojure.test.set>\n \n-        <aetherVersion>1.0.0.v20140518</aetherVersion>\n+        <aetherVersion>1.1.0</aetherVersion>\n         <mavenVersion>3.1.0</mavenVersion>\n         <wagonVersion>1.0</wagonVersion>\n         <qpid.version>0.32</qpid.version>", "filename": "pom.xml"}, {"additions": 53, "raw_url": "https://github.com/apache/storm/raw/e55f67a58210461abbbdb1318c3b2a73b448a645/storm-submit-tools/pom.xml", "blob_url": "https://github.com/apache/storm/blob/e55f67a58210461abbbdb1318c3b2a73b448a645/storm-submit-tools/pom.xml", "sha": "f311de7bb5f77e36952fecf350d65644ea24f95a", "changes": 132, "status": "modified", "deletions": 79, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-submit-tools/pom.xml?ref=e55f67a58210461abbbdb1318c3b2a73b448a645", "patch": "@@ -44,118 +44,86 @@\n \n         <!-- Aether :: maven dependency resolution -->\n         <dependency>\n-            <groupId>org.apache.maven</groupId>\n-            <artifactId>maven-plugin-api</artifactId>\n-            <version>3.0</version>\n-            <exclusions>\n-                <exclusion>\n-                    <groupId>org.codehaus.plexus</groupId>\n-                    <artifactId>plexus-utils</artifactId>\n-                </exclusion>\n-                <exclusion>\n-                    <groupId>org.sonatype.sisu</groupId>\n-                    <artifactId>sisu-inject-plexus</artifactId>\n-                </exclusion>\n-                <exclusion>\n-                    <groupId>org.apache.maven</groupId>\n-                    <artifactId>maven-model</artifactId>\n-                </exclusion>\n-            </exclusions>\n+            <groupId>org.eclipse.aether</groupId>\n+            <artifactId>aether-api</artifactId>\n+            <version>${aetherVersion}</version>\n         </dependency>\n \n         <dependency>\n-            <groupId>org.sonatype.aether</groupId>\n-            <artifactId>aether-api</artifactId>\n-            <version>1.12</version>\n+            <groupId>org.eclipse.aether</groupId>\n+            <artifactId>aether-spi</artifactId>\n+            <version>${aetherVersion}</version>\n         </dependency>\n \n         <dependency>\n-            <groupId>org.sonatype.aether</groupId>\n+            <groupId>org.eclipse.aether</groupId>\n             <artifactId>aether-util</artifactId>\n-            <version>1.12</version>\n+            <version>${aetherVersion}</version>\n         </dependency>\n \n         <dependency>\n-            <groupId>org.sonatype.aether</groupId>\n+            <groupId>org.eclipse.aether</groupId>\n             <artifactId>aether-impl</artifactId>\n-            <version>1.12</version>\n+            <version>${aetherVersion}</version>\n         </dependency>\n \n         <dependency>\n-            <groupId>org.apache.maven</groupId>\n-            <artifactId>maven-aether-provider</artifactId>\n-            <version>3.0.3</version>\n-            <exclusions>\n-                <exclusion>\n-                    <groupId>org.sonatype.aether</groupId>\n-                    <artifactId>aether-api</artifactId>\n-                </exclusion>\n-                <exclusion>\n-                    <groupId>org.sonatype.aether</groupId>\n-                    <artifactId>aether-spi</artifactId>\n-                </exclusion>\n-                <exclusion>\n-                    <groupId>org.sonatype.aether</groupId>\n-                    <artifactId>aether-util</artifactId>\n-                </exclusion>\n-                <exclusion>\n-                    <groupId>org.sonatype.aether</groupId>\n-                    <artifactId>aether-impl</artifactId>\n-                </exclusion>\n-                <exclusion>\n-                    <groupId>org.codehaus.plexus</groupId>\n-                    <artifactId>plexus-utils</artifactId>\n-                </exclusion>\n-            </exclusions>\n+            <groupId>org.eclipse.aether</groupId>\n+            <artifactId>aether-connector-basic</artifactId>\n+            <version>${aetherVersion}</version>\n         </dependency>\n \n         <dependency>\n-            <groupId>org.sonatype.aether</groupId>\n-            <artifactId>aether-connector-file</artifactId>\n-            <version>1.12</version>\n+            <groupId>org.eclipse.aether</groupId>\n+            <artifactId>aether-transport-file</artifactId>\n+            <version>${aetherVersion}</version>\n         </dependency>\n \n         <dependency>\n-            <groupId>org.sonatype.aether</groupId>\n-            <artifactId>aether-connector-wagon</artifactId>\n-            <version>1.12</version>\n-            <exclusions>\n-                <exclusion>\n-                    <groupId>org.apache.maven.wagon</groupId>\n-                    <artifactId>wagon-provider-api</artifactId>\n-                </exclusion>\n-            </exclusions>\n+            <groupId>org.eclipse.aether</groupId>\n+            <artifactId>aether-transport-http</artifactId>\n+            <version>${aetherVersion}</version>\n         </dependency>\n \n         <dependency>\n-            <groupId>org.apache.maven.wagon</groupId>\n-            <artifactId>wagon-provider-api</artifactId>\n-            <version>1.0</version>\n-            <exclusions>\n-                <exclusion>\n-                    <groupId>org.codehaus.plexus</groupId>\n-                    <artifactId>plexus-utils</artifactId>\n-                </exclusion>\n-            </exclusions>\n+            <groupId>org.apache.maven</groupId>\n+            <artifactId>maven-aether-provider</artifactId>\n+            <version>${mavenVersion}</version>\n         </dependency>\n \n         <dependency>\n-            <groupId>org.apache.maven.wagon</groupId>\n-            <artifactId>wagon-http-lightweight</artifactId>\n-            <version>1.0</version>\n+            <groupId>org.codehaus.plexus</groupId>\n+            <artifactId>plexus-utils</artifactId>\n+            <version>2.1</version>\n+        </dependency>\n+\n+        <dependency>\n+            <groupId>org.eclipse.sisu</groupId>\n+            <artifactId>org.eclipse.sisu.plexus</artifactId>\n+            <version>0.1.1</version>\n+            <optional>true</optional>\n             <exclusions>\n                 <exclusion>\n-                    <groupId>org.apache.maven.wagon</groupId>\n-                    <artifactId>wagon-http-shared</artifactId>\n+                    <groupId>javax.enterprise</groupId>\n+                    <artifactId>cdi-api</artifactId>\n                 </exclusion>\n             </exclusions>\n         </dependency>\n-\n         <dependency>\n-            <groupId>org.apache.maven.wagon</groupId>\n-            <artifactId>wagon-http</artifactId>\n-            <version>1.0</version>\n+            <groupId>org.sonatype.sisu</groupId>\n+            <artifactId>sisu-guice</artifactId>\n+            <version>3.1.6</version>\n+            <classifier>no_aop</classifier>\n+            <optional>true</optional>\n             <exclusions>\n+                <exclusion>\n+                    <groupId>aopalliance</groupId>\n+                    <artifactId>aopalliance</artifactId>\n+                </exclusion>\n+                <exclusion>\n+                    <groupId>com.google.code.findbugs</groupId>\n+                    <artifactId>jsr305</artifactId>\n+                </exclusion>\n             </exclusions>\n         </dependency>\n \n@@ -166,6 +134,12 @@\n             <version>${project.version}</version>\n             <scope>test</scope>\n         </dependency>\n+        <dependency>\n+            <groupId>commons-io</groupId>\n+            <artifactId>commons-io</artifactId>\n+            <version>2.5</version>\n+            <scope>test</scope>\n+        </dependency>\n     </dependencies>\n \n     <build>", "filename": "storm-submit-tools/pom.xml"}, {"additions": 8, "raw_url": "https://github.com/apache/storm/raw/e55f67a58210461abbbdb1318c3b2a73b448a645/storm-submit-tools/src/main/java/org/apache/storm/submit/command/DependencyResolverMain.java", "blob_url": "https://github.com/apache/storm/blob/e55f67a58210461abbbdb1318c3b2a73b448a645/storm-submit-tools/src/main/java/org/apache/storm/submit/command/DependencyResolverMain.java", "sha": "300a202c815ed154dabc3ce61b1dbdd11557c0fb", "changes": 12, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-submit-tools/src/main/java/org/apache/storm/submit/command/DependencyResolverMain.java?ref=e55f67a58210461abbbdb1318c3b2a73b448a645", "patch": "@@ -24,10 +24,10 @@\n import org.apache.storm.submit.dependency.AetherUtils;\n import org.apache.storm.submit.dependency.DependencyResolver;\n import org.json.simple.JSONValue;\n-import org.sonatype.aether.artifact.Artifact;\n-import org.sonatype.aether.graph.Dependency;\n-import org.sonatype.aether.repository.RemoteRepository;\n-import org.sonatype.aether.resolution.ArtifactResult;\n+import org.eclipse.aether.artifact.Artifact;\n+import org.eclipse.aether.graph.Dependency;\n+import org.eclipse.aether.repository.RemoteRepository;\n+import org.eclipse.aether.resolution.ArtifactResult;\n \n import java.io.File;\n import java.nio.file.Files;\n@@ -66,6 +66,10 @@ public static void main(String[] args) {\n \n         try {\n             String localMavenRepoPath = getOrDefaultLocalMavenRepositoryPath(\"local-repo\");\n+\n+            // create root directory if not exist\n+            Files.createDirectories(new File(localMavenRepoPath).toPath());\n+\n             DependencyResolver resolver = new DependencyResolver(localMavenRepoPath, repositories);\n \n             List<ArtifactResult> artifactResults = resolver.resolve(dependencies);", "filename": "storm-submit-tools/src/main/java/org/apache/storm/submit/command/DependencyResolverMain.java"}, {"additions": 7, "raw_url": "https://github.com/apache/storm/raw/e55f67a58210461abbbdb1318c3b2a73b448a645/storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/AetherUtils.java", "blob_url": "https://github.com/apache/storm/blob/e55f67a58210461abbbdb1318c3b2a73b448a645/storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/AetherUtils.java", "sha": "a0b71465ff2423c14b1c23d7bf122f2a20f478c0", "changes": 14, "status": "modified", "deletions": 7, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/AetherUtils.java?ref=e55f67a58210461abbbdb1318c3b2a73b448a645", "patch": "@@ -17,12 +17,12 @@\n  */\n package org.apache.storm.submit.dependency;\n \n-import org.sonatype.aether.artifact.Artifact;\n-import org.sonatype.aether.graph.Dependency;\n-import org.sonatype.aether.graph.Exclusion;\n-import org.sonatype.aether.repository.RemoteRepository;\n-import org.sonatype.aether.util.artifact.DefaultArtifact;\n-import org.sonatype.aether.util.artifact.JavaScopes;\n+import org.eclipse.aether.artifact.Artifact;\n+import org.eclipse.aether.artifact.DefaultArtifact;\n+import org.eclipse.aether.graph.Dependency;\n+import org.eclipse.aether.graph.Exclusion;\n+import org.eclipse.aether.repository.RemoteRepository;\n+import org.eclipse.aether.util.artifact.JavaScopes;\n \n import java.util.ArrayList;\n import java.util.Arrays;\n@@ -86,6 +86,6 @@ public static RemoteRepository parseRemoteRepository(String repository) {\n             throw new IllegalArgumentException(\"Bad remote repository form: \" + repository);\n         }\n \n-        return new RemoteRepository(parts[0], \"default\", parts[1]);\n+        return new RemoteRepository.Builder(parts[0], \"default\", parts[1]).build();\n     }\n }\n\\ No newline at end of file", "filename": "storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/AetherUtils.java"}, {"additions": 16, "raw_url": "https://github.com/apache/storm/raw/e55f67a58210461abbbdb1318c3b2a73b448a645/storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/Booter.java", "blob_url": "https://github.com/apache/storm/blob/e55f67a58210461abbbdb1318c3b2a73b448a645/storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/Booter.java", "sha": "a9632e8d7dbc44d2b999a942cc7d27e35c6edcb5", "changes": 26, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/Booter.java?ref=e55f67a58210461abbbdb1318c3b2a73b448a645", "patch": "@@ -18,11 +18,12 @@\n \n package org.apache.storm.submit.dependency;\n \n-import org.apache.maven.repository.internal.MavenRepositorySystemSession;\n-import org.sonatype.aether.RepositorySystem;\n-import org.sonatype.aether.RepositorySystemSession;\n-import org.sonatype.aether.repository.LocalRepository;\n-import org.sonatype.aether.repository.RemoteRepository;\n+import org.apache.maven.repository.internal.MavenRepositorySystemUtils;\n+import org.eclipse.aether.DefaultRepositorySystemSession;\n+import org.eclipse.aether.RepositorySystem;\n+import org.eclipse.aether.RepositorySystemSession;\n+import org.eclipse.aether.repository.LocalRepository;\n+import org.eclipse.aether.repository.RemoteRepository;\n \n import java.io.File;\n \n@@ -35,17 +36,22 @@ public static RepositorySystem newRepositorySystem() {\n     }\n \n     public static RepositorySystemSession newRepositorySystemSession(\n-            RepositorySystem system, String localRepoPath) {\n-        MavenRepositorySystemSession session = new MavenRepositorySystemSession();\n+        RepositorySystem system, String localRepoPath) {\n+        DefaultRepositorySystemSession session = MavenRepositorySystemUtils.newSession();\n \n         LocalRepository localRepo =\n                 new LocalRepository(new File(localRepoPath).getAbsolutePath());\n-        session.setLocalRepositoryManager(system.newLocalRepositoryManager(localRepo));\n+        session.setLocalRepositoryManager(system.newLocalRepositoryManager(session, localRepo));\n \n         return session;\n     }\n \n     public static RemoteRepository newCentralRepository() {\n-        return new RemoteRepository(\"central\", \"default\", \"http://repo1.maven.org/maven2/\");\n+        return new RemoteRepository.Builder(\"central\", \"default\", \"http://repo1.maven.org/maven2/\").build();\n     }\n-}\n\\ No newline at end of file\n+\n+    public static RemoteRepository newLocalRepository() {\n+        return new RemoteRepository.Builder(\"local\",\n+                \"default\", \"file://\" + System.getProperty(\"user.home\") + \"/.m2/repository\").build();\n+    }\n+}", "filename": "storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/Booter.java"}, {"additions": 12, "raw_url": "https://github.com/apache/storm/raw/e55f67a58210461abbbdb1318c3b2a73b448a645/storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/DependencyResolver.java", "blob_url": "https://github.com/apache/storm/blob/e55f67a58210461abbbdb1318c3b2a73b448a645/storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/DependencyResolver.java", "sha": "a2241fa42068bc82a4711800a9fc2efc456e8d5d", "changes": 24, "status": "modified", "deletions": 12, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/DependencyResolver.java?ref=e55f67a58210461abbbdb1318c3b2a73b448a645", "patch": "@@ -18,18 +18,18 @@\n \n package org.apache.storm.submit.dependency;\n \n-import org.sonatype.aether.RepositorySystem;\n-import org.sonatype.aether.RepositorySystemSession;\n-import org.sonatype.aether.collection.CollectRequest;\n-import org.sonatype.aether.graph.Dependency;\n-import org.sonatype.aether.graph.DependencyFilter;\n-import org.sonatype.aether.repository.RemoteRepository;\n-import org.sonatype.aether.resolution.ArtifactResolutionException;\n-import org.sonatype.aether.resolution.ArtifactResult;\n-import org.sonatype.aether.resolution.DependencyRequest;\n-import org.sonatype.aether.resolution.DependencyResolutionException;\n-import org.sonatype.aether.util.artifact.JavaScopes;\n-import org.sonatype.aether.util.filter.DependencyFilterUtils;\n+import org.eclipse.aether.RepositorySystem;\n+import org.eclipse.aether.RepositorySystemSession;\n+import org.eclipse.aether.collection.CollectRequest;\n+import org.eclipse.aether.graph.Dependency;\n+import org.eclipse.aether.graph.DependencyFilter;\n+import org.eclipse.aether.repository.RemoteRepository;\n+import org.eclipse.aether.resolution.ArtifactResolutionException;\n+import org.eclipse.aether.resolution.ArtifactResult;\n+import org.eclipse.aether.resolution.DependencyRequest;\n+import org.eclipse.aether.resolution.DependencyResolutionException;\n+import org.eclipse.aether.util.artifact.JavaScopes;\n+import org.eclipse.aether.util.filter.DependencyFilterUtils;\n \n import java.io.File;\n import java.net.MalformedURLException;", "filename": "storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/DependencyResolver.java"}, {"additions": 22, "raw_url": "https://github.com/apache/storm/raw/e55f67a58210461abbbdb1318c3b2a73b448a645/storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/RepositorySystemFactory.java", "blob_url": "https://github.com/apache/storm/blob/e55f67a58210461abbbdb1318c3b2a73b448a645/storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/RepositorySystemFactory.java", "sha": "ae1c03a5528aeffdb6acc12bc126404d419758f0", "changes": 61, "status": "modified", "deletions": 39, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/RepositorySystemFactory.java?ref=e55f67a58210461abbbdb1318c3b2a73b448a645", "patch": "@@ -6,9 +6,9 @@\n  * to you under the Apache License, Version 2.0 (the\n  * \"License\"); you may not use this file except in compliance\n  * with the License.  You may obtain a copy of the License at\n- *\n+ * <p>\n  * http://www.apache.org/licenses/LICENSE-2.0\n- *\n+ * <p>\n  * Unless required by applicable law or agreed to in writing, software\n  * distributed under the License is distributed on an \"AS IS\" BASIS,\n  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n@@ -18,50 +18,33 @@\n \n package org.apache.storm.submit.dependency;\n \n-import org.apache.maven.repository.internal.DefaultServiceLocator;\n-import org.apache.maven.wagon.Wagon;\n-import org.apache.maven.wagon.providers.http.HttpWagon;\n-import org.apache.maven.wagon.providers.http.LightweightHttpWagon;\n-import org.sonatype.aether.RepositorySystem;\n-import org.sonatype.aether.connector.file.FileRepositoryConnectorFactory;\n-import org.sonatype.aether.connector.wagon.WagonProvider;\n-import org.sonatype.aether.connector.wagon.WagonRepositoryConnectorFactory;\n-import org.sonatype.aether.spi.connector.RepositoryConnectorFactory;\n+import org.eclipse.aether.connector.basic.BasicRepositoryConnectorFactory;\n+import org.eclipse.aether.impl.DefaultServiceLocator;\n+import org.apache.maven.repository.internal.MavenRepositorySystemUtils;\n+import org.eclipse.aether.RepositorySystem;\n+import org.eclipse.aether.spi.connector.transport.TransporterFactory;\n+import org.eclipse.aether.transport.file.FileTransporterFactory;\n+import org.eclipse.aether.transport.http.HttpTransporterFactory;\n+import org.eclipse.aether.spi.connector.RepositoryConnectorFactory;\n \n /**\n  * Get maven repository instance.\n  */\n public class RepositorySystemFactory {\n     public static RepositorySystem newRepositorySystem() {\n-        DefaultServiceLocator locator = new DefaultServiceLocator();\n-        locator.addService(RepositoryConnectorFactory.class, FileRepositoryConnectorFactory.class);\n-        locator.addService(RepositoryConnectorFactory.class, WagonRepositoryConnectorFactory.class);\n-        locator.setServices(WagonProvider.class, new ManualWagonProvider());\n+        DefaultServiceLocator locator = MavenRepositorySystemUtils.newServiceLocator();\n+        locator.addService(RepositoryConnectorFactory.class, BasicRepositoryConnectorFactory.class);\n+        locator.addService(TransporterFactory.class, FileTransporterFactory.class);\n+        locator.addService(TransporterFactory.class, HttpTransporterFactory.class);\n+\n+        locator.setErrorHandler(new DefaultServiceLocator.ErrorHandler() {\n+            @Override\n+            public void serviceCreationFailed(Class<?> type, Class<?> impl, Throwable exception) {\n+                exception.printStackTrace();\n+            }\n+        });\n \n         return locator.getService(RepositorySystem.class);\n     }\n \n-    /**\n-     * ManualWagonProvider\n-     */\n-    public static class ManualWagonProvider implements WagonProvider {\n-\n-        @Override\n-        public Wagon lookup(String roleHint) throws Exception {\n-            if (\"http\".equals(roleHint)) {\n-                return new LightweightHttpWagon();\n-            }\n-\n-            if (\"https\".equals(roleHint)) {\n-                return new HttpWagon();\n-            }\n-\n-            return null;\n-        }\n-\n-        @Override\n-        public void release(Wagon arg0) {\n-\n-        }\n-    }\n-}\n\\ No newline at end of file\n+}", "filename": "storm-submit-tools/src/main/java/org/apache/storm/submit/dependency/RepositorySystemFactory.java"}, {"additions": 5, "raw_url": "https://github.com/apache/storm/raw/e55f67a58210461abbbdb1318c3b2a73b448a645/storm-submit-tools/src/test/java/org/apache/storm/submit/dependency/AetherUtilsTest.java", "blob_url": "https://github.com/apache/storm/blob/e55f67a58210461abbbdb1318c3b2a73b448a645/storm-submit-tools/src/test/java/org/apache/storm/submit/dependency/AetherUtilsTest.java", "sha": "5b5ce3edf7976f1d9ad1c477f5bd87188f9282a1", "changes": 10, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-submit-tools/src/test/java/org/apache/storm/submit/dependency/AetherUtilsTest.java?ref=e55f67a58210461abbbdb1318c3b2a73b448a645", "patch": "@@ -19,11 +19,11 @@\n \n import com.google.common.collect.Lists;\n import org.junit.Test;\n-import org.sonatype.aether.artifact.Artifact;\n-import org.sonatype.aether.graph.Dependency;\n-import org.sonatype.aether.graph.Exclusion;\n-import org.sonatype.aether.util.artifact.DefaultArtifact;\n-import org.sonatype.aether.util.artifact.JavaScopes;\n+import org.eclipse.aether.artifact.Artifact;\n+import org.eclipse.aether.graph.Dependency;\n+import org.eclipse.aether.graph.Exclusion;\n+import org.eclipse.aether.artifact.DefaultArtifact;\n+import org.eclipse.aether.util.artifact.JavaScopes;\n \n import java.util.List;\n ", "filename": "storm-submit-tools/src/test/java/org/apache/storm/submit/dependency/AetherUtilsTest.java"}, {"additions": 4, "raw_url": "https://github.com/apache/storm/raw/e55f67a58210461abbbdb1318c3b2a73b448a645/storm-submit-tools/src/test/java/org/apache/storm/submit/dependency/DependencyResolverTest.java", "blob_url": "https://github.com/apache/storm/blob/e55f67a58210461abbbdb1318c3b2a73b448a645/storm-submit-tools/src/test/java/org/apache/storm/submit/dependency/DependencyResolverTest.java", "sha": "8f7b750281af18880e026bc10943167ff16b58c3", "changes": 8, "status": "modified", "deletions": 4, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-submit-tools/src/test/java/org/apache/storm/submit/dependency/DependencyResolverTest.java?ref=e55f67a58210461abbbdb1318c3b2a73b448a645", "patch": "@@ -23,10 +23,10 @@\n import org.junit.Before;\n import org.junit.BeforeClass;\n import org.junit.Test;\n-import org.sonatype.aether.graph.Dependency;\n-import org.sonatype.aether.resolution.ArtifactResult;\n-import org.sonatype.aether.util.artifact.DefaultArtifact;\n-import org.sonatype.aether.util.artifact.JavaScopes;\n+import org.eclipse.aether.graph.Dependency;\n+import org.eclipse.aether.resolution.ArtifactResult;\n+import org.eclipse.aether.artifact.DefaultArtifact;\n+import org.eclipse.aether.util.artifact.JavaScopes;\n \n import java.nio.file.Files;\n import java.nio.file.Path;", "filename": "storm-submit-tools/src/test/java/org/apache/storm/submit/dependency/DependencyResolverTest.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/e3f6cb96a1da5bb6e570a356e47ca9a6b8bc1e70", "parent": "https://github.com/apache/storm/commit/bf6e41c8909fee918eed879e7cf2c8f4a28424f7", "message": "Merge branch 'STORM-565' of https://github.com/harshach/incubator-storm into STORM-565\n\nSTORM-565: FIx NPE if topology.groups is null.", "bug_id": "storm_59", "file": [{"additions": 1, "raw_url": "https://github.com/apache/storm/raw/e3f6cb96a1da5bb6e570a356e47ca9a6b8bc1e70/storm-core/src/jvm/backtype/storm/security/auth/authorizer/SimpleACLAuthorizer.java", "blob_url": "https://github.com/apache/storm/blob/e3f6cb96a1da5bb6e570a356e47ca9a6b8bc1e70/storm-core/src/jvm/backtype/storm/security/auth/authorizer/SimpleACLAuthorizer.java", "sha": "1a3433ed6ac7c7b7c97cac52e4a34d695514257c", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/backtype/storm/security/auth/authorizer/SimpleACLAuthorizer.java?ref=e3f6cb96a1da5bb6e570a356e47ca9a6b8bc1e70", "patch": "@@ -110,7 +110,7 @@ public boolean permit(ReqContext context, String operation, Map topology_conf) {\n             }\n \n             Set<String> topoGroups = new HashSet<String>();\n-            if (topology_conf.containsKey(Config.TOPOLOGY_GROUPS)) {\n+            if (topology_conf.containsKey(Config.TOPOLOGY_GROUPS) && topology_conf.get(Config.TOPOLOGY_GROUPS) != null) {\n                 topoGroups.addAll((Collection<String>)topology_conf.get(Config.TOPOLOGY_GROUPS));\n             }\n ", "filename": "storm-core/src/jvm/backtype/storm/security/auth/authorizer/SimpleACLAuthorizer.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/440f1b5d3dd6194feed81a7da21b63b51cde6544", "parent": "https://github.com/apache/storm/commit/9755ff547de3247fe4aa1b60a778983145f43f76", "message": "[STORM-2505] Spout to support topic compaction\n\n[STORM-2505] Maintaining a emitted set in OffsetManager to handle the voids in the topic\n\n[STORM-2505] Handling NPE in Boxed Long to primitive type comparison\n\n[STORM-2505] Rephrased the log message when a non contiguous offset is acked by the spout\n\n[STORM-2505] Updated comment\n\n[STORM-2505] Renamed the methods ack/emit to addToAckMsgs and addToEmitMsgs in OffsetManager", "bug_id": "storm_60", "file": [{"additions": 11, "raw_url": "https://github.com/apache/storm/raw/440f1b5d3dd6194feed81a7da21b63b51cde6544/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java", "blob_url": "https://github.com/apache/storm/blob/440f1b5d3dd6194feed81a7da21b63b51cde6544/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java", "sha": "310902e89c813c0137d4da5c83d6b5e1103b0d3b", "changes": 21, "status": "modified", "deletions": 10, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java?ref=440f1b5d3dd6194feed81a7da21b63b51cde6544", "patch": "@@ -77,7 +77,7 @@\n     private transient boolean initialized;                              // Flag indicating that the spout is still undergoing initialization process.\n     // Initialization is only complete after the first call to  KafkaSpoutConsumerRebalanceListener.onPartitionsAssigned()\n \n-    private transient Map<TopicPartition, OffsetManager> acked;         // Tuples that were successfully acked. These tuples will be committed periodically when the commit timer expires, or after a consumer rebalance, or during close/deactivate\n+    private transient Map<TopicPartition, OffsetManager> offsetManagers;// Tuples that were successfully acked/emitted. These tuples will be committed periodically when the commit timer expires, or after a consumer rebalance, or during close/deactivate\n     private transient Set<KafkaSpoutMessageId> emitted;                 // Tuples that have been emitted but that are \"on the wire\", i.e. pending being acked or failed. Not used if it's AutoCommitMode\n     private transient Iterator<ConsumerRecord<K, V>> waitingToEmit;     // Records that have been polled and are queued to be emitted in the nextTuple() call. One record is emitted per nextTuple()\n     private transient long numUncommittedOffsets;                       // Number of offsets that have been polled and emitted but not yet been committed. Not used if auto commit mode is enabled.\n@@ -117,7 +117,7 @@ public void open(Map conf, TopologyContext context, SpoutOutputCollector collect\n         }\n         refreshSubscriptionTimer = new Timer(TIMER_DELAY_MS, kafkaSpoutConfig.getPartitionRefreshPeriodMs(), TimeUnit.MILLISECONDS);\n \n-        acked = new HashMap<>();\n+        offsetManagers = new HashMap<>();\n         emitted = new HashSet<>();\n         waitingToEmit = Collections.emptyListIterator();\n \n@@ -147,7 +147,7 @@ public void onPartitionsAssigned(Collection<TopicPartition> partitions) {\n \n         private void initialize(Collection<TopicPartition> partitions) {\n             if (!consumerAutoCommitMode) {\n-                acked.keySet().retainAll(partitions);   // remove from acked all partitions that are no longer assigned to this spout\n+                offsetManagers.keySet().retainAll(partitions);   // remove from acked all partitions that are no longer assigned to this spout\n             }\n \n             retryService.retainAll(partitions);\n@@ -205,8 +205,8 @@ private long doSeek(TopicPartition tp, OffsetAndMetadata committedOffset) {\n \n     private void setAcked(TopicPartition tp, long fetchOffset) {\n         // If this partition was previously assigned to this spout, leave the acked offsets as they were to resume where it left off\n-        if (!consumerAutoCommitMode && !acked.containsKey(tp)) {\n-            acked.put(tp, new OffsetManager(tp, fetchOffset));\n+        if (!consumerAutoCommitMode && !offsetManagers.containsKey(tp)) {\n+            offsetManagers.put(tp, new OffsetManager(tp, fetchOffset));\n         }\n     }\n \n@@ -319,7 +319,7 @@ private void emit() {\n     private boolean emitTupleIfNotEmitted(ConsumerRecord<K, V> record) {\n         final TopicPartition tp = new TopicPartition(record.topic(), record.partition());\n         final KafkaSpoutMessageId msgId = new KafkaSpoutMessageId(record);\n-        if (acked.containsKey(tp) && acked.get(tp).contains(msgId)) {   // has been acked\n+        if (offsetManagers.containsKey(tp) && offsetManagers.get(tp).contains(msgId)) {   // has been acked\n             LOG.trace(\"Tuple for record [{}] has already been acked. Skipping\", record);\n         } else if (emitted.contains(msgId)) {   // has been emitted and it's pending ack or fail\n             LOG.trace(\"Tuple for record [{}] has already been emitted. Skipping\", record);\n@@ -337,6 +337,7 @@ private boolean emitTupleIfNotEmitted(ConsumerRecord<K, V> record) {\n                         }\n                     } else {\n                         emitted.add(msgId);\n+                        offsetManagers.get(tp).addToEmitMsgs(msgId.offset());\n                         if (isScheduled) {  // Was scheduled for retry and re-emitted, so remove from schedule.\n                             retryService.remove(msgId);\n                         } else {            //New tuple, hence increment the uncommitted offset counter\n@@ -371,7 +372,7 @@ private boolean isEmitTuple(List<Object> tuple) {\n     private void commitOffsetsForAckedTuples() {\n         // Find offsets that are ready to be committed for every topic partition\n         final Map<TopicPartition, OffsetAndMetadata> nextCommitOffsets = new HashMap<>();\n-        for (Map.Entry<TopicPartition, OffsetManager> tpOffset : acked.entrySet()) {\n+        for (Map.Entry<TopicPartition, OffsetManager> tpOffset : offsetManagers.entrySet()) {\n             final OffsetAndMetadata nextCommitOffset = tpOffset.getValue().findNextCommitOffset();\n             if (nextCommitOffset != null) {\n                 nextCommitOffsets.put(tpOffset.getKey(), nextCommitOffset);\n@@ -387,7 +388,7 @@ private void commitOffsetsForAckedTuples() {\n             for (Map.Entry<TopicPartition, OffsetAndMetadata> tpOffset : nextCommitOffsets.entrySet()) {\n                 //Update the OffsetManager for each committed partition, and update numUncommittedOffsets\n                 final TopicPartition tp = tpOffset.getKey();\n-                final OffsetManager offsetManager = acked.get(tp);\n+                final OffsetManager offsetManager = offsetManagers.get(tp);\n                 long numCommittedOffsets = offsetManager.commit(tpOffset.getValue());\n                 numUncommittedOffsets -= numCommittedOffsets;\n                 LOG.debug(\"[{}] uncommitted offsets across all topic partitions\",\n@@ -413,7 +414,7 @@ public void ack(Object messageId) {\n             }\n         } else {\n             if (!consumerAutoCommitMode) {  // Only need to keep track of acked tuples if commits are not done automatically\n-                acked.get(msgId.getTopicPartition()).add(msgId);\n+                offsetManagers.get(msgId.getTopicPartition()).addToAckMsgs(msgId);\n             }\n             emitted.remove(msgId);\n         }\n@@ -493,7 +494,7 @@ public void declareOutputFields(OutputFieldsDeclarer declarer) {\n     @Override\n     public String toString() {\n         return \"KafkaSpout{\" +\n-                \"acked=\" + acked +\n+                \"offsetManagers =\" + offsetManagers +\n                 \", emitted=\" + emitted +\n                 \"}\";\n     }", "filename": "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/KafkaSpout.java"}, {"additions": 42, "raw_url": "https://github.com/apache/storm/raw/440f1b5d3dd6194feed81a7da21b63b51cde6544/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java", "blob_url": "https://github.com/apache/storm/blob/440f1b5d3dd6194feed81a7da21b63b51cde6544/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java", "sha": "0bf41325d65e270653fcf24339cce4b191592fe0", "changes": 47, "status": "modified", "deletions": 5, "contents_url": "https://api.github.com/repos/apache/storm/contents/external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java?ref=440f1b5d3dd6194feed81a7da21b63b51cde6544", "patch": "@@ -39,6 +39,8 @@\n     private final long initialFetchOffset;\n     // Last offset committed to Kafka. Initially it is set to fetchOffset - 1\n     private long committedOffset;\n+    // Emitted Offsets List\n+    private final NavigableSet<Long> emittedOffsets = new TreeSet<>();\n     // Acked messages sorted by ascending order of offset\n     private final NavigableSet<KafkaSpoutMessageId> ackedMsgs = new TreeSet<>(OFFSET_COMPARATOR);\n \n@@ -49,10 +51,14 @@ public OffsetManager(TopicPartition tp, long initialFetchOffset) {\n         LOG.debug(\"Instantiated {}\", this);\n     }\n \n-    public void add(KafkaSpoutMessageId msgId) {          // O(Log N)\n+    public void addToAckMsgs(KafkaSpoutMessageId msgId) {          // O(Log N)\n         ackedMsgs.add(msgId);\n     }\n \n+    public void addToEmitMsgs(long offset) {\n+        this.emittedOffsets.add(offset);                  // O(Log N)\n+    }\n+\n     /**\n      * An offset is only committed when all records with lower offset have been\n      * acked. This guarantees that all offsets smaller than the committedOffset\n@@ -68,13 +74,34 @@ public OffsetAndMetadata findNextCommitOffset() {\n         KafkaSpoutMessageId nextCommitMsg = null;     // this is a convenience variable to make it faster to create OffsetAndMetadata\n \n         for (KafkaSpoutMessageId currAckedMsg : ackedMsgs) {  // complexity is that of a linear scan on a TreeMap\n-            if ((currOffset = currAckedMsg.offset()) == nextCommitOffset + 1) {            // found the next offset to commit\n+            currOffset = currAckedMsg.offset();\n+            if (currOffset == nextCommitOffset + 1) {            // found the next offset to commit\n                 found = true;\n                 nextCommitMsg = currAckedMsg;\n                 nextCommitOffset = currOffset;\n-            } else if (currAckedMsg.offset() > nextCommitOffset + 1) {    // offset found is not continuous to the offsets listed to go in the next commit, so stop search\n-                LOG.debug(\"topic-partition [{}] has non-continuous offset [{}]. It will be processed in a subsequent batch.\", tp, currOffset);\n-                break;\n+            } else if (currOffset > nextCommitOffset + 1) {\n+                if (emittedOffsets.contains(nextCommitOffset + 1)) {\n+                    LOG.debug(\"topic-partition [{}] has non-continuous offset [{}]. It will be processed in a subsequent batch.\", tp, currOffset);\n+                    break;\n+                } else {\n+                    /*\n+                        This case will arise in case of non contiguous offset being processed.\n+                        So, if the topic doesn't contain offset = committedOffset + 1 (possible\n+                        if the topic is compacted or deleted), the consumer should jump to\n+                        the next logical point in the topic. Next logical offset should be the\n+                        first element after committedOffset in the ascending ordered emitted set.\n+                     */\n+                    LOG.debug(\"Processed non contiguous offset. (committedOffset+1) is no longer part of the topic. Committed: [{}], Processed: [{}]\", committedOffset, currOffset);\n+                    final Long nextEmittedOffset = emittedOffsets.ceiling(nextCommitOffset);\n+                    if (nextEmittedOffset != null && currOffset == nextEmittedOffset) {\n+                        found = true;\n+                        nextCommitMsg = currAckedMsg;\n+                        nextCommitOffset = currOffset;\n+                    } else {\n+                        LOG.debug(\"topic-partition [{}] has non-continuous offset [{}]. Next Offset to commit should be [{}]\", tp, currOffset, nextEmittedOffset);\n+                        break;\n+                    }\n+                }\n             } else {\n                 //Received a redundant ack. Ignore and continue processing.\n                 LOG.warn(\"topic-partition [{}] has unexpected offset [{}]. Current committed Offset [{}]\",\n@@ -113,6 +140,15 @@ public long commit(OffsetAndMetadata committedOffset) {\n                 break;\n             }\n         }\n+\n+        for (Iterator<Long> iterator = emittedOffsets.iterator(); iterator.hasNext();) {\n+            if (iterator.next() <= committedOffset.offset()) {\n+                iterator.remove();\n+            } else {\n+                break;\n+            }\n+        }\n+\n         LOG.trace(\"{}\", this);\n         \n         LOG.debug(\"Committed offsets [{}-{} = {}] for topic-partition [{}].\",\n@@ -143,6 +179,7 @@ public String toString() {\n             + \"topic-partition=\" + tp\n             + \", fetchOffset=\" + initialFetchOffset\n             + \", committedOffset=\" + committedOffset\n+            + \", emittedOffsets=\" + emittedOffsets\n             + \", ackedMsgs=\" + ackedMsgs\n             + '}';\n     }", "filename": "external/storm-kafka-client/src/main/java/org/apache/storm/kafka/spout/internal/OffsetManager.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/943561f4adcff1fa156a45d47014b3b6f94ca986", "parent": "https://github.com/apache/storm/commit/df6a4186f43d701572fe7779eb51519a851f088a", "message": "Merge pull request #113 from danharvey/drpc-conf-npe\n\nAdded check for null servers list on the setup of the DRPCSpout. Fixes #...", "bug_id": "storm_61", "file": [{"additions": 1, "raw_url": "https://github.com/apache/storm/raw/943561f4adcff1fa156a45d47014b3b6f94ca986/src/jvm/backtype/storm/drpc/DRPCSpout.java", "blob_url": "https://github.com/apache/storm/blob/943561f4adcff1fa156a45d47014b3b6f94ca986/src/jvm/backtype/storm/drpc/DRPCSpout.java", "sha": "cbbc501de1900f199518055f7cf3ba3456bac413", "changes": 2, "status": "modified", "deletions": 1, "contents_url": "https://api.github.com/repos/apache/storm/contents/src/jvm/backtype/storm/drpc/DRPCSpout.java?ref=943561f4adcff1fa156a45d47014b3b6f94ca986", "patch": "@@ -57,7 +57,7 @@ public void open(Map conf, TopologyContext context, SpoutOutputCollector collect\n \n             int port = Utils.getInt(conf.get(Config.DRPC_INVOCATIONS_PORT));\n             List<String> servers = (List<String>) conf.get(Config.DRPC_SERVERS);\n-            if(servers.isEmpty()) {\n+            if(servers == null || servers.isEmpty()) {\n                 throw new RuntimeException(\"No DRPC servers configured for topology\");   \n             }\n             if(numTasks < servers.size()) {", "filename": "src/jvm/backtype/storm/drpc/DRPCSpout.java"}], "repo": "storm"}, {"commit": "https://github.com/apache/storm/commit/28e65a82bfcd60f38c93115974f6a8653756f9b5", "parent": "https://github.com/apache/storm/commit/d5dee0ef5f8ad403d1eff05f569c8e9b1e44508c", "message": "Fix race condition in Time.java\n\nSome of my test runs were occasionally failing with a NullPointerException on `backtype.storm.utils.Time.java:64`.\r\n\r\nAfter a bit of investigation, it seems there's a race condition here; if we disable simulating mode while a thread is currently sleeping, then when it wakes up it won't re-check if it's still in \"simulating\" mode, it'll try to remove the sleep time, and get the NPE.\r\n\r\nRefs [STORM-260](https://issues.apache.org/jira/browse/STORM-260).", "bug_id": "storm_62", "file": [{"additions": 12, "raw_url": "https://github.com/apache/storm/raw/28e65a82bfcd60f38c93115974f6a8653756f9b5/storm-core/src/jvm/backtype/storm/utils/Time.java", "blob_url": "https://github.com/apache/storm/blob/28e65a82bfcd60f38c93115974f6a8653756f9b5/storm-core/src/jvm/backtype/storm/utils/Time.java", "sha": "50a79fd673a446712b7234251c69b8c229d5d910", "changes": 18, "status": "modified", "deletions": 6, "contents_url": "https://api.github.com/repos/apache/storm/contents/storm-core/src/jvm/backtype/storm/utils/Time.java?ref=28e65a82bfcd60f38c93115974f6a8653756f9b5", "patch": "@@ -36,14 +36,18 @@\n     private static AtomicLong simulatedCurrTimeMs; //should this be a thread local that's allowed to keep advancing?\n     \n     public static void startSimulating() {\n-        simulating.set(true);\n-        simulatedCurrTimeMs = new AtomicLong(0);\n-        threadSleepTimes = new ConcurrentHashMap<Thread, AtomicLong>();\n+        synchronized(sleepTimesLock) {\n+            simulating.set(true);\n+            simulatedCurrTimeMs = new AtomicLong(0);\n+            threadSleepTimes = new ConcurrentHashMap<Thread, AtomicLong>();\n+        }\n     }\n     \n     public static void stopSimulating() {\n-        simulating.set(false);             \n-        threadSleepTimes = null;  \n+        synchronized(sleepTimesLock) {\n+            simulating.set(false);             \n+            threadSleepTimes = null;  \n+        }\n     }\n     \n     public static boolean isSimulating() {\n@@ -61,7 +65,9 @@ public static void sleepUntil(long targetTimeMs) throws InterruptedException {\n                 }\n             } finally {\n                 synchronized(sleepTimesLock) {\n-                    threadSleepTimes.remove(Thread.currentThread());\n+                    if (simulating.get()) {\n+                        threadSleepTimes.remove(Thread.currentThread());\n+                    }\n                 }\n             }\n         } else {", "filename": "storm-core/src/jvm/backtype/storm/utils/Time.java"}], "repo": "storm"}]
