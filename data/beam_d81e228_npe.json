[
    {
        "repo": "beam",
        "commit": "https://github.com/apache/beam/commit/d81e2284a1d95ded469ad74b45698381d4181594",
        "bug_id": "beam_d81e228",
        "message": "Allow getFractionConsumed to be called before reading has started\n\nCalls to getFractionConsumed resulted in a NPE if a block had not been read from\nthe input.\n----Release Notes----\n[]\n-------------\nCreated by MOE: https://github.com/google/moe\nMOE_MIGRATED_REVID=102791238",
        "parent": "https://github.com/apache/beam/commit/d1b46b1c98fff9f94e2d6d021407d13f49a40e01",
        "patched_files": [
            "BlockBasedSource.java",
            "AvroSource.java"
        ],
        "file": [
            {
                "status": "modified",
                "additions": 21,
                "raw_url": "https://github.com/apache/beam/raw/d81e2284a1d95ded469ad74b45698381d4181594/sdk/src/main/java/com/google/cloud/dataflow/sdk/io/BlockBasedSource.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdk/src/main/java/com/google/cloud/dataflow/sdk/io/BlockBasedSource.java?ref=d81e2284a1d95ded469ad74b45698381d4181594",
                "filename": "sdk/src/main/java/com/google/cloud/dataflow/sdk/io/BlockBasedSource.java",
                "deletions": 7,
                "sha": "8ec55499de0a25a17359de6f6513c0e0c0af3853",
                "blob_url": "https://github.com/apache/beam/blob/d81e2284a1d95ded469ad74b45698381d4181594/sdk/src/main/java/com/google/cloud/dataflow/sdk/io/BlockBasedSource.java",
                "patch": "@@ -22,6 +22,8 @@\n import java.io.IOException;\n import java.util.NoSuchElementException;\n \n+import javax.annotation.Nullable;\n+\n /**\n  * A {@code BlockBasedSource} is a {@link FileBasedSource} where a file consists of blocks of\n  * records.\n@@ -122,7 +124,6 @@ public BlockBasedSource(String fileName, long minBundleSize, long startOffset, l\n    */\n   @Experimental(Experimental.Kind.SOURCE_SINK)\n   protected abstract static class BlockBasedReader<T> extends FileBasedReader<T> {\n-    private Block<T> currentBlock;\n     private boolean atSplitPoint;\n \n     protected BlockBasedReader(BlockBasedSource<T> source) {\n@@ -135,10 +136,12 @@ protected BlockBasedReader(BlockBasedSource<T> source) {\n     public abstract boolean readNextBlock() throws IOException;\n \n     /**\n-     * Returns the current block (the block that was read by the previous call to\n-     * {@link BlockBasedReader#readNextBlock}).\n+     * Returns the current block (the block that was read by the last successful call to\n+     * {@link BlockBasedReader#readNextBlock}). May return null initially, or if no block has been\n+     * successfully read.\n      */\n-    public abstract Block<T> getCurrentBlock() throws NoSuchElementException;\n+    @Nullable\n+    public abstract Block<T> getCurrentBlock();\n \n     /**\n      * Returns the size of the current block in bytes as it is represented in the underlying file,\n@@ -150,7 +153,7 @@ protected BlockBasedReader(BlockBasedSource<T> source) {\n      * (but not correctness) of dynamic work rebalancing.\n      *\n      * <p>This method and {@link Block#getFractionOfBlockConsumed} are used to provide an estimate\n-     * of progress within a block ({@code currentBlock.getFractionOfBlockConsumed() *\n+     * of progress within a block ({@code getCurrentBlock().getFractionOfBlockConsumed() *\n      * getCurrentBlockSize()}). It is acceptable for the result of this computation to be 0, but\n      * progress estimation will be inaccurate.\n      */\n@@ -164,6 +167,11 @@ protected BlockBasedReader(BlockBasedSource<T> source) {\n \n     @Override\n     public final T getCurrent() throws NoSuchElementException {\n+      Block<T> currentBlock = getCurrentBlock();\n+      if (currentBlock == null) {\n+        throw new NoSuchElementException(\n+            \"No block has been successfully read from \" + getCurrentSource());\n+      }\n       return currentBlock.getCurrentRecord();\n     }\n \n@@ -180,11 +188,12 @@ protected boolean isAtSplitPoint() {\n     @Override\n     protected final boolean readNextRecord() throws IOException {\n       atSplitPoint = false;\n-      while (currentBlock == null || !currentBlock.readNextRecord()) {\n+\n+      while (getCurrentBlock() == null || !getCurrentBlock().readNextRecord()) {\n         if (!readNextBlock()) {\n           return false;\n         }\n-        currentBlock = getCurrentBlock();\n+        // The first record in a block is a split point.\n         atSplitPoint = true;\n       }\n       return true;\n@@ -195,6 +204,11 @@ public Double getFractionConsumed() {\n       if (getCurrentSource().getEndOffset() == Long.MAX_VALUE) {\n         return null;\n       }\n+      Block<T> currentBlock = getCurrentBlock();\n+      if (currentBlock == null) {\n+        // There is no current block (i.e., the read has not yet begun).\n+        return 0.0;\n+      }\n       long currentBlockOffset = getCurrentBlockOffset();\n       long startOffset = getCurrentSource().getStartOffset();\n       long endOffset = getCurrentSource().getEndOffset();",
                "changes": 28
            },
            {
                "status": "modified",
                "additions": 40,
                "raw_url": "https://github.com/apache/beam/raw/d81e2284a1d95ded469ad74b45698381d4181594/sdk/src/test/java/com/google/cloud/dataflow/sdk/io/AvroSourceTest.java",
                "contents_url": "https://api.github.com/repos/apache/beam/contents/sdk/src/test/java/com/google/cloud/dataflow/sdk/io/AvroSourceTest.java?ref=d81e2284a1d95ded469ad74b45698381d4181594",
                "filename": "sdk/src/test/java/com/google/cloud/dataflow/sdk/io/AvroSourceTest.java",
                "deletions": 2,
                "sha": "9c465241e91e70344a871f2f4052efc1fa1ab331",
                "blob_url": "https://github.com/apache/beam/blob/d81e2284a1d95ded469ad74b45698381d4181594/sdk/src/test/java/com/google/cloud/dataflow/sdk/io/AvroSourceTest.java",
                "patch": "@@ -27,7 +27,6 @@\n import com.google.cloud.dataflow.sdk.io.AvroSource.AvroReader.Seeker;\n import com.google.cloud.dataflow.sdk.options.PipelineOptions;\n import com.google.cloud.dataflow.sdk.options.PipelineOptionsFactory;\n-import com.google.cloud.dataflow.sdk.testing.SourceTestUtils;\n \n import org.apache.avro.Schema;\n import org.apache.avro.file.CodecFactory;\n@@ -50,6 +49,7 @@\n import java.io.PushbackInputStream;\n import java.util.ArrayList;\n import java.util.List;\n+import java.util.NoSuchElementException;\n import java.util.Random;\n \n /**\n@@ -160,9 +160,47 @@ public void testSplitAtFraction() throws Exception {\n     }\n   }\n \n+  @Test\n+  public void testGetProgressFromUnstartedReader() throws Exception {\n+    List<FixedRecord> records = createFixedRecords(DEFAULT_RECORD_COUNT);\n+    String filename = generateTestFile(\"tmp.avro\", records, SyncBehavior.SYNC_DEFAULT, 1000,\n+        AvroCoder.of(FixedRecord.class), DataFileConstants.NULL_CODEC);\n+    File file = new File(filename);\n+\n+    AvroSource<FixedRecord> source = AvroSource.from(filename).withSchema(FixedRecord.class);\n+    try (BoundedSource.BoundedReader<FixedRecord> reader = source.createReader(null)) {\n+      assertEquals(new Double(0.0), reader.getFractionConsumed());\n+    }\n+\n+    List<? extends BoundedSource<FixedRecord>> splits =\n+        source.splitIntoBundles(file.length() / 3, null);\n+    for (BoundedSource<FixedRecord> subSource : splits) {\n+      try (BoundedSource.BoundedReader<FixedRecord> reader = subSource.createReader(null)) {\n+        assertEquals(new Double(0.0), reader.getFractionConsumed());\n+      }\n+    }\n+  }\n+\n+  @Test\n+  public void testGetCurrentFromUnstartedReader() throws Exception {\n+    List<FixedRecord> records = createFixedRecords(DEFAULT_RECORD_COUNT);\n+    String filename = generateTestFile(\"tmp.avro\", records, SyncBehavior.SYNC_DEFAULT, 1000,\n+        AvroCoder.of(FixedRecord.class), DataFileConstants.NULL_CODEC);\n+\n+    AvroSource<FixedRecord> source = AvroSource.from(filename).withSchema(FixedRecord.class);\n+    try (BlockBasedSource.BlockBasedReader<FixedRecord> reader =\n+        (BlockBasedSource.BlockBasedReader<FixedRecord>) source.createReader(null)) {\n+      assertEquals(null, reader.getCurrentBlock());\n+\n+      expectedException.expect(NoSuchElementException.class);\n+      expectedException.expectMessage(\"No block has been successfully read from\");\n+      reader.getCurrent();\n+    }\n+  }\n+\n   @Test\n   public void testSplitAtFractionExhaustive() throws Exception {\n-    List<FixedRecord> expected = createFixedRecords(100);\n+    List<FixedRecord> expected = createFixedRecords(50);\n     String filename = generateTestFile(\"tmp.avro\", expected, SyncBehavior.SYNC_REGULAR, 5,\n         AvroCoder.of(FixedRecord.class), DataFileConstants.NULL_CODEC);\n ",
                "changes": 42
            }
        ],
        "unit_tests": [
            "AvroSourceTest.java"
        ]
    },
    {
        "buggy": false,
        "test_file": "sdk/src/test/java/com/google/cloud/dataflow/sdk/io/AvroSourceTest.java",
        "buggy_files": [
            "sdk/src/main/java/com/google/cloud/dataflow/sdk/io/BlockBasedSource.java",
            "sdk/src/main/java/com/google/cloud/dataflow/sdk/io/AvroSource.java"
        ],
        "fixed": true
    }
]