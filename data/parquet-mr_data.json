[
    {
        "commit": "https://github.com/apache/parquet-mr/commit/b34b077486473c46ff5199421c79cd2e797e5817",
        "file": [
            {
                "patch": "@@ -38,6 +38,21 @@\n    */\n   public UserDefinedPredicate() { }\n \n+  /**\n+   * Returns whether this predicate accepts {@code null} values.\n+   *\n+   * @return {@code true} if this predicate accepts {@code null} values, {@code false} otherwise\n+   */\n+  public boolean acceptsNullValue() {\n+    try {\n+      return keep(null);\n+    } catch (NullPointerException e) {\n+      // The implementor might not be prepared to handle null values;\n+      // in this case this predicate obviously does not accept nulls\n+      return false;\n+    }\n+  }\n+\n   /**\n    * Return true to keep the record with this value, false to drop it.\n    * <p>",
                "additions": 15,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b34b077486473c46ff5199421c79cd2e797e5817/parquet-column/src/main/java/org/apache/parquet/filter2/predicate/UserDefinedPredicate.java",
                "status": "modified",
                "changes": 15,
                "deletions": 0,
                "sha": "fea9ca9c1d65001a2166a37d98e186b1bf0651b8",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b34b077486473c46ff5199421c79cd2e797e5817/parquet-column/src/main/java/org/apache/parquet/filter2/predicate/UserDefinedPredicate.java",
                "filename": "parquet-column/src/main/java/org/apache/parquet/filter2/predicate/UserDefinedPredicate.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-column/src/main/java/org/apache/parquet/filter2/predicate/UserDefinedPredicate.java?ref=b34b077486473c46ff5199421c79cd2e797e5817"
            },
            {
                "patch": "@@ -291,7 +291,7 @@ boolean isNullPage(int pageIndex) {\n     public <T extends Comparable<T>, U extends UserDefinedPredicate<T>> PrimitiveIterator.OfInt visit(\n         UserDefined<T, U> udp) {\n       final UserDefinedPredicate<T> predicate = udp.getUserDefinedPredicate();\n-      final boolean acceptNulls = predicate.keep(null);\n+      final boolean acceptNulls = predicate.acceptsNullValue();\n \n       if (acceptNulls && nullCounts == null) {\n         // Nulls match so if we don't have null related statistics we have to return all pages\n@@ -321,7 +321,7 @@ public boolean test(int pageIndex) {\n     public <T extends Comparable<T>, U extends UserDefinedPredicate<T>> PrimitiveIterator.OfInt visit(\n         LogicalNotUserDefined<T, U> udp) {\n       final UserDefinedPredicate<T> inversePredicate = udp.getUserDefined().getUserDefinedPredicate();\n-      final boolean acceptNulls = !inversePredicate.keep(null);\n+      final boolean acceptNulls = !inversePredicate.acceptsNullValue();\n \n       if (acceptNulls && nullCounts == null) {\n         // Nulls match so if we don't have null related statistics we have to return all pages",
                "additions": 2,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b34b077486473c46ff5199421c79cd2e797e5817/parquet-column/src/main/java/org/apache/parquet/internal/column/columnindex/ColumnIndexBuilder.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "15be50e55d842f0383e08b10ad60560b89a73aba",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b34b077486473c46ff5199421c79cd2e797e5817/parquet-column/src/main/java/org/apache/parquet/internal/column/columnindex/ColumnIndexBuilder.java",
                "filename": "parquet-column/src/main/java/org/apache/parquet/internal/column/columnindex/ColumnIndexBuilder.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-column/src/main/java/org/apache/parquet/internal/column/columnindex/ColumnIndexBuilder.java?ref=b34b077486473c46ff5199421c79cd2e797e5817"
            },
            {
                "patch": "@@ -149,14 +149,14 @@ private RowRanges allRows() {\n   @Override\n   public <T extends Comparable<T>, U extends UserDefinedPredicate<T>> RowRanges visit(UserDefined<T, U> udp) {\n     return applyPredicate(udp.getColumn(), ci -> ci.visit(udp),\n-        udp.getUserDefinedPredicate().keep(null) ? allRows() : RowRanges.EMPTY);\n+        udp.getUserDefinedPredicate().acceptsNullValue() ? allRows() : RowRanges.EMPTY);\n   }\n \n   @Override\n   public <T extends Comparable<T>, U extends UserDefinedPredicate<T>> RowRanges visit(\n       LogicalNotUserDefined<T, U> udp) {\n     return applyPredicate(udp.getUserDefined().getColumn(), ci -> ci.visit(udp),\n-        udp.getUserDefined().getUserDefinedPredicate().keep(null) ? RowRanges.EMPTY : allRows());\n+        udp.getUserDefined().getUserDefinedPredicate().acceptsNullValue() ? RowRanges.EMPTY : allRows());\n   }\n \n   private RowRanges applyPredicate(Column<?> column, Function<ColumnIndex, PrimitiveIterator.OfInt> func,",
                "additions": 2,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b34b077486473c46ff5199421c79cd2e797e5817/parquet-column/src/main/java/org/apache/parquet/internal/filter2/columnindex/ColumnIndexFilter.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "6dec7741dd176613de0a3eeb67b024c967f59e2b",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b34b077486473c46ff5199421c79cd2e797e5817/parquet-column/src/main/java/org/apache/parquet/internal/filter2/columnindex/ColumnIndexFilter.java",
                "filename": "parquet-column/src/main/java/org/apache/parquet/internal/filter2/columnindex/ColumnIndexFilter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-column/src/main/java/org/apache/parquet/internal/filter2/columnindex/ColumnIndexFilter.java?ref=b34b077486473c46ff5199421c79cd2e797e5817"
            },
            {
                "patch": "@@ -248,7 +248,7 @@ private void addUdpCase(TypeInfo info, boolean invert)throws IOException {\n         \"      valueInspector = new ValueInspector() {\\n\" +\n         \"        @Override\\n\" +\n         \"        public void updateNull() {\\n\" +\n-        \"          setResult(\" + (invert ? \"!\" : \"\") + \"udp.keep(null));\\n\" +\n+        \"          setResult(\" + (invert ? \"!\" : \"\") + \"udp.acceptsNullValue());\\n\" +\n         \"        }\\n\" +\n         \"\\n\" +\n         \"        @SuppressWarnings(\\\"unchecked\\\")\\n\" +",
                "additions": 1,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b34b077486473c46ff5199421c79cd2e797e5817/parquet-generator/src/main/java/org/apache/parquet/filter2/IncrementallyUpdatedFilterPredicateGenerator.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "3c1cf4866ca9b9376cbee84109f5411548da5232",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b34b077486473c46ff5199421c79cd2e797e5817/parquet-generator/src/main/java/org/apache/parquet/filter2/IncrementallyUpdatedFilterPredicateGenerator.java",
                "filename": "parquet-generator/src/main/java/org/apache/parquet/filter2/IncrementallyUpdatedFilterPredicateGenerator.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-generator/src/main/java/org/apache/parquet/filter2/IncrementallyUpdatedFilterPredicateGenerator.java?ref=b34b077486473c46ff5199421c79cd2e797e5817"
            },
            {
                "patch": "@@ -390,9 +390,9 @@ public Boolean visit(Not not) {\n     // The column is missing, thus all null. Check if the predicate keeps null.\n     if (meta == null) {\n       if (inverted) {\n-        return udp.keep(null);\n+        return udp.acceptsNullValue();\n       } else {\n-        return !udp.keep(null);\n+        return !udp.acceptsNullValue();\n       }\n     }\n ",
                "additions": 2,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b34b077486473c46ff5199421c79cd2e797e5817/parquet-hadoop/src/main/java/org/apache/parquet/filter2/dictionarylevel/DictionaryFilter.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "c43380b1209bf2d9efb2dc174a139cb765d0b8fb",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b34b077486473c46ff5199421c79cd2e797e5817/parquet-hadoop/src/main/java/org/apache/parquet/filter2/dictionarylevel/DictionaryFilter.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/filter2/dictionarylevel/DictionaryFilter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/filter2/dictionarylevel/DictionaryFilter.java?ref=b34b077486473c46ff5199421c79cd2e797e5817"
            },
            {
                "patch": "@@ -366,9 +366,9 @@ public Boolean visit(Not not) {\n       // the column isn't in this file so all values are null.\n       // lets run the udp with null value to see if it keeps null or not.\n       if (inverted) {\n-        return udp.keep(null);\n+        return udp.acceptsNullValue();\n       } else {\n-        return !udp.keep(null);\n+        return !udp.acceptsNullValue();\n       }\n     }\n \n@@ -382,9 +382,9 @@ public Boolean visit(Not not) {\n     if (isAllNulls(columnChunk)) {\n       // lets run the udp with null value to see if it keeps null or not.\n       if (inverted) {\n-        return udp.keep(null);\n+        return udp.acceptsNullValue();\n       } else {\n-        return !udp.keep(null);\n+        return !udp.acceptsNullValue();\n       }\n     }\n ",
                "additions": 4,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b34b077486473c46ff5199421c79cd2e797e5817/parquet-hadoop/src/main/java/org/apache/parquet/filter2/statisticslevel/StatisticsFilter.java",
                "status": "modified",
                "changes": 8,
                "deletions": 4,
                "sha": "31b6c45f7f159dd342fe37a476cbea8f7106acdb",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b34b077486473c46ff5199421c79cd2e797e5817/parquet-hadoop/src/main/java/org/apache/parquet/filter2/statisticslevel/StatisticsFilter.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/filter2/statisticslevel/StatisticsFilter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/filter2/statisticslevel/StatisticsFilter.java?ref=b34b077486473c46ff5199421c79cd2e797e5817"
            },
            {
                "patch": "@@ -385,7 +385,9 @@ public boolean inverseCanDrop(Statistics<Binary> statistics) {\n \n     @Override\n     public boolean keep(Long value) {\n-      return value != null && value % divisor == 0;\n+      // Deliberately not checking for null to verify the handling of NPE\n+      // Implementors shall always checks the value for null and return accordingly\n+      return value % divisor == 0;\n     }\n \n     @Override",
                "additions": 3,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b34b077486473c46ff5199421c79cd2e797e5817/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestColumnIndexFiltering.java",
                "status": "modified",
                "changes": 4,
                "deletions": 1,
                "sha": "ccb6a03d512713a6727f447a88e1399ef040bf98",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b34b077486473c46ff5199421c79cd2e797e5817/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestColumnIndexFiltering.java",
                "filename": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestColumnIndexFiltering.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestColumnIndexFiltering.java?ref=b34b077486473c46ff5199421c79cd2e797e5817"
            }
        ],
        "bug_id": "parquet-mr_1",
        "parent": "https://github.com/apache/parquet-mr/commit/0861ddff9572fef8b9002d7ebaba3bf62455cee2",
        "message": "PARQUET-1488: UserDefinedPredicate throw NPE (#663)",
        "repo": "parquet-mr"
    },
    {
        "commit": "https://github.com/apache/parquet-mr/commit/10f57a3779264ba222288defd1472d66ac2ae135",
        "file": [
            {
                "patch": "@@ -226,7 +226,9 @@ public void setConf(Configuration conf) {\n \n   @Override\n   public Configuration getConf() {\n-    return conf;\n+    // In case conf is null, we'll return an empty configuration\n+    // this can be on a local development machine\n+    return null != conf ? conf : new Configuration();\n   }\n \n   /**",
                "additions": 3,
                "raw_url": "https://github.com/apache/parquet-mr/raw/10f57a3779264ba222288defd1472d66ac2ae135/parquet-cli/src/main/java/org/apache/parquet/cli/BaseCommand.java",
                "status": "modified",
                "changes": 4,
                "deletions": 1,
                "sha": "cdef53d52ca3242e73cccdce560649ab8e797828",
                "blob_url": "https://github.com/apache/parquet-mr/blob/10f57a3779264ba222288defd1472d66ac2ae135/parquet-cli/src/main/java/org/apache/parquet/cli/BaseCommand.java",
                "filename": "parquet-cli/src/main/java/org/apache/parquet/cli/BaseCommand.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-cli/src/main/java/org/apache/parquet/cli/BaseCommand.java?ref=10f57a3779264ba222288defd1472d66ac2ae135"
            },
            {
                "patch": "@@ -86,30 +86,30 @@ public int run() throws IOException {\n \n     CodecFactory codecFactory = Codecs.avroCodec(compressionCodecName);\n \n-    Schema schema;\n+    final Schema schema;\n     if (avroSchemaFile != null) {\n       schema = Schemas.fromAvsc(open(avroSchemaFile));\n     } else {\n       schema = getAvroSchema(source);\n     }\n-    Schema projection = filterSchema(schema, columns);\n+    final Schema projection = filterSchema(schema, columns);\n \n     Path outPath = qualifiedPath(outputPath);\n-    FileSystem outFS = outPath.getFileSystem(getConf());\n-    if (overwrite && outFS.exists(outPath)) {\n-      console.debug(\"Deleting output file {} (already exists)\", outPath);\n-      outFS.delete(outPath);\n+    try (FileSystem outFS = outPath.getFileSystem(getConf())) {\n+      if (overwrite && outFS.exists(outPath)) {\n+        console.debug(\"Deleting output file {} (already exists)\", outPath);\n+        outFS.delete(outPath);\n+      }\n     }\n \n     Iterable<Record> reader = openDataFile(source, projection);\n     boolean threw = true;\n     long count = 0;\n-    try {\n-      DatumWriter<Record> datumWriter = new GenericDatumWriter<>(schema);\n-      DataFileWriter<Record> w = new DataFileWriter<>(datumWriter);\n-      w.setCodec(codecFactory);\n \n-      try (DataFileWriter<Record> writer = w.create(projection, create(outputPath))) {\n+    DatumWriter<Record> datumWriter = new GenericDatumWriter<>(schema);\n+    try (DataFileWriter<Record> fileWriter = new DataFileWriter<>(datumWriter)) {\n+      fileWriter.setCodec(codecFactory);\n+      try (DataFileWriter<Record> writer=fileWriter.create(projection, create(outputPath))) {\n         for (Record record : reader) {\n           writer.append(record);\n           count += 1;",
                "additions": 11,
                "raw_url": "https://github.com/apache/parquet-mr/raw/10f57a3779264ba222288defd1472d66ac2ae135/parquet-cli/src/main/java/org/apache/parquet/cli/commands/ToAvroCommand.java",
                "status": "modified",
                "changes": 22,
                "deletions": 11,
                "sha": "d659109f97deb29ef2ae9e8da368ba6973da783d",
                "blob_url": "https://github.com/apache/parquet-mr/blob/10f57a3779264ba222288defd1472d66ac2ae135/parquet-cli/src/main/java/org/apache/parquet/cli/commands/ToAvroCommand.java",
                "filename": "parquet-cli/src/main/java/org/apache/parquet/cli/commands/ToAvroCommand.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-cli/src/main/java/org/apache/parquet/cli/commands/ToAvroCommand.java?ref=10f57a3779264ba222288defd1472d66ac2ae135"
            },
            {
                "patch": "@@ -64,7 +64,9 @@\n   private static final JsonFactory FACTORY = new JsonFactory(MAPPER);\n \n   public static Iterator<JsonNode> parser(final InputStream stream) {\n-    try(JsonParser parser = FACTORY.createParser(stream)) {\n+    try {\n+      // Don't close the parser until the iterator has been consumed\n+      JsonParser parser = FACTORY.createParser(stream);\n       return parser.readValuesAs(JsonNode.class);\n     } catch (IOException e) {\n       throw new RuntimeIOException(\"Cannot read from stream\", e);",
                "additions": 3,
                "raw_url": "https://github.com/apache/parquet-mr/raw/10f57a3779264ba222288defd1472d66ac2ae135/parquet-cli/src/main/java/org/apache/parquet/cli/json/AvroJson.java",
                "status": "modified",
                "changes": 4,
                "deletions": 1,
                "sha": "f67b99f323048ca3d24f9f2c230e21cfe4cd2b2a",
                "blob_url": "https://github.com/apache/parquet-mr/blob/10f57a3779264ba222288defd1472d66ac2ae135/parquet-cli/src/main/java/org/apache/parquet/cli/json/AvroJson.java",
                "filename": "parquet-cli/src/main/java/org/apache/parquet/cli/json/AvroJson.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-cli/src/main/java/org/apache/parquet/cli/json/AvroJson.java?ref=10f57a3779264ba222288defd1472d66ac2ae135"
            },
            {
                "patch": "@@ -16,22 +16,60 @@\n  * specific language governing permissions and limitations\n  * under the License.\n  */\n+\n package org.apache.parquet.cli.commands;\n \n+import com.beust.jcommander.JCommander;\n import org.junit.Assert;\n+import org.junit.Rule;\n import org.junit.Test;\n+import org.junit.rules.TemporaryFolder;\n \n+import java.io.BufferedWriter;\n import java.io.File;\n+import java.io.FileWriter;\n import java.io.IOException;\n \n public class ToAvroCommandTest extends AvroFileTest {\n+  @Rule\n+  public TemporaryFolder folder = new TemporaryFolder();\n+\n   @Test\n-  public void testToAvroCommand() throws IOException {\n+  public void testToAvroCommandFromParquet() throws IOException {\n     File avroFile = toAvro(parquetFile());\n     Assert.assertTrue(avroFile.exists());\n   }\n \n   @Test\n+  public void testToAvroCommandFromJson() throws IOException {\n+    final File jsonInputFile = folder.newFile(\"sample.json\");\n+    final File avroOutputFile = folder.newFile(\"sample.avro\");\n+\n+    // Write the json to the file, so we can read it again.\n+    final String inputJson = \"{\\\"id\\\": 1, \\\"name\\\": \\\"Alice\\\"}\\n\" +\n+      \"{\\\"id\\\": 2, \\\"name\\\": \\\"Bob\\\"}\\n\" +\n+      \"{\\\"id\\\": 3, \\\"name\\\": \\\"Carol\\\"}\\n\" +\n+      \"{\\\"id\\\": 4, \\\"name\\\": \\\"Dave\\\"}\";\n+\n+    try (BufferedWriter writer = new BufferedWriter(new FileWriter(jsonInputFile))) {\n+      writer.write(inputJson);\n+    }\n+\n+    ToAvroCommand cmd = new ToAvroCommand(null);\n+\n+    JCommander\n+      .newBuilder()\n+      .addObject(cmd)\n+      .build()\n+      .parse(\n+        jsonInputFile.getAbsolutePath(),\n+        \"--output\",\n+        avroOutputFile.getAbsolutePath()\n+      );\n+\n+    assert (cmd.run() == 0);\n+  }\n+\n   public void testToAvroCommandWithGzipCompression() throws IOException {\n     File avroFile = toAvro(parquetFile(), \"GZIP\");\n     Assert.assertTrue(avroFile.exists());",
                "additions": 39,
                "raw_url": "https://github.com/apache/parquet-mr/raw/10f57a3779264ba222288defd1472d66ac2ae135/parquet-cli/src/test/java/org/apache/parquet/cli/commands/ToAvroCommandTest.java",
                "status": "modified",
                "changes": 40,
                "deletions": 1,
                "sha": "9344a785dafa3f718047d1f2bb2529e8da48dd83",
                "blob_url": "https://github.com/apache/parquet-mr/blob/10f57a3779264ba222288defd1472d66ac2ae135/parquet-cli/src/test/java/org/apache/parquet/cli/commands/ToAvroCommandTest.java",
                "filename": "parquet-cli/src/test/java/org/apache/parquet/cli/commands/ToAvroCommandTest.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-cli/src/test/java/org/apache/parquet/cli/commands/ToAvroCommandTest.java?ref=10f57a3779264ba222288defd1472d66ac2ae135"
            },
            {
                "patch": "@@ -105,7 +105,7 @@\n \n     <!-- parquet-cli dependencies -->\n     <opencsv.version>2.3</opencsv.version>\n-    <jcommander.version>1.35</jcommander.version>\n+    <jcommander.version>1.72</jcommander.version>\n     <zstd-jni.version>1.4.0-1</zstd-jni.version>\n   </properties>\n ",
                "additions": 1,
                "raw_url": "https://github.com/apache/parquet-mr/raw/10f57a3779264ba222288defd1472d66ac2ae135/pom.xml",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "daea6197e63ed49533050f1fc2bfb3852a491813",
                "blob_url": "https://github.com/apache/parquet-mr/blob/10f57a3779264ba222288defd1472d66ac2ae135/pom.xml",
                "filename": "pom.xml",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/pom.xml?ref=10f57a3779264ba222288defd1472d66ac2ae135"
            }
        ],
        "bug_id": "parquet-mr_2",
        "parent": "https://github.com/apache/parquet-mr/commit/59ae0346cdda2c2fa00698276d9cba82a73c6856",
        "message": "PARQUET-1596: PARQUET-1375 broke parquet-cli's to-avro command (#648)\n\n* PARQUET-1596: PARQUET-1375 broke parquet-cli's to-avro command\r\n\r\nThe expected NPE:\r\n\r\ncat /Users/fokkodriesprong/Desktop/parquet-mr/parquet-cli/target/surefire-reports/org.apache.parquet.cli.commands.ToAvroCommandTest.txt\r\n-------------------------------------------------------------------------------\r\nTest set: org.apache.parquet.cli.commands.ToAvroCommandTest\r\n-------------------------------------------------------------------------------\r\nTests run: 2, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.154 sec <<< FAILURE!\r\ntestToAvroCommandFromJson(org.apache.parquet.cli.commands.ToAvroCommandTest)  Time elapsed: 0.052 sec  <<< ERROR!\r\njava.lang.NullPointerException\r\n\tat org.apache.hadoop.fs.FileSystem.getDefaultUri(FileSystem.java:180)\r\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:361)\r\n\tat org.apache.hadoop.fs.FileSystem.getLocal(FileSystem.java:344)\r\n\tat org.apache.parquet.cli.BaseCommand.defaultFS(BaseCommand.java:81)\r\n\tat org.apache.parquet.cli.BaseCommand.qualifiedPath(BaseCommand.java:164)\r\n\tat org.apache.parquet.cli.BaseCommand.openSeekable(BaseCommand.java:215)\r\n\tat org.apache.parquet.cli.BaseCommand.getAvroSchema(BaseCommand.java:375)\r\n\tat org.apache.parquet.cli.commands.ToAvroCommand.run(ToAvroCommand.java:93)\r\n\tat org.apache.parquet.cli.commands.ToAvroCommandTest.testToAvroCommandFromJson(ToAvroCommandTest.java:72)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)\r\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\r\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)\r\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\r\n\tat org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)\r\n\tat org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)\r\n\tat org.junit.rules.ExternalResource$1.evaluate(ExternalResource.java:48)\r\n\tat org.junit.rules.RunRules.evaluate(RunRules.java:20)\r\n\tat org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)\r\n\tat org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)\r\n\tat org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)\r\n\tat org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)\r\n\tat org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)\r\n\tat org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)\r\n\tat org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)\r\n\tat org.junit.runners.ParentRunner.run(ParentRunner.java:363)\r\n\tat org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:53)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.executeTestSet(JUnit4Provider.java:123)\r\n\tat org.apache.maven.surefire.junit4.JUnit4Provider.invoke(JUnit4Provider.java:104)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat org.apache.maven.surefire.util.ReflectionUtils.invokeMethodWithArray(ReflectionUtils.java:164)\r\n\tat org.apache.maven.surefire.booter.ProviderFactory$ProviderProxy.invoke(ProviderFactory.java:110)\r\n\tat org.apache.maven.surefire.booter.SurefireStarter.invokeProvider(SurefireStarter.java:175)\r\n\tat org.apache.maven.surefire.booter.SurefireStarter.runSuitesInProcessWhenForked(SurefireStarter.java:107)\r\n\tat org.apache.maven.surefire.booter.ForkedBooter.main(ForkedBooter.java:68)",
        "repo": "parquet-mr"
    },
    {
        "commit": "https://github.com/apache/parquet-mr/commit/a918c493296c88da94b36600213c7c188f2589b4",
        "file": [
            {
                "patch": "@@ -146,6 +146,22 @@ public void testSchemaConverterDecimal() {\n     Assert.assertEquals(expected, schemaElements);\n   }\n \n+  @Test\n+  public void testLogicalTypesBackwardCompatibleWithConvertedTypes() {\n+    ParquetMetadataConverter parquetMetadataConverter = new ParquetMetadataConverter();\n+    MessageType expected = Types.buildMessage()\n+      .required(PrimitiveTypeName.BINARY)\n+      .as(OriginalType.DECIMAL).precision(9).scale(2)\n+      .named(\"aBinaryDecimal\")\n+      .named(\"Message\");\n+    List<SchemaElement> parquetSchema = parquetMetadataConverter.toParquetSchema(expected);\n+    // Set logical type field to null to test backward compatibility with files written by older API,\n+    // where converted_types are written to the metadata, but logicalType is missing\n+    parquetSchema.get(1).setLogicalType(null);\n+    MessageType schema = parquetMetadataConverter.fromParquetSchema(parquetSchema, null);\n+    assertEquals(expected, schema);\n+  }\n+\n   @Test\n   public void testEnumEquivalence() {\n     ParquetMetadataConverter parquetMetadataConverter = new ParquetMetadataConverter();",
                "additions": 16,
                "raw_url": "https://github.com/apache/parquet-mr/raw/a918c493296c88da94b36600213c7c188f2589b4/parquet-hadoop/src/test/java/org/apache/parquet/format/converter/TestParquetMetadataConverter.java",
                "status": "modified",
                "changes": 16,
                "deletions": 0,
                "sha": "b3eebd6aee4f28f65f9906b2ae63610873df7fe4",
                "blob_url": "https://github.com/apache/parquet-mr/blob/a918c493296c88da94b36600213c7c188f2589b4/parquet-hadoop/src/test/java/org/apache/parquet/format/converter/TestParquetMetadataConverter.java",
                "filename": "parquet-hadoop/src/test/java/org/apache/parquet/format/converter/TestParquetMetadataConverter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/test/java/org/apache/parquet/format/converter/TestParquetMetadataConverter.java?ref=a918c493296c88da94b36600213c7c188f2589b4"
            }
        ],
        "bug_id": "parquet-mr_3",
        "parent": "https://github.com/apache/parquet-mr/commit/3fd2492fcce073f0c36e4d7e23e34881557e6e5e",
        "message": "PARQUET-1317: Fix ParquetMetadataConverter throw NPE (#491)\n\nNew test case in TestParquetMetadataConverter to reproduce NPE and ensure backward compatibility",
        "repo": "parquet-mr"
    },
    {
        "commit": "https://github.com/apache/parquet-mr/commit/3fd2492fcce073f0c36e4d7e23e34881557e6e5e",
        "file": [
            {
                "patch": "@@ -1170,9 +1170,9 @@ private void buildChildren(Types.GroupBuilder builder,\n       }\n       if (schemaElement.isSetConverted_type()) {\n         LogicalTypeAnnotation originalType = getOriginalType(schemaElement.converted_type, schemaElement);\n-        LogicalTypeAnnotation newLogicalType = getOriginalType(schemaElement.logicalType);\n+        LogicalTypeAnnotation newLogicalType = schemaElement.isSetLogicalType() ? getOriginalType(schemaElement.logicalType) : null;\n         if (!originalType.equals(newLogicalType)) {\n-          childBuilder.as(getOriginalType(schemaElement.converted_type, schemaElement));\n+          childBuilder.as(originalType);\n         }\n       }\n       if (schemaElement.isSetField_id()) {",
                "additions": 2,
                "raw_url": "https://github.com/apache/parquet-mr/raw/3fd2492fcce073f0c36e4d7e23e34881557e6e5e/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "2baad15cd33e32bc15a00b30880b11e3d74bf0f9",
                "blob_url": "https://github.com/apache/parquet-mr/blob/3fd2492fcce073f0c36e4d7e23e34881557e6e5e/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java?ref=3fd2492fcce073f0c36e4d7e23e34881557e6e5e"
            }
        ],
        "bug_id": "parquet-mr_4",
        "parent": "https://github.com/apache/parquet-mr/commit/aed9097640c7adffe1151b32e86b5efc3702c657",
        "message": "PARQUET-1317: Fix ParquetMetadataConverter throw NPE (#489)",
        "repo": "parquet-mr"
    },
    {
        "commit": "https://github.com/apache/parquet-mr/commit/fc2c29df71c8455346a00b43dd1c4f118c335d2c",
        "file": [
            {
                "patch": "@@ -106,9 +106,9 @@ public ParquetRecordReaderWrapper(\n     } else {\n       realReader = null;\n       eof = true;\n-      if (valueObj == null) { // Should initialize the value for createValue\n-        valueObj = new ArrayWritable(Writable.class, new Writable[schemaSize]);\n-      }\n+    }\n+    if (valueObj == null) { // Should initialize the value for createValue\n+      valueObj = new ArrayWritable(Writable.class, new Writable[schemaSize]);\n     }\n   }\n ",
                "additions": 3,
                "raw_url": "https://github.com/apache/parquet-mr/raw/fc2c29df71c8455346a00b43dd1c4f118c335d2c/parquet-hive/parquet-hive-storage-handler/src/main/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java",
                "status": "modified",
                "changes": 6,
                "deletions": 3,
                "sha": "9e9878122e0573159cbb9b2f9228f2d7826f16e2",
                "blob_url": "https://github.com/apache/parquet-mr/blob/fc2c29df71c8455346a00b43dd1c4f118c335d2c/parquet-hive/parquet-hive-storage-handler/src/main/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java",
                "filename": "parquet-hive/parquet-hive-storage-handler/src/main/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hive/parquet-hive-storage-handler/src/main/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java?ref=fc2c29df71c8455346a00b43dd1c4f118c335d2c"
            }
        ],
        "bug_id": "parquet-mr_5",
        "parent": "https://github.com/apache/parquet-mr/commit/17864dfc0711d52d5af330469a1c2bd76128d46e",
        "message": "PARQUET-19: Fix NPE when an empty file is included in a Hive query that uses CombineHiveInputFormat\n\nMake sure the valueObj instance variable is always initialized.  This change is neeeded when running a Hive query that uses the CombineHiveInputFormat and the first file in the combined split is empty.  This can lead to a NullPointerException because the valueObj is null when the CombineHiveInputFormat calls the createValue method.\n\nAuthor: Matthieu Martin <ma.tt.b.ma.rt.in+parquet@gmail.com>\n\nCloses #19 from matt-martin/fix_for_empty_files_NPE_with_CombineHiveInputFormat and squashes the following commits:\n\n6c3a7f5 [Matthieu Martin] Make sure the valueObj instance variable is always initialized.  This change is neeeded when running a Hive query that uses the CombineHiveInputFormat and the first file in the combined split is empty.  This can lead to a NullPointerException because the valueObj is null when the CombineHiveInputFormat calls the createValue method.",
        "repo": "parquet-mr"
    },
    {
        "commit": "https://github.com/apache/parquet-mr/commit/04ad0c4461f75fec456e2e61fa4b68d960a088b5",
        "file": [
            {
                "patch": "@@ -195,7 +195,7 @@ public String memUsageString(String prefix) {\n         plainValuesWriter.\n         memUsageString(prefix + \" plain:\"),\n         prefix + \" dict:\" + dictionaryByteSize, \n-        prefix + \" values:\" + (encodedValues.size() * 4), \n+        prefix + \" values:\" + ((encodedValues == null ? 0 : encodedValues.size()) * 4), \n         prefix\n         );\n   }",
                "additions": 1,
                "raw_url": "https://github.com/apache/parquet-mr/raw/04ad0c4461f75fec456e2e61fa4b68d960a088b5/parquet-column/src/main/java/parquet/column/values/dictionary/DictionaryValuesWriter.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "7ab37701ea76f16a4e09b60932f3f8f10f4cbb00",
                "blob_url": "https://github.com/apache/parquet-mr/blob/04ad0c4461f75fec456e2e61fa4b68d960a088b5/parquet-column/src/main/java/parquet/column/values/dictionary/DictionaryValuesWriter.java",
                "filename": "parquet-column/src/main/java/parquet/column/values/dictionary/DictionaryValuesWriter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-column/src/main/java/parquet/column/values/dictionary/DictionaryValuesWriter.java?ref=04ad0c4461f75fec456e2e61fa4b68d960a088b5"
            }
        ],
        "bug_id": "parquet-mr_6",
        "parent": "https://github.com/apache/parquet-mr/commit/ff55567cf448df98bafe3a3512349bf7d9f4d78a",
        "message": "Merge pull request #190 from wesleypeck/fix_dvw_npe\n\nFixes #189: NPE in DictionaryValuesWriter.",
        "repo": "parquet-mr"
    },
    {
        "commit": "https://github.com/apache/parquet-mr/commit/14097c64d243794610788d3ebb2e81ba8fd867c0",
        "file": [
            {
                "patch": "@@ -452,8 +452,24 @@ public void writeCollection(GroupType schema, Schema avroSchema,\n                                 Collection<?> array) {\n       if (array.size() > 0) {\n         recordConsumer.startField(OLD_LIST_REPEATED_NAME, 0);\n-        for (Object elt : array) {\n-          writeValue(schema.getType(0), avroSchema.getElementType(), elt);\n+        try {\n+          for (Object elt : array) {\n+            writeValue(schema.getType(0), avroSchema.getElementType(), elt);\n+          }\n+        } catch (NullPointerException e) {\n+          // find the null element and throw a better error message\n+          int i = 0;\n+          for (Object elt : array) {\n+            if (elt == null) {\n+              throw new NullPointerException(\n+                  \"Array contains a null element at \" + i + \"\\n\" +\n+                  \"Set parquet.avro.write-old-list-structure=false to turn \" +\n+                  \"on support for arrays with null elements.\");\n+            }\n+            i += 1;\n+          }\n+          // no element was null, throw the original exception\n+          throw e;\n         }\n         recordConsumer.endField(OLD_LIST_REPEATED_NAME, 0);\n       }\n@@ -464,8 +480,22 @@ protected void writeObjectArray(GroupType type, Schema schema,\n                                     Object[] array) {\n       if (array.length > 0) {\n         recordConsumer.startField(OLD_LIST_REPEATED_NAME, 0);\n-        for (Object element : array) {\n-          writeValue(type.getType(0), schema.getElementType(), element);\n+        try {\n+          for (Object element : array) {\n+            writeValue(type.getType(0), schema.getElementType(), element);\n+          }\n+        } catch (NullPointerException e) {\n+          // find the null element and throw a better error message\n+          for (int i = 0; i < array.length; i += 1) {\n+            if (array[i] == null) {\n+              throw new NullPointerException(\n+                  \"Array contains a null element at \" + i + \"\\n\" +\n+                  \"Set parquet.avro.write-old-list-structure=false to turn \" +\n+                  \"on support for arrays with null elements.\");\n+            }\n+          }\n+          // no element was null, throw the original exception\n+          throw e;\n         }\n         recordConsumer.endField(OLD_LIST_REPEATED_NAME, 0);\n       }",
                "additions": 34,
                "raw_url": "https://github.com/apache/parquet-mr/raw/14097c64d243794610788d3ebb2e81ba8fd867c0/parquet-avro/src/main/java/org/apache/parquet/avro/AvroWriteSupport.java",
                "status": "modified",
                "changes": 38,
                "deletions": 4,
                "sha": "48fc01ebf2c1d28b51a902acbb84781059b1884a",
                "blob_url": "https://github.com/apache/parquet-mr/blob/14097c64d243794610788d3ebb2e81ba8fd867c0/parquet-avro/src/main/java/org/apache/parquet/avro/AvroWriteSupport.java",
                "filename": "parquet-avro/src/main/java/org/apache/parquet/avro/AvroWriteSupport.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-avro/src/main/java/org/apache/parquet/avro/AvroWriteSupport.java?ref=14097c64d243794610788d3ebb2e81ba8fd867c0"
            },
            {
                "patch": "@@ -43,6 +43,7 @@\n import org.apache.parquet.io.api.Binary;\n import org.apache.parquet.io.api.RecordConsumer;\n import org.apache.parquet.schema.MessageTypeParser;\n+import org.junit.Assert;\n import org.junit.Test;\n import org.junit.runner.RunWith;\n import org.junit.runners.Parameterized;\n@@ -347,7 +348,8 @@ public void testArrayWithNullValues() throws Exception {\n       writer.write(record);\n       fail(\"Should not succeed writing an array with null values\");\n     } catch (Exception e) {\n-      // expected\n+      Assert.assertTrue(\"Error message should provide context and help\",\n+          e.getMessage().contains(\"parquet.avro.write-old-list-structure\"));\n     } finally {\n       writer.close();\n     }",
                "additions": 3,
                "raw_url": "https://github.com/apache/parquet-mr/raw/14097c64d243794610788d3ebb2e81ba8fd867c0/parquet-avro/src/test/java/org/apache/parquet/avro/TestReadWriteOldListBehavior.java",
                "status": "modified",
                "changes": 4,
                "deletions": 1,
                "sha": "64caacc8b0dc8bd6141249f86cbf74a3aad0496b",
                "blob_url": "https://github.com/apache/parquet-mr/blob/14097c64d243794610788d3ebb2e81ba8fd867c0/parquet-avro/src/test/java/org/apache/parquet/avro/TestReadWriteOldListBehavior.java",
                "filename": "parquet-avro/src/test/java/org/apache/parquet/avro/TestReadWriteOldListBehavior.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-avro/src/test/java/org/apache/parquet/avro/TestReadWriteOldListBehavior.java?ref=14097c64d243794610788d3ebb2e81ba8fd867c0"
            }
        ],
        "bug_id": "parquet-mr_7",
        "parent": "https://github.com/apache/parquet-mr/commit/e32aa6fe0d5260c21b35c34075eb5b69afbca464",
        "message": "PARQUET-387: Improve NPE message when avro arrays contain null.\n\nPreviously, the NPE had no error message but the Avro support accepts\nschemas that have nullable array elements.\n\nAuthor: Ryan Blue <blue@apache.org>\n\nCloses #291 from rdblue/PARQUET-387-fix-npe-message and squashes the following commits:\n\n39d3c83 [Ryan Blue] PARQUET-387: Update test case to verify help message.\nd6b6bd8 [Ryan Blue] PARQUET-387: Improve NPE message when avro arrays contain null.",
        "repo": "parquet-mr"
    },
    {
        "commit": "https://github.com/apache/parquet-mr/commit/0a36e35cf6c52a9e79fdfbbb8584a8adc3a17b6c",
        "file": [
            {
                "patch": "@@ -195,7 +195,7 @@ public String memUsageString(String prefix) {\n         plainValuesWriter.\n         memUsageString(prefix + \" plain:\"),\n         prefix + \" dict:\" + dictionaryByteSize, \n-        prefix + \" values:\" + (encodedValues.size() * 4), \n+        prefix + \" values:\" + ((encodedValues == null ? 0 : encodedValues.size()) * 4), \n         prefix\n         );\n   }",
                "additions": 1,
                "raw_url": "https://github.com/apache/parquet-mr/raw/0a36e35cf6c52a9e79fdfbbb8584a8adc3a17b6c/parquet-column/src/main/java/parquet/column/values/dictionary/DictionaryValuesWriter.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "7ab37701ea76f16a4e09b60932f3f8f10f4cbb00",
                "blob_url": "https://github.com/apache/parquet-mr/blob/0a36e35cf6c52a9e79fdfbbb8584a8adc3a17b6c/parquet-column/src/main/java/parquet/column/values/dictionary/DictionaryValuesWriter.java",
                "filename": "parquet-column/src/main/java/parquet/column/values/dictionary/DictionaryValuesWriter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-column/src/main/java/parquet/column/values/dictionary/DictionaryValuesWriter.java?ref=0a36e35cf6c52a9e79fdfbbb8584a8adc3a17b6c"
            }
        ],
        "bug_id": "parquet-mr_8",
        "parent": "https://github.com/apache/parquet-mr/commit/fd3b05cf4d62cd098f85510fd6ed05469911cba2",
        "message": "Fixes #189: NPE in DictionaryValuesWriter.",
        "repo": "parquet-mr"
    },
    {
        "commit": "https://github.com/apache/parquet-mr/commit/70eada470f069ea27c5e2d47d1004fec56f7dcca",
        "file": [
            {
                "patch": "@@ -56,6 +56,10 @@ public void write(TupleEntry record) {\n     final List<Type> fields = rootSchema.getFields();\n     int i = 0;\n     for (Type field : fields) {\n+      if (record.getObject(field.getName())==null) {\n+        i++;\n+        continue;\n+      }\n       recordConsumer.startField(field.getName(), i);\n       if (field.isPrimitive()) {\n         writePrimitive(record, field.asPrimitiveType());",
                "additions": 4,
                "raw_url": "https://github.com/apache/parquet-mr/raw/70eada470f069ea27c5e2d47d1004fec56f7dcca/parquet-cascading/src/main/java/parquet/cascading/TupleWriteSupport.java",
                "status": "modified",
                "changes": 4,
                "deletions": 0,
                "sha": "5d93ef4a5518c5c917c5c548721c465576e5dab3",
                "blob_url": "https://github.com/apache/parquet-mr/blob/70eada470f069ea27c5e2d47d1004fec56f7dcca/parquet-cascading/src/main/java/parquet/cascading/TupleWriteSupport.java",
                "filename": "parquet-cascading/src/main/java/parquet/cascading/TupleWriteSupport.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-cascading/src/main/java/parquet/cascading/TupleWriteSupport.java?ref=70eada470f069ea27c5e2d47d1004fec56f7dcca"
            }
        ],
        "bug_id": "parquet-mr_9",
        "parent": "https://github.com/apache/parquet-mr/commit/76bbf4a88645abc657ba6e4c2dc636712f03b944",
        "message": "NULL tuples cause NPE when writing",
        "repo": "parquet-mr"
    },
    {
        "commit": "https://github.com/apache/parquet-mr/commit/08c8f82011925d3ba083f8b23b8f1c0af29c0d7e",
        "file": [
            {
                "patch": "@@ -16,7 +16,7 @@\n   <url>https://github.com/Parquet/parquet-mr</url>\n \n   <properties>\n-    <elephant-bird.version>3.0.9-SNAPSHOT</elephant-bird.version>\n+    <elephant-bird.version>3.0.8</elephant-bird.version>\n   </properties>\n \n   <dependencies>",
                "additions": 1,
                "raw_url": "https://github.com/apache/parquet-mr/raw/08c8f82011925d3ba083f8b23b8f1c0af29c0d7e/parquet-thrift/pom.xml",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "7510464b527579a596c518afd2ca2fc9b5ebd284",
                "blob_url": "https://github.com/apache/parquet-mr/blob/08c8f82011925d3ba083f8b23b8f1c0af29c0d7e/parquet-thrift/pom.xml",
                "filename": "parquet-thrift/pom.xml",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-thrift/pom.xml?ref=08c8f82011925d3ba083f8b23b8f1c0af29c0d7e"
            },
            {
                "patch": "@@ -88,7 +88,7 @@ public static ThriftMetaData fromExtraMetaData(\n \n   public Map<String, String> toExtraMetaData() {\n     final Map<String, String> map = new HashMap<String, String>();\n-    map.put(THRIFT_CLASS, thriftClass.getName());\n+    map.put(THRIFT_CLASS, getThriftClass().getName());\n     map.put(THRIFT_DESCRIPTOR, descriptor.toJSON());\n     return map;\n   }",
                "additions": 1,
                "raw_url": "https://github.com/apache/parquet-mr/raw/08c8f82011925d3ba083f8b23b8f1c0af29c0d7e/parquet-thrift/src/main/java/parquet/thrift/ThriftMetaData.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "5eb09152d23e79d460bab7b87d6f51c83d4d15b2",
                "blob_url": "https://github.com/apache/parquet-mr/blob/08c8f82011925d3ba083f8b23b8f1c0af29c0d7e/parquet-thrift/src/main/java/parquet/thrift/ThriftMetaData.java",
                "filename": "parquet-thrift/src/main/java/parquet/thrift/ThriftMetaData.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-thrift/src/main/java/parquet/thrift/ThriftMetaData.java?ref=08c8f82011925d3ba083f8b23b8f1c0af29c0d7e"
            }
        ],
        "bug_id": "parquet-mr_10",
        "parent": "https://github.com/apache/parquet-mr/commit/073421762e172b68af81c97374b5c45234ce4f17",
        "message": "use published EB, fix NPE in ThriftMetaData",
        "repo": "parquet-mr"
    },
    {
        "commit": "https://github.com/apache/parquet-mr/commit/000659a2d80ca4584e0e39fae4164009944d3549",
        "file": [
            {
                "patch": "@@ -56,6 +56,10 @@ public void write(TupleEntry record) {\n     final List<Type> fields = rootSchema.getFields();\n     int i = 0;\n     for (Type field : fields) {\n+      if (record.getObject(field.getName())==null) {\n+        i++;\n+        continue;\n+      }\n       recordConsumer.startField(field.getName(), i);\n       if (field.isPrimitive()) {\n         writePrimitive(record, field.asPrimitiveType());",
                "additions": 4,
                "raw_url": "https://github.com/apache/parquet-mr/raw/000659a2d80ca4584e0e39fae4164009944d3549/parquet-cascading/src/main/java/parquet/cascading/TupleWriteSupport.java",
                "status": "modified",
                "changes": 4,
                "deletions": 0,
                "sha": "5d93ef4a5518c5c917c5c548721c465576e5dab3",
                "blob_url": "https://github.com/apache/parquet-mr/blob/000659a2d80ca4584e0e39fae4164009944d3549/parquet-cascading/src/main/java/parquet/cascading/TupleWriteSupport.java",
                "filename": "parquet-cascading/src/main/java/parquet/cascading/TupleWriteSupport.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-cascading/src/main/java/parquet/cascading/TupleWriteSupport.java?ref=000659a2d80ca4584e0e39fae4164009944d3549"
            }
        ],
        "bug_id": "parquet-mr_11",
        "parent": "https://github.com/apache/parquet-mr/commit/76bbf4a88645abc657ba6e4c2dc636712f03b944",
        "message": "Merge pull request #1 from jalkjaer/cascading_sink\n\nNULL tuples causes NPE when writing",
        "repo": "parquet-mr"
    },
    {
        "commit": "https://github.com/apache/parquet-mr/commit/89321a2dee438328e75a11954e972175c78f0a2a",
        "file": [
            {
                "patch": "@@ -53,12 +53,12 @@ public void setMinMaxFromBytes(byte[] minBytes, byte[] maxBytes) {\n \n   @Override\n   public byte[] getMaxBytes() {\n-    return max.getBytes();\n+    return max == null ? null : max.getBytes();\n   }\n \n   @Override\n   public byte[] getMinBytes() {\n-    return min.getBytes();\n+    return min == null ? null : min.getBytes();\n   }\n \n   @Override",
                "additions": 2,
                "raw_url": "https://github.com/apache/parquet-mr/raw/89321a2dee438328e75a11954e972175c78f0a2a/parquet-column/src/main/java/org/apache/parquet/column/statistics/BinaryStatistics.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "6341f96a368221f132b12629e94132bf4de206ed",
                "blob_url": "https://github.com/apache/parquet-mr/blob/89321a2dee438328e75a11954e972175c78f0a2a/parquet-column/src/main/java/org/apache/parquet/column/statistics/BinaryStatistics.java",
                "filename": "parquet-column/src/main/java/org/apache/parquet/column/statistics/BinaryStatistics.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-column/src/main/java/org/apache/parquet/column/statistics/BinaryStatistics.java?ref=89321a2dee438328e75a11954e972175c78f0a2a"
            },
            {
                "patch": "@@ -19,6 +19,7 @@\n package org.apache.parquet.format.converter;\n \n import static java.util.Collections.emptyList;\n+import static org.apache.parquet.schema.MessageTypeParser.parseMessageType;\n import static org.junit.Assert.assertEquals;\n import static org.junit.Assert.fail;\n import static org.apache.parquet.format.CompressionCodec.UNCOMPRESSED;\n@@ -34,11 +35,19 @@\n import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n import java.util.List;\n import java.util.Random;\n import java.util.Set;\n import java.util.TreeSet;\n \n+import org.apache.parquet.column.statistics.BinaryStatistics;\n+import org.apache.parquet.hadoop.metadata.BlockMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n+import org.apache.parquet.hadoop.metadata.ColumnPath;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n import org.junit.Assert;\n import org.junit.Test;\n \n@@ -252,4 +261,26 @@ public void randomTestFilterMetaData() {\n     }\n   }\n \n+  @Test\n+  public void testNullFieldMetadataDebugLogging() throws NoSuchFieldException, IllegalAccessException, IOException {\n+    MessageType schema = parseMessageType(\"message test { optional binary some_null_field; }\");\n+    org.apache.parquet.hadoop.metadata.FileMetaData fileMetaData = new org.apache.parquet.hadoop.metadata.FileMetaData(schema, new HashMap<String, String>(), null);\n+    List<BlockMetaData> blockMetaDataList = new ArrayList<BlockMetaData>();\n+    BlockMetaData blockMetaData = new BlockMetaData();\n+    blockMetaData.addColumn(createColumnChunkMetaData());\n+    blockMetaDataList.add(blockMetaData);\n+    ParquetMetadata metadata = new ParquetMetadata(fileMetaData, blockMetaDataList);\n+    ParquetMetadata.toJSON(metadata);\n+  }\n+\n+  private ColumnChunkMetaData createColumnChunkMetaData() {\n+    Set<Encoding> e = new HashSet<Encoding>();\n+    PrimitiveTypeName t = PrimitiveTypeName.BINARY;\n+    ColumnPath p = ColumnPath.get(\"foo\");\n+    CompressionCodecName c = CompressionCodecName.GZIP;\n+    BinaryStatistics s = new BinaryStatistics();\n+    ColumnChunkMetaData md = ColumnChunkMetaData.get(p, t, c, e, s,\n+            0, 0, 0, 0, 0);\n+    return md;\n+  }\n }",
                "additions": 31,
                "raw_url": "https://github.com/apache/parquet-mr/raw/89321a2dee438328e75a11954e972175c78f0a2a/parquet-hadoop/src/test/java/org/apache/parquet/format/converter/TestParquetMetadataConverter.java",
                "status": "modified",
                "changes": 31,
                "deletions": 0,
                "sha": "1a740fe8521f6ab87c631a839bf795bd97f44fa1",
                "blob_url": "https://github.com/apache/parquet-mr/blob/89321a2dee438328e75a11954e972175c78f0a2a/parquet-hadoop/src/test/java/org/apache/parquet/format/converter/TestParquetMetadataConverter.java",
                "filename": "parquet-hadoop/src/test/java/org/apache/parquet/format/converter/TestParquetMetadataConverter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/test/java/org/apache/parquet/format/converter/TestParquetMetadataConverter.java?ref=89321a2dee438328e75a11954e972175c78f0a2a"
            }
        ],
        "bug_id": "parquet-mr_12",
        "parent": "https://github.com/apache/parquet-mr/commit/29283b775291bf03cd9a7e1aaa496faaa5757578",
        "message": "PARQUET-311: Fix NPE when debug logging metadata\n\nFixes the issue reported at https://issues.apache.org/jira/browse/PARQUET-311\n\nAuthor: Nezih Yigitbasi <nyigitbasi@netflix.com>\n\nCloses #221 from nezihyigitbasi/debug-log-fix and squashes the following commits:\n\n59129ed [Nezih Yigitbasi] PARQUET-311: Fix NPE when debug logging metadata",
        "repo": "parquet-mr"
    },
    {
        "commit": "https://github.com/apache/parquet-mr/commit/f68dbc3ea20230cb14ed3364539ad16e114bcdd9",
        "file": [
            {
                "patch": "@@ -128,7 +128,8 @@ public boolean equals(Object obj) {\n //    }\n \n     public boolean equals(Case other) {\n-      return startLevel == other.startLevel\n+      return other != null\n+          && startLevel == other.startLevel\n           && depth == other.depth\n           && nextLevel == other.nextLevel\n           && nextState == other.nextState",
                "additions": 2,
                "raw_url": "https://github.com/apache/parquet-mr/raw/f68dbc3ea20230cb14ed3364539ad16e114bcdd9/parquet-column/src/main/java/org/apache/parquet/io/RecordReaderImplementation.java",
                "status": "modified",
                "changes": 3,
                "deletions": 1,
                "sha": "f883c4a4c098d50041c1fe823304c58cce0bd0a0",
                "blob_url": "https://github.com/apache/parquet-mr/blob/f68dbc3ea20230cb14ed3364539ad16e114bcdd9/parquet-column/src/main/java/org/apache/parquet/io/RecordReaderImplementation.java",
                "filename": "parquet-column/src/main/java/org/apache/parquet/io/RecordReaderImplementation.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-column/src/main/java/org/apache/parquet/io/RecordReaderImplementation.java?ref=f68dbc3ea20230cb14ed3364539ad16e114bcdd9"
            },
            {
                "patch": "@@ -58,7 +58,7 @@ public String get() {\n     @Override\n     public boolean equals(Object o) {\n       if (this == o) return true;\n-      return getClass() == o.getClass() && s.equals(((Atom) o).s);\n+      return o != null && getClass() == o.getClass() && s.equals(((Atom) o).s);\n     }\n \n     @Override\n@@ -97,7 +97,7 @@ public OneOf(List<GlobNode> children) {\n     @Override\n     public boolean equals(Object o) {\n       if (this == o) return true;\n-      return getClass() == o.getClass() && children.equals(((OneOf) o).children);\n+      return o != null && getClass() == o.getClass() && children.equals(((OneOf) o).children);\n     }\n \n     @Override\n@@ -136,7 +136,7 @@ public GlobNodeSequence(List<GlobNode> children) {\n     @Override\n     public boolean equals(Object o) {\n       if (this == o) return true;\n-      return getClass() == o.getClass() && children.equals(((OneOf) o).children);\n+      return o != null && getClass() == o.getClass() && children.equals(((OneOf) o).children);\n     }\n \n     @Override",
                "additions": 3,
                "raw_url": "https://github.com/apache/parquet-mr/raw/f68dbc3ea20230cb14ed3364539ad16e114bcdd9/parquet-common/src/main/java/org/apache/parquet/glob/GlobNode.java",
                "status": "modified",
                "changes": 6,
                "deletions": 3,
                "sha": "35c83ddacb6322b56cdd1f8a8801ca821151693e",
                "blob_url": "https://github.com/apache/parquet-mr/blob/f68dbc3ea20230cb14ed3364539ad16e114bcdd9/parquet-common/src/main/java/org/apache/parquet/glob/GlobNode.java",
                "filename": "parquet-common/src/main/java/org/apache/parquet/glob/GlobNode.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-common/src/main/java/org/apache/parquet/glob/GlobNode.java?ref=f68dbc3ea20230cb14ed3364539ad16e114bcdd9"
            },
            {
                "patch": "@@ -65,42 +65,42 @@ private static void generateScheme(boolean isLong, boolean msbFirst,\n     if (!file.getParentFile().exists()) {\n       file.getParentFile().mkdirs();\n     }\n-    FileWriter fw = new FileWriter(file);\n-    fw.append(\"package org.apache.parquet.column.values.bitpacking;\\n\");\n-    fw.append(\"import java.nio.ByteBuffer;\\n\");\n-    fw.append(\"\\n\");\n-    fw.append(\"/**\\n\");\n-    if (msbFirst) {\n-      fw.append(\" * Packs from the Most Significant Bit first\\n\");\n-    } else {\n-      fw.append(\" * Packs from the Least Significant Bit first\\n\");\n-    }\n-    fw.append(\" * \\n\");\n-    fw.append(\" * @author automatically generated\\n\");\n-    fw.append(\" * @see ByteBasedBitPackingGenerator\\n\");\n-    fw.append(\" *\\n\");\n-    fw.append(\" */\\n\");\n-    fw.append(\"public abstract class \" + className + \" {\\n\");\n-    fw.append(\"\\n\");\n-    fw.append(\"  private static final BytePacker\" + nameSuffix + \"[] packers = new BytePacker\" + nameSuffix + \"[\" + (maxBits + 1) + \"];\\n\");\n-    fw.append(\"  static {\\n\");\n-    for (int i = 0; i <= maxBits; i++) {\n-      fw.append(\"    packers[\" + i + \"] = new Packer\" + i + \"();\\n\");\n-    }\n-    fw.append(\"  }\\n\");\n-    fw.append(\"\\n\");\n-    fw.append(\"  public static final BytePacker\" + nameSuffix + \"Factory factory = new BytePacker\" + nameSuffix + \"Factory() {\\n\");\n-    fw.append(\"    public BytePacker\" + nameSuffix + \" newBytePacker\" + nameSuffix + \"(int bitWidth) {\\n\");\n-    fw.append(\"      return packers[bitWidth];\\n\");\n-    fw.append(\"    }\\n\");\n-    fw.append(\"  };\\n\");\n-    fw.append(\"\\n\");\n-    for (int i = 0; i <= maxBits; i++) {\n-      generateClass(fw, i, isLong, msbFirst);\n+    try (FileWriter fw = new FileWriter(file)) {\n+      fw.append(\"package org.apache.parquet.column.values.bitpacking;\\n\");\n+      fw.append(\"import java.nio.ByteBuffer;\\n\");\n+      fw.append(\"\\n\");\n+      fw.append(\"/**\\n\");\n+      if (msbFirst) {\n+        fw.append(\" * Packs from the Most Significant Bit first\\n\");\n+      } else {\n+        fw.append(\" * Packs from the Least Significant Bit first\\n\");\n+      }\n+      fw.append(\" * \\n\");\n+      fw.append(\" * @author automatically generated\\n\");\n+      fw.append(\" * @see ByteBasedBitPackingGenerator\\n\");\n+      fw.append(\" *\\n\");\n+      fw.append(\" */\\n\");\n+      fw.append(\"public abstract class \" + className + \" {\\n\");\n       fw.append(\"\\n\");\n+      fw.append(\"  private static final BytePacker\" + nameSuffix + \"[] packers = new BytePacker\" + nameSuffix + \"[\" + (maxBits + 1) + \"];\\n\");\n+      fw.append(\"  static {\\n\");\n+      for (int i = 0; i <= maxBits; i++) {\n+        fw.append(\"    packers[\" + i + \"] = new Packer\" + i + \"();\\n\");\n+      }\n+      fw.append(\"  }\\n\");\n+      fw.append(\"\\n\");\n+      fw.append(\"  public static final BytePacker\" + nameSuffix + \"Factory factory = new BytePacker\" + nameSuffix + \"Factory() {\\n\");\n+      fw.append(\"    public BytePacker\" + nameSuffix + \" newBytePacker\" + nameSuffix + \"(int bitWidth) {\\n\");\n+      fw.append(\"      return packers[bitWidth];\\n\");\n+      fw.append(\"    }\\n\");\n+      fw.append(\"  };\\n\");\n+      fw.append(\"\\n\");\n+      for (int i = 0; i <= maxBits; i++) {\n+        generateClass(fw, i, isLong, msbFirst);\n+        fw.append(\"\\n\");\n+      }\n+      fw.append(\"}\\n\");\n     }\n-    fw.append(\"}\\n\");\n-    fw.close();\n   }\n \n   private static void generateClass(FileWriter fw, int bitWidth, boolean isLong, boolean msbFirst) throws IOException {",
                "additions": 34,
                "raw_url": "https://github.com/apache/parquet-mr/raw/f68dbc3ea20230cb14ed3364539ad16e114bcdd9/parquet-generator/src/main/java/org/apache/parquet/encoding/bitpacking/ByteBasedBitPackingGenerator.java",
                "status": "modified",
                "changes": 68,
                "deletions": 34,
                "sha": "6b725928a4921d8e8de581d73644141d97eed79b",
                "blob_url": "https://github.com/apache/parquet-mr/blob/f68dbc3ea20230cb14ed3364539ad16e114bcdd9/parquet-generator/src/main/java/org/apache/parquet/encoding/bitpacking/ByteBasedBitPackingGenerator.java",
                "filename": "parquet-generator/src/main/java/org/apache/parquet/encoding/bitpacking/ByteBasedBitPackingGenerator.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-generator/src/main/java/org/apache/parquet/encoding/bitpacking/ByteBasedBitPackingGenerator.java?ref=f68dbc3ea20230cb14ed3364539ad16e114bcdd9"
            },
            {
                "patch": "@@ -56,45 +56,45 @@ private static void generateScheme(String className, boolean msbFirst, String ba\n     if (!file.getParentFile().exists()) {\n       file.getParentFile().mkdirs();\n     }\n-    FileWriter fw = new FileWriter(file);\n-    fw.append(\"package org.apache.parquet.column.values.bitpacking;\\n\");\n-    fw.append(\"\\n\");\n-    fw.append(\"/**\\n\");\n-    fw.append(\" * Based on the original implementation at at https://github.com/lemire/JavaFastPFOR/blob/master/src/integercompression/BitPacking.java\\n\");\n-    fw.append(\" * Which is released under the\\n\");\n-    fw.append(\" * Apache License Version 2.0 http://www.apache.org/licenses/.\\n\");\n-    fw.append(\" * By Daniel Lemire, http://lemire.me/en/\\n\");\n-    fw.append(\" * \\n\");\n-    fw.append(\" * Scheme designed by D. Lemire\\n\");\n-    if (msbFirst) {\n-      fw.append(\" * Adapted to pack from the Most Significant Bit first\\n\");\n-    }\n-    fw.append(\" * \\n\");\n-    fw.append(\" * @author automatically generated\\n\");\n-    fw.append(\" * @see IntBasedBitPackingGenerator\\n\");\n-    fw.append(\" *\\n\");\n-    fw.append(\" */\\n\");\n-    fw.append(\"abstract class \" + className + \" {\\n\");\n-    fw.append(\"\\n\");\n-    fw.append(\"  private static final IntPacker[] packers = new IntPacker[32];\\n\");\n-    fw.append(\"  static {\\n\");\n-    for (int i = 0; i < 32; i++) {\n-      fw.append(\"    packers[\" + i + \"] = new Packer\" + i + \"();\\n\");\n-    }\n-    fw.append(\"  }\\n\");\n-    fw.append(\"\\n\");\n-    fw.append(\"  public static final IntPackerFactory factory = new IntPackerFactory() {\\n\");\n-    fw.append(\"    public IntPacker newIntPacker(int bitWidth) {\\n\");\n-    fw.append(\"      return packers[bitWidth];\\n\");\n-    fw.append(\"    }\\n\");\n-    fw.append(\"  };\\n\");\n-    fw.append(\"\\n\");\n-    for (int i = 0; i < 32; i++) {\n-      generateClass(fw, i, msbFirst);\n+    try (FileWriter fw = new FileWriter(file)) {\n+      fw.append(\"package org.apache.parquet.column.values.bitpacking;\\n\");\n+      fw.append(\"\\n\");\n+      fw.append(\"/**\\n\");\n+      fw.append(\" * Based on the original implementation at at https://github.com/lemire/JavaFastPFOR/blob/master/src/integercompression/BitPacking.java\\n\");\n+      fw.append(\" * Which is released under the\\n\");\n+      fw.append(\" * Apache License Version 2.0 http://www.apache.org/licenses/.\\n\");\n+      fw.append(\" * By Daniel Lemire, http://lemire.me/en/\\n\");\n+      fw.append(\" * \\n\");\n+      fw.append(\" * Scheme designed by D. Lemire\\n\");\n+      if (msbFirst) {\n+        fw.append(\" * Adapted to pack from the Most Significant Bit first\\n\");\n+      }\n+      fw.append(\" * \\n\");\n+      fw.append(\" * @author automatically generated\\n\");\n+      fw.append(\" * @see IntBasedBitPackingGenerator\\n\");\n+      fw.append(\" *\\n\");\n+      fw.append(\" */\\n\");\n+      fw.append(\"abstract class \" + className + \" {\\n\");\n       fw.append(\"\\n\");\n+      fw.append(\"  private static final IntPacker[] packers = new IntPacker[32];\\n\");\n+      fw.append(\"  static {\\n\");\n+      for (int i = 0; i < 32; i++) {\n+        fw.append(\"    packers[\" + i + \"] = new Packer\" + i + \"();\\n\");\n+      }\n+      fw.append(\"  }\\n\");\n+      fw.append(\"\\n\");\n+      fw.append(\"  public static final IntPackerFactory factory = new IntPackerFactory() {\\n\");\n+      fw.append(\"    public IntPacker newIntPacker(int bitWidth) {\\n\");\n+      fw.append(\"      return packers[bitWidth];\\n\");\n+      fw.append(\"    }\\n\");\n+      fw.append(\"  };\\n\");\n+      fw.append(\"\\n\");\n+      for (int i = 0; i < 32; i++) {\n+        generateClass(fw, i, msbFirst);\n+        fw.append(\"\\n\");\n+      }\n+      fw.append(\"}\\n\");\n     }\n-    fw.append(\"}\\n\");\n-    fw.close();\n   }\n \n   private static void generateClass(FileWriter fw, int bitWidth, boolean msbFirst) throws IOException {",
                "additions": 37,
                "raw_url": "https://github.com/apache/parquet-mr/raw/f68dbc3ea20230cb14ed3364539ad16e114bcdd9/parquet-generator/src/main/java/org/apache/parquet/encoding/bitpacking/IntBasedBitPackingGenerator.java",
                "status": "modified",
                "changes": 74,
                "deletions": 37,
                "sha": "300b84d5dc5e13263aac4d91e3a1227155e05a4c",
                "blob_url": "https://github.com/apache/parquet-mr/blob/f68dbc3ea20230cb14ed3364539ad16e114bcdd9/parquet-generator/src/main/java/org/apache/parquet/encoding/bitpacking/IntBasedBitPackingGenerator.java",
                "filename": "parquet-generator/src/main/java/org/apache/parquet/encoding/bitpacking/IntBasedBitPackingGenerator.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-generator/src/main/java/org/apache/parquet/encoding/bitpacking/IntBasedBitPackingGenerator.java?ref=f68dbc3ea20230cb14ed3364539ad16e114bcdd9"
            },
            {
                "patch": "@@ -50,7 +50,11 @@ public void run() throws IOException {\n     \tthrow new IOException(\"/parquet-version.properties not found\");\n     }\n     Properties props = new Properties();\n-    props.load(in);\n+    try {\n+      props.load(in);\n+    } finally {\n+      in.close();\n+    }\n \n     add(\"package org.apache.parquet;\\n\" +\n         \"\\n\" +",
                "additions": 5,
                "raw_url": "https://github.com/apache/parquet-mr/raw/f68dbc3ea20230cb14ed3364539ad16e114bcdd9/parquet-generator/src/main/java/org/apache/parquet/version/VersionGenerator.java",
                "status": "modified",
                "changes": 6,
                "deletions": 1,
                "sha": "4ad59bf45523a9fcd727bafbf703741052702ed2",
                "blob_url": "https://github.com/apache/parquet-mr/blob/f68dbc3ea20230cb14ed3364539ad16e114bcdd9/parquet-generator/src/main/java/org/apache/parquet/version/VersionGenerator.java",
                "filename": "parquet-generator/src/main/java/org/apache/parquet/version/VersionGenerator.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-generator/src/main/java/org/apache/parquet/version/VersionGenerator.java?ref=f68dbc3ea20230cb14ed3364539ad16e114bcdd9"
            },
            {
                "patch": "@@ -331,7 +331,7 @@ public static String binaryToString(Binary value) {\n         try {\n             CharBuffer buffer = UTF8_DECODER.decode(value.toByteBuffer());\n             return buffer.toString();\n-        } catch (Throwable th) {\n+        } catch (Exception ex) {\n         }\n \n         return \"<bytes...>\";",
                "additions": 1,
                "raw_url": "https://github.com/apache/parquet-mr/raw/f68dbc3ea20230cb14ed3364539ad16e114bcdd9/parquet-tools/src/main/java/org/apache/parquet/tools/command/DumpCommand.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "ed84edd801b3646572130d711aee867a55340744",
                "blob_url": "https://github.com/apache/parquet-mr/blob/f68dbc3ea20230cb14ed3364539ad16e114bcdd9/parquet-tools/src/main/java/org/apache/parquet/tools/command/DumpCommand.java",
                "filename": "parquet-tools/src/main/java/org/apache/parquet/tools/command/DumpCommand.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-tools/src/main/java/org/apache/parquet/tools/command/DumpCommand.java?ref=f68dbc3ea20230cb14ed3364539ad16e114bcdd9"
            },
            {
                "patch": "@@ -39,7 +39,7 @@\n     for (Map.Entry<String,Class<? extends Command>> entry : registry.entrySet()) {\n       try {\n         results.put(entry.getKey(), entry.getValue().newInstance());\n-      } catch (Throwable th) {\n+      } catch (Exception ex) {\n       }\n     }\n \n@@ -54,7 +54,7 @@ public static Command getCommandByName(String name) {\n \n     try {\n       return clazz.newInstance();\n-    } catch (Throwable th) {\n+    } catch (Exception ex) {\n       return null;\n     }\n   }",
                "additions": 2,
                "raw_url": "https://github.com/apache/parquet-mr/raw/f68dbc3ea20230cb14ed3364539ad16e114bcdd9/parquet-tools/src/main/java/org/apache/parquet/tools/command/Registry.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "0e69f481c763b01f38343b668abf3c2d7cc2a6ce",
                "blob_url": "https://github.com/apache/parquet-mr/blob/f68dbc3ea20230cb14ed3364539ad16e114bcdd9/parquet-tools/src/main/java/org/apache/parquet/tools/command/Registry.java",
                "filename": "parquet-tools/src/main/java/org/apache/parquet/tools/command/Registry.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-tools/src/main/java/org/apache/parquet/tools/command/Registry.java?ref=f68dbc3ea20230cb14ed3364539ad16e114bcdd9"
            },
            {
                "patch": "@@ -79,7 +79,7 @@\n     if (columns != null && !columns.isEmpty()) {\n       try {\n         consoleWidth = Integer.parseInt(columns);\n-      } catch (Throwable th) {\n+      } catch (Exception ex) {\n       }\n     }\n \n@@ -88,7 +88,7 @@\n       try {\n         numColors = Integer.parseInt(colors);\n         if (numColors < 0) numColors = 0;\n-      } catch (Throwable th) {\n+      } catch (Exception exa) {\n       }\n     }\n ",
                "additions": 2,
                "raw_url": "https://github.com/apache/parquet-mr/raw/f68dbc3ea20230cb14ed3364539ad16e114bcdd9/parquet-tools/src/main/java/org/apache/parquet/tools/util/PrettyPrintWriter.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "0a12df21d022e18c1768773d1febb9fd066ba360",
                "blob_url": "https://github.com/apache/parquet-mr/blob/f68dbc3ea20230cb14ed3364539ad16e114bcdd9/parquet-tools/src/main/java/org/apache/parquet/tools/util/PrettyPrintWriter.java",
                "filename": "parquet-tools/src/main/java/org/apache/parquet/tools/util/PrettyPrintWriter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-tools/src/main/java/org/apache/parquet/tools/util/PrettyPrintWriter.java?ref=f68dbc3ea20230cb14ed3364539ad16e114bcdd9"
            }
        ],
        "bug_id": "parquet-mr_13",
        "parent": "https://github.com/apache/parquet-mr/commit/89e0607cf6470dda1a6a47b46abf37468df4e50f",
        "message": "PARQUET-825: Static analyzer findings (NPEs, resource leaks)\n\nSome trivial code fixes based on findings on static code analyzer tools (Sonar, Fortify)\n@piyushnarang: Sorry, renaming the branch caused the closing of the original PR...\n\nAuthor: Gabor Szadovszky <gabor.szadovszky@Budapests-MacBook-Pro-8.local>\nAuthor: Gabor Szadovszky <gabor.szadovszky@cloudera.com>\n\nCloses #399 from gszadovszky/PARQUET-825 and squashes the following commits:\n\n68a4764 [Gabor Szadovszky] PARQUET-825 - Static analyzer findings (NPEs, resource leaks)\na689c1c [Gabor Szadovszky] Code fixes related to null checks, exception handling and closing streams",
        "repo": "parquet-mr"
    },
    {
        "commit": "https://github.com/apache/parquet-mr/commit/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b",
        "file": [
            {
                "patch": "@@ -36,6 +36,12 @@\n   </properties>\n \n   <dependencies>\n+    <dependency>\n+      <groupId>org.apache.parquet</groupId>\n+      <artifactId>parquet-format</artifactId>\n+      <version>${parquet.format.version}</version>\n+    </dependency>\n+\n     <dependency>\n       <groupId>org.slf4j</groupId>\n       <artifactId>slf4j-api</artifactId>",
                "additions": 6,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/pom.xml",
                "status": "modified",
                "changes": 6,
                "deletions": 0,
                "sha": "7ae60685c914c80e87137a917678bb2f51bf5fc0",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/pom.xml",
                "filename": "parquet-common/pom.xml",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-common/pom.xml?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "additions": 0,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/bytes/BytesInput.java",
                "previous_filename": "parquet-encoding/src/main/java/org/apache/parquet/bytes/BytesInput.java",
                "status": "renamed",
                "changes": 0,
                "deletions": 0,
                "sha": "6e593c2409a341bb06a8257350614a965cc500eb",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/bytes/BytesInput.java",
                "filename": "parquet-common/src/main/java/org/apache/parquet/bytes/BytesInput.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-common/src/main/java/org/apache/parquet/bytes/BytesInput.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "additions": 0,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/bytes/CapacityByteArrayOutputStream.java",
                "previous_filename": "parquet-encoding/src/main/java/org/apache/parquet/bytes/CapacityByteArrayOutputStream.java",
                "status": "renamed",
                "changes": 0,
                "deletions": 0,
                "sha": "92674d4de64275bfd3c217f0c0b06a043cfde592",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/bytes/CapacityByteArrayOutputStream.java",
                "filename": "parquet-common/src/main/java/org/apache/parquet/bytes/CapacityByteArrayOutputStream.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-common/src/main/java/org/apache/parquet/bytes/CapacityByteArrayOutputStream.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "additions": 0,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/bytes/ConcatenatingByteArrayCollector.java",
                "previous_filename": "parquet-encoding/src/main/java/org/apache/parquet/bytes/ConcatenatingByteArrayCollector.java",
                "status": "renamed",
                "changes": 0,
                "deletions": 0,
                "sha": "d33316859be1ba528341ebf8293ecc2d82f35c92",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/bytes/ConcatenatingByteArrayCollector.java",
                "filename": "parquet-common/src/main/java/org/apache/parquet/bytes/ConcatenatingByteArrayCollector.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-common/src/main/java/org/apache/parquet/bytes/ConcatenatingByteArrayCollector.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "additions": 0,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/bytes/LittleEndianDataInputStream.java",
                "previous_filename": "parquet-encoding/src/main/java/org/apache/parquet/bytes/LittleEndianDataInputStream.java",
                "status": "renamed",
                "changes": 0,
                "deletions": 0,
                "sha": "a09275318e329295927e644a8f81049f135576f1",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/bytes/LittleEndianDataInputStream.java",
                "filename": "parquet-common/src/main/java/org/apache/parquet/bytes/LittleEndianDataInputStream.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-common/src/main/java/org/apache/parquet/bytes/LittleEndianDataInputStream.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "additions": 0,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/bytes/LittleEndianDataOutputStream.java",
                "previous_filename": "parquet-encoding/src/main/java/org/apache/parquet/bytes/LittleEndianDataOutputStream.java",
                "status": "renamed",
                "changes": 0,
                "deletions": 0,
                "sha": "9d4a8a9ec1764518cb2ceb07719fdf56be25e7b3",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/bytes/LittleEndianDataOutputStream.java",
                "filename": "parquet-common/src/main/java/org/apache/parquet/bytes/LittleEndianDataOutputStream.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-common/src/main/java/org/apache/parquet/bytes/LittleEndianDataOutputStream.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -0,0 +1,47 @@\n+/*\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing,\n+ *  software distributed under the License is distributed on an\n+ *  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ *  KIND, either express or implied.  See the License for the\n+ *  specific language governing permissions and limitations\n+ *  under the License.\n+ */\n+\n+package org.apache.parquet.compression;\n+\n+import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import java.io.IOException;\n+import java.nio.ByteBuffer;\n+\n+public interface CompressionCodecFactory {\n+\n+  BytesInputCompressor getCompressor(CompressionCodecName codecName);\n+\n+  BytesInputDecompressor getDecompressor(CompressionCodecName codecName);\n+\n+  void release();\n+\n+  interface BytesInputCompressor {\n+    BytesInput compress(BytesInput bytes) throws IOException;\n+    CompressionCodecName getCodecName();\n+    void release();\n+  }\n+\n+  interface BytesInputDecompressor {\n+    BytesInput decompress(BytesInput bytes, int uncompressedSize) throws IOException;\n+    void decompress(ByteBuffer input, int compressedSize, ByteBuffer output, int uncompressedSize) throws IOException;\n+    void release();\n+  }\n+\n+}",
                "additions": 47,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/compression/CompressionCodecFactory.java",
                "status": "added",
                "changes": 47,
                "deletions": 0,
                "sha": "5b1b65723023df002a89893edc3c442253c49d43",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/compression/CompressionCodecFactory.java",
                "filename": "parquet-common/src/main/java/org/apache/parquet/compression/CompressionCodecFactory.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-common/src/main/java/org/apache/parquet/compression/CompressionCodecFactory.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -0,0 +1,38 @@\n+/*\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing,\n+ *  software distributed under the License is distributed on an\n+ *  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ *  KIND, either express or implied.  See the License for the\n+ *  specific language governing permissions and limitations\n+ *  under the License.\n+ */\n+package org.apache.parquet.hadoop.codec;\n+\n+import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+\n+/**\n+ * This exception will be thrown when the codec is not supported by parquet, meaning there is no\n+ * matching codec defined in {@link CompressionCodecName}\n+ */\n+public class CompressionCodecNotSupportedException extends RuntimeException {\n+  private final Class codecClass;\n+\n+  public CompressionCodecNotSupportedException(Class codecClass) {\n+    super(\"codec not supported: \" + codecClass.getName());\n+    this.codecClass = codecClass;\n+  }\n+\n+  public Class getCodecClass() {\n+    return codecClass;\n+  }\n+}",
                "additions": 38,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/hadoop/codec/CompressionCodecNotSupportedException.java",
                "status": "added",
                "changes": 38,
                "deletions": 0,
                "sha": "bf2da3273327563ca460bbbcd6ba3245a8f14768",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/hadoop/codec/CompressionCodecNotSupportedException.java",
                "filename": "parquet-common/src/main/java/org/apache/parquet/hadoop/codec/CompressionCodecNotSupportedException.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-common/src/main/java/org/apache/parquet/hadoop/codec/CompressionCodecNotSupportedException.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -1,26 +1,26 @@\n-/* \n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- * \n- *   http://www.apache.org/licenses/LICENSE-2.0\n- * \n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n+/*\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing,\n+ *  software distributed under the License is distributed on an\n+ *  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ *  KIND, either express or implied.  See the License for the\n+ *  specific language governing permissions and limitations\n+ *  under the License.\n  */\n package org.apache.parquet.hadoop.metadata;\n \n+\n import org.apache.parquet.format.CompressionCodec;\n import org.apache.parquet.hadoop.codec.CompressionCodecNotSupportedException;\n-\n import java.util.Locale;\n \n public enum CompressionCodecName {",
                "additions": 18,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/hadoop/metadata/CompressionCodecName.java",
                "previous_filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata/CompressionCodecName.java",
                "status": "renamed",
                "changes": 36,
                "deletions": 18,
                "sha": "8cdede09cd9d5fdd074c28e8fa4f15a442f26f63",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/hadoop/metadata/CompressionCodecName.java",
                "filename": "parquet-common/src/main/java/org/apache/parquet/hadoop/metadata/CompressionCodecName.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-common/src/main/java/org/apache/parquet/hadoop/metadata/CompressionCodecName.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -0,0 +1,63 @@\n+/*\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing,\n+ *  software distributed under the License is distributed on an\n+ *  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ *  KIND, either express or implied.  See the License for the\n+ *  specific language governing permissions and limitations\n+ *  under the License.\n+ */\n+\n+package org.apache.parquet.io;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+\n+public abstract class DelegatingPositionOutputStream extends PositionOutputStream {\n+  private final OutputStream stream;\n+\n+  public DelegatingPositionOutputStream(OutputStream stream) {\n+    this.stream = stream;\n+  }\n+\n+  public OutputStream getStream() {\n+    return stream;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    stream.close();\n+  }\n+\n+  @Override\n+  public void flush() throws IOException {\n+    stream.flush();\n+  }\n+\n+  @Override\n+  public abstract long getPos() throws IOException;\n+\n+  @Override\n+  public void write(int b) throws IOException {\n+    stream.write(b);\n+  }\n+\n+  @Override\n+  public void write(byte[] b) throws IOException {\n+    stream.write(b);\n+  }\n+\n+  @Override\n+  public void write(byte[] b, int off, int len) throws IOException {\n+    stream.write(b, off, len);\n+  }\n+}",
                "additions": 63,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/io/DelegatingPositionOutputStream.java",
                "status": "added",
                "changes": 63,
                "deletions": 0,
                "sha": "9e524282bb13928f025ae8f759a9dc08bb441fb3",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/io/DelegatingPositionOutputStream.java",
                "filename": "parquet-common/src/main/java/org/apache/parquet/io/DelegatingPositionOutputStream.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-common/src/main/java/org/apache/parquet/io/DelegatingPositionOutputStream.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -0,0 +1,171 @@\n+/*\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing,\n+ *  software distributed under the License is distributed on an\n+ *  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ *  KIND, either express or implied.  See the License for the\n+ *  specific language governing permissions and limitations\n+ *  under the License.\n+ */\n+\n+package org.apache.parquet.io;\n+\n+import java.io.EOFException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.nio.ByteBuffer;\n+\n+/**\n+ * Implements read methods required by {@link SeekableInputStream} for generic input streams.\n+ * <p>\n+ * Implementations must implement {@link #getPos()} and {@link #seek(long)} and may optionally\n+ * implement other read methods to improve performance.\n+ */\n+public abstract class DelegatingSeekableInputStream extends SeekableInputStream {\n+\n+  private final int COPY_BUFFER_SIZE = 8192;\n+  private final byte[] temp = new byte[COPY_BUFFER_SIZE];\n+\n+  private final InputStream stream;\n+\n+  public DelegatingSeekableInputStream(InputStream stream) {\n+    this.stream = stream;\n+  }\n+\n+  public InputStream getStream() {\n+    return stream;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    stream.close();\n+  }\n+\n+  @Override\n+  public abstract long getPos() throws IOException;\n+\n+  @Override\n+  public abstract void seek(long newPos) throws IOException;\n+\n+  @Override\n+  public int read() throws IOException {\n+    return stream.read();\n+  }\n+\n+  @Override\n+  public int read(byte[] b, int off, int len) throws IOException {\n+    return stream.read(b, off, len);\n+  }\n+\n+  @Override\n+  public void readFully(byte[] bytes) throws IOException {\n+    readFully(stream, bytes, 0, bytes.length);\n+  }\n+\n+  @Override\n+  public void readFully(byte[] bytes, int start, int len) throws IOException {\n+    readFully(stream, bytes, start, len);\n+  }\n+\n+  @Override\n+  public int read(ByteBuffer buf) throws IOException {\n+    if (buf.hasArray()) {\n+      return readHeapBuffer(stream, buf);\n+    } else {\n+      return readDirectBuffer(stream, buf, temp);\n+    }\n+  }\n+\n+  @Override\n+  public void readFully(ByteBuffer buf) throws IOException {\n+    if (buf.hasArray()) {\n+      readFullyHeapBuffer(stream, buf);\n+    } else {\n+      readFullyDirectBuffer(stream, buf, temp);\n+    }\n+  }\n+\n+  // Visible for testing\n+  static void readFully(InputStream f, byte[] bytes, int start, int len) throws IOException {\n+    int offset = start;\n+    int remaining = len;\n+    while (remaining > 0) {\n+      int bytesRead = f.read(bytes, offset, remaining);\n+      if (bytesRead < 0) {\n+        throw new EOFException(\n+            \"Reached the end of stream with \" + remaining + \" bytes left to read\");\n+      }\n+\n+      remaining -= bytesRead;\n+      offset += bytesRead;\n+    }\n+  }\n+\n+  // Visible for testing\n+  static int readHeapBuffer(InputStream f, ByteBuffer buf) throws IOException {\n+    int bytesRead = f.read(buf.array(), buf.arrayOffset() + buf.position(), buf.remaining());\n+    if (bytesRead < 0) {\n+      // if this resulted in EOF, don't update position\n+      return bytesRead;\n+    } else {\n+      buf.position(buf.position() + bytesRead);\n+      return bytesRead;\n+    }\n+  }\n+\n+  // Visible for testing\n+  static void readFullyHeapBuffer(InputStream f, ByteBuffer buf) throws IOException {\n+    readFully(f, buf.array(), buf.arrayOffset() + buf.position(), buf.remaining());\n+    buf.position(buf.limit());\n+  }\n+\n+  // Visible for testing\n+  static int readDirectBuffer(InputStream f, ByteBuffer buf, byte[] temp) throws IOException {\n+    // copy all the bytes that return immediately, stopping at the first\n+    // read that doesn't return a full buffer.\n+    int nextReadLength = Math.min(buf.remaining(), temp.length);\n+    int totalBytesRead = 0;\n+    int bytesRead;\n+\n+    while ((bytesRead = f.read(temp, 0, nextReadLength)) == temp.length) {\n+      buf.put(temp);\n+      totalBytesRead += bytesRead;\n+      nextReadLength = Math.min(buf.remaining(), temp.length);\n+    }\n+\n+    if (bytesRead < 0) {\n+      // return -1 if nothing was read\n+      return totalBytesRead == 0 ? -1 : totalBytesRead;\n+    } else {\n+      // copy the last partial buffer\n+      buf.put(temp, 0, bytesRead);\n+      totalBytesRead += bytesRead;\n+      return totalBytesRead;\n+    }\n+  }\n+\n+  // Visible for testing\n+  static void readFullyDirectBuffer(InputStream f, ByteBuffer buf, byte[] temp) throws IOException {\n+    int nextReadLength = Math.min(buf.remaining(), temp.length);\n+    int bytesRead = 0;\n+\n+    while (nextReadLength > 0 && (bytesRead = f.read(temp, 0, nextReadLength)) >= 0) {\n+      buf.put(temp, 0, bytesRead);\n+      nextReadLength = Math.min(buf.remaining(), temp.length);\n+    }\n+\n+    if (bytesRead < 0 && buf.remaining() > 0) {\n+      throw new EOFException(\n+          \"Reached the end of stream with \" + buf.remaining() + \" bytes left to read\");\n+    }\n+  }\n+}",
                "additions": 171,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/io/DelegatingSeekableInputStream.java",
                "status": "added",
                "changes": 171,
                "deletions": 0,
                "sha": "bc4940c370e4869d64031cca00bc502c36ac6d76",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/io/DelegatingSeekableInputStream.java",
                "filename": "parquet-common/src/main/java/org/apache/parquet/io/DelegatingSeekableInputStream.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-common/src/main/java/org/apache/parquet/io/DelegatingSeekableInputStream.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -28,15 +28,16 @@\n public interface InputFile {\n \n   /**\n-   * Returns the total length of the file, in bytes.\n+   * @return the total length of the file, in bytes.\n    * @throws IOException if the length cannot be determined\n    */\n   long getLength() throws IOException;\n \n   /**\n-   * Opens a new {@link SeekableInputStream} for the underlying\n-   * data file.\n-   * @throws IOException if the stream cannot be opened.\n+   * Open a new {@link SeekableInputStream} for the underlying data file.\n+   *\n+   * @return a new {@link SeekableInputStream} to read the file\n+   * @throws IOException if the stream cannot be opened\n    */\n   SeekableInputStream newStream() throws IOException;\n ",
                "additions": 5,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/io/InputFile.java",
                "status": "modified",
                "changes": 9,
                "deletions": 4,
                "sha": "f9100742be93a1f8cce42d22471bce083ba88a68",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/io/InputFile.java",
                "filename": "parquet-common/src/main/java/org/apache/parquet/io/InputFile.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-common/src/main/java/org/apache/parquet/io/InputFile.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -0,0 +1,34 @@\n+/*\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing,\n+ *  software distributed under the License is distributed on an\n+ *  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ *  KIND, either express or implied.  See the License for the\n+ *  specific language governing permissions and limitations\n+ *  under the License.\n+ */\n+\n+package org.apache.parquet.io;\n+\n+import java.io.IOException;\n+\n+public interface OutputFile {\n+\n+  PositionOutputStream create(long blockSizeHint) throws IOException;\n+\n+  PositionOutputStream createOrOverwrite(long blockSizeHint) throws IOException;\n+\n+  boolean supportsBlockSize();\n+\n+  long defaultBlockSize();\n+\n+}",
                "additions": 34,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/io/OutputFile.java",
                "status": "added",
                "changes": 34,
                "deletions": 0,
                "sha": "2d6de44a2210b1914eb8ae6ca32cdb06425fa5f7",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/io/OutputFile.java",
                "filename": "parquet-common/src/main/java/org/apache/parquet/io/OutputFile.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-common/src/main/java/org/apache/parquet/io/OutputFile.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -0,0 +1,39 @@\n+/*\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing,\n+ *  software distributed under the License is distributed on an\n+ *  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ *  KIND, either express or implied.  See the License for the\n+ *  specific language governing permissions and limitations\n+ *  under the License.\n+ */\n+\n+package org.apache.parquet.io;\n+\n+import java.io.IOException;\n+import java.io.OutputStream;\n+\n+/**\n+ * {@code PositionOutputStream} is an interface with the methods needed by\n+ * Parquet to write data to a file or Hadoop data stream.\n+ */\n+public abstract class PositionOutputStream extends OutputStream {\n+\n+  /**\n+   * Reports the current position of this output stream.\n+   *\n+   * @return a long, the current position in bytes starting from 0\n+   * @throws IOException when the underlying stream throws IOException\n+   */\n+  public abstract long getPos() throws IOException;\n+\n+}",
                "additions": 39,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/io/PositionOutputStream.java",
                "status": "added",
                "changes": 39,
                "deletions": 0,
                "sha": "066c46b8f40a81e6c37d36c9d841e04d5a5813e2",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/main/java/org/apache/parquet/io/PositionOutputStream.java",
                "filename": "parquet-common/src/main/java/org/apache/parquet/io/PositionOutputStream.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-common/src/main/java/org/apache/parquet/io/PositionOutputStream.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -0,0 +1,56 @@\n+/*\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing,\n+ *  software distributed under the License is distributed on an\n+ *  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ *  KIND, either express or implied.  See the License for the\n+ *  specific language governing permissions and limitations\n+ *  under the License.\n+ */\n+\n+package org.apache.parquet.io;\n+\n+import java.io.ByteArrayInputStream;\n+\n+class MockInputStream extends ByteArrayInputStream {\n+\n+  static final byte[] TEST_ARRAY = new byte[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };\n+\n+  private int[] lengths;\n+  private int current = 0;\n+  MockInputStream(int... actualReadLengths) {\n+    super(TEST_ARRAY);\n+    this.lengths = actualReadLengths;\n+  }\n+\n+  @Override\n+  public synchronized int read(byte[] b, int off, int len) {\n+    if (current < lengths.length) {\n+      if (len <= lengths[current]) {\n+        // when len == lengths[current], the next read will by 0 bytes\n+        int bytesRead = super.read(b, off, len);\n+        lengths[current] -= bytesRead;\n+        return bytesRead;\n+      } else {\n+        int bytesRead = super.read(b, off, lengths[current]);\n+        current += 1;\n+        return bytesRead;\n+      }\n+    } else {\n+      return super.read(b, off, len);\n+    }\n+  }\n+\n+  public long getPos() {\n+    return this.pos;\n+  }\n+}",
                "additions": 56,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/test/java/org/apache/parquet/io/MockInputStream.java",
                "status": "added",
                "changes": 56,
                "deletions": 0,
                "sha": "42e3a8af845ab9317ac39364347776a4f0973d62",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/test/java/org/apache/parquet/io/MockInputStream.java",
                "filename": "parquet-common/src/test/java/org/apache/parquet/io/MockInputStream.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-common/src/test/java/org/apache/parquet/io/MockInputStream.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -17,19 +17,119 @@\n  *  under the License.\n  */\n \n-package org.apache.parquet.hadoop.util;\n+package org.apache.parquet.io;\n \n-import org.apache.hadoop.fs.FSDataInputStream;\n-import org.apache.parquet.hadoop.TestUtils;\n+import org.apache.parquet.TestUtils;\n import org.junit.Assert;\n import org.junit.Test;\n import java.io.EOFException;\n+import java.io.IOException;\n import java.nio.ByteBuffer;\n+import java.util.Arrays;\n import java.util.concurrent.Callable;\n \n-import static org.apache.parquet.hadoop.util.MockInputStream.TEST_ARRAY;\n+import static org.apache.parquet.io.MockInputStream.TEST_ARRAY;\n \n-public class TestHadoop1ByteBufferReads {\n+\n+public class TestDelegatingSeekableInputStream {\n+\n+  @Test\n+  public void testReadFully() throws Exception {\n+    byte[] buffer = new byte[5];\n+\n+    MockInputStream stream = new MockInputStream();\n+    DelegatingSeekableInputStream.readFully(stream, buffer, 0, buffer.length);\n+\n+    Assert.assertArrayEquals(\"Byte array contents should match\",\n+        Arrays.copyOfRange(TEST_ARRAY, 0, 5), buffer);\n+    Assert.assertEquals(\"Stream position should reflect bytes read\", 5, stream.getPos());\n+  }\n+\n+  @Test\n+  public void testReadFullySmallReads() throws Exception {\n+    byte[] buffer = new byte[5];\n+\n+    MockInputStream stream = new MockInputStream(2, 3, 3);\n+    DelegatingSeekableInputStream.readFully(stream, buffer, 0, buffer.length);\n+\n+    Assert.assertArrayEquals(\"Byte array contents should match\",\n+        Arrays.copyOfRange(TEST_ARRAY, 0, 5), buffer);\n+    Assert.assertEquals(\"Stream position should reflect bytes read\", 5, stream.getPos());\n+  }\n+\n+  @Test\n+  public void testReadFullyJustRight() throws Exception {\n+    final byte[] buffer = new byte[10];\n+\n+    final MockInputStream stream = new MockInputStream(2, 3, 3);\n+    DelegatingSeekableInputStream.readFully(stream, buffer, 0, buffer.length);\n+\n+    Assert.assertArrayEquals(\"Byte array contents should match\", TEST_ARRAY, buffer);\n+    Assert.assertEquals(\"Stream position should reflect bytes read\", 10, stream.getPos());\n+\n+    TestUtils.assertThrows(\"Should throw EOFException if no more bytes left\",\n+        EOFException.class, new Callable<Void>() {\n+          @Override\n+          public Void call() throws IOException {\n+            DelegatingSeekableInputStream.readFully(stream, buffer, 0, 1);\n+            return null;\n+          }\n+        });\n+  }\n+\n+  @Test\n+  public void testReadFullyUnderflow() throws Exception {\n+    final byte[] buffer = new byte[11];\n+\n+    final MockInputStream stream = new MockInputStream(2, 3, 3);\n+\n+    TestUtils.assertThrows(\"Should throw EOFException if no more bytes left\",\n+        EOFException.class, new Callable<Void>() {\n+          @Override\n+          public Void call() throws IOException {\n+            DelegatingSeekableInputStream.readFully(stream, buffer, 0, buffer.length);\n+            return null;\n+          }\n+        });\n+\n+    Assert.assertArrayEquals(\"Should have consumed bytes\",\n+        TEST_ARRAY, Arrays.copyOfRange(buffer, 0, 10));\n+    Assert.assertEquals(\"Stream position should reflect bytes read\", 10, stream.getPos());\n+  }\n+\n+  @Test\n+  public void testReadFullyStartAndLength() throws IOException {\n+    byte[] buffer = new byte[10];\n+\n+    MockInputStream stream = new MockInputStream();\n+    DelegatingSeekableInputStream.readFully(stream, buffer, 2, 5);\n+\n+    Assert.assertArrayEquals(\"Byte array contents should match\",\n+        Arrays.copyOfRange(TEST_ARRAY, 0, 5), Arrays.copyOfRange(buffer, 2, 7));\n+    Assert.assertEquals(\"Stream position should reflect bytes read\", 5, stream.getPos());\n+  }\n+\n+  @Test\n+  public void testReadFullyZeroByteRead() throws IOException {\n+    byte[] buffer = new byte[0];\n+\n+    MockInputStream stream = new MockInputStream();\n+    DelegatingSeekableInputStream.readFully(stream, buffer, 0, buffer.length);\n+\n+    Assert.assertEquals(\"Stream position should reflect bytes read\", 0, stream.getPos());\n+  }\n+\n+  @Test\n+  public void testReadFullySmallReadsWithStartAndLength() throws IOException {\n+    byte[] buffer = new byte[10];\n+\n+    MockInputStream stream = new MockInputStream(2, 2, 3);\n+    DelegatingSeekableInputStream.readFully(stream, buffer, 2, 5);\n+\n+    Assert.assertArrayEquals(\"Byte array contents should match\",\n+        Arrays.copyOfRange(TEST_ARRAY, 0, 5), Arrays.copyOfRange(buffer, 2, 7));\n+    Assert.assertEquals(\"Stream position should reflect bytes read\", 5, stream.getPos());\n+  }\n \n   private static final ThreadLocal<byte[]> TEMP = new ThreadLocal<byte[]>() {\n     @Override\n@@ -42,14 +142,14 @@\n   public void testHeapRead() throws Exception {\n     ByteBuffer readBuffer = ByteBuffer.allocate(20);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream());\n+    MockInputStream stream = new MockInputStream();\n \n-    int len = H1SeekableInputStream.readHeapBuffer(hadoopStream, readBuffer);\n+    int len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(10, len);\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(20, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readHeapBuffer(hadoopStream, readBuffer);\n+    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(-1, len);\n \n     readBuffer.flip();\n@@ -61,14 +161,14 @@ public void testHeapRead() throws Exception {\n   public void testHeapSmallBuffer() throws Exception {\n     ByteBuffer readBuffer = ByteBuffer.allocate(5);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream());\n+    MockInputStream stream = new MockInputStream();\n \n-    int len = H1SeekableInputStream.readHeapBuffer(hadoopStream, readBuffer);\n+    int len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(5, len);\n     Assert.assertEquals(5, readBuffer.position());\n     Assert.assertEquals(5, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readHeapBuffer(hadoopStream, readBuffer);\n+    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(0, len);\n \n     readBuffer.flip();\n@@ -80,24 +180,24 @@ public void testHeapSmallBuffer() throws Exception {\n   public void testHeapSmallReads() throws Exception {\n     ByteBuffer readBuffer = ByteBuffer.allocate(10);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    MockInputStream stream = new MockInputStream(2, 3, 3);\n \n-    int len = H1SeekableInputStream.readHeapBuffer(hadoopStream, readBuffer);\n+    int len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(2, len);\n     Assert.assertEquals(2, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readHeapBuffer(hadoopStream, readBuffer);\n+    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(3, len);\n     Assert.assertEquals(5, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readHeapBuffer(hadoopStream, readBuffer);\n+    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(3, len);\n     Assert.assertEquals(8, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readHeapBuffer(hadoopStream, readBuffer);\n+    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(2, len);\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n@@ -113,19 +213,19 @@ public void testHeapPosition() throws Exception {\n     readBuffer.position(10);\n     readBuffer.mark();\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(8));\n+    MockInputStream stream = new MockInputStream(8);\n \n-    int len = H1SeekableInputStream.readHeapBuffer(hadoopStream, readBuffer);\n+    int len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(8, len);\n     Assert.assertEquals(18, readBuffer.position());\n     Assert.assertEquals(20, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readHeapBuffer(hadoopStream, readBuffer);\n+    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(2, len);\n     Assert.assertEquals(20, readBuffer.position());\n     Assert.assertEquals(20, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readHeapBuffer(hadoopStream, readBuffer);\n+    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(-1, len);\n \n     readBuffer.reset();\n@@ -138,19 +238,19 @@ public void testHeapLimit() throws Exception {\n     ByteBuffer readBuffer = ByteBuffer.allocate(20);\n     readBuffer.limit(8);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(7));\n+    MockInputStream stream = new MockInputStream(7);\n \n-    int len = H1SeekableInputStream.readHeapBuffer(hadoopStream, readBuffer);\n+    int len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(7, len);\n     Assert.assertEquals(7, readBuffer.position());\n     Assert.assertEquals(8, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readHeapBuffer(hadoopStream, readBuffer);\n+    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(1, len);\n     Assert.assertEquals(8, readBuffer.position());\n     Assert.assertEquals(8, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readHeapBuffer(hadoopStream, readBuffer);\n+    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(0, len);\n \n     readBuffer.flip();\n@@ -165,19 +265,19 @@ public void testHeapPositionAndLimit() throws Exception {\n     readBuffer.limit(13);\n     readBuffer.mark();\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(7));\n+    MockInputStream stream = new MockInputStream(7);\n \n-    int len = H1SeekableInputStream.readHeapBuffer(hadoopStream, readBuffer);\n+    int len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(7, len);\n     Assert.assertEquals(12, readBuffer.position());\n     Assert.assertEquals(13, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readHeapBuffer(hadoopStream, readBuffer);\n+    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(1, len);\n     Assert.assertEquals(13, readBuffer.position());\n     Assert.assertEquals(13, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readHeapBuffer(hadoopStream, readBuffer);\n+    len = DelegatingSeekableInputStream.readHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(0, len);\n \n     readBuffer.reset();\n@@ -189,14 +289,14 @@ public void testHeapPositionAndLimit() throws Exception {\n   public void testDirectRead() throws Exception {\n     ByteBuffer readBuffer = ByteBuffer.allocateDirect(20);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream());\n+    MockInputStream stream = new MockInputStream();\n \n-    int len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(10, len);\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(20, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(-1, len);\n \n     readBuffer.flip();\n@@ -208,14 +308,14 @@ public void testDirectRead() throws Exception {\n   public void testDirectSmallBuffer() throws Exception {\n     ByteBuffer readBuffer = ByteBuffer.allocateDirect(5);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream());\n+    MockInputStream stream = new MockInputStream();\n \n-    int len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(5, len);\n     Assert.assertEquals(5, readBuffer.position());\n     Assert.assertEquals(5, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(0, len);\n \n     readBuffer.flip();\n@@ -227,24 +327,24 @@ public void testDirectSmallBuffer() throws Exception {\n   public void testDirectSmallReads() throws Exception {\n     ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    MockInputStream stream = new MockInputStream(2, 3, 3);\n \n-    int len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(2, len);\n     Assert.assertEquals(2, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(3, len);\n     Assert.assertEquals(5, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(3, len);\n     Assert.assertEquals(8, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(2, len);\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n@@ -260,19 +360,19 @@ public void testDirectPosition() throws Exception {\n     readBuffer.position(10);\n     readBuffer.mark();\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(8));\n+    MockInputStream stream = new MockInputStream(8);\n \n-    int len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(8, len);\n     Assert.assertEquals(18, readBuffer.position());\n     Assert.assertEquals(20, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(2, len);\n     Assert.assertEquals(20, readBuffer.position());\n     Assert.assertEquals(20, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(-1, len);\n \n     readBuffer.reset();\n@@ -285,19 +385,19 @@ public void testDirectLimit() throws Exception {\n     ByteBuffer readBuffer = ByteBuffer.allocate(20);\n     readBuffer.limit(8);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(7));\n+    MockInputStream stream = new MockInputStream(7);\n \n-    int len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(7, len);\n     Assert.assertEquals(7, readBuffer.position());\n     Assert.assertEquals(8, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(1, len);\n     Assert.assertEquals(8, readBuffer.position());\n     Assert.assertEquals(8, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(0, len);\n \n     readBuffer.flip();\n@@ -312,19 +412,19 @@ public void testDirectPositionAndLimit() throws Exception {\n     readBuffer.limit(13);\n     readBuffer.mark();\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(7));\n+    MockInputStream stream = new MockInputStream(7);\n \n-    int len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(7, len);\n     Assert.assertEquals(12, readBuffer.position());\n     Assert.assertEquals(13, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(1, len);\n     Assert.assertEquals(13, readBuffer.position());\n     Assert.assertEquals(13, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(0, len);\n \n     readBuffer.reset();\n@@ -338,29 +438,29 @@ public void testDirectSmallTempBufferSmallReads() throws Exception {\n \n     ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    MockInputStream stream = new MockInputStream(2, 3, 3);\n \n-    int len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, temp);\n+    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);\n     Assert.assertEquals(2, len);\n     Assert.assertEquals(2, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, temp);\n+    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);\n     Assert.assertEquals(3, len);\n     Assert.assertEquals(5, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, temp);\n+    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);\n     Assert.assertEquals(3, len);\n     Assert.assertEquals(8, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, temp);\n+    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);\n     Assert.assertEquals(2, len);\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, temp);\n+    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);\n     Assert.assertEquals(-1, len);\n \n     readBuffer.flip();\n@@ -377,19 +477,19 @@ public void testDirectSmallTempBufferWithPositionAndLimit() throws Exception {\n     readBuffer.limit(13);\n     readBuffer.mark();\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(7));\n+    MockInputStream stream = new MockInputStream(7);\n \n-    int len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, temp);\n+    int len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);\n     Assert.assertEquals(7, len);\n     Assert.assertEquals(12, readBuffer.position());\n     Assert.assertEquals(13, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, temp);\n+    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);\n     Assert.assertEquals(1, len);\n     Assert.assertEquals(13, readBuffer.position());\n     Assert.assertEquals(13, readBuffer.limit());\n \n-    len = H1SeekableInputStream.readDirectBuffer(hadoopStream, readBuffer, temp);\n+    len = DelegatingSeekableInputStream.readDirectBuffer(stream, readBuffer, temp);\n     Assert.assertEquals(0, len);\n \n     readBuffer.reset();\n@@ -401,13 +501,13 @@ public void testDirectSmallTempBufferWithPositionAndLimit() throws Exception {\n   public void testHeapReadFullySmallBuffer() throws Exception {\n     ByteBuffer readBuffer = ByteBuffer.allocate(8);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream());\n+    MockInputStream stream = new MockInputStream();\n \n-    H1SeekableInputStream.readFullyHeapBuffer(hadoopStream, readBuffer);\n+    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(8, readBuffer.position());\n     Assert.assertEquals(8, readBuffer.limit());\n \n-    H1SeekableInputStream.readFullyHeapBuffer(hadoopStream, readBuffer);\n+    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(8, readBuffer.position());\n     Assert.assertEquals(8, readBuffer.limit());\n \n@@ -420,13 +520,13 @@ public void testHeapReadFullySmallBuffer() throws Exception {\n   public void testHeapReadFullyLargeBuffer() throws Exception {\n     final ByteBuffer readBuffer = ByteBuffer.allocate(20);\n \n-    final FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream());\n+    final MockInputStream stream = new MockInputStream();\n \n     TestUtils.assertThrows(\"Should throw EOFException\",\n         EOFException.class, new Callable() {\n           @Override\n           public Object call() throws Exception {\n-            H1SeekableInputStream.readFullyHeapBuffer(hadoopStream, readBuffer);\n+            DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);\n             return null;\n           }\n         });\n@@ -439,15 +539,15 @@ public Object call() throws Exception {\n   public void testHeapReadFullyJustRight() throws Exception {\n     final ByteBuffer readBuffer = ByteBuffer.allocate(10);\n \n-    final FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream());\n+    MockInputStream stream = new MockInputStream();\n \n     // reads all of the bytes available without EOFException\n-    H1SeekableInputStream.readFullyHeapBuffer(hadoopStream, readBuffer);\n+    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n     // trying to read 0 more bytes doesn't result in EOFException\n-    H1SeekableInputStream.readFullyHeapBuffer(hadoopStream, readBuffer);\n+    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n@@ -460,13 +560,13 @@ public void testHeapReadFullyJustRight() throws Exception {\n   public void testHeapReadFullySmallReads() throws Exception {\n     final ByteBuffer readBuffer = ByteBuffer.allocate(10);\n \n-    final FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    MockInputStream stream = new MockInputStream(2, 3, 3);\n \n-    H1SeekableInputStream.readFullyHeapBuffer(hadoopStream, readBuffer);\n+    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n-    H1SeekableInputStream.readFullyHeapBuffer(hadoopStream, readBuffer);\n+    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n@@ -481,13 +581,13 @@ public void testHeapReadFullyPosition() throws Exception {\n     readBuffer.position(3);\n     readBuffer.mark();\n \n-    final FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    MockInputStream stream = new MockInputStream(2, 3, 3);\n \n-    H1SeekableInputStream.readFullyHeapBuffer(hadoopStream, readBuffer);\n+    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n-    H1SeekableInputStream.readFullyHeapBuffer(hadoopStream, readBuffer);\n+    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n@@ -501,13 +601,13 @@ public void testHeapReadFullyLimit() throws Exception {\n     final ByteBuffer readBuffer = ByteBuffer.allocate(10);\n     readBuffer.limit(7);\n \n-    final FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    MockInputStream stream = new MockInputStream(2, 3, 3);\n \n-    H1SeekableInputStream.readFullyHeapBuffer(hadoopStream, readBuffer);\n+    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(7, readBuffer.position());\n     Assert.assertEquals(7, readBuffer.limit());\n \n-    H1SeekableInputStream.readFullyHeapBuffer(hadoopStream, readBuffer);\n+    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(7, readBuffer.position());\n     Assert.assertEquals(7, readBuffer.limit());\n \n@@ -517,7 +617,7 @@ public void testHeapReadFullyLimit() throws Exception {\n \n     readBuffer.position(7);\n     readBuffer.limit(10);\n-    H1SeekableInputStream.readFullyHeapBuffer(hadoopStream, readBuffer);\n+    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n@@ -533,13 +633,13 @@ public void testHeapReadFullyPositionAndLimit() throws Exception {\n     readBuffer.limit(7);\n     readBuffer.mark();\n \n-    final FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    MockInputStream stream = new MockInputStream(2, 3, 3);\n \n-    H1SeekableInputStream.readFullyHeapBuffer(hadoopStream, readBuffer);\n+    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(7, readBuffer.position());\n     Assert.assertEquals(7, readBuffer.limit());\n \n-    H1SeekableInputStream.readFullyHeapBuffer(hadoopStream, readBuffer);\n+    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(7, readBuffer.position());\n     Assert.assertEquals(7, readBuffer.limit());\n \n@@ -549,7 +649,7 @@ public void testHeapReadFullyPositionAndLimit() throws Exception {\n \n     readBuffer.position(7);\n     readBuffer.limit(10);\n-    H1SeekableInputStream.readFullyHeapBuffer(hadoopStream, readBuffer);\n+    DelegatingSeekableInputStream.readFullyHeapBuffer(stream, readBuffer);\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n@@ -562,13 +662,13 @@ public void testHeapReadFullyPositionAndLimit() throws Exception {\n   public void testDirectReadFullySmallBuffer() throws Exception {\n     ByteBuffer readBuffer = ByteBuffer.allocateDirect(8);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream());\n+    MockInputStream stream = new MockInputStream();\n \n-    H1SeekableInputStream.readFullyDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(8, readBuffer.position());\n     Assert.assertEquals(8, readBuffer.limit());\n \n-    H1SeekableInputStream.readFullyDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(8, readBuffer.position());\n     Assert.assertEquals(8, readBuffer.limit());\n \n@@ -581,13 +681,13 @@ public void testDirectReadFullySmallBuffer() throws Exception {\n   public void testDirectReadFullyLargeBuffer() throws Exception {\n     final ByteBuffer readBuffer = ByteBuffer.allocateDirect(20);\n \n-    final FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream());\n+    final MockInputStream stream = new MockInputStream();\n \n     TestUtils.assertThrows(\"Should throw EOFException\",\n         EOFException.class, new Callable() {\n           @Override\n           public Object call() throws Exception {\n-            H1SeekableInputStream.readFullyDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+            DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());\n             return null;\n           }\n         });\n@@ -606,15 +706,15 @@ public Object call() throws Exception {\n   public void testDirectReadFullyJustRight() throws Exception {\n     final ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);\n \n-    final FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream());\n+    MockInputStream stream = new MockInputStream();\n \n     // reads all of the bytes available without EOFException\n-    H1SeekableInputStream.readFullyDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n     // trying to read 0 more bytes doesn't result in EOFException\n-    H1SeekableInputStream.readFullyDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n@@ -627,13 +727,13 @@ public void testDirectReadFullyJustRight() throws Exception {\n   public void testDirectReadFullySmallReads() throws Exception {\n     final ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);\n \n-    final FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    MockInputStream stream = new MockInputStream(2, 3, 3);\n \n-    H1SeekableInputStream.readFullyDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n-    H1SeekableInputStream.readFullyDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n@@ -648,13 +748,13 @@ public void testDirectReadFullyPosition() throws Exception {\n     readBuffer.position(3);\n     readBuffer.mark();\n \n-    final FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    MockInputStream stream = new MockInputStream(2, 3, 3);\n \n-    H1SeekableInputStream.readFullyDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n-    H1SeekableInputStream.readFullyDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n@@ -668,13 +768,13 @@ public void testDirectReadFullyLimit() throws Exception {\n     final ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);\n     readBuffer.limit(7);\n \n-    final FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    MockInputStream stream = new MockInputStream(2, 3, 3);\n \n-    H1SeekableInputStream.readFullyDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(7, readBuffer.position());\n     Assert.assertEquals(7, readBuffer.limit());\n \n-    H1SeekableInputStream.readFullyDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(7, readBuffer.position());\n     Assert.assertEquals(7, readBuffer.limit());\n \n@@ -684,7 +784,7 @@ public void testDirectReadFullyLimit() throws Exception {\n \n     readBuffer.position(7);\n     readBuffer.limit(10);\n-    H1SeekableInputStream.readFullyDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n@@ -700,13 +800,13 @@ public void testDirectReadFullyPositionAndLimit() throws Exception {\n     readBuffer.limit(7);\n     readBuffer.mark();\n \n-    final FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    MockInputStream stream = new MockInputStream(2, 3, 3);\n \n-    H1SeekableInputStream.readFullyDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(7, readBuffer.position());\n     Assert.assertEquals(7, readBuffer.limit());\n \n-    H1SeekableInputStream.readFullyDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(7, readBuffer.position());\n     Assert.assertEquals(7, readBuffer.limit());\n \n@@ -716,7 +816,7 @@ public void testDirectReadFullyPositionAndLimit() throws Exception {\n \n     readBuffer.position(7);\n     readBuffer.limit(10);\n-    H1SeekableInputStream.readFullyDirectBuffer(hadoopStream, readBuffer, TEMP.get());\n+    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, TEMP.get());\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n \n@@ -734,13 +834,13 @@ public void testDirectReadFullySmallTempBufferWithPositionAndLimit() throws Exce\n     readBuffer.limit(7);\n     readBuffer.mark();\n \n-    final FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    MockInputStream stream = new MockInputStream(2, 3, 3);\n \n-    H1SeekableInputStream.readFullyDirectBuffer(hadoopStream, readBuffer, temp);\n+    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, temp);\n     Assert.assertEquals(7, readBuffer.position());\n     Assert.assertEquals(7, readBuffer.limit());\n \n-    H1SeekableInputStream.readFullyDirectBuffer(hadoopStream, readBuffer, temp);\n+    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, temp);\n     Assert.assertEquals(7, readBuffer.position());\n     Assert.assertEquals(7, readBuffer.limit());\n \n@@ -750,7 +850,7 @@ public void testDirectReadFullySmallTempBufferWithPositionAndLimit() throws Exce\n \n     readBuffer.position(7);\n     readBuffer.limit(10);\n-    H1SeekableInputStream.readFullyDirectBuffer(hadoopStream, readBuffer, temp);\n+    DelegatingSeekableInputStream.readFullyDirectBuffer(stream, readBuffer, temp);\n     Assert.assertEquals(10, readBuffer.position());\n     Assert.assertEquals(10, readBuffer.limit());\n ",
                "additions": 209,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/test/java/org/apache/parquet/io/TestDelegatingSeekableInputStream.java",
                "previous_filename": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/util/TestHadoop1ByteBufferReads.java",
                "status": "renamed",
                "changes": 318,
                "deletions": 109,
                "sha": "078bc8f78317e9c711fae5ca7b0b76845ffdedc9",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-common/src/test/java/org/apache/parquet/io/TestDelegatingSeekableInputStream.java",
                "filename": "parquet-common/src/test/java/org/apache/parquet/io/TestDelegatingSeekableInputStream.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-common/src/test/java/org/apache/parquet/io/TestDelegatingSeekableInputStream.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -0,0 +1,98 @@\n+/*\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing,\n+ *  software distributed under the License is distributed on an\n+ *  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ *  KIND, either express or implied.  See the License for the\n+ *  specific language governing permissions and limitations\n+ *  under the License.\n+ */\n+\n+package org.apache.parquet;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.parquet.bytes.ByteBufferAllocator;\n+import org.apache.parquet.compression.CompressionCodecFactory;\n+import org.apache.parquet.filter2.compat.FilterCompat;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter.MetadataFilter;\n+import org.apache.parquet.hadoop.util.HadoopCodecs;\n+\n+import java.util.Map;\n+\n+import static org.apache.parquet.hadoop.ParquetInputFormat.DICTIONARY_FILTERING_ENABLED;\n+import static org.apache.parquet.hadoop.ParquetInputFormat.RECORD_FILTERING_ENABLED;\n+import static org.apache.parquet.hadoop.ParquetInputFormat.STATS_FILTERING_ENABLED;\n+import static org.apache.parquet.hadoop.ParquetInputFormat.getFilter;\n+import static org.apache.parquet.hadoop.UnmaterializableRecordCounter.BAD_RECORD_THRESHOLD_CONF_KEY;\n+\n+public class HadoopReadOptions extends ParquetReadOptions {\n+  private final Configuration conf;\n+\n+  private HadoopReadOptions(boolean useSignedStringMinMax,\n+                            boolean useStatsFilter,\n+                            boolean useDictionaryFilter,\n+                            boolean useRecordFilter,\n+                            FilterCompat.Filter recordFilter,\n+                            MetadataFilter metadataFilter,\n+                            CompressionCodecFactory codecFactory,\n+                            ByteBufferAllocator allocator,\n+                            Map<String, String> properties,\n+                            Configuration conf) {\n+    super(\n+        useSignedStringMinMax, useStatsFilter, useDictionaryFilter, useRecordFilter, recordFilter,\n+        metadataFilter, codecFactory, allocator, properties\n+    );\n+    this.conf = conf;\n+  }\n+\n+  @Override\n+  public String getProperty(String property) {\n+    String value = super.getProperty(property);\n+    if (value != null) {\n+      return value;\n+    }\n+    return conf.get(property);\n+  }\n+\n+  public Configuration getConf() {\n+    return conf;\n+  }\n+\n+  public static Builder builder(Configuration conf) {\n+    return new Builder(conf);\n+  }\n+\n+  public static class Builder extends ParquetReadOptions.Builder {\n+    private final Configuration conf;\n+\n+    public Builder(Configuration conf) {\n+      this.conf = conf;\n+      useSignedStringMinMax(conf.getBoolean(\"parquet.strings.signed-min-max.enabled\", false));\n+      useDictionaryFilter(conf.getBoolean(STATS_FILTERING_ENABLED, true));\n+      useStatsFilter(conf.getBoolean(DICTIONARY_FILTERING_ENABLED, true));\n+      useRecordFilter(conf.getBoolean(RECORD_FILTERING_ENABLED, true));\n+      withCodecFactory(HadoopCodecs.newFactory(conf, 0));\n+      withRecordFilter(getFilter(conf));\n+      String badRecordThresh = conf.get(BAD_RECORD_THRESHOLD_CONF_KEY);\n+      if (badRecordThresh != null) {\n+        set(BAD_RECORD_THRESHOLD_CONF_KEY, badRecordThresh);\n+      }\n+    }\n+\n+    @Override\n+    public ParquetReadOptions build() {\n+      return new HadoopReadOptions(\n+          useSignedStringMinMax, useStatsFilter, useDictionaryFilter, useRecordFilter,\n+          recordFilter, metadataFilter, codecFactory, allocator, properties, conf);\n+    }\n+  }\n+}",
                "additions": 98,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/HadoopReadOptions.java",
                "status": "added",
                "changes": 98,
                "deletions": 0,
                "sha": "87c8ac97d81294f82b9262a3efbbd050ec743be1",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/HadoopReadOptions.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/HadoopReadOptions.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/HadoopReadOptions.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -0,0 +1,232 @@\n+/*\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing,\n+ *  software distributed under the License is distributed on an\n+ *  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ *  KIND, either express or implied.  See the License for the\n+ *  specific language governing permissions and limitations\n+ *  under the License.\n+ */\n+\n+package org.apache.parquet;\n+\n+import org.apache.parquet.bytes.ByteBufferAllocator;\n+import org.apache.parquet.bytes.HeapByteBufferAllocator;\n+import org.apache.parquet.compression.CompressionCodecFactory;\n+import org.apache.parquet.filter2.compat.FilterCompat;\n+import org.apache.parquet.format.converter.ParquetMetadataConverter;\n+import org.apache.parquet.hadoop.util.HadoopCodecs;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Set;\n+\n+import static org.apache.parquet.format.converter.ParquetMetadataConverter.NO_FILTER;\n+\n+// Internal use only\n+public class ParquetReadOptions {\n+  private static final boolean RECORD_FILTERING_ENABLED_DEFAULT = true;\n+  private static final boolean STATS_FILTERING_ENABLED_DEFAULT = true;\n+  private static final boolean DICTIONARY_FILTERING_ENABLED_DEFAULT = true;\n+\n+  private final boolean useSignedStringMinMax;\n+  private final boolean useStatsFilter;\n+  private final boolean useDictionaryFilter;\n+  private final boolean useRecordFilter;\n+  private final FilterCompat.Filter recordFilter;\n+  private final ParquetMetadataConverter.MetadataFilter metadataFilter;\n+  private final CompressionCodecFactory codecFactory;\n+  private final ByteBufferAllocator allocator;\n+  private final Map<String, String> properties;\n+\n+  ParquetReadOptions(boolean useSignedStringMinMax,\n+                             boolean useStatsFilter,\n+                             boolean useDictionaryFilter,\n+                             boolean useRecordFilter,\n+                             FilterCompat.Filter recordFilter,\n+                             ParquetMetadataConverter.MetadataFilter metadataFilter,\n+                             CompressionCodecFactory codecFactory,\n+                             ByteBufferAllocator allocator,\n+                             Map<String, String> properties) {\n+    this.useSignedStringMinMax = useSignedStringMinMax;\n+    this.useStatsFilter = useStatsFilter;\n+    this.useDictionaryFilter = useDictionaryFilter;\n+    this.useRecordFilter = useRecordFilter;\n+    this.recordFilter = recordFilter;\n+    this.metadataFilter = metadataFilter;\n+    this.codecFactory = codecFactory;\n+    this.allocator = allocator;\n+    this.properties = Collections.unmodifiableMap(properties);\n+  }\n+\n+  public boolean useSignedStringMinMax() {\n+    return useSignedStringMinMax;\n+  }\n+\n+  public boolean useStatsFilter() {\n+    return useStatsFilter;\n+  }\n+\n+  public boolean useDictionaryFilter() {\n+    return useDictionaryFilter;\n+  }\n+\n+  public boolean useRecordFilter() {\n+    return useRecordFilter;\n+  }\n+\n+  public FilterCompat.Filter getRecordFilter() {\n+    return recordFilter;\n+  }\n+\n+  public ParquetMetadataConverter.MetadataFilter getMetadataFilter() {\n+    return metadataFilter;\n+  }\n+\n+  public CompressionCodecFactory getCodecFactory() {\n+    return codecFactory;\n+  }\n+\n+  public ByteBufferAllocator getAllocator() {\n+    return allocator;\n+  }\n+\n+  public Set<String> getPropertyNames() {\n+    return properties.keySet();\n+  }\n+\n+  public String getProperty(String property) {\n+    return properties.get(property);\n+  }\n+\n+  public boolean isEnabled(String property, boolean defaultValue) {\n+    if (properties.containsKey(property)) {\n+      return Boolean.valueOf(properties.get(property));\n+    } else {\n+      return defaultValue;\n+    }\n+  }\n+\n+  public static Builder builder() {\n+    return new Builder();\n+  }\n+\n+  public static class Builder {\n+    boolean useSignedStringMinMax = false;\n+    boolean useStatsFilter = STATS_FILTERING_ENABLED_DEFAULT;\n+    boolean useDictionaryFilter = DICTIONARY_FILTERING_ENABLED_DEFAULT;\n+    boolean useRecordFilter = RECORD_FILTERING_ENABLED_DEFAULT;\n+    FilterCompat.Filter recordFilter = null;\n+    ParquetMetadataConverter.MetadataFilter metadataFilter = NO_FILTER;\n+    // the page size parameter isn't used when only using the codec factory to get decompressors\n+    CompressionCodecFactory codecFactory = HadoopCodecs.newFactory(0);\n+    ByteBufferAllocator allocator = new HeapByteBufferAllocator();\n+    Map<String, String> properties = new HashMap<>();\n+\n+    public Builder useSignedStringMinMax(boolean useSignedStringMinMax) {\n+      this.useSignedStringMinMax = useSignedStringMinMax;\n+      return this;\n+    }\n+\n+    public Builder useSignedStringMinMax() {\n+      this.useSignedStringMinMax = true;\n+      return this;\n+    }\n+\n+    public Builder useStatsFilter(boolean useStatsFilter) {\n+      this.useStatsFilter = useStatsFilter;\n+      return this;\n+    }\n+\n+    public Builder useStatsFilter() {\n+      this.useStatsFilter = true;\n+      return this;\n+    }\n+\n+    public Builder useDictionaryFilter(boolean useDictionaryFilter) {\n+      this.useDictionaryFilter = useDictionaryFilter;\n+      return this;\n+    }\n+\n+    public Builder useDictionaryFilter() {\n+      this.useDictionaryFilter = true;\n+      return this;\n+    }\n+\n+    public Builder useRecordFilter(boolean useRecordFilter) {\n+      this.useRecordFilter = useRecordFilter;\n+      return this;\n+    }\n+\n+    public Builder useRecordFilter() {\n+      this.useRecordFilter = true;\n+      return this;\n+    }\n+\n+    public Builder withRecordFilter(FilterCompat.Filter rowGroupFilter) {\n+      this.recordFilter = rowGroupFilter;\n+      return this;\n+    }\n+\n+    public Builder withRange(long start, long end) {\n+      this.metadataFilter = ParquetMetadataConverter.range(start, end);\n+      return this;\n+    }\n+\n+    public Builder withOffsets(long... rowGroupOffsets) {\n+      this.metadataFilter = ParquetMetadataConverter.offsets(rowGroupOffsets);\n+      return this;\n+    }\n+\n+    public Builder withMetadataFilter(ParquetMetadataConverter.MetadataFilter metadataFilter) {\n+      this.metadataFilter = metadataFilter;\n+      return this;\n+    }\n+\n+    public Builder withCodecFactory(CompressionCodecFactory codecFactory) {\n+      this.codecFactory = codecFactory;\n+      return this;\n+    }\n+\n+    public Builder withAllocator(ByteBufferAllocator allocator) {\n+      this.allocator = allocator;\n+      return this;\n+    }\n+\n+    public Builder set(String key, String value) {\n+      properties.put(key, value);\n+      return this;\n+    }\n+\n+    public Builder copy(ParquetReadOptions options) {\n+      useSignedStringMinMax(options.useSignedStringMinMax);\n+      useStatsFilter(options.useStatsFilter);\n+      useDictionaryFilter(options.useDictionaryFilter);\n+      useRecordFilter(options.useRecordFilter);\n+      withRecordFilter(options.recordFilter);\n+      withMetadataFilter(options.metadataFilter);\n+      withCodecFactory(options.codecFactory);\n+      withAllocator(options.allocator);\n+      for (Map.Entry<String, String> keyValue : options.properties.entrySet()) {\n+        set(keyValue.getKey(), keyValue.getValue());\n+      }\n+      return this;\n+    }\n+\n+    public ParquetReadOptions build() {\n+      return new ParquetReadOptions(\n+          useSignedStringMinMax, useStatsFilter, useDictionaryFilter, useRecordFilter,\n+          recordFilter, metadataFilter, codecFactory, allocator, properties);\n+    }\n+  }\n+}",
                "additions": 232,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/ParquetReadOptions.java",
                "status": "added",
                "changes": 232,
                "deletions": 0,
                "sha": "5f2f0a85291e7672be75cbde931277eb7e191c46",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/ParquetReadOptions.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/ParquetReadOptions.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/ParquetReadOptions.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -51,6 +51,10 @@\n     DICTIONARY\n   }\n \n+  /**\n+   * @deprecated will be removed in 2.0.0.\n+   */\n+  @Deprecated\n   public static List<BlockMetaData> filterRowGroups(Filter filter, List<BlockMetaData> blocks, MessageType schema) {\n     checkNotNull(filter, \"filter\");\n     return filter.accept(new RowGroupFilter(blocks, schema));",
                "additions": 4,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/filter2/compat/RowGroupFilter.java",
                "status": "modified",
                "changes": 4,
                "deletions": 0,
                "sha": "68c38ce4d6dbc9fba555f18cc81be6fce007ad69",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/filter2/compat/RowGroupFilter.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/filter2/compat/RowGroupFilter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/filter2/compat/RowGroupFilter.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -38,6 +38,8 @@\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.parquet.CorruptStatistics;\n+import org.apache.parquet.ParquetReadOptions;\n+import org.apache.parquet.format.CompressionCodec;\n import org.apache.parquet.format.PageEncodingStats;\n import org.apache.parquet.hadoop.metadata.ColumnPath;\n import org.apache.parquet.format.ColumnChunk;\n@@ -89,10 +91,18 @@ public ParquetMetadataConverter() {\n     this(false);\n   }\n \n+  /**\n+   * @deprecated will be removed in 2.0.0; use {@code ParquetMetadataConverter(ParquetReadOptions)}\n+   */\n+  @Deprecated\n   public ParquetMetadataConverter(Configuration conf) {\n     this(conf.getBoolean(\"parquet.strings.signed-min-max.enabled\", false));\n   }\n \n+  public ParquetMetadataConverter(ParquetReadOptions options) {\n+    this(options.useSignedStringMinMax());\n+  }\n+\n   private ParquetMetadataConverter(boolean useSignedStringMinMax) {\n     this.useSignedStringMinMax = useSignedStringMinMax;\n   }\n@@ -193,7 +203,7 @@ private void addRowGroup(ParquetMetadata parquetMetadata, List<RowGroup> rowGrou\n           getType(columnMetaData.getType()),\n           toFormatEncodings(columnMetaData.getEncodings()),\n           Arrays.asList(columnMetaData.getPath().toArray()),\n-          columnMetaData.getCodec().getParquetCompressionCodec(),\n+          toFormatCodec(columnMetaData.getCodec()),\n           columnMetaData.getValueCount(),\n           columnMetaData.getTotalUncompressedSize(),\n           columnMetaData.getTotalSize(),\n@@ -246,6 +256,14 @@ private void addRowGroup(ParquetMetadata parquetMetadata, List<RowGroup> rowGrou\n     return cached;\n   }\n \n+  private CompressionCodecName fromFormatCodec(CompressionCodec codec) {\n+    return CompressionCodecName.valueOf(codec.toString());\n+  }\n+\n+  private CompressionCodec toFormatCodec(CompressionCodecName codec) {\n+    return CompressionCodec.valueOf(codec.toString());\n+  }\n+\n   public org.apache.parquet.column.Encoding getEncoding(Encoding encoding) {\n     return org.apache.parquet.column.Encoding.valueOf(encoding.name());\n   }\n@@ -820,7 +838,7 @@ public ParquetMetadata fromParquetMetadata(FileMetaData parquetMetadata) throws\n           ColumnChunkMetaData column = ColumnChunkMetaData.get(\n               path,\n               messageType.getType(path.toArray()).asPrimitiveType().getPrimitiveTypeName(),\n-              CompressionCodecName.fromParquet(metaData.codec),\n+              fromFormatCodec(metaData.codec),\n               convertEncodingStats(metaData.getEncoding_stats()),\n               fromFormatEncodings(metaData.encodings),\n               fromParquetStatistics(",
                "additions": 20,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java",
                "status": "modified",
                "changes": 22,
                "deletions": 2,
                "sha": "bba7e62e6dc8003442760feb35c49b4f08a98159",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -36,9 +36,10 @@\n \n import org.apache.parquet.bytes.ByteBufferAllocator;\n import org.apache.parquet.bytes.BytesInput;\n+import org.apache.parquet.compression.CompressionCodecFactory;\n import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n \n-public class CodecFactory {\n+public class CodecFactory implements CompressionCodecFactory {\n \n   protected static final Map<String, CompressionCodec> CODEC_BY_NAME = Collections\n       .synchronizedMap(new HashMap<String, CompressionCodec>());\n@@ -118,7 +119,7 @@ public void decompress(ByteBuffer input, int compressedSize, ByteBuffer output,\n       output.put(decompressed);\n     }\n \n-    protected void release() {\n+    public void release() {\n       if (decompressor != null) {\n         CodecPool.returnDecompressor(decompressor);\n       }\n@@ -171,7 +172,7 @@ public BytesInput compress(BytesInput bytes) throws IOException {\n     }\n \n     @Override\n-    protected void release() {\n+    public void release() {\n       if (compressor != null) {\n         CodecPool.returnCompressor(compressor);\n       }\n@@ -183,6 +184,7 @@ public CompressionCodecName getCodecName() {\n \n   }\n \n+  @Override\n   public BytesCompressor getCompressor(CompressionCodecName codecName) {\n     BytesCompressor comp = compressors.get(codecName);\n     if (comp == null) {\n@@ -192,6 +194,7 @@ public BytesCompressor getCompressor(CompressionCodecName codecName) {\n     return comp;\n   }\n \n+  @Override\n   public BytesDecompressor getDecompressor(CompressionCodecName codecName) {\n     BytesDecompressor decomp = decompressors.get(codecName);\n     if (decomp == null) {\n@@ -235,6 +238,7 @@ protected CompressionCodec getCodec(CompressionCodecName codecName) {\n     }\n   }\n \n+  @Override\n   public void release() {\n     for (BytesCompressor compressor : compressors.values()) {\n       compressor.release();\n@@ -246,15 +250,23 @@ public void release() {\n     decompressors.clear();\n   }\n \n-  public static abstract class BytesCompressor {\n+  /**\n+   * @deprecated will be removed in 2.0.0; use CompressionCodecFactory.BytesInputCompressor instead.\n+   */\n+  @Deprecated\n+  public static abstract class BytesCompressor implements CompressionCodecFactory.BytesInputCompressor {\n     public abstract BytesInput compress(BytesInput bytes) throws IOException;\n     public abstract CompressionCodecName getCodecName();\n-    protected abstract void release();\n+    public abstract void release();\n   }\n \n-  public static abstract class BytesDecompressor {\n+  /**\n+   * @deprecated will be removed in 2.0.0; use CompressionCodecFactory.BytesInputDecompressor instead.\n+   */\n+  @Deprecated\n+  public static abstract class BytesDecompressor implements CompressionCodecFactory.BytesInputDecompressor {\n     public abstract BytesInput decompress(BytesInput bytes, int uncompressedSize) throws IOException;\n     public abstract void decompress(ByteBuffer input, int compressedSize, ByteBuffer output, int uncompressedSize) throws IOException;\n-    protected abstract void release();\n+    public abstract void release();\n   }\n }",
                "additions": 19,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/CodecFactory.java",
                "status": "modified",
                "changes": 26,
                "deletions": 7,
                "sha": "8befa79592a647cdcc8d0534abc8d2065a330a7b",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/CodecFactory.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/CodecFactory.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/CodecFactory.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -33,6 +33,8 @@\n import org.apache.parquet.column.page.DictionaryPageReadStore;\n import org.apache.parquet.column.page.PageReadStore;\n import org.apache.parquet.column.page.PageReader;\n+import org.apache.parquet.compression.CompressionCodecFactory;\n+import org.apache.parquet.compression.CompressionCodecFactory.BytesInputDecompressor;\n import org.apache.parquet.hadoop.CodecFactory.BytesDecompressor;\n import org.apache.parquet.io.ParquetDecodingException;\n import org.slf4j.Logger;\n@@ -56,12 +58,12 @@\n    */\n   static final class ColumnChunkPageReader implements PageReader {\n \n-    private final BytesDecompressor decompressor;\n+    private final BytesInputDecompressor decompressor;\n     private final long valueCount;\n     private final List<DataPage> compressedPages;\n     private final DictionaryPage compressedDictionaryPage;\n \n-    ColumnChunkPageReader(BytesDecompressor decompressor, List<DataPage> compressedPages, DictionaryPage compressedDictionaryPage) {\n+    ColumnChunkPageReader(BytesInputDecompressor decompressor, List<DataPage> compressedPages, DictionaryPage compressedDictionaryPage) {\n       this.decompressor = decompressor;\n       this.compressedPages = new LinkedList<DataPage>(compressedPages);\n       this.compressedDictionaryPage = compressedDictionaryPage;",
                "additions": 4,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ColumnChunkPageReadStore.java",
                "status": "modified",
                "changes": 6,
                "deletions": 2,
                "sha": "37dfd6d39496ab466b7a87c5ae35a2ae0f2e730e",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ColumnChunkPageReadStore.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ColumnChunkPageReadStore.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ColumnChunkPageReadStore.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -179,7 +179,7 @@ public void decompress(ByteBuffer input, int compressedSize, ByteBuffer output,\n     }\n \n     @Override\n-    protected void release() {\n+    public void release() {\n       DirectCodecPool.INSTANCE.returnDecompressor(decompressor);\n     }\n   }\n@@ -221,7 +221,7 @@ public void decompress(ByteBuffer input, int compressedSize, ByteBuffer output,\n     }\n \n     @Override\n-    protected void release() {\n+    public void release() {\n       DirectCodecPool.INSTANCE.returnDirectDecompressor(decompressor);\n       extraDecompressor.release();\n     }\n@@ -245,7 +245,7 @@ public BytesInput decompress(BytesInput bytes, int uncompressedSize) throws IOEx\n     }\n \n     @Override\n-    protected void release() {}\n+    public void release() {}\n \n   }\n \n@@ -269,7 +269,7 @@ public void decompress(ByteBuffer src, int compressedSize, ByteBuffer dst, int u\n     }\n \n     @Override\n-    protected void release() {}\n+    public void release() {}\n   }\n \n   public class SnappyCompressor extends BytesCompressor {\n@@ -311,7 +311,7 @@ public CompressionCodecName getCodecName() {\n     }\n \n     @Override\n-    protected void release() {\n+    public void release() {\n       outgoing = DirectCodecFactory.this.release(outgoing);\n       incoming = DirectCodecFactory.this.release(incoming);\n     }\n@@ -333,7 +333,7 @@ public CompressionCodecName getCodecName() {\n     }\n \n     @Override\n-    protected void release() {}\n+    public void release() {}\n   }\n \n   static class DirectCodecPool {",
                "additions": 6,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/DirectCodecFactory.java",
                "status": "modified",
                "changes": 12,
                "deletions": 6,
                "sha": "58e79ace067dafac6ca300f9783994b6a0f2da4f",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/DirectCodecFactory.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/DirectCodecFactory.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/DirectCodecFactory.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -27,6 +27,8 @@\n \n import org.apache.hadoop.conf.Configuration;\n \n+import org.apache.parquet.HadoopReadOptions;\n+import org.apache.parquet.ParquetReadOptions;\n import org.apache.parquet.column.page.PageReadStore;\n import org.apache.parquet.filter.UnboundRecordFilter;\n import org.apache.parquet.filter2.compat.FilterCompat;\n@@ -47,7 +49,6 @@\n import static java.lang.String.format;\n import static org.apache.parquet.Preconditions.checkNotNull;\n import static org.apache.parquet.hadoop.ParquetInputFormat.RECORD_FILTERING_ENABLED;\n-import static org.apache.parquet.hadoop.ParquetInputFormat.RECORD_FILTERING_ENABLED_DEFAULT;\n import static org.apache.parquet.hadoop.ParquetInputFormat.STRICT_TYPE_CHECKING;\n \n class InternalParquetRecordReader<T> {\n@@ -160,6 +161,34 @@ public float getProgress() throws IOException, InterruptedException {\n     return (float) current / total;\n   }\n \n+  public void initialize(ParquetFileReader reader, ParquetReadOptions options) {\n+    // copy custom configuration to the Configuration passed to the ReadSupport\n+    Configuration conf = new Configuration();\n+    if (options instanceof HadoopReadOptions) {\n+      conf = ((HadoopReadOptions) options).getConf();\n+    }\n+    for (String property : options.getPropertyNames()) {\n+      conf.set(property, options.getProperty(property));\n+    }\n+\n+    // initialize a ReadContext for this file\n+    this.reader = reader;\n+    FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+    this.fileSchema = parquetFileMetadata.getSchema();\n+    Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+    ReadSupport.ReadContext readContext = readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+    this.columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+    this.requestedSchema = readContext.getRequestedSchema();\n+    this.columnCount = requestedSchema.getPaths().size();\n+    this.recordConverter = readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+    this.strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+    this.total = reader.getRecordCount();\n+    this.unmaterializableRecordCounter = new UnmaterializableRecordCounter(options, total);\n+    this.filterRecords = options.useRecordFilter();\n+    reader.setRequestedSchema(requestedSchema);\n+    LOG.info(\"RecordReader initialized will read a total of {} records.\", total);\n+  }\n+\n   public void initialize(ParquetFileReader reader, Configuration configuration)\n       throws IOException {\n     // initialize a ReadContext for this file\n@@ -177,8 +206,7 @@ public void initialize(ParquetFileReader reader, Configuration configuration)\n     this.strictTypeChecking = configuration.getBoolean(STRICT_TYPE_CHECKING, true);\n     this.total = reader.getRecordCount();\n     this.unmaterializableRecordCounter = new UnmaterializableRecordCounter(configuration, total);\n-    this.filterRecords = configuration.getBoolean(\n-        RECORD_FILTERING_ENABLED, RECORD_FILTERING_ENABLED_DEFAULT);\n+    this.filterRecords = configuration.getBoolean(RECORD_FILTERING_ENABLED, true);\n     reader.setRequestedSchema(requestedSchema);\n     LOG.info(\"RecordReader initialized will read a total of {} records.\", total);\n   }",
                "additions": 31,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/InternalParquetRecordReader.java",
                "status": "modified",
                "changes": 34,
                "deletions": 3,
                "sha": "a048878693de37a32771fe8195db6a6f58fd2b6f",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/InternalParquetRecordReader.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/InternalParquetRecordReader.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/InternalParquetRecordReader.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -26,10 +26,6 @@\n import static org.apache.parquet.hadoop.ParquetFileWriter.MAGIC;\n import static org.apache.parquet.hadoop.ParquetFileWriter.PARQUET_COMMON_METADATA_FILE;\n import static org.apache.parquet.hadoop.ParquetFileWriter.PARQUET_METADATA_FILE;\n-import static org.apache.parquet.hadoop.ParquetInputFormat.DICTIONARY_FILTERING_ENABLED;\n-import static org.apache.parquet.hadoop.ParquetInputFormat.DICTIONARY_FILTERING_ENABLED_DEFAULT;\n-import static org.apache.parquet.hadoop.ParquetInputFormat.STATS_FILTERING_ENABLED;\n-import static org.apache.parquet.hadoop.ParquetInputFormat.STATS_FILTERING_ENABLED_DEFAULT;\n \n import java.io.Closeable;\n import java.io.IOException;\n@@ -51,17 +47,16 @@\n import java.util.concurrent.Executors;\n import java.util.concurrent.Future;\n \n-import org.apache.hadoop.conf.Configurable;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n \n-import org.apache.parquet.bytes.ByteBufferAllocator;\n+import org.apache.parquet.ParquetReadOptions;\n import org.apache.parquet.bytes.ByteBufferInputStream;\n-import org.apache.parquet.bytes.HeapByteBufferAllocator;\n import org.apache.parquet.column.Encoding;\n import org.apache.parquet.column.page.DictionaryPageReadStore;\n+import org.apache.parquet.compression.CompressionCodecFactory.BytesInputDecompressor;\n import org.apache.parquet.filter2.compat.FilterCompat;\n import org.apache.parquet.filter2.compat.RowGroupFilter;\n \n@@ -80,15 +75,14 @@\n import org.apache.parquet.format.Util;\n import org.apache.parquet.format.converter.ParquetMetadataConverter;\n import org.apache.parquet.format.converter.ParquetMetadataConverter.MetadataFilter;\n-import org.apache.parquet.hadoop.CodecFactory.BytesDecompressor;\n import org.apache.parquet.hadoop.ColumnChunkPageReadStore.ColumnChunkPageReader;\n import org.apache.parquet.hadoop.metadata.BlockMetaData;\n import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n import org.apache.parquet.hadoop.metadata.FileMetaData;\n import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.HadoopReadOptions;\n import org.apache.parquet.hadoop.util.HiddenFileFilter;\n-import org.apache.parquet.hadoop.util.HadoopStreams;\n import org.apache.parquet.io.SeekableInputStream;\n import org.apache.parquet.hadoop.util.counters.BenchmarkCounter;\n import org.apache.parquet.io.ParquetDecodingException;\n@@ -119,6 +113,7 @@\n    * @param partFiles the part files to read\n    * @return the footers for those files using the summary file if possible.\n    * @throws IOException\n+   * @deprecated metadata files are not recommended and will be removed in 2.0.0\n    */\n   @Deprecated\n   public static List<Footer> readAllFootersInParallelUsingSummaryFiles(Configuration configuration, List<FileStatus> partFiles) throws IOException {\n@@ -137,7 +132,9 @@ private static MetadataFilter filter(boolean skipRowGroups) {\n    * @param skipRowGroups to skipRowGroups in the footers\n    * @return the footers for those files using the summary file if possible.\n    * @throws IOException\n+   * @deprecated metadata files are not recommended and will be removed in 2.0.0\n    */\n+  @Deprecated\n   public static List<Footer> readAllFootersInParallelUsingSummaryFiles(\n       final Configuration configuration,\n       final Collection<FileStatus> partFiles,\n@@ -233,6 +230,9 @@ private static MetadataFilter filter(boolean skipRowGroups) {\n     }\n   }\n \n+  /**\n+   * @deprecated metadata files are not recommended and will be removed in 2.0.0\n+   */\n   @Deprecated\n   public static List<Footer> readAllFootersInParallel(final Configuration configuration, List<FileStatus> partFiles) throws IOException {\n     return readAllFootersInParallel(configuration, partFiles, false);\n@@ -246,7 +246,10 @@ private static MetadataFilter filter(boolean skipRowGroups) {\n    * @param skipRowGroups to skip the rowGroup info\n    * @return the footers\n    * @throws IOException\n+   * @deprecated will be removed in 2.0.0;\n+   *             use {@link ParquetFileReader#open(InputFile, ParquetReadOptions)}\n    */\n+  @Deprecated\n   public static List<Footer> readAllFootersInParallel(final Configuration configuration, List<FileStatus> partFiles, final boolean skipRowGroups) throws IOException {\n     List<Callable<Footer>> footers = new ArrayList<Callable<Footer>>();\n     for (final FileStatus currentFile : partFiles) {\n@@ -271,7 +274,10 @@ public Footer call() throws Exception {\n   /**\n    * Read the footers of all the files under that path (recursively)\n    * not using summary files.\n+   * @deprecated will be removed in 2.0.0;\n+   *             use {@link ParquetFileReader#open(InputFile, ParquetReadOptions)}\n    */\n+  @Deprecated\n   public static List<Footer> readAllFootersInParallel(Configuration configuration, FileStatus fileStatus, boolean skipRowGroups) throws IOException {\n     List<FileStatus> statuses = listFiles(configuration, fileStatus);\n     return readAllFootersInParallel(configuration, statuses, skipRowGroups);\n@@ -285,12 +291,18 @@ public Footer call() throws Exception {\n    * @param fileStatus the root dir\n    * @return all the footers\n    * @throws IOException\n+   * @deprecated will be removed in 2.0.0;\n+   *             use {@link ParquetFileReader#open(InputFile, ParquetReadOptions)}\n    */\n+  @Deprecated\n   public static List<Footer> readAllFootersInParallel(Configuration configuration, FileStatus fileStatus) throws IOException {\n     return readAllFootersInParallel(configuration, fileStatus, false);\n   }\n \n-\n+  /**\n+   * @deprecated will be removed in 2.0.0;\n+   *             use {@link ParquetFileReader#open(InputFile, ParquetReadOptions)}\n+   */\n   @Deprecated\n   public static List<Footer> readFooters(Configuration configuration, Path path) throws IOException {\n     return readFooters(configuration, status(configuration, path));\n@@ -306,6 +318,8 @@ private static FileStatus status(Configuration configuration, Path path) throws\n    * @param pathStatus\n    * @return\n    * @throws IOException\n+   * @deprecated will be removed in 2.0.0;\n+   *             use {@link ParquetFileReader#open(InputFile, ParquetReadOptions)}\n    */\n   @Deprecated\n   public static List<Footer> readFooters(Configuration configuration, FileStatus pathStatus) throws IOException {\n@@ -319,7 +333,10 @@ private static FileStatus status(Configuration configuration, Path path) throws\n    * @param pathStatus the root dir\n    * @return all the footers\n    * @throws IOException\n+   * @deprecated will be removed in 2.0.0;\n+   *             use {@link ParquetFileReader#open(InputFile, ParquetReadOptions)}\n    */\n+  @Deprecated\n   public static List<Footer> readFooters(Configuration configuration, FileStatus pathStatus, boolean skipRowGroups) throws IOException {\n     List<FileStatus> files = listFiles(configuration, pathStatus);\n     return readAllFootersInParallelUsingSummaryFiles(configuration, files, skipRowGroups);\n@@ -345,7 +362,9 @@ private static FileStatus status(Configuration configuration, Path path) throws\n    * @param summaryStatus\n    * @return the metadata translated for each file\n    * @throws IOException\n+   * @deprecated metadata files are not recommended and will be removed in 2.0.0\n    */\n+  @Deprecated\n   public static List<Footer> readSummaryFile(Configuration configuration, FileStatus summaryStatus) throws IOException {\n     final Path parent = summaryStatus.getPath().getParent();\n     ParquetMetadata mergedFooters = readFooter(configuration, summaryStatus, filter(false));\n@@ -394,6 +413,8 @@ static ParquetMetadata readSummaryMetadata(Configuration configuration, Path bas\n    * @param file the parquet File\n    * @return the metadata blocks in the footer\n    * @throws IOException if an error occurs while reading the file\n+   * @deprecated will be removed in 2.0.0;\n+   *             use {@link ParquetFileReader#open(InputFile, ParquetReadOptions)}\n    */\n   @Deprecated\n   public static final ParquetMetadata readFooter(Configuration configuration, Path file) throws IOException {\n@@ -408,13 +429,16 @@ public static final ParquetMetadata readFooter(Configuration configuration, Path\n    * @param filter the filter to apply to row groups\n    * @return the metadata with row groups filtered.\n    * @throws IOException  if an error occurs while reading the file\n+   * @deprecated will be removed in 2.0.0;\n+   *             use {@link ParquetFileReader#open(InputFile, ParquetReadOptions)}\n    */\n   public static ParquetMetadata readFooter(Configuration configuration, Path file, MetadataFilter filter) throws IOException {\n     return readFooter(HadoopInputFile.fromPath(file, configuration), filter);\n   }\n \n   /**\n-   * @deprecated use {@link ParquetFileReader#readFooter(Configuration, FileStatus, MetadataFilter)}\n+   * @deprecated will be removed in 2.0.0;\n+   *             use {@link ParquetFileReader#open(InputFile, ParquetReadOptions)}\n    */\n   @Deprecated\n   public static final ParquetMetadata readFooter(Configuration configuration, FileStatus file) throws IOException {\n@@ -428,7 +452,10 @@ public static final ParquetMetadata readFooter(Configuration configuration, File\n    * @param filter the filter to apply to row groups\n    * @return the metadata blocks in the footer\n    * @throws IOException if an error occurs while reading the file\n+   * @deprecated will be removed in 2.0.0;\n+   *             use {@link ParquetFileReader#open(InputFile, ParquetReadOptions)}\n    */\n+  @Deprecated\n   public static final ParquetMetadata readFooter(Configuration configuration, FileStatus file, MetadataFilter filter) throws IOException {\n     return readFooter(HadoopInputFile.fromStatus(file, configuration), filter);\n   }\n@@ -439,35 +466,32 @@ public static final ParquetMetadata readFooter(Configuration configuration, File\n    * @param filter the filter to apply to row groups\n    * @return the metadata blocks in the footer\n    * @throws IOException if an error occurs while reading the file\n+   * @deprecated will be removed in 2.0.0;\n+   *             use {@link ParquetFileReader#open(InputFile, ParquetReadOptions)}\n    */\n-  public static final ParquetMetadata readFooter(\n-      InputFile file, MetadataFilter filter) throws IOException {\n-    ParquetMetadataConverter converter;\n-    // TODO: remove this temporary work-around.\n-    // this is necessary to pass the Configuration to ParquetMetadataConverter\n-    // and should be removed when there is a non-Hadoop configuration.\n+  @Deprecated\n+  public static final ParquetMetadata readFooter(InputFile file, MetadataFilter filter) throws IOException {\n+    ParquetReadOptions options;\n     if (file instanceof HadoopInputFile) {\n-      converter = new ParquetMetadataConverter(\n-          ((HadoopInputFile) file).getConfiguration());\n+      options = HadoopReadOptions.builder(((HadoopInputFile) file).getConfiguration())\n+          .withMetadataFilter(filter).build();\n     } else {\n-      converter = new ParquetMetadataConverter();\n+      options = ParquetReadOptions.builder().withMetadataFilter(filter).build();\n     }\n-    try (SeekableInputStream in = file.newStream()) {\n \n-      return readFooter(converter, file.getLength(), file.toString(), in, filter);\n+    try (SeekableInputStream in = file.newStream()) {\n+      return readFooter(file, options, in);\n     }\n   }\n \n-  /**\n-   * Reads the meta data block in the footer of the file using provided input stream\n-   * @param fileLen length of the file\n-   * @param filePath file location\n-   * @param f input stream for the file\n-   * @param filter the filter to apply to row groups\n-   * @return the metadata blocks in the footer\n-   * @throws IOException if an error occurs while reading the file\n-   */\n-  private static final ParquetMetadata readFooter(ParquetMetadataConverter converter, long fileLen, String filePath, SeekableInputStream f, MetadataFilter filter) throws IOException {\n+  private static final ParquetMetadata readFooter(InputFile file, ParquetReadOptions options, SeekableInputStream f) throws IOException {\n+    ParquetMetadataConverter converter = new ParquetMetadataConverter(options);\n+    return readFooter(file, options, f, converter);\n+  }\n+\n+  private static final ParquetMetadata readFooter(InputFile file, ParquetReadOptions options, SeekableInputStream f, ParquetMetadataConverter converter) throws IOException {\n+    long fileLen = file.getLength();\n+    String filePath = file.toString();\n     LOG.debug(\"File length {}\", fileLen);\n     int FOOTER_LENGTH_SIZE = 4;\n     if (fileLen < MAGIC.length + FOOTER_LENGTH_SIZE + MAGIC.length) { // MAGIC + data + footer + footerIndex + MAGIC\n@@ -489,43 +513,75 @@ private static final ParquetMetadata readFooter(ParquetMetadataConverter convert\n       throw new RuntimeException(\"corrupted file: the footer index is not within the file: \" + footerIndex);\n     }\n     f.seek(footerIndex);\n-    return converter.readParquetMetadata(f, filter);\n+    return converter.readParquetMetadata(f, options.getMetadataFilter());\n   }\n \n+  /**\n+   * @deprecated will be removed in 2.0.0; use {@link #open(InputFile)}\n+   */\n+  @Deprecated\n   public static ParquetFileReader open(Configuration conf, Path file) throws IOException {\n-    return new ParquetFileReader(conf, file);\n+    return new ParquetFileReader(HadoopInputFile.fromPath(file, conf),\n+        HadoopReadOptions.builder(conf).build());\n   }\n \n+  /**\n+   * @deprecated will be removed in 2.0.0; use {@link #open(InputFile,ParquetReadOptions)}\n+   */\n+  @Deprecated\n   public static ParquetFileReader open(Configuration conf, Path file, MetadataFilter filter) throws IOException {\n-    return new ParquetFileReader(conf, file, filter);\n+    return open(HadoopInputFile.fromPath(file, conf),\n+        HadoopReadOptions.builder(conf).withMetadataFilter(filter).build());\n   }\n \n+  /**\n+   * @deprecated will be removed in 2.0.0\n+   */\n+  @Deprecated\n   public static ParquetFileReader open(Configuration conf, Path file, ParquetMetadata footer) throws IOException {\n     return new ParquetFileReader(conf, file, footer);\n   }\n \n-  private final CodecFactory codecFactory;\n+  /**\n+   * Open a {@link InputFile file}.\n+   *\n+   * @param file an input file\n+   * @return an open ParquetFileReader\n+   */\n+  public static ParquetFileReader open(InputFile file) throws IOException {\n+    return new ParquetFileReader(file, ParquetReadOptions.builder().build());\n+  }\n+\n+  /**\n+   * Open a {@link InputFile file} with {@link ParquetReadOptions options}.\n+   *\n+   * @param file an input file\n+   * @return an open ParquetFileReader\n+   */\n+  public static ParquetFileReader open(InputFile file, ParquetReadOptions options) throws IOException {\n+    return new ParquetFileReader(file, options);\n+  }\n+\n+  private final InputFile file;\n   private final SeekableInputStream f;\n-  private final FileStatus fileStatus;\n-  private final Map<ColumnPath, ColumnDescriptor> paths = new HashMap<ColumnPath, ColumnDescriptor>();\n+  private final ParquetReadOptions options;\n+  private final Map<ColumnPath, ColumnDescriptor> paths = new HashMap<>();\n   private final FileMetaData fileMetaData; // may be null\n-  private final ByteBufferAllocator allocator;\n-  private final Configuration conf;\n+  private final List<BlockMetaData> blocks;\n \n   // not final. in some cases, this may be lazily loaded for backward-compat.\n   private ParquetMetadata footer;\n-  // blocks can be filtered after they are read (or set in the constructor)\n-  private List<BlockMetaData> blocks;\n \n   private int currentBlock = 0;\n   private ColumnChunkPageReadStore currentRowGroup = null;\n   private DictionaryPageReader nextDictionaryReader = null;\n \n   /**\n-   * @deprecated use @link{ParquetFileReader(Configuration configuration, FileMetaData fileMetaData,\n-   * Path filePath, List<BlockMetaData> blocks, List<ColumnDescriptor> columns)} instead\n+   * @deprecated use {@link ParquetFileReader(Configuration,FileMetaData,Path,List,List)} instead.\n    */\n-  public ParquetFileReader(Configuration configuration, Path filePath, List<BlockMetaData> blocks, List<ColumnDescriptor> columns) throws IOException {\n+  @Deprecated\n+  public ParquetFileReader(Configuration configuration, Path filePath, List<BlockMetaData> blocks,\n+                           List<ColumnDescriptor> columns) throws IOException {\n     this(configuration, null, filePath, blocks, columns);\n   }\n \n@@ -541,52 +597,28 @@ public ParquetFileReader(\n       Configuration configuration, FileMetaData fileMetaData,\n       Path filePath, List<BlockMetaData> blocks, List<ColumnDescriptor> columns) throws IOException {\n     this.converter = new ParquetMetadataConverter(configuration);\n-    this.conf = configuration;\n+    this.file = HadoopInputFile.fromPath(filePath, configuration);\n     this.fileMetaData = fileMetaData;\n-    FileSystem fs = filePath.getFileSystem(configuration);\n-    this.f = HadoopStreams.wrap(fs.open(filePath));\n-    this.fileStatus = fs.getFileStatus(filePath);\n-    this.blocks = blocks;\n+    this.f = file.newStream();\n+    this.options = HadoopReadOptions.builder(configuration).build();\n+    this.blocks = filterRowGroups(blocks);\n     for (ColumnDescriptor col : columns) {\n       paths.put(ColumnPath.get(col.getPath()), col);\n     }\n-    // the page size parameter isn't meaningful when only using\n-    // the codec factory to get decompressors\n-    this.codecFactory = new CodecFactory(configuration, 0);\n-    this.allocator = new HeapByteBufferAllocator();\n-  }\n-\n-  /**\n-   * @param configuration the Hadoop Configuration\n-   * @param file Path to a parquet file\n-   * @throws IOException if the file can not be opened\n-   */\n-  private ParquetFileReader(Configuration configuration, Path file) throws IOException {\n-    this(configuration, file, NO_FILTER);\n   }\n \n   /**\n    * @param conf the Hadoop Configuration\n    * @param file Path to a parquet file\n    * @param filter a {@link MetadataFilter} for selecting row groups\n    * @throws IOException if the file can not be opened\n+   * @deprecated will be removed in 2.0.0;\n+   *             use {@link ParquetFileReader(InputFile,MetadataFilter)} instead\n    */\n+  @Deprecated\n   public ParquetFileReader(Configuration conf, Path file, MetadataFilter filter) throws IOException {\n-    this.converter = new ParquetMetadataConverter(conf);\n-    this.conf = conf;\n-    FileSystem fs = file.getFileSystem(conf);\n-    this.fileStatus = fs.getFileStatus(file);\n-    this.f = HadoopStreams.wrap(fs.open(file));\n-    this.footer = readFooter(converter, fileStatus.getLen(), fileStatus.getPath().toString(), f, filter);\n-    this.fileMetaData = footer.getFileMetaData();\n-    this.blocks = footer.getBlocks();\n-    for (ColumnDescriptor col : footer.getFileMetaData().getSchema().getColumns()) {\n-      paths.put(ColumnPath.get(col.getPath()), col);\n-    }\n-    // the page size parameter isn't meaningful when only using\n-    // the codec factory to get decompressors\n-    this.codecFactory = new CodecFactory(conf, 0);\n-    this.allocator = new HeapByteBufferAllocator();\n+    this(HadoopInputFile.fromPath(file, conf),\n+        HadoopReadOptions.builder(conf).withMetadataFilter(filter).build());\n   }\n \n   /**\n@@ -595,29 +627,38 @@ public ParquetFileReader(Configuration conf, Path file, MetadataFilter filter) t\n    * @param footer a {@link ParquetMetadata} footer already read from the file\n    * @throws IOException if the file can not be opened\n    */\n+  @Deprecated\n   public ParquetFileReader(Configuration conf, Path file, ParquetMetadata footer) throws IOException {\n     this.converter = new ParquetMetadataConverter(conf);\n-    this.conf = conf;\n-    FileSystem fs = file.getFileSystem(conf);\n-    this.fileStatus = fs.getFileStatus(file);\n-    this.f = HadoopStreams.wrap(fs.open(file));\n+    this.file = HadoopInputFile.fromPath(file, conf);\n+    this.f = this.file.newStream();\n+    this.options = HadoopReadOptions.builder(conf).build();\n     this.footer = footer;\n     this.fileMetaData = footer.getFileMetaData();\n-    this.blocks = footer.getBlocks();\n+    this.blocks = filterRowGroups(footer.getBlocks());\n+    for (ColumnDescriptor col : footer.getFileMetaData().getSchema().getColumns()) {\n+      paths.put(ColumnPath.get(col.getPath()), col);\n+    }\n+  }\n+\n+  public ParquetFileReader(InputFile file, ParquetReadOptions options) throws IOException {\n+    this.converter = new ParquetMetadataConverter(options);\n+    this.file = file;\n+    this.f = file.newStream();\n+    this.options = options;\n+    this.footer = readFooter(file, options, f, converter);\n+    this.fileMetaData = footer.getFileMetaData();\n+    this.blocks = filterRowGroups(footer.getBlocks());\n     for (ColumnDescriptor col : footer.getFileMetaData().getSchema().getColumns()) {\n       paths.put(ColumnPath.get(col.getPath()), col);\n     }\n-    // the page size parameter isn't meaningful when only using\n-    // the codec factory to get decompressors\n-    this.codecFactory = new CodecFactory(conf, 0);\n-    this.allocator = new HeapByteBufferAllocator();\n   }\n \n   public ParquetMetadata getFooter() {\n     if (footer == null) {\n       try {\n         // don't read the row groups because this.blocks is always set\n-        this.footer = readFooter(converter, fileStatus.getLen(), fileStatus.getPath().toString(), f, SKIP_ROW_GROUPS);\n+        this.footer = readFooter(file, options, f, converter);\n       } catch (IOException e) {\n         throw new ParquetDecodingException(\"Unable to read file footer\", e);\n       }\n@@ -640,25 +681,36 @@ public long getRecordCount() {\n     return total;\n   }\n \n+  /**\n+   * @deprecated will be removed in 2.0.0; use {@link #getFile()} instead\n+   */\n+  @Deprecated\n   public Path getPath() {\n-    return fileStatus.getPath();\n+    return new Path(file.toString());\n+  }\n+\n+  public String getFile() {\n+    return file.toString();\n   }\n \n-  void filterRowGroups(FilterCompat.Filter filter) throws IOException {\n+  private List<BlockMetaData> filterRowGroups(List<BlockMetaData> blocks) throws IOException {\n     // set up data filters based on configured levels\n-    List<RowGroupFilter.FilterLevel> levels = new ArrayList<RowGroupFilter.FilterLevel>();\n+    List<RowGroupFilter.FilterLevel> levels = new ArrayList<>();\n \n-    if (conf.getBoolean(\n-        STATS_FILTERING_ENABLED, STATS_FILTERING_ENABLED_DEFAULT)) {\n+    if (options.useStatsFilter()) {\n       levels.add(STATISTICS);\n     }\n \n-    if (conf.getBoolean(\n-        DICTIONARY_FILTERING_ENABLED, DICTIONARY_FILTERING_ENABLED_DEFAULT)) {\n+    if (options.useDictionaryFilter()) {\n       levels.add(DICTIONARY);\n     }\n \n-    this.blocks = RowGroupFilter.filterRowGroups(levels, filter, blocks, this);\n+    FilterCompat.Filter recordFilter = options.getRecordFilter();\n+    if (recordFilter != null) {\n+      return RowGroupFilter.filterRowGroups(levels, recordFilter, blocks, this);\n+    }\n+\n+    return blocks;\n   }\n \n   public List<BlockMetaData> getRowGroups() {\n@@ -785,7 +837,7 @@ DictionaryPage readDictionary(ColumnChunkMetaData meta) throws IOException {\n     }\n \n     DictionaryPage compressedPage = readCompressedDictionary(pageHeader, f);\n-    BytesDecompressor decompressor = codecFactory.getDecompressor(meta.getCodec());\n+    BytesInputDecompressor decompressor = options.getCodecFactory().getDecompressor(meta.getCodec());\n \n     return new DictionaryPage(\n         decompressor.decompress(compressedPage.getBytes(), compressedPage.getUncompressedSize()),\n@@ -817,9 +869,7 @@ public void close() throws IOException {\n         f.close();\n       }\n     } finally {\n-      if (codecFactory != null) {\n-        codecFactory.release();\n-      }\n+      options.getCodecFactory().release();\n     }\n   }\n \n@@ -929,7 +979,7 @@ public ColumnChunkPageReader readAllPages() throws IOException {\n             \" but got \" + valuesCountReadSoFar + \" values instead over \" + pagesInChunk.size()\n             + \" pages ending at file offset \" + (descriptor.fileOffset + pos()));\n       }\n-      BytesDecompressor decompressor = codecFactory.getDecompressor(descriptor.metadata.getCodec());\n+      BytesInputDecompressor decompressor = options.getCodecFactory().getDecompressor(descriptor.metadata.getCodec());\n       return new ColumnChunkPageReader(decompressor, pagesInChunk, dictionaryPage);\n     }\n \n@@ -1077,7 +1127,7 @@ public void addChunk(ChunkDescriptor descriptor) {\n       f.seek(offset);\n \n       // Allocate the bytebuffer based on whether the FS can support it.\n-      ByteBuffer chunksByteBuffer = allocator.allocate(length);\n+      ByteBuffer chunksByteBuffer = options.getAllocator().allocate(length);\n       f.readFully(chunksByteBuffer);\n \n       // report in a counter the data we just scanned",
                "additions": 152,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java",
                "status": "modified",
                "changes": 254,
                "deletions": 102,
                "sha": "1ace040bf968bc7eb12c17fb59f0bfc9528fc056",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileReader.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -23,7 +23,6 @@\n import static org.apache.parquet.hadoop.ParquetWriter.MAX_PADDING_SIZE_DEFAULT;\n \n import java.io.IOException;\n-import java.io.OutputStream;\n import java.nio.charset.Charset;\n import java.util.ArrayList;\n import java.util.HashMap;\n@@ -36,7 +35,6 @@\n \n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FSDataInputStream;\n-import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n \n@@ -59,9 +57,13 @@\n import org.apache.parquet.hadoop.metadata.FileMetaData;\n import org.apache.parquet.hadoop.metadata.GlobalMetaData;\n import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopOutputFile;\n import org.apache.parquet.hadoop.util.HadoopStreams;\n+import org.apache.parquet.io.InputFile;\n+import org.apache.parquet.io.OutputFile;\n import org.apache.parquet.io.SeekableInputStream;\n import org.apache.parquet.io.ParquetEncodingException;\n+import org.apache.parquet.io.PositionOutputStream;\n import org.apache.parquet.schema.MessageType;\n import org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName;\n import org.apache.parquet.schema.TypeUtil;\n@@ -85,30 +87,14 @@\n   public static final String PARQUET_COMMON_METADATA_FILE = \"_common_metadata\";\n   public static final int CURRENT_VERSION = 1;\n \n-  // need to supply a buffer size when setting block size. this is the default\n-  // for hadoop 1 to present. copying it avoids loading DFSConfigKeys.\n-  private static final int DFS_BUFFER_SIZE_DEFAULT = 4096;\n-\n-  // visible for testing\n-  static final Set<String> BLOCK_FS_SCHEMES = new HashSet<String>();\n-  static {\n-    BLOCK_FS_SCHEMES.add(\"hdfs\");\n-    BLOCK_FS_SCHEMES.add(\"webhdfs\");\n-    BLOCK_FS_SCHEMES.add(\"viewfs\");\n-  }\n-\n-  private static boolean supportsBlockSize(FileSystem fs) {\n-    return BLOCK_FS_SCHEMES.contains(fs.getUri().getScheme());\n-  }\n-\n   // File creation modes\n   public static enum Mode {\n     CREATE,\n     OVERWRITE\n   }\n \n   private final MessageType schema;\n-  private final FSDataOutputStream out;\n+  private final PositionOutputStream out;\n   private final AlignmentStrategy alignment;\n \n   // file data\n@@ -193,11 +179,14 @@ private final STATE error() throws IOException {\n    * @param schema the schema of the data\n    * @param file the file to write to\n    * @throws IOException if the file can not be created\n+   * @deprecated will be removed in 2.0.0;\n+   *             use {@link ParquetFileWriter(OutputFile,MessageType,Mode,long,long)} instead\n    */\n+  @Deprecated\n   public ParquetFileWriter(Configuration configuration, MessageType schema,\n       Path file) throws IOException {\n-    this(configuration, schema, file, Mode.CREATE, DEFAULT_BLOCK_SIZE,\n-        MAX_PADDING_SIZE_DEFAULT);\n+    this(HadoopOutputFile.fromPath(file, configuration),\n+        schema, Mode.CREATE, DEFAULT_BLOCK_SIZE, MAX_PADDING_SIZE_DEFAULT);\n   }\n \n   /**\n@@ -206,11 +195,14 @@ public ParquetFileWriter(Configuration configuration, MessageType schema,\n    * @param file the file to write to\n    * @param mode file creation mode\n    * @throws IOException if the file can not be created\n+   * @deprecated will be removed in 2.0.0;\n+   *             use {@link ParquetFileWriter(OutputFile,MessageType,Mode,long,long)} instead\n    */\n+  @Deprecated\n   public ParquetFileWriter(Configuration configuration, MessageType schema,\n                            Path file, Mode mode) throws IOException {\n-    this(configuration, schema, file, mode, DEFAULT_BLOCK_SIZE,\n-        MAX_PADDING_SIZE_DEFAULT);\n+    this(HadoopOutputFile.fromPath(file, configuration),\n+        schema, mode, DEFAULT_BLOCK_SIZE, MAX_PADDING_SIZE_DEFAULT);\n   }\n \n   /**\n@@ -219,36 +211,54 @@ public ParquetFileWriter(Configuration configuration, MessageType schema,\n    * @param file the file to write to\n    * @param mode file creation mode\n    * @param rowGroupSize the row group size\n+   * @param maxPaddingSize the maximum padding\n    * @throws IOException if the file can not be created\n+   * @deprecated will be removed in 2.0.0;\n+   *             use {@link ParquetFileWriter(OutputFile,MessageType,Mode,long,long)} instead\n    */\n+  @Deprecated\n   public ParquetFileWriter(Configuration configuration, MessageType schema,\n                            Path file, Mode mode, long rowGroupSize,\n                            int maxPaddingSize)\n       throws IOException {\n-    TypeUtil.checkValidWriteSchema(schema);\n-    this.schema = schema;\n-    FileSystem fs = file.getFileSystem(configuration);\n-    boolean overwriteFlag = (mode == Mode.OVERWRITE);\n+    this(HadoopOutputFile.fromPath(file, configuration),\n+        schema, mode, rowGroupSize, maxPaddingSize);\n+  }\n \n-    if (supportsBlockSize(fs)) {\n-      // use the default block size, unless row group size is larger\n-      long dfsBlockSize = Math.max(fs.getDefaultBlockSize(file), rowGroupSize);\n+  /**\n+   * @param file OutputFile to create or overwrite\n+   * @param schema the schema of the data\n+   * @param mode file creation mode\n+   * @param rowGroupSize the row group size\n+   * @param maxPaddingSize the maximum padding\n+   * @throws IOException if the file can not be created\n+   */\n+  public ParquetFileWriter(OutputFile file, MessageType schema, Mode mode,\n+                           long rowGroupSize, int maxPaddingSize)\n+      throws IOException {\n+    TypeUtil.checkValidWriteSchema(schema);\n \n-      this.alignment = PaddingAlignment.get(\n-          dfsBlockSize, rowGroupSize, maxPaddingSize);\n-      this.out = fs.create(file, overwriteFlag, DFS_BUFFER_SIZE_DEFAULT,\n-          fs.getDefaultReplication(file), dfsBlockSize);\n+    this.schema = schema;\n \n+    long blockSize = rowGroupSize;\n+    if (file.supportsBlockSize()) {\n+      blockSize = Math.max(file.defaultBlockSize(), rowGroupSize);\n+      this.alignment = PaddingAlignment.get(blockSize, rowGroupSize, maxPaddingSize);\n     } else {\n       this.alignment = NoAlignment.get(rowGroupSize);\n-      this.out = fs.create(file, overwriteFlag);\n+    }\n+\n+    if (mode == Mode.OVERWRITE) {\n+      this.out = file.createOrOverwrite(blockSize);\n+    } else {\n+      this.out = file.create(blockSize);\n     }\n \n     this.encodingStatsBuilder = new EncodingStats.Builder();\n   }\n \n   /**\n-   * FOR TESTING ONLY.\n+   * FOR TESTING ONLY. This supports testing block padding behavior on the local FS.\n    *\n    * @param configuration Hadoop configuration\n    * @param schema the schema of the data\n@@ -263,11 +273,10 @@ public ParquetFileWriter(Configuration configuration, MessageType schema,\n     this.schema = schema;\n     this.alignment = PaddingAlignment.get(\n         rowAndBlockSize, rowAndBlockSize, maxPaddingSize);\n-    this.out = fs.create(file, true, DFS_BUFFER_SIZE_DEFAULT,\n-        fs.getDefaultReplication(file), rowAndBlockSize);\n+    this.out = HadoopStreams.wrap(\n+        fs.create(file, true, 8192, fs.getDefaultReplication(file), rowAndBlockSize));\n     this.encodingStatsBuilder = new EncodingStats.Builder();\n   }\n-\n   /**\n    * start the file\n    * @throws IOException\n@@ -490,10 +499,23 @@ public void endBlock() throws IOException {\n     currentBlock = null;\n   }\n \n+  /**\n+   * @deprecated will be removed in 2.0.0; use {@link #appendFile(InputFile)} instead\n+   */\n+  @Deprecated\n   public void appendFile(Configuration conf, Path file) throws IOException {\n     ParquetFileReader.open(conf, file).appendTo(this);\n   }\n \n+  public void appendFile(InputFile file) throws IOException {\n+    ParquetFileReader.open(file).appendTo(this);\n+  }\n+\n+  /**\n+   * @deprecated will be removed in 2.0.0;\n+   *             use {@link #appendRowGroups(SeekableInputStream,List,boolean)} instead\n+   */\n+  @Deprecated\n   public void appendRowGroups(FSDataInputStream file,\n                               List<BlockMetaData> rowGroups,\n                               boolean dropColumns) throws IOException {\n@@ -508,13 +530,18 @@ public void appendRowGroups(SeekableInputStream file,\n     }\n   }\n \n+  /**\n+   * @deprecated will be removed in 2.0.0;\n+   *             use {@link #appendRowGroup(SeekableInputStream,BlockMetaData,boolean)} instead\n+   */\n+  @Deprecated\n   public void appendRowGroup(FSDataInputStream from, BlockMetaData rowGroup,\n                              boolean dropColumns) throws IOException {\n-    appendRowGroup(from, rowGroup, dropColumns);\n+    appendRowGroup(HadoopStreams.wrap(from), rowGroup, dropColumns);\n   }\n \n   public void appendRowGroup(SeekableInputStream from, BlockMetaData rowGroup,\n-    boolean dropColumns) throws IOException {\n+                             boolean dropColumns) throws IOException {\n     startBlock(rowGroup.getRowCount());\n \n     Map<String, ColumnChunkMetaData> columnsToCopy =\n@@ -603,13 +630,13 @@ public void appendRowGroup(SeekableInputStream from, BlockMetaData rowGroup,\n   /**\n    * Copy from a FS input stream to an output stream. Thread-safe\n    *\n-   * @param from a {@link FSDataInputStream}\n-   * @param to any {@link OutputStream}\n+   * @param from a {@link SeekableInputStream}\n+   * @param to any {@link PositionOutputStream}\n    * @param start where in the from stream to start copying\n    * @param length the number of bytes to copy\n    * @throws IOException\n    */\n-  private static void copy(SeekableInputStream from, FSDataOutputStream to,\n+  private static void copy(SeekableInputStream from, PositionOutputStream to,\n                            long start, long length) throws IOException{\n     LOG.debug(\"Copying {} bytes at {} to {}\" ,length , start , to.getPos());\n     from.seek(start);\n@@ -642,7 +669,7 @@ public void end(Map<String, String> extraMetaData) throws IOException {\n     out.close();\n   }\n \n-  private static void serializeFooter(ParquetMetadata footer, FSDataOutputStream out) throws IOException {\n+  private static void serializeFooter(ParquetMetadata footer, PositionOutputStream out) throws IOException {\n     long footerIndex = out.getPos();\n     org.apache.parquet.format.FileMetaData parquetMetadata = metadataConverter.toParquetMetadata(CURRENT_VERSION, footer);\n     writeFileMetaData(parquetMetadata, out);\n@@ -654,7 +681,9 @@ private static void serializeFooter(ParquetMetadata footer, FSDataOutputStream o\n   /**\n    * Given a list of metadata files, merge them into a single ParquetMetadata\n    * Requires that the schemas be compatible, and the extraMetadata be exactly equal.\n+   * @deprecated metadata files are not recommended and will be removed in 2.0.0\n    */\n+  @Deprecated\n   public static ParquetMetadata mergeMetadataFiles(List<Path> files,  Configuration conf) throws IOException {\n     Preconditions.checkArgument(!files.isEmpty(), \"Cannot merge an empty list of metadata\");\n \n@@ -677,7 +706,9 @@ public static ParquetMetadata mergeMetadataFiles(List<Path> files,  Configuratio\n    * Requires that the schemas be compatible, and the extraMetaData be exactly equal.\n    * This is useful when merging 2 directories of parquet files into a single directory, as long\n    * as both directories were written with compatible schemas and equal extraMetaData.\n+   * @deprecated metadata files are not recommended and will be removed in 2.0.0\n    */\n+  @Deprecated\n   public static void writeMergedMetadataFile(List<Path> files, Path outputPath, Configuration conf) throws IOException {\n     ParquetMetadata merged = mergeMetadataFiles(files, conf);\n     writeMetadataFile(outputPath, merged, outputPath.getFileSystem(conf));\n@@ -688,8 +719,8 @@ public static void writeMergedMetadataFile(List<Path> files, Path outputPath, Co\n    * @param configuration the configuration to use to get the FileSystem\n    * @param outputPath the directory to write the _metadata file to\n    * @param footers the list of footers to merge\n-   * @deprecated use the variant of writeMetadataFile that takes a {@link JobSummaryLevel} as an argument.\n    * @throws IOException\n+   * @deprecated metadata files are not recommended and will be removed in 2.0.0\n    */\n   @Deprecated\n   public static void writeMetadataFile(Configuration configuration, Path outputPath, List<Footer> footers) throws IOException {\n@@ -698,7 +729,9 @@ public static void writeMetadataFile(Configuration configuration, Path outputPat\n \n   /**\n    * writes _common_metadata file, and optionally a _metadata file depending on the {@link JobSummaryLevel} provided\n+   * @deprecated metadata files are not recommended and will be removed in 2.0.0\n    */\n+  @Deprecated\n   public static void writeMetadataFile(Configuration configuration, Path outputPath, List<Footer> footers, JobSummaryLevel level) throws IOException {\n     Preconditions.checkArgument(level == JobSummaryLevel.ALL || level == JobSummaryLevel.COMMON_ONLY,\n         \"Unsupported level: \" + level);\n@@ -715,15 +748,23 @@ public static void writeMetadataFile(Configuration configuration, Path outputPat\n     writeMetadataFile(outputPath, metadataFooter, fs, PARQUET_COMMON_METADATA_FILE);\n   }\n \n+  /**\n+   * @deprecated metadata files are not recommended and will be removed in 2.0.0\n+   */\n+  @Deprecated\n   private static void writeMetadataFile(Path outputPathRoot, ParquetMetadata metadataFooter, FileSystem fs, String parquetMetadataFile)\n       throws IOException {\n     Path metaDataPath = new Path(outputPathRoot, parquetMetadataFile);\n     writeMetadataFile(metaDataPath, metadataFooter, fs);\n   }\n \n+  /**\n+   * @deprecated metadata files are not recommended and will be removed in 2.0.0\n+   */\n+  @Deprecated\n   private static void writeMetadataFile(Path outputPath, ParquetMetadata metadataFooter, FileSystem fs)\n       throws IOException {\n-    FSDataOutputStream metadata = fs.create(outputPath);\n+    PositionOutputStream metadata = HadoopStreams.wrap(fs.create(outputPath));\n     metadata.write(MAGIC);\n     serializeFooter(metadataFooter, metadata);\n     metadata.close();\n@@ -850,9 +891,9 @@ static MessageType mergeInto(MessageType toMerge, MessageType mergedSchema, bool\n   }\n \n   private interface AlignmentStrategy {\n-    void alignForRowGroup(FSDataOutputStream out) throws IOException;\n+    void alignForRowGroup(PositionOutputStream out) throws IOException;\n \n-    long nextRowGroupSize(FSDataOutputStream out) throws IOException;\n+    long nextRowGroupSize(PositionOutputStream out) throws IOException;\n   }\n \n   private static class NoAlignment implements AlignmentStrategy {\n@@ -867,11 +908,11 @@ private NoAlignment(long rowGroupSize) {\n     }\n \n     @Override\n-    public void alignForRowGroup(FSDataOutputStream out) {\n+    public void alignForRowGroup(PositionOutputStream out) {\n     }\n \n     @Override\n-    public long nextRowGroupSize(FSDataOutputStream out) {\n+    public long nextRowGroupSize(PositionOutputStream out) {\n       return rowGroupSize;\n     }\n   }\n@@ -900,7 +941,7 @@ private PaddingAlignment(long dfsBlockSize, long rowGroupSize,\n     }\n \n     @Override\n-    public void alignForRowGroup(FSDataOutputStream out) throws IOException {\n+    public void alignForRowGroup(PositionOutputStream out) throws IOException {\n       long remaining = dfsBlockSize - (out.getPos() % dfsBlockSize);\n \n       if (isPaddingNeeded(remaining)) {\n@@ -912,7 +953,7 @@ public void alignForRowGroup(FSDataOutputStream out) throws IOException {\n     }\n \n     @Override\n-    public long nextRowGroupSize(FSDataOutputStream out) throws IOException {\n+    public long nextRowGroupSize(PositionOutputStream out) throws IOException {\n       if (maxPaddingSize <= 0) {\n         return rowGroupSize;\n       }",
                "additions": 94,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java",
                "status": "modified",
                "changes": 147,
                "deletions": 53,
                "sha": "da8635d099544294ed9ba175274b8e2c3ce661d3",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetFileWriter.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -120,19 +120,16 @@\n    * key to configure whether record-level filtering is enabled\n    */\n   public static final String RECORD_FILTERING_ENABLED = \"parquet.filter.record-level.enabled\";\n-  static final boolean RECORD_FILTERING_ENABLED_DEFAULT = true;\n \n   /**\n    * key to configure whether row group stats filtering is enabled\n    */\n   public static final String STATS_FILTERING_ENABLED = \"parquet.filter.stats.enabled\";\n-  static final boolean STATS_FILTERING_ENABLED_DEFAULT = true;\n \n   /**\n    * key to configure whether row group dictionary filtering is enabled\n    */\n   public static final String DICTIONARY_FILTERING_ENABLED = \"parquet.filter.dictionary.enabled\";\n-  static final boolean DICTIONARY_FILTERING_ENABLED_DEFAULT = false;\n \n   /**\n    * key to turn on or off task side metadata loading (default true)",
                "additions": 0,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java",
                "status": "modified",
                "changes": 3,
                "deletions": 3,
                "sha": "979388d397729219bf828af3a54596254998c5d1",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -42,6 +42,7 @@\n import org.apache.parquet.hadoop.codec.CodecConfig;\n import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n import org.apache.parquet.hadoop.util.ConfigurationUtil;\n+import org.apache.parquet.hadoop.util.HadoopOutputFile;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -383,8 +384,8 @@ private static int getMaxPaddingSize(Configuration conf) {\n     }\n \n     WriteContext init = writeSupport.init(conf);\n-    ParquetFileWriter w = new ParquetFileWriter(\n-        conf, init.getSchema(), file, Mode.CREATE, blockSize, maxPaddingSize);\n+    ParquetFileWriter w = new ParquetFileWriter(HadoopOutputFile.fromPath(file, conf),\n+        init.getSchema(), Mode.CREATE, blockSize, maxPaddingSize);\n     w.start();\n \n     float maxLoad = conf.getFloat(ParquetOutputFormat.MEMORY_POOL_RATIO,",
                "additions": 3,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java",
                "status": "modified",
                "changes": 5,
                "deletions": 2,
                "sha": "340ec11cbcc88f90fb4e490249d89db7a6400a4f",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputFormat.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -22,7 +22,8 @@\n \n import java.io.Closeable;\n import java.io.IOException;\n-import java.util.Arrays;\n+import java.util.ArrayList;\n+import java.util.Collections;\n import java.util.Iterator;\n import java.util.List;\n \n@@ -31,12 +32,17 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n \n+import org.apache.parquet.ParquetReadOptions;\n import org.apache.parquet.Preconditions;\n+import org.apache.parquet.compression.CompressionCodecFactory;\n import org.apache.parquet.filter.UnboundRecordFilter;\n import org.apache.parquet.filter2.compat.FilterCompat;\n import org.apache.parquet.filter2.compat.FilterCompat.Filter;\n import org.apache.parquet.hadoop.api.ReadSupport;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.HadoopReadOptions;\n import org.apache.parquet.hadoop.util.HiddenFileFilter;\n+import org.apache.parquet.io.InputFile;\n \n /**\n  * Read records from a Parquet file.\n@@ -45,9 +51,8 @@\n public class ParquetReader<T> implements Closeable {\n \n   private final ReadSupport<T> readSupport;\n-  private final Configuration conf;\n-  private final Iterator<Footer> footersIterator;\n-  private final Filter filter;\n+  private final Iterator<InputFile> filesIterator;\n+  private final ParquetReadOptions options;\n \n   private InternalParquetRecordReader<T> reader;\n \n@@ -100,17 +105,22 @@ public ParquetReader(Configuration conf, Path file, ReadSupport<T> readSupport,\n   }\n \n   private ParquetReader(Configuration conf,\n-                       Path file,\n-                       ReadSupport<T> readSupport,\n-                       Filter filter) throws IOException {\n-    this.readSupport = readSupport;\n-    this.filter = checkNotNull(filter, \"filter\");\n-    this.conf = conf;\n+                        Path file,\n+                        ReadSupport<T> readSupport,\n+                        FilterCompat.Filter filter) throws IOException {\n+    this(Collections.singletonList((InputFile) HadoopInputFile.fromPath(file, conf)),\n+        HadoopReadOptions.builder(conf)\n+            .withRecordFilter(checkNotNull(filter, \"filter\"))\n+            .build(),\n+        readSupport);\n+  }\n \n-    FileSystem fs = file.getFileSystem(conf);\n-    List<FileStatus> statuses = Arrays.asList(fs.listStatus(file, HiddenFileFilter.INSTANCE));\n-    List<Footer> footers = ParquetFileReader.readAllFootersInParallelUsingSummaryFiles(conf, statuses, false);\n-    this.footersIterator = footers.iterator();\n+  private ParquetReader(List<InputFile> files,\n+                        ParquetReadOptions options,\n+                        ReadSupport<T> readSupport) throws IOException {\n+    this.readSupport = readSupport;\n+    this.options = options;\n+    this.filesIterator = files.iterator();\n   }\n \n   /**\n@@ -135,18 +145,15 @@ private void initReader() throws IOException {\n       reader.close();\n       reader = null;\n     }\n-    if (footersIterator.hasNext()) {\n-      Footer footer = footersIterator.next();\n \n-      ParquetFileReader fileReader = ParquetFileReader.open(\n-          conf, footer.getFile(), footer.getParquetMetadata());\n+    if (filesIterator.hasNext()) {\n+      InputFile file = filesIterator.next();\n \n-      // apply data filters\n-      fileReader.filterRowGroups(filter);\n+      ParquetFileReader fileReader = ParquetFileReader.open(file, options);\n \n-      reader = new InternalParquetRecordReader<T>(readSupport, filter);\n+      reader = new InternalParquetRecordReader<>(readSupport, options.getRecordFilter());\n \n-      reader.initialize(fileReader, conf);\n+      reader.initialize(fileReader, options);\n     }\n   }\n \n@@ -157,37 +164,114 @@ public void close() throws IOException {\n     }\n   }\n \n+  public static <T> Builder<T> read(InputFile file) throws IOException {\n+    return new Builder<>(file);\n+  }\n+\n   public static <T> Builder<T> builder(ReadSupport<T> readSupport, Path path) {\n-    return new Builder<T>(readSupport, path);\n+    return new Builder<>(readSupport, path);\n   }\n \n   public static class Builder<T> {\n     private final ReadSupport<T> readSupport;\n-    private final Path file;\n-    private Filter filter;\n-    protected Configuration conf;\n+    private final InputFile file;\n+    private final Path path;\n+    private Filter filter = null;\n+    protected Configuration conf = new Configuration();\n+    private ParquetReadOptions.Builder optionsBuilder = HadoopReadOptions.builder(conf);\n \n+    @Deprecated\n     private Builder(ReadSupport<T> readSupport, Path path) {\n       this.readSupport = checkNotNull(readSupport, \"readSupport\");\n-      this.file = checkNotNull(path, \"path\");\n-      this.conf = new Configuration();\n-      this.filter = FilterCompat.NOOP;\n+      this.file = null;\n+      this.path = checkNotNull(path, \"path\");\n     }\n \n+    @Deprecated\n     protected Builder(Path path) {\n       this.readSupport = null;\n-      this.file = checkNotNull(path, \"path\");\n-      this.conf = new Configuration();\n-      this.filter = FilterCompat.NOOP;\n+      this.file = null;\n+      this.path = checkNotNull(path, \"path\");\n     }\n \n+    protected Builder(InputFile file) {\n+      this.readSupport = null;\n+      this.file = checkNotNull(file, \"file\");\n+      this.path = null;\n+    }\n+\n+    // when called, resets options to the defaults from conf\n     public Builder<T> withConf(Configuration conf) {\n       this.conf = checkNotNull(conf, \"conf\");\n+\n+      // previous versions didn't use the builder, so may set filter before conf. this maintains\n+      // compatibility for filter. other options are reset by a new conf.\n+      this.optionsBuilder = HadoopReadOptions.builder(conf);\n+      if (filter != null) {\n+        optionsBuilder.withRecordFilter(filter);\n+      }\n+\n       return this;\n     }\n \n     public Builder<T> withFilter(Filter filter) {\n-      this.filter = checkNotNull(filter, \"filter\");\n+      this.filter = filter;\n+      optionsBuilder.withRecordFilter(filter);\n+      return this;\n+    }\n+\n+    public Builder<T> useSignedStringMinMax(boolean useSignedStringMinMax) {\n+      optionsBuilder.useSignedStringMinMax(useSignedStringMinMax);\n+      return this;\n+    }\n+\n+    public Builder<T> useSignedStringMinMax() {\n+      optionsBuilder.useSignedStringMinMax();\n+      return this;\n+    }\n+\n+    public Builder<T> useStatsFilter(boolean useStatsFilter) {\n+      optionsBuilder.useStatsFilter(useStatsFilter);\n+      return this;\n+    }\n+\n+    public Builder<T> useStatsFilter() {\n+      optionsBuilder.useStatsFilter();\n+      return this;\n+    }\n+\n+    public Builder<T> useDictionaryFilter(boolean useDictionaryFilter) {\n+      optionsBuilder.useDictionaryFilter(useDictionaryFilter);\n+      return this;\n+    }\n+\n+    public Builder<T> useDictionaryFilter() {\n+      optionsBuilder.useDictionaryFilter();\n+      return this;\n+    }\n+\n+    public Builder<T> useRecordFilter(boolean useRecordFilter) {\n+      optionsBuilder.useRecordFilter(useRecordFilter);\n+      return this;\n+    }\n+\n+    public Builder<T> useRecordFilter() {\n+      optionsBuilder.useRecordFilter();\n+      return this;\n+    }\n+\n+    public Builder<T> withFileRange(long start, long end) {\n+      optionsBuilder.withRange(start, end);\n+      return this;\n+    }\n+\n+    public Builder<T> withCodecFactory(CompressionCodecFactory codecFactory) {\n+      optionsBuilder.withCodecFactory(codecFactory);\n+      return this;\n+    }\n+\n+    public Builder<T> set(String key, String value) {\n+      optionsBuilder.set(key, value);\n       return this;\n     }\n \n@@ -199,7 +283,29 @@ protected Builder(Path path) {\n     }\n \n     public ParquetReader<T> build() throws IOException {\n-      return new ParquetReader<T>(conf, file, getReadSupport(), filter);\n+      ParquetReadOptions options = optionsBuilder.build();\n+\n+      if (path != null) {\n+        FileSystem fs = path.getFileSystem(conf);\n+        FileStatus stat = fs.getFileStatus(path);\n+\n+        if (stat.isFile()) {\n+          return new ParquetReader<>(\n+              Collections.singletonList((InputFile) HadoopInputFile.fromStatus(stat, conf)),\n+              options,\n+              getReadSupport());\n+\n+        } else {\n+          List<InputFile> files = new ArrayList<>();\n+          for (FileStatus fileStatus : fs.listStatus(path, HiddenFileFilter.INSTANCE)) {\n+            files.add(HadoopInputFile.fromStatus(fileStatus, conf));\n+          }\n+          return new ParquetReader<T>(files, options, getReadSupport());\n+        }\n+\n+      } else {\n+        return new ParquetReader<>(Collections.singletonList(file), options, getReadSupport());\n+      }\n     }\n   }\n }",
                "additions": 140,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetReader.java",
                "status": "modified",
                "changes": 174,
                "deletions": 34,
                "sha": "1ba5380c85b8dc46accacf66fe84150e073de841",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetReader.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetReader.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetReader.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -18,14 +18,9 @@\n  */\n package org.apache.parquet.hadoop;\n \n-import static org.apache.parquet.filter2.compat.RowGroupFilter.FilterLevel.*;\n-import static org.apache.parquet.format.converter.ParquetMetadataConverter.offsets;\n-import static org.apache.parquet.format.converter.ParquetMetadataConverter.range;\n import static org.apache.parquet.hadoop.ParquetInputFormat.SPLIT_FILES;\n-import static org.apache.parquet.hadoop.ParquetInputFormat.getFilter;\n \n import java.io.IOException;\n-import java.util.ArrayList;\n import java.util.Arrays;\n import java.util.HashSet;\n import java.util.List;\n@@ -37,21 +32,21 @@\n import org.apache.hadoop.mapreduce.InputSplit;\n import org.apache.hadoop.mapreduce.RecordReader;\n import org.apache.hadoop.mapreduce.TaskAttemptContext;\n-import org.apache.hadoop.mapreduce.TaskInputOutputContext;\n \n import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n import org.apache.parquet.CorruptDeltaByteArrays;\n+import org.apache.parquet.ParquetReadOptions;\n import org.apache.parquet.column.Encoding;\n import org.apache.parquet.filter.UnboundRecordFilter;\n import org.apache.parquet.filter2.compat.FilterCompat;\n import org.apache.parquet.filter2.compat.FilterCompat.Filter;\n-import org.apache.parquet.filter2.compat.RowGroupFilter.FilterLevel;\n-import org.apache.parquet.format.converter.ParquetMetadataConverter.MetadataFilter;\n import org.apache.parquet.hadoop.api.ReadSupport;\n import org.apache.parquet.hadoop.metadata.BlockMetaData;\n import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n import org.apache.parquet.hadoop.metadata.FileMetaData;\n import org.apache.parquet.hadoop.util.ContextUtil;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n+import org.apache.parquet.HadoopReadOptions;\n import org.apache.parquet.hadoop.util.counters.BenchmarkCounter;\n import org.apache.parquet.io.ParquetDecodingException;\n import org.slf4j.Logger;\n@@ -158,13 +153,16 @@ private void initializeInternalReader(ParquetInputSplit split, Configuration con\n     long[] rowGroupOffsets = split.getRowGroupOffsets();\n \n     // if task.side.metadata is set, rowGroupOffsets is null\n-    MetadataFilter metadataFilter = (rowGroupOffsets != null ?\n-        offsets(rowGroupOffsets) :\n-        range(split.getStart(), split.getEnd()));\n+    ParquetReadOptions.Builder optionsBuilder = HadoopReadOptions.builder(configuration);\n+    if (rowGroupOffsets != null) {\n+      optionsBuilder.withOffsets(rowGroupOffsets);\n+    } else {\n+      optionsBuilder.withRange(split.getStart(), split.getEnd());\n+    }\n \n     // open a reader with the metadata filter\n     ParquetFileReader reader = ParquetFileReader.open(\n-        configuration, path, metadataFilter);\n+        HadoopInputFile.fromPath(path, configuration), optionsBuilder.build());\n \n     if (rowGroupOffsets != null) {\n       // verify a row group was found for each offset\n@@ -175,10 +173,6 @@ private void initializeInternalReader(ParquetInputSplit split, Configuration con\n             + \" expected: \" + Arrays.toString(rowGroupOffsets)\n             + \" found: \" + blocks);\n       }\n-\n-    } else {\n-      // apply data filters\n-      reader.filterRowGroups(getFilter(configuration));\n     }\n \n     if (!reader.getRowGroups().isEmpty()) {",
                "additions": 10,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetRecordReader.java",
                "status": "modified",
                "changes": 26,
                "deletions": 16,
                "sha": "9ca8be925ecd6b161fc37228e3141b3aea97481b",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetRecordReader.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetRecordReader.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetRecordReader.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -28,6 +28,8 @@\n import org.apache.parquet.column.ParquetProperties.WriterVersion;\n import org.apache.parquet.hadoop.api.WriteSupport;\n import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n+import org.apache.parquet.hadoop.util.HadoopOutputFile;\n+import org.apache.parquet.io.OutputFile;\n import org.apache.parquet.schema.MessageType;\n \n /**\n@@ -219,7 +221,8 @@ public ParquetWriter(\n       boolean validating,\n       WriterVersion writerVersion,\n       Configuration conf) throws IOException {\n-    this(file, mode, writeSupport, compressionCodecName, blockSize,\n+    this(HadoopOutputFile.fromPath(file, conf),\n+        mode, writeSupport, compressionCodecName, blockSize,\n         validating, conf, MAX_PADDING_SIZE_DEFAULT,\n         ParquetProperties.builder()\n             .withPageSize(pageSize)\n@@ -257,11 +260,11 @@ public ParquetWriter(Path file, Configuration conf, WriteSupport<T> writeSupport\n   }\n \n   ParquetWriter(\n-      Path file,\n+      OutputFile file,\n       ParquetFileWriter.Mode mode,\n       WriteSupport<T> writeSupport,\n       CompressionCodecName compressionCodecName,\n-      int blockSize,\n+      int rowGroupSize,\n       boolean validating,\n       Configuration conf,\n       int maxPaddingSize,\n@@ -271,7 +274,7 @@ public ParquetWriter(Path file, Configuration conf, WriteSupport<T> writeSupport\n     MessageType schema = writeContext.getSchema();\n \n     ParquetFileWriter fileWriter = new ParquetFileWriter(\n-        conf, schema, file, mode, blockSize, maxPaddingSize);\n+        file, schema, mode, rowGroupSize, maxPaddingSize);\n     fileWriter.start();\n \n     this.codecFactory = new CodecFactory(conf, encodingProps.getPageSizeThreshold());\n@@ -281,7 +284,7 @@ public ParquetWriter(Path file, Configuration conf, WriteSupport<T> writeSupport\n         writeSupport,\n         schema,\n         writeContext.getExtraMetaData(),\n-        blockSize,\n+        rowGroupSize,\n         compressor,\n         validating,\n         encodingProps);\n@@ -324,7 +327,8 @@ public long getDataSize() {\n    * @param <SELF> The type of this builder that is returned by builder methods\n    */\n   public abstract static class Builder<T, SELF extends Builder<T, SELF>> {\n-    private final Path file;\n+    private OutputFile file = null;\n+    private Path path = null;\n     private Configuration conf = new Configuration();\n     private ParquetFileWriter.Mode mode;\n     private CompressionCodecName codecName = DEFAULT_COMPRESSION_CODEC_NAME;\n@@ -334,8 +338,12 @@ public long getDataSize() {\n     private ParquetProperties.Builder encodingPropsBuilder =\n         ParquetProperties.builder();\n \n-    protected Builder(Path file) {\n-      this.file = file;\n+    protected Builder(Path path) {\n+      this.path = path;\n+    }\n+\n+    protected Builder(OutputFile path) {\n+      this.file = path;\n     }\n \n     /**\n@@ -484,16 +492,36 @@ public SELF withWriterVersion(WriterVersion version) {\n       return self();\n     }\n \n+    /**\n+     * Set a property that will be available to the read path. For writers that use a Hadoop\n+     * configuration, this is the recommended way to add configuration values.\n+     *\n+     * @param property a String property name\n+     * @param value a String property value\n+     * @return this builder for method chaining.\n+     */\n+    public SELF config(String property, String value) {\n+      conf.set(property, value);\n+      return self();\n+    }\n+\n     /**\n      * Build a {@link ParquetWriter} with the accumulated configuration.\n      *\n      * @return a configured {@code ParquetWriter} instance.\n      * @throws IOException\n      */\n     public ParquetWriter<T> build() throws IOException {\n-      return new ParquetWriter<T>(file, mode, getWriteSupport(conf), codecName,\n-          rowGroupSize, enableValidation, conf, maxPaddingSize,\n-          encodingPropsBuilder.build());\n+      if (file != null) {\n+        return new ParquetWriter<>(file,\n+            mode, getWriteSupport(conf), codecName, rowGroupSize, enableValidation, conf,\n+            maxPaddingSize, encodingPropsBuilder.build());\n+      } else {\n+        return new ParquetWriter<>(HadoopOutputFile.fromPath(path, conf),\n+            mode, getWriteSupport(conf), codecName,\n+            rowGroupSize, enableValidation, conf, maxPaddingSize,\n+            encodingPropsBuilder.build());\n+      }\n     }\n   }\n }",
                "additions": 39,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java",
                "status": "modified",
                "changes": 50,
                "deletions": 11,
                "sha": "bdde70e25970d9ef8d4a5b5bafb8d34267064415",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetWriter.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -20,10 +20,12 @@\n \n \n import org.apache.hadoop.conf.Configuration;\n+import org.apache.parquet.ParquetReadOptions;\n import org.apache.parquet.io.ParquetDecodingException;\n import org.apache.parquet.io.api.RecordMaterializer.RecordMaterializationException;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n+import java.util.Map;\n \n // Essentially taken from:\n // https://github.com/twitter/elephant-bird/blob/master/core/src/main/java/com/twitter/elephantbird/mapreduce/input/LzoRecordReader.java#L124\n@@ -60,6 +62,10 @@ public UnmaterializableRecordCounter(Configuration conf, long totalNumRecords) {\n      );\n   }\n \n+  public UnmaterializableRecordCounter(ParquetReadOptions options, long totalNumRecords) {\n+    this(getFloat(options, BAD_RECORD_THRESHOLD_CONF_KEY, DEFAULT_THRESHOLD), totalNumRecords);\n+  }\n+\n   public UnmaterializableRecordCounter(double errorThreshold, long totalNumRecords) {\n     this.errorThreshold = errorThreshold;\n     this.totalNumRecords = totalNumRecords;\n@@ -85,4 +91,13 @@ public void incErrors(RecordMaterializationException cause) throws ParquetDecodi\n       throw new ParquetDecodingException(message, cause);\n     }\n   }\n+\n+  private static float getFloat(ParquetReadOptions options, String key, float defaultValue) {\n+    String value = options.getProperty(key);\n+    if (value != null) {\n+      return Float.valueOf(value);\n+    } else {\n+      return defaultValue;\n+    }\n+  }\n }",
                "additions": 15,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/UnmaterializableRecordCounter.java",
                "status": "modified",
                "changes": 15,
                "deletions": 0,
                "sha": "a70a0d01cbaf5bb744bf0ebbffe76cbe5c367911",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/UnmaterializableRecordCounter.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/UnmaterializableRecordCounter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/UnmaterializableRecordCounter.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -1,36 +0,0 @@\n-/* \n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- * \n- *   http://www.apache.org/licenses/LICENSE-2.0\n- * \n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.parquet.hadoop.codec;\n-\n-/**\n- * This exception will be thrown when the codec is not supported by parquet, meaning there is no\n- * matching codec defined in {@link org.apache.parquet.hadoop.metadata.CompressionCodecName}\n- */\n-public class CompressionCodecNotSupportedException extends RuntimeException {\n-  private final Class codecClass;\n-\n-  public CompressionCodecNotSupportedException(Class codecClass) {\n-    super(\"codec not supported: \" + codecClass.getName());\n-    this.codecClass = codecClass;\n-  }\n-\n-  public Class getCodecClass() {\n-    return codecClass;\n-  }\n-}",
                "additions": 0,
                "raw_url": "https://github.com/apache/parquet-mr/raw/81f480149054399d28b0609482c978788d9f5895/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/codec/CompressionCodecNotSupportedException.java",
                "status": "removed",
                "changes": 36,
                "deletions": 36,
                "sha": "9657cc12b232ec7a5a8412e5eb7b785ca8d11fda",
                "blob_url": "https://github.com/apache/parquet-mr/blob/81f480149054399d28b0609482c978788d9f5895/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/codec/CompressionCodecNotSupportedException.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/codec/CompressionCodecNotSupportedException.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/codec/CompressionCodecNotSupportedException.java?ref=81f480149054399d28b0609482c978788d9f5895"
            },
            {
                "patch": "@@ -20,31 +20,22 @@\n package org.apache.parquet.hadoop.util;\n \n import org.apache.hadoop.fs.FSDataInputStream;\n-import org.apache.parquet.io.SeekableInputStream;\n-import java.io.EOFException;\n+import org.apache.parquet.io.DelegatingSeekableInputStream;\n import java.io.IOException;\n-import java.nio.ByteBuffer;\n \n /**\n  * SeekableInputStream implementation that implements read(ByteBuffer) for\n  * Hadoop 1 FSDataInputStream.\n  */\n-class H1SeekableInputStream extends SeekableInputStream {\n-\n-  private final int COPY_BUFFER_SIZE = 8192;\n-  private final byte[] temp = new byte[COPY_BUFFER_SIZE];\n+class H1SeekableInputStream extends DelegatingSeekableInputStream {\n \n   private final FSDataInputStream stream;\n \n   public H1SeekableInputStream(FSDataInputStream stream) {\n+    super(stream);\n     this.stream = stream;\n   }\n \n-  @Override\n-  public void close() throws IOException {\n-    stream.close();\n-  }\n-\n   @Override\n   public long getPos() throws IOException {\n     return stream.getPos();\n@@ -55,16 +46,6 @@ public void seek(long newPos) throws IOException {\n     stream.seek(newPos);\n   }\n \n-  @Override\n-  public int read() throws IOException {\n-    return stream.read();\n-  }\n-\n-  @Override\n-  public int read(byte[] b, int off, int len) throws IOException {\n-    return stream.read(b, off, len);\n-  }\n-\n   @Override\n   public void readFully(byte[] bytes) throws IOException {\n     stream.readFully(bytes, 0, bytes.length);\n@@ -75,80 +56,4 @@ public void readFully(byte[] bytes, int start, int len) throws IOException {\n     stream.readFully(bytes);\n   }\n \n-  @Override\n-  public int read(ByteBuffer buf) throws IOException {\n-    if (buf.hasArray()) {\n-      return readHeapBuffer(stream, buf);\n-    } else {\n-      return readDirectBuffer(stream, buf, temp);\n-    }\n-  }\n-\n-  @Override\n-  public void readFully(ByteBuffer buf) throws IOException {\n-    if (buf.hasArray()) {\n-      readFullyHeapBuffer(stream, buf);\n-    } else {\n-      readFullyDirectBuffer(stream, buf, temp);\n-    }\n-  }\n-\n-  // Visible for testing\n-  static int readHeapBuffer(FSDataInputStream f, ByteBuffer buf) throws IOException {\n-    int bytesRead = f.read(buf.array(), buf.arrayOffset() + buf.position(), buf.remaining());\n-    if (bytesRead < 0) {\n-      // if this resulted in EOF, don't update position\n-      return bytesRead;\n-    } else {\n-      buf.position(buf.position() + bytesRead);\n-      return bytesRead;\n-    }\n-  }\n-\n-  // Visible for testing\n-  static void readFullyHeapBuffer(FSDataInputStream f, ByteBuffer buf) throws IOException {\n-    f.readFully(buf.array(), buf.arrayOffset() + buf.position(), buf.remaining());\n-    buf.position(buf.limit());\n-  }\n-\n-  // Visible for testing\n-  static int readDirectBuffer(FSDataInputStream f, ByteBuffer buf, byte[] temp) throws IOException {\n-    // copy all the bytes that return immediately, stopping at the first\n-    // read that doesn't return a full buffer.\n-    int nextReadLength = Math.min(buf.remaining(), temp.length);\n-    int totalBytesRead = 0;\n-    int bytesRead;\n-\n-    while ((bytesRead = f.read(temp, 0, nextReadLength)) == temp.length) {\n-      buf.put(temp);\n-      totalBytesRead += bytesRead;\n-      nextReadLength = Math.min(buf.remaining(), temp.length);\n-    }\n-\n-    if (bytesRead < 0) {\n-      // return -1 if nothing was read\n-      return totalBytesRead == 0 ? -1 : totalBytesRead;\n-    } else {\n-      // copy the last partial buffer\n-      buf.put(temp, 0, bytesRead);\n-      totalBytesRead += bytesRead;\n-      return totalBytesRead;\n-    }\n-  }\n-\n-  // Visible for testing\n-  static void readFullyDirectBuffer(FSDataInputStream f, ByteBuffer buf, byte[] temp) throws IOException {\n-    int nextReadLength = Math.min(buf.remaining(), temp.length);\n-    int bytesRead = 0;\n-\n-    while (nextReadLength > 0 && (bytesRead = f.read(temp, 0, nextReadLength)) >= 0) {\n-      buf.put(temp, 0, bytesRead);\n-      nextReadLength = Math.min(buf.remaining(), temp.length);\n-    }\n-\n-    if (bytesRead < 0 && buf.remaining() > 0) {\n-      throw new EOFException(\n-          \"Reached the end of stream. Still have: \" + buf.remaining() + \" bytes left\");\n-    }\n-  }\n }",
                "additions": 3,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/H1SeekableInputStream.java",
                "status": "modified",
                "changes": 101,
                "deletions": 98,
                "sha": "876a1f372f5b49ae6dfd9ca0eca5d2e6086eaffa",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/H1SeekableInputStream.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/H1SeekableInputStream.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/H1SeekableInputStream.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -20,7 +20,7 @@\n package org.apache.parquet.hadoop.util;\n \n import org.apache.hadoop.fs.FSDataInputStream;\n-import org.apache.parquet.io.SeekableInputStream;\n+import org.apache.parquet.io.DelegatingSeekableInputStream;\n import java.io.EOFException;\n import java.io.IOException;\n import java.nio.ByteBuffer;\n@@ -29,7 +29,7 @@\n  * SeekableInputStream implementation for FSDataInputStream that implements\n  * ByteBufferReadable in Hadoop 2.\n  */\n-class H2SeekableInputStream extends SeekableInputStream {\n+class H2SeekableInputStream extends DelegatingSeekableInputStream {\n \n   // Visible for testing\n   interface Reader {\n@@ -40,6 +40,7 @@\n   private final Reader reader;\n \n   public H2SeekableInputStream(FSDataInputStream stream) {\n+    super(stream);\n     this.stream = stream;\n     this.reader = new H2Reader();\n   }\n@@ -59,21 +60,6 @@ public void seek(long newPos) throws IOException {\n     stream.seek(newPos);\n   }\n \n-  @Override\n-  public int read() throws IOException {\n-    return stream.read();\n-  }\n-\n-  @Override\n-  public int read(byte[] b, int off, int len) throws IOException {\n-    return stream.read(b, off, len);\n-  }\n-\n-  @Override\n-  public void readFully(byte[] bytes) throws IOException {\n-    stream.readFully(bytes, 0, bytes.length);\n-  }\n-\n   @Override\n   public void readFully(byte[] bytes, int start, int len) throws IOException {\n     stream.readFully(bytes);",
                "additions": 3,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/H2SeekableInputStream.java",
                "status": "modified",
                "changes": 20,
                "deletions": 17,
                "sha": "c68f6b6f7d3a293329a9b3d3f0da80a74154ef0f",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/H2SeekableInputStream.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/H2SeekableInputStream.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/H2SeekableInputStream.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -0,0 +1,39 @@\n+/*\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing,\n+ *  software distributed under the License is distributed on an\n+ *  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ *  KIND, either express or implied.  See the License for the\n+ *  specific language governing permissions and limitations\n+ *  under the License.\n+ */\n+\n+package org.apache.parquet.hadoop.util;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.parquet.bytes.ByteBufferAllocator;\n+import org.apache.parquet.compression.CompressionCodecFactory;\n+import org.apache.parquet.hadoop.CodecFactory;\n+\n+public class HadoopCodecs {\n+  public static CompressionCodecFactory newFactory(int sizeHint) {\n+    return new CodecFactory(new Configuration(), sizeHint);\n+  }\n+\n+  public static CompressionCodecFactory newFactory(Configuration conf, int sizeHint) {\n+    return new CodecFactory(conf, sizeHint);\n+  }\n+\n+  public static CompressionCodecFactory newDirectFactory(Configuration conf, ByteBufferAllocator allocator, int sizeHint) {\n+    return CodecFactory.createDirectCodecFactory(conf, allocator, sizeHint);\n+  }\n+}",
                "additions": 39,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/HadoopCodecs.java",
                "status": "added",
                "changes": 39,
                "deletions": 0,
                "sha": "a46c8db216de007a13be6c5b7d300198629838a3",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/HadoopCodecs.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/HadoopCodecs.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/HadoopCodecs.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -0,0 +1,100 @@\n+/*\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing,\n+ *  software distributed under the License is distributed on an\n+ *  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ *  KIND, either express or implied.  See the License for the\n+ *  specific language governing permissions and limitations\n+ *  under the License.\n+ */\n+\n+package org.apache.parquet.hadoop.util;\n+\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.io.OutputFile;\n+import org.apache.parquet.io.PositionOutputStream;\n+import java.io.IOException;\n+import java.util.HashSet;\n+import java.util.Set;\n+\n+public class HadoopOutputFile implements OutputFile {\n+  // need to supply a buffer size when setting block size. this is the default\n+  // for hadoop 1 to present. copying it avoids loading DFSConfigKeys.\n+  private static final int DFS_BUFFER_SIZE_DEFAULT = 4096;\n+\n+  private static final Set<String> BLOCK_FS_SCHEMES = new HashSet<String>();\n+  static {\n+    BLOCK_FS_SCHEMES.add(\"hdfs\");\n+    BLOCK_FS_SCHEMES.add(\"webhdfs\");\n+    BLOCK_FS_SCHEMES.add(\"viewfs\");\n+  }\n+\n+  // visible for testing\n+  public static Set<String> getBlockFileSystems() {\n+    return BLOCK_FS_SCHEMES;\n+  }\n+\n+  private static boolean supportsBlockSize(FileSystem fs) {\n+    return BLOCK_FS_SCHEMES.contains(fs.getUri().getScheme());\n+  }\n+\n+  private final FileSystem fs;\n+  private final Path path;\n+  private final Configuration conf;\n+\n+  public static HadoopOutputFile fromPath(Path path, Configuration conf)\n+      throws IOException {\n+    FileSystem fs = path.getFileSystem(conf);\n+    return new HadoopOutputFile(fs, fs.makeQualified(path), conf);\n+  }\n+\n+  private HadoopOutputFile(FileSystem fs, Path path, Configuration conf) {\n+    this.fs = fs;\n+    this.path = path;\n+    this.conf = conf;\n+  }\n+\n+  public Configuration getConfiguration() {\n+    return conf;\n+  }\n+\n+  @Override\n+  public PositionOutputStream create(long blockSizeHint) throws IOException {\n+    return HadoopStreams.wrap(fs.create(path, false /* do not overwrite */,\n+        DFS_BUFFER_SIZE_DEFAULT, fs.getDefaultReplication(path),\n+        Math.max(fs.getDefaultBlockSize(path), blockSizeHint)));\n+  }\n+\n+  @Override\n+  public PositionOutputStream createOrOverwrite(long blockSizeHint) throws IOException {\n+    return HadoopStreams.wrap(fs.create(path, true /* overwrite if exists */,\n+        DFS_BUFFER_SIZE_DEFAULT, fs.getDefaultReplication(path),\n+        Math.max(fs.getDefaultBlockSize(path), blockSizeHint)));\n+  }\n+\n+  @Override\n+  public boolean supportsBlockSize() {\n+    return supportsBlockSize(fs);\n+  }\n+\n+  @Override\n+  public long defaultBlockSize() {\n+    return fs.getDefaultBlockSize(path);\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return path.toString();\n+  }\n+}",
                "additions": 100,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/HadoopOutputFile.java",
                "status": "added",
                "changes": 100,
                "deletions": 0,
                "sha": "4740fd4ea3e5d0e86668a6821249f18033537539",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/HadoopOutputFile.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/HadoopOutputFile.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/HadoopOutputFile.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -0,0 +1,66 @@\n+/*\n+ *  Licensed to the Apache Software Foundation (ASF) under one\n+ *  or more contributor license agreements.  See the NOTICE file\n+ *  distributed with this work for additional information\n+ *  regarding copyright ownership.  The ASF licenses this file\n+ *  to you under the Apache License, Version 2.0 (the\n+ *  \"License\"); you may not use this file except in compliance\n+ *  with the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ *  Unless required by applicable law or agreed to in writing,\n+ *  software distributed under the License is distributed on an\n+ *  \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ *  KIND, either express or implied.  See the License for the\n+ *  specific language governing permissions and limitations\n+ *  under the License.\n+ */\n+\n+package org.apache.parquet.hadoop.util;\n+\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.parquet.io.PositionOutputStream;\n+import java.io.IOException;\n+\n+public class HadoopPositionOutputStream extends PositionOutputStream {\n+  private final FSDataOutputStream wrapped;\n+\n+  HadoopPositionOutputStream(FSDataOutputStream wrapped) {\n+    this.wrapped = wrapped;\n+  }\n+\n+  @Override\n+  public long getPos() throws IOException {\n+    return wrapped.getPos();\n+  }\n+\n+  @Override\n+  public void write(int b) throws IOException {\n+    wrapped.write(b);\n+  }\n+\n+  @Override\n+  public void write(byte[] b) throws IOException {\n+    wrapped.write(b);\n+  }\n+\n+  @Override\n+  public void write(byte[] b, int off, int len) throws IOException {\n+    wrapped.write(b, off, len);\n+  }\n+\n+  public void sync() throws IOException {\n+    wrapped.hsync();\n+  }\n+\n+  @Override\n+  public void flush() throws IOException {\n+    wrapped.flush();\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    wrapped.close();\n+  }\n+}",
                "additions": 66,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/HadoopPositionOutputStream.java",
                "status": "added",
                "changes": 66,
                "deletions": 0,
                "sha": "4b194aac8d0e6c4082df24009c09e5e10c2f6eae",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/HadoopPositionOutputStream.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/HadoopPositionOutputStream.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/HadoopPositionOutputStream.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -20,8 +20,11 @@\n package org.apache.parquet.hadoop.util;\n \n import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.parquet.Preconditions;\n import org.apache.parquet.io.ParquetDecodingException;\n import org.apache.parquet.io.SeekableInputStream;\n+import org.apache.parquet.io.PositionOutputStream;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -46,6 +49,7 @@\n    * @return a SeekableInputStream\n    */\n   public static SeekableInputStream wrap(FSDataInputStream stream) {\n+    Preconditions.checkNotNull(stream, \"Cannot wrap a null input stream\");\n     if (byteBufferReadableClass != null && h2SeekableConstructor != null &&\n         byteBufferReadableClass.isInstance(stream.getWrappedStream())) {\n       try {\n@@ -99,4 +103,15 @@ public static SeekableInputStream wrap(FSDataInputStream stream) {\n     return null;\n   }\n \n+  /**\n+   * Wraps a {@link FSDataOutputStream} in a {@link PositionOutputStream}\n+   * implementation for Parquet writers.\n+   *\n+   * @param stream a Hadoop FSDataOutputStream\n+   * @return a SeekableOutputStream\n+   */\n+  public static PositionOutputStream wrap(FSDataOutputStream stream) {\n+    Preconditions.checkNotNull(stream, \"Cannot wrap a null output stream\");\n+    return new HadoopPositionOutputStream(stream);\n+  }\n }",
                "additions": 15,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/HadoopStreams.java",
                "status": "modified",
                "changes": 15,
                "deletions": 0,
                "sha": "c35e98f2ed1ceda64ec7c20097ea9cf51f93ddab",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/HadoopStreams.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/HadoopStreams.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/util/HadoopStreams.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -35,6 +35,8 @@\n import org.apache.parquet.hadoop.example.GroupWriteSupport;\n import org.apache.parquet.hadoop.metadata.BlockMetaData;\n import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n+import org.apache.parquet.hadoop.util.HadoopOutputFile;\n+import org.apache.parquet.io.OutputFile;\n import org.apache.parquet.io.api.Binary;\n import org.apache.parquet.schema.MessageType;\n import org.apache.parquet.schema.Types;\n@@ -105,7 +107,7 @@ protected void map(Void key, Group value, Context context)\n \n   @Test\n   public void testBasicBehaviorWithPadding() throws Exception {\n-    ParquetFileWriter.BLOCK_FS_SCHEMES.add(\"file\");\n+    HadoopOutputFile.getBlockFileSystems().add(\"file\");\n \n     File inputFile = temp.newFile();\n     FileOutputStream out = new FileOutputStream(inputFile);\n@@ -186,7 +188,7 @@ public void testBasicBehaviorWithPadding() throws Exception {\n     Assert.assertEquals(\"Should match written file content\",\n         FILE_CONTENT, reconstructed);\n \n-    ParquetFileWriter.BLOCK_FS_SCHEMES.remove(\"file\");\n+    HadoopOutputFile.getBlockFileSystems().remove(\"file\");\n   }\n \n   private void waitForJob(Job job) throws Exception {",
                "additions": 4,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestInputOutputFormatWithPadding.java",
                "status": "modified",
                "changes": 6,
                "deletions": 2,
                "sha": "8e3e6c76bd20c0c509ca91ab651b2a6703d41c11",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestInputOutputFormatWithPadding.java",
                "filename": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestInputOutputFormatWithPadding.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestInputOutputFormatWithPadding.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -27,6 +27,7 @@\n import org.apache.parquet.Version;\n import org.apache.parquet.bytes.BytesUtils;\n import org.apache.parquet.hadoop.ParquetOutputFormat.JobSummaryLevel;\n+import org.apache.parquet.hadoop.util.HadoopOutputFile;\n import org.junit.Assume;\n import org.junit.Rule;\n import org.junit.Test;",
                "additions": 1,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestParquetFileWriter.java",
                "status": "modified",
                "changes": 1,
                "deletions": 0,
                "sha": "6915c86ec3d6bbf0ab1423ec419ea6ac89d2f3cf",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestParquetFileWriter.java",
                "filename": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestParquetFileWriter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestParquetFileWriter.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -24,13 +24,13 @@\n import java.io.ByteArrayInputStream;\n import java.io.IOException;\n \n-class MockInputStream extends ByteArrayInputStream\n+class MockHadoopInputStream extends ByteArrayInputStream\n     implements Seekable, PositionedReadable {\n   static final byte[] TEST_ARRAY = new byte[] { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10 };\n \n   private int[] lengths;\n   private int current = 0;\n-  MockInputStream(int... actualReadLengths) {\n+  MockHadoopInputStream(int... actualReadLengths) {\n     super(TEST_ARRAY);\n     this.lengths = actualReadLengths;\n   }",
                "additions": 2,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/util/MockHadoopInputStream.java",
                "previous_filename": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/util/MockInputStream.java",
                "status": "renamed",
                "changes": 4,
                "deletions": 2,
                "sha": "b41b3c889e28d93c50ececb825ce728bc9173bed",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/util/MockHadoopInputStream.java",
                "filename": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/util/MockHadoopInputStream.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/util/MockHadoopInputStream.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -28,7 +28,7 @@\n import java.nio.ByteBuffer;\n import java.util.concurrent.Callable;\n \n-import static org.apache.parquet.hadoop.util.MockInputStream.TEST_ARRAY;\n+import static org.apache.parquet.hadoop.util.MockHadoopInputStream.TEST_ARRAY;\n \n public class TestHadoop2ByteBufferReads {\n \n@@ -59,7 +59,7 @@ public int read(ByteBuffer buf) throws IOException {\n   public void testHeapReadFullySmallBuffer() throws Exception {\n     ByteBuffer readBuffer = ByteBuffer.allocate(8);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream());\n+    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream());\n     MockBufferReader reader = new MockBufferReader(hadoopStream);\n \n     H2SeekableInputStream.readFully(reader, readBuffer);\n@@ -79,7 +79,7 @@ public void testHeapReadFullySmallBuffer() throws Exception {\n   public void testHeapReadFullyLargeBuffer() throws Exception {\n     final ByteBuffer readBuffer = ByteBuffer.allocate(20);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream());\n+    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream());\n     final MockBufferReader reader = new MockBufferReader(hadoopStream);\n \n     TestUtils.assertThrows(\"Should throw EOFException\",\n@@ -105,7 +105,7 @@ public Object call() throws Exception {\n   public void testHeapReadFullyJustRight() throws Exception {\n     ByteBuffer readBuffer = ByteBuffer.allocate(10);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream());\n+    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream());\n     MockBufferReader reader = new MockBufferReader(hadoopStream);\n \n     // reads all of the bytes available without EOFException\n@@ -127,7 +127,7 @@ public void testHeapReadFullyJustRight() throws Exception {\n   public void testHeapReadFullySmallReads() throws Exception {\n     ByteBuffer readBuffer = ByteBuffer.allocate(10);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));\n     MockBufferReader reader = new MockBufferReader(hadoopStream);\n \n     H2SeekableInputStream.readFully(reader, readBuffer);\n@@ -149,7 +149,7 @@ public void testHeapReadFullyPosition() throws Exception {\n     readBuffer.position(3);\n     readBuffer.mark();\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));\n     MockBufferReader reader = new MockBufferReader(hadoopStream);\n \n     H2SeekableInputStream.readFully(reader, readBuffer);\n@@ -170,7 +170,7 @@ public void testHeapReadFullyLimit() throws Exception {\n     ByteBuffer readBuffer = ByteBuffer.allocate(10);\n     readBuffer.limit(7);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));\n     MockBufferReader reader = new MockBufferReader(hadoopStream);\n \n     H2SeekableInputStream.readFully(reader, readBuffer);\n@@ -203,7 +203,7 @@ public void testHeapReadFullyPositionAndLimit() throws Exception {\n     readBuffer.limit(7);\n     readBuffer.mark();\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));\n     MockBufferReader reader = new MockBufferReader(hadoopStream);\n \n     H2SeekableInputStream.readFully(reader, readBuffer);\n@@ -233,7 +233,7 @@ public void testHeapReadFullyPositionAndLimit() throws Exception {\n   public void testDirectReadFullySmallBuffer() throws Exception {\n     ByteBuffer readBuffer = ByteBuffer.allocateDirect(8);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream());\n+    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream());\n     MockBufferReader reader = new MockBufferReader(hadoopStream);\n \n     H2SeekableInputStream.readFully(reader, readBuffer);\n@@ -253,7 +253,7 @@ public void testDirectReadFullySmallBuffer() throws Exception {\n   public void testDirectReadFullyLargeBuffer() throws Exception {\n     final ByteBuffer readBuffer = ByteBuffer.allocateDirect(20);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream());\n+    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream());\n     final MockBufferReader reader = new MockBufferReader(hadoopStream);\n \n     TestUtils.assertThrows(\"Should throw EOFException\",\n@@ -279,7 +279,7 @@ public Object call() throws Exception {\n   public void testDirectReadFullyJustRight() throws Exception {\n     ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream());\n+    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream());\n     MockBufferReader reader = new MockBufferReader(hadoopStream);\n \n     // reads all of the bytes available without EOFException\n@@ -301,7 +301,7 @@ public void testDirectReadFullyJustRight() throws Exception {\n   public void testDirectReadFullySmallReads() throws Exception {\n     ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));\n     MockBufferReader reader = new MockBufferReader(hadoopStream);\n \n     H2SeekableInputStream.readFully(reader, readBuffer);\n@@ -323,7 +323,7 @@ public void testDirectReadFullyPosition() throws Exception {\n     readBuffer.position(3);\n     readBuffer.mark();\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));\n     MockBufferReader reader = new MockBufferReader(hadoopStream);\n \n     H2SeekableInputStream.readFully(reader, readBuffer);\n@@ -344,7 +344,7 @@ public void testDirectReadFullyLimit() throws Exception {\n     ByteBuffer readBuffer = ByteBuffer.allocateDirect(10);\n     readBuffer.limit(7);\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));\n     H2SeekableInputStream.Reader reader = new MockBufferReader(hadoopStream);\n \n     H2SeekableInputStream.readFully(reader, readBuffer);\n@@ -377,7 +377,7 @@ public void testDirectReadFullyPositionAndLimit() throws Exception {\n     readBuffer.limit(7);\n     readBuffer.mark();\n \n-    FSDataInputStream hadoopStream = new FSDataInputStream(new MockInputStream(2, 3, 3));\n+    FSDataInputStream hadoopStream = new FSDataInputStream(new MockHadoopInputStream(2, 3, 3));\n     MockBufferReader reader = new MockBufferReader(hadoopStream);\n \n     H2SeekableInputStream.readFully(reader, readBuffer);",
                "additions": 15,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/util/TestHadoop2ByteBufferReads.java",
                "status": "modified",
                "changes": 30,
                "deletions": 15,
                "sha": "68c9b3bfd244ea2cb5f864900a76ceeaa32f71bd",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/util/TestHadoop2ByteBufferReads.java",
                "filename": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/util/TestHadoop2ByteBufferReads.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/util/TestHadoop2ByteBufferReads.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -23,6 +23,7 @@\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.parquet.hadoop.util.HadoopInputFile;\n import org.apache.parquet.hadoop.util.HiddenFileFilter;\n import org.apache.parquet.hadoop.ParquetFileWriter;\n import org.apache.parquet.hadoop.metadata.FileMetaData;\n@@ -91,7 +92,7 @@ public void execute(CommandLine options) throws Exception {\n         tooSmallFilesMerged = true;\n       }\n \n-      writer.appendFile(conf, input);\n+      writer.appendFile(HadoopInputFile.fromPath(input, conf));\n     }\n \n     if (tooSmallFilesMerged) {",
                "additions": 2,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-tools/src/main/java/org/apache/parquet/tools/command/MergeCommand.java",
                "status": "modified",
                "changes": 3,
                "deletions": 1,
                "sha": "fe6458756051ec69a6a68426a2623e4b978a55dd",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/parquet-tools/src/main/java/org/apache/parquet/tools/command/MergeCommand.java",
                "filename": "parquet-tools/src/main/java/org/apache/parquet/tools/command/MergeCommand.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-tools/src/main/java/org/apache/parquet/tools/command/MergeCommand.java?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            },
            {
                "patch": "@@ -262,7 +262,14 @@\n                      <exclude>org/apache/parquet/avro/SpecificDataSupplier</exclude> <!-- made public -->\n                      <exclude>org/apache/parquet/io/ColumnIOFactory$ColumnIOCreatorVisitor</exclude> <!-- removed non-API class -->\n                      <exclude>org/apache/parquet/io/ColumnIOFactory/**</exclude> <!-- removed non-API class and methods-->\n-\t\t     <exclude>org/apache/parquet/hadoop/codec/SnappyCompressor</exclude> <!-- added synchronized modifier -->\n+                     <exclude>org/apache/parquet/hadoop/codec/SnappyCompressor</exclude> <!-- added synchronized modifier -->\n+                     <exclude>org/apache/parquet/bytes/BytesInput</exclude> <!-- moved to parquet-common -->\n+                     <exclude>org/apache/parquet/bytes/CapacityByteArrayOutputStream</exclude> <!-- moved to parquet-common -->\n+                     <exclude>org/apache/parquet/bytes/ConcatenatingByteArrayCollector</exclude> <!-- moved to parquet-common -->\n+                     <exclude>org/apache/parquet/bytes/LittleEndianDataInputStream</exclude> <!-- moved to parquet-common -->\n+                     <exclude>org/apache/parquet/bytes/LittleEndianDataOutputStream</exclude> <!-- moved to parquet-common -->\n+                     <exclude>org/apache/parquet/hadoop/metadata/CompressionCodecName</exclude> <!-- moved to parquet-common -->\n+                     <exclude>org/apache/parquet/hadoop/codec/CompressionCodecNotSupportedException</exclude> <!-- moved to parquet-common -->\n                    </excludes>\n                  </requireBackwardCompatibility>\n                </rules>",
                "additions": 8,
                "raw_url": "https://github.com/apache/parquet-mr/raw/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/pom.xml",
                "status": "modified",
                "changes": 9,
                "deletions": 1,
                "sha": "05e3e470649bae106be0983d00796d671d3633da",
                "blob_url": "https://github.com/apache/parquet-mr/blob/8bfd9b4d8f4fb0a2b522c9328f67eb642066306b/pom.xml",
                "filename": "pom.xml",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/pom.xml?ref=8bfd9b4d8f4fb0a2b522c9328f67eb642066306b"
            }
        ],
        "bug_id": "parquet-mr_14",
        "parent": "https://github.com/apache/parquet-mr/commit/81f480149054399d28b0609482c978788d9f5895",
        "message": "PARQUET-1142: Add alternatives to Hadoop classes in the API\n\nThis updates the read and write paths to avoid using Hadoop classes where possible.\n\n* Adds a generic compression interface, `CompressionCodecFactory`\n* Adds `OutputFile` and `PositionOutputStream`\n* Adds classes to help implementations wrap input and output streams: `DelegatingSeekableInputStream` and `DelegatingPositionOutputStream`\n* Adds `ParquetReadOptions` to avoid passing options with `Configuration`\n* Updates the read and write APIs to use new abstractions instead of Hadoop\n\nAuthor: Ryan Blue <blue@apache.org>\n\nCloses #429 from rdblue/PARQUET-1142-add-hadoop-alternatives and squashes the following commits:\n\n21500337b [Ryan Blue] PARQUET-1142: Fix NPE when not filtering with new read API.\n35eddd735 [Ryan Blue] PARQUET-1142: Fix problems from Gabor's review.\nda391b0d4 [Ryan Blue] PARQUET-1142: Fix binary incompatibilities.\n2e3d693ab [Ryan Blue] PARQUET-1142: Update the read and write paths to use new files and streams.\n8d57e089f [Ryan Blue] PARQUET-1142: Add OutputFile and PositionOutputStream.\n42908a95e [Ryan Blue] PARQUET-1142: Extract non-Hadoop API from CodecFactory.",
        "repo": "parquet-mr"
    },
    {
        "commit": "https://github.com/apache/parquet-mr/commit/b82d96218bfd37f6df95a2e8d7675d091ab61970",
        "file": [
            {
                "patch": "@@ -168,12 +168,12 @@ private void printColumnChunk(Logger console, int width, ColumnChunkMetaData col\n     if (typeName == PrimitiveType.PrimitiveTypeName.FIXED_LEN_BYTE_ARRAY) {\n       console.info(String.format(\"%-\" + width + \"s  FIXED[%d] %s %-7s %-9d %-8s %-7s %s\",\n           name, type.getTypeLength(), shortCodec(codec), encodingSummary, count,\n-          humanReadable(perValue), stats == null ? \"\" : String.valueOf(stats.getNumNulls()),\n+          humanReadable(perValue), stats == null || !stats.isNumNullsSet() ? \"\" : String.valueOf(stats.getNumNulls()),\n           minMaxAsString(stats, type.getOriginalType())));\n     } else {\n       console.info(String.format(\"%-\" + width + \"s  %-9s %s %-7s %-9d %-10s %-7s %s\",\n           name, typeName, shortCodec(codec), encodingSummary, count, humanReadable(perValue),\n-          stats == null ? \"\" : String.valueOf(stats.getNumNulls()),\n+          stats == null || !stats.isNumNullsSet() ? \"\" : String.valueOf(stats.getNumNulls()),\n           minMaxAsString(stats, type.getOriginalType())));\n     }\n   }",
                "additions": 2,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-cli/src/main/java/org/apache/parquet/cli/commands/ParquetMetadataCommand.java",
                "status": "modified",
                "changes": 4,
                "deletions": 2,
                "sha": "54fe6579b9ac0a9cbd511e1e4e229d5f5bf01df5",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-cli/src/main/java/org/apache/parquet/cli/commands/ParquetMetadataCommand.java",
                "filename": "parquet-cli/src/main/java/org/apache/parquet/cli/commands/ParquetMetadataCommand.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-cli/src/main/java/org/apache/parquet/cli/commands/ParquetMetadataCommand.java?ref=b82d96218bfd37f6df95a2e8d7675d091ab61970"
            },
            {
                "patch": "@@ -191,7 +191,7 @@ public String visit(DataPageV1 page) {\n       String enc = encodingAsString(page.getValueEncoding(), false);\n       long totalSize = page.getCompressedSize();\n       int count = page.getValueCount();\n-      long numNulls = page.getStatistics().getNumNulls();\n+      String numNulls = page.getStatistics().isNumNullsSet() ? Long.toString(page.getStatistics().getNumNulls()) : \"\";\n       float perValue = ((float) totalSize) / count;\n       String minMax = minMaxAsString(page.getStatistics(), type.getOriginalType());\n       return String.format(\"%3d-%-3d  %-5s %s %-2s %-7d %-10s %-10s %-8s %-7s %s\",",
                "additions": 1,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-cli/src/main/java/org/apache/parquet/cli/commands/ShowPagesCommand.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "4d0e2c9ba50abd7c37a5c8ed702dbe985f79754f",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-cli/src/main/java/org/apache/parquet/cli/commands/ShowPagesCommand.java",
                "filename": "parquet-cli/src/main/java/org/apache/parquet/cli/commands/ShowPagesCommand.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-cli/src/main/java/org/apache/parquet/cli/commands/ShowPagesCommand.java?ref=b82d96218bfd37f6df95a2e8d7675d091ab61970"
            },
            {
                "patch": "@@ -35,6 +35,44 @@\n  */\n public abstract class Statistics<T extends Comparable<T>> {\n \n+  /**\n+   * Builder class to build Statistics objects. Used to read the statistics from the Parquet file.\n+   */\n+  public static class Builder {\n+    private final PrimitiveType type;\n+    private byte[] min;\n+    private byte[] max;\n+    private long numNulls = -1;\n+\n+    private Builder(PrimitiveType type) {\n+      this.type = type;\n+    }\n+\n+    public Builder withMin(byte[] min) {\n+      this.min = min;\n+      return this;\n+    }\n+\n+    public Builder withMax(byte[] max) {\n+      this.max = max;\n+      return this;\n+    }\n+\n+    public Builder withNumNulls(long numNulls) {\n+      this.numNulls = numNulls;\n+      return this;\n+    }\n+\n+    public Statistics<?> build() {\n+      Statistics<?> stats = createStats(type);\n+      if (min != null && max != null) {\n+        stats.setMinMaxFromBytes(min, max);\n+      }\n+      stats.num_nulls = this.numNulls;\n+      return stats;\n+    }\n+  }\n+\n   private final PrimitiveType type;\n   private final PrimitiveComparator<T> comparator;\n   private boolean hasNonNullValue;\n@@ -109,6 +147,17 @@ public static Statistics getStatsBasedOnType(PrimitiveTypeName type) {\n     }\n   }\n \n+  /**\n+   * Returns a builder to create new statistics object. Used to read the statistics from the parquet file.\n+   *\n+   * @param type\n+   *          type of the column\n+   * @return builder to create new statistics object\n+   */\n+  public static Builder getBuilder(PrimitiveType type) {\n+    return new Builder(type);\n+  }\n+\n   /**\n    * updates statistics min and max using the passed value\n    * @param value value to use to update min and max\n@@ -217,7 +266,9 @@ public void mergeStatistics(Statistics stats) {\n    * Abstract method to set min and max values from byte arrays.\n    * @param minBytes byte array to set the min value to\n    * @param maxBytes byte array to set the max value to\n+   * @deprecated will be removed in 2.0.0. Use {@link #getBuilder(PrimitiveType)} instead.\n    */\n+  @Deprecated\n   abstract public void setMinMaxFromBytes(byte[] minBytes, byte[] maxBytes);\n \n   /**\n@@ -310,9 +361,13 @@ public String maxAsString() {\n \n   @Override\n   public String toString() {\n-    if (this.hasNonNullValue())\n-      return String.format(\"min: %s, max: %s, num_nulls: %d\", minAsString(), maxAsString(), this.getNumNulls());\n-    else if (!this.isEmpty())\n+    if (this.hasNonNullValue()) {\n+      if (isNumNullsSet()) {\n+        return String.format(\"min: %s, max: %s, num_nulls: %d\", minAsString(), maxAsString(), this.getNumNulls());\n+      } else {\n+        return String.format(\"min: %s, max: %s, num_nulls not defined\", minAsString(), maxAsString());\n+      }\n+    } else if (!this.isEmpty())\n       return String.format(\"num_nulls: %d, min/max not defined\", this.getNumNulls());\n     else\n       return \"no stats for this column\";\n@@ -335,16 +390,20 @@ public void incrementNumNulls(long increment) {\n \n   /**\n    * Returns the null count\n-   * @return null count\n+   * @return null count or {@code -1} if the null count is not set\n    */\n   public long getNumNulls() {\n     return num_nulls;\n   }\n \n   /**\n    * Sets the number of nulls to the parameter value\n-   * @param nulls null count to set the count to\n+   *\n+   * @param nulls\n+   *          null count to set the count to\n+   * @deprecated will be removed in 2.0.0. Use {@link #getBuilder(PrimitiveType)} instead.\n    */\n+  @Deprecated\n   public void setNumNulls(long nulls) {\n     num_nulls = nulls;\n   }\n@@ -355,7 +414,7 @@ public void setNumNulls(long nulls) {\n    * @return true if object is empty, false otherwise\n    */\n   public boolean isEmpty() {\n-    return !hasNonNullValue && num_nulls == 0;\n+    return !hasNonNullValue && !isNumNullsSet();\n   }\n \n   /**\n@@ -365,6 +424,13 @@ public boolean hasNonNullValue() {\n     return hasNonNullValue;\n   }\n \n+  /**\n+   * @return whether numNulls is set and can be used\n+   */\n+  public boolean isNumNullsSet() {\n+    return num_nulls >= 0;\n+  }\n+\n   /**\n    * Sets the page/column as having a valid non-null value\n    * kind of misnomer here",
                "additions": 72,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-column/src/main/java/org/apache/parquet/column/statistics/Statistics.java",
                "status": "modified",
                "changes": 78,
                "deletions": 6,
                "sha": "a087c5f70e7a5e15e07e1761d0a32b53c3c65d11",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-column/src/main/java/org/apache/parquet/column/statistics/Statistics.java",
                "filename": "parquet-column/src/main/java/org/apache/parquet/column/statistics/Statistics.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-column/src/main/java/org/apache/parquet/column/statistics/Statistics.java?ref=b82d96218bfd37f6df95a2e8d7675d091ab61970"
            },
            {
                "patch": "@@ -42,6 +42,7 @@\n   @Test\n   public void testNumNulls() {\n     IntStatistics stats = new IntStatistics();\n+    assertTrue(stats.isNumNullsSet());\n     assertEquals(stats.getNumNulls(), 0);\n \n     stats.incrementNumNulls();",
                "additions": 1,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-column/src/test/java/org/apache/parquet/column/statistics/TestStatistics.java",
                "status": "modified",
                "changes": 1,
                "deletions": 0,
                "sha": "5e5d5fd26e0df74724d6d793537a3bae717b5f0b",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-column/src/test/java/org/apache/parquet/column/statistics/TestStatistics.java",
                "filename": "parquet-column/src/test/java/org/apache/parquet/column/statistics/TestStatistics.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-column/src/test/java/org/apache/parquet/column/statistics/TestStatistics.java?ref=b82d96218bfd37f6df95a2e8d7675d091ab61970"
            },
            {
                "patch": "@@ -40,7 +40,6 @@\n import org.apache.parquet.filter2.predicate.UserDefinedPredicate;\n import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n \n-import static org.apache.parquet.Preconditions.checkArgument;\n import static org.apache.parquet.Preconditions.checkNotNull;\n \n /**\n@@ -122,6 +121,10 @@ private boolean hasNulls(ColumnChunkMetaData column) {\n     }\n \n     if (value == null) {\n+      // We don't know anything about the nulls in this chunk\n+      if (!stats.isNumNullsSet()) {\n+        return BLOCK_MIGHT_MATCH;\n+      }\n       // we are looking for records where v eq(null)\n       // so drop if there are no nulls in this chunk\n       return !hasNulls(meta);\n@@ -133,6 +136,11 @@ private boolean hasNulls(ColumnChunkMetaData column) {\n       return BLOCK_CANNOT_MATCH;\n     }\n \n+    if (!stats.hasNonNullValue()) {\n+      // stats does not contain min/max values, we cannot drop any chunks\n+      return BLOCK_MIGHT_MATCH;\n+    }\n+\n     // drop if value < min || value > max\n     return stats.compareMinToValue(value) > 0 || stats.compareMaxToValue(value) < 0;\n   }\n@@ -166,12 +174,17 @@ private boolean hasNulls(ColumnChunkMetaData column) {\n       return isAllNulls(meta);\n     }\n \n-    if (hasNulls(meta)) {\n+    if (stats.isNumNullsSet() && hasNulls(meta)) {\n       // we are looking for records where v notEq(someNonNull)\n       // but this chunk contains nulls, we cannot drop it\n       return BLOCK_MIGHT_MATCH;\n     }\n \n+    if (!stats.hasNonNullValue()) {\n+      // stats does not contain min/max values, we cannot drop any chunks\n+      return BLOCK_MIGHT_MATCH;\n+    }\n+\n     // drop if this is a column where min = max = value\n     return stats.compareMinToValue(value) == 0 && stats.compareMaxToValue(value) == 0;\n   }\n@@ -201,6 +214,11 @@ private boolean hasNulls(ColumnChunkMetaData column) {\n       return BLOCK_CANNOT_MATCH;\n     }\n \n+    if (!stats.hasNonNullValue()) {\n+      // stats does not contain min/max values, we cannot drop any chunks\n+      return BLOCK_MIGHT_MATCH;\n+    }\n+\n     T value = lt.getValue();\n \n     // drop if value <= min\n@@ -232,6 +250,11 @@ private boolean hasNulls(ColumnChunkMetaData column) {\n       return BLOCK_CANNOT_MATCH;\n     }\n \n+    if (!stats.hasNonNullValue()) {\n+      // stats does not contain min/max values, we cannot drop any chunks\n+      return BLOCK_MIGHT_MATCH;\n+    }\n+\n     T value = ltEq.getValue();\n \n     // drop if value < min\n@@ -263,6 +286,11 @@ private boolean hasNulls(ColumnChunkMetaData column) {\n       return BLOCK_CANNOT_MATCH;\n     }\n \n+    if (!stats.hasNonNullValue()) {\n+      // stats does not contain min/max values, we cannot drop any chunks\n+      return BLOCK_MIGHT_MATCH;\n+    }\n+\n     T value = gt.getValue();\n \n     // drop if value >= max\n@@ -294,6 +322,11 @@ private boolean hasNulls(ColumnChunkMetaData column) {\n       return BLOCK_CANNOT_MATCH;\n     }\n \n+    if (!stats.hasNonNullValue()) {\n+      // stats does not contain min/max values, we cannot drop any chunks\n+      return BLOCK_MIGHT_MATCH;\n+    }\n+\n     T value = gtEq.getValue();\n \n     // drop if value > max\n@@ -355,6 +388,11 @@ public Boolean visit(Not not) {\n       }\n     }\n \n+    if (!stats.hasNonNullValue()) {\n+      // stats does not contain min/max values, we cannot drop any chunks\n+      return BLOCK_MIGHT_MATCH;\n+    }\n+\n     org.apache.parquet.filter2.predicate.Statistics<T> udpStats =\n       new org.apache.parquet.filter2.predicate.Statistics<T>(stats.genericGetMin(), stats.genericGetMax(),\n         stats.comparator());",
                "additions": 40,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-hadoop/src/main/java/org/apache/parquet/filter2/statisticslevel/StatisticsFilter.java",
                "status": "modified",
                "changes": 42,
                "deletions": 2,
                "sha": "446c8a3d49c76bfc03067d01dc8377198b579585",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-hadoop/src/main/java/org/apache/parquet/filter2/statisticslevel/StatisticsFilter.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/filter2/statisticslevel/StatisticsFilter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/filter2/statisticslevel/StatisticsFilter.java?ref=b82d96218bfd37f6df95a2e8d7675d091ab61970"
            },
            {
                "patch": "@@ -401,17 +401,21 @@ private static boolean isMinMaxStatsSupported(PrimitiveType type) {\n   static org.apache.parquet.column.statistics.Statistics fromParquetStatisticsInternal\n       (String createdBy, Statistics formatStats, PrimitiveType type, SortOrder typeSortOrder) {\n     // create stats object based on the column type\n-    org.apache.parquet.column.statistics.Statistics stats = org.apache.parquet.column.statistics.Statistics.createStats(type);\n+    org.apache.parquet.column.statistics.Statistics.Builder statsBuilder =\n+        org.apache.parquet.column.statistics.Statistics.getBuilder(type);\n \n     if (formatStats != null) {\n       // Use the new V2 min-max statistics over the former one if it is filled\n       if (formatStats.isSetMin_value() && formatStats.isSetMax_value()) {\n         byte[] min = formatStats.min_value.array();\n         byte[] max = formatStats.max_value.array();\n         if (isMinMaxStatsSupported(type) || Arrays.equals(min, max)) {\n-          stats.setMinMaxFromBytes(min, max);\n+          statsBuilder.withMin(min);\n+          statsBuilder.withMax(max);\n+        }\n+        if (formatStats.isSetNull_count()) {\n+          statsBuilder.withNumNulls(formatStats.null_count);\n         }\n-        stats.setNumNulls(formatStats.null_count);\n       } else {\n         boolean isSet = formatStats.isSetMax() && formatStats.isSetMin();\n         boolean maxEqualsMin = isSet ? Arrays.equals(formatStats.getMin(), formatStats.getMax()) : false;\n@@ -424,13 +428,16 @@ private static boolean isMinMaxStatsSupported(PrimitiveType type) {\n         if (!CorruptStatistics.shouldIgnoreStatistics(createdBy, type.getPrimitiveTypeName()) &&\n             (sortOrdersMatch || maxEqualsMin)) {\n           if (isSet) {\n-            stats.setMinMaxFromBytes(formatStats.min.array(), formatStats.max.array());\n+            statsBuilder.withMin(formatStats.min.array());\n+            statsBuilder.withMax(formatStats.max.array());\n+          }\n+          if (formatStats.isSetNull_count()) {\n+            statsBuilder.withNumNulls(formatStats.null_count);\n           }\n-          stats.setNumNulls(formatStats.null_count);\n         }\n       }\n     }\n-    return stats;\n+    return statsBuilder.build();\n   }\n \n   public org.apache.parquet.column.statistics.Statistics fromParquetStatistics(",
                "additions": 13,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java",
                "status": "modified",
                "changes": 19,
                "deletions": 6,
                "sha": "0daabb6fe0d4a0edfb009439e7591ca9a30771e7",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/format/converter/ParquetMetadataConverter.java?ref=b82d96218bfd37f6df95a2e8d7675d091ab61970"
            },
            {
                "patch": "@@ -22,7 +22,6 @@\n import java.util.HashSet;\n import java.util.List;\n \n-import org.apache.parquet.io.api.Binary;\n import org.junit.Test;\n \n import org.apache.parquet.column.Encoding;\n@@ -39,6 +38,7 @@\n import org.apache.parquet.hadoop.metadata.ColumnChunkMetaData;\n import org.apache.parquet.hadoop.metadata.CompressionCodecName;\n import org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName;\n+import org.apache.parquet.schema.Types;\n \n import static org.apache.parquet.filter2.predicate.FilterApi.binaryColumn;\n import static org.apache.parquet.io.api.Binary.fromString;\n@@ -62,7 +62,8 @@\n \n public class TestStatisticsFilter {\n \n-  private static ColumnChunkMetaData getIntColumnMeta(IntStatistics stats, long valueCount) {\n+  private static ColumnChunkMetaData getIntColumnMeta(org.apache.parquet.column.statistics.Statistics<?> stats,\n+      long valueCount) {\n     return ColumnChunkMetaData.get(ColumnPath.get(\"int\", \"column\"),\n         PrimitiveTypeName.INT32,\n         CompressionCodecName.GZIP,\n@@ -71,7 +72,8 @@ private static ColumnChunkMetaData getIntColumnMeta(IntStatistics stats, long va\n         0L, 0L, valueCount, 0L, 0L);\n   }\n \n-  private static ColumnChunkMetaData getDoubleColumnMeta(DoubleStatistics stats, long valueCount) {\n+  private static ColumnChunkMetaData getDoubleColumnMeta(org.apache.parquet.column.statistics.Statistics<?> stats,\n+      long valueCount) {\n     return ColumnChunkMetaData.get(ColumnPath.get(\"double\", \"column\"),\n         PrimitiveTypeName.DOUBLE,\n         CompressionCodecName.GZIP,\n@@ -87,13 +89,16 @@ private static ColumnChunkMetaData getDoubleColumnMeta(DoubleStatistics stats, l\n \n   private static final IntStatistics intStats = new IntStatistics();\n   private static final IntStatistics nullIntStats = new IntStatistics();\n+  private static final org.apache.parquet.column.statistics.Statistics<?> emptyIntStats = org.apache.parquet.column.statistics.Statistics\n+      .getBuilder(Types.required(PrimitiveTypeName.INT32).named(\"test_int32\")).build();\n   private static final DoubleStatistics doubleStats = new DoubleStatistics();\n+  private static final org.apache.parquet.column.statistics.Statistics<?> missingMinMaxDoubleStats = org.apache.parquet.column.statistics.Statistics\n+      .getBuilder(Types.required(PrimitiveTypeName.DOUBLE).named(\"test_double\")).withNumNulls(100).build();\n \n   static {\n     intStats.setMinMax(10, 100);\n     doubleStats.setMinMax(10, 100);\n \n-    nullIntStats.setMinMax(0, 0);\n     nullIntStats.setNumNulls(177);\n   }\n \n@@ -105,6 +110,9 @@ private static ColumnChunkMetaData getDoubleColumnMeta(DoubleStatistics stats, l\n       getIntColumnMeta(nullIntStats, 177L), // column of all nulls\n       getDoubleColumnMeta(doubleStats, 177L));\n \n+  private static final List<ColumnChunkMetaData> missingMinMaxColumnMetas = Arrays.asList(\n+      getIntColumnMeta(emptyIntStats, 177L),                // missing min/max values and numNulls => stats is empty\n+      getDoubleColumnMeta(missingMinMaxDoubleStats, 177L)); // missing min/max, some null values\n \n   @Test\n   public void testEqNonNull() {\n@@ -116,6 +124,9 @@ public void testEqNonNull() {\n     // drop columns of all nulls when looking for non-null value\n     assertTrue(canDrop(eq(intColumn, 0), nullColumnMetas));\n     assertTrue(canDrop(eq(missingColumn, fromString(\"any\")), columnMetas));\n+\n+    assertFalse(canDrop(eq(intColumn, 50), missingMinMaxColumnMetas));\n+    assertFalse(canDrop(eq(doubleColumn, 50.0), missingMinMaxColumnMetas));\n   }\n \n   @Test\n@@ -137,6 +148,9 @@ public void testEqNull() {\n         getDoubleColumnMeta(doubleStats, 177L))));\n \n     assertFalse(canDrop(eq(missingColumn, null), columnMetas));\n+\n+    assertFalse(canDrop(eq(intColumn, null), missingMinMaxColumnMetas));\n+    assertFalse(canDrop(eq(doubleColumn, null), missingMinMaxColumnMetas));\n   }\n \n   @Test\n@@ -163,6 +177,9 @@ public void testNotEqNonNull() {\n         getDoubleColumnMeta(doubleStats, 177L))));\n \n     assertFalse(canDrop(notEq(missingColumn, fromString(\"any\")), columnMetas));\n+\n+    assertFalse(canDrop(notEq(intColumn, 50), missingMinMaxColumnMetas));\n+    assertFalse(canDrop(notEq(doubleColumn, 50.0), missingMinMaxColumnMetas));\n   }\n \n   @Test\n@@ -192,6 +209,9 @@ public void testNotEqNull() {\n         getDoubleColumnMeta(doubleStats, 177L))));\n \n     assertTrue(canDrop(notEq(missingColumn, null), columnMetas));\n+\n+    assertFalse(canDrop(notEq(intColumn, null), missingMinMaxColumnMetas));\n+    assertFalse(canDrop(notEq(doubleColumn, null), missingMinMaxColumnMetas));\n   }\n \n   @Test\n@@ -205,6 +225,9 @@ public void testLt() {\n     assertTrue(canDrop(lt(intColumn, 7), nullColumnMetas));\n \n     assertTrue(canDrop(lt(missingColumn, fromString(\"any\")), columnMetas));\n+\n+    assertFalse(canDrop(lt(intColumn, 0), missingMinMaxColumnMetas));\n+    assertFalse(canDrop(lt(doubleColumn, 0.0), missingMinMaxColumnMetas));\n   }\n \n   @Test\n@@ -218,6 +241,9 @@ public void testLtEq() {\n     assertTrue(canDrop(ltEq(intColumn, 7), nullColumnMetas));\n \n     assertTrue(canDrop(ltEq(missingColumn, fromString(\"any\")), columnMetas));\n+\n+    assertFalse(canDrop(ltEq(intColumn, -1), missingMinMaxColumnMetas));\n+    assertFalse(canDrop(ltEq(doubleColumn, -0.1), missingMinMaxColumnMetas));\n   }\n \n   @Test\n@@ -231,6 +257,9 @@ public void testGt() {\n     assertTrue(canDrop(gt(intColumn, 7), nullColumnMetas));\n \n     assertTrue(canDrop(gt(missingColumn, fromString(\"any\")), columnMetas));\n+\n+    assertFalse(canDrop(gt(intColumn, 0), missingMinMaxColumnMetas));\n+    assertFalse(canDrop(gt(doubleColumn, 0.0), missingMinMaxColumnMetas));\n   }\n \n   @Test\n@@ -244,6 +273,9 @@ public void testGtEq() {\n     assertTrue(canDrop(gtEq(intColumn, 7), nullColumnMetas));\n \n     assertTrue(canDrop(gtEq(missingColumn, fromString(\"any\")), columnMetas));\n+\n+    assertFalse(canDrop(gtEq(intColumn, 1), missingMinMaxColumnMetas));\n+    assertFalse(canDrop(gtEq(doubleColumn, 0.1), missingMinMaxColumnMetas));\n   }\n \n   @Test\n@@ -297,6 +329,26 @@ public boolean keep(Integer value) {\n     }\n   }\n \n+  public static class AllPositiveUdp extends UserDefinedPredicate<Double> {\n+    @Override\n+    public boolean keep(Double value) {\n+      if (value == null) {\n+        return true;\n+      }\n+      throw new RuntimeException(\"this method should not be called with value != null\");\n+    }\n+\n+    @Override\n+    public boolean canDrop(Statistics<Double> statistics) {\n+      return statistics.getMin() <= 0.0;\n+    }\n+\n+    @Override\n+    public boolean inverseCanDrop(Statistics<Double> statistics) {\n+      return statistics.getMin() > 0.0;\n+    }\n+  }\n+\n   @Test\n   public void testUdp() {\n     FilterPredicate pred = userDefined(intColumn, SevensAndEightsUdp.class);\n@@ -308,6 +360,8 @@ public void testUdp() {\n     FilterPredicate udpKeepMissingColumn = userDefined(missingColumn2, SevensAndEightsUdp.class);\n     FilterPredicate invUdpKeepMissingColumn = LogicalInverseRewriter.rewrite(not(userDefined(missingColumn2, SevensAndEightsUdp.class)));\n \n+    FilterPredicate allPositivePred = userDefined(doubleColumn, AllPositiveUdp.class);\n+\n     IntStatistics seven = new IntStatistics();\n     seven.setMinMax(7, 7);\n \n@@ -392,6 +446,8 @@ public void testUdp() {\n     assertTrue(canDrop(invUdpKeepMissingColumn, Arrays.asList(\n         getIntColumnMeta(neither, 177L),\n         getDoubleColumnMeta(doubleStats, 177L))));\n+\n+    assertFalse(canDrop(allPositivePred, missingMinMaxColumnMetas));\n   }\n \n   @Test",
                "additions": 60,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-hadoop/src/test/java/org/apache/parquet/filter2/statisticslevel/TestStatisticsFilter.java",
                "status": "modified",
                "changes": 64,
                "deletions": 4,
                "sha": "6fdec2a6cd25557dc4879d004042967b2b7cc655",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-hadoop/src/test/java/org/apache/parquet/filter2/statisticslevel/TestStatisticsFilter.java",
                "filename": "parquet-hadoop/src/test/java/org/apache/parquet/filter2/statisticslevel/TestStatisticsFilter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/test/java/org/apache/parquet/filter2/statisticslevel/TestStatisticsFilter.java?ref=b82d96218bfd37f6df95a2e8d7675d091ab61970"
            },
            {
                "patch": "@@ -658,6 +658,7 @@ private void testUseStatsWithSignedSortOrder(StatsHelper helper) {\n         binaryType);\n \n     Assert.assertFalse(\"Stats should not be empty\", convertedStats.isEmpty());\n+    Assert.assertTrue(convertedStats.isNumNullsSet());\n     Assert.assertEquals(\"Should have 3 nulls\", 3, convertedStats.getNumNulls());\n     if (helper == StatsHelper.V1) {\n       assertFalse(\"Min-max should be null for V1 stats\", convertedStats.hasNonNullValue());\n@@ -669,6 +670,38 @@ private void testUseStatsWithSignedSortOrder(StatsHelper helper) {\n     }\n   }\n \n+  @Test\n+  public void testMissingValuesFromStats() {\n+    ParquetMetadataConverter converter = new ParquetMetadataConverter();\n+    PrimitiveType type = Types.required(PrimitiveTypeName.INT32).named(\"test_int32\");\n+\n+    org.apache.parquet.format.Statistics formatStats = new org.apache.parquet.format.Statistics();\n+    Statistics<?> stats = converter.fromParquetStatistics(Version.FULL_VERSION, formatStats, type);\n+    assertFalse(stats.isNumNullsSet());\n+    assertFalse(stats.hasNonNullValue());\n+    assertTrue(stats.isEmpty());\n+    assertEquals(-1, stats.getNumNulls());\n+\n+    formatStats.clear();\n+    formatStats.setMin(BytesUtils.intToBytes(-100));\n+    formatStats.setMax(BytesUtils.intToBytes(100));\n+    stats = converter.fromParquetStatistics(Version.FULL_VERSION, formatStats, type);\n+    assertFalse(stats.isNumNullsSet());\n+    assertTrue(stats.hasNonNullValue());\n+    assertFalse(stats.isEmpty());\n+    assertEquals(-1, stats.getNumNulls());\n+    assertEquals(-100, stats.genericGetMin());\n+    assertEquals(100, stats.genericGetMax());\n+\n+    formatStats.clear();\n+    formatStats.setNull_count(2000);\n+    stats = converter.fromParquetStatistics(Version.FULL_VERSION, formatStats, type);\n+    assertTrue(stats.isNumNullsSet());\n+    assertFalse(stats.hasNonNullValue());\n+    assertFalse(stats.isEmpty());\n+    assertEquals(2000, stats.getNumNulls());\n+  }\n+\n   @Test\n   public void testSkippedV2Stats() {\n     testSkippedV2Stats(",
                "additions": 33,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-hadoop/src/test/java/org/apache/parquet/format/converter/TestParquetMetadataConverter.java",
                "status": "modified",
                "changes": 33,
                "deletions": 0,
                "sha": "6cce32ff9584c38116b0f17bc3a6c1e4b16047c7",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-hadoop/src/test/java/org/apache/parquet/format/converter/TestParquetMetadataConverter.java",
                "filename": "parquet-hadoop/src/test/java/org/apache/parquet/format/converter/TestParquetMetadataConverter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/test/java/org/apache/parquet/format/converter/TestParquetMetadataConverter.java?ref=b82d96218bfd37f6df95a2e8d7675d091ab61970"
            },
            {
                "patch": "@@ -60,6 +60,7 @@\n import org.apache.parquet.hadoop.metadata.ParquetMetadata;\n import org.apache.parquet.schema.MessageType;\n import org.apache.parquet.schema.MessageTypeParser;\n+import org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName;\n import org.apache.parquet.schema.Types;\n import org.apache.parquet.bytes.HeapByteBufferAllocator;\n \n@@ -92,7 +93,8 @@ public void test() throws Exception {\n     int v = 3;\n     BytesInput definitionLevels = BytesInput.fromInt(d);\n     BytesInput repetitionLevels = BytesInput.fromInt(r);\n-    Statistics<?> statistics = new BinaryStatistics();\n+    Statistics<?> statistics = Statistics.getBuilder(Types.required(PrimitiveTypeName.BINARY).named(\"test_binary\"))\n+        .build();\n     BytesInput data = BytesInput.fromInt(v);\n     int rowCount = 5;\n     int nullCount = 1;",
                "additions": 3,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestColumnChunkPageWriteStore.java",
                "status": "modified",
                "changes": 4,
                "deletions": 1,
                "sha": "0b7b9517a15f01edd23eed666c102bea25f00683",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestColumnChunkPageWriteStore.java",
                "filename": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestColumnChunkPageWriteStore.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestColumnChunkPageWriteStore.java?ref=b82d96218bfd37f6df95a2e8d7675d091ab61970"
            },
            {
                "patch": "@@ -27,7 +27,6 @@\n import org.apache.parquet.Version;\n import org.apache.parquet.bytes.BytesUtils;\n import org.apache.parquet.hadoop.ParquetOutputFormat.JobSummaryLevel;\n-import org.apache.parquet.hadoop.util.HadoopOutputFile;\n import org.junit.Assume;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -48,6 +47,7 @@\n import org.apache.parquet.schema.MessageTypeParser;\n import org.apache.parquet.schema.PrimitiveType;\n import org.apache.parquet.schema.PrimitiveType.PrimitiveTypeName;\n+import org.apache.parquet.schema.Types;\n \n import java.io.File;\n import java.io.IOException;\n@@ -95,8 +95,8 @@\n   private static final byte[] BYTES4 = { 3, 4, 5, 6 };\n   private static final CompressionCodecName CODEC = CompressionCodecName.UNCOMPRESSED;\n \n-  private static final BinaryStatistics STATS1 = new BinaryStatistics();\n-  private static final BinaryStatistics STATS2 = new BinaryStatistics();\n+  private static final org.apache.parquet.column.statistics.Statistics<?> EMPTY_STATS = org.apache.parquet.column.statistics.Statistics\n+      .getBuilder(Types.required(PrimitiveTypeName.BINARY).named(\"test_binary\")).build();\n \n   private String writeSchema;\n \n@@ -145,24 +145,24 @@ public void testWriteRead() throws Exception {\n     w.startBlock(3);\n     w.startColumn(C1, 5, CODEC);\n     long c1Starts = w.getPos();\n-    w.writeDataPage(2, 4, BytesInput.from(BYTES1), STATS1, BIT_PACKED, BIT_PACKED, PLAIN);\n-    w.writeDataPage(3, 4, BytesInput.from(BYTES1), STATS1, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(2, 4, BytesInput.from(BYTES1), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(3, 4, BytesInput.from(BYTES1), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n     w.endColumn();\n     long c1Ends = w.getPos();\n     w.startColumn(C2, 6, CODEC);\n     long c2Starts = w.getPos();\n-    w.writeDataPage(2, 4, BytesInput.from(BYTES2), STATS2, BIT_PACKED, BIT_PACKED, PLAIN);\n-    w.writeDataPage(3, 4, BytesInput.from(BYTES2), STATS2, BIT_PACKED, BIT_PACKED, PLAIN);\n-    w.writeDataPage(1, 4, BytesInput.from(BYTES2), STATS2, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(2, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(3, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(1, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n     w.endColumn();\n     long c2Ends = w.getPos();\n     w.endBlock();\n     w.startBlock(4);\n     w.startColumn(C1, 7, CODEC);\n-    w.writeDataPage(7, 4, BytesInput.from(BYTES3), STATS1, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(7, 4, BytesInput.from(BYTES3), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n     w.endColumn();\n     w.startColumn(C2, 8, CODEC);\n-    w.writeDataPage(8, 4, BytesInput.from(BYTES4), STATS2, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(8, 4, BytesInput.from(BYTES4), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n     w.endColumn();\n     w.endBlock();\n     w.end(new HashMap<String, String>());\n@@ -225,15 +225,15 @@ public void testAlignmentWithPadding() throws Exception {\n     w.startBlock(3);\n     w.startColumn(C1, 5, CODEC);\n     long c1Starts = w.getPos();\n-    w.writeDataPage(2, 4, BytesInput.from(BYTES1), STATS1, BIT_PACKED, BIT_PACKED, PLAIN);\n-    w.writeDataPage(3, 4, BytesInput.from(BYTES1), STATS1, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(2, 4, BytesInput.from(BYTES1), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(3, 4, BytesInput.from(BYTES1), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n     w.endColumn();\n     long c1Ends = w.getPos();\n     w.startColumn(C2, 6, CODEC);\n     long c2Starts = w.getPos();\n-    w.writeDataPage(2, 4, BytesInput.from(BYTES2), STATS2, BIT_PACKED, BIT_PACKED, PLAIN);\n-    w.writeDataPage(3, 4, BytesInput.from(BYTES2), STATS2, BIT_PACKED, BIT_PACKED, PLAIN);\n-    w.writeDataPage(1, 4, BytesInput.from(BYTES2), STATS2, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(2, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(3, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(1, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n     w.endColumn();\n     long c2Ends = w.getPos();\n     w.endBlock();\n@@ -242,10 +242,10 @@ public void testAlignmentWithPadding() throws Exception {\n \n     w.startBlock(4);\n     w.startColumn(C1, 7, CODEC);\n-    w.writeDataPage(7, 4, BytesInput.from(BYTES3), STATS1, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(7, 4, BytesInput.from(BYTES3), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n     w.endColumn();\n     w.startColumn(C2, 8, CODEC);\n-    w.writeDataPage(8, 4, BytesInput.from(BYTES4), STATS2, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(8, 4, BytesInput.from(BYTES4), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n     w.endColumn();\n     w.endBlock();\n \n@@ -330,15 +330,15 @@ public void testAlignmentWithNoPaddingNeeded() throws Exception {\n     w.startBlock(3);\n     w.startColumn(C1, 5, CODEC);\n     long c1Starts = w.getPos();\n-    w.writeDataPage(2, 4, BytesInput.from(BYTES1), STATS1, BIT_PACKED, BIT_PACKED, PLAIN);\n-    w.writeDataPage(3, 4, BytesInput.from(BYTES1), STATS1, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(2, 4, BytesInput.from(BYTES1), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(3, 4, BytesInput.from(BYTES1), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n     w.endColumn();\n     long c1Ends = w.getPos();\n     w.startColumn(C2, 6, CODEC);\n     long c2Starts = w.getPos();\n-    w.writeDataPage(2, 4, BytesInput.from(BYTES2), STATS2, BIT_PACKED, BIT_PACKED, PLAIN);\n-    w.writeDataPage(3, 4, BytesInput.from(BYTES2), STATS2, BIT_PACKED, BIT_PACKED, PLAIN);\n-    w.writeDataPage(1, 4, BytesInput.from(BYTES2), STATS2, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(2, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(3, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(1, 4, BytesInput.from(BYTES2), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n     w.endColumn();\n     long c2Ends = w.getPos();\n     w.endBlock();\n@@ -347,10 +347,10 @@ public void testAlignmentWithNoPaddingNeeded() throws Exception {\n \n     w.startBlock(4);\n     w.startColumn(C1, 7, CODEC);\n-    w.writeDataPage(7, 4, BytesInput.from(BYTES3), STATS1, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(7, 4, BytesInput.from(BYTES3), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n     w.endColumn();\n     w.startColumn(C2, 8, CODEC);\n-    w.writeDataPage(8, 4, BytesInput.from(BYTES4), STATS2, BIT_PACKED, BIT_PACKED, PLAIN);\n+    w.writeDataPage(8, 4, BytesInput.from(BYTES4), EMPTY_STATS, BIT_PACKED, BIT_PACKED, PLAIN);\n     w.endColumn();\n     w.endBlock();\n ",
                "additions": 24,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestParquetFileWriter.java",
                "status": "modified",
                "changes": 48,
                "deletions": 24,
                "sha": "c73e569271ec0cc3a3dba8402432683edd7a9e19",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b82d96218bfd37f6df95a2e8d7675d091ab61970/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestParquetFileWriter.java",
                "filename": "parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestParquetFileWriter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/test/java/org/apache/parquet/hadoop/TestParquetFileWriter.java?ref=b82d96218bfd37f6df95a2e8d7675d091ab61970"
            }
        ],
        "bug_id": "parquet-mr_15",
        "parent": "https://github.com/apache/parquet-mr/commit/8bbc6cb95fd9b4b9e86c924ca1e40fd555ecac1d",
        "message": "PARQUET-1217: Incorrect handling of missing values in Statistics\n\nIn parquet-format every value in Statistics is optional while parquet-mr does not properly handle these scenarios:\n- null_count is set but min/max or min_value/max_value are not: filtering may fail with NPE or incorrect filtering occurs\n  fix: check if min/max is set before comparing to the related values\n- null_count is not set: filtering handles null_count as if it would be 0 -> incorrect filtering may occur\n  fix: introduce new method in Statistics object to check if num_nulls is set; check if num_nulls is set by the new method before using its value for filtering\n\nAuthor: Gabor Szadovszky <gabor.szadovszky@cloudera.com>\n\nCloses #458 from gszadovszky/PARQUET-1217 and squashes the following commits:\n\n9d14090 [Gabor Szadovszky] Updates according to rdblue's comments\n116d1d3 [Gabor Szadovszky] PARQUET-1217: Updates according to zi's comments\nc264b50 [Gabor Szadovszky] PARQUET-1217: fix handling of unset nullCount\n2ec2fb1 [Gabor Szadovszky] PARQUET-1217: Incorrect handling of missing values in Statistics",
        "repo": "parquet-mr"
    },
    {
        "commit": "https://github.com/apache/parquet-mr/commit/4b5cda5a2c6ca613db5129d50ffffce2604ad9eb",
        "file": [
            {
                "patch": "@@ -54,6 +54,11 @@ public static void writeMetaDataFile(Configuration configuration, Path outputPat\n         final FileSystem fileSystem = outputPath.getFileSystem(configuration);\n         FileStatus outputStatus = fileSystem.getFileStatus(outputPath);\n         List<Footer> footers = ParquetFileReader.readAllFootersInParallel(configuration, outputStatus);\n+        // If there are no footers, _metadata file cannot be written since there is no way to determine schema!\n+        // Onus of writing any summary files lies with the caller in this case.\n+        if (footers.isEmpty()) {\n+          return;\n+        }\n         try {\n           ParquetFileWriter.writeMetadataFile(configuration, outputPath, footers);\n         } catch (Exception e) {",
                "additions": 5,
                "raw_url": "https://github.com/apache/parquet-mr/raw/4b5cda5a2c6ca613db5129d50ffffce2604ad9eb/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputCommitter.java",
                "status": "modified",
                "changes": 5,
                "deletions": 0,
                "sha": "9a0930a904f0eaf8b770e964d97e33c505f771b4",
                "blob_url": "https://github.com/apache/parquet-mr/blob/4b5cda5a2c6ca613db5129d50ffffce2604ad9eb/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputCommitter.java",
                "filename": "parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputCommitter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetOutputCommitter.java?ref=4b5cda5a2c6ca613db5129d50ffffce2604ad9eb"
            }
        ],
        "bug_id": "parquet-mr_16",
        "parent": "https://github.com/apache/parquet-mr/commit/33a2202603e27132fdfff21c902cf07b0ff12073",
        "message": "PARQUET-151: Skip writing _metadata file in case of no footers since schema cannot be determined.\n\nThis fixes npe seen during mergeFooters in such a case.\n For this scenario onus of writing any summary files lies with the caller (It might have some global schema available) So for example spark does it when persisting empty RDD.\n\nAuthor: Yash Datta <Yash.Datta@guavus.com>\n\nCloses #205 from saucam/footer_bug and squashes the following commits:\n\nb2b3ddf [Yash Datta] PARQUET-151: Skip writing _metadata file in case of no footers since schema cannot be determined. This fixes npe seen during mergeFooters in such a case.              For this scenario onus of writing any summary files lies with the caller (It might have some global schema available)",
        "repo": "parquet-mr"
    },
    {
        "commit": "https://github.com/apache/parquet-mr/commit/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74",
        "file": [
            {
                "patch": "@@ -24,7 +24,7 @@\n \n   @Override\n   public void updateStats(Binary value) {\n-    if (this.isEmpty()) {\n+    if (!this.hasNonNullValue()) {\n       initializeStats(value, value);\n     } else {\n       updateStats(value, value);\n@@ -34,7 +34,7 @@ public void updateStats(Binary value) {\n   @Override\n   public void mergeStatisticsMinMax(Statistics stats) {\n     BinaryStatistics binaryStats = (BinaryStatistics)stats;\n-    if (this.isEmpty()) {\n+    if (!this.hasNonNullValue()) {\n       initializeStats(binaryStats.getMin(), binaryStats.getMax());\n     } else {\n       updateStats(binaryStats.getMin(), binaryStats.getMax());\n@@ -60,9 +60,11 @@ public void setMinMaxFromBytes(byte[] minBytes, byte[] maxBytes) {\n \n   @Override\n   public String toString() {\n-    if(!this.isEmpty())\n+    if (this.hasNonNullValue())\n       return String.format(\"min: %s, max: %s, num_nulls: %d\", min.toStringUsingUTF8(), max.toStringUsingUTF8(), this.getNumNulls());\n-    else\n+   else if (!this.isEmpty())\n+      return String.format(\"num_nulls: %d, min/max not defined\", this.getNumNulls());\n+   else\n       return \"no stats for this column\";\n   }\n \n@@ -100,4 +102,4 @@ public void setMinMax(Binary min, Binary max) {\n     this.min = min;\n     this.markAsNotEmpty();\n   }\n-}\n\\ No newline at end of file\n+}",
                "additions": 7,
                "raw_url": "https://github.com/apache/parquet-mr/raw/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-column/src/main/java/parquet/column/statistics/BinaryStatistics.java",
                "status": "modified",
                "changes": 12,
                "deletions": 5,
                "sha": "6ef167835f387a8e06e62e047b1b4d89ff523b05",
                "blob_url": "https://github.com/apache/parquet-mr/blob/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-column/src/main/java/parquet/column/statistics/BinaryStatistics.java",
                "filename": "parquet-column/src/main/java/parquet/column/statistics/BinaryStatistics.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-column/src/main/java/parquet/column/statistics/BinaryStatistics.java?ref=4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74"
            },
            {
                "patch": "@@ -24,7 +24,7 @@\n \n   @Override\n   public void updateStats(boolean value) {\n-    if (this.isEmpty()) {\n+    if (!this.hasNonNullValue()) {\n       initializeStats(value, value);\n     } else {\n       updateStats(value, value);\n@@ -34,7 +34,7 @@ public void updateStats(boolean value) {\n   @Override\n   public void mergeStatisticsMinMax(Statistics stats) {\n     BooleanStatistics boolStats = (BooleanStatistics)stats;\n-    if (this.isEmpty()) {\n+    if (!this.hasNonNullValue()) {\n       initializeStats(boolStats.getMin(), boolStats.getMax());\n     } else {\n       updateStats(boolStats.getMin(), boolStats.getMax());\n@@ -60,9 +60,11 @@ public void setMinMaxFromBytes(byte[] minBytes, byte[] maxBytes) {\n \n   @Override\n   public String toString() {\n-    if(!this.isEmpty())\n+    if (this.hasNonNullValue())\n       return String.format(\"min: %b, max: %b, num_nulls: %d\", min, max, this.getNumNulls());\n-    else\n+    else if(!this.isEmpty())\n+      return String.format(\"num_nulls: %d, min/max not defined\", this.getNumNulls());\n+    else  \n       return \"no stats for this column\";\n   }\n ",
                "additions": 6,
                "raw_url": "https://github.com/apache/parquet-mr/raw/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-column/src/main/java/parquet/column/statistics/BooleanStatistics.java",
                "status": "modified",
                "changes": 10,
                "deletions": 4,
                "sha": "8e5b233fdedeec41d452a0ce7e856cbd292ca7c4",
                "blob_url": "https://github.com/apache/parquet-mr/blob/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-column/src/main/java/parquet/column/statistics/BooleanStatistics.java",
                "filename": "parquet-column/src/main/java/parquet/column/statistics/BooleanStatistics.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-column/src/main/java/parquet/column/statistics/BooleanStatistics.java?ref=4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74"
            },
            {
                "patch": "@@ -24,7 +24,7 @@\n \n   @Override\n   public void updateStats(double value) {\n-    if (this.isEmpty()) {\n+    if (!this.hasNonNullValue()) {\n       initializeStats(value, value);\n     } else {\n       updateStats(value, value);\n@@ -34,7 +34,7 @@ public void updateStats(double value) {\n   @Override\n   public void mergeStatisticsMinMax(Statistics stats) {\n     DoubleStatistics doubleStats = (DoubleStatistics)stats;\n-    if (this.isEmpty()) {\n+    if (!this.hasNonNullValue()) {\n       initializeStats(doubleStats.getMin(), doubleStats.getMax());\n     } else {\n       updateStats(doubleStats.getMin(), doubleStats.getMax());\n@@ -60,8 +60,10 @@ public void setMinMaxFromBytes(byte[] minBytes, byte[] maxBytes) {\n \n   @Override\n   public String toString() {\n-    if(!this.isEmpty())\n+    if(this.hasNonNullValue())\n       return String.format(\"min: %.5f, max: %.5f, num_nulls: %d\", min, max, this.getNumNulls());\n+    else if (!this.isEmpty())\n+      return String.format(\"num_nulls: %d, min/max not defined\", this.getNumNulls());\n     else\n       return \"no stats for this column\";\n   }\n@@ -100,4 +102,4 @@ public void setMinMax(double min, double max) {\n     this.min = min;\n     this.markAsNotEmpty();\n   }\n-}\n\\ No newline at end of file\n+}",
                "additions": 6,
                "raw_url": "https://github.com/apache/parquet-mr/raw/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-column/src/main/java/parquet/column/statistics/DoubleStatistics.java",
                "status": "modified",
                "changes": 10,
                "deletions": 4,
                "sha": "ccbf7009e63de50ce603b61aeb24d085a2faf48c",
                "blob_url": "https://github.com/apache/parquet-mr/blob/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-column/src/main/java/parquet/column/statistics/DoubleStatistics.java",
                "filename": "parquet-column/src/main/java/parquet/column/statistics/DoubleStatistics.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-column/src/main/java/parquet/column/statistics/DoubleStatistics.java?ref=4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74"
            },
            {
                "patch": "@@ -24,7 +24,7 @@\n \n   @Override\n   public void updateStats(float value) {\n-    if (this.isEmpty()) {\n+    if (!this.hasNonNullValue()) {\n       initializeStats(value, value);\n     } else {\n       updateStats(value, value);\n@@ -34,7 +34,7 @@ public void updateStats(float value) {\n   @Override\n   public void mergeStatisticsMinMax(Statistics stats) {\n     FloatStatistics floatStats = (FloatStatistics)stats;\n-    if (this.isEmpty()) {\n+    if (!this.hasNonNullValue()) {\n       initializeStats(floatStats.getMin(), floatStats.getMax());\n     } else {\n       updateStats(floatStats.getMin(), floatStats.getMax());\n@@ -60,8 +60,10 @@ public void setMinMaxFromBytes(byte[] minBytes, byte[] maxBytes) {\n \n   @Override\n   public String toString() {\n-    if(!this.isEmpty())\n+    if (this.hasNonNullValue())\n       return String.format(\"min: %.5f, max: %.5f, num_nulls: %d\", min, max, this.getNumNulls());\n+    else if (!this.isEmpty())\n+      return String.format(\"num_nulls: %d, min/max not defined\", this.getNumNulls());\n     else\n       return \"no stats for this column\";\n   }",
                "additions": 5,
                "raw_url": "https://github.com/apache/parquet-mr/raw/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-column/src/main/java/parquet/column/statistics/FloatStatistics.java",
                "status": "modified",
                "changes": 8,
                "deletions": 3,
                "sha": "54a550d175769e819aa5fbc1855ba69c75230cb1",
                "blob_url": "https://github.com/apache/parquet-mr/blob/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-column/src/main/java/parquet/column/statistics/FloatStatistics.java",
                "filename": "parquet-column/src/main/java/parquet/column/statistics/FloatStatistics.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-column/src/main/java/parquet/column/statistics/FloatStatistics.java?ref=4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74"
            },
            {
                "patch": "@@ -24,7 +24,7 @@\n \n   @Override\n   public void updateStats(int value) {\n-    if (this.isEmpty()) {\n+    if (!this.hasNonNullValue()) {\n       initializeStats(value, value);\n     } else {\n       updateStats(value, value);\n@@ -34,7 +34,7 @@ public void updateStats(int value) {\n   @Override\n   public void mergeStatisticsMinMax(Statistics stats) {\n     IntStatistics intStats = (IntStatistics)stats;\n-    if (this.isEmpty()) {\n+    if (!this.hasNonNullValue()) {\n       initializeStats(intStats.getMin(), intStats.getMax());\n     } else {\n       updateStats(intStats.getMin(), intStats.getMax());\n@@ -60,8 +60,10 @@ public void setMinMaxFromBytes(byte[] minBytes, byte[] maxBytes) {\n \n   @Override\n   public String toString() {\n-    if(!this.isEmpty())\n+    if (this.hasNonNullValue())\n       return String.format(\"min: %d, max: %d, num_nulls: %d\", min, max, this.getNumNulls());\n+    else if (!this.isEmpty())\n+      return String.format(\"num_nulls: %d, min/max is not defined\", this.getNumNulls());\n     else\n       return \"no stats for this column\";\n   }",
                "additions": 5,
                "raw_url": "https://github.com/apache/parquet-mr/raw/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-column/src/main/java/parquet/column/statistics/IntStatistics.java",
                "status": "modified",
                "changes": 8,
                "deletions": 3,
                "sha": "065147235f3098aff753aa867e4f0d4bd57dbd0a",
                "blob_url": "https://github.com/apache/parquet-mr/blob/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-column/src/main/java/parquet/column/statistics/IntStatistics.java",
                "filename": "parquet-column/src/main/java/parquet/column/statistics/IntStatistics.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-column/src/main/java/parquet/column/statistics/IntStatistics.java?ref=4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74"
            },
            {
                "patch": "@@ -24,7 +24,7 @@\n \n   @Override\n   public void updateStats(long value) {\n-    if (this.isEmpty()) {\n+    if (!this.hasNonNullValue()) {\n       initializeStats(value, value);\n     } else {\n       updateStats(value, value);\n@@ -34,7 +34,7 @@ public void updateStats(long value) {\n   @Override\n   public void mergeStatisticsMinMax(Statistics stats) {\n     LongStatistics longStats = (LongStatistics)stats;\n-    if (this.isEmpty()) {\n+    if (!this.hasNonNullValue()) {\n       initializeStats(longStats.getMin(), longStats.getMax());\n     } else {\n       updateStats(longStats.getMin(), longStats.getMax());\n@@ -60,8 +60,10 @@ public void setMinMaxFromBytes(byte[] minBytes, byte[] maxBytes) {\n \n   @Override\n   public String toString() {\n-    if(!this.isEmpty())\n+    if (this.hasNonNullValue())\n       return String.format(\"min: %d, max: %d, num_nulls: %d\", min, max, this.getNumNulls());\n+    else if (!this.isEmpty())\n+      return String.format(\"num_nulls: %d, min/max not defined\", this.getNumNulls());\n     else\n       return \"no stats for this column\";\n   }\n@@ -100,4 +102,4 @@ public void setMinMax(long min, long max) {\n     this.min = min;\n     this.markAsNotEmpty();\n   }\n-}\n\\ No newline at end of file\n+}",
                "additions": 6,
                "raw_url": "https://github.com/apache/parquet-mr/raw/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-column/src/main/java/parquet/column/statistics/LongStatistics.java",
                "status": "modified",
                "changes": 10,
                "deletions": 4,
                "sha": "70407473cada1ce07aef2f8595803f660a83fe62",
                "blob_url": "https://github.com/apache/parquet-mr/blob/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-column/src/main/java/parquet/column/statistics/LongStatistics.java",
                "filename": "parquet-column/src/main/java/parquet/column/statistics/LongStatistics.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-column/src/main/java/parquet/column/statistics/LongStatistics.java?ref=4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74"
            },
            {
                "patch": "@@ -28,11 +28,11 @@\n  */\n public abstract class Statistics<T extends Comparable<T>> {\n \n-  private boolean firstValueAccountedFor;\n+  private boolean hasNonNullValue;\n   private long num_nulls;\n \n   public Statistics() {\n-    firstValueAccountedFor = false;\n+    hasNonNullValue = false;\n     num_nulls = 0;\n   }\n \n@@ -142,7 +142,10 @@ public void mergeStatistics(Statistics stats) {\n \n     if (this.getClass() == stats.getClass()) {\n       incrementNumNulls(stats.getNumNulls());\n-      mergeStatisticsMinMax(stats);\n+      if (stats.hasNonNullValue()) {\n+        mergeStatisticsMinMax(stats);\n+        markAsNotEmpty();\n+      }\n     } else {\n       throw new StatisticsClassException(this.getClass().toString(), stats.getClass().toString());\n     }\n@@ -220,11 +223,22 @@ public void setNumNulls(long nulls) {\n    * @return true if object is empty, false otherwise\n    */\n   public boolean isEmpty() {\n-    return !firstValueAccountedFor;\n+    return !hasNonNullValue && num_nulls == 0;\n   }\n \n+  /**\n+   * Returns whether there have been non-null values added to this statistics\n+   */\n+  public boolean hasNonNullValue() {\n+    return hasNonNullValue;\n+  }\n+ \n+  /**\n+   * Sets the page/column as having a valid non-null value\n+   * kind of misnomer here\n+   */ \n   protected void markAsNotEmpty() {\n-    firstValueAccountedFor = true;\n+    hasNonNullValue = true;\n   }\n }\n ",
                "additions": 19,
                "raw_url": "https://github.com/apache/parquet-mr/raw/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-column/src/main/java/parquet/column/statistics/Statistics.java",
                "status": "modified",
                "changes": 24,
                "deletions": 5,
                "sha": "7920e3ea24304c0d2f31cba472b04413d83c8b1e",
                "blob_url": "https://github.com/apache/parquet-mr/blob/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-column/src/main/java/parquet/column/statistics/Statistics.java",
                "filename": "parquet-column/src/main/java/parquet/column/statistics/Statistics.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-column/src/main/java/parquet/column/statistics/Statistics.java?ref=4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74"
            },
            {
                "patch": "@@ -67,11 +67,13 @@ private ColumnChunkMetaData getColumnChunk(ColumnPath columnPath) {\n   }\n \n   // is this column chunk composed entirely of nulls?\n+  // assumes the column chunk's statistics is not empty\n   private boolean isAllNulls(ColumnChunkMetaData column) {\n     return column.getStatistics().getNumNulls() == column.getValueCount();\n   }\n \n   // are there any nulls in this column chunk?\n+  // assumes the column chunk's statistics is not empty\n   private boolean hasNulls(ColumnChunkMetaData column) {\n     return column.getStatistics().getNumNulls() > 0;\n   }\n@@ -81,6 +83,12 @@ private boolean hasNulls(ColumnChunkMetaData column) {\n     Column<T> filterColumn = eq.getColumn();\n     T value = eq.getValue();\n     ColumnChunkMetaData columnChunk = getColumnChunk(filterColumn.getColumnPath());\n+    Statistics<T> stats = columnChunk.getStatistics();\n+\n+    if (stats.isEmpty()) {\n+      // we have no statistics available, we cannot drop any chunks\n+      return false;\n+    }\n \n     if (value == null) {\n       // we are looking for records where v eq(null)\n@@ -94,8 +102,6 @@ private boolean hasNulls(ColumnChunkMetaData column) {\n       return true;\n     }\n \n-    Statistics<T> stats = columnChunk.getStatistics();\n-\n     // drop if value < min || value > max\n     return value.compareTo(stats.genericGetMin()) < 0 || value.compareTo(stats.genericGetMax()) > 0;\n   }\n@@ -105,6 +111,12 @@ private boolean hasNulls(ColumnChunkMetaData column) {\n     Column<T> filterColumn = notEq.getColumn();\n     T value = notEq.getValue();\n     ColumnChunkMetaData columnChunk = getColumnChunk(filterColumn.getColumnPath());\n+    Statistics<T> stats = columnChunk.getStatistics();\n+\n+    if (stats.isEmpty()) {\n+      // we have no statistics available, we cannot drop any chunks\n+      return false;\n+    }\n \n     if (value == null) {\n       // we are looking for records where v notEq(null)\n@@ -118,8 +130,6 @@ private boolean hasNulls(ColumnChunkMetaData column) {\n       return false;\n     }\n \n-    Statistics<T> stats = columnChunk.getStatistics();\n-\n     // drop if this is a column where min = max = value\n     return value.compareTo(stats.genericGetMin()) == 0 && value.compareTo(stats.genericGetMax()) == 0;\n   }\n@@ -129,15 +139,19 @@ private boolean hasNulls(ColumnChunkMetaData column) {\n     Column<T> filterColumn = lt.getColumn();\n     T value = lt.getValue();\n     ColumnChunkMetaData columnChunk = getColumnChunk(filterColumn.getColumnPath());\n+    Statistics<T> stats = columnChunk.getStatistics();\n+\n+    if (stats.isEmpty()) {\n+      // we have no statistics available, we cannot drop any chunks\n+      return false;\n+    }\n \n     if (isAllNulls(columnChunk)) {\n       // we are looking for records where v < someValue\n       // this chunk is all nulls, so we can drop it\n       return true;\n     }\n \n-    Statistics<T> stats = columnChunk.getStatistics();\n-\n     // drop if value <= min\n     return  value.compareTo(stats.genericGetMin()) <= 0;\n   }\n@@ -147,15 +161,19 @@ private boolean hasNulls(ColumnChunkMetaData column) {\n     Column<T> filterColumn = ltEq.getColumn();\n     T value = ltEq.getValue();\n     ColumnChunkMetaData columnChunk = getColumnChunk(filterColumn.getColumnPath());\n+    Statistics<T> stats = columnChunk.getStatistics();\n+\n+    if (stats.isEmpty()) {\n+      // we have no statistics available, we cannot drop any chunks\n+      return false;\n+    }\n \n     if (isAllNulls(columnChunk)) {\n       // we are looking for records where v <= someValue\n       // this chunk is all nulls, so we can drop it\n       return true;\n     }\n \n-    Statistics<T> stats = columnChunk.getStatistics();\n-\n     // drop if value < min\n     return value.compareTo(stats.genericGetMin()) < 0;\n   }\n@@ -165,15 +183,19 @@ private boolean hasNulls(ColumnChunkMetaData column) {\n     Column<T> filterColumn = gt.getColumn();\n     T value = gt.getValue();\n     ColumnChunkMetaData columnChunk = getColumnChunk(filterColumn.getColumnPath());\n+    Statistics<T> stats = columnChunk.getStatistics();\n+\n+    if (stats.isEmpty()) {\n+      // we have no statistics available, we cannot drop any chunks\n+      return false;\n+    }\n \n     if (isAllNulls(columnChunk)) {\n       // we are looking for records where v > someValue\n       // this chunk is all nulls, so we can drop it\n       return true;\n     }\n \n-    Statistics<T> stats = columnChunk.getStatistics();\n-\n     // drop if value >= max\n     return value.compareTo(stats.genericGetMax()) >= 0;\n   }\n@@ -183,15 +205,19 @@ private boolean hasNulls(ColumnChunkMetaData column) {\n     Column<T> filterColumn = gtEq.getColumn();\n     T value = gtEq.getValue();\n     ColumnChunkMetaData columnChunk = getColumnChunk(filterColumn.getColumnPath());\n+    Statistics<T> stats = columnChunk.getStatistics();\n+\n+    if (stats.isEmpty()) {\n+      // we have no statistics available, we cannot drop any chunks\n+      return false;\n+    }\n \n     if (isAllNulls(columnChunk)) {\n       // we are looking for records where v >= someValue\n       // this chunk is all nulls, so we can drop it\n       return true;\n     }\n \n-    Statistics<T> stats = columnChunk.getStatistics();\n-\n     // drop if value >= max\n     return value.compareTo(stats.genericGetMax()) > 0;\n   }\n@@ -221,6 +247,19 @@ public Boolean visit(Not not) {\n     ColumnChunkMetaData columnChunk = getColumnChunk(filterColumn.getColumnPath());\n     U udp = ud.getUserDefinedPredicate();\n     Statistics<T> stats = columnChunk.getStatistics();\n+\n+    if (stats.isEmpty()) {\n+      // we have no statistics available, we cannot drop any chunks\n+      return false;\n+    }\n+\n+    if (isAllNulls(columnChunk)) {\n+      // there is no min max, there is nothing\n+      // else we can say about this chunk, we\n+      // cannot drop it.\n+      return false;\n+    }\n+\n     parquet.filter2.predicate.Statistics<T> udpStats =\n         new parquet.filter2.predicate.Statistics<T>(stats.genericGetMin(), stats.genericGetMax());\n ",
                "additions": 51,
                "raw_url": "https://github.com/apache/parquet-mr/raw/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-hadoop/src/main/java/parquet/filter2/statisticslevel/StatisticsFilter.java",
                "status": "modified",
                "changes": 63,
                "deletions": 12,
                "sha": "02a22e925726681268cd79b03124538063c65d45",
                "blob_url": "https://github.com/apache/parquet-mr/blob/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-hadoop/src/main/java/parquet/filter2/statisticslevel/StatisticsFilter.java",
                "filename": "parquet-hadoop/src/main/java/parquet/filter2/statisticslevel/StatisticsFilter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/parquet/filter2/statisticslevel/StatisticsFilter.java?ref=4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74"
            },
            {
                "patch": "@@ -234,9 +234,11 @@ public Encoding getEncoding(parquet.column.Encoding encoding) {\n   public static Statistics toParquetStatistics(parquet.column.statistics.Statistics statistics) {\n     Statistics stats = new Statistics();\n     if (!statistics.isEmpty()) {\n-      stats.setMax(statistics.getMaxBytes());\n-      stats.setMin(statistics.getMinBytes());\n       stats.setNull_count(statistics.getNumNulls());\n+      if(statistics.hasNonNullValue()) {\n+        stats.setMax(statistics.getMaxBytes());\n+        stats.setMin(statistics.getMinBytes());\n+     }\n     }\n     return stats;\n   }\n@@ -246,7 +248,9 @@ public static Statistics toParquetStatistics(parquet.column.statistics.Statistic\n     parquet.column.statistics.Statistics stats = parquet.column.statistics.Statistics.getStatsBasedOnType(type);\n     // If there was no statistics written to the footer, create an empty Statistics object and return\n     if (statistics != null) {\n-      stats.setMinMaxFromBytes(statistics.min.array(), statistics.max.array());\n+      if (statistics.isSetMax() && statistics.isSetMin()) {\n+        stats.setMinMaxFromBytes(statistics.min.array(), statistics.max.array());\n+      }\n       stats.setNumNulls(statistics.null_count);\n     }\n     return stats;",
                "additions": 7,
                "raw_url": "https://github.com/apache/parquet-mr/raw/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-hadoop/src/main/java/parquet/format/converter/ParquetMetadataConverter.java",
                "status": "modified",
                "changes": 10,
                "deletions": 3,
                "sha": "b43429bb10560cd34fcf47ea65dd3a9230538090",
                "blob_url": "https://github.com/apache/parquet-mr/blob/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-hadoop/src/main/java/parquet/format/converter/ParquetMetadataConverter.java",
                "filename": "parquet-hadoop/src/main/java/parquet/format/converter/ParquetMetadataConverter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/main/java/parquet/format/converter/ParquetMetadataConverter.java?ref=4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74"
            },
            {
                "patch": "@@ -279,10 +279,10 @@ public void testUdp() {\n   @Test\n   public void testClearExceptionForNots() {\n     List<ColumnChunkMetaData> columnMetas = Arrays.asList(\n-        getIntColumnMeta(new IntStatistics(), 0L),\n-        getDoubleColumnMeta(new DoubleStatistics(), 0L));\n+        getDoubleColumnMeta(new DoubleStatistics(), 0L),\n+        getIntColumnMeta(new IntStatistics(), 0L));\n \n-    FilterPredicate pred = and(eq(intColumn, 17), not(eq(doubleColumn, 12.0)));\n+    FilterPredicate pred = and(not(eq(doubleColumn, 12.0)), eq(intColumn, 17));\n \n     try {\n       canDrop(pred, columnMetas);\n@@ -297,7 +297,7 @@ public void testClearExceptionForNots() {\n   public void testMissingColumn() {\n     List<ColumnChunkMetaData> columnMetas = Arrays.asList(getIntColumnMeta(new IntStatistics(), 0L));\n     try {\n-      canDrop(and(eq(intColumn, 17), eq(doubleColumn, 12.0)), columnMetas);\n+      canDrop(and(eq(doubleColumn, 12.0), eq(intColumn, 17)), columnMetas);\n       fail(\"This should throw\");\n     } catch (IllegalArgumentException e) {\n       assertEquals(\"Column double.column not found in schema!\", e.getMessage());",
                "additions": 4,
                "raw_url": "https://github.com/apache/parquet-mr/raw/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-hadoop/src/test/java/parquet/filter2/statisticslevel/TestStatisticsFilter.java",
                "status": "modified",
                "changes": 8,
                "deletions": 4,
                "sha": "b7ac931e96678fdab00a74ae8afa10585c1f66cd",
                "blob_url": "https://github.com/apache/parquet-mr/blob/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-hadoop/src/test/java/parquet/filter2/statisticslevel/TestStatisticsFilter.java",
                "filename": "parquet-hadoop/src/test/java/parquet/filter2/statisticslevel/TestStatisticsFilter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/test/java/parquet/filter2/statisticslevel/TestStatisticsFilter.java?ref=4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74"
            },
            {
                "patch": "@@ -62,8 +62,14 @@\n import parquet.format.Statistics;\n import parquet.format.converter.ParquetMetadataConverter;\n \n+import parquet.example.data.Group;\n+import parquet.example.data.simple.SimpleGroup;\n+\n+import parquet.hadoop.example.GroupWriteSupport;\n+\n public class TestParquetFileWriter {\n   private static final Log LOG = Log.getLog(TestParquetFileWriter.class);\n+  private String writeSchema;\n \n   @Test\n   public void testWriteRead() throws Exception {\n@@ -310,6 +316,36 @@ public boolean accept(Path p) {\n \n   }\n \n+  @Test\n+  public void testWriteReadStatisticsAllNulls() throws Exception {\n+\n+    File testFile = new File(\"target/test/TestParquetFileWriter/testParquetFile\").getAbsoluteFile();\n+    testFile.delete();\n+\n+    writeSchema = \"message example {\\n\" +\n+            \"required binary content;\\n\" +\n+            \"}\";\n+\n+    Path path = new Path(testFile.toURI());\n+\n+    MessageType schema = MessageTypeParser.parseMessageType(writeSchema);\n+    Configuration configuration = new Configuration();\n+    GroupWriteSupport.setSchema(schema, configuration);\n+\n+    ParquetWriter<Group> writer = new ParquetWriter<Group>(path, configuration, new GroupWriteSupport());\n+   \n+    Group r1 = new SimpleGroup(schema);\n+    writer.write(r1);\n+    writer.close();\n+    \n+    ParquetMetadata readFooter = ParquetFileReader.readFooter(configuration, path);\n+    \n+    // assert the statistics object is not empty\n+    assertTrue((readFooter.getBlocks().get(0).getColumns().get(0).getStatistics().isEmpty()) == false);\n+    // assert the number of nulls are correct for the first block\n+    assertEquals(1, (readFooter.getBlocks().get(0).getColumns().get(0).getStatistics().getNumNulls()));\n+  }\n+\n   private void validateFooters(final List<Footer> metadata) {\n     LOG.debug(metadata);\n     assertEquals(String.valueOf(metadata), 3, metadata.size());",
                "additions": 36,
                "raw_url": "https://github.com/apache/parquet-mr/raw/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-hadoop/src/test/java/parquet/hadoop/TestParquetFileWriter.java",
                "status": "modified",
                "changes": 36,
                "deletions": 0,
                "sha": "5d0b17f3f1b8edfad4566a8e565cf7a30737b268",
                "blob_url": "https://github.com/apache/parquet-mr/blob/4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74/parquet-hadoop/src/test/java/parquet/hadoop/TestParquetFileWriter.java",
                "filename": "parquet-hadoop/src/test/java/parquet/hadoop/TestParquetFileWriter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-hadoop/src/test/java/parquet/hadoop/TestParquetFileWriter.java?ref=4bf9be34a87b51d07e0b0c9e74831bbcdbce0f74"
            }
        ],
        "bug_id": "parquet-mr_17",
        "parent": "https://github.com/apache/parquet-mr/commit/d70fdbc40195077057a1edb14ccd16a26435d007",
        "message": "PARQUET-136: NPE thrown in StatisticsFilter when all values in a string/binary column trunk are null\n\nIn case of all nulls in a binary column, statistics object read from file metadata is empty, and should return true for all nulls check for the column. Even if column has no values, it can be ignored.\n\nThe other way is to fix this behaviour in the writer, but is that what we want ?\n\nAuthor: Yash Datta <Yash.Datta@guavus.com>\nAuthor: Alex Levenson <alexlevenson@twitter.com>\nAuthor: Yash Datta <saucam@gmail.com>\n\nCloses #99 from saucam/npe and squashes the following commits:\n\n5138e44 [Yash Datta] PARQUET-136: Remove unreachable block\nb17cd38 [Yash Datta] Revert \"PARQUET-161: Trigger tests\"\n82209e6 [Yash Datta] PARQUET-161: Trigger tests\naab2f81 [Yash Datta] PARQUET-161: Review comments for the test case\n2217ee2 [Yash Datta] PARQUET-161: Add a test case for checking the correct statistics info is recorded in case of all nulls in a column\nc2f8d6f [Yash Datta] PARQUET-161: Fix the write path to write statistics object in case of only nulls in the column\n97bb517 [Yash Datta] Revert \"revert TestStatisticsFilter.java\"\na06f0d0 [Yash Datta] Merge pull request #1 from isnotinvain/alexlevenson/PARQUET-161-136\nb1001eb [Alex Levenson] Fix statistics isEmpty, handle more edge cases in statistics filter\n0c88be0 [Alex Levenson] revert TestStatisticsFilter.java\n1ac9192 [Yash Datta] PARQUET-136: Its better to not filter chunks for which empty statistics object is returned. Empty statistics can be read in case of 1. pre-statistics files, 2. files written from current writer that has a bug, as it does not write the statistics if column has all nulls\ne5e924e [Yash Datta] Revert \"PARQUET-136: In case of all nulls in a binary column, statistics object read from file metadata is empty, and should return true for all nulls check for the column\"\n8cc5106 [Yash Datta] Revert \"PARQUET-136: fix hasNulls to cater to the case where all values are nulls\"\nc7c126f [Yash Datta] PARQUET-136: fix hasNulls to cater to the case where all values are nulls\n974a22b [Yash Datta] PARQUET-136: In case of all nulls in a binary column, statistics object read from file metadata is empty, and should return true for all nulls check for the column",
        "repo": "parquet-mr"
    },
    {
        "commit": "https://github.com/apache/parquet-mr/commit/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0",
        "file": [
            {
                "patch": "@@ -18,6 +18,8 @@\n  */\n package org.apache.parquet;\n \n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n import org.apache.parquet.SemanticVersion.SemanticVersionParseException;\n import org.apache.parquet.VersionParser.ParsedVersion;\n import org.apache.parquet.VersionParser.VersionParseException;\n@@ -31,6 +33,8 @@\n  * and thus it's statistics should be ignored / not trusted.\n  */\n public class CorruptStatistics {\n+  private static final AtomicBoolean alreadyLogged = new AtomicBoolean(false);\n+\n   private static final Log LOG = Log.getLog(CorruptStatistics.class);\n \n   // the version in which the bug described by jira: PARQUET-251 was fixed\n@@ -52,7 +56,7 @@ public static boolean shouldIgnoreStatistics(String createdBy, PrimitiveTypeName\n     if (Strings.isNullOrEmpty(createdBy)) {\n       // created_by is not populated, which could have been caused by\n       // parquet-mr during the same time as PARQUET-251, see PARQUET-297\n-      LOG.info(\"Ignoring statistics because created_by is null or empty! See PARQUET-251 and PARQUET-297\");\n+      warnOnce(\"Ignoring statistics because created_by is null or empty! See PARQUET-251 and PARQUET-297\");\n       return true;\n     }\n \n@@ -65,16 +69,16 @@ public static boolean shouldIgnoreStatistics(String createdBy, PrimitiveTypeName\n       }\n \n       if (Strings.isNullOrEmpty(version.version)) {\n-        LOG.warn(\"Ignoring statistics because created_by did not contain a semver (see PARQUET-251): \" + createdBy);\n+        warnOnce(\"Ignoring statistics because created_by did not contain a semver (see PARQUET-251): \" + createdBy);\n         return true;\n       }\n \n       SemanticVersion semver = SemanticVersion.parse(version.version);\n \n       if (semver.compareTo(PARQUET_251_FIXED_VERSION) < 0) {\n-        LOG.info(\"Ignoring statistics because this file was created prior to \"\n+        warnOnce(\"Ignoring statistics because this file was created prior to \"\n             + PARQUET_251_FIXED_VERSION\n-            + \", see PARQUET-251\" );\n+            + \", see PARQUET-251\");\n         return true;\n       }\n \n@@ -83,22 +87,30 @@ public static boolean shouldIgnoreStatistics(String createdBy, PrimitiveTypeName\n     } catch (RuntimeException e) {\n       // couldn't parse the created_by field, log what went wrong, don't trust the stats,\n       // but don't make this fatal.\n-      warnParseError(createdBy, e);\n+      warnParseErrorOnce(createdBy, e);\n       return true;\n     } catch (SemanticVersionParseException e) {\n       // couldn't parse the created_by field, log what went wrong, don't trust the stats,\n       // but don't make this fatal.\n-      warnParseError(createdBy, e);\n+      warnParseErrorOnce(createdBy, e);\n       return true;\n     } catch (VersionParseException e) {\n       // couldn't parse the created_by field, log what went wrong, don't trust the stats,\n       // but don't make this fatal.\n-      warnParseError(createdBy, e);\n+      warnParseErrorOnce(createdBy, e);\n       return true;\n     }\n   }\n \n-  private static void warnParseError(String createdBy, Throwable e) {\n-    LOG.warn(\"Ignoring statistics because created_by could not be parsed (see PARQUET-251): \" + createdBy, e);\n+  private static void warnParseErrorOnce(String createdBy, Throwable e) {\n+    if(!alreadyLogged.getAndSet(true)) {\n+      LOG.warn(\"Ignoring statistics because created_by could not be parsed (see PARQUET-251): \" + createdBy, e);\n+    }\n+  }\n+\n+  private static void warnOnce(String message) {\n+    if(!alreadyLogged.getAndSet(true)) {\n+      LOG.warn(message);\n+    }\n   }\n }",
                "additions": 21,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-column/src/main/java/org/apache/parquet/CorruptStatistics.java",
                "status": "modified",
                "changes": 30,
                "deletions": 9,
                "sha": "3869cdac48c8703ca74ec756cef1799e86411932",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-column/src/main/java/org/apache/parquet/CorruptStatistics.java",
                "filename": "parquet-column/src/main/java/org/apache/parquet/CorruptStatistics.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-column/src/main/java/org/apache/parquet/CorruptStatistics.java?ref=b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0"
            },
            {
                "patch": "@@ -61,15 +61,14 @@\n    */\n   private void shouldConvertConsistentlyWithThriftStructConverter(Class scroogeClass) throws ClassNotFoundException {\n       Class<? extends TBase<?, ?>> thriftClass = (Class<? extends TBase<?, ?>>)Class.forName(scroogeClass.getName().replaceFirst(\"org.apache.parquet.scrooge.test\", \"org.apache.parquet.thrift.test\"));\n-      ThriftType.StructType structFromThriftSchemaConverter = new ThriftSchemaConverter().toStructType(thriftClass);\n+      ThriftType.StructType structFromThriftSchemaConverter = ThriftSchemaConverter.toStructType(thriftClass);\n       ThriftType.StructType structFromScroogeSchemaConverter = new ScroogeStructConverter().convert(scroogeClass);\n \n       assertEquals(toParquetSchema(structFromThriftSchemaConverter), toParquetSchema(structFromScroogeSchemaConverter));\n   }\n \n   private MessageType toParquetSchema(ThriftType.StructType struct) {\n-    ThriftSchemaConverter sc = new ThriftSchemaConverter();\n-    return sc.convert(struct);\n+    return ThriftSchemaConverter.convertWithoutProjection(struct);\n   }\n \n   @Test",
                "additions": 2,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-scrooge/src/test/java/org/apache/parquet/scrooge/ScroogeStructConverterTest.java",
                "status": "modified",
                "changes": 5,
                "deletions": 3,
                "sha": "8acbf965f7cbce600292d55cdf2fa51122dd1912",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-scrooge/src/test/java/org/apache/parquet/scrooge/ScroogeStructConverterTest.java",
                "filename": "parquet-scrooge/src/test/java/org/apache/parquet/scrooge/ScroogeStructConverterTest.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-scrooge/src/test/java/org/apache/parquet/scrooge/ScroogeStructConverterTest.java?ref=b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0"
            },
            {
                "patch": "@@ -84,8 +84,7 @@ protected void init(Class<T> thriftClass) {\n     this.thriftClass = thriftClass;\n     this.thriftStruct = getThriftStruct();\n \n-    ThriftSchemaConverter thriftSchemaConverter = new ThriftSchemaConverter();\n-    this.schema = thriftSchemaConverter.convert(thriftStruct);\n+    this.schema = ThriftSchemaConverter.convertWithoutProjection(thriftStruct);\n \n     final Map<String, String> extraMetaData = new ThriftMetaData(thriftClass.getName(), thriftStruct).toExtraMetaData();\n     // adding the Pig schema as it would have been mapped from thrift",
                "additions": 1,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/main/java/org/apache/parquet/hadoop/thrift/AbstractThriftWriteSupport.java",
                "status": "modified",
                "changes": 3,
                "deletions": 2,
                "sha": "5f210d3280dde963cb08cf981d34d430ef061d8f",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/main/java/org/apache/parquet/hadoop/thrift/AbstractThriftWriteSupport.java",
                "filename": "parquet-thrift/src/main/java/org/apache/parquet/hadoop/thrift/AbstractThriftWriteSupport.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-thrift/src/main/java/org/apache/parquet/hadoop/thrift/AbstractThriftWriteSupport.java?ref=b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0"
            },
            {
                "patch": "@@ -47,8 +47,7 @@ public TBaseWriteSupport(Class<T> thriftClass) {\n \n   @Override\n   protected StructType getThriftStruct() {\n-    ThriftSchemaConverter thriftSchemaConverter = new ThriftSchemaConverter();\n-    return thriftSchemaConverter.toStructType((Class<TBase<?, ?>>)thriftClass);\n+    return ThriftSchemaConverter.toStructType(thriftClass);\n   }\n \n   @Override",
                "additions": 1,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/main/java/org/apache/parquet/hadoop/thrift/TBaseWriteSupport.java",
                "status": "modified",
                "changes": 3,
                "deletions": 2,
                "sha": "b45727829deaeec5d41180db775859afedf0747c",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/main/java/org/apache/parquet/hadoop/thrift/TBaseWriteSupport.java",
                "filename": "parquet-thrift/src/main/java/org/apache/parquet/hadoop/thrift/TBaseWriteSupport.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-thrift/src/main/java/org/apache/parquet/hadoop/thrift/TBaseWriteSupport.java?ref=b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0"
            },
            {
                "patch": "@@ -108,9 +108,8 @@ public WriteContext init(Configuration configuration) {\n     } else {\n       thriftClass = TBaseWriteSupport.getThriftClass(configuration);\n     }\n-    ThriftSchemaConverter thriftSchemaConverter = new ThriftSchemaConverter();\n-    this.thriftStruct = thriftSchemaConverter.toStructType(thriftClass);\n-    this.schema = thriftSchemaConverter.convert(thriftStruct);\n+    this.thriftStruct = ThriftSchemaConverter.toStructType(thriftClass);\n+    this.schema = ThriftSchemaConverter.convertWithoutProjection(thriftStruct);\n     if (buffered) {\n       readToWrite = new BufferedProtocolReadToWrite(thriftStruct, errorHandler);\n     } else {",
                "additions": 2,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/main/java/org/apache/parquet/hadoop/thrift/ThriftBytesWriteSupport.java",
                "status": "modified",
                "changes": 5,
                "deletions": 3,
                "sha": "6db769ecb7216b03c03e12d6f7419cce131cde6e",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/main/java/org/apache/parquet/hadoop/thrift/ThriftBytesWriteSupport.java",
                "filename": "parquet-thrift/src/main/java/org/apache/parquet/hadoop/thrift/ThriftBytesWriteSupport.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-thrift/src/main/java/org/apache/parquet/hadoop/thrift/ThriftBytesWriteSupport.java?ref=b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0"
            },
            {
                "patch": "@@ -128,7 +128,6 @@ private static StructType parseDescriptor(String json) {\n \n   @Override\n   public String toString() {\n-    return \"ThriftMetaData\" + toExtraMetaData();\n+    return String.format(\"ThriftMetaData(thriftClassName: %s, descriptor: %s)\", thriftClassName, descriptor);\n   }\n-\n }",
                "additions": 1,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/main/java/org/apache/parquet/thrift/ThriftMetaData.java",
                "status": "modified",
                "changes": 3,
                "deletions": 2,
                "sha": "a89f8d97c1bd5c7bd6e036324e83a68b81f14042",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/main/java/org/apache/parquet/thrift/ThriftMetaData.java",
                "filename": "parquet-thrift/src/main/java/org/apache/parquet/thrift/ThriftMetaData.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-thrift/src/main/java/org/apache/parquet/thrift/ThriftMetaData.java?ref=b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0"
            },
            {
                "patch": "@@ -24,6 +24,7 @@\n import java.util.List;\n import java.util.Map;\n \n+import org.apache.parquet.io.ParquetDecodingException;\n import org.apache.thrift.TException;\n import org.apache.thrift.protocol.TField;\n import org.apache.thrift.protocol.TList;\n@@ -62,7 +63,7 @@\n  */\n public class ThriftRecordConverter<T> extends RecordMaterializer<T> {\n \n-  final ParquetProtocol readFieldEnd = new ParquetProtocol(\"readFieldEnd()\") {\n+  final static ParquetProtocol readFieldEnd = new ParquetProtocol(\"readFieldEnd()\") {\n     @Override\n     public void readFieldEnd() throws TException {\n     }\n@@ -75,7 +76,7 @@ public void readFieldEnd() throws TException {\n    * @author Julien Le Dem\n    *\n    */\n-  class PrimitiveFieldHandler extends PrimitiveConverter {\n+  static class PrimitiveFieldHandler extends PrimitiveConverter {\n \n     private final PrimitiveConverter delegate;\n     private final List<TProtocol> events;\n@@ -154,7 +155,7 @@ public void addLong(long value) {\n    * @author Julien Le Dem\n    *\n    */\n-  class GroupFieldhandler extends GroupConverter {\n+  static class GroupFieldhandler extends GroupConverter {\n \n     private final GroupConverter delegate;\n     private final List<TProtocol> events;\n@@ -203,7 +204,7 @@ public void end() {\n    * @author Julien Le Dem\n    *\n    */\n-  class GroupCounter extends GroupConverter implements Counter {\n+  static class GroupCounter extends GroupConverter implements Counter {\n \n     private final GroupConverter delegate;\n     private int count;\n@@ -246,7 +247,7 @@ public int getCount() {\n    * @author Julien Le Dem\n    *\n    */\n-  class PrimitiveCounter extends PrimitiveConverter implements Counter {\n+  static class PrimitiveCounter extends PrimitiveConverter implements Counter {\n \n     private final PrimitiveConverter delegate;\n     private int count;\n@@ -309,7 +310,7 @@ public int getCount() {\n    * @author Julien Le Dem\n    *\n    */\n-  class FieldPrimitiveConverter extends PrimitiveConverter {\n+  static class FieldPrimitiveConverter extends PrimitiveConverter {\n \n     private final List<TProtocol> events;\n     private ThriftTypeID type;\n@@ -400,7 +401,7 @@ public long readI64() throws TException {\n    * @author Julien Le Dem\n    *\n    */\n-  class FieldStringConverter extends PrimitiveConverter {\n+  static class FieldStringConverter extends PrimitiveConverter {\n \n     private final List<TProtocol> events;\n \n@@ -429,14 +430,15 @@ public ByteBuffer readBinary() throws TException {\n    * @author Julien Le Dem\n    *\n    */\n-  class FieldEnumConverter extends PrimitiveConverter {\n+   static class FieldEnumConverter extends PrimitiveConverter {\n \n     private final List<TProtocol> events;\n-\n-    private Map<Binary, Integer> enumLookup = new HashMap<Binary, Integer>();\n+    private final Map<Binary, Integer> enumLookup = new HashMap<Binary, Integer>();\n+    private final ThriftField field;\n \n     public FieldEnumConverter(List<TProtocol> events, ThriftField field) {\n       this.events = events;\n+      this.field = field;\n       final Iterable<EnumValue> values = ((EnumType)field.getType()).getValues();\n       for (EnumValue enumValue : values) {\n         enumLookup.put(Binary.fromString(enumValue.getName()), enumValue.getId());\n@@ -445,7 +447,16 @@ public FieldEnumConverter(List<TProtocol> events, ThriftField field) {\n \n     @Override\n     public void addBinary(final Binary value) {\n-      final int id = enumLookup.get(value);\n+      final Integer id = enumLookup.get(value);\n+\n+      if (id == null) {\n+        throw new ParquetDecodingException(\"Unrecognized enum value: \"\n+            + value.toStringUsingUTF8()\n+            + \" known values: \"\n+            + enumLookup\n+            + \" in \" + this.field);\n+      }\n+\n       events.add(new ParquetProtocol(\"readI32() enum\") {\n         @Override\n         public int readI32() throws TException {\n@@ -461,7 +472,7 @@ public int readI32() throws TException {\n    * @author Julien Le Dem\n    *\n    */\n-  class MapConverter extends GroupConverter {\n+  static class MapConverter extends GroupConverter {\n \n     private final GroupCounter child;\n     private final List<TProtocol> mapEvents = new ArrayList<TProtocol>();\n@@ -523,7 +534,7 @@ public TMap readMapBegin() throws TException {\n    * @author Julien Le Dem\n    *\n    */\n-  class MapKeyValueConverter extends GroupConverter {\n+  static class MapKeyValueConverter extends GroupConverter {\n \n     private Converter keyConverter;\n     private Converter valueConverter;\n@@ -561,7 +572,7 @@ public void end() {\n    * @author Julien Le Dem\n    *\n    */\n-  class SetConverter extends CollectionConverter {\n+  static class SetConverter extends CollectionConverter {\n \n     final ParquetProtocol readSetEnd = new ParquetProtocol(\"readSetEnd()\") {\n       @Override\n@@ -598,7 +609,7 @@ void collectionEnd() {\n    * @author Julien Le Dem\n    *\n    */\n-  class ListConverter extends CollectionConverter {\n+  static class ListConverter extends CollectionConverter {\n \n     final ParquetProtocol readListEnd = new ParquetProtocol(\"readListEnd()\") {\n       @Override\n@@ -635,7 +646,7 @@ void collectionEnd() {\n    * @author Julien Le Dem\n    *\n    */\n-  abstract class CollectionConverter extends GroupConverter {\n+  static abstract class CollectionConverter extends GroupConverter {\n \n     private final Converter child;\n     private final Counter childCounter;\n@@ -696,7 +707,7 @@ public void end() {\n    * @author Julien Le Dem\n    *\n    */\n-  class StructConverter extends GroupConverter {\n+  static class StructConverter extends GroupConverter {\n \n     private final int schemaSize;\n \n@@ -794,7 +805,7 @@ public ThriftRecordConverter(ThriftReader<T> thriftReader, String name, MessageT\n     this.thriftReader = thriftReader;\n     this.protocol = new ParquetReadProtocol();\n     this.thriftType = thriftType;\n-    MessageType fullSchema = new ThriftSchemaConverter().convert(thriftType);\n+    MessageType fullSchema = ThriftSchemaConverter.convertWithoutProjection(thriftType);\n     missingRequiredFieldsInProjection = hasMissingRequiredFieldInGroupType(requestedParquetSchema, fullSchema);\n     this.structConverter = new StructConverter(rootEvents, requestedParquetSchema, new ThriftField(name, (short)0, Requirement.REQUIRED, thriftType));\n   }\n@@ -863,7 +874,7 @@ public GroupConverter getRootConverter() {\n     return structConverter;\n   }\n \n-  private Converter newConverter(List<TProtocol> events, Type type, ThriftField field) {\n+  private static Converter newConverter(List<TProtocol> events, Type type, ThriftField field) {\n     switch (field.getType().getType()) {\n     case LIST:\n       return new ListConverter(events, type.asGroupType(), field);",
                "additions": 30,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/main/java/org/apache/parquet/thrift/ThriftRecordConverter.java",
                "status": "modified",
                "changes": 49,
                "deletions": 19,
                "sha": "e18b0e6d179bb8e63ebef5be5160913ae8ef0ac5",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/main/java/org/apache/parquet/thrift/ThriftRecordConverter.java",
                "filename": "parquet-thrift/src/main/java/org/apache/parquet/thrift/ThriftRecordConverter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-thrift/src/main/java/org/apache/parquet/thrift/ThriftRecordConverter.java?ref=b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0"
            },
            {
                "patch": "@@ -76,16 +76,23 @@\n class ThriftSchemaConvertVisitor implements ThriftType.StateVisitor<ConvertedField, ThriftSchemaConvertVisitor.State> {\n   private final FieldProjectionFilter fieldProjectionFilter;\n   private final boolean doProjection;\n+  private final boolean keepOneOfEachUnion;\n \n-  private ThriftSchemaConvertVisitor(FieldProjectionFilter fieldProjectionFilter, boolean doProjection) {\n+  private ThriftSchemaConvertVisitor(FieldProjectionFilter fieldProjectionFilter, boolean doProjection, boolean keepOneOfEachUnion) {\n     this.fieldProjectionFilter = checkNotNull(fieldProjectionFilter, \"fieldProjectionFilter\");\n     this.doProjection = doProjection;\n+    this.keepOneOfEachUnion = keepOneOfEachUnion;\n   }\n \n+  @Deprecated\n   public static MessageType convert(StructType struct, FieldProjectionFilter filter) {\n+    return convert(struct, filter, true);\n+  }\n+\n+  public static MessageType convert(StructType struct, FieldProjectionFilter filter, boolean keepOneOfEachUnion) {\n     State state = new State(new FieldsPath(), REPEATED, \"ParquetSchema\");\n \n-    ConvertedField converted = struct.accept(new ThriftSchemaConvertVisitor(filter, true), state);\n+    ConvertedField converted = struct.accept(new ThriftSchemaConvertVisitor(filter, true, keepOneOfEachUnion), state);\n \n     if (!converted.isKeep()) {\n       throw new ThriftProjectionException(\"No columns have been selected\");\n@@ -134,7 +141,7 @@ public ConvertedField visit(MapType mapType, State state) {\n     if (doProjection) {\n       ConvertedField fullConvKey = keyField\n           .getType()\n-          .accept(new ThriftSchemaConvertVisitor(FieldProjectionFilter.ALL_COLUMNS, false), keyState);\n+          .accept(new ThriftSchemaConvertVisitor(FieldProjectionFilter.ALL_COLUMNS, false, keepOneOfEachUnion), keyState);\n \n       if (!fullConvKey.asKeep().getType().equals(convertedKey.asKeep().getType())) {\n         throw new ThriftProjectionException(\"Cannot select only a subset of the fields in a map key, \" +\n@@ -160,7 +167,7 @@ public ConvertedField visit(MapType mapType, State state) {\n     // keep only the key, not the value\n \n     ConvertedField sentinelValue =\n-        valueField.getType().accept(new ThriftSchemaConvertVisitor(new KeepOnlyFirstPrimitiveFilter(), true), valueState);\n+        valueField.getType().accept(new ThriftSchemaConvertVisitor(new KeepOnlyFirstPrimitiveFilter(), true, keepOneOfEachUnion), valueState);\n \n     Type mapField = mapType(\n         state.repetition,\n@@ -181,7 +188,7 @@ private ConvertedField visitListLike(ThriftField listLike, State state, boolean\n       if (isSet && doProjection) {\n         ConvertedField fullConv = listLike\n             .getType()\n-            .accept(new ThriftSchemaConvertVisitor(FieldProjectionFilter.ALL_COLUMNS, false), childState);\n+            .accept(new ThriftSchemaConvertVisitor(FieldProjectionFilter.ALL_COLUMNS, false, keepOneOfEachUnion), childState);\n         if (!converted.asKeep().getType().equals(fullConv.asKeep().getType())) {\n           throw new ThriftProjectionException(\"Cannot select only a subset of the fields in a set, \" +\n               \"for path \" + state.path);\n@@ -210,7 +217,7 @@ public ConvertedField visit(StructType structType, State state) {\n     // special care is taken when converting unions,\n     // because we are actually both converting + projecting in\n     // one pass, and unions need special handling when projecting.\n-    final boolean isUnion = isUnion(structType.getStructOrUnionType());\n+    final boolean needsToKeepOneOfEachUnion = keepOneOfEachUnion && isUnion(structType.getStructOrUnionType());\n \n     boolean hasSentinelUnionColumns = false;\n     boolean hasNonSentinelUnionColumns = false;\n@@ -223,7 +230,7 @@ public ConvertedField visit(StructType structType, State state) {\n \n       ConvertedField converted = child.getType().accept(this, childState);\n \n-      if (isUnion && !converted.isKeep()) {\n+      if (!converted.isKeep() && needsToKeepOneOfEachUnion) {\n         // user is not keeping this \"kind\" of union, but we still need\n         // to keep at least one of the primitives of this union around.\n         // in order to know what \"kind\" of union each record is.\n@@ -232,7 +239,7 @@ public ConvertedField visit(StructType structType, State state) {\n         // re-do the recursion, with a new projection filter that keeps only\n         // the first primitive it encounters\n         ConvertedField firstPrimitive = child.getType().accept(\n-            new ThriftSchemaConvertVisitor(new KeepOnlyFirstPrimitiveFilter(), true), childState);\n+            new ThriftSchemaConvertVisitor(new KeepOnlyFirstPrimitiveFilter(), true, keepOneOfEachUnion), childState);\n \n         convertedChildren.add(firstPrimitive.asKeep().getType().withId(child.getFieldId()));\n         hasSentinelUnionColumns = true;",
                "additions": 15,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/main/java/org/apache/parquet/thrift/ThriftSchemaConvertVisitor.java",
                "status": "modified",
                "changes": 23,
                "deletions": 8,
                "sha": "88effc50e0234e5f1917d36de4a0d9358b5b8930",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/main/java/org/apache/parquet/thrift/ThriftSchemaConvertVisitor.java",
                "filename": "parquet-thrift/src/main/java/org/apache/parquet/thrift/ThriftSchemaConvertVisitor.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-thrift/src/main/java/org/apache/parquet/thrift/ThriftSchemaConvertVisitor.java?ref=b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0"
            },
            {
                "patch": "@@ -56,12 +56,26 @@ public MessageType convert(Class<? extends TBase<?, ?>> thriftClass) {\n     return convert(toStructType(thriftClass));\n   }\n \n+  /**\n+   * struct is assumed to contain valid structOrUnionType metadata when used with this method.\n+   * This method may throw if structOrUnionType is unknown.\n+   *\n+   * Use convertWithoutProjection below to convert a StructType to MessageType\n+   */\n   public MessageType convert(StructType struct) {\n-    MessageType messageType = ThriftSchemaConvertVisitor.convert(struct, fieldProjectionFilter);\n+    MessageType messageType = ThriftSchemaConvertVisitor.convert(struct, fieldProjectionFilter, true);\n     fieldProjectionFilter.assertNoUnmatchedPatterns();\n     return messageType;\n   }\n \n+  /**\n+   * struct is not required to have known structOrUnionType, which is useful\n+   * for converting a StructType from an (older) file schema to a MessageType\n+   */\n+  public static MessageType convertWithoutProjection(StructType struct) {\n+    return ThriftSchemaConvertVisitor.convert(struct, FieldProjectionFilter.ALL_COLUMNS, false);\n+  }\n+\n   public static <T extends TBase<?,?>> StructOrUnionType structOrUnionType(Class<T> klass) {\n     return TUnion.class.isAssignableFrom(klass) ? StructOrUnionType.UNION : StructOrUnionType.STRUCT;\n   }",
                "additions": 15,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/main/java/org/apache/parquet/thrift/ThriftSchemaConverter.java",
                "status": "modified",
                "changes": 16,
                "deletions": 1,
                "sha": "98820c37eedd0b397f0606f8e969f958e874bec2",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/main/java/org/apache/parquet/thrift/ThriftSchemaConverter.java",
                "filename": "parquet-thrift/src/main/java/org/apache/parquet/thrift/ThriftSchemaConverter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-thrift/src/main/java/org/apache/parquet/thrift/ThriftSchemaConverter.java?ref=b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0"
            },
            {
                "patch": "@@ -95,7 +95,7 @@ private static void generateJson(LinkedList<String> arguments) throws ClassNotFo\n     String className = arguments.pollFirst();\n     String storedPath = arguments.pollFirst();\n     File storeDir = new File(storedPath);\n-    ThriftType.StructType structType = new ThriftSchemaConverter().toStructType((Class<? extends TBase<?, ?>>) Class.forName(className));\n+    ThriftType.StructType structType = ThriftSchemaConverter.toStructType((Class<? extends TBase<?, ?>>) Class.forName(className));\n     ObjectMapper mapper = new ObjectMapper();\n \n     String fileName = catName + \".json\";",
                "additions": 1,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/main/java/org/apache/parquet/thrift/struct/CompatibilityRunner.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "b8d577d4a6ebfb087760384310a44aee52ff6d2b",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/main/java/org/apache/parquet/thrift/struct/CompatibilityRunner.java",
                "filename": "parquet-thrift/src/main/java/org/apache/parquet/thrift/struct/CompatibilityRunner.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-thrift/src/main/java/org/apache/parquet/thrift/struct/CompatibilityRunner.java?ref=b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0"
            },
            {
                "patch": "@@ -92,7 +92,7 @@ public void testMapSet() throws Exception {\n \n   private void writeReadCompare(TBase<?, ?> a)\n           throws TException, InstantiationException, IllegalAccessException {\n-    ProtocolPipe[] pipes = {new ProtocolReadToWrite(), new BufferedProtocolReadToWrite(new ThriftSchemaConverter().toStructType((Class<TBase<?, ?>>)a.getClass()))};\n+    ProtocolPipe[] pipes = {new ProtocolReadToWrite(), new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType((Class<TBase<?, ?>>)a.getClass()))};\n     for (ProtocolPipe p : pipes) {\n       final ByteArrayOutputStream in = new ByteArrayOutputStream();\n       final ByteArrayOutputStream out = new ByteArrayOutputStream();\n@@ -110,7 +110,7 @@ public void testIncompatibleSchemaRecord() throws Exception {\n     //handler will rethrow the exception for verifying purpose\n     CountingErrorHandler countingHandler = new CountingErrorHandler();\n \n-    BufferedProtocolReadToWrite p = new BufferedProtocolReadToWrite(new ThriftSchemaConverter().toStructType(AddressBook.class), countingHandler);\n+    BufferedProtocolReadToWrite p = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(AddressBook.class), countingHandler);\n \n     final ByteArrayOutputStream in = new ByteArrayOutputStream();\n     final ByteArrayOutputStream out = new ByteArrayOutputStream();\n@@ -134,7 +134,7 @@ public void testIncompatibleSchemaRecord() throws Exception {\n   @Test\n   public void testUnrecognizedUnionMemberSchema() throws Exception {\n     CountingErrorHandler countingHandler = new CountingErrorHandler();\n-    BufferedProtocolReadToWrite p = new BufferedProtocolReadToWrite(new ThriftSchemaConverter().toStructType(StructWithUnionV1.class), countingHandler);\n+    BufferedProtocolReadToWrite p = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(StructWithUnionV1.class), countingHandler);\n     final ByteArrayOutputStream in = new ByteArrayOutputStream();\n     final ByteArrayOutputStream out = new ByteArrayOutputStream();\n     StructWithUnionV1 validUnion = new StructWithUnionV1(\"a valid struct\", UnionV1.aLong(new ALong(17L)));\n@@ -164,7 +164,7 @@ public void testUnrecognizedUnionMemberSchema() throws Exception {\n   @Test\n   public void testUnionWithExtraOrNoValues() throws Exception {\n     CountingErrorHandler countingHandler = new CountingErrorHandler();\n-    BufferedProtocolReadToWrite p = new BufferedProtocolReadToWrite(new ThriftSchemaConverter().toStructType(StructWithUnionV2.class), countingHandler);\n+    BufferedProtocolReadToWrite p = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(StructWithUnionV2.class), countingHandler);\n     ByteArrayOutputStream in = new ByteArrayOutputStream();\n     final ByteArrayOutputStream out = new ByteArrayOutputStream();\n \n@@ -229,7 +229,7 @@ public void testUnionWithExtraOrNoValues() throws Exception {\n   @Test\n   public void testEnumMissingSchema() throws Exception {\n     CountingErrorHandler countingHandler = new CountingErrorHandler();\n-    BufferedProtocolReadToWrite p = new BufferedProtocolReadToWrite(new ThriftSchemaConverter().toStructType(StructWithEnum.class), countingHandler);\n+    BufferedProtocolReadToWrite p = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(StructWithEnum.class), countingHandler);\n     final ByteArrayOutputStream in = new ByteArrayOutputStream();\n     final ByteArrayOutputStream out = new ByteArrayOutputStream();\n     StructWithMoreEnum enumDefinedInOldDefinition = new StructWithMoreEnum(NumberEnumWithMoreValue.THREE);\n@@ -268,7 +268,7 @@ public void handleFieldIgnored(TField field) {\n         fieldIgnoredCount++;\n       }\n     };\n-    BufferedProtocolReadToWrite structForRead = new BufferedProtocolReadToWrite(new ThriftSchemaConverter().toStructType(StructV3.class), countingHandler);\n+    BufferedProtocolReadToWrite structForRead = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(StructV3.class), countingHandler);\n \n     //Data has an extra field of type struct\n     final ByteArrayOutputStream in = new ByteArrayOutputStream();\n@@ -306,7 +306,7 @@ public void handleFieldIgnored(TField field) {\n       }\n     };\n \n-    BufferedProtocolReadToWrite structForRead = new BufferedProtocolReadToWrite(new ThriftSchemaConverter().toStructType(StructWithIndexStartsFrom4.class), countingHandler);\n+    BufferedProtocolReadToWrite structForRead = new BufferedProtocolReadToWrite(ThriftSchemaConverter.toStructType(StructWithIndexStartsFrom4.class), countingHandler);\n \n     //Data has an extra field of type struct\n     final ByteArrayOutputStream in = new ByteArrayOutputStream();",
                "additions": 7,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/test/java/org/apache/parquet/thrift/TestProtocolReadToWrite.java",
                "status": "modified",
                "changes": 14,
                "deletions": 7,
                "sha": "ba27166ae31d70f122b36ddc7144d8cb295350bf",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/test/java/org/apache/parquet/thrift/TestProtocolReadToWrite.java",
                "filename": "parquet-thrift/src/test/java/org/apache/parquet/thrift/TestProtocolReadToWrite.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-thrift/src/test/java/org/apache/parquet/thrift/TestProtocolReadToWrite.java?ref=b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0"
            },
            {
                "patch": "@@ -0,0 +1,55 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.thrift;\n+\n+import java.util.ArrayList;\n+\n+import org.apache.parquet.thrift.struct.ThriftField;\n+import org.apache.parquet.thrift.struct.ThriftType.StructType;\n+import org.apache.parquet.thrift.struct.ThriftType.StructType.StructOrUnionType;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+public class TestThriftMetaData {\n+\n+  /**\n+   * Previously, ThriftMetaData.toString would try to instantiate thriftClassName,\n+   * but there is no guarantee that that class is on the classpath, and it is in fact\n+   * normal for that to be the case (for example, when a file was written with TBase objects\n+   * but is being read with scrooge objects).\n+   *\n+   * See PARQUET-345\n+   */\n+  @Test\n+  public void testToStringDoesNotThrow() {\n+\n+    StructType descriptor = new StructType(new ArrayList<ThriftField>(), StructOrUnionType.STRUCT);\n+    ThriftMetaData tmd = new ThriftMetaData(\"non existent class!!!\", descriptor);\n+    assertEquals(\"ThriftMetaData(thriftClassName: non existent class!!!, descriptor: {\\n\" +\n+        \"  \\\"id\\\" : \\\"STRUCT\\\",\\n\" +\n+        \"  \\\"children\\\" : [ ],\\n\" +\n+        \"  \\\"structOrUnionType\\\" : \\\"STRUCT\\\"\\n\" +\n+        \"})\", tmd.toString());\n+\n+    tmd = new ThriftMetaData(\"non existent class!!!\", null);\n+    assertEquals(\"ThriftMetaData(thriftClassName: non existent class!!!, descriptor: null)\", tmd.toString());\n+\n+  }\n+}",
                "additions": 55,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/test/java/org/apache/parquet/thrift/TestThriftMetaData.java",
                "status": "added",
                "changes": 55,
                "deletions": 0,
                "sha": "e7f42ce24870ef4897cfe6ed57fe2d947ad56fff",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/test/java/org/apache/parquet/thrift/TestThriftMetaData.java",
                "filename": "parquet-thrift/src/test/java/org/apache/parquet/thrift/TestThriftMetaData.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-thrift/src/test/java/org/apache/parquet/thrift/TestThriftMetaData.java?ref=b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0"
            },
            {
                "patch": "@@ -0,0 +1,101 @@\n+/* \n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ * \n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ * \n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.parquet.thrift;\n+\n+import java.io.File;\n+import java.nio.charset.Charset;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+\n+import org.apache.parquet.Files;\n+import org.apache.parquet.Strings;\n+import org.apache.parquet.io.ParquetDecodingException;\n+import org.apache.parquet.io.api.Binary;\n+import org.apache.parquet.thrift.ThriftRecordConverter.FieldEnumConverter;\n+import org.apache.parquet.thrift.struct.ThriftField;\n+import org.apache.parquet.thrift.struct.ThriftField.Requirement;\n+import org.apache.parquet.thrift.struct.ThriftType;\n+import org.apache.parquet.thrift.struct.ThriftType.EnumType;\n+import org.apache.parquet.thrift.struct.ThriftType.EnumValue;\n+import org.apache.parquet.thrift.struct.ThriftType.StructType;\n+import org.apache.parquet.thrift.test.compat.StructWithUnionV1;\n+import org.apache.thrift.TException;\n+import org.apache.thrift.protocol.TProtocol;\n+import org.junit.Test;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.junit.Assert.fail;\n+\n+public class TestThriftRecordConverter {\n+  @Test\n+  public void testUnknownEnumThrowsGoodException() throws Exception {\n+    EnumType et = new EnumType(Arrays.asList(new EnumValue(77, \"hello\")));\n+    ThriftField field = new ThriftField(\"name\", (short) 1, Requirement.REQUIRED, et);\n+\n+    ArrayList<TProtocol> events = new ArrayList<TProtocol>();\n+\n+    FieldEnumConverter conv = new  FieldEnumConverter(events, field);\n+\n+    conv.addBinary(Binary.fromString(\"hello\"));\n+\n+    assertEquals(1, events.size());\n+    assertEquals(77, events.get(0).readI32());\n+\n+    try {\n+      conv.addBinary(Binary.fromString(\"FAKE_ENUM_VALUE\"));\n+      fail(\"this should throw\");\n+    } catch (ParquetDecodingException e) {\n+      assertEquals(\"Unrecognized enum value: FAKE_ENUM_VALUE known values: {Binary{\\\"hello\\\"}=77} in {\\n\" +\n+          \"  \\\"name\\\" : \\\"name\\\",\\n\" +\n+          \"  \\\"fieldId\\\" : 1,\\n\" +\n+          \"  \\\"requirement\\\" : \\\"REQUIRED\\\",\\n\" +\n+          \"  \\\"type\\\" : {\\n\" +\n+          \"    \\\"id\\\" : \\\"ENUM\\\",\\n\" +\n+          \"    \\\"values\\\" : [ {\\n\" +\n+          \"      \\\"id\\\" : 77,\\n\" +\n+          \"      \\\"name\\\" : \\\"hello\\\"\\n\" +\n+          \"    } ]\\n\" +\n+          \"  }\\n\" +\n+          \"}\", e.getMessage());\n+    }\n+  }\n+\n+  @Test\n+  public void constructorDoesNotRequireStructOrUnionTypeMeta() throws Exception {\n+    String jsonWithNoStructOrUnionMeta = Strings.join(\n+        Files.readAllLines(\n+            new File(\"src/test/resources/org/apache/parquet/thrift/StructWithUnionV1NoStructOrUnionMeta.json\"),\n+            Charset.forName(\"UTF-8\")), \"\\n\");\n+\n+    StructType noStructOrUnionMeta  = (StructType) ThriftType.fromJSON(jsonWithNoStructOrUnionMeta);\n+\n+    // this used to throw, see PARQUET-346\n+    new ThriftRecordConverter<StructWithUnionV1>(\n+        new ThriftReader<StructWithUnionV1>() {\n+          @Override\n+          public StructWithUnionV1 readOneRecord(TProtocol protocol) throws TException {\n+            return null;\n+          }\n+        },\n+        \"name\",\n+        new ThriftSchemaConverter().convert(StructWithUnionV1.class),\n+        noStructOrUnionMeta\n+    );\n+  }\n+}",
                "additions": 101,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/test/java/org/apache/parquet/thrift/TestThriftRecordConverter.java",
                "status": "added",
                "changes": 101,
                "deletions": 0,
                "sha": "1619dd57e8317a737205ce750df906ffb260137c",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/test/java/org/apache/parquet/thrift/TestThriftRecordConverter.java",
                "filename": "parquet-thrift/src/test/java/org/apache/parquet/thrift/TestThriftRecordConverter.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-thrift/src/test/java/org/apache/parquet/thrift/TestThriftRecordConverter.java?ref=b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0"
            },
            {
                "patch": "@@ -154,7 +154,7 @@ public void testStringList() throws Exception {\n     final Class<T> class1 = (Class<T>) o.getClass();\n     final MessageType schema = thriftSchemaConverter.convert(class1);\n \n-    final StructType structType = thriftSchemaConverter.toStructType(class1);\n+    final StructType structType = ThriftSchemaConverter.toStructType(class1);\n     final ThriftToPig<T> thriftToPig = new ThriftToPig<T>(class1);\n     final Schema pigSchema = thriftToPig.toSchema();\n     final TupleRecordMaterializer tupleRecordConverter = new TupleRecordMaterializer(schema, pigSchema, true);",
                "additions": 1,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/test/java/org/apache/parquet/thrift/TestThriftToPigCompatibility.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "c320f71378161a1883a29ffcd565f09aa4d39083",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/test/java/org/apache/parquet/thrift/TestThriftToPigCompatibility.java",
                "filename": "parquet-thrift/src/test/java/org/apache/parquet/thrift/TestThriftToPigCompatibility.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-thrift/src/test/java/org/apache/parquet/thrift/TestThriftToPigCompatibility.java?ref=b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0"
            },
            {
                "patch": "@@ -116,7 +116,7 @@ public void testEmptyStruct() {\n   }\n \n   private ThriftType.StructType struct(Class thriftClass) {\n-    return new ThriftSchemaConverter().toStructType(thriftClass);\n+    return ThriftSchemaConverter.toStructType(thriftClass);\n   }\n \n   private CompatibilityReport getCompatibilityReport(Class oldClass, Class newClass) {",
                "additions": 1,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/test/java/org/apache/parquet/thrift/struct/CompatibilityCheckerTest.java",
                "status": "modified",
                "changes": 2,
                "deletions": 1,
                "sha": "df034bafc108c7fa7707f2693ed870836f4be2c3",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/test/java/org/apache/parquet/thrift/struct/CompatibilityCheckerTest.java",
                "filename": "parquet-thrift/src/test/java/org/apache/parquet/thrift/struct/CompatibilityCheckerTest.java",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-thrift/src/test/java/org/apache/parquet/thrift/struct/CompatibilityCheckerTest.java?ref=b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0"
            },
            {
                "patch": "@@ -0,0 +1,49 @@\n+{\n+  \"id\" : \"STRUCT\",\n+  \"children\" : [ {\n+    \"name\" : \"name\",\n+    \"fieldId\" : 1,\n+    \"requirement\" : \"REQUIRED\",\n+    \"type\" : {\n+      \"id\" : \"STRING\"\n+    }\n+  }, {\n+    \"name\" : \"aUnion\",\n+    \"fieldId\" : 2,\n+    \"requirement\" : \"REQUIRED\",\n+    \"type\" : {\n+      \"id\" : \"STRUCT\",\n+      \"children\" : [ {\n+        \"name\" : \"aString\",\n+        \"fieldId\" : 1,\n+        \"requirement\" : \"DEFAULT\",\n+        \"type\" : {\n+          \"id\" : \"STRUCT\",\n+          \"children\" : [ {\n+            \"name\" : \"s\",\n+            \"fieldId\" : 1,\n+            \"requirement\" : \"REQUIRED\",\n+            \"type\" : {\n+              \"id\" : \"STRING\"\n+            }\n+          } ]\n+        }\n+      }, {\n+        \"name\" : \"aLong\",\n+        \"fieldId\" : 2,\n+        \"requirement\" : \"DEFAULT\",\n+        \"type\" : {\n+          \"id\" : \"STRUCT\",\n+          \"children\" : [ {\n+            \"name\" : \"l\",\n+            \"fieldId\" : 1,\n+            \"requirement\" : \"REQUIRED\",\n+            \"type\" : {\n+              \"id\" : \"I64\"\n+            }\n+          } ]\n+        }\n+      } ]\n+    }\n+  } ]\n+}\n\\ No newline at end of file",
                "additions": 49,
                "raw_url": "https://github.com/apache/parquet-mr/raw/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/test/resources/org/apache/parquet/thrift/StructWithUnionV1NoStructOrUnionMeta.json",
                "status": "added",
                "changes": 49,
                "deletions": 0,
                "sha": "ac42b76a6e93ca794ab01d59bc3b8eb5160c1369",
                "blob_url": "https://github.com/apache/parquet-mr/blob/b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0/parquet-thrift/src/test/resources/org/apache/parquet/thrift/StructWithUnionV1NoStructOrUnionMeta.json",
                "filename": "parquet-thrift/src/test/resources/org/apache/parquet/thrift/StructWithUnionV1NoStructOrUnionMeta.json",
                "contents_url": "https://api.github.com/repos/apache/parquet-mr/contents/parquet-thrift/src/test/resources/org/apache/parquet/thrift/StructWithUnionV1NoStructOrUnionMeta.json?ref=b86f68e39dc7b6a7c2bff1e4fea3bb7c28d103f0"
            }
        ],
        "bug_id": "parquet-mr_18",
        "parent": "https://github.com/apache/parquet-mr/commit/454fc3655509f1f4f47ce44acaff7c1566ede108",
        "message": "PARQUET-346: Minor fixes for PARQUET-350, PARQUET-348, PARQUET-346, PARQUET-345\n\nPARQUET-346:\nThriftSchemaConverter throws for unknown struct or union type\nThis is triggered when passing a StructType that comes from old file metadata\n\nPARQUET-350:\nThriftRecordConverter throws NPE for unrecognized enum values\nThis is just some better error reporting.\n\nPARQUET-348:\nshouldIgnoreStatistics too noisy\nThis is just a case of way over logging something, to the point that it make the logs unreadable\n\nPARQUET-345\nThriftMetaData toString() should not try to load class reflectively\nThis is a case where the error reporting itself crashes, which results in the real error message getting lost\n\nAuthor: Alex Levenson <alexlevenson@twitter.com>\n\nCloses #252 from isnotinvain/alexlevenson/various-fixes and squashes the following commits:\n\n9b5cb0e [Alex Levenson] Add comments, cleanup some minor use of ThriftSchemaConverter\n376343e [Alex Levenson] Fix test\nd9d5dad [Alex Levenson] add license headers\ne26dc0c [Alex Levenson] Add tests\n8d9dde0 [Alex Levenson] Fixes for PARQUET-350, PARQUET-348, PARQUET-346, PARQUET-345",
        "repo": "parquet-mr"
    }
]