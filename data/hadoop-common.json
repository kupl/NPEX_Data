{
    "hadoop-common_0a0dc3f": {
        "bug_id": "hadoop-common_0a0dc3f",
        "commit": "https://github.com/apache/hadoop-common/commit/0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -289,6 +289,9 @@ Trunk (unreleased changes)\n     HADOOP-6991.  Fix SequenceFile::Reader to honor file lengths and call\n     openFile (cdouglas via omalley)\n \n+    HADOOP-7011.  Fix KerberosName.main() to not throw an NPE.\n+    (Aaron T. Myers via tomwhite)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop-common/raw/0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2/CHANGES.txt",
                "sha": "e607a0c1062c4eef025df0df61256d286a37a9af",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2/src/java/org/apache/hadoop/security/KerberosName.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/security/KerberosName.java?ref=0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/security/KerberosName.java",
                "patch": "@@ -399,9 +399,10 @@ static void printRules() throws IOException {\n   }\n \n   public static void main(String[] args) throws Exception {\n+    setConfiguration(new Configuration());\n     for(String arg: args) {\n       KerberosName name = new KerberosName(arg);\n       System.out.println(\"Name: \" + name + \" to \" + name.getShortName());\n     }\n   }\n-}\n\\ No newline at end of file\n+}",
                "raw_url": "https://github.com/apache/hadoop-common/raw/0a0dc3f01bed08cbbef863e45c4c9f441c50b2a2/src/java/org/apache/hadoop/security/KerberosName.java",
                "sha": "b533cd22f77d6dbf0aaa0a42edf6338dc6d987c6",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7011.  Fix KerberosName.main() to not throw an NPE.  Contributed by Aaron T. Myers.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1028938 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/22f7d55eb833d49fa0cba03b5275d4462d5b2c97",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestKerberosName.java"
        ]
    },
    "hadoop-common_11131d7": {
        "bug_id": "hadoop-common_11131d7",
        "commit": "https://github.com/apache/hadoop-common/commit/11131d72040c250f897a8d1c130b83ce1b065050",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/11131d72040c250f897a8d1c130b83ce1b065050/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=11131d72040c250f897a8d1c130b83ce1b065050",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -2198,3 +2198,5 @@ Release 0.21.0 - 2010-08-13\n \n     MAPREDUCE-1856. Extract a subset of tests for smoke (DOA) validation (cos)\n \n+    MAPREDUCE-2317. Fix a NPE in HadoopArchives.  (Devaraj K via szetszwo)\n+",
                "raw_url": "https://github.com/apache/hadoop-common/raw/11131d72040c250f897a8d1c130b83ce1b065050/CHANGES.txt",
                "sha": "9f26015d60ab79f23ebb727a7c2cdc543fbbf831",
                "status": "modified"
            },
            {
                "additions": 11,
                "blob_url": "https://github.com/apache/hadoop-common/blob/11131d72040c250f897a8d1c130b83ce1b065050/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
                "changes": 20,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/tools/org/apache/hadoop/tools/HadoopArchives.java?ref=11131d72040c250f897a8d1c130b83ce1b065050",
                "deletions": 9,
                "filename": "src/tools/org/apache/hadoop/tools/HadoopArchives.java",
                "patch": "@@ -367,16 +367,18 @@ private void writeTopLevelDirs(SequenceFile.Writer srcWriter,\n         }\n         else {\n           Path parent = p.getParent();\n-          if (allpaths.containsKey(parent.toString())) {\n-            HashSet<String> children = allpaths.get(parent.toString());\n-            children.add(p.getName());\n-          }\n-          else {\n-            HashSet<String> children = new HashSet<String>();\n-            children.add(p.getName());\n-            allpaths.put(parent.toString(), children);\n+          if (null != parent) {\n+            if (allpaths.containsKey(parent.toString())) {\n+              HashSet<String> children = allpaths.get(parent.toString());\n+              children.add(p.getName());\n+            } \n+            else {\n+              HashSet<String> children = new HashSet<String>();\n+              children.add(p.getName());\n+              allpaths.put(parent.toString(), children);\n+            }\n+            parents.add(parent);\n           }\n-          parents.add(parent);\n         }\n       }\n       justDirs = parents;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/11131d72040c250f897a8d1c130b83ce1b065050/src/tools/org/apache/hadoop/tools/HadoopArchives.java",
                "sha": "41d149ea952f98e877d56de2deb3c5ba482ae120",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-2317. Fix a NPE in HadoopArchives.  Contributed by Devaraj K\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@1096022 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/ac3445f84c3699f5032374587c3c80087b598e0f",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestHadoopArchives.java"
        ]
    },
    "hadoop-common_1bd18c3": {
        "bug_id": "hadoop-common_1bd18c3",
        "commit": "https://github.com/apache/hadoop-common/commit/1bd18c32a5309ab58f971eaee74dc935b59ae9c3",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1bd18c32a5309ab58f971eaee74dc935b59ae9c3/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=1bd18c32a5309ab58f971eaee74dc935b59ae9c3",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -37,6 +37,9 @@ Trunk (unreleased changes)\n \n   BUG FIXES\n \n+    MAPREDUCE-1089. Fix NPE in fair scheduler preemption when tasks are\n+    scheduled but not running. (Todd Lipcon via matei)\n+\n     MAPREDUCE-1014. Fix the libraries for common and hdfs. (omalley)\n \n     MAPREDUCE-1111. JT Jetty UI not working if we run mumak.sh ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1bd18c32a5309ab58f971eaee74dc935b59ae9c3/CHANGES.txt",
                "sha": "0571389832cb58552b200f56fa0479f7524b073d",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1bd18c32a5309ab58f971eaee74dc935b59ae9c3/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java?ref=1bd18c32a5309ab58f971eaee74dc935b59ae9c3",
                "deletions": 1,
                "filename": "src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java",
                "patch": "@@ -831,7 +831,11 @@ protected int tasksToPreempt(PoolSchedulable sched, long curTime) {\n     List<TaskStatus> statuses = new ArrayList<TaskStatus>();\n     for (TaskInProgress tip: tips) {\n       for (TaskAttemptID id: tip.getActiveTasks().keySet()) {\n-        statuses.add(tip.getTaskStatus(id));\n+        TaskStatus stat = tip.getTaskStatus(id);\n+        // status is null when the task has been scheduled but not yet running\n+        if (stat != null) {\n+          statuses.add(stat);\n+        }\n       }\n     }\n     return statuses;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1bd18c32a5309ab58f971eaee74dc935b59ae9c3/src/contrib/fairscheduler/src/java/org/apache/hadoop/mapred/FairScheduler.java",
                "sha": "ae0930c46581cef366f6fcd59d3ca7408b5383e3",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-1089. Fix NPE in fair scheduler preemption when tasks are  \nscheduled but not running. Contributed by Todd Lipcon.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/mapreduce/trunk@830821 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/a98327008f5b124724f3577bde06f2b4ddec6a18",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFairScheduler.java"
        ]
    },
    "hadoop-common_1f04f4b": {
        "bug_id": "hadoop-common_1f04f4b",
        "commit": "https://github.com/apache/hadoop-common/commit/1f04f4baf46b45468f0d15d16c71c769c4820827",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1f04f4baf46b45468f0d15d16c71c769c4820827/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=1f04f4baf46b45468f0d15d16c71c769c4820827",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -324,6 +324,9 @@ Release 2.0.4-beta - UNRELEASED\n     but not in dfs.namenode.edits.dir are silently ignored.  (Arpit Agarwal\n     via szetszwo)\n \n+    HDFS-4482. ReplicationMonitor thread can exit with NPE due to the race \n+    between delete and replication of same file. (umamahesh)\n+\n Release 2.0.3-alpha - 2013-02-06\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1f04f4baf46b45468f0d15d16c71c769c4820827/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "3b1a1e36bceb5c82791b0db68a2ddae8471b5c0a",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1f04f4baf46b45468f0d15d16c71c769c4820827/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "changes": 5,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java?ref=1f04f4baf46b45468f0d15d16c71c769c4820827",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "patch": "@@ -1343,6 +1343,11 @@ static String getFullPathName(INode inode) {\n \n     // fill up the inodes in the path from this inode to root\n     for (int i = 0; i < depth; i++) {\n+      if (inode == null) {\n+        NameNode.stateChangeLog.warn(\"Could not get full path.\"\n+            + \" Corresponding file might have deleted already.\");\n+        return null;\n+      }\n       inodes[depth-i-1] = inode;\n       inode = inode.parent;\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1f04f4baf46b45468f0d15d16c71c769c4820827/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSDirectory.java",
                "sha": "b11059a4bb47c865d14e439b1bbabfa418b90e2b",
                "status": "modified"
            },
            {
                "additions": 5,
                "blob_url": "https://github.com/apache/hadoop-common/blob/1f04f4baf46b45468f0d15d16c71c769c4820827/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "changes": 6,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java?ref=1f04f4baf46b45468f0d15d16c71c769c4820827",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "patch": "@@ -282,7 +282,11 @@ String getLocalName() {\n \n   String getLocalParentDir() {\n     INode inode = isRoot() ? this : getParent();\n-    return (inode != null) ? inode.getFullPathName() : \"\";\n+    String parentDir = \"\";\n+    if (inode != null) {\n+      parentDir = inode.getFullPathName();\n+    }\n+    return (parentDir != null) ? parentDir : \"\";\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop-common/raw/1f04f4baf46b45468f0d15d16c71c769c4820827/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/INode.java",
                "sha": "b407a62da97b1808bbbc8d7169ecd48a9b05292a",
                "status": "modified"
            }
        ],
        "message": "HDFS-4482. ReplicationMonitor thread can exit with NPE due to the race between delete and replication of same file. Contributed by Uma Maheswara Rao G.\n\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1448708 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/57181e75b9305bf18ed50eb905d7f7a6c013b340",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestINode.java"
        ]
    },
    "hadoop-common_2e96190": {
        "bug_id": "hadoop-common_2e96190",
        "commit": "https://github.com/apache/hadoop-common/commit/2e961906c6b086e43b502ee2a8d0b7f171d11bb2",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2e961906c6b086e43b502ee2a8d0b7f171d11bb2/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=2e961906c6b086e43b502ee2a8d0b7f171d11bb2",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -248,6 +248,9 @@ Release 0.23.6 - UNRELEASED\n     YARN-280. RM does not reject app submission with invalid tokens \n     (Daryn Sharp via tgraves)\n \n+    YARN-225. Proxy Link in RM UI thows NPE in Secure mode \n+    (Devaraj K via bobby)\n+\n Release 0.23.5 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2e961906c6b086e43b502ee2a8d0b7f171d11bb2/hadoop-yarn-project/CHANGES.txt",
                "sha": "ed4f895cfacc5776d9999665c87c150dbc80c7a8",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop-common/blob/2e961906c6b086e43b502ee2a8d0b7f171d11bb2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java?ref=2e961906c6b086e43b502ee2a8d0b7f171d11bb2",
                "deletions": 5,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java",
                "patch": "@@ -254,11 +254,14 @@ protected void doGet(HttpServletRequest req, HttpServletResponse resp)\n       \n       if(securityEnabled) {\n         String cookieName = getCheckCookieName(id); \n-        for(Cookie c: req.getCookies()) {\n-          if(cookieName.equals(c.getName())) {\n-            userWasWarned = true;\n-            userApproved = userApproved || Boolean.valueOf(c.getValue());\n-            break;\n+        Cookie[] cookies = req.getCookies();\n+        if (cookies != null) {\n+          for (Cookie c : cookies) {\n+            if (cookieName.equals(c.getName())) {\n+              userWasWarned = true;\n+              userApproved = userApproved || Boolean.valueOf(c.getValue());\n+              break;\n+            }\n           }\n         }\n       }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/2e961906c6b086e43b502ee2a8d0b7f171d11bb2/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/src/main/java/org/apache/hadoop/yarn/server/webproxy/WebAppProxyServlet.java",
                "sha": "7f6bba14e98e09d7634d9d6cc712868c191d126a",
                "status": "modified"
            }
        ],
        "message": "YARN-225. Proxy Link in RM UI thows NPE in Secure mode (Devaraj K via bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1426515 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/df7619819dc34d77f5e00b806611444d9b48ec0b",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestWebAppProxyServlet.java"
        ]
    },
    "hadoop-common_36cabb6": {
        "bug_id": "hadoop-common_36cabb6",
        "commit": "https://github.com/apache/hadoop-common/commit/36cabb654e315d62182b47c6bed6ce804a535e3e",
        "file": [
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=36cabb654e315d62182b47c6bed6ce804a535e3e",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -509,6 +509,10 @@ Release 0.23.0 - Unreleased\n     HADOOP-7360. Preserve relative paths that do not contain globs in FsShell.\n     (Daryn Sharp and Kihwal Lee via szetszwo)\n \n+    HADOOP-7771. FsShell -copyToLocal, -get, etc. commands throw NPE if the\n+    destination directory does not exist.  (John George and Daryn Sharp\n+    via szetszwo)\n+\n   OPTIMIZATIONS\n   \n     HADOOP-7333. Performance improvement in PureJavaCrc32. (Eric Caspole",
                "raw_url": "https://github.com/apache/hadoop-common/raw/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "0b0e1beb76248ba3b27554efd8cb2bd9b239c27d",
                "status": "modified"
            },
            {
                "additions": 12,
                "blob_url": "https://github.com/apache/hadoop-common/blob/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java?ref=36cabb654e315d62182b47c6bed6ce804a535e3e",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java",
                "patch": "@@ -55,6 +55,7 @@\n   protected int exitCode = 0;\n   protected int numErrors = 0;\n   protected boolean recursive = false;\n+  private int depth = 0;\n   protected ArrayList<Exception> exceptions = new ArrayList<Exception>();\n \n   private static final Log LOG = LogFactory.getLog(Command.class);\n@@ -86,6 +87,10 @@ protected boolean isRecursive() {\n     return recursive;\n   }\n \n+  protected int getDepth() {\n+    return depth;\n+  }\n+  \n   /** \n    * Execute the command on the input path\n    * \n@@ -269,6 +274,7 @@ protected void processArgument(PathData item) throws IOException {\n   protected void processPathArgument(PathData item) throws IOException {\n     // null indicates that the call is not via recursion, ie. there is\n     // no parent directory that was expanded\n+    depth = 0;\n     processPaths(null, item);\n   }\n   \n@@ -326,7 +332,12 @@ protected void processPath(PathData item) throws IOException {\n    *  @throws IOException if anything goes wrong...\n    */\n   protected void recursePath(PathData item) throws IOException {\n-    processPaths(item, item.getDirectoryContents());\n+    try {\n+      depth++;\n+      processPaths(item, item.getDirectoryContents());\n+    } finally {\n+      depth--;\n+    }\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop-common/raw/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/Command.java",
                "sha": "b24d47e02b1027b5decd17b36d9ab8a71b589875",
                "status": "modified"
            },
            {
                "additions": 116,
                "blob_url": "https://github.com/apache/hadoop-common/blob/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java",
                "changes": 142,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java?ref=36cabb654e315d62182b47c6bed6ce804a535e3e",
                "deletions": 26,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java",
                "patch": "@@ -20,13 +20,18 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.io.InputStream;\n import java.util.LinkedList;\n \n+import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.shell.PathExceptions.PathExistsException;\n import org.apache.hadoop.fs.shell.PathExceptions.PathIOException;\n+import org.apache.hadoop.fs.shell.PathExceptions.PathIsDirectoryException;\n import org.apache.hadoop.fs.shell.PathExceptions.PathIsNotDirectoryException;\n import org.apache.hadoop.fs.shell.PathExceptions.PathNotFoundException;\n+import org.apache.hadoop.fs.shell.PathExceptions.PathOperationException;\n+import org.apache.hadoop.io.IOUtils;\n \n /**\n  * Provides: argument processing to ensure the destination is valid\n@@ -106,51 +111,136 @@ protected void processArguments(LinkedList<PathData> args)\n   }\n \n   @Override\n-  protected void processPaths(PathData parent, PathData ... items)\n+  protected void processPathArgument(PathData src)\n   throws IOException {\n+    if (src.stat.isDirectory() && src.fs.equals(dst.fs)) {\n+      PathData target = getTargetPath(src);\n+      String srcPath = src.fs.makeQualified(src.path).toString();\n+      String dstPath = dst.fs.makeQualified(target.path).toString();\n+      if (dstPath.equals(srcPath)) {\n+        PathIOException e = new PathIOException(src.toString(),\n+            \"are identical\");\n+        e.setTargetPath(dstPath.toString());\n+        throw e;\n+      }\n+      if (dstPath.startsWith(srcPath+Path.SEPARATOR)) {\n+        PathIOException e = new PathIOException(src.toString(),\n+            \"is a subdirectory of itself\");\n+        e.setTargetPath(target.toString());\n+        throw e;\n+      }\n+    }\n+    super.processPathArgument(src);\n+  }\n+\n+  @Override\n+  protected void processPath(PathData src) throws IOException {\n+    processPath(src, getTargetPath(src));\n+  }\n+  \n+  /**\n+   * Called with a source and target destination pair\n+   * @param src for the operation\n+   * @param target for the operation\n+   * @throws IOException if anything goes wrong\n+   */\n+  protected void processPath(PathData src, PathData dst) throws IOException {\n+    if (src.stat.isSymlink()) {\n+      // TODO: remove when FileContext is supported, this needs to either\n+      // copy the symlink or deref the symlink\n+      throw new PathOperationException(src.toString());        \n+    } else if (src.stat.isFile()) {\n+      copyFileToTarget(src, dst);\n+    } else if (src.stat.isDirectory() && !isRecursive()) {\n+      throw new PathIsDirectoryException(src.toString());\n+    }\n+  }\n+\n+  @Override\n+  protected void recursePath(PathData src) throws IOException {\n     PathData savedDst = dst;\n     try {\n       // modify dst as we descend to append the basename of the\n       // current directory being processed\n-      if (parent != null) dst = dst.getPathDataForChild(parent);\n-      super.processPaths(parent, items);\n+      dst = getTargetPath(src);\n+      if (dst.exists) {\n+        if (!dst.stat.isDirectory()) {\n+          throw new PathIsNotDirectoryException(dst.toString());\n+        }\n+      } else {\n+        if (!dst.fs.mkdirs(dst.path)) {\n+          // too bad we have no clue what failed\n+          PathIOException e = new PathIOException(dst.toString());\n+          e.setOperation(\"mkdir\");\n+          throw e;\n+        }    \n+        dst.refreshStatus(); // need to update stat to know it exists now\n+      }      \n+      super.recursePath(src);\n     } finally {\n       dst = savedDst;\n     }\n   }\n   \n-  @Override\n-  protected void processPath(PathData src) throws IOException {\n+  protected PathData getTargetPath(PathData src) throws IOException {\n     PathData target;\n-    // if the destination is a directory, make target a child path,\n-    // else use the destination as-is\n-    if (dst.exists && dst.stat.isDirectory()) {\n+    // on the first loop, the dst may be directory or a file, so only create\n+    // a child path if dst is a dir; after recursion, it's always a dir\n+    if ((getDepth() > 0) || (dst.exists && dst.stat.isDirectory())) {\n       target = dst.getPathDataForChild(src);\n     } else {\n       target = dst;\n     }\n-    if (target.exists && !overwrite) {\n+    return target;\n+  }\n+  \n+  /**\n+   * Copies the source file to the target.\n+   * @param src item to copy\n+   * @param target where to copy the item\n+   * @throws IOException if copy fails\n+   */ \n+  protected void copyFileToTarget(PathData src, PathData target) throws IOException {\n+    copyStreamToTarget(src.fs.open(src.path), target);\n+  }\n+  \n+  /**\n+   * Copies the stream contents to a temporary file.  If the copy is\n+   * successful, the temporary file will be renamed to the real path,\n+   * else the temporary file will be deleted.\n+   * @param in the input stream for the copy\n+   * @param target where to store the contents of the stream\n+   * @throws IOException if copy fails\n+   */ \n+  protected void copyStreamToTarget(InputStream in, PathData target)\n+  throws IOException {\n+    if (target.exists && (target.stat.isDirectory() || !overwrite)) {\n       throw new PathExistsException(target.toString());\n     }\n-\n-    try { \n-      // invoke processPath with both a source and resolved target\n-      processPath(src, target);\n-    } catch (PathIOException e) {\n-      // add the target unless it already has one\n-      if (e.getTargetPath() == null) {\n+    PathData tempFile = null;\n+    try {\n+      tempFile = target.createTempFile(target+\"._COPYING_\");\n+      FSDataOutputStream out = target.fs.create(tempFile.path, true);\n+      IOUtils.copyBytes(in, out, getConf(), true);\n+      // the rename method with an option to delete the target is deprecated\n+      if (target.exists && !target.fs.delete(target.path, false)) {\n+        // too bad we don't know why it failed\n+        PathIOException e = new PathIOException(target.toString());\n+        e.setOperation(\"delete\");\n+        throw e;\n+      }\n+      if (!tempFile.fs.rename(tempFile.path, target.path)) {\n+        // too bad we don't know why it failed\n+        PathIOException e = new PathIOException(tempFile.toString());\n+        e.setOperation(\"rename\");\n         e.setTargetPath(target.toString());\n+        throw e;\n+      }\n+      tempFile = null;\n+    } finally {\n+      if (tempFile != null) {\n+        tempFile.fs.delete(tempFile.path, false);\n       }\n-      throw e;\n     }\n   }\n-\n-  /**\n-   * Called with a source and target destination pair\n-   * @param src for the operation\n-   * @param target for the operation\n-   * @throws IOException if anything goes wrong\n-   */\n-  protected abstract void processPath(PathData src, PathData target)\n-  throws IOException;\n }\n\\ No newline at end of file",
                "raw_url": "https://github.com/apache/hadoop-common/raw/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CommandWithDestination.java",
                "sha": "6b3b40389f9f72528cdc248ee23af4af559df101",
                "status": "modified"
            },
            {
                "additions": 10,
                "blob_url": "https://github.com/apache/hadoop-common/blob/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java",
                "changes": 83,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java?ref=36cabb654e315d62182b47c6bed6ce804a535e3e",
                "deletions": 73,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java",
                "patch": "@@ -26,13 +26,7 @@\n import org.apache.hadoop.classification.InterfaceAudience;\n import org.apache.hadoop.classification.InterfaceStability;\n import org.apache.hadoop.fs.ChecksumFileSystem;\n-import org.apache.hadoop.fs.FSDataOutputStream;\n import org.apache.hadoop.fs.FileUtil;\n-import org.apache.hadoop.fs.LocalFileSystem;\n-import org.apache.hadoop.fs.shell.PathExceptions.PathExistsException;\n-import org.apache.hadoop.fs.shell.PathExceptions.PathIOException;\n-import org.apache.hadoop.fs.shell.PathExceptions.PathOperationException;\n-import org.apache.hadoop.io.IOUtils;\n \n /** Various commands for copy files */\n @InterfaceAudience.Private\n@@ -95,18 +89,10 @@ protected void processOptions(LinkedList<String> args) throws IOException {\n       CommandFormat cf = new CommandFormat(2, Integer.MAX_VALUE, \"f\");\n       cf.parse(args);\n       setOverwrite(cf.getOpt(\"f\"));\n+      // should have a -r option\n+      setRecursive(true);\n       getRemoteDestination(args);\n     }\n-\n-    @Override\n-    protected void processPath(PathData src, PathData target)\n-    throws IOException {\n-      if (!FileUtil.copy(src.fs, src.path, target.fs, target.path, false, overwrite, getConf())) {\n-        // we have no idea what the error is...  FileUtils masks it and in\n-        // some cases won't even report an error\n-        throw new PathIOException(src.toString());\n-      }\n-    }\n   }\n   \n   /** \n@@ -126,7 +112,6 @@ protected void processPath(PathData src, PathData target)\n      * It must be at least three characters long, required by\n      * {@link java.io.File#createTempFile(String, String, File)}.\n      */\n-    private static final String COPYTOLOCAL_PREFIX = \"_copyToLocal_\";\n     private boolean copyCrc;\n     private boolean verifyChecksum;\n \n@@ -144,7 +129,7 @@ protected void processOptions(LinkedList<String> args)\n     }\n \n     @Override\n-    protected void processPath(PathData src, PathData target)\n+    protected void copyFileToTarget(PathData src, PathData target)\n     throws IOException {\n       src.fs.setVerifyChecksum(verifyChecksum);\n \n@@ -153,41 +138,10 @@ protected void processPath(PathData src, PathData target)\n         copyCrc = false;\n       }      \n \n-      if (src.stat.isFile()) {\n-        // copy the file and maybe its crc\n-        copyFileToLocal(src, target);\n-        if (copyCrc) {\n-          copyFileToLocal(src.getChecksumFile(), target.getChecksumFile());\n-        }\n-      } else if (src.stat.isDirectory()) {\n-        // create the remote directory structure locally\n-        if (!target.toFile().mkdirs()) {\n-          throw new PathIOException(target.toString());\n-        }\n-      } else {\n-        throw new PathOperationException(src.toString());\n-      }\n-    }\n-\n-    private void copyFileToLocal(PathData src, PathData target)\n-    throws IOException {\n-      File targetFile = target.toFile();\n-      File tmpFile = FileUtil.createLocalTempFile(\n-          targetFile, COPYTOLOCAL_PREFIX, true);\n-      // too bad we can't tell exactly why it failed...\n-      if (!FileUtil.copy(src.fs, src.path, tmpFile, false, getConf())) {\n-        PathIOException e = new PathIOException(src.toString());\n-        e.setOperation(\"copy\");\n-        e.setTargetPath(tmpFile.toString());\n-        throw e;\n-      }\n-\n-      // too bad we can't tell exactly why it failed...\n-      if (!tmpFile.renameTo(targetFile)) {\n-        PathIOException e = new PathIOException(tmpFile.toString());\n-        e.setOperation(\"rename\");\n-        e.setTargetPath(targetFile.toString());\n-        throw e;\n+      super.copyFileToTarget(src, target);\n+      if (copyCrc) {\n+        // should we delete real file if crc copy fails?\n+        super.copyFileToTarget(src.getChecksumFile(), target.getChecksumFile());\n       }\n     }\n   }\n@@ -208,6 +162,8 @@ protected void processOptions(LinkedList<String> args) throws IOException {\n       cf.parse(args);\n       setOverwrite(cf.getOpt(\"f\"));\n       getRemoteDestination(args);\n+      // should have a -r option\n+      setRecursive(true);\n     }\n \n     // commands operating on local paths have no need for glob expansion\n@@ -223,30 +179,11 @@ protected void processArguments(LinkedList<PathData> args)\n     throws IOException {\n       // NOTE: this logic should be better, mimics previous implementation\n       if (args.size() == 1 && args.get(0).toString().equals(\"-\")) {\n-        if (dst.exists && !overwrite) {\n-          throw new PathExistsException(dst.toString());\n-        }\n-        copyFromStdin();\n+        copyStreamToTarget(System.in, getTargetPath(args.get(0)));\n         return;\n       }\n       super.processArguments(args);\n     }\n-\n-    @Override\n-    protected void processPath(PathData src, PathData target)\n-    throws IOException {\n-      target.fs.copyFromLocalFile(false, overwrite, src.path, target.path);\n-    }\n-\n-    /** Copies from stdin to the destination file. */\n-    protected void copyFromStdin() throws IOException {\n-      FSDataOutputStream out = dst.fs.create(dst.path); \n-      try {\n-        IOUtils.copyBytes(System.in, out, getConf(), false);\n-      } finally {\n-        out.close();\n-      }\n-    }\n   }\n \n   public static class CopyFromLocal extends Put {",
                "raw_url": "https://github.com/apache/hadoop-common/raw/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/CopyCommands.java",
                "sha": "066e5fdb899df7f517fb508451caf3a86f7caf68",
                "status": "modified"
            },
            {
                "additions": 13,
                "blob_url": "https://github.com/apache/hadoop-common/blob/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java",
                "changes": 13,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java?ref=36cabb654e315d62182b47c6bed6ce804a535e3e",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java",
                "patch": "@@ -182,6 +182,19 @@ public PathData getChecksumFile() throws IOException {\n     return new PathData(srcFs.getRawFileSystem(), srcPath.toString());\n   }\n \n+  /**\n+   * Returns a temporary file for this PathData with the given extension.\n+   * The file will be deleted on exit.\n+   * @param extension for the temporary file\n+   * @return PathData\n+   * @throws IOException shouldn't happen\n+   */\n+  public PathData createTempFile(String extension) throws IOException {\n+    PathData tmpFile = new PathData(fs, uri+\"._COPYING_\");\n+    fs.deleteOnExit(tmpFile.path);\n+    return tmpFile;\n+  }\n+\n   /**\n    * Returns a list of PathData objects of the items contained in the given\n    * directory.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/36cabb654e315d62182b47c6bed6ce804a535e3e/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/shell/PathData.java",
                "sha": "a3c88f1f2af2736442e7db320919298356c681b4",
                "status": "modified"
            }
        ],
        "message": "HADOOP-7771. FsShell -copyToLocal, -get, etc. commands throw NPE if the destination directory does not exist.  Contributed by John George and Daryn Sharp\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1195760 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/f3215940dc0fbbf0c75e744ede43605a42580d5a",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestPathData.java"
        ]
    },
    "hadoop-common_755572a": {
        "bug_id": "hadoop-common_755572a",
        "commit": "https://github.com/apache/hadoop-common/commit/755572a81a5ff1eeef41242abb050124b6146533",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/755572a81a5ff1eeef41242abb050124b6146533/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=755572a81a5ff1eeef41242abb050124b6146533",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -1833,6 +1833,8 @@ Release 0.23.0 - Unreleased\n     MAPREDUCE-3258. Fixed AM & JobHistory web-ui to display counters properly.\n     (Siddharth Seth via acmurthy)\n \n+    MAPREDUCE-3290. Fixed a NPE in ClientRMService. (acmurthy) \n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/755572a81a5ff1eeef41242abb050124b6146533/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "c9d61a6ec79ccd272013d739e90575c699f2c4a3",
                "status": "modified"
            },
            {
                "additions": 15,
                "blob_url": "https://github.com/apache/hadoop-common/blob/755572a81a5ff1eeef41242abb050124b6146533/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "changes": 19,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java?ref=755572a81a5ff1eeef41242abb050124b6146533",
                "deletions": 4,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "patch": "@@ -59,6 +59,7 @@\n import org.apache.hadoop.yarn.api.records.ApplicationSubmissionContext;\n import org.apache.hadoop.yarn.api.records.NodeReport;\n import org.apache.hadoop.yarn.api.records.QueueInfo;\n+import org.apache.hadoop.yarn.api.records.Resource;\n import org.apache.hadoop.yarn.api.records.YarnClusterMetrics;\n import org.apache.hadoop.yarn.conf.YarnConfiguration;\n import org.apache.hadoop.yarn.exceptions.YarnRemoteException;\n@@ -67,10 +68,12 @@\n import org.apache.hadoop.yarn.ipc.RPCUtil;\n import org.apache.hadoop.yarn.ipc.YarnRPC;\n import org.apache.hadoop.yarn.server.resourcemanager.RMAuditLogger.AuditConstants;\n+import org.apache.hadoop.yarn.server.resourcemanager.resource.Resources;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMApp;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEvent;\n import org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppEventType;\n import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n+import org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNodeReport;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.YarnScheduler;\n import org.apache.hadoop.yarn.server.resourcemanager.security.authorize.RMPolicyProvider;\n import org.apache.hadoop.yarn.server.security.ApplicationACLsManager;\n@@ -396,10 +399,18 @@ private NodeReport createNodeReports(RMNode rmNode) {\n     report.setRackName(rmNode.getRackName());\n     report.setCapability(rmNode.getTotalCapability());\n     report.setNodeHealthStatus(rmNode.getNodeHealthStatus());\n-    org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerNodeReport schedulerNodeReport = scheduler\n-        .getNodeReport(rmNode.getNodeID());\n-    report.setUsed(schedulerNodeReport.getUsedResource());\n-    report.setNumContainers(schedulerNodeReport.getNumContainers());\n+    \n+    SchedulerNodeReport schedulerNodeReport = \n+        scheduler.getNodeReport(rmNode.getNodeID());\n+    Resource used = Resources.none();\n+    int numContainers = 0;\n+    if (schedulerNodeReport != null) {\n+      used = schedulerNodeReport.getUsedResource();\n+      numContainers = schedulerNodeReport.getNumContainers();\n+    } \n+    report.setUsed(used);\n+    report.setNumContainers(numContainers);\n+\n     return report;\n   }\n ",
                "raw_url": "https://github.com/apache/hadoop-common/raw/755572a81a5ff1eeef41242abb050124b6146533/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/ClientRMService.java",
                "sha": "b19c1c17c5e6656c934d4d92ceb72cd6120b6042",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-3290. Fixed a NPE in ClientRMService.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1190162 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/38f94704dfb6ecde5856d4027b82dfe2f825fe9d",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestClientRMService.java"
        ]
    },
    "hadoop-common_90dbbfc": {
        "bug_id": "hadoop-common_90dbbfc",
        "commit": "https://github.com/apache/hadoop-common/commit/90dbbfc5743e80f016a5906c5d16e56bd0f874bf",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/90dbbfc5743e80f016a5906c5d16e56bd0f874bf/hadoop-mapreduce-project/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/CHANGES.txt?ref=90dbbfc5743e80f016a5906c5d16e56bd0f874bf",
                "deletions": 0,
                "filename": "hadoop-mapreduce-project/CHANGES.txt",
                "patch": "@@ -481,6 +481,8 @@ Release 0.23.3 - UNRELEASED\n     MAPREDUCE-4237. TestNodeStatusUpdater can fail if localhost has a domain\n     associated with it (bobby)\n \n+    MAPREDUCE-4233. NPE can happen in RMNMNodeInfo. (bobby)\n+\n Release 0.23.2 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/90dbbfc5743e80f016a5906c5d16e56bd0f874bf/hadoop-mapreduce-project/CHANGES.txt",
                "sha": "ff4664675d44bb97f2aff87288a3d2c7127d3307",
                "status": "modified"
            },
            {
                "additions": 41,
                "blob_url": "https://github.com/apache/hadoop-common/blob/90dbbfc5743e80f016a5906c5d16e56bd0f874bf/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java",
                "changes": 43,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java?ref=90dbbfc5743e80f016a5906c5d16e56bd0f874bf",
                "deletions": 2,
                "filename": "hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java",
                "patch": "@@ -21,22 +21,28 @@\n import java.io.File;\n import java.io.IOException;\n import java.util.Iterator;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ConcurrentMap;\n \n import org.apache.commons.logging.Log;\n import org.apache.commons.logging.LogFactory;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.yarn.api.records.NodeId;\n+import org.apache.hadoop.yarn.server.resourcemanager.MockNodes;\n import org.apache.hadoop.yarn.server.resourcemanager.RMContext;\n import org.apache.hadoop.yarn.server.resourcemanager.RMNMInfo;\n+import org.apache.hadoop.yarn.server.resourcemanager.rmnode.RMNode;\n import org.apache.hadoop.yarn.server.resourcemanager.scheduler.ResourceScheduler;\n import org.codehaus.jackson.JsonNode;\n import org.codehaus.jackson.map.ObjectMapper;\n import org.junit.AfterClass;\n import org.junit.Assert;\n import org.junit.BeforeClass;\n import org.junit.Test;\n+import static org.mockito.Mockito.*;\n \n public class TestRMNMInfo {\n   private static final Log LOG = LogFactory.getLog(TestRMNMInfo.class);\n@@ -116,14 +122,47 @@ public void testRMNMInfo() throws Exception {\n               n.get(\"HealthStatus\").getValueAsText().contains(\"Healthy\"));\n       Assert.assertNotNull(n.get(\"LastHealthUpdate\"));\n       Assert.assertNotNull(n.get(\"HealthReport\"));\n-      Assert.assertNotNull(n.get(\"NumContainersMB\"));\n+      Assert.assertNotNull(n.get(\"NumContainers\"));\n       Assert.assertEquals(\n               n.get(\"NodeId\") + \": Unexpected number of used containers\",\n-              0, n.get(\"NumContainersMB\").getValueAsInt());\n+              0, n.get(\"NumContainers\").getValueAsInt());\n       Assert.assertEquals(\n               n.get(\"NodeId\") + \": Unexpected amount of used memory\",\n               0, n.get(\"UsedMemoryMB\").getValueAsInt());\n       Assert.assertNotNull(n.get(\"AvailableMemoryMB\"));\n     }\n   }\n+  \n+  @Test\n+  public void testRMNMInfoMissmatch() throws Exception {\n+    RMContext rmc = mock(RMContext.class);\n+    ResourceScheduler rms = mock(ResourceScheduler.class);\n+    ConcurrentMap<NodeId, RMNode> map = new ConcurrentHashMap<NodeId, RMNode>();\n+    RMNode node = MockNodes.newNodeInfo(1, MockNodes.newResource(4 * 1024));\n+    map.put(node.getNodeID(), node);\n+    when(rmc.getRMNodes()).thenReturn(map);\n+    \n+    RMNMInfo rmInfo = new RMNMInfo(rmc,rms);\n+    String liveNMs = rmInfo.getLiveNodeManagers();\n+    ObjectMapper mapper = new ObjectMapper();\n+    JsonNode jn = mapper.readTree(liveNMs);\n+    Assert.assertEquals(\"Unexpected number of live nodes:\",\n+                                               1, jn.size());\n+    Iterator<JsonNode> it = jn.iterator();\n+    while (it.hasNext()) {\n+      JsonNode n = it.next();\n+      Assert.assertNotNull(n.get(\"HostName\"));\n+      Assert.assertNotNull(n.get(\"Rack\"));\n+      Assert.assertTrue(\"Node \" + n.get(\"NodeId\") + \" should be RUNNING\",\n+              n.get(\"State\").getValueAsText().contains(\"RUNNING\"));\n+      Assert.assertNotNull(n.get(\"NodeHTTPAddress\"));\n+      Assert.assertTrue(\"Node \" + n.get(\"NodeId\") + \" should be Healthy\",\n+              n.get(\"HealthStatus\").getValueAsText().contains(\"Healthy\"));\n+      Assert.assertNotNull(n.get(\"LastHealthUpdate\"));\n+      Assert.assertNotNull(n.get(\"HealthReport\"));\n+      Assert.assertNull(n.get(\"NumContainers\"));\n+      Assert.assertNull(n.get(\"UsedMemoryMB\"));\n+      Assert.assertNull(n.get(\"AvailableMemoryMB\"));\n+    }\n+  }\n }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/90dbbfc5743e80f016a5906c5d16e56bd0f874bf/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-jobclient/src/test/java/org/apache/hadoop/mapreduce/v2/TestRMNMInfo.java",
                "sha": "4ee485644d91ad5b1fcffde9846301c36d237353",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/90dbbfc5743e80f016a5906c5d16e56bd0f874bf/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java",
                "changes": 10,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java?ref=90dbbfc5743e80f016a5906c5d16e56bd0f874bf",
                "deletions": 4,
                "filename": "hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java",
                "patch": "@@ -93,10 +93,12 @@ public String getLiveNodeManagers() {\n                         ni.getNodeHealthStatus().getLastHealthReportTime());\n         info.put(\"HealthReport\",\n                         ni.getNodeHealthStatus().getHealthReport());\n-        info.put(\"NumContainersMB\", report.getNumContainers());\n-        info.put(\"UsedMemoryMB\", report.getUsedResource().getMemory());\n-        info.put(\"AvailableMemoryMB\",\n-                                report.getAvailableResource().getMemory());\n+        if(report != null) {\n+          info.put(\"NumContainers\", report.getNumContainers());\n+          info.put(\"UsedMemoryMB\", report.getUsedResource().getMemory());\n+          info.put(\"AvailableMemoryMB\",\n+              report.getAvailableResource().getMemory());\n+        }\n \n         nodesInfo.add(info);\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/90dbbfc5743e80f016a5906c5d16e56bd0f874bf/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/RMNMInfo.java",
                "sha": "0db42e40ec0e08d72413048956baf0393062157d",
                "status": "modified"
            }
        ],
        "message": "MAPREDUCE-4233. NPE can happen in RMNMNodeInfo. (bobby)\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1337363 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/6135cf06692f8691f49a57704827b3bb729ae8bd",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestRMNMInfo.java"
        ]
    },
    "hadoop-common_a0692b2": {
        "bug_id": "hadoop-common_a0692b2",
        "commit": "https://github.com/apache/hadoop-common/commit/a0692b2452f97c31ccd9088f6e9b2568047799f7",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a0692b2452f97c31ccd9088f6e9b2568047799f7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=a0692b2452f97c31ccd9088f6e9b2568047799f7",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -3681,7 +3681,8 @@ public boolean isInStartupSafeMode() {\n \n   @Override\n   public boolean isPopulatingReplQueues() {\n-    if (!haContext.getState().shouldPopulateReplQueues()) {\n+    if (haContext != null && // null during startup!\n+        !haContext.getState().shouldPopulateReplQueues()) {\n       return false;\n     }\n     // safeMode is volatile, and may be set to null at any time",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a0692b2452f97c31ccd9088f6e9b2568047799f7/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "258cb53186ad15632cf9899da3f0304a189f942c",
                "status": "modified"
            }
        ],
        "message": "Amend HDFS-2795. Fix PersistBlocks failure due to an NPE in isPopulatingReplQueues()\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1232510 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/835b4872937f693034d83003c6f7f0ded1d0df4d",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop-common_a5ab28a": {
        "bug_id": "hadoop-common_a5ab28a",
        "commit": "https://github.com/apache/hadoop-common/commit/a5ab28a12f3485d2c70fe795d1ba5e45645dd949",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a5ab28a12f3485d2c70fe795d1ba5e45645dd949/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=a5ab28a12f3485d2c70fe795d1ba5e45645dd949",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -100,6 +100,9 @@ Trunk (unreleased changes)\n     HADOOP-7131. Exceptions thrown by Text methods should include the causing\n     exception. (Uma Maheswara Rao G via todd)\n \n+    HADOOP-6912. Guard against NPE when calling UGI.isLoginKeytabBased().\n+    (Kan Zhang via jitendra)\n+\n Release 0.22.0 - Unreleased\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a5ab28a12f3485d2c70fe795d1ba5e45645dd949/CHANGES.txt",
                "sha": "ead2d3f88a9ba1f5b25e5526666fa9d823086f02",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a5ab28a12f3485d2c70fe795d1ba5e45645dd949/src/java/org/apache/hadoop/security/UserGroupInformation.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/security/UserGroupInformation.java?ref=a5ab28a12f3485d2c70fe795d1ba5e45645dd949",
                "deletions": 2,
                "filename": "src/java/org/apache/hadoop/security/UserGroupInformation.java",
                "patch": "@@ -811,8 +811,8 @@ private boolean hasSufficientTimeElapsed(long now) {\n    * Did the login happen via keytab\n    * @return true or false\n    */\n-  public synchronized static boolean isLoginKeytabBased() {\n-    return loginUser.isKeytab;\n+  public synchronized static boolean isLoginKeytabBased() throws IOException {\n+    return getLoginUser().isKeytab;\n   }\n \n   /**",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a5ab28a12f3485d2c70fe795d1ba5e45645dd949/src/java/org/apache/hadoop/security/UserGroupInformation.java",
                "sha": "085ce61719eefec5cd06738b846867a1335f08cd",
                "status": "modified"
            }
        ],
        "message": "HADOOP-6912. Guard against NPE when calling UGI.isLoginKeytabBased(). Contributed by Kan Zhang.\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1079068 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/d232e914e91769f80269af43f07509e918b9d06a",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestUserGroupInformation.java"
        ]
    },
    "hadoop-common_a5e6943": {
        "bug_id": "hadoop-common_a5e6943",
        "commit": "https://github.com/apache/hadoop-common/commit/a5e6943c593cc2952744f015426b1af6672a135a",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a5e6943c593cc2952744f015426b1af6672a135a/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/CHANGES.txt?ref=a5e6943c593cc2952744f015426b1af6672a135a",
                "deletions": 0,
                "filename": "CHANGES.txt",
                "patch": "@@ -601,6 +601,9 @@ Release 0.22.0 - Unreleased\n \n     HDFS-1781. Fix the path for jsvc in bin/hdfs.  (John George via szetszwo)\n \n+    HDFS-1782. Fix an NPE in RFSNamesystem.startFileInternal(..).\n+    (John George via szetszwo)\n+\n Release 0.21.1 - Unreleased\n \n   IMPROVEMENTS",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a5e6943c593cc2952744f015426b1af6672a135a/CHANGES.txt",
                "sha": "ae520af159c4a1e32822ca61ff846df15247d6aa",
                "status": "modified"
            },
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/a5e6943c593cc2952744f015426b1af6672a135a/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java?ref=a5e6943c593cc2952744f015426b1af6672a135a",
                "deletions": 1,
                "filename": "src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "patch": "@@ -1394,7 +1394,8 @@ private LocatedBlock startFileInternal(String src,\n                 \". Lease recovery is in progress. Try again later.\");\n \n         } else {\n-          if(pendingFile.getLastBlock().getBlockUCState() ==\n+          BlockInfoUnderConstruction lastBlock=pendingFile.getLastBlock();\n+          if(lastBlock != null && lastBlock.getBlockUCState() ==\n             BlockUCState.UNDER_RECOVERY) {\n             throw new RecoveryInProgressException(\n               \"Recovery in progress, file [\" + src + \"], \" +",
                "raw_url": "https://github.com/apache/hadoop-common/raw/a5e6943c593cc2952744f015426b1af6672a135a/src/java/org/apache/hadoop/hdfs/server/namenode/FSNamesystem.java",
                "sha": "448f11792f49ba644bd4dadf264c39ae263ed547",
                "status": "modified"
            }
        ],
        "message": "HDFS-1782. Fix an NPE in RFSNamesystem.startFileInternal(..).  Contributed by John George\n\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/hdfs/trunk@1087115 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/682bd024a199d86e43023eaf1a952d7b85eecfc6",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestFSNamesystem.java"
        ]
    },
    "hadoop-common_bee7247": {
        "bug_id": "hadoop-common_bee7247",
        "commit": "https://github.com/apache/hadoop-common/commit/bee72473f36c24d0103236153044bd7836bc19e3",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/bee72473f36c24d0103236153044bd7836bc19e3/hadoop-common-project/hadoop-common/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/CHANGES.txt?ref=bee72473f36c24d0103236153044bd7836bc19e3",
                "deletions": 0,
                "filename": "hadoop-common-project/hadoop-common/CHANGES.txt",
                "patch": "@@ -439,6 +439,9 @@ Release 2.3.0 - UNRELEASED\n \n     HADOOP-10100. MiniKDC shouldn't use apacheds-all artifact. (rkanter via tucu)\n \n+    HADOOP-10107. Server.getNumOpenConnections may throw NPE. (Kihwal Lee via\n+    jing9)\n+\n Release 2.2.1 - UNRELEASED\n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/bee72473f36c24d0103236153044bd7836bc19e3/hadoop-common-project/hadoop-common/CHANGES.txt",
                "sha": "2b9aeb88a5a759ab34e1eccab28d261d53a65826",
                "status": "modified"
            },
            {
                "additions": 1,
                "blob_url": "https://github.com/apache/hadoop-common/blob/bee72473f36c24d0103236153044bd7836bc19e3/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java?ref=bee72473f36c24d0103236153044bd7836bc19e3",
                "deletions": 1,
                "filename": "hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "patch": "@@ -2109,6 +2109,7 @@ protected Server(String bindAddress, int port,\n     // Start the listener here and let it bind to the port\n     listener = new Listener();\n     this.port = listener.getAddress().getPort();    \n+    connectionManager = new ConnectionManager();\n     this.rpcMetrics = RpcMetrics.create(this);\n     this.rpcDetailedMetrics = RpcDetailedMetrics.create(this.port);\n     this.tcpNoDelay = conf.getBoolean(\n@@ -2117,7 +2118,6 @@ protected Server(String bindAddress, int port,\n \n     // Create the responder here\n     responder = new Responder();\n-    connectionManager = new ConnectionManager();\n     \n     if (secretManager != null) {\n       SaslRpcServer.init(conf);",
                "raw_url": "https://github.com/apache/hadoop-common/raw/bee72473f36c24d0103236153044bd7836bc19e3/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java",
                "sha": "7fb395cdb056029d85a7940a7e7fbedd941a5a4c",
                "status": "modified"
            }
        ],
        "message": "HADOOP-10107. Server.getNumOpenConnections may throw NPE. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1543335 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/66f8ea400a5d97d9602405b5e9ff597d75e30b3f",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestServer.java"
        ]
    },
    "hadoop-common_d1e971c": {
        "bug_id": "hadoop-common_d1e971c",
        "commit": "https://github.com/apache/hadoop-common/commit/d1e971c25f7c8e4e8e7aebbb5555a57829114236",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d1e971c25f7c8e4e8e7aebbb5555a57829114236/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=d1e971c25f7c8e4e8e7aebbb5555a57829114236",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -149,3 +149,5 @@ HDFS-2845. SBN should not allow browsing of the file system via web UI. (Bikas S\n HDFS-2742. HA: observed dataloss in replication stress test. (todd via eli)\n \n HDFS-2870. Fix log level for block debug info in processMisReplicatedBlocks (todd)\n+\n+HDFS-2859. LOCAL_ADDRESS_MATCHER.match has NPE when called from DFSUtil.getSuffixIDs when the host is incorrect (Bikas Saha via todd)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d1e971c25f7c8e4e8e7aebbb5555a57829114236/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "7a4ef27f19513e6bb421cf8ad4d79f49e29b0397",
                "status": "modified"
            },
            {
                "additions": 6,
                "blob_url": "https://github.com/apache/hadoop-common/blob/d1e971c25f7c8e4e8e7aebbb5555a57829114236/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "changes": 7,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java?ref=d1e971c25f7c8e4e8e7aebbb5555a57829114236",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "patch": "@@ -61,6 +61,8 @@\n import org.apache.hadoop.net.NetUtils;\n import org.apache.hadoop.net.NodeBase;\n import org.apache.hadoop.security.UserGroupInformation;\n+import org.apache.commons.logging.Log;\n+import org.apache.commons.logging.LogFactory;\n \n import com.google.common.base.Joiner;\n import com.google.common.collect.Lists;\n@@ -69,6 +71,8 @@\n \n @InterfaceAudience.Private\n public class DFSUtil {\n+  private static final Log LOG = LogFactory.getLog(DFSUtil.class.getName());\n+  \n   private DFSUtil() { /* Hidden constructor */ }\n   private static final ThreadLocal<Random> RANDOM = new ThreadLocal<Random>() {\n     @Override\n@@ -935,9 +939,10 @@ private static String getNameServiceId(Configuration conf, String addressKey) {\n         try {\n           s = NetUtils.createSocketAddr(addr);\n         } catch (Exception e) {\n+          LOG.warn(\"Exception in creating socket address\", e);\n           continue;\n         }\n-        if (matcher.match(s)) {\n+        if (!s.isUnresolved() && matcher.match(s)) {\n           nameserviceId = nsId;\n           namenodeId = nnId;\n           found++;",
                "raw_url": "https://github.com/apache/hadoop-common/raw/d1e971c25f7c8e4e8e7aebbb5555a57829114236/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/DFSUtil.java",
                "sha": "c9ccf9f38c7cafd26872165b4890710acc96be21",
                "status": "modified"
            }
        ],
        "message": "HDFS-2859. LOCAL_ADDRESS_MATCHER.match has NPE when called from DFSUtil.getSuffixIDs when the host is incorrect. Contributed by Bikas Saha.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1239356 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/35f474465df629f5b7f6199a2c029139922588db",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestDFSUtil.java"
        ]
    },
    "hadoop-common_ee85268": {
        "bug_id": "hadoop-common_ee85268",
        "commit": "https://github.com/apache/hadoop-common/commit/ee852685a827dcd7e077b81bf8c4ba95ec5464a8",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ee852685a827dcd7e077b81bf8c4ba95ec5464a8/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt?ref=ee852685a827dcd7e077b81bf8c4ba95ec5464a8",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "patch": "@@ -19,3 +19,5 @@ HDFS-2231. Configuration changes for HA namenode. (suresh)\n HDFS-2418. Change ConfiguredFailoverProxyProvider to take advantage of HDFS-2231. (atm)\n \n HDFS-2393. Mark appropriate methods of ClientProtocol with the idempotent annotation. (atm)\n+\n+HDFS-2523. Small NN fixes to include HAServiceProtocol and prevent NPE on shutdown. (todd)",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ee852685a827dcd7e077b81bf8c4ba95ec5464a8/hadoop-hdfs-project/hadoop-hdfs/CHANGES.HDFS-1623.txt",
                "sha": "37e6e4acac308b051fc5814e9a4d03cc957ddcf4",
                "status": "modified"
            },
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ee852685a827dcd7e077b81bf8c4ba95ec5464a8/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java?ref=ee852685a827dcd7e077b81bf8c4ba95ec5464a8",
                "deletions": 1,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "patch": "@@ -572,7 +572,9 @@ public void stop() {\n       stopRequested = true;\n     }\n     try {\n-      state.exitState(haContext);\n+      if (state != null) {\n+        state.exitState(haContext);\n+      }\n     } catch (ServiceFailedException e) {\n       LOG.warn(\"Encountered exception while exiting state \", e);\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ee852685a827dcd7e077b81bf8c4ba95ec5464a8/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNode.java",
                "sha": "4eb080105f050c928e37e4d27e2071eafa6753b7",
                "status": "modified"
            },
            {
                "additions": 4,
                "blob_url": "https://github.com/apache/hadoop-common/blob/ee852685a827dcd7e077b81bf8c4ba95ec5464a8/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "changes": 4,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java?ref=ee852685a827dcd7e077b81bf8c4ba95ec5464a8",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "patch": "@@ -39,6 +39,7 @@\n import org.apache.hadoop.fs.permission.PermissionStatus;\n import static org.apache.hadoop.hdfs.DFSConfigKeys.*;\n \n+import org.apache.hadoop.ha.HAServiceProtocol;\n import org.apache.hadoop.ha.HealthCheckFailedException;\n import org.apache.hadoop.ha.ServiceFailedException;\n import org.apache.hadoop.hdfs.HDFSPolicyProvider;\n@@ -156,6 +157,7 @@ public NameNodeRpcServer(Configuration conf, NameNode nn)\n     this.server.addProtocol(RefreshAuthorizationPolicyProtocol.class, this);\n     this.server.addProtocol(RefreshUserMappingsProtocol.class, this);\n     this.server.addProtocol(GetUserMappingsProtocol.class, this);\n+    this.server.addProtocol(HAServiceProtocol.class, this);\n     \n \n     // set service-level authorization security policy\n@@ -225,6 +227,8 @@ public long getProtocolVersion(String protocol,\n       return RefreshUserMappingsProtocol.versionID;\n     } else if (protocol.equals(GetUserMappingsProtocol.class.getName())){\n       return GetUserMappingsProtocol.versionID;\n+    } else if (protocol.equals(HAServiceProtocol.class.getName())) {\n+      return HAServiceProtocol.versionID;\n     } else {\n       throw new IOException(\"Unknown protocol to name node: \" + protocol);\n     }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/ee852685a827dcd7e077b81bf8c4ba95ec5464a8/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/namenode/NameNodeRpcServer.java",
                "sha": "6546b8fe06b07bfe73b85c60f3e143e84e847717",
                "status": "modified"
            }
        ],
        "message": "HDFS-2523. Small NN fixes to include HAServiceProtocol and prevent NPE on shutdown. Contributed by Todd Lipcon.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/branches/HDFS-1623@1195753 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/32851dcf1b0b95b2552fea1071fa8f78944fb67f",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestNameNodeRpcServer.java"
        ]
    },
    "hadoop-common_f56d99c": {
        "bug_id": "hadoop-common_f56d99c",
        "commit": "https://github.com/apache/hadoop-common/commit/f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea",
        "file": [
            {
                "additions": 2,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "changes": 2,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt?ref=f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea",
                "deletions": 0,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "patch": "@@ -561,6 +561,8 @@ Release 2.1.0-beta - UNRELEASED\n \n     HDFS-4382. Fix typo MAX_NOT_CHANGED_INTERATIONS. (Ted Yu via suresh)\n \n+    HDFS-4840. ReplicationMonitor gets NPE during shutdown. (kihwal)\n+\n   BREAKDOWN OF HDFS-347 SUBTASKS AND RELATED JIRAS\n \n     HDFS-4353. Encapsulate connections to peers in Peer and PeerServer classes.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea/hadoop-hdfs-project/hadoop-hdfs/CHANGES.txt",
                "sha": "a85341ec5480ca225f9dfb62af0e0a0ffabd4601",
                "status": "modified"
            },
            {
                "additions": 8,
                "blob_url": "https://github.com/apache/hadoop-common/blob/f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "changes": 11,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java?ref=f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea",
                "deletions": 3,
                "filename": "hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "patch": "@@ -3094,10 +3094,15 @@ public void run() {\n           computeDatanodeWork();\n           processPendingReplications();\n           Thread.sleep(replicationRecheckInterval);\n-        } catch (InterruptedException ie) {\n-          LOG.warn(\"ReplicationMonitor thread received InterruptedException.\", ie);\n-          break;\n         } catch (Throwable t) {\n+          if (!namesystem.isRunning()) {\n+            LOG.info(\"Stopping ReplicationMonitor.\");\n+            if (!(t instanceof InterruptedException)) {\n+              LOG.info(\"ReplicationMonitor received an exception\"\n+                  + \" while shutting down.\", t);\n+            }\n+            break;\n+          }\n           LOG.fatal(\"ReplicationMonitor thread received Runtime exception. \", t);\n           terminate(1, t);\n         }",
                "raw_url": "https://github.com/apache/hadoop-common/raw/f56d99c7eaec6c74dae50fdd3e2b369c73ecbfea/hadoop-hdfs-project/hadoop-hdfs/src/main/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockManager.java",
                "sha": "46809da5cb1dca4eaeaef0696b662f86e6bdd197",
                "status": "modified"
            }
        ],
        "message": "HDFS-4840. ReplicationMonitor gets NPE during shutdown. Contributed by Kihwal Lee.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1489634 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/60f27393764d63af7b14b66e106ee9055385873f",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestBlockManager.java"
        ]
    },
    "hadoop-common_fb7ece9": {
        "bug_id": "hadoop-common_fb7ece9",
        "commit": "https://github.com/apache/hadoop-common/commit/fb7ece969d62f859de9fdc18941b53b816760161",
        "file": [
            {
                "additions": 3,
                "blob_url": "https://github.com/apache/hadoop-common/blob/fb7ece969d62f859de9fdc18941b53b816760161/hadoop-yarn-project/CHANGES.txt",
                "changes": 3,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/CHANGES.txt?ref=fb7ece969d62f859de9fdc18941b53b816760161",
                "deletions": 0,
                "filename": "hadoop-yarn-project/CHANGES.txt",
                "patch": "@@ -126,6 +126,9 @@ Release 2.4.1 - UNRELEASED\n     YARN-1928. Fixed a race condition in TestAMRMRPCNodeUpdates which caused it\n     to fail occassionally. (Zhijie Shen via vinodkv)\n \n+    YARN-1934. Fixed a potential NPE in ZKRMStateStore caused by handling\n+    Disconnected event from ZK. (Karthik Kambatla via jianhe)\n+\n Release 2.4.0 - 2014-04-07 \n \n   INCOMPATIBLE CHANGES",
                "raw_url": "https://github.com/apache/hadoop-common/raw/fb7ece969d62f859de9fdc18941b53b816760161/hadoop-yarn-project/CHANGES.txt",
                "sha": "ed12a7bc8acf7df38decfbfa771b2a7dea07b881",
                "status": "modified"
            },
            {
                "additions": 31,
                "blob_url": "https://github.com/apache/hadoop-common/blob/fb7ece969d62f859de9fdc18941b53b816760161/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "changes": 42,
                "contents_url": "https://api.github.com/repos/apache/hadoop-common/contents/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java?ref=fb7ece969d62f859de9fdc18941b53b816760161",
                "deletions": 11,
                "filename": "hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "patch": "@@ -280,10 +280,9 @@ public String run() throws KeeperException, InterruptedException {\n     }\n   }\n \n-  private void logRootNodeAcls(String prefix) throws KeeperException,\n-      InterruptedException {\n+  private void logRootNodeAcls(String prefix) throws Exception {\n     Stat getStat = new Stat();\n-    List<ACL> getAcls = zkClient.getACL(zkRootNodePath, getStat);\n+    List<ACL> getAcls = getACLWithRetries(zkRootNodePath, getStat);\n \n     StringBuilder builder = new StringBuilder();\n     builder.append(prefix);\n@@ -363,7 +362,7 @@ protected synchronized void storeVersion() throws Exception {\n     String versionNodePath = getNodePath(zkRootNodePath, VERSION_NODE);\n     byte[] data =\n         ((RMStateVersionPBImpl) CURRENT_VERSION_INFO).getProto().toByteArray();\n-    if (zkClient.exists(versionNodePath, true) != null) {\n+    if (existsWithRetries(versionNodePath, true) != null) {\n       setDataWithRetries(versionNodePath, data, -1);\n     } else {\n       createWithRetries(versionNodePath, data, zkAcl, CreateMode.PERSISTENT);\n@@ -374,7 +373,7 @@ protected synchronized void storeVersion() throws Exception {\n   protected synchronized RMStateVersion loadVersion() throws Exception {\n     String versionNodePath = getNodePath(zkRootNodePath, VERSION_NODE);\n \n-    if (zkClient.exists(versionNodePath, true) != null) {\n+    if (existsWithRetries(versionNodePath, true) != null) {\n       byte[] data = getDataWithRetries(versionNodePath, true);\n       RMStateVersion version =\n           new RMStateVersionPBImpl(RMStateVersionProto.parseFrom(data));\n@@ -442,7 +441,8 @@ private void loadRMSequentialNumberState(RMState rmState) throws Exception {\n   }\n \n   private void loadRMDelegationTokenState(RMState rmState) throws Exception {\n-    List<String> childNodes = zkClient.getChildren(delegationTokensRootPath, true);\n+    List<String> childNodes =\n+        getChildrenWithRetries(delegationTokensRootPath, true);\n     for (String childNodeName : childNodes) {\n       String childNodePath =\n           getNodePath(delegationTokensRootPath, childNodeName);\n@@ -567,7 +567,7 @@ public synchronized void updateApplicationStateInternal(ApplicationId appId,\n     }\n     byte[] appStateData = appStateDataPB.getProto().toByteArray();\n \n-    if (zkClient.exists(nodeUpdatePath, true) != null) {\n+    if (existsWithRetries(nodeUpdatePath, true) != null) {\n       setDataWithRetries(nodeUpdatePath, appStateData, -1);\n     } else {\n       createWithRetries(nodeUpdatePath, appStateData, zkAcl,\n@@ -610,7 +610,7 @@ public synchronized void updateApplicationAttemptStateInternal(\n     }\n     byte[] attemptStateData = attemptStateDataPB.getProto().toByteArray();\n \n-    if (zkClient.exists(nodeUpdatePath, true) != null) {\n+    if (existsWithRetries(nodeUpdatePath, true) != null) {\n       setDataWithRetries(nodeUpdatePath, attemptStateData, -1);\n     } else {\n       createWithRetries(nodeUpdatePath, attemptStateData, zkAcl,\n@@ -661,7 +661,7 @@ protected synchronized void removeRMDelegationTokenState(\n       LOG.debug(\"Removing RMDelegationToken_\"\n           + rmDTIdentifier.getSequenceNumber());\n     }\n-    if (zkClient.exists(nodeRemovePath, true) != null) {\n+    if (existsWithRetries(nodeRemovePath, true) != null) {\n       opList.add(Op.delete(nodeRemovePath, -1));\n     } else {\n       LOG.info(\"Attempted to delete a non-existing znode \" + nodeRemovePath);\n@@ -677,7 +677,7 @@ protected void updateRMDelegationTokenAndSequenceNumberInternal(\n     String nodeRemovePath =\n         getNodePath(delegationTokensRootPath, DELEGATION_TOKEN_PREFIX\n             + rmDTIdentifier.getSequenceNumber());\n-    if (zkClient.exists(nodeRemovePath, true) == null) {\n+    if (existsWithRetries(nodeRemovePath, true) == null) {\n       // in case znode doesn't exist\n       addStoreOrUpdateOps(\n           opList, rmDTIdentifier, renewDate, latestSequenceNumber, false);\n@@ -760,7 +760,7 @@ protected synchronized void removeRMDTMasterKeyState(\n     if (LOG.isDebugEnabled()) {\n       LOG.debug(\"Removing RMDelegationKey_\" + delegationKey.getKeyId());\n     }\n-    if (zkClient.exists(nodeRemovePath, true) != null) {\n+    if (existsWithRetries(nodeRemovePath, true) != null) {\n       doMultiWithRetries(Op.delete(nodeRemovePath, -1));\n     } else {\n       LOG.info(\"Attempted to delete a non-existing znode \" + nodeRemovePath);\n@@ -891,6 +891,16 @@ public void setDataWithRetries(final String path, final byte[] data,\n     }.runWithRetries();\n   }\n \n+  private List<ACL> getACLWithRetries(\n+      final String path, final Stat stat) throws Exception {\n+    return new ZKAction<List<ACL>>() {\n+      @Override\n+      public List<ACL> run() throws KeeperException, InterruptedException {\n+        return zkClient.getACL(path, stat);\n+      }\n+    }.runWithRetries();\n+  }\n+\n   private List<String> getChildrenWithRetries(\n       final String path, final boolean watch) throws Exception {\n     return new ZKAction<List<String>>() {\n@@ -901,6 +911,16 @@ public void setDataWithRetries(final String path, final byte[] data,\n     }.runWithRetries();\n   }\n \n+  private Stat existsWithRetries(\n+      final String path, final boolean watch) throws Exception {\n+    return new ZKAction<Stat>() {\n+      @Override\n+      Stat run() throws KeeperException, InterruptedException {\n+        return zkClient.exists(path, watch);\n+      }\n+    }.runWithRetries();\n+  }\n+\n   /**\n    * Helper class that periodically attempts creating a znode to ensure that\n    * this RM continues to be the Active.",
                "raw_url": "https://github.com/apache/hadoop-common/raw/fb7ece969d62f859de9fdc18941b53b816760161/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/src/main/java/org/apache/hadoop/yarn/server/resourcemanager/recovery/ZKRMStateStore.java",
                "sha": "9b15bb21e7e62a1dab06a6799dd5a39621c06a4f",
                "status": "modified"
            }
        ],
        "message": "YARN-1934. Fixed a potential NPE in ZKRMStateStore caused by handling Disconnected event from ZK. Contributed by Karthik Kambatla.\n\ngit-svn-id: https://svn.apache.org/repos/asf/hadoop/common/trunk@1587776 13f79535-47bb-0310-9956-ffa450edef68",
        "parent": "https://github.com/apache/hadoop-common/commit/31f22748b38fda8442888c8991ba541d7590dbaa",
        "repo": "hadoop-common",
        "unit_tests": [
            "TestZKRMStateStore.java"
        ]
    }
}